Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=168, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9408-9463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025204
Iteration 2/25 | Loss: 0.00168414
Iteration 3/25 | Loss: 0.00142663
Iteration 4/25 | Loss: 0.00139937
Iteration 5/25 | Loss: 0.00139440
Iteration 6/25 | Loss: 0.00139440
Iteration 7/25 | Loss: 0.00139440
Iteration 8/25 | Loss: 0.00139440
Iteration 9/25 | Loss: 0.00139440
Iteration 10/25 | Loss: 0.00139440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013944029342383146, 0.0013944029342383146, 0.0013944029342383146, 0.0013944029342383146, 0.0013944029342383146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013944029342383146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.50853634
Iteration 2/25 | Loss: 0.00197333
Iteration 3/25 | Loss: 0.00197331
Iteration 4/25 | Loss: 0.00197331
Iteration 5/25 | Loss: 0.00197331
Iteration 6/25 | Loss: 0.00197331
Iteration 7/25 | Loss: 0.00197331
Iteration 8/25 | Loss: 0.00197331
Iteration 9/25 | Loss: 0.00197331
Iteration 10/25 | Loss: 0.00197331
Iteration 11/25 | Loss: 0.00197331
Iteration 12/25 | Loss: 0.00197331
Iteration 13/25 | Loss: 0.00197331
Iteration 14/25 | Loss: 0.00197331
Iteration 15/25 | Loss: 0.00197331
Iteration 16/25 | Loss: 0.00197331
Iteration 17/25 | Loss: 0.00197331
Iteration 18/25 | Loss: 0.00197331
Iteration 19/25 | Loss: 0.00197331
Iteration 20/25 | Loss: 0.00197331
Iteration 21/25 | Loss: 0.00197331
Iteration 22/25 | Loss: 0.00197331
Iteration 23/25 | Loss: 0.00197331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001973310485482216, 0.001973310485482216, 0.001973310485482216, 0.001973310485482216, 0.001973310485482216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001973310485482216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197331
Iteration 2/1000 | Loss: 0.00007189
Iteration 3/1000 | Loss: 0.00004144
Iteration 4/1000 | Loss: 0.00003277
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00002964
Iteration 7/1000 | Loss: 0.00002872
Iteration 8/1000 | Loss: 0.00002819
Iteration 9/1000 | Loss: 0.00002783
Iteration 10/1000 | Loss: 0.00002748
Iteration 11/1000 | Loss: 0.00002720
Iteration 12/1000 | Loss: 0.00002694
Iteration 13/1000 | Loss: 0.00002675
Iteration 14/1000 | Loss: 0.00002662
Iteration 15/1000 | Loss: 0.00002661
Iteration 16/1000 | Loss: 0.00002649
Iteration 17/1000 | Loss: 0.00002646
Iteration 18/1000 | Loss: 0.00002646
Iteration 19/1000 | Loss: 0.00002641
Iteration 20/1000 | Loss: 0.00002638
Iteration 21/1000 | Loss: 0.00002638
Iteration 22/1000 | Loss: 0.00002638
Iteration 23/1000 | Loss: 0.00002638
Iteration 24/1000 | Loss: 0.00002638
Iteration 25/1000 | Loss: 0.00002638
Iteration 26/1000 | Loss: 0.00002638
Iteration 27/1000 | Loss: 0.00002638
Iteration 28/1000 | Loss: 0.00002638
Iteration 29/1000 | Loss: 0.00002638
Iteration 30/1000 | Loss: 0.00002638
Iteration 31/1000 | Loss: 0.00002638
Iteration 32/1000 | Loss: 0.00002637
Iteration 33/1000 | Loss: 0.00002637
Iteration 34/1000 | Loss: 0.00002637
Iteration 35/1000 | Loss: 0.00002637
Iteration 36/1000 | Loss: 0.00002637
Iteration 37/1000 | Loss: 0.00002637
Iteration 38/1000 | Loss: 0.00002637
Iteration 39/1000 | Loss: 0.00002636
Iteration 40/1000 | Loss: 0.00002635
Iteration 41/1000 | Loss: 0.00002635
Iteration 42/1000 | Loss: 0.00002635
Iteration 43/1000 | Loss: 0.00002635
Iteration 44/1000 | Loss: 0.00002634
Iteration 45/1000 | Loss: 0.00002634
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002633
Iteration 48/1000 | Loss: 0.00002633
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002632
Iteration 51/1000 | Loss: 0.00002632
Iteration 52/1000 | Loss: 0.00002632
Iteration 53/1000 | Loss: 0.00002632
Iteration 54/1000 | Loss: 0.00002632
Iteration 55/1000 | Loss: 0.00002632
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002632
Iteration 58/1000 | Loss: 0.00002632
Iteration 59/1000 | Loss: 0.00002631
Iteration 60/1000 | Loss: 0.00002631
Iteration 61/1000 | Loss: 0.00002631
Iteration 62/1000 | Loss: 0.00002631
Iteration 63/1000 | Loss: 0.00002631
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002630
Iteration 76/1000 | Loss: 0.00002629
Iteration 77/1000 | Loss: 0.00002629
Iteration 78/1000 | Loss: 0.00002629
Iteration 79/1000 | Loss: 0.00002629
Iteration 80/1000 | Loss: 0.00002628
Iteration 81/1000 | Loss: 0.00002628
Iteration 82/1000 | Loss: 0.00002628
Iteration 83/1000 | Loss: 0.00002628
Iteration 84/1000 | Loss: 0.00002627
Iteration 85/1000 | Loss: 0.00002627
Iteration 86/1000 | Loss: 0.00002627
Iteration 87/1000 | Loss: 0.00002626
Iteration 88/1000 | Loss: 0.00002626
Iteration 89/1000 | Loss: 0.00002626
Iteration 90/1000 | Loss: 0.00002626
Iteration 91/1000 | Loss: 0.00002625
Iteration 92/1000 | Loss: 0.00002625
Iteration 93/1000 | Loss: 0.00002625
Iteration 94/1000 | Loss: 0.00002625
Iteration 95/1000 | Loss: 0.00002624
Iteration 96/1000 | Loss: 0.00002623
Iteration 97/1000 | Loss: 0.00002623
Iteration 98/1000 | Loss: 0.00002623
Iteration 99/1000 | Loss: 0.00002622
Iteration 100/1000 | Loss: 0.00002622
Iteration 101/1000 | Loss: 0.00002622
Iteration 102/1000 | Loss: 0.00002622
Iteration 103/1000 | Loss: 0.00002622
Iteration 104/1000 | Loss: 0.00002622
Iteration 105/1000 | Loss: 0.00002622
Iteration 106/1000 | Loss: 0.00002622
Iteration 107/1000 | Loss: 0.00002622
Iteration 108/1000 | Loss: 0.00002622
Iteration 109/1000 | Loss: 0.00002621
Iteration 110/1000 | Loss: 0.00002621
Iteration 111/1000 | Loss: 0.00002621
Iteration 112/1000 | Loss: 0.00002620
Iteration 113/1000 | Loss: 0.00002620
Iteration 114/1000 | Loss: 0.00002620
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002619
Iteration 117/1000 | Loss: 0.00002619
Iteration 118/1000 | Loss: 0.00002619
Iteration 119/1000 | Loss: 0.00002619
Iteration 120/1000 | Loss: 0.00002619
Iteration 121/1000 | Loss: 0.00002619
Iteration 122/1000 | Loss: 0.00002619
Iteration 123/1000 | Loss: 0.00002619
Iteration 124/1000 | Loss: 0.00002619
Iteration 125/1000 | Loss: 0.00002618
Iteration 126/1000 | Loss: 0.00002618
Iteration 127/1000 | Loss: 0.00002618
Iteration 128/1000 | Loss: 0.00002618
Iteration 129/1000 | Loss: 0.00002617
Iteration 130/1000 | Loss: 0.00002617
Iteration 131/1000 | Loss: 0.00002617
Iteration 132/1000 | Loss: 0.00002617
Iteration 133/1000 | Loss: 0.00002617
Iteration 134/1000 | Loss: 0.00002617
Iteration 135/1000 | Loss: 0.00002617
Iteration 136/1000 | Loss: 0.00002617
Iteration 137/1000 | Loss: 0.00002617
Iteration 138/1000 | Loss: 0.00002616
Iteration 139/1000 | Loss: 0.00002616
Iteration 140/1000 | Loss: 0.00002616
Iteration 141/1000 | Loss: 0.00002616
Iteration 142/1000 | Loss: 0.00002616
Iteration 143/1000 | Loss: 0.00002616
Iteration 144/1000 | Loss: 0.00002616
Iteration 145/1000 | Loss: 0.00002616
Iteration 146/1000 | Loss: 0.00002616
Iteration 147/1000 | Loss: 0.00002616
Iteration 148/1000 | Loss: 0.00002616
Iteration 149/1000 | Loss: 0.00002616
Iteration 150/1000 | Loss: 0.00002616
Iteration 151/1000 | Loss: 0.00002616
Iteration 152/1000 | Loss: 0.00002615
Iteration 153/1000 | Loss: 0.00002615
Iteration 154/1000 | Loss: 0.00002615
Iteration 155/1000 | Loss: 0.00002615
Iteration 156/1000 | Loss: 0.00002615
Iteration 157/1000 | Loss: 0.00002615
Iteration 158/1000 | Loss: 0.00002615
Iteration 159/1000 | Loss: 0.00002615
Iteration 160/1000 | Loss: 0.00002615
Iteration 161/1000 | Loss: 0.00002615
Iteration 162/1000 | Loss: 0.00002615
Iteration 163/1000 | Loss: 0.00002615
Iteration 164/1000 | Loss: 0.00002614
Iteration 165/1000 | Loss: 0.00002614
Iteration 166/1000 | Loss: 0.00002614
Iteration 167/1000 | Loss: 0.00002614
Iteration 168/1000 | Loss: 0.00002614
Iteration 169/1000 | Loss: 0.00002614
Iteration 170/1000 | Loss: 0.00002614
Iteration 171/1000 | Loss: 0.00002614
Iteration 172/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.6142717615584843e-05, 2.6142717615584843e-05, 2.6142717615584843e-05, 2.6142717615584843e-05, 2.6142717615584843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6142717615584843e-05

Optimization complete. Final v2v error: 4.291841506958008 mm

Highest mean error: 5.186379432678223 mm for frame 195

Lowest mean error: 3.7145352363586426 mm for frame 65

Saving results

Total time: 47.232099533081055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413103
Iteration 2/25 | Loss: 0.00132001
Iteration 3/25 | Loss: 0.00124878
Iteration 4/25 | Loss: 0.00123933
Iteration 5/25 | Loss: 0.00123703
Iteration 6/25 | Loss: 0.00123658
Iteration 7/25 | Loss: 0.00123658
Iteration 8/25 | Loss: 0.00123658
Iteration 9/25 | Loss: 0.00123658
Iteration 10/25 | Loss: 0.00123658
Iteration 11/25 | Loss: 0.00123658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012365813599899411, 0.0012365813599899411, 0.0012365813599899411, 0.0012365813599899411, 0.0012365813599899411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012365813599899411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.62379265
Iteration 2/25 | Loss: 0.00201081
Iteration 3/25 | Loss: 0.00201080
Iteration 4/25 | Loss: 0.00201079
Iteration 5/25 | Loss: 0.00201079
Iteration 6/25 | Loss: 0.00201079
Iteration 7/25 | Loss: 0.00201079
Iteration 8/25 | Loss: 0.00201079
Iteration 9/25 | Loss: 0.00201079
Iteration 10/25 | Loss: 0.00201079
Iteration 11/25 | Loss: 0.00201079
Iteration 12/25 | Loss: 0.00201079
Iteration 13/25 | Loss: 0.00201079
Iteration 14/25 | Loss: 0.00201079
Iteration 15/25 | Loss: 0.00201079
Iteration 16/25 | Loss: 0.00201079
Iteration 17/25 | Loss: 0.00201079
Iteration 18/25 | Loss: 0.00201079
Iteration 19/25 | Loss: 0.00201079
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002010791329666972, 0.002010791329666972, 0.002010791329666972, 0.002010791329666972, 0.002010791329666972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002010791329666972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201079
Iteration 2/1000 | Loss: 0.00002458
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001159
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001098
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001091
Iteration 35/1000 | Loss: 0.00001088
Iteration 36/1000 | Loss: 0.00001088
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001084
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001082
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001080
Iteration 46/1000 | Loss: 0.00001080
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001074
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001070
Iteration 77/1000 | Loss: 0.00001070
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001069
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001067
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001066
Iteration 91/1000 | Loss: 0.00001066
Iteration 92/1000 | Loss: 0.00001066
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001063
Iteration 106/1000 | Loss: 0.00001063
Iteration 107/1000 | Loss: 0.00001063
Iteration 108/1000 | Loss: 0.00001063
Iteration 109/1000 | Loss: 0.00001063
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001062
Iteration 112/1000 | Loss: 0.00001062
Iteration 113/1000 | Loss: 0.00001062
Iteration 114/1000 | Loss: 0.00001062
Iteration 115/1000 | Loss: 0.00001062
Iteration 116/1000 | Loss: 0.00001062
Iteration 117/1000 | Loss: 0.00001062
Iteration 118/1000 | Loss: 0.00001062
Iteration 119/1000 | Loss: 0.00001062
Iteration 120/1000 | Loss: 0.00001062
Iteration 121/1000 | Loss: 0.00001062
Iteration 122/1000 | Loss: 0.00001062
Iteration 123/1000 | Loss: 0.00001061
Iteration 124/1000 | Loss: 0.00001061
Iteration 125/1000 | Loss: 0.00001061
Iteration 126/1000 | Loss: 0.00001061
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001061
Iteration 132/1000 | Loss: 0.00001060
Iteration 133/1000 | Loss: 0.00001060
Iteration 134/1000 | Loss: 0.00001060
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001059
Iteration 139/1000 | Loss: 0.00001059
Iteration 140/1000 | Loss: 0.00001059
Iteration 141/1000 | Loss: 0.00001059
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001058
Iteration 148/1000 | Loss: 0.00001058
Iteration 149/1000 | Loss: 0.00001058
Iteration 150/1000 | Loss: 0.00001058
Iteration 151/1000 | Loss: 0.00001058
Iteration 152/1000 | Loss: 0.00001058
Iteration 153/1000 | Loss: 0.00001058
Iteration 154/1000 | Loss: 0.00001058
Iteration 155/1000 | Loss: 0.00001058
Iteration 156/1000 | Loss: 0.00001058
Iteration 157/1000 | Loss: 0.00001058
Iteration 158/1000 | Loss: 0.00001058
Iteration 159/1000 | Loss: 0.00001058
Iteration 160/1000 | Loss: 0.00001058
Iteration 161/1000 | Loss: 0.00001058
Iteration 162/1000 | Loss: 0.00001058
Iteration 163/1000 | Loss: 0.00001058
Iteration 164/1000 | Loss: 0.00001058
Iteration 165/1000 | Loss: 0.00001057
Iteration 166/1000 | Loss: 0.00001057
Iteration 167/1000 | Loss: 0.00001057
Iteration 168/1000 | Loss: 0.00001057
Iteration 169/1000 | Loss: 0.00001057
Iteration 170/1000 | Loss: 0.00001057
Iteration 171/1000 | Loss: 0.00001057
Iteration 172/1000 | Loss: 0.00001057
Iteration 173/1000 | Loss: 0.00001057
Iteration 174/1000 | Loss: 0.00001057
Iteration 175/1000 | Loss: 0.00001057
Iteration 176/1000 | Loss: 0.00001057
Iteration 177/1000 | Loss: 0.00001057
Iteration 178/1000 | Loss: 0.00001057
Iteration 179/1000 | Loss: 0.00001057
Iteration 180/1000 | Loss: 0.00001056
Iteration 181/1000 | Loss: 0.00001056
Iteration 182/1000 | Loss: 0.00001056
Iteration 183/1000 | Loss: 0.00001056
Iteration 184/1000 | Loss: 0.00001056
Iteration 185/1000 | Loss: 0.00001056
Iteration 186/1000 | Loss: 0.00001056
Iteration 187/1000 | Loss: 0.00001056
Iteration 188/1000 | Loss: 0.00001056
Iteration 189/1000 | Loss: 0.00001056
Iteration 190/1000 | Loss: 0.00001056
Iteration 191/1000 | Loss: 0.00001056
Iteration 192/1000 | Loss: 0.00001056
Iteration 193/1000 | Loss: 0.00001056
Iteration 194/1000 | Loss: 0.00001056
Iteration 195/1000 | Loss: 0.00001055
Iteration 196/1000 | Loss: 0.00001055
Iteration 197/1000 | Loss: 0.00001055
Iteration 198/1000 | Loss: 0.00001055
Iteration 199/1000 | Loss: 0.00001055
Iteration 200/1000 | Loss: 0.00001055
Iteration 201/1000 | Loss: 0.00001055
Iteration 202/1000 | Loss: 0.00001055
Iteration 203/1000 | Loss: 0.00001055
Iteration 204/1000 | Loss: 0.00001055
Iteration 205/1000 | Loss: 0.00001055
Iteration 206/1000 | Loss: 0.00001055
Iteration 207/1000 | Loss: 0.00001055
Iteration 208/1000 | Loss: 0.00001055
Iteration 209/1000 | Loss: 0.00001054
Iteration 210/1000 | Loss: 0.00001054
Iteration 211/1000 | Loss: 0.00001054
Iteration 212/1000 | Loss: 0.00001054
Iteration 213/1000 | Loss: 0.00001054
Iteration 214/1000 | Loss: 0.00001054
Iteration 215/1000 | Loss: 0.00001054
Iteration 216/1000 | Loss: 0.00001054
Iteration 217/1000 | Loss: 0.00001054
Iteration 218/1000 | Loss: 0.00001054
Iteration 219/1000 | Loss: 0.00001054
Iteration 220/1000 | Loss: 0.00001054
Iteration 221/1000 | Loss: 0.00001054
Iteration 222/1000 | Loss: 0.00001054
Iteration 223/1000 | Loss: 0.00001053
Iteration 224/1000 | Loss: 0.00001053
Iteration 225/1000 | Loss: 0.00001053
Iteration 226/1000 | Loss: 0.00001053
Iteration 227/1000 | Loss: 0.00001053
Iteration 228/1000 | Loss: 0.00001053
Iteration 229/1000 | Loss: 0.00001053
Iteration 230/1000 | Loss: 0.00001053
Iteration 231/1000 | Loss: 0.00001053
Iteration 232/1000 | Loss: 0.00001053
Iteration 233/1000 | Loss: 0.00001053
Iteration 234/1000 | Loss: 0.00001053
Iteration 235/1000 | Loss: 0.00001053
Iteration 236/1000 | Loss: 0.00001053
Iteration 237/1000 | Loss: 0.00001053
Iteration 238/1000 | Loss: 0.00001053
Iteration 239/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.0531132829783019e-05, 1.0531132829783019e-05, 1.0531132829783019e-05, 1.0531132829783019e-05, 1.0531132829783019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0531132829783019e-05

Optimization complete. Final v2v error: 2.808612823486328 mm

Highest mean error: 3.4186148643493652 mm for frame 85

Lowest mean error: 2.524928092956543 mm for frame 42

Saving results

Total time: 44.00281739234924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938240
Iteration 2/25 | Loss: 0.00326158
Iteration 3/25 | Loss: 0.00225772
Iteration 4/25 | Loss: 0.00179394
Iteration 5/25 | Loss: 0.00176067
Iteration 6/25 | Loss: 0.00172119
Iteration 7/25 | Loss: 0.00169984
Iteration 8/25 | Loss: 0.00165787
Iteration 9/25 | Loss: 0.00162753
Iteration 10/25 | Loss: 0.00160861
Iteration 11/25 | Loss: 0.00160651
Iteration 12/25 | Loss: 0.00160610
Iteration 13/25 | Loss: 0.00160030
Iteration 14/25 | Loss: 0.00159253
Iteration 15/25 | Loss: 0.00158727
Iteration 16/25 | Loss: 0.00158695
Iteration 17/25 | Loss: 0.00158340
Iteration 18/25 | Loss: 0.00158444
Iteration 19/25 | Loss: 0.00158464
Iteration 20/25 | Loss: 0.00157857
Iteration 21/25 | Loss: 0.00157985
Iteration 22/25 | Loss: 0.00157680
Iteration 23/25 | Loss: 0.00157251
Iteration 24/25 | Loss: 0.00157141
Iteration 25/25 | Loss: 0.00157098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.18881273
Iteration 2/25 | Loss: 0.00546489
Iteration 3/25 | Loss: 0.00510812
Iteration 4/25 | Loss: 0.00510812
Iteration 5/25 | Loss: 0.00510812
Iteration 6/25 | Loss: 0.00510812
Iteration 7/25 | Loss: 0.00510812
Iteration 8/25 | Loss: 0.00510811
Iteration 9/25 | Loss: 0.00510811
Iteration 10/25 | Loss: 0.00510811
Iteration 11/25 | Loss: 0.00510811
Iteration 12/25 | Loss: 0.00510811
Iteration 13/25 | Loss: 0.00510811
Iteration 14/25 | Loss: 0.00510811
Iteration 15/25 | Loss: 0.00510811
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0051081134006381035, 0.0051081134006381035, 0.0051081134006381035, 0.0051081134006381035, 0.0051081134006381035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0051081134006381035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00510811
Iteration 2/1000 | Loss: 0.00106988
Iteration 3/1000 | Loss: 0.00082021
Iteration 4/1000 | Loss: 0.00026371
Iteration 5/1000 | Loss: 0.00086790
Iteration 6/1000 | Loss: 0.00044557
Iteration 7/1000 | Loss: 0.00040278
Iteration 8/1000 | Loss: 0.00021045
Iteration 9/1000 | Loss: 0.00020680
Iteration 10/1000 | Loss: 0.00044791
Iteration 11/1000 | Loss: 0.00027451
Iteration 12/1000 | Loss: 0.00069369
Iteration 13/1000 | Loss: 0.00293022
Iteration 14/1000 | Loss: 0.00227721
Iteration 15/1000 | Loss: 0.00028584
Iteration 16/1000 | Loss: 0.00047713
Iteration 17/1000 | Loss: 0.00037706
Iteration 18/1000 | Loss: 0.00120164
Iteration 19/1000 | Loss: 0.00092661
Iteration 20/1000 | Loss: 0.00018098
Iteration 21/1000 | Loss: 0.00037825
Iteration 22/1000 | Loss: 0.00027187
Iteration 23/1000 | Loss: 0.00025562
Iteration 24/1000 | Loss: 0.00013374
Iteration 25/1000 | Loss: 0.00019502
Iteration 26/1000 | Loss: 0.00019160
Iteration 27/1000 | Loss: 0.00019242
Iteration 28/1000 | Loss: 0.00023597
Iteration 29/1000 | Loss: 0.00013826
Iteration 30/1000 | Loss: 0.00045204
Iteration 31/1000 | Loss: 0.00017913
Iteration 32/1000 | Loss: 0.00018241
Iteration 33/1000 | Loss: 0.00030182
Iteration 34/1000 | Loss: 0.00078327
Iteration 35/1000 | Loss: 0.00010091
Iteration 36/1000 | Loss: 0.00009271
Iteration 37/1000 | Loss: 0.00008812
Iteration 38/1000 | Loss: 0.00015497
Iteration 39/1000 | Loss: 0.00008601
Iteration 40/1000 | Loss: 0.00019000
Iteration 41/1000 | Loss: 0.00008409
Iteration 42/1000 | Loss: 0.00025056
Iteration 43/1000 | Loss: 0.00081172
Iteration 44/1000 | Loss: 0.00222416
Iteration 45/1000 | Loss: 0.00048892
Iteration 46/1000 | Loss: 0.00108330
Iteration 47/1000 | Loss: 0.00041824
Iteration 48/1000 | Loss: 0.00058073
Iteration 49/1000 | Loss: 0.00010633
Iteration 50/1000 | Loss: 0.00008872
Iteration 51/1000 | Loss: 0.00012568
Iteration 52/1000 | Loss: 0.00008115
Iteration 53/1000 | Loss: 0.00049259
Iteration 54/1000 | Loss: 0.00019942
Iteration 55/1000 | Loss: 0.00050299
Iteration 56/1000 | Loss: 0.00008131
Iteration 57/1000 | Loss: 0.00050167
Iteration 58/1000 | Loss: 0.00007847
Iteration 59/1000 | Loss: 0.00007271
Iteration 60/1000 | Loss: 0.00106512
Iteration 61/1000 | Loss: 0.00051473
Iteration 62/1000 | Loss: 0.00007865
Iteration 63/1000 | Loss: 0.00079694
Iteration 64/1000 | Loss: 0.00049200
Iteration 65/1000 | Loss: 0.00007107
Iteration 66/1000 | Loss: 0.00048516
Iteration 67/1000 | Loss: 0.00006534
Iteration 68/1000 | Loss: 0.00049080
Iteration 69/1000 | Loss: 0.00006395
Iteration 70/1000 | Loss: 0.00100158
Iteration 71/1000 | Loss: 0.00091937
Iteration 72/1000 | Loss: 0.00049698
Iteration 73/1000 | Loss: 0.00037711
Iteration 74/1000 | Loss: 0.00005526
Iteration 75/1000 | Loss: 0.00004770
Iteration 76/1000 | Loss: 0.00004407
Iteration 77/1000 | Loss: 0.00004158
Iteration 78/1000 | Loss: 0.00033859
Iteration 79/1000 | Loss: 0.00036563
Iteration 80/1000 | Loss: 0.00004619
Iteration 81/1000 | Loss: 0.00004034
Iteration 82/1000 | Loss: 0.00003742
Iteration 83/1000 | Loss: 0.00003625
Iteration 84/1000 | Loss: 0.00003510
Iteration 85/1000 | Loss: 0.00033456
Iteration 86/1000 | Loss: 0.00003942
Iteration 87/1000 | Loss: 0.00003471
Iteration 88/1000 | Loss: 0.00003319
Iteration 89/1000 | Loss: 0.00003233
Iteration 90/1000 | Loss: 0.00003163
Iteration 91/1000 | Loss: 0.00003127
Iteration 92/1000 | Loss: 0.00003107
Iteration 93/1000 | Loss: 0.00003082
Iteration 94/1000 | Loss: 0.00034483
Iteration 95/1000 | Loss: 0.00003222
Iteration 96/1000 | Loss: 0.00003042
Iteration 97/1000 | Loss: 0.00002958
Iteration 98/1000 | Loss: 0.00002879
Iteration 99/1000 | Loss: 0.00002837
Iteration 100/1000 | Loss: 0.00002814
Iteration 101/1000 | Loss: 0.00002798
Iteration 102/1000 | Loss: 0.00002780
Iteration 103/1000 | Loss: 0.00002780
Iteration 104/1000 | Loss: 0.00002778
Iteration 105/1000 | Loss: 0.00002777
Iteration 106/1000 | Loss: 0.00002777
Iteration 107/1000 | Loss: 0.00002772
Iteration 108/1000 | Loss: 0.00002772
Iteration 109/1000 | Loss: 0.00028635
Iteration 110/1000 | Loss: 0.00021451
Iteration 111/1000 | Loss: 0.00027567
Iteration 112/1000 | Loss: 0.00005821
Iteration 113/1000 | Loss: 0.00002731
Iteration 114/1000 | Loss: 0.00002677
Iteration 115/1000 | Loss: 0.00002626
Iteration 116/1000 | Loss: 0.00002587
Iteration 117/1000 | Loss: 0.00002569
Iteration 118/1000 | Loss: 0.00002556
Iteration 119/1000 | Loss: 0.00002554
Iteration 120/1000 | Loss: 0.00002552
Iteration 121/1000 | Loss: 0.00002548
Iteration 122/1000 | Loss: 0.00002548
Iteration 123/1000 | Loss: 0.00002544
Iteration 124/1000 | Loss: 0.00002543
Iteration 125/1000 | Loss: 0.00002542
Iteration 126/1000 | Loss: 0.00002541
Iteration 127/1000 | Loss: 0.00002540
Iteration 128/1000 | Loss: 0.00002540
Iteration 129/1000 | Loss: 0.00002537
Iteration 130/1000 | Loss: 0.00002537
Iteration 131/1000 | Loss: 0.00002536
Iteration 132/1000 | Loss: 0.00002536
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002535
Iteration 135/1000 | Loss: 0.00002535
Iteration 136/1000 | Loss: 0.00002534
Iteration 137/1000 | Loss: 0.00002534
Iteration 138/1000 | Loss: 0.00002534
Iteration 139/1000 | Loss: 0.00002534
Iteration 140/1000 | Loss: 0.00002534
Iteration 141/1000 | Loss: 0.00002534
Iteration 142/1000 | Loss: 0.00002534
Iteration 143/1000 | Loss: 0.00002534
Iteration 144/1000 | Loss: 0.00002534
Iteration 145/1000 | Loss: 0.00002534
Iteration 146/1000 | Loss: 0.00002533
Iteration 147/1000 | Loss: 0.00002533
Iteration 148/1000 | Loss: 0.00002533
Iteration 149/1000 | Loss: 0.00002531
Iteration 150/1000 | Loss: 0.00002531
Iteration 151/1000 | Loss: 0.00002530
Iteration 152/1000 | Loss: 0.00002529
Iteration 153/1000 | Loss: 0.00002528
Iteration 154/1000 | Loss: 0.00002528
Iteration 155/1000 | Loss: 0.00002528
Iteration 156/1000 | Loss: 0.00002528
Iteration 157/1000 | Loss: 0.00002528
Iteration 158/1000 | Loss: 0.00002528
Iteration 159/1000 | Loss: 0.00002528
Iteration 160/1000 | Loss: 0.00002528
Iteration 161/1000 | Loss: 0.00002527
Iteration 162/1000 | Loss: 0.00002527
Iteration 163/1000 | Loss: 0.00002527
Iteration 164/1000 | Loss: 0.00002526
Iteration 165/1000 | Loss: 0.00002526
Iteration 166/1000 | Loss: 0.00002526
Iteration 167/1000 | Loss: 0.00002526
Iteration 168/1000 | Loss: 0.00002526
Iteration 169/1000 | Loss: 0.00002526
Iteration 170/1000 | Loss: 0.00002526
Iteration 171/1000 | Loss: 0.00002526
Iteration 172/1000 | Loss: 0.00002526
Iteration 173/1000 | Loss: 0.00002526
Iteration 174/1000 | Loss: 0.00002525
Iteration 175/1000 | Loss: 0.00002525
Iteration 176/1000 | Loss: 0.00002525
Iteration 177/1000 | Loss: 0.00002525
Iteration 178/1000 | Loss: 0.00002525
Iteration 179/1000 | Loss: 0.00002524
Iteration 180/1000 | Loss: 0.00002524
Iteration 181/1000 | Loss: 0.00002524
Iteration 182/1000 | Loss: 0.00002524
Iteration 183/1000 | Loss: 0.00002524
Iteration 184/1000 | Loss: 0.00002524
Iteration 185/1000 | Loss: 0.00002524
Iteration 186/1000 | Loss: 0.00002524
Iteration 187/1000 | Loss: 0.00002524
Iteration 188/1000 | Loss: 0.00002524
Iteration 189/1000 | Loss: 0.00002524
Iteration 190/1000 | Loss: 0.00002523
Iteration 191/1000 | Loss: 0.00002523
Iteration 192/1000 | Loss: 0.00002523
Iteration 193/1000 | Loss: 0.00002523
Iteration 194/1000 | Loss: 0.00002523
Iteration 195/1000 | Loss: 0.00002523
Iteration 196/1000 | Loss: 0.00002523
Iteration 197/1000 | Loss: 0.00002522
Iteration 198/1000 | Loss: 0.00002522
Iteration 199/1000 | Loss: 0.00002522
Iteration 200/1000 | Loss: 0.00002522
Iteration 201/1000 | Loss: 0.00002522
Iteration 202/1000 | Loss: 0.00002522
Iteration 203/1000 | Loss: 0.00002522
Iteration 204/1000 | Loss: 0.00002522
Iteration 205/1000 | Loss: 0.00002522
Iteration 206/1000 | Loss: 0.00002522
Iteration 207/1000 | Loss: 0.00002522
Iteration 208/1000 | Loss: 0.00002521
Iteration 209/1000 | Loss: 0.00002521
Iteration 210/1000 | Loss: 0.00002521
Iteration 211/1000 | Loss: 0.00002521
Iteration 212/1000 | Loss: 0.00002521
Iteration 213/1000 | Loss: 0.00002521
Iteration 214/1000 | Loss: 0.00002521
Iteration 215/1000 | Loss: 0.00002521
Iteration 216/1000 | Loss: 0.00002521
Iteration 217/1000 | Loss: 0.00002521
Iteration 218/1000 | Loss: 0.00002521
Iteration 219/1000 | Loss: 0.00002521
Iteration 220/1000 | Loss: 0.00002521
Iteration 221/1000 | Loss: 0.00002521
Iteration 222/1000 | Loss: 0.00002521
Iteration 223/1000 | Loss: 0.00002520
Iteration 224/1000 | Loss: 0.00002520
Iteration 225/1000 | Loss: 0.00002520
Iteration 226/1000 | Loss: 0.00002520
Iteration 227/1000 | Loss: 0.00002520
Iteration 228/1000 | Loss: 0.00002520
Iteration 229/1000 | Loss: 0.00002520
Iteration 230/1000 | Loss: 0.00002520
Iteration 231/1000 | Loss: 0.00002520
Iteration 232/1000 | Loss: 0.00002520
Iteration 233/1000 | Loss: 0.00002520
Iteration 234/1000 | Loss: 0.00002520
Iteration 235/1000 | Loss: 0.00002520
Iteration 236/1000 | Loss: 0.00002520
Iteration 237/1000 | Loss: 0.00002520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.520373345760163e-05, 2.520373345760163e-05, 2.520373345760163e-05, 2.520373345760163e-05, 2.520373345760163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.520373345760163e-05

Optimization complete. Final v2v error: 3.5437822341918945 mm

Highest mean error: 12.363369941711426 mm for frame 134

Lowest mean error: 2.5845530033111572 mm for frame 151

Saving results

Total time: 228.7610535621643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713040
Iteration 2/25 | Loss: 0.00181308
Iteration 3/25 | Loss: 0.00143838
Iteration 4/25 | Loss: 0.00137929
Iteration 5/25 | Loss: 0.00135806
Iteration 6/25 | Loss: 0.00135115
Iteration 7/25 | Loss: 0.00134935
Iteration 8/25 | Loss: 0.00134838
Iteration 9/25 | Loss: 0.00134757
Iteration 10/25 | Loss: 0.00134683
Iteration 11/25 | Loss: 0.00135175
Iteration 12/25 | Loss: 0.00135123
Iteration 13/25 | Loss: 0.00134909
Iteration 14/25 | Loss: 0.00134840
Iteration 15/25 | Loss: 0.00134945
Iteration 16/25 | Loss: 0.00134889
Iteration 17/25 | Loss: 0.00134955
Iteration 18/25 | Loss: 0.00134877
Iteration 19/25 | Loss: 0.00135058
Iteration 20/25 | Loss: 0.00134914
Iteration 21/25 | Loss: 0.00134955
Iteration 22/25 | Loss: 0.00134637
Iteration 23/25 | Loss: 0.00134831
Iteration 24/25 | Loss: 0.00134909
Iteration 25/25 | Loss: 0.00135313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.98340321
Iteration 2/25 | Loss: 0.02662553
Iteration 3/25 | Loss: 0.01593030
Iteration 4/25 | Loss: 0.01326805
Iteration 5/25 | Loss: 0.00870088
Iteration 6/25 | Loss: 0.01155015
Iteration 7/25 | Loss: 0.01053720
Iteration 8/25 | Loss: 0.00361862
Iteration 9/25 | Loss: 0.00361862
Iteration 10/25 | Loss: 0.00361862
Iteration 11/25 | Loss: 0.00361862
Iteration 12/25 | Loss: 0.00361862
Iteration 13/25 | Loss: 0.00361862
Iteration 14/25 | Loss: 0.00361862
Iteration 15/25 | Loss: 0.00361862
Iteration 16/25 | Loss: 0.00361862
Iteration 17/25 | Loss: 0.00361862
Iteration 18/25 | Loss: 0.00361862
Iteration 19/25 | Loss: 0.00361862
Iteration 20/25 | Loss: 0.00361862
Iteration 21/25 | Loss: 0.00361862
Iteration 22/25 | Loss: 0.00361862
Iteration 23/25 | Loss: 0.00361862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003618615912273526, 0.003618615912273526, 0.003618615912273526, 0.003618615912273526, 0.003618615912273526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003618615912273526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00361862
Iteration 2/1000 | Loss: 0.00025438
Iteration 3/1000 | Loss: 0.00028762
Iteration 4/1000 | Loss: 0.00027473
Iteration 5/1000 | Loss: 0.00018517
Iteration 6/1000 | Loss: 0.00008423
Iteration 7/1000 | Loss: 0.00018650
Iteration 8/1000 | Loss: 0.00008590
Iteration 9/1000 | Loss: 0.00017592
Iteration 10/1000 | Loss: 0.00011411
Iteration 11/1000 | Loss: 0.00007895
Iteration 12/1000 | Loss: 0.00018881
Iteration 13/1000 | Loss: 0.00010803
Iteration 14/1000 | Loss: 0.00018564
Iteration 15/1000 | Loss: 0.00010686
Iteration 16/1000 | Loss: 0.00012523
Iteration 17/1000 | Loss: 0.00007146
Iteration 18/1000 | Loss: 0.00013517
Iteration 19/1000 | Loss: 0.00011068
Iteration 20/1000 | Loss: 0.00006945
Iteration 21/1000 | Loss: 0.00006516
Iteration 22/1000 | Loss: 0.00012307
Iteration 23/1000 | Loss: 0.00009325
Iteration 24/1000 | Loss: 0.00021782
Iteration 25/1000 | Loss: 0.00016057
Iteration 26/1000 | Loss: 0.00010113
Iteration 27/1000 | Loss: 0.00018129
Iteration 28/1000 | Loss: 0.00018825
Iteration 29/1000 | Loss: 0.00015246
Iteration 30/1000 | Loss: 0.00013861
Iteration 31/1000 | Loss: 0.00018684
Iteration 32/1000 | Loss: 0.00020871
Iteration 33/1000 | Loss: 0.00013272
Iteration 34/1000 | Loss: 0.00010745
Iteration 35/1000 | Loss: 0.00015567
Iteration 36/1000 | Loss: 0.00009585
Iteration 37/1000 | Loss: 0.00011171
Iteration 38/1000 | Loss: 0.00014791
Iteration 39/1000 | Loss: 0.00020459
Iteration 40/1000 | Loss: 0.00015833
Iteration 41/1000 | Loss: 0.00015797
Iteration 42/1000 | Loss: 0.00016168
Iteration 43/1000 | Loss: 0.00017522
Iteration 44/1000 | Loss: 0.00014555
Iteration 45/1000 | Loss: 0.00006364
Iteration 46/1000 | Loss: 0.00011237
Iteration 47/1000 | Loss: 0.00019677
Iteration 48/1000 | Loss: 0.00016353
Iteration 49/1000 | Loss: 0.00022022
Iteration 50/1000 | Loss: 0.00014072
Iteration 51/1000 | Loss: 0.00009086
Iteration 52/1000 | Loss: 0.00025202
Iteration 53/1000 | Loss: 0.00013422
Iteration 54/1000 | Loss: 0.00016806
Iteration 55/1000 | Loss: 0.00014328
Iteration 56/1000 | Loss: 0.00019595
Iteration 57/1000 | Loss: 0.00016787
Iteration 58/1000 | Loss: 0.00022120
Iteration 59/1000 | Loss: 0.00015948
Iteration 60/1000 | Loss: 0.00015447
Iteration 61/1000 | Loss: 0.00016865
Iteration 62/1000 | Loss: 0.00007319
Iteration 63/1000 | Loss: 0.00019973
Iteration 64/1000 | Loss: 0.00010650
Iteration 65/1000 | Loss: 0.00012634
Iteration 66/1000 | Loss: 0.00022006
Iteration 67/1000 | Loss: 0.00016828
Iteration 68/1000 | Loss: 0.00016477
Iteration 69/1000 | Loss: 0.00014780
Iteration 70/1000 | Loss: 0.00016050
Iteration 71/1000 | Loss: 0.00010234
Iteration 72/1000 | Loss: 0.00029146
Iteration 73/1000 | Loss: 0.00014521
Iteration 74/1000 | Loss: 0.00016103
Iteration 75/1000 | Loss: 0.00017810
Iteration 76/1000 | Loss: 0.00025223
Iteration 77/1000 | Loss: 0.00014286
Iteration 78/1000 | Loss: 0.00022680
Iteration 79/1000 | Loss: 0.00021557
Iteration 80/1000 | Loss: 0.00010092
Iteration 81/1000 | Loss: 0.00023602
Iteration 82/1000 | Loss: 0.00018502
Iteration 83/1000 | Loss: 0.00022352
Iteration 84/1000 | Loss: 0.00019466
Iteration 85/1000 | Loss: 0.00015542
Iteration 86/1000 | Loss: 0.00017655
Iteration 87/1000 | Loss: 0.00014984
Iteration 88/1000 | Loss: 0.00026150
Iteration 89/1000 | Loss: 0.00019087
Iteration 90/1000 | Loss: 0.00019834
Iteration 91/1000 | Loss: 0.00024079
Iteration 92/1000 | Loss: 0.00015805
Iteration 93/1000 | Loss: 0.00006935
Iteration 94/1000 | Loss: 0.00006229
Iteration 95/1000 | Loss: 0.00006130
Iteration 96/1000 | Loss: 0.00006065
Iteration 97/1000 | Loss: 0.00006017
Iteration 98/1000 | Loss: 0.00005946
Iteration 99/1000 | Loss: 0.00005876
Iteration 100/1000 | Loss: 0.00005831
Iteration 101/1000 | Loss: 0.00005791
Iteration 102/1000 | Loss: 0.00005768
Iteration 103/1000 | Loss: 0.00005749
Iteration 104/1000 | Loss: 0.00005729
Iteration 105/1000 | Loss: 0.00005720
Iteration 106/1000 | Loss: 0.00005701
Iteration 107/1000 | Loss: 0.00005691
Iteration 108/1000 | Loss: 0.00005678
Iteration 109/1000 | Loss: 0.00005678
Iteration 110/1000 | Loss: 0.00005657
Iteration 111/1000 | Loss: 0.00005644
Iteration 112/1000 | Loss: 0.00042116
Iteration 113/1000 | Loss: 0.00170108
Iteration 114/1000 | Loss: 0.00201608
Iteration 115/1000 | Loss: 0.00123907
Iteration 116/1000 | Loss: 0.00147009
Iteration 117/1000 | Loss: 0.00146945
Iteration 118/1000 | Loss: 0.00122599
Iteration 119/1000 | Loss: 0.00011297
Iteration 120/1000 | Loss: 0.00068994
Iteration 121/1000 | Loss: 0.00058978
Iteration 122/1000 | Loss: 0.00006400
Iteration 123/1000 | Loss: 0.00009531
Iteration 124/1000 | Loss: 0.00010142
Iteration 125/1000 | Loss: 0.00008204
Iteration 126/1000 | Loss: 0.00006271
Iteration 127/1000 | Loss: 0.00004803
Iteration 128/1000 | Loss: 0.00004597
Iteration 129/1000 | Loss: 0.00004439
Iteration 130/1000 | Loss: 0.00004321
Iteration 131/1000 | Loss: 0.00004241
Iteration 132/1000 | Loss: 0.00004193
Iteration 133/1000 | Loss: 0.00004145
Iteration 134/1000 | Loss: 0.00004112
Iteration 135/1000 | Loss: 0.00004079
Iteration 136/1000 | Loss: 0.00004053
Iteration 137/1000 | Loss: 0.00004030
Iteration 138/1000 | Loss: 0.00004016
Iteration 139/1000 | Loss: 0.00004011
Iteration 140/1000 | Loss: 0.00004007
Iteration 141/1000 | Loss: 0.00004007
Iteration 142/1000 | Loss: 0.00004006
Iteration 143/1000 | Loss: 0.00003999
Iteration 144/1000 | Loss: 0.00003997
Iteration 145/1000 | Loss: 0.00003996
Iteration 146/1000 | Loss: 0.00003988
Iteration 147/1000 | Loss: 0.00003987
Iteration 148/1000 | Loss: 0.00003986
Iteration 149/1000 | Loss: 0.00003985
Iteration 150/1000 | Loss: 0.00003985
Iteration 151/1000 | Loss: 0.00003983
Iteration 152/1000 | Loss: 0.00003983
Iteration 153/1000 | Loss: 0.00003982
Iteration 154/1000 | Loss: 0.00003982
Iteration 155/1000 | Loss: 0.00003982
Iteration 156/1000 | Loss: 0.00003981
Iteration 157/1000 | Loss: 0.00003981
Iteration 158/1000 | Loss: 0.00003981
Iteration 159/1000 | Loss: 0.00003981
Iteration 160/1000 | Loss: 0.00003980
Iteration 161/1000 | Loss: 0.00003980
Iteration 162/1000 | Loss: 0.00003979
Iteration 163/1000 | Loss: 0.00003979
Iteration 164/1000 | Loss: 0.00003978
Iteration 165/1000 | Loss: 0.00003978
Iteration 166/1000 | Loss: 0.00003978
Iteration 167/1000 | Loss: 0.00003978
Iteration 168/1000 | Loss: 0.00003978
Iteration 169/1000 | Loss: 0.00003977
Iteration 170/1000 | Loss: 0.00003977
Iteration 171/1000 | Loss: 0.00003977
Iteration 172/1000 | Loss: 0.00003976
Iteration 173/1000 | Loss: 0.00003976
Iteration 174/1000 | Loss: 0.00003976
Iteration 175/1000 | Loss: 0.00003976
Iteration 176/1000 | Loss: 0.00003975
Iteration 177/1000 | Loss: 0.00003975
Iteration 178/1000 | Loss: 0.00003975
Iteration 179/1000 | Loss: 0.00003974
Iteration 180/1000 | Loss: 0.00003973
Iteration 181/1000 | Loss: 0.00003973
Iteration 182/1000 | Loss: 0.00003973
Iteration 183/1000 | Loss: 0.00003972
Iteration 184/1000 | Loss: 0.00003972
Iteration 185/1000 | Loss: 0.00003972
Iteration 186/1000 | Loss: 0.00003972
Iteration 187/1000 | Loss: 0.00003972
Iteration 188/1000 | Loss: 0.00003971
Iteration 189/1000 | Loss: 0.00003971
Iteration 190/1000 | Loss: 0.00003971
Iteration 191/1000 | Loss: 0.00003971
Iteration 192/1000 | Loss: 0.00003970
Iteration 193/1000 | Loss: 0.00003970
Iteration 194/1000 | Loss: 0.00003970
Iteration 195/1000 | Loss: 0.00003970
Iteration 196/1000 | Loss: 0.00003970
Iteration 197/1000 | Loss: 0.00003970
Iteration 198/1000 | Loss: 0.00003969
Iteration 199/1000 | Loss: 0.00003969
Iteration 200/1000 | Loss: 0.00003969
Iteration 201/1000 | Loss: 0.00003969
Iteration 202/1000 | Loss: 0.00003969
Iteration 203/1000 | Loss: 0.00003969
Iteration 204/1000 | Loss: 0.00003968
Iteration 205/1000 | Loss: 0.00003968
Iteration 206/1000 | Loss: 0.00003968
Iteration 207/1000 | Loss: 0.00003968
Iteration 208/1000 | Loss: 0.00003967
Iteration 209/1000 | Loss: 0.00003967
Iteration 210/1000 | Loss: 0.00003967
Iteration 211/1000 | Loss: 0.00003967
Iteration 212/1000 | Loss: 0.00003966
Iteration 213/1000 | Loss: 0.00003966
Iteration 214/1000 | Loss: 0.00003966
Iteration 215/1000 | Loss: 0.00003966
Iteration 216/1000 | Loss: 0.00003965
Iteration 217/1000 | Loss: 0.00003965
Iteration 218/1000 | Loss: 0.00003965
Iteration 219/1000 | Loss: 0.00003964
Iteration 220/1000 | Loss: 0.00003964
Iteration 221/1000 | Loss: 0.00003964
Iteration 222/1000 | Loss: 0.00003964
Iteration 223/1000 | Loss: 0.00003964
Iteration 224/1000 | Loss: 0.00003964
Iteration 225/1000 | Loss: 0.00003964
Iteration 226/1000 | Loss: 0.00003964
Iteration 227/1000 | Loss: 0.00003964
Iteration 228/1000 | Loss: 0.00003963
Iteration 229/1000 | Loss: 0.00003963
Iteration 230/1000 | Loss: 0.00003963
Iteration 231/1000 | Loss: 0.00003962
Iteration 232/1000 | Loss: 0.00003962
Iteration 233/1000 | Loss: 0.00003962
Iteration 234/1000 | Loss: 0.00003962
Iteration 235/1000 | Loss: 0.00003962
Iteration 236/1000 | Loss: 0.00003962
Iteration 237/1000 | Loss: 0.00003962
Iteration 238/1000 | Loss: 0.00003962
Iteration 239/1000 | Loss: 0.00003962
Iteration 240/1000 | Loss: 0.00003961
Iteration 241/1000 | Loss: 0.00003961
Iteration 242/1000 | Loss: 0.00003961
Iteration 243/1000 | Loss: 0.00003960
Iteration 244/1000 | Loss: 0.00003960
Iteration 245/1000 | Loss: 0.00003960
Iteration 246/1000 | Loss: 0.00003960
Iteration 247/1000 | Loss: 0.00003960
Iteration 248/1000 | Loss: 0.00003960
Iteration 249/1000 | Loss: 0.00003960
Iteration 250/1000 | Loss: 0.00003960
Iteration 251/1000 | Loss: 0.00003959
Iteration 252/1000 | Loss: 0.00003959
Iteration 253/1000 | Loss: 0.00003959
Iteration 254/1000 | Loss: 0.00003958
Iteration 255/1000 | Loss: 0.00003958
Iteration 256/1000 | Loss: 0.00003958
Iteration 257/1000 | Loss: 0.00003958
Iteration 258/1000 | Loss: 0.00003958
Iteration 259/1000 | Loss: 0.00003958
Iteration 260/1000 | Loss: 0.00003957
Iteration 261/1000 | Loss: 0.00003957
Iteration 262/1000 | Loss: 0.00003957
Iteration 263/1000 | Loss: 0.00003957
Iteration 264/1000 | Loss: 0.00003957
Iteration 265/1000 | Loss: 0.00003956
Iteration 266/1000 | Loss: 0.00003956
Iteration 267/1000 | Loss: 0.00003956
Iteration 268/1000 | Loss: 0.00003956
Iteration 269/1000 | Loss: 0.00003956
Iteration 270/1000 | Loss: 0.00003956
Iteration 271/1000 | Loss: 0.00003956
Iteration 272/1000 | Loss: 0.00003956
Iteration 273/1000 | Loss: 0.00003956
Iteration 274/1000 | Loss: 0.00003956
Iteration 275/1000 | Loss: 0.00003956
Iteration 276/1000 | Loss: 0.00003956
Iteration 277/1000 | Loss: 0.00003956
Iteration 278/1000 | Loss: 0.00003955
Iteration 279/1000 | Loss: 0.00003955
Iteration 280/1000 | Loss: 0.00003955
Iteration 281/1000 | Loss: 0.00003955
Iteration 282/1000 | Loss: 0.00003955
Iteration 283/1000 | Loss: 0.00003955
Iteration 284/1000 | Loss: 0.00003955
Iteration 285/1000 | Loss: 0.00003954
Iteration 286/1000 | Loss: 0.00003954
Iteration 287/1000 | Loss: 0.00003954
Iteration 288/1000 | Loss: 0.00003954
Iteration 289/1000 | Loss: 0.00003954
Iteration 290/1000 | Loss: 0.00003954
Iteration 291/1000 | Loss: 0.00003954
Iteration 292/1000 | Loss: 0.00003954
Iteration 293/1000 | Loss: 0.00003954
Iteration 294/1000 | Loss: 0.00003953
Iteration 295/1000 | Loss: 0.00003953
Iteration 296/1000 | Loss: 0.00003953
Iteration 297/1000 | Loss: 0.00003953
Iteration 298/1000 | Loss: 0.00003953
Iteration 299/1000 | Loss: 0.00003953
Iteration 300/1000 | Loss: 0.00003953
Iteration 301/1000 | Loss: 0.00003953
Iteration 302/1000 | Loss: 0.00003953
Iteration 303/1000 | Loss: 0.00003953
Iteration 304/1000 | Loss: 0.00003953
Iteration 305/1000 | Loss: 0.00003953
Iteration 306/1000 | Loss: 0.00003953
Iteration 307/1000 | Loss: 0.00003953
Iteration 308/1000 | Loss: 0.00003953
Iteration 309/1000 | Loss: 0.00003953
Iteration 310/1000 | Loss: 0.00003952
Iteration 311/1000 | Loss: 0.00003952
Iteration 312/1000 | Loss: 0.00003952
Iteration 313/1000 | Loss: 0.00003952
Iteration 314/1000 | Loss: 0.00003952
Iteration 315/1000 | Loss: 0.00003952
Iteration 316/1000 | Loss: 0.00003952
Iteration 317/1000 | Loss: 0.00003951
Iteration 318/1000 | Loss: 0.00003951
Iteration 319/1000 | Loss: 0.00003951
Iteration 320/1000 | Loss: 0.00003951
Iteration 321/1000 | Loss: 0.00003951
Iteration 322/1000 | Loss: 0.00003951
Iteration 323/1000 | Loss: 0.00003951
Iteration 324/1000 | Loss: 0.00003951
Iteration 325/1000 | Loss: 0.00003951
Iteration 326/1000 | Loss: 0.00003951
Iteration 327/1000 | Loss: 0.00003951
Iteration 328/1000 | Loss: 0.00003951
Iteration 329/1000 | Loss: 0.00003951
Iteration 330/1000 | Loss: 0.00003950
Iteration 331/1000 | Loss: 0.00003950
Iteration 332/1000 | Loss: 0.00003950
Iteration 333/1000 | Loss: 0.00003950
Iteration 334/1000 | Loss: 0.00003950
Iteration 335/1000 | Loss: 0.00003950
Iteration 336/1000 | Loss: 0.00003950
Iteration 337/1000 | Loss: 0.00003950
Iteration 338/1000 | Loss: 0.00003949
Iteration 339/1000 | Loss: 0.00003949
Iteration 340/1000 | Loss: 0.00003949
Iteration 341/1000 | Loss: 0.00003949
Iteration 342/1000 | Loss: 0.00003949
Iteration 343/1000 | Loss: 0.00003949
Iteration 344/1000 | Loss: 0.00003949
Iteration 345/1000 | Loss: 0.00003949
Iteration 346/1000 | Loss: 0.00003949
Iteration 347/1000 | Loss: 0.00003949
Iteration 348/1000 | Loss: 0.00003949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 348. Stopping optimization.
Last 5 losses: [3.949375604861416e-05, 3.949375604861416e-05, 3.949375604861416e-05, 3.949375604861416e-05, 3.949375604861416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.949375604861416e-05

Optimization complete. Final v2v error: 4.261376857757568 mm

Highest mean error: 12.559577941894531 mm for frame 126

Lowest mean error: 2.8465769290924072 mm for frame 13

Saving results

Total time: 269.1075437068939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557100
Iteration 2/25 | Loss: 0.00142737
Iteration 3/25 | Loss: 0.00131706
Iteration 4/25 | Loss: 0.00130590
Iteration 5/25 | Loss: 0.00130437
Iteration 6/25 | Loss: 0.00130437
Iteration 7/25 | Loss: 0.00130437
Iteration 8/25 | Loss: 0.00130437
Iteration 9/25 | Loss: 0.00130437
Iteration 10/25 | Loss: 0.00130437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001304365461692214, 0.001304365461692214, 0.001304365461692214, 0.001304365461692214, 0.001304365461692214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001304365461692214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57105935
Iteration 2/25 | Loss: 0.00145798
Iteration 3/25 | Loss: 0.00145797
Iteration 4/25 | Loss: 0.00145797
Iteration 5/25 | Loss: 0.00145797
Iteration 6/25 | Loss: 0.00145797
Iteration 7/25 | Loss: 0.00145797
Iteration 8/25 | Loss: 0.00145797
Iteration 9/25 | Loss: 0.00145797
Iteration 10/25 | Loss: 0.00145797
Iteration 11/25 | Loss: 0.00145797
Iteration 12/25 | Loss: 0.00145797
Iteration 13/25 | Loss: 0.00145797
Iteration 14/25 | Loss: 0.00145797
Iteration 15/25 | Loss: 0.00145797
Iteration 16/25 | Loss: 0.00145797
Iteration 17/25 | Loss: 0.00145797
Iteration 18/25 | Loss: 0.00145797
Iteration 19/25 | Loss: 0.00145797
Iteration 20/25 | Loss: 0.00145797
Iteration 21/25 | Loss: 0.00145797
Iteration 22/25 | Loss: 0.00145797
Iteration 23/25 | Loss: 0.00145797
Iteration 24/25 | Loss: 0.00145797
Iteration 25/25 | Loss: 0.00145797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145797
Iteration 2/1000 | Loss: 0.00003220
Iteration 3/1000 | Loss: 0.00002460
Iteration 4/1000 | Loss: 0.00002306
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002140
Iteration 7/1000 | Loss: 0.00002092
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001859
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001793
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001764
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001710
Iteration 27/1000 | Loss: 0.00001710
Iteration 28/1000 | Loss: 0.00001710
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001702
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001699
Iteration 52/1000 | Loss: 0.00001699
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001697
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001697
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001695
Iteration 83/1000 | Loss: 0.00001695
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001694
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001693
Iteration 96/1000 | Loss: 0.00001693
Iteration 97/1000 | Loss: 0.00001693
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001692
Iteration 108/1000 | Loss: 0.00001692
Iteration 109/1000 | Loss: 0.00001692
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001692
Iteration 114/1000 | Loss: 0.00001691
Iteration 115/1000 | Loss: 0.00001691
Iteration 116/1000 | Loss: 0.00001691
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001690
Iteration 126/1000 | Loss: 0.00001690
Iteration 127/1000 | Loss: 0.00001690
Iteration 128/1000 | Loss: 0.00001690
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001689
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001689
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001689
Iteration 142/1000 | Loss: 0.00001689
Iteration 143/1000 | Loss: 0.00001689
Iteration 144/1000 | Loss: 0.00001689
Iteration 145/1000 | Loss: 0.00001689
Iteration 146/1000 | Loss: 0.00001689
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Iteration 154/1000 | Loss: 0.00001688
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001687
Iteration 157/1000 | Loss: 0.00001687
Iteration 158/1000 | Loss: 0.00001687
Iteration 159/1000 | Loss: 0.00001687
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001687
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.687319490883965e-05, 1.687319490883965e-05, 1.687319490883965e-05, 1.687319490883965e-05, 1.687319490883965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.687319490883965e-05

Optimization complete. Final v2v error: 3.3580710887908936 mm

Highest mean error: 3.4227395057678223 mm for frame 0

Lowest mean error: 3.3114795684814453 mm for frame 181

Saving results

Total time: 50.34787344932556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604743
Iteration 2/25 | Loss: 0.00130413
Iteration 3/25 | Loss: 0.00124359
Iteration 4/25 | Loss: 0.00123552
Iteration 5/25 | Loss: 0.00123457
Iteration 6/25 | Loss: 0.00123457
Iteration 7/25 | Loss: 0.00123457
Iteration 8/25 | Loss: 0.00123457
Iteration 9/25 | Loss: 0.00123457
Iteration 10/25 | Loss: 0.00123457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012345656286925077, 0.0012345656286925077, 0.0012345656286925077, 0.0012345656286925077, 0.0012345656286925077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012345656286925077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.59820843
Iteration 2/25 | Loss: 0.00198572
Iteration 3/25 | Loss: 0.00198569
Iteration 4/25 | Loss: 0.00198569
Iteration 5/25 | Loss: 0.00198569
Iteration 6/25 | Loss: 0.00198569
Iteration 7/25 | Loss: 0.00198569
Iteration 8/25 | Loss: 0.00198569
Iteration 9/25 | Loss: 0.00198569
Iteration 10/25 | Loss: 0.00198569
Iteration 11/25 | Loss: 0.00198569
Iteration 12/25 | Loss: 0.00198569
Iteration 13/25 | Loss: 0.00198569
Iteration 14/25 | Loss: 0.00198569
Iteration 15/25 | Loss: 0.00198569
Iteration 16/25 | Loss: 0.00198569
Iteration 17/25 | Loss: 0.00198569
Iteration 18/25 | Loss: 0.00198569
Iteration 19/25 | Loss: 0.00198569
Iteration 20/25 | Loss: 0.00198569
Iteration 21/25 | Loss: 0.00198569
Iteration 22/25 | Loss: 0.00198569
Iteration 23/25 | Loss: 0.00198569
Iteration 24/25 | Loss: 0.00198569
Iteration 25/25 | Loss: 0.00198569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198569
Iteration 2/1000 | Loss: 0.00001807
Iteration 3/1000 | Loss: 0.00001410
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00001169
Iteration 7/1000 | Loss: 0.00001139
Iteration 8/1000 | Loss: 0.00001092
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001055
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001023
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001000
Iteration 18/1000 | Loss: 0.00000996
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000995
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000986
Iteration 26/1000 | Loss: 0.00000985
Iteration 27/1000 | Loss: 0.00000984
Iteration 28/1000 | Loss: 0.00000984
Iteration 29/1000 | Loss: 0.00000981
Iteration 30/1000 | Loss: 0.00000979
Iteration 31/1000 | Loss: 0.00000979
Iteration 32/1000 | Loss: 0.00000978
Iteration 33/1000 | Loss: 0.00000977
Iteration 34/1000 | Loss: 0.00000975
Iteration 35/1000 | Loss: 0.00000974
Iteration 36/1000 | Loss: 0.00000973
Iteration 37/1000 | Loss: 0.00000973
Iteration 38/1000 | Loss: 0.00000973
Iteration 39/1000 | Loss: 0.00000972
Iteration 40/1000 | Loss: 0.00000972
Iteration 41/1000 | Loss: 0.00000972
Iteration 42/1000 | Loss: 0.00000971
Iteration 43/1000 | Loss: 0.00000970
Iteration 44/1000 | Loss: 0.00000969
Iteration 45/1000 | Loss: 0.00000969
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000968
Iteration 49/1000 | Loss: 0.00000968
Iteration 50/1000 | Loss: 0.00000967
Iteration 51/1000 | Loss: 0.00000967
Iteration 52/1000 | Loss: 0.00000967
Iteration 53/1000 | Loss: 0.00000966
Iteration 54/1000 | Loss: 0.00000966
Iteration 55/1000 | Loss: 0.00000966
Iteration 56/1000 | Loss: 0.00000965
Iteration 57/1000 | Loss: 0.00000965
Iteration 58/1000 | Loss: 0.00000964
Iteration 59/1000 | Loss: 0.00000964
Iteration 60/1000 | Loss: 0.00000964
Iteration 61/1000 | Loss: 0.00000964
Iteration 62/1000 | Loss: 0.00000964
Iteration 63/1000 | Loss: 0.00000964
Iteration 64/1000 | Loss: 0.00000964
Iteration 65/1000 | Loss: 0.00000964
Iteration 66/1000 | Loss: 0.00000963
Iteration 67/1000 | Loss: 0.00000963
Iteration 68/1000 | Loss: 0.00000963
Iteration 69/1000 | Loss: 0.00000963
Iteration 70/1000 | Loss: 0.00000962
Iteration 71/1000 | Loss: 0.00000962
Iteration 72/1000 | Loss: 0.00000962
Iteration 73/1000 | Loss: 0.00000962
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000961
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000959
Iteration 81/1000 | Loss: 0.00000959
Iteration 82/1000 | Loss: 0.00000959
Iteration 83/1000 | Loss: 0.00000959
Iteration 84/1000 | Loss: 0.00000959
Iteration 85/1000 | Loss: 0.00000959
Iteration 86/1000 | Loss: 0.00000959
Iteration 87/1000 | Loss: 0.00000958
Iteration 88/1000 | Loss: 0.00000958
Iteration 89/1000 | Loss: 0.00000957
Iteration 90/1000 | Loss: 0.00000957
Iteration 91/1000 | Loss: 0.00000957
Iteration 92/1000 | Loss: 0.00000956
Iteration 93/1000 | Loss: 0.00000955
Iteration 94/1000 | Loss: 0.00000955
Iteration 95/1000 | Loss: 0.00000954
Iteration 96/1000 | Loss: 0.00000954
Iteration 97/1000 | Loss: 0.00000954
Iteration 98/1000 | Loss: 0.00000953
Iteration 99/1000 | Loss: 0.00000953
Iteration 100/1000 | Loss: 0.00000953
Iteration 101/1000 | Loss: 0.00000951
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000950
Iteration 104/1000 | Loss: 0.00000950
Iteration 105/1000 | Loss: 0.00000950
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000948
Iteration 110/1000 | Loss: 0.00000947
Iteration 111/1000 | Loss: 0.00000947
Iteration 112/1000 | Loss: 0.00000947
Iteration 113/1000 | Loss: 0.00000947
Iteration 114/1000 | Loss: 0.00000947
Iteration 115/1000 | Loss: 0.00000947
Iteration 116/1000 | Loss: 0.00000947
Iteration 117/1000 | Loss: 0.00000947
Iteration 118/1000 | Loss: 0.00000946
Iteration 119/1000 | Loss: 0.00000946
Iteration 120/1000 | Loss: 0.00000946
Iteration 121/1000 | Loss: 0.00000946
Iteration 122/1000 | Loss: 0.00000946
Iteration 123/1000 | Loss: 0.00000946
Iteration 124/1000 | Loss: 0.00000946
Iteration 125/1000 | Loss: 0.00000946
Iteration 126/1000 | Loss: 0.00000946
Iteration 127/1000 | Loss: 0.00000946
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000945
Iteration 130/1000 | Loss: 0.00000945
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000945
Iteration 135/1000 | Loss: 0.00000945
Iteration 136/1000 | Loss: 0.00000945
Iteration 137/1000 | Loss: 0.00000945
Iteration 138/1000 | Loss: 0.00000945
Iteration 139/1000 | Loss: 0.00000944
Iteration 140/1000 | Loss: 0.00000944
Iteration 141/1000 | Loss: 0.00000944
Iteration 142/1000 | Loss: 0.00000944
Iteration 143/1000 | Loss: 0.00000944
Iteration 144/1000 | Loss: 0.00000944
Iteration 145/1000 | Loss: 0.00000944
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000944
Iteration 152/1000 | Loss: 0.00000944
Iteration 153/1000 | Loss: 0.00000944
Iteration 154/1000 | Loss: 0.00000944
Iteration 155/1000 | Loss: 0.00000944
Iteration 156/1000 | Loss: 0.00000944
Iteration 157/1000 | Loss: 0.00000944
Iteration 158/1000 | Loss: 0.00000944
Iteration 159/1000 | Loss: 0.00000944
Iteration 160/1000 | Loss: 0.00000944
Iteration 161/1000 | Loss: 0.00000944
Iteration 162/1000 | Loss: 0.00000944
Iteration 163/1000 | Loss: 0.00000944
Iteration 164/1000 | Loss: 0.00000944
Iteration 165/1000 | Loss: 0.00000944
Iteration 166/1000 | Loss: 0.00000944
Iteration 167/1000 | Loss: 0.00000944
Iteration 168/1000 | Loss: 0.00000944
Iteration 169/1000 | Loss: 0.00000944
Iteration 170/1000 | Loss: 0.00000944
Iteration 171/1000 | Loss: 0.00000944
Iteration 172/1000 | Loss: 0.00000944
Iteration 173/1000 | Loss: 0.00000944
Iteration 174/1000 | Loss: 0.00000944
Iteration 175/1000 | Loss: 0.00000944
Iteration 176/1000 | Loss: 0.00000944
Iteration 177/1000 | Loss: 0.00000944
Iteration 178/1000 | Loss: 0.00000944
Iteration 179/1000 | Loss: 0.00000944
Iteration 180/1000 | Loss: 0.00000944
Iteration 181/1000 | Loss: 0.00000944
Iteration 182/1000 | Loss: 0.00000944
Iteration 183/1000 | Loss: 0.00000944
Iteration 184/1000 | Loss: 0.00000944
Iteration 185/1000 | Loss: 0.00000944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [9.4403012553812e-06, 9.4403012553812e-06, 9.4403012553812e-06, 9.4403012553812e-06, 9.4403012553812e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.4403012553812e-06

Optimization complete. Final v2v error: 2.661703109741211 mm

Highest mean error: 2.932915687561035 mm for frame 118

Lowest mean error: 2.483504295349121 mm for frame 17

Saving results

Total time: 43.20908212661743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729486
Iteration 2/25 | Loss: 0.00159790
Iteration 3/25 | Loss: 0.00147673
Iteration 4/25 | Loss: 0.00145539
Iteration 5/25 | Loss: 0.00144928
Iteration 6/25 | Loss: 0.00142604
Iteration 7/25 | Loss: 0.00136535
Iteration 8/25 | Loss: 0.00136030
Iteration 9/25 | Loss: 0.00135841
Iteration 10/25 | Loss: 0.00135624
Iteration 11/25 | Loss: 0.00138006
Iteration 12/25 | Loss: 0.00136075
Iteration 13/25 | Loss: 0.00135224
Iteration 14/25 | Loss: 0.00134273
Iteration 15/25 | Loss: 0.00134185
Iteration 16/25 | Loss: 0.00134164
Iteration 17/25 | Loss: 0.00134151
Iteration 18/25 | Loss: 0.00134135
Iteration 19/25 | Loss: 0.00134122
Iteration 20/25 | Loss: 0.00138785
Iteration 21/25 | Loss: 0.00133511
Iteration 22/25 | Loss: 0.00131378
Iteration 23/25 | Loss: 0.00130676
Iteration 24/25 | Loss: 0.00130633
Iteration 25/25 | Loss: 0.00130625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27157629
Iteration 2/25 | Loss: 0.00335541
Iteration 3/25 | Loss: 0.00335540
Iteration 4/25 | Loss: 0.00335540
Iteration 5/25 | Loss: 0.00335540
Iteration 6/25 | Loss: 0.00335540
Iteration 7/25 | Loss: 0.00335540
Iteration 8/25 | Loss: 0.00335540
Iteration 9/25 | Loss: 0.00335540
Iteration 10/25 | Loss: 0.00335539
Iteration 11/25 | Loss: 0.00335539
Iteration 12/25 | Loss: 0.00335539
Iteration 13/25 | Loss: 0.00335539
Iteration 14/25 | Loss: 0.00335539
Iteration 15/25 | Loss: 0.00335539
Iteration 16/25 | Loss: 0.00335539
Iteration 17/25 | Loss: 0.00335539
Iteration 18/25 | Loss: 0.00335539
Iteration 19/25 | Loss: 0.00335539
Iteration 20/25 | Loss: 0.00335539
Iteration 21/25 | Loss: 0.00335539
Iteration 22/25 | Loss: 0.00335539
Iteration 23/25 | Loss: 0.00335539
Iteration 24/25 | Loss: 0.00335539
Iteration 25/25 | Loss: 0.00335539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335539
Iteration 2/1000 | Loss: 0.00133006
Iteration 3/1000 | Loss: 0.00514830
Iteration 4/1000 | Loss: 0.00294540
Iteration 5/1000 | Loss: 0.00010196
Iteration 6/1000 | Loss: 0.00005511
Iteration 7/1000 | Loss: 0.00442454
Iteration 8/1000 | Loss: 0.00074862
Iteration 9/1000 | Loss: 0.00014370
Iteration 10/1000 | Loss: 0.00003717
Iteration 11/1000 | Loss: 0.00003503
Iteration 12/1000 | Loss: 0.00003428
Iteration 13/1000 | Loss: 0.00003343
Iteration 14/1000 | Loss: 0.00003268
Iteration 15/1000 | Loss: 0.00003203
Iteration 16/1000 | Loss: 0.00003159
Iteration 17/1000 | Loss: 0.00003116
Iteration 18/1000 | Loss: 0.00003080
Iteration 19/1000 | Loss: 0.00003053
Iteration 20/1000 | Loss: 0.00397330
Iteration 21/1000 | Loss: 0.00178429
Iteration 22/1000 | Loss: 0.00007597
Iteration 23/1000 | Loss: 0.00022067
Iteration 24/1000 | Loss: 0.00003572
Iteration 25/1000 | Loss: 0.00007924
Iteration 26/1000 | Loss: 0.00002723
Iteration 27/1000 | Loss: 0.00002448
Iteration 28/1000 | Loss: 0.00002318
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002223
Iteration 31/1000 | Loss: 0.00002216
Iteration 32/1000 | Loss: 0.00002197
Iteration 33/1000 | Loss: 0.00002180
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002175
Iteration 36/1000 | Loss: 0.00002175
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002173
Iteration 40/1000 | Loss: 0.00002172
Iteration 41/1000 | Loss: 0.00002171
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002167
Iteration 45/1000 | Loss: 0.00002167
Iteration 46/1000 | Loss: 0.00002166
Iteration 47/1000 | Loss: 0.00002166
Iteration 48/1000 | Loss: 0.00002166
Iteration 49/1000 | Loss: 0.00002166
Iteration 50/1000 | Loss: 0.00002166
Iteration 51/1000 | Loss: 0.00002166
Iteration 52/1000 | Loss: 0.00002166
Iteration 53/1000 | Loss: 0.00002166
Iteration 54/1000 | Loss: 0.00002166
Iteration 55/1000 | Loss: 0.00002165
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00002165
Iteration 58/1000 | Loss: 0.00002165
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002165
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002165
Iteration 67/1000 | Loss: 0.00002165
Iteration 68/1000 | Loss: 0.00002165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [2.1649033442372456e-05, 2.1649033442372456e-05, 2.1649033442372456e-05, 2.1649033442372456e-05, 2.1649033442372456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1649033442372456e-05

Optimization complete. Final v2v error: 3.887478828430176 mm

Highest mean error: 5.503647804260254 mm for frame 62

Lowest mean error: 2.9107935428619385 mm for frame 2

Saving results

Total time: 90.3762001991272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566851
Iteration 2/25 | Loss: 0.00155824
Iteration 3/25 | Loss: 0.00135633
Iteration 4/25 | Loss: 0.00133279
Iteration 5/25 | Loss: 0.00132692
Iteration 6/25 | Loss: 0.00132538
Iteration 7/25 | Loss: 0.00132475
Iteration 8/25 | Loss: 0.00132442
Iteration 9/25 | Loss: 0.00132436
Iteration 10/25 | Loss: 0.00132436
Iteration 11/25 | Loss: 0.00132436
Iteration 12/25 | Loss: 0.00132436
Iteration 13/25 | Loss: 0.00132436
Iteration 14/25 | Loss: 0.00132436
Iteration 15/25 | Loss: 0.00132436
Iteration 16/25 | Loss: 0.00132436
Iteration 17/25 | Loss: 0.00132436
Iteration 18/25 | Loss: 0.00132436
Iteration 19/25 | Loss: 0.00132436
Iteration 20/25 | Loss: 0.00132436
Iteration 21/25 | Loss: 0.00132436
Iteration 22/25 | Loss: 0.00132436
Iteration 23/25 | Loss: 0.00132436
Iteration 24/25 | Loss: 0.00132436
Iteration 25/25 | Loss: 0.00132436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33360755
Iteration 2/25 | Loss: 0.00225177
Iteration 3/25 | Loss: 0.00225177
Iteration 4/25 | Loss: 0.00225176
Iteration 5/25 | Loss: 0.00225176
Iteration 6/25 | Loss: 0.00225176
Iteration 7/25 | Loss: 0.00225176
Iteration 8/25 | Loss: 0.00225176
Iteration 9/25 | Loss: 0.00225176
Iteration 10/25 | Loss: 0.00225176
Iteration 11/25 | Loss: 0.00225176
Iteration 12/25 | Loss: 0.00225176
Iteration 13/25 | Loss: 0.00225176
Iteration 14/25 | Loss: 0.00225176
Iteration 15/25 | Loss: 0.00225176
Iteration 16/25 | Loss: 0.00225176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022517614997923374, 0.0022517614997923374, 0.0022517614997923374, 0.0022517614997923374, 0.0022517614997923374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022517614997923374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225176
Iteration 2/1000 | Loss: 0.00005003
Iteration 3/1000 | Loss: 0.00003486
Iteration 4/1000 | Loss: 0.00003899
Iteration 5/1000 | Loss: 0.00003441
Iteration 6/1000 | Loss: 0.00003358
Iteration 7/1000 | Loss: 0.00002649
Iteration 8/1000 | Loss: 0.00002528
Iteration 9/1000 | Loss: 0.00002518
Iteration 10/1000 | Loss: 0.00002379
Iteration 11/1000 | Loss: 0.00022660
Iteration 12/1000 | Loss: 0.00027602
Iteration 13/1000 | Loss: 0.00005270
Iteration 14/1000 | Loss: 0.00003367
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002669
Iteration 17/1000 | Loss: 0.00023811
Iteration 18/1000 | Loss: 0.00007295
Iteration 19/1000 | Loss: 0.00017103
Iteration 20/1000 | Loss: 0.00025511
Iteration 21/1000 | Loss: 0.00004163
Iteration 22/1000 | Loss: 0.00004618
Iteration 23/1000 | Loss: 0.00003239
Iteration 24/1000 | Loss: 0.00002572
Iteration 25/1000 | Loss: 0.00002500
Iteration 26/1000 | Loss: 0.00002334
Iteration 27/1000 | Loss: 0.00003816
Iteration 28/1000 | Loss: 0.00002321
Iteration 29/1000 | Loss: 0.00002139
Iteration 30/1000 | Loss: 0.00013427
Iteration 31/1000 | Loss: 0.00006255
Iteration 32/1000 | Loss: 0.00002661
Iteration 33/1000 | Loss: 0.00002033
Iteration 34/1000 | Loss: 0.00001989
Iteration 35/1000 | Loss: 0.00019909
Iteration 36/1000 | Loss: 0.00014625
Iteration 37/1000 | Loss: 0.00011220
Iteration 38/1000 | Loss: 0.00006252
Iteration 39/1000 | Loss: 0.00007254
Iteration 40/1000 | Loss: 0.00005189
Iteration 41/1000 | Loss: 0.00005813
Iteration 42/1000 | Loss: 0.00002769
Iteration 43/1000 | Loss: 0.00004260
Iteration 44/1000 | Loss: 0.00011866
Iteration 45/1000 | Loss: 0.00004710
Iteration 46/1000 | Loss: 0.00003227
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002607
Iteration 49/1000 | Loss: 0.00002394
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002001
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001831
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001769
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001720
Iteration 63/1000 | Loss: 0.00001720
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001719
Iteration 66/1000 | Loss: 0.00001719
Iteration 67/1000 | Loss: 0.00001719
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001717
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00001717
Iteration 78/1000 | Loss: 0.00001716
Iteration 79/1000 | Loss: 0.00001716
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001714
Iteration 83/1000 | Loss: 0.00001714
Iteration 84/1000 | Loss: 0.00001714
Iteration 85/1000 | Loss: 0.00001713
Iteration 86/1000 | Loss: 0.00001711
Iteration 87/1000 | Loss: 0.00001706
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001700
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001689
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001686
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001685
Iteration 105/1000 | Loss: 0.00001685
Iteration 106/1000 | Loss: 0.00001684
Iteration 107/1000 | Loss: 0.00001680
Iteration 108/1000 | Loss: 0.00001680
Iteration 109/1000 | Loss: 0.00001679
Iteration 110/1000 | Loss: 0.00001679
Iteration 111/1000 | Loss: 0.00001679
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001679
Iteration 118/1000 | Loss: 0.00001678
Iteration 119/1000 | Loss: 0.00001678
Iteration 120/1000 | Loss: 0.00001678
Iteration 121/1000 | Loss: 0.00001678
Iteration 122/1000 | Loss: 0.00001678
Iteration 123/1000 | Loss: 0.00001678
Iteration 124/1000 | Loss: 0.00001677
Iteration 125/1000 | Loss: 0.00001677
Iteration 126/1000 | Loss: 0.00001677
Iteration 127/1000 | Loss: 0.00001677
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001676
Iteration 140/1000 | Loss: 0.00001676
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001676
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001675
Iteration 154/1000 | Loss: 0.00001675
Iteration 155/1000 | Loss: 0.00001675
Iteration 156/1000 | Loss: 0.00001675
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001674
Iteration 159/1000 | Loss: 0.00001674
Iteration 160/1000 | Loss: 0.00001674
Iteration 161/1000 | Loss: 0.00001674
Iteration 162/1000 | Loss: 0.00001674
Iteration 163/1000 | Loss: 0.00001674
Iteration 164/1000 | Loss: 0.00001674
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001674
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001674
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00001673
Iteration 177/1000 | Loss: 0.00001673
Iteration 178/1000 | Loss: 0.00001673
Iteration 179/1000 | Loss: 0.00001673
Iteration 180/1000 | Loss: 0.00001673
Iteration 181/1000 | Loss: 0.00001673
Iteration 182/1000 | Loss: 0.00001673
Iteration 183/1000 | Loss: 0.00001673
Iteration 184/1000 | Loss: 0.00001673
Iteration 185/1000 | Loss: 0.00001673
Iteration 186/1000 | Loss: 0.00001673
Iteration 187/1000 | Loss: 0.00001673
Iteration 188/1000 | Loss: 0.00001673
Iteration 189/1000 | Loss: 0.00001673
Iteration 190/1000 | Loss: 0.00001673
Iteration 191/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.672953658271581e-05, 1.672953658271581e-05, 1.672953658271581e-05, 1.672953658271581e-05, 1.672953658271581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.672953658271581e-05

Optimization complete. Final v2v error: 3.448760747909546 mm

Highest mean error: 4.601668357849121 mm for frame 147

Lowest mean error: 2.584524154663086 mm for frame 74

Saving results

Total time: 124.8213324546814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002439
Iteration 2/25 | Loss: 0.00226976
Iteration 3/25 | Loss: 0.00196704
Iteration 4/25 | Loss: 0.00148467
Iteration 5/25 | Loss: 0.00146802
Iteration 6/25 | Loss: 0.00139744
Iteration 7/25 | Loss: 0.00133532
Iteration 8/25 | Loss: 0.00131449
Iteration 9/25 | Loss: 0.00130506
Iteration 10/25 | Loss: 0.00130175
Iteration 11/25 | Loss: 0.00130039
Iteration 12/25 | Loss: 0.00129957
Iteration 13/25 | Loss: 0.00129744
Iteration 14/25 | Loss: 0.00129620
Iteration 15/25 | Loss: 0.00129588
Iteration 16/25 | Loss: 0.00129578
Iteration 17/25 | Loss: 0.00129577
Iteration 18/25 | Loss: 0.00129577
Iteration 19/25 | Loss: 0.00129577
Iteration 20/25 | Loss: 0.00129577
Iteration 21/25 | Loss: 0.00129577
Iteration 22/25 | Loss: 0.00129577
Iteration 23/25 | Loss: 0.00129577
Iteration 24/25 | Loss: 0.00129577
Iteration 25/25 | Loss: 0.00129577

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23281419
Iteration 2/25 | Loss: 0.00216840
Iteration 3/25 | Loss: 0.00211144
Iteration 4/25 | Loss: 0.00211143
Iteration 5/25 | Loss: 0.00211143
Iteration 6/25 | Loss: 0.00211143
Iteration 7/25 | Loss: 0.00211143
Iteration 8/25 | Loss: 0.00211143
Iteration 9/25 | Loss: 0.00211143
Iteration 10/25 | Loss: 0.00211143
Iteration 11/25 | Loss: 0.00211143
Iteration 12/25 | Loss: 0.00211143
Iteration 13/25 | Loss: 0.00211143
Iteration 14/25 | Loss: 0.00211143
Iteration 15/25 | Loss: 0.00211143
Iteration 16/25 | Loss: 0.00211143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021114281844347715, 0.0021114281844347715, 0.0021114281844347715, 0.0021114281844347715, 0.0021114281844347715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021114281844347715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211143
Iteration 2/1000 | Loss: 0.00003340
Iteration 3/1000 | Loss: 0.00012214
Iteration 4/1000 | Loss: 0.00004344
Iteration 5/1000 | Loss: 0.00007952
Iteration 6/1000 | Loss: 0.00017349
Iteration 7/1000 | Loss: 0.00004193
Iteration 8/1000 | Loss: 0.00004492
Iteration 9/1000 | Loss: 0.00002221
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00032637
Iteration 12/1000 | Loss: 0.00002226
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001618
Iteration 17/1000 | Loss: 0.00001597
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00011404
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00001574
Iteration 22/1000 | Loss: 0.00003961
Iteration 23/1000 | Loss: 0.00006685
Iteration 24/1000 | Loss: 0.00017998
Iteration 25/1000 | Loss: 0.00001610
Iteration 26/1000 | Loss: 0.00002865
Iteration 27/1000 | Loss: 0.00001557
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00006082
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001551
Iteration 34/1000 | Loss: 0.00001551
Iteration 35/1000 | Loss: 0.00001549
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001547
Iteration 46/1000 | Loss: 0.00001547
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001546
Iteration 51/1000 | Loss: 0.00001546
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001546
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001542
Iteration 92/1000 | Loss: 0.00001542
Iteration 93/1000 | Loss: 0.00001542
Iteration 94/1000 | Loss: 0.00001542
Iteration 95/1000 | Loss: 0.00001542
Iteration 96/1000 | Loss: 0.00001542
Iteration 97/1000 | Loss: 0.00001542
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001541
Iteration 102/1000 | Loss: 0.00001541
Iteration 103/1000 | Loss: 0.00001541
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001540
Iteration 109/1000 | Loss: 0.00001540
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.5395426089526154e-05, 1.5395426089526154e-05, 1.5395426089526154e-05, 1.5395426089526154e-05, 1.5395426089526154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5395426089526154e-05

Optimization complete. Final v2v error: 3.35079026222229 mm

Highest mean error: 8.703544616699219 mm for frame 212

Lowest mean error: 3.0918219089508057 mm for frame 59

Saving results

Total time: 81.13296341896057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583173
Iteration 2/25 | Loss: 0.00141699
Iteration 3/25 | Loss: 0.00134380
Iteration 4/25 | Loss: 0.00132177
Iteration 5/25 | Loss: 0.00130918
Iteration 6/25 | Loss: 0.00130590
Iteration 7/25 | Loss: 0.00130482
Iteration 8/25 | Loss: 0.00130477
Iteration 9/25 | Loss: 0.00130477
Iteration 10/25 | Loss: 0.00130477
Iteration 11/25 | Loss: 0.00130477
Iteration 12/25 | Loss: 0.00130477
Iteration 13/25 | Loss: 0.00130477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013047694228589535, 0.0013047694228589535, 0.0013047694228589535, 0.0013047694228589535, 0.0013047694228589535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013047694228589535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92569953
Iteration 2/25 | Loss: 0.00354200
Iteration 3/25 | Loss: 0.00354192
Iteration 4/25 | Loss: 0.00354192
Iteration 5/25 | Loss: 0.00354192
Iteration 6/25 | Loss: 0.00354192
Iteration 7/25 | Loss: 0.00354192
Iteration 8/25 | Loss: 0.00354192
Iteration 9/25 | Loss: 0.00354192
Iteration 10/25 | Loss: 0.00354192
Iteration 11/25 | Loss: 0.00354192
Iteration 12/25 | Loss: 0.00354192
Iteration 13/25 | Loss: 0.00354192
Iteration 14/25 | Loss: 0.00354192
Iteration 15/25 | Loss: 0.00354192
Iteration 16/25 | Loss: 0.00354192
Iteration 17/25 | Loss: 0.00354192
Iteration 18/25 | Loss: 0.00354192
Iteration 19/25 | Loss: 0.00354192
Iteration 20/25 | Loss: 0.00354192
Iteration 21/25 | Loss: 0.00354192
Iteration 22/25 | Loss: 0.00354192
Iteration 23/25 | Loss: 0.00354192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003541920566931367, 0.003541920566931367, 0.003541920566931367, 0.003541920566931367, 0.003541920566931367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003541920566931367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00354192
Iteration 2/1000 | Loss: 0.00007471
Iteration 3/1000 | Loss: 0.00004636
Iteration 4/1000 | Loss: 0.00003394
Iteration 5/1000 | Loss: 0.00003053
Iteration 6/1000 | Loss: 0.00002938
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002782
Iteration 9/1000 | Loss: 0.00002721
Iteration 10/1000 | Loss: 0.00002677
Iteration 11/1000 | Loss: 0.00002647
Iteration 12/1000 | Loss: 0.00002646
Iteration 13/1000 | Loss: 0.00002625
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002609
Iteration 16/1000 | Loss: 0.00002606
Iteration 17/1000 | Loss: 0.00002587
Iteration 18/1000 | Loss: 0.00002585
Iteration 19/1000 | Loss: 0.00002582
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00002578
Iteration 22/1000 | Loss: 0.00002577
Iteration 23/1000 | Loss: 0.00002570
Iteration 24/1000 | Loss: 0.00002569
Iteration 25/1000 | Loss: 0.00002569
Iteration 26/1000 | Loss: 0.00002567
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002562
Iteration 29/1000 | Loss: 0.00002561
Iteration 30/1000 | Loss: 0.00002561
Iteration 31/1000 | Loss: 0.00002560
Iteration 32/1000 | Loss: 0.00002552
Iteration 33/1000 | Loss: 0.00002552
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002535
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002535
Iteration 40/1000 | Loss: 0.00002535
Iteration 41/1000 | Loss: 0.00002535
Iteration 42/1000 | Loss: 0.00002534
Iteration 43/1000 | Loss: 0.00002534
Iteration 44/1000 | Loss: 0.00002533
Iteration 45/1000 | Loss: 0.00002528
Iteration 46/1000 | Loss: 0.00002527
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002525
Iteration 50/1000 | Loss: 0.00002524
Iteration 51/1000 | Loss: 0.00002522
Iteration 52/1000 | Loss: 0.00002517
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002512
Iteration 55/1000 | Loss: 0.00002510
Iteration 56/1000 | Loss: 0.00002510
Iteration 57/1000 | Loss: 0.00002510
Iteration 58/1000 | Loss: 0.00002510
Iteration 59/1000 | Loss: 0.00002510
Iteration 60/1000 | Loss: 0.00002509
Iteration 61/1000 | Loss: 0.00002509
Iteration 62/1000 | Loss: 0.00002509
Iteration 63/1000 | Loss: 0.00002509
Iteration 64/1000 | Loss: 0.00002509
Iteration 65/1000 | Loss: 0.00002509
Iteration 66/1000 | Loss: 0.00002508
Iteration 67/1000 | Loss: 0.00002508
Iteration 68/1000 | Loss: 0.00002508
Iteration 69/1000 | Loss: 0.00002508
Iteration 70/1000 | Loss: 0.00002508
Iteration 71/1000 | Loss: 0.00002507
Iteration 72/1000 | Loss: 0.00002507
Iteration 73/1000 | Loss: 0.00002507
Iteration 74/1000 | Loss: 0.00002506
Iteration 75/1000 | Loss: 0.00002506
Iteration 76/1000 | Loss: 0.00002506
Iteration 77/1000 | Loss: 0.00002505
Iteration 78/1000 | Loss: 0.00002505
Iteration 79/1000 | Loss: 0.00002504
Iteration 80/1000 | Loss: 0.00002504
Iteration 81/1000 | Loss: 0.00002504
Iteration 82/1000 | Loss: 0.00002503
Iteration 83/1000 | Loss: 0.00002503
Iteration 84/1000 | Loss: 0.00002502
Iteration 85/1000 | Loss: 0.00002501
Iteration 86/1000 | Loss: 0.00002501
Iteration 87/1000 | Loss: 0.00002500
Iteration 88/1000 | Loss: 0.00002500
Iteration 89/1000 | Loss: 0.00002499
Iteration 90/1000 | Loss: 0.00002499
Iteration 91/1000 | Loss: 0.00002499
Iteration 92/1000 | Loss: 0.00002498
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002497
Iteration 95/1000 | Loss: 0.00002497
Iteration 96/1000 | Loss: 0.00002497
Iteration 97/1000 | Loss: 0.00002497
Iteration 98/1000 | Loss: 0.00002497
Iteration 99/1000 | Loss: 0.00002497
Iteration 100/1000 | Loss: 0.00002496
Iteration 101/1000 | Loss: 0.00002496
Iteration 102/1000 | Loss: 0.00002496
Iteration 103/1000 | Loss: 0.00002496
Iteration 104/1000 | Loss: 0.00002496
Iteration 105/1000 | Loss: 0.00002496
Iteration 106/1000 | Loss: 0.00002496
Iteration 107/1000 | Loss: 0.00002496
Iteration 108/1000 | Loss: 0.00002496
Iteration 109/1000 | Loss: 0.00002496
Iteration 110/1000 | Loss: 0.00002496
Iteration 111/1000 | Loss: 0.00002496
Iteration 112/1000 | Loss: 0.00002495
Iteration 113/1000 | Loss: 0.00002494
Iteration 114/1000 | Loss: 0.00002494
Iteration 115/1000 | Loss: 0.00002493
Iteration 116/1000 | Loss: 0.00002492
Iteration 117/1000 | Loss: 0.00002492
Iteration 118/1000 | Loss: 0.00002491
Iteration 119/1000 | Loss: 0.00002491
Iteration 120/1000 | Loss: 0.00002491
Iteration 121/1000 | Loss: 0.00002491
Iteration 122/1000 | Loss: 0.00002490
Iteration 123/1000 | Loss: 0.00002490
Iteration 124/1000 | Loss: 0.00002490
Iteration 125/1000 | Loss: 0.00002489
Iteration 126/1000 | Loss: 0.00002489
Iteration 127/1000 | Loss: 0.00002489
Iteration 128/1000 | Loss: 0.00002489
Iteration 129/1000 | Loss: 0.00002489
Iteration 130/1000 | Loss: 0.00002488
Iteration 131/1000 | Loss: 0.00002488
Iteration 132/1000 | Loss: 0.00002488
Iteration 133/1000 | Loss: 0.00002487
Iteration 134/1000 | Loss: 0.00002487
Iteration 135/1000 | Loss: 0.00002487
Iteration 136/1000 | Loss: 0.00002487
Iteration 137/1000 | Loss: 0.00002487
Iteration 138/1000 | Loss: 0.00002487
Iteration 139/1000 | Loss: 0.00002486
Iteration 140/1000 | Loss: 0.00002486
Iteration 141/1000 | Loss: 0.00002486
Iteration 142/1000 | Loss: 0.00002486
Iteration 143/1000 | Loss: 0.00002485
Iteration 144/1000 | Loss: 0.00002485
Iteration 145/1000 | Loss: 0.00002484
Iteration 146/1000 | Loss: 0.00002484
Iteration 147/1000 | Loss: 0.00002484
Iteration 148/1000 | Loss: 0.00002484
Iteration 149/1000 | Loss: 0.00002483
Iteration 150/1000 | Loss: 0.00002483
Iteration 151/1000 | Loss: 0.00002483
Iteration 152/1000 | Loss: 0.00002483
Iteration 153/1000 | Loss: 0.00002482
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002481
Iteration 158/1000 | Loss: 0.00002481
Iteration 159/1000 | Loss: 0.00002481
Iteration 160/1000 | Loss: 0.00002480
Iteration 161/1000 | Loss: 0.00002480
Iteration 162/1000 | Loss: 0.00002480
Iteration 163/1000 | Loss: 0.00002480
Iteration 164/1000 | Loss: 0.00002480
Iteration 165/1000 | Loss: 0.00002480
Iteration 166/1000 | Loss: 0.00002480
Iteration 167/1000 | Loss: 0.00002479
Iteration 168/1000 | Loss: 0.00002479
Iteration 169/1000 | Loss: 0.00002479
Iteration 170/1000 | Loss: 0.00002479
Iteration 171/1000 | Loss: 0.00002478
Iteration 172/1000 | Loss: 0.00002478
Iteration 173/1000 | Loss: 0.00002478
Iteration 174/1000 | Loss: 0.00002478
Iteration 175/1000 | Loss: 0.00002478
Iteration 176/1000 | Loss: 0.00002478
Iteration 177/1000 | Loss: 0.00002478
Iteration 178/1000 | Loss: 0.00002478
Iteration 179/1000 | Loss: 0.00002477
Iteration 180/1000 | Loss: 0.00002477
Iteration 181/1000 | Loss: 0.00002477
Iteration 182/1000 | Loss: 0.00002477
Iteration 183/1000 | Loss: 0.00002477
Iteration 184/1000 | Loss: 0.00002477
Iteration 185/1000 | Loss: 0.00002477
Iteration 186/1000 | Loss: 0.00002477
Iteration 187/1000 | Loss: 0.00002476
Iteration 188/1000 | Loss: 0.00002476
Iteration 189/1000 | Loss: 0.00002476
Iteration 190/1000 | Loss: 0.00002475
Iteration 191/1000 | Loss: 0.00002475
Iteration 192/1000 | Loss: 0.00002475
Iteration 193/1000 | Loss: 0.00002474
Iteration 194/1000 | Loss: 0.00002474
Iteration 195/1000 | Loss: 0.00002474
Iteration 196/1000 | Loss: 0.00002474
Iteration 197/1000 | Loss: 0.00002474
Iteration 198/1000 | Loss: 0.00002474
Iteration 199/1000 | Loss: 0.00002474
Iteration 200/1000 | Loss: 0.00002474
Iteration 201/1000 | Loss: 0.00002474
Iteration 202/1000 | Loss: 0.00002474
Iteration 203/1000 | Loss: 0.00002474
Iteration 204/1000 | Loss: 0.00002474
Iteration 205/1000 | Loss: 0.00002474
Iteration 206/1000 | Loss: 0.00002474
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002473
Iteration 210/1000 | Loss: 0.00002473
Iteration 211/1000 | Loss: 0.00002473
Iteration 212/1000 | Loss: 0.00002473
Iteration 213/1000 | Loss: 0.00002473
Iteration 214/1000 | Loss: 0.00002473
Iteration 215/1000 | Loss: 0.00002473
Iteration 216/1000 | Loss: 0.00002473
Iteration 217/1000 | Loss: 0.00002473
Iteration 218/1000 | Loss: 0.00002473
Iteration 219/1000 | Loss: 0.00002472
Iteration 220/1000 | Loss: 0.00002472
Iteration 221/1000 | Loss: 0.00002472
Iteration 222/1000 | Loss: 0.00002472
Iteration 223/1000 | Loss: 0.00002472
Iteration 224/1000 | Loss: 0.00002472
Iteration 225/1000 | Loss: 0.00002472
Iteration 226/1000 | Loss: 0.00002472
Iteration 227/1000 | Loss: 0.00002472
Iteration 228/1000 | Loss: 0.00002472
Iteration 229/1000 | Loss: 0.00002472
Iteration 230/1000 | Loss: 0.00002472
Iteration 231/1000 | Loss: 0.00002472
Iteration 232/1000 | Loss: 0.00002472
Iteration 233/1000 | Loss: 0.00002472
Iteration 234/1000 | Loss: 0.00002472
Iteration 235/1000 | Loss: 0.00002472
Iteration 236/1000 | Loss: 0.00002472
Iteration 237/1000 | Loss: 0.00002472
Iteration 238/1000 | Loss: 0.00002472
Iteration 239/1000 | Loss: 0.00002472
Iteration 240/1000 | Loss: 0.00002472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [2.4720709916437045e-05, 2.4720709916437045e-05, 2.4720709916437045e-05, 2.4720709916437045e-05, 2.4720709916437045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4720709916437045e-05

Optimization complete. Final v2v error: 4.185240268707275 mm

Highest mean error: 4.545015811920166 mm for frame 67

Lowest mean error: 3.8396029472351074 mm for frame 13

Saving results

Total time: 52.027968883514404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528844
Iteration 2/25 | Loss: 0.00145062
Iteration 3/25 | Loss: 0.00130011
Iteration 4/25 | Loss: 0.00128695
Iteration 5/25 | Loss: 0.00128606
Iteration 6/25 | Loss: 0.00128606
Iteration 7/25 | Loss: 0.00128606
Iteration 8/25 | Loss: 0.00128606
Iteration 9/25 | Loss: 0.00128606
Iteration 10/25 | Loss: 0.00128606
Iteration 11/25 | Loss: 0.00128606
Iteration 12/25 | Loss: 0.00128606
Iteration 13/25 | Loss: 0.00128606
Iteration 14/25 | Loss: 0.00128606
Iteration 15/25 | Loss: 0.00128606
Iteration 16/25 | Loss: 0.00128606
Iteration 17/25 | Loss: 0.00128606
Iteration 18/25 | Loss: 0.00128606
Iteration 19/25 | Loss: 0.00128606
Iteration 20/25 | Loss: 0.00128606
Iteration 21/25 | Loss: 0.00128606
Iteration 22/25 | Loss: 0.00128606
Iteration 23/25 | Loss: 0.00128606
Iteration 24/25 | Loss: 0.00128606
Iteration 25/25 | Loss: 0.00128606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78275794
Iteration 2/25 | Loss: 0.00169084
Iteration 3/25 | Loss: 0.00169083
Iteration 4/25 | Loss: 0.00169083
Iteration 5/25 | Loss: 0.00169083
Iteration 6/25 | Loss: 0.00169083
Iteration 7/25 | Loss: 0.00169083
Iteration 8/25 | Loss: 0.00169083
Iteration 9/25 | Loss: 0.00169083
Iteration 10/25 | Loss: 0.00169083
Iteration 11/25 | Loss: 0.00169083
Iteration 12/25 | Loss: 0.00169083
Iteration 13/25 | Loss: 0.00169083
Iteration 14/25 | Loss: 0.00169083
Iteration 15/25 | Loss: 0.00169083
Iteration 16/25 | Loss: 0.00169083
Iteration 17/25 | Loss: 0.00169083
Iteration 18/25 | Loss: 0.00169083
Iteration 19/25 | Loss: 0.00169083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001690831733867526, 0.001690831733867526, 0.001690831733867526, 0.001690831733867526, 0.001690831733867526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001690831733867526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169083
Iteration 2/1000 | Loss: 0.00002781
Iteration 3/1000 | Loss: 0.00002112
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001692
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001547
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001412
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001395
Iteration 28/1000 | Loss: 0.00001394
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001394
Iteration 31/1000 | Loss: 0.00001394
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001391
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001390
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001387
Iteration 48/1000 | Loss: 0.00001387
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001386
Iteration 52/1000 | Loss: 0.00001386
Iteration 53/1000 | Loss: 0.00001386
Iteration 54/1000 | Loss: 0.00001386
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001385
Iteration 62/1000 | Loss: 0.00001385
Iteration 63/1000 | Loss: 0.00001385
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001384
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001383
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001383
Iteration 72/1000 | Loss: 0.00001383
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001382
Iteration 82/1000 | Loss: 0.00001382
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001380
Iteration 86/1000 | Loss: 0.00001380
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001376
Iteration 104/1000 | Loss: 0.00001376
Iteration 105/1000 | Loss: 0.00001376
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001376
Iteration 108/1000 | Loss: 0.00001376
Iteration 109/1000 | Loss: 0.00001376
Iteration 110/1000 | Loss: 0.00001376
Iteration 111/1000 | Loss: 0.00001376
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001374
Iteration 116/1000 | Loss: 0.00001374
Iteration 117/1000 | Loss: 0.00001374
Iteration 118/1000 | Loss: 0.00001374
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001373
Iteration 127/1000 | Loss: 0.00001373
Iteration 128/1000 | Loss: 0.00001373
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001371
Iteration 134/1000 | Loss: 0.00001371
Iteration 135/1000 | Loss: 0.00001371
Iteration 136/1000 | Loss: 0.00001371
Iteration 137/1000 | Loss: 0.00001371
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001370
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001369
Iteration 154/1000 | Loss: 0.00001369
Iteration 155/1000 | Loss: 0.00001369
Iteration 156/1000 | Loss: 0.00001369
Iteration 157/1000 | Loss: 0.00001369
Iteration 158/1000 | Loss: 0.00001369
Iteration 159/1000 | Loss: 0.00001369
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001368
Iteration 168/1000 | Loss: 0.00001368
Iteration 169/1000 | Loss: 0.00001368
Iteration 170/1000 | Loss: 0.00001368
Iteration 171/1000 | Loss: 0.00001368
Iteration 172/1000 | Loss: 0.00001368
Iteration 173/1000 | Loss: 0.00001368
Iteration 174/1000 | Loss: 0.00001368
Iteration 175/1000 | Loss: 0.00001368
Iteration 176/1000 | Loss: 0.00001368
Iteration 177/1000 | Loss: 0.00001368
Iteration 178/1000 | Loss: 0.00001368
Iteration 179/1000 | Loss: 0.00001368
Iteration 180/1000 | Loss: 0.00001368
Iteration 181/1000 | Loss: 0.00001368
Iteration 182/1000 | Loss: 0.00001368
Iteration 183/1000 | Loss: 0.00001368
Iteration 184/1000 | Loss: 0.00001368
Iteration 185/1000 | Loss: 0.00001368
Iteration 186/1000 | Loss: 0.00001368
Iteration 187/1000 | Loss: 0.00001368
Iteration 188/1000 | Loss: 0.00001368
Iteration 189/1000 | Loss: 0.00001368
Iteration 190/1000 | Loss: 0.00001368
Iteration 191/1000 | Loss: 0.00001368
Iteration 192/1000 | Loss: 0.00001368
Iteration 193/1000 | Loss: 0.00001368
Iteration 194/1000 | Loss: 0.00001368
Iteration 195/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3679786206921563e-05, 1.3679786206921563e-05, 1.3679786206921563e-05, 1.3679786206921563e-05, 1.3679786206921563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3679786206921563e-05

Optimization complete. Final v2v error: 3.1100261211395264 mm

Highest mean error: 3.3993847370147705 mm for frame 56

Lowest mean error: 2.73478627204895 mm for frame 217

Saving results

Total time: 53.41943717002869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043834
Iteration 2/25 | Loss: 0.00142155
Iteration 3/25 | Loss: 0.00125176
Iteration 4/25 | Loss: 0.00123593
Iteration 5/25 | Loss: 0.00123360
Iteration 6/25 | Loss: 0.00123303
Iteration 7/25 | Loss: 0.00123278
Iteration 8/25 | Loss: 0.00123266
Iteration 9/25 | Loss: 0.00123256
Iteration 10/25 | Loss: 0.00123247
Iteration 11/25 | Loss: 0.00123242
Iteration 12/25 | Loss: 0.00123240
Iteration 13/25 | Loss: 0.00123239
Iteration 14/25 | Loss: 0.00123239
Iteration 15/25 | Loss: 0.00123239
Iteration 16/25 | Loss: 0.00123239
Iteration 17/25 | Loss: 0.00123239
Iteration 18/25 | Loss: 0.00123239
Iteration 19/25 | Loss: 0.00123239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012323928531259298, 0.0012323928531259298, 0.0012323928531259298, 0.0012323928531259298, 0.0012323928531259298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012323928531259298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18950796
Iteration 2/25 | Loss: 0.00163594
Iteration 3/25 | Loss: 0.00163594
Iteration 4/25 | Loss: 0.00163594
Iteration 5/25 | Loss: 0.00163594
Iteration 6/25 | Loss: 0.00163594
Iteration 7/25 | Loss: 0.00163594
Iteration 8/25 | Loss: 0.00163594
Iteration 9/25 | Loss: 0.00163594
Iteration 10/25 | Loss: 0.00163594
Iteration 11/25 | Loss: 0.00163594
Iteration 12/25 | Loss: 0.00163593
Iteration 13/25 | Loss: 0.00163593
Iteration 14/25 | Loss: 0.00163593
Iteration 15/25 | Loss: 0.00163593
Iteration 16/25 | Loss: 0.00163593
Iteration 17/25 | Loss: 0.00163593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016359349247068167, 0.0016359349247068167, 0.0016359349247068167, 0.0016359349247068167, 0.0016359349247068167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016359349247068167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163593
Iteration 2/1000 | Loss: 0.00008111
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001513
Iteration 7/1000 | Loss: 0.00001456
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001392
Iteration 10/1000 | Loss: 0.00001372
Iteration 11/1000 | Loss: 0.00001347
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001321
Iteration 14/1000 | Loss: 0.00051995
Iteration 15/1000 | Loss: 0.00009387
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00003661
Iteration 18/1000 | Loss: 0.00009000
Iteration 19/1000 | Loss: 0.00003149
Iteration 20/1000 | Loss: 0.00001820
Iteration 21/1000 | Loss: 0.00001132
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00004068
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001044
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001034
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001027
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001023
Iteration 42/1000 | Loss: 0.00001022
Iteration 43/1000 | Loss: 0.00001019
Iteration 44/1000 | Loss: 0.00001019
Iteration 45/1000 | Loss: 0.00001018
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001013
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001013
Iteration 51/1000 | Loss: 0.00001013
Iteration 52/1000 | Loss: 0.00001013
Iteration 53/1000 | Loss: 0.00001012
Iteration 54/1000 | Loss: 0.00001012
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001010
Iteration 59/1000 | Loss: 0.00001009
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001007
Iteration 65/1000 | Loss: 0.00001007
Iteration 66/1000 | Loss: 0.00001007
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001006
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001004
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001002
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001000
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000998
Iteration 100/1000 | Loss: 0.00000998
Iteration 101/1000 | Loss: 0.00000998
Iteration 102/1000 | Loss: 0.00000998
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000997
Iteration 113/1000 | Loss: 0.00000997
Iteration 114/1000 | Loss: 0.00000997
Iteration 115/1000 | Loss: 0.00000997
Iteration 116/1000 | Loss: 0.00000997
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000997
Iteration 130/1000 | Loss: 0.00000997
Iteration 131/1000 | Loss: 0.00000997
Iteration 132/1000 | Loss: 0.00000997
Iteration 133/1000 | Loss: 0.00000997
Iteration 134/1000 | Loss: 0.00000997
Iteration 135/1000 | Loss: 0.00000997
Iteration 136/1000 | Loss: 0.00000997
Iteration 137/1000 | Loss: 0.00000997
Iteration 138/1000 | Loss: 0.00000997
Iteration 139/1000 | Loss: 0.00000997
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000997
Iteration 144/1000 | Loss: 0.00000997
Iteration 145/1000 | Loss: 0.00000997
Iteration 146/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.965542631107382e-06, 9.965542631107382e-06, 9.965542631107382e-06, 9.965542631107382e-06, 9.965542631107382e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.965542631107382e-06

Optimization complete. Final v2v error: 2.7467877864837646 mm

Highest mean error: 3.5297741889953613 mm for frame 62

Lowest mean error: 2.6432483196258545 mm for frame 22

Saving results

Total time: 61.83697485923767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859917
Iteration 2/25 | Loss: 0.00314663
Iteration 3/25 | Loss: 0.00226262
Iteration 4/25 | Loss: 0.00186151
Iteration 5/25 | Loss: 0.00187034
Iteration 6/25 | Loss: 0.00180124
Iteration 7/25 | Loss: 0.00174861
Iteration 8/25 | Loss: 0.00168019
Iteration 9/25 | Loss: 0.00160038
Iteration 10/25 | Loss: 0.00158134
Iteration 11/25 | Loss: 0.00158768
Iteration 12/25 | Loss: 0.00157955
Iteration 13/25 | Loss: 0.00154719
Iteration 14/25 | Loss: 0.00153910
Iteration 15/25 | Loss: 0.00154793
Iteration 16/25 | Loss: 0.00152034
Iteration 17/25 | Loss: 0.00151495
Iteration 18/25 | Loss: 0.00153460
Iteration 19/25 | Loss: 0.00151187
Iteration 20/25 | Loss: 0.00151586
Iteration 21/25 | Loss: 0.00149592
Iteration 22/25 | Loss: 0.00149425
Iteration 23/25 | Loss: 0.00149406
Iteration 24/25 | Loss: 0.00149376
Iteration 25/25 | Loss: 0.00149351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25286579
Iteration 2/25 | Loss: 0.00494508
Iteration 3/25 | Loss: 0.00466115
Iteration 4/25 | Loss: 0.00466115
Iteration 5/25 | Loss: 0.00466115
Iteration 6/25 | Loss: 0.00466115
Iteration 7/25 | Loss: 0.00466115
Iteration 8/25 | Loss: 0.00466115
Iteration 9/25 | Loss: 0.00466115
Iteration 10/25 | Loss: 0.00466115
Iteration 11/25 | Loss: 0.00466115
Iteration 12/25 | Loss: 0.00466115
Iteration 13/25 | Loss: 0.00466115
Iteration 14/25 | Loss: 0.00466115
Iteration 15/25 | Loss: 0.00466115
Iteration 16/25 | Loss: 0.00466115
Iteration 17/25 | Loss: 0.00466115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00466114841401577, 0.00466114841401577, 0.00466114841401577, 0.00466114841401577, 0.00466114841401577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00466114841401577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00466115
Iteration 2/1000 | Loss: 0.00117150
Iteration 3/1000 | Loss: 0.00071787
Iteration 4/1000 | Loss: 0.00289602
Iteration 5/1000 | Loss: 0.00091378
Iteration 6/1000 | Loss: 0.00171982
Iteration 7/1000 | Loss: 0.00058888
Iteration 8/1000 | Loss: 0.00077760
Iteration 9/1000 | Loss: 0.00059987
Iteration 10/1000 | Loss: 0.00213329
Iteration 11/1000 | Loss: 0.00077263
Iteration 12/1000 | Loss: 0.00051334
Iteration 13/1000 | Loss: 0.00012932
Iteration 14/1000 | Loss: 0.00025009
Iteration 15/1000 | Loss: 0.00080438
Iteration 16/1000 | Loss: 0.00012578
Iteration 17/1000 | Loss: 0.00015206
Iteration 18/1000 | Loss: 0.00145326
Iteration 19/1000 | Loss: 0.00021824
Iteration 20/1000 | Loss: 0.00010489
Iteration 21/1000 | Loss: 0.00294710
Iteration 22/1000 | Loss: 0.00258136
Iteration 23/1000 | Loss: 0.00038814
Iteration 24/1000 | Loss: 0.00074251
Iteration 25/1000 | Loss: 0.00031518
Iteration 26/1000 | Loss: 0.00078719
Iteration 27/1000 | Loss: 0.00053665
Iteration 28/1000 | Loss: 0.00151197
Iteration 29/1000 | Loss: 0.00042386
Iteration 30/1000 | Loss: 0.00022949
Iteration 31/1000 | Loss: 0.00018445
Iteration 32/1000 | Loss: 0.00015438
Iteration 33/1000 | Loss: 0.00014301
Iteration 34/1000 | Loss: 0.00013394
Iteration 35/1000 | Loss: 0.00070275
Iteration 36/1000 | Loss: 0.00199016
Iteration 37/1000 | Loss: 0.00058596
Iteration 38/1000 | Loss: 0.00050443
Iteration 39/1000 | Loss: 0.00069786
Iteration 40/1000 | Loss: 0.00034922
Iteration 41/1000 | Loss: 0.00027405
Iteration 42/1000 | Loss: 0.00008344
Iteration 43/1000 | Loss: 0.00007017
Iteration 44/1000 | Loss: 0.00015283
Iteration 45/1000 | Loss: 0.00007331
Iteration 46/1000 | Loss: 0.00030348
Iteration 47/1000 | Loss: 0.00042538
Iteration 48/1000 | Loss: 0.00063622
Iteration 49/1000 | Loss: 0.00036824
Iteration 50/1000 | Loss: 0.00039690
Iteration 51/1000 | Loss: 0.00088846
Iteration 52/1000 | Loss: 0.00060049
Iteration 53/1000 | Loss: 0.00045003
Iteration 54/1000 | Loss: 0.00040800
Iteration 55/1000 | Loss: 0.00023483
Iteration 56/1000 | Loss: 0.00014919
Iteration 57/1000 | Loss: 0.00069618
Iteration 58/1000 | Loss: 0.00025415
Iteration 59/1000 | Loss: 0.00059840
Iteration 60/1000 | Loss: 0.00036316
Iteration 61/1000 | Loss: 0.00017994
Iteration 62/1000 | Loss: 0.00022750
Iteration 63/1000 | Loss: 0.00011896
Iteration 64/1000 | Loss: 0.00009417
Iteration 65/1000 | Loss: 0.00005167
Iteration 66/1000 | Loss: 0.00036357
Iteration 67/1000 | Loss: 0.00029232
Iteration 68/1000 | Loss: 0.00007471
Iteration 69/1000 | Loss: 0.00021814
Iteration 70/1000 | Loss: 0.00016626
Iteration 71/1000 | Loss: 0.00038538
Iteration 72/1000 | Loss: 0.00027020
Iteration 73/1000 | Loss: 0.00005064
Iteration 74/1000 | Loss: 0.00141924
Iteration 75/1000 | Loss: 0.00071078
Iteration 76/1000 | Loss: 0.00011185
Iteration 77/1000 | Loss: 0.00021350
Iteration 78/1000 | Loss: 0.00023597
Iteration 79/1000 | Loss: 0.00018086
Iteration 80/1000 | Loss: 0.00021161
Iteration 81/1000 | Loss: 0.00056058
Iteration 82/1000 | Loss: 0.00019072
Iteration 83/1000 | Loss: 0.00010625
Iteration 84/1000 | Loss: 0.00014738
Iteration 85/1000 | Loss: 0.00027783
Iteration 86/1000 | Loss: 0.00009985
Iteration 87/1000 | Loss: 0.00004510
Iteration 88/1000 | Loss: 0.00036030
Iteration 89/1000 | Loss: 0.00028198
Iteration 90/1000 | Loss: 0.00018607
Iteration 91/1000 | Loss: 0.00008834
Iteration 92/1000 | Loss: 0.00009094
Iteration 93/1000 | Loss: 0.00004100
Iteration 94/1000 | Loss: 0.00026937
Iteration 95/1000 | Loss: 0.00011243
Iteration 96/1000 | Loss: 0.00013753
Iteration 97/1000 | Loss: 0.00013055
Iteration 98/1000 | Loss: 0.00012484
Iteration 99/1000 | Loss: 0.00023728
Iteration 100/1000 | Loss: 0.00025371
Iteration 101/1000 | Loss: 0.00027177
Iteration 102/1000 | Loss: 0.00023816
Iteration 103/1000 | Loss: 0.00061647
Iteration 104/1000 | Loss: 0.00088082
Iteration 105/1000 | Loss: 0.00069883
Iteration 106/1000 | Loss: 0.00015031
Iteration 107/1000 | Loss: 0.00005439
Iteration 108/1000 | Loss: 0.00003752
Iteration 109/1000 | Loss: 0.00003414
Iteration 110/1000 | Loss: 0.00013600
Iteration 111/1000 | Loss: 0.00005941
Iteration 112/1000 | Loss: 0.00004106
Iteration 113/1000 | Loss: 0.00004757
Iteration 114/1000 | Loss: 0.00003149
Iteration 115/1000 | Loss: 0.00026241
Iteration 116/1000 | Loss: 0.00021037
Iteration 117/1000 | Loss: 0.00056092
Iteration 118/1000 | Loss: 0.00027083
Iteration 119/1000 | Loss: 0.00014542
Iteration 120/1000 | Loss: 0.00040435
Iteration 121/1000 | Loss: 0.00048239
Iteration 122/1000 | Loss: 0.00050440
Iteration 123/1000 | Loss: 0.00031407
Iteration 124/1000 | Loss: 0.00058434
Iteration 125/1000 | Loss: 0.00032251
Iteration 126/1000 | Loss: 0.00065858
Iteration 127/1000 | Loss: 0.00080577
Iteration 128/1000 | Loss: 0.00027071
Iteration 129/1000 | Loss: 0.00051891
Iteration 130/1000 | Loss: 0.00046946
Iteration 131/1000 | Loss: 0.00043199
Iteration 132/1000 | Loss: 0.00015133
Iteration 133/1000 | Loss: 0.00030622
Iteration 134/1000 | Loss: 0.00035343
Iteration 135/1000 | Loss: 0.00026544
Iteration 136/1000 | Loss: 0.00012707
Iteration 137/1000 | Loss: 0.00014723
Iteration 138/1000 | Loss: 0.00030292
Iteration 139/1000 | Loss: 0.00026583
Iteration 140/1000 | Loss: 0.00068494
Iteration 141/1000 | Loss: 0.00011102
Iteration 142/1000 | Loss: 0.00003852
Iteration 143/1000 | Loss: 0.00003430
Iteration 144/1000 | Loss: 0.00003201
Iteration 145/1000 | Loss: 0.00004217
Iteration 146/1000 | Loss: 0.00002971
Iteration 147/1000 | Loss: 0.00002883
Iteration 148/1000 | Loss: 0.00023910
Iteration 149/1000 | Loss: 0.00010439
Iteration 150/1000 | Loss: 0.00016274
Iteration 151/1000 | Loss: 0.00003392
Iteration 152/1000 | Loss: 0.00003001
Iteration 153/1000 | Loss: 0.00002799
Iteration 154/1000 | Loss: 0.00002710
Iteration 155/1000 | Loss: 0.00002595
Iteration 156/1000 | Loss: 0.00092478
Iteration 157/1000 | Loss: 0.00058230
Iteration 158/1000 | Loss: 0.00023725
Iteration 159/1000 | Loss: 0.00023865
Iteration 160/1000 | Loss: 0.00036971
Iteration 161/1000 | Loss: 0.00002632
Iteration 162/1000 | Loss: 0.00002333
Iteration 163/1000 | Loss: 0.00002159
Iteration 164/1000 | Loss: 0.00002074
Iteration 165/1000 | Loss: 0.00002013
Iteration 166/1000 | Loss: 0.00001981
Iteration 167/1000 | Loss: 0.00032648
Iteration 168/1000 | Loss: 0.00020748
Iteration 169/1000 | Loss: 0.00002517
Iteration 170/1000 | Loss: 0.00002125
Iteration 171/1000 | Loss: 0.00001993
Iteration 172/1000 | Loss: 0.00001951
Iteration 173/1000 | Loss: 0.00001932
Iteration 174/1000 | Loss: 0.00001924
Iteration 175/1000 | Loss: 0.00001919
Iteration 176/1000 | Loss: 0.00036249
Iteration 177/1000 | Loss: 0.00019585
Iteration 178/1000 | Loss: 0.00035183
Iteration 179/1000 | Loss: 0.00018030
Iteration 180/1000 | Loss: 0.00025390
Iteration 181/1000 | Loss: 0.00019101
Iteration 182/1000 | Loss: 0.00008931
Iteration 183/1000 | Loss: 0.00027960
Iteration 184/1000 | Loss: 0.00006484
Iteration 185/1000 | Loss: 0.00040866
Iteration 186/1000 | Loss: 0.00003471
Iteration 187/1000 | Loss: 0.00008118
Iteration 188/1000 | Loss: 0.00002334
Iteration 189/1000 | Loss: 0.00002087
Iteration 190/1000 | Loss: 0.00028694
Iteration 191/1000 | Loss: 0.00002784
Iteration 192/1000 | Loss: 0.00013335
Iteration 193/1000 | Loss: 0.00002285
Iteration 194/1000 | Loss: 0.00029197
Iteration 195/1000 | Loss: 0.00002746
Iteration 196/1000 | Loss: 0.00012448
Iteration 197/1000 | Loss: 0.00002120
Iteration 198/1000 | Loss: 0.00036361
Iteration 199/1000 | Loss: 0.00023463
Iteration 200/1000 | Loss: 0.00035493
Iteration 201/1000 | Loss: 0.00030609
Iteration 202/1000 | Loss: 0.00026195
Iteration 203/1000 | Loss: 0.00021312
Iteration 204/1000 | Loss: 0.00020686
Iteration 205/1000 | Loss: 0.00036248
Iteration 206/1000 | Loss: 0.00028538
Iteration 207/1000 | Loss: 0.00003596
Iteration 208/1000 | Loss: 0.00029025
Iteration 209/1000 | Loss: 0.00031032
Iteration 210/1000 | Loss: 0.00031606
Iteration 211/1000 | Loss: 0.00028701
Iteration 212/1000 | Loss: 0.00031700
Iteration 213/1000 | Loss: 0.00028025
Iteration 214/1000 | Loss: 0.00011119
Iteration 215/1000 | Loss: 0.00002611
Iteration 216/1000 | Loss: 0.00041206
Iteration 217/1000 | Loss: 0.00009567
Iteration 218/1000 | Loss: 0.00003893
Iteration 219/1000 | Loss: 0.00002230
Iteration 220/1000 | Loss: 0.00002955
Iteration 221/1000 | Loss: 0.00002898
Iteration 222/1000 | Loss: 0.00001977
Iteration 223/1000 | Loss: 0.00002151
Iteration 224/1000 | Loss: 0.00001780
Iteration 225/1000 | Loss: 0.00001734
Iteration 226/1000 | Loss: 0.00001706
Iteration 227/1000 | Loss: 0.00001698
Iteration 228/1000 | Loss: 0.00001692
Iteration 229/1000 | Loss: 0.00001692
Iteration 230/1000 | Loss: 0.00001692
Iteration 231/1000 | Loss: 0.00001692
Iteration 232/1000 | Loss: 0.00001692
Iteration 233/1000 | Loss: 0.00001691
Iteration 234/1000 | Loss: 0.00001691
Iteration 235/1000 | Loss: 0.00001691
Iteration 236/1000 | Loss: 0.00001691
Iteration 237/1000 | Loss: 0.00001691
Iteration 238/1000 | Loss: 0.00001691
Iteration 239/1000 | Loss: 0.00001691
Iteration 240/1000 | Loss: 0.00001689
Iteration 241/1000 | Loss: 0.00001689
Iteration 242/1000 | Loss: 0.00001689
Iteration 243/1000 | Loss: 0.00001689
Iteration 244/1000 | Loss: 0.00001688
Iteration 245/1000 | Loss: 0.00001688
Iteration 246/1000 | Loss: 0.00001688
Iteration 247/1000 | Loss: 0.00001688
Iteration 248/1000 | Loss: 0.00001688
Iteration 249/1000 | Loss: 0.00001688
Iteration 250/1000 | Loss: 0.00001688
Iteration 251/1000 | Loss: 0.00001688
Iteration 252/1000 | Loss: 0.00001688
Iteration 253/1000 | Loss: 0.00001687
Iteration 254/1000 | Loss: 0.00001687
Iteration 255/1000 | Loss: 0.00001686
Iteration 256/1000 | Loss: 0.00001685
Iteration 257/1000 | Loss: 0.00001685
Iteration 258/1000 | Loss: 0.00001681
Iteration 259/1000 | Loss: 0.00001681
Iteration 260/1000 | Loss: 0.00001681
Iteration 261/1000 | Loss: 0.00001681
Iteration 262/1000 | Loss: 0.00001681
Iteration 263/1000 | Loss: 0.00001680
Iteration 264/1000 | Loss: 0.00001680
Iteration 265/1000 | Loss: 0.00001680
Iteration 266/1000 | Loss: 0.00001680
Iteration 267/1000 | Loss: 0.00001680
Iteration 268/1000 | Loss: 0.00001680
Iteration 269/1000 | Loss: 0.00001680
Iteration 270/1000 | Loss: 0.00001680
Iteration 271/1000 | Loss: 0.00001679
Iteration 272/1000 | Loss: 0.00001679
Iteration 273/1000 | Loss: 0.00001679
Iteration 274/1000 | Loss: 0.00001678
Iteration 275/1000 | Loss: 0.00001678
Iteration 276/1000 | Loss: 0.00001678
Iteration 277/1000 | Loss: 0.00001677
Iteration 278/1000 | Loss: 0.00001677
Iteration 279/1000 | Loss: 0.00001677
Iteration 280/1000 | Loss: 0.00001677
Iteration 281/1000 | Loss: 0.00001677
Iteration 282/1000 | Loss: 0.00001676
Iteration 283/1000 | Loss: 0.00001676
Iteration 284/1000 | Loss: 0.00001676
Iteration 285/1000 | Loss: 0.00001675
Iteration 286/1000 | Loss: 0.00001674
Iteration 287/1000 | Loss: 0.00001674
Iteration 288/1000 | Loss: 0.00001674
Iteration 289/1000 | Loss: 0.00001673
Iteration 290/1000 | Loss: 0.00001673
Iteration 291/1000 | Loss: 0.00042698
Iteration 292/1000 | Loss: 0.00017317
Iteration 293/1000 | Loss: 0.00044800
Iteration 294/1000 | Loss: 0.00032639
Iteration 295/1000 | Loss: 0.00002169
Iteration 296/1000 | Loss: 0.00001779
Iteration 297/1000 | Loss: 0.00001700
Iteration 298/1000 | Loss: 0.00001684
Iteration 299/1000 | Loss: 0.00001683
Iteration 300/1000 | Loss: 0.00001680
Iteration 301/1000 | Loss: 0.00001680
Iteration 302/1000 | Loss: 0.00001678
Iteration 303/1000 | Loss: 0.00001678
Iteration 304/1000 | Loss: 0.00001678
Iteration 305/1000 | Loss: 0.00001678
Iteration 306/1000 | Loss: 0.00001678
Iteration 307/1000 | Loss: 0.00001678
Iteration 308/1000 | Loss: 0.00001677
Iteration 309/1000 | Loss: 0.00001677
Iteration 310/1000 | Loss: 0.00001677
Iteration 311/1000 | Loss: 0.00001676
Iteration 312/1000 | Loss: 0.00001676
Iteration 313/1000 | Loss: 0.00001675
Iteration 314/1000 | Loss: 0.00001672
Iteration 315/1000 | Loss: 0.00001672
Iteration 316/1000 | Loss: 0.00001671
Iteration 317/1000 | Loss: 0.00001671
Iteration 318/1000 | Loss: 0.00001671
Iteration 319/1000 | Loss: 0.00001670
Iteration 320/1000 | Loss: 0.00001670
Iteration 321/1000 | Loss: 0.00001670
Iteration 322/1000 | Loss: 0.00001670
Iteration 323/1000 | Loss: 0.00001669
Iteration 324/1000 | Loss: 0.00001669
Iteration 325/1000 | Loss: 0.00001668
Iteration 326/1000 | Loss: 0.00001668
Iteration 327/1000 | Loss: 0.00001668
Iteration 328/1000 | Loss: 0.00001668
Iteration 329/1000 | Loss: 0.00001667
Iteration 330/1000 | Loss: 0.00001667
Iteration 331/1000 | Loss: 0.00001667
Iteration 332/1000 | Loss: 0.00001667
Iteration 333/1000 | Loss: 0.00001666
Iteration 334/1000 | Loss: 0.00001666
Iteration 335/1000 | Loss: 0.00001665
Iteration 336/1000 | Loss: 0.00001665
Iteration 337/1000 | Loss: 0.00001665
Iteration 338/1000 | Loss: 0.00001665
Iteration 339/1000 | Loss: 0.00001665
Iteration 340/1000 | Loss: 0.00001665
Iteration 341/1000 | Loss: 0.00001664
Iteration 342/1000 | Loss: 0.00001664
Iteration 343/1000 | Loss: 0.00001664
Iteration 344/1000 | Loss: 0.00001664
Iteration 345/1000 | Loss: 0.00001664
Iteration 346/1000 | Loss: 0.00001663
Iteration 347/1000 | Loss: 0.00001663
Iteration 348/1000 | Loss: 0.00001663
Iteration 349/1000 | Loss: 0.00001663
Iteration 350/1000 | Loss: 0.00001662
Iteration 351/1000 | Loss: 0.00001662
Iteration 352/1000 | Loss: 0.00001662
Iteration 353/1000 | Loss: 0.00001662
Iteration 354/1000 | Loss: 0.00001662
Iteration 355/1000 | Loss: 0.00001662
Iteration 356/1000 | Loss: 0.00001662
Iteration 357/1000 | Loss: 0.00001662
Iteration 358/1000 | Loss: 0.00001662
Iteration 359/1000 | Loss: 0.00001662
Iteration 360/1000 | Loss: 0.00001661
Iteration 361/1000 | Loss: 0.00001661
Iteration 362/1000 | Loss: 0.00001661
Iteration 363/1000 | Loss: 0.00001661
Iteration 364/1000 | Loss: 0.00001661
Iteration 365/1000 | Loss: 0.00001661
Iteration 366/1000 | Loss: 0.00001661
Iteration 367/1000 | Loss: 0.00001661
Iteration 368/1000 | Loss: 0.00001661
Iteration 369/1000 | Loss: 0.00001660
Iteration 370/1000 | Loss: 0.00001660
Iteration 371/1000 | Loss: 0.00001660
Iteration 372/1000 | Loss: 0.00001660
Iteration 373/1000 | Loss: 0.00001660
Iteration 374/1000 | Loss: 0.00001660
Iteration 375/1000 | Loss: 0.00001660
Iteration 376/1000 | Loss: 0.00001660
Iteration 377/1000 | Loss: 0.00001660
Iteration 378/1000 | Loss: 0.00001660
Iteration 379/1000 | Loss: 0.00001659
Iteration 380/1000 | Loss: 0.00001659
Iteration 381/1000 | Loss: 0.00001659
Iteration 382/1000 | Loss: 0.00001659
Iteration 383/1000 | Loss: 0.00001659
Iteration 384/1000 | Loss: 0.00001659
Iteration 385/1000 | Loss: 0.00001659
Iteration 386/1000 | Loss: 0.00001658
Iteration 387/1000 | Loss: 0.00001658
Iteration 388/1000 | Loss: 0.00001658
Iteration 389/1000 | Loss: 0.00001658
Iteration 390/1000 | Loss: 0.00001658
Iteration 391/1000 | Loss: 0.00001658
Iteration 392/1000 | Loss: 0.00001658
Iteration 393/1000 | Loss: 0.00001658
Iteration 394/1000 | Loss: 0.00001658
Iteration 395/1000 | Loss: 0.00001657
Iteration 396/1000 | Loss: 0.00001657
Iteration 397/1000 | Loss: 0.00001657
Iteration 398/1000 | Loss: 0.00001656
Iteration 399/1000 | Loss: 0.00001656
Iteration 400/1000 | Loss: 0.00001656
Iteration 401/1000 | Loss: 0.00001656
Iteration 402/1000 | Loss: 0.00001656
Iteration 403/1000 | Loss: 0.00001656
Iteration 404/1000 | Loss: 0.00001656
Iteration 405/1000 | Loss: 0.00001656
Iteration 406/1000 | Loss: 0.00001656
Iteration 407/1000 | Loss: 0.00001656
Iteration 408/1000 | Loss: 0.00001656
Iteration 409/1000 | Loss: 0.00001656
Iteration 410/1000 | Loss: 0.00001655
Iteration 411/1000 | Loss: 0.00001655
Iteration 412/1000 | Loss: 0.00001655
Iteration 413/1000 | Loss: 0.00001655
Iteration 414/1000 | Loss: 0.00001655
Iteration 415/1000 | Loss: 0.00001655
Iteration 416/1000 | Loss: 0.00001655
Iteration 417/1000 | Loss: 0.00001655
Iteration 418/1000 | Loss: 0.00001655
Iteration 419/1000 | Loss: 0.00001655
Iteration 420/1000 | Loss: 0.00001655
Iteration 421/1000 | Loss: 0.00001655
Iteration 422/1000 | Loss: 0.00001655
Iteration 423/1000 | Loss: 0.00001655
Iteration 424/1000 | Loss: 0.00001655
Iteration 425/1000 | Loss: 0.00001655
Iteration 426/1000 | Loss: 0.00001655
Iteration 427/1000 | Loss: 0.00001655
Iteration 428/1000 | Loss: 0.00001655
Iteration 429/1000 | Loss: 0.00001655
Iteration 430/1000 | Loss: 0.00001655
Iteration 431/1000 | Loss: 0.00001655
Iteration 432/1000 | Loss: 0.00001655
Iteration 433/1000 | Loss: 0.00001655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 433. Stopping optimization.
Last 5 losses: [1.6553984096390195e-05, 1.6553984096390195e-05, 1.6553984096390195e-05, 1.6553984096390195e-05, 1.6553984096390195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6553984096390195e-05

Optimization complete. Final v2v error: 3.0391845703125 mm

Highest mean error: 12.907787322998047 mm for frame 111

Lowest mean error: 2.538282632827759 mm for frame 153

Saving results

Total time: 438.9329445362091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596388
Iteration 2/25 | Loss: 0.00166344
Iteration 3/25 | Loss: 0.00137613
Iteration 4/25 | Loss: 0.00136331
Iteration 5/25 | Loss: 0.00136011
Iteration 6/25 | Loss: 0.00135921
Iteration 7/25 | Loss: 0.00135921
Iteration 8/25 | Loss: 0.00135921
Iteration 9/25 | Loss: 0.00135921
Iteration 10/25 | Loss: 0.00135921
Iteration 11/25 | Loss: 0.00135921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013592086033895612, 0.0013592086033895612, 0.0013592086033895612, 0.0013592086033895612, 0.0013592086033895612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013592086033895612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98833871
Iteration 2/25 | Loss: 0.00167812
Iteration 3/25 | Loss: 0.00167810
Iteration 4/25 | Loss: 0.00167810
Iteration 5/25 | Loss: 0.00167810
Iteration 6/25 | Loss: 0.00167810
Iteration 7/25 | Loss: 0.00167810
Iteration 8/25 | Loss: 0.00167810
Iteration 9/25 | Loss: 0.00167810
Iteration 10/25 | Loss: 0.00167810
Iteration 11/25 | Loss: 0.00167810
Iteration 12/25 | Loss: 0.00167810
Iteration 13/25 | Loss: 0.00167810
Iteration 14/25 | Loss: 0.00167810
Iteration 15/25 | Loss: 0.00167810
Iteration 16/25 | Loss: 0.00167810
Iteration 17/25 | Loss: 0.00167810
Iteration 18/25 | Loss: 0.00167810
Iteration 19/25 | Loss: 0.00167810
Iteration 20/25 | Loss: 0.00167810
Iteration 21/25 | Loss: 0.00167810
Iteration 22/25 | Loss: 0.00167810
Iteration 23/25 | Loss: 0.00167810
Iteration 24/25 | Loss: 0.00167810
Iteration 25/25 | Loss: 0.00167810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167810
Iteration 2/1000 | Loss: 0.00007817
Iteration 3/1000 | Loss: 0.00004989
Iteration 4/1000 | Loss: 0.00003987
Iteration 5/1000 | Loss: 0.00003649
Iteration 6/1000 | Loss: 0.00003539
Iteration 7/1000 | Loss: 0.00003482
Iteration 8/1000 | Loss: 0.00003417
Iteration 9/1000 | Loss: 0.00003332
Iteration 10/1000 | Loss: 0.00003280
Iteration 11/1000 | Loss: 0.00003230
Iteration 12/1000 | Loss: 0.00003188
Iteration 13/1000 | Loss: 0.00003159
Iteration 14/1000 | Loss: 0.00003128
Iteration 15/1000 | Loss: 0.00003100
Iteration 16/1000 | Loss: 0.00003077
Iteration 17/1000 | Loss: 0.00003061
Iteration 18/1000 | Loss: 0.00003046
Iteration 19/1000 | Loss: 0.00003028
Iteration 20/1000 | Loss: 0.00003015
Iteration 21/1000 | Loss: 0.00003004
Iteration 22/1000 | Loss: 0.00003000
Iteration 23/1000 | Loss: 0.00002994
Iteration 24/1000 | Loss: 0.00002994
Iteration 25/1000 | Loss: 0.00002993
Iteration 26/1000 | Loss: 0.00002992
Iteration 27/1000 | Loss: 0.00002989
Iteration 28/1000 | Loss: 0.00002989
Iteration 29/1000 | Loss: 0.00002989
Iteration 30/1000 | Loss: 0.00002988
Iteration 31/1000 | Loss: 0.00002988
Iteration 32/1000 | Loss: 0.00002988
Iteration 33/1000 | Loss: 0.00002988
Iteration 34/1000 | Loss: 0.00002987
Iteration 35/1000 | Loss: 0.00002987
Iteration 36/1000 | Loss: 0.00002987
Iteration 37/1000 | Loss: 0.00002986
Iteration 38/1000 | Loss: 0.00002986
Iteration 39/1000 | Loss: 0.00002985
Iteration 40/1000 | Loss: 0.00002984
Iteration 41/1000 | Loss: 0.00002984
Iteration 42/1000 | Loss: 0.00002984
Iteration 43/1000 | Loss: 0.00002984
Iteration 44/1000 | Loss: 0.00002984
Iteration 45/1000 | Loss: 0.00002984
Iteration 46/1000 | Loss: 0.00002983
Iteration 47/1000 | Loss: 0.00002983
Iteration 48/1000 | Loss: 0.00002983
Iteration 49/1000 | Loss: 0.00002983
Iteration 50/1000 | Loss: 0.00002983
Iteration 51/1000 | Loss: 0.00002983
Iteration 52/1000 | Loss: 0.00002983
Iteration 53/1000 | Loss: 0.00002983
Iteration 54/1000 | Loss: 0.00002983
Iteration 55/1000 | Loss: 0.00002983
Iteration 56/1000 | Loss: 0.00002983
Iteration 57/1000 | Loss: 0.00002982
Iteration 58/1000 | Loss: 0.00002982
Iteration 59/1000 | Loss: 0.00002981
Iteration 60/1000 | Loss: 0.00002981
Iteration 61/1000 | Loss: 0.00002981
Iteration 62/1000 | Loss: 0.00002980
Iteration 63/1000 | Loss: 0.00002980
Iteration 64/1000 | Loss: 0.00002980
Iteration 65/1000 | Loss: 0.00002980
Iteration 66/1000 | Loss: 0.00002980
Iteration 67/1000 | Loss: 0.00002980
Iteration 68/1000 | Loss: 0.00002980
Iteration 69/1000 | Loss: 0.00002980
Iteration 70/1000 | Loss: 0.00002980
Iteration 71/1000 | Loss: 0.00002980
Iteration 72/1000 | Loss: 0.00002980
Iteration 73/1000 | Loss: 0.00002980
Iteration 74/1000 | Loss: 0.00002980
Iteration 75/1000 | Loss: 0.00002979
Iteration 76/1000 | Loss: 0.00002979
Iteration 77/1000 | Loss: 0.00002979
Iteration 78/1000 | Loss: 0.00002978
Iteration 79/1000 | Loss: 0.00002978
Iteration 80/1000 | Loss: 0.00002978
Iteration 81/1000 | Loss: 0.00002978
Iteration 82/1000 | Loss: 0.00002978
Iteration 83/1000 | Loss: 0.00002978
Iteration 84/1000 | Loss: 0.00002978
Iteration 85/1000 | Loss: 0.00002978
Iteration 86/1000 | Loss: 0.00002978
Iteration 87/1000 | Loss: 0.00002978
Iteration 88/1000 | Loss: 0.00002977
Iteration 89/1000 | Loss: 0.00002977
Iteration 90/1000 | Loss: 0.00002977
Iteration 91/1000 | Loss: 0.00002977
Iteration 92/1000 | Loss: 0.00002976
Iteration 93/1000 | Loss: 0.00002976
Iteration 94/1000 | Loss: 0.00002976
Iteration 95/1000 | Loss: 0.00002975
Iteration 96/1000 | Loss: 0.00002975
Iteration 97/1000 | Loss: 0.00002975
Iteration 98/1000 | Loss: 0.00002975
Iteration 99/1000 | Loss: 0.00002975
Iteration 100/1000 | Loss: 0.00002975
Iteration 101/1000 | Loss: 0.00002975
Iteration 102/1000 | Loss: 0.00002974
Iteration 103/1000 | Loss: 0.00002974
Iteration 104/1000 | Loss: 0.00002974
Iteration 105/1000 | Loss: 0.00002974
Iteration 106/1000 | Loss: 0.00002974
Iteration 107/1000 | Loss: 0.00002974
Iteration 108/1000 | Loss: 0.00002974
Iteration 109/1000 | Loss: 0.00002973
Iteration 110/1000 | Loss: 0.00002973
Iteration 111/1000 | Loss: 0.00002973
Iteration 112/1000 | Loss: 0.00002973
Iteration 113/1000 | Loss: 0.00002973
Iteration 114/1000 | Loss: 0.00002973
Iteration 115/1000 | Loss: 0.00002972
Iteration 116/1000 | Loss: 0.00002972
Iteration 117/1000 | Loss: 0.00002972
Iteration 118/1000 | Loss: 0.00002972
Iteration 119/1000 | Loss: 0.00002971
Iteration 120/1000 | Loss: 0.00002971
Iteration 121/1000 | Loss: 0.00002971
Iteration 122/1000 | Loss: 0.00002971
Iteration 123/1000 | Loss: 0.00002971
Iteration 124/1000 | Loss: 0.00002971
Iteration 125/1000 | Loss: 0.00002971
Iteration 126/1000 | Loss: 0.00002971
Iteration 127/1000 | Loss: 0.00002970
Iteration 128/1000 | Loss: 0.00002970
Iteration 129/1000 | Loss: 0.00002970
Iteration 130/1000 | Loss: 0.00002970
Iteration 131/1000 | Loss: 0.00002970
Iteration 132/1000 | Loss: 0.00002970
Iteration 133/1000 | Loss: 0.00002970
Iteration 134/1000 | Loss: 0.00002970
Iteration 135/1000 | Loss: 0.00002970
Iteration 136/1000 | Loss: 0.00002970
Iteration 137/1000 | Loss: 0.00002970
Iteration 138/1000 | Loss: 0.00002970
Iteration 139/1000 | Loss: 0.00002970
Iteration 140/1000 | Loss: 0.00002970
Iteration 141/1000 | Loss: 0.00002970
Iteration 142/1000 | Loss: 0.00002970
Iteration 143/1000 | Loss: 0.00002970
Iteration 144/1000 | Loss: 0.00002969
Iteration 145/1000 | Loss: 0.00002969
Iteration 146/1000 | Loss: 0.00002969
Iteration 147/1000 | Loss: 0.00002969
Iteration 148/1000 | Loss: 0.00002969
Iteration 149/1000 | Loss: 0.00002969
Iteration 150/1000 | Loss: 0.00002969
Iteration 151/1000 | Loss: 0.00002969
Iteration 152/1000 | Loss: 0.00002969
Iteration 153/1000 | Loss: 0.00002969
Iteration 154/1000 | Loss: 0.00002969
Iteration 155/1000 | Loss: 0.00002969
Iteration 156/1000 | Loss: 0.00002969
Iteration 157/1000 | Loss: 0.00002969
Iteration 158/1000 | Loss: 0.00002969
Iteration 159/1000 | Loss: 0.00002969
Iteration 160/1000 | Loss: 0.00002968
Iteration 161/1000 | Loss: 0.00002968
Iteration 162/1000 | Loss: 0.00002968
Iteration 163/1000 | Loss: 0.00002968
Iteration 164/1000 | Loss: 0.00002968
Iteration 165/1000 | Loss: 0.00002968
Iteration 166/1000 | Loss: 0.00002968
Iteration 167/1000 | Loss: 0.00002968
Iteration 168/1000 | Loss: 0.00002968
Iteration 169/1000 | Loss: 0.00002968
Iteration 170/1000 | Loss: 0.00002968
Iteration 171/1000 | Loss: 0.00002968
Iteration 172/1000 | Loss: 0.00002968
Iteration 173/1000 | Loss: 0.00002968
Iteration 174/1000 | Loss: 0.00002968
Iteration 175/1000 | Loss: 0.00002968
Iteration 176/1000 | Loss: 0.00002968
Iteration 177/1000 | Loss: 0.00002968
Iteration 178/1000 | Loss: 0.00002968
Iteration 179/1000 | Loss: 0.00002968
Iteration 180/1000 | Loss: 0.00002968
Iteration 181/1000 | Loss: 0.00002968
Iteration 182/1000 | Loss: 0.00002967
Iteration 183/1000 | Loss: 0.00002967
Iteration 184/1000 | Loss: 0.00002967
Iteration 185/1000 | Loss: 0.00002967
Iteration 186/1000 | Loss: 0.00002967
Iteration 187/1000 | Loss: 0.00002967
Iteration 188/1000 | Loss: 0.00002967
Iteration 189/1000 | Loss: 0.00002967
Iteration 190/1000 | Loss: 0.00002967
Iteration 191/1000 | Loss: 0.00002967
Iteration 192/1000 | Loss: 0.00002967
Iteration 193/1000 | Loss: 0.00002967
Iteration 194/1000 | Loss: 0.00002967
Iteration 195/1000 | Loss: 0.00002967
Iteration 196/1000 | Loss: 0.00002967
Iteration 197/1000 | Loss: 0.00002967
Iteration 198/1000 | Loss: 0.00002967
Iteration 199/1000 | Loss: 0.00002967
Iteration 200/1000 | Loss: 0.00002967
Iteration 201/1000 | Loss: 0.00002967
Iteration 202/1000 | Loss: 0.00002967
Iteration 203/1000 | Loss: 0.00002967
Iteration 204/1000 | Loss: 0.00002967
Iteration 205/1000 | Loss: 0.00002967
Iteration 206/1000 | Loss: 0.00002967
Iteration 207/1000 | Loss: 0.00002967
Iteration 208/1000 | Loss: 0.00002967
Iteration 209/1000 | Loss: 0.00002967
Iteration 210/1000 | Loss: 0.00002967
Iteration 211/1000 | Loss: 0.00002967
Iteration 212/1000 | Loss: 0.00002967
Iteration 213/1000 | Loss: 0.00002967
Iteration 214/1000 | Loss: 0.00002967
Iteration 215/1000 | Loss: 0.00002967
Iteration 216/1000 | Loss: 0.00002967
Iteration 217/1000 | Loss: 0.00002967
Iteration 218/1000 | Loss: 0.00002967
Iteration 219/1000 | Loss: 0.00002967
Iteration 220/1000 | Loss: 0.00002967
Iteration 221/1000 | Loss: 0.00002967
Iteration 222/1000 | Loss: 0.00002967
Iteration 223/1000 | Loss: 0.00002967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.966735883092042e-05, 2.966735883092042e-05, 2.966735883092042e-05, 2.966735883092042e-05, 2.966735883092042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.966735883092042e-05

Optimization complete. Final v2v error: 4.116045951843262 mm

Highest mean error: 5.169551372528076 mm for frame 105

Lowest mean error: 3.0777812004089355 mm for frame 48

Saving results

Total time: 52.80510711669922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387419
Iteration 2/25 | Loss: 0.00134743
Iteration 3/25 | Loss: 0.00124366
Iteration 4/25 | Loss: 0.00123401
Iteration 5/25 | Loss: 0.00123113
Iteration 6/25 | Loss: 0.00123033
Iteration 7/25 | Loss: 0.00123033
Iteration 8/25 | Loss: 0.00123033
Iteration 9/25 | Loss: 0.00123033
Iteration 10/25 | Loss: 0.00123033
Iteration 11/25 | Loss: 0.00123033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012303258990868926, 0.0012303258990868926, 0.0012303258990868926, 0.0012303258990868926, 0.0012303258990868926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012303258990868926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20949113
Iteration 2/25 | Loss: 0.00272981
Iteration 3/25 | Loss: 0.00272980
Iteration 4/25 | Loss: 0.00272980
Iteration 5/25 | Loss: 0.00272980
Iteration 6/25 | Loss: 0.00272980
Iteration 7/25 | Loss: 0.00272980
Iteration 8/25 | Loss: 0.00272980
Iteration 9/25 | Loss: 0.00272980
Iteration 10/25 | Loss: 0.00272980
Iteration 11/25 | Loss: 0.00272980
Iteration 12/25 | Loss: 0.00272980
Iteration 13/25 | Loss: 0.00272980
Iteration 14/25 | Loss: 0.00272980
Iteration 15/25 | Loss: 0.00272980
Iteration 16/25 | Loss: 0.00272980
Iteration 17/25 | Loss: 0.00272980
Iteration 18/25 | Loss: 0.00272980
Iteration 19/25 | Loss: 0.00272980
Iteration 20/25 | Loss: 0.00272980
Iteration 21/25 | Loss: 0.00272980
Iteration 22/25 | Loss: 0.00272980
Iteration 23/25 | Loss: 0.00272980
Iteration 24/25 | Loss: 0.00272980
Iteration 25/25 | Loss: 0.00272980

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272980
Iteration 2/1000 | Loss: 0.00004152
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002194
Iteration 5/1000 | Loss: 0.00001886
Iteration 6/1000 | Loss: 0.00001728
Iteration 7/1000 | Loss: 0.00001600
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001491
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001425
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001348
Iteration 20/1000 | Loss: 0.00001347
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001345
Iteration 23/1000 | Loss: 0.00001342
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001340
Iteration 26/1000 | Loss: 0.00001340
Iteration 27/1000 | Loss: 0.00001339
Iteration 28/1000 | Loss: 0.00001339
Iteration 29/1000 | Loss: 0.00001338
Iteration 30/1000 | Loss: 0.00001337
Iteration 31/1000 | Loss: 0.00001337
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001335
Iteration 37/1000 | Loss: 0.00001334
Iteration 38/1000 | Loss: 0.00001334
Iteration 39/1000 | Loss: 0.00001334
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001332
Iteration 44/1000 | Loss: 0.00001332
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001331
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001329
Iteration 52/1000 | Loss: 0.00001329
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001327
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001327
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001325
Iteration 62/1000 | Loss: 0.00001325
Iteration 63/1000 | Loss: 0.00001322
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001319
Iteration 68/1000 | Loss: 0.00001319
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001317
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001315
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001313
Iteration 99/1000 | Loss: 0.00001313
Iteration 100/1000 | Loss: 0.00001313
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001311
Iteration 111/1000 | Loss: 0.00001311
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001310
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001308
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001307
Iteration 123/1000 | Loss: 0.00001307
Iteration 124/1000 | Loss: 0.00001307
Iteration 125/1000 | Loss: 0.00001307
Iteration 126/1000 | Loss: 0.00001307
Iteration 127/1000 | Loss: 0.00001307
Iteration 128/1000 | Loss: 0.00001307
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001306
Iteration 131/1000 | Loss: 0.00001306
Iteration 132/1000 | Loss: 0.00001306
Iteration 133/1000 | Loss: 0.00001306
Iteration 134/1000 | Loss: 0.00001306
Iteration 135/1000 | Loss: 0.00001306
Iteration 136/1000 | Loss: 0.00001306
Iteration 137/1000 | Loss: 0.00001306
Iteration 138/1000 | Loss: 0.00001306
Iteration 139/1000 | Loss: 0.00001305
Iteration 140/1000 | Loss: 0.00001305
Iteration 141/1000 | Loss: 0.00001305
Iteration 142/1000 | Loss: 0.00001305
Iteration 143/1000 | Loss: 0.00001305
Iteration 144/1000 | Loss: 0.00001305
Iteration 145/1000 | Loss: 0.00001305
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001304
Iteration 148/1000 | Loss: 0.00001304
Iteration 149/1000 | Loss: 0.00001304
Iteration 150/1000 | Loss: 0.00001304
Iteration 151/1000 | Loss: 0.00001303
Iteration 152/1000 | Loss: 0.00001303
Iteration 153/1000 | Loss: 0.00001303
Iteration 154/1000 | Loss: 0.00001303
Iteration 155/1000 | Loss: 0.00001303
Iteration 156/1000 | Loss: 0.00001303
Iteration 157/1000 | Loss: 0.00001303
Iteration 158/1000 | Loss: 0.00001303
Iteration 159/1000 | Loss: 0.00001302
Iteration 160/1000 | Loss: 0.00001302
Iteration 161/1000 | Loss: 0.00001302
Iteration 162/1000 | Loss: 0.00001302
Iteration 163/1000 | Loss: 0.00001302
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Iteration 168/1000 | Loss: 0.00001301
Iteration 169/1000 | Loss: 0.00001300
Iteration 170/1000 | Loss: 0.00001300
Iteration 171/1000 | Loss: 0.00001300
Iteration 172/1000 | Loss: 0.00001300
Iteration 173/1000 | Loss: 0.00001300
Iteration 174/1000 | Loss: 0.00001300
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001299
Iteration 180/1000 | Loss: 0.00001299
Iteration 181/1000 | Loss: 0.00001299
Iteration 182/1000 | Loss: 0.00001298
Iteration 183/1000 | Loss: 0.00001298
Iteration 184/1000 | Loss: 0.00001298
Iteration 185/1000 | Loss: 0.00001298
Iteration 186/1000 | Loss: 0.00001298
Iteration 187/1000 | Loss: 0.00001298
Iteration 188/1000 | Loss: 0.00001298
Iteration 189/1000 | Loss: 0.00001298
Iteration 190/1000 | Loss: 0.00001298
Iteration 191/1000 | Loss: 0.00001298
Iteration 192/1000 | Loss: 0.00001298
Iteration 193/1000 | Loss: 0.00001298
Iteration 194/1000 | Loss: 0.00001298
Iteration 195/1000 | Loss: 0.00001298
Iteration 196/1000 | Loss: 0.00001298
Iteration 197/1000 | Loss: 0.00001298
Iteration 198/1000 | Loss: 0.00001298
Iteration 199/1000 | Loss: 0.00001298
Iteration 200/1000 | Loss: 0.00001298
Iteration 201/1000 | Loss: 0.00001298
Iteration 202/1000 | Loss: 0.00001298
Iteration 203/1000 | Loss: 0.00001298
Iteration 204/1000 | Loss: 0.00001298
Iteration 205/1000 | Loss: 0.00001298
Iteration 206/1000 | Loss: 0.00001298
Iteration 207/1000 | Loss: 0.00001298
Iteration 208/1000 | Loss: 0.00001298
Iteration 209/1000 | Loss: 0.00001298
Iteration 210/1000 | Loss: 0.00001298
Iteration 211/1000 | Loss: 0.00001298
Iteration 212/1000 | Loss: 0.00001298
Iteration 213/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.2984694876649883e-05, 1.2984694876649883e-05, 1.2984694876649883e-05, 1.2984694876649883e-05, 1.2984694876649883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2984694876649883e-05

Optimization complete. Final v2v error: 3.067547559738159 mm

Highest mean error: 3.9296345710754395 mm for frame 24

Lowest mean error: 2.585517168045044 mm for frame 10

Saving results

Total time: 45.65939164161682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850968
Iteration 2/25 | Loss: 0.00149153
Iteration 3/25 | Loss: 0.00132188
Iteration 4/25 | Loss: 0.00130692
Iteration 5/25 | Loss: 0.00130383
Iteration 6/25 | Loss: 0.00130379
Iteration 7/25 | Loss: 0.00130379
Iteration 8/25 | Loss: 0.00130379
Iteration 9/25 | Loss: 0.00130379
Iteration 10/25 | Loss: 0.00130379
Iteration 11/25 | Loss: 0.00130379
Iteration 12/25 | Loss: 0.00130379
Iteration 13/25 | Loss: 0.00130379
Iteration 14/25 | Loss: 0.00130379
Iteration 15/25 | Loss: 0.00130379
Iteration 16/25 | Loss: 0.00130379
Iteration 17/25 | Loss: 0.00130379
Iteration 18/25 | Loss: 0.00130379
Iteration 19/25 | Loss: 0.00130379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013037881581112742, 0.0013037881581112742, 0.0013037881581112742, 0.0013037881581112742, 0.0013037881581112742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013037881581112742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83950454
Iteration 2/25 | Loss: 0.00122077
Iteration 3/25 | Loss: 0.00122077
Iteration 4/25 | Loss: 0.00122077
Iteration 5/25 | Loss: 0.00122077
Iteration 6/25 | Loss: 0.00122077
Iteration 7/25 | Loss: 0.00122077
Iteration 8/25 | Loss: 0.00122077
Iteration 9/25 | Loss: 0.00122077
Iteration 10/25 | Loss: 0.00122077
Iteration 11/25 | Loss: 0.00122076
Iteration 12/25 | Loss: 0.00122076
Iteration 13/25 | Loss: 0.00122076
Iteration 14/25 | Loss: 0.00122076
Iteration 15/25 | Loss: 0.00122076
Iteration 16/25 | Loss: 0.00122076
Iteration 17/25 | Loss: 0.00122076
Iteration 18/25 | Loss: 0.00122076
Iteration 19/25 | Loss: 0.00122076
Iteration 20/25 | Loss: 0.00122076
Iteration 21/25 | Loss: 0.00122076
Iteration 22/25 | Loss: 0.00122076
Iteration 23/25 | Loss: 0.00122076
Iteration 24/25 | Loss: 0.00122076
Iteration 25/25 | Loss: 0.00122076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122076
Iteration 2/1000 | Loss: 0.00003898
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002739
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002589
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002478
Iteration 9/1000 | Loss: 0.00002446
Iteration 10/1000 | Loss: 0.00002419
Iteration 11/1000 | Loss: 0.00002397
Iteration 12/1000 | Loss: 0.00002397
Iteration 13/1000 | Loss: 0.00002389
Iteration 14/1000 | Loss: 0.00002377
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002375
Iteration 17/1000 | Loss: 0.00002361
Iteration 18/1000 | Loss: 0.00002360
Iteration 19/1000 | Loss: 0.00002360
Iteration 20/1000 | Loss: 0.00002355
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002355
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002355
Iteration 26/1000 | Loss: 0.00002355
Iteration 27/1000 | Loss: 0.00002355
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002354
Iteration 33/1000 | Loss: 0.00002354
Iteration 34/1000 | Loss: 0.00002344
Iteration 35/1000 | Loss: 0.00002344
Iteration 36/1000 | Loss: 0.00002343
Iteration 37/1000 | Loss: 0.00002343
Iteration 38/1000 | Loss: 0.00002343
Iteration 39/1000 | Loss: 0.00002343
Iteration 40/1000 | Loss: 0.00002343
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002343
Iteration 44/1000 | Loss: 0.00002343
Iteration 45/1000 | Loss: 0.00002343
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002343
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002343
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00002343
Iteration 54/1000 | Loss: 0.00002343
Iteration 55/1000 | Loss: 0.00002343
Iteration 56/1000 | Loss: 0.00002343
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002343
Iteration 62/1000 | Loss: 0.00002343
Iteration 63/1000 | Loss: 0.00002343
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00002343
Iteration 66/1000 | Loss: 0.00002343
Iteration 67/1000 | Loss: 0.00002343
Iteration 68/1000 | Loss: 0.00002343
Iteration 69/1000 | Loss: 0.00002343
Iteration 70/1000 | Loss: 0.00002343
Iteration 71/1000 | Loss: 0.00002343
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002343
Iteration 77/1000 | Loss: 0.00002343
Iteration 78/1000 | Loss: 0.00002343
Iteration 79/1000 | Loss: 0.00002343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.342930019949563e-05, 2.342930019949563e-05, 2.342930019949563e-05, 2.342930019949563e-05, 2.342930019949563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.342930019949563e-05

Optimization complete. Final v2v error: 4.185452938079834 mm

Highest mean error: 4.374291896820068 mm for frame 114

Lowest mean error: 4.055916786193848 mm for frame 4

Saving results

Total time: 30.943987369537354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_bianca_posed_019/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_bianca_posed_019/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812913
Iteration 2/25 | Loss: 0.00157447
Iteration 3/25 | Loss: 0.00138272
Iteration 4/25 | Loss: 0.00136443
Iteration 5/25 | Loss: 0.00136013
Iteration 6/25 | Loss: 0.00135955
Iteration 7/25 | Loss: 0.00135955
Iteration 8/25 | Loss: 0.00135955
Iteration 9/25 | Loss: 0.00135955
Iteration 10/25 | Loss: 0.00135955
Iteration 11/25 | Loss: 0.00135955
Iteration 12/25 | Loss: 0.00135955
Iteration 13/25 | Loss: 0.00135955
Iteration 14/25 | Loss: 0.00135955
Iteration 15/25 | Loss: 0.00135955
Iteration 16/25 | Loss: 0.00135955
Iteration 17/25 | Loss: 0.00135955
Iteration 18/25 | Loss: 0.00135955
Iteration 19/25 | Loss: 0.00135955
Iteration 20/25 | Loss: 0.00135955
Iteration 21/25 | Loss: 0.00135955
Iteration 22/25 | Loss: 0.00135955
Iteration 23/25 | Loss: 0.00135955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013595472555607557, 0.0013595472555607557, 0.0013595472555607557, 0.0013595472555607557, 0.0013595472555607557]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013595472555607557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05034029
Iteration 2/25 | Loss: 0.00215023
Iteration 3/25 | Loss: 0.00215020
Iteration 4/25 | Loss: 0.00215020
Iteration 5/25 | Loss: 0.00215020
Iteration 6/25 | Loss: 0.00215020
Iteration 7/25 | Loss: 0.00215020
Iteration 8/25 | Loss: 0.00215020
Iteration 9/25 | Loss: 0.00215020
Iteration 10/25 | Loss: 0.00215020
Iteration 11/25 | Loss: 0.00215020
Iteration 12/25 | Loss: 0.00215020
Iteration 13/25 | Loss: 0.00215020
Iteration 14/25 | Loss: 0.00215020
Iteration 15/25 | Loss: 0.00215020
Iteration 16/25 | Loss: 0.00215020
Iteration 17/25 | Loss: 0.00215020
Iteration 18/25 | Loss: 0.00215020
Iteration 19/25 | Loss: 0.00215020
Iteration 20/25 | Loss: 0.00215020
Iteration 21/25 | Loss: 0.00215020
Iteration 22/25 | Loss: 0.00215020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002150197047740221, 0.002150197047740221, 0.002150197047740221, 0.002150197047740221, 0.002150197047740221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002150197047740221

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215020
Iteration 2/1000 | Loss: 0.00004500
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002613
Iteration 5/1000 | Loss: 0.00002469
Iteration 6/1000 | Loss: 0.00002385
Iteration 7/1000 | Loss: 0.00002326
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002201
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002173
Iteration 14/1000 | Loss: 0.00002172
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002159
Iteration 17/1000 | Loss: 0.00002158
Iteration 18/1000 | Loss: 0.00002158
Iteration 19/1000 | Loss: 0.00002157
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002153
Iteration 23/1000 | Loss: 0.00002153
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002140
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002136
Iteration 29/1000 | Loss: 0.00002136
Iteration 30/1000 | Loss: 0.00002136
Iteration 31/1000 | Loss: 0.00002135
Iteration 32/1000 | Loss: 0.00002135
Iteration 33/1000 | Loss: 0.00002135
Iteration 34/1000 | Loss: 0.00002135
Iteration 35/1000 | Loss: 0.00002133
Iteration 36/1000 | Loss: 0.00002133
Iteration 37/1000 | Loss: 0.00002132
Iteration 38/1000 | Loss: 0.00002131
Iteration 39/1000 | Loss: 0.00002130
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002128
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002125
Iteration 45/1000 | Loss: 0.00002125
Iteration 46/1000 | Loss: 0.00002124
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002116
Iteration 62/1000 | Loss: 0.00002116
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002116
Iteration 66/1000 | Loss: 0.00002115
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002113
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002113
Iteration 86/1000 | Loss: 0.00002113
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002112
Iteration 90/1000 | Loss: 0.00002112
Iteration 91/1000 | Loss: 0.00002112
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002110
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002108
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002108
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002108
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002107
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002106
Iteration 109/1000 | Loss: 0.00002106
Iteration 110/1000 | Loss: 0.00002106
Iteration 111/1000 | Loss: 0.00002105
Iteration 112/1000 | Loss: 0.00002105
Iteration 113/1000 | Loss: 0.00002105
Iteration 114/1000 | Loss: 0.00002105
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002105
Iteration 120/1000 | Loss: 0.00002105
Iteration 121/1000 | Loss: 0.00002105
Iteration 122/1000 | Loss: 0.00002104
Iteration 123/1000 | Loss: 0.00002104
Iteration 124/1000 | Loss: 0.00002103
Iteration 125/1000 | Loss: 0.00002103
Iteration 126/1000 | Loss: 0.00002103
Iteration 127/1000 | Loss: 0.00002103
Iteration 128/1000 | Loss: 0.00002103
Iteration 129/1000 | Loss: 0.00002103
Iteration 130/1000 | Loss: 0.00002103
Iteration 131/1000 | Loss: 0.00002102
Iteration 132/1000 | Loss: 0.00002102
Iteration 133/1000 | Loss: 0.00002102
Iteration 134/1000 | Loss: 0.00002102
Iteration 135/1000 | Loss: 0.00002102
Iteration 136/1000 | Loss: 0.00002101
Iteration 137/1000 | Loss: 0.00002101
Iteration 138/1000 | Loss: 0.00002100
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002099
Iteration 142/1000 | Loss: 0.00002099
Iteration 143/1000 | Loss: 0.00002099
Iteration 144/1000 | Loss: 0.00002099
Iteration 145/1000 | Loss: 0.00002099
Iteration 146/1000 | Loss: 0.00002098
Iteration 147/1000 | Loss: 0.00002098
Iteration 148/1000 | Loss: 0.00002098
Iteration 149/1000 | Loss: 0.00002098
Iteration 150/1000 | Loss: 0.00002097
Iteration 151/1000 | Loss: 0.00002097
Iteration 152/1000 | Loss: 0.00002096
Iteration 153/1000 | Loss: 0.00002096
Iteration 154/1000 | Loss: 0.00002096
Iteration 155/1000 | Loss: 0.00002096
Iteration 156/1000 | Loss: 0.00002096
Iteration 157/1000 | Loss: 0.00002096
Iteration 158/1000 | Loss: 0.00002096
Iteration 159/1000 | Loss: 0.00002096
Iteration 160/1000 | Loss: 0.00002096
Iteration 161/1000 | Loss: 0.00002096
Iteration 162/1000 | Loss: 0.00002095
Iteration 163/1000 | Loss: 0.00002095
Iteration 164/1000 | Loss: 0.00002095
Iteration 165/1000 | Loss: 0.00002095
Iteration 166/1000 | Loss: 0.00002095
Iteration 167/1000 | Loss: 0.00002095
Iteration 168/1000 | Loss: 0.00002095
Iteration 169/1000 | Loss: 0.00002095
Iteration 170/1000 | Loss: 0.00002094
Iteration 171/1000 | Loss: 0.00002094
Iteration 172/1000 | Loss: 0.00002094
Iteration 173/1000 | Loss: 0.00002094
Iteration 174/1000 | Loss: 0.00002094
Iteration 175/1000 | Loss: 0.00002094
Iteration 176/1000 | Loss: 0.00002094
Iteration 177/1000 | Loss: 0.00002094
Iteration 178/1000 | Loss: 0.00002094
Iteration 179/1000 | Loss: 0.00002094
Iteration 180/1000 | Loss: 0.00002094
Iteration 181/1000 | Loss: 0.00002094
Iteration 182/1000 | Loss: 0.00002094
Iteration 183/1000 | Loss: 0.00002093
Iteration 184/1000 | Loss: 0.00002093
Iteration 185/1000 | Loss: 0.00002093
Iteration 186/1000 | Loss: 0.00002093
Iteration 187/1000 | Loss: 0.00002093
Iteration 188/1000 | Loss: 0.00002093
Iteration 189/1000 | Loss: 0.00002093
Iteration 190/1000 | Loss: 0.00002093
Iteration 191/1000 | Loss: 0.00002093
Iteration 192/1000 | Loss: 0.00002093
Iteration 193/1000 | Loss: 0.00002093
Iteration 194/1000 | Loss: 0.00002093
Iteration 195/1000 | Loss: 0.00002093
Iteration 196/1000 | Loss: 0.00002092
Iteration 197/1000 | Loss: 0.00002092
Iteration 198/1000 | Loss: 0.00002092
Iteration 199/1000 | Loss: 0.00002092
Iteration 200/1000 | Loss: 0.00002092
Iteration 201/1000 | Loss: 0.00002091
Iteration 202/1000 | Loss: 0.00002091
Iteration 203/1000 | Loss: 0.00002091
Iteration 204/1000 | Loss: 0.00002091
Iteration 205/1000 | Loss: 0.00002091
Iteration 206/1000 | Loss: 0.00002091
Iteration 207/1000 | Loss: 0.00002091
Iteration 208/1000 | Loss: 0.00002091
Iteration 209/1000 | Loss: 0.00002091
Iteration 210/1000 | Loss: 0.00002091
Iteration 211/1000 | Loss: 0.00002091
Iteration 212/1000 | Loss: 0.00002091
Iteration 213/1000 | Loss: 0.00002091
Iteration 214/1000 | Loss: 0.00002091
Iteration 215/1000 | Loss: 0.00002090
Iteration 216/1000 | Loss: 0.00002090
Iteration 217/1000 | Loss: 0.00002090
Iteration 218/1000 | Loss: 0.00002090
Iteration 219/1000 | Loss: 0.00002090
Iteration 220/1000 | Loss: 0.00002090
Iteration 221/1000 | Loss: 0.00002090
Iteration 222/1000 | Loss: 0.00002090
Iteration 223/1000 | Loss: 0.00002090
Iteration 224/1000 | Loss: 0.00002090
Iteration 225/1000 | Loss: 0.00002090
Iteration 226/1000 | Loss: 0.00002089
Iteration 227/1000 | Loss: 0.00002089
Iteration 228/1000 | Loss: 0.00002089
Iteration 229/1000 | Loss: 0.00002089
Iteration 230/1000 | Loss: 0.00002089
Iteration 231/1000 | Loss: 0.00002089
Iteration 232/1000 | Loss: 0.00002089
Iteration 233/1000 | Loss: 0.00002089
Iteration 234/1000 | Loss: 0.00002089
Iteration 235/1000 | Loss: 0.00002088
Iteration 236/1000 | Loss: 0.00002088
Iteration 237/1000 | Loss: 0.00002088
Iteration 238/1000 | Loss: 0.00002088
Iteration 239/1000 | Loss: 0.00002088
Iteration 240/1000 | Loss: 0.00002088
Iteration 241/1000 | Loss: 0.00002088
Iteration 242/1000 | Loss: 0.00002088
Iteration 243/1000 | Loss: 0.00002088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.0884113837382756e-05, 2.0884113837382756e-05, 2.0884113837382756e-05, 2.0884113837382756e-05, 2.0884113837382756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0884113837382756e-05

Optimization complete. Final v2v error: 3.700573682785034 mm

Highest mean error: 4.491611480712891 mm for frame 60

Lowest mean error: 3.1493592262268066 mm for frame 28

Saving results

Total time: 46.19150733947754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905943
Iteration 2/25 | Loss: 0.00153783
Iteration 3/25 | Loss: 0.00128933
Iteration 4/25 | Loss: 0.00126119
Iteration 5/25 | Loss: 0.00125584
Iteration 6/25 | Loss: 0.00125475
Iteration 7/25 | Loss: 0.00125475
Iteration 8/25 | Loss: 0.00125475
Iteration 9/25 | Loss: 0.00125475
Iteration 10/25 | Loss: 0.00125475
Iteration 11/25 | Loss: 0.00125475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012547500664368272, 0.0012547500664368272, 0.0012547500664368272, 0.0012547500664368272, 0.0012547500664368272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012547500664368272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23592687
Iteration 2/25 | Loss: 0.00072789
Iteration 3/25 | Loss: 0.00072785
Iteration 4/25 | Loss: 0.00072785
Iteration 5/25 | Loss: 0.00072785
Iteration 6/25 | Loss: 0.00072785
Iteration 7/25 | Loss: 0.00072785
Iteration 8/25 | Loss: 0.00072785
Iteration 9/25 | Loss: 0.00072785
Iteration 10/25 | Loss: 0.00072785
Iteration 11/25 | Loss: 0.00072785
Iteration 12/25 | Loss: 0.00072785
Iteration 13/25 | Loss: 0.00072785
Iteration 14/25 | Loss: 0.00072785
Iteration 15/25 | Loss: 0.00072785
Iteration 16/25 | Loss: 0.00072785
Iteration 17/25 | Loss: 0.00072785
Iteration 18/25 | Loss: 0.00072785
Iteration 19/25 | Loss: 0.00072785
Iteration 20/25 | Loss: 0.00072785
Iteration 21/25 | Loss: 0.00072785
Iteration 22/25 | Loss: 0.00072785
Iteration 23/25 | Loss: 0.00072785
Iteration 24/25 | Loss: 0.00072785
Iteration 25/25 | Loss: 0.00072785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072785
Iteration 2/1000 | Loss: 0.00005699
Iteration 3/1000 | Loss: 0.00004065
Iteration 4/1000 | Loss: 0.00003175
Iteration 5/1000 | Loss: 0.00002853
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00002603
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002480
Iteration 10/1000 | Loss: 0.00002454
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002396
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002378
Iteration 15/1000 | Loss: 0.00002376
Iteration 16/1000 | Loss: 0.00002371
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002363
Iteration 19/1000 | Loss: 0.00002363
Iteration 20/1000 | Loss: 0.00002363
Iteration 21/1000 | Loss: 0.00002363
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002363
Iteration 24/1000 | Loss: 0.00002363
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002359
Iteration 27/1000 | Loss: 0.00002358
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002358
Iteration 31/1000 | Loss: 0.00002358
Iteration 32/1000 | Loss: 0.00002358
Iteration 33/1000 | Loss: 0.00002358
Iteration 34/1000 | Loss: 0.00002358
Iteration 35/1000 | Loss: 0.00002358
Iteration 36/1000 | Loss: 0.00002358
Iteration 37/1000 | Loss: 0.00002358
Iteration 38/1000 | Loss: 0.00002358
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00002358
Iteration 41/1000 | Loss: 0.00002357
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002356
Iteration 44/1000 | Loss: 0.00002355
Iteration 45/1000 | Loss: 0.00002355
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002354
Iteration 50/1000 | Loss: 0.00002354
Iteration 51/1000 | Loss: 0.00002354
Iteration 52/1000 | Loss: 0.00002354
Iteration 53/1000 | Loss: 0.00002354
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002352
Iteration 58/1000 | Loss: 0.00002352
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002351
Iteration 63/1000 | Loss: 0.00002351
Iteration 64/1000 | Loss: 0.00002351
Iteration 65/1000 | Loss: 0.00002351
Iteration 66/1000 | Loss: 0.00002350
Iteration 67/1000 | Loss: 0.00002350
Iteration 68/1000 | Loss: 0.00002350
Iteration 69/1000 | Loss: 0.00002350
Iteration 70/1000 | Loss: 0.00002350
Iteration 71/1000 | Loss: 0.00002348
Iteration 72/1000 | Loss: 0.00002348
Iteration 73/1000 | Loss: 0.00002347
Iteration 74/1000 | Loss: 0.00002347
Iteration 75/1000 | Loss: 0.00002347
Iteration 76/1000 | Loss: 0.00002347
Iteration 77/1000 | Loss: 0.00002347
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002347
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002345
Iteration 84/1000 | Loss: 0.00002345
Iteration 85/1000 | Loss: 0.00002345
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002343
Iteration 93/1000 | Loss: 0.00002343
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002342
Iteration 98/1000 | Loss: 0.00002342
Iteration 99/1000 | Loss: 0.00002342
Iteration 100/1000 | Loss: 0.00002342
Iteration 101/1000 | Loss: 0.00002341
Iteration 102/1000 | Loss: 0.00002341
Iteration 103/1000 | Loss: 0.00002341
Iteration 104/1000 | Loss: 0.00002341
Iteration 105/1000 | Loss: 0.00002341
Iteration 106/1000 | Loss: 0.00002341
Iteration 107/1000 | Loss: 0.00002341
Iteration 108/1000 | Loss: 0.00002340
Iteration 109/1000 | Loss: 0.00002340
Iteration 110/1000 | Loss: 0.00002340
Iteration 111/1000 | Loss: 0.00002340
Iteration 112/1000 | Loss: 0.00002340
Iteration 113/1000 | Loss: 0.00002339
Iteration 114/1000 | Loss: 0.00002339
Iteration 115/1000 | Loss: 0.00002339
Iteration 116/1000 | Loss: 0.00002339
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002339
Iteration 119/1000 | Loss: 0.00002339
Iteration 120/1000 | Loss: 0.00002339
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00002339
Iteration 123/1000 | Loss: 0.00002338
Iteration 124/1000 | Loss: 0.00002338
Iteration 125/1000 | Loss: 0.00002338
Iteration 126/1000 | Loss: 0.00002338
Iteration 127/1000 | Loss: 0.00002338
Iteration 128/1000 | Loss: 0.00002338
Iteration 129/1000 | Loss: 0.00002338
Iteration 130/1000 | Loss: 0.00002338
Iteration 131/1000 | Loss: 0.00002338
Iteration 132/1000 | Loss: 0.00002337
Iteration 133/1000 | Loss: 0.00002337
Iteration 134/1000 | Loss: 0.00002337
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002336
Iteration 137/1000 | Loss: 0.00002336
Iteration 138/1000 | Loss: 0.00002336
Iteration 139/1000 | Loss: 0.00002336
Iteration 140/1000 | Loss: 0.00002336
Iteration 141/1000 | Loss: 0.00002336
Iteration 142/1000 | Loss: 0.00002336
Iteration 143/1000 | Loss: 0.00002336
Iteration 144/1000 | Loss: 0.00002335
Iteration 145/1000 | Loss: 0.00002335
Iteration 146/1000 | Loss: 0.00002335
Iteration 147/1000 | Loss: 0.00002335
Iteration 148/1000 | Loss: 0.00002335
Iteration 149/1000 | Loss: 0.00002335
Iteration 150/1000 | Loss: 0.00002335
Iteration 151/1000 | Loss: 0.00002335
Iteration 152/1000 | Loss: 0.00002335
Iteration 153/1000 | Loss: 0.00002335
Iteration 154/1000 | Loss: 0.00002335
Iteration 155/1000 | Loss: 0.00002335
Iteration 156/1000 | Loss: 0.00002335
Iteration 157/1000 | Loss: 0.00002335
Iteration 158/1000 | Loss: 0.00002335
Iteration 159/1000 | Loss: 0.00002335
Iteration 160/1000 | Loss: 0.00002335
Iteration 161/1000 | Loss: 0.00002335
Iteration 162/1000 | Loss: 0.00002335
Iteration 163/1000 | Loss: 0.00002335
Iteration 164/1000 | Loss: 0.00002335
Iteration 165/1000 | Loss: 0.00002335
Iteration 166/1000 | Loss: 0.00002335
Iteration 167/1000 | Loss: 0.00002335
Iteration 168/1000 | Loss: 0.00002335
Iteration 169/1000 | Loss: 0.00002335
Iteration 170/1000 | Loss: 0.00002335
Iteration 171/1000 | Loss: 0.00002335
Iteration 172/1000 | Loss: 0.00002335
Iteration 173/1000 | Loss: 0.00002335
Iteration 174/1000 | Loss: 0.00002335
Iteration 175/1000 | Loss: 0.00002335
Iteration 176/1000 | Loss: 0.00002335
Iteration 177/1000 | Loss: 0.00002335
Iteration 178/1000 | Loss: 0.00002335
Iteration 179/1000 | Loss: 0.00002335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.3352553398581222e-05, 2.3352553398581222e-05, 2.3352553398581222e-05, 2.3352553398581222e-05, 2.3352553398581222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3352553398581222e-05

Optimization complete. Final v2v error: 3.985570192337036 mm

Highest mean error: 4.658505439758301 mm for frame 0

Lowest mean error: 3.5374717712402344 mm for frame 141

Saving results

Total time: 39.81711673736572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498134
Iteration 2/25 | Loss: 0.00146010
Iteration 3/25 | Loss: 0.00121366
Iteration 4/25 | Loss: 0.00119073
Iteration 5/25 | Loss: 0.00118859
Iteration 6/25 | Loss: 0.00118807
Iteration 7/25 | Loss: 0.00118807
Iteration 8/25 | Loss: 0.00118807
Iteration 9/25 | Loss: 0.00118807
Iteration 10/25 | Loss: 0.00118807
Iteration 11/25 | Loss: 0.00118807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011880651582032442, 0.0011880651582032442, 0.0011880651582032442, 0.0011880651582032442, 0.0011880651582032442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011880651582032442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45701444
Iteration 2/25 | Loss: 0.00090005
Iteration 3/25 | Loss: 0.00090002
Iteration 4/25 | Loss: 0.00090002
Iteration 5/25 | Loss: 0.00090001
Iteration 6/25 | Loss: 0.00090001
Iteration 7/25 | Loss: 0.00090001
Iteration 8/25 | Loss: 0.00090001
Iteration 9/25 | Loss: 0.00090001
Iteration 10/25 | Loss: 0.00090001
Iteration 11/25 | Loss: 0.00090001
Iteration 12/25 | Loss: 0.00090001
Iteration 13/25 | Loss: 0.00090001
Iteration 14/25 | Loss: 0.00090001
Iteration 15/25 | Loss: 0.00090001
Iteration 16/25 | Loss: 0.00090001
Iteration 17/25 | Loss: 0.00090001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009000130230560899, 0.0009000130230560899, 0.0009000130230560899, 0.0009000130230560899, 0.0009000130230560899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009000130230560899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090001
Iteration 2/1000 | Loss: 0.00005676
Iteration 3/1000 | Loss: 0.00003134
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002033
Iteration 6/1000 | Loss: 0.00001877
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001702
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001697
Iteration 17/1000 | Loss: 0.00001693
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001680
Iteration 20/1000 | Loss: 0.00001679
Iteration 21/1000 | Loss: 0.00001679
Iteration 22/1000 | Loss: 0.00001678
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001656
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001656
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001656
Iteration 41/1000 | Loss: 0.00001656
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001655
Iteration 45/1000 | Loss: 0.00001655
Iteration 46/1000 | Loss: 0.00001655
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001654
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001653
Iteration 51/1000 | Loss: 0.00001653
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001653
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001652
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001652
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001652
Iteration 60/1000 | Loss: 0.00001652
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001651
Iteration 63/1000 | Loss: 0.00001651
Iteration 64/1000 | Loss: 0.00001651
Iteration 65/1000 | Loss: 0.00001651
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001650
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001650
Iteration 73/1000 | Loss: 0.00001650
Iteration 74/1000 | Loss: 0.00001650
Iteration 75/1000 | Loss: 0.00001650
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001648
Iteration 84/1000 | Loss: 0.00001648
Iteration 85/1000 | Loss: 0.00001648
Iteration 86/1000 | Loss: 0.00001648
Iteration 87/1000 | Loss: 0.00001648
Iteration 88/1000 | Loss: 0.00001648
Iteration 89/1000 | Loss: 0.00001647
Iteration 90/1000 | Loss: 0.00001647
Iteration 91/1000 | Loss: 0.00001647
Iteration 92/1000 | Loss: 0.00001647
Iteration 93/1000 | Loss: 0.00001647
Iteration 94/1000 | Loss: 0.00001647
Iteration 95/1000 | Loss: 0.00001647
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001646
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001646
Iteration 103/1000 | Loss: 0.00001646
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001645
Iteration 107/1000 | Loss: 0.00001645
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001644
Iteration 111/1000 | Loss: 0.00001644
Iteration 112/1000 | Loss: 0.00001644
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001644
Iteration 116/1000 | Loss: 0.00001644
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001643
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001643
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001642
Iteration 135/1000 | Loss: 0.00001642
Iteration 136/1000 | Loss: 0.00001641
Iteration 137/1000 | Loss: 0.00001641
Iteration 138/1000 | Loss: 0.00001641
Iteration 139/1000 | Loss: 0.00001641
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001640
Iteration 144/1000 | Loss: 0.00001640
Iteration 145/1000 | Loss: 0.00001640
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001639
Iteration 152/1000 | Loss: 0.00001639
Iteration 153/1000 | Loss: 0.00001639
Iteration 154/1000 | Loss: 0.00001639
Iteration 155/1000 | Loss: 0.00001639
Iteration 156/1000 | Loss: 0.00001639
Iteration 157/1000 | Loss: 0.00001639
Iteration 158/1000 | Loss: 0.00001639
Iteration 159/1000 | Loss: 0.00001639
Iteration 160/1000 | Loss: 0.00001639
Iteration 161/1000 | Loss: 0.00001639
Iteration 162/1000 | Loss: 0.00001639
Iteration 163/1000 | Loss: 0.00001639
Iteration 164/1000 | Loss: 0.00001639
Iteration 165/1000 | Loss: 0.00001639
Iteration 166/1000 | Loss: 0.00001639
Iteration 167/1000 | Loss: 0.00001639
Iteration 168/1000 | Loss: 0.00001639
Iteration 169/1000 | Loss: 0.00001639
Iteration 170/1000 | Loss: 0.00001639
Iteration 171/1000 | Loss: 0.00001639
Iteration 172/1000 | Loss: 0.00001639
Iteration 173/1000 | Loss: 0.00001639
Iteration 174/1000 | Loss: 0.00001639
Iteration 175/1000 | Loss: 0.00001639
Iteration 176/1000 | Loss: 0.00001639
Iteration 177/1000 | Loss: 0.00001639
Iteration 178/1000 | Loss: 0.00001639
Iteration 179/1000 | Loss: 0.00001639
Iteration 180/1000 | Loss: 0.00001639
Iteration 181/1000 | Loss: 0.00001639
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.6389330994570628e-05, 1.6389330994570628e-05, 1.6389330994570628e-05, 1.6389330994570628e-05, 1.6389330994570628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6389330994570628e-05

Optimization complete. Final v2v error: 3.3197948932647705 mm

Highest mean error: 3.684727907180786 mm for frame 61

Lowest mean error: 2.8733487129211426 mm for frame 1

Saving results

Total time: 37.0301079750061
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101668
Iteration 2/25 | Loss: 0.00199834
Iteration 3/25 | Loss: 0.00148209
Iteration 4/25 | Loss: 0.00132003
Iteration 5/25 | Loss: 0.00120097
Iteration 6/25 | Loss: 0.00121916
Iteration 7/25 | Loss: 0.00122020
Iteration 8/25 | Loss: 0.00122650
Iteration 9/25 | Loss: 0.00114217
Iteration 10/25 | Loss: 0.00113048
Iteration 11/25 | Loss: 0.00110524
Iteration 12/25 | Loss: 0.00110423
Iteration 13/25 | Loss: 0.00110464
Iteration 14/25 | Loss: 0.00110443
Iteration 15/25 | Loss: 0.00110433
Iteration 16/25 | Loss: 0.00110446
Iteration 17/25 | Loss: 0.00110427
Iteration 18/25 | Loss: 0.00110441
Iteration 19/25 | Loss: 0.00110423
Iteration 20/25 | Loss: 0.00110424
Iteration 21/25 | Loss: 0.00110425
Iteration 22/25 | Loss: 0.00110431
Iteration 23/25 | Loss: 0.00110408
Iteration 24/25 | Loss: 0.00110432
Iteration 25/25 | Loss: 0.00110413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31087530
Iteration 2/25 | Loss: 0.00104916
Iteration 3/25 | Loss: 0.00104916
Iteration 4/25 | Loss: 0.00104915
Iteration 5/25 | Loss: 0.00104915
Iteration 6/25 | Loss: 0.00104915
Iteration 7/25 | Loss: 0.00104915
Iteration 8/25 | Loss: 0.00104915
Iteration 9/25 | Loss: 0.00104915
Iteration 10/25 | Loss: 0.00104915
Iteration 11/25 | Loss: 0.00104915
Iteration 12/25 | Loss: 0.00104915
Iteration 13/25 | Loss: 0.00104915
Iteration 14/25 | Loss: 0.00104915
Iteration 15/25 | Loss: 0.00104915
Iteration 16/25 | Loss: 0.00104915
Iteration 17/25 | Loss: 0.00104915
Iteration 18/25 | Loss: 0.00104915
Iteration 19/25 | Loss: 0.00104915
Iteration 20/25 | Loss: 0.00104915
Iteration 21/25 | Loss: 0.00104915
Iteration 22/25 | Loss: 0.00104915
Iteration 23/25 | Loss: 0.00104915
Iteration 24/25 | Loss: 0.00104915
Iteration 25/25 | Loss: 0.00104915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104915
Iteration 2/1000 | Loss: 0.00004724
Iteration 3/1000 | Loss: 0.00003141
Iteration 4/1000 | Loss: 0.00003089
Iteration 5/1000 | Loss: 0.00002675
Iteration 6/1000 | Loss: 0.00002616
Iteration 7/1000 | Loss: 0.00002531
Iteration 8/1000 | Loss: 0.00003197
Iteration 9/1000 | Loss: 0.00002922
Iteration 10/1000 | Loss: 0.00002529
Iteration 11/1000 | Loss: 0.00002444
Iteration 12/1000 | Loss: 0.00002780
Iteration 13/1000 | Loss: 0.00002756
Iteration 14/1000 | Loss: 0.00020439
Iteration 15/1000 | Loss: 0.00008244
Iteration 16/1000 | Loss: 0.00011557
Iteration 17/1000 | Loss: 0.00002702
Iteration 18/1000 | Loss: 0.00002861
Iteration 19/1000 | Loss: 0.00002811
Iteration 20/1000 | Loss: 0.00002639
Iteration 21/1000 | Loss: 0.00002702
Iteration 22/1000 | Loss: 0.00002419
Iteration 23/1000 | Loss: 0.00002580
Iteration 24/1000 | Loss: 0.00002404
Iteration 25/1000 | Loss: 0.00002670
Iteration 26/1000 | Loss: 0.00002382
Iteration 27/1000 | Loss: 0.00002628
Iteration 28/1000 | Loss: 0.00002746
Iteration 29/1000 | Loss: 0.00002747
Iteration 30/1000 | Loss: 0.00002729
Iteration 31/1000 | Loss: 0.00002742
Iteration 32/1000 | Loss: 0.00002701
Iteration 33/1000 | Loss: 0.00002749
Iteration 34/1000 | Loss: 0.00002383
Iteration 35/1000 | Loss: 0.00002480
Iteration 36/1000 | Loss: 0.00002487
Iteration 37/1000 | Loss: 0.00002495
Iteration 38/1000 | Loss: 0.00002482
Iteration 39/1000 | Loss: 0.00002484
Iteration 40/1000 | Loss: 0.00002645
Iteration 41/1000 | Loss: 0.00002754
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002431
Iteration 44/1000 | Loss: 0.00002604
Iteration 45/1000 | Loss: 0.00002425
Iteration 46/1000 | Loss: 0.00002486
Iteration 47/1000 | Loss: 0.00002500
Iteration 48/1000 | Loss: 0.00002483
Iteration 49/1000 | Loss: 0.00002489
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002677
Iteration 52/1000 | Loss: 0.00002527
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002481
Iteration 55/1000 | Loss: 0.00002484
Iteration 56/1000 | Loss: 0.00002482
Iteration 57/1000 | Loss: 0.00002495
Iteration 58/1000 | Loss: 0.00002491
Iteration 59/1000 | Loss: 0.00002488
Iteration 60/1000 | Loss: 0.00002483
Iteration 61/1000 | Loss: 0.00002681
Iteration 62/1000 | Loss: 0.00002741
Iteration 63/1000 | Loss: 0.00002710
Iteration 64/1000 | Loss: 0.00002728
Iteration 65/1000 | Loss: 0.00002713
Iteration 66/1000 | Loss: 0.00003147
Iteration 67/1000 | Loss: 0.00002591
Iteration 68/1000 | Loss: 0.00002678
Iteration 69/1000 | Loss: 0.00002552
Iteration 70/1000 | Loss: 0.00002697
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002492
Iteration 73/1000 | Loss: 0.00002489
Iteration 74/1000 | Loss: 0.00002482
Iteration 75/1000 | Loss: 0.00002496
Iteration 76/1000 | Loss: 0.00002477
Iteration 77/1000 | Loss: 0.00002491
Iteration 78/1000 | Loss: 0.00002485
Iteration 79/1000 | Loss: 0.00002491
Iteration 80/1000 | Loss: 0.00002490
Iteration 81/1000 | Loss: 0.00002500
Iteration 82/1000 | Loss: 0.00002499
Iteration 83/1000 | Loss: 0.00002491
Iteration 84/1000 | Loss: 0.00002490
Iteration 85/1000 | Loss: 0.00002490
Iteration 86/1000 | Loss: 0.00002482
Iteration 87/1000 | Loss: 0.00002489
Iteration 88/1000 | Loss: 0.00002489
Iteration 89/1000 | Loss: 0.00018653
Iteration 90/1000 | Loss: 0.00003038
Iteration 91/1000 | Loss: 0.00002581
Iteration 92/1000 | Loss: 0.00011486
Iteration 93/1000 | Loss: 0.00002592
Iteration 94/1000 | Loss: 0.00002520
Iteration 95/1000 | Loss: 0.00002708
Iteration 96/1000 | Loss: 0.00002970
Iteration 97/1000 | Loss: 0.00002513
Iteration 98/1000 | Loss: 0.00002513
Iteration 99/1000 | Loss: 0.00002707
Iteration 100/1000 | Loss: 0.00002447
Iteration 101/1000 | Loss: 0.00002692
Iteration 102/1000 | Loss: 0.00002691
Iteration 103/1000 | Loss: 0.00012285
Iteration 104/1000 | Loss: 0.00006672
Iteration 105/1000 | Loss: 0.00002597
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002569
Iteration 108/1000 | Loss: 0.00002492
Iteration 109/1000 | Loss: 0.00002653
Iteration 110/1000 | Loss: 0.00002757
Iteration 111/1000 | Loss: 0.00002991
Iteration 112/1000 | Loss: 0.00002727
Iteration 113/1000 | Loss: 0.00002733
Iteration 114/1000 | Loss: 0.00002772
Iteration 115/1000 | Loss: 0.00002510
Iteration 116/1000 | Loss: 0.00002435
Iteration 117/1000 | Loss: 0.00002393
Iteration 118/1000 | Loss: 0.00002341
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002321
Iteration 121/1000 | Loss: 0.00002320
Iteration 122/1000 | Loss: 0.00002320
Iteration 123/1000 | Loss: 0.00002319
Iteration 124/1000 | Loss: 0.00002318
Iteration 125/1000 | Loss: 0.00002318
Iteration 126/1000 | Loss: 0.00002312
Iteration 127/1000 | Loss: 0.00002312
Iteration 128/1000 | Loss: 0.00002312
Iteration 129/1000 | Loss: 0.00002312
Iteration 130/1000 | Loss: 0.00002311
Iteration 131/1000 | Loss: 0.00002311
Iteration 132/1000 | Loss: 0.00002311
Iteration 133/1000 | Loss: 0.00002310
Iteration 134/1000 | Loss: 0.00002310
Iteration 135/1000 | Loss: 0.00002309
Iteration 136/1000 | Loss: 0.00002307
Iteration 137/1000 | Loss: 0.00002307
Iteration 138/1000 | Loss: 0.00002307
Iteration 139/1000 | Loss: 0.00002307
Iteration 140/1000 | Loss: 0.00002307
Iteration 141/1000 | Loss: 0.00002307
Iteration 142/1000 | Loss: 0.00002306
Iteration 143/1000 | Loss: 0.00002306
Iteration 144/1000 | Loss: 0.00002306
Iteration 145/1000 | Loss: 0.00002306
Iteration 146/1000 | Loss: 0.00002306
Iteration 147/1000 | Loss: 0.00002306
Iteration 148/1000 | Loss: 0.00002306
Iteration 149/1000 | Loss: 0.00002306
Iteration 150/1000 | Loss: 0.00002306
Iteration 151/1000 | Loss: 0.00002305
Iteration 152/1000 | Loss: 0.00002304
Iteration 153/1000 | Loss: 0.00002304
Iteration 154/1000 | Loss: 0.00002304
Iteration 155/1000 | Loss: 0.00002304
Iteration 156/1000 | Loss: 0.00002304
Iteration 157/1000 | Loss: 0.00002304
Iteration 158/1000 | Loss: 0.00002304
Iteration 159/1000 | Loss: 0.00002304
Iteration 160/1000 | Loss: 0.00002304
Iteration 161/1000 | Loss: 0.00002304
Iteration 162/1000 | Loss: 0.00002304
Iteration 163/1000 | Loss: 0.00002302
Iteration 164/1000 | Loss: 0.00002301
Iteration 165/1000 | Loss: 0.00002301
Iteration 166/1000 | Loss: 0.00002301
Iteration 167/1000 | Loss: 0.00002300
Iteration 168/1000 | Loss: 0.00002300
Iteration 169/1000 | Loss: 0.00002300
Iteration 170/1000 | Loss: 0.00002300
Iteration 171/1000 | Loss: 0.00002300
Iteration 172/1000 | Loss: 0.00002297
Iteration 173/1000 | Loss: 0.00002297
Iteration 174/1000 | Loss: 0.00002297
Iteration 175/1000 | Loss: 0.00002297
Iteration 176/1000 | Loss: 0.00002297
Iteration 177/1000 | Loss: 0.00002297
Iteration 178/1000 | Loss: 0.00002297
Iteration 179/1000 | Loss: 0.00002297
Iteration 180/1000 | Loss: 0.00002297
Iteration 181/1000 | Loss: 0.00002297
Iteration 182/1000 | Loss: 0.00002296
Iteration 183/1000 | Loss: 0.00002296
Iteration 184/1000 | Loss: 0.00002296
Iteration 185/1000 | Loss: 0.00002296
Iteration 186/1000 | Loss: 0.00002295
Iteration 187/1000 | Loss: 0.00002294
Iteration 188/1000 | Loss: 0.00002294
Iteration 189/1000 | Loss: 0.00002294
Iteration 190/1000 | Loss: 0.00002294
Iteration 191/1000 | Loss: 0.00002294
Iteration 192/1000 | Loss: 0.00002294
Iteration 193/1000 | Loss: 0.00002294
Iteration 194/1000 | Loss: 0.00002294
Iteration 195/1000 | Loss: 0.00002294
Iteration 196/1000 | Loss: 0.00002294
Iteration 197/1000 | Loss: 0.00002294
Iteration 198/1000 | Loss: 0.00002294
Iteration 199/1000 | Loss: 0.00002293
Iteration 200/1000 | Loss: 0.00002293
Iteration 201/1000 | Loss: 0.00002293
Iteration 202/1000 | Loss: 0.00002293
Iteration 203/1000 | Loss: 0.00002292
Iteration 204/1000 | Loss: 0.00002292
Iteration 205/1000 | Loss: 0.00002292
Iteration 206/1000 | Loss: 0.00002292
Iteration 207/1000 | Loss: 0.00002291
Iteration 208/1000 | Loss: 0.00002291
Iteration 209/1000 | Loss: 0.00002291
Iteration 210/1000 | Loss: 0.00002291
Iteration 211/1000 | Loss: 0.00002291
Iteration 212/1000 | Loss: 0.00002291
Iteration 213/1000 | Loss: 0.00002291
Iteration 214/1000 | Loss: 0.00002291
Iteration 215/1000 | Loss: 0.00002291
Iteration 216/1000 | Loss: 0.00002291
Iteration 217/1000 | Loss: 0.00002291
Iteration 218/1000 | Loss: 0.00002291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.2910771804163232e-05, 2.2910771804163232e-05, 2.2910771804163232e-05, 2.2910771804163232e-05, 2.2910771804163232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2910771804163232e-05

Optimization complete. Final v2v error: 3.825773239135742 mm

Highest mean error: 8.72646427154541 mm for frame 49

Lowest mean error: 3.5482370853424072 mm for frame 129

Saving results

Total time: 247.5793890953064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078501
Iteration 2/25 | Loss: 0.01078501
Iteration 3/25 | Loss: 0.00405455
Iteration 4/25 | Loss: 0.00257781
Iteration 5/25 | Loss: 0.00233595
Iteration 6/25 | Loss: 0.00215578
Iteration 7/25 | Loss: 0.00199388
Iteration 8/25 | Loss: 0.00177963
Iteration 9/25 | Loss: 0.00162621
Iteration 10/25 | Loss: 0.00147008
Iteration 11/25 | Loss: 0.00137750
Iteration 12/25 | Loss: 0.00135634
Iteration 13/25 | Loss: 0.00133820
Iteration 14/25 | Loss: 0.00132970
Iteration 15/25 | Loss: 0.00133017
Iteration 16/25 | Loss: 0.00132753
Iteration 17/25 | Loss: 0.00132576
Iteration 18/25 | Loss: 0.00132565
Iteration 19/25 | Loss: 0.00132512
Iteration 20/25 | Loss: 0.00132469
Iteration 21/25 | Loss: 0.00132463
Iteration 22/25 | Loss: 0.00132531
Iteration 23/25 | Loss: 0.00132438
Iteration 24/25 | Loss: 0.00132451
Iteration 25/25 | Loss: 0.00132525

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30415988
Iteration 2/25 | Loss: 0.00143684
Iteration 3/25 | Loss: 0.00143684
Iteration 4/25 | Loss: 0.00143683
Iteration 5/25 | Loss: 0.00143683
Iteration 6/25 | Loss: 0.00143683
Iteration 7/25 | Loss: 0.00143683
Iteration 8/25 | Loss: 0.00143683
Iteration 9/25 | Loss: 0.00143683
Iteration 10/25 | Loss: 0.00143683
Iteration 11/25 | Loss: 0.00143683
Iteration 12/25 | Loss: 0.00143683
Iteration 13/25 | Loss: 0.00143683
Iteration 14/25 | Loss: 0.00143683
Iteration 15/25 | Loss: 0.00143683
Iteration 16/25 | Loss: 0.00143683
Iteration 17/25 | Loss: 0.00143683
Iteration 18/25 | Loss: 0.00143683
Iteration 19/25 | Loss: 0.00143683
Iteration 20/25 | Loss: 0.00143683
Iteration 21/25 | Loss: 0.00143683
Iteration 22/25 | Loss: 0.00143683
Iteration 23/25 | Loss: 0.00143683
Iteration 24/25 | Loss: 0.00143683
Iteration 25/25 | Loss: 0.00143683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143683
Iteration 2/1000 | Loss: 0.00053691
Iteration 3/1000 | Loss: 0.00020077
Iteration 4/1000 | Loss: 0.00013195
Iteration 5/1000 | Loss: 0.00011504
Iteration 6/1000 | Loss: 0.00009391
Iteration 7/1000 | Loss: 0.00009542
Iteration 8/1000 | Loss: 0.00008506
Iteration 9/1000 | Loss: 0.00008311
Iteration 10/1000 | Loss: 0.00007935
Iteration 11/1000 | Loss: 0.00007926
Iteration 12/1000 | Loss: 0.00007016
Iteration 13/1000 | Loss: 0.00008107
Iteration 14/1000 | Loss: 0.00007686
Iteration 15/1000 | Loss: 0.00007362
Iteration 16/1000 | Loss: 0.00009164
Iteration 17/1000 | Loss: 0.00008038
Iteration 18/1000 | Loss: 0.00008099
Iteration 19/1000 | Loss: 0.00007207
Iteration 20/1000 | Loss: 0.00007586
Iteration 21/1000 | Loss: 0.00007106
Iteration 22/1000 | Loss: 0.00007656
Iteration 23/1000 | Loss: 0.00007325
Iteration 24/1000 | Loss: 0.00007683
Iteration 25/1000 | Loss: 0.00007295
Iteration 26/1000 | Loss: 0.00007872
Iteration 27/1000 | Loss: 0.00007184
Iteration 28/1000 | Loss: 0.00006838
Iteration 29/1000 | Loss: 0.00007212
Iteration 30/1000 | Loss: 0.00006590
Iteration 31/1000 | Loss: 0.00008010
Iteration 32/1000 | Loss: 0.00007046
Iteration 33/1000 | Loss: 0.00006858
Iteration 34/1000 | Loss: 0.00007590
Iteration 35/1000 | Loss: 0.00006332
Iteration 36/1000 | Loss: 0.00006194
Iteration 37/1000 | Loss: 0.00006132
Iteration 38/1000 | Loss: 0.00006069
Iteration 39/1000 | Loss: 0.00006027
Iteration 40/1000 | Loss: 0.00005991
Iteration 41/1000 | Loss: 0.00069551
Iteration 42/1000 | Loss: 0.00178159
Iteration 43/1000 | Loss: 0.00029872
Iteration 44/1000 | Loss: 0.00020746
Iteration 45/1000 | Loss: 0.00009822
Iteration 46/1000 | Loss: 0.00007497
Iteration 47/1000 | Loss: 0.00006184
Iteration 48/1000 | Loss: 0.00004620
Iteration 49/1000 | Loss: 0.00004061
Iteration 50/1000 | Loss: 0.00003485
Iteration 51/1000 | Loss: 0.00003046
Iteration 52/1000 | Loss: 0.00002704
Iteration 53/1000 | Loss: 0.00002472
Iteration 54/1000 | Loss: 0.00002280
Iteration 55/1000 | Loss: 0.00002154
Iteration 56/1000 | Loss: 0.00002063
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001863
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001847
Iteration 70/1000 | Loss: 0.00001847
Iteration 71/1000 | Loss: 0.00001847
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001837
Iteration 94/1000 | Loss: 0.00001837
Iteration 95/1000 | Loss: 0.00001837
Iteration 96/1000 | Loss: 0.00001837
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001836
Iteration 102/1000 | Loss: 0.00001836
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001835
Iteration 105/1000 | Loss: 0.00001835
Iteration 106/1000 | Loss: 0.00001835
Iteration 107/1000 | Loss: 0.00001835
Iteration 108/1000 | Loss: 0.00001835
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001834
Iteration 113/1000 | Loss: 0.00001834
Iteration 114/1000 | Loss: 0.00001834
Iteration 115/1000 | Loss: 0.00001834
Iteration 116/1000 | Loss: 0.00001834
Iteration 117/1000 | Loss: 0.00001834
Iteration 118/1000 | Loss: 0.00001834
Iteration 119/1000 | Loss: 0.00001834
Iteration 120/1000 | Loss: 0.00001834
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001833
Iteration 123/1000 | Loss: 0.00001833
Iteration 124/1000 | Loss: 0.00001833
Iteration 125/1000 | Loss: 0.00001833
Iteration 126/1000 | Loss: 0.00001833
Iteration 127/1000 | Loss: 0.00001833
Iteration 128/1000 | Loss: 0.00001833
Iteration 129/1000 | Loss: 0.00001833
Iteration 130/1000 | Loss: 0.00001833
Iteration 131/1000 | Loss: 0.00001833
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001833
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001833
Iteration 143/1000 | Loss: 0.00001833
Iteration 144/1000 | Loss: 0.00001833
Iteration 145/1000 | Loss: 0.00001833
Iteration 146/1000 | Loss: 0.00001833
Iteration 147/1000 | Loss: 0.00001833
Iteration 148/1000 | Loss: 0.00001833
Iteration 149/1000 | Loss: 0.00001833
Iteration 150/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8329083104617894e-05, 1.8329083104617894e-05, 1.8329083104617894e-05, 1.8329083104617894e-05, 1.8329083104617894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8329083104617894e-05

Optimization complete. Final v2v error: 3.625842332839966 mm

Highest mean error: 4.098049163818359 mm for frame 13

Lowest mean error: 3.444260358810425 mm for frame 238

Saving results

Total time: 154.60555481910706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416436
Iteration 2/25 | Loss: 0.00127258
Iteration 3/25 | Loss: 0.00115086
Iteration 4/25 | Loss: 0.00114241
Iteration 5/25 | Loss: 0.00114058
Iteration 6/25 | Loss: 0.00113994
Iteration 7/25 | Loss: 0.00113990
Iteration 8/25 | Loss: 0.00113990
Iteration 9/25 | Loss: 0.00113990
Iteration 10/25 | Loss: 0.00113990
Iteration 11/25 | Loss: 0.00113990
Iteration 12/25 | Loss: 0.00113990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011399011127650738, 0.0011399011127650738, 0.0011399011127650738, 0.0011399011127650738, 0.0011399011127650738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011399011127650738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34298694
Iteration 2/25 | Loss: 0.00103442
Iteration 3/25 | Loss: 0.00103441
Iteration 4/25 | Loss: 0.00103441
Iteration 5/25 | Loss: 0.00103441
Iteration 6/25 | Loss: 0.00103441
Iteration 7/25 | Loss: 0.00103441
Iteration 8/25 | Loss: 0.00103441
Iteration 9/25 | Loss: 0.00103441
Iteration 10/25 | Loss: 0.00103441
Iteration 11/25 | Loss: 0.00103441
Iteration 12/25 | Loss: 0.00103441
Iteration 13/25 | Loss: 0.00103441
Iteration 14/25 | Loss: 0.00103441
Iteration 15/25 | Loss: 0.00103441
Iteration 16/25 | Loss: 0.00103441
Iteration 17/25 | Loss: 0.00103441
Iteration 18/25 | Loss: 0.00103441
Iteration 19/25 | Loss: 0.00103441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010344093898311257, 0.0010344093898311257, 0.0010344093898311257, 0.0010344093898311257, 0.0010344093898311257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010344093898311257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103441
Iteration 2/1000 | Loss: 0.00003938
Iteration 3/1000 | Loss: 0.00001944
Iteration 4/1000 | Loss: 0.00001633
Iteration 5/1000 | Loss: 0.00001499
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001332
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001304
Iteration 26/1000 | Loss: 0.00001304
Iteration 27/1000 | Loss: 0.00001303
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001294
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001293
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001290
Iteration 49/1000 | Loss: 0.00001290
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001289
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001288
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001287
Iteration 59/1000 | Loss: 0.00001287
Iteration 60/1000 | Loss: 0.00001287
Iteration 61/1000 | Loss: 0.00001287
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001287
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001286
Iteration 66/1000 | Loss: 0.00001286
Iteration 67/1000 | Loss: 0.00001285
Iteration 68/1000 | Loss: 0.00001285
Iteration 69/1000 | Loss: 0.00001285
Iteration 70/1000 | Loss: 0.00001285
Iteration 71/1000 | Loss: 0.00001285
Iteration 72/1000 | Loss: 0.00001285
Iteration 73/1000 | Loss: 0.00001285
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001284
Iteration 82/1000 | Loss: 0.00001284
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001281
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001281
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001280
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001278
Iteration 116/1000 | Loss: 0.00001278
Iteration 117/1000 | Loss: 0.00001278
Iteration 118/1000 | Loss: 0.00001278
Iteration 119/1000 | Loss: 0.00001278
Iteration 120/1000 | Loss: 0.00001278
Iteration 121/1000 | Loss: 0.00001277
Iteration 122/1000 | Loss: 0.00001277
Iteration 123/1000 | Loss: 0.00001276
Iteration 124/1000 | Loss: 0.00001276
Iteration 125/1000 | Loss: 0.00001276
Iteration 126/1000 | Loss: 0.00001276
Iteration 127/1000 | Loss: 0.00001276
Iteration 128/1000 | Loss: 0.00001276
Iteration 129/1000 | Loss: 0.00001276
Iteration 130/1000 | Loss: 0.00001275
Iteration 131/1000 | Loss: 0.00001275
Iteration 132/1000 | Loss: 0.00001275
Iteration 133/1000 | Loss: 0.00001275
Iteration 134/1000 | Loss: 0.00001275
Iteration 135/1000 | Loss: 0.00001275
Iteration 136/1000 | Loss: 0.00001274
Iteration 137/1000 | Loss: 0.00001274
Iteration 138/1000 | Loss: 0.00001274
Iteration 139/1000 | Loss: 0.00001274
Iteration 140/1000 | Loss: 0.00001274
Iteration 141/1000 | Loss: 0.00001274
Iteration 142/1000 | Loss: 0.00001274
Iteration 143/1000 | Loss: 0.00001274
Iteration 144/1000 | Loss: 0.00001273
Iteration 145/1000 | Loss: 0.00001273
Iteration 146/1000 | Loss: 0.00001273
Iteration 147/1000 | Loss: 0.00001273
Iteration 148/1000 | Loss: 0.00001273
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001272
Iteration 151/1000 | Loss: 0.00001272
Iteration 152/1000 | Loss: 0.00001272
Iteration 153/1000 | Loss: 0.00001271
Iteration 154/1000 | Loss: 0.00001271
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001270
Iteration 157/1000 | Loss: 0.00001269
Iteration 158/1000 | Loss: 0.00001269
Iteration 159/1000 | Loss: 0.00001269
Iteration 160/1000 | Loss: 0.00001269
Iteration 161/1000 | Loss: 0.00001269
Iteration 162/1000 | Loss: 0.00001269
Iteration 163/1000 | Loss: 0.00001269
Iteration 164/1000 | Loss: 0.00001269
Iteration 165/1000 | Loss: 0.00001268
Iteration 166/1000 | Loss: 0.00001268
Iteration 167/1000 | Loss: 0.00001268
Iteration 168/1000 | Loss: 0.00001268
Iteration 169/1000 | Loss: 0.00001268
Iteration 170/1000 | Loss: 0.00001268
Iteration 171/1000 | Loss: 0.00001267
Iteration 172/1000 | Loss: 0.00001267
Iteration 173/1000 | Loss: 0.00001267
Iteration 174/1000 | Loss: 0.00001267
Iteration 175/1000 | Loss: 0.00001267
Iteration 176/1000 | Loss: 0.00001267
Iteration 177/1000 | Loss: 0.00001267
Iteration 178/1000 | Loss: 0.00001267
Iteration 179/1000 | Loss: 0.00001267
Iteration 180/1000 | Loss: 0.00001267
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001267
Iteration 183/1000 | Loss: 0.00001267
Iteration 184/1000 | Loss: 0.00001267
Iteration 185/1000 | Loss: 0.00001267
Iteration 186/1000 | Loss: 0.00001266
Iteration 187/1000 | Loss: 0.00001266
Iteration 188/1000 | Loss: 0.00001266
Iteration 189/1000 | Loss: 0.00001266
Iteration 190/1000 | Loss: 0.00001266
Iteration 191/1000 | Loss: 0.00001266
Iteration 192/1000 | Loss: 0.00001266
Iteration 193/1000 | Loss: 0.00001266
Iteration 194/1000 | Loss: 0.00001266
Iteration 195/1000 | Loss: 0.00001266
Iteration 196/1000 | Loss: 0.00001266
Iteration 197/1000 | Loss: 0.00001266
Iteration 198/1000 | Loss: 0.00001266
Iteration 199/1000 | Loss: 0.00001266
Iteration 200/1000 | Loss: 0.00001266
Iteration 201/1000 | Loss: 0.00001266
Iteration 202/1000 | Loss: 0.00001266
Iteration 203/1000 | Loss: 0.00001266
Iteration 204/1000 | Loss: 0.00001266
Iteration 205/1000 | Loss: 0.00001266
Iteration 206/1000 | Loss: 0.00001266
Iteration 207/1000 | Loss: 0.00001266
Iteration 208/1000 | Loss: 0.00001266
Iteration 209/1000 | Loss: 0.00001266
Iteration 210/1000 | Loss: 0.00001266
Iteration 211/1000 | Loss: 0.00001266
Iteration 212/1000 | Loss: 0.00001266
Iteration 213/1000 | Loss: 0.00001266
Iteration 214/1000 | Loss: 0.00001266
Iteration 215/1000 | Loss: 0.00001266
Iteration 216/1000 | Loss: 0.00001266
Iteration 217/1000 | Loss: 0.00001266
Iteration 218/1000 | Loss: 0.00001266
Iteration 219/1000 | Loss: 0.00001266
Iteration 220/1000 | Loss: 0.00001266
Iteration 221/1000 | Loss: 0.00001266
Iteration 222/1000 | Loss: 0.00001266
Iteration 223/1000 | Loss: 0.00001266
Iteration 224/1000 | Loss: 0.00001266
Iteration 225/1000 | Loss: 0.00001266
Iteration 226/1000 | Loss: 0.00001266
Iteration 227/1000 | Loss: 0.00001266
Iteration 228/1000 | Loss: 0.00001266
Iteration 229/1000 | Loss: 0.00001266
Iteration 230/1000 | Loss: 0.00001266
Iteration 231/1000 | Loss: 0.00001266
Iteration 232/1000 | Loss: 0.00001266
Iteration 233/1000 | Loss: 0.00001266
Iteration 234/1000 | Loss: 0.00001266
Iteration 235/1000 | Loss: 0.00001266
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2658444575208705e-05, 1.2658444575208705e-05, 1.2658444575208705e-05, 1.2658444575208705e-05, 1.2658444575208705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2658444575208705e-05

Optimization complete. Final v2v error: 2.9717414379119873 mm

Highest mean error: 4.058413028717041 mm for frame 60

Lowest mean error: 2.648667573928833 mm for frame 134

Saving results

Total time: 38.44827675819397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403466
Iteration 2/25 | Loss: 0.00122229
Iteration 3/25 | Loss: 0.00110787
Iteration 4/25 | Loss: 0.00110387
Iteration 5/25 | Loss: 0.00110292
Iteration 6/25 | Loss: 0.00110274
Iteration 7/25 | Loss: 0.00110274
Iteration 8/25 | Loss: 0.00110274
Iteration 9/25 | Loss: 0.00110274
Iteration 10/25 | Loss: 0.00110274
Iteration 11/25 | Loss: 0.00110274
Iteration 12/25 | Loss: 0.00110274
Iteration 13/25 | Loss: 0.00110274
Iteration 14/25 | Loss: 0.00110274
Iteration 15/25 | Loss: 0.00110274
Iteration 16/25 | Loss: 0.00110274
Iteration 17/25 | Loss: 0.00110274
Iteration 18/25 | Loss: 0.00110274
Iteration 19/25 | Loss: 0.00110274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001102739479392767, 0.001102739479392767, 0.001102739479392767, 0.001102739479392767, 0.001102739479392767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102739479392767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33748972
Iteration 2/25 | Loss: 0.00095772
Iteration 3/25 | Loss: 0.00095772
Iteration 4/25 | Loss: 0.00095772
Iteration 5/25 | Loss: 0.00095772
Iteration 6/25 | Loss: 0.00095772
Iteration 7/25 | Loss: 0.00095772
Iteration 8/25 | Loss: 0.00095772
Iteration 9/25 | Loss: 0.00095772
Iteration 10/25 | Loss: 0.00095772
Iteration 11/25 | Loss: 0.00095772
Iteration 12/25 | Loss: 0.00095772
Iteration 13/25 | Loss: 0.00095772
Iteration 14/25 | Loss: 0.00095772
Iteration 15/25 | Loss: 0.00095772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009577166056260467, 0.0009577166056260467, 0.0009577166056260467, 0.0009577166056260467, 0.0009577166056260467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009577166056260467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095772
Iteration 2/1000 | Loss: 0.00002811
Iteration 3/1000 | Loss: 0.00001708
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001244
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001231
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001230
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001230
Iteration 47/1000 | Loss: 0.00001230
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001229
Iteration 51/1000 | Loss: 0.00001229
Iteration 52/1000 | Loss: 0.00001229
Iteration 53/1000 | Loss: 0.00001229
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001228
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001225
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001224
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001221
Iteration 80/1000 | Loss: 0.00001220
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001218
Iteration 92/1000 | Loss: 0.00001218
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001215
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001213
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001212
Iteration 106/1000 | Loss: 0.00001212
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001211
Iteration 110/1000 | Loss: 0.00001211
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001209
Iteration 115/1000 | Loss: 0.00001209
Iteration 116/1000 | Loss: 0.00001208
Iteration 117/1000 | Loss: 0.00001208
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001206
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001206
Iteration 142/1000 | Loss: 0.00001206
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001206
Iteration 146/1000 | Loss: 0.00001206
Iteration 147/1000 | Loss: 0.00001206
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001206
Iteration 162/1000 | Loss: 0.00001206
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Iteration 167/1000 | Loss: 0.00001206
Iteration 168/1000 | Loss: 0.00001206
Iteration 169/1000 | Loss: 0.00001206
Iteration 170/1000 | Loss: 0.00001206
Iteration 171/1000 | Loss: 0.00001206
Iteration 172/1000 | Loss: 0.00001206
Iteration 173/1000 | Loss: 0.00001206
Iteration 174/1000 | Loss: 0.00001206
Iteration 175/1000 | Loss: 0.00001206
Iteration 176/1000 | Loss: 0.00001206
Iteration 177/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.2058984793839045e-05, 1.2058984793839045e-05, 1.2058984793839045e-05, 1.2058984793839045e-05, 1.2058984793839045e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2058984793839045e-05

Optimization complete. Final v2v error: 2.7931673526763916 mm

Highest mean error: 4.605831146240234 mm for frame 70

Lowest mean error: 2.474705457687378 mm for frame 149

Saving results

Total time: 33.65242123603821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491122
Iteration 2/25 | Loss: 0.00144441
Iteration 3/25 | Loss: 0.00124659
Iteration 4/25 | Loss: 0.00122754
Iteration 5/25 | Loss: 0.00122066
Iteration 6/25 | Loss: 0.00121971
Iteration 7/25 | Loss: 0.00121971
Iteration 8/25 | Loss: 0.00121971
Iteration 9/25 | Loss: 0.00121971
Iteration 10/25 | Loss: 0.00121971
Iteration 11/25 | Loss: 0.00121971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012197104515507817, 0.0012197104515507817, 0.0012197104515507817, 0.0012197104515507817, 0.0012197104515507817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012197104515507817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35164297
Iteration 2/25 | Loss: 0.00116719
Iteration 3/25 | Loss: 0.00116717
Iteration 4/25 | Loss: 0.00116717
Iteration 5/25 | Loss: 0.00116717
Iteration 6/25 | Loss: 0.00116717
Iteration 7/25 | Loss: 0.00116717
Iteration 8/25 | Loss: 0.00116717
Iteration 9/25 | Loss: 0.00116717
Iteration 10/25 | Loss: 0.00116717
Iteration 11/25 | Loss: 0.00116717
Iteration 12/25 | Loss: 0.00116717
Iteration 13/25 | Loss: 0.00116717
Iteration 14/25 | Loss: 0.00116717
Iteration 15/25 | Loss: 0.00116717
Iteration 16/25 | Loss: 0.00116717
Iteration 17/25 | Loss: 0.00116717
Iteration 18/25 | Loss: 0.00116717
Iteration 19/25 | Loss: 0.00116717
Iteration 20/25 | Loss: 0.00116717
Iteration 21/25 | Loss: 0.00116717
Iteration 22/25 | Loss: 0.00116717
Iteration 23/25 | Loss: 0.00116717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011671698885038495, 0.0011671698885038495, 0.0011671698885038495, 0.0011671698885038495, 0.0011671698885038495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011671698885038495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116717
Iteration 2/1000 | Loss: 0.00005713
Iteration 3/1000 | Loss: 0.00003421
Iteration 4/1000 | Loss: 0.00002651
Iteration 5/1000 | Loss: 0.00002394
Iteration 6/1000 | Loss: 0.00002288
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002174
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002102
Iteration 11/1000 | Loss: 0.00002073
Iteration 12/1000 | Loss: 0.00002062
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00002051
Iteration 16/1000 | Loss: 0.00002050
Iteration 17/1000 | Loss: 0.00002049
Iteration 18/1000 | Loss: 0.00002043
Iteration 19/1000 | Loss: 0.00002039
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002024
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002023
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002021
Iteration 35/1000 | Loss: 0.00002021
Iteration 36/1000 | Loss: 0.00002021
Iteration 37/1000 | Loss: 0.00002020
Iteration 38/1000 | Loss: 0.00002020
Iteration 39/1000 | Loss: 0.00002019
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002017
Iteration 43/1000 | Loss: 0.00002017
Iteration 44/1000 | Loss: 0.00002017
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002016
Iteration 47/1000 | Loss: 0.00002016
Iteration 48/1000 | Loss: 0.00002016
Iteration 49/1000 | Loss: 0.00002016
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002016
Iteration 53/1000 | Loss: 0.00002016
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002015
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002013
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002013
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002012
Iteration 64/1000 | Loss: 0.00002012
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002012
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002010
Iteration 72/1000 | Loss: 0.00002010
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002008
Iteration 79/1000 | Loss: 0.00002008
Iteration 80/1000 | Loss: 0.00002008
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002007
Iteration 83/1000 | Loss: 0.00002007
Iteration 84/1000 | Loss: 0.00002006
Iteration 85/1000 | Loss: 0.00002006
Iteration 86/1000 | Loss: 0.00002006
Iteration 87/1000 | Loss: 0.00002006
Iteration 88/1000 | Loss: 0.00002006
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002005
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002004
Iteration 96/1000 | Loss: 0.00002004
Iteration 97/1000 | Loss: 0.00002004
Iteration 98/1000 | Loss: 0.00002004
Iteration 99/1000 | Loss: 0.00002004
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002003
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002002
Iteration 110/1000 | Loss: 0.00002002
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00002002
Iteration 115/1000 | Loss: 0.00002002
Iteration 116/1000 | Loss: 0.00002002
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.0018740542582236e-05, 2.0018740542582236e-05, 2.0018740542582236e-05, 2.0018740542582236e-05, 2.0018740542582236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0018740542582236e-05

Optimization complete. Final v2v error: 3.6844241619110107 mm

Highest mean error: 5.4182820320129395 mm for frame 214

Lowest mean error: 2.8891379833221436 mm for frame 52

Saving results

Total time: 41.667030334472656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053746
Iteration 2/25 | Loss: 0.01053746
Iteration 3/25 | Loss: 0.01053745
Iteration 4/25 | Loss: 0.00263469
Iteration 5/25 | Loss: 0.00203435
Iteration 6/25 | Loss: 0.00164755
Iteration 7/25 | Loss: 0.00143779
Iteration 8/25 | Loss: 0.00133673
Iteration 9/25 | Loss: 0.00121832
Iteration 10/25 | Loss: 0.00115594
Iteration 11/25 | Loss: 0.00111728
Iteration 12/25 | Loss: 0.00109804
Iteration 13/25 | Loss: 0.00108841
Iteration 14/25 | Loss: 0.00108716
Iteration 15/25 | Loss: 0.00108616
Iteration 16/25 | Loss: 0.00108540
Iteration 17/25 | Loss: 0.00108517
Iteration 18/25 | Loss: 0.00108510
Iteration 19/25 | Loss: 0.00108509
Iteration 20/25 | Loss: 0.00108509
Iteration 21/25 | Loss: 0.00108509
Iteration 22/25 | Loss: 0.00108509
Iteration 23/25 | Loss: 0.00108509
Iteration 24/25 | Loss: 0.00108508
Iteration 25/25 | Loss: 0.00108508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32617676
Iteration 2/25 | Loss: 0.00116509
Iteration 3/25 | Loss: 0.00116509
Iteration 4/25 | Loss: 0.00116509
Iteration 5/25 | Loss: 0.00116509
Iteration 6/25 | Loss: 0.00116509
Iteration 7/25 | Loss: 0.00116509
Iteration 8/25 | Loss: 0.00116509
Iteration 9/25 | Loss: 0.00116509
Iteration 10/25 | Loss: 0.00116509
Iteration 11/25 | Loss: 0.00116509
Iteration 12/25 | Loss: 0.00116509
Iteration 13/25 | Loss: 0.00116509
Iteration 14/25 | Loss: 0.00116509
Iteration 15/25 | Loss: 0.00116509
Iteration 16/25 | Loss: 0.00116509
Iteration 17/25 | Loss: 0.00116509
Iteration 18/25 | Loss: 0.00116509
Iteration 19/25 | Loss: 0.00116509
Iteration 20/25 | Loss: 0.00116509
Iteration 21/25 | Loss: 0.00116509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011650859378278255, 0.0011650859378278255, 0.0011650859378278255, 0.0011650859378278255, 0.0011650859378278255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011650859378278255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116509
Iteration 2/1000 | Loss: 0.00004620
Iteration 3/1000 | Loss: 0.00002775
Iteration 4/1000 | Loss: 0.00002349
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00047606
Iteration 7/1000 | Loss: 0.00002104
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00006177
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00006051
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001802
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001766
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001764
Iteration 22/1000 | Loss: 0.00001763
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001756
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001756
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00007550
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001753
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00001751
Iteration 42/1000 | Loss: 0.00001751
Iteration 43/1000 | Loss: 0.00001751
Iteration 44/1000 | Loss: 0.00001751
Iteration 45/1000 | Loss: 0.00001750
Iteration 46/1000 | Loss: 0.00001750
Iteration 47/1000 | Loss: 0.00001749
Iteration 48/1000 | Loss: 0.00001749
Iteration 49/1000 | Loss: 0.00001748
Iteration 50/1000 | Loss: 0.00001748
Iteration 51/1000 | Loss: 0.00001748
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001747
Iteration 54/1000 | Loss: 0.00001747
Iteration 55/1000 | Loss: 0.00001747
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001746
Iteration 59/1000 | Loss: 0.00001746
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001745
Iteration 65/1000 | Loss: 0.00001745
Iteration 66/1000 | Loss: 0.00001745
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.7441185264033265e-05, 1.7441185264033265e-05, 1.7441185264033265e-05, 1.7441185264033265e-05, 1.7441185264033265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7441185264033265e-05

Optimization complete. Final v2v error: 3.2920775413513184 mm

Highest mean error: 9.759003639221191 mm for frame 112

Lowest mean error: 2.771638870239258 mm for frame 188

Saving results

Total time: 63.09044361114502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034366
Iteration 2/25 | Loss: 0.00221838
Iteration 3/25 | Loss: 0.00138117
Iteration 4/25 | Loss: 0.00127920
Iteration 5/25 | Loss: 0.00124823
Iteration 6/25 | Loss: 0.00121475
Iteration 7/25 | Loss: 0.00118880
Iteration 8/25 | Loss: 0.00118297
Iteration 9/25 | Loss: 0.00117403
Iteration 10/25 | Loss: 0.00117020
Iteration 11/25 | Loss: 0.00116944
Iteration 12/25 | Loss: 0.00116925
Iteration 13/25 | Loss: 0.00116924
Iteration 14/25 | Loss: 0.00116924
Iteration 15/25 | Loss: 0.00116924
Iteration 16/25 | Loss: 0.00116924
Iteration 17/25 | Loss: 0.00116924
Iteration 18/25 | Loss: 0.00116924
Iteration 19/25 | Loss: 0.00116924
Iteration 20/25 | Loss: 0.00116923
Iteration 21/25 | Loss: 0.00116923
Iteration 22/25 | Loss: 0.00116923
Iteration 23/25 | Loss: 0.00116923
Iteration 24/25 | Loss: 0.00116923
Iteration 25/25 | Loss: 0.00116923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31781626
Iteration 2/25 | Loss: 0.00064523
Iteration 3/25 | Loss: 0.00064523
Iteration 4/25 | Loss: 0.00064523
Iteration 5/25 | Loss: 0.00064523
Iteration 6/25 | Loss: 0.00064522
Iteration 7/25 | Loss: 0.00064522
Iteration 8/25 | Loss: 0.00064522
Iteration 9/25 | Loss: 0.00064522
Iteration 10/25 | Loss: 0.00064522
Iteration 11/25 | Loss: 0.00064522
Iteration 12/25 | Loss: 0.00064522
Iteration 13/25 | Loss: 0.00064522
Iteration 14/25 | Loss: 0.00064522
Iteration 15/25 | Loss: 0.00064522
Iteration 16/25 | Loss: 0.00064522
Iteration 17/25 | Loss: 0.00064522
Iteration 18/25 | Loss: 0.00064522
Iteration 19/25 | Loss: 0.00064522
Iteration 20/25 | Loss: 0.00064522
Iteration 21/25 | Loss: 0.00064522
Iteration 22/25 | Loss: 0.00064522
Iteration 23/25 | Loss: 0.00064522
Iteration 24/25 | Loss: 0.00064522
Iteration 25/25 | Loss: 0.00064522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064522
Iteration 2/1000 | Loss: 0.00005009
Iteration 3/1000 | Loss: 0.00002808
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00006757
Iteration 7/1000 | Loss: 0.00001777
Iteration 8/1000 | Loss: 0.00001553
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001415
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001409
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001392
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001392
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001388
Iteration 33/1000 | Loss: 0.00001388
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001388
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001386
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001385
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001380
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001377
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001376
Iteration 86/1000 | Loss: 0.00001376
Iteration 87/1000 | Loss: 0.00001375
Iteration 88/1000 | Loss: 0.00001375
Iteration 89/1000 | Loss: 0.00001375
Iteration 90/1000 | Loss: 0.00001375
Iteration 91/1000 | Loss: 0.00001374
Iteration 92/1000 | Loss: 0.00001374
Iteration 93/1000 | Loss: 0.00001374
Iteration 94/1000 | Loss: 0.00001374
Iteration 95/1000 | Loss: 0.00001373
Iteration 96/1000 | Loss: 0.00001373
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001371
Iteration 101/1000 | Loss: 0.00001371
Iteration 102/1000 | Loss: 0.00001371
Iteration 103/1000 | Loss: 0.00001371
Iteration 104/1000 | Loss: 0.00001371
Iteration 105/1000 | Loss: 0.00001370
Iteration 106/1000 | Loss: 0.00001370
Iteration 107/1000 | Loss: 0.00001370
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001369
Iteration 111/1000 | Loss: 0.00001369
Iteration 112/1000 | Loss: 0.00001369
Iteration 113/1000 | Loss: 0.00001369
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001368
Iteration 118/1000 | Loss: 0.00001368
Iteration 119/1000 | Loss: 0.00001368
Iteration 120/1000 | Loss: 0.00001368
Iteration 121/1000 | Loss: 0.00001368
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Iteration 125/1000 | Loss: 0.00001368
Iteration 126/1000 | Loss: 0.00001368
Iteration 127/1000 | Loss: 0.00001368
Iteration 128/1000 | Loss: 0.00001368
Iteration 129/1000 | Loss: 0.00001368
Iteration 130/1000 | Loss: 0.00001368
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001367
Iteration 141/1000 | Loss: 0.00001367
Iteration 142/1000 | Loss: 0.00001367
Iteration 143/1000 | Loss: 0.00001367
Iteration 144/1000 | Loss: 0.00001367
Iteration 145/1000 | Loss: 0.00001367
Iteration 146/1000 | Loss: 0.00001366
Iteration 147/1000 | Loss: 0.00001366
Iteration 148/1000 | Loss: 0.00001366
Iteration 149/1000 | Loss: 0.00001366
Iteration 150/1000 | Loss: 0.00001366
Iteration 151/1000 | Loss: 0.00001366
Iteration 152/1000 | Loss: 0.00001366
Iteration 153/1000 | Loss: 0.00001366
Iteration 154/1000 | Loss: 0.00001366
Iteration 155/1000 | Loss: 0.00001366
Iteration 156/1000 | Loss: 0.00001366
Iteration 157/1000 | Loss: 0.00001366
Iteration 158/1000 | Loss: 0.00001365
Iteration 159/1000 | Loss: 0.00001365
Iteration 160/1000 | Loss: 0.00001365
Iteration 161/1000 | Loss: 0.00001365
Iteration 162/1000 | Loss: 0.00001365
Iteration 163/1000 | Loss: 0.00001365
Iteration 164/1000 | Loss: 0.00001365
Iteration 165/1000 | Loss: 0.00001364
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001364
Iteration 169/1000 | Loss: 0.00001364
Iteration 170/1000 | Loss: 0.00001364
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001362
Iteration 179/1000 | Loss: 0.00001362
Iteration 180/1000 | Loss: 0.00001362
Iteration 181/1000 | Loss: 0.00001362
Iteration 182/1000 | Loss: 0.00001362
Iteration 183/1000 | Loss: 0.00001362
Iteration 184/1000 | Loss: 0.00001362
Iteration 185/1000 | Loss: 0.00001362
Iteration 186/1000 | Loss: 0.00001362
Iteration 187/1000 | Loss: 0.00001362
Iteration 188/1000 | Loss: 0.00001362
Iteration 189/1000 | Loss: 0.00001362
Iteration 190/1000 | Loss: 0.00001362
Iteration 191/1000 | Loss: 0.00001362
Iteration 192/1000 | Loss: 0.00001362
Iteration 193/1000 | Loss: 0.00001361
Iteration 194/1000 | Loss: 0.00001361
Iteration 195/1000 | Loss: 0.00001361
Iteration 196/1000 | Loss: 0.00001361
Iteration 197/1000 | Loss: 0.00001361
Iteration 198/1000 | Loss: 0.00001361
Iteration 199/1000 | Loss: 0.00001361
Iteration 200/1000 | Loss: 0.00001361
Iteration 201/1000 | Loss: 0.00001360
Iteration 202/1000 | Loss: 0.00001360
Iteration 203/1000 | Loss: 0.00001360
Iteration 204/1000 | Loss: 0.00001360
Iteration 205/1000 | Loss: 0.00001360
Iteration 206/1000 | Loss: 0.00001360
Iteration 207/1000 | Loss: 0.00001360
Iteration 208/1000 | Loss: 0.00001360
Iteration 209/1000 | Loss: 0.00001360
Iteration 210/1000 | Loss: 0.00001360
Iteration 211/1000 | Loss: 0.00001360
Iteration 212/1000 | Loss: 0.00001360
Iteration 213/1000 | Loss: 0.00001360
Iteration 214/1000 | Loss: 0.00001360
Iteration 215/1000 | Loss: 0.00001360
Iteration 216/1000 | Loss: 0.00001360
Iteration 217/1000 | Loss: 0.00001360
Iteration 218/1000 | Loss: 0.00001360
Iteration 219/1000 | Loss: 0.00001360
Iteration 220/1000 | Loss: 0.00001360
Iteration 221/1000 | Loss: 0.00001360
Iteration 222/1000 | Loss: 0.00001360
Iteration 223/1000 | Loss: 0.00001360
Iteration 224/1000 | Loss: 0.00001360
Iteration 225/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.360170244879555e-05, 1.360170244879555e-05, 1.360170244879555e-05, 1.360170244879555e-05, 1.360170244879555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.360170244879555e-05

Optimization complete. Final v2v error: 3.1563637256622314 mm

Highest mean error: 4.010231018066406 mm for frame 85

Lowest mean error: 2.565011978149414 mm for frame 94

Saving results

Total time: 64.88804197311401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540821
Iteration 2/25 | Loss: 0.00131492
Iteration 3/25 | Loss: 0.00116091
Iteration 4/25 | Loss: 0.00115133
Iteration 5/25 | Loss: 0.00114902
Iteration 6/25 | Loss: 0.00114864
Iteration 7/25 | Loss: 0.00114864
Iteration 8/25 | Loss: 0.00114864
Iteration 9/25 | Loss: 0.00114864
Iteration 10/25 | Loss: 0.00114864
Iteration 11/25 | Loss: 0.00114864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011486384319141507, 0.0011486384319141507, 0.0011486384319141507, 0.0011486384319141507, 0.0011486384319141507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011486384319141507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37067461
Iteration 2/25 | Loss: 0.00083277
Iteration 3/25 | Loss: 0.00083275
Iteration 4/25 | Loss: 0.00083275
Iteration 5/25 | Loss: 0.00083275
Iteration 6/25 | Loss: 0.00083275
Iteration 7/25 | Loss: 0.00083275
Iteration 8/25 | Loss: 0.00083275
Iteration 9/25 | Loss: 0.00083275
Iteration 10/25 | Loss: 0.00083275
Iteration 11/25 | Loss: 0.00083275
Iteration 12/25 | Loss: 0.00083275
Iteration 13/25 | Loss: 0.00083275
Iteration 14/25 | Loss: 0.00083275
Iteration 15/25 | Loss: 0.00083275
Iteration 16/25 | Loss: 0.00083275
Iteration 17/25 | Loss: 0.00083275
Iteration 18/25 | Loss: 0.00083275
Iteration 19/25 | Loss: 0.00083275
Iteration 20/25 | Loss: 0.00083275
Iteration 21/25 | Loss: 0.00083275
Iteration 22/25 | Loss: 0.00083275
Iteration 23/25 | Loss: 0.00083275
Iteration 24/25 | Loss: 0.00083275
Iteration 25/25 | Loss: 0.00083275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083275
Iteration 2/1000 | Loss: 0.00003646
Iteration 3/1000 | Loss: 0.00001957
Iteration 4/1000 | Loss: 0.00001568
Iteration 5/1000 | Loss: 0.00001407
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00001303
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001243
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001217
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001217
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001213
Iteration 42/1000 | Loss: 0.00001213
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001212
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001211
Iteration 57/1000 | Loss: 0.00001211
Iteration 58/1000 | Loss: 0.00001211
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001209
Iteration 76/1000 | Loss: 0.00001209
Iteration 77/1000 | Loss: 0.00001209
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001209
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001205
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001204
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001201
Iteration 152/1000 | Loss: 0.00001201
Iteration 153/1000 | Loss: 0.00001201
Iteration 154/1000 | Loss: 0.00001201
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001200
Iteration 159/1000 | Loss: 0.00001200
Iteration 160/1000 | Loss: 0.00001200
Iteration 161/1000 | Loss: 0.00001200
Iteration 162/1000 | Loss: 0.00001200
Iteration 163/1000 | Loss: 0.00001200
Iteration 164/1000 | Loss: 0.00001200
Iteration 165/1000 | Loss: 0.00001200
Iteration 166/1000 | Loss: 0.00001199
Iteration 167/1000 | Loss: 0.00001199
Iteration 168/1000 | Loss: 0.00001199
Iteration 169/1000 | Loss: 0.00001199
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001198
Iteration 172/1000 | Loss: 0.00001198
Iteration 173/1000 | Loss: 0.00001198
Iteration 174/1000 | Loss: 0.00001198
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00001198
Iteration 177/1000 | Loss: 0.00001198
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001197
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Iteration 184/1000 | Loss: 0.00001197
Iteration 185/1000 | Loss: 0.00001197
Iteration 186/1000 | Loss: 0.00001197
Iteration 187/1000 | Loss: 0.00001197
Iteration 188/1000 | Loss: 0.00001197
Iteration 189/1000 | Loss: 0.00001197
Iteration 190/1000 | Loss: 0.00001197
Iteration 191/1000 | Loss: 0.00001197
Iteration 192/1000 | Loss: 0.00001197
Iteration 193/1000 | Loss: 0.00001197
Iteration 194/1000 | Loss: 0.00001197
Iteration 195/1000 | Loss: 0.00001197
Iteration 196/1000 | Loss: 0.00001197
Iteration 197/1000 | Loss: 0.00001197
Iteration 198/1000 | Loss: 0.00001197
Iteration 199/1000 | Loss: 0.00001197
Iteration 200/1000 | Loss: 0.00001197
Iteration 201/1000 | Loss: 0.00001197
Iteration 202/1000 | Loss: 0.00001197
Iteration 203/1000 | Loss: 0.00001197
Iteration 204/1000 | Loss: 0.00001197
Iteration 205/1000 | Loss: 0.00001197
Iteration 206/1000 | Loss: 0.00001197
Iteration 207/1000 | Loss: 0.00001197
Iteration 208/1000 | Loss: 0.00001197
Iteration 209/1000 | Loss: 0.00001197
Iteration 210/1000 | Loss: 0.00001197
Iteration 211/1000 | Loss: 0.00001197
Iteration 212/1000 | Loss: 0.00001197
Iteration 213/1000 | Loss: 0.00001197
Iteration 214/1000 | Loss: 0.00001197
Iteration 215/1000 | Loss: 0.00001197
Iteration 216/1000 | Loss: 0.00001197
Iteration 217/1000 | Loss: 0.00001197
Iteration 218/1000 | Loss: 0.00001197
Iteration 219/1000 | Loss: 0.00001197
Iteration 220/1000 | Loss: 0.00001197
Iteration 221/1000 | Loss: 0.00001197
Iteration 222/1000 | Loss: 0.00001197
Iteration 223/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.196766515931813e-05, 1.196766515931813e-05, 1.196766515931813e-05, 1.196766515931813e-05, 1.196766515931813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.196766515931813e-05

Optimization complete. Final v2v error: 2.875438690185547 mm

Highest mean error: 3.7237043380737305 mm for frame 47

Lowest mean error: 2.4265589714050293 mm for frame 112

Saving results

Total time: 37.96886706352234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453558
Iteration 2/25 | Loss: 0.00144899
Iteration 3/25 | Loss: 0.00120147
Iteration 4/25 | Loss: 0.00116388
Iteration 5/25 | Loss: 0.00115917
Iteration 6/25 | Loss: 0.00115848
Iteration 7/25 | Loss: 0.00115848
Iteration 8/25 | Loss: 0.00115848
Iteration 9/25 | Loss: 0.00115848
Iteration 10/25 | Loss: 0.00115848
Iteration 11/25 | Loss: 0.00115848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011584830936044455, 0.0011584830936044455, 0.0011584830936044455, 0.0011584830936044455, 0.0011584830936044455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011584830936044455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31426883
Iteration 2/25 | Loss: 0.00088934
Iteration 3/25 | Loss: 0.00088933
Iteration 4/25 | Loss: 0.00088933
Iteration 5/25 | Loss: 0.00088933
Iteration 6/25 | Loss: 0.00088933
Iteration 7/25 | Loss: 0.00088933
Iteration 8/25 | Loss: 0.00088933
Iteration 9/25 | Loss: 0.00088933
Iteration 10/25 | Loss: 0.00088933
Iteration 11/25 | Loss: 0.00088933
Iteration 12/25 | Loss: 0.00088933
Iteration 13/25 | Loss: 0.00088933
Iteration 14/25 | Loss: 0.00088933
Iteration 15/25 | Loss: 0.00088933
Iteration 16/25 | Loss: 0.00088933
Iteration 17/25 | Loss: 0.00088933
Iteration 18/25 | Loss: 0.00088933
Iteration 19/25 | Loss: 0.00088933
Iteration 20/25 | Loss: 0.00088933
Iteration 21/25 | Loss: 0.00088933
Iteration 22/25 | Loss: 0.00088933
Iteration 23/25 | Loss: 0.00088933
Iteration 24/25 | Loss: 0.00088933
Iteration 25/25 | Loss: 0.00088933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088933
Iteration 2/1000 | Loss: 0.00004532
Iteration 3/1000 | Loss: 0.00002874
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001815
Iteration 6/1000 | Loss: 0.00001680
Iteration 7/1000 | Loss: 0.00001606
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001419
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001403
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001395
Iteration 17/1000 | Loss: 0.00001393
Iteration 18/1000 | Loss: 0.00001392
Iteration 19/1000 | Loss: 0.00001390
Iteration 20/1000 | Loss: 0.00001390
Iteration 21/1000 | Loss: 0.00001389
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001388
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001386
Iteration 26/1000 | Loss: 0.00001385
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001382
Iteration 31/1000 | Loss: 0.00001382
Iteration 32/1000 | Loss: 0.00001381
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001376
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001375
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001375
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001373
Iteration 51/1000 | Loss: 0.00001373
Iteration 52/1000 | Loss: 0.00001373
Iteration 53/1000 | Loss: 0.00001373
Iteration 54/1000 | Loss: 0.00001372
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001372
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001371
Iteration 64/1000 | Loss: 0.00001371
Iteration 65/1000 | Loss: 0.00001371
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001371
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001367
Iteration 101/1000 | Loss: 0.00001367
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001366
Iteration 104/1000 | Loss: 0.00001366
Iteration 105/1000 | Loss: 0.00001366
Iteration 106/1000 | Loss: 0.00001366
Iteration 107/1000 | Loss: 0.00001366
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001366
Iteration 110/1000 | Loss: 0.00001366
Iteration 111/1000 | Loss: 0.00001366
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001365
Iteration 114/1000 | Loss: 0.00001365
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001364
Iteration 121/1000 | Loss: 0.00001364
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001363
Iteration 125/1000 | Loss: 0.00001363
Iteration 126/1000 | Loss: 0.00001363
Iteration 127/1000 | Loss: 0.00001363
Iteration 128/1000 | Loss: 0.00001363
Iteration 129/1000 | Loss: 0.00001363
Iteration 130/1000 | Loss: 0.00001363
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001362
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001362
Iteration 140/1000 | Loss: 0.00001362
Iteration 141/1000 | Loss: 0.00001362
Iteration 142/1000 | Loss: 0.00001362
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Iteration 146/1000 | Loss: 0.00001362
Iteration 147/1000 | Loss: 0.00001362
Iteration 148/1000 | Loss: 0.00001362
Iteration 149/1000 | Loss: 0.00001361
Iteration 150/1000 | Loss: 0.00001361
Iteration 151/1000 | Loss: 0.00001361
Iteration 152/1000 | Loss: 0.00001361
Iteration 153/1000 | Loss: 0.00001361
Iteration 154/1000 | Loss: 0.00001361
Iteration 155/1000 | Loss: 0.00001361
Iteration 156/1000 | Loss: 0.00001361
Iteration 157/1000 | Loss: 0.00001361
Iteration 158/1000 | Loss: 0.00001361
Iteration 159/1000 | Loss: 0.00001361
Iteration 160/1000 | Loss: 0.00001361
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001360
Iteration 165/1000 | Loss: 0.00001360
Iteration 166/1000 | Loss: 0.00001360
Iteration 167/1000 | Loss: 0.00001360
Iteration 168/1000 | Loss: 0.00001360
Iteration 169/1000 | Loss: 0.00001360
Iteration 170/1000 | Loss: 0.00001360
Iteration 171/1000 | Loss: 0.00001360
Iteration 172/1000 | Loss: 0.00001360
Iteration 173/1000 | Loss: 0.00001360
Iteration 174/1000 | Loss: 0.00001360
Iteration 175/1000 | Loss: 0.00001360
Iteration 176/1000 | Loss: 0.00001360
Iteration 177/1000 | Loss: 0.00001359
Iteration 178/1000 | Loss: 0.00001359
Iteration 179/1000 | Loss: 0.00001359
Iteration 180/1000 | Loss: 0.00001359
Iteration 181/1000 | Loss: 0.00001359
Iteration 182/1000 | Loss: 0.00001359
Iteration 183/1000 | Loss: 0.00001359
Iteration 184/1000 | Loss: 0.00001359
Iteration 185/1000 | Loss: 0.00001359
Iteration 186/1000 | Loss: 0.00001359
Iteration 187/1000 | Loss: 0.00001359
Iteration 188/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.3593547919299453e-05, 1.3593547919299453e-05, 1.3593547919299453e-05, 1.3593547919299453e-05, 1.3593547919299453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3593547919299453e-05

Optimization complete. Final v2v error: 3.0685527324676514 mm

Highest mean error: 3.6545462608337402 mm for frame 202

Lowest mean error: 2.722388982772827 mm for frame 71

Saving results

Total time: 46.22844576835632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046569
Iteration 2/25 | Loss: 0.00198572
Iteration 3/25 | Loss: 0.00137751
Iteration 4/25 | Loss: 0.00126381
Iteration 5/25 | Loss: 0.00124013
Iteration 6/25 | Loss: 0.00121502
Iteration 7/25 | Loss: 0.00119349
Iteration 8/25 | Loss: 0.00116715
Iteration 9/25 | Loss: 0.00116180
Iteration 10/25 | Loss: 0.00115877
Iteration 11/25 | Loss: 0.00115709
Iteration 12/25 | Loss: 0.00115660
Iteration 13/25 | Loss: 0.00115633
Iteration 14/25 | Loss: 0.00115617
Iteration 15/25 | Loss: 0.00115601
Iteration 16/25 | Loss: 0.00115991
Iteration 17/25 | Loss: 0.00116675
Iteration 18/25 | Loss: 0.00116381
Iteration 19/25 | Loss: 0.00116111
Iteration 20/25 | Loss: 0.00115787
Iteration 21/25 | Loss: 0.00115995
Iteration 22/25 | Loss: 0.00115833
Iteration 23/25 | Loss: 0.00115935
Iteration 24/25 | Loss: 0.00115777
Iteration 25/25 | Loss: 0.00115883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34344792
Iteration 2/25 | Loss: 0.00112467
Iteration 3/25 | Loss: 0.00112379
Iteration 4/25 | Loss: 0.00112379
Iteration 5/25 | Loss: 0.00112379
Iteration 6/25 | Loss: 0.00112379
Iteration 7/25 | Loss: 0.00112379
Iteration 8/25 | Loss: 0.00112379
Iteration 9/25 | Loss: 0.00112379
Iteration 10/25 | Loss: 0.00112379
Iteration 11/25 | Loss: 0.00112379
Iteration 12/25 | Loss: 0.00112379
Iteration 13/25 | Loss: 0.00112379
Iteration 14/25 | Loss: 0.00112379
Iteration 15/25 | Loss: 0.00112379
Iteration 16/25 | Loss: 0.00112379
Iteration 17/25 | Loss: 0.00112379
Iteration 18/25 | Loss: 0.00112379
Iteration 19/25 | Loss: 0.00112379
Iteration 20/25 | Loss: 0.00112379
Iteration 21/25 | Loss: 0.00112379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011237921426072717, 0.0011237921426072717, 0.0011237921426072717, 0.0011237921426072717, 0.0011237921426072717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011237921426072717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112379
Iteration 2/1000 | Loss: 0.00007328
Iteration 3/1000 | Loss: 0.00017221
Iteration 4/1000 | Loss: 0.00008475
Iteration 5/1000 | Loss: 0.00012732
Iteration 6/1000 | Loss: 0.00015381
Iteration 7/1000 | Loss: 0.00010751
Iteration 8/1000 | Loss: 0.00008520
Iteration 9/1000 | Loss: 0.00004941
Iteration 10/1000 | Loss: 0.00010830
Iteration 11/1000 | Loss: 0.00007927
Iteration 12/1000 | Loss: 0.00014973
Iteration 13/1000 | Loss: 0.00002879
Iteration 14/1000 | Loss: 0.00002935
Iteration 15/1000 | Loss: 0.00003325
Iteration 16/1000 | Loss: 0.00002234
Iteration 17/1000 | Loss: 0.00002086
Iteration 18/1000 | Loss: 0.00003358
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00002586
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00020511
Iteration 24/1000 | Loss: 0.00004927
Iteration 25/1000 | Loss: 0.00007665
Iteration 26/1000 | Loss: 0.00004644
Iteration 27/1000 | Loss: 0.00006049
Iteration 28/1000 | Loss: 0.00002839
Iteration 29/1000 | Loss: 0.00005086
Iteration 30/1000 | Loss: 0.00004385
Iteration 31/1000 | Loss: 0.00002978
Iteration 32/1000 | Loss: 0.00004429
Iteration 33/1000 | Loss: 0.00004011
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00003265
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00003562
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00003221
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001552
Iteration 42/1000 | Loss: 0.00002246
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001403
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001985
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001389
Iteration 55/1000 | Loss: 0.00001389
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001385
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001384
Iteration 69/1000 | Loss: 0.00001384
Iteration 70/1000 | Loss: 0.00001383
Iteration 71/1000 | Loss: 0.00001383
Iteration 72/1000 | Loss: 0.00001383
Iteration 73/1000 | Loss: 0.00001383
Iteration 74/1000 | Loss: 0.00001383
Iteration 75/1000 | Loss: 0.00001383
Iteration 76/1000 | Loss: 0.00001382
Iteration 77/1000 | Loss: 0.00001382
Iteration 78/1000 | Loss: 0.00001382
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001382
Iteration 81/1000 | Loss: 0.00001381
Iteration 82/1000 | Loss: 0.00001381
Iteration 83/1000 | Loss: 0.00001381
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001380
Iteration 86/1000 | Loss: 0.00001380
Iteration 87/1000 | Loss: 0.00001380
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001375
Iteration 97/1000 | Loss: 0.00001375
Iteration 98/1000 | Loss: 0.00001375
Iteration 99/1000 | Loss: 0.00001374
Iteration 100/1000 | Loss: 0.00001374
Iteration 101/1000 | Loss: 0.00001374
Iteration 102/1000 | Loss: 0.00001373
Iteration 103/1000 | Loss: 0.00001373
Iteration 104/1000 | Loss: 0.00002039
Iteration 105/1000 | Loss: 0.00001641
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001370
Iteration 109/1000 | Loss: 0.00001370
Iteration 110/1000 | Loss: 0.00001370
Iteration 111/1000 | Loss: 0.00001370
Iteration 112/1000 | Loss: 0.00001370
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001369
Iteration 115/1000 | Loss: 0.00001369
Iteration 116/1000 | Loss: 0.00001369
Iteration 117/1000 | Loss: 0.00001369
Iteration 118/1000 | Loss: 0.00001369
Iteration 119/1000 | Loss: 0.00001560
Iteration 120/1000 | Loss: 0.00001665
Iteration 121/1000 | Loss: 0.00002181
Iteration 122/1000 | Loss: 0.00001368
Iteration 123/1000 | Loss: 0.00001368
Iteration 124/1000 | Loss: 0.00001368
Iteration 125/1000 | Loss: 0.00001367
Iteration 126/1000 | Loss: 0.00001367
Iteration 127/1000 | Loss: 0.00001367
Iteration 128/1000 | Loss: 0.00001367
Iteration 129/1000 | Loss: 0.00001367
Iteration 130/1000 | Loss: 0.00001367
Iteration 131/1000 | Loss: 0.00001367
Iteration 132/1000 | Loss: 0.00001367
Iteration 133/1000 | Loss: 0.00001367
Iteration 134/1000 | Loss: 0.00001367
Iteration 135/1000 | Loss: 0.00001367
Iteration 136/1000 | Loss: 0.00001367
Iteration 137/1000 | Loss: 0.00001367
Iteration 138/1000 | Loss: 0.00001367
Iteration 139/1000 | Loss: 0.00001367
Iteration 140/1000 | Loss: 0.00001367
Iteration 141/1000 | Loss: 0.00001367
Iteration 142/1000 | Loss: 0.00001367
Iteration 143/1000 | Loss: 0.00001367
Iteration 144/1000 | Loss: 0.00001367
Iteration 145/1000 | Loss: 0.00001367
Iteration 146/1000 | Loss: 0.00001367
Iteration 147/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3673398825631011e-05, 1.3673398825631011e-05, 1.3673398825631011e-05, 1.3673398825631011e-05, 1.3673398825631011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3673398825631011e-05

Optimization complete. Final v2v error: 3.053603172302246 mm

Highest mean error: 9.886857986450195 mm for frame 7

Lowest mean error: 2.537869930267334 mm for frame 157

Saving results

Total time: 143.90961837768555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878440
Iteration 2/25 | Loss: 0.00123806
Iteration 3/25 | Loss: 0.00111522
Iteration 4/25 | Loss: 0.00110300
Iteration 5/25 | Loss: 0.00110014
Iteration 6/25 | Loss: 0.00109957
Iteration 7/25 | Loss: 0.00109957
Iteration 8/25 | Loss: 0.00109957
Iteration 9/25 | Loss: 0.00109957
Iteration 10/25 | Loss: 0.00109957
Iteration 11/25 | Loss: 0.00109957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001099574496038258, 0.001099574496038258, 0.001099574496038258, 0.001099574496038258, 0.001099574496038258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001099574496038258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34402871
Iteration 2/25 | Loss: 0.00087276
Iteration 3/25 | Loss: 0.00087274
Iteration 4/25 | Loss: 0.00087274
Iteration 5/25 | Loss: 0.00087274
Iteration 6/25 | Loss: 0.00087274
Iteration 7/25 | Loss: 0.00087274
Iteration 8/25 | Loss: 0.00087274
Iteration 9/25 | Loss: 0.00087274
Iteration 10/25 | Loss: 0.00087274
Iteration 11/25 | Loss: 0.00087274
Iteration 12/25 | Loss: 0.00087274
Iteration 13/25 | Loss: 0.00087274
Iteration 14/25 | Loss: 0.00087274
Iteration 15/25 | Loss: 0.00087274
Iteration 16/25 | Loss: 0.00087274
Iteration 17/25 | Loss: 0.00087274
Iteration 18/25 | Loss: 0.00087274
Iteration 19/25 | Loss: 0.00087274
Iteration 20/25 | Loss: 0.00087274
Iteration 21/25 | Loss: 0.00087274
Iteration 22/25 | Loss: 0.00087274
Iteration 23/25 | Loss: 0.00087274
Iteration 24/25 | Loss: 0.00087274
Iteration 25/25 | Loss: 0.00087274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087274
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001450
Iteration 5/1000 | Loss: 0.00001310
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001143
Iteration 9/1000 | Loss: 0.00001114
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001049
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001048
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001043
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001041
Iteration 38/1000 | Loss: 0.00001041
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001040
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001040
Iteration 49/1000 | Loss: 0.00001039
Iteration 50/1000 | Loss: 0.00001039
Iteration 51/1000 | Loss: 0.00001039
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001039
Iteration 55/1000 | Loss: 0.00001038
Iteration 56/1000 | Loss: 0.00001038
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001038
Iteration 62/1000 | Loss: 0.00001038
Iteration 63/1000 | Loss: 0.00001038
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001037
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001037
Iteration 71/1000 | Loss: 0.00001037
Iteration 72/1000 | Loss: 0.00001037
Iteration 73/1000 | Loss: 0.00001037
Iteration 74/1000 | Loss: 0.00001037
Iteration 75/1000 | Loss: 0.00001037
Iteration 76/1000 | Loss: 0.00001037
Iteration 77/1000 | Loss: 0.00001037
Iteration 78/1000 | Loss: 0.00001037
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001037
Iteration 82/1000 | Loss: 0.00001036
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001036
Iteration 86/1000 | Loss: 0.00001036
Iteration 87/1000 | Loss: 0.00001036
Iteration 88/1000 | Loss: 0.00001036
Iteration 89/1000 | Loss: 0.00001036
Iteration 90/1000 | Loss: 0.00001036
Iteration 91/1000 | Loss: 0.00001036
Iteration 92/1000 | Loss: 0.00001035
Iteration 93/1000 | Loss: 0.00001035
Iteration 94/1000 | Loss: 0.00001035
Iteration 95/1000 | Loss: 0.00001035
Iteration 96/1000 | Loss: 0.00001035
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001034
Iteration 100/1000 | Loss: 0.00001034
Iteration 101/1000 | Loss: 0.00001034
Iteration 102/1000 | Loss: 0.00001034
Iteration 103/1000 | Loss: 0.00001034
Iteration 104/1000 | Loss: 0.00001034
Iteration 105/1000 | Loss: 0.00001034
Iteration 106/1000 | Loss: 0.00001034
Iteration 107/1000 | Loss: 0.00001033
Iteration 108/1000 | Loss: 0.00001033
Iteration 109/1000 | Loss: 0.00001033
Iteration 110/1000 | Loss: 0.00001033
Iteration 111/1000 | Loss: 0.00001033
Iteration 112/1000 | Loss: 0.00001033
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001032
Iteration 115/1000 | Loss: 0.00001032
Iteration 116/1000 | Loss: 0.00001032
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001031
Iteration 120/1000 | Loss: 0.00001031
Iteration 121/1000 | Loss: 0.00001031
Iteration 122/1000 | Loss: 0.00001031
Iteration 123/1000 | Loss: 0.00001031
Iteration 124/1000 | Loss: 0.00001031
Iteration 125/1000 | Loss: 0.00001031
Iteration 126/1000 | Loss: 0.00001031
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001031
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001030
Iteration 132/1000 | Loss: 0.00001030
Iteration 133/1000 | Loss: 0.00001030
Iteration 134/1000 | Loss: 0.00001030
Iteration 135/1000 | Loss: 0.00001030
Iteration 136/1000 | Loss: 0.00001030
Iteration 137/1000 | Loss: 0.00001030
Iteration 138/1000 | Loss: 0.00001030
Iteration 139/1000 | Loss: 0.00001030
Iteration 140/1000 | Loss: 0.00001030
Iteration 141/1000 | Loss: 0.00001030
Iteration 142/1000 | Loss: 0.00001030
Iteration 143/1000 | Loss: 0.00001030
Iteration 144/1000 | Loss: 0.00001029
Iteration 145/1000 | Loss: 0.00001029
Iteration 146/1000 | Loss: 0.00001029
Iteration 147/1000 | Loss: 0.00001029
Iteration 148/1000 | Loss: 0.00001029
Iteration 149/1000 | Loss: 0.00001029
Iteration 150/1000 | Loss: 0.00001029
Iteration 151/1000 | Loss: 0.00001029
Iteration 152/1000 | Loss: 0.00001029
Iteration 153/1000 | Loss: 0.00001029
Iteration 154/1000 | Loss: 0.00001029
Iteration 155/1000 | Loss: 0.00001029
Iteration 156/1000 | Loss: 0.00001029
Iteration 157/1000 | Loss: 0.00001029
Iteration 158/1000 | Loss: 0.00001029
Iteration 159/1000 | Loss: 0.00001029
Iteration 160/1000 | Loss: 0.00001029
Iteration 161/1000 | Loss: 0.00001029
Iteration 162/1000 | Loss: 0.00001029
Iteration 163/1000 | Loss: 0.00001029
Iteration 164/1000 | Loss: 0.00001029
Iteration 165/1000 | Loss: 0.00001029
Iteration 166/1000 | Loss: 0.00001029
Iteration 167/1000 | Loss: 0.00001029
Iteration 168/1000 | Loss: 0.00001029
Iteration 169/1000 | Loss: 0.00001029
Iteration 170/1000 | Loss: 0.00001029
Iteration 171/1000 | Loss: 0.00001029
Iteration 172/1000 | Loss: 0.00001029
Iteration 173/1000 | Loss: 0.00001029
Iteration 174/1000 | Loss: 0.00001029
Iteration 175/1000 | Loss: 0.00001029
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001029
Iteration 179/1000 | Loss: 0.00001029
Iteration 180/1000 | Loss: 0.00001029
Iteration 181/1000 | Loss: 0.00001029
Iteration 182/1000 | Loss: 0.00001029
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Iteration 187/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0291523722116835e-05, 1.0291523722116835e-05, 1.0291523722116835e-05, 1.0291523722116835e-05, 1.0291523722116835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0291523722116835e-05

Optimization complete. Final v2v error: 2.7429263591766357 mm

Highest mean error: 2.966171979904175 mm for frame 26

Lowest mean error: 2.4634008407592773 mm for frame 137

Saving results

Total time: 35.99629592895508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00701584
Iteration 2/25 | Loss: 0.00161146
Iteration 3/25 | Loss: 0.00137650
Iteration 4/25 | Loss: 0.00136880
Iteration 5/25 | Loss: 0.00136765
Iteration 6/25 | Loss: 0.00136765
Iteration 7/25 | Loss: 0.00136765
Iteration 8/25 | Loss: 0.00136765
Iteration 9/25 | Loss: 0.00136765
Iteration 10/25 | Loss: 0.00136765
Iteration 11/25 | Loss: 0.00136765
Iteration 12/25 | Loss: 0.00136765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013676488306373358, 0.0013676488306373358, 0.0013676488306373358, 0.0013676488306373358, 0.0013676488306373358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013676488306373358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56313443
Iteration 2/25 | Loss: 0.00069166
Iteration 3/25 | Loss: 0.00069166
Iteration 4/25 | Loss: 0.00069166
Iteration 5/25 | Loss: 0.00069166
Iteration 6/25 | Loss: 0.00069165
Iteration 7/25 | Loss: 0.00069165
Iteration 8/25 | Loss: 0.00069165
Iteration 9/25 | Loss: 0.00069165
Iteration 10/25 | Loss: 0.00069165
Iteration 11/25 | Loss: 0.00069165
Iteration 12/25 | Loss: 0.00069165
Iteration 13/25 | Loss: 0.00069165
Iteration 14/25 | Loss: 0.00069165
Iteration 15/25 | Loss: 0.00069165
Iteration 16/25 | Loss: 0.00069165
Iteration 17/25 | Loss: 0.00069165
Iteration 18/25 | Loss: 0.00069165
Iteration 19/25 | Loss: 0.00069165
Iteration 20/25 | Loss: 0.00069165
Iteration 21/25 | Loss: 0.00069165
Iteration 22/25 | Loss: 0.00069165
Iteration 23/25 | Loss: 0.00069165
Iteration 24/25 | Loss: 0.00069165
Iteration 25/25 | Loss: 0.00069165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069165
Iteration 2/1000 | Loss: 0.00010575
Iteration 3/1000 | Loss: 0.00007522
Iteration 4/1000 | Loss: 0.00006611
Iteration 5/1000 | Loss: 0.00006206
Iteration 6/1000 | Loss: 0.00006043
Iteration 7/1000 | Loss: 0.00005892
Iteration 8/1000 | Loss: 0.00005786
Iteration 9/1000 | Loss: 0.00005692
Iteration 10/1000 | Loss: 0.00005586
Iteration 11/1000 | Loss: 0.00005454
Iteration 12/1000 | Loss: 0.00005375
Iteration 13/1000 | Loss: 0.00005303
Iteration 14/1000 | Loss: 0.00005240
Iteration 15/1000 | Loss: 0.00005203
Iteration 16/1000 | Loss: 0.00005156
Iteration 17/1000 | Loss: 0.00005117
Iteration 18/1000 | Loss: 0.00005077
Iteration 19/1000 | Loss: 0.00005042
Iteration 20/1000 | Loss: 0.00005020
Iteration 21/1000 | Loss: 0.00005017
Iteration 22/1000 | Loss: 0.00004995
Iteration 23/1000 | Loss: 0.00004975
Iteration 24/1000 | Loss: 0.00004966
Iteration 25/1000 | Loss: 0.00004964
Iteration 26/1000 | Loss: 0.00004963
Iteration 27/1000 | Loss: 0.00004953
Iteration 28/1000 | Loss: 0.00004951
Iteration 29/1000 | Loss: 0.00004951
Iteration 30/1000 | Loss: 0.00004950
Iteration 31/1000 | Loss: 0.00004950
Iteration 32/1000 | Loss: 0.00004950
Iteration 33/1000 | Loss: 0.00004950
Iteration 34/1000 | Loss: 0.00004950
Iteration 35/1000 | Loss: 0.00004950
Iteration 36/1000 | Loss: 0.00004949
Iteration 37/1000 | Loss: 0.00004949
Iteration 38/1000 | Loss: 0.00004949
Iteration 39/1000 | Loss: 0.00004949
Iteration 40/1000 | Loss: 0.00004949
Iteration 41/1000 | Loss: 0.00004949
Iteration 42/1000 | Loss: 0.00004949
Iteration 43/1000 | Loss: 0.00004949
Iteration 44/1000 | Loss: 0.00004949
Iteration 45/1000 | Loss: 0.00004949
Iteration 46/1000 | Loss: 0.00004940
Iteration 47/1000 | Loss: 0.00004940
Iteration 48/1000 | Loss: 0.00004940
Iteration 49/1000 | Loss: 0.00004940
Iteration 50/1000 | Loss: 0.00004940
Iteration 51/1000 | Loss: 0.00004940
Iteration 52/1000 | Loss: 0.00004940
Iteration 53/1000 | Loss: 0.00004940
Iteration 54/1000 | Loss: 0.00004939
Iteration 55/1000 | Loss: 0.00004938
Iteration 56/1000 | Loss: 0.00004938
Iteration 57/1000 | Loss: 0.00004938
Iteration 58/1000 | Loss: 0.00004938
Iteration 59/1000 | Loss: 0.00004938
Iteration 60/1000 | Loss: 0.00004938
Iteration 61/1000 | Loss: 0.00004938
Iteration 62/1000 | Loss: 0.00004938
Iteration 63/1000 | Loss: 0.00004938
Iteration 64/1000 | Loss: 0.00004938
Iteration 65/1000 | Loss: 0.00004938
Iteration 66/1000 | Loss: 0.00004938
Iteration 67/1000 | Loss: 0.00004938
Iteration 68/1000 | Loss: 0.00004938
Iteration 69/1000 | Loss: 0.00004938
Iteration 70/1000 | Loss: 0.00004936
Iteration 71/1000 | Loss: 0.00004936
Iteration 72/1000 | Loss: 0.00004936
Iteration 73/1000 | Loss: 0.00004936
Iteration 74/1000 | Loss: 0.00004936
Iteration 75/1000 | Loss: 0.00004936
Iteration 76/1000 | Loss: 0.00004936
Iteration 77/1000 | Loss: 0.00004936
Iteration 78/1000 | Loss: 0.00004936
Iteration 79/1000 | Loss: 0.00004936
Iteration 80/1000 | Loss: 0.00004936
Iteration 81/1000 | Loss: 0.00004935
Iteration 82/1000 | Loss: 0.00004935
Iteration 83/1000 | Loss: 0.00004935
Iteration 84/1000 | Loss: 0.00004935
Iteration 85/1000 | Loss: 0.00004935
Iteration 86/1000 | Loss: 0.00004935
Iteration 87/1000 | Loss: 0.00004935
Iteration 88/1000 | Loss: 0.00004935
Iteration 89/1000 | Loss: 0.00004935
Iteration 90/1000 | Loss: 0.00004935
Iteration 91/1000 | Loss: 0.00004935
Iteration 92/1000 | Loss: 0.00004935
Iteration 93/1000 | Loss: 0.00004934
Iteration 94/1000 | Loss: 0.00004934
Iteration 95/1000 | Loss: 0.00004934
Iteration 96/1000 | Loss: 0.00004934
Iteration 97/1000 | Loss: 0.00004934
Iteration 98/1000 | Loss: 0.00004934
Iteration 99/1000 | Loss: 0.00004934
Iteration 100/1000 | Loss: 0.00004934
Iteration 101/1000 | Loss: 0.00004934
Iteration 102/1000 | Loss: 0.00004934
Iteration 103/1000 | Loss: 0.00004933
Iteration 104/1000 | Loss: 0.00004933
Iteration 105/1000 | Loss: 0.00004933
Iteration 106/1000 | Loss: 0.00004933
Iteration 107/1000 | Loss: 0.00004932
Iteration 108/1000 | Loss: 0.00004932
Iteration 109/1000 | Loss: 0.00004932
Iteration 110/1000 | Loss: 0.00004932
Iteration 111/1000 | Loss: 0.00004932
Iteration 112/1000 | Loss: 0.00004932
Iteration 113/1000 | Loss: 0.00004931
Iteration 114/1000 | Loss: 0.00004931
Iteration 115/1000 | Loss: 0.00004931
Iteration 116/1000 | Loss: 0.00004931
Iteration 117/1000 | Loss: 0.00004931
Iteration 118/1000 | Loss: 0.00004931
Iteration 119/1000 | Loss: 0.00004931
Iteration 120/1000 | Loss: 0.00004931
Iteration 121/1000 | Loss: 0.00004931
Iteration 122/1000 | Loss: 0.00004930
Iteration 123/1000 | Loss: 0.00004930
Iteration 124/1000 | Loss: 0.00004930
Iteration 125/1000 | Loss: 0.00004929
Iteration 126/1000 | Loss: 0.00004929
Iteration 127/1000 | Loss: 0.00004929
Iteration 128/1000 | Loss: 0.00004929
Iteration 129/1000 | Loss: 0.00004929
Iteration 130/1000 | Loss: 0.00004929
Iteration 131/1000 | Loss: 0.00004929
Iteration 132/1000 | Loss: 0.00004929
Iteration 133/1000 | Loss: 0.00004929
Iteration 134/1000 | Loss: 0.00004929
Iteration 135/1000 | Loss: 0.00004929
Iteration 136/1000 | Loss: 0.00004929
Iteration 137/1000 | Loss: 0.00004929
Iteration 138/1000 | Loss: 0.00004929
Iteration 139/1000 | Loss: 0.00004929
Iteration 140/1000 | Loss: 0.00004929
Iteration 141/1000 | Loss: 0.00004929
Iteration 142/1000 | Loss: 0.00004929
Iteration 143/1000 | Loss: 0.00004929
Iteration 144/1000 | Loss: 0.00004929
Iteration 145/1000 | Loss: 0.00004928
Iteration 146/1000 | Loss: 0.00004928
Iteration 147/1000 | Loss: 0.00004928
Iteration 148/1000 | Loss: 0.00004928
Iteration 149/1000 | Loss: 0.00004928
Iteration 150/1000 | Loss: 0.00004928
Iteration 151/1000 | Loss: 0.00004928
Iteration 152/1000 | Loss: 0.00004928
Iteration 153/1000 | Loss: 0.00004928
Iteration 154/1000 | Loss: 0.00004928
Iteration 155/1000 | Loss: 0.00004928
Iteration 156/1000 | Loss: 0.00004928
Iteration 157/1000 | Loss: 0.00004928
Iteration 158/1000 | Loss: 0.00004928
Iteration 159/1000 | Loss: 0.00004928
Iteration 160/1000 | Loss: 0.00004928
Iteration 161/1000 | Loss: 0.00004928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [4.927788904751651e-05, 4.927788904751651e-05, 4.927788904751651e-05, 4.927788904751651e-05, 4.927788904751651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.927788904751651e-05

Optimization complete. Final v2v error: 5.566677570343018 mm

Highest mean error: 6.295666694641113 mm for frame 14

Lowest mean error: 4.957461357116699 mm for frame 105

Saving results

Total time: 50.36639595031738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066591
Iteration 2/25 | Loss: 0.01066591
Iteration 3/25 | Loss: 0.01066591
Iteration 4/25 | Loss: 0.01066591
Iteration 5/25 | Loss: 0.01066591
Iteration 6/25 | Loss: 0.01066591
Iteration 7/25 | Loss: 0.01066591
Iteration 8/25 | Loss: 0.01066591
Iteration 9/25 | Loss: 0.01066591
Iteration 10/25 | Loss: 0.01066591
Iteration 11/25 | Loss: 0.01066591
Iteration 12/25 | Loss: 0.01066590
Iteration 13/25 | Loss: 0.01066590
Iteration 14/25 | Loss: 0.01066590
Iteration 15/25 | Loss: 0.01066590
Iteration 16/25 | Loss: 0.01066590
Iteration 17/25 | Loss: 0.01066590
Iteration 18/25 | Loss: 0.01066590
Iteration 19/25 | Loss: 0.01066590
Iteration 20/25 | Loss: 0.01066590
Iteration 21/25 | Loss: 0.01066590
Iteration 22/25 | Loss: 0.01066590
Iteration 23/25 | Loss: 0.01066590
Iteration 24/25 | Loss: 0.01066590
Iteration 25/25 | Loss: 0.01066590

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77398288
Iteration 2/25 | Loss: 0.09352580
Iteration 3/25 | Loss: 0.09336692
Iteration 4/25 | Loss: 0.09336542
Iteration 5/25 | Loss: 0.09336542
Iteration 6/25 | Loss: 0.09336542
Iteration 7/25 | Loss: 0.09336542
Iteration 8/25 | Loss: 0.09336542
Iteration 9/25 | Loss: 0.09336542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.09336541593074799, 0.09336541593074799, 0.09336541593074799, 0.09336541593074799, 0.09336541593074799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09336541593074799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09336542
Iteration 2/1000 | Loss: 0.00454348
Iteration 3/1000 | Loss: 0.00229410
Iteration 4/1000 | Loss: 0.00136132
Iteration 5/1000 | Loss: 0.00040933
Iteration 6/1000 | Loss: 0.00054674
Iteration 7/1000 | Loss: 0.00085746
Iteration 8/1000 | Loss: 0.00020369
Iteration 9/1000 | Loss: 0.00036905
Iteration 10/1000 | Loss: 0.00044911
Iteration 11/1000 | Loss: 0.00013332
Iteration 12/1000 | Loss: 0.00020281
Iteration 13/1000 | Loss: 0.00024410
Iteration 14/1000 | Loss: 0.00045982
Iteration 15/1000 | Loss: 0.00027513
Iteration 16/1000 | Loss: 0.00091291
Iteration 17/1000 | Loss: 0.00008516
Iteration 18/1000 | Loss: 0.00054239
Iteration 19/1000 | Loss: 0.00013888
Iteration 20/1000 | Loss: 0.00078333
Iteration 21/1000 | Loss: 0.00219129
Iteration 22/1000 | Loss: 0.00244610
Iteration 23/1000 | Loss: 0.00008279
Iteration 24/1000 | Loss: 0.00086172
Iteration 25/1000 | Loss: 0.00016332
Iteration 26/1000 | Loss: 0.00084737
Iteration 27/1000 | Loss: 0.00024430
Iteration 28/1000 | Loss: 0.00012306
Iteration 29/1000 | Loss: 0.00005096
Iteration 30/1000 | Loss: 0.00004583
Iteration 31/1000 | Loss: 0.00029833
Iteration 32/1000 | Loss: 0.00093453
Iteration 33/1000 | Loss: 0.00054825
Iteration 34/1000 | Loss: 0.00028946
Iteration 35/1000 | Loss: 0.00004121
Iteration 36/1000 | Loss: 0.00008140
Iteration 37/1000 | Loss: 0.00003694
Iteration 38/1000 | Loss: 0.00003455
Iteration 39/1000 | Loss: 0.00003254
Iteration 40/1000 | Loss: 0.00047386
Iteration 41/1000 | Loss: 0.00032147
Iteration 42/1000 | Loss: 0.00005227
Iteration 43/1000 | Loss: 0.00003006
Iteration 44/1000 | Loss: 0.00002986
Iteration 45/1000 | Loss: 0.00002874
Iteration 46/1000 | Loss: 0.00002803
Iteration 47/1000 | Loss: 0.00002752
Iteration 48/1000 | Loss: 0.00002716
Iteration 49/1000 | Loss: 0.00002676
Iteration 50/1000 | Loss: 0.00002645
Iteration 51/1000 | Loss: 0.00002784
Iteration 52/1000 | Loss: 0.00002661
Iteration 53/1000 | Loss: 0.00002608
Iteration 54/1000 | Loss: 0.00002606
Iteration 55/1000 | Loss: 0.00002604
Iteration 56/1000 | Loss: 0.00002603
Iteration 57/1000 | Loss: 0.00002711
Iteration 58/1000 | Loss: 0.00002663
Iteration 59/1000 | Loss: 0.00019370
Iteration 60/1000 | Loss: 0.00029567
Iteration 61/1000 | Loss: 0.00013543
Iteration 62/1000 | Loss: 0.00003134
Iteration 63/1000 | Loss: 0.00003318
Iteration 64/1000 | Loss: 0.00002820
Iteration 65/1000 | Loss: 0.00002705
Iteration 66/1000 | Loss: 0.00002720
Iteration 67/1000 | Loss: 0.00003711
Iteration 68/1000 | Loss: 0.00003434
Iteration 69/1000 | Loss: 0.00002608
Iteration 70/1000 | Loss: 0.00003331
Iteration 71/1000 | Loss: 0.00009847
Iteration 72/1000 | Loss: 0.00004259
Iteration 73/1000 | Loss: 0.00003121
Iteration 74/1000 | Loss: 0.00002822
Iteration 75/1000 | Loss: 0.00002490
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002496
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002376
Iteration 80/1000 | Loss: 0.00002422
Iteration 81/1000 | Loss: 0.00002367
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00002392
Iteration 85/1000 | Loss: 0.00002343
Iteration 86/1000 | Loss: 0.00002318
Iteration 87/1000 | Loss: 0.00002316
Iteration 88/1000 | Loss: 0.00002315
Iteration 89/1000 | Loss: 0.00002314
Iteration 90/1000 | Loss: 0.00002312
Iteration 91/1000 | Loss: 0.00002373
Iteration 92/1000 | Loss: 0.00002373
Iteration 93/1000 | Loss: 0.00002340
Iteration 94/1000 | Loss: 0.00002339
Iteration 95/1000 | Loss: 0.00002313
Iteration 96/1000 | Loss: 0.00002366
Iteration 97/1000 | Loss: 0.00002636
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002462
Iteration 100/1000 | Loss: 0.00002304
Iteration 101/1000 | Loss: 0.00002304
Iteration 102/1000 | Loss: 0.00002304
Iteration 103/1000 | Loss: 0.00002304
Iteration 104/1000 | Loss: 0.00002304
Iteration 105/1000 | Loss: 0.00002306
Iteration 106/1000 | Loss: 0.00002363
Iteration 107/1000 | Loss: 0.00002363
Iteration 108/1000 | Loss: 0.00002304
Iteration 109/1000 | Loss: 0.00002304
Iteration 110/1000 | Loss: 0.00002304
Iteration 111/1000 | Loss: 0.00002304
Iteration 112/1000 | Loss: 0.00002304
Iteration 113/1000 | Loss: 0.00002304
Iteration 114/1000 | Loss: 0.00002304
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002361
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002303
Iteration 119/1000 | Loss: 0.00002303
Iteration 120/1000 | Loss: 0.00002303
Iteration 121/1000 | Loss: 0.00002302
Iteration 122/1000 | Loss: 0.00002302
Iteration 123/1000 | Loss: 0.00002361
Iteration 124/1000 | Loss: 0.00002361
Iteration 125/1000 | Loss: 0.00002605
Iteration 126/1000 | Loss: 0.00002302
Iteration 127/1000 | Loss: 0.00002302
Iteration 128/1000 | Loss: 0.00002302
Iteration 129/1000 | Loss: 0.00002302
Iteration 130/1000 | Loss: 0.00002302
Iteration 131/1000 | Loss: 0.00002302
Iteration 132/1000 | Loss: 0.00002302
Iteration 133/1000 | Loss: 0.00002302
Iteration 134/1000 | Loss: 0.00002302
Iteration 135/1000 | Loss: 0.00002302
Iteration 136/1000 | Loss: 0.00002302
Iteration 137/1000 | Loss: 0.00002302
Iteration 138/1000 | Loss: 0.00002302
Iteration 139/1000 | Loss: 0.00002302
Iteration 140/1000 | Loss: 0.00002302
Iteration 141/1000 | Loss: 0.00002302
Iteration 142/1000 | Loss: 0.00002302
Iteration 143/1000 | Loss: 0.00002302
Iteration 144/1000 | Loss: 0.00002302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.301558924955316e-05, 2.301558924955316e-05, 2.301558924955316e-05, 2.301558924955316e-05, 2.301558924955316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.301558924955316e-05

Optimization complete. Final v2v error: 3.123386859893799 mm

Highest mean error: 20.749813079833984 mm for frame 220

Lowest mean error: 2.523890495300293 mm for frame 123

Saving results

Total time: 157.24693417549133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425773
Iteration 2/25 | Loss: 0.00130614
Iteration 3/25 | Loss: 0.00116359
Iteration 4/25 | Loss: 0.00114706
Iteration 5/25 | Loss: 0.00114123
Iteration 6/25 | Loss: 0.00113919
Iteration 7/25 | Loss: 0.00113870
Iteration 8/25 | Loss: 0.00113870
Iteration 9/25 | Loss: 0.00113870
Iteration 10/25 | Loss: 0.00113870
Iteration 11/25 | Loss: 0.00113870
Iteration 12/25 | Loss: 0.00113870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011386963305994868, 0.0011386963305994868, 0.0011386963305994868, 0.0011386963305994868, 0.0011386963305994868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011386963305994868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33432829
Iteration 2/25 | Loss: 0.00118307
Iteration 3/25 | Loss: 0.00118307
Iteration 4/25 | Loss: 0.00118307
Iteration 5/25 | Loss: 0.00118307
Iteration 6/25 | Loss: 0.00118307
Iteration 7/25 | Loss: 0.00118307
Iteration 8/25 | Loss: 0.00118307
Iteration 9/25 | Loss: 0.00118307
Iteration 10/25 | Loss: 0.00118307
Iteration 11/25 | Loss: 0.00118307
Iteration 12/25 | Loss: 0.00118307
Iteration 13/25 | Loss: 0.00118307
Iteration 14/25 | Loss: 0.00118307
Iteration 15/25 | Loss: 0.00118307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011830662842839956, 0.0011830662842839956, 0.0011830662842839956, 0.0011830662842839956, 0.0011830662842839956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011830662842839956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118307
Iteration 2/1000 | Loss: 0.00004805
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001614
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001355
Iteration 12/1000 | Loss: 0.00001353
Iteration 13/1000 | Loss: 0.00001350
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001339
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001328
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001313
Iteration 26/1000 | Loss: 0.00001312
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001308
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001307
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001306
Iteration 43/1000 | Loss: 0.00001306
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001304
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001303
Iteration 58/1000 | Loss: 0.00001303
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001301
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001296
Iteration 112/1000 | Loss: 0.00001296
Iteration 113/1000 | Loss: 0.00001296
Iteration 114/1000 | Loss: 0.00001296
Iteration 115/1000 | Loss: 0.00001296
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001295
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001291
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001290
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001290
Iteration 137/1000 | Loss: 0.00001290
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001287
Iteration 145/1000 | Loss: 0.00001287
Iteration 146/1000 | Loss: 0.00001287
Iteration 147/1000 | Loss: 0.00001287
Iteration 148/1000 | Loss: 0.00001286
Iteration 149/1000 | Loss: 0.00001286
Iteration 150/1000 | Loss: 0.00001286
Iteration 151/1000 | Loss: 0.00001286
Iteration 152/1000 | Loss: 0.00001286
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001285
Iteration 158/1000 | Loss: 0.00001285
Iteration 159/1000 | Loss: 0.00001285
Iteration 160/1000 | Loss: 0.00001285
Iteration 161/1000 | Loss: 0.00001285
Iteration 162/1000 | Loss: 0.00001285
Iteration 163/1000 | Loss: 0.00001285
Iteration 164/1000 | Loss: 0.00001285
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001285
Iteration 168/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2853793123213109e-05, 1.2853793123213109e-05, 1.2853793123213109e-05, 1.2853793123213109e-05, 1.2853793123213109e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2853793123213109e-05

Optimization complete. Final v2v error: 2.986755609512329 mm

Highest mean error: 3.759244441986084 mm for frame 28

Lowest mean error: 2.636538028717041 mm for frame 91

Saving results

Total time: 39.56116056442261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108644
Iteration 2/25 | Loss: 0.01108644
Iteration 3/25 | Loss: 0.01108644
Iteration 4/25 | Loss: 0.01108644
Iteration 5/25 | Loss: 0.00388023
Iteration 6/25 | Loss: 0.00254418
Iteration 7/25 | Loss: 0.00224076
Iteration 8/25 | Loss: 0.00205481
Iteration 9/25 | Loss: 0.00194496
Iteration 10/25 | Loss: 0.00186092
Iteration 11/25 | Loss: 0.00175621
Iteration 12/25 | Loss: 0.00174191
Iteration 13/25 | Loss: 0.00163694
Iteration 14/25 | Loss: 0.00159471
Iteration 15/25 | Loss: 0.00154396
Iteration 16/25 | Loss: 0.00153112
Iteration 17/25 | Loss: 0.00153715
Iteration 18/25 | Loss: 0.00152690
Iteration 19/25 | Loss: 0.00153144
Iteration 20/25 | Loss: 0.00152330
Iteration 21/25 | Loss: 0.00151752
Iteration 22/25 | Loss: 0.00151624
Iteration 23/25 | Loss: 0.00152158
Iteration 24/25 | Loss: 0.00150067
Iteration 25/25 | Loss: 0.00149686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32309520
Iteration 2/25 | Loss: 0.00348388
Iteration 3/25 | Loss: 0.00348388
Iteration 4/25 | Loss: 0.00348388
Iteration 5/25 | Loss: 0.00348388
Iteration 6/25 | Loss: 0.00348388
Iteration 7/25 | Loss: 0.00348388
Iteration 8/25 | Loss: 0.00348388
Iteration 9/25 | Loss: 0.00348388
Iteration 10/25 | Loss: 0.00348388
Iteration 11/25 | Loss: 0.00348388
Iteration 12/25 | Loss: 0.00348388
Iteration 13/25 | Loss: 0.00348388
Iteration 14/25 | Loss: 0.00348388
Iteration 15/25 | Loss: 0.00348388
Iteration 16/25 | Loss: 0.00348388
Iteration 17/25 | Loss: 0.00348388
Iteration 18/25 | Loss: 0.00348388
Iteration 19/25 | Loss: 0.00348388
Iteration 20/25 | Loss: 0.00348388
Iteration 21/25 | Loss: 0.00348388
Iteration 22/25 | Loss: 0.00348388
Iteration 23/25 | Loss: 0.00348388
Iteration 24/25 | Loss: 0.00348388
Iteration 25/25 | Loss: 0.00348388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348388
Iteration 2/1000 | Loss: 0.00069243
Iteration 3/1000 | Loss: 0.00083628
Iteration 4/1000 | Loss: 0.00143066
Iteration 5/1000 | Loss: 0.00041767
Iteration 6/1000 | Loss: 0.00108450
Iteration 7/1000 | Loss: 0.00047689
Iteration 8/1000 | Loss: 0.00058580
Iteration 9/1000 | Loss: 0.00027315
Iteration 10/1000 | Loss: 0.00150123
Iteration 11/1000 | Loss: 0.00183604
Iteration 12/1000 | Loss: 0.00025374
Iteration 13/1000 | Loss: 0.00149996
Iteration 14/1000 | Loss: 0.00020776
Iteration 15/1000 | Loss: 0.00028913
Iteration 16/1000 | Loss: 0.00028619
Iteration 17/1000 | Loss: 0.00031309
Iteration 18/1000 | Loss: 0.00025469
Iteration 19/1000 | Loss: 0.00026317
Iteration 20/1000 | Loss: 0.00028990
Iteration 21/1000 | Loss: 0.00028321
Iteration 22/1000 | Loss: 0.00022413
Iteration 23/1000 | Loss: 0.00033416
Iteration 24/1000 | Loss: 0.00026836
Iteration 25/1000 | Loss: 0.00043615
Iteration 26/1000 | Loss: 0.00154443
Iteration 27/1000 | Loss: 0.00063291
Iteration 28/1000 | Loss: 0.00078616
Iteration 29/1000 | Loss: 0.00053019
Iteration 30/1000 | Loss: 0.00415994
Iteration 31/1000 | Loss: 0.00109313
Iteration 32/1000 | Loss: 0.00165184
Iteration 33/1000 | Loss: 0.00099459
Iteration 34/1000 | Loss: 0.00099454
Iteration 35/1000 | Loss: 0.00158488
Iteration 36/1000 | Loss: 0.00063062
Iteration 37/1000 | Loss: 0.00149928
Iteration 38/1000 | Loss: 0.00075932
Iteration 39/1000 | Loss: 0.00116102
Iteration 40/1000 | Loss: 0.00072890
Iteration 41/1000 | Loss: 0.00032630
Iteration 42/1000 | Loss: 0.00028954
Iteration 43/1000 | Loss: 0.00133970
Iteration 44/1000 | Loss: 0.00248977
Iteration 45/1000 | Loss: 0.00182133
Iteration 46/1000 | Loss: 0.00168839
Iteration 47/1000 | Loss: 0.00123219
Iteration 48/1000 | Loss: 0.00175492
Iteration 49/1000 | Loss: 0.00181885
Iteration 50/1000 | Loss: 0.00099831
Iteration 51/1000 | Loss: 0.00129481
Iteration 52/1000 | Loss: 0.00116680
Iteration 53/1000 | Loss: 0.00183028
Iteration 54/1000 | Loss: 0.00043159
Iteration 55/1000 | Loss: 0.00036285
Iteration 56/1000 | Loss: 0.00094523
Iteration 57/1000 | Loss: 0.00042085
Iteration 58/1000 | Loss: 0.00036362
Iteration 59/1000 | Loss: 0.00056276
Iteration 60/1000 | Loss: 0.00062280
Iteration 61/1000 | Loss: 0.00039101
Iteration 62/1000 | Loss: 0.00054008
Iteration 63/1000 | Loss: 0.00044051
Iteration 64/1000 | Loss: 0.00016167
Iteration 65/1000 | Loss: 0.00018379
Iteration 66/1000 | Loss: 0.00020923
Iteration 67/1000 | Loss: 0.00050726
Iteration 68/1000 | Loss: 0.00044948
Iteration 69/1000 | Loss: 0.00040202
Iteration 70/1000 | Loss: 0.00103326
Iteration 71/1000 | Loss: 0.00057658
Iteration 72/1000 | Loss: 0.00041010
Iteration 73/1000 | Loss: 0.00051120
Iteration 74/1000 | Loss: 0.00045440
Iteration 75/1000 | Loss: 0.00031970
Iteration 76/1000 | Loss: 0.00024201
Iteration 77/1000 | Loss: 0.00048677
Iteration 78/1000 | Loss: 0.00023153
Iteration 79/1000 | Loss: 0.00055335
Iteration 80/1000 | Loss: 0.00064332
Iteration 81/1000 | Loss: 0.00078681
Iteration 82/1000 | Loss: 0.00035653
Iteration 83/1000 | Loss: 0.00024718
Iteration 84/1000 | Loss: 0.00049951
Iteration 85/1000 | Loss: 0.00139618
Iteration 86/1000 | Loss: 0.00150502
Iteration 87/1000 | Loss: 0.00105501
Iteration 88/1000 | Loss: 0.00074007
Iteration 89/1000 | Loss: 0.00040709
Iteration 90/1000 | Loss: 0.00070574
Iteration 91/1000 | Loss: 0.00196514
Iteration 92/1000 | Loss: 0.00076418
Iteration 93/1000 | Loss: 0.00061876
Iteration 94/1000 | Loss: 0.00146172
Iteration 95/1000 | Loss: 0.00043161
Iteration 96/1000 | Loss: 0.00103107
Iteration 97/1000 | Loss: 0.00070742
Iteration 98/1000 | Loss: 0.00048136
Iteration 99/1000 | Loss: 0.00112471
Iteration 100/1000 | Loss: 0.00065803
Iteration 101/1000 | Loss: 0.00069451
Iteration 102/1000 | Loss: 0.00053965
Iteration 103/1000 | Loss: 0.00043446
Iteration 104/1000 | Loss: 0.00021769
Iteration 105/1000 | Loss: 0.00037160
Iteration 106/1000 | Loss: 0.00048770
Iteration 107/1000 | Loss: 0.00036886
Iteration 108/1000 | Loss: 0.00035732
Iteration 109/1000 | Loss: 0.00040211
Iteration 110/1000 | Loss: 0.00056803
Iteration 111/1000 | Loss: 0.00034490
Iteration 112/1000 | Loss: 0.00129217
Iteration 113/1000 | Loss: 0.00061103
Iteration 114/1000 | Loss: 0.00013895
Iteration 115/1000 | Loss: 0.00014996
Iteration 116/1000 | Loss: 0.00017083
Iteration 117/1000 | Loss: 0.00090283
Iteration 118/1000 | Loss: 0.00071178
Iteration 119/1000 | Loss: 0.00093657
Iteration 120/1000 | Loss: 0.00072127
Iteration 121/1000 | Loss: 0.00056915
Iteration 122/1000 | Loss: 0.00022975
Iteration 123/1000 | Loss: 0.00019681
Iteration 124/1000 | Loss: 0.00045424
Iteration 125/1000 | Loss: 0.00031440
Iteration 126/1000 | Loss: 0.00049224
Iteration 127/1000 | Loss: 0.00043089
Iteration 128/1000 | Loss: 0.00042227
Iteration 129/1000 | Loss: 0.00041497
Iteration 130/1000 | Loss: 0.00048745
Iteration 131/1000 | Loss: 0.00038786
Iteration 132/1000 | Loss: 0.00036699
Iteration 133/1000 | Loss: 0.00074234
Iteration 134/1000 | Loss: 0.00052151
Iteration 135/1000 | Loss: 0.00025040
Iteration 136/1000 | Loss: 0.00015434
Iteration 137/1000 | Loss: 0.00022854
Iteration 138/1000 | Loss: 0.00016374
Iteration 139/1000 | Loss: 0.00021948
Iteration 140/1000 | Loss: 0.00016449
Iteration 141/1000 | Loss: 0.00023813
Iteration 142/1000 | Loss: 0.00029364
Iteration 143/1000 | Loss: 0.00028376
Iteration 144/1000 | Loss: 0.00035224
Iteration 145/1000 | Loss: 0.00022921
Iteration 146/1000 | Loss: 0.00024957
Iteration 147/1000 | Loss: 0.00007491
Iteration 148/1000 | Loss: 0.00012729
Iteration 149/1000 | Loss: 0.00007633
Iteration 150/1000 | Loss: 0.00037855
Iteration 151/1000 | Loss: 0.00036750
Iteration 152/1000 | Loss: 0.00021908
Iteration 153/1000 | Loss: 0.00032579
Iteration 154/1000 | Loss: 0.00046543
Iteration 155/1000 | Loss: 0.00078348
Iteration 156/1000 | Loss: 0.00039589
Iteration 157/1000 | Loss: 0.00031071
Iteration 158/1000 | Loss: 0.00015486
Iteration 159/1000 | Loss: 0.00026505
Iteration 160/1000 | Loss: 0.00015785
Iteration 161/1000 | Loss: 0.00072927
Iteration 162/1000 | Loss: 0.00046063
Iteration 163/1000 | Loss: 0.00043243
Iteration 164/1000 | Loss: 0.00027954
Iteration 165/1000 | Loss: 0.00050030
Iteration 166/1000 | Loss: 0.00026234
Iteration 167/1000 | Loss: 0.00019247
Iteration 168/1000 | Loss: 0.00030464
Iteration 169/1000 | Loss: 0.00045526
Iteration 170/1000 | Loss: 0.00037676
Iteration 171/1000 | Loss: 0.00048161
Iteration 172/1000 | Loss: 0.00019914
Iteration 173/1000 | Loss: 0.00043983
Iteration 174/1000 | Loss: 0.00030073
Iteration 175/1000 | Loss: 0.00026513
Iteration 176/1000 | Loss: 0.00043310
Iteration 177/1000 | Loss: 0.00121437
Iteration 178/1000 | Loss: 0.00040472
Iteration 179/1000 | Loss: 0.00007338
Iteration 180/1000 | Loss: 0.00012378
Iteration 181/1000 | Loss: 0.00070280
Iteration 182/1000 | Loss: 0.00055073
Iteration 183/1000 | Loss: 0.00034902
Iteration 184/1000 | Loss: 0.00009719
Iteration 185/1000 | Loss: 0.00034605
Iteration 186/1000 | Loss: 0.00037844
Iteration 187/1000 | Loss: 0.00027070
Iteration 188/1000 | Loss: 0.00024295
Iteration 189/1000 | Loss: 0.00019422
Iteration 190/1000 | Loss: 0.00038298
Iteration 191/1000 | Loss: 0.00028579
Iteration 192/1000 | Loss: 0.00038523
Iteration 193/1000 | Loss: 0.00022708
Iteration 194/1000 | Loss: 0.00041100
Iteration 195/1000 | Loss: 0.00008965
Iteration 196/1000 | Loss: 0.00009436
Iteration 197/1000 | Loss: 0.00007295
Iteration 198/1000 | Loss: 0.00027674
Iteration 199/1000 | Loss: 0.00011554
Iteration 200/1000 | Loss: 0.00055263
Iteration 201/1000 | Loss: 0.00053070
Iteration 202/1000 | Loss: 0.00016353
Iteration 203/1000 | Loss: 0.00010335
Iteration 204/1000 | Loss: 0.00025967
Iteration 205/1000 | Loss: 0.00027620
Iteration 206/1000 | Loss: 0.00070018
Iteration 207/1000 | Loss: 0.00040649
Iteration 208/1000 | Loss: 0.00058210
Iteration 209/1000 | Loss: 0.00043088
Iteration 210/1000 | Loss: 0.00068248
Iteration 211/1000 | Loss: 0.00047674
Iteration 212/1000 | Loss: 0.00055394
Iteration 213/1000 | Loss: 0.00093238
Iteration 214/1000 | Loss: 0.00058586
Iteration 215/1000 | Loss: 0.00093488
Iteration 216/1000 | Loss: 0.00062025
Iteration 217/1000 | Loss: 0.00046972
Iteration 218/1000 | Loss: 0.00039494
Iteration 219/1000 | Loss: 0.00008476
Iteration 220/1000 | Loss: 0.00007719
Iteration 221/1000 | Loss: 0.00007476
Iteration 222/1000 | Loss: 0.00007145
Iteration 223/1000 | Loss: 0.00006617
Iteration 224/1000 | Loss: 0.00006978
Iteration 225/1000 | Loss: 0.00007364
Iteration 226/1000 | Loss: 0.00007146
Iteration 227/1000 | Loss: 0.00007382
Iteration 228/1000 | Loss: 0.00008848
Iteration 229/1000 | Loss: 0.00008069
Iteration 230/1000 | Loss: 0.00007870
Iteration 231/1000 | Loss: 0.00007035
Iteration 232/1000 | Loss: 0.00009208
Iteration 233/1000 | Loss: 0.00006783
Iteration 234/1000 | Loss: 0.00007352
Iteration 235/1000 | Loss: 0.00008980
Iteration 236/1000 | Loss: 0.00007208
Iteration 237/1000 | Loss: 0.00006868
Iteration 238/1000 | Loss: 0.00007451
Iteration 239/1000 | Loss: 0.00007011
Iteration 240/1000 | Loss: 0.00007657
Iteration 241/1000 | Loss: 0.00006765
Iteration 242/1000 | Loss: 0.00007665
Iteration 243/1000 | Loss: 0.00006815
Iteration 244/1000 | Loss: 0.00007625
Iteration 245/1000 | Loss: 0.00007024
Iteration 246/1000 | Loss: 0.00007547
Iteration 247/1000 | Loss: 0.00006807
Iteration 248/1000 | Loss: 0.00007442
Iteration 249/1000 | Loss: 0.00006837
Iteration 250/1000 | Loss: 0.00007414
Iteration 251/1000 | Loss: 0.00007442
Iteration 252/1000 | Loss: 0.00007299
Iteration 253/1000 | Loss: 0.00006757
Iteration 254/1000 | Loss: 0.00007292
Iteration 255/1000 | Loss: 0.00007109
Iteration 256/1000 | Loss: 0.00008026
Iteration 257/1000 | Loss: 0.00007667
Iteration 258/1000 | Loss: 0.00007968
Iteration 259/1000 | Loss: 0.00006872
Iteration 260/1000 | Loss: 0.00006744
Iteration 261/1000 | Loss: 0.00007555
Iteration 262/1000 | Loss: 0.00007588
Iteration 263/1000 | Loss: 0.00007535
Iteration 264/1000 | Loss: 0.00007636
Iteration 265/1000 | Loss: 0.00070453
Iteration 266/1000 | Loss: 0.00018429
Iteration 267/1000 | Loss: 0.00006428
Iteration 268/1000 | Loss: 0.00006201
Iteration 269/1000 | Loss: 0.00006109
Iteration 270/1000 | Loss: 0.00006058
Iteration 271/1000 | Loss: 0.00065535
Iteration 272/1000 | Loss: 0.00015655
Iteration 273/1000 | Loss: 0.00006959
Iteration 274/1000 | Loss: 0.00006038
Iteration 275/1000 | Loss: 0.00006011
Iteration 276/1000 | Loss: 0.00006010
Iteration 277/1000 | Loss: 0.00006008
Iteration 278/1000 | Loss: 0.00006007
Iteration 279/1000 | Loss: 0.00006007
Iteration 280/1000 | Loss: 0.00006007
Iteration 281/1000 | Loss: 0.00006007
Iteration 282/1000 | Loss: 0.00006007
Iteration 283/1000 | Loss: 0.00006006
Iteration 284/1000 | Loss: 0.00064921
Iteration 285/1000 | Loss: 0.00016626
Iteration 286/1000 | Loss: 0.00006160
Iteration 287/1000 | Loss: 0.00006015
Iteration 288/1000 | Loss: 0.00005999
Iteration 289/1000 | Loss: 0.00005998
Iteration 290/1000 | Loss: 0.00005997
Iteration 291/1000 | Loss: 0.00005997
Iteration 292/1000 | Loss: 0.00005997
Iteration 293/1000 | Loss: 0.00005996
Iteration 294/1000 | Loss: 0.00005996
Iteration 295/1000 | Loss: 0.00005995
Iteration 296/1000 | Loss: 0.00005995
Iteration 297/1000 | Loss: 0.00005992
Iteration 298/1000 | Loss: 0.00005992
Iteration 299/1000 | Loss: 0.00005992
Iteration 300/1000 | Loss: 0.00005991
Iteration 301/1000 | Loss: 0.00005991
Iteration 302/1000 | Loss: 0.00005990
Iteration 303/1000 | Loss: 0.00005989
Iteration 304/1000 | Loss: 0.00005988
Iteration 305/1000 | Loss: 0.00060950
Iteration 306/1000 | Loss: 0.00015842
Iteration 307/1000 | Loss: 0.00006393
Iteration 308/1000 | Loss: 0.00049521
Iteration 309/1000 | Loss: 0.00011572
Iteration 310/1000 | Loss: 0.00022364
Iteration 311/1000 | Loss: 0.00012704
Iteration 312/1000 | Loss: 0.00006388
Iteration 313/1000 | Loss: 0.00006169
Iteration 314/1000 | Loss: 0.00038027
Iteration 315/1000 | Loss: 0.00015755
Iteration 316/1000 | Loss: 0.00006807
Iteration 317/1000 | Loss: 0.00028585
Iteration 318/1000 | Loss: 0.00014756
Iteration 319/1000 | Loss: 0.00012419
Iteration 320/1000 | Loss: 0.00010691
Iteration 321/1000 | Loss: 0.00017025
Iteration 322/1000 | Loss: 0.00007384
Iteration 323/1000 | Loss: 0.00012740
Iteration 324/1000 | Loss: 0.00010485
Iteration 325/1000 | Loss: 0.00006278
Iteration 326/1000 | Loss: 0.00006115
Iteration 327/1000 | Loss: 0.00006055
Iteration 328/1000 | Loss: 0.00027797
Iteration 329/1000 | Loss: 0.00006271
Iteration 330/1000 | Loss: 0.00016537
Iteration 331/1000 | Loss: 0.00006855
Iteration 332/1000 | Loss: 0.00005854
Iteration 333/1000 | Loss: 0.00005814
Iteration 334/1000 | Loss: 0.00005787
Iteration 335/1000 | Loss: 0.00005771
Iteration 336/1000 | Loss: 0.00005769
Iteration 337/1000 | Loss: 0.00005763
Iteration 338/1000 | Loss: 0.00005759
Iteration 339/1000 | Loss: 0.00005748
Iteration 340/1000 | Loss: 0.00005742
Iteration 341/1000 | Loss: 0.00005742
Iteration 342/1000 | Loss: 0.00005733
Iteration 343/1000 | Loss: 0.00005733
Iteration 344/1000 | Loss: 0.00005733
Iteration 345/1000 | Loss: 0.00005733
Iteration 346/1000 | Loss: 0.00005732
Iteration 347/1000 | Loss: 0.00005732
Iteration 348/1000 | Loss: 0.00005732
Iteration 349/1000 | Loss: 0.00005732
Iteration 350/1000 | Loss: 0.00005732
Iteration 351/1000 | Loss: 0.00005732
Iteration 352/1000 | Loss: 0.00005731
Iteration 353/1000 | Loss: 0.00005730
Iteration 354/1000 | Loss: 0.00005730
Iteration 355/1000 | Loss: 0.00005730
Iteration 356/1000 | Loss: 0.00005730
Iteration 357/1000 | Loss: 0.00005730
Iteration 358/1000 | Loss: 0.00005730
Iteration 359/1000 | Loss: 0.00005730
Iteration 360/1000 | Loss: 0.00005730
Iteration 361/1000 | Loss: 0.00005730
Iteration 362/1000 | Loss: 0.00005730
Iteration 363/1000 | Loss: 0.00005730
Iteration 364/1000 | Loss: 0.00005729
Iteration 365/1000 | Loss: 0.00005729
Iteration 366/1000 | Loss: 0.00005729
Iteration 367/1000 | Loss: 0.00005729
Iteration 368/1000 | Loss: 0.00005729
Iteration 369/1000 | Loss: 0.00005729
Iteration 370/1000 | Loss: 0.00005729
Iteration 371/1000 | Loss: 0.00005729
Iteration 372/1000 | Loss: 0.00005729
Iteration 373/1000 | Loss: 0.00005729
Iteration 374/1000 | Loss: 0.00005728
Iteration 375/1000 | Loss: 0.00005728
Iteration 376/1000 | Loss: 0.00005728
Iteration 377/1000 | Loss: 0.00005728
Iteration 378/1000 | Loss: 0.00005728
Iteration 379/1000 | Loss: 0.00005728
Iteration 380/1000 | Loss: 0.00005728
Iteration 381/1000 | Loss: 0.00005728
Iteration 382/1000 | Loss: 0.00005728
Iteration 383/1000 | Loss: 0.00005728
Iteration 384/1000 | Loss: 0.00005728
Iteration 385/1000 | Loss: 0.00005728
Iteration 386/1000 | Loss: 0.00005728
Iteration 387/1000 | Loss: 0.00005728
Iteration 388/1000 | Loss: 0.00005727
Iteration 389/1000 | Loss: 0.00005727
Iteration 390/1000 | Loss: 0.00005727
Iteration 391/1000 | Loss: 0.00005727
Iteration 392/1000 | Loss: 0.00005727
Iteration 393/1000 | Loss: 0.00005727
Iteration 394/1000 | Loss: 0.00005727
Iteration 395/1000 | Loss: 0.00005727
Iteration 396/1000 | Loss: 0.00005727
Iteration 397/1000 | Loss: 0.00005727
Iteration 398/1000 | Loss: 0.00005727
Iteration 399/1000 | Loss: 0.00005727
Iteration 400/1000 | Loss: 0.00005727
Iteration 401/1000 | Loss: 0.00005727
Iteration 402/1000 | Loss: 0.00005727
Iteration 403/1000 | Loss: 0.00005726
Iteration 404/1000 | Loss: 0.00005726
Iteration 405/1000 | Loss: 0.00005726
Iteration 406/1000 | Loss: 0.00005726
Iteration 407/1000 | Loss: 0.00005726
Iteration 408/1000 | Loss: 0.00005726
Iteration 409/1000 | Loss: 0.00005726
Iteration 410/1000 | Loss: 0.00005726
Iteration 411/1000 | Loss: 0.00005725
Iteration 412/1000 | Loss: 0.00005725
Iteration 413/1000 | Loss: 0.00005725
Iteration 414/1000 | Loss: 0.00005725
Iteration 415/1000 | Loss: 0.00005725
Iteration 416/1000 | Loss: 0.00005725
Iteration 417/1000 | Loss: 0.00005725
Iteration 418/1000 | Loss: 0.00005725
Iteration 419/1000 | Loss: 0.00005725
Iteration 420/1000 | Loss: 0.00005725
Iteration 421/1000 | Loss: 0.00005725
Iteration 422/1000 | Loss: 0.00005724
Iteration 423/1000 | Loss: 0.00005724
Iteration 424/1000 | Loss: 0.00005724
Iteration 425/1000 | Loss: 0.00005724
Iteration 426/1000 | Loss: 0.00005724
Iteration 427/1000 | Loss: 0.00005724
Iteration 428/1000 | Loss: 0.00005724
Iteration 429/1000 | Loss: 0.00005724
Iteration 430/1000 | Loss: 0.00005724
Iteration 431/1000 | Loss: 0.00005724
Iteration 432/1000 | Loss: 0.00005724
Iteration 433/1000 | Loss: 0.00005724
Iteration 434/1000 | Loss: 0.00005724
Iteration 435/1000 | Loss: 0.00005724
Iteration 436/1000 | Loss: 0.00005724
Iteration 437/1000 | Loss: 0.00005724
Iteration 438/1000 | Loss: 0.00005724
Iteration 439/1000 | Loss: 0.00005724
Iteration 440/1000 | Loss: 0.00005724
Iteration 441/1000 | Loss: 0.00005724
Iteration 442/1000 | Loss: 0.00005724
Iteration 443/1000 | Loss: 0.00005724
Iteration 444/1000 | Loss: 0.00005724
Iteration 445/1000 | Loss: 0.00005724
Iteration 446/1000 | Loss: 0.00005724
Iteration 447/1000 | Loss: 0.00005724
Iteration 448/1000 | Loss: 0.00005724
Iteration 449/1000 | Loss: 0.00005724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 449. Stopping optimization.
Last 5 losses: [5.723555659642443e-05, 5.723555659642443e-05, 5.723555659642443e-05, 5.723555659642443e-05, 5.723555659642443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.723555659642443e-05

Optimization complete. Final v2v error: 3.911940336227417 mm

Highest mean error: 12.705362319946289 mm for frame 57

Lowest mean error: 2.5677692890167236 mm for frame 127

Saving results

Total time: 555.4287509918213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455482
Iteration 2/25 | Loss: 0.00131626
Iteration 3/25 | Loss: 0.00116244
Iteration 4/25 | Loss: 0.00114907
Iteration 5/25 | Loss: 0.00114634
Iteration 6/25 | Loss: 0.00114609
Iteration 7/25 | Loss: 0.00114609
Iteration 8/25 | Loss: 0.00114609
Iteration 9/25 | Loss: 0.00114609
Iteration 10/25 | Loss: 0.00114609
Iteration 11/25 | Loss: 0.00114609
Iteration 12/25 | Loss: 0.00114609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011460937093943357, 0.0011460937093943357, 0.0011460937093943357, 0.0011460937093943357, 0.0011460937093943357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011460937093943357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.71741676
Iteration 2/25 | Loss: 0.00075466
Iteration 3/25 | Loss: 0.00075461
Iteration 4/25 | Loss: 0.00075461
Iteration 5/25 | Loss: 0.00075461
Iteration 6/25 | Loss: 0.00075461
Iteration 7/25 | Loss: 0.00075461
Iteration 8/25 | Loss: 0.00075461
Iteration 9/25 | Loss: 0.00075461
Iteration 10/25 | Loss: 0.00075461
Iteration 11/25 | Loss: 0.00075461
Iteration 12/25 | Loss: 0.00075461
Iteration 13/25 | Loss: 0.00075461
Iteration 14/25 | Loss: 0.00075461
Iteration 15/25 | Loss: 0.00075461
Iteration 16/25 | Loss: 0.00075461
Iteration 17/25 | Loss: 0.00075461
Iteration 18/25 | Loss: 0.00075461
Iteration 19/25 | Loss: 0.00075461
Iteration 20/25 | Loss: 0.00075461
Iteration 21/25 | Loss: 0.00075461
Iteration 22/25 | Loss: 0.00075461
Iteration 23/25 | Loss: 0.00075461
Iteration 24/25 | Loss: 0.00075461
Iteration 25/25 | Loss: 0.00075461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075461
Iteration 2/1000 | Loss: 0.00003090
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001392
Iteration 7/1000 | Loss: 0.00001351
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001264
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001254
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001253
Iteration 51/1000 | Loss: 0.00001253
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001249
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001242
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001240
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001239
Iteration 107/1000 | Loss: 0.00001239
Iteration 108/1000 | Loss: 0.00001239
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001235
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001234
Iteration 130/1000 | Loss: 0.00001234
Iteration 131/1000 | Loss: 0.00001234
Iteration 132/1000 | Loss: 0.00001234
Iteration 133/1000 | Loss: 0.00001234
Iteration 134/1000 | Loss: 0.00001234
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001234
Iteration 137/1000 | Loss: 0.00001234
Iteration 138/1000 | Loss: 0.00001234
Iteration 139/1000 | Loss: 0.00001234
Iteration 140/1000 | Loss: 0.00001234
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001232
Iteration 148/1000 | Loss: 0.00001232
Iteration 149/1000 | Loss: 0.00001232
Iteration 150/1000 | Loss: 0.00001232
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001231
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Iteration 158/1000 | Loss: 0.00001231
Iteration 159/1000 | Loss: 0.00001231
Iteration 160/1000 | Loss: 0.00001231
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001231
Iteration 163/1000 | Loss: 0.00001231
Iteration 164/1000 | Loss: 0.00001231
Iteration 165/1000 | Loss: 0.00001231
Iteration 166/1000 | Loss: 0.00001231
Iteration 167/1000 | Loss: 0.00001231
Iteration 168/1000 | Loss: 0.00001231
Iteration 169/1000 | Loss: 0.00001231
Iteration 170/1000 | Loss: 0.00001231
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Iteration 175/1000 | Loss: 0.00001231
Iteration 176/1000 | Loss: 0.00001231
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001231
Iteration 179/1000 | Loss: 0.00001231
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001231
Iteration 182/1000 | Loss: 0.00001231
Iteration 183/1000 | Loss: 0.00001231
Iteration 184/1000 | Loss: 0.00001231
Iteration 185/1000 | Loss: 0.00001231
Iteration 186/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2307017641433049e-05, 1.2307017641433049e-05, 1.2307017641433049e-05, 1.2307017641433049e-05, 1.2307017641433049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2307017641433049e-05

Optimization complete. Final v2v error: 2.9533655643463135 mm

Highest mean error: 3.423557758331299 mm for frame 56

Lowest mean error: 2.7161688804626465 mm for frame 120

Saving results

Total time: 36.84562301635742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881074
Iteration 2/25 | Loss: 0.00139777
Iteration 3/25 | Loss: 0.00123564
Iteration 4/25 | Loss: 0.00119060
Iteration 5/25 | Loss: 0.00118101
Iteration 6/25 | Loss: 0.00117949
Iteration 7/25 | Loss: 0.00117895
Iteration 8/25 | Loss: 0.00117895
Iteration 9/25 | Loss: 0.00117895
Iteration 10/25 | Loss: 0.00117895
Iteration 11/25 | Loss: 0.00117895
Iteration 12/25 | Loss: 0.00117895
Iteration 13/25 | Loss: 0.00117895
Iteration 14/25 | Loss: 0.00117895
Iteration 15/25 | Loss: 0.00117895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011789483251050115, 0.0011789483251050115, 0.0011789483251050115, 0.0011789483251050115, 0.0011789483251050115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011789483251050115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28587830
Iteration 2/25 | Loss: 0.00115871
Iteration 3/25 | Loss: 0.00115871
Iteration 4/25 | Loss: 0.00115871
Iteration 5/25 | Loss: 0.00115871
Iteration 6/25 | Loss: 0.00115871
Iteration 7/25 | Loss: 0.00115871
Iteration 8/25 | Loss: 0.00115871
Iteration 9/25 | Loss: 0.00115871
Iteration 10/25 | Loss: 0.00115871
Iteration 11/25 | Loss: 0.00115871
Iteration 12/25 | Loss: 0.00115871
Iteration 13/25 | Loss: 0.00115871
Iteration 14/25 | Loss: 0.00115871
Iteration 15/25 | Loss: 0.00115871
Iteration 16/25 | Loss: 0.00115871
Iteration 17/25 | Loss: 0.00115871
Iteration 18/25 | Loss: 0.00115871
Iteration 19/25 | Loss: 0.00115871
Iteration 20/25 | Loss: 0.00115871
Iteration 21/25 | Loss: 0.00115871
Iteration 22/25 | Loss: 0.00115871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001158706028945744, 0.001158706028945744, 0.001158706028945744, 0.001158706028945744, 0.001158706028945744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001158706028945744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115871
Iteration 2/1000 | Loss: 0.00006400
Iteration 3/1000 | Loss: 0.00003672
Iteration 4/1000 | Loss: 0.00002738
Iteration 5/1000 | Loss: 0.00002441
Iteration 6/1000 | Loss: 0.00002285
Iteration 7/1000 | Loss: 0.00002146
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001958
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001915
Iteration 15/1000 | Loss: 0.00001908
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00001906
Iteration 19/1000 | Loss: 0.00001905
Iteration 20/1000 | Loss: 0.00001905
Iteration 21/1000 | Loss: 0.00001905
Iteration 22/1000 | Loss: 0.00001904
Iteration 23/1000 | Loss: 0.00001904
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001902
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001902
Iteration 31/1000 | Loss: 0.00001902
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001902
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001899
Iteration 39/1000 | Loss: 0.00001899
Iteration 40/1000 | Loss: 0.00001899
Iteration 41/1000 | Loss: 0.00001899
Iteration 42/1000 | Loss: 0.00001898
Iteration 43/1000 | Loss: 0.00001898
Iteration 44/1000 | Loss: 0.00001898
Iteration 45/1000 | Loss: 0.00001898
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001897
Iteration 50/1000 | Loss: 0.00001897
Iteration 51/1000 | Loss: 0.00001897
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001897
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001895
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001892
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001889
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001888
Iteration 76/1000 | Loss: 0.00001888
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001888
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001887
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001887
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001886
Iteration 98/1000 | Loss: 0.00001886
Iteration 99/1000 | Loss: 0.00001886
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001885
Iteration 102/1000 | Loss: 0.00001885
Iteration 103/1000 | Loss: 0.00001885
Iteration 104/1000 | Loss: 0.00001885
Iteration 105/1000 | Loss: 0.00001885
Iteration 106/1000 | Loss: 0.00001885
Iteration 107/1000 | Loss: 0.00001885
Iteration 108/1000 | Loss: 0.00001885
Iteration 109/1000 | Loss: 0.00001884
Iteration 110/1000 | Loss: 0.00001884
Iteration 111/1000 | Loss: 0.00001884
Iteration 112/1000 | Loss: 0.00001884
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001883
Iteration 116/1000 | Loss: 0.00001883
Iteration 117/1000 | Loss: 0.00001883
Iteration 118/1000 | Loss: 0.00001883
Iteration 119/1000 | Loss: 0.00001883
Iteration 120/1000 | Loss: 0.00001883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.8830849512596615e-05, 1.8830849512596615e-05, 1.8830849512596615e-05, 1.8830849512596615e-05, 1.8830849512596615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8830849512596615e-05

Optimization complete. Final v2v error: 3.6633145809173584 mm

Highest mean error: 3.888751983642578 mm for frame 144

Lowest mean error: 3.527279853820801 mm for frame 1

Saving results

Total time: 37.81155180931091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871413
Iteration 2/25 | Loss: 0.00143865
Iteration 3/25 | Loss: 0.00120612
Iteration 4/25 | Loss: 0.00117043
Iteration 5/25 | Loss: 0.00115746
Iteration 6/25 | Loss: 0.00114639
Iteration 7/25 | Loss: 0.00115460
Iteration 8/25 | Loss: 0.00114000
Iteration 9/25 | Loss: 0.00113962
Iteration 10/25 | Loss: 0.00113960
Iteration 11/25 | Loss: 0.00113960
Iteration 12/25 | Loss: 0.00113959
Iteration 13/25 | Loss: 0.00113959
Iteration 14/25 | Loss: 0.00113959
Iteration 15/25 | Loss: 0.00113959
Iteration 16/25 | Loss: 0.00113958
Iteration 17/25 | Loss: 0.00113958
Iteration 18/25 | Loss: 0.00113958
Iteration 19/25 | Loss: 0.00113958
Iteration 20/25 | Loss: 0.00113958
Iteration 21/25 | Loss: 0.00113958
Iteration 22/25 | Loss: 0.00113958
Iteration 23/25 | Loss: 0.00113958
Iteration 24/25 | Loss: 0.00113958
Iteration 25/25 | Loss: 0.00113958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90250707
Iteration 2/25 | Loss: 0.00110690
Iteration 3/25 | Loss: 0.00110690
Iteration 4/25 | Loss: 0.00110690
Iteration 5/25 | Loss: 0.00110690
Iteration 6/25 | Loss: 0.00110690
Iteration 7/25 | Loss: 0.00110689
Iteration 8/25 | Loss: 0.00110689
Iteration 9/25 | Loss: 0.00110689
Iteration 10/25 | Loss: 0.00110689
Iteration 11/25 | Loss: 0.00110689
Iteration 12/25 | Loss: 0.00110689
Iteration 13/25 | Loss: 0.00110689
Iteration 14/25 | Loss: 0.00110689
Iteration 15/25 | Loss: 0.00110689
Iteration 16/25 | Loss: 0.00110689
Iteration 17/25 | Loss: 0.00110689
Iteration 18/25 | Loss: 0.00110689
Iteration 19/25 | Loss: 0.00110689
Iteration 20/25 | Loss: 0.00110689
Iteration 21/25 | Loss: 0.00110689
Iteration 22/25 | Loss: 0.00110689
Iteration 23/25 | Loss: 0.00110689
Iteration 24/25 | Loss: 0.00110689
Iteration 25/25 | Loss: 0.00110689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110689
Iteration 2/1000 | Loss: 0.00006259
Iteration 3/1000 | Loss: 0.00003610
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001759
Iteration 7/1000 | Loss: 0.00001672
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001588
Iteration 10/1000 | Loss: 0.00001561
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001497
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001492
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001491
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001488
Iteration 28/1000 | Loss: 0.00001488
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001486
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001474
Iteration 68/1000 | Loss: 0.00001474
Iteration 69/1000 | Loss: 0.00001474
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001473
Iteration 72/1000 | Loss: 0.00001473
Iteration 73/1000 | Loss: 0.00001473
Iteration 74/1000 | Loss: 0.00001473
Iteration 75/1000 | Loss: 0.00001473
Iteration 76/1000 | Loss: 0.00001473
Iteration 77/1000 | Loss: 0.00001473
Iteration 78/1000 | Loss: 0.00001473
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001473
Iteration 82/1000 | Loss: 0.00001472
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001472
Iteration 93/1000 | Loss: 0.00001472
Iteration 94/1000 | Loss: 0.00001472
Iteration 95/1000 | Loss: 0.00001472
Iteration 96/1000 | Loss: 0.00001472
Iteration 97/1000 | Loss: 0.00001472
Iteration 98/1000 | Loss: 0.00001472
Iteration 99/1000 | Loss: 0.00001472
Iteration 100/1000 | Loss: 0.00001472
Iteration 101/1000 | Loss: 0.00001472
Iteration 102/1000 | Loss: 0.00001472
Iteration 103/1000 | Loss: 0.00001472
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001472
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001472
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001472
Iteration 133/1000 | Loss: 0.00001472
Iteration 134/1000 | Loss: 0.00001472
Iteration 135/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.4716729310748633e-05, 1.4716729310748633e-05, 1.4716729310748633e-05, 1.4716729310748633e-05, 1.4716729310748633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4716729310748633e-05

Optimization complete. Final v2v error: 3.236236572265625 mm

Highest mean error: 3.7500429153442383 mm for frame 81

Lowest mean error: 2.822218179702759 mm for frame 121

Saving results

Total time: 43.5440399646759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936396
Iteration 2/25 | Loss: 0.00130288
Iteration 3/25 | Loss: 0.00119134
Iteration 4/25 | Loss: 0.00118074
Iteration 5/25 | Loss: 0.00117780
Iteration 6/25 | Loss: 0.00117721
Iteration 7/25 | Loss: 0.00117721
Iteration 8/25 | Loss: 0.00117721
Iteration 9/25 | Loss: 0.00117721
Iteration 10/25 | Loss: 0.00117721
Iteration 11/25 | Loss: 0.00117721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011772108264267445, 0.0011772108264267445, 0.0011772108264267445, 0.0011772108264267445, 0.0011772108264267445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011772108264267445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74571800
Iteration 2/25 | Loss: 0.00112119
Iteration 3/25 | Loss: 0.00112119
Iteration 4/25 | Loss: 0.00112119
Iteration 5/25 | Loss: 0.00112119
Iteration 6/25 | Loss: 0.00112119
Iteration 7/25 | Loss: 0.00112119
Iteration 8/25 | Loss: 0.00112119
Iteration 9/25 | Loss: 0.00112119
Iteration 10/25 | Loss: 0.00112119
Iteration 11/25 | Loss: 0.00112119
Iteration 12/25 | Loss: 0.00112119
Iteration 13/25 | Loss: 0.00112119
Iteration 14/25 | Loss: 0.00112119
Iteration 15/25 | Loss: 0.00112119
Iteration 16/25 | Loss: 0.00112119
Iteration 17/25 | Loss: 0.00112119
Iteration 18/25 | Loss: 0.00112119
Iteration 19/25 | Loss: 0.00112119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011211902601644397, 0.0011211902601644397, 0.0011211902601644397, 0.0011211902601644397, 0.0011211902601644397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011211902601644397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112119
Iteration 2/1000 | Loss: 0.00003500
Iteration 3/1000 | Loss: 0.00002082
Iteration 4/1000 | Loss: 0.00001731
Iteration 5/1000 | Loss: 0.00001587
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001377
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001375
Iteration 28/1000 | Loss: 0.00001374
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001368
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001365
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001364
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001363
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001363
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001362
Iteration 63/1000 | Loss: 0.00001362
Iteration 64/1000 | Loss: 0.00001362
Iteration 65/1000 | Loss: 0.00001362
Iteration 66/1000 | Loss: 0.00001362
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001362
Iteration 71/1000 | Loss: 0.00001362
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001361
Iteration 74/1000 | Loss: 0.00001361
Iteration 75/1000 | Loss: 0.00001361
Iteration 76/1000 | Loss: 0.00001361
Iteration 77/1000 | Loss: 0.00001361
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001360
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001360
Iteration 101/1000 | Loss: 0.00001360
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001359
Iteration 112/1000 | Loss: 0.00001359
Iteration 113/1000 | Loss: 0.00001359
Iteration 114/1000 | Loss: 0.00001359
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001359
Iteration 123/1000 | Loss: 0.00001359
Iteration 124/1000 | Loss: 0.00001359
Iteration 125/1000 | Loss: 0.00001359
Iteration 126/1000 | Loss: 0.00001359
Iteration 127/1000 | Loss: 0.00001359
Iteration 128/1000 | Loss: 0.00001359
Iteration 129/1000 | Loss: 0.00001359
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001359
Iteration 136/1000 | Loss: 0.00001359
Iteration 137/1000 | Loss: 0.00001359
Iteration 138/1000 | Loss: 0.00001359
Iteration 139/1000 | Loss: 0.00001359
Iteration 140/1000 | Loss: 0.00001359
Iteration 141/1000 | Loss: 0.00001359
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.3591275092039723e-05, 1.3591275092039723e-05, 1.3591275092039723e-05, 1.3591275092039723e-05, 1.3591275092039723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3591275092039723e-05

Optimization complete. Final v2v error: 3.105617046356201 mm

Highest mean error: 3.472116231918335 mm for frame 161

Lowest mean error: 2.763747453689575 mm for frame 15

Saving results

Total time: 35.961899757385254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915131
Iteration 2/25 | Loss: 0.00125096
Iteration 3/25 | Loss: 0.00114864
Iteration 4/25 | Loss: 0.00113290
Iteration 5/25 | Loss: 0.00112865
Iteration 6/25 | Loss: 0.00112814
Iteration 7/25 | Loss: 0.00112814
Iteration 8/25 | Loss: 0.00112814
Iteration 9/25 | Loss: 0.00112814
Iteration 10/25 | Loss: 0.00112814
Iteration 11/25 | Loss: 0.00112814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011281449114903808, 0.0011281449114903808, 0.0011281449114903808, 0.0011281449114903808, 0.0011281449114903808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011281449114903808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39396036
Iteration 2/25 | Loss: 0.00107823
Iteration 3/25 | Loss: 0.00107821
Iteration 4/25 | Loss: 0.00107821
Iteration 5/25 | Loss: 0.00107821
Iteration 6/25 | Loss: 0.00107821
Iteration 7/25 | Loss: 0.00107821
Iteration 8/25 | Loss: 0.00107821
Iteration 9/25 | Loss: 0.00107821
Iteration 10/25 | Loss: 0.00107821
Iteration 11/25 | Loss: 0.00107821
Iteration 12/25 | Loss: 0.00107821
Iteration 13/25 | Loss: 0.00107821
Iteration 14/25 | Loss: 0.00107821
Iteration 15/25 | Loss: 0.00107821
Iteration 16/25 | Loss: 0.00107821
Iteration 17/25 | Loss: 0.00107821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010782110039144754, 0.0010782110039144754, 0.0010782110039144754, 0.0010782110039144754, 0.0010782110039144754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010782110039144754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107821
Iteration 2/1000 | Loss: 0.00004196
Iteration 3/1000 | Loss: 0.00002509
Iteration 4/1000 | Loss: 0.00001959
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001632
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001572
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001552
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001539
Iteration 15/1000 | Loss: 0.00001534
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001531
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001530
Iteration 20/1000 | Loss: 0.00001529
Iteration 21/1000 | Loss: 0.00001529
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001527
Iteration 28/1000 | Loss: 0.00001526
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001523
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001520
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001517
Iteration 47/1000 | Loss: 0.00001517
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001516
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001515
Iteration 62/1000 | Loss: 0.00001515
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001513
Iteration 67/1000 | Loss: 0.00001513
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001513
Iteration 70/1000 | Loss: 0.00001513
Iteration 71/1000 | Loss: 0.00001513
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001511
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001511
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001510
Iteration 88/1000 | Loss: 0.00001510
Iteration 89/1000 | Loss: 0.00001510
Iteration 90/1000 | Loss: 0.00001510
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001508
Iteration 93/1000 | Loss: 0.00001508
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001507
Iteration 108/1000 | Loss: 0.00001507
Iteration 109/1000 | Loss: 0.00001507
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001505
Iteration 141/1000 | Loss: 0.00001505
Iteration 142/1000 | Loss: 0.00001505
Iteration 143/1000 | Loss: 0.00001504
Iteration 144/1000 | Loss: 0.00001504
Iteration 145/1000 | Loss: 0.00001504
Iteration 146/1000 | Loss: 0.00001504
Iteration 147/1000 | Loss: 0.00001504
Iteration 148/1000 | Loss: 0.00001504
Iteration 149/1000 | Loss: 0.00001504
Iteration 150/1000 | Loss: 0.00001504
Iteration 151/1000 | Loss: 0.00001504
Iteration 152/1000 | Loss: 0.00001504
Iteration 153/1000 | Loss: 0.00001504
Iteration 154/1000 | Loss: 0.00001504
Iteration 155/1000 | Loss: 0.00001504
Iteration 156/1000 | Loss: 0.00001504
Iteration 157/1000 | Loss: 0.00001504
Iteration 158/1000 | Loss: 0.00001504
Iteration 159/1000 | Loss: 0.00001503
Iteration 160/1000 | Loss: 0.00001503
Iteration 161/1000 | Loss: 0.00001503
Iteration 162/1000 | Loss: 0.00001503
Iteration 163/1000 | Loss: 0.00001503
Iteration 164/1000 | Loss: 0.00001503
Iteration 165/1000 | Loss: 0.00001503
Iteration 166/1000 | Loss: 0.00001503
Iteration 167/1000 | Loss: 0.00001503
Iteration 168/1000 | Loss: 0.00001503
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001503
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001503
Iteration 177/1000 | Loss: 0.00001503
Iteration 178/1000 | Loss: 0.00001503
Iteration 179/1000 | Loss: 0.00001503
Iteration 180/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.5026895198388956e-05, 1.5026895198388956e-05, 1.5026895198388956e-05, 1.5026895198388956e-05, 1.5026895198388956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5026895198388956e-05

Optimization complete. Final v2v error: 3.2607672214508057 mm

Highest mean error: 3.976456880569458 mm for frame 93

Lowest mean error: 2.9891905784606934 mm for frame 1

Saving results

Total time: 38.31725740432739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916656
Iteration 2/25 | Loss: 0.00183533
Iteration 3/25 | Loss: 0.00134993
Iteration 4/25 | Loss: 0.00130617
Iteration 5/25 | Loss: 0.00130385
Iteration 6/25 | Loss: 0.00130378
Iteration 7/25 | Loss: 0.00130378
Iteration 8/25 | Loss: 0.00130378
Iteration 9/25 | Loss: 0.00130378
Iteration 10/25 | Loss: 0.00130378
Iteration 11/25 | Loss: 0.00130378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013037837343290448, 0.0013037837343290448, 0.0013037837343290448, 0.0013037837343290448, 0.0013037837343290448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013037837343290448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17573488
Iteration 2/25 | Loss: 0.00059708
Iteration 3/25 | Loss: 0.00059702
Iteration 4/25 | Loss: 0.00059702
Iteration 5/25 | Loss: 0.00059702
Iteration 6/25 | Loss: 0.00059702
Iteration 7/25 | Loss: 0.00059702
Iteration 8/25 | Loss: 0.00059702
Iteration 9/25 | Loss: 0.00059702
Iteration 10/25 | Loss: 0.00059702
Iteration 11/25 | Loss: 0.00059702
Iteration 12/25 | Loss: 0.00059702
Iteration 13/25 | Loss: 0.00059702
Iteration 14/25 | Loss: 0.00059702
Iteration 15/25 | Loss: 0.00059702
Iteration 16/25 | Loss: 0.00059702
Iteration 17/25 | Loss: 0.00059702
Iteration 18/25 | Loss: 0.00059702
Iteration 19/25 | Loss: 0.00059702
Iteration 20/25 | Loss: 0.00059702
Iteration 21/25 | Loss: 0.00059702
Iteration 22/25 | Loss: 0.00059702
Iteration 23/25 | Loss: 0.00059702
Iteration 24/25 | Loss: 0.00059702
Iteration 25/25 | Loss: 0.00059702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059702
Iteration 2/1000 | Loss: 0.00006607
Iteration 3/1000 | Loss: 0.00004024
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00002865
Iteration 6/1000 | Loss: 0.00002716
Iteration 7/1000 | Loss: 0.00002575
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002402
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002309
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002281
Iteration 16/1000 | Loss: 0.00002278
Iteration 17/1000 | Loss: 0.00002277
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002277
Iteration 20/1000 | Loss: 0.00002276
Iteration 21/1000 | Loss: 0.00002276
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002275
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002273
Iteration 28/1000 | Loss: 0.00002273
Iteration 29/1000 | Loss: 0.00002272
Iteration 30/1000 | Loss: 0.00002272
Iteration 31/1000 | Loss: 0.00002272
Iteration 32/1000 | Loss: 0.00002272
Iteration 33/1000 | Loss: 0.00002272
Iteration 34/1000 | Loss: 0.00002272
Iteration 35/1000 | Loss: 0.00002272
Iteration 36/1000 | Loss: 0.00002272
Iteration 37/1000 | Loss: 0.00002272
Iteration 38/1000 | Loss: 0.00002272
Iteration 39/1000 | Loss: 0.00002272
Iteration 40/1000 | Loss: 0.00002270
Iteration 41/1000 | Loss: 0.00002270
Iteration 42/1000 | Loss: 0.00002270
Iteration 43/1000 | Loss: 0.00002269
Iteration 44/1000 | Loss: 0.00002269
Iteration 45/1000 | Loss: 0.00002269
Iteration 46/1000 | Loss: 0.00002268
Iteration 47/1000 | Loss: 0.00002268
Iteration 48/1000 | Loss: 0.00002267
Iteration 49/1000 | Loss: 0.00002266
Iteration 50/1000 | Loss: 0.00002266
Iteration 51/1000 | Loss: 0.00002265
Iteration 52/1000 | Loss: 0.00002265
Iteration 53/1000 | Loss: 0.00002265
Iteration 54/1000 | Loss: 0.00002264
Iteration 55/1000 | Loss: 0.00002264
Iteration 56/1000 | Loss: 0.00002264
Iteration 57/1000 | Loss: 0.00002263
Iteration 58/1000 | Loss: 0.00002263
Iteration 59/1000 | Loss: 0.00002263
Iteration 60/1000 | Loss: 0.00002262
Iteration 61/1000 | Loss: 0.00002262
Iteration 62/1000 | Loss: 0.00002262
Iteration 63/1000 | Loss: 0.00002262
Iteration 64/1000 | Loss: 0.00002262
Iteration 65/1000 | Loss: 0.00002262
Iteration 66/1000 | Loss: 0.00002261
Iteration 67/1000 | Loss: 0.00002261
Iteration 68/1000 | Loss: 0.00002261
Iteration 69/1000 | Loss: 0.00002261
Iteration 70/1000 | Loss: 0.00002261
Iteration 71/1000 | Loss: 0.00002261
Iteration 72/1000 | Loss: 0.00002260
Iteration 73/1000 | Loss: 0.00002260
Iteration 74/1000 | Loss: 0.00002260
Iteration 75/1000 | Loss: 0.00002259
Iteration 76/1000 | Loss: 0.00002259
Iteration 77/1000 | Loss: 0.00002259
Iteration 78/1000 | Loss: 0.00002259
Iteration 79/1000 | Loss: 0.00002258
Iteration 80/1000 | Loss: 0.00002258
Iteration 81/1000 | Loss: 0.00002258
Iteration 82/1000 | Loss: 0.00002258
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002257
Iteration 93/1000 | Loss: 0.00002257
Iteration 94/1000 | Loss: 0.00002257
Iteration 95/1000 | Loss: 0.00002257
Iteration 96/1000 | Loss: 0.00002257
Iteration 97/1000 | Loss: 0.00002257
Iteration 98/1000 | Loss: 0.00002257
Iteration 99/1000 | Loss: 0.00002257
Iteration 100/1000 | Loss: 0.00002257
Iteration 101/1000 | Loss: 0.00002257
Iteration 102/1000 | Loss: 0.00002257
Iteration 103/1000 | Loss: 0.00002256
Iteration 104/1000 | Loss: 0.00002256
Iteration 105/1000 | Loss: 0.00002256
Iteration 106/1000 | Loss: 0.00002256
Iteration 107/1000 | Loss: 0.00002256
Iteration 108/1000 | Loss: 0.00002256
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002255
Iteration 113/1000 | Loss: 0.00002255
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00002255
Iteration 118/1000 | Loss: 0.00002255
Iteration 119/1000 | Loss: 0.00002255
Iteration 120/1000 | Loss: 0.00002255
Iteration 121/1000 | Loss: 0.00002255
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002255
Iteration 125/1000 | Loss: 0.00002255
Iteration 126/1000 | Loss: 0.00002255
Iteration 127/1000 | Loss: 0.00002255
Iteration 128/1000 | Loss: 0.00002255
Iteration 129/1000 | Loss: 0.00002255
Iteration 130/1000 | Loss: 0.00002255
Iteration 131/1000 | Loss: 0.00002255
Iteration 132/1000 | Loss: 0.00002255
Iteration 133/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.254621904285159e-05, 2.254621904285159e-05, 2.254621904285159e-05, 2.254621904285159e-05, 2.254621904285159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.254621904285159e-05

Optimization complete. Final v2v error: 4.083169460296631 mm

Highest mean error: 4.272446155548096 mm for frame 126

Lowest mean error: 3.847649574279785 mm for frame 37

Saving results

Total time: 35.715173959732056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044843
Iteration 2/25 | Loss: 0.00162996
Iteration 3/25 | Loss: 0.00146930
Iteration 4/25 | Loss: 0.00137802
Iteration 5/25 | Loss: 0.00141215
Iteration 6/25 | Loss: 0.00145735
Iteration 7/25 | Loss: 0.00133540
Iteration 8/25 | Loss: 0.00126369
Iteration 9/25 | Loss: 0.00121724
Iteration 10/25 | Loss: 0.00120054
Iteration 11/25 | Loss: 0.00119881
Iteration 12/25 | Loss: 0.00119726
Iteration 13/25 | Loss: 0.00119227
Iteration 14/25 | Loss: 0.00119095
Iteration 15/25 | Loss: 0.00119018
Iteration 16/25 | Loss: 0.00118985
Iteration 17/25 | Loss: 0.00118961
Iteration 18/25 | Loss: 0.00118951
Iteration 19/25 | Loss: 0.00118925
Iteration 20/25 | Loss: 0.00119066
Iteration 21/25 | Loss: 0.00118887
Iteration 22/25 | Loss: 0.00118609
Iteration 23/25 | Loss: 0.00118462
Iteration 24/25 | Loss: 0.00118421
Iteration 25/25 | Loss: 0.00118410

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96000457
Iteration 2/25 | Loss: 0.00119665
Iteration 3/25 | Loss: 0.00119664
Iteration 4/25 | Loss: 0.00119664
Iteration 5/25 | Loss: 0.00119664
Iteration 6/25 | Loss: 0.00119664
Iteration 7/25 | Loss: 0.00119664
Iteration 8/25 | Loss: 0.00119664
Iteration 9/25 | Loss: 0.00119664
Iteration 10/25 | Loss: 0.00119664
Iteration 11/25 | Loss: 0.00119664
Iteration 12/25 | Loss: 0.00119664
Iteration 13/25 | Loss: 0.00119664
Iteration 14/25 | Loss: 0.00119664
Iteration 15/25 | Loss: 0.00119664
Iteration 16/25 | Loss: 0.00119664
Iteration 17/25 | Loss: 0.00119664
Iteration 18/25 | Loss: 0.00119664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011966386809945107, 0.0011966386809945107, 0.0011966386809945107, 0.0011966386809945107, 0.0011966386809945107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011966386809945107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119664
Iteration 2/1000 | Loss: 0.00009495
Iteration 3/1000 | Loss: 0.00005703
Iteration 4/1000 | Loss: 0.00004139
Iteration 5/1000 | Loss: 0.00003719
Iteration 6/1000 | Loss: 0.00003409
Iteration 7/1000 | Loss: 0.00003256
Iteration 8/1000 | Loss: 0.00102274
Iteration 9/1000 | Loss: 0.00003417
Iteration 10/1000 | Loss: 0.00003104
Iteration 11/1000 | Loss: 0.00003034
Iteration 12/1000 | Loss: 0.00037812
Iteration 13/1000 | Loss: 0.00075489
Iteration 14/1000 | Loss: 0.00005114
Iteration 15/1000 | Loss: 0.00003542
Iteration 16/1000 | Loss: 0.00002959
Iteration 17/1000 | Loss: 0.00002601
Iteration 18/1000 | Loss: 0.00002335
Iteration 19/1000 | Loss: 0.00002191
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001820
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001817
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001815
Iteration 37/1000 | Loss: 0.00001815
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001814
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001814
Iteration 43/1000 | Loss: 0.00001814
Iteration 44/1000 | Loss: 0.00001814
Iteration 45/1000 | Loss: 0.00001814
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001811
Iteration 50/1000 | Loss: 0.00001811
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001811
Iteration 53/1000 | Loss: 0.00001811
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001810
Iteration 57/1000 | Loss: 0.00001810
Iteration 58/1000 | Loss: 0.00001810
Iteration 59/1000 | Loss: 0.00001810
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001807
Iteration 67/1000 | Loss: 0.00001807
Iteration 68/1000 | Loss: 0.00001807
Iteration 69/1000 | Loss: 0.00001807
Iteration 70/1000 | Loss: 0.00001807
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001806
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001804
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001803
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001802
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.8014539818977937e-05, 1.8014539818977937e-05, 1.8014539818977937e-05, 1.8014539818977937e-05, 1.8014539818977937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8014539818977937e-05

Optimization complete. Final v2v error: 3.1619067192077637 mm

Highest mean error: 13.008515357971191 mm for frame 88

Lowest mean error: 2.6181046962738037 mm for frame 0

Saving results

Total time: 92.04817128181458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_it_4476/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_it_4476/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374251
Iteration 2/25 | Loss: 0.00118196
Iteration 3/25 | Loss: 0.00111724
Iteration 4/25 | Loss: 0.00111084
Iteration 5/25 | Loss: 0.00110938
Iteration 6/25 | Loss: 0.00110938
Iteration 7/25 | Loss: 0.00110938
Iteration 8/25 | Loss: 0.00110938
Iteration 9/25 | Loss: 0.00110938
Iteration 10/25 | Loss: 0.00110938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011093756183981895, 0.0011093756183981895, 0.0011093756183981895, 0.0011093756183981895, 0.0011093756183981895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011093756183981895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59922040
Iteration 2/25 | Loss: 0.00095591
Iteration 3/25 | Loss: 0.00095591
Iteration 4/25 | Loss: 0.00095591
Iteration 5/25 | Loss: 0.00095591
Iteration 6/25 | Loss: 0.00095591
Iteration 7/25 | Loss: 0.00095590
Iteration 8/25 | Loss: 0.00095590
Iteration 9/25 | Loss: 0.00095590
Iteration 10/25 | Loss: 0.00095590
Iteration 11/25 | Loss: 0.00095590
Iteration 12/25 | Loss: 0.00095590
Iteration 13/25 | Loss: 0.00095590
Iteration 14/25 | Loss: 0.00095590
Iteration 15/25 | Loss: 0.00095590
Iteration 16/25 | Loss: 0.00095590
Iteration 17/25 | Loss: 0.00095590
Iteration 18/25 | Loss: 0.00095590
Iteration 19/25 | Loss: 0.00095590
Iteration 20/25 | Loss: 0.00095590
Iteration 21/25 | Loss: 0.00095590
Iteration 22/25 | Loss: 0.00095590
Iteration 23/25 | Loss: 0.00095590
Iteration 24/25 | Loss: 0.00095590
Iteration 25/25 | Loss: 0.00095590

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095590
Iteration 2/1000 | Loss: 0.00002648
Iteration 3/1000 | Loss: 0.00001638
Iteration 4/1000 | Loss: 0.00001451
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001331
Iteration 8/1000 | Loss: 0.00001299
Iteration 9/1000 | Loss: 0.00001277
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001259
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001257
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001256
Iteration 25/1000 | Loss: 0.00001256
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001256
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001255
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001255
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001253
Iteration 49/1000 | Loss: 0.00001253
Iteration 50/1000 | Loss: 0.00001253
Iteration 51/1000 | Loss: 0.00001253
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001253
Iteration 55/1000 | Loss: 0.00001253
Iteration 56/1000 | Loss: 0.00001253
Iteration 57/1000 | Loss: 0.00001253
Iteration 58/1000 | Loss: 0.00001253
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001252
Iteration 61/1000 | Loss: 0.00001252
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001251
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001248
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001246
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001245
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001243
Iteration 98/1000 | Loss: 0.00001243
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001243
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001243
Iteration 104/1000 | Loss: 0.00001243
Iteration 105/1000 | Loss: 0.00001243
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.2407926078594755e-05, 1.2407926078594755e-05, 1.2407926078594755e-05, 1.2407926078594755e-05, 1.2407926078594755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2407926078594755e-05

Optimization complete. Final v2v error: 2.966586112976074 mm

Highest mean error: 3.7464497089385986 mm for frame 140

Lowest mean error: 2.7330875396728516 mm for frame 195

Saving results

Total time: 31.056635856628418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00318353
Iteration 2/25 | Loss: 0.00109620
Iteration 3/25 | Loss: 0.00096531
Iteration 4/25 | Loss: 0.00094890
Iteration 5/25 | Loss: 0.00094329
Iteration 6/25 | Loss: 0.00094136
Iteration 7/25 | Loss: 0.00094136
Iteration 8/25 | Loss: 0.00094136
Iteration 9/25 | Loss: 0.00094136
Iteration 10/25 | Loss: 0.00094136
Iteration 11/25 | Loss: 0.00094136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009413560037501156, 0.0009413560037501156, 0.0009413560037501156, 0.0009413560037501156, 0.0009413560037501156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009413560037501156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35982502
Iteration 2/25 | Loss: 0.00073259
Iteration 3/25 | Loss: 0.00073259
Iteration 4/25 | Loss: 0.00073259
Iteration 5/25 | Loss: 0.00073259
Iteration 6/25 | Loss: 0.00073259
Iteration 7/25 | Loss: 0.00073258
Iteration 8/25 | Loss: 0.00073258
Iteration 9/25 | Loss: 0.00073258
Iteration 10/25 | Loss: 0.00073258
Iteration 11/25 | Loss: 0.00073258
Iteration 12/25 | Loss: 0.00073258
Iteration 13/25 | Loss: 0.00073258
Iteration 14/25 | Loss: 0.00073258
Iteration 15/25 | Loss: 0.00073258
Iteration 16/25 | Loss: 0.00073258
Iteration 17/25 | Loss: 0.00073258
Iteration 18/25 | Loss: 0.00073258
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007325836922973394, 0.0007325836922973394, 0.0007325836922973394, 0.0007325836922973394, 0.0007325836922973394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007325836922973394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073258
Iteration 2/1000 | Loss: 0.00003001
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001402
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001186
Iteration 7/1000 | Loss: 0.00001134
Iteration 8/1000 | Loss: 0.00001096
Iteration 9/1000 | Loss: 0.00001068
Iteration 10/1000 | Loss: 0.00001043
Iteration 11/1000 | Loss: 0.00001041
Iteration 12/1000 | Loss: 0.00001033
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001026
Iteration 15/1000 | Loss: 0.00001025
Iteration 16/1000 | Loss: 0.00001023
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001018
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001013
Iteration 21/1000 | Loss: 0.00001013
Iteration 22/1000 | Loss: 0.00001012
Iteration 23/1000 | Loss: 0.00001012
Iteration 24/1000 | Loss: 0.00001011
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001010
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001009
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001008
Iteration 33/1000 | Loss: 0.00001008
Iteration 34/1000 | Loss: 0.00001007
Iteration 35/1000 | Loss: 0.00001007
Iteration 36/1000 | Loss: 0.00001006
Iteration 37/1000 | Loss: 0.00001006
Iteration 38/1000 | Loss: 0.00001006
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001005
Iteration 41/1000 | Loss: 0.00001005
Iteration 42/1000 | Loss: 0.00001005
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001004
Iteration 45/1000 | Loss: 0.00001004
Iteration 46/1000 | Loss: 0.00001004
Iteration 47/1000 | Loss: 0.00001004
Iteration 48/1000 | Loss: 0.00001004
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001003
Iteration 54/1000 | Loss: 0.00001003
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001001
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00000999
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000997
Iteration 69/1000 | Loss: 0.00000997
Iteration 70/1000 | Loss: 0.00000996
Iteration 71/1000 | Loss: 0.00000996
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000995
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000994
Iteration 77/1000 | Loss: 0.00000994
Iteration 78/1000 | Loss: 0.00000993
Iteration 79/1000 | Loss: 0.00000993
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000991
Iteration 83/1000 | Loss: 0.00000991
Iteration 84/1000 | Loss: 0.00000990
Iteration 85/1000 | Loss: 0.00000990
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000989
Iteration 88/1000 | Loss: 0.00000988
Iteration 89/1000 | Loss: 0.00000988
Iteration 90/1000 | Loss: 0.00000987
Iteration 91/1000 | Loss: 0.00000987
Iteration 92/1000 | Loss: 0.00000987
Iteration 93/1000 | Loss: 0.00000986
Iteration 94/1000 | Loss: 0.00000986
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000984
Iteration 100/1000 | Loss: 0.00000984
Iteration 101/1000 | Loss: 0.00000984
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000984
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000984
Iteration 110/1000 | Loss: 0.00000984
Iteration 111/1000 | Loss: 0.00000984
Iteration 112/1000 | Loss: 0.00000984
Iteration 113/1000 | Loss: 0.00000984
Iteration 114/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [9.842889085120987e-06, 9.842889085120987e-06, 9.842889085120987e-06, 9.842889085120987e-06, 9.842889085120987e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.842889085120987e-06

Optimization complete. Final v2v error: 2.6478750705718994 mm

Highest mean error: 3.2221219539642334 mm for frame 94

Lowest mean error: 2.2700390815734863 mm for frame 265

Saving results

Total time: 38.96308374404907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445558
Iteration 2/25 | Loss: 0.00110749
Iteration 3/25 | Loss: 0.00101466
Iteration 4/25 | Loss: 0.00099597
Iteration 5/25 | Loss: 0.00099081
Iteration 6/25 | Loss: 0.00099003
Iteration 7/25 | Loss: 0.00099003
Iteration 8/25 | Loss: 0.00099003
Iteration 9/25 | Loss: 0.00099003
Iteration 10/25 | Loss: 0.00099003
Iteration 11/25 | Loss: 0.00099003
Iteration 12/25 | Loss: 0.00099003
Iteration 13/25 | Loss: 0.00099003
Iteration 14/25 | Loss: 0.00099003
Iteration 15/25 | Loss: 0.00099003
Iteration 16/25 | Loss: 0.00099003
Iteration 17/25 | Loss: 0.00099003
Iteration 18/25 | Loss: 0.00099003
Iteration 19/25 | Loss: 0.00099003
Iteration 20/25 | Loss: 0.00099003
Iteration 21/25 | Loss: 0.00099003
Iteration 22/25 | Loss: 0.00099003
Iteration 23/25 | Loss: 0.00099003
Iteration 24/25 | Loss: 0.00099003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009900283766910434, 0.0009900283766910434, 0.0009900283766910434, 0.0009900283766910434, 0.0009900283766910434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009900283766910434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37415028
Iteration 2/25 | Loss: 0.00063712
Iteration 3/25 | Loss: 0.00063712
Iteration 4/25 | Loss: 0.00063712
Iteration 5/25 | Loss: 0.00063712
Iteration 6/25 | Loss: 0.00063711
Iteration 7/25 | Loss: 0.00063711
Iteration 8/25 | Loss: 0.00063711
Iteration 9/25 | Loss: 0.00063711
Iteration 10/25 | Loss: 0.00063711
Iteration 11/25 | Loss: 0.00063711
Iteration 12/25 | Loss: 0.00063711
Iteration 13/25 | Loss: 0.00063711
Iteration 14/25 | Loss: 0.00063711
Iteration 15/25 | Loss: 0.00063711
Iteration 16/25 | Loss: 0.00063711
Iteration 17/25 | Loss: 0.00063711
Iteration 18/25 | Loss: 0.00063711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000637113000266254, 0.000637113000266254, 0.000637113000266254, 0.000637113000266254, 0.000637113000266254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000637113000266254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063711
Iteration 2/1000 | Loss: 0.00002834
Iteration 3/1000 | Loss: 0.00002100
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001613
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001508
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001487
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001459
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001458
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001457
Iteration 33/1000 | Loss: 0.00001457
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001457
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001456
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001452
Iteration 52/1000 | Loss: 0.00001452
Iteration 53/1000 | Loss: 0.00001452
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001447
Iteration 62/1000 | Loss: 0.00001447
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001439
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001435
Iteration 69/1000 | Loss: 0.00001435
Iteration 70/1000 | Loss: 0.00001435
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00001435
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001434
Iteration 76/1000 | Loss: 0.00001433
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001427
Iteration 90/1000 | Loss: 0.00001427
Iteration 91/1000 | Loss: 0.00001427
Iteration 92/1000 | Loss: 0.00001427
Iteration 93/1000 | Loss: 0.00001427
Iteration 94/1000 | Loss: 0.00001426
Iteration 95/1000 | Loss: 0.00001426
Iteration 96/1000 | Loss: 0.00001426
Iteration 97/1000 | Loss: 0.00001426
Iteration 98/1000 | Loss: 0.00001426
Iteration 99/1000 | Loss: 0.00001426
Iteration 100/1000 | Loss: 0.00001425
Iteration 101/1000 | Loss: 0.00001425
Iteration 102/1000 | Loss: 0.00001425
Iteration 103/1000 | Loss: 0.00001425
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001424
Iteration 108/1000 | Loss: 0.00001424
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001421
Iteration 119/1000 | Loss: 0.00001421
Iteration 120/1000 | Loss: 0.00001421
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001419
Iteration 125/1000 | Loss: 0.00001419
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001418
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001416
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001415
Iteration 157/1000 | Loss: 0.00001415
Iteration 158/1000 | Loss: 0.00001415
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001415
Iteration 165/1000 | Loss: 0.00001415
Iteration 166/1000 | Loss: 0.00001415
Iteration 167/1000 | Loss: 0.00001415
Iteration 168/1000 | Loss: 0.00001414
Iteration 169/1000 | Loss: 0.00001414
Iteration 170/1000 | Loss: 0.00001414
Iteration 171/1000 | Loss: 0.00001414
Iteration 172/1000 | Loss: 0.00001414
Iteration 173/1000 | Loss: 0.00001414
Iteration 174/1000 | Loss: 0.00001414
Iteration 175/1000 | Loss: 0.00001414
Iteration 176/1000 | Loss: 0.00001414
Iteration 177/1000 | Loss: 0.00001414
Iteration 178/1000 | Loss: 0.00001414
Iteration 179/1000 | Loss: 0.00001414
Iteration 180/1000 | Loss: 0.00001414
Iteration 181/1000 | Loss: 0.00001414
Iteration 182/1000 | Loss: 0.00001414
Iteration 183/1000 | Loss: 0.00001414
Iteration 184/1000 | Loss: 0.00001414
Iteration 185/1000 | Loss: 0.00001414
Iteration 186/1000 | Loss: 0.00001414
Iteration 187/1000 | Loss: 0.00001414
Iteration 188/1000 | Loss: 0.00001414
Iteration 189/1000 | Loss: 0.00001414
Iteration 190/1000 | Loss: 0.00001414
Iteration 191/1000 | Loss: 0.00001414
Iteration 192/1000 | Loss: 0.00001413
Iteration 193/1000 | Loss: 0.00001413
Iteration 194/1000 | Loss: 0.00001413
Iteration 195/1000 | Loss: 0.00001413
Iteration 196/1000 | Loss: 0.00001413
Iteration 197/1000 | Loss: 0.00001413
Iteration 198/1000 | Loss: 0.00001413
Iteration 199/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4131684110907372e-05, 1.4131684110907372e-05, 1.4131684110907372e-05, 1.4131684110907372e-05, 1.4131684110907372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4131684110907372e-05

Optimization complete. Final v2v error: 3.193258285522461 mm

Highest mean error: 3.3277244567871094 mm for frame 116

Lowest mean error: 3.0174708366394043 mm for frame 53

Saving results

Total time: 41.55324602127075
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409283
Iteration 2/25 | Loss: 0.00108526
Iteration 3/25 | Loss: 0.00101374
Iteration 4/25 | Loss: 0.00100105
Iteration 5/25 | Loss: 0.00099675
Iteration 6/25 | Loss: 0.00099601
Iteration 7/25 | Loss: 0.00099588
Iteration 8/25 | Loss: 0.00099588
Iteration 9/25 | Loss: 0.00099588
Iteration 10/25 | Loss: 0.00099588
Iteration 11/25 | Loss: 0.00099588
Iteration 12/25 | Loss: 0.00099588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009958838345482945, 0.0009958838345482945, 0.0009958838345482945, 0.0009958838345482945, 0.0009958838345482945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009958838345482945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42286956
Iteration 2/25 | Loss: 0.00070767
Iteration 3/25 | Loss: 0.00070767
Iteration 4/25 | Loss: 0.00070767
Iteration 5/25 | Loss: 0.00070767
Iteration 6/25 | Loss: 0.00070767
Iteration 7/25 | Loss: 0.00070767
Iteration 8/25 | Loss: 0.00070767
Iteration 9/25 | Loss: 0.00070767
Iteration 10/25 | Loss: 0.00070767
Iteration 11/25 | Loss: 0.00070767
Iteration 12/25 | Loss: 0.00070767
Iteration 13/25 | Loss: 0.00070767
Iteration 14/25 | Loss: 0.00070767
Iteration 15/25 | Loss: 0.00070767
Iteration 16/25 | Loss: 0.00070767
Iteration 17/25 | Loss: 0.00070767
Iteration 18/25 | Loss: 0.00070767
Iteration 19/25 | Loss: 0.00070767
Iteration 20/25 | Loss: 0.00070767
Iteration 21/25 | Loss: 0.00070767
Iteration 22/25 | Loss: 0.00070767
Iteration 23/25 | Loss: 0.00070767
Iteration 24/25 | Loss: 0.00070766
Iteration 25/25 | Loss: 0.00070766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070766
Iteration 2/1000 | Loss: 0.00002567
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001744
Iteration 5/1000 | Loss: 0.00001667
Iteration 6/1000 | Loss: 0.00001617
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001532
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001457
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001450
Iteration 20/1000 | Loss: 0.00001450
Iteration 21/1000 | Loss: 0.00001449
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001448
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001443
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001439
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001439
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001438
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001437
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001433
Iteration 50/1000 | Loss: 0.00001433
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001429
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001429
Iteration 64/1000 | Loss: 0.00001429
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001425
Iteration 69/1000 | Loss: 0.00001425
Iteration 70/1000 | Loss: 0.00001425
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001424
Iteration 74/1000 | Loss: 0.00001424
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001420
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001419
Iteration 100/1000 | Loss: 0.00001419
Iteration 101/1000 | Loss: 0.00001419
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001415
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001414
Iteration 127/1000 | Loss: 0.00001414
Iteration 128/1000 | Loss: 0.00001414
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001413
Iteration 133/1000 | Loss: 0.00001413
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001412
Iteration 139/1000 | Loss: 0.00001412
Iteration 140/1000 | Loss: 0.00001412
Iteration 141/1000 | Loss: 0.00001412
Iteration 142/1000 | Loss: 0.00001412
Iteration 143/1000 | Loss: 0.00001412
Iteration 144/1000 | Loss: 0.00001412
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Iteration 150/1000 | Loss: 0.00001411
Iteration 151/1000 | Loss: 0.00001411
Iteration 152/1000 | Loss: 0.00001411
Iteration 153/1000 | Loss: 0.00001411
Iteration 154/1000 | Loss: 0.00001411
Iteration 155/1000 | Loss: 0.00001411
Iteration 156/1000 | Loss: 0.00001411
Iteration 157/1000 | Loss: 0.00001411
Iteration 158/1000 | Loss: 0.00001411
Iteration 159/1000 | Loss: 0.00001411
Iteration 160/1000 | Loss: 0.00001411
Iteration 161/1000 | Loss: 0.00001411
Iteration 162/1000 | Loss: 0.00001411
Iteration 163/1000 | Loss: 0.00001411
Iteration 164/1000 | Loss: 0.00001411
Iteration 165/1000 | Loss: 0.00001411
Iteration 166/1000 | Loss: 0.00001411
Iteration 167/1000 | Loss: 0.00001411
Iteration 168/1000 | Loss: 0.00001411
Iteration 169/1000 | Loss: 0.00001411
Iteration 170/1000 | Loss: 0.00001411
Iteration 171/1000 | Loss: 0.00001411
Iteration 172/1000 | Loss: 0.00001411
Iteration 173/1000 | Loss: 0.00001411
Iteration 174/1000 | Loss: 0.00001411
Iteration 175/1000 | Loss: 0.00001411
Iteration 176/1000 | Loss: 0.00001411
Iteration 177/1000 | Loss: 0.00001411
Iteration 178/1000 | Loss: 0.00001411
Iteration 179/1000 | Loss: 0.00001411
Iteration 180/1000 | Loss: 0.00001411
Iteration 181/1000 | Loss: 0.00001411
Iteration 182/1000 | Loss: 0.00001411
Iteration 183/1000 | Loss: 0.00001411
Iteration 184/1000 | Loss: 0.00001411
Iteration 185/1000 | Loss: 0.00001411
Iteration 186/1000 | Loss: 0.00001411
Iteration 187/1000 | Loss: 0.00001411
Iteration 188/1000 | Loss: 0.00001411
Iteration 189/1000 | Loss: 0.00001411
Iteration 190/1000 | Loss: 0.00001411
Iteration 191/1000 | Loss: 0.00001411
Iteration 192/1000 | Loss: 0.00001411
Iteration 193/1000 | Loss: 0.00001411
Iteration 194/1000 | Loss: 0.00001411
Iteration 195/1000 | Loss: 0.00001411
Iteration 196/1000 | Loss: 0.00001411
Iteration 197/1000 | Loss: 0.00001411
Iteration 198/1000 | Loss: 0.00001411
Iteration 199/1000 | Loss: 0.00001411
Iteration 200/1000 | Loss: 0.00001411
Iteration 201/1000 | Loss: 0.00001411
Iteration 202/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.4109617950452957e-05, 1.4109617950452957e-05, 1.4109617950452957e-05, 1.4109617950452957e-05, 1.4109617950452957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4109617950452957e-05

Optimization complete. Final v2v error: 3.2002432346343994 mm

Highest mean error: 3.658822774887085 mm for frame 24

Lowest mean error: 2.842376708984375 mm for frame 46

Saving results

Total time: 40.526169300079346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369146
Iteration 2/25 | Loss: 0.00112993
Iteration 3/25 | Loss: 0.00099941
Iteration 4/25 | Loss: 0.00097674
Iteration 5/25 | Loss: 0.00096947
Iteration 6/25 | Loss: 0.00096777
Iteration 7/25 | Loss: 0.00096730
Iteration 8/25 | Loss: 0.00096730
Iteration 9/25 | Loss: 0.00096730
Iteration 10/25 | Loss: 0.00096730
Iteration 11/25 | Loss: 0.00096730
Iteration 12/25 | Loss: 0.00096730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009672983433119953, 0.0009672983433119953, 0.0009672983433119953, 0.0009672983433119953, 0.0009672983433119953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009672983433119953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32993925
Iteration 2/25 | Loss: 0.00068726
Iteration 3/25 | Loss: 0.00068726
Iteration 4/25 | Loss: 0.00068726
Iteration 5/25 | Loss: 0.00068726
Iteration 6/25 | Loss: 0.00068726
Iteration 7/25 | Loss: 0.00068726
Iteration 8/25 | Loss: 0.00068726
Iteration 9/25 | Loss: 0.00068726
Iteration 10/25 | Loss: 0.00068726
Iteration 11/25 | Loss: 0.00068726
Iteration 12/25 | Loss: 0.00068726
Iteration 13/25 | Loss: 0.00068726
Iteration 14/25 | Loss: 0.00068726
Iteration 15/25 | Loss: 0.00068726
Iteration 16/25 | Loss: 0.00068726
Iteration 17/25 | Loss: 0.00068726
Iteration 18/25 | Loss: 0.00068726
Iteration 19/25 | Loss: 0.00068726
Iteration 20/25 | Loss: 0.00068726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006872587837278843, 0.0006872587837278843, 0.0006872587837278843, 0.0006872587837278843, 0.0006872587837278843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006872587837278843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068726
Iteration 2/1000 | Loss: 0.00003796
Iteration 3/1000 | Loss: 0.00002756
Iteration 4/1000 | Loss: 0.00002152
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00001708
Iteration 8/1000 | Loss: 0.00001648
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001554
Iteration 12/1000 | Loss: 0.00001539
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001495
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001493
Iteration 20/1000 | Loss: 0.00001492
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001491
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001475
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001471
Iteration 54/1000 | Loss: 0.00001471
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001470
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001469
Iteration 67/1000 | Loss: 0.00001469
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001469
Iteration 73/1000 | Loss: 0.00001468
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001466
Iteration 82/1000 | Loss: 0.00001466
Iteration 83/1000 | Loss: 0.00001466
Iteration 84/1000 | Loss: 0.00001466
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001466
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001466
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001465
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001463
Iteration 110/1000 | Loss: 0.00001463
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001463
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.462776526750531e-05, 1.462776526750531e-05, 1.462776526750531e-05, 1.462776526750531e-05, 1.462776526750531e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.462776526750531e-05

Optimization complete. Final v2v error: 3.1759490966796875 mm

Highest mean error: 3.6844899654388428 mm for frame 92

Lowest mean error: 2.487092971801758 mm for frame 11

Saving results

Total time: 41.06888294219971
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073306
Iteration 2/25 | Loss: 0.00186180
Iteration 3/25 | Loss: 0.00150692
Iteration 4/25 | Loss: 0.00107881
Iteration 5/25 | Loss: 0.00108551
Iteration 6/25 | Loss: 0.00103049
Iteration 7/25 | Loss: 0.00101786
Iteration 8/25 | Loss: 0.00101456
Iteration 9/25 | Loss: 0.00101219
Iteration 10/25 | Loss: 0.00101004
Iteration 11/25 | Loss: 0.00101025
Iteration 12/25 | Loss: 0.00100966
Iteration 13/25 | Loss: 0.00100946
Iteration 14/25 | Loss: 0.00100946
Iteration 15/25 | Loss: 0.00100946
Iteration 16/25 | Loss: 0.00100946
Iteration 17/25 | Loss: 0.00100946
Iteration 18/25 | Loss: 0.00100946
Iteration 19/25 | Loss: 0.00100946
Iteration 20/25 | Loss: 0.00100946
Iteration 21/25 | Loss: 0.00100946
Iteration 22/25 | Loss: 0.00100946
Iteration 23/25 | Loss: 0.00100946
Iteration 24/25 | Loss: 0.00100945
Iteration 25/25 | Loss: 0.00100945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89023089
Iteration 2/25 | Loss: 0.00080202
Iteration 3/25 | Loss: 0.00080202
Iteration 4/25 | Loss: 0.00080202
Iteration 5/25 | Loss: 0.00080202
Iteration 6/25 | Loss: 0.00080163
Iteration 7/25 | Loss: 0.00080163
Iteration 8/25 | Loss: 0.00080163
Iteration 9/25 | Loss: 0.00080163
Iteration 10/25 | Loss: 0.00080163
Iteration 11/25 | Loss: 0.00080163
Iteration 12/25 | Loss: 0.00080163
Iteration 13/25 | Loss: 0.00080163
Iteration 14/25 | Loss: 0.00080163
Iteration 15/25 | Loss: 0.00080162
Iteration 16/25 | Loss: 0.00080162
Iteration 17/25 | Loss: 0.00080162
Iteration 18/25 | Loss: 0.00080162
Iteration 19/25 | Loss: 0.00080162
Iteration 20/25 | Loss: 0.00080162
Iteration 21/25 | Loss: 0.00080162
Iteration 22/25 | Loss: 0.00080162
Iteration 23/25 | Loss: 0.00080162
Iteration 24/25 | Loss: 0.00080162
Iteration 25/25 | Loss: 0.00080162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080162
Iteration 2/1000 | Loss: 0.00003158
Iteration 3/1000 | Loss: 0.00007764
Iteration 4/1000 | Loss: 0.00002876
Iteration 5/1000 | Loss: 0.00002193
Iteration 6/1000 | Loss: 0.00002099
Iteration 7/1000 | Loss: 0.00007213
Iteration 8/1000 | Loss: 0.00002078
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00003756
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00001934
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001923
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001922
Iteration 22/1000 | Loss: 0.00002872
Iteration 23/1000 | Loss: 0.00002020
Iteration 24/1000 | Loss: 0.00001997
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001892
Iteration 31/1000 | Loss: 0.00001892
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001887
Iteration 34/1000 | Loss: 0.00002284
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001872
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001871
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001867
Iteration 44/1000 | Loss: 0.00001866
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001865
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00005940
Iteration 51/1000 | Loss: 0.00002283
Iteration 52/1000 | Loss: 0.00002343
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00002746
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001847
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00002822
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001852
Iteration 102/1000 | Loss: 0.00001851
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001847
Iteration 118/1000 | Loss: 0.00001847
Iteration 119/1000 | Loss: 0.00001847
Iteration 120/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.846895793278236e-05, 1.846895793278236e-05, 1.846895793278236e-05, 1.846895793278236e-05, 1.846895793278236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.846895793278236e-05

Optimization complete. Final v2v error: 3.6589651107788086 mm

Highest mean error: 4.230559349060059 mm for frame 36

Lowest mean error: 3.3797476291656494 mm for frame 115

Saving results

Total time: 66.7142071723938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393469
Iteration 2/25 | Loss: 0.00103924
Iteration 3/25 | Loss: 0.00095403
Iteration 4/25 | Loss: 0.00093965
Iteration 5/25 | Loss: 0.00093489
Iteration 6/25 | Loss: 0.00093360
Iteration 7/25 | Loss: 0.00093360
Iteration 8/25 | Loss: 0.00093360
Iteration 9/25 | Loss: 0.00093360
Iteration 10/25 | Loss: 0.00093360
Iteration 11/25 | Loss: 0.00093360
Iteration 12/25 | Loss: 0.00093360
Iteration 13/25 | Loss: 0.00093360
Iteration 14/25 | Loss: 0.00093360
Iteration 15/25 | Loss: 0.00093360
Iteration 16/25 | Loss: 0.00093360
Iteration 17/25 | Loss: 0.00093360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009336035582236946, 0.0009336035582236946, 0.0009336035582236946, 0.0009336035582236946, 0.0009336035582236946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009336035582236946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77160883
Iteration 2/25 | Loss: 0.00065798
Iteration 3/25 | Loss: 0.00065798
Iteration 4/25 | Loss: 0.00065798
Iteration 5/25 | Loss: 0.00065798
Iteration 6/25 | Loss: 0.00065798
Iteration 7/25 | Loss: 0.00065798
Iteration 8/25 | Loss: 0.00065798
Iteration 9/25 | Loss: 0.00065798
Iteration 10/25 | Loss: 0.00065798
Iteration 11/25 | Loss: 0.00065798
Iteration 12/25 | Loss: 0.00065798
Iteration 13/25 | Loss: 0.00065798
Iteration 14/25 | Loss: 0.00065798
Iteration 15/25 | Loss: 0.00065798
Iteration 16/25 | Loss: 0.00065798
Iteration 17/25 | Loss: 0.00065798
Iteration 18/25 | Loss: 0.00065798
Iteration 19/25 | Loss: 0.00065798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006579766632057726, 0.0006579766632057726, 0.0006579766632057726, 0.0006579766632057726, 0.0006579766632057726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006579766632057726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065798
Iteration 2/1000 | Loss: 0.00001783
Iteration 3/1000 | Loss: 0.00001166
Iteration 4/1000 | Loss: 0.00001048
Iteration 5/1000 | Loss: 0.00000989
Iteration 6/1000 | Loss: 0.00000949
Iteration 7/1000 | Loss: 0.00000914
Iteration 8/1000 | Loss: 0.00000894
Iteration 9/1000 | Loss: 0.00000874
Iteration 10/1000 | Loss: 0.00000866
Iteration 11/1000 | Loss: 0.00000865
Iteration 12/1000 | Loss: 0.00000858
Iteration 13/1000 | Loss: 0.00000854
Iteration 14/1000 | Loss: 0.00000854
Iteration 15/1000 | Loss: 0.00000853
Iteration 16/1000 | Loss: 0.00000853
Iteration 17/1000 | Loss: 0.00000853
Iteration 18/1000 | Loss: 0.00000852
Iteration 19/1000 | Loss: 0.00000849
Iteration 20/1000 | Loss: 0.00000848
Iteration 21/1000 | Loss: 0.00000847
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000845
Iteration 25/1000 | Loss: 0.00000844
Iteration 26/1000 | Loss: 0.00000844
Iteration 27/1000 | Loss: 0.00000844
Iteration 28/1000 | Loss: 0.00000843
Iteration 29/1000 | Loss: 0.00000843
Iteration 30/1000 | Loss: 0.00000843
Iteration 31/1000 | Loss: 0.00000843
Iteration 32/1000 | Loss: 0.00000842
Iteration 33/1000 | Loss: 0.00000842
Iteration 34/1000 | Loss: 0.00000840
Iteration 35/1000 | Loss: 0.00000839
Iteration 36/1000 | Loss: 0.00000839
Iteration 37/1000 | Loss: 0.00000839
Iteration 38/1000 | Loss: 0.00000838
Iteration 39/1000 | Loss: 0.00000838
Iteration 40/1000 | Loss: 0.00000838
Iteration 41/1000 | Loss: 0.00000837
Iteration 42/1000 | Loss: 0.00000837
Iteration 43/1000 | Loss: 0.00000836
Iteration 44/1000 | Loss: 0.00000836
Iteration 45/1000 | Loss: 0.00000836
Iteration 46/1000 | Loss: 0.00000836
Iteration 47/1000 | Loss: 0.00000836
Iteration 48/1000 | Loss: 0.00000836
Iteration 49/1000 | Loss: 0.00000835
Iteration 50/1000 | Loss: 0.00000834
Iteration 51/1000 | Loss: 0.00000834
Iteration 52/1000 | Loss: 0.00000833
Iteration 53/1000 | Loss: 0.00000833
Iteration 54/1000 | Loss: 0.00000833
Iteration 55/1000 | Loss: 0.00000832
Iteration 56/1000 | Loss: 0.00000832
Iteration 57/1000 | Loss: 0.00000832
Iteration 58/1000 | Loss: 0.00000832
Iteration 59/1000 | Loss: 0.00000831
Iteration 60/1000 | Loss: 0.00000831
Iteration 61/1000 | Loss: 0.00000830
Iteration 62/1000 | Loss: 0.00000829
Iteration 63/1000 | Loss: 0.00000829
Iteration 64/1000 | Loss: 0.00000829
Iteration 65/1000 | Loss: 0.00000829
Iteration 66/1000 | Loss: 0.00000829
Iteration 67/1000 | Loss: 0.00000829
Iteration 68/1000 | Loss: 0.00000829
Iteration 69/1000 | Loss: 0.00000829
Iteration 70/1000 | Loss: 0.00000828
Iteration 71/1000 | Loss: 0.00000828
Iteration 72/1000 | Loss: 0.00000828
Iteration 73/1000 | Loss: 0.00000827
Iteration 74/1000 | Loss: 0.00000826
Iteration 75/1000 | Loss: 0.00000826
Iteration 76/1000 | Loss: 0.00000826
Iteration 77/1000 | Loss: 0.00000826
Iteration 78/1000 | Loss: 0.00000825
Iteration 79/1000 | Loss: 0.00000825
Iteration 80/1000 | Loss: 0.00000825
Iteration 81/1000 | Loss: 0.00000825
Iteration 82/1000 | Loss: 0.00000825
Iteration 83/1000 | Loss: 0.00000824
Iteration 84/1000 | Loss: 0.00000824
Iteration 85/1000 | Loss: 0.00000823
Iteration 86/1000 | Loss: 0.00000823
Iteration 87/1000 | Loss: 0.00000823
Iteration 88/1000 | Loss: 0.00000823
Iteration 89/1000 | Loss: 0.00000822
Iteration 90/1000 | Loss: 0.00000822
Iteration 91/1000 | Loss: 0.00000822
Iteration 92/1000 | Loss: 0.00000821
Iteration 93/1000 | Loss: 0.00000821
Iteration 94/1000 | Loss: 0.00000821
Iteration 95/1000 | Loss: 0.00000821
Iteration 96/1000 | Loss: 0.00000820
Iteration 97/1000 | Loss: 0.00000820
Iteration 98/1000 | Loss: 0.00000820
Iteration 99/1000 | Loss: 0.00000820
Iteration 100/1000 | Loss: 0.00000820
Iteration 101/1000 | Loss: 0.00000820
Iteration 102/1000 | Loss: 0.00000819
Iteration 103/1000 | Loss: 0.00000819
Iteration 104/1000 | Loss: 0.00000819
Iteration 105/1000 | Loss: 0.00000818
Iteration 106/1000 | Loss: 0.00000818
Iteration 107/1000 | Loss: 0.00000818
Iteration 108/1000 | Loss: 0.00000818
Iteration 109/1000 | Loss: 0.00000818
Iteration 110/1000 | Loss: 0.00000818
Iteration 111/1000 | Loss: 0.00000817
Iteration 112/1000 | Loss: 0.00000817
Iteration 113/1000 | Loss: 0.00000817
Iteration 114/1000 | Loss: 0.00000817
Iteration 115/1000 | Loss: 0.00000817
Iteration 116/1000 | Loss: 0.00000817
Iteration 117/1000 | Loss: 0.00000817
Iteration 118/1000 | Loss: 0.00000816
Iteration 119/1000 | Loss: 0.00000816
Iteration 120/1000 | Loss: 0.00000816
Iteration 121/1000 | Loss: 0.00000816
Iteration 122/1000 | Loss: 0.00000816
Iteration 123/1000 | Loss: 0.00000816
Iteration 124/1000 | Loss: 0.00000816
Iteration 125/1000 | Loss: 0.00000815
Iteration 126/1000 | Loss: 0.00000815
Iteration 127/1000 | Loss: 0.00000815
Iteration 128/1000 | Loss: 0.00000815
Iteration 129/1000 | Loss: 0.00000815
Iteration 130/1000 | Loss: 0.00000815
Iteration 131/1000 | Loss: 0.00000815
Iteration 132/1000 | Loss: 0.00000815
Iteration 133/1000 | Loss: 0.00000815
Iteration 134/1000 | Loss: 0.00000815
Iteration 135/1000 | Loss: 0.00000814
Iteration 136/1000 | Loss: 0.00000814
Iteration 137/1000 | Loss: 0.00000814
Iteration 138/1000 | Loss: 0.00000814
Iteration 139/1000 | Loss: 0.00000814
Iteration 140/1000 | Loss: 0.00000814
Iteration 141/1000 | Loss: 0.00000814
Iteration 142/1000 | Loss: 0.00000814
Iteration 143/1000 | Loss: 0.00000813
Iteration 144/1000 | Loss: 0.00000813
Iteration 145/1000 | Loss: 0.00000813
Iteration 146/1000 | Loss: 0.00000813
Iteration 147/1000 | Loss: 0.00000813
Iteration 148/1000 | Loss: 0.00000813
Iteration 149/1000 | Loss: 0.00000813
Iteration 150/1000 | Loss: 0.00000812
Iteration 151/1000 | Loss: 0.00000812
Iteration 152/1000 | Loss: 0.00000812
Iteration 153/1000 | Loss: 0.00000812
Iteration 154/1000 | Loss: 0.00000811
Iteration 155/1000 | Loss: 0.00000811
Iteration 156/1000 | Loss: 0.00000810
Iteration 157/1000 | Loss: 0.00000810
Iteration 158/1000 | Loss: 0.00000810
Iteration 159/1000 | Loss: 0.00000810
Iteration 160/1000 | Loss: 0.00000810
Iteration 161/1000 | Loss: 0.00000810
Iteration 162/1000 | Loss: 0.00000810
Iteration 163/1000 | Loss: 0.00000810
Iteration 164/1000 | Loss: 0.00000810
Iteration 165/1000 | Loss: 0.00000809
Iteration 166/1000 | Loss: 0.00000809
Iteration 167/1000 | Loss: 0.00000809
Iteration 168/1000 | Loss: 0.00000809
Iteration 169/1000 | Loss: 0.00000809
Iteration 170/1000 | Loss: 0.00000809
Iteration 171/1000 | Loss: 0.00000809
Iteration 172/1000 | Loss: 0.00000809
Iteration 173/1000 | Loss: 0.00000809
Iteration 174/1000 | Loss: 0.00000809
Iteration 175/1000 | Loss: 0.00000809
Iteration 176/1000 | Loss: 0.00000809
Iteration 177/1000 | Loss: 0.00000809
Iteration 178/1000 | Loss: 0.00000808
Iteration 179/1000 | Loss: 0.00000808
Iteration 180/1000 | Loss: 0.00000808
Iteration 181/1000 | Loss: 0.00000808
Iteration 182/1000 | Loss: 0.00000808
Iteration 183/1000 | Loss: 0.00000808
Iteration 184/1000 | Loss: 0.00000808
Iteration 185/1000 | Loss: 0.00000808
Iteration 186/1000 | Loss: 0.00000808
Iteration 187/1000 | Loss: 0.00000808
Iteration 188/1000 | Loss: 0.00000808
Iteration 189/1000 | Loss: 0.00000808
Iteration 190/1000 | Loss: 0.00000808
Iteration 191/1000 | Loss: 0.00000808
Iteration 192/1000 | Loss: 0.00000808
Iteration 193/1000 | Loss: 0.00000808
Iteration 194/1000 | Loss: 0.00000808
Iteration 195/1000 | Loss: 0.00000808
Iteration 196/1000 | Loss: 0.00000808
Iteration 197/1000 | Loss: 0.00000808
Iteration 198/1000 | Loss: 0.00000808
Iteration 199/1000 | Loss: 0.00000808
Iteration 200/1000 | Loss: 0.00000808
Iteration 201/1000 | Loss: 0.00000808
Iteration 202/1000 | Loss: 0.00000807
Iteration 203/1000 | Loss: 0.00000807
Iteration 204/1000 | Loss: 0.00000807
Iteration 205/1000 | Loss: 0.00000807
Iteration 206/1000 | Loss: 0.00000807
Iteration 207/1000 | Loss: 0.00000807
Iteration 208/1000 | Loss: 0.00000807
Iteration 209/1000 | Loss: 0.00000807
Iteration 210/1000 | Loss: 0.00000807
Iteration 211/1000 | Loss: 0.00000807
Iteration 212/1000 | Loss: 0.00000807
Iteration 213/1000 | Loss: 0.00000807
Iteration 214/1000 | Loss: 0.00000806
Iteration 215/1000 | Loss: 0.00000806
Iteration 216/1000 | Loss: 0.00000806
Iteration 217/1000 | Loss: 0.00000806
Iteration 218/1000 | Loss: 0.00000806
Iteration 219/1000 | Loss: 0.00000806
Iteration 220/1000 | Loss: 0.00000806
Iteration 221/1000 | Loss: 0.00000806
Iteration 222/1000 | Loss: 0.00000806
Iteration 223/1000 | Loss: 0.00000806
Iteration 224/1000 | Loss: 0.00000806
Iteration 225/1000 | Loss: 0.00000806
Iteration 226/1000 | Loss: 0.00000806
Iteration 227/1000 | Loss: 0.00000805
Iteration 228/1000 | Loss: 0.00000805
Iteration 229/1000 | Loss: 0.00000805
Iteration 230/1000 | Loss: 0.00000805
Iteration 231/1000 | Loss: 0.00000805
Iteration 232/1000 | Loss: 0.00000805
Iteration 233/1000 | Loss: 0.00000805
Iteration 234/1000 | Loss: 0.00000805
Iteration 235/1000 | Loss: 0.00000805
Iteration 236/1000 | Loss: 0.00000805
Iteration 237/1000 | Loss: 0.00000805
Iteration 238/1000 | Loss: 0.00000805
Iteration 239/1000 | Loss: 0.00000805
Iteration 240/1000 | Loss: 0.00000805
Iteration 241/1000 | Loss: 0.00000805
Iteration 242/1000 | Loss: 0.00000805
Iteration 243/1000 | Loss: 0.00000805
Iteration 244/1000 | Loss: 0.00000805
Iteration 245/1000 | Loss: 0.00000805
Iteration 246/1000 | Loss: 0.00000805
Iteration 247/1000 | Loss: 0.00000805
Iteration 248/1000 | Loss: 0.00000805
Iteration 249/1000 | Loss: 0.00000805
Iteration 250/1000 | Loss: 0.00000805
Iteration 251/1000 | Loss: 0.00000805
Iteration 252/1000 | Loss: 0.00000805
Iteration 253/1000 | Loss: 0.00000805
Iteration 254/1000 | Loss: 0.00000805
Iteration 255/1000 | Loss: 0.00000805
Iteration 256/1000 | Loss: 0.00000805
Iteration 257/1000 | Loss: 0.00000805
Iteration 258/1000 | Loss: 0.00000805
Iteration 259/1000 | Loss: 0.00000805
Iteration 260/1000 | Loss: 0.00000805
Iteration 261/1000 | Loss: 0.00000805
Iteration 262/1000 | Loss: 0.00000805
Iteration 263/1000 | Loss: 0.00000805
Iteration 264/1000 | Loss: 0.00000805
Iteration 265/1000 | Loss: 0.00000805
Iteration 266/1000 | Loss: 0.00000805
Iteration 267/1000 | Loss: 0.00000805
Iteration 268/1000 | Loss: 0.00000805
Iteration 269/1000 | Loss: 0.00000805
Iteration 270/1000 | Loss: 0.00000805
Iteration 271/1000 | Loss: 0.00000805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [8.053973033383954e-06, 8.053973033383954e-06, 8.053973033383954e-06, 8.053973033383954e-06, 8.053973033383954e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.053973033383954e-06

Optimization complete. Final v2v error: 2.462674856185913 mm

Highest mean error: 2.7602932453155518 mm for frame 125

Lowest mean error: 2.305326461791992 mm for frame 152

Saving results

Total time: 42.24721932411194
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607534
Iteration 2/25 | Loss: 0.00105247
Iteration 3/25 | Loss: 0.00097436
Iteration 4/25 | Loss: 0.00096531
Iteration 5/25 | Loss: 0.00096329
Iteration 6/25 | Loss: 0.00096329
Iteration 7/25 | Loss: 0.00096329
Iteration 8/25 | Loss: 0.00096329
Iteration 9/25 | Loss: 0.00096329
Iteration 10/25 | Loss: 0.00096329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009632933652028441, 0.0009632933652028441, 0.0009632933652028441, 0.0009632933652028441, 0.0009632933652028441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009632933652028441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.76448154
Iteration 2/25 | Loss: 0.00065932
Iteration 3/25 | Loss: 0.00065929
Iteration 4/25 | Loss: 0.00065929
Iteration 5/25 | Loss: 0.00065929
Iteration 6/25 | Loss: 0.00065929
Iteration 7/25 | Loss: 0.00065929
Iteration 8/25 | Loss: 0.00065929
Iteration 9/25 | Loss: 0.00065929
Iteration 10/25 | Loss: 0.00065929
Iteration 11/25 | Loss: 0.00065929
Iteration 12/25 | Loss: 0.00065929
Iteration 13/25 | Loss: 0.00065929
Iteration 14/25 | Loss: 0.00065929
Iteration 15/25 | Loss: 0.00065929
Iteration 16/25 | Loss: 0.00065929
Iteration 17/25 | Loss: 0.00065929
Iteration 18/25 | Loss: 0.00065929
Iteration 19/25 | Loss: 0.00065929
Iteration 20/25 | Loss: 0.00065929
Iteration 21/25 | Loss: 0.00065929
Iteration 22/25 | Loss: 0.00065929
Iteration 23/25 | Loss: 0.00065929
Iteration 24/25 | Loss: 0.00065929
Iteration 25/25 | Loss: 0.00065929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065929
Iteration 2/1000 | Loss: 0.00001610
Iteration 3/1000 | Loss: 0.00001199
Iteration 4/1000 | Loss: 0.00001084
Iteration 5/1000 | Loss: 0.00001035
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000946
Iteration 9/1000 | Loss: 0.00000924
Iteration 10/1000 | Loss: 0.00000921
Iteration 11/1000 | Loss: 0.00000918
Iteration 12/1000 | Loss: 0.00000909
Iteration 13/1000 | Loss: 0.00000909
Iteration 14/1000 | Loss: 0.00000905
Iteration 15/1000 | Loss: 0.00000904
Iteration 16/1000 | Loss: 0.00000904
Iteration 17/1000 | Loss: 0.00000902
Iteration 18/1000 | Loss: 0.00000897
Iteration 19/1000 | Loss: 0.00000897
Iteration 20/1000 | Loss: 0.00000897
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000897
Iteration 25/1000 | Loss: 0.00000894
Iteration 26/1000 | Loss: 0.00000893
Iteration 27/1000 | Loss: 0.00000893
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000891
Iteration 30/1000 | Loss: 0.00000890
Iteration 31/1000 | Loss: 0.00000890
Iteration 32/1000 | Loss: 0.00000889
Iteration 33/1000 | Loss: 0.00000889
Iteration 34/1000 | Loss: 0.00000889
Iteration 35/1000 | Loss: 0.00000888
Iteration 36/1000 | Loss: 0.00000888
Iteration 37/1000 | Loss: 0.00000887
Iteration 38/1000 | Loss: 0.00000887
Iteration 39/1000 | Loss: 0.00000887
Iteration 40/1000 | Loss: 0.00000885
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000885
Iteration 43/1000 | Loss: 0.00000884
Iteration 44/1000 | Loss: 0.00000884
Iteration 45/1000 | Loss: 0.00000883
Iteration 46/1000 | Loss: 0.00000883
Iteration 47/1000 | Loss: 0.00000883
Iteration 48/1000 | Loss: 0.00000882
Iteration 49/1000 | Loss: 0.00000882
Iteration 50/1000 | Loss: 0.00000882
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000880
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000880
Iteration 55/1000 | Loss: 0.00000880
Iteration 56/1000 | Loss: 0.00000880
Iteration 57/1000 | Loss: 0.00000879
Iteration 58/1000 | Loss: 0.00000879
Iteration 59/1000 | Loss: 0.00000879
Iteration 60/1000 | Loss: 0.00000879
Iteration 61/1000 | Loss: 0.00000878
Iteration 62/1000 | Loss: 0.00000878
Iteration 63/1000 | Loss: 0.00000878
Iteration 64/1000 | Loss: 0.00000878
Iteration 65/1000 | Loss: 0.00000878
Iteration 66/1000 | Loss: 0.00000878
Iteration 67/1000 | Loss: 0.00000877
Iteration 68/1000 | Loss: 0.00000877
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000876
Iteration 73/1000 | Loss: 0.00000876
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000875
Iteration 76/1000 | Loss: 0.00000875
Iteration 77/1000 | Loss: 0.00000875
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000875
Iteration 80/1000 | Loss: 0.00000875
Iteration 81/1000 | Loss: 0.00000875
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000874
Iteration 84/1000 | Loss: 0.00000873
Iteration 85/1000 | Loss: 0.00000873
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000872
Iteration 88/1000 | Loss: 0.00000872
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000871
Iteration 91/1000 | Loss: 0.00000871
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000870
Iteration 94/1000 | Loss: 0.00000870
Iteration 95/1000 | Loss: 0.00000870
Iteration 96/1000 | Loss: 0.00000870
Iteration 97/1000 | Loss: 0.00000870
Iteration 98/1000 | Loss: 0.00000869
Iteration 99/1000 | Loss: 0.00000869
Iteration 100/1000 | Loss: 0.00000869
Iteration 101/1000 | Loss: 0.00000869
Iteration 102/1000 | Loss: 0.00000869
Iteration 103/1000 | Loss: 0.00000869
Iteration 104/1000 | Loss: 0.00000868
Iteration 105/1000 | Loss: 0.00000868
Iteration 106/1000 | Loss: 0.00000868
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000867
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000867
Iteration 113/1000 | Loss: 0.00000867
Iteration 114/1000 | Loss: 0.00000867
Iteration 115/1000 | Loss: 0.00000867
Iteration 116/1000 | Loss: 0.00000867
Iteration 117/1000 | Loss: 0.00000867
Iteration 118/1000 | Loss: 0.00000867
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000866
Iteration 122/1000 | Loss: 0.00000866
Iteration 123/1000 | Loss: 0.00000866
Iteration 124/1000 | Loss: 0.00000866
Iteration 125/1000 | Loss: 0.00000866
Iteration 126/1000 | Loss: 0.00000866
Iteration 127/1000 | Loss: 0.00000866
Iteration 128/1000 | Loss: 0.00000865
Iteration 129/1000 | Loss: 0.00000865
Iteration 130/1000 | Loss: 0.00000865
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000864
Iteration 140/1000 | Loss: 0.00000864
Iteration 141/1000 | Loss: 0.00000864
Iteration 142/1000 | Loss: 0.00000864
Iteration 143/1000 | Loss: 0.00000864
Iteration 144/1000 | Loss: 0.00000864
Iteration 145/1000 | Loss: 0.00000864
Iteration 146/1000 | Loss: 0.00000864
Iteration 147/1000 | Loss: 0.00000864
Iteration 148/1000 | Loss: 0.00000864
Iteration 149/1000 | Loss: 0.00000864
Iteration 150/1000 | Loss: 0.00000864
Iteration 151/1000 | Loss: 0.00000864
Iteration 152/1000 | Loss: 0.00000864
Iteration 153/1000 | Loss: 0.00000864
Iteration 154/1000 | Loss: 0.00000864
Iteration 155/1000 | Loss: 0.00000864
Iteration 156/1000 | Loss: 0.00000864
Iteration 157/1000 | Loss: 0.00000864
Iteration 158/1000 | Loss: 0.00000864
Iteration 159/1000 | Loss: 0.00000864
Iteration 160/1000 | Loss: 0.00000864
Iteration 161/1000 | Loss: 0.00000864
Iteration 162/1000 | Loss: 0.00000864
Iteration 163/1000 | Loss: 0.00000864
Iteration 164/1000 | Loss: 0.00000864
Iteration 165/1000 | Loss: 0.00000864
Iteration 166/1000 | Loss: 0.00000864
Iteration 167/1000 | Loss: 0.00000864
Iteration 168/1000 | Loss: 0.00000864
Iteration 169/1000 | Loss: 0.00000864
Iteration 170/1000 | Loss: 0.00000864
Iteration 171/1000 | Loss: 0.00000864
Iteration 172/1000 | Loss: 0.00000864
Iteration 173/1000 | Loss: 0.00000864
Iteration 174/1000 | Loss: 0.00000864
Iteration 175/1000 | Loss: 0.00000864
Iteration 176/1000 | Loss: 0.00000864
Iteration 177/1000 | Loss: 0.00000864
Iteration 178/1000 | Loss: 0.00000864
Iteration 179/1000 | Loss: 0.00000864
Iteration 180/1000 | Loss: 0.00000864
Iteration 181/1000 | Loss: 0.00000864
Iteration 182/1000 | Loss: 0.00000864
Iteration 183/1000 | Loss: 0.00000864
Iteration 184/1000 | Loss: 0.00000864
Iteration 185/1000 | Loss: 0.00000864
Iteration 186/1000 | Loss: 0.00000864
Iteration 187/1000 | Loss: 0.00000864
Iteration 188/1000 | Loss: 0.00000864
Iteration 189/1000 | Loss: 0.00000864
Iteration 190/1000 | Loss: 0.00000864
Iteration 191/1000 | Loss: 0.00000864
Iteration 192/1000 | Loss: 0.00000864
Iteration 193/1000 | Loss: 0.00000864
Iteration 194/1000 | Loss: 0.00000864
Iteration 195/1000 | Loss: 0.00000864
Iteration 196/1000 | Loss: 0.00000864
Iteration 197/1000 | Loss: 0.00000864
Iteration 198/1000 | Loss: 0.00000864
Iteration 199/1000 | Loss: 0.00000864
Iteration 200/1000 | Loss: 0.00000864
Iteration 201/1000 | Loss: 0.00000864
Iteration 202/1000 | Loss: 0.00000864
Iteration 203/1000 | Loss: 0.00000864
Iteration 204/1000 | Loss: 0.00000864
Iteration 205/1000 | Loss: 0.00000864
Iteration 206/1000 | Loss: 0.00000864
Iteration 207/1000 | Loss: 0.00000864
Iteration 208/1000 | Loss: 0.00000864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [8.63664536154829e-06, 8.63664536154829e-06, 8.63664536154829e-06, 8.63664536154829e-06, 8.63664536154829e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.63664536154829e-06

Optimization complete. Final v2v error: 2.5265259742736816 mm

Highest mean error: 2.7959210872650146 mm for frame 118

Lowest mean error: 2.296586036682129 mm for frame 18

Saving results

Total time: 39.84387826919556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480923
Iteration 2/25 | Loss: 0.00112045
Iteration 3/25 | Loss: 0.00102779
Iteration 4/25 | Loss: 0.00101222
Iteration 5/25 | Loss: 0.00100717
Iteration 6/25 | Loss: 0.00100570
Iteration 7/25 | Loss: 0.00100526
Iteration 8/25 | Loss: 0.00100526
Iteration 9/25 | Loss: 0.00100526
Iteration 10/25 | Loss: 0.00100526
Iteration 11/25 | Loss: 0.00100526
Iteration 12/25 | Loss: 0.00100526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010052602738142014, 0.0010052602738142014, 0.0010052602738142014, 0.0010052602738142014, 0.0010052602738142014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010052602738142014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30664825
Iteration 2/25 | Loss: 0.00082182
Iteration 3/25 | Loss: 0.00082181
Iteration 4/25 | Loss: 0.00082181
Iteration 5/25 | Loss: 0.00082181
Iteration 6/25 | Loss: 0.00082181
Iteration 7/25 | Loss: 0.00082181
Iteration 8/25 | Loss: 0.00082181
Iteration 9/25 | Loss: 0.00082181
Iteration 10/25 | Loss: 0.00082181
Iteration 11/25 | Loss: 0.00082181
Iteration 12/25 | Loss: 0.00082181
Iteration 13/25 | Loss: 0.00082181
Iteration 14/25 | Loss: 0.00082181
Iteration 15/25 | Loss: 0.00082181
Iteration 16/25 | Loss: 0.00082181
Iteration 17/25 | Loss: 0.00082181
Iteration 18/25 | Loss: 0.00082181
Iteration 19/25 | Loss: 0.00082181
Iteration 20/25 | Loss: 0.00082181
Iteration 21/25 | Loss: 0.00082181
Iteration 22/25 | Loss: 0.00082181
Iteration 23/25 | Loss: 0.00082181
Iteration 24/25 | Loss: 0.00082181
Iteration 25/25 | Loss: 0.00082181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008218106813728809, 0.0008218106813728809, 0.0008218106813728809, 0.0008218106813728809, 0.0008218106813728809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008218106813728809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082181
Iteration 2/1000 | Loss: 0.00003238
Iteration 3/1000 | Loss: 0.00002174
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001776
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001699
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001684
Iteration 16/1000 | Loss: 0.00001683
Iteration 17/1000 | Loss: 0.00001682
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001677
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001651
Iteration 26/1000 | Loss: 0.00001651
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001650
Iteration 29/1000 | Loss: 0.00001650
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001647
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001645
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001641
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001640
Iteration 51/1000 | Loss: 0.00001640
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001639
Iteration 54/1000 | Loss: 0.00001639
Iteration 55/1000 | Loss: 0.00001639
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001637
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001636
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001635
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001634
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001634
Iteration 92/1000 | Loss: 0.00001634
Iteration 93/1000 | Loss: 0.00001634
Iteration 94/1000 | Loss: 0.00001634
Iteration 95/1000 | Loss: 0.00001634
Iteration 96/1000 | Loss: 0.00001634
Iteration 97/1000 | Loss: 0.00001634
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001633
Iteration 100/1000 | Loss: 0.00001633
Iteration 101/1000 | Loss: 0.00001633
Iteration 102/1000 | Loss: 0.00001633
Iteration 103/1000 | Loss: 0.00001633
Iteration 104/1000 | Loss: 0.00001633
Iteration 105/1000 | Loss: 0.00001633
Iteration 106/1000 | Loss: 0.00001633
Iteration 107/1000 | Loss: 0.00001633
Iteration 108/1000 | Loss: 0.00001633
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001632
Iteration 111/1000 | Loss: 0.00001632
Iteration 112/1000 | Loss: 0.00001632
Iteration 113/1000 | Loss: 0.00001632
Iteration 114/1000 | Loss: 0.00001632
Iteration 115/1000 | Loss: 0.00001632
Iteration 116/1000 | Loss: 0.00001632
Iteration 117/1000 | Loss: 0.00001632
Iteration 118/1000 | Loss: 0.00001632
Iteration 119/1000 | Loss: 0.00001632
Iteration 120/1000 | Loss: 0.00001632
Iteration 121/1000 | Loss: 0.00001632
Iteration 122/1000 | Loss: 0.00001632
Iteration 123/1000 | Loss: 0.00001631
Iteration 124/1000 | Loss: 0.00001631
Iteration 125/1000 | Loss: 0.00001631
Iteration 126/1000 | Loss: 0.00001631
Iteration 127/1000 | Loss: 0.00001631
Iteration 128/1000 | Loss: 0.00001631
Iteration 129/1000 | Loss: 0.00001631
Iteration 130/1000 | Loss: 0.00001631
Iteration 131/1000 | Loss: 0.00001631
Iteration 132/1000 | Loss: 0.00001631
Iteration 133/1000 | Loss: 0.00001631
Iteration 134/1000 | Loss: 0.00001630
Iteration 135/1000 | Loss: 0.00001630
Iteration 136/1000 | Loss: 0.00001630
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001630
Iteration 139/1000 | Loss: 0.00001630
Iteration 140/1000 | Loss: 0.00001630
Iteration 141/1000 | Loss: 0.00001630
Iteration 142/1000 | Loss: 0.00001630
Iteration 143/1000 | Loss: 0.00001630
Iteration 144/1000 | Loss: 0.00001629
Iteration 145/1000 | Loss: 0.00001629
Iteration 146/1000 | Loss: 0.00001629
Iteration 147/1000 | Loss: 0.00001629
Iteration 148/1000 | Loss: 0.00001629
Iteration 149/1000 | Loss: 0.00001629
Iteration 150/1000 | Loss: 0.00001629
Iteration 151/1000 | Loss: 0.00001629
Iteration 152/1000 | Loss: 0.00001629
Iteration 153/1000 | Loss: 0.00001629
Iteration 154/1000 | Loss: 0.00001629
Iteration 155/1000 | Loss: 0.00001629
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001629
Iteration 163/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.6290445273625664e-05, 1.6290445273625664e-05, 1.6290445273625664e-05, 1.6290445273625664e-05, 1.6290445273625664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6290445273625664e-05

Optimization complete. Final v2v error: 3.3918588161468506 mm

Highest mean error: 4.063970565795898 mm for frame 17

Lowest mean error: 2.637563467025757 mm for frame 1

Saving results

Total time: 39.962273836135864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004335
Iteration 2/25 | Loss: 0.01004334
Iteration 3/25 | Loss: 0.01004334
Iteration 4/25 | Loss: 0.00263493
Iteration 5/25 | Loss: 0.00221868
Iteration 6/25 | Loss: 0.00193188
Iteration 7/25 | Loss: 0.00179392
Iteration 8/25 | Loss: 0.00157884
Iteration 9/25 | Loss: 0.00152138
Iteration 10/25 | Loss: 0.00148116
Iteration 11/25 | Loss: 0.00139965
Iteration 12/25 | Loss: 0.00140340
Iteration 13/25 | Loss: 0.00136119
Iteration 14/25 | Loss: 0.00134385
Iteration 15/25 | Loss: 0.00132088
Iteration 16/25 | Loss: 0.00128719
Iteration 17/25 | Loss: 0.00126487
Iteration 18/25 | Loss: 0.00125390
Iteration 19/25 | Loss: 0.00125065
Iteration 20/25 | Loss: 0.00124021
Iteration 21/25 | Loss: 0.00123802
Iteration 22/25 | Loss: 0.00123505
Iteration 23/25 | Loss: 0.00123581
Iteration 24/25 | Loss: 0.00122340
Iteration 25/25 | Loss: 0.00122066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67713928
Iteration 2/25 | Loss: 0.00258185
Iteration 3/25 | Loss: 0.00164643
Iteration 4/25 | Loss: 0.00164643
Iteration 5/25 | Loss: 0.00164643
Iteration 6/25 | Loss: 0.00164643
Iteration 7/25 | Loss: 0.00164643
Iteration 8/25 | Loss: 0.00164643
Iteration 9/25 | Loss: 0.00164643
Iteration 10/25 | Loss: 0.00164643
Iteration 11/25 | Loss: 0.00164643
Iteration 12/25 | Loss: 0.00164643
Iteration 13/25 | Loss: 0.00164643
Iteration 14/25 | Loss: 0.00164643
Iteration 15/25 | Loss: 0.00164643
Iteration 16/25 | Loss: 0.00164643
Iteration 17/25 | Loss: 0.00164643
Iteration 18/25 | Loss: 0.00164643
Iteration 19/25 | Loss: 0.00164643
Iteration 20/25 | Loss: 0.00164643
Iteration 21/25 | Loss: 0.00164643
Iteration 22/25 | Loss: 0.00164643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016464290674775839, 0.0016464290674775839, 0.0016464290674775839, 0.0016464290674775839, 0.0016464290674775839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016464290674775839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164643
Iteration 2/1000 | Loss: 0.00090306
Iteration 3/1000 | Loss: 0.00020612
Iteration 4/1000 | Loss: 0.00011669
Iteration 5/1000 | Loss: 0.00022426
Iteration 6/1000 | Loss: 0.00020129
Iteration 7/1000 | Loss: 0.00056720
Iteration 8/1000 | Loss: 0.00011064
Iteration 9/1000 | Loss: 0.00052748
Iteration 10/1000 | Loss: 0.00071996
Iteration 11/1000 | Loss: 0.00011276
Iteration 12/1000 | Loss: 0.00041016
Iteration 13/1000 | Loss: 0.00014513
Iteration 14/1000 | Loss: 0.00063848
Iteration 15/1000 | Loss: 0.00096375
Iteration 16/1000 | Loss: 0.00027018
Iteration 17/1000 | Loss: 0.00043649
Iteration 18/1000 | Loss: 0.00037244
Iteration 19/1000 | Loss: 0.00030380
Iteration 20/1000 | Loss: 0.00020026
Iteration 21/1000 | Loss: 0.00011551
Iteration 22/1000 | Loss: 0.00034166
Iteration 23/1000 | Loss: 0.00009964
Iteration 24/1000 | Loss: 0.00008993
Iteration 25/1000 | Loss: 0.00008637
Iteration 26/1000 | Loss: 0.00009699
Iteration 27/1000 | Loss: 0.00051148
Iteration 28/1000 | Loss: 0.00012286
Iteration 29/1000 | Loss: 0.00009574
Iteration 30/1000 | Loss: 0.00008410
Iteration 31/1000 | Loss: 0.00047826
Iteration 32/1000 | Loss: 0.00024600
Iteration 33/1000 | Loss: 0.00031211
Iteration 34/1000 | Loss: 0.00034488
Iteration 35/1000 | Loss: 0.00019272
Iteration 36/1000 | Loss: 0.00015587
Iteration 37/1000 | Loss: 0.00009984
Iteration 38/1000 | Loss: 0.00008229
Iteration 39/1000 | Loss: 0.00007986
Iteration 40/1000 | Loss: 0.00009990
Iteration 41/1000 | Loss: 0.00027162
Iteration 42/1000 | Loss: 0.00008021
Iteration 43/1000 | Loss: 0.00007870
Iteration 44/1000 | Loss: 0.00020778
Iteration 45/1000 | Loss: 0.00010508
Iteration 46/1000 | Loss: 0.00007838
Iteration 47/1000 | Loss: 0.00007613
Iteration 48/1000 | Loss: 0.00008981
Iteration 49/1000 | Loss: 0.00025720
Iteration 50/1000 | Loss: 0.00008243
Iteration 51/1000 | Loss: 0.00015585
Iteration 52/1000 | Loss: 0.00021611
Iteration 53/1000 | Loss: 0.00007536
Iteration 54/1000 | Loss: 0.00013641
Iteration 55/1000 | Loss: 0.00011685
Iteration 56/1000 | Loss: 0.00007818
Iteration 57/1000 | Loss: 0.00014632
Iteration 58/1000 | Loss: 0.00015977
Iteration 59/1000 | Loss: 0.00012716
Iteration 60/1000 | Loss: 0.00007527
Iteration 61/1000 | Loss: 0.00010695
Iteration 62/1000 | Loss: 0.00008505
Iteration 63/1000 | Loss: 0.00008504
Iteration 64/1000 | Loss: 0.00012036
Iteration 65/1000 | Loss: 0.00010060
Iteration 66/1000 | Loss: 0.00019475
Iteration 67/1000 | Loss: 0.00015617
Iteration 68/1000 | Loss: 0.00008093
Iteration 69/1000 | Loss: 0.00008464
Iteration 70/1000 | Loss: 0.00008166
Iteration 71/1000 | Loss: 0.00008583
Iteration 72/1000 | Loss: 0.00011392
Iteration 73/1000 | Loss: 0.00009250
Iteration 74/1000 | Loss: 0.00008554
Iteration 75/1000 | Loss: 0.00008437
Iteration 76/1000 | Loss: 0.00008335
Iteration 77/1000 | Loss: 0.00008848
Iteration 78/1000 | Loss: 0.00012453
Iteration 79/1000 | Loss: 0.00009665
Iteration 80/1000 | Loss: 0.00010361
Iteration 81/1000 | Loss: 0.00008897
Iteration 82/1000 | Loss: 0.00008410
Iteration 83/1000 | Loss: 0.00009260
Iteration 84/1000 | Loss: 0.00010047
Iteration 85/1000 | Loss: 0.00009088
Iteration 86/1000 | Loss: 0.00008554
Iteration 87/1000 | Loss: 0.00007676
Iteration 88/1000 | Loss: 0.00015258
Iteration 89/1000 | Loss: 0.00052634
Iteration 90/1000 | Loss: 0.00045510
Iteration 91/1000 | Loss: 0.00007502
Iteration 92/1000 | Loss: 0.00007328
Iteration 93/1000 | Loss: 0.00007257
Iteration 94/1000 | Loss: 0.00007228
Iteration 95/1000 | Loss: 0.00007224
Iteration 96/1000 | Loss: 0.00013035
Iteration 97/1000 | Loss: 0.00007202
Iteration 98/1000 | Loss: 0.00013302
Iteration 99/1000 | Loss: 0.00007299
Iteration 100/1000 | Loss: 0.00008000
Iteration 101/1000 | Loss: 0.00007166
Iteration 102/1000 | Loss: 0.00021455
Iteration 103/1000 | Loss: 0.00042534
Iteration 104/1000 | Loss: 0.00007337
Iteration 105/1000 | Loss: 0.00007189
Iteration 106/1000 | Loss: 0.00007092
Iteration 107/1000 | Loss: 0.00007059
Iteration 108/1000 | Loss: 0.00007030
Iteration 109/1000 | Loss: 0.00013333
Iteration 110/1000 | Loss: 0.00015408
Iteration 111/1000 | Loss: 0.00056080
Iteration 112/1000 | Loss: 0.00007198
Iteration 113/1000 | Loss: 0.00010761
Iteration 114/1000 | Loss: 0.00007015
Iteration 115/1000 | Loss: 0.00006989
Iteration 116/1000 | Loss: 0.00006971
Iteration 117/1000 | Loss: 0.00017173
Iteration 118/1000 | Loss: 0.00011203
Iteration 119/1000 | Loss: 0.00007490
Iteration 120/1000 | Loss: 0.00006974
Iteration 121/1000 | Loss: 0.00012185
Iteration 122/1000 | Loss: 0.00020356
Iteration 123/1000 | Loss: 0.00008275
Iteration 124/1000 | Loss: 0.00006946
Iteration 125/1000 | Loss: 0.00006913
Iteration 126/1000 | Loss: 0.00022446
Iteration 127/1000 | Loss: 0.00007340
Iteration 128/1000 | Loss: 0.00007593
Iteration 129/1000 | Loss: 0.00006890
Iteration 130/1000 | Loss: 0.00006860
Iteration 131/1000 | Loss: 0.00006837
Iteration 132/1000 | Loss: 0.00006822
Iteration 133/1000 | Loss: 0.00006820
Iteration 134/1000 | Loss: 0.00006811
Iteration 135/1000 | Loss: 0.00006802
Iteration 136/1000 | Loss: 0.00006786
Iteration 137/1000 | Loss: 0.00006785
Iteration 138/1000 | Loss: 0.00006779
Iteration 139/1000 | Loss: 0.00006779
Iteration 140/1000 | Loss: 0.00006775
Iteration 141/1000 | Loss: 0.00006774
Iteration 142/1000 | Loss: 0.00006774
Iteration 143/1000 | Loss: 0.00006774
Iteration 144/1000 | Loss: 0.00006773
Iteration 145/1000 | Loss: 0.00006773
Iteration 146/1000 | Loss: 0.00006772
Iteration 147/1000 | Loss: 0.00006770
Iteration 148/1000 | Loss: 0.00006770
Iteration 149/1000 | Loss: 0.00006767
Iteration 150/1000 | Loss: 0.00006766
Iteration 151/1000 | Loss: 0.00006765
Iteration 152/1000 | Loss: 0.00006765
Iteration 153/1000 | Loss: 0.00006764
Iteration 154/1000 | Loss: 0.00006764
Iteration 155/1000 | Loss: 0.00016373
Iteration 156/1000 | Loss: 0.00006896
Iteration 157/1000 | Loss: 0.00006770
Iteration 158/1000 | Loss: 0.00006748
Iteration 159/1000 | Loss: 0.00006748
Iteration 160/1000 | Loss: 0.00006747
Iteration 161/1000 | Loss: 0.00006747
Iteration 162/1000 | Loss: 0.00006746
Iteration 163/1000 | Loss: 0.00006746
Iteration 164/1000 | Loss: 0.00006745
Iteration 165/1000 | Loss: 0.00006745
Iteration 166/1000 | Loss: 0.00006745
Iteration 167/1000 | Loss: 0.00006745
Iteration 168/1000 | Loss: 0.00006744
Iteration 169/1000 | Loss: 0.00006744
Iteration 170/1000 | Loss: 0.00006744
Iteration 171/1000 | Loss: 0.00006744
Iteration 172/1000 | Loss: 0.00006744
Iteration 173/1000 | Loss: 0.00006743
Iteration 174/1000 | Loss: 0.00006743
Iteration 175/1000 | Loss: 0.00006743
Iteration 176/1000 | Loss: 0.00006743
Iteration 177/1000 | Loss: 0.00006742
Iteration 178/1000 | Loss: 0.00006742
Iteration 179/1000 | Loss: 0.00006742
Iteration 180/1000 | Loss: 0.00006741
Iteration 181/1000 | Loss: 0.00006741
Iteration 182/1000 | Loss: 0.00006741
Iteration 183/1000 | Loss: 0.00006741
Iteration 184/1000 | Loss: 0.00006740
Iteration 185/1000 | Loss: 0.00006740
Iteration 186/1000 | Loss: 0.00006740
Iteration 187/1000 | Loss: 0.00006739
Iteration 188/1000 | Loss: 0.00006739
Iteration 189/1000 | Loss: 0.00006738
Iteration 190/1000 | Loss: 0.00006738
Iteration 191/1000 | Loss: 0.00006738
Iteration 192/1000 | Loss: 0.00006738
Iteration 193/1000 | Loss: 0.00006737
Iteration 194/1000 | Loss: 0.00006737
Iteration 195/1000 | Loss: 0.00006737
Iteration 196/1000 | Loss: 0.00006737
Iteration 197/1000 | Loss: 0.00006736
Iteration 198/1000 | Loss: 0.00006736
Iteration 199/1000 | Loss: 0.00006736
Iteration 200/1000 | Loss: 0.00006736
Iteration 201/1000 | Loss: 0.00006736
Iteration 202/1000 | Loss: 0.00006736
Iteration 203/1000 | Loss: 0.00006736
Iteration 204/1000 | Loss: 0.00006735
Iteration 205/1000 | Loss: 0.00006735
Iteration 206/1000 | Loss: 0.00006735
Iteration 207/1000 | Loss: 0.00006735
Iteration 208/1000 | Loss: 0.00006735
Iteration 209/1000 | Loss: 0.00006735
Iteration 210/1000 | Loss: 0.00006735
Iteration 211/1000 | Loss: 0.00006735
Iteration 212/1000 | Loss: 0.00006735
Iteration 213/1000 | Loss: 0.00006734
Iteration 214/1000 | Loss: 0.00006734
Iteration 215/1000 | Loss: 0.00006734
Iteration 216/1000 | Loss: 0.00006734
Iteration 217/1000 | Loss: 0.00006734
Iteration 218/1000 | Loss: 0.00006734
Iteration 219/1000 | Loss: 0.00006734
Iteration 220/1000 | Loss: 0.00006734
Iteration 221/1000 | Loss: 0.00006734
Iteration 222/1000 | Loss: 0.00006734
Iteration 223/1000 | Loss: 0.00006734
Iteration 224/1000 | Loss: 0.00006733
Iteration 225/1000 | Loss: 0.00006733
Iteration 226/1000 | Loss: 0.00006733
Iteration 227/1000 | Loss: 0.00006732
Iteration 228/1000 | Loss: 0.00006732
Iteration 229/1000 | Loss: 0.00006732
Iteration 230/1000 | Loss: 0.00006732
Iteration 231/1000 | Loss: 0.00006732
Iteration 232/1000 | Loss: 0.00006732
Iteration 233/1000 | Loss: 0.00006731
Iteration 234/1000 | Loss: 0.00006731
Iteration 235/1000 | Loss: 0.00006731
Iteration 236/1000 | Loss: 0.00006730
Iteration 237/1000 | Loss: 0.00006730
Iteration 238/1000 | Loss: 0.00006730
Iteration 239/1000 | Loss: 0.00006729
Iteration 240/1000 | Loss: 0.00006727
Iteration 241/1000 | Loss: 0.00006726
Iteration 242/1000 | Loss: 0.00006726
Iteration 243/1000 | Loss: 0.00006726
Iteration 244/1000 | Loss: 0.00006726
Iteration 245/1000 | Loss: 0.00006725
Iteration 246/1000 | Loss: 0.00006725
Iteration 247/1000 | Loss: 0.00006724
Iteration 248/1000 | Loss: 0.00006723
Iteration 249/1000 | Loss: 0.00006723
Iteration 250/1000 | Loss: 0.00006722
Iteration 251/1000 | Loss: 0.00006722
Iteration 252/1000 | Loss: 0.00006722
Iteration 253/1000 | Loss: 0.00006722
Iteration 254/1000 | Loss: 0.00006721
Iteration 255/1000 | Loss: 0.00006721
Iteration 256/1000 | Loss: 0.00006721
Iteration 257/1000 | Loss: 0.00006721
Iteration 258/1000 | Loss: 0.00006721
Iteration 259/1000 | Loss: 0.00006720
Iteration 260/1000 | Loss: 0.00006720
Iteration 261/1000 | Loss: 0.00006720
Iteration 262/1000 | Loss: 0.00006720
Iteration 263/1000 | Loss: 0.00006720
Iteration 264/1000 | Loss: 0.00006719
Iteration 265/1000 | Loss: 0.00006719
Iteration 266/1000 | Loss: 0.00006718
Iteration 267/1000 | Loss: 0.00006717
Iteration 268/1000 | Loss: 0.00006717
Iteration 269/1000 | Loss: 0.00006717
Iteration 270/1000 | Loss: 0.00006717
Iteration 271/1000 | Loss: 0.00006716
Iteration 272/1000 | Loss: 0.00006716
Iteration 273/1000 | Loss: 0.00006716
Iteration 274/1000 | Loss: 0.00006716
Iteration 275/1000 | Loss: 0.00006715
Iteration 276/1000 | Loss: 0.00006715
Iteration 277/1000 | Loss: 0.00006715
Iteration 278/1000 | Loss: 0.00006715
Iteration 279/1000 | Loss: 0.00006715
Iteration 280/1000 | Loss: 0.00006714
Iteration 281/1000 | Loss: 0.00006714
Iteration 282/1000 | Loss: 0.00006714
Iteration 283/1000 | Loss: 0.00006714
Iteration 284/1000 | Loss: 0.00006714
Iteration 285/1000 | Loss: 0.00006714
Iteration 286/1000 | Loss: 0.00006714
Iteration 287/1000 | Loss: 0.00006714
Iteration 288/1000 | Loss: 0.00006713
Iteration 289/1000 | Loss: 0.00006713
Iteration 290/1000 | Loss: 0.00006713
Iteration 291/1000 | Loss: 0.00006713
Iteration 292/1000 | Loss: 0.00006712
Iteration 293/1000 | Loss: 0.00006712
Iteration 294/1000 | Loss: 0.00006712
Iteration 295/1000 | Loss: 0.00006712
Iteration 296/1000 | Loss: 0.00006712
Iteration 297/1000 | Loss: 0.00006711
Iteration 298/1000 | Loss: 0.00006711
Iteration 299/1000 | Loss: 0.00006711
Iteration 300/1000 | Loss: 0.00006711
Iteration 301/1000 | Loss: 0.00006710
Iteration 302/1000 | Loss: 0.00006710
Iteration 303/1000 | Loss: 0.00006710
Iteration 304/1000 | Loss: 0.00006710
Iteration 305/1000 | Loss: 0.00006710
Iteration 306/1000 | Loss: 0.00006710
Iteration 307/1000 | Loss: 0.00006710
Iteration 308/1000 | Loss: 0.00006710
Iteration 309/1000 | Loss: 0.00006710
Iteration 310/1000 | Loss: 0.00006710
Iteration 311/1000 | Loss: 0.00006710
Iteration 312/1000 | Loss: 0.00006710
Iteration 313/1000 | Loss: 0.00006710
Iteration 314/1000 | Loss: 0.00006710
Iteration 315/1000 | Loss: 0.00006709
Iteration 316/1000 | Loss: 0.00006709
Iteration 317/1000 | Loss: 0.00006709
Iteration 318/1000 | Loss: 0.00006709
Iteration 319/1000 | Loss: 0.00006709
Iteration 320/1000 | Loss: 0.00006709
Iteration 321/1000 | Loss: 0.00006709
Iteration 322/1000 | Loss: 0.00006709
Iteration 323/1000 | Loss: 0.00006709
Iteration 324/1000 | Loss: 0.00006709
Iteration 325/1000 | Loss: 0.00006709
Iteration 326/1000 | Loss: 0.00006708
Iteration 327/1000 | Loss: 0.00006708
Iteration 328/1000 | Loss: 0.00006708
Iteration 329/1000 | Loss: 0.00006708
Iteration 330/1000 | Loss: 0.00006708
Iteration 331/1000 | Loss: 0.00006708
Iteration 332/1000 | Loss: 0.00006708
Iteration 333/1000 | Loss: 0.00006708
Iteration 334/1000 | Loss: 0.00006708
Iteration 335/1000 | Loss: 0.00006708
Iteration 336/1000 | Loss: 0.00006708
Iteration 337/1000 | Loss: 0.00006708
Iteration 338/1000 | Loss: 0.00006708
Iteration 339/1000 | Loss: 0.00006708
Iteration 340/1000 | Loss: 0.00006708
Iteration 341/1000 | Loss: 0.00006707
Iteration 342/1000 | Loss: 0.00006707
Iteration 343/1000 | Loss: 0.00006707
Iteration 344/1000 | Loss: 0.00006707
Iteration 345/1000 | Loss: 0.00006707
Iteration 346/1000 | Loss: 0.00006707
Iteration 347/1000 | Loss: 0.00006707
Iteration 348/1000 | Loss: 0.00006707
Iteration 349/1000 | Loss: 0.00006707
Iteration 350/1000 | Loss: 0.00006707
Iteration 351/1000 | Loss: 0.00006706
Iteration 352/1000 | Loss: 0.00006706
Iteration 353/1000 | Loss: 0.00006706
Iteration 354/1000 | Loss: 0.00006706
Iteration 355/1000 | Loss: 0.00006706
Iteration 356/1000 | Loss: 0.00006706
Iteration 357/1000 | Loss: 0.00006706
Iteration 358/1000 | Loss: 0.00006706
Iteration 359/1000 | Loss: 0.00006706
Iteration 360/1000 | Loss: 0.00006706
Iteration 361/1000 | Loss: 0.00006706
Iteration 362/1000 | Loss: 0.00006706
Iteration 363/1000 | Loss: 0.00006706
Iteration 364/1000 | Loss: 0.00006706
Iteration 365/1000 | Loss: 0.00006706
Iteration 366/1000 | Loss: 0.00006705
Iteration 367/1000 | Loss: 0.00006705
Iteration 368/1000 | Loss: 0.00006705
Iteration 369/1000 | Loss: 0.00006705
Iteration 370/1000 | Loss: 0.00006705
Iteration 371/1000 | Loss: 0.00006705
Iteration 372/1000 | Loss: 0.00006705
Iteration 373/1000 | Loss: 0.00006705
Iteration 374/1000 | Loss: 0.00006705
Iteration 375/1000 | Loss: 0.00006705
Iteration 376/1000 | Loss: 0.00006705
Iteration 377/1000 | Loss: 0.00006705
Iteration 378/1000 | Loss: 0.00006705
Iteration 379/1000 | Loss: 0.00006705
Iteration 380/1000 | Loss: 0.00006705
Iteration 381/1000 | Loss: 0.00006705
Iteration 382/1000 | Loss: 0.00006705
Iteration 383/1000 | Loss: 0.00006705
Iteration 384/1000 | Loss: 0.00006705
Iteration 385/1000 | Loss: 0.00006705
Iteration 386/1000 | Loss: 0.00006705
Iteration 387/1000 | Loss: 0.00006705
Iteration 388/1000 | Loss: 0.00006705
Iteration 389/1000 | Loss: 0.00006705
Iteration 390/1000 | Loss: 0.00006705
Iteration 391/1000 | Loss: 0.00006705
Iteration 392/1000 | Loss: 0.00006705
Iteration 393/1000 | Loss: 0.00006705
Iteration 394/1000 | Loss: 0.00006705
Iteration 395/1000 | Loss: 0.00006705
Iteration 396/1000 | Loss: 0.00006705
Iteration 397/1000 | Loss: 0.00006705
Iteration 398/1000 | Loss: 0.00006705
Iteration 399/1000 | Loss: 0.00006705
Iteration 400/1000 | Loss: 0.00006705
Iteration 401/1000 | Loss: 0.00006705
Iteration 402/1000 | Loss: 0.00006705
Iteration 403/1000 | Loss: 0.00006705
Iteration 404/1000 | Loss: 0.00006705
Iteration 405/1000 | Loss: 0.00006705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [6.70537119731307e-05, 6.70537119731307e-05, 6.70537119731307e-05, 6.70537119731307e-05, 6.70537119731307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.70537119731307e-05

Optimization complete. Final v2v error: 4.5936598777771 mm

Highest mean error: 12.056306838989258 mm for frame 17

Lowest mean error: 2.8753864765167236 mm for frame 43

Saving results

Total time: 293.8267114162445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079962
Iteration 2/25 | Loss: 0.01079962
Iteration 3/25 | Loss: 0.00189655
Iteration 4/25 | Loss: 0.00136673
Iteration 5/25 | Loss: 0.00117698
Iteration 6/25 | Loss: 0.00114891
Iteration 7/25 | Loss: 0.00110010
Iteration 8/25 | Loss: 0.00108510
Iteration 9/25 | Loss: 0.00104878
Iteration 10/25 | Loss: 0.00102403
Iteration 11/25 | Loss: 0.00100843
Iteration 12/25 | Loss: 0.00100033
Iteration 13/25 | Loss: 0.00099942
Iteration 14/25 | Loss: 0.00099910
Iteration 15/25 | Loss: 0.00099928
Iteration 16/25 | Loss: 0.00099896
Iteration 17/25 | Loss: 0.00099885
Iteration 18/25 | Loss: 0.00099885
Iteration 19/25 | Loss: 0.00099885
Iteration 20/25 | Loss: 0.00099884
Iteration 21/25 | Loss: 0.00099884
Iteration 22/25 | Loss: 0.00099884
Iteration 23/25 | Loss: 0.00099884
Iteration 24/25 | Loss: 0.00099884
Iteration 25/25 | Loss: 0.00099884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.54512835
Iteration 2/25 | Loss: 0.00071736
Iteration 3/25 | Loss: 0.00071736
Iteration 4/25 | Loss: 0.00071736
Iteration 5/25 | Loss: 0.00071736
Iteration 6/25 | Loss: 0.00071736
Iteration 7/25 | Loss: 0.00071736
Iteration 8/25 | Loss: 0.00071736
Iteration 9/25 | Loss: 0.00071736
Iteration 10/25 | Loss: 0.00071736
Iteration 11/25 | Loss: 0.00071736
Iteration 12/25 | Loss: 0.00071736
Iteration 13/25 | Loss: 0.00071736
Iteration 14/25 | Loss: 0.00071736
Iteration 15/25 | Loss: 0.00071736
Iteration 16/25 | Loss: 0.00071736
Iteration 17/25 | Loss: 0.00071736
Iteration 18/25 | Loss: 0.00071736
Iteration 19/25 | Loss: 0.00071736
Iteration 20/25 | Loss: 0.00071736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007173554040491581, 0.0007173554040491581, 0.0007173554040491581, 0.0007173554040491581, 0.0007173554040491581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007173554040491581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071736
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00009538
Iteration 4/1000 | Loss: 0.00002021
Iteration 5/1000 | Loss: 0.00006729
Iteration 6/1000 | Loss: 0.00002479
Iteration 7/1000 | Loss: 0.00001953
Iteration 8/1000 | Loss: 0.00001812
Iteration 9/1000 | Loss: 0.00008423
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001717
Iteration 12/1000 | Loss: 0.00014643
Iteration 13/1000 | Loss: 0.00021494
Iteration 14/1000 | Loss: 0.00005494
Iteration 15/1000 | Loss: 0.00004704
Iteration 16/1000 | Loss: 0.00002092
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001671
Iteration 20/1000 | Loss: 0.00001671
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001666
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001665
Iteration 29/1000 | Loss: 0.00001664
Iteration 30/1000 | Loss: 0.00001664
Iteration 31/1000 | Loss: 0.00001663
Iteration 32/1000 | Loss: 0.00001663
Iteration 33/1000 | Loss: 0.00001662
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00001659
Iteration 43/1000 | Loss: 0.00001659
Iteration 44/1000 | Loss: 0.00001659
Iteration 45/1000 | Loss: 0.00001659
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001651
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001641
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001633
Iteration 64/1000 | Loss: 0.00001633
Iteration 65/1000 | Loss: 0.00001633
Iteration 66/1000 | Loss: 0.00001633
Iteration 67/1000 | Loss: 0.00001633
Iteration 68/1000 | Loss: 0.00001633
Iteration 69/1000 | Loss: 0.00001633
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001630
Iteration 76/1000 | Loss: 0.00001629
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001629
Iteration 79/1000 | Loss: 0.00001629
Iteration 80/1000 | Loss: 0.00001629
Iteration 81/1000 | Loss: 0.00001629
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001626
Iteration 86/1000 | Loss: 0.00001626
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001625
Iteration 90/1000 | Loss: 0.00001625
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001624
Iteration 93/1000 | Loss: 0.00001624
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001621
Iteration 99/1000 | Loss: 0.00001621
Iteration 100/1000 | Loss: 0.00001621
Iteration 101/1000 | Loss: 0.00001621
Iteration 102/1000 | Loss: 0.00001620
Iteration 103/1000 | Loss: 0.00001620
Iteration 104/1000 | Loss: 0.00001620
Iteration 105/1000 | Loss: 0.00001620
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001619
Iteration 113/1000 | Loss: 0.00001619
Iteration 114/1000 | Loss: 0.00001619
Iteration 115/1000 | Loss: 0.00001619
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.6178710211534053e-05, 1.6178710211534053e-05, 1.6178710211534053e-05, 1.6178710211534053e-05, 1.6178710211534053e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6178710211534053e-05

Optimization complete. Final v2v error: 3.101559638977051 mm

Highest mean error: 19.88823127746582 mm for frame 159

Lowest mean error: 2.67488694190979 mm for frame 13

Saving results

Total time: 74.80716323852539
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811948
Iteration 2/25 | Loss: 0.00118187
Iteration 3/25 | Loss: 0.00102787
Iteration 4/25 | Loss: 0.00101020
Iteration 5/25 | Loss: 0.00100482
Iteration 6/25 | Loss: 0.00100369
Iteration 7/25 | Loss: 0.00100369
Iteration 8/25 | Loss: 0.00100369
Iteration 9/25 | Loss: 0.00100369
Iteration 10/25 | Loss: 0.00100369
Iteration 11/25 | Loss: 0.00100369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010036866879090667, 0.0010036866879090667, 0.0010036866879090667, 0.0010036866879090667, 0.0010036866879090667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010036866879090667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26126206
Iteration 2/25 | Loss: 0.00054654
Iteration 3/25 | Loss: 0.00054650
Iteration 4/25 | Loss: 0.00054650
Iteration 5/25 | Loss: 0.00054650
Iteration 6/25 | Loss: 0.00054650
Iteration 7/25 | Loss: 0.00054650
Iteration 8/25 | Loss: 0.00054650
Iteration 9/25 | Loss: 0.00054650
Iteration 10/25 | Loss: 0.00054650
Iteration 11/25 | Loss: 0.00054650
Iteration 12/25 | Loss: 0.00054650
Iteration 13/25 | Loss: 0.00054650
Iteration 14/25 | Loss: 0.00054650
Iteration 15/25 | Loss: 0.00054650
Iteration 16/25 | Loss: 0.00054650
Iteration 17/25 | Loss: 0.00054650
Iteration 18/25 | Loss: 0.00054650
Iteration 19/25 | Loss: 0.00054650
Iteration 20/25 | Loss: 0.00054650
Iteration 21/25 | Loss: 0.00054650
Iteration 22/25 | Loss: 0.00054650
Iteration 23/25 | Loss: 0.00054650
Iteration 24/25 | Loss: 0.00054650
Iteration 25/25 | Loss: 0.00054650

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054650
Iteration 2/1000 | Loss: 0.00002977
Iteration 3/1000 | Loss: 0.00002064
Iteration 4/1000 | Loss: 0.00001814
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001605
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001434
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001393
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001373
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001363
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001358
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001352
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001351
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001348
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001344
Iteration 109/1000 | Loss: 0.00001344
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001344
Iteration 114/1000 | Loss: 0.00001344
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.3436382687359583e-05, 1.3436382687359583e-05, 1.3436382687359583e-05, 1.3436382687359583e-05, 1.3436382687359583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3436382687359583e-05

Optimization complete. Final v2v error: 3.0381243228912354 mm

Highest mean error: 3.7444729804992676 mm for frame 61

Lowest mean error: 2.718592882156372 mm for frame 198

Saving results

Total time: 41.41562294960022
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830636
Iteration 2/25 | Loss: 0.00109355
Iteration 3/25 | Loss: 0.00098800
Iteration 4/25 | Loss: 0.00097361
Iteration 5/25 | Loss: 0.00096934
Iteration 6/25 | Loss: 0.00096833
Iteration 7/25 | Loss: 0.00096833
Iteration 8/25 | Loss: 0.00096833
Iteration 9/25 | Loss: 0.00096833
Iteration 10/25 | Loss: 0.00096833
Iteration 11/25 | Loss: 0.00096833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009683321695774794, 0.0009683321695774794, 0.0009683321695774794, 0.0009683321695774794, 0.0009683321695774794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009683321695774794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30859447
Iteration 2/25 | Loss: 0.00074796
Iteration 3/25 | Loss: 0.00074796
Iteration 4/25 | Loss: 0.00074796
Iteration 5/25 | Loss: 0.00074796
Iteration 6/25 | Loss: 0.00074795
Iteration 7/25 | Loss: 0.00074795
Iteration 8/25 | Loss: 0.00074795
Iteration 9/25 | Loss: 0.00074795
Iteration 10/25 | Loss: 0.00074795
Iteration 11/25 | Loss: 0.00074795
Iteration 12/25 | Loss: 0.00074795
Iteration 13/25 | Loss: 0.00074795
Iteration 14/25 | Loss: 0.00074795
Iteration 15/25 | Loss: 0.00074795
Iteration 16/25 | Loss: 0.00074795
Iteration 17/25 | Loss: 0.00074795
Iteration 18/25 | Loss: 0.00074795
Iteration 19/25 | Loss: 0.00074795
Iteration 20/25 | Loss: 0.00074795
Iteration 21/25 | Loss: 0.00074795
Iteration 22/25 | Loss: 0.00074795
Iteration 23/25 | Loss: 0.00074795
Iteration 24/25 | Loss: 0.00074795
Iteration 25/25 | Loss: 0.00074795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074795
Iteration 2/1000 | Loss: 0.00001959
Iteration 3/1000 | Loss: 0.00001262
Iteration 4/1000 | Loss: 0.00001136
Iteration 5/1000 | Loss: 0.00001067
Iteration 6/1000 | Loss: 0.00001036
Iteration 7/1000 | Loss: 0.00001023
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00001009
Iteration 10/1000 | Loss: 0.00001005
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00001001
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000987
Iteration 15/1000 | Loss: 0.00000986
Iteration 16/1000 | Loss: 0.00000986
Iteration 17/1000 | Loss: 0.00000981
Iteration 18/1000 | Loss: 0.00000979
Iteration 19/1000 | Loss: 0.00000978
Iteration 20/1000 | Loss: 0.00000978
Iteration 21/1000 | Loss: 0.00000975
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000973
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000972
Iteration 27/1000 | Loss: 0.00000971
Iteration 28/1000 | Loss: 0.00000971
Iteration 29/1000 | Loss: 0.00000970
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000969
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000967
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000966
Iteration 40/1000 | Loss: 0.00000965
Iteration 41/1000 | Loss: 0.00000965
Iteration 42/1000 | Loss: 0.00000964
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000961
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000960
Iteration 50/1000 | Loss: 0.00000960
Iteration 51/1000 | Loss: 0.00000960
Iteration 52/1000 | Loss: 0.00000960
Iteration 53/1000 | Loss: 0.00000960
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000960
Iteration 56/1000 | Loss: 0.00000960
Iteration 57/1000 | Loss: 0.00000959
Iteration 58/1000 | Loss: 0.00000959
Iteration 59/1000 | Loss: 0.00000959
Iteration 60/1000 | Loss: 0.00000959
Iteration 61/1000 | Loss: 0.00000959
Iteration 62/1000 | Loss: 0.00000958
Iteration 63/1000 | Loss: 0.00000958
Iteration 64/1000 | Loss: 0.00000958
Iteration 65/1000 | Loss: 0.00000957
Iteration 66/1000 | Loss: 0.00000957
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000955
Iteration 72/1000 | Loss: 0.00000955
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000954
Iteration 76/1000 | Loss: 0.00000954
Iteration 77/1000 | Loss: 0.00000953
Iteration 78/1000 | Loss: 0.00000953
Iteration 79/1000 | Loss: 0.00000953
Iteration 80/1000 | Loss: 0.00000953
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000952
Iteration 83/1000 | Loss: 0.00000951
Iteration 84/1000 | Loss: 0.00000951
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000951
Iteration 89/1000 | Loss: 0.00000950
Iteration 90/1000 | Loss: 0.00000950
Iteration 91/1000 | Loss: 0.00000950
Iteration 92/1000 | Loss: 0.00000950
Iteration 93/1000 | Loss: 0.00000950
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000948
Iteration 107/1000 | Loss: 0.00000948
Iteration 108/1000 | Loss: 0.00000948
Iteration 109/1000 | Loss: 0.00000948
Iteration 110/1000 | Loss: 0.00000948
Iteration 111/1000 | Loss: 0.00000948
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000948
Iteration 114/1000 | Loss: 0.00000948
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000948
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000948
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000948
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000947
Iteration 124/1000 | Loss: 0.00000947
Iteration 125/1000 | Loss: 0.00000947
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000946
Iteration 132/1000 | Loss: 0.00000946
Iteration 133/1000 | Loss: 0.00000946
Iteration 134/1000 | Loss: 0.00000946
Iteration 135/1000 | Loss: 0.00000946
Iteration 136/1000 | Loss: 0.00000946
Iteration 137/1000 | Loss: 0.00000946
Iteration 138/1000 | Loss: 0.00000946
Iteration 139/1000 | Loss: 0.00000946
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000945
Iteration 144/1000 | Loss: 0.00000945
Iteration 145/1000 | Loss: 0.00000945
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000943
Iteration 152/1000 | Loss: 0.00000943
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000943
Iteration 155/1000 | Loss: 0.00000943
Iteration 156/1000 | Loss: 0.00000943
Iteration 157/1000 | Loss: 0.00000943
Iteration 158/1000 | Loss: 0.00000942
Iteration 159/1000 | Loss: 0.00000942
Iteration 160/1000 | Loss: 0.00000942
Iteration 161/1000 | Loss: 0.00000942
Iteration 162/1000 | Loss: 0.00000941
Iteration 163/1000 | Loss: 0.00000941
Iteration 164/1000 | Loss: 0.00000941
Iteration 165/1000 | Loss: 0.00000941
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000940
Iteration 172/1000 | Loss: 0.00000940
Iteration 173/1000 | Loss: 0.00000940
Iteration 174/1000 | Loss: 0.00000940
Iteration 175/1000 | Loss: 0.00000939
Iteration 176/1000 | Loss: 0.00000939
Iteration 177/1000 | Loss: 0.00000939
Iteration 178/1000 | Loss: 0.00000939
Iteration 179/1000 | Loss: 0.00000939
Iteration 180/1000 | Loss: 0.00000938
Iteration 181/1000 | Loss: 0.00000938
Iteration 182/1000 | Loss: 0.00000938
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000937
Iteration 186/1000 | Loss: 0.00000937
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000937
Iteration 189/1000 | Loss: 0.00000937
Iteration 190/1000 | Loss: 0.00000937
Iteration 191/1000 | Loss: 0.00000937
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000935
Iteration 198/1000 | Loss: 0.00000935
Iteration 199/1000 | Loss: 0.00000935
Iteration 200/1000 | Loss: 0.00000935
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000933
Iteration 205/1000 | Loss: 0.00000933
Iteration 206/1000 | Loss: 0.00000933
Iteration 207/1000 | Loss: 0.00000933
Iteration 208/1000 | Loss: 0.00000933
Iteration 209/1000 | Loss: 0.00000933
Iteration 210/1000 | Loss: 0.00000933
Iteration 211/1000 | Loss: 0.00000932
Iteration 212/1000 | Loss: 0.00000932
Iteration 213/1000 | Loss: 0.00000932
Iteration 214/1000 | Loss: 0.00000932
Iteration 215/1000 | Loss: 0.00000931
Iteration 216/1000 | Loss: 0.00000931
Iteration 217/1000 | Loss: 0.00000931
Iteration 218/1000 | Loss: 0.00000931
Iteration 219/1000 | Loss: 0.00000931
Iteration 220/1000 | Loss: 0.00000931
Iteration 221/1000 | Loss: 0.00000931
Iteration 222/1000 | Loss: 0.00000931
Iteration 223/1000 | Loss: 0.00000931
Iteration 224/1000 | Loss: 0.00000931
Iteration 225/1000 | Loss: 0.00000931
Iteration 226/1000 | Loss: 0.00000931
Iteration 227/1000 | Loss: 0.00000931
Iteration 228/1000 | Loss: 0.00000931
Iteration 229/1000 | Loss: 0.00000931
Iteration 230/1000 | Loss: 0.00000931
Iteration 231/1000 | Loss: 0.00000931
Iteration 232/1000 | Loss: 0.00000931
Iteration 233/1000 | Loss: 0.00000931
Iteration 234/1000 | Loss: 0.00000931
Iteration 235/1000 | Loss: 0.00000931
Iteration 236/1000 | Loss: 0.00000931
Iteration 237/1000 | Loss: 0.00000931
Iteration 238/1000 | Loss: 0.00000931
Iteration 239/1000 | Loss: 0.00000930
Iteration 240/1000 | Loss: 0.00000930
Iteration 241/1000 | Loss: 0.00000930
Iteration 242/1000 | Loss: 0.00000930
Iteration 243/1000 | Loss: 0.00000930
Iteration 244/1000 | Loss: 0.00000930
Iteration 245/1000 | Loss: 0.00000930
Iteration 246/1000 | Loss: 0.00000930
Iteration 247/1000 | Loss: 0.00000930
Iteration 248/1000 | Loss: 0.00000930
Iteration 249/1000 | Loss: 0.00000930
Iteration 250/1000 | Loss: 0.00000930
Iteration 251/1000 | Loss: 0.00000930
Iteration 252/1000 | Loss: 0.00000930
Iteration 253/1000 | Loss: 0.00000930
Iteration 254/1000 | Loss: 0.00000930
Iteration 255/1000 | Loss: 0.00000930
Iteration 256/1000 | Loss: 0.00000930
Iteration 257/1000 | Loss: 0.00000930
Iteration 258/1000 | Loss: 0.00000930
Iteration 259/1000 | Loss: 0.00000930
Iteration 260/1000 | Loss: 0.00000930
Iteration 261/1000 | Loss: 0.00000930
Iteration 262/1000 | Loss: 0.00000930
Iteration 263/1000 | Loss: 0.00000930
Iteration 264/1000 | Loss: 0.00000930
Iteration 265/1000 | Loss: 0.00000930
Iteration 266/1000 | Loss: 0.00000930
Iteration 267/1000 | Loss: 0.00000930
Iteration 268/1000 | Loss: 0.00000930
Iteration 269/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [9.295759809901938e-06, 9.295759809901938e-06, 9.295759809901938e-06, 9.295759809901938e-06, 9.295759809901938e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.295759809901938e-06

Optimization complete. Final v2v error: 2.5352225303649902 mm

Highest mean error: 3.5408482551574707 mm for frame 56

Lowest mean error: 2.206024408340454 mm for frame 106

Saving results

Total time: 39.601059675216675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817466
Iteration 2/25 | Loss: 0.00151329
Iteration 3/25 | Loss: 0.00120793
Iteration 4/25 | Loss: 0.00116306
Iteration 5/25 | Loss: 0.00113724
Iteration 6/25 | Loss: 0.00112751
Iteration 7/25 | Loss: 0.00112843
Iteration 8/25 | Loss: 0.00112068
Iteration 9/25 | Loss: 0.00112765
Iteration 10/25 | Loss: 0.00112049
Iteration 11/25 | Loss: 0.00111834
Iteration 12/25 | Loss: 0.00111580
Iteration 13/25 | Loss: 0.00111655
Iteration 14/25 | Loss: 0.00111722
Iteration 15/25 | Loss: 0.00111738
Iteration 16/25 | Loss: 0.00111558
Iteration 17/25 | Loss: 0.00111621
Iteration 18/25 | Loss: 0.00111545
Iteration 19/25 | Loss: 0.00111568
Iteration 20/25 | Loss: 0.00111676
Iteration 21/25 | Loss: 0.00111516
Iteration 22/25 | Loss: 0.00111468
Iteration 23/25 | Loss: 0.00111497
Iteration 24/25 | Loss: 0.00111662
Iteration 25/25 | Loss: 0.00111466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.46960831
Iteration 2/25 | Loss: 0.00145842
Iteration 3/25 | Loss: 0.00145829
Iteration 4/25 | Loss: 0.00145828
Iteration 5/25 | Loss: 0.00145828
Iteration 6/25 | Loss: 0.00145828
Iteration 7/25 | Loss: 0.00145828
Iteration 8/25 | Loss: 0.00145828
Iteration 9/25 | Loss: 0.00145828
Iteration 10/25 | Loss: 0.00145828
Iteration 11/25 | Loss: 0.00145828
Iteration 12/25 | Loss: 0.00145828
Iteration 13/25 | Loss: 0.00145828
Iteration 14/25 | Loss: 0.00145828
Iteration 15/25 | Loss: 0.00145828
Iteration 16/25 | Loss: 0.00145828
Iteration 17/25 | Loss: 0.00145828
Iteration 18/25 | Loss: 0.00145828
Iteration 19/25 | Loss: 0.00145828
Iteration 20/25 | Loss: 0.00145828
Iteration 21/25 | Loss: 0.00145828
Iteration 22/25 | Loss: 0.00145828
Iteration 23/25 | Loss: 0.00145828
Iteration 24/25 | Loss: 0.00145828
Iteration 25/25 | Loss: 0.00145828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145828
Iteration 2/1000 | Loss: 0.00035476
Iteration 3/1000 | Loss: 0.00017054
Iteration 4/1000 | Loss: 0.00024831
Iteration 5/1000 | Loss: 0.00023810
Iteration 6/1000 | Loss: 0.00031066
Iteration 7/1000 | Loss: 0.00008678
Iteration 8/1000 | Loss: 0.00023864
Iteration 9/1000 | Loss: 0.00023824
Iteration 10/1000 | Loss: 0.00025528
Iteration 11/1000 | Loss: 0.00026030
Iteration 12/1000 | Loss: 0.00019271
Iteration 13/1000 | Loss: 0.00040727
Iteration 14/1000 | Loss: 0.00026004
Iteration 15/1000 | Loss: 0.00039563
Iteration 16/1000 | Loss: 0.00016957
Iteration 17/1000 | Loss: 0.00008178
Iteration 18/1000 | Loss: 0.00007210
Iteration 19/1000 | Loss: 0.00006936
Iteration 20/1000 | Loss: 0.00006754
Iteration 21/1000 | Loss: 0.00006585
Iteration 22/1000 | Loss: 0.00006419
Iteration 23/1000 | Loss: 0.00006308
Iteration 24/1000 | Loss: 0.00006219
Iteration 25/1000 | Loss: 0.00006174
Iteration 26/1000 | Loss: 0.00006105
Iteration 27/1000 | Loss: 0.00158533
Iteration 28/1000 | Loss: 0.00106424
Iteration 29/1000 | Loss: 0.00128704
Iteration 30/1000 | Loss: 0.00104176
Iteration 31/1000 | Loss: 0.00569522
Iteration 32/1000 | Loss: 0.00115827
Iteration 33/1000 | Loss: 0.00146062
Iteration 34/1000 | Loss: 0.00081441
Iteration 35/1000 | Loss: 0.00015586
Iteration 36/1000 | Loss: 0.00054997
Iteration 37/1000 | Loss: 0.00007160
Iteration 38/1000 | Loss: 0.00219883
Iteration 39/1000 | Loss: 0.00009705
Iteration 40/1000 | Loss: 0.00055276
Iteration 41/1000 | Loss: 0.00006200
Iteration 42/1000 | Loss: 0.00005382
Iteration 43/1000 | Loss: 0.00004946
Iteration 44/1000 | Loss: 0.00004723
Iteration 45/1000 | Loss: 0.00004554
Iteration 46/1000 | Loss: 0.00004461
Iteration 47/1000 | Loss: 0.00004391
Iteration 48/1000 | Loss: 0.00004318
Iteration 49/1000 | Loss: 0.00004270
Iteration 50/1000 | Loss: 0.00004227
Iteration 51/1000 | Loss: 0.00004186
Iteration 52/1000 | Loss: 0.00004153
Iteration 53/1000 | Loss: 0.00004134
Iteration 54/1000 | Loss: 0.00004124
Iteration 55/1000 | Loss: 0.00004108
Iteration 56/1000 | Loss: 0.00004105
Iteration 57/1000 | Loss: 0.00004088
Iteration 58/1000 | Loss: 0.00004073
Iteration 59/1000 | Loss: 0.00004068
Iteration 60/1000 | Loss: 0.00004064
Iteration 61/1000 | Loss: 0.00004062
Iteration 62/1000 | Loss: 0.00004061
Iteration 63/1000 | Loss: 0.00004061
Iteration 64/1000 | Loss: 0.00004060
Iteration 65/1000 | Loss: 0.00004060
Iteration 66/1000 | Loss: 0.00004059
Iteration 67/1000 | Loss: 0.00004057
Iteration 68/1000 | Loss: 0.00004052
Iteration 69/1000 | Loss: 0.00004049
Iteration 70/1000 | Loss: 0.00004046
Iteration 71/1000 | Loss: 0.00004045
Iteration 72/1000 | Loss: 0.00004045
Iteration 73/1000 | Loss: 0.00004044
Iteration 74/1000 | Loss: 0.00004043
Iteration 75/1000 | Loss: 0.00004042
Iteration 76/1000 | Loss: 0.00004040
Iteration 77/1000 | Loss: 0.00004040
Iteration 78/1000 | Loss: 0.00004037
Iteration 79/1000 | Loss: 0.00004036
Iteration 80/1000 | Loss: 0.00004036
Iteration 81/1000 | Loss: 0.00004036
Iteration 82/1000 | Loss: 0.00004036
Iteration 83/1000 | Loss: 0.00004035
Iteration 84/1000 | Loss: 0.00004035
Iteration 85/1000 | Loss: 0.00004034
Iteration 86/1000 | Loss: 0.00004033
Iteration 87/1000 | Loss: 0.00004033
Iteration 88/1000 | Loss: 0.00004033
Iteration 89/1000 | Loss: 0.00004033
Iteration 90/1000 | Loss: 0.00004032
Iteration 91/1000 | Loss: 0.00004031
Iteration 92/1000 | Loss: 0.00004031
Iteration 93/1000 | Loss: 0.00004031
Iteration 94/1000 | Loss: 0.00004031
Iteration 95/1000 | Loss: 0.00004031
Iteration 96/1000 | Loss: 0.00004030
Iteration 97/1000 | Loss: 0.00004030
Iteration 98/1000 | Loss: 0.00004030
Iteration 99/1000 | Loss: 0.00004030
Iteration 100/1000 | Loss: 0.00004030
Iteration 101/1000 | Loss: 0.00004030
Iteration 102/1000 | Loss: 0.00004030
Iteration 103/1000 | Loss: 0.00004030
Iteration 104/1000 | Loss: 0.00004030
Iteration 105/1000 | Loss: 0.00004030
Iteration 106/1000 | Loss: 0.00004030
Iteration 107/1000 | Loss: 0.00004029
Iteration 108/1000 | Loss: 0.00004029
Iteration 109/1000 | Loss: 0.00004029
Iteration 110/1000 | Loss: 0.00004028
Iteration 111/1000 | Loss: 0.00004028
Iteration 112/1000 | Loss: 0.00004027
Iteration 113/1000 | Loss: 0.00004026
Iteration 114/1000 | Loss: 0.00004026
Iteration 115/1000 | Loss: 0.00004026
Iteration 116/1000 | Loss: 0.00004023
Iteration 117/1000 | Loss: 0.00004023
Iteration 118/1000 | Loss: 0.00004022
Iteration 119/1000 | Loss: 0.00004021
Iteration 120/1000 | Loss: 0.00004021
Iteration 121/1000 | Loss: 0.00004021
Iteration 122/1000 | Loss: 0.00004020
Iteration 123/1000 | Loss: 0.00004020
Iteration 124/1000 | Loss: 0.00004020
Iteration 125/1000 | Loss: 0.00004020
Iteration 126/1000 | Loss: 0.00004020
Iteration 127/1000 | Loss: 0.00004020
Iteration 128/1000 | Loss: 0.00004020
Iteration 129/1000 | Loss: 0.00004020
Iteration 130/1000 | Loss: 0.00004020
Iteration 131/1000 | Loss: 0.00004020
Iteration 132/1000 | Loss: 0.00004019
Iteration 133/1000 | Loss: 0.00004019
Iteration 134/1000 | Loss: 0.00004019
Iteration 135/1000 | Loss: 0.00004019
Iteration 136/1000 | Loss: 0.00004019
Iteration 137/1000 | Loss: 0.00004019
Iteration 138/1000 | Loss: 0.00004019
Iteration 139/1000 | Loss: 0.00004019
Iteration 140/1000 | Loss: 0.00004018
Iteration 141/1000 | Loss: 0.00004018
Iteration 142/1000 | Loss: 0.00004018
Iteration 143/1000 | Loss: 0.00004017
Iteration 144/1000 | Loss: 0.00004017
Iteration 145/1000 | Loss: 0.00004017
Iteration 146/1000 | Loss: 0.00004016
Iteration 147/1000 | Loss: 0.00004016
Iteration 148/1000 | Loss: 0.00004016
Iteration 149/1000 | Loss: 0.00004014
Iteration 150/1000 | Loss: 0.00004014
Iteration 151/1000 | Loss: 0.00004013
Iteration 152/1000 | Loss: 0.00004013
Iteration 153/1000 | Loss: 0.00004012
Iteration 154/1000 | Loss: 0.00004011
Iteration 155/1000 | Loss: 0.00004011
Iteration 156/1000 | Loss: 0.00004011
Iteration 157/1000 | Loss: 0.00004011
Iteration 158/1000 | Loss: 0.00004010
Iteration 159/1000 | Loss: 0.00004010
Iteration 160/1000 | Loss: 0.00004010
Iteration 161/1000 | Loss: 0.00004009
Iteration 162/1000 | Loss: 0.00004009
Iteration 163/1000 | Loss: 0.00004009
Iteration 164/1000 | Loss: 0.00004009
Iteration 165/1000 | Loss: 0.00004008
Iteration 166/1000 | Loss: 0.00004008
Iteration 167/1000 | Loss: 0.00004008
Iteration 168/1000 | Loss: 0.00004007
Iteration 169/1000 | Loss: 0.00004007
Iteration 170/1000 | Loss: 0.00004006
Iteration 171/1000 | Loss: 0.00004006
Iteration 172/1000 | Loss: 0.00004006
Iteration 173/1000 | Loss: 0.00004005
Iteration 174/1000 | Loss: 0.00004005
Iteration 175/1000 | Loss: 0.00004005
Iteration 176/1000 | Loss: 0.00004005
Iteration 177/1000 | Loss: 0.00004005
Iteration 178/1000 | Loss: 0.00004004
Iteration 179/1000 | Loss: 0.00004004
Iteration 180/1000 | Loss: 0.00004004
Iteration 181/1000 | Loss: 0.00004003
Iteration 182/1000 | Loss: 0.00004003
Iteration 183/1000 | Loss: 0.00004003
Iteration 184/1000 | Loss: 0.00004003
Iteration 185/1000 | Loss: 0.00004003
Iteration 186/1000 | Loss: 0.00004002
Iteration 187/1000 | Loss: 0.00004002
Iteration 188/1000 | Loss: 0.00004002
Iteration 189/1000 | Loss: 0.00004002
Iteration 190/1000 | Loss: 0.00004002
Iteration 191/1000 | Loss: 0.00004002
Iteration 192/1000 | Loss: 0.00004002
Iteration 193/1000 | Loss: 0.00004001
Iteration 194/1000 | Loss: 0.00004001
Iteration 195/1000 | Loss: 0.00004001
Iteration 196/1000 | Loss: 0.00004001
Iteration 197/1000 | Loss: 0.00004001
Iteration 198/1000 | Loss: 0.00004001
Iteration 199/1000 | Loss: 0.00004001
Iteration 200/1000 | Loss: 0.00004001
Iteration 201/1000 | Loss: 0.00004000
Iteration 202/1000 | Loss: 0.00004000
Iteration 203/1000 | Loss: 0.00004000
Iteration 204/1000 | Loss: 0.00003999
Iteration 205/1000 | Loss: 0.00003999
Iteration 206/1000 | Loss: 0.00003999
Iteration 207/1000 | Loss: 0.00003999
Iteration 208/1000 | Loss: 0.00003999
Iteration 209/1000 | Loss: 0.00003999
Iteration 210/1000 | Loss: 0.00003998
Iteration 211/1000 | Loss: 0.00003998
Iteration 212/1000 | Loss: 0.00003998
Iteration 213/1000 | Loss: 0.00003998
Iteration 214/1000 | Loss: 0.00003997
Iteration 215/1000 | Loss: 0.00003997
Iteration 216/1000 | Loss: 0.00003997
Iteration 217/1000 | Loss: 0.00003997
Iteration 218/1000 | Loss: 0.00003997
Iteration 219/1000 | Loss: 0.00003997
Iteration 220/1000 | Loss: 0.00003997
Iteration 221/1000 | Loss: 0.00003996
Iteration 222/1000 | Loss: 0.00003996
Iteration 223/1000 | Loss: 0.00003996
Iteration 224/1000 | Loss: 0.00003996
Iteration 225/1000 | Loss: 0.00003996
Iteration 226/1000 | Loss: 0.00003996
Iteration 227/1000 | Loss: 0.00003996
Iteration 228/1000 | Loss: 0.00003995
Iteration 229/1000 | Loss: 0.00003995
Iteration 230/1000 | Loss: 0.00003995
Iteration 231/1000 | Loss: 0.00003995
Iteration 232/1000 | Loss: 0.00003995
Iteration 233/1000 | Loss: 0.00003995
Iteration 234/1000 | Loss: 0.00003995
Iteration 235/1000 | Loss: 0.00003995
Iteration 236/1000 | Loss: 0.00003994
Iteration 237/1000 | Loss: 0.00003994
Iteration 238/1000 | Loss: 0.00003994
Iteration 239/1000 | Loss: 0.00003994
Iteration 240/1000 | Loss: 0.00003994
Iteration 241/1000 | Loss: 0.00003993
Iteration 242/1000 | Loss: 0.00003993
Iteration 243/1000 | Loss: 0.00003993
Iteration 244/1000 | Loss: 0.00003993
Iteration 245/1000 | Loss: 0.00003992
Iteration 246/1000 | Loss: 0.00003992
Iteration 247/1000 | Loss: 0.00003992
Iteration 248/1000 | Loss: 0.00003992
Iteration 249/1000 | Loss: 0.00003992
Iteration 250/1000 | Loss: 0.00003992
Iteration 251/1000 | Loss: 0.00003992
Iteration 252/1000 | Loss: 0.00003992
Iteration 253/1000 | Loss: 0.00003992
Iteration 254/1000 | Loss: 0.00003992
Iteration 255/1000 | Loss: 0.00003992
Iteration 256/1000 | Loss: 0.00003992
Iteration 257/1000 | Loss: 0.00003992
Iteration 258/1000 | Loss: 0.00003992
Iteration 259/1000 | Loss: 0.00003992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [3.991827179561369e-05, 3.991827179561369e-05, 3.991827179561369e-05, 3.991827179561369e-05, 3.991827179561369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.991827179561369e-05

Optimization complete. Final v2v error: 4.244739532470703 mm

Highest mean error: 13.178298950195312 mm for frame 62

Lowest mean error: 2.5776922702789307 mm for frame 104

Saving results

Total time: 143.97567534446716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_021/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_021/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591643
Iteration 2/25 | Loss: 0.00105636
Iteration 3/25 | Loss: 0.00097286
Iteration 4/25 | Loss: 0.00095892
Iteration 5/25 | Loss: 0.00095414
Iteration 6/25 | Loss: 0.00095269
Iteration 7/25 | Loss: 0.00095269
Iteration 8/25 | Loss: 0.00095269
Iteration 9/25 | Loss: 0.00095269
Iteration 10/25 | Loss: 0.00095269
Iteration 11/25 | Loss: 0.00095269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009526864159852266, 0.0009526864159852266, 0.0009526864159852266, 0.0009526864159852266, 0.0009526864159852266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009526864159852266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94007993
Iteration 2/25 | Loss: 0.00072410
Iteration 3/25 | Loss: 0.00072410
Iteration 4/25 | Loss: 0.00072410
Iteration 5/25 | Loss: 0.00072410
Iteration 6/25 | Loss: 0.00072410
Iteration 7/25 | Loss: 0.00072410
Iteration 8/25 | Loss: 0.00072410
Iteration 9/25 | Loss: 0.00072410
Iteration 10/25 | Loss: 0.00072410
Iteration 11/25 | Loss: 0.00072410
Iteration 12/25 | Loss: 0.00072410
Iteration 13/25 | Loss: 0.00072410
Iteration 14/25 | Loss: 0.00072410
Iteration 15/25 | Loss: 0.00072410
Iteration 16/25 | Loss: 0.00072410
Iteration 17/25 | Loss: 0.00072410
Iteration 18/25 | Loss: 0.00072410
Iteration 19/25 | Loss: 0.00072410
Iteration 20/25 | Loss: 0.00072410
Iteration 21/25 | Loss: 0.00072410
Iteration 22/25 | Loss: 0.00072410
Iteration 23/25 | Loss: 0.00072410
Iteration 24/25 | Loss: 0.00072410
Iteration 25/25 | Loss: 0.00072410

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072410
Iteration 2/1000 | Loss: 0.00001647
Iteration 3/1000 | Loss: 0.00001128
Iteration 4/1000 | Loss: 0.00001025
Iteration 5/1000 | Loss: 0.00000979
Iteration 6/1000 | Loss: 0.00000946
Iteration 7/1000 | Loss: 0.00000917
Iteration 8/1000 | Loss: 0.00000900
Iteration 9/1000 | Loss: 0.00000897
Iteration 10/1000 | Loss: 0.00000878
Iteration 11/1000 | Loss: 0.00000873
Iteration 12/1000 | Loss: 0.00000870
Iteration 13/1000 | Loss: 0.00000869
Iteration 14/1000 | Loss: 0.00000868
Iteration 15/1000 | Loss: 0.00000867
Iteration 16/1000 | Loss: 0.00000867
Iteration 17/1000 | Loss: 0.00000866
Iteration 18/1000 | Loss: 0.00000866
Iteration 19/1000 | Loss: 0.00000865
Iteration 20/1000 | Loss: 0.00000862
Iteration 21/1000 | Loss: 0.00000860
Iteration 22/1000 | Loss: 0.00000860
Iteration 23/1000 | Loss: 0.00000860
Iteration 24/1000 | Loss: 0.00000860
Iteration 25/1000 | Loss: 0.00000860
Iteration 26/1000 | Loss: 0.00000860
Iteration 27/1000 | Loss: 0.00000860
Iteration 28/1000 | Loss: 0.00000860
Iteration 29/1000 | Loss: 0.00000857
Iteration 30/1000 | Loss: 0.00000853
Iteration 31/1000 | Loss: 0.00000853
Iteration 32/1000 | Loss: 0.00000853
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000853
Iteration 35/1000 | Loss: 0.00000853
Iteration 36/1000 | Loss: 0.00000852
Iteration 37/1000 | Loss: 0.00000852
Iteration 38/1000 | Loss: 0.00000852
Iteration 39/1000 | Loss: 0.00000852
Iteration 40/1000 | Loss: 0.00000850
Iteration 41/1000 | Loss: 0.00000848
Iteration 42/1000 | Loss: 0.00000848
Iteration 43/1000 | Loss: 0.00000847
Iteration 44/1000 | Loss: 0.00000846
Iteration 45/1000 | Loss: 0.00000846
Iteration 46/1000 | Loss: 0.00000846
Iteration 47/1000 | Loss: 0.00000845
Iteration 48/1000 | Loss: 0.00000845
Iteration 49/1000 | Loss: 0.00000845
Iteration 50/1000 | Loss: 0.00000845
Iteration 51/1000 | Loss: 0.00000845
Iteration 52/1000 | Loss: 0.00000845
Iteration 53/1000 | Loss: 0.00000845
Iteration 54/1000 | Loss: 0.00000844
Iteration 55/1000 | Loss: 0.00000844
Iteration 56/1000 | Loss: 0.00000843
Iteration 57/1000 | Loss: 0.00000843
Iteration 58/1000 | Loss: 0.00000842
Iteration 59/1000 | Loss: 0.00000842
Iteration 60/1000 | Loss: 0.00000842
Iteration 61/1000 | Loss: 0.00000841
Iteration 62/1000 | Loss: 0.00000841
Iteration 63/1000 | Loss: 0.00000841
Iteration 64/1000 | Loss: 0.00000841
Iteration 65/1000 | Loss: 0.00000840
Iteration 66/1000 | Loss: 0.00000840
Iteration 67/1000 | Loss: 0.00000840
Iteration 68/1000 | Loss: 0.00000837
Iteration 69/1000 | Loss: 0.00000837
Iteration 70/1000 | Loss: 0.00000836
Iteration 71/1000 | Loss: 0.00000835
Iteration 72/1000 | Loss: 0.00000835
Iteration 73/1000 | Loss: 0.00000834
Iteration 74/1000 | Loss: 0.00000833
Iteration 75/1000 | Loss: 0.00000833
Iteration 76/1000 | Loss: 0.00000833
Iteration 77/1000 | Loss: 0.00000833
Iteration 78/1000 | Loss: 0.00000832
Iteration 79/1000 | Loss: 0.00000832
Iteration 80/1000 | Loss: 0.00000832
Iteration 81/1000 | Loss: 0.00000832
Iteration 82/1000 | Loss: 0.00000832
Iteration 83/1000 | Loss: 0.00000831
Iteration 84/1000 | Loss: 0.00000831
Iteration 85/1000 | Loss: 0.00000831
Iteration 86/1000 | Loss: 0.00000830
Iteration 87/1000 | Loss: 0.00000830
Iteration 88/1000 | Loss: 0.00000830
Iteration 89/1000 | Loss: 0.00000830
Iteration 90/1000 | Loss: 0.00000830
Iteration 91/1000 | Loss: 0.00000830
Iteration 92/1000 | Loss: 0.00000829
Iteration 93/1000 | Loss: 0.00000829
Iteration 94/1000 | Loss: 0.00000829
Iteration 95/1000 | Loss: 0.00000828
Iteration 96/1000 | Loss: 0.00000828
Iteration 97/1000 | Loss: 0.00000828
Iteration 98/1000 | Loss: 0.00000828
Iteration 99/1000 | Loss: 0.00000827
Iteration 100/1000 | Loss: 0.00000826
Iteration 101/1000 | Loss: 0.00000826
Iteration 102/1000 | Loss: 0.00000826
Iteration 103/1000 | Loss: 0.00000826
Iteration 104/1000 | Loss: 0.00000826
Iteration 105/1000 | Loss: 0.00000826
Iteration 106/1000 | Loss: 0.00000826
Iteration 107/1000 | Loss: 0.00000825
Iteration 108/1000 | Loss: 0.00000825
Iteration 109/1000 | Loss: 0.00000825
Iteration 110/1000 | Loss: 0.00000824
Iteration 111/1000 | Loss: 0.00000824
Iteration 112/1000 | Loss: 0.00000824
Iteration 113/1000 | Loss: 0.00000824
Iteration 114/1000 | Loss: 0.00000824
Iteration 115/1000 | Loss: 0.00000824
Iteration 116/1000 | Loss: 0.00000823
Iteration 117/1000 | Loss: 0.00000823
Iteration 118/1000 | Loss: 0.00000823
Iteration 119/1000 | Loss: 0.00000823
Iteration 120/1000 | Loss: 0.00000823
Iteration 121/1000 | Loss: 0.00000823
Iteration 122/1000 | Loss: 0.00000822
Iteration 123/1000 | Loss: 0.00000822
Iteration 124/1000 | Loss: 0.00000822
Iteration 125/1000 | Loss: 0.00000822
Iteration 126/1000 | Loss: 0.00000821
Iteration 127/1000 | Loss: 0.00000821
Iteration 128/1000 | Loss: 0.00000821
Iteration 129/1000 | Loss: 0.00000821
Iteration 130/1000 | Loss: 0.00000821
Iteration 131/1000 | Loss: 0.00000821
Iteration 132/1000 | Loss: 0.00000821
Iteration 133/1000 | Loss: 0.00000820
Iteration 134/1000 | Loss: 0.00000820
Iteration 135/1000 | Loss: 0.00000820
Iteration 136/1000 | Loss: 0.00000820
Iteration 137/1000 | Loss: 0.00000820
Iteration 138/1000 | Loss: 0.00000820
Iteration 139/1000 | Loss: 0.00000820
Iteration 140/1000 | Loss: 0.00000819
Iteration 141/1000 | Loss: 0.00000819
Iteration 142/1000 | Loss: 0.00000819
Iteration 143/1000 | Loss: 0.00000819
Iteration 144/1000 | Loss: 0.00000819
Iteration 145/1000 | Loss: 0.00000818
Iteration 146/1000 | Loss: 0.00000818
Iteration 147/1000 | Loss: 0.00000818
Iteration 148/1000 | Loss: 0.00000818
Iteration 149/1000 | Loss: 0.00000818
Iteration 150/1000 | Loss: 0.00000818
Iteration 151/1000 | Loss: 0.00000818
Iteration 152/1000 | Loss: 0.00000818
Iteration 153/1000 | Loss: 0.00000818
Iteration 154/1000 | Loss: 0.00000818
Iteration 155/1000 | Loss: 0.00000817
Iteration 156/1000 | Loss: 0.00000817
Iteration 157/1000 | Loss: 0.00000817
Iteration 158/1000 | Loss: 0.00000817
Iteration 159/1000 | Loss: 0.00000816
Iteration 160/1000 | Loss: 0.00000816
Iteration 161/1000 | Loss: 0.00000816
Iteration 162/1000 | Loss: 0.00000815
Iteration 163/1000 | Loss: 0.00000815
Iteration 164/1000 | Loss: 0.00000815
Iteration 165/1000 | Loss: 0.00000815
Iteration 166/1000 | Loss: 0.00000815
Iteration 167/1000 | Loss: 0.00000814
Iteration 168/1000 | Loss: 0.00000814
Iteration 169/1000 | Loss: 0.00000814
Iteration 170/1000 | Loss: 0.00000814
Iteration 171/1000 | Loss: 0.00000814
Iteration 172/1000 | Loss: 0.00000814
Iteration 173/1000 | Loss: 0.00000814
Iteration 174/1000 | Loss: 0.00000814
Iteration 175/1000 | Loss: 0.00000813
Iteration 176/1000 | Loss: 0.00000813
Iteration 177/1000 | Loss: 0.00000813
Iteration 178/1000 | Loss: 0.00000812
Iteration 179/1000 | Loss: 0.00000812
Iteration 180/1000 | Loss: 0.00000812
Iteration 181/1000 | Loss: 0.00000812
Iteration 182/1000 | Loss: 0.00000812
Iteration 183/1000 | Loss: 0.00000812
Iteration 184/1000 | Loss: 0.00000812
Iteration 185/1000 | Loss: 0.00000812
Iteration 186/1000 | Loss: 0.00000811
Iteration 187/1000 | Loss: 0.00000811
Iteration 188/1000 | Loss: 0.00000811
Iteration 189/1000 | Loss: 0.00000811
Iteration 190/1000 | Loss: 0.00000811
Iteration 191/1000 | Loss: 0.00000811
Iteration 192/1000 | Loss: 0.00000811
Iteration 193/1000 | Loss: 0.00000811
Iteration 194/1000 | Loss: 0.00000811
Iteration 195/1000 | Loss: 0.00000811
Iteration 196/1000 | Loss: 0.00000811
Iteration 197/1000 | Loss: 0.00000811
Iteration 198/1000 | Loss: 0.00000811
Iteration 199/1000 | Loss: 0.00000811
Iteration 200/1000 | Loss: 0.00000811
Iteration 201/1000 | Loss: 0.00000810
Iteration 202/1000 | Loss: 0.00000810
Iteration 203/1000 | Loss: 0.00000810
Iteration 204/1000 | Loss: 0.00000810
Iteration 205/1000 | Loss: 0.00000810
Iteration 206/1000 | Loss: 0.00000810
Iteration 207/1000 | Loss: 0.00000810
Iteration 208/1000 | Loss: 0.00000810
Iteration 209/1000 | Loss: 0.00000810
Iteration 210/1000 | Loss: 0.00000810
Iteration 211/1000 | Loss: 0.00000810
Iteration 212/1000 | Loss: 0.00000810
Iteration 213/1000 | Loss: 0.00000810
Iteration 214/1000 | Loss: 0.00000810
Iteration 215/1000 | Loss: 0.00000810
Iteration 216/1000 | Loss: 0.00000810
Iteration 217/1000 | Loss: 0.00000810
Iteration 218/1000 | Loss: 0.00000810
Iteration 219/1000 | Loss: 0.00000810
Iteration 220/1000 | Loss: 0.00000810
Iteration 221/1000 | Loss: 0.00000810
Iteration 222/1000 | Loss: 0.00000810
Iteration 223/1000 | Loss: 0.00000810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [8.097529644146562e-06, 8.097529644146562e-06, 8.097529644146562e-06, 8.097529644146562e-06, 8.097529644146562e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.097529644146562e-06

Optimization complete. Final v2v error: 2.4576969146728516 mm

Highest mean error: 2.6761574745178223 mm for frame 97

Lowest mean error: 2.2965657711029053 mm for frame 159

Saving results

Total time: 41.22395873069763
