Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=116, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6496-6551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474253
Iteration 2/25 | Loss: 0.00120033
Iteration 3/25 | Loss: 0.00109228
Iteration 4/25 | Loss: 0.00106814
Iteration 5/25 | Loss: 0.00105796
Iteration 6/25 | Loss: 0.00105478
Iteration 7/25 | Loss: 0.00105427
Iteration 8/25 | Loss: 0.00105427
Iteration 9/25 | Loss: 0.00105427
Iteration 10/25 | Loss: 0.00105427
Iteration 11/25 | Loss: 0.00105427
Iteration 12/25 | Loss: 0.00105427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010542696109041572, 0.0010542696109041572, 0.0010542696109041572, 0.0010542696109041572, 0.0010542696109041572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010542696109041572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.34193373
Iteration 2/25 | Loss: 0.00201747
Iteration 3/25 | Loss: 0.00201747
Iteration 4/25 | Loss: 0.00201747
Iteration 5/25 | Loss: 0.00201747
Iteration 6/25 | Loss: 0.00201747
Iteration 7/25 | Loss: 0.00201747
Iteration 8/25 | Loss: 0.00201747
Iteration 9/25 | Loss: 0.00201747
Iteration 10/25 | Loss: 0.00201747
Iteration 11/25 | Loss: 0.00201747
Iteration 12/25 | Loss: 0.00201747
Iteration 13/25 | Loss: 0.00201747
Iteration 14/25 | Loss: 0.00201747
Iteration 15/25 | Loss: 0.00201747
Iteration 16/25 | Loss: 0.00201747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002017471008002758, 0.002017471008002758, 0.002017471008002758, 0.002017471008002758, 0.002017471008002758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002017471008002758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201747
Iteration 2/1000 | Loss: 0.00004058
Iteration 3/1000 | Loss: 0.00002367
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00001982
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001749
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001716
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001691
Iteration 14/1000 | Loss: 0.00001680
Iteration 15/1000 | Loss: 0.00001679
Iteration 16/1000 | Loss: 0.00001678
Iteration 17/1000 | Loss: 0.00001677
Iteration 18/1000 | Loss: 0.00001677
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001674
Iteration 28/1000 | Loss: 0.00001674
Iteration 29/1000 | Loss: 0.00001673
Iteration 30/1000 | Loss: 0.00001673
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001672
Iteration 33/1000 | Loss: 0.00001672
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001668
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001668
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001668
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001666
Iteration 69/1000 | Loss: 0.00001666
Iteration 70/1000 | Loss: 0.00001666
Iteration 71/1000 | Loss: 0.00001666
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001665
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001663
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001662
Iteration 88/1000 | Loss: 0.00001662
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001662
Iteration 94/1000 | Loss: 0.00001662
Iteration 95/1000 | Loss: 0.00001662
Iteration 96/1000 | Loss: 0.00001662
Iteration 97/1000 | Loss: 0.00001662
Iteration 98/1000 | Loss: 0.00001662
Iteration 99/1000 | Loss: 0.00001662
Iteration 100/1000 | Loss: 0.00001661
Iteration 101/1000 | Loss: 0.00001661
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001660
Iteration 106/1000 | Loss: 0.00001660
Iteration 107/1000 | Loss: 0.00001660
Iteration 108/1000 | Loss: 0.00001660
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001659
Iteration 115/1000 | Loss: 0.00001659
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001659
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001657
Iteration 132/1000 | Loss: 0.00001657
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00001657
Iteration 144/1000 | Loss: 0.00001657
Iteration 145/1000 | Loss: 0.00001657
Iteration 146/1000 | Loss: 0.00001656
Iteration 147/1000 | Loss: 0.00001656
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001656
Iteration 154/1000 | Loss: 0.00001656
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001655
Iteration 159/1000 | Loss: 0.00001655
Iteration 160/1000 | Loss: 0.00001655
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001654
Iteration 165/1000 | Loss: 0.00001654
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001653
Iteration 170/1000 | Loss: 0.00001653
Iteration 171/1000 | Loss: 0.00001653
Iteration 172/1000 | Loss: 0.00001653
Iteration 173/1000 | Loss: 0.00001653
Iteration 174/1000 | Loss: 0.00001653
Iteration 175/1000 | Loss: 0.00001653
Iteration 176/1000 | Loss: 0.00001653
Iteration 177/1000 | Loss: 0.00001652
Iteration 178/1000 | Loss: 0.00001652
Iteration 179/1000 | Loss: 0.00001652
Iteration 180/1000 | Loss: 0.00001652
Iteration 181/1000 | Loss: 0.00001652
Iteration 182/1000 | Loss: 0.00001652
Iteration 183/1000 | Loss: 0.00001652
Iteration 184/1000 | Loss: 0.00001652
Iteration 185/1000 | Loss: 0.00001652
Iteration 186/1000 | Loss: 0.00001652
Iteration 187/1000 | Loss: 0.00001652
Iteration 188/1000 | Loss: 0.00001652
Iteration 189/1000 | Loss: 0.00001651
Iteration 190/1000 | Loss: 0.00001651
Iteration 191/1000 | Loss: 0.00001651
Iteration 192/1000 | Loss: 0.00001651
Iteration 193/1000 | Loss: 0.00001651
Iteration 194/1000 | Loss: 0.00001651
Iteration 195/1000 | Loss: 0.00001651
Iteration 196/1000 | Loss: 0.00001650
Iteration 197/1000 | Loss: 0.00001650
Iteration 198/1000 | Loss: 0.00001650
Iteration 199/1000 | Loss: 0.00001650
Iteration 200/1000 | Loss: 0.00001649
Iteration 201/1000 | Loss: 0.00001649
Iteration 202/1000 | Loss: 0.00001649
Iteration 203/1000 | Loss: 0.00001649
Iteration 204/1000 | Loss: 0.00001649
Iteration 205/1000 | Loss: 0.00001649
Iteration 206/1000 | Loss: 0.00001649
Iteration 207/1000 | Loss: 0.00001649
Iteration 208/1000 | Loss: 0.00001649
Iteration 209/1000 | Loss: 0.00001649
Iteration 210/1000 | Loss: 0.00001649
Iteration 211/1000 | Loss: 0.00001649
Iteration 212/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.6490363123011775e-05, 1.6490363123011775e-05, 1.6490363123011775e-05, 1.6490363123011775e-05, 1.6490363123011775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6490363123011775e-05

Optimization complete. Final v2v error: 3.4672627449035645 mm

Highest mean error: 3.8424437046051025 mm for frame 31

Lowest mean error: 2.876599073410034 mm for frame 3

Saving results

Total time: 42.26672697067261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052078
Iteration 2/25 | Loss: 0.00227979
Iteration 3/25 | Loss: 0.00197931
Iteration 4/25 | Loss: 0.00184587
Iteration 5/25 | Loss: 0.00168831
Iteration 6/25 | Loss: 0.00145653
Iteration 7/25 | Loss: 0.00123369
Iteration 8/25 | Loss: 0.00118056
Iteration 9/25 | Loss: 0.00116812
Iteration 10/25 | Loss: 0.00116516
Iteration 11/25 | Loss: 0.00116449
Iteration 12/25 | Loss: 0.00116428
Iteration 13/25 | Loss: 0.00116428
Iteration 14/25 | Loss: 0.00116428
Iteration 15/25 | Loss: 0.00116428
Iteration 16/25 | Loss: 0.00116428
Iteration 17/25 | Loss: 0.00116428
Iteration 18/25 | Loss: 0.00116428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011642791796475649, 0.0011642791796475649, 0.0011642791796475649, 0.0011642791796475649, 0.0011642791796475649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011642791796475649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18671846
Iteration 2/25 | Loss: 0.00183519
Iteration 3/25 | Loss: 0.00183519
Iteration 4/25 | Loss: 0.00183519
Iteration 5/25 | Loss: 0.00183519
Iteration 6/25 | Loss: 0.00183519
Iteration 7/25 | Loss: 0.00183519
Iteration 8/25 | Loss: 0.00183519
Iteration 9/25 | Loss: 0.00183519
Iteration 10/25 | Loss: 0.00183519
Iteration 11/25 | Loss: 0.00183519
Iteration 12/25 | Loss: 0.00183519
Iteration 13/25 | Loss: 0.00183519
Iteration 14/25 | Loss: 0.00183519
Iteration 15/25 | Loss: 0.00183519
Iteration 16/25 | Loss: 0.00183519
Iteration 17/25 | Loss: 0.00183519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018351888284087181, 0.0018351888284087181, 0.0018351888284087181, 0.0018351888284087181, 0.0018351888284087181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018351888284087181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183519
Iteration 2/1000 | Loss: 0.00005414
Iteration 3/1000 | Loss: 0.00003098
Iteration 4/1000 | Loss: 0.00002698
Iteration 5/1000 | Loss: 0.00002508
Iteration 6/1000 | Loss: 0.00002407
Iteration 7/1000 | Loss: 0.00002278
Iteration 8/1000 | Loss: 0.00002184
Iteration 9/1000 | Loss: 0.00002133
Iteration 10/1000 | Loss: 0.00002096
Iteration 11/1000 | Loss: 0.00002059
Iteration 12/1000 | Loss: 0.00002035
Iteration 13/1000 | Loss: 0.00002031
Iteration 14/1000 | Loss: 0.00002029
Iteration 15/1000 | Loss: 0.00002008
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00002007
Iteration 18/1000 | Loss: 0.00002006
Iteration 19/1000 | Loss: 0.00002006
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00002005
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002000
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001987
Iteration 29/1000 | Loss: 0.00001987
Iteration 30/1000 | Loss: 0.00001982
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001982
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001981
Iteration 36/1000 | Loss: 0.00001981
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00001981
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001980
Iteration 42/1000 | Loss: 0.00001980
Iteration 43/1000 | Loss: 0.00001980
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001978
Iteration 47/1000 | Loss: 0.00001978
Iteration 48/1000 | Loss: 0.00001977
Iteration 49/1000 | Loss: 0.00001977
Iteration 50/1000 | Loss: 0.00001977
Iteration 51/1000 | Loss: 0.00001976
Iteration 52/1000 | Loss: 0.00001976
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00001974
Iteration 58/1000 | Loss: 0.00001974
Iteration 59/1000 | Loss: 0.00001973
Iteration 60/1000 | Loss: 0.00001973
Iteration 61/1000 | Loss: 0.00001973
Iteration 62/1000 | Loss: 0.00001973
Iteration 63/1000 | Loss: 0.00001973
Iteration 64/1000 | Loss: 0.00001972
Iteration 65/1000 | Loss: 0.00001972
Iteration 66/1000 | Loss: 0.00001972
Iteration 67/1000 | Loss: 0.00001972
Iteration 68/1000 | Loss: 0.00001972
Iteration 69/1000 | Loss: 0.00001972
Iteration 70/1000 | Loss: 0.00001972
Iteration 71/1000 | Loss: 0.00001972
Iteration 72/1000 | Loss: 0.00001972
Iteration 73/1000 | Loss: 0.00001972
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001971
Iteration 76/1000 | Loss: 0.00001971
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001971
Iteration 81/1000 | Loss: 0.00001971
Iteration 82/1000 | Loss: 0.00001971
Iteration 83/1000 | Loss: 0.00001971
Iteration 84/1000 | Loss: 0.00001971
Iteration 85/1000 | Loss: 0.00001971
Iteration 86/1000 | Loss: 0.00001971
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Iteration 90/1000 | Loss: 0.00001970
Iteration 91/1000 | Loss: 0.00001970
Iteration 92/1000 | Loss: 0.00001970
Iteration 93/1000 | Loss: 0.00001970
Iteration 94/1000 | Loss: 0.00001970
Iteration 95/1000 | Loss: 0.00001970
Iteration 96/1000 | Loss: 0.00001970
Iteration 97/1000 | Loss: 0.00001970
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001970
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.970218181668315e-05, 1.970218181668315e-05, 1.970218181668315e-05, 1.970218181668315e-05, 1.970218181668315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.970218181668315e-05

Optimization complete. Final v2v error: 3.846517562866211 mm

Highest mean error: 4.008147716522217 mm for frame 0

Lowest mean error: 3.6735312938690186 mm for frame 234

Saving results

Total time: 52.71384859085083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383061
Iteration 2/25 | Loss: 0.00111084
Iteration 3/25 | Loss: 0.00104623
Iteration 4/25 | Loss: 0.00103946
Iteration 5/25 | Loss: 0.00103801
Iteration 6/25 | Loss: 0.00103801
Iteration 7/25 | Loss: 0.00103801
Iteration 8/25 | Loss: 0.00103801
Iteration 9/25 | Loss: 0.00103801
Iteration 10/25 | Loss: 0.00103801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010380076710134745, 0.0010380076710134745, 0.0010380076710134745, 0.0010380076710134745, 0.0010380076710134745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010380076710134745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20841384
Iteration 2/25 | Loss: 0.00207958
Iteration 3/25 | Loss: 0.00207958
Iteration 4/25 | Loss: 0.00207958
Iteration 5/25 | Loss: 0.00207958
Iteration 6/25 | Loss: 0.00207958
Iteration 7/25 | Loss: 0.00207958
Iteration 8/25 | Loss: 0.00207958
Iteration 9/25 | Loss: 0.00207958
Iteration 10/25 | Loss: 0.00207958
Iteration 11/25 | Loss: 0.00207958
Iteration 12/25 | Loss: 0.00207958
Iteration 13/25 | Loss: 0.00207958
Iteration 14/25 | Loss: 0.00207958
Iteration 15/25 | Loss: 0.00207958
Iteration 16/25 | Loss: 0.00207958
Iteration 17/25 | Loss: 0.00207958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002079577650874853, 0.002079577650874853, 0.002079577650874853, 0.002079577650874853, 0.002079577650874853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002079577650874853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207958
Iteration 2/1000 | Loss: 0.00003065
Iteration 3/1000 | Loss: 0.00001535
Iteration 4/1000 | Loss: 0.00001382
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001113
Iteration 9/1000 | Loss: 0.00001082
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001070
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001069
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001064
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001055
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001040
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001039
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001037
Iteration 35/1000 | Loss: 0.00001036
Iteration 36/1000 | Loss: 0.00001035
Iteration 37/1000 | Loss: 0.00001035
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001032
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001032
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001030
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001030
Iteration 57/1000 | Loss: 0.00001030
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001029
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001028
Iteration 63/1000 | Loss: 0.00001028
Iteration 64/1000 | Loss: 0.00001027
Iteration 65/1000 | Loss: 0.00001027
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001027
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.0265845048706979e-05, 1.0265845048706979e-05, 1.0265845048706979e-05, 1.0265845048706979e-05, 1.0265845048706979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0265845048706979e-05

Optimization complete. Final v2v error: 2.7010326385498047 mm

Highest mean error: 2.840920925140381 mm for frame 226

Lowest mean error: 2.4821701049804688 mm for frame 0

Saving results

Total time: 29.42068362236023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495662
Iteration 2/25 | Loss: 0.00121006
Iteration 3/25 | Loss: 0.00107571
Iteration 4/25 | Loss: 0.00106491
Iteration 5/25 | Loss: 0.00106117
Iteration 6/25 | Loss: 0.00105981
Iteration 7/25 | Loss: 0.00105981
Iteration 8/25 | Loss: 0.00105981
Iteration 9/25 | Loss: 0.00105981
Iteration 10/25 | Loss: 0.00105981
Iteration 11/25 | Loss: 0.00105981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001059814472682774, 0.001059814472682774, 0.001059814472682774, 0.001059814472682774, 0.001059814472682774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001059814472682774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23532915
Iteration 2/25 | Loss: 0.00199049
Iteration 3/25 | Loss: 0.00199046
Iteration 4/25 | Loss: 0.00199045
Iteration 5/25 | Loss: 0.00199045
Iteration 6/25 | Loss: 0.00199045
Iteration 7/25 | Loss: 0.00199045
Iteration 8/25 | Loss: 0.00199045
Iteration 9/25 | Loss: 0.00199045
Iteration 10/25 | Loss: 0.00199045
Iteration 11/25 | Loss: 0.00199045
Iteration 12/25 | Loss: 0.00199045
Iteration 13/25 | Loss: 0.00199045
Iteration 14/25 | Loss: 0.00199045
Iteration 15/25 | Loss: 0.00199045
Iteration 16/25 | Loss: 0.00199045
Iteration 17/25 | Loss: 0.00199045
Iteration 18/25 | Loss: 0.00199045
Iteration 19/25 | Loss: 0.00199045
Iteration 20/25 | Loss: 0.00199045
Iteration 21/25 | Loss: 0.00199045
Iteration 22/25 | Loss: 0.00199045
Iteration 23/25 | Loss: 0.00199045
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019904528744518757, 0.0019904528744518757, 0.0019904528744518757, 0.0019904528744518757, 0.0019904528744518757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019904528744518757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199045
Iteration 2/1000 | Loss: 0.00003254
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001195
Iteration 9/1000 | Loss: 0.00001175
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001147
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001140
Iteration 17/1000 | Loss: 0.00001140
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001140
Iteration 22/1000 | Loss: 0.00001140
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001137
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001136
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001116
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001110
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001110
Iteration 124/1000 | Loss: 0.00001110
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1090620319009759e-05, 1.1090620319009759e-05, 1.1090620319009759e-05, 1.1090620319009759e-05, 1.1090620319009759e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1090620319009759e-05

Optimization complete. Final v2v error: 2.870323657989502 mm

Highest mean error: 3.2951061725616455 mm for frame 144

Lowest mean error: 2.590353488922119 mm for frame 114

Saving results

Total time: 32.77949619293213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973440
Iteration 2/25 | Loss: 0.00973439
Iteration 3/25 | Loss: 0.00277214
Iteration 4/25 | Loss: 0.00215977
Iteration 5/25 | Loss: 0.00209677
Iteration 6/25 | Loss: 0.00185209
Iteration 7/25 | Loss: 0.00168471
Iteration 8/25 | Loss: 0.00159312
Iteration 9/25 | Loss: 0.00153692
Iteration 10/25 | Loss: 0.00147638
Iteration 11/25 | Loss: 0.00142779
Iteration 12/25 | Loss: 0.00140034
Iteration 13/25 | Loss: 0.00137893
Iteration 14/25 | Loss: 0.00137259
Iteration 15/25 | Loss: 0.00135285
Iteration 16/25 | Loss: 0.00134903
Iteration 17/25 | Loss: 0.00134266
Iteration 18/25 | Loss: 0.00133941
Iteration 19/25 | Loss: 0.00133772
Iteration 20/25 | Loss: 0.00133704
Iteration 21/25 | Loss: 0.00133756
Iteration 22/25 | Loss: 0.00133685
Iteration 23/25 | Loss: 0.00134426
Iteration 24/25 | Loss: 0.00134397
Iteration 25/25 | Loss: 0.00133504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40553617
Iteration 2/25 | Loss: 0.00107933
Iteration 3/25 | Loss: 0.00107932
Iteration 4/25 | Loss: 0.00107932
Iteration 5/25 | Loss: 0.00107932
Iteration 6/25 | Loss: 0.00107932
Iteration 7/25 | Loss: 0.00107932
Iteration 8/25 | Loss: 0.00107932
Iteration 9/25 | Loss: 0.00107932
Iteration 10/25 | Loss: 0.00107932
Iteration 11/25 | Loss: 0.00107932
Iteration 12/25 | Loss: 0.00107932
Iteration 13/25 | Loss: 0.00107932
Iteration 14/25 | Loss: 0.00107932
Iteration 15/25 | Loss: 0.00107932
Iteration 16/25 | Loss: 0.00107932
Iteration 17/25 | Loss: 0.00107932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001079315086826682, 0.001079315086826682, 0.001079315086826682, 0.001079315086826682, 0.001079315086826682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001079315086826682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107932
Iteration 2/1000 | Loss: 0.00106055
Iteration 3/1000 | Loss: 0.00035257
Iteration 4/1000 | Loss: 0.00026948
Iteration 5/1000 | Loss: 0.00010133
Iteration 6/1000 | Loss: 0.00007716
Iteration 7/1000 | Loss: 0.00046979
Iteration 8/1000 | Loss: 0.00006735
Iteration 9/1000 | Loss: 0.00015215
Iteration 10/1000 | Loss: 0.00005669
Iteration 11/1000 | Loss: 0.00029292
Iteration 12/1000 | Loss: 0.00005545
Iteration 13/1000 | Loss: 0.00014406
Iteration 14/1000 | Loss: 0.00005504
Iteration 15/1000 | Loss: 0.00004181
Iteration 16/1000 | Loss: 0.00006862
Iteration 17/1000 | Loss: 0.00003859
Iteration 18/1000 | Loss: 0.00004713
Iteration 19/1000 | Loss: 0.00018651
Iteration 20/1000 | Loss: 0.00009375
Iteration 21/1000 | Loss: 0.00003608
Iteration 22/1000 | Loss: 0.00003260
Iteration 23/1000 | Loss: 0.00003022
Iteration 24/1000 | Loss: 0.00017744
Iteration 25/1000 | Loss: 0.00004269
Iteration 26/1000 | Loss: 0.00008643
Iteration 27/1000 | Loss: 0.00013841
Iteration 28/1000 | Loss: 0.00014880
Iteration 29/1000 | Loss: 0.00002682
Iteration 30/1000 | Loss: 0.00002537
Iteration 31/1000 | Loss: 0.00002972
Iteration 32/1000 | Loss: 0.00005356
Iteration 33/1000 | Loss: 0.00016280
Iteration 34/1000 | Loss: 0.00002860
Iteration 35/1000 | Loss: 0.00002585
Iteration 36/1000 | Loss: 0.00002433
Iteration 37/1000 | Loss: 0.00002318
Iteration 38/1000 | Loss: 0.00002274
Iteration 39/1000 | Loss: 0.00002227
Iteration 40/1000 | Loss: 0.00002178
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002140
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002135
Iteration 45/1000 | Loss: 0.00002135
Iteration 46/1000 | Loss: 0.00002134
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002131
Iteration 50/1000 | Loss: 0.00002130
Iteration 51/1000 | Loss: 0.00002129
Iteration 52/1000 | Loss: 0.00002129
Iteration 53/1000 | Loss: 0.00002128
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002126
Iteration 57/1000 | Loss: 0.00002126
Iteration 58/1000 | Loss: 0.00002126
Iteration 59/1000 | Loss: 0.00002126
Iteration 60/1000 | Loss: 0.00002125
Iteration 61/1000 | Loss: 0.00002125
Iteration 62/1000 | Loss: 0.00002124
Iteration 63/1000 | Loss: 0.00002122
Iteration 64/1000 | Loss: 0.00002122
Iteration 65/1000 | Loss: 0.00002121
Iteration 66/1000 | Loss: 0.00002120
Iteration 67/1000 | Loss: 0.00002120
Iteration 68/1000 | Loss: 0.00002118
Iteration 69/1000 | Loss: 0.00002116
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002114
Iteration 72/1000 | Loss: 0.00002114
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002112
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002108
Iteration 84/1000 | Loss: 0.00002108
Iteration 85/1000 | Loss: 0.00002108
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002106
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002105
Iteration 92/1000 | Loss: 0.00002098
Iteration 93/1000 | Loss: 0.00002097
Iteration 94/1000 | Loss: 0.00002095
Iteration 95/1000 | Loss: 0.00002095
Iteration 96/1000 | Loss: 0.00002094
Iteration 97/1000 | Loss: 0.00002094
Iteration 98/1000 | Loss: 0.00002094
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00002090
Iteration 101/1000 | Loss: 0.00002090
Iteration 102/1000 | Loss: 0.00002090
Iteration 103/1000 | Loss: 0.00002090
Iteration 104/1000 | Loss: 0.00002090
Iteration 105/1000 | Loss: 0.00002090
Iteration 106/1000 | Loss: 0.00002090
Iteration 107/1000 | Loss: 0.00002090
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002089
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002087
Iteration 113/1000 | Loss: 0.00002086
Iteration 114/1000 | Loss: 0.00002086
Iteration 115/1000 | Loss: 0.00002085
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002085
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Iteration 120/1000 | Loss: 0.00002083
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002081
Iteration 124/1000 | Loss: 0.00002077
Iteration 125/1000 | Loss: 0.00002075
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002074
Iteration 129/1000 | Loss: 0.00002074
Iteration 130/1000 | Loss: 0.00002073
Iteration 131/1000 | Loss: 0.00002070
Iteration 132/1000 | Loss: 0.00002070
Iteration 133/1000 | Loss: 0.00002068
Iteration 134/1000 | Loss: 0.00002068
Iteration 135/1000 | Loss: 0.00002067
Iteration 136/1000 | Loss: 0.00002067
Iteration 137/1000 | Loss: 0.00002067
Iteration 138/1000 | Loss: 0.00002067
Iteration 139/1000 | Loss: 0.00002067
Iteration 140/1000 | Loss: 0.00002066
Iteration 141/1000 | Loss: 0.00002066
Iteration 142/1000 | Loss: 0.00002066
Iteration 143/1000 | Loss: 0.00002066
Iteration 144/1000 | Loss: 0.00002066
Iteration 145/1000 | Loss: 0.00002066
Iteration 146/1000 | Loss: 0.00002065
Iteration 147/1000 | Loss: 0.00002065
Iteration 148/1000 | Loss: 0.00002065
Iteration 149/1000 | Loss: 0.00002064
Iteration 150/1000 | Loss: 0.00002064
Iteration 151/1000 | Loss: 0.00002064
Iteration 152/1000 | Loss: 0.00002063
Iteration 153/1000 | Loss: 0.00002063
Iteration 154/1000 | Loss: 0.00002063
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002062
Iteration 158/1000 | Loss: 0.00002062
Iteration 159/1000 | Loss: 0.00002062
Iteration 160/1000 | Loss: 0.00002062
Iteration 161/1000 | Loss: 0.00002061
Iteration 162/1000 | Loss: 0.00002061
Iteration 163/1000 | Loss: 0.00002061
Iteration 164/1000 | Loss: 0.00002061
Iteration 165/1000 | Loss: 0.00002061
Iteration 166/1000 | Loss: 0.00002061
Iteration 167/1000 | Loss: 0.00002061
Iteration 168/1000 | Loss: 0.00002061
Iteration 169/1000 | Loss: 0.00002061
Iteration 170/1000 | Loss: 0.00002061
Iteration 171/1000 | Loss: 0.00002061
Iteration 172/1000 | Loss: 0.00002061
Iteration 173/1000 | Loss: 0.00002061
Iteration 174/1000 | Loss: 0.00002060
Iteration 175/1000 | Loss: 0.00002060
Iteration 176/1000 | Loss: 0.00002060
Iteration 177/1000 | Loss: 0.00002060
Iteration 178/1000 | Loss: 0.00002059
Iteration 179/1000 | Loss: 0.00002059
Iteration 180/1000 | Loss: 0.00002059
Iteration 181/1000 | Loss: 0.00002059
Iteration 182/1000 | Loss: 0.00002059
Iteration 183/1000 | Loss: 0.00002059
Iteration 184/1000 | Loss: 0.00002059
Iteration 185/1000 | Loss: 0.00002058
Iteration 186/1000 | Loss: 0.00002058
Iteration 187/1000 | Loss: 0.00002058
Iteration 188/1000 | Loss: 0.00002058
Iteration 189/1000 | Loss: 0.00002058
Iteration 190/1000 | Loss: 0.00002058
Iteration 191/1000 | Loss: 0.00002058
Iteration 192/1000 | Loss: 0.00002058
Iteration 193/1000 | Loss: 0.00002058
Iteration 194/1000 | Loss: 0.00002058
Iteration 195/1000 | Loss: 0.00002058
Iteration 196/1000 | Loss: 0.00002058
Iteration 197/1000 | Loss: 0.00002058
Iteration 198/1000 | Loss: 0.00002058
Iteration 199/1000 | Loss: 0.00002058
Iteration 200/1000 | Loss: 0.00002058
Iteration 201/1000 | Loss: 0.00002057
Iteration 202/1000 | Loss: 0.00002057
Iteration 203/1000 | Loss: 0.00002057
Iteration 204/1000 | Loss: 0.00002057
Iteration 205/1000 | Loss: 0.00002057
Iteration 206/1000 | Loss: 0.00002057
Iteration 207/1000 | Loss: 0.00002057
Iteration 208/1000 | Loss: 0.00002057
Iteration 209/1000 | Loss: 0.00002057
Iteration 210/1000 | Loss: 0.00002057
Iteration 211/1000 | Loss: 0.00002057
Iteration 212/1000 | Loss: 0.00002057
Iteration 213/1000 | Loss: 0.00002057
Iteration 214/1000 | Loss: 0.00002056
Iteration 215/1000 | Loss: 0.00002056
Iteration 216/1000 | Loss: 0.00002056
Iteration 217/1000 | Loss: 0.00002056
Iteration 218/1000 | Loss: 0.00002056
Iteration 219/1000 | Loss: 0.00002056
Iteration 220/1000 | Loss: 0.00002056
Iteration 221/1000 | Loss: 0.00002056
Iteration 222/1000 | Loss: 0.00002056
Iteration 223/1000 | Loss: 0.00002056
Iteration 224/1000 | Loss: 0.00002056
Iteration 225/1000 | Loss: 0.00002056
Iteration 226/1000 | Loss: 0.00002056
Iteration 227/1000 | Loss: 0.00002056
Iteration 228/1000 | Loss: 0.00002056
Iteration 229/1000 | Loss: 0.00002056
Iteration 230/1000 | Loss: 0.00002056
Iteration 231/1000 | Loss: 0.00002056
Iteration 232/1000 | Loss: 0.00002056
Iteration 233/1000 | Loss: 0.00002056
Iteration 234/1000 | Loss: 0.00002056
Iteration 235/1000 | Loss: 0.00002055
Iteration 236/1000 | Loss: 0.00002055
Iteration 237/1000 | Loss: 0.00002055
Iteration 238/1000 | Loss: 0.00002055
Iteration 239/1000 | Loss: 0.00002055
Iteration 240/1000 | Loss: 0.00002055
Iteration 241/1000 | Loss: 0.00002055
Iteration 242/1000 | Loss: 0.00002055
Iteration 243/1000 | Loss: 0.00002055
Iteration 244/1000 | Loss: 0.00002055
Iteration 245/1000 | Loss: 0.00002055
Iteration 246/1000 | Loss: 0.00002055
Iteration 247/1000 | Loss: 0.00002055
Iteration 248/1000 | Loss: 0.00002055
Iteration 249/1000 | Loss: 0.00002055
Iteration 250/1000 | Loss: 0.00002055
Iteration 251/1000 | Loss: 0.00002055
Iteration 252/1000 | Loss: 0.00002055
Iteration 253/1000 | Loss: 0.00002055
Iteration 254/1000 | Loss: 0.00002055
Iteration 255/1000 | Loss: 0.00002055
Iteration 256/1000 | Loss: 0.00002055
Iteration 257/1000 | Loss: 0.00002055
Iteration 258/1000 | Loss: 0.00002055
Iteration 259/1000 | Loss: 0.00002055
Iteration 260/1000 | Loss: 0.00002055
Iteration 261/1000 | Loss: 0.00002055
Iteration 262/1000 | Loss: 0.00002055
Iteration 263/1000 | Loss: 0.00002055
Iteration 264/1000 | Loss: 0.00002055
Iteration 265/1000 | Loss: 0.00002055
Iteration 266/1000 | Loss: 0.00002055
Iteration 267/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.0546960513456725e-05, 2.0546960513456725e-05, 2.0546960513456725e-05, 2.0546960513456725e-05, 2.0546960513456725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0546960513456725e-05

Optimization complete. Final v2v error: 3.695852518081665 mm

Highest mean error: 11.053302764892578 mm for frame 179

Lowest mean error: 3.2513515949249268 mm for frame 169

Saving results

Total time: 142.1254768371582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842139
Iteration 2/25 | Loss: 0.00146800
Iteration 3/25 | Loss: 0.00128974
Iteration 4/25 | Loss: 0.00126936
Iteration 5/25 | Loss: 0.00126282
Iteration 6/25 | Loss: 0.00126241
Iteration 7/25 | Loss: 0.00126240
Iteration 8/25 | Loss: 0.00126240
Iteration 9/25 | Loss: 0.00126240
Iteration 10/25 | Loss: 0.00126240
Iteration 11/25 | Loss: 0.00126240
Iteration 12/25 | Loss: 0.00126240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012624049559235573, 0.0012624049559235573, 0.0012624049559235573, 0.0012624049559235573, 0.0012624049559235573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012624049559235573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96192741
Iteration 2/25 | Loss: 0.00053322
Iteration 3/25 | Loss: 0.00053321
Iteration 4/25 | Loss: 0.00053321
Iteration 5/25 | Loss: 0.00053321
Iteration 6/25 | Loss: 0.00053321
Iteration 7/25 | Loss: 0.00053321
Iteration 8/25 | Loss: 0.00053321
Iteration 9/25 | Loss: 0.00053321
Iteration 10/25 | Loss: 0.00053321
Iteration 11/25 | Loss: 0.00053321
Iteration 12/25 | Loss: 0.00053321
Iteration 13/25 | Loss: 0.00053321
Iteration 14/25 | Loss: 0.00053321
Iteration 15/25 | Loss: 0.00053321
Iteration 16/25 | Loss: 0.00053321
Iteration 17/25 | Loss: 0.00053321
Iteration 18/25 | Loss: 0.00053321
Iteration 19/25 | Loss: 0.00053321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005332125583663583, 0.0005332125583663583, 0.0005332125583663583, 0.0005332125583663583, 0.0005332125583663583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005332125583663583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053321
Iteration 2/1000 | Loss: 0.00004143
Iteration 3/1000 | Loss: 0.00003300
Iteration 4/1000 | Loss: 0.00003108
Iteration 5/1000 | Loss: 0.00002969
Iteration 6/1000 | Loss: 0.00002891
Iteration 7/1000 | Loss: 0.00002842
Iteration 8/1000 | Loss: 0.00002814
Iteration 9/1000 | Loss: 0.00002788
Iteration 10/1000 | Loss: 0.00002781
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002765
Iteration 13/1000 | Loss: 0.00002746
Iteration 14/1000 | Loss: 0.00002746
Iteration 15/1000 | Loss: 0.00002745
Iteration 16/1000 | Loss: 0.00002745
Iteration 17/1000 | Loss: 0.00002745
Iteration 18/1000 | Loss: 0.00002745
Iteration 19/1000 | Loss: 0.00002744
Iteration 20/1000 | Loss: 0.00002744
Iteration 21/1000 | Loss: 0.00002744
Iteration 22/1000 | Loss: 0.00002744
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002744
Iteration 26/1000 | Loss: 0.00002743
Iteration 27/1000 | Loss: 0.00002743
Iteration 28/1000 | Loss: 0.00002736
Iteration 29/1000 | Loss: 0.00002736
Iteration 30/1000 | Loss: 0.00002735
Iteration 31/1000 | Loss: 0.00002734
Iteration 32/1000 | Loss: 0.00002733
Iteration 33/1000 | Loss: 0.00002730
Iteration 34/1000 | Loss: 0.00002728
Iteration 35/1000 | Loss: 0.00002727
Iteration 36/1000 | Loss: 0.00002727
Iteration 37/1000 | Loss: 0.00002727
Iteration 38/1000 | Loss: 0.00002727
Iteration 39/1000 | Loss: 0.00002727
Iteration 40/1000 | Loss: 0.00002727
Iteration 41/1000 | Loss: 0.00002727
Iteration 42/1000 | Loss: 0.00002727
Iteration 43/1000 | Loss: 0.00002727
Iteration 44/1000 | Loss: 0.00002727
Iteration 45/1000 | Loss: 0.00002727
Iteration 46/1000 | Loss: 0.00002727
Iteration 47/1000 | Loss: 0.00002727
Iteration 48/1000 | Loss: 0.00002727
Iteration 49/1000 | Loss: 0.00002727
Iteration 50/1000 | Loss: 0.00002727
Iteration 51/1000 | Loss: 0.00002727
Iteration 52/1000 | Loss: 0.00002727
Iteration 53/1000 | Loss: 0.00002727
Iteration 54/1000 | Loss: 0.00002727
Iteration 55/1000 | Loss: 0.00002727
Iteration 56/1000 | Loss: 0.00002727
Iteration 57/1000 | Loss: 0.00002727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [2.7267269615549594e-05, 2.7267269615549594e-05, 2.7267269615549594e-05, 2.7267269615549594e-05, 2.7267269615549594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7267269615549594e-05

Optimization complete. Final v2v error: 4.365950584411621 mm

Highest mean error: 4.474818229675293 mm for frame 19

Lowest mean error: 4.237021446228027 mm for frame 170

Saving results

Total time: 28.656665563583374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001899
Iteration 2/25 | Loss: 0.01001899
Iteration 3/25 | Loss: 0.01001899
Iteration 4/25 | Loss: 0.01001899
Iteration 5/25 | Loss: 0.01001899
Iteration 6/25 | Loss: 0.01001899
Iteration 7/25 | Loss: 0.01001899
Iteration 8/25 | Loss: 0.01001898
Iteration 9/25 | Loss: 0.01001898
Iteration 10/25 | Loss: 0.01001898
Iteration 11/25 | Loss: 0.01001898
Iteration 12/25 | Loss: 0.01001898
Iteration 13/25 | Loss: 0.01001898
Iteration 14/25 | Loss: 0.01001898
Iteration 15/25 | Loss: 0.01001898
Iteration 16/25 | Loss: 0.01001898
Iteration 17/25 | Loss: 0.01001898
Iteration 18/25 | Loss: 0.01001898
Iteration 19/25 | Loss: 0.01001897
Iteration 20/25 | Loss: 0.01001897
Iteration 21/25 | Loss: 0.01001897
Iteration 22/25 | Loss: 0.01001897
Iteration 23/25 | Loss: 0.01001897
Iteration 24/25 | Loss: 0.01001897
Iteration 25/25 | Loss: 0.01001897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68472207
Iteration 2/25 | Loss: 0.11814384
Iteration 3/25 | Loss: 0.11754593
Iteration 4/25 | Loss: 0.11754591
Iteration 5/25 | Loss: 0.11754591
Iteration 6/25 | Loss: 0.11754591
Iteration 7/25 | Loss: 0.11754590
Iteration 8/25 | Loss: 0.11754590
Iteration 9/25 | Loss: 0.11754590
Iteration 10/25 | Loss: 0.11754590
Iteration 11/25 | Loss: 0.11754590
Iteration 12/25 | Loss: 0.11754590
Iteration 13/25 | Loss: 0.11754590
Iteration 14/25 | Loss: 0.11754588
Iteration 15/25 | Loss: 0.11754590
Iteration 16/25 | Loss: 0.11754590
Iteration 17/25 | Loss: 0.11754590
Iteration 18/25 | Loss: 0.11754590
Iteration 19/25 | Loss: 0.11754590
Iteration 20/25 | Loss: 0.11754590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.11754589527845383, 0.11754589527845383, 0.11754589527845383, 0.11754589527845383, 0.11754589527845383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11754589527845383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11754590
Iteration 2/1000 | Loss: 0.00331785
Iteration 3/1000 | Loss: 0.00682781
Iteration 4/1000 | Loss: 0.00561142
Iteration 5/1000 | Loss: 0.00055195
Iteration 6/1000 | Loss: 0.00077343
Iteration 7/1000 | Loss: 0.00026760
Iteration 8/1000 | Loss: 0.00029803
Iteration 9/1000 | Loss: 0.00021862
Iteration 10/1000 | Loss: 0.00014095
Iteration 11/1000 | Loss: 0.00013504
Iteration 12/1000 | Loss: 0.00068026
Iteration 13/1000 | Loss: 0.00016786
Iteration 14/1000 | Loss: 0.00013675
Iteration 15/1000 | Loss: 0.00027356
Iteration 16/1000 | Loss: 0.00018881
Iteration 17/1000 | Loss: 0.00003437
Iteration 18/1000 | Loss: 0.00003100
Iteration 19/1000 | Loss: 0.00009644
Iteration 20/1000 | Loss: 0.00013071
Iteration 21/1000 | Loss: 0.00003627
Iteration 22/1000 | Loss: 0.00011697
Iteration 23/1000 | Loss: 0.00006385
Iteration 24/1000 | Loss: 0.00014586
Iteration 25/1000 | Loss: 0.00006083
Iteration 26/1000 | Loss: 0.00022749
Iteration 27/1000 | Loss: 0.00029458
Iteration 28/1000 | Loss: 0.00002476
Iteration 29/1000 | Loss: 0.00007318
Iteration 30/1000 | Loss: 0.00061735
Iteration 31/1000 | Loss: 0.00005190
Iteration 32/1000 | Loss: 0.00004126
Iteration 33/1000 | Loss: 0.00007957
Iteration 34/1000 | Loss: 0.00003244
Iteration 35/1000 | Loss: 0.00003802
Iteration 36/1000 | Loss: 0.00005621
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00015616
Iteration 39/1000 | Loss: 0.00098647
Iteration 40/1000 | Loss: 0.00018284
Iteration 41/1000 | Loss: 0.00002727
Iteration 42/1000 | Loss: 0.00005109
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00007382
Iteration 45/1000 | Loss: 0.00004751
Iteration 46/1000 | Loss: 0.00003340
Iteration 47/1000 | Loss: 0.00008702
Iteration 48/1000 | Loss: 0.00005746
Iteration 49/1000 | Loss: 0.00002198
Iteration 50/1000 | Loss: 0.00010555
Iteration 51/1000 | Loss: 0.00019651
Iteration 52/1000 | Loss: 0.00008868
Iteration 53/1000 | Loss: 0.00017308
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00002109
Iteration 56/1000 | Loss: 0.00002818
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001749
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00009878
Iteration 63/1000 | Loss: 0.00002934
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00001747
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002555
Iteration 68/1000 | Loss: 0.00006576
Iteration 69/1000 | Loss: 0.00003545
Iteration 70/1000 | Loss: 0.00002969
Iteration 71/1000 | Loss: 0.00002546
Iteration 72/1000 | Loss: 0.00003795
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00007201
Iteration 75/1000 | Loss: 0.00007201
Iteration 76/1000 | Loss: 0.00006822
Iteration 77/1000 | Loss: 0.00112342
Iteration 78/1000 | Loss: 0.00005541
Iteration 79/1000 | Loss: 0.00007790
Iteration 80/1000 | Loss: 0.00003317
Iteration 81/1000 | Loss: 0.00004168
Iteration 82/1000 | Loss: 0.00001938
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001736
Iteration 85/1000 | Loss: 0.00002261
Iteration 86/1000 | Loss: 0.00005906
Iteration 87/1000 | Loss: 0.00002028
Iteration 88/1000 | Loss: 0.00001946
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001728
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00002739
Iteration 100/1000 | Loss: 0.00002396
Iteration 101/1000 | Loss: 0.00002366
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001726
Iteration 114/1000 | Loss: 0.00001726
Iteration 115/1000 | Loss: 0.00001726
Iteration 116/1000 | Loss: 0.00002478
Iteration 117/1000 | Loss: 0.00004343
Iteration 118/1000 | Loss: 0.00019438
Iteration 119/1000 | Loss: 0.00003664
Iteration 120/1000 | Loss: 0.00004794
Iteration 121/1000 | Loss: 0.00030404
Iteration 122/1000 | Loss: 0.00002333
Iteration 123/1000 | Loss: 0.00004927
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00003082
Iteration 126/1000 | Loss: 0.00014237
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00002093
Iteration 129/1000 | Loss: 0.00004100
Iteration 130/1000 | Loss: 0.00002039
Iteration 131/1000 | Loss: 0.00002124
Iteration 132/1000 | Loss: 0.00003789
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001949
Iteration 135/1000 | Loss: 0.00002159
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00002619
Iteration 139/1000 | Loss: 0.00002323
Iteration 140/1000 | Loss: 0.00001762
Iteration 141/1000 | Loss: 0.00002274
Iteration 142/1000 | Loss: 0.00002445
Iteration 143/1000 | Loss: 0.00001718
Iteration 144/1000 | Loss: 0.00001718
Iteration 145/1000 | Loss: 0.00001718
Iteration 146/1000 | Loss: 0.00001718
Iteration 147/1000 | Loss: 0.00001717
Iteration 148/1000 | Loss: 0.00001717
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001716
Iteration 152/1000 | Loss: 0.00001716
Iteration 153/1000 | Loss: 0.00001716
Iteration 154/1000 | Loss: 0.00001716
Iteration 155/1000 | Loss: 0.00001716
Iteration 156/1000 | Loss: 0.00001716
Iteration 157/1000 | Loss: 0.00001716
Iteration 158/1000 | Loss: 0.00001716
Iteration 159/1000 | Loss: 0.00001716
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7160882634925656e-05, 1.7160882634925656e-05, 1.7160882634925656e-05, 1.7160882634925656e-05, 1.7160882634925656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7160882634925656e-05

Optimization complete. Final v2v error: 3.5113391876220703 mm

Highest mean error: 3.9120359420776367 mm for frame 224

Lowest mean error: 3.4135899543762207 mm for frame 202

Saving results

Total time: 180.57282066345215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414180
Iteration 2/25 | Loss: 0.00136638
Iteration 3/25 | Loss: 0.00123603
Iteration 4/25 | Loss: 0.00122335
Iteration 5/25 | Loss: 0.00122039
Iteration 6/25 | Loss: 0.00121966
Iteration 7/25 | Loss: 0.00121966
Iteration 8/25 | Loss: 0.00121966
Iteration 9/25 | Loss: 0.00121966
Iteration 10/25 | Loss: 0.00121966
Iteration 11/25 | Loss: 0.00121966
Iteration 12/25 | Loss: 0.00121966
Iteration 13/25 | Loss: 0.00121966
Iteration 14/25 | Loss: 0.00121966
Iteration 15/25 | Loss: 0.00121966
Iteration 16/25 | Loss: 0.00121966
Iteration 17/25 | Loss: 0.00121966
Iteration 18/25 | Loss: 0.00121966
Iteration 19/25 | Loss: 0.00121966
Iteration 20/25 | Loss: 0.00121966
Iteration 21/25 | Loss: 0.00121966
Iteration 22/25 | Loss: 0.00121966
Iteration 23/25 | Loss: 0.00121966
Iteration 24/25 | Loss: 0.00121966
Iteration 25/25 | Loss: 0.00121966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40187657
Iteration 2/25 | Loss: 0.00081876
Iteration 3/25 | Loss: 0.00081874
Iteration 4/25 | Loss: 0.00081874
Iteration 5/25 | Loss: 0.00081873
Iteration 6/25 | Loss: 0.00081873
Iteration 7/25 | Loss: 0.00081873
Iteration 8/25 | Loss: 0.00081873
Iteration 9/25 | Loss: 0.00081873
Iteration 10/25 | Loss: 0.00081873
Iteration 11/25 | Loss: 0.00081873
Iteration 12/25 | Loss: 0.00081873
Iteration 13/25 | Loss: 0.00081873
Iteration 14/25 | Loss: 0.00081873
Iteration 15/25 | Loss: 0.00081873
Iteration 16/25 | Loss: 0.00081873
Iteration 17/25 | Loss: 0.00081873
Iteration 18/25 | Loss: 0.00081873
Iteration 19/25 | Loss: 0.00081873
Iteration 20/25 | Loss: 0.00081873
Iteration 21/25 | Loss: 0.00081873
Iteration 22/25 | Loss: 0.00081873
Iteration 23/25 | Loss: 0.00081873
Iteration 24/25 | Loss: 0.00081873
Iteration 25/25 | Loss: 0.00081873

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081873
Iteration 2/1000 | Loss: 0.00003997
Iteration 3/1000 | Loss: 0.00002704
Iteration 4/1000 | Loss: 0.00002210
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001700
Iteration 10/1000 | Loss: 0.00001671
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001636
Iteration 14/1000 | Loss: 0.00001618
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001614
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001614
Iteration 22/1000 | Loss: 0.00001614
Iteration 23/1000 | Loss: 0.00001613
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001612
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001609
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001608
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001606
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001605
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001604
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001604
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001603
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001587
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001586
Iteration 84/1000 | Loss: 0.00001586
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001584
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001583
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001582
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Iteration 115/1000 | Loss: 0.00001579
Iteration 116/1000 | Loss: 0.00001579
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001577
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001576
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001575
Iteration 132/1000 | Loss: 0.00001575
Iteration 133/1000 | Loss: 0.00001575
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001574
Iteration 136/1000 | Loss: 0.00001574
Iteration 137/1000 | Loss: 0.00001574
Iteration 138/1000 | Loss: 0.00001574
Iteration 139/1000 | Loss: 0.00001574
Iteration 140/1000 | Loss: 0.00001574
Iteration 141/1000 | Loss: 0.00001574
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001573
Iteration 145/1000 | Loss: 0.00001573
Iteration 146/1000 | Loss: 0.00001573
Iteration 147/1000 | Loss: 0.00001573
Iteration 148/1000 | Loss: 0.00001573
Iteration 149/1000 | Loss: 0.00001573
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001572
Iteration 158/1000 | Loss: 0.00001572
Iteration 159/1000 | Loss: 0.00001572
Iteration 160/1000 | Loss: 0.00001572
Iteration 161/1000 | Loss: 0.00001572
Iteration 162/1000 | Loss: 0.00001572
Iteration 163/1000 | Loss: 0.00001572
Iteration 164/1000 | Loss: 0.00001572
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001571
Iteration 171/1000 | Loss: 0.00001571
Iteration 172/1000 | Loss: 0.00001571
Iteration 173/1000 | Loss: 0.00001571
Iteration 174/1000 | Loss: 0.00001570
Iteration 175/1000 | Loss: 0.00001570
Iteration 176/1000 | Loss: 0.00001570
Iteration 177/1000 | Loss: 0.00001570
Iteration 178/1000 | Loss: 0.00001570
Iteration 179/1000 | Loss: 0.00001570
Iteration 180/1000 | Loss: 0.00001570
Iteration 181/1000 | Loss: 0.00001570
Iteration 182/1000 | Loss: 0.00001570
Iteration 183/1000 | Loss: 0.00001570
Iteration 184/1000 | Loss: 0.00001570
Iteration 185/1000 | Loss: 0.00001570
Iteration 186/1000 | Loss: 0.00001569
Iteration 187/1000 | Loss: 0.00001569
Iteration 188/1000 | Loss: 0.00001569
Iteration 189/1000 | Loss: 0.00001569
Iteration 190/1000 | Loss: 0.00001569
Iteration 191/1000 | Loss: 0.00001569
Iteration 192/1000 | Loss: 0.00001569
Iteration 193/1000 | Loss: 0.00001569
Iteration 194/1000 | Loss: 0.00001569
Iteration 195/1000 | Loss: 0.00001569
Iteration 196/1000 | Loss: 0.00001569
Iteration 197/1000 | Loss: 0.00001569
Iteration 198/1000 | Loss: 0.00001568
Iteration 199/1000 | Loss: 0.00001568
Iteration 200/1000 | Loss: 0.00001568
Iteration 201/1000 | Loss: 0.00001568
Iteration 202/1000 | Loss: 0.00001568
Iteration 203/1000 | Loss: 0.00001568
Iteration 204/1000 | Loss: 0.00001568
Iteration 205/1000 | Loss: 0.00001568
Iteration 206/1000 | Loss: 0.00001568
Iteration 207/1000 | Loss: 0.00001568
Iteration 208/1000 | Loss: 0.00001568
Iteration 209/1000 | Loss: 0.00001567
Iteration 210/1000 | Loss: 0.00001567
Iteration 211/1000 | Loss: 0.00001567
Iteration 212/1000 | Loss: 0.00001567
Iteration 213/1000 | Loss: 0.00001567
Iteration 214/1000 | Loss: 0.00001567
Iteration 215/1000 | Loss: 0.00001567
Iteration 216/1000 | Loss: 0.00001567
Iteration 217/1000 | Loss: 0.00001567
Iteration 218/1000 | Loss: 0.00001567
Iteration 219/1000 | Loss: 0.00001566
Iteration 220/1000 | Loss: 0.00001566
Iteration 221/1000 | Loss: 0.00001566
Iteration 222/1000 | Loss: 0.00001566
Iteration 223/1000 | Loss: 0.00001566
Iteration 224/1000 | Loss: 0.00001566
Iteration 225/1000 | Loss: 0.00001566
Iteration 226/1000 | Loss: 0.00001566
Iteration 227/1000 | Loss: 0.00001566
Iteration 228/1000 | Loss: 0.00001566
Iteration 229/1000 | Loss: 0.00001566
Iteration 230/1000 | Loss: 0.00001566
Iteration 231/1000 | Loss: 0.00001566
Iteration 232/1000 | Loss: 0.00001566
Iteration 233/1000 | Loss: 0.00001565
Iteration 234/1000 | Loss: 0.00001565
Iteration 235/1000 | Loss: 0.00001565
Iteration 236/1000 | Loss: 0.00001565
Iteration 237/1000 | Loss: 0.00001565
Iteration 238/1000 | Loss: 0.00001565
Iteration 239/1000 | Loss: 0.00001565
Iteration 240/1000 | Loss: 0.00001564
Iteration 241/1000 | Loss: 0.00001564
Iteration 242/1000 | Loss: 0.00001564
Iteration 243/1000 | Loss: 0.00001564
Iteration 244/1000 | Loss: 0.00001564
Iteration 245/1000 | Loss: 0.00001564
Iteration 246/1000 | Loss: 0.00001564
Iteration 247/1000 | Loss: 0.00001564
Iteration 248/1000 | Loss: 0.00001564
Iteration 249/1000 | Loss: 0.00001563
Iteration 250/1000 | Loss: 0.00001563
Iteration 251/1000 | Loss: 0.00001563
Iteration 252/1000 | Loss: 0.00001563
Iteration 253/1000 | Loss: 0.00001563
Iteration 254/1000 | Loss: 0.00001563
Iteration 255/1000 | Loss: 0.00001563
Iteration 256/1000 | Loss: 0.00001563
Iteration 257/1000 | Loss: 0.00001562
Iteration 258/1000 | Loss: 0.00001562
Iteration 259/1000 | Loss: 0.00001562
Iteration 260/1000 | Loss: 0.00001562
Iteration 261/1000 | Loss: 0.00001562
Iteration 262/1000 | Loss: 0.00001562
Iteration 263/1000 | Loss: 0.00001562
Iteration 264/1000 | Loss: 0.00001562
Iteration 265/1000 | Loss: 0.00001562
Iteration 266/1000 | Loss: 0.00001561
Iteration 267/1000 | Loss: 0.00001561
Iteration 268/1000 | Loss: 0.00001561
Iteration 269/1000 | Loss: 0.00001561
Iteration 270/1000 | Loss: 0.00001561
Iteration 271/1000 | Loss: 0.00001560
Iteration 272/1000 | Loss: 0.00001560
Iteration 273/1000 | Loss: 0.00001560
Iteration 274/1000 | Loss: 0.00001560
Iteration 275/1000 | Loss: 0.00001560
Iteration 276/1000 | Loss: 0.00001560
Iteration 277/1000 | Loss: 0.00001560
Iteration 278/1000 | Loss: 0.00001560
Iteration 279/1000 | Loss: 0.00001560
Iteration 280/1000 | Loss: 0.00001560
Iteration 281/1000 | Loss: 0.00001560
Iteration 282/1000 | Loss: 0.00001560
Iteration 283/1000 | Loss: 0.00001560
Iteration 284/1000 | Loss: 0.00001560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.5600504411850125e-05, 1.5600504411850125e-05, 1.5600504411850125e-05, 1.5600504411850125e-05, 1.5600504411850125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5600504411850125e-05

Optimization complete. Final v2v error: 3.2545013427734375 mm

Highest mean error: 5.170172691345215 mm for frame 82

Lowest mean error: 2.7712485790252686 mm for frame 172

Saving results

Total time: 47.014880895614624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827642
Iteration 2/25 | Loss: 0.00157108
Iteration 3/25 | Loss: 0.00133943
Iteration 4/25 | Loss: 0.00131516
Iteration 5/25 | Loss: 0.00130941
Iteration 6/25 | Loss: 0.00132377
Iteration 7/25 | Loss: 0.00132788
Iteration 8/25 | Loss: 0.00130856
Iteration 9/25 | Loss: 0.00129623
Iteration 10/25 | Loss: 0.00128893
Iteration 11/25 | Loss: 0.00128399
Iteration 12/25 | Loss: 0.00128109
Iteration 13/25 | Loss: 0.00128468
Iteration 14/25 | Loss: 0.00127910
Iteration 15/25 | Loss: 0.00127791
Iteration 16/25 | Loss: 0.00127768
Iteration 17/25 | Loss: 0.00127765
Iteration 18/25 | Loss: 0.00127765
Iteration 19/25 | Loss: 0.00127764
Iteration 20/25 | Loss: 0.00127764
Iteration 21/25 | Loss: 0.00127764
Iteration 22/25 | Loss: 0.00127764
Iteration 23/25 | Loss: 0.00127764
Iteration 24/25 | Loss: 0.00127764
Iteration 25/25 | Loss: 0.00127764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.18205166
Iteration 2/25 | Loss: 0.00086466
Iteration 3/25 | Loss: 0.00086458
Iteration 4/25 | Loss: 0.00086458
Iteration 5/25 | Loss: 0.00086458
Iteration 6/25 | Loss: 0.00086458
Iteration 7/25 | Loss: 0.00086458
Iteration 8/25 | Loss: 0.00086458
Iteration 9/25 | Loss: 0.00086458
Iteration 10/25 | Loss: 0.00086458
Iteration 11/25 | Loss: 0.00086458
Iteration 12/25 | Loss: 0.00086458
Iteration 13/25 | Loss: 0.00086458
Iteration 14/25 | Loss: 0.00086458
Iteration 15/25 | Loss: 0.00086458
Iteration 16/25 | Loss: 0.00086458
Iteration 17/25 | Loss: 0.00086458
Iteration 18/25 | Loss: 0.00086458
Iteration 19/25 | Loss: 0.00086458
Iteration 20/25 | Loss: 0.00086458
Iteration 21/25 | Loss: 0.00086458
Iteration 22/25 | Loss: 0.00086458
Iteration 23/25 | Loss: 0.00086458
Iteration 24/25 | Loss: 0.00086458
Iteration 25/25 | Loss: 0.00086458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086458
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00002571
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00004301
Iteration 6/1000 | Loss: 0.00004187
Iteration 7/1000 | Loss: 0.00004333
Iteration 8/1000 | Loss: 0.00003318
Iteration 9/1000 | Loss: 0.00002857
Iteration 10/1000 | Loss: 0.00004048
Iteration 11/1000 | Loss: 0.00004000
Iteration 12/1000 | Loss: 0.00004149
Iteration 13/1000 | Loss: 0.00004039
Iteration 14/1000 | Loss: 0.00005357
Iteration 15/1000 | Loss: 0.00004637
Iteration 16/1000 | Loss: 0.00005459
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00005661
Iteration 19/1000 | Loss: 0.00027625
Iteration 20/1000 | Loss: 0.00002861
Iteration 21/1000 | Loss: 0.00002449
Iteration 22/1000 | Loss: 0.00002342
Iteration 23/1000 | Loss: 0.00002727
Iteration 24/1000 | Loss: 0.00018614
Iteration 25/1000 | Loss: 0.00018925
Iteration 26/1000 | Loss: 0.00029916
Iteration 27/1000 | Loss: 0.00005345
Iteration 28/1000 | Loss: 0.00004164
Iteration 29/1000 | Loss: 0.00002288
Iteration 30/1000 | Loss: 0.00002218
Iteration 31/1000 | Loss: 0.00002159
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00001935
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001803
Iteration 38/1000 | Loss: 0.00001803
Iteration 39/1000 | Loss: 0.00001802
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001790
Iteration 42/1000 | Loss: 0.00001790
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001785
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001780
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001779
Iteration 66/1000 | Loss: 0.00001779
Iteration 67/1000 | Loss: 0.00001779
Iteration 68/1000 | Loss: 0.00001779
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.7794067389331758e-05, 1.7794067389331758e-05, 1.7794067389331758e-05, 1.7794067389331758e-05, 1.7794067389331758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7794067389331758e-05

Optimization complete. Final v2v error: 3.5333895683288574 mm

Highest mean error: 5.516450881958008 mm for frame 58

Lowest mean error: 3.0683608055114746 mm for frame 32

Saving results

Total time: 85.3533821105957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422601
Iteration 2/25 | Loss: 0.00132003
Iteration 3/25 | Loss: 0.00124155
Iteration 4/25 | Loss: 0.00123615
Iteration 5/25 | Loss: 0.00123356
Iteration 6/25 | Loss: 0.00123356
Iteration 7/25 | Loss: 0.00123356
Iteration 8/25 | Loss: 0.00123356
Iteration 9/25 | Loss: 0.00123356
Iteration 10/25 | Loss: 0.00123356
Iteration 11/25 | Loss: 0.00123356
Iteration 12/25 | Loss: 0.00123356
Iteration 13/25 | Loss: 0.00123356
Iteration 14/25 | Loss: 0.00123356
Iteration 15/25 | Loss: 0.00123356
Iteration 16/25 | Loss: 0.00123356
Iteration 17/25 | Loss: 0.00123356
Iteration 18/25 | Loss: 0.00123356
Iteration 19/25 | Loss: 0.00123356
Iteration 20/25 | Loss: 0.00123356
Iteration 21/25 | Loss: 0.00123356
Iteration 22/25 | Loss: 0.00123356
Iteration 23/25 | Loss: 0.00123356
Iteration 24/25 | Loss: 0.00123356
Iteration 25/25 | Loss: 0.00123356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53534985
Iteration 2/25 | Loss: 0.00069248
Iteration 3/25 | Loss: 0.00069247
Iteration 4/25 | Loss: 0.00069247
Iteration 5/25 | Loss: 0.00069247
Iteration 6/25 | Loss: 0.00069247
Iteration 7/25 | Loss: 0.00069247
Iteration 8/25 | Loss: 0.00069247
Iteration 9/25 | Loss: 0.00069247
Iteration 10/25 | Loss: 0.00069247
Iteration 11/25 | Loss: 0.00069247
Iteration 12/25 | Loss: 0.00069247
Iteration 13/25 | Loss: 0.00069247
Iteration 14/25 | Loss: 0.00069247
Iteration 15/25 | Loss: 0.00069247
Iteration 16/25 | Loss: 0.00069247
Iteration 17/25 | Loss: 0.00069247
Iteration 18/25 | Loss: 0.00069247
Iteration 19/25 | Loss: 0.00069247
Iteration 20/25 | Loss: 0.00069247
Iteration 21/25 | Loss: 0.00069247
Iteration 22/25 | Loss: 0.00069247
Iteration 23/25 | Loss: 0.00069247
Iteration 24/25 | Loss: 0.00069247
Iteration 25/25 | Loss: 0.00069247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006924718036316335, 0.0006924718036316335, 0.0006924718036316335, 0.0006924718036316335, 0.0006924718036316335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006924718036316335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069247
Iteration 2/1000 | Loss: 0.00002469
Iteration 3/1000 | Loss: 0.00001753
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001281
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001245
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001245
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001245
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001245
Iteration 43/1000 | Loss: 0.00001245
Iteration 44/1000 | Loss: 0.00001245
Iteration 45/1000 | Loss: 0.00001245
Iteration 46/1000 | Loss: 0.00001245
Iteration 47/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [1.2447941116988659e-05, 1.2447941116988659e-05, 1.2447941116988659e-05, 1.2447941116988659e-05, 1.2447941116988659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2447941116988659e-05

Optimization complete. Final v2v error: 3.0346813201904297 mm

Highest mean error: 3.2753920555114746 mm for frame 163

Lowest mean error: 2.8703792095184326 mm for frame 13

Saving results

Total time: 27.926313161849976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483117
Iteration 2/25 | Loss: 0.00131732
Iteration 3/25 | Loss: 0.00124978
Iteration 4/25 | Loss: 0.00123922
Iteration 5/25 | Loss: 0.00123635
Iteration 6/25 | Loss: 0.00123635
Iteration 7/25 | Loss: 0.00123635
Iteration 8/25 | Loss: 0.00123635
Iteration 9/25 | Loss: 0.00123635
Iteration 10/25 | Loss: 0.00123635
Iteration 11/25 | Loss: 0.00123635
Iteration 12/25 | Loss: 0.00123635
Iteration 13/25 | Loss: 0.00123635
Iteration 14/25 | Loss: 0.00123635
Iteration 15/25 | Loss: 0.00123635
Iteration 16/25 | Loss: 0.00123635
Iteration 17/25 | Loss: 0.00123635
Iteration 18/25 | Loss: 0.00123635
Iteration 19/25 | Loss: 0.00123635
Iteration 20/25 | Loss: 0.00123635
Iteration 21/25 | Loss: 0.00123635
Iteration 22/25 | Loss: 0.00123635
Iteration 23/25 | Loss: 0.00123635
Iteration 24/25 | Loss: 0.00123635
Iteration 25/25 | Loss: 0.00123635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47136080
Iteration 2/25 | Loss: 0.00083302
Iteration 3/25 | Loss: 0.00083302
Iteration 4/25 | Loss: 0.00083302
Iteration 5/25 | Loss: 0.00083301
Iteration 6/25 | Loss: 0.00083301
Iteration 7/25 | Loss: 0.00083301
Iteration 8/25 | Loss: 0.00083301
Iteration 9/25 | Loss: 0.00083301
Iteration 10/25 | Loss: 0.00083301
Iteration 11/25 | Loss: 0.00083301
Iteration 12/25 | Loss: 0.00083301
Iteration 13/25 | Loss: 0.00083301
Iteration 14/25 | Loss: 0.00083301
Iteration 15/25 | Loss: 0.00083301
Iteration 16/25 | Loss: 0.00083301
Iteration 17/25 | Loss: 0.00083301
Iteration 18/25 | Loss: 0.00083301
Iteration 19/25 | Loss: 0.00083301
Iteration 20/25 | Loss: 0.00083301
Iteration 21/25 | Loss: 0.00083301
Iteration 22/25 | Loss: 0.00083301
Iteration 23/25 | Loss: 0.00083301
Iteration 24/25 | Loss: 0.00083301
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008330125128850341, 0.0008330125128850341, 0.0008330125128850341, 0.0008330125128850341, 0.0008330125128850341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008330125128850341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083301
Iteration 2/1000 | Loss: 0.00002879
Iteration 3/1000 | Loss: 0.00002069
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001806
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001680
Iteration 11/1000 | Loss: 0.00001675
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001629
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001624
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001623
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001620
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001614
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001613
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001605
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001604
Iteration 42/1000 | Loss: 0.00001604
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001602
Iteration 47/1000 | Loss: 0.00001601
Iteration 48/1000 | Loss: 0.00001601
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001598
Iteration 60/1000 | Loss: 0.00001598
Iteration 61/1000 | Loss: 0.00001598
Iteration 62/1000 | Loss: 0.00001597
Iteration 63/1000 | Loss: 0.00001597
Iteration 64/1000 | Loss: 0.00001597
Iteration 65/1000 | Loss: 0.00001596
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001595
Iteration 72/1000 | Loss: 0.00001594
Iteration 73/1000 | Loss: 0.00001594
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001593
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001593
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001592
Iteration 82/1000 | Loss: 0.00001592
Iteration 83/1000 | Loss: 0.00001592
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001591
Iteration 86/1000 | Loss: 0.00001591
Iteration 87/1000 | Loss: 0.00001591
Iteration 88/1000 | Loss: 0.00001591
Iteration 89/1000 | Loss: 0.00001590
Iteration 90/1000 | Loss: 0.00001590
Iteration 91/1000 | Loss: 0.00001590
Iteration 92/1000 | Loss: 0.00001590
Iteration 93/1000 | Loss: 0.00001590
Iteration 94/1000 | Loss: 0.00001590
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001589
Iteration 97/1000 | Loss: 0.00001589
Iteration 98/1000 | Loss: 0.00001589
Iteration 99/1000 | Loss: 0.00001588
Iteration 100/1000 | Loss: 0.00001588
Iteration 101/1000 | Loss: 0.00001588
Iteration 102/1000 | Loss: 0.00001588
Iteration 103/1000 | Loss: 0.00001588
Iteration 104/1000 | Loss: 0.00001588
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001587
Iteration 107/1000 | Loss: 0.00001587
Iteration 108/1000 | Loss: 0.00001587
Iteration 109/1000 | Loss: 0.00001587
Iteration 110/1000 | Loss: 0.00001587
Iteration 111/1000 | Loss: 0.00001587
Iteration 112/1000 | Loss: 0.00001587
Iteration 113/1000 | Loss: 0.00001587
Iteration 114/1000 | Loss: 0.00001587
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001586
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001585
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5853995137149468e-05, 1.5853995137149468e-05, 1.5853995137149468e-05, 1.5853995137149468e-05, 1.5853995137149468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5853995137149468e-05

Optimization complete. Final v2v error: 3.3051955699920654 mm

Highest mean error: 3.49934458732605 mm for frame 193

Lowest mean error: 3.095337152481079 mm for frame 18

Saving results

Total time: 41.98958110809326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381866
Iteration 2/25 | Loss: 0.00128783
Iteration 3/25 | Loss: 0.00120803
Iteration 4/25 | Loss: 0.00119821
Iteration 5/25 | Loss: 0.00119555
Iteration 6/25 | Loss: 0.00119512
Iteration 7/25 | Loss: 0.00119512
Iteration 8/25 | Loss: 0.00119512
Iteration 9/25 | Loss: 0.00119512
Iteration 10/25 | Loss: 0.00119512
Iteration 11/25 | Loss: 0.00119512
Iteration 12/25 | Loss: 0.00119512
Iteration 13/25 | Loss: 0.00119512
Iteration 14/25 | Loss: 0.00119512
Iteration 15/25 | Loss: 0.00119512
Iteration 16/25 | Loss: 0.00119512
Iteration 17/25 | Loss: 0.00119512
Iteration 18/25 | Loss: 0.00119512
Iteration 19/25 | Loss: 0.00119512
Iteration 20/25 | Loss: 0.00119512
Iteration 21/25 | Loss: 0.00119512
Iteration 22/25 | Loss: 0.00119512
Iteration 23/25 | Loss: 0.00119512
Iteration 24/25 | Loss: 0.00119512
Iteration 25/25 | Loss: 0.00119512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94693708
Iteration 2/25 | Loss: 0.00079657
Iteration 3/25 | Loss: 0.00079656
Iteration 4/25 | Loss: 0.00079656
Iteration 5/25 | Loss: 0.00079656
Iteration 6/25 | Loss: 0.00079656
Iteration 7/25 | Loss: 0.00079656
Iteration 8/25 | Loss: 0.00079656
Iteration 9/25 | Loss: 0.00079656
Iteration 10/25 | Loss: 0.00079656
Iteration 11/25 | Loss: 0.00079656
Iteration 12/25 | Loss: 0.00079656
Iteration 13/25 | Loss: 0.00079656
Iteration 14/25 | Loss: 0.00079656
Iteration 15/25 | Loss: 0.00079656
Iteration 16/25 | Loss: 0.00079656
Iteration 17/25 | Loss: 0.00079656
Iteration 18/25 | Loss: 0.00079656
Iteration 19/25 | Loss: 0.00079656
Iteration 20/25 | Loss: 0.00079656
Iteration 21/25 | Loss: 0.00079656
Iteration 22/25 | Loss: 0.00079656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000796558684669435, 0.000796558684669435, 0.000796558684669435, 0.000796558684669435, 0.000796558684669435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000796558684669435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079656
Iteration 2/1000 | Loss: 0.00002840
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001202
Iteration 7/1000 | Loss: 0.00001147
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001102
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001085
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001078
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001072
Iteration 23/1000 | Loss: 0.00001071
Iteration 24/1000 | Loss: 0.00001070
Iteration 25/1000 | Loss: 0.00001070
Iteration 26/1000 | Loss: 0.00001069
Iteration 27/1000 | Loss: 0.00001066
Iteration 28/1000 | Loss: 0.00001066
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001061
Iteration 32/1000 | Loss: 0.00001061
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001061
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001061
Iteration 40/1000 | Loss: 0.00001061
Iteration 41/1000 | Loss: 0.00001061
Iteration 42/1000 | Loss: 0.00001061
Iteration 43/1000 | Loss: 0.00001060
Iteration 44/1000 | Loss: 0.00001060
Iteration 45/1000 | Loss: 0.00001059
Iteration 46/1000 | Loss: 0.00001059
Iteration 47/1000 | Loss: 0.00001058
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001058
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001057
Iteration 52/1000 | Loss: 0.00001057
Iteration 53/1000 | Loss: 0.00001057
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001056
Iteration 56/1000 | Loss: 0.00001055
Iteration 57/1000 | Loss: 0.00001055
Iteration 58/1000 | Loss: 0.00001054
Iteration 59/1000 | Loss: 0.00001053
Iteration 60/1000 | Loss: 0.00001053
Iteration 61/1000 | Loss: 0.00001053
Iteration 62/1000 | Loss: 0.00001053
Iteration 63/1000 | Loss: 0.00001053
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001050
Iteration 70/1000 | Loss: 0.00001050
Iteration 71/1000 | Loss: 0.00001050
Iteration 72/1000 | Loss: 0.00001049
Iteration 73/1000 | Loss: 0.00001049
Iteration 74/1000 | Loss: 0.00001049
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001049
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001048
Iteration 79/1000 | Loss: 0.00001048
Iteration 80/1000 | Loss: 0.00001048
Iteration 81/1000 | Loss: 0.00001048
Iteration 82/1000 | Loss: 0.00001048
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001047
Iteration 85/1000 | Loss: 0.00001047
Iteration 86/1000 | Loss: 0.00001047
Iteration 87/1000 | Loss: 0.00001047
Iteration 88/1000 | Loss: 0.00001046
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001046
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001045
Iteration 94/1000 | Loss: 0.00001045
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001042
Iteration 102/1000 | Loss: 0.00001042
Iteration 103/1000 | Loss: 0.00001042
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001040
Iteration 108/1000 | Loss: 0.00001039
Iteration 109/1000 | Loss: 0.00001039
Iteration 110/1000 | Loss: 0.00001039
Iteration 111/1000 | Loss: 0.00001038
Iteration 112/1000 | Loss: 0.00001038
Iteration 113/1000 | Loss: 0.00001038
Iteration 114/1000 | Loss: 0.00001038
Iteration 115/1000 | Loss: 0.00001038
Iteration 116/1000 | Loss: 0.00001038
Iteration 117/1000 | Loss: 0.00001038
Iteration 118/1000 | Loss: 0.00001037
Iteration 119/1000 | Loss: 0.00001037
Iteration 120/1000 | Loss: 0.00001037
Iteration 121/1000 | Loss: 0.00001037
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001036
Iteration 125/1000 | Loss: 0.00001036
Iteration 126/1000 | Loss: 0.00001036
Iteration 127/1000 | Loss: 0.00001036
Iteration 128/1000 | Loss: 0.00001036
Iteration 129/1000 | Loss: 0.00001035
Iteration 130/1000 | Loss: 0.00001035
Iteration 131/1000 | Loss: 0.00001035
Iteration 132/1000 | Loss: 0.00001035
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001034
Iteration 135/1000 | Loss: 0.00001034
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001034
Iteration 138/1000 | Loss: 0.00001034
Iteration 139/1000 | Loss: 0.00001033
Iteration 140/1000 | Loss: 0.00001033
Iteration 141/1000 | Loss: 0.00001033
Iteration 142/1000 | Loss: 0.00001033
Iteration 143/1000 | Loss: 0.00001033
Iteration 144/1000 | Loss: 0.00001033
Iteration 145/1000 | Loss: 0.00001033
Iteration 146/1000 | Loss: 0.00001033
Iteration 147/1000 | Loss: 0.00001033
Iteration 148/1000 | Loss: 0.00001033
Iteration 149/1000 | Loss: 0.00001033
Iteration 150/1000 | Loss: 0.00001032
Iteration 151/1000 | Loss: 0.00001032
Iteration 152/1000 | Loss: 0.00001032
Iteration 153/1000 | Loss: 0.00001032
Iteration 154/1000 | Loss: 0.00001032
Iteration 155/1000 | Loss: 0.00001031
Iteration 156/1000 | Loss: 0.00001031
Iteration 157/1000 | Loss: 0.00001031
Iteration 158/1000 | Loss: 0.00001031
Iteration 159/1000 | Loss: 0.00001031
Iteration 160/1000 | Loss: 0.00001031
Iteration 161/1000 | Loss: 0.00001031
Iteration 162/1000 | Loss: 0.00001030
Iteration 163/1000 | Loss: 0.00001030
Iteration 164/1000 | Loss: 0.00001030
Iteration 165/1000 | Loss: 0.00001030
Iteration 166/1000 | Loss: 0.00001030
Iteration 167/1000 | Loss: 0.00001030
Iteration 168/1000 | Loss: 0.00001030
Iteration 169/1000 | Loss: 0.00001030
Iteration 170/1000 | Loss: 0.00001030
Iteration 171/1000 | Loss: 0.00001030
Iteration 172/1000 | Loss: 0.00001030
Iteration 173/1000 | Loss: 0.00001030
Iteration 174/1000 | Loss: 0.00001030
Iteration 175/1000 | Loss: 0.00001030
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001029
Iteration 179/1000 | Loss: 0.00001029
Iteration 180/1000 | Loss: 0.00001029
Iteration 181/1000 | Loss: 0.00001028
Iteration 182/1000 | Loss: 0.00001028
Iteration 183/1000 | Loss: 0.00001028
Iteration 184/1000 | Loss: 0.00001028
Iteration 185/1000 | Loss: 0.00001028
Iteration 186/1000 | Loss: 0.00001028
Iteration 187/1000 | Loss: 0.00001028
Iteration 188/1000 | Loss: 0.00001028
Iteration 189/1000 | Loss: 0.00001028
Iteration 190/1000 | Loss: 0.00001028
Iteration 191/1000 | Loss: 0.00001028
Iteration 192/1000 | Loss: 0.00001028
Iteration 193/1000 | Loss: 0.00001028
Iteration 194/1000 | Loss: 0.00001027
Iteration 195/1000 | Loss: 0.00001027
Iteration 196/1000 | Loss: 0.00001027
Iteration 197/1000 | Loss: 0.00001027
Iteration 198/1000 | Loss: 0.00001027
Iteration 199/1000 | Loss: 0.00001027
Iteration 200/1000 | Loss: 0.00001027
Iteration 201/1000 | Loss: 0.00001027
Iteration 202/1000 | Loss: 0.00001027
Iteration 203/1000 | Loss: 0.00001027
Iteration 204/1000 | Loss: 0.00001027
Iteration 205/1000 | Loss: 0.00001027
Iteration 206/1000 | Loss: 0.00001026
Iteration 207/1000 | Loss: 0.00001026
Iteration 208/1000 | Loss: 0.00001026
Iteration 209/1000 | Loss: 0.00001026
Iteration 210/1000 | Loss: 0.00001026
Iteration 211/1000 | Loss: 0.00001026
Iteration 212/1000 | Loss: 0.00001026
Iteration 213/1000 | Loss: 0.00001025
Iteration 214/1000 | Loss: 0.00001025
Iteration 215/1000 | Loss: 0.00001025
Iteration 216/1000 | Loss: 0.00001025
Iteration 217/1000 | Loss: 0.00001025
Iteration 218/1000 | Loss: 0.00001025
Iteration 219/1000 | Loss: 0.00001024
Iteration 220/1000 | Loss: 0.00001024
Iteration 221/1000 | Loss: 0.00001024
Iteration 222/1000 | Loss: 0.00001024
Iteration 223/1000 | Loss: 0.00001024
Iteration 224/1000 | Loss: 0.00001024
Iteration 225/1000 | Loss: 0.00001024
Iteration 226/1000 | Loss: 0.00001024
Iteration 227/1000 | Loss: 0.00001024
Iteration 228/1000 | Loss: 0.00001024
Iteration 229/1000 | Loss: 0.00001024
Iteration 230/1000 | Loss: 0.00001024
Iteration 231/1000 | Loss: 0.00001024
Iteration 232/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.024238190439064e-05, 1.024238190439064e-05, 1.024238190439064e-05, 1.024238190439064e-05, 1.024238190439064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.024238190439064e-05

Optimization complete. Final v2v error: 2.7340896129608154 mm

Highest mean error: 3.357076406478882 mm for frame 77

Lowest mean error: 2.576561212539673 mm for frame 154

Saving results

Total time: 40.79863095283508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00536382
Iteration 2/25 | Loss: 0.00145667
Iteration 3/25 | Loss: 0.00136671
Iteration 4/25 | Loss: 0.00135264
Iteration 5/25 | Loss: 0.00134765
Iteration 6/25 | Loss: 0.00134744
Iteration 7/25 | Loss: 0.00134744
Iteration 8/25 | Loss: 0.00134744
Iteration 9/25 | Loss: 0.00134744
Iteration 10/25 | Loss: 0.00134744
Iteration 11/25 | Loss: 0.00134744
Iteration 12/25 | Loss: 0.00134744
Iteration 13/25 | Loss: 0.00134744
Iteration 14/25 | Loss: 0.00134744
Iteration 15/25 | Loss: 0.00134744
Iteration 16/25 | Loss: 0.00134744
Iteration 17/25 | Loss: 0.00134744
Iteration 18/25 | Loss: 0.00134744
Iteration 19/25 | Loss: 0.00134744
Iteration 20/25 | Loss: 0.00134744
Iteration 21/25 | Loss: 0.00134744
Iteration 22/25 | Loss: 0.00134744
Iteration 23/25 | Loss: 0.00134744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013474394800141454, 0.0013474394800141454, 0.0013474394800141454, 0.0013474394800141454, 0.0013474394800141454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013474394800141454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79990286
Iteration 2/25 | Loss: 0.00064721
Iteration 3/25 | Loss: 0.00064721
Iteration 4/25 | Loss: 0.00064721
Iteration 5/25 | Loss: 0.00064721
Iteration 6/25 | Loss: 0.00064721
Iteration 7/25 | Loss: 0.00064720
Iteration 8/25 | Loss: 0.00064720
Iteration 9/25 | Loss: 0.00064720
Iteration 10/25 | Loss: 0.00064720
Iteration 11/25 | Loss: 0.00064720
Iteration 12/25 | Loss: 0.00064720
Iteration 13/25 | Loss: 0.00064720
Iteration 14/25 | Loss: 0.00064720
Iteration 15/25 | Loss: 0.00064720
Iteration 16/25 | Loss: 0.00064720
Iteration 17/25 | Loss: 0.00064720
Iteration 18/25 | Loss: 0.00064720
Iteration 19/25 | Loss: 0.00064720
Iteration 20/25 | Loss: 0.00064720
Iteration 21/25 | Loss: 0.00064720
Iteration 22/25 | Loss: 0.00064720
Iteration 23/25 | Loss: 0.00064720
Iteration 24/25 | Loss: 0.00064720
Iteration 25/25 | Loss: 0.00064720

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064720
Iteration 2/1000 | Loss: 0.00004951
Iteration 3/1000 | Loss: 0.00004336
Iteration 4/1000 | Loss: 0.00004085
Iteration 5/1000 | Loss: 0.00003973
Iteration 6/1000 | Loss: 0.00003914
Iteration 7/1000 | Loss: 0.00003856
Iteration 8/1000 | Loss: 0.00003806
Iteration 9/1000 | Loss: 0.00003769
Iteration 10/1000 | Loss: 0.00003707
Iteration 11/1000 | Loss: 0.00003675
Iteration 12/1000 | Loss: 0.00003641
Iteration 13/1000 | Loss: 0.00003605
Iteration 14/1000 | Loss: 0.00003585
Iteration 15/1000 | Loss: 0.00003564
Iteration 16/1000 | Loss: 0.00003552
Iteration 17/1000 | Loss: 0.00003543
Iteration 18/1000 | Loss: 0.00003541
Iteration 19/1000 | Loss: 0.00003540
Iteration 20/1000 | Loss: 0.00003540
Iteration 21/1000 | Loss: 0.00003536
Iteration 22/1000 | Loss: 0.00003535
Iteration 23/1000 | Loss: 0.00003530
Iteration 24/1000 | Loss: 0.00003528
Iteration 25/1000 | Loss: 0.00003528
Iteration 26/1000 | Loss: 0.00003528
Iteration 27/1000 | Loss: 0.00003528
Iteration 28/1000 | Loss: 0.00003527
Iteration 29/1000 | Loss: 0.00003527
Iteration 30/1000 | Loss: 0.00003527
Iteration 31/1000 | Loss: 0.00003527
Iteration 32/1000 | Loss: 0.00003526
Iteration 33/1000 | Loss: 0.00003526
Iteration 34/1000 | Loss: 0.00003524
Iteration 35/1000 | Loss: 0.00003524
Iteration 36/1000 | Loss: 0.00003524
Iteration 37/1000 | Loss: 0.00003524
Iteration 38/1000 | Loss: 0.00003524
Iteration 39/1000 | Loss: 0.00003524
Iteration 40/1000 | Loss: 0.00003524
Iteration 41/1000 | Loss: 0.00003523
Iteration 42/1000 | Loss: 0.00003520
Iteration 43/1000 | Loss: 0.00003520
Iteration 44/1000 | Loss: 0.00003520
Iteration 45/1000 | Loss: 0.00003520
Iteration 46/1000 | Loss: 0.00003520
Iteration 47/1000 | Loss: 0.00003519
Iteration 48/1000 | Loss: 0.00003519
Iteration 49/1000 | Loss: 0.00003519
Iteration 50/1000 | Loss: 0.00003519
Iteration 51/1000 | Loss: 0.00003519
Iteration 52/1000 | Loss: 0.00003518
Iteration 53/1000 | Loss: 0.00003518
Iteration 54/1000 | Loss: 0.00003518
Iteration 55/1000 | Loss: 0.00003517
Iteration 56/1000 | Loss: 0.00003517
Iteration 57/1000 | Loss: 0.00003517
Iteration 58/1000 | Loss: 0.00003516
Iteration 59/1000 | Loss: 0.00003516
Iteration 60/1000 | Loss: 0.00003516
Iteration 61/1000 | Loss: 0.00003516
Iteration 62/1000 | Loss: 0.00003516
Iteration 63/1000 | Loss: 0.00003516
Iteration 64/1000 | Loss: 0.00003516
Iteration 65/1000 | Loss: 0.00003516
Iteration 66/1000 | Loss: 0.00003515
Iteration 67/1000 | Loss: 0.00003515
Iteration 68/1000 | Loss: 0.00003515
Iteration 69/1000 | Loss: 0.00003514
Iteration 70/1000 | Loss: 0.00003514
Iteration 71/1000 | Loss: 0.00003514
Iteration 72/1000 | Loss: 0.00003514
Iteration 73/1000 | Loss: 0.00003514
Iteration 74/1000 | Loss: 0.00003514
Iteration 75/1000 | Loss: 0.00003514
Iteration 76/1000 | Loss: 0.00003514
Iteration 77/1000 | Loss: 0.00003514
Iteration 78/1000 | Loss: 0.00003514
Iteration 79/1000 | Loss: 0.00003514
Iteration 80/1000 | Loss: 0.00003514
Iteration 81/1000 | Loss: 0.00003514
Iteration 82/1000 | Loss: 0.00003514
Iteration 83/1000 | Loss: 0.00003514
Iteration 84/1000 | Loss: 0.00003514
Iteration 85/1000 | Loss: 0.00003513
Iteration 86/1000 | Loss: 0.00003513
Iteration 87/1000 | Loss: 0.00003513
Iteration 88/1000 | Loss: 0.00003513
Iteration 89/1000 | Loss: 0.00003513
Iteration 90/1000 | Loss: 0.00003513
Iteration 91/1000 | Loss: 0.00003513
Iteration 92/1000 | Loss: 0.00003513
Iteration 93/1000 | Loss: 0.00003513
Iteration 94/1000 | Loss: 0.00003513
Iteration 95/1000 | Loss: 0.00003513
Iteration 96/1000 | Loss: 0.00003513
Iteration 97/1000 | Loss: 0.00003513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [3.513442788971588e-05, 3.513442788971588e-05, 3.513442788971588e-05, 3.513442788971588e-05, 3.513442788971588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.513442788971588e-05

Optimization complete. Final v2v error: 4.699873924255371 mm

Highest mean error: 4.778753280639648 mm for frame 223

Lowest mean error: 4.659293174743652 mm for frame 53

Saving results

Total time: 43.770620584487915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851700
Iteration 2/25 | Loss: 0.00151551
Iteration 3/25 | Loss: 0.00134693
Iteration 4/25 | Loss: 0.00132518
Iteration 5/25 | Loss: 0.00132019
Iteration 6/25 | Loss: 0.00131912
Iteration 7/25 | Loss: 0.00131912
Iteration 8/25 | Loss: 0.00131912
Iteration 9/25 | Loss: 0.00131912
Iteration 10/25 | Loss: 0.00131912
Iteration 11/25 | Loss: 0.00131912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013191240141168237, 0.0013191240141168237, 0.0013191240141168237, 0.0013191240141168237, 0.0013191240141168237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013191240141168237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40144825
Iteration 2/25 | Loss: 0.00065460
Iteration 3/25 | Loss: 0.00065460
Iteration 4/25 | Loss: 0.00065460
Iteration 5/25 | Loss: 0.00065460
Iteration 6/25 | Loss: 0.00065460
Iteration 7/25 | Loss: 0.00065459
Iteration 8/25 | Loss: 0.00065459
Iteration 9/25 | Loss: 0.00065459
Iteration 10/25 | Loss: 0.00065459
Iteration 11/25 | Loss: 0.00065459
Iteration 12/25 | Loss: 0.00065459
Iteration 13/25 | Loss: 0.00065459
Iteration 14/25 | Loss: 0.00065459
Iteration 15/25 | Loss: 0.00065459
Iteration 16/25 | Loss: 0.00065459
Iteration 17/25 | Loss: 0.00065459
Iteration 18/25 | Loss: 0.00065459
Iteration 19/25 | Loss: 0.00065459
Iteration 20/25 | Loss: 0.00065459
Iteration 21/25 | Loss: 0.00065459
Iteration 22/25 | Loss: 0.00065459
Iteration 23/25 | Loss: 0.00065459
Iteration 24/25 | Loss: 0.00065459
Iteration 25/25 | Loss: 0.00065459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006545939831994474, 0.0006545939831994474, 0.0006545939831994474, 0.0006545939831994474, 0.0006545939831994474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006545939831994474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065459
Iteration 2/1000 | Loss: 0.00006837
Iteration 3/1000 | Loss: 0.00005162
Iteration 4/1000 | Loss: 0.00004553
Iteration 5/1000 | Loss: 0.00004204
Iteration 6/1000 | Loss: 0.00004059
Iteration 7/1000 | Loss: 0.00003914
Iteration 8/1000 | Loss: 0.00003841
Iteration 9/1000 | Loss: 0.00003771
Iteration 10/1000 | Loss: 0.00003728
Iteration 11/1000 | Loss: 0.00003695
Iteration 12/1000 | Loss: 0.00003668
Iteration 13/1000 | Loss: 0.00003643
Iteration 14/1000 | Loss: 0.00003640
Iteration 15/1000 | Loss: 0.00003617
Iteration 16/1000 | Loss: 0.00003600
Iteration 17/1000 | Loss: 0.00003596
Iteration 18/1000 | Loss: 0.00003593
Iteration 19/1000 | Loss: 0.00003591
Iteration 20/1000 | Loss: 0.00003591
Iteration 21/1000 | Loss: 0.00003590
Iteration 22/1000 | Loss: 0.00003590
Iteration 23/1000 | Loss: 0.00003589
Iteration 24/1000 | Loss: 0.00003587
Iteration 25/1000 | Loss: 0.00003585
Iteration 26/1000 | Loss: 0.00003584
Iteration 27/1000 | Loss: 0.00003572
Iteration 28/1000 | Loss: 0.00003572
Iteration 29/1000 | Loss: 0.00003569
Iteration 30/1000 | Loss: 0.00003568
Iteration 31/1000 | Loss: 0.00003568
Iteration 32/1000 | Loss: 0.00003568
Iteration 33/1000 | Loss: 0.00003568
Iteration 34/1000 | Loss: 0.00003568
Iteration 35/1000 | Loss: 0.00003568
Iteration 36/1000 | Loss: 0.00003568
Iteration 37/1000 | Loss: 0.00003568
Iteration 38/1000 | Loss: 0.00003568
Iteration 39/1000 | Loss: 0.00003568
Iteration 40/1000 | Loss: 0.00003568
Iteration 41/1000 | Loss: 0.00003568
Iteration 42/1000 | Loss: 0.00003568
Iteration 43/1000 | Loss: 0.00003567
Iteration 44/1000 | Loss: 0.00003567
Iteration 45/1000 | Loss: 0.00003566
Iteration 46/1000 | Loss: 0.00003565
Iteration 47/1000 | Loss: 0.00003565
Iteration 48/1000 | Loss: 0.00003565
Iteration 49/1000 | Loss: 0.00003564
Iteration 50/1000 | Loss: 0.00003564
Iteration 51/1000 | Loss: 0.00003564
Iteration 52/1000 | Loss: 0.00003564
Iteration 53/1000 | Loss: 0.00003564
Iteration 54/1000 | Loss: 0.00003564
Iteration 55/1000 | Loss: 0.00003564
Iteration 56/1000 | Loss: 0.00003563
Iteration 57/1000 | Loss: 0.00003563
Iteration 58/1000 | Loss: 0.00003563
Iteration 59/1000 | Loss: 0.00003563
Iteration 60/1000 | Loss: 0.00003563
Iteration 61/1000 | Loss: 0.00003563
Iteration 62/1000 | Loss: 0.00003563
Iteration 63/1000 | Loss: 0.00003563
Iteration 64/1000 | Loss: 0.00003563
Iteration 65/1000 | Loss: 0.00003561
Iteration 66/1000 | Loss: 0.00003561
Iteration 67/1000 | Loss: 0.00003561
Iteration 68/1000 | Loss: 0.00003561
Iteration 69/1000 | Loss: 0.00003561
Iteration 70/1000 | Loss: 0.00003561
Iteration 71/1000 | Loss: 0.00003561
Iteration 72/1000 | Loss: 0.00003561
Iteration 73/1000 | Loss: 0.00003560
Iteration 74/1000 | Loss: 0.00003560
Iteration 75/1000 | Loss: 0.00003560
Iteration 76/1000 | Loss: 0.00003560
Iteration 77/1000 | Loss: 0.00003560
Iteration 78/1000 | Loss: 0.00003559
Iteration 79/1000 | Loss: 0.00003559
Iteration 80/1000 | Loss: 0.00003559
Iteration 81/1000 | Loss: 0.00003559
Iteration 82/1000 | Loss: 0.00003558
Iteration 83/1000 | Loss: 0.00003558
Iteration 84/1000 | Loss: 0.00003558
Iteration 85/1000 | Loss: 0.00003558
Iteration 86/1000 | Loss: 0.00003557
Iteration 87/1000 | Loss: 0.00003557
Iteration 88/1000 | Loss: 0.00003557
Iteration 89/1000 | Loss: 0.00003557
Iteration 90/1000 | Loss: 0.00003557
Iteration 91/1000 | Loss: 0.00003557
Iteration 92/1000 | Loss: 0.00003557
Iteration 93/1000 | Loss: 0.00003557
Iteration 94/1000 | Loss: 0.00003556
Iteration 95/1000 | Loss: 0.00003556
Iteration 96/1000 | Loss: 0.00003556
Iteration 97/1000 | Loss: 0.00003556
Iteration 98/1000 | Loss: 0.00003556
Iteration 99/1000 | Loss: 0.00003556
Iteration 100/1000 | Loss: 0.00003555
Iteration 101/1000 | Loss: 0.00003555
Iteration 102/1000 | Loss: 0.00003555
Iteration 103/1000 | Loss: 0.00003555
Iteration 104/1000 | Loss: 0.00003555
Iteration 105/1000 | Loss: 0.00003555
Iteration 106/1000 | Loss: 0.00003555
Iteration 107/1000 | Loss: 0.00003555
Iteration 108/1000 | Loss: 0.00003555
Iteration 109/1000 | Loss: 0.00003555
Iteration 110/1000 | Loss: 0.00003555
Iteration 111/1000 | Loss: 0.00003555
Iteration 112/1000 | Loss: 0.00003555
Iteration 113/1000 | Loss: 0.00003555
Iteration 114/1000 | Loss: 0.00003555
Iteration 115/1000 | Loss: 0.00003555
Iteration 116/1000 | Loss: 0.00003555
Iteration 117/1000 | Loss: 0.00003555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [3.554712748154998e-05, 3.554712748154998e-05, 3.554712748154998e-05, 3.554712748154998e-05, 3.554712748154998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.554712748154998e-05

Optimization complete. Final v2v error: 4.9158782958984375 mm

Highest mean error: 5.4577765464782715 mm for frame 18

Lowest mean error: 4.420777320861816 mm for frame 87

Saving results

Total time: 38.37591552734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956529
Iteration 2/25 | Loss: 0.00956529
Iteration 3/25 | Loss: 0.00956529
Iteration 4/25 | Loss: 0.00956529
Iteration 5/25 | Loss: 0.00366649
Iteration 6/25 | Loss: 0.00240172
Iteration 7/25 | Loss: 0.00236786
Iteration 8/25 | Loss: 0.00222415
Iteration 9/25 | Loss: 0.00219935
Iteration 10/25 | Loss: 0.00211356
Iteration 11/25 | Loss: 0.00207577
Iteration 12/25 | Loss: 0.00205268
Iteration 13/25 | Loss: 0.00203292
Iteration 14/25 | Loss: 0.00202769
Iteration 15/25 | Loss: 0.00202047
Iteration 16/25 | Loss: 0.00202289
Iteration 17/25 | Loss: 0.00201887
Iteration 18/25 | Loss: 0.00201156
Iteration 19/25 | Loss: 0.00200965
Iteration 20/25 | Loss: 0.00200781
Iteration 21/25 | Loss: 0.00201110
Iteration 22/25 | Loss: 0.00201094
Iteration 23/25 | Loss: 0.00200478
Iteration 24/25 | Loss: 0.00200281
Iteration 25/25 | Loss: 0.00200234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41294050
Iteration 2/25 | Loss: 0.00513762
Iteration 3/25 | Loss: 0.00513762
Iteration 4/25 | Loss: 0.00513762
Iteration 5/25 | Loss: 0.00513762
Iteration 6/25 | Loss: 0.00513762
Iteration 7/25 | Loss: 0.00513762
Iteration 8/25 | Loss: 0.00513762
Iteration 9/25 | Loss: 0.00513762
Iteration 10/25 | Loss: 0.00513762
Iteration 11/25 | Loss: 0.00513762
Iteration 12/25 | Loss: 0.00513762
Iteration 13/25 | Loss: 0.00513762
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.005137615371495485, 0.005137615371495485, 0.005137615371495485, 0.005137615371495485, 0.005137615371495485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005137615371495485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00513762
Iteration 2/1000 | Loss: 0.00064562
Iteration 3/1000 | Loss: 0.00052223
Iteration 4/1000 | Loss: 0.00046345
Iteration 5/1000 | Loss: 0.00041595
Iteration 6/1000 | Loss: 0.00038280
Iteration 7/1000 | Loss: 0.00035733
Iteration 8/1000 | Loss: 0.00034721
Iteration 9/1000 | Loss: 0.00033692
Iteration 10/1000 | Loss: 0.00032998
Iteration 11/1000 | Loss: 0.00416337
Iteration 12/1000 | Loss: 0.01797645
Iteration 13/1000 | Loss: 0.00056882
Iteration 14/1000 | Loss: 0.00037094
Iteration 15/1000 | Loss: 0.00029353
Iteration 16/1000 | Loss: 0.00018250
Iteration 17/1000 | Loss: 0.00011922
Iteration 18/1000 | Loss: 0.00009247
Iteration 19/1000 | Loss: 0.00007241
Iteration 20/1000 | Loss: 0.00005544
Iteration 21/1000 | Loss: 0.00004350
Iteration 22/1000 | Loss: 0.00003582
Iteration 23/1000 | Loss: 0.00003076
Iteration 24/1000 | Loss: 0.00002819
Iteration 25/1000 | Loss: 0.00002583
Iteration 26/1000 | Loss: 0.00002430
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001778
Iteration 41/1000 | Loss: 0.00001778
Iteration 42/1000 | Loss: 0.00001778
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001777
Iteration 48/1000 | Loss: 0.00001777
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001776
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001776
Iteration 61/1000 | Loss: 0.00001776
Iteration 62/1000 | Loss: 0.00001776
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001773
Iteration 68/1000 | Loss: 0.00001773
Iteration 69/1000 | Loss: 0.00001773
Iteration 70/1000 | Loss: 0.00001773
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001772
Iteration 74/1000 | Loss: 0.00001771
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001770
Iteration 77/1000 | Loss: 0.00001770
Iteration 78/1000 | Loss: 0.00001770
Iteration 79/1000 | Loss: 0.00001770
Iteration 80/1000 | Loss: 0.00001770
Iteration 81/1000 | Loss: 0.00001770
Iteration 82/1000 | Loss: 0.00001770
Iteration 83/1000 | Loss: 0.00001770
Iteration 84/1000 | Loss: 0.00001769
Iteration 85/1000 | Loss: 0.00001769
Iteration 86/1000 | Loss: 0.00001769
Iteration 87/1000 | Loss: 0.00001769
Iteration 88/1000 | Loss: 0.00001769
Iteration 89/1000 | Loss: 0.00001769
Iteration 90/1000 | Loss: 0.00001768
Iteration 91/1000 | Loss: 0.00001768
Iteration 92/1000 | Loss: 0.00001768
Iteration 93/1000 | Loss: 0.00001768
Iteration 94/1000 | Loss: 0.00001768
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Iteration 100/1000 | Loss: 0.00001767
Iteration 101/1000 | Loss: 0.00001767
Iteration 102/1000 | Loss: 0.00001767
Iteration 103/1000 | Loss: 0.00001767
Iteration 104/1000 | Loss: 0.00001767
Iteration 105/1000 | Loss: 0.00001767
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001767
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001767
Iteration 137/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7669990484137088e-05, 1.7669990484137088e-05, 1.7669990484137088e-05, 1.7669990484137088e-05, 1.7669990484137088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7669990484137088e-05

Optimization complete. Final v2v error: 3.601069211959839 mm

Highest mean error: 3.8523292541503906 mm for frame 75

Lowest mean error: 3.4961555004119873 mm for frame 150

Saving results

Total time: 106.75823998451233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406482
Iteration 2/25 | Loss: 0.00132507
Iteration 3/25 | Loss: 0.00122386
Iteration 4/25 | Loss: 0.00121205
Iteration 5/25 | Loss: 0.00120883
Iteration 6/25 | Loss: 0.00120814
Iteration 7/25 | Loss: 0.00120814
Iteration 8/25 | Loss: 0.00120814
Iteration 9/25 | Loss: 0.00120814
Iteration 10/25 | Loss: 0.00120814
Iteration 11/25 | Loss: 0.00120814
Iteration 12/25 | Loss: 0.00120814
Iteration 13/25 | Loss: 0.00120814
Iteration 14/25 | Loss: 0.00120814
Iteration 15/25 | Loss: 0.00120814
Iteration 16/25 | Loss: 0.00120814
Iteration 17/25 | Loss: 0.00120814
Iteration 18/25 | Loss: 0.00120814
Iteration 19/25 | Loss: 0.00120814
Iteration 20/25 | Loss: 0.00120814
Iteration 21/25 | Loss: 0.00120814
Iteration 22/25 | Loss: 0.00120814
Iteration 23/25 | Loss: 0.00120814
Iteration 24/25 | Loss: 0.00120814
Iteration 25/25 | Loss: 0.00120814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43168139
Iteration 2/25 | Loss: 0.00059770
Iteration 3/25 | Loss: 0.00059769
Iteration 4/25 | Loss: 0.00059769
Iteration 5/25 | Loss: 0.00059769
Iteration 6/25 | Loss: 0.00059769
Iteration 7/25 | Loss: 0.00059769
Iteration 8/25 | Loss: 0.00059769
Iteration 9/25 | Loss: 0.00059769
Iteration 10/25 | Loss: 0.00059769
Iteration 11/25 | Loss: 0.00059769
Iteration 12/25 | Loss: 0.00059769
Iteration 13/25 | Loss: 0.00059769
Iteration 14/25 | Loss: 0.00059769
Iteration 15/25 | Loss: 0.00059769
Iteration 16/25 | Loss: 0.00059769
Iteration 17/25 | Loss: 0.00059769
Iteration 18/25 | Loss: 0.00059769
Iteration 19/25 | Loss: 0.00059769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005976894171908498, 0.0005976894171908498, 0.0005976894171908498, 0.0005976894171908498, 0.0005976894171908498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005976894171908498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059769
Iteration 2/1000 | Loss: 0.00003662
Iteration 3/1000 | Loss: 0.00002487
Iteration 4/1000 | Loss: 0.00002100
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00001830
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001701
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001638
Iteration 11/1000 | Loss: 0.00001619
Iteration 12/1000 | Loss: 0.00001613
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001598
Iteration 20/1000 | Loss: 0.00001597
Iteration 21/1000 | Loss: 0.00001597
Iteration 22/1000 | Loss: 0.00001596
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001562
Iteration 92/1000 | Loss: 0.00001562
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001561
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001560
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001559
Iteration 112/1000 | Loss: 0.00001559
Iteration 113/1000 | Loss: 0.00001559
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001556
Iteration 123/1000 | Loss: 0.00001556
Iteration 124/1000 | Loss: 0.00001556
Iteration 125/1000 | Loss: 0.00001556
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001556
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001555
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Iteration 137/1000 | Loss: 0.00001554
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Iteration 141/1000 | Loss: 0.00001553
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001552
Iteration 144/1000 | Loss: 0.00001552
Iteration 145/1000 | Loss: 0.00001552
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001550
Iteration 160/1000 | Loss: 0.00001550
Iteration 161/1000 | Loss: 0.00001550
Iteration 162/1000 | Loss: 0.00001550
Iteration 163/1000 | Loss: 0.00001550
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001549
Iteration 170/1000 | Loss: 0.00001549
Iteration 171/1000 | Loss: 0.00001549
Iteration 172/1000 | Loss: 0.00001549
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001548
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001548
Iteration 184/1000 | Loss: 0.00001547
Iteration 185/1000 | Loss: 0.00001547
Iteration 186/1000 | Loss: 0.00001547
Iteration 187/1000 | Loss: 0.00001547
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001546
Iteration 195/1000 | Loss: 0.00001546
Iteration 196/1000 | Loss: 0.00001546
Iteration 197/1000 | Loss: 0.00001546
Iteration 198/1000 | Loss: 0.00001546
Iteration 199/1000 | Loss: 0.00001546
Iteration 200/1000 | Loss: 0.00001546
Iteration 201/1000 | Loss: 0.00001546
Iteration 202/1000 | Loss: 0.00001545
Iteration 203/1000 | Loss: 0.00001545
Iteration 204/1000 | Loss: 0.00001545
Iteration 205/1000 | Loss: 0.00001545
Iteration 206/1000 | Loss: 0.00001545
Iteration 207/1000 | Loss: 0.00001544
Iteration 208/1000 | Loss: 0.00001544
Iteration 209/1000 | Loss: 0.00001544
Iteration 210/1000 | Loss: 0.00001544
Iteration 211/1000 | Loss: 0.00001544
Iteration 212/1000 | Loss: 0.00001544
Iteration 213/1000 | Loss: 0.00001544
Iteration 214/1000 | Loss: 0.00001543
Iteration 215/1000 | Loss: 0.00001543
Iteration 216/1000 | Loss: 0.00001543
Iteration 217/1000 | Loss: 0.00001543
Iteration 218/1000 | Loss: 0.00001543
Iteration 219/1000 | Loss: 0.00001543
Iteration 220/1000 | Loss: 0.00001543
Iteration 221/1000 | Loss: 0.00001543
Iteration 222/1000 | Loss: 0.00001543
Iteration 223/1000 | Loss: 0.00001543
Iteration 224/1000 | Loss: 0.00001543
Iteration 225/1000 | Loss: 0.00001543
Iteration 226/1000 | Loss: 0.00001543
Iteration 227/1000 | Loss: 0.00001543
Iteration 228/1000 | Loss: 0.00001543
Iteration 229/1000 | Loss: 0.00001542
Iteration 230/1000 | Loss: 0.00001542
Iteration 231/1000 | Loss: 0.00001542
Iteration 232/1000 | Loss: 0.00001542
Iteration 233/1000 | Loss: 0.00001542
Iteration 234/1000 | Loss: 0.00001542
Iteration 235/1000 | Loss: 0.00001542
Iteration 236/1000 | Loss: 0.00001542
Iteration 237/1000 | Loss: 0.00001542
Iteration 238/1000 | Loss: 0.00001542
Iteration 239/1000 | Loss: 0.00001542
Iteration 240/1000 | Loss: 0.00001542
Iteration 241/1000 | Loss: 0.00001541
Iteration 242/1000 | Loss: 0.00001541
Iteration 243/1000 | Loss: 0.00001541
Iteration 244/1000 | Loss: 0.00001541
Iteration 245/1000 | Loss: 0.00001541
Iteration 246/1000 | Loss: 0.00001541
Iteration 247/1000 | Loss: 0.00001541
Iteration 248/1000 | Loss: 0.00001541
Iteration 249/1000 | Loss: 0.00001541
Iteration 250/1000 | Loss: 0.00001541
Iteration 251/1000 | Loss: 0.00001541
Iteration 252/1000 | Loss: 0.00001541
Iteration 253/1000 | Loss: 0.00001541
Iteration 254/1000 | Loss: 0.00001541
Iteration 255/1000 | Loss: 0.00001541
Iteration 256/1000 | Loss: 0.00001541
Iteration 257/1000 | Loss: 0.00001541
Iteration 258/1000 | Loss: 0.00001541
Iteration 259/1000 | Loss: 0.00001541
Iteration 260/1000 | Loss: 0.00001541
Iteration 261/1000 | Loss: 0.00001541
Iteration 262/1000 | Loss: 0.00001541
Iteration 263/1000 | Loss: 0.00001541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.5408491890411824e-05, 1.5408491890411824e-05, 1.5408491890411824e-05, 1.5408491890411824e-05, 1.5408491890411824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5408491890411824e-05

Optimization complete. Final v2v error: 3.2735507488250732 mm

Highest mean error: 3.8416614532470703 mm for frame 66

Lowest mean error: 2.9610087871551514 mm for frame 12

Saving results

Total time: 43.92719483375549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998828
Iteration 2/25 | Loss: 0.00186189
Iteration 3/25 | Loss: 0.00167006
Iteration 4/25 | Loss: 0.00155223
Iteration 5/25 | Loss: 0.00148219
Iteration 6/25 | Loss: 0.00145301
Iteration 7/25 | Loss: 0.00141682
Iteration 8/25 | Loss: 0.00142398
Iteration 9/25 | Loss: 0.00146648
Iteration 10/25 | Loss: 0.00133767
Iteration 11/25 | Loss: 0.00131422
Iteration 12/25 | Loss: 0.00130897
Iteration 13/25 | Loss: 0.00130920
Iteration 14/25 | Loss: 0.00130196
Iteration 15/25 | Loss: 0.00129764
Iteration 16/25 | Loss: 0.00129331
Iteration 17/25 | Loss: 0.00129507
Iteration 18/25 | Loss: 0.00129315
Iteration 19/25 | Loss: 0.00128870
Iteration 20/25 | Loss: 0.00127925
Iteration 21/25 | Loss: 0.00127914
Iteration 22/25 | Loss: 0.00127479
Iteration 23/25 | Loss: 0.00127702
Iteration 24/25 | Loss: 0.00127294
Iteration 25/25 | Loss: 0.00127398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45257092
Iteration 2/25 | Loss: 0.00121944
Iteration 3/25 | Loss: 0.00120228
Iteration 4/25 | Loss: 0.00120228
Iteration 5/25 | Loss: 0.00120227
Iteration 6/25 | Loss: 0.00120227
Iteration 7/25 | Loss: 0.00120227
Iteration 8/25 | Loss: 0.00120227
Iteration 9/25 | Loss: 0.00120227
Iteration 10/25 | Loss: 0.00120227
Iteration 11/25 | Loss: 0.00120227
Iteration 12/25 | Loss: 0.00120227
Iteration 13/25 | Loss: 0.00120227
Iteration 14/25 | Loss: 0.00120227
Iteration 15/25 | Loss: 0.00120227
Iteration 16/25 | Loss: 0.00120227
Iteration 17/25 | Loss: 0.00120227
Iteration 18/25 | Loss: 0.00120227
Iteration 19/25 | Loss: 0.00120227
Iteration 20/25 | Loss: 0.00120227
Iteration 21/25 | Loss: 0.00120227
Iteration 22/25 | Loss: 0.00120227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012022712035104632, 0.0012022712035104632, 0.0012022712035104632, 0.0012022712035104632, 0.0012022712035104632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012022712035104632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120227
Iteration 2/1000 | Loss: 0.00025765
Iteration 3/1000 | Loss: 0.00042233
Iteration 4/1000 | Loss: 0.00033127
Iteration 5/1000 | Loss: 0.00018477
Iteration 6/1000 | Loss: 0.00034130
Iteration 7/1000 | Loss: 0.00052383
Iteration 8/1000 | Loss: 0.00019553
Iteration 9/1000 | Loss: 0.00025664
Iteration 10/1000 | Loss: 0.00040677
Iteration 11/1000 | Loss: 0.00017608
Iteration 12/1000 | Loss: 0.00036013
Iteration 13/1000 | Loss: 0.00029904
Iteration 14/1000 | Loss: 0.00015046
Iteration 15/1000 | Loss: 0.00030235
Iteration 16/1000 | Loss: 0.00021171
Iteration 17/1000 | Loss: 0.00016917
Iteration 18/1000 | Loss: 0.00030938
Iteration 19/1000 | Loss: 0.00004372
Iteration 20/1000 | Loss: 0.00016583
Iteration 21/1000 | Loss: 0.00017462
Iteration 22/1000 | Loss: 0.00016432
Iteration 23/1000 | Loss: 0.00015010
Iteration 24/1000 | Loss: 0.00003893
Iteration 25/1000 | Loss: 0.00014143
Iteration 26/1000 | Loss: 0.00013817
Iteration 27/1000 | Loss: 0.00017305
Iteration 28/1000 | Loss: 0.00017513
Iteration 29/1000 | Loss: 0.00017431
Iteration 30/1000 | Loss: 0.00022708
Iteration 31/1000 | Loss: 0.00018543
Iteration 32/1000 | Loss: 0.00040123
Iteration 33/1000 | Loss: 0.00031706
Iteration 34/1000 | Loss: 0.00027057
Iteration 35/1000 | Loss: 0.00021271
Iteration 36/1000 | Loss: 0.00067994
Iteration 37/1000 | Loss: 0.00016827
Iteration 38/1000 | Loss: 0.00025415
Iteration 39/1000 | Loss: 0.00016735
Iteration 40/1000 | Loss: 0.00023303
Iteration 41/1000 | Loss: 0.00048692
Iteration 42/1000 | Loss: 0.00043934
Iteration 43/1000 | Loss: 0.00032011
Iteration 44/1000 | Loss: 0.00004653
Iteration 45/1000 | Loss: 0.00007901
Iteration 46/1000 | Loss: 0.00020092
Iteration 47/1000 | Loss: 0.00071017
Iteration 48/1000 | Loss: 0.00024870
Iteration 49/1000 | Loss: 0.00026098
Iteration 50/1000 | Loss: 0.00011988
Iteration 51/1000 | Loss: 0.00029668
Iteration 52/1000 | Loss: 0.00016392
Iteration 53/1000 | Loss: 0.00005720
Iteration 54/1000 | Loss: 0.00012272
Iteration 55/1000 | Loss: 0.00008812
Iteration 56/1000 | Loss: 0.00064643
Iteration 57/1000 | Loss: 0.00010701
Iteration 58/1000 | Loss: 0.00010214
Iteration 59/1000 | Loss: 0.00004101
Iteration 60/1000 | Loss: 0.00004983
Iteration 61/1000 | Loss: 0.00006362
Iteration 62/1000 | Loss: 0.00007315
Iteration 63/1000 | Loss: 0.00007706
Iteration 64/1000 | Loss: 0.00014965
Iteration 65/1000 | Loss: 0.00008265
Iteration 66/1000 | Loss: 0.00006178
Iteration 67/1000 | Loss: 0.00016368
Iteration 68/1000 | Loss: 0.00018407
Iteration 69/1000 | Loss: 0.00018171
Iteration 70/1000 | Loss: 0.00016922
Iteration 71/1000 | Loss: 0.00016889
Iteration 72/1000 | Loss: 0.00018201
Iteration 73/1000 | Loss: 0.00009611
Iteration 74/1000 | Loss: 0.00004847
Iteration 75/1000 | Loss: 0.00005241
Iteration 76/1000 | Loss: 0.00006300
Iteration 77/1000 | Loss: 0.00016167
Iteration 78/1000 | Loss: 0.00017317
Iteration 79/1000 | Loss: 0.00019300
Iteration 80/1000 | Loss: 0.00012074
Iteration 81/1000 | Loss: 0.00012348
Iteration 82/1000 | Loss: 0.00020533
Iteration 83/1000 | Loss: 0.00018764
Iteration 84/1000 | Loss: 0.00010302
Iteration 85/1000 | Loss: 0.00009941
Iteration 86/1000 | Loss: 0.00025817
Iteration 87/1000 | Loss: 0.00020370
Iteration 88/1000 | Loss: 0.00008180
Iteration 89/1000 | Loss: 0.00009778
Iteration 90/1000 | Loss: 0.00028172
Iteration 91/1000 | Loss: 0.00020600
Iteration 92/1000 | Loss: 0.00019824
Iteration 93/1000 | Loss: 0.00024703
Iteration 94/1000 | Loss: 0.00025667
Iteration 95/1000 | Loss: 0.00024915
Iteration 96/1000 | Loss: 0.00026166
Iteration 97/1000 | Loss: 0.00022326
Iteration 98/1000 | Loss: 0.00014473
Iteration 99/1000 | Loss: 0.00003678
Iteration 100/1000 | Loss: 0.00008824
Iteration 101/1000 | Loss: 0.00013886
Iteration 102/1000 | Loss: 0.00012265
Iteration 103/1000 | Loss: 0.00018255
Iteration 104/1000 | Loss: 0.00021573
Iteration 105/1000 | Loss: 0.00020529
Iteration 106/1000 | Loss: 0.00020425
Iteration 107/1000 | Loss: 0.00017857
Iteration 108/1000 | Loss: 0.00022927
Iteration 109/1000 | Loss: 0.00017863
Iteration 110/1000 | Loss: 0.00020336
Iteration 111/1000 | Loss: 0.00018579
Iteration 112/1000 | Loss: 0.00019880
Iteration 113/1000 | Loss: 0.00015059
Iteration 114/1000 | Loss: 0.00017296
Iteration 115/1000 | Loss: 0.00014809
Iteration 116/1000 | Loss: 0.00018166
Iteration 117/1000 | Loss: 0.00005167
Iteration 118/1000 | Loss: 0.00005178
Iteration 119/1000 | Loss: 0.00009927
Iteration 120/1000 | Loss: 0.00007144
Iteration 121/1000 | Loss: 0.00007906
Iteration 122/1000 | Loss: 0.00006135
Iteration 123/1000 | Loss: 0.00007598
Iteration 124/1000 | Loss: 0.00005763
Iteration 125/1000 | Loss: 0.00007199
Iteration 126/1000 | Loss: 0.00005660
Iteration 127/1000 | Loss: 0.00006702
Iteration 128/1000 | Loss: 0.00005277
Iteration 129/1000 | Loss: 0.00006345
Iteration 130/1000 | Loss: 0.00004877
Iteration 131/1000 | Loss: 0.00005812
Iteration 132/1000 | Loss: 0.00004417
Iteration 133/1000 | Loss: 0.00005291
Iteration 134/1000 | Loss: 0.00004267
Iteration 135/1000 | Loss: 0.00004969
Iteration 136/1000 | Loss: 0.00004800
Iteration 137/1000 | Loss: 0.00004875
Iteration 138/1000 | Loss: 0.00004943
Iteration 139/1000 | Loss: 0.00004896
Iteration 140/1000 | Loss: 0.00004218
Iteration 141/1000 | Loss: 0.00003927
Iteration 142/1000 | Loss: 0.00004616
Iteration 143/1000 | Loss: 0.00004941
Iteration 144/1000 | Loss: 0.00004889
Iteration 145/1000 | Loss: 0.00004569
Iteration 146/1000 | Loss: 0.00007031
Iteration 147/1000 | Loss: 0.00005643
Iteration 148/1000 | Loss: 0.00004098
Iteration 149/1000 | Loss: 0.00004232
Iteration 150/1000 | Loss: 0.00004652
Iteration 151/1000 | Loss: 0.00005244
Iteration 152/1000 | Loss: 0.00004356
Iteration 153/1000 | Loss: 0.00007241
Iteration 154/1000 | Loss: 0.00004162
Iteration 155/1000 | Loss: 0.00004756
Iteration 156/1000 | Loss: 0.00007222
Iteration 157/1000 | Loss: 0.00004357
Iteration 158/1000 | Loss: 0.00002806
Iteration 159/1000 | Loss: 0.00003085
Iteration 160/1000 | Loss: 0.00004730
Iteration 161/1000 | Loss: 0.00004105
Iteration 162/1000 | Loss: 0.00004389
Iteration 163/1000 | Loss: 0.00004239
Iteration 164/1000 | Loss: 0.00007630
Iteration 165/1000 | Loss: 0.00002695
Iteration 166/1000 | Loss: 0.00002105
Iteration 167/1000 | Loss: 0.00001937
Iteration 168/1000 | Loss: 0.00001865
Iteration 169/1000 | Loss: 0.00001838
Iteration 170/1000 | Loss: 0.00001816
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001802
Iteration 173/1000 | Loss: 0.00001798
Iteration 174/1000 | Loss: 0.00001798
Iteration 175/1000 | Loss: 0.00001798
Iteration 176/1000 | Loss: 0.00001798
Iteration 177/1000 | Loss: 0.00001798
Iteration 178/1000 | Loss: 0.00001798
Iteration 179/1000 | Loss: 0.00001798
Iteration 180/1000 | Loss: 0.00001798
Iteration 181/1000 | Loss: 0.00001798
Iteration 182/1000 | Loss: 0.00001798
Iteration 183/1000 | Loss: 0.00001797
Iteration 184/1000 | Loss: 0.00001797
Iteration 185/1000 | Loss: 0.00001797
Iteration 186/1000 | Loss: 0.00001797
Iteration 187/1000 | Loss: 0.00001796
Iteration 188/1000 | Loss: 0.00001796
Iteration 189/1000 | Loss: 0.00001796
Iteration 190/1000 | Loss: 0.00001795
Iteration 191/1000 | Loss: 0.00001795
Iteration 192/1000 | Loss: 0.00001795
Iteration 193/1000 | Loss: 0.00001794
Iteration 194/1000 | Loss: 0.00001794
Iteration 195/1000 | Loss: 0.00001793
Iteration 196/1000 | Loss: 0.00001792
Iteration 197/1000 | Loss: 0.00001792
Iteration 198/1000 | Loss: 0.00001792
Iteration 199/1000 | Loss: 0.00001790
Iteration 200/1000 | Loss: 0.00001789
Iteration 201/1000 | Loss: 0.00001789
Iteration 202/1000 | Loss: 0.00001788
Iteration 203/1000 | Loss: 0.00001788
Iteration 204/1000 | Loss: 0.00001788
Iteration 205/1000 | Loss: 0.00001788
Iteration 206/1000 | Loss: 0.00001788
Iteration 207/1000 | Loss: 0.00001788
Iteration 208/1000 | Loss: 0.00001788
Iteration 209/1000 | Loss: 0.00001787
Iteration 210/1000 | Loss: 0.00001787
Iteration 211/1000 | Loss: 0.00001787
Iteration 212/1000 | Loss: 0.00001787
Iteration 213/1000 | Loss: 0.00001787
Iteration 214/1000 | Loss: 0.00001787
Iteration 215/1000 | Loss: 0.00001787
Iteration 216/1000 | Loss: 0.00001787
Iteration 217/1000 | Loss: 0.00001787
Iteration 218/1000 | Loss: 0.00001786
Iteration 219/1000 | Loss: 0.00001786
Iteration 220/1000 | Loss: 0.00001786
Iteration 221/1000 | Loss: 0.00001786
Iteration 222/1000 | Loss: 0.00001786
Iteration 223/1000 | Loss: 0.00001786
Iteration 224/1000 | Loss: 0.00001786
Iteration 225/1000 | Loss: 0.00001785
Iteration 226/1000 | Loss: 0.00001785
Iteration 227/1000 | Loss: 0.00001784
Iteration 228/1000 | Loss: 0.00001784
Iteration 229/1000 | Loss: 0.00001784
Iteration 230/1000 | Loss: 0.00001784
Iteration 231/1000 | Loss: 0.00001784
Iteration 232/1000 | Loss: 0.00001784
Iteration 233/1000 | Loss: 0.00001783
Iteration 234/1000 | Loss: 0.00001783
Iteration 235/1000 | Loss: 0.00001783
Iteration 236/1000 | Loss: 0.00001783
Iteration 237/1000 | Loss: 0.00001783
Iteration 238/1000 | Loss: 0.00001783
Iteration 239/1000 | Loss: 0.00001783
Iteration 240/1000 | Loss: 0.00001783
Iteration 241/1000 | Loss: 0.00001783
Iteration 242/1000 | Loss: 0.00001783
Iteration 243/1000 | Loss: 0.00001783
Iteration 244/1000 | Loss: 0.00001783
Iteration 245/1000 | Loss: 0.00001783
Iteration 246/1000 | Loss: 0.00001783
Iteration 247/1000 | Loss: 0.00001783
Iteration 248/1000 | Loss: 0.00001783
Iteration 249/1000 | Loss: 0.00001782
Iteration 250/1000 | Loss: 0.00001782
Iteration 251/1000 | Loss: 0.00001782
Iteration 252/1000 | Loss: 0.00001782
Iteration 253/1000 | Loss: 0.00001782
Iteration 254/1000 | Loss: 0.00001782
Iteration 255/1000 | Loss: 0.00001782
Iteration 256/1000 | Loss: 0.00001782
Iteration 257/1000 | Loss: 0.00001782
Iteration 258/1000 | Loss: 0.00001782
Iteration 259/1000 | Loss: 0.00001781
Iteration 260/1000 | Loss: 0.00001781
Iteration 261/1000 | Loss: 0.00001781
Iteration 262/1000 | Loss: 0.00001781
Iteration 263/1000 | Loss: 0.00001781
Iteration 264/1000 | Loss: 0.00001780
Iteration 265/1000 | Loss: 0.00001780
Iteration 266/1000 | Loss: 0.00001780
Iteration 267/1000 | Loss: 0.00001780
Iteration 268/1000 | Loss: 0.00001780
Iteration 269/1000 | Loss: 0.00001779
Iteration 270/1000 | Loss: 0.00001779
Iteration 271/1000 | Loss: 0.00001779
Iteration 272/1000 | Loss: 0.00001779
Iteration 273/1000 | Loss: 0.00001778
Iteration 274/1000 | Loss: 0.00001777
Iteration 275/1000 | Loss: 0.00001777
Iteration 276/1000 | Loss: 0.00001777
Iteration 277/1000 | Loss: 0.00001777
Iteration 278/1000 | Loss: 0.00001777
Iteration 279/1000 | Loss: 0.00001775
Iteration 280/1000 | Loss: 0.00001775
Iteration 281/1000 | Loss: 0.00001774
Iteration 282/1000 | Loss: 0.00001774
Iteration 283/1000 | Loss: 0.00001773
Iteration 284/1000 | Loss: 0.00001773
Iteration 285/1000 | Loss: 0.00001773
Iteration 286/1000 | Loss: 0.00001773
Iteration 287/1000 | Loss: 0.00001772
Iteration 288/1000 | Loss: 0.00001772
Iteration 289/1000 | Loss: 0.00001772
Iteration 290/1000 | Loss: 0.00001772
Iteration 291/1000 | Loss: 0.00001772
Iteration 292/1000 | Loss: 0.00001771
Iteration 293/1000 | Loss: 0.00001771
Iteration 294/1000 | Loss: 0.00001771
Iteration 295/1000 | Loss: 0.00001771
Iteration 296/1000 | Loss: 0.00001770
Iteration 297/1000 | Loss: 0.00001770
Iteration 298/1000 | Loss: 0.00001770
Iteration 299/1000 | Loss: 0.00001770
Iteration 300/1000 | Loss: 0.00001769
Iteration 301/1000 | Loss: 0.00001769
Iteration 302/1000 | Loss: 0.00001769
Iteration 303/1000 | Loss: 0.00001769
Iteration 304/1000 | Loss: 0.00001768
Iteration 305/1000 | Loss: 0.00001768
Iteration 306/1000 | Loss: 0.00001768
Iteration 307/1000 | Loss: 0.00001768
Iteration 308/1000 | Loss: 0.00001768
Iteration 309/1000 | Loss: 0.00001767
Iteration 310/1000 | Loss: 0.00001767
Iteration 311/1000 | Loss: 0.00001767
Iteration 312/1000 | Loss: 0.00001767
Iteration 313/1000 | Loss: 0.00001767
Iteration 314/1000 | Loss: 0.00001766
Iteration 315/1000 | Loss: 0.00001766
Iteration 316/1000 | Loss: 0.00001766
Iteration 317/1000 | Loss: 0.00001766
Iteration 318/1000 | Loss: 0.00001766
Iteration 319/1000 | Loss: 0.00001766
Iteration 320/1000 | Loss: 0.00001766
Iteration 321/1000 | Loss: 0.00001766
Iteration 322/1000 | Loss: 0.00001766
Iteration 323/1000 | Loss: 0.00001766
Iteration 324/1000 | Loss: 0.00001766
Iteration 325/1000 | Loss: 0.00001766
Iteration 326/1000 | Loss: 0.00001766
Iteration 327/1000 | Loss: 0.00001766
Iteration 328/1000 | Loss: 0.00001766
Iteration 329/1000 | Loss: 0.00001766
Iteration 330/1000 | Loss: 0.00001766
Iteration 331/1000 | Loss: 0.00001766
Iteration 332/1000 | Loss: 0.00001766
Iteration 333/1000 | Loss: 0.00001766
Iteration 334/1000 | Loss: 0.00001766
Iteration 335/1000 | Loss: 0.00001766
Iteration 336/1000 | Loss: 0.00001766
Iteration 337/1000 | Loss: 0.00001766
Iteration 338/1000 | Loss: 0.00001766
Iteration 339/1000 | Loss: 0.00001766
Iteration 340/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [1.765562774380669e-05, 1.765562774380669e-05, 1.765562774380669e-05, 1.765562774380669e-05, 1.765562774380669e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.765562774380669e-05

Optimization complete. Final v2v error: 3.510504961013794 mm

Highest mean error: 5.291545867919922 mm for frame 56

Lowest mean error: 3.022512435913086 mm for frame 18

Saving results

Total time: 290.1484191417694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047444
Iteration 2/25 | Loss: 0.01047444
Iteration 3/25 | Loss: 0.00514764
Iteration 4/25 | Loss: 0.00243822
Iteration 5/25 | Loss: 0.00218726
Iteration 6/25 | Loss: 0.00199767
Iteration 7/25 | Loss: 0.00188993
Iteration 8/25 | Loss: 0.00188293
Iteration 9/25 | Loss: 0.00190018
Iteration 10/25 | Loss: 0.00186969
Iteration 11/25 | Loss: 0.00173657
Iteration 12/25 | Loss: 0.00169081
Iteration 13/25 | Loss: 0.00166566
Iteration 14/25 | Loss: 0.00163895
Iteration 15/25 | Loss: 0.00161817
Iteration 16/25 | Loss: 0.00160813
Iteration 17/25 | Loss: 0.00161157
Iteration 18/25 | Loss: 0.00159957
Iteration 19/25 | Loss: 0.00161973
Iteration 20/25 | Loss: 0.00157621
Iteration 21/25 | Loss: 0.00155811
Iteration 22/25 | Loss: 0.00156417
Iteration 23/25 | Loss: 0.00155389
Iteration 24/25 | Loss: 0.00155890
Iteration 25/25 | Loss: 0.00156254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15969694
Iteration 2/25 | Loss: 0.00217966
Iteration 3/25 | Loss: 0.00203865
Iteration 4/25 | Loss: 0.00203864
Iteration 5/25 | Loss: 0.00203864
Iteration 6/25 | Loss: 0.00203864
Iteration 7/25 | Loss: 0.00203864
Iteration 8/25 | Loss: 0.00203864
Iteration 9/25 | Loss: 0.00203864
Iteration 10/25 | Loss: 0.00203864
Iteration 11/25 | Loss: 0.00203864
Iteration 12/25 | Loss: 0.00203864
Iteration 13/25 | Loss: 0.00203864
Iteration 14/25 | Loss: 0.00203864
Iteration 15/25 | Loss: 0.00203864
Iteration 16/25 | Loss: 0.00203864
Iteration 17/25 | Loss: 0.00203864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002038640668615699, 0.002038640668615699, 0.002038640668615699, 0.002038640668615699, 0.002038640668615699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002038640668615699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203864
Iteration 2/1000 | Loss: 0.00048497
Iteration 3/1000 | Loss: 0.00112924
Iteration 4/1000 | Loss: 0.00029692
Iteration 5/1000 | Loss: 0.00024067
Iteration 6/1000 | Loss: 0.00014535
Iteration 7/1000 | Loss: 0.00052999
Iteration 8/1000 | Loss: 0.00045905
Iteration 9/1000 | Loss: 0.00028920
Iteration 10/1000 | Loss: 0.00027214
Iteration 11/1000 | Loss: 0.00033556
Iteration 12/1000 | Loss: 0.00026955
Iteration 13/1000 | Loss: 0.00012343
Iteration 14/1000 | Loss: 0.00014950
Iteration 15/1000 | Loss: 0.00012648
Iteration 16/1000 | Loss: 0.00065994
Iteration 17/1000 | Loss: 0.00136656
Iteration 18/1000 | Loss: 0.00057792
Iteration 19/1000 | Loss: 0.00011728
Iteration 20/1000 | Loss: 0.00018656
Iteration 21/1000 | Loss: 0.00010686
Iteration 22/1000 | Loss: 0.00027879
Iteration 23/1000 | Loss: 0.00020477
Iteration 24/1000 | Loss: 0.00014086
Iteration 25/1000 | Loss: 0.00010524
Iteration 26/1000 | Loss: 0.00010440
Iteration 27/1000 | Loss: 0.00012722
Iteration 28/1000 | Loss: 0.00012760
Iteration 29/1000 | Loss: 0.00011430
Iteration 30/1000 | Loss: 0.00010327
Iteration 31/1000 | Loss: 0.00010541
Iteration 32/1000 | Loss: 0.00010087
Iteration 33/1000 | Loss: 0.00015083
Iteration 34/1000 | Loss: 0.00009628
Iteration 35/1000 | Loss: 0.00023331
Iteration 36/1000 | Loss: 0.00009389
Iteration 37/1000 | Loss: 0.00009305
Iteration 38/1000 | Loss: 0.00010258
Iteration 39/1000 | Loss: 0.00034701
Iteration 40/1000 | Loss: 0.00049140
Iteration 41/1000 | Loss: 0.00111406
Iteration 42/1000 | Loss: 0.00104062
Iteration 43/1000 | Loss: 0.00331291
Iteration 44/1000 | Loss: 0.00073523
Iteration 45/1000 | Loss: 0.00063372
Iteration 46/1000 | Loss: 0.00020956
Iteration 47/1000 | Loss: 0.00022288
Iteration 48/1000 | Loss: 0.00022311
Iteration 49/1000 | Loss: 0.00044768
Iteration 50/1000 | Loss: 0.00010433
Iteration 51/1000 | Loss: 0.00014604
Iteration 52/1000 | Loss: 0.00007091
Iteration 53/1000 | Loss: 0.00010517
Iteration 54/1000 | Loss: 0.00027901
Iteration 55/1000 | Loss: 0.00007589
Iteration 56/1000 | Loss: 0.00005259
Iteration 57/1000 | Loss: 0.00013406
Iteration 58/1000 | Loss: 0.00005018
Iteration 59/1000 | Loss: 0.00021792
Iteration 60/1000 | Loss: 0.00004912
Iteration 61/1000 | Loss: 0.00004828
Iteration 62/1000 | Loss: 0.00014797
Iteration 63/1000 | Loss: 0.00004774
Iteration 64/1000 | Loss: 0.00006344
Iteration 65/1000 | Loss: 0.00004724
Iteration 66/1000 | Loss: 0.00004715
Iteration 67/1000 | Loss: 0.00004697
Iteration 68/1000 | Loss: 0.00004690
Iteration 69/1000 | Loss: 0.00004689
Iteration 70/1000 | Loss: 0.00004678
Iteration 71/1000 | Loss: 0.00004672
Iteration 72/1000 | Loss: 0.00004672
Iteration 73/1000 | Loss: 0.00004670
Iteration 74/1000 | Loss: 0.00004669
Iteration 75/1000 | Loss: 0.00004669
Iteration 76/1000 | Loss: 0.00004668
Iteration 77/1000 | Loss: 0.00004668
Iteration 78/1000 | Loss: 0.00004668
Iteration 79/1000 | Loss: 0.00004667
Iteration 80/1000 | Loss: 0.00004663
Iteration 81/1000 | Loss: 0.00004663
Iteration 82/1000 | Loss: 0.00004663
Iteration 83/1000 | Loss: 0.00004663
Iteration 84/1000 | Loss: 0.00004662
Iteration 85/1000 | Loss: 0.00004662
Iteration 86/1000 | Loss: 0.00004662
Iteration 87/1000 | Loss: 0.00004660
Iteration 88/1000 | Loss: 0.00004659
Iteration 89/1000 | Loss: 0.00004659
Iteration 90/1000 | Loss: 0.00004659
Iteration 91/1000 | Loss: 0.00004659
Iteration 92/1000 | Loss: 0.00004659
Iteration 93/1000 | Loss: 0.00004659
Iteration 94/1000 | Loss: 0.00004659
Iteration 95/1000 | Loss: 0.00004659
Iteration 96/1000 | Loss: 0.00004659
Iteration 97/1000 | Loss: 0.00004659
Iteration 98/1000 | Loss: 0.00004658
Iteration 99/1000 | Loss: 0.00004658
Iteration 100/1000 | Loss: 0.00004658
Iteration 101/1000 | Loss: 0.00004658
Iteration 102/1000 | Loss: 0.00004657
Iteration 103/1000 | Loss: 0.00004657
Iteration 104/1000 | Loss: 0.00004657
Iteration 105/1000 | Loss: 0.00004656
Iteration 106/1000 | Loss: 0.00016712
Iteration 107/1000 | Loss: 0.00004666
Iteration 108/1000 | Loss: 0.00004657
Iteration 109/1000 | Loss: 0.00004656
Iteration 110/1000 | Loss: 0.00004656
Iteration 111/1000 | Loss: 0.00004655
Iteration 112/1000 | Loss: 0.00004655
Iteration 113/1000 | Loss: 0.00004654
Iteration 114/1000 | Loss: 0.00004654
Iteration 115/1000 | Loss: 0.00004653
Iteration 116/1000 | Loss: 0.00004650
Iteration 117/1000 | Loss: 0.00004649
Iteration 118/1000 | Loss: 0.00004648
Iteration 119/1000 | Loss: 0.00004648
Iteration 120/1000 | Loss: 0.00004648
Iteration 121/1000 | Loss: 0.00004648
Iteration 122/1000 | Loss: 0.00004647
Iteration 123/1000 | Loss: 0.00004647
Iteration 124/1000 | Loss: 0.00004647
Iteration 125/1000 | Loss: 0.00004647
Iteration 126/1000 | Loss: 0.00004647
Iteration 127/1000 | Loss: 0.00004647
Iteration 128/1000 | Loss: 0.00004647
Iteration 129/1000 | Loss: 0.00014975
Iteration 130/1000 | Loss: 0.00005397
Iteration 131/1000 | Loss: 0.00004654
Iteration 132/1000 | Loss: 0.00011500
Iteration 133/1000 | Loss: 0.00004661
Iteration 134/1000 | Loss: 0.00004645
Iteration 135/1000 | Loss: 0.00004645
Iteration 136/1000 | Loss: 0.00004645
Iteration 137/1000 | Loss: 0.00004644
Iteration 138/1000 | Loss: 0.00004644
Iteration 139/1000 | Loss: 0.00004644
Iteration 140/1000 | Loss: 0.00004644
Iteration 141/1000 | Loss: 0.00004644
Iteration 142/1000 | Loss: 0.00004644
Iteration 143/1000 | Loss: 0.00004644
Iteration 144/1000 | Loss: 0.00004644
Iteration 145/1000 | Loss: 0.00004644
Iteration 146/1000 | Loss: 0.00004644
Iteration 147/1000 | Loss: 0.00004643
Iteration 148/1000 | Loss: 0.00004643
Iteration 149/1000 | Loss: 0.00004643
Iteration 150/1000 | Loss: 0.00004643
Iteration 151/1000 | Loss: 0.00004643
Iteration 152/1000 | Loss: 0.00004643
Iteration 153/1000 | Loss: 0.00004642
Iteration 154/1000 | Loss: 0.00004642
Iteration 155/1000 | Loss: 0.00004642
Iteration 156/1000 | Loss: 0.00004642
Iteration 157/1000 | Loss: 0.00004642
Iteration 158/1000 | Loss: 0.00004642
Iteration 159/1000 | Loss: 0.00004642
Iteration 160/1000 | Loss: 0.00004642
Iteration 161/1000 | Loss: 0.00004642
Iteration 162/1000 | Loss: 0.00006568
Iteration 163/1000 | Loss: 0.00004759
Iteration 164/1000 | Loss: 0.00004641
Iteration 165/1000 | Loss: 0.00004641
Iteration 166/1000 | Loss: 0.00004641
Iteration 167/1000 | Loss: 0.00004641
Iteration 168/1000 | Loss: 0.00004641
Iteration 169/1000 | Loss: 0.00004641
Iteration 170/1000 | Loss: 0.00004641
Iteration 171/1000 | Loss: 0.00004641
Iteration 172/1000 | Loss: 0.00004641
Iteration 173/1000 | Loss: 0.00004641
Iteration 174/1000 | Loss: 0.00004641
Iteration 175/1000 | Loss: 0.00004641
Iteration 176/1000 | Loss: 0.00004641
Iteration 177/1000 | Loss: 0.00004640
Iteration 178/1000 | Loss: 0.00004640
Iteration 179/1000 | Loss: 0.00004640
Iteration 180/1000 | Loss: 0.00004640
Iteration 181/1000 | Loss: 0.00004640
Iteration 182/1000 | Loss: 0.00004640
Iteration 183/1000 | Loss: 0.00004640
Iteration 184/1000 | Loss: 0.00004640
Iteration 185/1000 | Loss: 0.00004640
Iteration 186/1000 | Loss: 0.00004640
Iteration 187/1000 | Loss: 0.00004640
Iteration 188/1000 | Loss: 0.00004640
Iteration 189/1000 | Loss: 0.00004640
Iteration 190/1000 | Loss: 0.00004640
Iteration 191/1000 | Loss: 0.00004640
Iteration 192/1000 | Loss: 0.00004640
Iteration 193/1000 | Loss: 0.00004640
Iteration 194/1000 | Loss: 0.00004640
Iteration 195/1000 | Loss: 0.00004640
Iteration 196/1000 | Loss: 0.00004640
Iteration 197/1000 | Loss: 0.00004640
Iteration 198/1000 | Loss: 0.00004640
Iteration 199/1000 | Loss: 0.00004640
Iteration 200/1000 | Loss: 0.00004640
Iteration 201/1000 | Loss: 0.00004640
Iteration 202/1000 | Loss: 0.00004640
Iteration 203/1000 | Loss: 0.00004640
Iteration 204/1000 | Loss: 0.00004640
Iteration 205/1000 | Loss: 0.00004640
Iteration 206/1000 | Loss: 0.00004640
Iteration 207/1000 | Loss: 0.00004640
Iteration 208/1000 | Loss: 0.00004640
Iteration 209/1000 | Loss: 0.00004640
Iteration 210/1000 | Loss: 0.00004640
Iteration 211/1000 | Loss: 0.00004640
Iteration 212/1000 | Loss: 0.00004640
Iteration 213/1000 | Loss: 0.00004640
Iteration 214/1000 | Loss: 0.00004640
Iteration 215/1000 | Loss: 0.00004640
Iteration 216/1000 | Loss: 0.00004640
Iteration 217/1000 | Loss: 0.00004640
Iteration 218/1000 | Loss: 0.00004640
Iteration 219/1000 | Loss: 0.00004640
Iteration 220/1000 | Loss: 0.00004640
Iteration 221/1000 | Loss: 0.00004640
Iteration 222/1000 | Loss: 0.00004640
Iteration 223/1000 | Loss: 0.00004640
Iteration 224/1000 | Loss: 0.00004640
Iteration 225/1000 | Loss: 0.00004640
Iteration 226/1000 | Loss: 0.00004640
Iteration 227/1000 | Loss: 0.00004640
Iteration 228/1000 | Loss: 0.00004640
Iteration 229/1000 | Loss: 0.00004640
Iteration 230/1000 | Loss: 0.00004640
Iteration 231/1000 | Loss: 0.00004640
Iteration 232/1000 | Loss: 0.00004640
Iteration 233/1000 | Loss: 0.00004640
Iteration 234/1000 | Loss: 0.00004640
Iteration 235/1000 | Loss: 0.00004640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [4.640012048184872e-05, 4.640012048184872e-05, 4.640012048184872e-05, 4.640012048184872e-05, 4.640012048184872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.640012048184872e-05

Optimization complete. Final v2v error: 4.800047874450684 mm

Highest mean error: 11.60882568359375 mm for frame 119

Lowest mean error: 3.5944952964782715 mm for frame 1

Saving results

Total time: 155.49118566513062
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039607
Iteration 2/25 | Loss: 0.01039607
Iteration 3/25 | Loss: 0.01039606
Iteration 4/25 | Loss: 0.01039606
Iteration 5/25 | Loss: 0.01039606
Iteration 6/25 | Loss: 0.01039606
Iteration 7/25 | Loss: 0.01039606
Iteration 8/25 | Loss: 0.01039606
Iteration 9/25 | Loss: 0.01039606
Iteration 10/25 | Loss: 0.01039606
Iteration 11/25 | Loss: 0.01039605
Iteration 12/25 | Loss: 0.01039605
Iteration 13/25 | Loss: 0.01039605
Iteration 14/25 | Loss: 0.01039605
Iteration 15/25 | Loss: 0.01039605
Iteration 16/25 | Loss: 0.01039605
Iteration 17/25 | Loss: 0.01039605
Iteration 18/25 | Loss: 0.01039605
Iteration 19/25 | Loss: 0.01039605
Iteration 20/25 | Loss: 0.01039604
Iteration 21/25 | Loss: 0.01039604
Iteration 22/25 | Loss: 0.01039604
Iteration 23/25 | Loss: 0.01039604
Iteration 24/25 | Loss: 0.01039604
Iteration 25/25 | Loss: 0.01039604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55241215
Iteration 2/25 | Loss: 0.13310702
Iteration 3/25 | Loss: 0.12808010
Iteration 4/25 | Loss: 0.11967675
Iteration 5/25 | Loss: 0.11958826
Iteration 6/25 | Loss: 0.11958825
Iteration 7/25 | Loss: 0.11958826
Iteration 8/25 | Loss: 0.11958826
Iteration 9/25 | Loss: 0.11958825
Iteration 10/25 | Loss: 0.11958825
Iteration 11/25 | Loss: 0.11958825
Iteration 12/25 | Loss: 0.11958825
Iteration 13/25 | Loss: 0.11958825
Iteration 14/25 | Loss: 0.11958825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.11958824843168259, 0.11958824843168259, 0.11958824843168259, 0.11958824843168259, 0.11958824843168259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11958824843168259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11958823
Iteration 2/1000 | Loss: 0.00159433
Iteration 3/1000 | Loss: 0.00077328
Iteration 4/1000 | Loss: 0.00027165
Iteration 5/1000 | Loss: 0.00007052
Iteration 6/1000 | Loss: 0.00007018
Iteration 7/1000 | Loss: 0.00003062
Iteration 8/1000 | Loss: 0.00003259
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00001954
Iteration 11/1000 | Loss: 0.00001777
Iteration 12/1000 | Loss: 0.00001637
Iteration 13/1000 | Loss: 0.00001546
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001414
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001305
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001247
Iteration 20/1000 | Loss: 0.00001236
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001125
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001117
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001106
Iteration 36/1000 | Loss: 0.00001106
Iteration 37/1000 | Loss: 0.00001105
Iteration 38/1000 | Loss: 0.00001105
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001104
Iteration 41/1000 | Loss: 0.00001104
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001098
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001095
Iteration 54/1000 | Loss: 0.00001095
Iteration 55/1000 | Loss: 0.00001095
Iteration 56/1000 | Loss: 0.00001094
Iteration 57/1000 | Loss: 0.00001094
Iteration 58/1000 | Loss: 0.00001094
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001091
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001088
Iteration 68/1000 | Loss: 0.00001088
Iteration 69/1000 | Loss: 0.00001087
Iteration 70/1000 | Loss: 0.00001087
Iteration 71/1000 | Loss: 0.00001086
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001085
Iteration 74/1000 | Loss: 0.00001084
Iteration 75/1000 | Loss: 0.00001084
Iteration 76/1000 | Loss: 0.00001084
Iteration 77/1000 | Loss: 0.00001084
Iteration 78/1000 | Loss: 0.00001084
Iteration 79/1000 | Loss: 0.00001084
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001083
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001079
Iteration 92/1000 | Loss: 0.00001079
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001078
Iteration 103/1000 | Loss: 0.00001078
Iteration 104/1000 | Loss: 0.00001078
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001077
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001072
Iteration 134/1000 | Loss: 0.00001072
Iteration 135/1000 | Loss: 0.00001072
Iteration 136/1000 | Loss: 0.00001072
Iteration 137/1000 | Loss: 0.00001072
Iteration 138/1000 | Loss: 0.00001072
Iteration 139/1000 | Loss: 0.00001072
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001070
Iteration 144/1000 | Loss: 0.00001070
Iteration 145/1000 | Loss: 0.00001070
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001069
Iteration 151/1000 | Loss: 0.00001069
Iteration 152/1000 | Loss: 0.00001068
Iteration 153/1000 | Loss: 0.00001068
Iteration 154/1000 | Loss: 0.00001068
Iteration 155/1000 | Loss: 0.00001068
Iteration 156/1000 | Loss: 0.00001068
Iteration 157/1000 | Loss: 0.00001068
Iteration 158/1000 | Loss: 0.00001068
Iteration 159/1000 | Loss: 0.00001068
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001067
Iteration 162/1000 | Loss: 0.00001067
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001066
Iteration 168/1000 | Loss: 0.00001066
Iteration 169/1000 | Loss: 0.00001066
Iteration 170/1000 | Loss: 0.00001066
Iteration 171/1000 | Loss: 0.00001066
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001066
Iteration 174/1000 | Loss: 0.00001066
Iteration 175/1000 | Loss: 0.00001066
Iteration 176/1000 | Loss: 0.00001066
Iteration 177/1000 | Loss: 0.00001066
Iteration 178/1000 | Loss: 0.00001066
Iteration 179/1000 | Loss: 0.00001066
Iteration 180/1000 | Loss: 0.00001066
Iteration 181/1000 | Loss: 0.00001066
Iteration 182/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.0658136488927994e-05, 1.0658136488927994e-05, 1.0658136488927994e-05, 1.0658136488927994e-05, 1.0658136488927994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0658136488927994e-05

Optimization complete. Final v2v error: 2.8064684867858887 mm

Highest mean error: 3.1385040283203125 mm for frame 79

Lowest mean error: 2.609400987625122 mm for frame 180

Saving results

Total time: 64.83003997802734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339479
Iteration 2/25 | Loss: 0.00135494
Iteration 3/25 | Loss: 0.00124280
Iteration 4/25 | Loss: 0.00121560
Iteration 5/25 | Loss: 0.00120811
Iteration 6/25 | Loss: 0.00120566
Iteration 7/25 | Loss: 0.00120566
Iteration 8/25 | Loss: 0.00120566
Iteration 9/25 | Loss: 0.00120566
Iteration 10/25 | Loss: 0.00120566
Iteration 11/25 | Loss: 0.00120566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001205658190883696, 0.001205658190883696, 0.001205658190883696, 0.001205658190883696, 0.001205658190883696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001205658190883696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38042998
Iteration 2/25 | Loss: 0.00072234
Iteration 3/25 | Loss: 0.00072234
Iteration 4/25 | Loss: 0.00072234
Iteration 5/25 | Loss: 0.00072234
Iteration 6/25 | Loss: 0.00072234
Iteration 7/25 | Loss: 0.00072234
Iteration 8/25 | Loss: 0.00072234
Iteration 9/25 | Loss: 0.00072234
Iteration 10/25 | Loss: 0.00072234
Iteration 11/25 | Loss: 0.00072234
Iteration 12/25 | Loss: 0.00072234
Iteration 13/25 | Loss: 0.00072234
Iteration 14/25 | Loss: 0.00072234
Iteration 15/25 | Loss: 0.00072234
Iteration 16/25 | Loss: 0.00072234
Iteration 17/25 | Loss: 0.00072234
Iteration 18/25 | Loss: 0.00072234
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007223368738777936, 0.0007223368738777936, 0.0007223368738777936, 0.0007223368738777936, 0.0007223368738777936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007223368738777936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072234
Iteration 2/1000 | Loss: 0.00003996
Iteration 3/1000 | Loss: 0.00002692
Iteration 4/1000 | Loss: 0.00002391
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002138
Iteration 7/1000 | Loss: 0.00002071
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001917
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001882
Iteration 14/1000 | Loss: 0.00001877
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001875
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001874
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001873
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001859
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001856
Iteration 38/1000 | Loss: 0.00001854
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001851
Iteration 42/1000 | Loss: 0.00001851
Iteration 43/1000 | Loss: 0.00001850
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001848
Iteration 46/1000 | Loss: 0.00001847
Iteration 47/1000 | Loss: 0.00001846
Iteration 48/1000 | Loss: 0.00001846
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001844
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001844
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001841
Iteration 81/1000 | Loss: 0.00001841
Iteration 82/1000 | Loss: 0.00001841
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001841
Iteration 95/1000 | Loss: 0.00001841
Iteration 96/1000 | Loss: 0.00001841
Iteration 97/1000 | Loss: 0.00001841
Iteration 98/1000 | Loss: 0.00001841
Iteration 99/1000 | Loss: 0.00001841
Iteration 100/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.841244193201419e-05, 1.841244193201419e-05, 1.841244193201419e-05, 1.841244193201419e-05, 1.841244193201419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.841244193201419e-05

Optimization complete. Final v2v error: 3.632763624191284 mm

Highest mean error: 4.3092145919799805 mm for frame 210

Lowest mean error: 2.9173226356506348 mm for frame 112

Saving results

Total time: 39.56297421455383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729739
Iteration 2/25 | Loss: 0.00144076
Iteration 3/25 | Loss: 0.00135827
Iteration 4/25 | Loss: 0.00135106
Iteration 5/25 | Loss: 0.00134885
Iteration 6/25 | Loss: 0.00134868
Iteration 7/25 | Loss: 0.00134868
Iteration 8/25 | Loss: 0.00134868
Iteration 9/25 | Loss: 0.00134868
Iteration 10/25 | Loss: 0.00134868
Iteration 11/25 | Loss: 0.00134868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013486840762197971, 0.0013486840762197971, 0.0013486840762197971, 0.0013486840762197971, 0.0013486840762197971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013486840762197971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24693573
Iteration 2/25 | Loss: 0.00093367
Iteration 3/25 | Loss: 0.00093365
Iteration 4/25 | Loss: 0.00093365
Iteration 5/25 | Loss: 0.00093365
Iteration 6/25 | Loss: 0.00093365
Iteration 7/25 | Loss: 0.00093365
Iteration 8/25 | Loss: 0.00093365
Iteration 9/25 | Loss: 0.00093365
Iteration 10/25 | Loss: 0.00093365
Iteration 11/25 | Loss: 0.00093365
Iteration 12/25 | Loss: 0.00093365
Iteration 13/25 | Loss: 0.00093365
Iteration 14/25 | Loss: 0.00093365
Iteration 15/25 | Loss: 0.00093365
Iteration 16/25 | Loss: 0.00093365
Iteration 17/25 | Loss: 0.00093365
Iteration 18/25 | Loss: 0.00093365
Iteration 19/25 | Loss: 0.00093365
Iteration 20/25 | Loss: 0.00093365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009336464572697878, 0.0009336464572697878, 0.0009336464572697878, 0.0009336464572697878, 0.0009336464572697878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009336464572697878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093365
Iteration 2/1000 | Loss: 0.00003935
Iteration 3/1000 | Loss: 0.00002763
Iteration 4/1000 | Loss: 0.00002496
Iteration 5/1000 | Loss: 0.00002381
Iteration 6/1000 | Loss: 0.00002328
Iteration 7/1000 | Loss: 0.00002280
Iteration 8/1000 | Loss: 0.00002237
Iteration 9/1000 | Loss: 0.00002211
Iteration 10/1000 | Loss: 0.00002183
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002114
Iteration 15/1000 | Loss: 0.00002108
Iteration 16/1000 | Loss: 0.00002107
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002092
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002087
Iteration 25/1000 | Loss: 0.00002087
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002079
Iteration 29/1000 | Loss: 0.00002076
Iteration 30/1000 | Loss: 0.00002075
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002073
Iteration 34/1000 | Loss: 0.00002073
Iteration 35/1000 | Loss: 0.00002067
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00002063
Iteration 39/1000 | Loss: 0.00002062
Iteration 40/1000 | Loss: 0.00002062
Iteration 41/1000 | Loss: 0.00002062
Iteration 42/1000 | Loss: 0.00002062
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002061
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002060
Iteration 50/1000 | Loss: 0.00002059
Iteration 51/1000 | Loss: 0.00002059
Iteration 52/1000 | Loss: 0.00002059
Iteration 53/1000 | Loss: 0.00002059
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002057
Iteration 56/1000 | Loss: 0.00002057
Iteration 57/1000 | Loss: 0.00002057
Iteration 58/1000 | Loss: 0.00002057
Iteration 59/1000 | Loss: 0.00002057
Iteration 60/1000 | Loss: 0.00002056
Iteration 61/1000 | Loss: 0.00002056
Iteration 62/1000 | Loss: 0.00002056
Iteration 63/1000 | Loss: 0.00002055
Iteration 64/1000 | Loss: 0.00002055
Iteration 65/1000 | Loss: 0.00002054
Iteration 66/1000 | Loss: 0.00002054
Iteration 67/1000 | Loss: 0.00002053
Iteration 68/1000 | Loss: 0.00002053
Iteration 69/1000 | Loss: 0.00002053
Iteration 70/1000 | Loss: 0.00002052
Iteration 71/1000 | Loss: 0.00002052
Iteration 72/1000 | Loss: 0.00002052
Iteration 73/1000 | Loss: 0.00002052
Iteration 74/1000 | Loss: 0.00002052
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002051
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002051
Iteration 88/1000 | Loss: 0.00002051
Iteration 89/1000 | Loss: 0.00002051
Iteration 90/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [2.0506839064182714e-05, 2.0506839064182714e-05, 2.0506839064182714e-05, 2.0506839064182714e-05, 2.0506839064182714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0506839064182714e-05

Optimization complete. Final v2v error: 3.753084421157837 mm

Highest mean error: 3.9238412380218506 mm for frame 9

Lowest mean error: 3.4959652423858643 mm for frame 131

Saving results

Total time: 35.078989028930664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797680
Iteration 2/25 | Loss: 0.00136547
Iteration 3/25 | Loss: 0.00122724
Iteration 4/25 | Loss: 0.00120425
Iteration 5/25 | Loss: 0.00119888
Iteration 6/25 | Loss: 0.00119849
Iteration 7/25 | Loss: 0.00119849
Iteration 8/25 | Loss: 0.00119849
Iteration 9/25 | Loss: 0.00119849
Iteration 10/25 | Loss: 0.00119849
Iteration 11/25 | Loss: 0.00119849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011984945740550756, 0.0011984945740550756, 0.0011984945740550756, 0.0011984945740550756, 0.0011984945740550756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011984945740550756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43583465
Iteration 2/25 | Loss: 0.00071301
Iteration 3/25 | Loss: 0.00071301
Iteration 4/25 | Loss: 0.00071301
Iteration 5/25 | Loss: 0.00071301
Iteration 6/25 | Loss: 0.00071301
Iteration 7/25 | Loss: 0.00071301
Iteration 8/25 | Loss: 0.00071301
Iteration 9/25 | Loss: 0.00071300
Iteration 10/25 | Loss: 0.00071300
Iteration 11/25 | Loss: 0.00071300
Iteration 12/25 | Loss: 0.00071300
Iteration 13/25 | Loss: 0.00071300
Iteration 14/25 | Loss: 0.00071300
Iteration 15/25 | Loss: 0.00071300
Iteration 16/25 | Loss: 0.00071300
Iteration 17/25 | Loss: 0.00071300
Iteration 18/25 | Loss: 0.00071300
Iteration 19/25 | Loss: 0.00071300
Iteration 20/25 | Loss: 0.00071300
Iteration 21/25 | Loss: 0.00071300
Iteration 22/25 | Loss: 0.00071300
Iteration 23/25 | Loss: 0.00071300
Iteration 24/25 | Loss: 0.00071300
Iteration 25/25 | Loss: 0.00071300

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071300
Iteration 2/1000 | Loss: 0.00003080
Iteration 3/1000 | Loss: 0.00001851
Iteration 4/1000 | Loss: 0.00001625
Iteration 5/1000 | Loss: 0.00001521
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001386
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001295
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001282
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001273
Iteration 20/1000 | Loss: 0.00001273
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001269
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001268
Iteration 30/1000 | Loss: 0.00001268
Iteration 31/1000 | Loss: 0.00001268
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001267
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001266
Iteration 39/1000 | Loss: 0.00001266
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001265
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001264
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001257
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001253
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001245
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001244
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001242
Iteration 72/1000 | Loss: 0.00001242
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001241
Iteration 75/1000 | Loss: 0.00001241
Iteration 76/1000 | Loss: 0.00001241
Iteration 77/1000 | Loss: 0.00001240
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001237
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001232
Iteration 105/1000 | Loss: 0.00001232
Iteration 106/1000 | Loss: 0.00001232
Iteration 107/1000 | Loss: 0.00001232
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001221
Iteration 152/1000 | Loss: 0.00001221
Iteration 153/1000 | Loss: 0.00001221
Iteration 154/1000 | Loss: 0.00001221
Iteration 155/1000 | Loss: 0.00001221
Iteration 156/1000 | Loss: 0.00001221
Iteration 157/1000 | Loss: 0.00001221
Iteration 158/1000 | Loss: 0.00001221
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001220
Iteration 164/1000 | Loss: 0.00001220
Iteration 165/1000 | Loss: 0.00001220
Iteration 166/1000 | Loss: 0.00001220
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001220
Iteration 173/1000 | Loss: 0.00001219
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001219
Iteration 182/1000 | Loss: 0.00001219
Iteration 183/1000 | Loss: 0.00001219
Iteration 184/1000 | Loss: 0.00001219
Iteration 185/1000 | Loss: 0.00001219
Iteration 186/1000 | Loss: 0.00001219
Iteration 187/1000 | Loss: 0.00001218
Iteration 188/1000 | Loss: 0.00001218
Iteration 189/1000 | Loss: 0.00001218
Iteration 190/1000 | Loss: 0.00001218
Iteration 191/1000 | Loss: 0.00001218
Iteration 192/1000 | Loss: 0.00001218
Iteration 193/1000 | Loss: 0.00001218
Iteration 194/1000 | Loss: 0.00001218
Iteration 195/1000 | Loss: 0.00001218
Iteration 196/1000 | Loss: 0.00001218
Iteration 197/1000 | Loss: 0.00001218
Iteration 198/1000 | Loss: 0.00001218
Iteration 199/1000 | Loss: 0.00001218
Iteration 200/1000 | Loss: 0.00001218
Iteration 201/1000 | Loss: 0.00001218
Iteration 202/1000 | Loss: 0.00001218
Iteration 203/1000 | Loss: 0.00001218
Iteration 204/1000 | Loss: 0.00001218
Iteration 205/1000 | Loss: 0.00001218
Iteration 206/1000 | Loss: 0.00001218
Iteration 207/1000 | Loss: 0.00001218
Iteration 208/1000 | Loss: 0.00001218
Iteration 209/1000 | Loss: 0.00001218
Iteration 210/1000 | Loss: 0.00001218
Iteration 211/1000 | Loss: 0.00001218
Iteration 212/1000 | Loss: 0.00001218
Iteration 213/1000 | Loss: 0.00001218
Iteration 214/1000 | Loss: 0.00001218
Iteration 215/1000 | Loss: 0.00001218
Iteration 216/1000 | Loss: 0.00001218
Iteration 217/1000 | Loss: 0.00001218
Iteration 218/1000 | Loss: 0.00001218
Iteration 219/1000 | Loss: 0.00001218
Iteration 220/1000 | Loss: 0.00001218
Iteration 221/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.2183945727883838e-05, 1.2183945727883838e-05, 1.2183945727883838e-05, 1.2183945727883838e-05, 1.2183945727883838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2183945727883838e-05

Optimization complete. Final v2v error: 2.9793052673339844 mm

Highest mean error: 3.3713607788085938 mm for frame 102

Lowest mean error: 2.7040107250213623 mm for frame 243

Saving results

Total time: 46.08803844451904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430838
Iteration 2/25 | Loss: 0.00136346
Iteration 3/25 | Loss: 0.00128440
Iteration 4/25 | Loss: 0.00127655
Iteration 5/25 | Loss: 0.00127466
Iteration 6/25 | Loss: 0.00127429
Iteration 7/25 | Loss: 0.00127429
Iteration 8/25 | Loss: 0.00127429
Iteration 9/25 | Loss: 0.00127429
Iteration 10/25 | Loss: 0.00127429
Iteration 11/25 | Loss: 0.00127429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012742939870804548, 0.0012742939870804548, 0.0012742939870804548, 0.0012742939870804548, 0.0012742939870804548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012742939870804548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46580482
Iteration 2/25 | Loss: 0.00092040
Iteration 3/25 | Loss: 0.00092039
Iteration 4/25 | Loss: 0.00092039
Iteration 5/25 | Loss: 0.00092039
Iteration 6/25 | Loss: 0.00092039
Iteration 7/25 | Loss: 0.00092039
Iteration 8/25 | Loss: 0.00092039
Iteration 9/25 | Loss: 0.00092039
Iteration 10/25 | Loss: 0.00092039
Iteration 11/25 | Loss: 0.00092039
Iteration 12/25 | Loss: 0.00092039
Iteration 13/25 | Loss: 0.00092039
Iteration 14/25 | Loss: 0.00092039
Iteration 15/25 | Loss: 0.00092039
Iteration 16/25 | Loss: 0.00092039
Iteration 17/25 | Loss: 0.00092039
Iteration 18/25 | Loss: 0.00092039
Iteration 19/25 | Loss: 0.00092039
Iteration 20/25 | Loss: 0.00092039
Iteration 21/25 | Loss: 0.00092039
Iteration 22/25 | Loss: 0.00092039
Iteration 23/25 | Loss: 0.00092039
Iteration 24/25 | Loss: 0.00092039
Iteration 25/25 | Loss: 0.00092039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092039
Iteration 2/1000 | Loss: 0.00004269
Iteration 3/1000 | Loss: 0.00002450
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001917
Iteration 8/1000 | Loss: 0.00001882
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001845
Iteration 15/1000 | Loss: 0.00001844
Iteration 16/1000 | Loss: 0.00001844
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001841
Iteration 19/1000 | Loss: 0.00001833
Iteration 20/1000 | Loss: 0.00001821
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001815
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001812
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001806
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001805
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001803
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001802
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001800
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001795
Iteration 51/1000 | Loss: 0.00001795
Iteration 52/1000 | Loss: 0.00001795
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001791
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001790
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001789
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00001789
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00001788
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001788
Iteration 89/1000 | Loss: 0.00001787
Iteration 90/1000 | Loss: 0.00001787
Iteration 91/1000 | Loss: 0.00001787
Iteration 92/1000 | Loss: 0.00001787
Iteration 93/1000 | Loss: 0.00001787
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001779
Iteration 132/1000 | Loss: 0.00001778
Iteration 133/1000 | Loss: 0.00001778
Iteration 134/1000 | Loss: 0.00001778
Iteration 135/1000 | Loss: 0.00001778
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001777
Iteration 143/1000 | Loss: 0.00001777
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001776
Iteration 148/1000 | Loss: 0.00001776
Iteration 149/1000 | Loss: 0.00001776
Iteration 150/1000 | Loss: 0.00001776
Iteration 151/1000 | Loss: 0.00001776
Iteration 152/1000 | Loss: 0.00001776
Iteration 153/1000 | Loss: 0.00001776
Iteration 154/1000 | Loss: 0.00001776
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001775
Iteration 162/1000 | Loss: 0.00001775
Iteration 163/1000 | Loss: 0.00001775
Iteration 164/1000 | Loss: 0.00001775
Iteration 165/1000 | Loss: 0.00001775
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001775
Iteration 168/1000 | Loss: 0.00001775
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001775
Iteration 171/1000 | Loss: 0.00001775
Iteration 172/1000 | Loss: 0.00001774
Iteration 173/1000 | Loss: 0.00001774
Iteration 174/1000 | Loss: 0.00001774
Iteration 175/1000 | Loss: 0.00001774
Iteration 176/1000 | Loss: 0.00001774
Iteration 177/1000 | Loss: 0.00001774
Iteration 178/1000 | Loss: 0.00001774
Iteration 179/1000 | Loss: 0.00001774
Iteration 180/1000 | Loss: 0.00001774
Iteration 181/1000 | Loss: 0.00001774
Iteration 182/1000 | Loss: 0.00001774
Iteration 183/1000 | Loss: 0.00001774
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001773
Iteration 186/1000 | Loss: 0.00001773
Iteration 187/1000 | Loss: 0.00001773
Iteration 188/1000 | Loss: 0.00001773
Iteration 189/1000 | Loss: 0.00001773
Iteration 190/1000 | Loss: 0.00001773
Iteration 191/1000 | Loss: 0.00001772
Iteration 192/1000 | Loss: 0.00001772
Iteration 193/1000 | Loss: 0.00001772
Iteration 194/1000 | Loss: 0.00001772
Iteration 195/1000 | Loss: 0.00001772
Iteration 196/1000 | Loss: 0.00001772
Iteration 197/1000 | Loss: 0.00001772
Iteration 198/1000 | Loss: 0.00001772
Iteration 199/1000 | Loss: 0.00001772
Iteration 200/1000 | Loss: 0.00001772
Iteration 201/1000 | Loss: 0.00001772
Iteration 202/1000 | Loss: 0.00001772
Iteration 203/1000 | Loss: 0.00001772
Iteration 204/1000 | Loss: 0.00001772
Iteration 205/1000 | Loss: 0.00001771
Iteration 206/1000 | Loss: 0.00001771
Iteration 207/1000 | Loss: 0.00001771
Iteration 208/1000 | Loss: 0.00001771
Iteration 209/1000 | Loss: 0.00001771
Iteration 210/1000 | Loss: 0.00001771
Iteration 211/1000 | Loss: 0.00001771
Iteration 212/1000 | Loss: 0.00001771
Iteration 213/1000 | Loss: 0.00001771
Iteration 214/1000 | Loss: 0.00001771
Iteration 215/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.7713591660140082e-05, 1.7713591660140082e-05, 1.7713591660140082e-05, 1.7713591660140082e-05, 1.7713591660140082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7713591660140082e-05

Optimization complete. Final v2v error: 3.425602674484253 mm

Highest mean error: 4.737850666046143 mm for frame 38

Lowest mean error: 3.000929355621338 mm for frame 101

Saving results

Total time: 39.47800278663635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586045
Iteration 2/25 | Loss: 0.00153223
Iteration 3/25 | Loss: 0.00131303
Iteration 4/25 | Loss: 0.00122601
Iteration 5/25 | Loss: 0.00122076
Iteration 6/25 | Loss: 0.00122262
Iteration 7/25 | Loss: 0.00122114
Iteration 8/25 | Loss: 0.00121919
Iteration 9/25 | Loss: 0.00121831
Iteration 10/25 | Loss: 0.00121767
Iteration 11/25 | Loss: 0.00121727
Iteration 12/25 | Loss: 0.00121708
Iteration 13/25 | Loss: 0.00121706
Iteration 14/25 | Loss: 0.00121705
Iteration 15/25 | Loss: 0.00121704
Iteration 16/25 | Loss: 0.00121704
Iteration 17/25 | Loss: 0.00121704
Iteration 18/25 | Loss: 0.00121703
Iteration 19/25 | Loss: 0.00121703
Iteration 20/25 | Loss: 0.00121703
Iteration 21/25 | Loss: 0.00121703
Iteration 22/25 | Loss: 0.00121703
Iteration 23/25 | Loss: 0.00121703
Iteration 24/25 | Loss: 0.00121703
Iteration 25/25 | Loss: 0.00121703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.60965729
Iteration 2/25 | Loss: 0.00078343
Iteration 3/25 | Loss: 0.00078339
Iteration 4/25 | Loss: 0.00078339
Iteration 5/25 | Loss: 0.00078339
Iteration 6/25 | Loss: 0.00078339
Iteration 7/25 | Loss: 0.00078339
Iteration 8/25 | Loss: 0.00078339
Iteration 9/25 | Loss: 0.00078339
Iteration 10/25 | Loss: 0.00078339
Iteration 11/25 | Loss: 0.00078339
Iteration 12/25 | Loss: 0.00078339
Iteration 13/25 | Loss: 0.00078338
Iteration 14/25 | Loss: 0.00078338
Iteration 15/25 | Loss: 0.00078338
Iteration 16/25 | Loss: 0.00078338
Iteration 17/25 | Loss: 0.00078338
Iteration 18/25 | Loss: 0.00078338
Iteration 19/25 | Loss: 0.00078338
Iteration 20/25 | Loss: 0.00078338
Iteration 21/25 | Loss: 0.00078338
Iteration 22/25 | Loss: 0.00078338
Iteration 23/25 | Loss: 0.00078338
Iteration 24/25 | Loss: 0.00078338
Iteration 25/25 | Loss: 0.00078338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078338
Iteration 2/1000 | Loss: 0.00002597
Iteration 3/1000 | Loss: 0.00001849
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001387
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001323
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001281
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001277
Iteration 19/1000 | Loss: 0.00001273
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001263
Iteration 24/1000 | Loss: 0.00001263
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001252
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001249
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001237
Iteration 99/1000 | Loss: 0.00001237
Iteration 100/1000 | Loss: 0.00001237
Iteration 101/1000 | Loss: 0.00001236
Iteration 102/1000 | Loss: 0.00001236
Iteration 103/1000 | Loss: 0.00001236
Iteration 104/1000 | Loss: 0.00001236
Iteration 105/1000 | Loss: 0.00001236
Iteration 106/1000 | Loss: 0.00001236
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001235
Iteration 111/1000 | Loss: 0.00001235
Iteration 112/1000 | Loss: 0.00001235
Iteration 113/1000 | Loss: 0.00001235
Iteration 114/1000 | Loss: 0.00001234
Iteration 115/1000 | Loss: 0.00001234
Iteration 116/1000 | Loss: 0.00001234
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001234
Iteration 123/1000 | Loss: 0.00001233
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001233
Iteration 129/1000 | Loss: 0.00001233
Iteration 130/1000 | Loss: 0.00001233
Iteration 131/1000 | Loss: 0.00001233
Iteration 132/1000 | Loss: 0.00001233
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001232
Iteration 138/1000 | Loss: 0.00001232
Iteration 139/1000 | Loss: 0.00001232
Iteration 140/1000 | Loss: 0.00001232
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001231
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001230
Iteration 149/1000 | Loss: 0.00001230
Iteration 150/1000 | Loss: 0.00001230
Iteration 151/1000 | Loss: 0.00001230
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001229
Iteration 155/1000 | Loss: 0.00001229
Iteration 156/1000 | Loss: 0.00001228
Iteration 157/1000 | Loss: 0.00001228
Iteration 158/1000 | Loss: 0.00001228
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001228
Iteration 161/1000 | Loss: 0.00001228
Iteration 162/1000 | Loss: 0.00001227
Iteration 163/1000 | Loss: 0.00001227
Iteration 164/1000 | Loss: 0.00001227
Iteration 165/1000 | Loss: 0.00001227
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001227
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Iteration 170/1000 | Loss: 0.00001227
Iteration 171/1000 | Loss: 0.00001227
Iteration 172/1000 | Loss: 0.00001227
Iteration 173/1000 | Loss: 0.00001227
Iteration 174/1000 | Loss: 0.00001227
Iteration 175/1000 | Loss: 0.00001226
Iteration 176/1000 | Loss: 0.00001226
Iteration 177/1000 | Loss: 0.00001226
Iteration 178/1000 | Loss: 0.00001226
Iteration 179/1000 | Loss: 0.00001226
Iteration 180/1000 | Loss: 0.00001226
Iteration 181/1000 | Loss: 0.00001226
Iteration 182/1000 | Loss: 0.00001226
Iteration 183/1000 | Loss: 0.00001226
Iteration 184/1000 | Loss: 0.00001226
Iteration 185/1000 | Loss: 0.00001225
Iteration 186/1000 | Loss: 0.00001225
Iteration 187/1000 | Loss: 0.00001225
Iteration 188/1000 | Loss: 0.00001225
Iteration 189/1000 | Loss: 0.00001225
Iteration 190/1000 | Loss: 0.00001225
Iteration 191/1000 | Loss: 0.00001225
Iteration 192/1000 | Loss: 0.00001225
Iteration 193/1000 | Loss: 0.00001224
Iteration 194/1000 | Loss: 0.00001224
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001224
Iteration 199/1000 | Loss: 0.00001224
Iteration 200/1000 | Loss: 0.00001224
Iteration 201/1000 | Loss: 0.00001224
Iteration 202/1000 | Loss: 0.00001224
Iteration 203/1000 | Loss: 0.00001224
Iteration 204/1000 | Loss: 0.00001224
Iteration 205/1000 | Loss: 0.00001224
Iteration 206/1000 | Loss: 0.00001224
Iteration 207/1000 | Loss: 0.00001224
Iteration 208/1000 | Loss: 0.00001224
Iteration 209/1000 | Loss: 0.00001224
Iteration 210/1000 | Loss: 0.00001224
Iteration 211/1000 | Loss: 0.00001224
Iteration 212/1000 | Loss: 0.00001224
Iteration 213/1000 | Loss: 0.00001224
Iteration 214/1000 | Loss: 0.00001224
Iteration 215/1000 | Loss: 0.00001224
Iteration 216/1000 | Loss: 0.00001224
Iteration 217/1000 | Loss: 0.00001224
Iteration 218/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.224087827722542e-05, 1.224087827722542e-05, 1.224087827722542e-05, 1.224087827722542e-05, 1.224087827722542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.224087827722542e-05

Optimization complete. Final v2v error: 2.9505655765533447 mm

Highest mean error: 3.4062588214874268 mm for frame 167

Lowest mean error: 2.6690053939819336 mm for frame 132

Saving results

Total time: 58.434751749038696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758186
Iteration 2/25 | Loss: 0.00167156
Iteration 3/25 | Loss: 0.00147371
Iteration 4/25 | Loss: 0.00143471
Iteration 5/25 | Loss: 0.00142728
Iteration 6/25 | Loss: 0.00139979
Iteration 7/25 | Loss: 0.00129770
Iteration 8/25 | Loss: 0.00128965
Iteration 9/25 | Loss: 0.00128905
Iteration 10/25 | Loss: 0.00129860
Iteration 11/25 | Loss: 0.00128422
Iteration 12/25 | Loss: 0.00128251
Iteration 13/25 | Loss: 0.00128234
Iteration 14/25 | Loss: 0.00128231
Iteration 15/25 | Loss: 0.00128231
Iteration 16/25 | Loss: 0.00128231
Iteration 17/25 | Loss: 0.00128230
Iteration 18/25 | Loss: 0.00128230
Iteration 19/25 | Loss: 0.00128230
Iteration 20/25 | Loss: 0.00128230
Iteration 21/25 | Loss: 0.00128230
Iteration 22/25 | Loss: 0.00128230
Iteration 23/25 | Loss: 0.00128230
Iteration 24/25 | Loss: 0.00128230
Iteration 25/25 | Loss: 0.00128229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87417412
Iteration 2/25 | Loss: 0.00078765
Iteration 3/25 | Loss: 0.00078762
Iteration 4/25 | Loss: 0.00078762
Iteration 5/25 | Loss: 0.00078762
Iteration 6/25 | Loss: 0.00078762
Iteration 7/25 | Loss: 0.00078761
Iteration 8/25 | Loss: 0.00078761
Iteration 9/25 | Loss: 0.00078761
Iteration 10/25 | Loss: 0.00078761
Iteration 11/25 | Loss: 0.00078761
Iteration 12/25 | Loss: 0.00078761
Iteration 13/25 | Loss: 0.00078761
Iteration 14/25 | Loss: 0.00078761
Iteration 15/25 | Loss: 0.00078761
Iteration 16/25 | Loss: 0.00078761
Iteration 17/25 | Loss: 0.00078761
Iteration 18/25 | Loss: 0.00078761
Iteration 19/25 | Loss: 0.00078761
Iteration 20/25 | Loss: 0.00078761
Iteration 21/25 | Loss: 0.00078761
Iteration 22/25 | Loss: 0.00078761
Iteration 23/25 | Loss: 0.00078761
Iteration 24/25 | Loss: 0.00078761
Iteration 25/25 | Loss: 0.00078761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078761
Iteration 2/1000 | Loss: 0.00004667
Iteration 3/1000 | Loss: 0.00003268
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002556
Iteration 6/1000 | Loss: 0.00187958
Iteration 7/1000 | Loss: 0.00005977
Iteration 8/1000 | Loss: 0.00003090
Iteration 9/1000 | Loss: 0.00002737
Iteration 10/1000 | Loss: 0.00002298
Iteration 11/1000 | Loss: 0.00002179
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002078
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002020
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001985
Iteration 18/1000 | Loss: 0.00001971
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001957
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001945
Iteration 38/1000 | Loss: 0.00001944
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001937
Iteration 45/1000 | Loss: 0.00001937
Iteration 46/1000 | Loss: 0.00001937
Iteration 47/1000 | Loss: 0.00001936
Iteration 48/1000 | Loss: 0.00001936
Iteration 49/1000 | Loss: 0.00001936
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001935
Iteration 52/1000 | Loss: 0.00001935
Iteration 53/1000 | Loss: 0.00001934
Iteration 54/1000 | Loss: 0.00001934
Iteration 55/1000 | Loss: 0.00001934
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001932
Iteration 60/1000 | Loss: 0.00001932
Iteration 61/1000 | Loss: 0.00001932
Iteration 62/1000 | Loss: 0.00001932
Iteration 63/1000 | Loss: 0.00001931
Iteration 64/1000 | Loss: 0.00001931
Iteration 65/1000 | Loss: 0.00001931
Iteration 66/1000 | Loss: 0.00001931
Iteration 67/1000 | Loss: 0.00001930
Iteration 68/1000 | Loss: 0.00001930
Iteration 69/1000 | Loss: 0.00001930
Iteration 70/1000 | Loss: 0.00001930
Iteration 71/1000 | Loss: 0.00001930
Iteration 72/1000 | Loss: 0.00001930
Iteration 73/1000 | Loss: 0.00001930
Iteration 74/1000 | Loss: 0.00001929
Iteration 75/1000 | Loss: 0.00001929
Iteration 76/1000 | Loss: 0.00001929
Iteration 77/1000 | Loss: 0.00001929
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001928
Iteration 80/1000 | Loss: 0.00001928
Iteration 81/1000 | Loss: 0.00001928
Iteration 82/1000 | Loss: 0.00001928
Iteration 83/1000 | Loss: 0.00001928
Iteration 84/1000 | Loss: 0.00001928
Iteration 85/1000 | Loss: 0.00001928
Iteration 86/1000 | Loss: 0.00001928
Iteration 87/1000 | Loss: 0.00001928
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00001928
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001927
Iteration 92/1000 | Loss: 0.00001927
Iteration 93/1000 | Loss: 0.00001927
Iteration 94/1000 | Loss: 0.00001927
Iteration 95/1000 | Loss: 0.00001927
Iteration 96/1000 | Loss: 0.00001926
Iteration 97/1000 | Loss: 0.00001926
Iteration 98/1000 | Loss: 0.00001926
Iteration 99/1000 | Loss: 0.00001926
Iteration 100/1000 | Loss: 0.00001925
Iteration 101/1000 | Loss: 0.00001925
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001925
Iteration 104/1000 | Loss: 0.00001925
Iteration 105/1000 | Loss: 0.00001925
Iteration 106/1000 | Loss: 0.00001924
Iteration 107/1000 | Loss: 0.00001924
Iteration 108/1000 | Loss: 0.00001924
Iteration 109/1000 | Loss: 0.00001924
Iteration 110/1000 | Loss: 0.00001924
Iteration 111/1000 | Loss: 0.00001924
Iteration 112/1000 | Loss: 0.00001924
Iteration 113/1000 | Loss: 0.00001923
Iteration 114/1000 | Loss: 0.00001923
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001922
Iteration 121/1000 | Loss: 0.00001922
Iteration 122/1000 | Loss: 0.00001922
Iteration 123/1000 | Loss: 0.00001921
Iteration 124/1000 | Loss: 0.00001921
Iteration 125/1000 | Loss: 0.00001921
Iteration 126/1000 | Loss: 0.00001921
Iteration 127/1000 | Loss: 0.00001921
Iteration 128/1000 | Loss: 0.00001921
Iteration 129/1000 | Loss: 0.00001921
Iteration 130/1000 | Loss: 0.00001921
Iteration 131/1000 | Loss: 0.00001921
Iteration 132/1000 | Loss: 0.00001921
Iteration 133/1000 | Loss: 0.00001921
Iteration 134/1000 | Loss: 0.00001921
Iteration 135/1000 | Loss: 0.00001921
Iteration 136/1000 | Loss: 0.00001921
Iteration 137/1000 | Loss: 0.00001921
Iteration 138/1000 | Loss: 0.00001920
Iteration 139/1000 | Loss: 0.00001920
Iteration 140/1000 | Loss: 0.00001920
Iteration 141/1000 | Loss: 0.00001920
Iteration 142/1000 | Loss: 0.00001920
Iteration 143/1000 | Loss: 0.00001920
Iteration 144/1000 | Loss: 0.00001920
Iteration 145/1000 | Loss: 0.00001920
Iteration 146/1000 | Loss: 0.00001920
Iteration 147/1000 | Loss: 0.00001919
Iteration 148/1000 | Loss: 0.00001919
Iteration 149/1000 | Loss: 0.00001919
Iteration 150/1000 | Loss: 0.00001919
Iteration 151/1000 | Loss: 0.00001919
Iteration 152/1000 | Loss: 0.00001919
Iteration 153/1000 | Loss: 0.00001919
Iteration 154/1000 | Loss: 0.00001919
Iteration 155/1000 | Loss: 0.00001919
Iteration 156/1000 | Loss: 0.00001918
Iteration 157/1000 | Loss: 0.00001918
Iteration 158/1000 | Loss: 0.00001918
Iteration 159/1000 | Loss: 0.00001918
Iteration 160/1000 | Loss: 0.00001918
Iteration 161/1000 | Loss: 0.00001918
Iteration 162/1000 | Loss: 0.00001918
Iteration 163/1000 | Loss: 0.00001918
Iteration 164/1000 | Loss: 0.00001918
Iteration 165/1000 | Loss: 0.00001918
Iteration 166/1000 | Loss: 0.00001918
Iteration 167/1000 | Loss: 0.00001918
Iteration 168/1000 | Loss: 0.00001918
Iteration 169/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.9183393305866048e-05, 1.9183393305866048e-05, 1.9183393305866048e-05, 1.9183393305866048e-05, 1.9183393305866048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9183393305866048e-05

Optimization complete. Final v2v error: 3.599618434906006 mm

Highest mean error: 4.414392471313477 mm for frame 25

Lowest mean error: 2.8892245292663574 mm for frame 142

Saving results

Total time: 68.92101168632507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377565
Iteration 2/25 | Loss: 0.00136848
Iteration 3/25 | Loss: 0.00123375
Iteration 4/25 | Loss: 0.00121731
Iteration 5/25 | Loss: 0.00121276
Iteration 6/25 | Loss: 0.00121215
Iteration 7/25 | Loss: 0.00121215
Iteration 8/25 | Loss: 0.00121215
Iteration 9/25 | Loss: 0.00121215
Iteration 10/25 | Loss: 0.00121215
Iteration 11/25 | Loss: 0.00121215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012121531181037426, 0.0012121531181037426, 0.0012121531181037426, 0.0012121531181037426, 0.0012121531181037426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012121531181037426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86252749
Iteration 2/25 | Loss: 0.00080894
Iteration 3/25 | Loss: 0.00080894
Iteration 4/25 | Loss: 0.00080893
Iteration 5/25 | Loss: 0.00080893
Iteration 6/25 | Loss: 0.00080893
Iteration 7/25 | Loss: 0.00080893
Iteration 8/25 | Loss: 0.00080893
Iteration 9/25 | Loss: 0.00080893
Iteration 10/25 | Loss: 0.00080893
Iteration 11/25 | Loss: 0.00080893
Iteration 12/25 | Loss: 0.00080893
Iteration 13/25 | Loss: 0.00080893
Iteration 14/25 | Loss: 0.00080893
Iteration 15/25 | Loss: 0.00080893
Iteration 16/25 | Loss: 0.00080893
Iteration 17/25 | Loss: 0.00080893
Iteration 18/25 | Loss: 0.00080893
Iteration 19/25 | Loss: 0.00080893
Iteration 20/25 | Loss: 0.00080893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008089321781881154, 0.0008089321781881154, 0.0008089321781881154, 0.0008089321781881154, 0.0008089321781881154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008089321781881154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080893
Iteration 2/1000 | Loss: 0.00003281
Iteration 3/1000 | Loss: 0.00002050
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001502
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001426
Iteration 10/1000 | Loss: 0.00001392
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001337
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001325
Iteration 19/1000 | Loss: 0.00001324
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001318
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001316
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001311
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001297
Iteration 59/1000 | Loss: 0.00001297
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001296
Iteration 62/1000 | Loss: 0.00001296
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001295
Iteration 68/1000 | Loss: 0.00001295
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001295
Iteration 72/1000 | Loss: 0.00001294
Iteration 73/1000 | Loss: 0.00001294
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001293
Iteration 81/1000 | Loss: 0.00001293
Iteration 82/1000 | Loss: 0.00001293
Iteration 83/1000 | Loss: 0.00001293
Iteration 84/1000 | Loss: 0.00001293
Iteration 85/1000 | Loss: 0.00001293
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001291
Iteration 92/1000 | Loss: 0.00001291
Iteration 93/1000 | Loss: 0.00001291
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001289
Iteration 102/1000 | Loss: 0.00001289
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001288
Iteration 105/1000 | Loss: 0.00001288
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001286
Iteration 113/1000 | Loss: 0.00001286
Iteration 114/1000 | Loss: 0.00001286
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001285
Iteration 117/1000 | Loss: 0.00001285
Iteration 118/1000 | Loss: 0.00001285
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001285
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001284
Iteration 128/1000 | Loss: 0.00001284
Iteration 129/1000 | Loss: 0.00001284
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001284
Iteration 132/1000 | Loss: 0.00001284
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Iteration 139/1000 | Loss: 0.00001283
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001282
Iteration 142/1000 | Loss: 0.00001282
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001282
Iteration 145/1000 | Loss: 0.00001282
Iteration 146/1000 | Loss: 0.00001282
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001281
Iteration 150/1000 | Loss: 0.00001281
Iteration 151/1000 | Loss: 0.00001281
Iteration 152/1000 | Loss: 0.00001281
Iteration 153/1000 | Loss: 0.00001281
Iteration 154/1000 | Loss: 0.00001281
Iteration 155/1000 | Loss: 0.00001281
Iteration 156/1000 | Loss: 0.00001281
Iteration 157/1000 | Loss: 0.00001281
Iteration 158/1000 | Loss: 0.00001281
Iteration 159/1000 | Loss: 0.00001281
Iteration 160/1000 | Loss: 0.00001281
Iteration 161/1000 | Loss: 0.00001281
Iteration 162/1000 | Loss: 0.00001280
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001280
Iteration 165/1000 | Loss: 0.00001280
Iteration 166/1000 | Loss: 0.00001280
Iteration 167/1000 | Loss: 0.00001280
Iteration 168/1000 | Loss: 0.00001280
Iteration 169/1000 | Loss: 0.00001280
Iteration 170/1000 | Loss: 0.00001280
Iteration 171/1000 | Loss: 0.00001280
Iteration 172/1000 | Loss: 0.00001280
Iteration 173/1000 | Loss: 0.00001280
Iteration 174/1000 | Loss: 0.00001280
Iteration 175/1000 | Loss: 0.00001279
Iteration 176/1000 | Loss: 0.00001279
Iteration 177/1000 | Loss: 0.00001279
Iteration 178/1000 | Loss: 0.00001279
Iteration 179/1000 | Loss: 0.00001279
Iteration 180/1000 | Loss: 0.00001279
Iteration 181/1000 | Loss: 0.00001279
Iteration 182/1000 | Loss: 0.00001279
Iteration 183/1000 | Loss: 0.00001279
Iteration 184/1000 | Loss: 0.00001279
Iteration 185/1000 | Loss: 0.00001279
Iteration 186/1000 | Loss: 0.00001279
Iteration 187/1000 | Loss: 0.00001279
Iteration 188/1000 | Loss: 0.00001279
Iteration 189/1000 | Loss: 0.00001278
Iteration 190/1000 | Loss: 0.00001278
Iteration 191/1000 | Loss: 0.00001278
Iteration 192/1000 | Loss: 0.00001278
Iteration 193/1000 | Loss: 0.00001278
Iteration 194/1000 | Loss: 0.00001278
Iteration 195/1000 | Loss: 0.00001278
Iteration 196/1000 | Loss: 0.00001278
Iteration 197/1000 | Loss: 0.00001278
Iteration 198/1000 | Loss: 0.00001278
Iteration 199/1000 | Loss: 0.00001278
Iteration 200/1000 | Loss: 0.00001278
Iteration 201/1000 | Loss: 0.00001278
Iteration 202/1000 | Loss: 0.00001278
Iteration 203/1000 | Loss: 0.00001278
Iteration 204/1000 | Loss: 0.00001278
Iteration 205/1000 | Loss: 0.00001278
Iteration 206/1000 | Loss: 0.00001278
Iteration 207/1000 | Loss: 0.00001278
Iteration 208/1000 | Loss: 0.00001278
Iteration 209/1000 | Loss: 0.00001278
Iteration 210/1000 | Loss: 0.00001278
Iteration 211/1000 | Loss: 0.00001278
Iteration 212/1000 | Loss: 0.00001278
Iteration 213/1000 | Loss: 0.00001278
Iteration 214/1000 | Loss: 0.00001278
Iteration 215/1000 | Loss: 0.00001278
Iteration 216/1000 | Loss: 0.00001278
Iteration 217/1000 | Loss: 0.00001278
Iteration 218/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.2775513823726214e-05, 1.2775513823726214e-05, 1.2775513823726214e-05, 1.2775513823726214e-05, 1.2775513823726214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2775513823726214e-05

Optimization complete. Final v2v error: 3.0729575157165527 mm

Highest mean error: 3.519141674041748 mm for frame 75

Lowest mean error: 2.6224513053894043 mm for frame 11

Saving results

Total time: 47.53824019432068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548361
Iteration 2/25 | Loss: 0.00125507
Iteration 3/25 | Loss: 0.00118949
Iteration 4/25 | Loss: 0.00118395
Iteration 5/25 | Loss: 0.00118261
Iteration 6/25 | Loss: 0.00118261
Iteration 7/25 | Loss: 0.00118261
Iteration 8/25 | Loss: 0.00118261
Iteration 9/25 | Loss: 0.00118261
Iteration 10/25 | Loss: 0.00118261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011826137779280543, 0.0011826137779280543, 0.0011826137779280543, 0.0011826137779280543, 0.0011826137779280543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011826137779280543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26874781
Iteration 2/25 | Loss: 0.00074207
Iteration 3/25 | Loss: 0.00074207
Iteration 4/25 | Loss: 0.00074207
Iteration 5/25 | Loss: 0.00074206
Iteration 6/25 | Loss: 0.00074206
Iteration 7/25 | Loss: 0.00074206
Iteration 8/25 | Loss: 0.00074206
Iteration 9/25 | Loss: 0.00074206
Iteration 10/25 | Loss: 0.00074206
Iteration 11/25 | Loss: 0.00074206
Iteration 12/25 | Loss: 0.00074206
Iteration 13/25 | Loss: 0.00074206
Iteration 14/25 | Loss: 0.00074206
Iteration 15/25 | Loss: 0.00074206
Iteration 16/25 | Loss: 0.00074206
Iteration 17/25 | Loss: 0.00074206
Iteration 18/25 | Loss: 0.00074206
Iteration 19/25 | Loss: 0.00074206
Iteration 20/25 | Loss: 0.00074206
Iteration 21/25 | Loss: 0.00074206
Iteration 22/25 | Loss: 0.00074206
Iteration 23/25 | Loss: 0.00074206
Iteration 24/25 | Loss: 0.00074206
Iteration 25/25 | Loss: 0.00074206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074206
Iteration 2/1000 | Loss: 0.00002155
Iteration 3/1000 | Loss: 0.00001535
Iteration 4/1000 | Loss: 0.00001402
Iteration 5/1000 | Loss: 0.00001299
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001224
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001183
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001173
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001168
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001147
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001145
Iteration 25/1000 | Loss: 0.00001144
Iteration 26/1000 | Loss: 0.00001141
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001141
Iteration 30/1000 | Loss: 0.00001140
Iteration 31/1000 | Loss: 0.00001140
Iteration 32/1000 | Loss: 0.00001140
Iteration 33/1000 | Loss: 0.00001140
Iteration 34/1000 | Loss: 0.00001140
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001140
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001136
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001132
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001129
Iteration 56/1000 | Loss: 0.00001129
Iteration 57/1000 | Loss: 0.00001129
Iteration 58/1000 | Loss: 0.00001129
Iteration 59/1000 | Loss: 0.00001128
Iteration 60/1000 | Loss: 0.00001128
Iteration 61/1000 | Loss: 0.00001127
Iteration 62/1000 | Loss: 0.00001127
Iteration 63/1000 | Loss: 0.00001127
Iteration 64/1000 | Loss: 0.00001126
Iteration 65/1000 | Loss: 0.00001126
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001124
Iteration 69/1000 | Loss: 0.00001124
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001121
Iteration 79/1000 | Loss: 0.00001121
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001121
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001115
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001107
Iteration 115/1000 | Loss: 0.00001107
Iteration 116/1000 | Loss: 0.00001107
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001106
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001104
Iteration 132/1000 | Loss: 0.00001104
Iteration 133/1000 | Loss: 0.00001104
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001101
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.1012793038389646e-05, 1.1012793038389646e-05, 1.1012793038389646e-05, 1.1012793038389646e-05, 1.1012793038389646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1012793038389646e-05

Optimization complete. Final v2v error: 2.842458486557007 mm

Highest mean error: 3.0656938552856445 mm for frame 77

Lowest mean error: 2.7380900382995605 mm for frame 138

Saving results

Total time: 36.059770822525024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981377
Iteration 2/25 | Loss: 0.00266405
Iteration 3/25 | Loss: 0.00172289
Iteration 4/25 | Loss: 0.00159507
Iteration 5/25 | Loss: 0.00175321
Iteration 6/25 | Loss: 0.00167114
Iteration 7/25 | Loss: 0.00149148
Iteration 8/25 | Loss: 0.00139561
Iteration 9/25 | Loss: 0.00135397
Iteration 10/25 | Loss: 0.00129944
Iteration 11/25 | Loss: 0.00128802
Iteration 12/25 | Loss: 0.00127434
Iteration 13/25 | Loss: 0.00126830
Iteration 14/25 | Loss: 0.00126539
Iteration 15/25 | Loss: 0.00126392
Iteration 16/25 | Loss: 0.00128974
Iteration 17/25 | Loss: 0.00126959
Iteration 18/25 | Loss: 0.00125956
Iteration 19/25 | Loss: 0.00126752
Iteration 20/25 | Loss: 0.00124980
Iteration 21/25 | Loss: 0.00124663
Iteration 22/25 | Loss: 0.00124221
Iteration 23/25 | Loss: 0.00124201
Iteration 24/25 | Loss: 0.00124035
Iteration 25/25 | Loss: 0.00123900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53422260
Iteration 2/25 | Loss: 0.00080457
Iteration 3/25 | Loss: 0.00080457
Iteration 4/25 | Loss: 0.00080145
Iteration 5/25 | Loss: 0.00080145
Iteration 6/25 | Loss: 0.00080145
Iteration 7/25 | Loss: 0.00080145
Iteration 8/25 | Loss: 0.00080145
Iteration 9/25 | Loss: 0.00080145
Iteration 10/25 | Loss: 0.00080145
Iteration 11/25 | Loss: 0.00080145
Iteration 12/25 | Loss: 0.00080145
Iteration 13/25 | Loss: 0.00080145
Iteration 14/25 | Loss: 0.00080145
Iteration 15/25 | Loss: 0.00080145
Iteration 16/25 | Loss: 0.00080145
Iteration 17/25 | Loss: 0.00080145
Iteration 18/25 | Loss: 0.00080145
Iteration 19/25 | Loss: 0.00080145
Iteration 20/25 | Loss: 0.00080145
Iteration 21/25 | Loss: 0.00080145
Iteration 22/25 | Loss: 0.00080145
Iteration 23/25 | Loss: 0.00080145
Iteration 24/25 | Loss: 0.00080145
Iteration 25/25 | Loss: 0.00080145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080145
Iteration 2/1000 | Loss: 0.00004607
Iteration 3/1000 | Loss: 0.00002886
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00033229
Iteration 8/1000 | Loss: 0.00042918
Iteration 9/1000 | Loss: 0.00095672
Iteration 10/1000 | Loss: 0.00008900
Iteration 11/1000 | Loss: 0.00005157
Iteration 12/1000 | Loss: 0.00003095
Iteration 13/1000 | Loss: 0.00026899
Iteration 14/1000 | Loss: 0.00003332
Iteration 15/1000 | Loss: 0.00002434
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00002060
Iteration 19/1000 | Loss: 0.00001856
Iteration 20/1000 | Loss: 0.00001827
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001779
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001739
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001735
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001729
Iteration 44/1000 | Loss: 0.00001728
Iteration 45/1000 | Loss: 0.00001727
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001725
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001725
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001724
Iteration 56/1000 | Loss: 0.00001724
Iteration 57/1000 | Loss: 0.00001722
Iteration 58/1000 | Loss: 0.00001722
Iteration 59/1000 | Loss: 0.00001722
Iteration 60/1000 | Loss: 0.00001722
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001720
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001719
Iteration 71/1000 | Loss: 0.00001719
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001719
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001718
Iteration 90/1000 | Loss: 0.00001718
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001718
Iteration 93/1000 | Loss: 0.00001718
Iteration 94/1000 | Loss: 0.00001718
Iteration 95/1000 | Loss: 0.00001718
Iteration 96/1000 | Loss: 0.00001718
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.718193198030349e-05, 1.718193198030349e-05, 1.718193198030349e-05, 1.718193198030349e-05, 1.718193198030349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718193198030349e-05

Optimization complete. Final v2v error: 3.275601387023926 mm

Highest mean error: 10.78005313873291 mm for frame 97

Lowest mean error: 2.846074104309082 mm for frame 125

Saving results

Total time: 88.10894536972046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015498
Iteration 2/25 | Loss: 0.00166149
Iteration 3/25 | Loss: 0.00132488
Iteration 4/25 | Loss: 0.00135790
Iteration 5/25 | Loss: 0.00135869
Iteration 6/25 | Loss: 0.00129460
Iteration 7/25 | Loss: 0.00124899
Iteration 8/25 | Loss: 0.00123893
Iteration 9/25 | Loss: 0.00119666
Iteration 10/25 | Loss: 0.00120672
Iteration 11/25 | Loss: 0.00118983
Iteration 12/25 | Loss: 0.00118586
Iteration 13/25 | Loss: 0.00118391
Iteration 14/25 | Loss: 0.00118493
Iteration 15/25 | Loss: 0.00120710
Iteration 16/25 | Loss: 0.00119812
Iteration 17/25 | Loss: 0.00119554
Iteration 18/25 | Loss: 0.00119265
Iteration 19/25 | Loss: 0.00118271
Iteration 20/25 | Loss: 0.00118511
Iteration 21/25 | Loss: 0.00118342
Iteration 22/25 | Loss: 0.00118260
Iteration 23/25 | Loss: 0.00118260
Iteration 24/25 | Loss: 0.00118252
Iteration 25/25 | Loss: 0.00118252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42224503
Iteration 2/25 | Loss: 0.00074548
Iteration 3/25 | Loss: 0.00074548
Iteration 4/25 | Loss: 0.00071601
Iteration 5/25 | Loss: 0.00071588
Iteration 6/25 | Loss: 0.00070912
Iteration 7/25 | Loss: 0.00071585
Iteration 8/25 | Loss: 0.00071604
Iteration 9/25 | Loss: 0.00070237
Iteration 10/25 | Loss: 0.00070237
Iteration 11/25 | Loss: 0.00070237
Iteration 12/25 | Loss: 0.00070237
Iteration 13/25 | Loss: 0.00070237
Iteration 14/25 | Loss: 0.00070237
Iteration 15/25 | Loss: 0.00070237
Iteration 16/25 | Loss: 0.00070237
Iteration 17/25 | Loss: 0.00070912
Iteration 18/25 | Loss: 0.00070495
Iteration 19/25 | Loss: 0.00070237
Iteration 20/25 | Loss: 0.00070237
Iteration 21/25 | Loss: 0.00070237
Iteration 22/25 | Loss: 0.00070237
Iteration 23/25 | Loss: 0.00070237
Iteration 24/25 | Loss: 0.00070237
Iteration 25/25 | Loss: 0.00070237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070237
Iteration 2/1000 | Loss: 0.00037021
Iteration 3/1000 | Loss: 0.00022686
Iteration 4/1000 | Loss: 0.00040273
Iteration 5/1000 | Loss: 0.00001827
Iteration 6/1000 | Loss: 0.00002951
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001406
Iteration 9/1000 | Loss: 0.00015693
Iteration 10/1000 | Loss: 0.00004132
Iteration 11/1000 | Loss: 0.00004317
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00002164
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001294
Iteration 21/1000 | Loss: 0.00001294
Iteration 22/1000 | Loss: 0.00001293
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001293
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00004286
Iteration 31/1000 | Loss: 0.00012341
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00003235
Iteration 34/1000 | Loss: 0.00014766
Iteration 35/1000 | Loss: 0.00002706
Iteration 36/1000 | Loss: 0.00001680
Iteration 37/1000 | Loss: 0.00002029
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00002925
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001278
Iteration 45/1000 | Loss: 0.00001278
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001278
Iteration 51/1000 | Loss: 0.00001614
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002220
Iteration 58/1000 | Loss: 0.00003102
Iteration 59/1000 | Loss: 0.00001432
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001287
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001266
Iteration 66/1000 | Loss: 0.00001266
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001265
Iteration 70/1000 | Loss: 0.00001265
Iteration 71/1000 | Loss: 0.00001265
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001280
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001264
Iteration 79/1000 | Loss: 0.00001264
Iteration 80/1000 | Loss: 0.00001264
Iteration 81/1000 | Loss: 0.00001264
Iteration 82/1000 | Loss: 0.00001264
Iteration 83/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.2643327863770537e-05, 1.2643327863770537e-05, 1.2643327863770537e-05, 1.2643327863770537e-05, 1.2643327863770537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2643327863770537e-05

Optimization complete. Final v2v error: 3.051488161087036 mm

Highest mean error: 3.270127773284912 mm for frame 96

Lowest mean error: 2.883388042449951 mm for frame 159

Saving results

Total time: 93.68159604072571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489709
Iteration 2/25 | Loss: 0.00127871
Iteration 3/25 | Loss: 0.00121882
Iteration 4/25 | Loss: 0.00120720
Iteration 5/25 | Loss: 0.00120313
Iteration 6/25 | Loss: 0.00120228
Iteration 7/25 | Loss: 0.00120228
Iteration 8/25 | Loss: 0.00120228
Iteration 9/25 | Loss: 0.00120228
Iteration 10/25 | Loss: 0.00120228
Iteration 11/25 | Loss: 0.00120228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012022805167362094, 0.0012022805167362094, 0.0012022805167362094, 0.0012022805167362094, 0.0012022805167362094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012022805167362094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54794836
Iteration 2/25 | Loss: 0.00077784
Iteration 3/25 | Loss: 0.00077784
Iteration 4/25 | Loss: 0.00077784
Iteration 5/25 | Loss: 0.00077784
Iteration 6/25 | Loss: 0.00077784
Iteration 7/25 | Loss: 0.00077784
Iteration 8/25 | Loss: 0.00077784
Iteration 9/25 | Loss: 0.00077784
Iteration 10/25 | Loss: 0.00077784
Iteration 11/25 | Loss: 0.00077783
Iteration 12/25 | Loss: 0.00077783
Iteration 13/25 | Loss: 0.00077783
Iteration 14/25 | Loss: 0.00077783
Iteration 15/25 | Loss: 0.00077783
Iteration 16/25 | Loss: 0.00077783
Iteration 17/25 | Loss: 0.00077783
Iteration 18/25 | Loss: 0.00077783
Iteration 19/25 | Loss: 0.00077783
Iteration 20/25 | Loss: 0.00077783
Iteration 21/25 | Loss: 0.00077783
Iteration 22/25 | Loss: 0.00077783
Iteration 23/25 | Loss: 0.00077783
Iteration 24/25 | Loss: 0.00077783
Iteration 25/25 | Loss: 0.00077783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077783
Iteration 2/1000 | Loss: 0.00003019
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001574
Iteration 6/1000 | Loss: 0.00001492
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001397
Iteration 10/1000 | Loss: 0.00001379
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001368
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001356
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001356
Iteration 18/1000 | Loss: 0.00001355
Iteration 19/1000 | Loss: 0.00001354
Iteration 20/1000 | Loss: 0.00001354
Iteration 21/1000 | Loss: 0.00001352
Iteration 22/1000 | Loss: 0.00001352
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001349
Iteration 27/1000 | Loss: 0.00001345
Iteration 28/1000 | Loss: 0.00001344
Iteration 29/1000 | Loss: 0.00001344
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001343
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001338
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001337
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001333
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001328
Iteration 71/1000 | Loss: 0.00001328
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001325
Iteration 80/1000 | Loss: 0.00001325
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001318
Iteration 90/1000 | Loss: 0.00001318
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001317
Iteration 94/1000 | Loss: 0.00001317
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001314
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001313
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001311
Iteration 111/1000 | Loss: 0.00001311
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001310
Iteration 114/1000 | Loss: 0.00001310
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001308
Iteration 118/1000 | Loss: 0.00001308
Iteration 119/1000 | Loss: 0.00001308
Iteration 120/1000 | Loss: 0.00001307
Iteration 121/1000 | Loss: 0.00001307
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001306
Iteration 126/1000 | Loss: 0.00001306
Iteration 127/1000 | Loss: 0.00001306
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.3042552382103167e-05, 1.3042552382103167e-05, 1.3042552382103167e-05, 1.3042552382103167e-05, 1.3042552382103167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3042552382103167e-05

Optimization complete. Final v2v error: 3.1253395080566406 mm

Highest mean error: 3.358281135559082 mm for frame 170

Lowest mean error: 3.0276756286621094 mm for frame 96

Saving results

Total time: 38.416099548339844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992291
Iteration 2/25 | Loss: 0.00992291
Iteration 3/25 | Loss: 0.00499520
Iteration 4/25 | Loss: 0.00426215
Iteration 5/25 | Loss: 0.00339045
Iteration 6/25 | Loss: 0.00271503
Iteration 7/25 | Loss: 0.00242624
Iteration 8/25 | Loss: 0.00223194
Iteration 9/25 | Loss: 0.00214647
Iteration 10/25 | Loss: 0.00210909
Iteration 11/25 | Loss: 0.00206940
Iteration 12/25 | Loss: 0.00204204
Iteration 13/25 | Loss: 0.00202675
Iteration 14/25 | Loss: 0.00201491
Iteration 15/25 | Loss: 0.00200341
Iteration 16/25 | Loss: 0.00199593
Iteration 17/25 | Loss: 0.00198871
Iteration 18/25 | Loss: 0.00197818
Iteration 19/25 | Loss: 0.00197708
Iteration 20/25 | Loss: 0.00197369
Iteration 21/25 | Loss: 0.00197376
Iteration 22/25 | Loss: 0.00197100
Iteration 23/25 | Loss: 0.00196848
Iteration 24/25 | Loss: 0.00196743
Iteration 25/25 | Loss: 0.00196663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35185802
Iteration 2/25 | Loss: 0.00769640
Iteration 3/25 | Loss: 0.00679268
Iteration 4/25 | Loss: 0.00679263
Iteration 5/25 | Loss: 0.00679263
Iteration 6/25 | Loss: 0.00679263
Iteration 7/25 | Loss: 0.00679263
Iteration 8/25 | Loss: 0.00679263
Iteration 9/25 | Loss: 0.00679263
Iteration 10/25 | Loss: 0.00679263
Iteration 11/25 | Loss: 0.00679263
Iteration 12/25 | Loss: 0.00679263
Iteration 13/25 | Loss: 0.00679263
Iteration 14/25 | Loss: 0.00679263
Iteration 15/25 | Loss: 0.00679263
Iteration 16/25 | Loss: 0.00679263
Iteration 17/25 | Loss: 0.00679263
Iteration 18/25 | Loss: 0.00679263
Iteration 19/25 | Loss: 0.00679263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006792628671973944, 0.006792628671973944, 0.006792628671973944, 0.006792628671973944, 0.006792628671973944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006792628671973944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00679263
Iteration 2/1000 | Loss: 0.00226908
Iteration 3/1000 | Loss: 0.00087256
Iteration 4/1000 | Loss: 0.00071598
Iteration 5/1000 | Loss: 0.00057646
Iteration 6/1000 | Loss: 0.00148330
Iteration 7/1000 | Loss: 0.00120031
Iteration 8/1000 | Loss: 0.00086311
Iteration 9/1000 | Loss: 0.00046045
Iteration 10/1000 | Loss: 0.00151983
Iteration 11/1000 | Loss: 0.00040693
Iteration 12/1000 | Loss: 0.00038538
Iteration 13/1000 | Loss: 0.00036659
Iteration 14/1000 | Loss: 0.00035368
Iteration 15/1000 | Loss: 0.00043309
Iteration 16/1000 | Loss: 0.00033933
Iteration 17/1000 | Loss: 0.00575715
Iteration 18/1000 | Loss: 0.02736284
Iteration 19/1000 | Loss: 0.01439397
Iteration 20/1000 | Loss: 0.00269777
Iteration 21/1000 | Loss: 0.00154299
Iteration 22/1000 | Loss: 0.00080578
Iteration 23/1000 | Loss: 0.00047688
Iteration 24/1000 | Loss: 0.00069693
Iteration 25/1000 | Loss: 0.00072602
Iteration 26/1000 | Loss: 0.00019521
Iteration 27/1000 | Loss: 0.00044383
Iteration 28/1000 | Loss: 0.00012577
Iteration 29/1000 | Loss: 0.00010574
Iteration 30/1000 | Loss: 0.00014079
Iteration 31/1000 | Loss: 0.00008209
Iteration 32/1000 | Loss: 0.00007638
Iteration 33/1000 | Loss: 0.00007009
Iteration 34/1000 | Loss: 0.00006598
Iteration 35/1000 | Loss: 0.00006313
Iteration 36/1000 | Loss: 0.00006004
Iteration 37/1000 | Loss: 0.00005747
Iteration 38/1000 | Loss: 0.00005572
Iteration 39/1000 | Loss: 0.00005382
Iteration 40/1000 | Loss: 0.00005245
Iteration 41/1000 | Loss: 0.00005160
Iteration 42/1000 | Loss: 0.00005079
Iteration 43/1000 | Loss: 0.00005026
Iteration 44/1000 | Loss: 0.00004975
Iteration 45/1000 | Loss: 0.00004925
Iteration 46/1000 | Loss: 0.00004898
Iteration 47/1000 | Loss: 0.00004876
Iteration 48/1000 | Loss: 0.00004858
Iteration 49/1000 | Loss: 0.00004850
Iteration 50/1000 | Loss: 0.00004845
Iteration 51/1000 | Loss: 0.00004828
Iteration 52/1000 | Loss: 0.00004828
Iteration 53/1000 | Loss: 0.00004818
Iteration 54/1000 | Loss: 0.00004814
Iteration 55/1000 | Loss: 0.00004810
Iteration 56/1000 | Loss: 0.00004810
Iteration 57/1000 | Loss: 0.00004799
Iteration 58/1000 | Loss: 0.00004791
Iteration 59/1000 | Loss: 0.00004788
Iteration 60/1000 | Loss: 0.00004787
Iteration 61/1000 | Loss: 0.00004785
Iteration 62/1000 | Loss: 0.00004782
Iteration 63/1000 | Loss: 0.00004781
Iteration 64/1000 | Loss: 0.00004780
Iteration 65/1000 | Loss: 0.00004779
Iteration 66/1000 | Loss: 0.00004779
Iteration 67/1000 | Loss: 0.00004779
Iteration 68/1000 | Loss: 0.00004778
Iteration 69/1000 | Loss: 0.00004775
Iteration 70/1000 | Loss: 0.00004773
Iteration 71/1000 | Loss: 0.00004772
Iteration 72/1000 | Loss: 0.00004772
Iteration 73/1000 | Loss: 0.00004772
Iteration 74/1000 | Loss: 0.00004771
Iteration 75/1000 | Loss: 0.00004771
Iteration 76/1000 | Loss: 0.00004771
Iteration 77/1000 | Loss: 0.00004771
Iteration 78/1000 | Loss: 0.00004771
Iteration 79/1000 | Loss: 0.00004771
Iteration 80/1000 | Loss: 0.00004771
Iteration 81/1000 | Loss: 0.00004771
Iteration 82/1000 | Loss: 0.00004771
Iteration 83/1000 | Loss: 0.00004771
Iteration 84/1000 | Loss: 0.00004770
Iteration 85/1000 | Loss: 0.00004770
Iteration 86/1000 | Loss: 0.00004770
Iteration 87/1000 | Loss: 0.00004769
Iteration 88/1000 | Loss: 0.00004769
Iteration 89/1000 | Loss: 0.00004769
Iteration 90/1000 | Loss: 0.00004768
Iteration 91/1000 | Loss: 0.00004768
Iteration 92/1000 | Loss: 0.00004768
Iteration 93/1000 | Loss: 0.00004768
Iteration 94/1000 | Loss: 0.00004768
Iteration 95/1000 | Loss: 0.00004768
Iteration 96/1000 | Loss: 0.00004767
Iteration 97/1000 | Loss: 0.00004766
Iteration 98/1000 | Loss: 0.00004766
Iteration 99/1000 | Loss: 0.00004765
Iteration 100/1000 | Loss: 0.00004765
Iteration 101/1000 | Loss: 0.00004765
Iteration 102/1000 | Loss: 0.00004764
Iteration 103/1000 | Loss: 0.00004764
Iteration 104/1000 | Loss: 0.00004763
Iteration 105/1000 | Loss: 0.00004763
Iteration 106/1000 | Loss: 0.00004763
Iteration 107/1000 | Loss: 0.00004762
Iteration 108/1000 | Loss: 0.00004761
Iteration 109/1000 | Loss: 0.00004761
Iteration 110/1000 | Loss: 0.00004761
Iteration 111/1000 | Loss: 0.00004761
Iteration 112/1000 | Loss: 0.00004761
Iteration 113/1000 | Loss: 0.00004761
Iteration 114/1000 | Loss: 0.00004761
Iteration 115/1000 | Loss: 0.00004761
Iteration 116/1000 | Loss: 0.00004761
Iteration 117/1000 | Loss: 0.00004761
Iteration 118/1000 | Loss: 0.00004761
Iteration 119/1000 | Loss: 0.00004761
Iteration 120/1000 | Loss: 0.00004761
Iteration 121/1000 | Loss: 0.00004761
Iteration 122/1000 | Loss: 0.00004761
Iteration 123/1000 | Loss: 0.00004761
Iteration 124/1000 | Loss: 0.00004761
Iteration 125/1000 | Loss: 0.00004761
Iteration 126/1000 | Loss: 0.00004761
Iteration 127/1000 | Loss: 0.00004761
Iteration 128/1000 | Loss: 0.00004761
Iteration 129/1000 | Loss: 0.00004761
Iteration 130/1000 | Loss: 0.00004761
Iteration 131/1000 | Loss: 0.00004761
Iteration 132/1000 | Loss: 0.00004761
Iteration 133/1000 | Loss: 0.00004761
Iteration 134/1000 | Loss: 0.00004761
Iteration 135/1000 | Loss: 0.00004761
Iteration 136/1000 | Loss: 0.00004761
Iteration 137/1000 | Loss: 0.00004761
Iteration 138/1000 | Loss: 0.00004761
Iteration 139/1000 | Loss: 0.00004761
Iteration 140/1000 | Loss: 0.00004761
Iteration 141/1000 | Loss: 0.00004761
Iteration 142/1000 | Loss: 0.00004761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [4.760891533805989e-05, 4.760891533805989e-05, 4.760891533805989e-05, 4.760891533805989e-05, 4.760891533805989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.760891533805989e-05

Optimization complete. Final v2v error: 4.227163791656494 mm

Highest mean error: 11.098348617553711 mm for frame 171

Lowest mean error: 3.465209484100342 mm for frame 11

Saving results

Total time: 143.98610949516296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560064
Iteration 2/25 | Loss: 0.00142418
Iteration 3/25 | Loss: 0.00128250
Iteration 4/25 | Loss: 0.00126007
Iteration 5/25 | Loss: 0.00125392
Iteration 6/25 | Loss: 0.00125010
Iteration 7/25 | Loss: 0.00125164
Iteration 8/25 | Loss: 0.00125056
Iteration 9/25 | Loss: 0.00125123
Iteration 10/25 | Loss: 0.00124863
Iteration 11/25 | Loss: 0.00125021
Iteration 12/25 | Loss: 0.00124911
Iteration 13/25 | Loss: 0.00124826
Iteration 14/25 | Loss: 0.00124821
Iteration 15/25 | Loss: 0.00124821
Iteration 16/25 | Loss: 0.00124821
Iteration 17/25 | Loss: 0.00124821
Iteration 18/25 | Loss: 0.00124821
Iteration 19/25 | Loss: 0.00124821
Iteration 20/25 | Loss: 0.00124821
Iteration 21/25 | Loss: 0.00124821
Iteration 22/25 | Loss: 0.00124821
Iteration 23/25 | Loss: 0.00124821
Iteration 24/25 | Loss: 0.00124821
Iteration 25/25 | Loss: 0.00124821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30775261
Iteration 2/25 | Loss: 0.00087961
Iteration 3/25 | Loss: 0.00082989
Iteration 4/25 | Loss: 0.00082989
Iteration 5/25 | Loss: 0.00082989
Iteration 6/25 | Loss: 0.00082989
Iteration 7/25 | Loss: 0.00082989
Iteration 8/25 | Loss: 0.00082989
Iteration 9/25 | Loss: 0.00082989
Iteration 10/25 | Loss: 0.00082989
Iteration 11/25 | Loss: 0.00082989
Iteration 12/25 | Loss: 0.00082989
Iteration 13/25 | Loss: 0.00082989
Iteration 14/25 | Loss: 0.00082989
Iteration 15/25 | Loss: 0.00082989
Iteration 16/25 | Loss: 0.00082989
Iteration 17/25 | Loss: 0.00082989
Iteration 18/25 | Loss: 0.00082989
Iteration 19/25 | Loss: 0.00082989
Iteration 20/25 | Loss: 0.00082989
Iteration 21/25 | Loss: 0.00082989
Iteration 22/25 | Loss: 0.00082989
Iteration 23/25 | Loss: 0.00082989
Iteration 24/25 | Loss: 0.00082989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008298910688608885, 0.0008298910688608885, 0.0008298910688608885, 0.0008298910688608885, 0.0008298910688608885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008298910688608885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082989
Iteration 2/1000 | Loss: 0.00008008
Iteration 3/1000 | Loss: 0.00006851
Iteration 4/1000 | Loss: 0.00005571
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001669
Iteration 7/1000 | Loss: 0.00003521
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00005638
Iteration 10/1000 | Loss: 0.00005638
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00006545
Iteration 16/1000 | Loss: 0.00003786
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00007087
Iteration 21/1000 | Loss: 0.00007087
Iteration 22/1000 | Loss: 0.00014742
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00003429
Iteration 26/1000 | Loss: 0.00002316
Iteration 27/1000 | Loss: 0.00001521
Iteration 28/1000 | Loss: 0.00003539
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001508
Iteration 35/1000 | Loss: 0.00001507
Iteration 36/1000 | Loss: 0.00001507
Iteration 37/1000 | Loss: 0.00001507
Iteration 38/1000 | Loss: 0.00001506
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00002132
Iteration 41/1000 | Loss: 0.00005957
Iteration 42/1000 | Loss: 0.00002159
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00003746
Iteration 51/1000 | Loss: 0.00002332
Iteration 52/1000 | Loss: 0.00002585
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001488
Iteration 68/1000 | Loss: 0.00001488
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001487
Iteration 75/1000 | Loss: 0.00001487
Iteration 76/1000 | Loss: 0.00001487
Iteration 77/1000 | Loss: 0.00001487
Iteration 78/1000 | Loss: 0.00001486
Iteration 79/1000 | Loss: 0.00001486
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001484
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001483
Iteration 90/1000 | Loss: 0.00001483
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00003172
Iteration 96/1000 | Loss: 0.00003488
Iteration 97/1000 | Loss: 0.00001503
Iteration 98/1000 | Loss: 0.00001967
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001480
Iteration 101/1000 | Loss: 0.00001480
Iteration 102/1000 | Loss: 0.00001479
Iteration 103/1000 | Loss: 0.00001479
Iteration 104/1000 | Loss: 0.00001479
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001479
Iteration 111/1000 | Loss: 0.00001479
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001698
Iteration 115/1000 | Loss: 0.00001479
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Iteration 126/1000 | Loss: 0.00001478
Iteration 127/1000 | Loss: 0.00001478
Iteration 128/1000 | Loss: 0.00001477
Iteration 129/1000 | Loss: 0.00001477
Iteration 130/1000 | Loss: 0.00001477
Iteration 131/1000 | Loss: 0.00001477
Iteration 132/1000 | Loss: 0.00001477
Iteration 133/1000 | Loss: 0.00001477
Iteration 134/1000 | Loss: 0.00001476
Iteration 135/1000 | Loss: 0.00001476
Iteration 136/1000 | Loss: 0.00001475
Iteration 137/1000 | Loss: 0.00001475
Iteration 138/1000 | Loss: 0.00001475
Iteration 139/1000 | Loss: 0.00001475
Iteration 140/1000 | Loss: 0.00001474
Iteration 141/1000 | Loss: 0.00001474
Iteration 142/1000 | Loss: 0.00001474
Iteration 143/1000 | Loss: 0.00001474
Iteration 144/1000 | Loss: 0.00001474
Iteration 145/1000 | Loss: 0.00001474
Iteration 146/1000 | Loss: 0.00001474
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001473
Iteration 150/1000 | Loss: 0.00001473
Iteration 151/1000 | Loss: 0.00001473
Iteration 152/1000 | Loss: 0.00002108
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001470
Iteration 161/1000 | Loss: 0.00001470
Iteration 162/1000 | Loss: 0.00001470
Iteration 163/1000 | Loss: 0.00001469
Iteration 164/1000 | Loss: 0.00001469
Iteration 165/1000 | Loss: 0.00001469
Iteration 166/1000 | Loss: 0.00001469
Iteration 167/1000 | Loss: 0.00001469
Iteration 168/1000 | Loss: 0.00001469
Iteration 169/1000 | Loss: 0.00001469
Iteration 170/1000 | Loss: 0.00001469
Iteration 171/1000 | Loss: 0.00001469
Iteration 172/1000 | Loss: 0.00001469
Iteration 173/1000 | Loss: 0.00001469
Iteration 174/1000 | Loss: 0.00001469
Iteration 175/1000 | Loss: 0.00001469
Iteration 176/1000 | Loss: 0.00001469
Iteration 177/1000 | Loss: 0.00001469
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Iteration 180/1000 | Loss: 0.00002774
Iteration 181/1000 | Loss: 0.00002774
Iteration 182/1000 | Loss: 0.00001618
Iteration 183/1000 | Loss: 0.00001484
Iteration 184/1000 | Loss: 0.00001466
Iteration 185/1000 | Loss: 0.00001464
Iteration 186/1000 | Loss: 0.00001464
Iteration 187/1000 | Loss: 0.00001464
Iteration 188/1000 | Loss: 0.00001464
Iteration 189/1000 | Loss: 0.00001464
Iteration 190/1000 | Loss: 0.00001463
Iteration 191/1000 | Loss: 0.00001463
Iteration 192/1000 | Loss: 0.00001463
Iteration 193/1000 | Loss: 0.00001463
Iteration 194/1000 | Loss: 0.00001463
Iteration 195/1000 | Loss: 0.00001463
Iteration 196/1000 | Loss: 0.00001463
Iteration 197/1000 | Loss: 0.00001463
Iteration 198/1000 | Loss: 0.00001463
Iteration 199/1000 | Loss: 0.00001463
Iteration 200/1000 | Loss: 0.00001463
Iteration 201/1000 | Loss: 0.00001463
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001462
Iteration 205/1000 | Loss: 0.00001462
Iteration 206/1000 | Loss: 0.00001462
Iteration 207/1000 | Loss: 0.00001462
Iteration 208/1000 | Loss: 0.00001462
Iteration 209/1000 | Loss: 0.00001462
Iteration 210/1000 | Loss: 0.00001462
Iteration 211/1000 | Loss: 0.00001462
Iteration 212/1000 | Loss: 0.00001462
Iteration 213/1000 | Loss: 0.00001462
Iteration 214/1000 | Loss: 0.00001462
Iteration 215/1000 | Loss: 0.00001462
Iteration 216/1000 | Loss: 0.00001462
Iteration 217/1000 | Loss: 0.00001462
Iteration 218/1000 | Loss: 0.00001461
Iteration 219/1000 | Loss: 0.00001461
Iteration 220/1000 | Loss: 0.00001461
Iteration 221/1000 | Loss: 0.00001461
Iteration 222/1000 | Loss: 0.00001461
Iteration 223/1000 | Loss: 0.00001461
Iteration 224/1000 | Loss: 0.00001461
Iteration 225/1000 | Loss: 0.00001461
Iteration 226/1000 | Loss: 0.00001461
Iteration 227/1000 | Loss: 0.00001461
Iteration 228/1000 | Loss: 0.00001461
Iteration 229/1000 | Loss: 0.00001461
Iteration 230/1000 | Loss: 0.00001461
Iteration 231/1000 | Loss: 0.00001461
Iteration 232/1000 | Loss: 0.00001461
Iteration 233/1000 | Loss: 0.00001461
Iteration 234/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.4610562175221276e-05, 1.4610562175221276e-05, 1.4610562175221276e-05, 1.4610562175221276e-05, 1.4610562175221276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4610562175221276e-05

Optimization complete. Final v2v error: 3.2178304195404053 mm

Highest mean error: 3.5885305404663086 mm for frame 213

Lowest mean error: 2.9773330688476562 mm for frame 171

Saving results

Total time: 107.59636616706848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498911
Iteration 2/25 | Loss: 0.00154954
Iteration 3/25 | Loss: 0.00129802
Iteration 4/25 | Loss: 0.00126244
Iteration 5/25 | Loss: 0.00125619
Iteration 6/25 | Loss: 0.00125456
Iteration 7/25 | Loss: 0.00125456
Iteration 8/25 | Loss: 0.00125456
Iteration 9/25 | Loss: 0.00125456
Iteration 10/25 | Loss: 0.00125456
Iteration 11/25 | Loss: 0.00125456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012545608915388584, 0.0012545608915388584, 0.0012545608915388584, 0.0012545608915388584, 0.0012545608915388584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012545608915388584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46010613
Iteration 2/25 | Loss: 0.00074955
Iteration 3/25 | Loss: 0.00074955
Iteration 4/25 | Loss: 0.00074954
Iteration 5/25 | Loss: 0.00074954
Iteration 6/25 | Loss: 0.00074954
Iteration 7/25 | Loss: 0.00074954
Iteration 8/25 | Loss: 0.00074954
Iteration 9/25 | Loss: 0.00074954
Iteration 10/25 | Loss: 0.00074954
Iteration 11/25 | Loss: 0.00074954
Iteration 12/25 | Loss: 0.00074954
Iteration 13/25 | Loss: 0.00074954
Iteration 14/25 | Loss: 0.00074954
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007495420868508518, 0.0007495420868508518, 0.0007495420868508518, 0.0007495420868508518, 0.0007495420868508518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007495420868508518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074954
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002360
Iteration 4/1000 | Loss: 0.00002077
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001888
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001730
Iteration 12/1000 | Loss: 0.00001722
Iteration 13/1000 | Loss: 0.00001721
Iteration 14/1000 | Loss: 0.00001718
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001712
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001702
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001691
Iteration 25/1000 | Loss: 0.00001691
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001691
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001691
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001690
Iteration 38/1000 | Loss: 0.00001690
Iteration 39/1000 | Loss: 0.00001689
Iteration 40/1000 | Loss: 0.00001689
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001687
Iteration 45/1000 | Loss: 0.00001687
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001685
Iteration 48/1000 | Loss: 0.00001685
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001684
Iteration 51/1000 | Loss: 0.00001684
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001680
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001673
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001672
Iteration 91/1000 | Loss: 0.00001672
Iteration 92/1000 | Loss: 0.00001672
Iteration 93/1000 | Loss: 0.00001672
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Iteration 99/1000 | Loss: 0.00001671
Iteration 100/1000 | Loss: 0.00001671
Iteration 101/1000 | Loss: 0.00001671
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001670
Iteration 107/1000 | Loss: 0.00001670
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001668
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001667
Iteration 120/1000 | Loss: 0.00001667
Iteration 121/1000 | Loss: 0.00001667
Iteration 122/1000 | Loss: 0.00001666
Iteration 123/1000 | Loss: 0.00001666
Iteration 124/1000 | Loss: 0.00001666
Iteration 125/1000 | Loss: 0.00001666
Iteration 126/1000 | Loss: 0.00001666
Iteration 127/1000 | Loss: 0.00001666
Iteration 128/1000 | Loss: 0.00001666
Iteration 129/1000 | Loss: 0.00001666
Iteration 130/1000 | Loss: 0.00001665
Iteration 131/1000 | Loss: 0.00001665
Iteration 132/1000 | Loss: 0.00001664
Iteration 133/1000 | Loss: 0.00001664
Iteration 134/1000 | Loss: 0.00001664
Iteration 135/1000 | Loss: 0.00001664
Iteration 136/1000 | Loss: 0.00001664
Iteration 137/1000 | Loss: 0.00001664
Iteration 138/1000 | Loss: 0.00001663
Iteration 139/1000 | Loss: 0.00001663
Iteration 140/1000 | Loss: 0.00001663
Iteration 141/1000 | Loss: 0.00001663
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001662
Iteration 147/1000 | Loss: 0.00001662
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001660
Iteration 156/1000 | Loss: 0.00001660
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001659
Iteration 159/1000 | Loss: 0.00001659
Iteration 160/1000 | Loss: 0.00001659
Iteration 161/1000 | Loss: 0.00001659
Iteration 162/1000 | Loss: 0.00001658
Iteration 163/1000 | Loss: 0.00001658
Iteration 164/1000 | Loss: 0.00001658
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001657
Iteration 167/1000 | Loss: 0.00001657
Iteration 168/1000 | Loss: 0.00001657
Iteration 169/1000 | Loss: 0.00001657
Iteration 170/1000 | Loss: 0.00001657
Iteration 171/1000 | Loss: 0.00001657
Iteration 172/1000 | Loss: 0.00001657
Iteration 173/1000 | Loss: 0.00001657
Iteration 174/1000 | Loss: 0.00001657
Iteration 175/1000 | Loss: 0.00001657
Iteration 176/1000 | Loss: 0.00001657
Iteration 177/1000 | Loss: 0.00001657
Iteration 178/1000 | Loss: 0.00001657
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001657
Iteration 183/1000 | Loss: 0.00001657
Iteration 184/1000 | Loss: 0.00001657
Iteration 185/1000 | Loss: 0.00001657
Iteration 186/1000 | Loss: 0.00001657
Iteration 187/1000 | Loss: 0.00001657
Iteration 188/1000 | Loss: 0.00001657
Iteration 189/1000 | Loss: 0.00001657
Iteration 190/1000 | Loss: 0.00001657
Iteration 191/1000 | Loss: 0.00001657
Iteration 192/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.656796484894585e-05, 1.656796484894585e-05, 1.656796484894585e-05, 1.656796484894585e-05, 1.656796484894585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.656796484894585e-05

Optimization complete. Final v2v error: 3.3968615531921387 mm

Highest mean error: 4.448986053466797 mm for frame 92

Lowest mean error: 3.051414966583252 mm for frame 27

Saving results

Total time: 41.906654596328735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401905
Iteration 2/25 | Loss: 0.00129497
Iteration 3/25 | Loss: 0.00120083
Iteration 4/25 | Loss: 0.00119442
Iteration 5/25 | Loss: 0.00119264
Iteration 6/25 | Loss: 0.00119264
Iteration 7/25 | Loss: 0.00119264
Iteration 8/25 | Loss: 0.00119264
Iteration 9/25 | Loss: 0.00119264
Iteration 10/25 | Loss: 0.00119264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011926384177058935, 0.0011926384177058935, 0.0011926384177058935, 0.0011926384177058935, 0.0011926384177058935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011926384177058935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42976654
Iteration 2/25 | Loss: 0.00059549
Iteration 3/25 | Loss: 0.00059548
Iteration 4/25 | Loss: 0.00059548
Iteration 5/25 | Loss: 0.00059548
Iteration 6/25 | Loss: 0.00059548
Iteration 7/25 | Loss: 0.00059548
Iteration 8/25 | Loss: 0.00059548
Iteration 9/25 | Loss: 0.00059548
Iteration 10/25 | Loss: 0.00059548
Iteration 11/25 | Loss: 0.00059548
Iteration 12/25 | Loss: 0.00059548
Iteration 13/25 | Loss: 0.00059548
Iteration 14/25 | Loss: 0.00059548
Iteration 15/25 | Loss: 0.00059548
Iteration 16/25 | Loss: 0.00059548
Iteration 17/25 | Loss: 0.00059548
Iteration 18/25 | Loss: 0.00059548
Iteration 19/25 | Loss: 0.00059548
Iteration 20/25 | Loss: 0.00059548
Iteration 21/25 | Loss: 0.00059548
Iteration 22/25 | Loss: 0.00059548
Iteration 23/25 | Loss: 0.00059548
Iteration 24/25 | Loss: 0.00059548
Iteration 25/25 | Loss: 0.00059548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005954785156063735, 0.0005954785156063735, 0.0005954785156063735, 0.0005954785156063735, 0.0005954785156063735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005954785156063735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059548
Iteration 2/1000 | Loss: 0.00003347
Iteration 3/1000 | Loss: 0.00002509
Iteration 4/1000 | Loss: 0.00002115
Iteration 5/1000 | Loss: 0.00001992
Iteration 6/1000 | Loss: 0.00001869
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001699
Iteration 10/1000 | Loss: 0.00001663
Iteration 11/1000 | Loss: 0.00001633
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001572
Iteration 18/1000 | Loss: 0.00001570
Iteration 19/1000 | Loss: 0.00001570
Iteration 20/1000 | Loss: 0.00001570
Iteration 21/1000 | Loss: 0.00001569
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001566
Iteration 27/1000 | Loss: 0.00001566
Iteration 28/1000 | Loss: 0.00001565
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001564
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001563
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001559
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00001542
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001534
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001531
Iteration 45/1000 | Loss: 0.00001530
Iteration 46/1000 | Loss: 0.00001530
Iteration 47/1000 | Loss: 0.00001529
Iteration 48/1000 | Loss: 0.00001529
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001528
Iteration 51/1000 | Loss: 0.00001528
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001521
Iteration 59/1000 | Loss: 0.00001521
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001521
Iteration 62/1000 | Loss: 0.00001521
Iteration 63/1000 | Loss: 0.00001520
Iteration 64/1000 | Loss: 0.00001520
Iteration 65/1000 | Loss: 0.00001520
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001516
Iteration 78/1000 | Loss: 0.00001516
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001504
Iteration 127/1000 | Loss: 0.00001504
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Iteration 133/1000 | Loss: 0.00001503
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001501
Iteration 142/1000 | Loss: 0.00001501
Iteration 143/1000 | Loss: 0.00001500
Iteration 144/1000 | Loss: 0.00001500
Iteration 145/1000 | Loss: 0.00001500
Iteration 146/1000 | Loss: 0.00001500
Iteration 147/1000 | Loss: 0.00001500
Iteration 148/1000 | Loss: 0.00001499
Iteration 149/1000 | Loss: 0.00001499
Iteration 150/1000 | Loss: 0.00001499
Iteration 151/1000 | Loss: 0.00001499
Iteration 152/1000 | Loss: 0.00001499
Iteration 153/1000 | Loss: 0.00001499
Iteration 154/1000 | Loss: 0.00001499
Iteration 155/1000 | Loss: 0.00001499
Iteration 156/1000 | Loss: 0.00001499
Iteration 157/1000 | Loss: 0.00001499
Iteration 158/1000 | Loss: 0.00001499
Iteration 159/1000 | Loss: 0.00001499
Iteration 160/1000 | Loss: 0.00001498
Iteration 161/1000 | Loss: 0.00001498
Iteration 162/1000 | Loss: 0.00001498
Iteration 163/1000 | Loss: 0.00001498
Iteration 164/1000 | Loss: 0.00001498
Iteration 165/1000 | Loss: 0.00001498
Iteration 166/1000 | Loss: 0.00001498
Iteration 167/1000 | Loss: 0.00001498
Iteration 168/1000 | Loss: 0.00001498
Iteration 169/1000 | Loss: 0.00001498
Iteration 170/1000 | Loss: 0.00001498
Iteration 171/1000 | Loss: 0.00001498
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001498
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001498
Iteration 181/1000 | Loss: 0.00001498
Iteration 182/1000 | Loss: 0.00001498
Iteration 183/1000 | Loss: 0.00001498
Iteration 184/1000 | Loss: 0.00001498
Iteration 185/1000 | Loss: 0.00001498
Iteration 186/1000 | Loss: 0.00001498
Iteration 187/1000 | Loss: 0.00001498
Iteration 188/1000 | Loss: 0.00001498
Iteration 189/1000 | Loss: 0.00001498
Iteration 190/1000 | Loss: 0.00001498
Iteration 191/1000 | Loss: 0.00001498
Iteration 192/1000 | Loss: 0.00001498
Iteration 193/1000 | Loss: 0.00001498
Iteration 194/1000 | Loss: 0.00001498
Iteration 195/1000 | Loss: 0.00001498
Iteration 196/1000 | Loss: 0.00001498
Iteration 197/1000 | Loss: 0.00001498
Iteration 198/1000 | Loss: 0.00001498
Iteration 199/1000 | Loss: 0.00001498
Iteration 200/1000 | Loss: 0.00001498
Iteration 201/1000 | Loss: 0.00001498
Iteration 202/1000 | Loss: 0.00001498
Iteration 203/1000 | Loss: 0.00001498
Iteration 204/1000 | Loss: 0.00001498
Iteration 205/1000 | Loss: 0.00001498
Iteration 206/1000 | Loss: 0.00001498
Iteration 207/1000 | Loss: 0.00001498
Iteration 208/1000 | Loss: 0.00001498
Iteration 209/1000 | Loss: 0.00001498
Iteration 210/1000 | Loss: 0.00001498
Iteration 211/1000 | Loss: 0.00001498
Iteration 212/1000 | Loss: 0.00001498
Iteration 213/1000 | Loss: 0.00001498
Iteration 214/1000 | Loss: 0.00001498
Iteration 215/1000 | Loss: 0.00001498
Iteration 216/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.4978559192968532e-05, 1.4978559192968532e-05, 1.4978559192968532e-05, 1.4978559192968532e-05, 1.4978559192968532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4978559192968532e-05

Optimization complete. Final v2v error: 3.260347366333008 mm

Highest mean error: 3.8223464488983154 mm for frame 121

Lowest mean error: 2.9000422954559326 mm for frame 27

Saving results

Total time: 45.179065465927124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815841
Iteration 2/25 | Loss: 0.00127984
Iteration 3/25 | Loss: 0.00119829
Iteration 4/25 | Loss: 0.00119088
Iteration 5/25 | Loss: 0.00119003
Iteration 6/25 | Loss: 0.00119003
Iteration 7/25 | Loss: 0.00119003
Iteration 8/25 | Loss: 0.00119003
Iteration 9/25 | Loss: 0.00119003
Iteration 10/25 | Loss: 0.00119003
Iteration 11/25 | Loss: 0.00119003
Iteration 12/25 | Loss: 0.00119003
Iteration 13/25 | Loss: 0.00119003
Iteration 14/25 | Loss: 0.00119003
Iteration 15/25 | Loss: 0.00119003
Iteration 16/25 | Loss: 0.00119003
Iteration 17/25 | Loss: 0.00119003
Iteration 18/25 | Loss: 0.00119003
Iteration 19/25 | Loss: 0.00119003
Iteration 20/25 | Loss: 0.00119003
Iteration 21/25 | Loss: 0.00119003
Iteration 22/25 | Loss: 0.00119003
Iteration 23/25 | Loss: 0.00119003
Iteration 24/25 | Loss: 0.00119003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001190025475807488, 0.001190025475807488, 0.001190025475807488, 0.001190025475807488, 0.001190025475807488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001190025475807488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43377531
Iteration 2/25 | Loss: 0.00063928
Iteration 3/25 | Loss: 0.00063928
Iteration 4/25 | Loss: 0.00063928
Iteration 5/25 | Loss: 0.00063928
Iteration 6/25 | Loss: 0.00063928
Iteration 7/25 | Loss: 0.00063928
Iteration 8/25 | Loss: 0.00063927
Iteration 9/25 | Loss: 0.00063927
Iteration 10/25 | Loss: 0.00063927
Iteration 11/25 | Loss: 0.00063927
Iteration 12/25 | Loss: 0.00063927
Iteration 13/25 | Loss: 0.00063927
Iteration 14/25 | Loss: 0.00063927
Iteration 15/25 | Loss: 0.00063927
Iteration 16/25 | Loss: 0.00063927
Iteration 17/25 | Loss: 0.00063927
Iteration 18/25 | Loss: 0.00063927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006392741343006492, 0.0006392741343006492, 0.0006392741343006492, 0.0006392741343006492, 0.0006392741343006492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006392741343006492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063927
Iteration 2/1000 | Loss: 0.00002705
Iteration 3/1000 | Loss: 0.00001814
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001238
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001215
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001202
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001200
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001199
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001187
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001176
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001168
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001162
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001156
Iteration 77/1000 | Loss: 0.00001156
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001156
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001155
Iteration 82/1000 | Loss: 0.00001155
Iteration 83/1000 | Loss: 0.00001155
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001150
Iteration 103/1000 | Loss: 0.00001149
Iteration 104/1000 | Loss: 0.00001149
Iteration 105/1000 | Loss: 0.00001149
Iteration 106/1000 | Loss: 0.00001149
Iteration 107/1000 | Loss: 0.00001149
Iteration 108/1000 | Loss: 0.00001149
Iteration 109/1000 | Loss: 0.00001149
Iteration 110/1000 | Loss: 0.00001149
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001147
Iteration 117/1000 | Loss: 0.00001147
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001142
Iteration 129/1000 | Loss: 0.00001142
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001137
Iteration 143/1000 | Loss: 0.00001137
Iteration 144/1000 | Loss: 0.00001137
Iteration 145/1000 | Loss: 0.00001137
Iteration 146/1000 | Loss: 0.00001137
Iteration 147/1000 | Loss: 0.00001137
Iteration 148/1000 | Loss: 0.00001137
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001136
Iteration 151/1000 | Loss: 0.00001136
Iteration 152/1000 | Loss: 0.00001136
Iteration 153/1000 | Loss: 0.00001136
Iteration 154/1000 | Loss: 0.00001136
Iteration 155/1000 | Loss: 0.00001136
Iteration 156/1000 | Loss: 0.00001136
Iteration 157/1000 | Loss: 0.00001136
Iteration 158/1000 | Loss: 0.00001136
Iteration 159/1000 | Loss: 0.00001135
Iteration 160/1000 | Loss: 0.00001135
Iteration 161/1000 | Loss: 0.00001135
Iteration 162/1000 | Loss: 0.00001135
Iteration 163/1000 | Loss: 0.00001135
Iteration 164/1000 | Loss: 0.00001135
Iteration 165/1000 | Loss: 0.00001135
Iteration 166/1000 | Loss: 0.00001135
Iteration 167/1000 | Loss: 0.00001134
Iteration 168/1000 | Loss: 0.00001134
Iteration 169/1000 | Loss: 0.00001134
Iteration 170/1000 | Loss: 0.00001134
Iteration 171/1000 | Loss: 0.00001134
Iteration 172/1000 | Loss: 0.00001134
Iteration 173/1000 | Loss: 0.00001134
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001134
Iteration 180/1000 | Loss: 0.00001134
Iteration 181/1000 | Loss: 0.00001133
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001133
Iteration 195/1000 | Loss: 0.00001133
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001132
Iteration 207/1000 | Loss: 0.00001132
Iteration 208/1000 | Loss: 0.00001132
Iteration 209/1000 | Loss: 0.00001132
Iteration 210/1000 | Loss: 0.00001132
Iteration 211/1000 | Loss: 0.00001132
Iteration 212/1000 | Loss: 0.00001132
Iteration 213/1000 | Loss: 0.00001132
Iteration 214/1000 | Loss: 0.00001132
Iteration 215/1000 | Loss: 0.00001132
Iteration 216/1000 | Loss: 0.00001132
Iteration 217/1000 | Loss: 0.00001132
Iteration 218/1000 | Loss: 0.00001132
Iteration 219/1000 | Loss: 0.00001132
Iteration 220/1000 | Loss: 0.00001132
Iteration 221/1000 | Loss: 0.00001132
Iteration 222/1000 | Loss: 0.00001132
Iteration 223/1000 | Loss: 0.00001132
Iteration 224/1000 | Loss: 0.00001132
Iteration 225/1000 | Loss: 0.00001132
Iteration 226/1000 | Loss: 0.00001132
Iteration 227/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1320097655698191e-05, 1.1320097655698191e-05, 1.1320097655698191e-05, 1.1320097655698191e-05, 1.1320097655698191e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1320097655698191e-05

Optimization complete. Final v2v error: 2.8890140056610107 mm

Highest mean error: 3.058636426925659 mm for frame 33

Lowest mean error: 2.7082202434539795 mm for frame 12

Saving results

Total time: 48.287145137786865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531879
Iteration 2/25 | Loss: 0.00144422
Iteration 3/25 | Loss: 0.00128422
Iteration 4/25 | Loss: 0.00125752
Iteration 5/25 | Loss: 0.00124650
Iteration 6/25 | Loss: 0.00125288
Iteration 7/25 | Loss: 0.00125441
Iteration 8/25 | Loss: 0.00123938
Iteration 9/25 | Loss: 0.00123449
Iteration 10/25 | Loss: 0.00122548
Iteration 11/25 | Loss: 0.00122483
Iteration 12/25 | Loss: 0.00121757
Iteration 13/25 | Loss: 0.00121708
Iteration 14/25 | Loss: 0.00121562
Iteration 15/25 | Loss: 0.00121149
Iteration 16/25 | Loss: 0.00121139
Iteration 17/25 | Loss: 0.00121214
Iteration 18/25 | Loss: 0.00121160
Iteration 19/25 | Loss: 0.00121109
Iteration 20/25 | Loss: 0.00121129
Iteration 21/25 | Loss: 0.00121066
Iteration 22/25 | Loss: 0.00121072
Iteration 23/25 | Loss: 0.00120871
Iteration 24/25 | Loss: 0.00120746
Iteration 25/25 | Loss: 0.00120880

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49584055
Iteration 2/25 | Loss: 0.00083617
Iteration 3/25 | Loss: 0.00083617
Iteration 4/25 | Loss: 0.00083617
Iteration 5/25 | Loss: 0.00083617
Iteration 6/25 | Loss: 0.00083617
Iteration 7/25 | Loss: 0.00083617
Iteration 8/25 | Loss: 0.00083617
Iteration 9/25 | Loss: 0.00083617
Iteration 10/25 | Loss: 0.00083617
Iteration 11/25 | Loss: 0.00083616
Iteration 12/25 | Loss: 0.00083616
Iteration 13/25 | Loss: 0.00083616
Iteration 14/25 | Loss: 0.00083616
Iteration 15/25 | Loss: 0.00083616
Iteration 16/25 | Loss: 0.00083616
Iteration 17/25 | Loss: 0.00083616
Iteration 18/25 | Loss: 0.00083616
Iteration 19/25 | Loss: 0.00083616
Iteration 20/25 | Loss: 0.00083616
Iteration 21/25 | Loss: 0.00083616
Iteration 22/25 | Loss: 0.00083616
Iteration 23/25 | Loss: 0.00083616
Iteration 24/25 | Loss: 0.00083616
Iteration 25/25 | Loss: 0.00083616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083616
Iteration 2/1000 | Loss: 0.00002945
Iteration 3/1000 | Loss: 0.00003767
Iteration 4/1000 | Loss: 0.00002514
Iteration 5/1000 | Loss: 0.00002853
Iteration 6/1000 | Loss: 0.00003229
Iteration 7/1000 | Loss: 0.00002222
Iteration 8/1000 | Loss: 0.00003227
Iteration 9/1000 | Loss: 0.00003697
Iteration 10/1000 | Loss: 0.00003827
Iteration 11/1000 | Loss: 0.00003631
Iteration 12/1000 | Loss: 0.00003304
Iteration 13/1000 | Loss: 0.00002106
Iteration 14/1000 | Loss: 0.00003267
Iteration 15/1000 | Loss: 0.00005012
Iteration 16/1000 | Loss: 0.00001980
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001462
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001285
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001269
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001264
Iteration 28/1000 | Loss: 0.00001264
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001262
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001237
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001236
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001233
Iteration 53/1000 | Loss: 0.00001233
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001231
Iteration 56/1000 | Loss: 0.00001231
Iteration 57/1000 | Loss: 0.00001231
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001229
Iteration 61/1000 | Loss: 0.00001229
Iteration 62/1000 | Loss: 0.00001228
Iteration 63/1000 | Loss: 0.00001228
Iteration 64/1000 | Loss: 0.00001228
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001226
Iteration 69/1000 | Loss: 0.00001226
Iteration 70/1000 | Loss: 0.00001226
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001225
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001224
Iteration 76/1000 | Loss: 0.00001224
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001219
Iteration 97/1000 | Loss: 0.00001219
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001219
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001218
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001217
Iteration 108/1000 | Loss: 0.00001216
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001214
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001211
Iteration 135/1000 | Loss: 0.00001211
Iteration 136/1000 | Loss: 0.00001211
Iteration 137/1000 | Loss: 0.00001211
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001210
Iteration 144/1000 | Loss: 0.00001210
Iteration 145/1000 | Loss: 0.00001210
Iteration 146/1000 | Loss: 0.00001210
Iteration 147/1000 | Loss: 0.00001210
Iteration 148/1000 | Loss: 0.00001210
Iteration 149/1000 | Loss: 0.00001209
Iteration 150/1000 | Loss: 0.00001209
Iteration 151/1000 | Loss: 0.00001209
Iteration 152/1000 | Loss: 0.00001209
Iteration 153/1000 | Loss: 0.00001209
Iteration 154/1000 | Loss: 0.00001208
Iteration 155/1000 | Loss: 0.00001208
Iteration 156/1000 | Loss: 0.00001208
Iteration 157/1000 | Loss: 0.00001208
Iteration 158/1000 | Loss: 0.00001208
Iteration 159/1000 | Loss: 0.00001208
Iteration 160/1000 | Loss: 0.00001208
Iteration 161/1000 | Loss: 0.00001208
Iteration 162/1000 | Loss: 0.00001208
Iteration 163/1000 | Loss: 0.00001207
Iteration 164/1000 | Loss: 0.00001207
Iteration 165/1000 | Loss: 0.00001207
Iteration 166/1000 | Loss: 0.00001207
Iteration 167/1000 | Loss: 0.00001207
Iteration 168/1000 | Loss: 0.00001207
Iteration 169/1000 | Loss: 0.00001207
Iteration 170/1000 | Loss: 0.00001207
Iteration 171/1000 | Loss: 0.00001207
Iteration 172/1000 | Loss: 0.00001206
Iteration 173/1000 | Loss: 0.00001206
Iteration 174/1000 | Loss: 0.00001206
Iteration 175/1000 | Loss: 0.00001206
Iteration 176/1000 | Loss: 0.00001206
Iteration 177/1000 | Loss: 0.00001206
Iteration 178/1000 | Loss: 0.00001206
Iteration 179/1000 | Loss: 0.00001206
Iteration 180/1000 | Loss: 0.00001206
Iteration 181/1000 | Loss: 0.00001205
Iteration 182/1000 | Loss: 0.00001205
Iteration 183/1000 | Loss: 0.00001205
Iteration 184/1000 | Loss: 0.00001205
Iteration 185/1000 | Loss: 0.00001205
Iteration 186/1000 | Loss: 0.00001205
Iteration 187/1000 | Loss: 0.00001205
Iteration 188/1000 | Loss: 0.00001205
Iteration 189/1000 | Loss: 0.00001205
Iteration 190/1000 | Loss: 0.00001205
Iteration 191/1000 | Loss: 0.00001205
Iteration 192/1000 | Loss: 0.00001205
Iteration 193/1000 | Loss: 0.00001205
Iteration 194/1000 | Loss: 0.00001205
Iteration 195/1000 | Loss: 0.00001205
Iteration 196/1000 | Loss: 0.00001205
Iteration 197/1000 | Loss: 0.00001205
Iteration 198/1000 | Loss: 0.00001204
Iteration 199/1000 | Loss: 0.00001204
Iteration 200/1000 | Loss: 0.00001204
Iteration 201/1000 | Loss: 0.00001204
Iteration 202/1000 | Loss: 0.00001204
Iteration 203/1000 | Loss: 0.00001204
Iteration 204/1000 | Loss: 0.00001204
Iteration 205/1000 | Loss: 0.00001204
Iteration 206/1000 | Loss: 0.00001204
Iteration 207/1000 | Loss: 0.00001204
Iteration 208/1000 | Loss: 0.00001204
Iteration 209/1000 | Loss: 0.00001204
Iteration 210/1000 | Loss: 0.00001204
Iteration 211/1000 | Loss: 0.00001204
Iteration 212/1000 | Loss: 0.00001204
Iteration 213/1000 | Loss: 0.00001204
Iteration 214/1000 | Loss: 0.00001204
Iteration 215/1000 | Loss: 0.00001204
Iteration 216/1000 | Loss: 0.00001204
Iteration 217/1000 | Loss: 0.00001203
Iteration 218/1000 | Loss: 0.00001203
Iteration 219/1000 | Loss: 0.00001203
Iteration 220/1000 | Loss: 0.00001203
Iteration 221/1000 | Loss: 0.00001203
Iteration 222/1000 | Loss: 0.00001203
Iteration 223/1000 | Loss: 0.00001203
Iteration 224/1000 | Loss: 0.00001203
Iteration 225/1000 | Loss: 0.00001203
Iteration 226/1000 | Loss: 0.00001203
Iteration 227/1000 | Loss: 0.00001203
Iteration 228/1000 | Loss: 0.00001203
Iteration 229/1000 | Loss: 0.00001203
Iteration 230/1000 | Loss: 0.00001203
Iteration 231/1000 | Loss: 0.00001203
Iteration 232/1000 | Loss: 0.00001203
Iteration 233/1000 | Loss: 0.00001203
Iteration 234/1000 | Loss: 0.00001203
Iteration 235/1000 | Loss: 0.00001202
Iteration 236/1000 | Loss: 0.00001202
Iteration 237/1000 | Loss: 0.00001202
Iteration 238/1000 | Loss: 0.00001202
Iteration 239/1000 | Loss: 0.00001202
Iteration 240/1000 | Loss: 0.00001202
Iteration 241/1000 | Loss: 0.00001202
Iteration 242/1000 | Loss: 0.00001202
Iteration 243/1000 | Loss: 0.00001202
Iteration 244/1000 | Loss: 0.00001202
Iteration 245/1000 | Loss: 0.00001202
Iteration 246/1000 | Loss: 0.00001202
Iteration 247/1000 | Loss: 0.00001202
Iteration 248/1000 | Loss: 0.00001202
Iteration 249/1000 | Loss: 0.00001202
Iteration 250/1000 | Loss: 0.00001202
Iteration 251/1000 | Loss: 0.00001202
Iteration 252/1000 | Loss: 0.00001202
Iteration 253/1000 | Loss: 0.00001202
Iteration 254/1000 | Loss: 0.00001202
Iteration 255/1000 | Loss: 0.00001202
Iteration 256/1000 | Loss: 0.00001202
Iteration 257/1000 | Loss: 0.00001201
Iteration 258/1000 | Loss: 0.00001201
Iteration 259/1000 | Loss: 0.00001201
Iteration 260/1000 | Loss: 0.00001201
Iteration 261/1000 | Loss: 0.00001201
Iteration 262/1000 | Loss: 0.00001201
Iteration 263/1000 | Loss: 0.00001201
Iteration 264/1000 | Loss: 0.00001201
Iteration 265/1000 | Loss: 0.00001201
Iteration 266/1000 | Loss: 0.00001201
Iteration 267/1000 | Loss: 0.00001201
Iteration 268/1000 | Loss: 0.00001201
Iteration 269/1000 | Loss: 0.00001201
Iteration 270/1000 | Loss: 0.00001201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.2013594641757663e-05, 1.2013594641757663e-05, 1.2013594641757663e-05, 1.2013594641757663e-05, 1.2013594641757663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2013594641757663e-05

Optimization complete. Final v2v error: 2.9711270332336426 mm

Highest mean error: 4.140745162963867 mm for frame 82

Lowest mean error: 2.6861844062805176 mm for frame 119

Saving results

Total time: 100.76786160469055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426543
Iteration 2/25 | Loss: 0.00129332
Iteration 3/25 | Loss: 0.00122992
Iteration 4/25 | Loss: 0.00121530
Iteration 5/25 | Loss: 0.00121125
Iteration 6/25 | Loss: 0.00121045
Iteration 7/25 | Loss: 0.00121045
Iteration 8/25 | Loss: 0.00121045
Iteration 9/25 | Loss: 0.00121045
Iteration 10/25 | Loss: 0.00121045
Iteration 11/25 | Loss: 0.00121045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001210449612699449, 0.001210449612699449, 0.001210449612699449, 0.001210449612699449, 0.001210449612699449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210449612699449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45443833
Iteration 2/25 | Loss: 0.00074644
Iteration 3/25 | Loss: 0.00074644
Iteration 4/25 | Loss: 0.00074643
Iteration 5/25 | Loss: 0.00074643
Iteration 6/25 | Loss: 0.00074643
Iteration 7/25 | Loss: 0.00074643
Iteration 8/25 | Loss: 0.00074643
Iteration 9/25 | Loss: 0.00074643
Iteration 10/25 | Loss: 0.00074643
Iteration 11/25 | Loss: 0.00074643
Iteration 12/25 | Loss: 0.00074643
Iteration 13/25 | Loss: 0.00074643
Iteration 14/25 | Loss: 0.00074643
Iteration 15/25 | Loss: 0.00074643
Iteration 16/25 | Loss: 0.00074643
Iteration 17/25 | Loss: 0.00074643
Iteration 18/25 | Loss: 0.00074643
Iteration 19/25 | Loss: 0.00074643
Iteration 20/25 | Loss: 0.00074643
Iteration 21/25 | Loss: 0.00074643
Iteration 22/25 | Loss: 0.00074643
Iteration 23/25 | Loss: 0.00074643
Iteration 24/25 | Loss: 0.00074643
Iteration 25/25 | Loss: 0.00074643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074643
Iteration 2/1000 | Loss: 0.00003100
Iteration 3/1000 | Loss: 0.00002035
Iteration 4/1000 | Loss: 0.00001763
Iteration 5/1000 | Loss: 0.00001652
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001512
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001456
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001441
Iteration 15/1000 | Loss: 0.00001438
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001432
Iteration 20/1000 | Loss: 0.00001431
Iteration 21/1000 | Loss: 0.00001431
Iteration 22/1000 | Loss: 0.00001430
Iteration 23/1000 | Loss: 0.00001430
Iteration 24/1000 | Loss: 0.00001429
Iteration 25/1000 | Loss: 0.00001429
Iteration 26/1000 | Loss: 0.00001429
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001426
Iteration 29/1000 | Loss: 0.00001425
Iteration 30/1000 | Loss: 0.00001424
Iteration 31/1000 | Loss: 0.00001423
Iteration 32/1000 | Loss: 0.00001423
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001416
Iteration 35/1000 | Loss: 0.00001415
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001407
Iteration 44/1000 | Loss: 0.00001406
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001403
Iteration 47/1000 | Loss: 0.00001403
Iteration 48/1000 | Loss: 0.00001403
Iteration 49/1000 | Loss: 0.00001403
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001403
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001402
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001402
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [1.402306224917993e-05, 1.402306224917993e-05, 1.402306224917993e-05, 1.402306224917993e-05, 1.402306224917993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.402306224917993e-05

Optimization complete. Final v2v error: 3.216529369354248 mm

Highest mean error: 3.4511940479278564 mm for frame 103

Lowest mean error: 3.0247740745544434 mm for frame 117

Saving results

Total time: 31.456676244735718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00934699
Iteration 2/25 | Loss: 0.00227348
Iteration 3/25 | Loss: 0.00164740
Iteration 4/25 | Loss: 0.00152860
Iteration 5/25 | Loss: 0.00146423
Iteration 6/25 | Loss: 0.00142295
Iteration 7/25 | Loss: 0.00138824
Iteration 8/25 | Loss: 0.00134962
Iteration 9/25 | Loss: 0.00133273
Iteration 10/25 | Loss: 0.00131276
Iteration 11/25 | Loss: 0.00129574
Iteration 12/25 | Loss: 0.00129110
Iteration 13/25 | Loss: 0.00128614
Iteration 14/25 | Loss: 0.00128493
Iteration 15/25 | Loss: 0.00128421
Iteration 16/25 | Loss: 0.00128325
Iteration 17/25 | Loss: 0.00128539
Iteration 18/25 | Loss: 0.00128177
Iteration 19/25 | Loss: 0.00128027
Iteration 20/25 | Loss: 0.00128300
Iteration 21/25 | Loss: 0.00128480
Iteration 22/25 | Loss: 0.00127620
Iteration 23/25 | Loss: 0.00127448
Iteration 24/25 | Loss: 0.00127385
Iteration 25/25 | Loss: 0.00127309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50990522
Iteration 2/25 | Loss: 0.00120071
Iteration 3/25 | Loss: 0.00076633
Iteration 4/25 | Loss: 0.00076633
Iteration 5/25 | Loss: 0.00076633
Iteration 6/25 | Loss: 0.00076633
Iteration 7/25 | Loss: 0.00076633
Iteration 8/25 | Loss: 0.00076633
Iteration 9/25 | Loss: 0.00076633
Iteration 10/25 | Loss: 0.00076633
Iteration 11/25 | Loss: 0.00076633
Iteration 12/25 | Loss: 0.00076633
Iteration 13/25 | Loss: 0.00076633
Iteration 14/25 | Loss: 0.00076633
Iteration 15/25 | Loss: 0.00076633
Iteration 16/25 | Loss: 0.00076633
Iteration 17/25 | Loss: 0.00076633
Iteration 18/25 | Loss: 0.00076633
Iteration 19/25 | Loss: 0.00076633
Iteration 20/25 | Loss: 0.00076633
Iteration 21/25 | Loss: 0.00076633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007663287688046694, 0.0007663287688046694, 0.0007663287688046694, 0.0007663287688046694, 0.0007663287688046694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007663287688046694

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076633
Iteration 2/1000 | Loss: 0.00016747
Iteration 3/1000 | Loss: 0.00040923
Iteration 4/1000 | Loss: 0.00038686
Iteration 5/1000 | Loss: 0.00216815
Iteration 6/1000 | Loss: 0.00039509
Iteration 7/1000 | Loss: 0.00039848
Iteration 8/1000 | Loss: 0.00063586
Iteration 9/1000 | Loss: 0.00005078
Iteration 10/1000 | Loss: 0.00003066
Iteration 11/1000 | Loss: 0.00002835
Iteration 12/1000 | Loss: 0.00034805
Iteration 13/1000 | Loss: 0.00154050
Iteration 14/1000 | Loss: 0.00012236
Iteration 15/1000 | Loss: 0.00005356
Iteration 16/1000 | Loss: 0.00002647
Iteration 17/1000 | Loss: 0.00002512
Iteration 18/1000 | Loss: 0.00002353
Iteration 19/1000 | Loss: 0.00002282
Iteration 20/1000 | Loss: 0.00002227
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002138
Iteration 23/1000 | Loss: 0.00002104
Iteration 24/1000 | Loss: 0.00002080
Iteration 25/1000 | Loss: 0.00002057
Iteration 26/1000 | Loss: 0.00002045
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002040
Iteration 30/1000 | Loss: 0.00002040
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002040
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002031
Iteration 38/1000 | Loss: 0.00002029
Iteration 39/1000 | Loss: 0.00002029
Iteration 40/1000 | Loss: 0.00002029
Iteration 41/1000 | Loss: 0.00002029
Iteration 42/1000 | Loss: 0.00002028
Iteration 43/1000 | Loss: 0.00002028
Iteration 44/1000 | Loss: 0.00002028
Iteration 45/1000 | Loss: 0.00002028
Iteration 46/1000 | Loss: 0.00002027
Iteration 47/1000 | Loss: 0.00002024
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002023
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002022
Iteration 53/1000 | Loss: 0.00002022
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002018
Iteration 62/1000 | Loss: 0.00002018
Iteration 63/1000 | Loss: 0.00002016
Iteration 64/1000 | Loss: 0.00002016
Iteration 65/1000 | Loss: 0.00002015
Iteration 66/1000 | Loss: 0.00002015
Iteration 67/1000 | Loss: 0.00002014
Iteration 68/1000 | Loss: 0.00002013
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002010
Iteration 74/1000 | Loss: 0.00002010
Iteration 75/1000 | Loss: 0.00002010
Iteration 76/1000 | Loss: 0.00002010
Iteration 77/1000 | Loss: 0.00002010
Iteration 78/1000 | Loss: 0.00002010
Iteration 79/1000 | Loss: 0.00002010
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00002010
Iteration 82/1000 | Loss: 0.00002010
Iteration 83/1000 | Loss: 0.00002010
Iteration 84/1000 | Loss: 0.00002010
Iteration 85/1000 | Loss: 0.00002009
Iteration 86/1000 | Loss: 0.00002009
Iteration 87/1000 | Loss: 0.00002009
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00002009
Iteration 90/1000 | Loss: 0.00002009
Iteration 91/1000 | Loss: 0.00002008
Iteration 92/1000 | Loss: 0.00002008
Iteration 93/1000 | Loss: 0.00002008
Iteration 94/1000 | Loss: 0.00002008
Iteration 95/1000 | Loss: 0.00002008
Iteration 96/1000 | Loss: 0.00002008
Iteration 97/1000 | Loss: 0.00002008
Iteration 98/1000 | Loss: 0.00002008
Iteration 99/1000 | Loss: 0.00002008
Iteration 100/1000 | Loss: 0.00002007
Iteration 101/1000 | Loss: 0.00002007
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002006
Iteration 104/1000 | Loss: 0.00002006
Iteration 105/1000 | Loss: 0.00002006
Iteration 106/1000 | Loss: 0.00002006
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002006
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002005
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002003
Iteration 120/1000 | Loss: 0.00002003
Iteration 121/1000 | Loss: 0.00002003
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002002
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002001
Iteration 127/1000 | Loss: 0.00002001
Iteration 128/1000 | Loss: 0.00002000
Iteration 129/1000 | Loss: 0.00002000
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00002000
Iteration 132/1000 | Loss: 0.00001999
Iteration 133/1000 | Loss: 0.00001999
Iteration 134/1000 | Loss: 0.00001999
Iteration 135/1000 | Loss: 0.00001999
Iteration 136/1000 | Loss: 0.00001998
Iteration 137/1000 | Loss: 0.00001998
Iteration 138/1000 | Loss: 0.00001998
Iteration 139/1000 | Loss: 0.00001998
Iteration 140/1000 | Loss: 0.00001998
Iteration 141/1000 | Loss: 0.00001997
Iteration 142/1000 | Loss: 0.00001997
Iteration 143/1000 | Loss: 0.00001997
Iteration 144/1000 | Loss: 0.00001997
Iteration 145/1000 | Loss: 0.00001997
Iteration 146/1000 | Loss: 0.00001997
Iteration 147/1000 | Loss: 0.00001997
Iteration 148/1000 | Loss: 0.00001997
Iteration 149/1000 | Loss: 0.00001997
Iteration 150/1000 | Loss: 0.00001997
Iteration 151/1000 | Loss: 0.00001997
Iteration 152/1000 | Loss: 0.00001997
Iteration 153/1000 | Loss: 0.00001997
Iteration 154/1000 | Loss: 0.00001997
Iteration 155/1000 | Loss: 0.00001997
Iteration 156/1000 | Loss: 0.00001996
Iteration 157/1000 | Loss: 0.00001996
Iteration 158/1000 | Loss: 0.00001996
Iteration 159/1000 | Loss: 0.00001996
Iteration 160/1000 | Loss: 0.00001996
Iteration 161/1000 | Loss: 0.00001996
Iteration 162/1000 | Loss: 0.00001996
Iteration 163/1000 | Loss: 0.00001996
Iteration 164/1000 | Loss: 0.00001995
Iteration 165/1000 | Loss: 0.00001995
Iteration 166/1000 | Loss: 0.00001995
Iteration 167/1000 | Loss: 0.00001995
Iteration 168/1000 | Loss: 0.00001995
Iteration 169/1000 | Loss: 0.00001995
Iteration 170/1000 | Loss: 0.00001995
Iteration 171/1000 | Loss: 0.00001995
Iteration 172/1000 | Loss: 0.00001995
Iteration 173/1000 | Loss: 0.00001994
Iteration 174/1000 | Loss: 0.00001994
Iteration 175/1000 | Loss: 0.00001994
Iteration 176/1000 | Loss: 0.00001994
Iteration 177/1000 | Loss: 0.00001994
Iteration 178/1000 | Loss: 0.00001994
Iteration 179/1000 | Loss: 0.00001994
Iteration 180/1000 | Loss: 0.00001994
Iteration 181/1000 | Loss: 0.00001994
Iteration 182/1000 | Loss: 0.00001994
Iteration 183/1000 | Loss: 0.00001994
Iteration 184/1000 | Loss: 0.00001993
Iteration 185/1000 | Loss: 0.00001993
Iteration 186/1000 | Loss: 0.00001993
Iteration 187/1000 | Loss: 0.00001993
Iteration 188/1000 | Loss: 0.00001993
Iteration 189/1000 | Loss: 0.00001993
Iteration 190/1000 | Loss: 0.00001993
Iteration 191/1000 | Loss: 0.00001993
Iteration 192/1000 | Loss: 0.00001992
Iteration 193/1000 | Loss: 0.00001992
Iteration 194/1000 | Loss: 0.00001992
Iteration 195/1000 | Loss: 0.00001992
Iteration 196/1000 | Loss: 0.00001992
Iteration 197/1000 | Loss: 0.00001992
Iteration 198/1000 | Loss: 0.00001992
Iteration 199/1000 | Loss: 0.00001992
Iteration 200/1000 | Loss: 0.00001992
Iteration 201/1000 | Loss: 0.00001992
Iteration 202/1000 | Loss: 0.00001991
Iteration 203/1000 | Loss: 0.00001991
Iteration 204/1000 | Loss: 0.00001991
Iteration 205/1000 | Loss: 0.00001991
Iteration 206/1000 | Loss: 0.00001991
Iteration 207/1000 | Loss: 0.00001991
Iteration 208/1000 | Loss: 0.00001991
Iteration 209/1000 | Loss: 0.00001991
Iteration 210/1000 | Loss: 0.00001991
Iteration 211/1000 | Loss: 0.00001990
Iteration 212/1000 | Loss: 0.00001990
Iteration 213/1000 | Loss: 0.00001990
Iteration 214/1000 | Loss: 0.00001990
Iteration 215/1000 | Loss: 0.00001990
Iteration 216/1000 | Loss: 0.00001990
Iteration 217/1000 | Loss: 0.00001990
Iteration 218/1000 | Loss: 0.00001990
Iteration 219/1000 | Loss: 0.00001989
Iteration 220/1000 | Loss: 0.00001989
Iteration 221/1000 | Loss: 0.00001989
Iteration 222/1000 | Loss: 0.00001989
Iteration 223/1000 | Loss: 0.00001989
Iteration 224/1000 | Loss: 0.00001989
Iteration 225/1000 | Loss: 0.00001988
Iteration 226/1000 | Loss: 0.00001988
Iteration 227/1000 | Loss: 0.00001988
Iteration 228/1000 | Loss: 0.00001988
Iteration 229/1000 | Loss: 0.00001988
Iteration 230/1000 | Loss: 0.00001987
Iteration 231/1000 | Loss: 0.00001987
Iteration 232/1000 | Loss: 0.00001987
Iteration 233/1000 | Loss: 0.00001987
Iteration 234/1000 | Loss: 0.00001987
Iteration 235/1000 | Loss: 0.00001987
Iteration 236/1000 | Loss: 0.00001987
Iteration 237/1000 | Loss: 0.00001987
Iteration 238/1000 | Loss: 0.00001987
Iteration 239/1000 | Loss: 0.00001987
Iteration 240/1000 | Loss: 0.00001987
Iteration 241/1000 | Loss: 0.00001987
Iteration 242/1000 | Loss: 0.00001987
Iteration 243/1000 | Loss: 0.00001987
Iteration 244/1000 | Loss: 0.00001987
Iteration 245/1000 | Loss: 0.00001987
Iteration 246/1000 | Loss: 0.00001987
Iteration 247/1000 | Loss: 0.00001987
Iteration 248/1000 | Loss: 0.00001987
Iteration 249/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.987265386560466e-05, 1.987265386560466e-05, 1.987265386560466e-05, 1.987265386560466e-05, 1.987265386560466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.987265386560466e-05

Optimization complete. Final v2v error: 3.8068671226501465 mm

Highest mean error: 4.236651420593262 mm for frame 25

Lowest mean error: 3.3893380165100098 mm for frame 128

Saving results

Total time: 114.64601421356201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459835
Iteration 2/25 | Loss: 0.00131182
Iteration 3/25 | Loss: 0.00124677
Iteration 4/25 | Loss: 0.00123792
Iteration 5/25 | Loss: 0.00123583
Iteration 6/25 | Loss: 0.00123583
Iteration 7/25 | Loss: 0.00123583
Iteration 8/25 | Loss: 0.00123583
Iteration 9/25 | Loss: 0.00123583
Iteration 10/25 | Loss: 0.00123583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012358305975794792, 0.0012358305975794792, 0.0012358305975794792, 0.0012358305975794792, 0.0012358305975794792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012358305975794792

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42576170
Iteration 2/25 | Loss: 0.00072120
Iteration 3/25 | Loss: 0.00072120
Iteration 4/25 | Loss: 0.00072119
Iteration 5/25 | Loss: 0.00072119
Iteration 6/25 | Loss: 0.00072119
Iteration 7/25 | Loss: 0.00072119
Iteration 8/25 | Loss: 0.00072119
Iteration 9/25 | Loss: 0.00072119
Iteration 10/25 | Loss: 0.00072119
Iteration 11/25 | Loss: 0.00072119
Iteration 12/25 | Loss: 0.00072119
Iteration 13/25 | Loss: 0.00072119
Iteration 14/25 | Loss: 0.00072119
Iteration 15/25 | Loss: 0.00072119
Iteration 16/25 | Loss: 0.00072119
Iteration 17/25 | Loss: 0.00072119
Iteration 18/25 | Loss: 0.00072119
Iteration 19/25 | Loss: 0.00072119
Iteration 20/25 | Loss: 0.00072119
Iteration 21/25 | Loss: 0.00072119
Iteration 22/25 | Loss: 0.00072119
Iteration 23/25 | Loss: 0.00072119
Iteration 24/25 | Loss: 0.00072119
Iteration 25/25 | Loss: 0.00072119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072119
Iteration 2/1000 | Loss: 0.00003094
Iteration 3/1000 | Loss: 0.00002420
Iteration 4/1000 | Loss: 0.00002246
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001902
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001889
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001870
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001869
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001861
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001857
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001849
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001848
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001845
Iteration 118/1000 | Loss: 0.00001845
Iteration 119/1000 | Loss: 0.00001845
Iteration 120/1000 | Loss: 0.00001845
Iteration 121/1000 | Loss: 0.00001845
Iteration 122/1000 | Loss: 0.00001845
Iteration 123/1000 | Loss: 0.00001845
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.8449522031005472e-05, 1.8449522031005472e-05, 1.8449522031005472e-05, 1.8449522031005472e-05, 1.8449522031005472e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8449522031005472e-05

Optimization complete. Final v2v error: 3.6456305980682373 mm

Highest mean error: 3.768484592437744 mm for frame 123

Lowest mean error: 3.472945213317871 mm for frame 43

Saving results

Total time: 37.711419105529785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823268
Iteration 2/25 | Loss: 0.00203956
Iteration 3/25 | Loss: 0.00171423
Iteration 4/25 | Loss: 0.00201216
Iteration 5/25 | Loss: 0.00185301
Iteration 6/25 | Loss: 0.00152822
Iteration 7/25 | Loss: 0.00152054
Iteration 8/25 | Loss: 0.00150153
Iteration 9/25 | Loss: 0.00149115
Iteration 10/25 | Loss: 0.00148857
Iteration 11/25 | Loss: 0.00148829
Iteration 12/25 | Loss: 0.00148825
Iteration 13/25 | Loss: 0.00148825
Iteration 14/25 | Loss: 0.00148825
Iteration 15/25 | Loss: 0.00148825
Iteration 16/25 | Loss: 0.00148825
Iteration 17/25 | Loss: 0.00148825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014882463729009032, 0.0014882463729009032, 0.0014882463729009032, 0.0014882463729009032, 0.0014882463729009032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014882463729009032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14871454
Iteration 2/25 | Loss: 0.00091569
Iteration 3/25 | Loss: 0.00091569
Iteration 4/25 | Loss: 0.00091569
Iteration 5/25 | Loss: 0.00091569
Iteration 6/25 | Loss: 0.00091569
Iteration 7/25 | Loss: 0.00091569
Iteration 8/25 | Loss: 0.00091569
Iteration 9/25 | Loss: 0.00091569
Iteration 10/25 | Loss: 0.00091569
Iteration 11/25 | Loss: 0.00091569
Iteration 12/25 | Loss: 0.00091569
Iteration 13/25 | Loss: 0.00091569
Iteration 14/25 | Loss: 0.00091569
Iteration 15/25 | Loss: 0.00091569
Iteration 16/25 | Loss: 0.00091569
Iteration 17/25 | Loss: 0.00091569
Iteration 18/25 | Loss: 0.00091569
Iteration 19/25 | Loss: 0.00091569
Iteration 20/25 | Loss: 0.00091569
Iteration 21/25 | Loss: 0.00091569
Iteration 22/25 | Loss: 0.00091569
Iteration 23/25 | Loss: 0.00091569
Iteration 24/25 | Loss: 0.00091569
Iteration 25/25 | Loss: 0.00091569

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091569
Iteration 2/1000 | Loss: 0.00047297
Iteration 3/1000 | Loss: 0.00008210
Iteration 4/1000 | Loss: 0.00007279
Iteration 5/1000 | Loss: 0.00006741
Iteration 6/1000 | Loss: 0.00006218
Iteration 7/1000 | Loss: 0.00005796
Iteration 8/1000 | Loss: 0.00005453
Iteration 9/1000 | Loss: 0.00005183
Iteration 10/1000 | Loss: 0.00005050
Iteration 11/1000 | Loss: 0.00004947
Iteration 12/1000 | Loss: 0.00004876
Iteration 13/1000 | Loss: 0.00004824
Iteration 14/1000 | Loss: 0.00004791
Iteration 15/1000 | Loss: 0.00004768
Iteration 16/1000 | Loss: 0.00004757
Iteration 17/1000 | Loss: 0.00004736
Iteration 18/1000 | Loss: 0.00004720
Iteration 19/1000 | Loss: 0.00004717
Iteration 20/1000 | Loss: 0.00004714
Iteration 21/1000 | Loss: 0.00004714
Iteration 22/1000 | Loss: 0.00004713
Iteration 23/1000 | Loss: 0.00004711
Iteration 24/1000 | Loss: 0.00004710
Iteration 25/1000 | Loss: 0.00004709
Iteration 26/1000 | Loss: 0.00004709
Iteration 27/1000 | Loss: 0.00004708
Iteration 28/1000 | Loss: 0.00004707
Iteration 29/1000 | Loss: 0.00004703
Iteration 30/1000 | Loss: 0.00004703
Iteration 31/1000 | Loss: 0.00004701
Iteration 32/1000 | Loss: 0.00004701
Iteration 33/1000 | Loss: 0.00004701
Iteration 34/1000 | Loss: 0.00004701
Iteration 35/1000 | Loss: 0.00004700
Iteration 36/1000 | Loss: 0.00004700
Iteration 37/1000 | Loss: 0.00004700
Iteration 38/1000 | Loss: 0.00004699
Iteration 39/1000 | Loss: 0.00004699
Iteration 40/1000 | Loss: 0.00004698
Iteration 41/1000 | Loss: 0.00004697
Iteration 42/1000 | Loss: 0.00004697
Iteration 43/1000 | Loss: 0.00004697
Iteration 44/1000 | Loss: 0.00004696
Iteration 45/1000 | Loss: 0.00004694
Iteration 46/1000 | Loss: 0.00004694
Iteration 47/1000 | Loss: 0.00004694
Iteration 48/1000 | Loss: 0.00004694
Iteration 49/1000 | Loss: 0.00004694
Iteration 50/1000 | Loss: 0.00004694
Iteration 51/1000 | Loss: 0.00004694
Iteration 52/1000 | Loss: 0.00004694
Iteration 53/1000 | Loss: 0.00004694
Iteration 54/1000 | Loss: 0.00004693
Iteration 55/1000 | Loss: 0.00004693
Iteration 56/1000 | Loss: 0.00004693
Iteration 57/1000 | Loss: 0.00004692
Iteration 58/1000 | Loss: 0.00004692
Iteration 59/1000 | Loss: 0.00004692
Iteration 60/1000 | Loss: 0.00004692
Iteration 61/1000 | Loss: 0.00004692
Iteration 62/1000 | Loss: 0.00004692
Iteration 63/1000 | Loss: 0.00004692
Iteration 64/1000 | Loss: 0.00004691
Iteration 65/1000 | Loss: 0.00004691
Iteration 66/1000 | Loss: 0.00004691
Iteration 67/1000 | Loss: 0.00004691
Iteration 68/1000 | Loss: 0.00004691
Iteration 69/1000 | Loss: 0.00004691
Iteration 70/1000 | Loss: 0.00004690
Iteration 71/1000 | Loss: 0.00004689
Iteration 72/1000 | Loss: 0.00004689
Iteration 73/1000 | Loss: 0.00004689
Iteration 74/1000 | Loss: 0.00004689
Iteration 75/1000 | Loss: 0.00004688
Iteration 76/1000 | Loss: 0.00004688
Iteration 77/1000 | Loss: 0.00004688
Iteration 78/1000 | Loss: 0.00004688
Iteration 79/1000 | Loss: 0.00004688
Iteration 80/1000 | Loss: 0.00004687
Iteration 81/1000 | Loss: 0.00004687
Iteration 82/1000 | Loss: 0.00004687
Iteration 83/1000 | Loss: 0.00004687
Iteration 84/1000 | Loss: 0.00004686
Iteration 85/1000 | Loss: 0.00004686
Iteration 86/1000 | Loss: 0.00004686
Iteration 87/1000 | Loss: 0.00004686
Iteration 88/1000 | Loss: 0.00004685
Iteration 89/1000 | Loss: 0.00004685
Iteration 90/1000 | Loss: 0.00004685
Iteration 91/1000 | Loss: 0.00004684
Iteration 92/1000 | Loss: 0.00004684
Iteration 93/1000 | Loss: 0.00004684
Iteration 94/1000 | Loss: 0.00004683
Iteration 95/1000 | Loss: 0.00004683
Iteration 96/1000 | Loss: 0.00004683
Iteration 97/1000 | Loss: 0.00004683
Iteration 98/1000 | Loss: 0.00004683
Iteration 99/1000 | Loss: 0.00004683
Iteration 100/1000 | Loss: 0.00004683
Iteration 101/1000 | Loss: 0.00004683
Iteration 102/1000 | Loss: 0.00004683
Iteration 103/1000 | Loss: 0.00004682
Iteration 104/1000 | Loss: 0.00004681
Iteration 105/1000 | Loss: 0.00004681
Iteration 106/1000 | Loss: 0.00004681
Iteration 107/1000 | Loss: 0.00004681
Iteration 108/1000 | Loss: 0.00004680
Iteration 109/1000 | Loss: 0.00004680
Iteration 110/1000 | Loss: 0.00004680
Iteration 111/1000 | Loss: 0.00004680
Iteration 112/1000 | Loss: 0.00004680
Iteration 113/1000 | Loss: 0.00004680
Iteration 114/1000 | Loss: 0.00004680
Iteration 115/1000 | Loss: 0.00004680
Iteration 116/1000 | Loss: 0.00004680
Iteration 117/1000 | Loss: 0.00004679
Iteration 118/1000 | Loss: 0.00004679
Iteration 119/1000 | Loss: 0.00004679
Iteration 120/1000 | Loss: 0.00004679
Iteration 121/1000 | Loss: 0.00004678
Iteration 122/1000 | Loss: 0.00004678
Iteration 123/1000 | Loss: 0.00004677
Iteration 124/1000 | Loss: 0.00004677
Iteration 125/1000 | Loss: 0.00004677
Iteration 126/1000 | Loss: 0.00004676
Iteration 127/1000 | Loss: 0.00004675
Iteration 128/1000 | Loss: 0.00004674
Iteration 129/1000 | Loss: 0.00004674
Iteration 130/1000 | Loss: 0.00004674
Iteration 131/1000 | Loss: 0.00004674
Iteration 132/1000 | Loss: 0.00004673
Iteration 133/1000 | Loss: 0.00004673
Iteration 134/1000 | Loss: 0.00004673
Iteration 135/1000 | Loss: 0.00004673
Iteration 136/1000 | Loss: 0.00004673
Iteration 137/1000 | Loss: 0.00004673
Iteration 138/1000 | Loss: 0.00004673
Iteration 139/1000 | Loss: 0.00004672
Iteration 140/1000 | Loss: 0.00004672
Iteration 141/1000 | Loss: 0.00004672
Iteration 142/1000 | Loss: 0.00004672
Iteration 143/1000 | Loss: 0.00004672
Iteration 144/1000 | Loss: 0.00004672
Iteration 145/1000 | Loss: 0.00004672
Iteration 146/1000 | Loss: 0.00004671
Iteration 147/1000 | Loss: 0.00004671
Iteration 148/1000 | Loss: 0.00004671
Iteration 149/1000 | Loss: 0.00004670
Iteration 150/1000 | Loss: 0.00004670
Iteration 151/1000 | Loss: 0.00004670
Iteration 152/1000 | Loss: 0.00004670
Iteration 153/1000 | Loss: 0.00004669
Iteration 154/1000 | Loss: 0.00004669
Iteration 155/1000 | Loss: 0.00004668
Iteration 156/1000 | Loss: 0.00004668
Iteration 157/1000 | Loss: 0.00004668
Iteration 158/1000 | Loss: 0.00004668
Iteration 159/1000 | Loss: 0.00004668
Iteration 160/1000 | Loss: 0.00004668
Iteration 161/1000 | Loss: 0.00004667
Iteration 162/1000 | Loss: 0.00004667
Iteration 163/1000 | Loss: 0.00004667
Iteration 164/1000 | Loss: 0.00004667
Iteration 165/1000 | Loss: 0.00004667
Iteration 166/1000 | Loss: 0.00004667
Iteration 167/1000 | Loss: 0.00004667
Iteration 168/1000 | Loss: 0.00004667
Iteration 169/1000 | Loss: 0.00004667
Iteration 170/1000 | Loss: 0.00004667
Iteration 171/1000 | Loss: 0.00004667
Iteration 172/1000 | Loss: 0.00004667
Iteration 173/1000 | Loss: 0.00004667
Iteration 174/1000 | Loss: 0.00004667
Iteration 175/1000 | Loss: 0.00004667
Iteration 176/1000 | Loss: 0.00004666
Iteration 177/1000 | Loss: 0.00004666
Iteration 178/1000 | Loss: 0.00004666
Iteration 179/1000 | Loss: 0.00004665
Iteration 180/1000 | Loss: 0.00004665
Iteration 181/1000 | Loss: 0.00004665
Iteration 182/1000 | Loss: 0.00004665
Iteration 183/1000 | Loss: 0.00004665
Iteration 184/1000 | Loss: 0.00004664
Iteration 185/1000 | Loss: 0.00004664
Iteration 186/1000 | Loss: 0.00004664
Iteration 187/1000 | Loss: 0.00004664
Iteration 188/1000 | Loss: 0.00004664
Iteration 189/1000 | Loss: 0.00004664
Iteration 190/1000 | Loss: 0.00004664
Iteration 191/1000 | Loss: 0.00004664
Iteration 192/1000 | Loss: 0.00004664
Iteration 193/1000 | Loss: 0.00004664
Iteration 194/1000 | Loss: 0.00004664
Iteration 195/1000 | Loss: 0.00004664
Iteration 196/1000 | Loss: 0.00004664
Iteration 197/1000 | Loss: 0.00004664
Iteration 198/1000 | Loss: 0.00004664
Iteration 199/1000 | Loss: 0.00004664
Iteration 200/1000 | Loss: 0.00004663
Iteration 201/1000 | Loss: 0.00004663
Iteration 202/1000 | Loss: 0.00004663
Iteration 203/1000 | Loss: 0.00004663
Iteration 204/1000 | Loss: 0.00004663
Iteration 205/1000 | Loss: 0.00004663
Iteration 206/1000 | Loss: 0.00004662
Iteration 207/1000 | Loss: 0.00004662
Iteration 208/1000 | Loss: 0.00004662
Iteration 209/1000 | Loss: 0.00004662
Iteration 210/1000 | Loss: 0.00004662
Iteration 211/1000 | Loss: 0.00004662
Iteration 212/1000 | Loss: 0.00004662
Iteration 213/1000 | Loss: 0.00004662
Iteration 214/1000 | Loss: 0.00004662
Iteration 215/1000 | Loss: 0.00004662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [4.662370338337496e-05, 4.662370338337496e-05, 4.662370338337496e-05, 4.662370338337496e-05, 4.662370338337496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.662370338337496e-05

Optimization complete. Final v2v error: 5.4594340324401855 mm

Highest mean error: 5.653385639190674 mm for frame 26

Lowest mean error: 5.395750045776367 mm for frame 55

Saving results

Total time: 59.30108332633972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003758
Iteration 2/25 | Loss: 0.00229011
Iteration 3/25 | Loss: 0.00179149
Iteration 4/25 | Loss: 0.00210450
Iteration 5/25 | Loss: 0.00144736
Iteration 6/25 | Loss: 0.00135218
Iteration 7/25 | Loss: 0.00133682
Iteration 8/25 | Loss: 0.00134489
Iteration 9/25 | Loss: 0.00133007
Iteration 10/25 | Loss: 0.00133013
Iteration 11/25 | Loss: 0.00132557
Iteration 12/25 | Loss: 0.00132866
Iteration 13/25 | Loss: 0.00132029
Iteration 14/25 | Loss: 0.00131737
Iteration 15/25 | Loss: 0.00131675
Iteration 16/25 | Loss: 0.00131665
Iteration 17/25 | Loss: 0.00131663
Iteration 18/25 | Loss: 0.00131660
Iteration 19/25 | Loss: 0.00131660
Iteration 20/25 | Loss: 0.00131660
Iteration 21/25 | Loss: 0.00131660
Iteration 22/25 | Loss: 0.00131659
Iteration 23/25 | Loss: 0.00131659
Iteration 24/25 | Loss: 0.00131659
Iteration 25/25 | Loss: 0.00131659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42227578
Iteration 2/25 | Loss: 0.00073946
Iteration 3/25 | Loss: 0.00073946
Iteration 4/25 | Loss: 0.00073946
Iteration 5/25 | Loss: 0.00073946
Iteration 6/25 | Loss: 0.00073945
Iteration 7/25 | Loss: 0.00073945
Iteration 8/25 | Loss: 0.00073945
Iteration 9/25 | Loss: 0.00073945
Iteration 10/25 | Loss: 0.00073945
Iteration 11/25 | Loss: 0.00073945
Iteration 12/25 | Loss: 0.00073945
Iteration 13/25 | Loss: 0.00073945
Iteration 14/25 | Loss: 0.00073945
Iteration 15/25 | Loss: 0.00073945
Iteration 16/25 | Loss: 0.00073945
Iteration 17/25 | Loss: 0.00073945
Iteration 18/25 | Loss: 0.00073945
Iteration 19/25 | Loss: 0.00073945
Iteration 20/25 | Loss: 0.00073945
Iteration 21/25 | Loss: 0.00073945
Iteration 22/25 | Loss: 0.00073945
Iteration 23/25 | Loss: 0.00073945
Iteration 24/25 | Loss: 0.00073945
Iteration 25/25 | Loss: 0.00073945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073945
Iteration 2/1000 | Loss: 0.00015329
Iteration 3/1000 | Loss: 0.00004131
Iteration 4/1000 | Loss: 0.00002796
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002412
Iteration 7/1000 | Loss: 0.00002324
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002223
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002167
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002117
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002104
Iteration 21/1000 | Loss: 0.00002104
Iteration 22/1000 | Loss: 0.00002103
Iteration 23/1000 | Loss: 0.00002102
Iteration 24/1000 | Loss: 0.00002102
Iteration 25/1000 | Loss: 0.00002102
Iteration 26/1000 | Loss: 0.00002102
Iteration 27/1000 | Loss: 0.00002101
Iteration 28/1000 | Loss: 0.00002101
Iteration 29/1000 | Loss: 0.00002101
Iteration 30/1000 | Loss: 0.00002101
Iteration 31/1000 | Loss: 0.00002100
Iteration 32/1000 | Loss: 0.00002100
Iteration 33/1000 | Loss: 0.00002100
Iteration 34/1000 | Loss: 0.00002099
Iteration 35/1000 | Loss: 0.00002099
Iteration 36/1000 | Loss: 0.00002099
Iteration 37/1000 | Loss: 0.00002099
Iteration 38/1000 | Loss: 0.00002099
Iteration 39/1000 | Loss: 0.00002099
Iteration 40/1000 | Loss: 0.00002098
Iteration 41/1000 | Loss: 0.00002098
Iteration 42/1000 | Loss: 0.00002098
Iteration 43/1000 | Loss: 0.00002098
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002096
Iteration 47/1000 | Loss: 0.00002096
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002095
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002092
Iteration 56/1000 | Loss: 0.00002092
Iteration 57/1000 | Loss: 0.00002092
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002090
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002090
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002089
Iteration 64/1000 | Loss: 0.00002089
Iteration 65/1000 | Loss: 0.00002089
Iteration 66/1000 | Loss: 0.00002089
Iteration 67/1000 | Loss: 0.00002089
Iteration 68/1000 | Loss: 0.00002089
Iteration 69/1000 | Loss: 0.00002089
Iteration 70/1000 | Loss: 0.00002088
Iteration 71/1000 | Loss: 0.00002088
Iteration 72/1000 | Loss: 0.00002088
Iteration 73/1000 | Loss: 0.00002088
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002087
Iteration 77/1000 | Loss: 0.00002087
Iteration 78/1000 | Loss: 0.00002087
Iteration 79/1000 | Loss: 0.00002087
Iteration 80/1000 | Loss: 0.00002087
Iteration 81/1000 | Loss: 0.00002087
Iteration 82/1000 | Loss: 0.00002087
Iteration 83/1000 | Loss: 0.00002087
Iteration 84/1000 | Loss: 0.00002087
Iteration 85/1000 | Loss: 0.00002087
Iteration 86/1000 | Loss: 0.00002087
Iteration 87/1000 | Loss: 0.00002086
Iteration 88/1000 | Loss: 0.00002086
Iteration 89/1000 | Loss: 0.00002086
Iteration 90/1000 | Loss: 0.00002086
Iteration 91/1000 | Loss: 0.00002086
Iteration 92/1000 | Loss: 0.00002086
Iteration 93/1000 | Loss: 0.00002086
Iteration 94/1000 | Loss: 0.00002086
Iteration 95/1000 | Loss: 0.00002086
Iteration 96/1000 | Loss: 0.00002086
Iteration 97/1000 | Loss: 0.00002086
Iteration 98/1000 | Loss: 0.00002085
Iteration 99/1000 | Loss: 0.00002085
Iteration 100/1000 | Loss: 0.00002085
Iteration 101/1000 | Loss: 0.00002085
Iteration 102/1000 | Loss: 0.00002085
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002085
Iteration 107/1000 | Loss: 0.00002085
Iteration 108/1000 | Loss: 0.00002085
Iteration 109/1000 | Loss: 0.00002085
Iteration 110/1000 | Loss: 0.00002085
Iteration 111/1000 | Loss: 0.00002085
Iteration 112/1000 | Loss: 0.00002085
Iteration 113/1000 | Loss: 0.00002085
Iteration 114/1000 | Loss: 0.00002085
Iteration 115/1000 | Loss: 0.00002085
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002085
Iteration 118/1000 | Loss: 0.00002085
Iteration 119/1000 | Loss: 0.00002085
Iteration 120/1000 | Loss: 0.00002085
Iteration 121/1000 | Loss: 0.00002085
Iteration 122/1000 | Loss: 0.00002085
Iteration 123/1000 | Loss: 0.00002085
Iteration 124/1000 | Loss: 0.00002085
Iteration 125/1000 | Loss: 0.00002085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.0845931430812925e-05, 2.0845931430812925e-05, 2.0845931430812925e-05, 2.0845931430812925e-05, 2.0845931430812925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0845931430812925e-05

Optimization complete. Final v2v error: 3.7797365188598633 mm

Highest mean error: 4.154892444610596 mm for frame 4

Lowest mean error: 3.7105727195739746 mm for frame 113

Saving results

Total time: 56.3537163734436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560067
Iteration 2/25 | Loss: 0.00147495
Iteration 3/25 | Loss: 0.00137822
Iteration 4/25 | Loss: 0.00136606
Iteration 5/25 | Loss: 0.00136258
Iteration 6/25 | Loss: 0.00136207
Iteration 7/25 | Loss: 0.00136207
Iteration 8/25 | Loss: 0.00136207
Iteration 9/25 | Loss: 0.00136207
Iteration 10/25 | Loss: 0.00136207
Iteration 11/25 | Loss: 0.00136207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001362074981443584, 0.001362074981443584, 0.001362074981443584, 0.001362074981443584, 0.001362074981443584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001362074981443584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47308564
Iteration 2/25 | Loss: 0.00089009
Iteration 3/25 | Loss: 0.00089009
Iteration 4/25 | Loss: 0.00089009
Iteration 5/25 | Loss: 0.00089009
Iteration 6/25 | Loss: 0.00089009
Iteration 7/25 | Loss: 0.00089009
Iteration 8/25 | Loss: 0.00089009
Iteration 9/25 | Loss: 0.00089009
Iteration 10/25 | Loss: 0.00089008
Iteration 11/25 | Loss: 0.00089008
Iteration 12/25 | Loss: 0.00089008
Iteration 13/25 | Loss: 0.00089008
Iteration 14/25 | Loss: 0.00089008
Iteration 15/25 | Loss: 0.00089008
Iteration 16/25 | Loss: 0.00089008
Iteration 17/25 | Loss: 0.00089008
Iteration 18/25 | Loss: 0.00089008
Iteration 19/25 | Loss: 0.00089008
Iteration 20/25 | Loss: 0.00089008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008900845423340797, 0.0008900845423340797, 0.0008900845423340797, 0.0008900845423340797, 0.0008900845423340797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008900845423340797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089008
Iteration 2/1000 | Loss: 0.00005178
Iteration 3/1000 | Loss: 0.00003233
Iteration 4/1000 | Loss: 0.00002943
Iteration 5/1000 | Loss: 0.00002820
Iteration 6/1000 | Loss: 0.00002723
Iteration 7/1000 | Loss: 0.00002667
Iteration 8/1000 | Loss: 0.00002622
Iteration 9/1000 | Loss: 0.00002608
Iteration 10/1000 | Loss: 0.00002583
Iteration 11/1000 | Loss: 0.00002576
Iteration 12/1000 | Loss: 0.00002561
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002548
Iteration 15/1000 | Loss: 0.00002548
Iteration 16/1000 | Loss: 0.00002547
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00002531
Iteration 19/1000 | Loss: 0.00002522
Iteration 20/1000 | Loss: 0.00002515
Iteration 21/1000 | Loss: 0.00002514
Iteration 22/1000 | Loss: 0.00002514
Iteration 23/1000 | Loss: 0.00002512
Iteration 24/1000 | Loss: 0.00002508
Iteration 25/1000 | Loss: 0.00002507
Iteration 26/1000 | Loss: 0.00002507
Iteration 27/1000 | Loss: 0.00002506
Iteration 28/1000 | Loss: 0.00002506
Iteration 29/1000 | Loss: 0.00002506
Iteration 30/1000 | Loss: 0.00002504
Iteration 31/1000 | Loss: 0.00002504
Iteration 32/1000 | Loss: 0.00002504
Iteration 33/1000 | Loss: 0.00002504
Iteration 34/1000 | Loss: 0.00002503
Iteration 35/1000 | Loss: 0.00002503
Iteration 36/1000 | Loss: 0.00002503
Iteration 37/1000 | Loss: 0.00002503
Iteration 38/1000 | Loss: 0.00002503
Iteration 39/1000 | Loss: 0.00002503
Iteration 40/1000 | Loss: 0.00002503
Iteration 41/1000 | Loss: 0.00002503
Iteration 42/1000 | Loss: 0.00002503
Iteration 43/1000 | Loss: 0.00002503
Iteration 44/1000 | Loss: 0.00002503
Iteration 45/1000 | Loss: 0.00002503
Iteration 46/1000 | Loss: 0.00002503
Iteration 47/1000 | Loss: 0.00002502
Iteration 48/1000 | Loss: 0.00002502
Iteration 49/1000 | Loss: 0.00002502
Iteration 50/1000 | Loss: 0.00002502
Iteration 51/1000 | Loss: 0.00002501
Iteration 52/1000 | Loss: 0.00002501
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002499
Iteration 60/1000 | Loss: 0.00002499
Iteration 61/1000 | Loss: 0.00002499
Iteration 62/1000 | Loss: 0.00002499
Iteration 63/1000 | Loss: 0.00002499
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002498
Iteration 66/1000 | Loss: 0.00002498
Iteration 67/1000 | Loss: 0.00002498
Iteration 68/1000 | Loss: 0.00002498
Iteration 69/1000 | Loss: 0.00002498
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002498
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002497
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002497
Iteration 80/1000 | Loss: 0.00002497
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002496
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002495
Iteration 88/1000 | Loss: 0.00002495
Iteration 89/1000 | Loss: 0.00002495
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002495
Iteration 94/1000 | Loss: 0.00002495
Iteration 95/1000 | Loss: 0.00002495
Iteration 96/1000 | Loss: 0.00002495
Iteration 97/1000 | Loss: 0.00002495
Iteration 98/1000 | Loss: 0.00002495
Iteration 99/1000 | Loss: 0.00002495
Iteration 100/1000 | Loss: 0.00002495
Iteration 101/1000 | Loss: 0.00002495
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002495
Iteration 109/1000 | Loss: 0.00002495
Iteration 110/1000 | Loss: 0.00002495
Iteration 111/1000 | Loss: 0.00002495
Iteration 112/1000 | Loss: 0.00002495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.4951812520157546e-05, 2.4951812520157546e-05, 2.4951812520157546e-05, 2.4951812520157546e-05, 2.4951812520157546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4951812520157546e-05

Optimization complete. Final v2v error: 4.085884094238281 mm

Highest mean error: 4.316619396209717 mm for frame 233

Lowest mean error: 3.7903926372528076 mm for frame 16

Saving results

Total time: 38.52110552787781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00717140
Iteration 2/25 | Loss: 0.00185315
Iteration 3/25 | Loss: 0.00138818
Iteration 4/25 | Loss: 0.00132376
Iteration 5/25 | Loss: 0.00131821
Iteration 6/25 | Loss: 0.00125740
Iteration 7/25 | Loss: 0.00125286
Iteration 8/25 | Loss: 0.00124849
Iteration 9/25 | Loss: 0.00124103
Iteration 10/25 | Loss: 0.00123416
Iteration 11/25 | Loss: 0.00123295
Iteration 12/25 | Loss: 0.00123283
Iteration 13/25 | Loss: 0.00123272
Iteration 14/25 | Loss: 0.00123265
Iteration 15/25 | Loss: 0.00123212
Iteration 16/25 | Loss: 0.00123088
Iteration 17/25 | Loss: 0.00123062
Iteration 18/25 | Loss: 0.00123060
Iteration 19/25 | Loss: 0.00123060
Iteration 20/25 | Loss: 0.00123059
Iteration 21/25 | Loss: 0.00123059
Iteration 22/25 | Loss: 0.00123059
Iteration 23/25 | Loss: 0.00123059
Iteration 24/25 | Loss: 0.00123059
Iteration 25/25 | Loss: 0.00123059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40545607
Iteration 2/25 | Loss: 0.00055095
Iteration 3/25 | Loss: 0.00055092
Iteration 4/25 | Loss: 0.00055092
Iteration 5/25 | Loss: 0.00055092
Iteration 6/25 | Loss: 0.00055092
Iteration 7/25 | Loss: 0.00055092
Iteration 8/25 | Loss: 0.00055092
Iteration 9/25 | Loss: 0.00055092
Iteration 10/25 | Loss: 0.00055092
Iteration 11/25 | Loss: 0.00055092
Iteration 12/25 | Loss: 0.00055092
Iteration 13/25 | Loss: 0.00055092
Iteration 14/25 | Loss: 0.00055092
Iteration 15/25 | Loss: 0.00055092
Iteration 16/25 | Loss: 0.00055092
Iteration 17/25 | Loss: 0.00055092
Iteration 18/25 | Loss: 0.00055092
Iteration 19/25 | Loss: 0.00055092
Iteration 20/25 | Loss: 0.00055092
Iteration 21/25 | Loss: 0.00055092
Iteration 22/25 | Loss: 0.00055092
Iteration 23/25 | Loss: 0.00055092
Iteration 24/25 | Loss: 0.00055092
Iteration 25/25 | Loss: 0.00055092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055092
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002664
Iteration 4/1000 | Loss: 0.00002280
Iteration 5/1000 | Loss: 0.00002167
Iteration 6/1000 | Loss: 0.00002073
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00001773
Iteration 15/1000 | Loss: 0.00001765
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001759
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001755
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001754
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001749
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001748
Iteration 38/1000 | Loss: 0.00001748
Iteration 39/1000 | Loss: 0.00001748
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001746
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001745
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001744
Iteration 48/1000 | Loss: 0.00001744
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001744
Iteration 55/1000 | Loss: 0.00001744
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001743
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001743
Iteration 61/1000 | Loss: 0.00001743
Iteration 62/1000 | Loss: 0.00001743
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001742
Iteration 65/1000 | Loss: 0.00001742
Iteration 66/1000 | Loss: 0.00001742
Iteration 67/1000 | Loss: 0.00001742
Iteration 68/1000 | Loss: 0.00001742
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001742
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001741
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001740
Iteration 83/1000 | Loss: 0.00001740
Iteration 84/1000 | Loss: 0.00001739
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001738
Iteration 92/1000 | Loss: 0.00001738
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001738
Iteration 95/1000 | Loss: 0.00001738
Iteration 96/1000 | Loss: 0.00001738
Iteration 97/1000 | Loss: 0.00001737
Iteration 98/1000 | Loss: 0.00001737
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001736
Iteration 101/1000 | Loss: 0.00001736
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001734
Iteration 109/1000 | Loss: 0.00001734
Iteration 110/1000 | Loss: 0.00001734
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001733
Iteration 113/1000 | Loss: 0.00001733
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001733
Iteration 118/1000 | Loss: 0.00001733
Iteration 119/1000 | Loss: 0.00001733
Iteration 120/1000 | Loss: 0.00001733
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001732
Iteration 131/1000 | Loss: 0.00001732
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001731
Iteration 138/1000 | Loss: 0.00001730
Iteration 139/1000 | Loss: 0.00001730
Iteration 140/1000 | Loss: 0.00001730
Iteration 141/1000 | Loss: 0.00001730
Iteration 142/1000 | Loss: 0.00001730
Iteration 143/1000 | Loss: 0.00001730
Iteration 144/1000 | Loss: 0.00001730
Iteration 145/1000 | Loss: 0.00001730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.730260919430293e-05, 1.730260919430293e-05, 1.730260919430293e-05, 1.730260919430293e-05, 1.730260919430293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.730260919430293e-05

Optimization complete. Final v2v error: 3.5186386108398438 mm

Highest mean error: 4.289244651794434 mm for frame 235

Lowest mean error: 3.3068387508392334 mm for frame 173

Saving results

Total time: 68.93004870414734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041899
Iteration 2/25 | Loss: 0.00173400
Iteration 3/25 | Loss: 0.00129098
Iteration 4/25 | Loss: 0.00125359
Iteration 5/25 | Loss: 0.00121800
Iteration 6/25 | Loss: 0.00120020
Iteration 7/25 | Loss: 0.00119009
Iteration 8/25 | Loss: 0.00119248
Iteration 9/25 | Loss: 0.00118404
Iteration 10/25 | Loss: 0.00118106
Iteration 11/25 | Loss: 0.00117581
Iteration 12/25 | Loss: 0.00117468
Iteration 13/25 | Loss: 0.00117406
Iteration 14/25 | Loss: 0.00117488
Iteration 15/25 | Loss: 0.00117338
Iteration 16/25 | Loss: 0.00117338
Iteration 17/25 | Loss: 0.00117338
Iteration 18/25 | Loss: 0.00117338
Iteration 19/25 | Loss: 0.00117338
Iteration 20/25 | Loss: 0.00117337
Iteration 21/25 | Loss: 0.00117337
Iteration 22/25 | Loss: 0.00117337
Iteration 23/25 | Loss: 0.00117337
Iteration 24/25 | Loss: 0.00117337
Iteration 25/25 | Loss: 0.00117337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50408733
Iteration 2/25 | Loss: 0.00085860
Iteration 3/25 | Loss: 0.00081844
Iteration 4/25 | Loss: 0.00081844
Iteration 5/25 | Loss: 0.00081844
Iteration 6/25 | Loss: 0.00081844
Iteration 7/25 | Loss: 0.00081844
Iteration 8/25 | Loss: 0.00081844
Iteration 9/25 | Loss: 0.00081844
Iteration 10/25 | Loss: 0.00081844
Iteration 11/25 | Loss: 0.00081844
Iteration 12/25 | Loss: 0.00081844
Iteration 13/25 | Loss: 0.00081844
Iteration 14/25 | Loss: 0.00081844
Iteration 15/25 | Loss: 0.00081844
Iteration 16/25 | Loss: 0.00081844
Iteration 17/25 | Loss: 0.00081844
Iteration 18/25 | Loss: 0.00081844
Iteration 19/25 | Loss: 0.00081844
Iteration 20/25 | Loss: 0.00081844
Iteration 21/25 | Loss: 0.00081844
Iteration 22/25 | Loss: 0.00081844
Iteration 23/25 | Loss: 0.00081844
Iteration 24/25 | Loss: 0.00081844
Iteration 25/25 | Loss: 0.00081844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081844
Iteration 2/1000 | Loss: 0.00011635
Iteration 3/1000 | Loss: 0.00007594
Iteration 4/1000 | Loss: 0.00004990
Iteration 5/1000 | Loss: 0.00076524
Iteration 6/1000 | Loss: 0.00003387
Iteration 7/1000 | Loss: 0.00006229
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00015566
Iteration 10/1000 | Loss: 0.00001490
Iteration 11/1000 | Loss: 0.00004757
Iteration 12/1000 | Loss: 0.00020572
Iteration 13/1000 | Loss: 0.00001928
Iteration 14/1000 | Loss: 0.00001862
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001123
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001123
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001121
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001117
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00003701
Iteration 28/1000 | Loss: 0.00009530
Iteration 29/1000 | Loss: 0.00001422
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001092
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001092
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001092
Iteration 51/1000 | Loss: 0.00001092
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001092
Iteration 56/1000 | Loss: 0.00001092
Iteration 57/1000 | Loss: 0.00001092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.0923371519311331e-05, 1.0923371519311331e-05, 1.0923371519311331e-05, 1.0923371519311331e-05, 1.0923371519311331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0923371519311331e-05

Optimization complete. Final v2v error: 2.867816686630249 mm

Highest mean error: 3.0428576469421387 mm for frame 123

Lowest mean error: 2.7459309101104736 mm for frame 48

Saving results

Total time: 56.099430084228516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006040
Iteration 2/25 | Loss: 0.00272329
Iteration 3/25 | Loss: 0.00216775
Iteration 4/25 | Loss: 0.00195455
Iteration 5/25 | Loss: 0.00173673
Iteration 6/25 | Loss: 0.00213231
Iteration 7/25 | Loss: 0.00141753
Iteration 8/25 | Loss: 0.00148835
Iteration 9/25 | Loss: 0.00136934
Iteration 10/25 | Loss: 0.00136236
Iteration 11/25 | Loss: 0.00132857
Iteration 12/25 | Loss: 0.00131147
Iteration 13/25 | Loss: 0.00130019
Iteration 14/25 | Loss: 0.00130787
Iteration 15/25 | Loss: 0.00128993
Iteration 16/25 | Loss: 0.00128955
Iteration 17/25 | Loss: 0.00129703
Iteration 18/25 | Loss: 0.00128259
Iteration 19/25 | Loss: 0.00129390
Iteration 20/25 | Loss: 0.00128276
Iteration 21/25 | Loss: 0.00128249
Iteration 22/25 | Loss: 0.00128249
Iteration 23/25 | Loss: 0.00128249
Iteration 24/25 | Loss: 0.00128249
Iteration 25/25 | Loss: 0.00128249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40277648
Iteration 2/25 | Loss: 0.00075540
Iteration 3/25 | Loss: 0.00075540
Iteration 4/25 | Loss: 0.00075540
Iteration 5/25 | Loss: 0.00075540
Iteration 6/25 | Loss: 0.00075539
Iteration 7/25 | Loss: 0.00075539
Iteration 8/25 | Loss: 0.00075539
Iteration 9/25 | Loss: 0.00075539
Iteration 10/25 | Loss: 0.00075539
Iteration 11/25 | Loss: 0.00075539
Iteration 12/25 | Loss: 0.00075539
Iteration 13/25 | Loss: 0.00075539
Iteration 14/25 | Loss: 0.00075539
Iteration 15/25 | Loss: 0.00075539
Iteration 16/25 | Loss: 0.00075539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007553935865871608, 0.0007553935865871608, 0.0007553935865871608, 0.0007553935865871608, 0.0007553935865871608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007553935865871608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075539
Iteration 2/1000 | Loss: 0.00003814
Iteration 3/1000 | Loss: 0.00002511
Iteration 4/1000 | Loss: 0.00002224
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002080
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00002016
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001990
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001965
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001953
Iteration 17/1000 | Loss: 0.00001953
Iteration 18/1000 | Loss: 0.00001952
Iteration 19/1000 | Loss: 0.00001950
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001939
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001936
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001934
Iteration 59/1000 | Loss: 0.00001934
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001934
Iteration 66/1000 | Loss: 0.00001934
Iteration 67/1000 | Loss: 0.00001933
Iteration 68/1000 | Loss: 0.00001933
Iteration 69/1000 | Loss: 0.00001933
Iteration 70/1000 | Loss: 0.00001933
Iteration 71/1000 | Loss: 0.00001933
Iteration 72/1000 | Loss: 0.00001933
Iteration 73/1000 | Loss: 0.00001933
Iteration 74/1000 | Loss: 0.00001933
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001931
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001930
Iteration 94/1000 | Loss: 0.00001930
Iteration 95/1000 | Loss: 0.00001930
Iteration 96/1000 | Loss: 0.00001930
Iteration 97/1000 | Loss: 0.00001930
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001928
Iteration 107/1000 | Loss: 0.00001927
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001927
Iteration 111/1000 | Loss: 0.00001927
Iteration 112/1000 | Loss: 0.00001927
Iteration 113/1000 | Loss: 0.00001927
Iteration 114/1000 | Loss: 0.00001927
Iteration 115/1000 | Loss: 0.00001927
Iteration 116/1000 | Loss: 0.00001927
Iteration 117/1000 | Loss: 0.00001927
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.9271046767244115e-05, 1.9271046767244115e-05, 1.9271046767244115e-05, 1.9271046767244115e-05, 1.9271046767244115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9271046767244115e-05

Optimization complete. Final v2v error: 3.6541383266448975 mm

Highest mean error: 3.6978933811187744 mm for frame 98

Lowest mean error: 3.4795641899108887 mm for frame 0

Saving results

Total time: 56.3800528049469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381913
Iteration 2/25 | Loss: 0.00140939
Iteration 3/25 | Loss: 0.00126707
Iteration 4/25 | Loss: 0.00123636
Iteration 5/25 | Loss: 0.00122835
Iteration 6/25 | Loss: 0.00122573
Iteration 7/25 | Loss: 0.00122553
Iteration 8/25 | Loss: 0.00122553
Iteration 9/25 | Loss: 0.00122553
Iteration 10/25 | Loss: 0.00122553
Iteration 11/25 | Loss: 0.00122553
Iteration 12/25 | Loss: 0.00122553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012255319161340594, 0.0012255319161340594, 0.0012255319161340594, 0.0012255319161340594, 0.0012255319161340594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012255319161340594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35696936
Iteration 2/25 | Loss: 0.00072183
Iteration 3/25 | Loss: 0.00072182
Iteration 4/25 | Loss: 0.00072182
Iteration 5/25 | Loss: 0.00072182
Iteration 6/25 | Loss: 0.00072182
Iteration 7/25 | Loss: 0.00072182
Iteration 8/25 | Loss: 0.00072182
Iteration 9/25 | Loss: 0.00072182
Iteration 10/25 | Loss: 0.00072182
Iteration 11/25 | Loss: 0.00072182
Iteration 12/25 | Loss: 0.00072182
Iteration 13/25 | Loss: 0.00072182
Iteration 14/25 | Loss: 0.00072182
Iteration 15/25 | Loss: 0.00072182
Iteration 16/25 | Loss: 0.00072182
Iteration 17/25 | Loss: 0.00072182
Iteration 18/25 | Loss: 0.00072182
Iteration 19/25 | Loss: 0.00072182
Iteration 20/25 | Loss: 0.00072182
Iteration 21/25 | Loss: 0.00072182
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007218215614557266, 0.0007218215614557266, 0.0007218215614557266, 0.0007218215614557266, 0.0007218215614557266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007218215614557266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072182
Iteration 2/1000 | Loss: 0.00004411
Iteration 3/1000 | Loss: 0.00002917
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002381
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002172
Iteration 8/1000 | Loss: 0.00002116
Iteration 9/1000 | Loss: 0.00002076
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00001994
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001970
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001969
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001966
Iteration 21/1000 | Loss: 0.00001965
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001963
Iteration 25/1000 | Loss: 0.00001962
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001958
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001953
Iteration 32/1000 | Loss: 0.00001952
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001944
Iteration 42/1000 | Loss: 0.00001944
Iteration 43/1000 | Loss: 0.00001943
Iteration 44/1000 | Loss: 0.00001943
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001941
Iteration 49/1000 | Loss: 0.00001941
Iteration 50/1000 | Loss: 0.00001941
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001940
Iteration 53/1000 | Loss: 0.00001940
Iteration 54/1000 | Loss: 0.00001940
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001940
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001940
Iteration 61/1000 | Loss: 0.00001940
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001939
Iteration 66/1000 | Loss: 0.00001939
Iteration 67/1000 | Loss: 0.00001939
Iteration 68/1000 | Loss: 0.00001939
Iteration 69/1000 | Loss: 0.00001938
Iteration 70/1000 | Loss: 0.00001938
Iteration 71/1000 | Loss: 0.00001938
Iteration 72/1000 | Loss: 0.00001938
Iteration 73/1000 | Loss: 0.00001938
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.93675314221764e-05, 1.93675314221764e-05, 1.93675314221764e-05, 1.93675314221764e-05, 1.93675314221764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.93675314221764e-05

Optimization complete. Final v2v error: 3.7068028450012207 mm

Highest mean error: 4.259478569030762 mm for frame 179

Lowest mean error: 2.9254891872406006 mm for frame 81

Saving results

Total time: 41.022350549697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038911
Iteration 2/25 | Loss: 0.00208230
Iteration 3/25 | Loss: 0.00175689
Iteration 4/25 | Loss: 0.00152043
Iteration 5/25 | Loss: 0.00152113
Iteration 6/25 | Loss: 0.00147138
Iteration 7/25 | Loss: 0.00136872
Iteration 8/25 | Loss: 0.00140498
Iteration 9/25 | Loss: 0.00140947
Iteration 10/25 | Loss: 0.00137638
Iteration 11/25 | Loss: 0.00132578
Iteration 12/25 | Loss: 0.00135454
Iteration 13/25 | Loss: 0.00129394
Iteration 14/25 | Loss: 0.00130338
Iteration 15/25 | Loss: 0.00126918
Iteration 16/25 | Loss: 0.00126189
Iteration 17/25 | Loss: 0.00125962
Iteration 18/25 | Loss: 0.00126374
Iteration 19/25 | Loss: 0.00125897
Iteration 20/25 | Loss: 0.00126340
Iteration 21/25 | Loss: 0.00125821
Iteration 22/25 | Loss: 0.00125764
Iteration 23/25 | Loss: 0.00125989
Iteration 24/25 | Loss: 0.00125775
Iteration 25/25 | Loss: 0.00125609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46188688
Iteration 2/25 | Loss: 0.00100472
Iteration 3/25 | Loss: 0.00100472
Iteration 4/25 | Loss: 0.00100472
Iteration 5/25 | Loss: 0.00100472
Iteration 6/25 | Loss: 0.00100472
Iteration 7/25 | Loss: 0.00100472
Iteration 8/25 | Loss: 0.00100472
Iteration 9/25 | Loss: 0.00100472
Iteration 10/25 | Loss: 0.00100472
Iteration 11/25 | Loss: 0.00100472
Iteration 12/25 | Loss: 0.00100472
Iteration 13/25 | Loss: 0.00100472
Iteration 14/25 | Loss: 0.00100472
Iteration 15/25 | Loss: 0.00100472
Iteration 16/25 | Loss: 0.00100472
Iteration 17/25 | Loss: 0.00100472
Iteration 18/25 | Loss: 0.00100472
Iteration 19/25 | Loss: 0.00100472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010047187097370625, 0.0010047187097370625, 0.0010047187097370625, 0.0010047187097370625, 0.0010047187097370625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010047187097370625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100472
Iteration 2/1000 | Loss: 0.00016759
Iteration 3/1000 | Loss: 0.00003441
Iteration 4/1000 | Loss: 0.00002609
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00003338
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00005475
Iteration 9/1000 | Loss: 0.00001803
Iteration 10/1000 | Loss: 0.00002936
Iteration 11/1000 | Loss: 0.00001735
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001613
Iteration 18/1000 | Loss: 0.00001608
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001602
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001602
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001601
Iteration 29/1000 | Loss: 0.00001601
Iteration 30/1000 | Loss: 0.00001601
Iteration 31/1000 | Loss: 0.00001601
Iteration 32/1000 | Loss: 0.00001601
Iteration 33/1000 | Loss: 0.00001601
Iteration 34/1000 | Loss: 0.00001599
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001596
Iteration 40/1000 | Loss: 0.00001596
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00003109
Iteration 46/1000 | Loss: 0.00003109
Iteration 47/1000 | Loss: 0.00013248
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001583
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00003114
Iteration 57/1000 | Loss: 0.00001580
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00002006
Iteration 60/1000 | Loss: 0.00001578
Iteration 61/1000 | Loss: 0.00001577
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001576
Iteration 68/1000 | Loss: 0.00001576
Iteration 69/1000 | Loss: 0.00001576
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001575
Iteration 72/1000 | Loss: 0.00001575
Iteration 73/1000 | Loss: 0.00001575
Iteration 74/1000 | Loss: 0.00001575
Iteration 75/1000 | Loss: 0.00001575
Iteration 76/1000 | Loss: 0.00001575
Iteration 77/1000 | Loss: 0.00001575
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001575
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001574
Iteration 86/1000 | Loss: 0.00001574
Iteration 87/1000 | Loss: 0.00001574
Iteration 88/1000 | Loss: 0.00001574
Iteration 89/1000 | Loss: 0.00001574
Iteration 90/1000 | Loss: 0.00001574
Iteration 91/1000 | Loss: 0.00001574
Iteration 92/1000 | Loss: 0.00001574
Iteration 93/1000 | Loss: 0.00001574
Iteration 94/1000 | Loss: 0.00001573
Iteration 95/1000 | Loss: 0.00001573
Iteration 96/1000 | Loss: 0.00001573
Iteration 97/1000 | Loss: 0.00001573
Iteration 98/1000 | Loss: 0.00001573
Iteration 99/1000 | Loss: 0.00001573
Iteration 100/1000 | Loss: 0.00001573
Iteration 101/1000 | Loss: 0.00001573
Iteration 102/1000 | Loss: 0.00001573
Iteration 103/1000 | Loss: 0.00001573
Iteration 104/1000 | Loss: 0.00001573
Iteration 105/1000 | Loss: 0.00001573
Iteration 106/1000 | Loss: 0.00001573
Iteration 107/1000 | Loss: 0.00001572
Iteration 108/1000 | Loss: 0.00001572
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00001572
Iteration 111/1000 | Loss: 0.00001571
Iteration 112/1000 | Loss: 0.00001571
Iteration 113/1000 | Loss: 0.00001571
Iteration 114/1000 | Loss: 0.00001571
Iteration 115/1000 | Loss: 0.00001571
Iteration 116/1000 | Loss: 0.00001570
Iteration 117/1000 | Loss: 0.00001570
Iteration 118/1000 | Loss: 0.00001569
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001567
Iteration 123/1000 | Loss: 0.00001567
Iteration 124/1000 | Loss: 0.00001567
Iteration 125/1000 | Loss: 0.00001567
Iteration 126/1000 | Loss: 0.00001567
Iteration 127/1000 | Loss: 0.00001567
Iteration 128/1000 | Loss: 0.00001567
Iteration 129/1000 | Loss: 0.00001567
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001566
Iteration 137/1000 | Loss: 0.00002465
Iteration 138/1000 | Loss: 0.00001568
Iteration 139/1000 | Loss: 0.00001564
Iteration 140/1000 | Loss: 0.00001564
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001563
Iteration 145/1000 | Loss: 0.00001563
Iteration 146/1000 | Loss: 0.00001563
Iteration 147/1000 | Loss: 0.00001563
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001562
Iteration 151/1000 | Loss: 0.00001562
Iteration 152/1000 | Loss: 0.00001562
Iteration 153/1000 | Loss: 0.00001562
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001561
Iteration 158/1000 | Loss: 0.00001561
Iteration 159/1000 | Loss: 0.00001561
Iteration 160/1000 | Loss: 0.00001561
Iteration 161/1000 | Loss: 0.00001561
Iteration 162/1000 | Loss: 0.00001561
Iteration 163/1000 | Loss: 0.00001561
Iteration 164/1000 | Loss: 0.00001561
Iteration 165/1000 | Loss: 0.00001561
Iteration 166/1000 | Loss: 0.00001561
Iteration 167/1000 | Loss: 0.00001561
Iteration 168/1000 | Loss: 0.00001561
Iteration 169/1000 | Loss: 0.00001561
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001560
Iteration 174/1000 | Loss: 0.00001560
Iteration 175/1000 | Loss: 0.00001560
Iteration 176/1000 | Loss: 0.00001560
Iteration 177/1000 | Loss: 0.00001560
Iteration 178/1000 | Loss: 0.00001560
Iteration 179/1000 | Loss: 0.00001560
Iteration 180/1000 | Loss: 0.00001560
Iteration 181/1000 | Loss: 0.00001560
Iteration 182/1000 | Loss: 0.00001560
Iteration 183/1000 | Loss: 0.00001560
Iteration 184/1000 | Loss: 0.00001560
Iteration 185/1000 | Loss: 0.00001560
Iteration 186/1000 | Loss: 0.00001560
Iteration 187/1000 | Loss: 0.00001559
Iteration 188/1000 | Loss: 0.00001559
Iteration 189/1000 | Loss: 0.00001559
Iteration 190/1000 | Loss: 0.00001559
Iteration 191/1000 | Loss: 0.00001559
Iteration 192/1000 | Loss: 0.00001559
Iteration 193/1000 | Loss: 0.00001559
Iteration 194/1000 | Loss: 0.00001559
Iteration 195/1000 | Loss: 0.00001559
Iteration 196/1000 | Loss: 0.00001559
Iteration 197/1000 | Loss: 0.00001559
Iteration 198/1000 | Loss: 0.00001559
Iteration 199/1000 | Loss: 0.00001558
Iteration 200/1000 | Loss: 0.00001558
Iteration 201/1000 | Loss: 0.00001558
Iteration 202/1000 | Loss: 0.00001558
Iteration 203/1000 | Loss: 0.00001558
Iteration 204/1000 | Loss: 0.00001558
Iteration 205/1000 | Loss: 0.00001558
Iteration 206/1000 | Loss: 0.00001558
Iteration 207/1000 | Loss: 0.00001558
Iteration 208/1000 | Loss: 0.00001558
Iteration 209/1000 | Loss: 0.00001558
Iteration 210/1000 | Loss: 0.00001558
Iteration 211/1000 | Loss: 0.00001558
Iteration 212/1000 | Loss: 0.00001558
Iteration 213/1000 | Loss: 0.00001557
Iteration 214/1000 | Loss: 0.00001557
Iteration 215/1000 | Loss: 0.00001557
Iteration 216/1000 | Loss: 0.00001557
Iteration 217/1000 | Loss: 0.00001557
Iteration 218/1000 | Loss: 0.00001557
Iteration 219/1000 | Loss: 0.00001557
Iteration 220/1000 | Loss: 0.00001557
Iteration 221/1000 | Loss: 0.00001557
Iteration 222/1000 | Loss: 0.00001557
Iteration 223/1000 | Loss: 0.00001557
Iteration 224/1000 | Loss: 0.00001556
Iteration 225/1000 | Loss: 0.00001556
Iteration 226/1000 | Loss: 0.00001556
Iteration 227/1000 | Loss: 0.00001556
Iteration 228/1000 | Loss: 0.00001556
Iteration 229/1000 | Loss: 0.00001556
Iteration 230/1000 | Loss: 0.00001556
Iteration 231/1000 | Loss: 0.00001556
Iteration 232/1000 | Loss: 0.00001556
Iteration 233/1000 | Loss: 0.00001556
Iteration 234/1000 | Loss: 0.00001556
Iteration 235/1000 | Loss: 0.00001556
Iteration 236/1000 | Loss: 0.00001556
Iteration 237/1000 | Loss: 0.00001556
Iteration 238/1000 | Loss: 0.00001556
Iteration 239/1000 | Loss: 0.00001556
Iteration 240/1000 | Loss: 0.00001556
Iteration 241/1000 | Loss: 0.00001556
Iteration 242/1000 | Loss: 0.00001556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.556175084260758e-05, 1.556175084260758e-05, 1.556175084260758e-05, 1.556175084260758e-05, 1.556175084260758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556175084260758e-05

Optimization complete. Final v2v error: 3.2852165699005127 mm

Highest mean error: 4.84969425201416 mm for frame 69

Lowest mean error: 2.871957302093506 mm for frame 139

Saving results

Total time: 91.16024923324585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586173
Iteration 2/25 | Loss: 0.00165540
Iteration 3/25 | Loss: 0.00131289
Iteration 4/25 | Loss: 0.00128164
Iteration 5/25 | Loss: 0.00127597
Iteration 6/25 | Loss: 0.00127434
Iteration 7/25 | Loss: 0.00127424
Iteration 8/25 | Loss: 0.00127424
Iteration 9/25 | Loss: 0.00127424
Iteration 10/25 | Loss: 0.00127424
Iteration 11/25 | Loss: 0.00127424
Iteration 12/25 | Loss: 0.00127424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012742436956614256, 0.0012742436956614256, 0.0012742436956614256, 0.0012742436956614256, 0.0012742436956614256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012742436956614256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28329575
Iteration 2/25 | Loss: 0.00065652
Iteration 3/25 | Loss: 0.00065652
Iteration 4/25 | Loss: 0.00065652
Iteration 5/25 | Loss: 0.00065652
Iteration 6/25 | Loss: 0.00065652
Iteration 7/25 | Loss: 0.00065652
Iteration 8/25 | Loss: 0.00065652
Iteration 9/25 | Loss: 0.00065652
Iteration 10/25 | Loss: 0.00065652
Iteration 11/25 | Loss: 0.00065652
Iteration 12/25 | Loss: 0.00065652
Iteration 13/25 | Loss: 0.00065652
Iteration 14/25 | Loss: 0.00065652
Iteration 15/25 | Loss: 0.00065652
Iteration 16/25 | Loss: 0.00065652
Iteration 17/25 | Loss: 0.00065652
Iteration 18/25 | Loss: 0.00065652
Iteration 19/25 | Loss: 0.00065652
Iteration 20/25 | Loss: 0.00065652
Iteration 21/25 | Loss: 0.00065652
Iteration 22/25 | Loss: 0.00065652
Iteration 23/25 | Loss: 0.00065652
Iteration 24/25 | Loss: 0.00065652
Iteration 25/25 | Loss: 0.00065652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065652
Iteration 2/1000 | Loss: 0.00004159
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00002064
Iteration 5/1000 | Loss: 0.00001936
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001791
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001708
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001634
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001625
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001619
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001609
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001609
Iteration 30/1000 | Loss: 0.00001609
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001608
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001602
Iteration 60/1000 | Loss: 0.00001602
Iteration 61/1000 | Loss: 0.00001602
Iteration 62/1000 | Loss: 0.00001602
Iteration 63/1000 | Loss: 0.00001601
Iteration 64/1000 | Loss: 0.00001601
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001601
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001600
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001598
Iteration 72/1000 | Loss: 0.00001598
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001597
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001594
Iteration 80/1000 | Loss: 0.00001593
Iteration 81/1000 | Loss: 0.00001593
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001593
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001593
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001593
Iteration 88/1000 | Loss: 0.00001593
Iteration 89/1000 | Loss: 0.00001593
Iteration 90/1000 | Loss: 0.00001593
Iteration 91/1000 | Loss: 0.00001593
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001592
Iteration 95/1000 | Loss: 0.00001592
Iteration 96/1000 | Loss: 0.00001592
Iteration 97/1000 | Loss: 0.00001592
Iteration 98/1000 | Loss: 0.00001592
Iteration 99/1000 | Loss: 0.00001592
Iteration 100/1000 | Loss: 0.00001592
Iteration 101/1000 | Loss: 0.00001592
Iteration 102/1000 | Loss: 0.00001592
Iteration 103/1000 | Loss: 0.00001592
Iteration 104/1000 | Loss: 0.00001592
Iteration 105/1000 | Loss: 0.00001592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5919980796752498e-05, 1.5919980796752498e-05, 1.5919980796752498e-05, 1.5919980796752498e-05, 1.5919980796752498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5919980796752498e-05

Optimization complete. Final v2v error: 3.326748847961426 mm

Highest mean error: 3.7124130725860596 mm for frame 79

Lowest mean error: 3.203462600708008 mm for frame 174

Saving results

Total time: 34.96710419654846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476164
Iteration 2/25 | Loss: 0.00131963
Iteration 3/25 | Loss: 0.00124486
Iteration 4/25 | Loss: 0.00123578
Iteration 5/25 | Loss: 0.00123287
Iteration 6/25 | Loss: 0.00123249
Iteration 7/25 | Loss: 0.00123249
Iteration 8/25 | Loss: 0.00123249
Iteration 9/25 | Loss: 0.00123249
Iteration 10/25 | Loss: 0.00123249
Iteration 11/25 | Loss: 0.00123249
Iteration 12/25 | Loss: 0.00123249
Iteration 13/25 | Loss: 0.00123249
Iteration 14/25 | Loss: 0.00123249
Iteration 15/25 | Loss: 0.00123249
Iteration 16/25 | Loss: 0.00123249
Iteration 17/25 | Loss: 0.00123249
Iteration 18/25 | Loss: 0.00123249
Iteration 19/25 | Loss: 0.00123249
Iteration 20/25 | Loss: 0.00123249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012324871495366096, 0.0012324871495366096, 0.0012324871495366096, 0.0012324871495366096, 0.0012324871495366096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012324871495366096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59011865
Iteration 2/25 | Loss: 0.00078310
Iteration 3/25 | Loss: 0.00078310
Iteration 4/25 | Loss: 0.00078310
Iteration 5/25 | Loss: 0.00078310
Iteration 6/25 | Loss: 0.00078310
Iteration 7/25 | Loss: 0.00078310
Iteration 8/25 | Loss: 0.00078310
Iteration 9/25 | Loss: 0.00078310
Iteration 10/25 | Loss: 0.00078310
Iteration 11/25 | Loss: 0.00078310
Iteration 12/25 | Loss: 0.00078310
Iteration 13/25 | Loss: 0.00078309
Iteration 14/25 | Loss: 0.00078310
Iteration 15/25 | Loss: 0.00078309
Iteration 16/25 | Loss: 0.00078309
Iteration 17/25 | Loss: 0.00078309
Iteration 18/25 | Loss: 0.00078309
Iteration 19/25 | Loss: 0.00078309
Iteration 20/25 | Loss: 0.00078309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007830949616618454, 0.0007830949616618454, 0.0007830949616618454, 0.0007830949616618454, 0.0007830949616618454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007830949616618454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078309
Iteration 2/1000 | Loss: 0.00003572
Iteration 3/1000 | Loss: 0.00002085
Iteration 4/1000 | Loss: 0.00001750
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001523
Iteration 7/1000 | Loss: 0.00001479
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001425
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001419
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001403
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001394
Iteration 16/1000 | Loss: 0.00001388
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001362
Iteration 26/1000 | Loss: 0.00001361
Iteration 27/1000 | Loss: 0.00001361
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001356
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001353
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001352
Iteration 41/1000 | Loss: 0.00001352
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001351
Iteration 44/1000 | Loss: 0.00001351
Iteration 45/1000 | Loss: 0.00001351
Iteration 46/1000 | Loss: 0.00001350
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001348
Iteration 52/1000 | Loss: 0.00001348
Iteration 53/1000 | Loss: 0.00001348
Iteration 54/1000 | Loss: 0.00001348
Iteration 55/1000 | Loss: 0.00001348
Iteration 56/1000 | Loss: 0.00001348
Iteration 57/1000 | Loss: 0.00001348
Iteration 58/1000 | Loss: 0.00001348
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001347
Iteration 61/1000 | Loss: 0.00001347
Iteration 62/1000 | Loss: 0.00001347
Iteration 63/1000 | Loss: 0.00001347
Iteration 64/1000 | Loss: 0.00001347
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001345
Iteration 70/1000 | Loss: 0.00001345
Iteration 71/1000 | Loss: 0.00001345
Iteration 72/1000 | Loss: 0.00001345
Iteration 73/1000 | Loss: 0.00001344
Iteration 74/1000 | Loss: 0.00001344
Iteration 75/1000 | Loss: 0.00001344
Iteration 76/1000 | Loss: 0.00001344
Iteration 77/1000 | Loss: 0.00001344
Iteration 78/1000 | Loss: 0.00001344
Iteration 79/1000 | Loss: 0.00001344
Iteration 80/1000 | Loss: 0.00001343
Iteration 81/1000 | Loss: 0.00001343
Iteration 82/1000 | Loss: 0.00001342
Iteration 83/1000 | Loss: 0.00001342
Iteration 84/1000 | Loss: 0.00001342
Iteration 85/1000 | Loss: 0.00001341
Iteration 86/1000 | Loss: 0.00001341
Iteration 87/1000 | Loss: 0.00001341
Iteration 88/1000 | Loss: 0.00001341
Iteration 89/1000 | Loss: 0.00001341
Iteration 90/1000 | Loss: 0.00001341
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001340
Iteration 94/1000 | Loss: 0.00001339
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001338
Iteration 100/1000 | Loss: 0.00001338
Iteration 101/1000 | Loss: 0.00001338
Iteration 102/1000 | Loss: 0.00001338
Iteration 103/1000 | Loss: 0.00001338
Iteration 104/1000 | Loss: 0.00001337
Iteration 105/1000 | Loss: 0.00001337
Iteration 106/1000 | Loss: 0.00001337
Iteration 107/1000 | Loss: 0.00001337
Iteration 108/1000 | Loss: 0.00001337
Iteration 109/1000 | Loss: 0.00001337
Iteration 110/1000 | Loss: 0.00001336
Iteration 111/1000 | Loss: 0.00001336
Iteration 112/1000 | Loss: 0.00001336
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001336
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001335
Iteration 119/1000 | Loss: 0.00001335
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001332
Iteration 127/1000 | Loss: 0.00001332
Iteration 128/1000 | Loss: 0.00001331
Iteration 129/1000 | Loss: 0.00001331
Iteration 130/1000 | Loss: 0.00001331
Iteration 131/1000 | Loss: 0.00001331
Iteration 132/1000 | Loss: 0.00001331
Iteration 133/1000 | Loss: 0.00001331
Iteration 134/1000 | Loss: 0.00001331
Iteration 135/1000 | Loss: 0.00001331
Iteration 136/1000 | Loss: 0.00001331
Iteration 137/1000 | Loss: 0.00001330
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00001330
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001329
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001329
Iteration 145/1000 | Loss: 0.00001329
Iteration 146/1000 | Loss: 0.00001329
Iteration 147/1000 | Loss: 0.00001329
Iteration 148/1000 | Loss: 0.00001329
Iteration 149/1000 | Loss: 0.00001329
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001328
Iteration 153/1000 | Loss: 0.00001328
Iteration 154/1000 | Loss: 0.00001328
Iteration 155/1000 | Loss: 0.00001328
Iteration 156/1000 | Loss: 0.00001328
Iteration 157/1000 | Loss: 0.00001328
Iteration 158/1000 | Loss: 0.00001328
Iteration 159/1000 | Loss: 0.00001328
Iteration 160/1000 | Loss: 0.00001328
Iteration 161/1000 | Loss: 0.00001328
Iteration 162/1000 | Loss: 0.00001328
Iteration 163/1000 | Loss: 0.00001328
Iteration 164/1000 | Loss: 0.00001328
Iteration 165/1000 | Loss: 0.00001328
Iteration 166/1000 | Loss: 0.00001328
Iteration 167/1000 | Loss: 0.00001327
Iteration 168/1000 | Loss: 0.00001327
Iteration 169/1000 | Loss: 0.00001327
Iteration 170/1000 | Loss: 0.00001327
Iteration 171/1000 | Loss: 0.00001327
Iteration 172/1000 | Loss: 0.00001327
Iteration 173/1000 | Loss: 0.00001327
Iteration 174/1000 | Loss: 0.00001327
Iteration 175/1000 | Loss: 0.00001327
Iteration 176/1000 | Loss: 0.00001326
Iteration 177/1000 | Loss: 0.00001326
Iteration 178/1000 | Loss: 0.00001326
Iteration 179/1000 | Loss: 0.00001326
Iteration 180/1000 | Loss: 0.00001326
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001326
Iteration 185/1000 | Loss: 0.00001326
Iteration 186/1000 | Loss: 0.00001326
Iteration 187/1000 | Loss: 0.00001326
Iteration 188/1000 | Loss: 0.00001326
Iteration 189/1000 | Loss: 0.00001326
Iteration 190/1000 | Loss: 0.00001326
Iteration 191/1000 | Loss: 0.00001325
Iteration 192/1000 | Loss: 0.00001325
Iteration 193/1000 | Loss: 0.00001325
Iteration 194/1000 | Loss: 0.00001325
Iteration 195/1000 | Loss: 0.00001325
Iteration 196/1000 | Loss: 0.00001325
Iteration 197/1000 | Loss: 0.00001325
Iteration 198/1000 | Loss: 0.00001325
Iteration 199/1000 | Loss: 0.00001325
Iteration 200/1000 | Loss: 0.00001325
Iteration 201/1000 | Loss: 0.00001325
Iteration 202/1000 | Loss: 0.00001325
Iteration 203/1000 | Loss: 0.00001325
Iteration 204/1000 | Loss: 0.00001325
Iteration 205/1000 | Loss: 0.00001325
Iteration 206/1000 | Loss: 0.00001325
Iteration 207/1000 | Loss: 0.00001324
Iteration 208/1000 | Loss: 0.00001324
Iteration 209/1000 | Loss: 0.00001324
Iteration 210/1000 | Loss: 0.00001324
Iteration 211/1000 | Loss: 0.00001324
Iteration 212/1000 | Loss: 0.00001324
Iteration 213/1000 | Loss: 0.00001324
Iteration 214/1000 | Loss: 0.00001324
Iteration 215/1000 | Loss: 0.00001324
Iteration 216/1000 | Loss: 0.00001324
Iteration 217/1000 | Loss: 0.00001324
Iteration 218/1000 | Loss: 0.00001324
Iteration 219/1000 | Loss: 0.00001324
Iteration 220/1000 | Loss: 0.00001324
Iteration 221/1000 | Loss: 0.00001324
Iteration 222/1000 | Loss: 0.00001324
Iteration 223/1000 | Loss: 0.00001324
Iteration 224/1000 | Loss: 0.00001324
Iteration 225/1000 | Loss: 0.00001324
Iteration 226/1000 | Loss: 0.00001324
Iteration 227/1000 | Loss: 0.00001324
Iteration 228/1000 | Loss: 0.00001323
Iteration 229/1000 | Loss: 0.00001323
Iteration 230/1000 | Loss: 0.00001323
Iteration 231/1000 | Loss: 0.00001323
Iteration 232/1000 | Loss: 0.00001323
Iteration 233/1000 | Loss: 0.00001323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.3234837751952e-05, 1.3234837751952e-05, 1.3234837751952e-05, 1.3234837751952e-05, 1.3234837751952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3234837751952e-05

Optimization complete. Final v2v error: 3.0505893230438232 mm

Highest mean error: 3.59539532661438 mm for frame 39

Lowest mean error: 2.8250651359558105 mm for frame 96

Saving results

Total time: 41.9176070690155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968423
Iteration 2/25 | Loss: 0.00968422
Iteration 3/25 | Loss: 0.00289056
Iteration 4/25 | Loss: 0.00191488
Iteration 5/25 | Loss: 0.00184785
Iteration 6/25 | Loss: 0.00183327
Iteration 7/25 | Loss: 0.00183049
Iteration 8/25 | Loss: 0.00181530
Iteration 9/25 | Loss: 0.00181535
Iteration 10/25 | Loss: 0.00181285
Iteration 11/25 | Loss: 0.00181188
Iteration 12/25 | Loss: 0.00181216
Iteration 13/25 | Loss: 0.00180977
Iteration 14/25 | Loss: 0.00180915
Iteration 15/25 | Loss: 0.00180905
Iteration 16/25 | Loss: 0.00180905
Iteration 17/25 | Loss: 0.00180905
Iteration 18/25 | Loss: 0.00180905
Iteration 19/25 | Loss: 0.00180905
Iteration 20/25 | Loss: 0.00180905
Iteration 21/25 | Loss: 0.00180905
Iteration 22/25 | Loss: 0.00180905
Iteration 23/25 | Loss: 0.00180905
Iteration 24/25 | Loss: 0.00180905
Iteration 25/25 | Loss: 0.00180905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40122402
Iteration 2/25 | Loss: 0.00404587
Iteration 3/25 | Loss: 0.00404587
Iteration 4/25 | Loss: 0.00404587
Iteration 5/25 | Loss: 0.00404587
Iteration 6/25 | Loss: 0.00404587
Iteration 7/25 | Loss: 0.00404587
Iteration 8/25 | Loss: 0.00404587
Iteration 9/25 | Loss: 0.00404587
Iteration 10/25 | Loss: 0.00404587
Iteration 11/25 | Loss: 0.00404587
Iteration 12/25 | Loss: 0.00404587
Iteration 13/25 | Loss: 0.00404587
Iteration 14/25 | Loss: 0.00404587
Iteration 15/25 | Loss: 0.00404587
Iteration 16/25 | Loss: 0.00404587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004045868758112192, 0.004045868758112192, 0.004045868758112192, 0.004045868758112192, 0.004045868758112192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004045868758112192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00404587
Iteration 2/1000 | Loss: 0.00056771
Iteration 3/1000 | Loss: 0.00041858
Iteration 4/1000 | Loss: 0.00036258
Iteration 5/1000 | Loss: 0.00032948
Iteration 6/1000 | Loss: 0.00030961
Iteration 7/1000 | Loss: 0.00029281
Iteration 8/1000 | Loss: 0.00028037
Iteration 9/1000 | Loss: 0.00026884
Iteration 10/1000 | Loss: 0.00025995
Iteration 11/1000 | Loss: 0.00057202
Iteration 12/1000 | Loss: 0.00133550
Iteration 13/1000 | Loss: 0.01148088
Iteration 14/1000 | Loss: 0.00177766
Iteration 15/1000 | Loss: 0.00033610
Iteration 16/1000 | Loss: 0.00024614
Iteration 17/1000 | Loss: 0.00017341
Iteration 18/1000 | Loss: 0.00031046
Iteration 19/1000 | Loss: 0.00013245
Iteration 20/1000 | Loss: 0.00006180
Iteration 21/1000 | Loss: 0.00005001
Iteration 22/1000 | Loss: 0.00004091
Iteration 23/1000 | Loss: 0.00003566
Iteration 24/1000 | Loss: 0.00003123
Iteration 25/1000 | Loss: 0.00002793
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002264
Iteration 28/1000 | Loss: 0.00002124
Iteration 29/1000 | Loss: 0.00002034
Iteration 30/1000 | Loss: 0.00001920
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001699
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001691
Iteration 55/1000 | Loss: 0.00001691
Iteration 56/1000 | Loss: 0.00001691
Iteration 57/1000 | Loss: 0.00001691
Iteration 58/1000 | Loss: 0.00001691
Iteration 59/1000 | Loss: 0.00001691
Iteration 60/1000 | Loss: 0.00001691
Iteration 61/1000 | Loss: 0.00001691
Iteration 62/1000 | Loss: 0.00001691
Iteration 63/1000 | Loss: 0.00001691
Iteration 64/1000 | Loss: 0.00001691
Iteration 65/1000 | Loss: 0.00001691
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.6908372344914824e-05, 1.6908372344914824e-05, 1.6908372344914824e-05, 1.6908372344914824e-05, 1.6908372344914824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6908372344914824e-05

Optimization complete. Final v2v error: 3.4829299449920654 mm

Highest mean error: 3.625781774520874 mm for frame 205

Lowest mean error: 3.335205554962158 mm for frame 102

Saving results

Total time: 85.5195484161377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806671
Iteration 2/25 | Loss: 0.00156014
Iteration 3/25 | Loss: 0.00134513
Iteration 4/25 | Loss: 0.00130742
Iteration 5/25 | Loss: 0.00129940
Iteration 6/25 | Loss: 0.00129818
Iteration 7/25 | Loss: 0.00129818
Iteration 8/25 | Loss: 0.00129818
Iteration 9/25 | Loss: 0.00129818
Iteration 10/25 | Loss: 0.00129818
Iteration 11/25 | Loss: 0.00129818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012981784529983997, 0.0012981784529983997, 0.0012981784529983997, 0.0012981784529983997, 0.0012981784529983997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012981784529983997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43009973
Iteration 2/25 | Loss: 0.00069716
Iteration 3/25 | Loss: 0.00069716
Iteration 4/25 | Loss: 0.00069716
Iteration 5/25 | Loss: 0.00069716
Iteration 6/25 | Loss: 0.00069716
Iteration 7/25 | Loss: 0.00069716
Iteration 8/25 | Loss: 0.00069715
Iteration 9/25 | Loss: 0.00069715
Iteration 10/25 | Loss: 0.00069715
Iteration 11/25 | Loss: 0.00069715
Iteration 12/25 | Loss: 0.00069715
Iteration 13/25 | Loss: 0.00069715
Iteration 14/25 | Loss: 0.00069715
Iteration 15/25 | Loss: 0.00069715
Iteration 16/25 | Loss: 0.00069715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006971542607061565, 0.0006971542607061565, 0.0006971542607061565, 0.0006971542607061565, 0.0006971542607061565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006971542607061565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069715
Iteration 2/1000 | Loss: 0.00004253
Iteration 3/1000 | Loss: 0.00002772
Iteration 4/1000 | Loss: 0.00002450
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002264
Iteration 7/1000 | Loss: 0.00002220
Iteration 8/1000 | Loss: 0.00002187
Iteration 9/1000 | Loss: 0.00002184
Iteration 10/1000 | Loss: 0.00002151
Iteration 11/1000 | Loss: 0.00002123
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002118
Iteration 14/1000 | Loss: 0.00002116
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002108
Iteration 17/1000 | Loss: 0.00002107
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002099
Iteration 20/1000 | Loss: 0.00002093
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002085
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002083
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002082
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00002082
Iteration 33/1000 | Loss: 0.00002082
Iteration 34/1000 | Loss: 0.00002082
Iteration 35/1000 | Loss: 0.00002082
Iteration 36/1000 | Loss: 0.00002082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 36. Stopping optimization.
Last 5 losses: [2.0824749299208634e-05, 2.0824749299208634e-05, 2.0824749299208634e-05, 2.0824749299208634e-05, 2.0824749299208634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0824749299208634e-05

Optimization complete. Final v2v error: 3.8488388061523438 mm

Highest mean error: 4.448025226593018 mm for frame 217

Lowest mean error: 3.4616050720214844 mm for frame 5

Saving results

Total time: 31.336215019226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956730
Iteration 2/25 | Loss: 0.00227967
Iteration 3/25 | Loss: 0.00166117
Iteration 4/25 | Loss: 0.00156231
Iteration 5/25 | Loss: 0.00144960
Iteration 6/25 | Loss: 0.00144010
Iteration 7/25 | Loss: 0.00142122
Iteration 8/25 | Loss: 0.00139012
Iteration 9/25 | Loss: 0.00139316
Iteration 10/25 | Loss: 0.00137841
Iteration 11/25 | Loss: 0.00136984
Iteration 12/25 | Loss: 0.00137282
Iteration 13/25 | Loss: 0.00136858
Iteration 14/25 | Loss: 0.00136529
Iteration 15/25 | Loss: 0.00136395
Iteration 16/25 | Loss: 0.00136328
Iteration 17/25 | Loss: 0.00136147
Iteration 18/25 | Loss: 0.00136058
Iteration 19/25 | Loss: 0.00135988
Iteration 20/25 | Loss: 0.00135964
Iteration 21/25 | Loss: 0.00136156
Iteration 22/25 | Loss: 0.00135569
Iteration 23/25 | Loss: 0.00135470
Iteration 24/25 | Loss: 0.00135445
Iteration 25/25 | Loss: 0.00135445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39136910
Iteration 2/25 | Loss: 0.00100211
Iteration 3/25 | Loss: 0.00100210
Iteration 4/25 | Loss: 0.00100210
Iteration 5/25 | Loss: 0.00100210
Iteration 6/25 | Loss: 0.00100210
Iteration 7/25 | Loss: 0.00100210
Iteration 8/25 | Loss: 0.00100210
Iteration 9/25 | Loss: 0.00100210
Iteration 10/25 | Loss: 0.00100210
Iteration 11/25 | Loss: 0.00100210
Iteration 12/25 | Loss: 0.00100210
Iteration 13/25 | Loss: 0.00100210
Iteration 14/25 | Loss: 0.00100210
Iteration 15/25 | Loss: 0.00100210
Iteration 16/25 | Loss: 0.00100210
Iteration 17/25 | Loss: 0.00100210
Iteration 18/25 | Loss: 0.00100210
Iteration 19/25 | Loss: 0.00100210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010021021589636803, 0.0010021021589636803, 0.0010021021589636803, 0.0010021021589636803, 0.0010021021589636803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010021021589636803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100210
Iteration 2/1000 | Loss: 0.00007794
Iteration 3/1000 | Loss: 0.00005356
Iteration 4/1000 | Loss: 0.00004371
Iteration 5/1000 | Loss: 0.00003918
Iteration 6/1000 | Loss: 0.00003674
Iteration 7/1000 | Loss: 0.00003564
Iteration 8/1000 | Loss: 0.00003459
Iteration 9/1000 | Loss: 0.00003395
Iteration 10/1000 | Loss: 0.00003327
Iteration 11/1000 | Loss: 0.00003283
Iteration 12/1000 | Loss: 0.00003251
Iteration 13/1000 | Loss: 0.00003237
Iteration 14/1000 | Loss: 0.00003216
Iteration 15/1000 | Loss: 0.00003198
Iteration 16/1000 | Loss: 0.00003179
Iteration 17/1000 | Loss: 0.00003162
Iteration 18/1000 | Loss: 0.00003149
Iteration 19/1000 | Loss: 0.00003145
Iteration 20/1000 | Loss: 0.00003144
Iteration 21/1000 | Loss: 0.00003143
Iteration 22/1000 | Loss: 0.00003142
Iteration 23/1000 | Loss: 0.00003140
Iteration 24/1000 | Loss: 0.00003140
Iteration 25/1000 | Loss: 0.00003139
Iteration 26/1000 | Loss: 0.00003133
Iteration 27/1000 | Loss: 0.00003132
Iteration 28/1000 | Loss: 0.00003131
Iteration 29/1000 | Loss: 0.00003131
Iteration 30/1000 | Loss: 0.00003128
Iteration 31/1000 | Loss: 0.00003127
Iteration 32/1000 | Loss: 0.00003126
Iteration 33/1000 | Loss: 0.00003126
Iteration 34/1000 | Loss: 0.00003126
Iteration 35/1000 | Loss: 0.00003126
Iteration 36/1000 | Loss: 0.00003126
Iteration 37/1000 | Loss: 0.00003125
Iteration 38/1000 | Loss: 0.00003125
Iteration 39/1000 | Loss: 0.00003125
Iteration 40/1000 | Loss: 0.00003125
Iteration 41/1000 | Loss: 0.00003124
Iteration 42/1000 | Loss: 0.00003124
Iteration 43/1000 | Loss: 0.00003124
Iteration 44/1000 | Loss: 0.00003124
Iteration 45/1000 | Loss: 0.00003124
Iteration 46/1000 | Loss: 0.00003124
Iteration 47/1000 | Loss: 0.00003124
Iteration 48/1000 | Loss: 0.00003124
Iteration 49/1000 | Loss: 0.00003124
Iteration 50/1000 | Loss: 0.00003124
Iteration 51/1000 | Loss: 0.00003124
Iteration 52/1000 | Loss: 0.00003124
Iteration 53/1000 | Loss: 0.00003124
Iteration 54/1000 | Loss: 0.00003124
Iteration 55/1000 | Loss: 0.00003124
Iteration 56/1000 | Loss: 0.00003124
Iteration 57/1000 | Loss: 0.00003123
Iteration 58/1000 | Loss: 0.00003123
Iteration 59/1000 | Loss: 0.00003123
Iteration 60/1000 | Loss: 0.00003123
Iteration 61/1000 | Loss: 0.00003123
Iteration 62/1000 | Loss: 0.00003122
Iteration 63/1000 | Loss: 0.00003122
Iteration 64/1000 | Loss: 0.00003122
Iteration 65/1000 | Loss: 0.00003122
Iteration 66/1000 | Loss: 0.00003122
Iteration 67/1000 | Loss: 0.00003122
Iteration 68/1000 | Loss: 0.00003122
Iteration 69/1000 | Loss: 0.00003122
Iteration 70/1000 | Loss: 0.00003122
Iteration 71/1000 | Loss: 0.00003122
Iteration 72/1000 | Loss: 0.00003121
Iteration 73/1000 | Loss: 0.00003121
Iteration 74/1000 | Loss: 0.00003121
Iteration 75/1000 | Loss: 0.00003121
Iteration 76/1000 | Loss: 0.00003121
Iteration 77/1000 | Loss: 0.00003121
Iteration 78/1000 | Loss: 0.00003121
Iteration 79/1000 | Loss: 0.00003121
Iteration 80/1000 | Loss: 0.00003121
Iteration 81/1000 | Loss: 0.00003121
Iteration 82/1000 | Loss: 0.00003121
Iteration 83/1000 | Loss: 0.00003121
Iteration 84/1000 | Loss: 0.00003121
Iteration 85/1000 | Loss: 0.00003121
Iteration 86/1000 | Loss: 0.00003121
Iteration 87/1000 | Loss: 0.00003121
Iteration 88/1000 | Loss: 0.00003121
Iteration 89/1000 | Loss: 0.00003121
Iteration 90/1000 | Loss: 0.00003121
Iteration 91/1000 | Loss: 0.00003121
Iteration 92/1000 | Loss: 0.00003121
Iteration 93/1000 | Loss: 0.00003121
Iteration 94/1000 | Loss: 0.00003121
Iteration 95/1000 | Loss: 0.00003121
Iteration 96/1000 | Loss: 0.00003121
Iteration 97/1000 | Loss: 0.00003121
Iteration 98/1000 | Loss: 0.00003121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [3.1212599424179643e-05, 3.1212599424179643e-05, 3.1212599424179643e-05, 3.1212599424179643e-05, 3.1212599424179643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1212599424179643e-05

Optimization complete. Final v2v error: 4.135031700134277 mm

Highest mean error: 11.680198669433594 mm for frame 47

Lowest mean error: 3.6625242233276367 mm for frame 55

Saving results

Total time: 71.09587049484253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088100
Iteration 2/25 | Loss: 0.00159620
Iteration 3/25 | Loss: 0.00136655
Iteration 4/25 | Loss: 0.00135380
Iteration 5/25 | Loss: 0.00135789
Iteration 6/25 | Loss: 0.00135430
Iteration 7/25 | Loss: 0.00135134
Iteration 8/25 | Loss: 0.00135081
Iteration 9/25 | Loss: 0.00135081
Iteration 10/25 | Loss: 0.00135081
Iteration 11/25 | Loss: 0.00135081
Iteration 12/25 | Loss: 0.00135081
Iteration 13/25 | Loss: 0.00135080
Iteration 14/25 | Loss: 0.00135080
Iteration 15/25 | Loss: 0.00135080
Iteration 16/25 | Loss: 0.00135080
Iteration 17/25 | Loss: 0.00135080
Iteration 18/25 | Loss: 0.00135080
Iteration 19/25 | Loss: 0.00135080
Iteration 20/25 | Loss: 0.00135080
Iteration 21/25 | Loss: 0.00135080
Iteration 22/25 | Loss: 0.00135080
Iteration 23/25 | Loss: 0.00135080
Iteration 24/25 | Loss: 0.00135080
Iteration 25/25 | Loss: 0.00135080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91600084
Iteration 2/25 | Loss: 0.00068846
Iteration 3/25 | Loss: 0.00068845
Iteration 4/25 | Loss: 0.00068844
Iteration 5/25 | Loss: 0.00068844
Iteration 6/25 | Loss: 0.00068844
Iteration 7/25 | Loss: 0.00068844
Iteration 8/25 | Loss: 0.00068844
Iteration 9/25 | Loss: 0.00068844
Iteration 10/25 | Loss: 0.00068844
Iteration 11/25 | Loss: 0.00068844
Iteration 12/25 | Loss: 0.00068844
Iteration 13/25 | Loss: 0.00068844
Iteration 14/25 | Loss: 0.00068844
Iteration 15/25 | Loss: 0.00068844
Iteration 16/25 | Loss: 0.00068844
Iteration 17/25 | Loss: 0.00068844
Iteration 18/25 | Loss: 0.00068844
Iteration 19/25 | Loss: 0.00068844
Iteration 20/25 | Loss: 0.00068844
Iteration 21/25 | Loss: 0.00068844
Iteration 22/25 | Loss: 0.00068844
Iteration 23/25 | Loss: 0.00068844
Iteration 24/25 | Loss: 0.00068844
Iteration 25/25 | Loss: 0.00068844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068844
Iteration 2/1000 | Loss: 0.00004446
Iteration 3/1000 | Loss: 0.00003035
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002493
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002378
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002306
Iteration 10/1000 | Loss: 0.00002277
Iteration 11/1000 | Loss: 0.00002259
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002230
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00002208
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002206
Iteration 20/1000 | Loss: 0.00002206
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002205
Iteration 24/1000 | Loss: 0.00002205
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002205
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002204
Iteration 31/1000 | Loss: 0.00002204
Iteration 32/1000 | Loss: 0.00002204
Iteration 33/1000 | Loss: 0.00002204
Iteration 34/1000 | Loss: 0.00002204
Iteration 35/1000 | Loss: 0.00002204
Iteration 36/1000 | Loss: 0.00002204
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002203
Iteration 40/1000 | Loss: 0.00002203
Iteration 41/1000 | Loss: 0.00002203
Iteration 42/1000 | Loss: 0.00002202
Iteration 43/1000 | Loss: 0.00002202
Iteration 44/1000 | Loss: 0.00002202
Iteration 45/1000 | Loss: 0.00002202
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002202
Iteration 48/1000 | Loss: 0.00002202
Iteration 49/1000 | Loss: 0.00002202
Iteration 50/1000 | Loss: 0.00002201
Iteration 51/1000 | Loss: 0.00002201
Iteration 52/1000 | Loss: 0.00002201
Iteration 53/1000 | Loss: 0.00002201
Iteration 54/1000 | Loss: 0.00002201
Iteration 55/1000 | Loss: 0.00002200
Iteration 56/1000 | Loss: 0.00002200
Iteration 57/1000 | Loss: 0.00002200
Iteration 58/1000 | Loss: 0.00002199
Iteration 59/1000 | Loss: 0.00002199
Iteration 60/1000 | Loss: 0.00002199
Iteration 61/1000 | Loss: 0.00002199
Iteration 62/1000 | Loss: 0.00002198
Iteration 63/1000 | Loss: 0.00002197
Iteration 64/1000 | Loss: 0.00002197
Iteration 65/1000 | Loss: 0.00002197
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002196
Iteration 68/1000 | Loss: 0.00002196
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002196
Iteration 71/1000 | Loss: 0.00002196
Iteration 72/1000 | Loss: 0.00002196
Iteration 73/1000 | Loss: 0.00002196
Iteration 74/1000 | Loss: 0.00002196
Iteration 75/1000 | Loss: 0.00002196
Iteration 76/1000 | Loss: 0.00002196
Iteration 77/1000 | Loss: 0.00002195
Iteration 78/1000 | Loss: 0.00002195
Iteration 79/1000 | Loss: 0.00002194
Iteration 80/1000 | Loss: 0.00002194
Iteration 81/1000 | Loss: 0.00002194
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002193
Iteration 86/1000 | Loss: 0.00002193
Iteration 87/1000 | Loss: 0.00002193
Iteration 88/1000 | Loss: 0.00002193
Iteration 89/1000 | Loss: 0.00002192
Iteration 90/1000 | Loss: 0.00002192
Iteration 91/1000 | Loss: 0.00002192
Iteration 92/1000 | Loss: 0.00002192
Iteration 93/1000 | Loss: 0.00002192
Iteration 94/1000 | Loss: 0.00002192
Iteration 95/1000 | Loss: 0.00002192
Iteration 96/1000 | Loss: 0.00002192
Iteration 97/1000 | Loss: 0.00002191
Iteration 98/1000 | Loss: 0.00002191
Iteration 99/1000 | Loss: 0.00002191
Iteration 100/1000 | Loss: 0.00002191
Iteration 101/1000 | Loss: 0.00002191
Iteration 102/1000 | Loss: 0.00002191
Iteration 103/1000 | Loss: 0.00002190
Iteration 104/1000 | Loss: 0.00002190
Iteration 105/1000 | Loss: 0.00002190
Iteration 106/1000 | Loss: 0.00002190
Iteration 107/1000 | Loss: 0.00002190
Iteration 108/1000 | Loss: 0.00002190
Iteration 109/1000 | Loss: 0.00002190
Iteration 110/1000 | Loss: 0.00002190
Iteration 111/1000 | Loss: 0.00002190
Iteration 112/1000 | Loss: 0.00002190
Iteration 113/1000 | Loss: 0.00002189
Iteration 114/1000 | Loss: 0.00002189
Iteration 115/1000 | Loss: 0.00002189
Iteration 116/1000 | Loss: 0.00002189
Iteration 117/1000 | Loss: 0.00002189
Iteration 118/1000 | Loss: 0.00002189
Iteration 119/1000 | Loss: 0.00002189
Iteration 120/1000 | Loss: 0.00002189
Iteration 121/1000 | Loss: 0.00002189
Iteration 122/1000 | Loss: 0.00002189
Iteration 123/1000 | Loss: 0.00002189
Iteration 124/1000 | Loss: 0.00002189
Iteration 125/1000 | Loss: 0.00002189
Iteration 126/1000 | Loss: 0.00002189
Iteration 127/1000 | Loss: 0.00002189
Iteration 128/1000 | Loss: 0.00002189
Iteration 129/1000 | Loss: 0.00002189
Iteration 130/1000 | Loss: 0.00002189
Iteration 131/1000 | Loss: 0.00002189
Iteration 132/1000 | Loss: 0.00002189
Iteration 133/1000 | Loss: 0.00002189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.1890173229621723e-05, 2.1890173229621723e-05, 2.1890173229621723e-05, 2.1890173229621723e-05, 2.1890173229621723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1890173229621723e-05

Optimization complete. Final v2v error: 3.7933690547943115 mm

Highest mean error: 4.261643409729004 mm for frame 104

Lowest mean error: 3.4617397785186768 mm for frame 52

Saving results

Total time: 40.72250008583069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811090
Iteration 2/25 | Loss: 0.00158335
Iteration 3/25 | Loss: 0.00134248
Iteration 4/25 | Loss: 0.00130159
Iteration 5/25 | Loss: 0.00128934
Iteration 6/25 | Loss: 0.00128584
Iteration 7/25 | Loss: 0.00128162
Iteration 8/25 | Loss: 0.00129191
Iteration 9/25 | Loss: 0.00128249
Iteration 10/25 | Loss: 0.00128259
Iteration 11/25 | Loss: 0.00129060
Iteration 12/25 | Loss: 0.00128487
Iteration 13/25 | Loss: 0.00128891
Iteration 14/25 | Loss: 0.00128958
Iteration 15/25 | Loss: 0.00128591
Iteration 16/25 | Loss: 0.00128573
Iteration 17/25 | Loss: 0.00128458
Iteration 18/25 | Loss: 0.00128338
Iteration 19/25 | Loss: 0.00128335
Iteration 20/25 | Loss: 0.00128204
Iteration 21/25 | Loss: 0.00127696
Iteration 22/25 | Loss: 0.00127758
Iteration 23/25 | Loss: 0.00127689
Iteration 24/25 | Loss: 0.00127612
Iteration 25/25 | Loss: 0.00127487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43878484
Iteration 2/25 | Loss: 0.00094209
Iteration 3/25 | Loss: 0.00094209
Iteration 4/25 | Loss: 0.00094209
Iteration 5/25 | Loss: 0.00094208
Iteration 6/25 | Loss: 0.00094208
Iteration 7/25 | Loss: 0.00094208
Iteration 8/25 | Loss: 0.00094208
Iteration 9/25 | Loss: 0.00094208
Iteration 10/25 | Loss: 0.00094208
Iteration 11/25 | Loss: 0.00094208
Iteration 12/25 | Loss: 0.00094208
Iteration 13/25 | Loss: 0.00094208
Iteration 14/25 | Loss: 0.00094208
Iteration 15/25 | Loss: 0.00094208
Iteration 16/25 | Loss: 0.00094208
Iteration 17/25 | Loss: 0.00094208
Iteration 18/25 | Loss: 0.00094208
Iteration 19/25 | Loss: 0.00094208
Iteration 20/25 | Loss: 0.00094208
Iteration 21/25 | Loss: 0.00094208
Iteration 22/25 | Loss: 0.00094208
Iteration 23/25 | Loss: 0.00094208
Iteration 24/25 | Loss: 0.00094208
Iteration 25/25 | Loss: 0.00094208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094208
Iteration 2/1000 | Loss: 0.00009099
Iteration 3/1000 | Loss: 0.00012909
Iteration 4/1000 | Loss: 0.00016159
Iteration 5/1000 | Loss: 0.00012472
Iteration 6/1000 | Loss: 0.00012037
Iteration 7/1000 | Loss: 0.00008772
Iteration 8/1000 | Loss: 0.00012210
Iteration 9/1000 | Loss: 0.00016410
Iteration 10/1000 | Loss: 0.00017037
Iteration 11/1000 | Loss: 0.00019004
Iteration 12/1000 | Loss: 0.00018549
Iteration 13/1000 | Loss: 0.00011522
Iteration 14/1000 | Loss: 0.00013376
Iteration 15/1000 | Loss: 0.00016465
Iteration 16/1000 | Loss: 0.00018065
Iteration 17/1000 | Loss: 0.00012402
Iteration 18/1000 | Loss: 0.00010058
Iteration 19/1000 | Loss: 0.00011385
Iteration 20/1000 | Loss: 0.00008560
Iteration 21/1000 | Loss: 0.00018547
Iteration 22/1000 | Loss: 0.00008678
Iteration 23/1000 | Loss: 0.00019326
Iteration 24/1000 | Loss: 0.00013140
Iteration 25/1000 | Loss: 0.00013639
Iteration 26/1000 | Loss: 0.00015429
Iteration 27/1000 | Loss: 0.00016087
Iteration 28/1000 | Loss: 0.00015204
Iteration 29/1000 | Loss: 0.00016701
Iteration 30/1000 | Loss: 0.00015005
Iteration 31/1000 | Loss: 0.00017287
Iteration 32/1000 | Loss: 0.00014888
Iteration 33/1000 | Loss: 0.00016355
Iteration 34/1000 | Loss: 0.00016678
Iteration 35/1000 | Loss: 0.00016848
Iteration 36/1000 | Loss: 0.00015237
Iteration 37/1000 | Loss: 0.00018434
Iteration 38/1000 | Loss: 0.00013265
Iteration 39/1000 | Loss: 0.00015588
Iteration 40/1000 | Loss: 0.00014356
Iteration 41/1000 | Loss: 0.00019462
Iteration 42/1000 | Loss: 0.00013094
Iteration 43/1000 | Loss: 0.00019958
Iteration 44/1000 | Loss: 0.00013928
Iteration 45/1000 | Loss: 0.00018145
Iteration 46/1000 | Loss: 0.00010059
Iteration 47/1000 | Loss: 0.00010070
Iteration 48/1000 | Loss: 0.00012779
Iteration 49/1000 | Loss: 0.00007741
Iteration 50/1000 | Loss: 0.00011783
Iteration 51/1000 | Loss: 0.00011693
Iteration 52/1000 | Loss: 0.00014793
Iteration 53/1000 | Loss: 0.00008912
Iteration 54/1000 | Loss: 0.00009782
Iteration 55/1000 | Loss: 0.00008435
Iteration 56/1000 | Loss: 0.00012476
Iteration 57/1000 | Loss: 0.00016737
Iteration 58/1000 | Loss: 0.00009297
Iteration 59/1000 | Loss: 0.00015388
Iteration 60/1000 | Loss: 0.00016532
Iteration 61/1000 | Loss: 0.00014391
Iteration 62/1000 | Loss: 0.00015438
Iteration 63/1000 | Loss: 0.00011326
Iteration 64/1000 | Loss: 0.00011548
Iteration 65/1000 | Loss: 0.00014447
Iteration 66/1000 | Loss: 0.00015212
Iteration 67/1000 | Loss: 0.00016987
Iteration 68/1000 | Loss: 0.00015587
Iteration 69/1000 | Loss: 0.00010458
Iteration 70/1000 | Loss: 0.00013424
Iteration 71/1000 | Loss: 0.00018815
Iteration 72/1000 | Loss: 0.00017244
Iteration 73/1000 | Loss: 0.00017350
Iteration 74/1000 | Loss: 0.00016087
Iteration 75/1000 | Loss: 0.00015996
Iteration 76/1000 | Loss: 0.00015736
Iteration 77/1000 | Loss: 0.00017320
Iteration 78/1000 | Loss: 0.00028224
Iteration 79/1000 | Loss: 0.00009283
Iteration 80/1000 | Loss: 0.00005690
Iteration 81/1000 | Loss: 0.00010799
Iteration 82/1000 | Loss: 0.00008162
Iteration 83/1000 | Loss: 0.00006091
Iteration 84/1000 | Loss: 0.00009485
Iteration 85/1000 | Loss: 0.00006838
Iteration 86/1000 | Loss: 0.00009700
Iteration 87/1000 | Loss: 0.00007008
Iteration 88/1000 | Loss: 0.00005829
Iteration 89/1000 | Loss: 0.00008579
Iteration 90/1000 | Loss: 0.00007030
Iteration 91/1000 | Loss: 0.00009015
Iteration 92/1000 | Loss: 0.00007720
Iteration 93/1000 | Loss: 0.00008544
Iteration 94/1000 | Loss: 0.00009956
Iteration 95/1000 | Loss: 0.00008946
Iteration 96/1000 | Loss: 0.00007998
Iteration 97/1000 | Loss: 0.00006840
Iteration 98/1000 | Loss: 0.00006458
Iteration 99/1000 | Loss: 0.00021129
Iteration 100/1000 | Loss: 0.00004981
Iteration 101/1000 | Loss: 0.00005396
Iteration 102/1000 | Loss: 0.00003663
Iteration 103/1000 | Loss: 0.00004154
Iteration 104/1000 | Loss: 0.00005975
Iteration 105/1000 | Loss: 0.00006120
Iteration 106/1000 | Loss: 0.00006371
Iteration 107/1000 | Loss: 0.00006152
Iteration 108/1000 | Loss: 0.00006262
Iteration 109/1000 | Loss: 0.00007231
Iteration 110/1000 | Loss: 0.00004013
Iteration 111/1000 | Loss: 0.00004502
Iteration 112/1000 | Loss: 0.00002657
Iteration 113/1000 | Loss: 0.00003497
Iteration 114/1000 | Loss: 0.00002200
Iteration 115/1000 | Loss: 0.00003252
Iteration 116/1000 | Loss: 0.00004700
Iteration 117/1000 | Loss: 0.00003959
Iteration 118/1000 | Loss: 0.00003098
Iteration 119/1000 | Loss: 0.00003958
Iteration 120/1000 | Loss: 0.00003603
Iteration 121/1000 | Loss: 0.00004029
Iteration 122/1000 | Loss: 0.00004694
Iteration 123/1000 | Loss: 0.00005331
Iteration 124/1000 | Loss: 0.00004936
Iteration 125/1000 | Loss: 0.00005801
Iteration 126/1000 | Loss: 0.00006655
Iteration 127/1000 | Loss: 0.00005931
Iteration 128/1000 | Loss: 0.00009791
Iteration 129/1000 | Loss: 0.00004145
Iteration 130/1000 | Loss: 0.00004722
Iteration 131/1000 | Loss: 0.00012158
Iteration 132/1000 | Loss: 0.00006080
Iteration 133/1000 | Loss: 0.00005824
Iteration 134/1000 | Loss: 0.00006332
Iteration 135/1000 | Loss: 0.00004612
Iteration 136/1000 | Loss: 0.00002797
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00004138
Iteration 139/1000 | Loss: 0.00004288
Iteration 140/1000 | Loss: 0.00005464
Iteration 141/1000 | Loss: 0.00004464
Iteration 142/1000 | Loss: 0.00004063
Iteration 143/1000 | Loss: 0.00002314
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00003409
Iteration 146/1000 | Loss: 0.00003367
Iteration 147/1000 | Loss: 0.00003400
Iteration 148/1000 | Loss: 0.00004082
Iteration 149/1000 | Loss: 0.00002340
Iteration 150/1000 | Loss: 0.00001837
Iteration 151/1000 | Loss: 0.00001835
Iteration 152/1000 | Loss: 0.00002696
Iteration 153/1000 | Loss: 0.00002672
Iteration 154/1000 | Loss: 0.00002880
Iteration 155/1000 | Loss: 0.00002475
Iteration 156/1000 | Loss: 0.00002041
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00003928
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00003559
Iteration 161/1000 | Loss: 0.00001709
Iteration 162/1000 | Loss: 0.00002348
Iteration 163/1000 | Loss: 0.00001828
Iteration 164/1000 | Loss: 0.00003566
Iteration 165/1000 | Loss: 0.00003806
Iteration 166/1000 | Loss: 0.00003108
Iteration 167/1000 | Loss: 0.00004625
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002104
Iteration 170/1000 | Loss: 0.00002535
Iteration 171/1000 | Loss: 0.00003877
Iteration 172/1000 | Loss: 0.00002889
Iteration 173/1000 | Loss: 0.00002817
Iteration 174/1000 | Loss: 0.00003188
Iteration 175/1000 | Loss: 0.00002468
Iteration 176/1000 | Loss: 0.00002800
Iteration 177/1000 | Loss: 0.00002924
Iteration 178/1000 | Loss: 0.00002125
Iteration 179/1000 | Loss: 0.00002689
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00002565
Iteration 182/1000 | Loss: 0.00002219
Iteration 183/1000 | Loss: 0.00002795
Iteration 184/1000 | Loss: 0.00002272
Iteration 185/1000 | Loss: 0.00001812
Iteration 186/1000 | Loss: 0.00001767
Iteration 187/1000 | Loss: 0.00001698
Iteration 188/1000 | Loss: 0.00001629
Iteration 189/1000 | Loss: 0.00003947
Iteration 190/1000 | Loss: 0.00002976
Iteration 191/1000 | Loss: 0.00002004
Iteration 192/1000 | Loss: 0.00001889
Iteration 193/1000 | Loss: 0.00001739
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001848
Iteration 197/1000 | Loss: 0.00001712
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001639
Iteration 200/1000 | Loss: 0.00001427
Iteration 201/1000 | Loss: 0.00001557
Iteration 202/1000 | Loss: 0.00001528
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001422
Iteration 205/1000 | Loss: 0.00001398
Iteration 206/1000 | Loss: 0.00001387
Iteration 207/1000 | Loss: 0.00001370
Iteration 208/1000 | Loss: 0.00001365
Iteration 209/1000 | Loss: 0.00001353
Iteration 210/1000 | Loss: 0.00001346
Iteration 211/1000 | Loss: 0.00001344
Iteration 212/1000 | Loss: 0.00001340
Iteration 213/1000 | Loss: 0.00001336
Iteration 214/1000 | Loss: 0.00001335
Iteration 215/1000 | Loss: 0.00001331
Iteration 216/1000 | Loss: 0.00001326
Iteration 217/1000 | Loss: 0.00001325
Iteration 218/1000 | Loss: 0.00001324
Iteration 219/1000 | Loss: 0.00001316
Iteration 220/1000 | Loss: 0.00001315
Iteration 221/1000 | Loss: 0.00001315
Iteration 222/1000 | Loss: 0.00001315
Iteration 223/1000 | Loss: 0.00001312
Iteration 224/1000 | Loss: 0.00001311
Iteration 225/1000 | Loss: 0.00001309
Iteration 226/1000 | Loss: 0.00001304
Iteration 227/1000 | Loss: 0.00001302
Iteration 228/1000 | Loss: 0.00001299
Iteration 229/1000 | Loss: 0.00001299
Iteration 230/1000 | Loss: 0.00001298
Iteration 231/1000 | Loss: 0.00001297
Iteration 232/1000 | Loss: 0.00001297
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001296
Iteration 236/1000 | Loss: 0.00001296
Iteration 237/1000 | Loss: 0.00001295
Iteration 238/1000 | Loss: 0.00001295
Iteration 239/1000 | Loss: 0.00001295
Iteration 240/1000 | Loss: 0.00001294
Iteration 241/1000 | Loss: 0.00001294
Iteration 242/1000 | Loss: 0.00001294
Iteration 243/1000 | Loss: 0.00001293
Iteration 244/1000 | Loss: 0.00001293
Iteration 245/1000 | Loss: 0.00001292
Iteration 246/1000 | Loss: 0.00001292
Iteration 247/1000 | Loss: 0.00001291
Iteration 248/1000 | Loss: 0.00001291
Iteration 249/1000 | Loss: 0.00001291
Iteration 250/1000 | Loss: 0.00001291
Iteration 251/1000 | Loss: 0.00001291
Iteration 252/1000 | Loss: 0.00001290
Iteration 253/1000 | Loss: 0.00001290
Iteration 254/1000 | Loss: 0.00001290
Iteration 255/1000 | Loss: 0.00001289
Iteration 256/1000 | Loss: 0.00001289
Iteration 257/1000 | Loss: 0.00001288
Iteration 258/1000 | Loss: 0.00001288
Iteration 259/1000 | Loss: 0.00001287
Iteration 260/1000 | Loss: 0.00001287
Iteration 261/1000 | Loss: 0.00001287
Iteration 262/1000 | Loss: 0.00001287
Iteration 263/1000 | Loss: 0.00001286
Iteration 264/1000 | Loss: 0.00001286
Iteration 265/1000 | Loss: 0.00001286
Iteration 266/1000 | Loss: 0.00001286
Iteration 267/1000 | Loss: 0.00001286
Iteration 268/1000 | Loss: 0.00001286
Iteration 269/1000 | Loss: 0.00001285
Iteration 270/1000 | Loss: 0.00001285
Iteration 271/1000 | Loss: 0.00001285
Iteration 272/1000 | Loss: 0.00001285
Iteration 273/1000 | Loss: 0.00001285
Iteration 274/1000 | Loss: 0.00001285
Iteration 275/1000 | Loss: 0.00001285
Iteration 276/1000 | Loss: 0.00001285
Iteration 277/1000 | Loss: 0.00001285
Iteration 278/1000 | Loss: 0.00001284
Iteration 279/1000 | Loss: 0.00001284
Iteration 280/1000 | Loss: 0.00001284
Iteration 281/1000 | Loss: 0.00001284
Iteration 282/1000 | Loss: 0.00001284
Iteration 283/1000 | Loss: 0.00001284
Iteration 284/1000 | Loss: 0.00001284
Iteration 285/1000 | Loss: 0.00001284
Iteration 286/1000 | Loss: 0.00001283
Iteration 287/1000 | Loss: 0.00001283
Iteration 288/1000 | Loss: 0.00001283
Iteration 289/1000 | Loss: 0.00001283
Iteration 290/1000 | Loss: 0.00001283
Iteration 291/1000 | Loss: 0.00001283
Iteration 292/1000 | Loss: 0.00001283
Iteration 293/1000 | Loss: 0.00001283
Iteration 294/1000 | Loss: 0.00001283
Iteration 295/1000 | Loss: 0.00001283
Iteration 296/1000 | Loss: 0.00001283
Iteration 297/1000 | Loss: 0.00001283
Iteration 298/1000 | Loss: 0.00001283
Iteration 299/1000 | Loss: 0.00001283
Iteration 300/1000 | Loss: 0.00001283
Iteration 301/1000 | Loss: 0.00001283
Iteration 302/1000 | Loss: 0.00001283
Iteration 303/1000 | Loss: 0.00001283
Iteration 304/1000 | Loss: 0.00001283
Iteration 305/1000 | Loss: 0.00001283
Iteration 306/1000 | Loss: 0.00001283
Iteration 307/1000 | Loss: 0.00001283
Iteration 308/1000 | Loss: 0.00001283
Iteration 309/1000 | Loss: 0.00001283
Iteration 310/1000 | Loss: 0.00001283
Iteration 311/1000 | Loss: 0.00001283
Iteration 312/1000 | Loss: 0.00001283
Iteration 313/1000 | Loss: 0.00001283
Iteration 314/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [1.2834319932153448e-05, 1.2834319932153448e-05, 1.2834319932153448e-05, 1.2834319932153448e-05, 1.2834319932153448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2834319932153448e-05

Optimization complete. Final v2v error: 3.0734987258911133 mm

Highest mean error: 4.014682769775391 mm for frame 146

Lowest mean error: 2.8900809288024902 mm for frame 1

Saving results

Total time: 386.7135736942291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439037
Iteration 2/25 | Loss: 0.00126944
Iteration 3/25 | Loss: 0.00121737
Iteration 4/25 | Loss: 0.00121113
Iteration 5/25 | Loss: 0.00120981
Iteration 6/25 | Loss: 0.00120981
Iteration 7/25 | Loss: 0.00120981
Iteration 8/25 | Loss: 0.00120981
Iteration 9/25 | Loss: 0.00120981
Iteration 10/25 | Loss: 0.00120981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012098068837076426, 0.0012098068837076426, 0.0012098068837076426, 0.0012098068837076426, 0.0012098068837076426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012098068837076426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.40345550
Iteration 2/25 | Loss: 0.00072352
Iteration 3/25 | Loss: 0.00072352
Iteration 4/25 | Loss: 0.00072352
Iteration 5/25 | Loss: 0.00072352
Iteration 6/25 | Loss: 0.00072352
Iteration 7/25 | Loss: 0.00072352
Iteration 8/25 | Loss: 0.00072352
Iteration 9/25 | Loss: 0.00072352
Iteration 10/25 | Loss: 0.00072352
Iteration 11/25 | Loss: 0.00072352
Iteration 12/25 | Loss: 0.00072352
Iteration 13/25 | Loss: 0.00072352
Iteration 14/25 | Loss: 0.00072352
Iteration 15/25 | Loss: 0.00072352
Iteration 16/25 | Loss: 0.00072352
Iteration 17/25 | Loss: 0.00072352
Iteration 18/25 | Loss: 0.00072352
Iteration 19/25 | Loss: 0.00072352
Iteration 20/25 | Loss: 0.00072352
Iteration 21/25 | Loss: 0.00072352
Iteration 22/25 | Loss: 0.00072352
Iteration 23/25 | Loss: 0.00072352
Iteration 24/25 | Loss: 0.00072352
Iteration 25/25 | Loss: 0.00072352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072352
Iteration 2/1000 | Loss: 0.00002819
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001801
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001649
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001530
Iteration 16/1000 | Loss: 0.00001527
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001525
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001504
Iteration 46/1000 | Loss: 0.00001504
Iteration 47/1000 | Loss: 0.00001504
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001504
Iteration 51/1000 | Loss: 0.00001504
Iteration 52/1000 | Loss: 0.00001503
Iteration 53/1000 | Loss: 0.00001502
Iteration 54/1000 | Loss: 0.00001501
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.4997001017036382e-05, 1.4997001017036382e-05, 1.4997001017036382e-05, 1.4997001017036382e-05, 1.4997001017036382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4997001017036382e-05

Optimization complete. Final v2v error: 3.2753827571868896 mm

Highest mean error: 3.596464157104492 mm for frame 109

Lowest mean error: 3.1078436374664307 mm for frame 14

Saving results

Total time: 31.278114318847656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597514
Iteration 2/25 | Loss: 0.00154629
Iteration 3/25 | Loss: 0.00132315
Iteration 4/25 | Loss: 0.00129962
Iteration 5/25 | Loss: 0.00129552
Iteration 6/25 | Loss: 0.00129389
Iteration 7/25 | Loss: 0.00129386
Iteration 8/25 | Loss: 0.00129386
Iteration 9/25 | Loss: 0.00129386
Iteration 10/25 | Loss: 0.00129386
Iteration 11/25 | Loss: 0.00129386
Iteration 12/25 | Loss: 0.00129386
Iteration 13/25 | Loss: 0.00129386
Iteration 14/25 | Loss: 0.00129386
Iteration 15/25 | Loss: 0.00129386
Iteration 16/25 | Loss: 0.00129386
Iteration 17/25 | Loss: 0.00129386
Iteration 18/25 | Loss: 0.00129386
Iteration 19/25 | Loss: 0.00129386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012938593281432986, 0.0012938593281432986, 0.0012938593281432986, 0.0012938593281432986, 0.0012938593281432986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012938593281432986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23147213
Iteration 2/25 | Loss: 0.00073623
Iteration 3/25 | Loss: 0.00073623
Iteration 4/25 | Loss: 0.00073623
Iteration 5/25 | Loss: 0.00073623
Iteration 6/25 | Loss: 0.00073623
Iteration 7/25 | Loss: 0.00073623
Iteration 8/25 | Loss: 0.00073623
Iteration 9/25 | Loss: 0.00073623
Iteration 10/25 | Loss: 0.00073623
Iteration 11/25 | Loss: 0.00073623
Iteration 12/25 | Loss: 0.00073623
Iteration 13/25 | Loss: 0.00073623
Iteration 14/25 | Loss: 0.00073623
Iteration 15/25 | Loss: 0.00073623
Iteration 16/25 | Loss: 0.00073623
Iteration 17/25 | Loss: 0.00073623
Iteration 18/25 | Loss: 0.00073623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007362259784713387, 0.0007362259784713387, 0.0007362259784713387, 0.0007362259784713387, 0.0007362259784713387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007362259784713387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073623
Iteration 2/1000 | Loss: 0.00005064
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00002345
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00001983
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001826
Iteration 21/1000 | Loss: 0.00001825
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001820
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001815
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001814
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001809
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001806
Iteration 43/1000 | Loss: 0.00001806
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001805
Iteration 48/1000 | Loss: 0.00001805
Iteration 49/1000 | Loss: 0.00001804
Iteration 50/1000 | Loss: 0.00001804
Iteration 51/1000 | Loss: 0.00001804
Iteration 52/1000 | Loss: 0.00001804
Iteration 53/1000 | Loss: 0.00001804
Iteration 54/1000 | Loss: 0.00001804
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001801
Iteration 64/1000 | Loss: 0.00001801
Iteration 65/1000 | Loss: 0.00001801
Iteration 66/1000 | Loss: 0.00001800
Iteration 67/1000 | Loss: 0.00001800
Iteration 68/1000 | Loss: 0.00001800
Iteration 69/1000 | Loss: 0.00001799
Iteration 70/1000 | Loss: 0.00001799
Iteration 71/1000 | Loss: 0.00001798
Iteration 72/1000 | Loss: 0.00001798
Iteration 73/1000 | Loss: 0.00001798
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001797
Iteration 77/1000 | Loss: 0.00001797
Iteration 78/1000 | Loss: 0.00001797
Iteration 79/1000 | Loss: 0.00001797
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001796
Iteration 86/1000 | Loss: 0.00001796
Iteration 87/1000 | Loss: 0.00001796
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001796
Iteration 93/1000 | Loss: 0.00001796
Iteration 94/1000 | Loss: 0.00001796
Iteration 95/1000 | Loss: 0.00001796
Iteration 96/1000 | Loss: 0.00001796
Iteration 97/1000 | Loss: 0.00001796
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001796
Iteration 105/1000 | Loss: 0.00001796
Iteration 106/1000 | Loss: 0.00001796
Iteration 107/1000 | Loss: 0.00001796
Iteration 108/1000 | Loss: 0.00001796
Iteration 109/1000 | Loss: 0.00001796
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.7961850971914828e-05, 1.7961850971914828e-05, 1.7961850971914828e-05, 1.7961850971914828e-05, 1.7961850971914828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7961850971914828e-05

Optimization complete. Final v2v error: 3.425445318222046 mm

Highest mean error: 4.991746425628662 mm for frame 56

Lowest mean error: 3.064406633377075 mm for frame 131

Saving results

Total time: 33.69673728942871
