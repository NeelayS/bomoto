Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=144, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8064-8119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017048
Iteration 2/25 | Loss: 0.00248781
Iteration 3/25 | Loss: 0.00195186
Iteration 4/25 | Loss: 0.00189459
Iteration 5/25 | Loss: 0.00172818
Iteration 6/25 | Loss: 0.00156261
Iteration 7/25 | Loss: 0.00146567
Iteration 8/25 | Loss: 0.00130583
Iteration 9/25 | Loss: 0.00118573
Iteration 10/25 | Loss: 0.00112936
Iteration 11/25 | Loss: 0.00109777
Iteration 12/25 | Loss: 0.00107586
Iteration 13/25 | Loss: 0.00105847
Iteration 14/25 | Loss: 0.00104269
Iteration 15/25 | Loss: 0.00103707
Iteration 16/25 | Loss: 0.00103402
Iteration 17/25 | Loss: 0.00103015
Iteration 18/25 | Loss: 0.00102674
Iteration 19/25 | Loss: 0.00102667
Iteration 20/25 | Loss: 0.00102568
Iteration 21/25 | Loss: 0.00102535
Iteration 22/25 | Loss: 0.00102526
Iteration 23/25 | Loss: 0.00102526
Iteration 24/25 | Loss: 0.00102526
Iteration 25/25 | Loss: 0.00102526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34203029
Iteration 2/25 | Loss: 0.00041049
Iteration 3/25 | Loss: 0.00041049
Iteration 4/25 | Loss: 0.00041049
Iteration 5/25 | Loss: 0.00041049
Iteration 6/25 | Loss: 0.00041049
Iteration 7/25 | Loss: 0.00041049
Iteration 8/25 | Loss: 0.00041049
Iteration 9/25 | Loss: 0.00041049
Iteration 10/25 | Loss: 0.00041049
Iteration 11/25 | Loss: 0.00041049
Iteration 12/25 | Loss: 0.00041049
Iteration 13/25 | Loss: 0.00041049
Iteration 14/25 | Loss: 0.00041049
Iteration 15/25 | Loss: 0.00041049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0004104909603483975, 0.0004104909603483975, 0.0004104909603483975, 0.0004104909603483975, 0.0004104909603483975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004104909603483975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041049
Iteration 2/1000 | Loss: 0.00002315
Iteration 3/1000 | Loss: 0.00001793
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001490
Iteration 8/1000 | Loss: 0.00001454
Iteration 9/1000 | Loss: 0.00001432
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001373
Iteration 26/1000 | Loss: 0.00001373
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001372
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001371
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001370
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001368
Iteration 41/1000 | Loss: 0.00001368
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001367
Iteration 46/1000 | Loss: 0.00001367
Iteration 47/1000 | Loss: 0.00001367
Iteration 48/1000 | Loss: 0.00001367
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001366
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001366
Iteration 54/1000 | Loss: 0.00001366
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001361
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001360
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001357
Iteration 120/1000 | Loss: 0.00001357
Iteration 121/1000 | Loss: 0.00001357
Iteration 122/1000 | Loss: 0.00001357
Iteration 123/1000 | Loss: 0.00001356
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001355
Iteration 127/1000 | Loss: 0.00001355
Iteration 128/1000 | Loss: 0.00001355
Iteration 129/1000 | Loss: 0.00001355
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001354
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001352
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001351
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001351
Iteration 151/1000 | Loss: 0.00001351
Iteration 152/1000 | Loss: 0.00001351
Iteration 153/1000 | Loss: 0.00001351
Iteration 154/1000 | Loss: 0.00001351
Iteration 155/1000 | Loss: 0.00001351
Iteration 156/1000 | Loss: 0.00001351
Iteration 157/1000 | Loss: 0.00001351
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001349
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001349
Iteration 173/1000 | Loss: 0.00001349
Iteration 174/1000 | Loss: 0.00001349
Iteration 175/1000 | Loss: 0.00001349
Iteration 176/1000 | Loss: 0.00001349
Iteration 177/1000 | Loss: 0.00001349
Iteration 178/1000 | Loss: 0.00001349
Iteration 179/1000 | Loss: 0.00001349
Iteration 180/1000 | Loss: 0.00001349
Iteration 181/1000 | Loss: 0.00001349
Iteration 182/1000 | Loss: 0.00001349
Iteration 183/1000 | Loss: 0.00001349
Iteration 184/1000 | Loss: 0.00001349
Iteration 185/1000 | Loss: 0.00001349
Iteration 186/1000 | Loss: 0.00001349
Iteration 187/1000 | Loss: 0.00001349
Iteration 188/1000 | Loss: 0.00001349
Iteration 189/1000 | Loss: 0.00001349
Iteration 190/1000 | Loss: 0.00001349
Iteration 191/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.3486357602232601e-05, 1.3486357602232601e-05, 1.3486357602232601e-05, 1.3486357602232601e-05, 1.3486357602232601e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3486357602232601e-05

Optimization complete. Final v2v error: 3.07645583152771 mm

Highest mean error: 3.2130696773529053 mm for frame 93

Lowest mean error: 2.9312334060668945 mm for frame 10

Saving results

Total time: 78.16942691802979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448361
Iteration 2/25 | Loss: 0.00126063
Iteration 3/25 | Loss: 0.00108590
Iteration 4/25 | Loss: 0.00107093
Iteration 5/25 | Loss: 0.00106894
Iteration 6/25 | Loss: 0.00106873
Iteration 7/25 | Loss: 0.00106873
Iteration 8/25 | Loss: 0.00106873
Iteration 9/25 | Loss: 0.00106873
Iteration 10/25 | Loss: 0.00106873
Iteration 11/25 | Loss: 0.00106873
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010687339818105102, 0.0010687339818105102, 0.0010687339818105102, 0.0010687339818105102, 0.0010687339818105102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010687339818105102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.13339710
Iteration 2/25 | Loss: 0.00049565
Iteration 3/25 | Loss: 0.00049562
Iteration 4/25 | Loss: 0.00049562
Iteration 5/25 | Loss: 0.00049562
Iteration 6/25 | Loss: 0.00049562
Iteration 7/25 | Loss: 0.00049562
Iteration 8/25 | Loss: 0.00049562
Iteration 9/25 | Loss: 0.00049562
Iteration 10/25 | Loss: 0.00049562
Iteration 11/25 | Loss: 0.00049562
Iteration 12/25 | Loss: 0.00049561
Iteration 13/25 | Loss: 0.00049561
Iteration 14/25 | Loss: 0.00049562
Iteration 15/25 | Loss: 0.00049561
Iteration 16/25 | Loss: 0.00049561
Iteration 17/25 | Loss: 0.00049561
Iteration 18/25 | Loss: 0.00049561
Iteration 19/25 | Loss: 0.00049561
Iteration 20/25 | Loss: 0.00049561
Iteration 21/25 | Loss: 0.00049561
Iteration 22/25 | Loss: 0.00049561
Iteration 23/25 | Loss: 0.00049561
Iteration 24/25 | Loss: 0.00049561
Iteration 25/25 | Loss: 0.00049561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049561
Iteration 2/1000 | Loss: 0.00003279
Iteration 3/1000 | Loss: 0.00002136
Iteration 4/1000 | Loss: 0.00001858
Iteration 5/1000 | Loss: 0.00001758
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001576
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001528
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001525
Iteration 16/1000 | Loss: 0.00001525
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001520
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001517
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001517
Iteration 29/1000 | Loss: 0.00001516
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001511
Iteration 39/1000 | Loss: 0.00001511
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001508
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001507
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001505
Iteration 49/1000 | Loss: 0.00001505
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001504
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001502
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001502
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001499
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001493
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001492
Iteration 92/1000 | Loss: 0.00001492
Iteration 93/1000 | Loss: 0.00001492
Iteration 94/1000 | Loss: 0.00001492
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001490
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001490
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001486
Iteration 121/1000 | Loss: 0.00001486
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001485
Iteration 128/1000 | Loss: 0.00001485
Iteration 129/1000 | Loss: 0.00001485
Iteration 130/1000 | Loss: 0.00001485
Iteration 131/1000 | Loss: 0.00001485
Iteration 132/1000 | Loss: 0.00001485
Iteration 133/1000 | Loss: 0.00001484
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001484
Iteration 136/1000 | Loss: 0.00001484
Iteration 137/1000 | Loss: 0.00001484
Iteration 138/1000 | Loss: 0.00001484
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001483
Iteration 141/1000 | Loss: 0.00001483
Iteration 142/1000 | Loss: 0.00001483
Iteration 143/1000 | Loss: 0.00001483
Iteration 144/1000 | Loss: 0.00001483
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001482
Iteration 148/1000 | Loss: 0.00001482
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001482
Iteration 158/1000 | Loss: 0.00001481
Iteration 159/1000 | Loss: 0.00001481
Iteration 160/1000 | Loss: 0.00001481
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Iteration 164/1000 | Loss: 0.00001481
Iteration 165/1000 | Loss: 0.00001480
Iteration 166/1000 | Loss: 0.00001480
Iteration 167/1000 | Loss: 0.00001480
Iteration 168/1000 | Loss: 0.00001480
Iteration 169/1000 | Loss: 0.00001480
Iteration 170/1000 | Loss: 0.00001480
Iteration 171/1000 | Loss: 0.00001480
Iteration 172/1000 | Loss: 0.00001480
Iteration 173/1000 | Loss: 0.00001480
Iteration 174/1000 | Loss: 0.00001480
Iteration 175/1000 | Loss: 0.00001480
Iteration 176/1000 | Loss: 0.00001480
Iteration 177/1000 | Loss: 0.00001480
Iteration 178/1000 | Loss: 0.00001480
Iteration 179/1000 | Loss: 0.00001480
Iteration 180/1000 | Loss: 0.00001480
Iteration 181/1000 | Loss: 0.00001480
Iteration 182/1000 | Loss: 0.00001480
Iteration 183/1000 | Loss: 0.00001480
Iteration 184/1000 | Loss: 0.00001480
Iteration 185/1000 | Loss: 0.00001480
Iteration 186/1000 | Loss: 0.00001480
Iteration 187/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.479506136092823e-05, 1.479506136092823e-05, 1.479506136092823e-05, 1.479506136092823e-05, 1.479506136092823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.479506136092823e-05

Optimization complete. Final v2v error: 3.2497403621673584 mm

Highest mean error: 3.8671350479125977 mm for frame 47

Lowest mean error: 2.854769468307495 mm for frame 1

Saving results

Total time: 37.694839000701904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836383
Iteration 2/25 | Loss: 0.00127968
Iteration 3/25 | Loss: 0.00115153
Iteration 4/25 | Loss: 0.00113193
Iteration 5/25 | Loss: 0.00112558
Iteration 6/25 | Loss: 0.00112471
Iteration 7/25 | Loss: 0.00112471
Iteration 8/25 | Loss: 0.00112471
Iteration 9/25 | Loss: 0.00112471
Iteration 10/25 | Loss: 0.00112471
Iteration 11/25 | Loss: 0.00112471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011247125221416354, 0.0011247125221416354, 0.0011247125221416354, 0.0011247125221416354, 0.0011247125221416354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011247125221416354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19823396
Iteration 2/25 | Loss: 0.00054307
Iteration 3/25 | Loss: 0.00054306
Iteration 4/25 | Loss: 0.00054306
Iteration 5/25 | Loss: 0.00054306
Iteration 6/25 | Loss: 0.00054306
Iteration 7/25 | Loss: 0.00054306
Iteration 8/25 | Loss: 0.00054306
Iteration 9/25 | Loss: 0.00054306
Iteration 10/25 | Loss: 0.00054306
Iteration 11/25 | Loss: 0.00054306
Iteration 12/25 | Loss: 0.00054306
Iteration 13/25 | Loss: 0.00054306
Iteration 14/25 | Loss: 0.00054306
Iteration 15/25 | Loss: 0.00054306
Iteration 16/25 | Loss: 0.00054306
Iteration 17/25 | Loss: 0.00054306
Iteration 18/25 | Loss: 0.00054306
Iteration 19/25 | Loss: 0.00054306
Iteration 20/25 | Loss: 0.00054306
Iteration 21/25 | Loss: 0.00054306
Iteration 22/25 | Loss: 0.00054306
Iteration 23/25 | Loss: 0.00054306
Iteration 24/25 | Loss: 0.00054306
Iteration 25/25 | Loss: 0.00054306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054306
Iteration 2/1000 | Loss: 0.00004499
Iteration 3/1000 | Loss: 0.00003157
Iteration 4/1000 | Loss: 0.00002841
Iteration 5/1000 | Loss: 0.00002651
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002470
Iteration 8/1000 | Loss: 0.00002427
Iteration 9/1000 | Loss: 0.00002377
Iteration 10/1000 | Loss: 0.00002351
Iteration 11/1000 | Loss: 0.00002338
Iteration 12/1000 | Loss: 0.00002323
Iteration 13/1000 | Loss: 0.00002320
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002292
Iteration 17/1000 | Loss: 0.00002287
Iteration 18/1000 | Loss: 0.00002284
Iteration 19/1000 | Loss: 0.00002284
Iteration 20/1000 | Loss: 0.00002284
Iteration 21/1000 | Loss: 0.00002283
Iteration 22/1000 | Loss: 0.00002282
Iteration 23/1000 | Loss: 0.00002281
Iteration 24/1000 | Loss: 0.00002280
Iteration 25/1000 | Loss: 0.00002280
Iteration 26/1000 | Loss: 0.00002280
Iteration 27/1000 | Loss: 0.00002279
Iteration 28/1000 | Loss: 0.00002279
Iteration 29/1000 | Loss: 0.00002279
Iteration 30/1000 | Loss: 0.00002279
Iteration 31/1000 | Loss: 0.00002279
Iteration 32/1000 | Loss: 0.00002278
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002276
Iteration 35/1000 | Loss: 0.00002276
Iteration 36/1000 | Loss: 0.00002275
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002275
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002275
Iteration 41/1000 | Loss: 0.00002275
Iteration 42/1000 | Loss: 0.00002275
Iteration 43/1000 | Loss: 0.00002275
Iteration 44/1000 | Loss: 0.00002275
Iteration 45/1000 | Loss: 0.00002275
Iteration 46/1000 | Loss: 0.00002274
Iteration 47/1000 | Loss: 0.00002274
Iteration 48/1000 | Loss: 0.00002274
Iteration 49/1000 | Loss: 0.00002274
Iteration 50/1000 | Loss: 0.00002274
Iteration 51/1000 | Loss: 0.00002272
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002271
Iteration 55/1000 | Loss: 0.00002271
Iteration 56/1000 | Loss: 0.00002271
Iteration 57/1000 | Loss: 0.00002270
Iteration 58/1000 | Loss: 0.00002270
Iteration 59/1000 | Loss: 0.00002270
Iteration 60/1000 | Loss: 0.00002270
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00002270
Iteration 64/1000 | Loss: 0.00002270
Iteration 65/1000 | Loss: 0.00002270
Iteration 66/1000 | Loss: 0.00002269
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002268
Iteration 69/1000 | Loss: 0.00002267
Iteration 70/1000 | Loss: 0.00002267
Iteration 71/1000 | Loss: 0.00002267
Iteration 72/1000 | Loss: 0.00002267
Iteration 73/1000 | Loss: 0.00002266
Iteration 74/1000 | Loss: 0.00002266
Iteration 75/1000 | Loss: 0.00002265
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002264
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002263
Iteration 83/1000 | Loss: 0.00002263
Iteration 84/1000 | Loss: 0.00002263
Iteration 85/1000 | Loss: 0.00002263
Iteration 86/1000 | Loss: 0.00002263
Iteration 87/1000 | Loss: 0.00002263
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002261
Iteration 90/1000 | Loss: 0.00002261
Iteration 91/1000 | Loss: 0.00002261
Iteration 92/1000 | Loss: 0.00002260
Iteration 93/1000 | Loss: 0.00002260
Iteration 94/1000 | Loss: 0.00002260
Iteration 95/1000 | Loss: 0.00002259
Iteration 96/1000 | Loss: 0.00002259
Iteration 97/1000 | Loss: 0.00002258
Iteration 98/1000 | Loss: 0.00002258
Iteration 99/1000 | Loss: 0.00002258
Iteration 100/1000 | Loss: 0.00002258
Iteration 101/1000 | Loss: 0.00002258
Iteration 102/1000 | Loss: 0.00002258
Iteration 103/1000 | Loss: 0.00002258
Iteration 104/1000 | Loss: 0.00002258
Iteration 105/1000 | Loss: 0.00002257
Iteration 106/1000 | Loss: 0.00002257
Iteration 107/1000 | Loss: 0.00002257
Iteration 108/1000 | Loss: 0.00002257
Iteration 109/1000 | Loss: 0.00002256
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002255
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00002255
Iteration 118/1000 | Loss: 0.00002255
Iteration 119/1000 | Loss: 0.00002255
Iteration 120/1000 | Loss: 0.00002255
Iteration 121/1000 | Loss: 0.00002255
Iteration 122/1000 | Loss: 0.00002255
Iteration 123/1000 | Loss: 0.00002255
Iteration 124/1000 | Loss: 0.00002255
Iteration 125/1000 | Loss: 0.00002255
Iteration 126/1000 | Loss: 0.00002255
Iteration 127/1000 | Loss: 0.00002255
Iteration 128/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.254551145597361e-05, 2.254551145597361e-05, 2.254551145597361e-05, 2.254551145597361e-05, 2.254551145597361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.254551145597361e-05

Optimization complete. Final v2v error: 3.884711265563965 mm

Highest mean error: 5.188381195068359 mm for frame 127

Lowest mean error: 2.919607639312744 mm for frame 52

Saving results

Total time: 40.565985679626465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786510
Iteration 2/25 | Loss: 0.00120854
Iteration 3/25 | Loss: 0.00102545
Iteration 4/25 | Loss: 0.00100216
Iteration 5/25 | Loss: 0.00099871
Iteration 6/25 | Loss: 0.00099868
Iteration 7/25 | Loss: 0.00099868
Iteration 8/25 | Loss: 0.00099868
Iteration 9/25 | Loss: 0.00099868
Iteration 10/25 | Loss: 0.00099868
Iteration 11/25 | Loss: 0.00099868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009986840886995196, 0.0009986840886995196, 0.0009986840886995196, 0.0009986840886995196, 0.0009986840886995196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009986840886995196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37334967
Iteration 2/25 | Loss: 0.00068888
Iteration 3/25 | Loss: 0.00068888
Iteration 4/25 | Loss: 0.00068888
Iteration 5/25 | Loss: 0.00068888
Iteration 6/25 | Loss: 0.00068888
Iteration 7/25 | Loss: 0.00068888
Iteration 8/25 | Loss: 0.00068888
Iteration 9/25 | Loss: 0.00068888
Iteration 10/25 | Loss: 0.00068888
Iteration 11/25 | Loss: 0.00068888
Iteration 12/25 | Loss: 0.00068888
Iteration 13/25 | Loss: 0.00068888
Iteration 14/25 | Loss: 0.00068888
Iteration 15/25 | Loss: 0.00068888
Iteration 16/25 | Loss: 0.00068888
Iteration 17/25 | Loss: 0.00068888
Iteration 18/25 | Loss: 0.00068888
Iteration 19/25 | Loss: 0.00068888
Iteration 20/25 | Loss: 0.00068888
Iteration 21/25 | Loss: 0.00068888
Iteration 22/25 | Loss: 0.00068888
Iteration 23/25 | Loss: 0.00068888
Iteration 24/25 | Loss: 0.00068888
Iteration 25/25 | Loss: 0.00068888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068888
Iteration 2/1000 | Loss: 0.00002315
Iteration 3/1000 | Loss: 0.00001339
Iteration 4/1000 | Loss: 0.00001205
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001080
Iteration 7/1000 | Loss: 0.00001049
Iteration 8/1000 | Loss: 0.00001041
Iteration 9/1000 | Loss: 0.00001019
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00001002
Iteration 12/1000 | Loss: 0.00000997
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00000996
Iteration 15/1000 | Loss: 0.00000995
Iteration 16/1000 | Loss: 0.00000994
Iteration 17/1000 | Loss: 0.00000994
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000990
Iteration 20/1000 | Loss: 0.00000987
Iteration 21/1000 | Loss: 0.00000983
Iteration 22/1000 | Loss: 0.00000983
Iteration 23/1000 | Loss: 0.00000982
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000978
Iteration 26/1000 | Loss: 0.00000977
Iteration 27/1000 | Loss: 0.00000977
Iteration 28/1000 | Loss: 0.00000977
Iteration 29/1000 | Loss: 0.00000977
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000976
Iteration 32/1000 | Loss: 0.00000976
Iteration 33/1000 | Loss: 0.00000976
Iteration 34/1000 | Loss: 0.00000975
Iteration 35/1000 | Loss: 0.00000975
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000974
Iteration 38/1000 | Loss: 0.00000974
Iteration 39/1000 | Loss: 0.00000973
Iteration 40/1000 | Loss: 0.00000973
Iteration 41/1000 | Loss: 0.00000973
Iteration 42/1000 | Loss: 0.00000973
Iteration 43/1000 | Loss: 0.00000972
Iteration 44/1000 | Loss: 0.00000972
Iteration 45/1000 | Loss: 0.00000972
Iteration 46/1000 | Loss: 0.00000971
Iteration 47/1000 | Loss: 0.00000971
Iteration 48/1000 | Loss: 0.00000971
Iteration 49/1000 | Loss: 0.00000970
Iteration 50/1000 | Loss: 0.00000970
Iteration 51/1000 | Loss: 0.00000970
Iteration 52/1000 | Loss: 0.00000970
Iteration 53/1000 | Loss: 0.00000970
Iteration 54/1000 | Loss: 0.00000970
Iteration 55/1000 | Loss: 0.00000969
Iteration 56/1000 | Loss: 0.00000969
Iteration 57/1000 | Loss: 0.00000969
Iteration 58/1000 | Loss: 0.00000969
Iteration 59/1000 | Loss: 0.00000968
Iteration 60/1000 | Loss: 0.00000968
Iteration 61/1000 | Loss: 0.00000968
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000967
Iteration 64/1000 | Loss: 0.00000967
Iteration 65/1000 | Loss: 0.00000967
Iteration 66/1000 | Loss: 0.00000967
Iteration 67/1000 | Loss: 0.00000967
Iteration 68/1000 | Loss: 0.00000966
Iteration 69/1000 | Loss: 0.00000965
Iteration 70/1000 | Loss: 0.00000965
Iteration 71/1000 | Loss: 0.00000964
Iteration 72/1000 | Loss: 0.00000964
Iteration 73/1000 | Loss: 0.00000964
Iteration 74/1000 | Loss: 0.00000963
Iteration 75/1000 | Loss: 0.00000963
Iteration 76/1000 | Loss: 0.00000963
Iteration 77/1000 | Loss: 0.00000963
Iteration 78/1000 | Loss: 0.00000963
Iteration 79/1000 | Loss: 0.00000963
Iteration 80/1000 | Loss: 0.00000963
Iteration 81/1000 | Loss: 0.00000962
Iteration 82/1000 | Loss: 0.00000962
Iteration 83/1000 | Loss: 0.00000962
Iteration 84/1000 | Loss: 0.00000961
Iteration 85/1000 | Loss: 0.00000961
Iteration 86/1000 | Loss: 0.00000961
Iteration 87/1000 | Loss: 0.00000961
Iteration 88/1000 | Loss: 0.00000960
Iteration 89/1000 | Loss: 0.00000960
Iteration 90/1000 | Loss: 0.00000960
Iteration 91/1000 | Loss: 0.00000960
Iteration 92/1000 | Loss: 0.00000960
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000960
Iteration 95/1000 | Loss: 0.00000960
Iteration 96/1000 | Loss: 0.00000960
Iteration 97/1000 | Loss: 0.00000960
Iteration 98/1000 | Loss: 0.00000960
Iteration 99/1000 | Loss: 0.00000960
Iteration 100/1000 | Loss: 0.00000960
Iteration 101/1000 | Loss: 0.00000960
Iteration 102/1000 | Loss: 0.00000960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [9.596659765520599e-06, 9.596659765520599e-06, 9.596659765520599e-06, 9.596659765520599e-06, 9.596659765520599e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.596659765520599e-06

Optimization complete. Final v2v error: 2.622680902481079 mm

Highest mean error: 2.8023486137390137 mm for frame 61

Lowest mean error: 2.39585542678833 mm for frame 233

Saving results

Total time: 33.759483098983765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071899
Iteration 2/25 | Loss: 0.01071899
Iteration 3/25 | Loss: 0.00156400
Iteration 4/25 | Loss: 0.00120249
Iteration 5/25 | Loss: 0.00106463
Iteration 6/25 | Loss: 0.00104606
Iteration 7/25 | Loss: 0.00105014
Iteration 8/25 | Loss: 0.00103362
Iteration 9/25 | Loss: 0.00103080
Iteration 10/25 | Loss: 0.00102695
Iteration 11/25 | Loss: 0.00102521
Iteration 12/25 | Loss: 0.00102384
Iteration 13/25 | Loss: 0.00102331
Iteration 14/25 | Loss: 0.00102306
Iteration 15/25 | Loss: 0.00102300
Iteration 16/25 | Loss: 0.00102299
Iteration 17/25 | Loss: 0.00102299
Iteration 18/25 | Loss: 0.00102299
Iteration 19/25 | Loss: 0.00102299
Iteration 20/25 | Loss: 0.00102299
Iteration 21/25 | Loss: 0.00102299
Iteration 22/25 | Loss: 0.00102299
Iteration 23/25 | Loss: 0.00102299
Iteration 24/25 | Loss: 0.00102298
Iteration 25/25 | Loss: 0.00102298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.45540905
Iteration 2/25 | Loss: 0.00077386
Iteration 3/25 | Loss: 0.00077386
Iteration 4/25 | Loss: 0.00077386
Iteration 5/25 | Loss: 0.00077386
Iteration 6/25 | Loss: 0.00077386
Iteration 7/25 | Loss: 0.00077386
Iteration 8/25 | Loss: 0.00077386
Iteration 9/25 | Loss: 0.00077386
Iteration 10/25 | Loss: 0.00077386
Iteration 11/25 | Loss: 0.00077386
Iteration 12/25 | Loss: 0.00077386
Iteration 13/25 | Loss: 0.00077386
Iteration 14/25 | Loss: 0.00077386
Iteration 15/25 | Loss: 0.00077386
Iteration 16/25 | Loss: 0.00077386
Iteration 17/25 | Loss: 0.00077386
Iteration 18/25 | Loss: 0.00077386
Iteration 19/25 | Loss: 0.00077386
Iteration 20/25 | Loss: 0.00077386
Iteration 21/25 | Loss: 0.00077386
Iteration 22/25 | Loss: 0.00077386
Iteration 23/25 | Loss: 0.00077386
Iteration 24/25 | Loss: 0.00077386
Iteration 25/25 | Loss: 0.00077386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077386
Iteration 2/1000 | Loss: 0.00002149
Iteration 3/1000 | Loss: 0.00007444
Iteration 4/1000 | Loss: 0.00014016
Iteration 5/1000 | Loss: 0.00001520
Iteration 6/1000 | Loss: 0.00001100
Iteration 7/1000 | Loss: 0.00002464
Iteration 8/1000 | Loss: 0.00001057
Iteration 9/1000 | Loss: 0.00001030
Iteration 10/1000 | Loss: 0.00001015
Iteration 11/1000 | Loss: 0.00002488
Iteration 12/1000 | Loss: 0.00008470
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00000991
Iteration 15/1000 | Loss: 0.00000982
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000976
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000974
Iteration 21/1000 | Loss: 0.00000973
Iteration 22/1000 | Loss: 0.00000972
Iteration 23/1000 | Loss: 0.00000971
Iteration 24/1000 | Loss: 0.00000971
Iteration 25/1000 | Loss: 0.00000970
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000969
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000964
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000959
Iteration 32/1000 | Loss: 0.00000958
Iteration 33/1000 | Loss: 0.00000958
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000957
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000957
Iteration 39/1000 | Loss: 0.00000957
Iteration 40/1000 | Loss: 0.00000957
Iteration 41/1000 | Loss: 0.00000957
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000954
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000954
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000953
Iteration 51/1000 | Loss: 0.00000953
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000953
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000953
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000952
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000951
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000950
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000950
Iteration 68/1000 | Loss: 0.00000950
Iteration 69/1000 | Loss: 0.00000950
Iteration 70/1000 | Loss: 0.00000950
Iteration 71/1000 | Loss: 0.00000950
Iteration 72/1000 | Loss: 0.00000949
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000949
Iteration 76/1000 | Loss: 0.00000949
Iteration 77/1000 | Loss: 0.00000948
Iteration 78/1000 | Loss: 0.00000948
Iteration 79/1000 | Loss: 0.00000948
Iteration 80/1000 | Loss: 0.00000948
Iteration 81/1000 | Loss: 0.00000947
Iteration 82/1000 | Loss: 0.00000947
Iteration 83/1000 | Loss: 0.00000947
Iteration 84/1000 | Loss: 0.00000947
Iteration 85/1000 | Loss: 0.00000947
Iteration 86/1000 | Loss: 0.00000947
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000945
Iteration 101/1000 | Loss: 0.00000945
Iteration 102/1000 | Loss: 0.00000945
Iteration 103/1000 | Loss: 0.00000945
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000943
Iteration 116/1000 | Loss: 0.00000943
Iteration 117/1000 | Loss: 0.00000943
Iteration 118/1000 | Loss: 0.00000943
Iteration 119/1000 | Loss: 0.00000942
Iteration 120/1000 | Loss: 0.00000942
Iteration 121/1000 | Loss: 0.00000942
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000941
Iteration 126/1000 | Loss: 0.00000941
Iteration 127/1000 | Loss: 0.00000941
Iteration 128/1000 | Loss: 0.00000941
Iteration 129/1000 | Loss: 0.00000941
Iteration 130/1000 | Loss: 0.00000941
Iteration 131/1000 | Loss: 0.00000941
Iteration 132/1000 | Loss: 0.00000941
Iteration 133/1000 | Loss: 0.00000941
Iteration 134/1000 | Loss: 0.00000940
Iteration 135/1000 | Loss: 0.00000940
Iteration 136/1000 | Loss: 0.00000940
Iteration 137/1000 | Loss: 0.00000940
Iteration 138/1000 | Loss: 0.00000940
Iteration 139/1000 | Loss: 0.00000940
Iteration 140/1000 | Loss: 0.00000940
Iteration 141/1000 | Loss: 0.00000939
Iteration 142/1000 | Loss: 0.00000939
Iteration 143/1000 | Loss: 0.00000939
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000939
Iteration 147/1000 | Loss: 0.00000939
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000939
Iteration 150/1000 | Loss: 0.00000939
Iteration 151/1000 | Loss: 0.00000938
Iteration 152/1000 | Loss: 0.00000938
Iteration 153/1000 | Loss: 0.00000938
Iteration 154/1000 | Loss: 0.00000938
Iteration 155/1000 | Loss: 0.00000937
Iteration 156/1000 | Loss: 0.00000937
Iteration 157/1000 | Loss: 0.00000937
Iteration 158/1000 | Loss: 0.00000937
Iteration 159/1000 | Loss: 0.00000937
Iteration 160/1000 | Loss: 0.00000937
Iteration 161/1000 | Loss: 0.00000937
Iteration 162/1000 | Loss: 0.00000937
Iteration 163/1000 | Loss: 0.00000937
Iteration 164/1000 | Loss: 0.00000937
Iteration 165/1000 | Loss: 0.00000937
Iteration 166/1000 | Loss: 0.00000937
Iteration 167/1000 | Loss: 0.00000937
Iteration 168/1000 | Loss: 0.00000937
Iteration 169/1000 | Loss: 0.00000937
Iteration 170/1000 | Loss: 0.00000937
Iteration 171/1000 | Loss: 0.00000937
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000937
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000936
Iteration 177/1000 | Loss: 0.00000936
Iteration 178/1000 | Loss: 0.00000936
Iteration 179/1000 | Loss: 0.00000936
Iteration 180/1000 | Loss: 0.00000936
Iteration 181/1000 | Loss: 0.00000936
Iteration 182/1000 | Loss: 0.00000936
Iteration 183/1000 | Loss: 0.00000936
Iteration 184/1000 | Loss: 0.00000936
Iteration 185/1000 | Loss: 0.00000936
Iteration 186/1000 | Loss: 0.00000936
Iteration 187/1000 | Loss: 0.00000936
Iteration 188/1000 | Loss: 0.00000936
Iteration 189/1000 | Loss: 0.00000936
Iteration 190/1000 | Loss: 0.00000936
Iteration 191/1000 | Loss: 0.00000936
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000936
Iteration 198/1000 | Loss: 0.00000936
Iteration 199/1000 | Loss: 0.00000936
Iteration 200/1000 | Loss: 0.00000936
Iteration 201/1000 | Loss: 0.00000936
Iteration 202/1000 | Loss: 0.00000936
Iteration 203/1000 | Loss: 0.00000936
Iteration 204/1000 | Loss: 0.00000936
Iteration 205/1000 | Loss: 0.00000936
Iteration 206/1000 | Loss: 0.00000936
Iteration 207/1000 | Loss: 0.00000936
Iteration 208/1000 | Loss: 0.00000936
Iteration 209/1000 | Loss: 0.00000936
Iteration 210/1000 | Loss: 0.00000936
Iteration 211/1000 | Loss: 0.00000936
Iteration 212/1000 | Loss: 0.00000936
Iteration 213/1000 | Loss: 0.00000936
Iteration 214/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [9.36429660214344e-06, 9.36429660214344e-06, 9.36429660214344e-06, 9.36429660214344e-06, 9.36429660214344e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.36429660214344e-06

Optimization complete. Final v2v error: 2.595301866531372 mm

Highest mean error: 2.942169189453125 mm for frame 177

Lowest mean error: 2.4052529335021973 mm for frame 150

Saving results

Total time: 58.61093831062317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804186
Iteration 2/25 | Loss: 0.00114714
Iteration 3/25 | Loss: 0.00099983
Iteration 4/25 | Loss: 0.00097469
Iteration 5/25 | Loss: 0.00096962
Iteration 6/25 | Loss: 0.00096863
Iteration 7/25 | Loss: 0.00096863
Iteration 8/25 | Loss: 0.00096863
Iteration 9/25 | Loss: 0.00096863
Iteration 10/25 | Loss: 0.00096863
Iteration 11/25 | Loss: 0.00096863
Iteration 12/25 | Loss: 0.00096863
Iteration 13/25 | Loss: 0.00096863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009686322300694883, 0.0009686322300694883, 0.0009686322300694883, 0.0009686322300694883, 0.0009686322300694883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009686322300694883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37590373
Iteration 2/25 | Loss: 0.00071501
Iteration 3/25 | Loss: 0.00071501
Iteration 4/25 | Loss: 0.00071501
Iteration 5/25 | Loss: 0.00071501
Iteration 6/25 | Loss: 0.00071501
Iteration 7/25 | Loss: 0.00071501
Iteration 8/25 | Loss: 0.00071501
Iteration 9/25 | Loss: 0.00071501
Iteration 10/25 | Loss: 0.00071501
Iteration 11/25 | Loss: 0.00071501
Iteration 12/25 | Loss: 0.00071501
Iteration 13/25 | Loss: 0.00071501
Iteration 14/25 | Loss: 0.00071501
Iteration 15/25 | Loss: 0.00071501
Iteration 16/25 | Loss: 0.00071501
Iteration 17/25 | Loss: 0.00071501
Iteration 18/25 | Loss: 0.00071501
Iteration 19/25 | Loss: 0.00071501
Iteration 20/25 | Loss: 0.00071501
Iteration 21/25 | Loss: 0.00071501
Iteration 22/25 | Loss: 0.00071501
Iteration 23/25 | Loss: 0.00071501
Iteration 24/25 | Loss: 0.00071501
Iteration 25/25 | Loss: 0.00071501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071501
Iteration 2/1000 | Loss: 0.00002287
Iteration 3/1000 | Loss: 0.00001394
Iteration 4/1000 | Loss: 0.00001194
Iteration 5/1000 | Loss: 0.00001101
Iteration 6/1000 | Loss: 0.00001031
Iteration 7/1000 | Loss: 0.00000987
Iteration 8/1000 | Loss: 0.00000959
Iteration 9/1000 | Loss: 0.00000929
Iteration 10/1000 | Loss: 0.00000917
Iteration 11/1000 | Loss: 0.00000915
Iteration 12/1000 | Loss: 0.00000910
Iteration 13/1000 | Loss: 0.00000909
Iteration 14/1000 | Loss: 0.00000909
Iteration 15/1000 | Loss: 0.00000908
Iteration 16/1000 | Loss: 0.00000908
Iteration 17/1000 | Loss: 0.00000908
Iteration 18/1000 | Loss: 0.00000907
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000901
Iteration 22/1000 | Loss: 0.00000901
Iteration 23/1000 | Loss: 0.00000901
Iteration 24/1000 | Loss: 0.00000899
Iteration 25/1000 | Loss: 0.00000898
Iteration 26/1000 | Loss: 0.00000895
Iteration 27/1000 | Loss: 0.00000895
Iteration 28/1000 | Loss: 0.00000895
Iteration 29/1000 | Loss: 0.00000893
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000893
Iteration 32/1000 | Loss: 0.00000893
Iteration 33/1000 | Loss: 0.00000892
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000890
Iteration 37/1000 | Loss: 0.00000890
Iteration 38/1000 | Loss: 0.00000889
Iteration 39/1000 | Loss: 0.00000889
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000888
Iteration 45/1000 | Loss: 0.00000888
Iteration 46/1000 | Loss: 0.00000887
Iteration 47/1000 | Loss: 0.00000887
Iteration 48/1000 | Loss: 0.00000887
Iteration 49/1000 | Loss: 0.00000886
Iteration 50/1000 | Loss: 0.00000886
Iteration 51/1000 | Loss: 0.00000885
Iteration 52/1000 | Loss: 0.00000885
Iteration 53/1000 | Loss: 0.00000885
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000884
Iteration 56/1000 | Loss: 0.00000884
Iteration 57/1000 | Loss: 0.00000884
Iteration 58/1000 | Loss: 0.00000884
Iteration 59/1000 | Loss: 0.00000883
Iteration 60/1000 | Loss: 0.00000882
Iteration 61/1000 | Loss: 0.00000881
Iteration 62/1000 | Loss: 0.00000881
Iteration 63/1000 | Loss: 0.00000880
Iteration 64/1000 | Loss: 0.00000880
Iteration 65/1000 | Loss: 0.00000879
Iteration 66/1000 | Loss: 0.00000879
Iteration 67/1000 | Loss: 0.00000877
Iteration 68/1000 | Loss: 0.00000877
Iteration 69/1000 | Loss: 0.00000876
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000875
Iteration 73/1000 | Loss: 0.00000875
Iteration 74/1000 | Loss: 0.00000874
Iteration 75/1000 | Loss: 0.00000874
Iteration 76/1000 | Loss: 0.00000874
Iteration 77/1000 | Loss: 0.00000874
Iteration 78/1000 | Loss: 0.00000874
Iteration 79/1000 | Loss: 0.00000874
Iteration 80/1000 | Loss: 0.00000874
Iteration 81/1000 | Loss: 0.00000873
Iteration 82/1000 | Loss: 0.00000873
Iteration 83/1000 | Loss: 0.00000873
Iteration 84/1000 | Loss: 0.00000872
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000872
Iteration 88/1000 | Loss: 0.00000872
Iteration 89/1000 | Loss: 0.00000872
Iteration 90/1000 | Loss: 0.00000872
Iteration 91/1000 | Loss: 0.00000872
Iteration 92/1000 | Loss: 0.00000872
Iteration 93/1000 | Loss: 0.00000871
Iteration 94/1000 | Loss: 0.00000871
Iteration 95/1000 | Loss: 0.00000871
Iteration 96/1000 | Loss: 0.00000870
Iteration 97/1000 | Loss: 0.00000870
Iteration 98/1000 | Loss: 0.00000869
Iteration 99/1000 | Loss: 0.00000869
Iteration 100/1000 | Loss: 0.00000869
Iteration 101/1000 | Loss: 0.00000869
Iteration 102/1000 | Loss: 0.00000869
Iteration 103/1000 | Loss: 0.00000868
Iteration 104/1000 | Loss: 0.00000868
Iteration 105/1000 | Loss: 0.00000868
Iteration 106/1000 | Loss: 0.00000868
Iteration 107/1000 | Loss: 0.00000868
Iteration 108/1000 | Loss: 0.00000868
Iteration 109/1000 | Loss: 0.00000868
Iteration 110/1000 | Loss: 0.00000867
Iteration 111/1000 | Loss: 0.00000867
Iteration 112/1000 | Loss: 0.00000867
Iteration 113/1000 | Loss: 0.00000867
Iteration 114/1000 | Loss: 0.00000867
Iteration 115/1000 | Loss: 0.00000867
Iteration 116/1000 | Loss: 0.00000866
Iteration 117/1000 | Loss: 0.00000866
Iteration 118/1000 | Loss: 0.00000866
Iteration 119/1000 | Loss: 0.00000866
Iteration 120/1000 | Loss: 0.00000866
Iteration 121/1000 | Loss: 0.00000866
Iteration 122/1000 | Loss: 0.00000866
Iteration 123/1000 | Loss: 0.00000866
Iteration 124/1000 | Loss: 0.00000866
Iteration 125/1000 | Loss: 0.00000866
Iteration 126/1000 | Loss: 0.00000866
Iteration 127/1000 | Loss: 0.00000866
Iteration 128/1000 | Loss: 0.00000866
Iteration 129/1000 | Loss: 0.00000866
Iteration 130/1000 | Loss: 0.00000866
Iteration 131/1000 | Loss: 0.00000866
Iteration 132/1000 | Loss: 0.00000866
Iteration 133/1000 | Loss: 0.00000866
Iteration 134/1000 | Loss: 0.00000866
Iteration 135/1000 | Loss: 0.00000866
Iteration 136/1000 | Loss: 0.00000866
Iteration 137/1000 | Loss: 0.00000866
Iteration 138/1000 | Loss: 0.00000866
Iteration 139/1000 | Loss: 0.00000866
Iteration 140/1000 | Loss: 0.00000866
Iteration 141/1000 | Loss: 0.00000866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [8.658906153868884e-06, 8.658906153868884e-06, 8.658906153868884e-06, 8.658906153868884e-06, 8.658906153868884e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.658906153868884e-06

Optimization complete. Final v2v error: 2.4958810806274414 mm

Highest mean error: 2.8541059494018555 mm for frame 98

Lowest mean error: 2.2560949325561523 mm for frame 245

Saving results

Total time: 38.407660245895386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125779
Iteration 2/25 | Loss: 0.00217356
Iteration 3/25 | Loss: 0.00161535
Iteration 4/25 | Loss: 0.00147709
Iteration 5/25 | Loss: 0.00148885
Iteration 6/25 | Loss: 0.00144321
Iteration 7/25 | Loss: 0.00142756
Iteration 8/25 | Loss: 0.00137641
Iteration 9/25 | Loss: 0.00133240
Iteration 10/25 | Loss: 0.00131370
Iteration 11/25 | Loss: 0.00130623
Iteration 12/25 | Loss: 0.00130168
Iteration 13/25 | Loss: 0.00129761
Iteration 14/25 | Loss: 0.00129820
Iteration 15/25 | Loss: 0.00129787
Iteration 16/25 | Loss: 0.00129676
Iteration 17/25 | Loss: 0.00129749
Iteration 18/25 | Loss: 0.00129706
Iteration 19/25 | Loss: 0.00129661
Iteration 20/25 | Loss: 0.00129942
Iteration 21/25 | Loss: 0.00130022
Iteration 22/25 | Loss: 0.00129973
Iteration 23/25 | Loss: 0.00129987
Iteration 24/25 | Loss: 0.00129970
Iteration 25/25 | Loss: 0.00130043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30233753
Iteration 2/25 | Loss: 0.00173952
Iteration 3/25 | Loss: 0.00173949
Iteration 4/25 | Loss: 0.00173949
Iteration 5/25 | Loss: 0.00173949
Iteration 6/25 | Loss: 0.00173949
Iteration 7/25 | Loss: 0.00173949
Iteration 8/25 | Loss: 0.00173949
Iteration 9/25 | Loss: 0.00173949
Iteration 10/25 | Loss: 0.00173949
Iteration 11/25 | Loss: 0.00173949
Iteration 12/25 | Loss: 0.00173949
Iteration 13/25 | Loss: 0.00173949
Iteration 14/25 | Loss: 0.00173949
Iteration 15/25 | Loss: 0.00173949
Iteration 16/25 | Loss: 0.00173949
Iteration 17/25 | Loss: 0.00173949
Iteration 18/25 | Loss: 0.00173949
Iteration 19/25 | Loss: 0.00173949
Iteration 20/25 | Loss: 0.00173949
Iteration 21/25 | Loss: 0.00173949
Iteration 22/25 | Loss: 0.00173949
Iteration 23/25 | Loss: 0.00173949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017394894966855645, 0.0017394894966855645, 0.0017394894966855645, 0.0017394894966855645, 0.0017394894966855645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017394894966855645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173949
Iteration 2/1000 | Loss: 0.00024456
Iteration 3/1000 | Loss: 0.00200442
Iteration 4/1000 | Loss: 0.00149272
Iteration 5/1000 | Loss: 0.00121423
Iteration 6/1000 | Loss: 0.00104786
Iteration 7/1000 | Loss: 0.00023237
Iteration 8/1000 | Loss: 0.00074562
Iteration 9/1000 | Loss: 0.00105030
Iteration 10/1000 | Loss: 0.00038911
Iteration 11/1000 | Loss: 0.00013521
Iteration 12/1000 | Loss: 0.00065410
Iteration 13/1000 | Loss: 0.00009240
Iteration 14/1000 | Loss: 0.00008320
Iteration 15/1000 | Loss: 0.00007756
Iteration 16/1000 | Loss: 0.00015576
Iteration 17/1000 | Loss: 0.00053913
Iteration 18/1000 | Loss: 0.00020308
Iteration 19/1000 | Loss: 0.00035699
Iteration 20/1000 | Loss: 0.00031114
Iteration 21/1000 | Loss: 0.00037820
Iteration 22/1000 | Loss: 0.00031412
Iteration 23/1000 | Loss: 0.00023782
Iteration 24/1000 | Loss: 0.00023335
Iteration 25/1000 | Loss: 0.00029239
Iteration 26/1000 | Loss: 0.00016821
Iteration 27/1000 | Loss: 0.00010712
Iteration 28/1000 | Loss: 0.00027317
Iteration 29/1000 | Loss: 0.00031106
Iteration 30/1000 | Loss: 0.00019421
Iteration 31/1000 | Loss: 0.00024325
Iteration 32/1000 | Loss: 0.00026921
Iteration 33/1000 | Loss: 0.00038498
Iteration 34/1000 | Loss: 0.00036024
Iteration 35/1000 | Loss: 0.00023208
Iteration 36/1000 | Loss: 0.00027394
Iteration 37/1000 | Loss: 0.00052321
Iteration 38/1000 | Loss: 0.00033502
Iteration 39/1000 | Loss: 0.00156432
Iteration 40/1000 | Loss: 0.00036942
Iteration 41/1000 | Loss: 0.00066835
Iteration 42/1000 | Loss: 0.00042692
Iteration 43/1000 | Loss: 0.00052898
Iteration 44/1000 | Loss: 0.00067237
Iteration 45/1000 | Loss: 0.00008304
Iteration 46/1000 | Loss: 0.00026238
Iteration 47/1000 | Loss: 0.00025817
Iteration 48/1000 | Loss: 0.00024516
Iteration 49/1000 | Loss: 0.00026443
Iteration 50/1000 | Loss: 0.00027053
Iteration 51/1000 | Loss: 0.00030116
Iteration 52/1000 | Loss: 0.00021037
Iteration 53/1000 | Loss: 0.00027111
Iteration 54/1000 | Loss: 0.00023739
Iteration 55/1000 | Loss: 0.00005895
Iteration 56/1000 | Loss: 0.00005670
Iteration 57/1000 | Loss: 0.00031239
Iteration 58/1000 | Loss: 0.00016693
Iteration 59/1000 | Loss: 0.00007280
Iteration 60/1000 | Loss: 0.00026885
Iteration 61/1000 | Loss: 0.00047765
Iteration 62/1000 | Loss: 0.00023608
Iteration 63/1000 | Loss: 0.00042470
Iteration 64/1000 | Loss: 0.00020528
Iteration 65/1000 | Loss: 0.00008811
Iteration 66/1000 | Loss: 0.00034814
Iteration 67/1000 | Loss: 0.00008894
Iteration 68/1000 | Loss: 0.00005728
Iteration 69/1000 | Loss: 0.00006194
Iteration 70/1000 | Loss: 0.00012317
Iteration 71/1000 | Loss: 0.00053042
Iteration 72/1000 | Loss: 0.00040865
Iteration 73/1000 | Loss: 0.00008116
Iteration 74/1000 | Loss: 0.00011959
Iteration 75/1000 | Loss: 0.00006608
Iteration 76/1000 | Loss: 0.00005529
Iteration 77/1000 | Loss: 0.00005984
Iteration 78/1000 | Loss: 0.00005307
Iteration 79/1000 | Loss: 0.00004929
Iteration 80/1000 | Loss: 0.00072768
Iteration 81/1000 | Loss: 0.00251350
Iteration 82/1000 | Loss: 0.00169483
Iteration 83/1000 | Loss: 0.00284973
Iteration 84/1000 | Loss: 0.00097850
Iteration 85/1000 | Loss: 0.00071874
Iteration 86/1000 | Loss: 0.00051536
Iteration 87/1000 | Loss: 0.00095459
Iteration 88/1000 | Loss: 0.00106909
Iteration 89/1000 | Loss: 0.00198668
Iteration 90/1000 | Loss: 0.00059728
Iteration 91/1000 | Loss: 0.00092701
Iteration 92/1000 | Loss: 0.00077584
Iteration 93/1000 | Loss: 0.00066536
Iteration 94/1000 | Loss: 0.00103219
Iteration 95/1000 | Loss: 0.00100951
Iteration 96/1000 | Loss: 0.00098560
Iteration 97/1000 | Loss: 0.00126853
Iteration 98/1000 | Loss: 0.00077575
Iteration 99/1000 | Loss: 0.00091466
Iteration 100/1000 | Loss: 0.00029958
Iteration 101/1000 | Loss: 0.00042735
Iteration 102/1000 | Loss: 0.00006417
Iteration 103/1000 | Loss: 0.00006088
Iteration 104/1000 | Loss: 0.00004808
Iteration 105/1000 | Loss: 0.00020233
Iteration 106/1000 | Loss: 0.00019022
Iteration 107/1000 | Loss: 0.00024389
Iteration 108/1000 | Loss: 0.00024453
Iteration 109/1000 | Loss: 0.00030910
Iteration 110/1000 | Loss: 0.00005110
Iteration 111/1000 | Loss: 0.00004147
Iteration 112/1000 | Loss: 0.00003853
Iteration 113/1000 | Loss: 0.00023063
Iteration 114/1000 | Loss: 0.00027063
Iteration 115/1000 | Loss: 0.00023389
Iteration 116/1000 | Loss: 0.00024492
Iteration 117/1000 | Loss: 0.00023078
Iteration 118/1000 | Loss: 0.00004471
Iteration 119/1000 | Loss: 0.00003809
Iteration 120/1000 | Loss: 0.00003518
Iteration 121/1000 | Loss: 0.00004719
Iteration 122/1000 | Loss: 0.00004480
Iteration 123/1000 | Loss: 0.00004015
Iteration 124/1000 | Loss: 0.00003433
Iteration 125/1000 | Loss: 0.00055047
Iteration 126/1000 | Loss: 0.00003819
Iteration 127/1000 | Loss: 0.00003291
Iteration 128/1000 | Loss: 0.00003194
Iteration 129/1000 | Loss: 0.00003146
Iteration 130/1000 | Loss: 0.00003109
Iteration 131/1000 | Loss: 0.00006539
Iteration 132/1000 | Loss: 0.00080683
Iteration 133/1000 | Loss: 0.00054143
Iteration 134/1000 | Loss: 0.00066299
Iteration 135/1000 | Loss: 0.00022151
Iteration 136/1000 | Loss: 0.00003741
Iteration 137/1000 | Loss: 0.00016357
Iteration 138/1000 | Loss: 0.00082030
Iteration 139/1000 | Loss: 0.00039747
Iteration 140/1000 | Loss: 0.00108190
Iteration 141/1000 | Loss: 0.00086185
Iteration 142/1000 | Loss: 0.00020880
Iteration 143/1000 | Loss: 0.00019463
Iteration 144/1000 | Loss: 0.00033248
Iteration 145/1000 | Loss: 0.00022638
Iteration 146/1000 | Loss: 0.00005375
Iteration 147/1000 | Loss: 0.00025222
Iteration 148/1000 | Loss: 0.00031232
Iteration 149/1000 | Loss: 0.00026710
Iteration 150/1000 | Loss: 0.00032895
Iteration 151/1000 | Loss: 0.00031564
Iteration 152/1000 | Loss: 0.00021135
Iteration 153/1000 | Loss: 0.00008805
Iteration 154/1000 | Loss: 0.00025945
Iteration 155/1000 | Loss: 0.00030147
Iteration 156/1000 | Loss: 0.00032801
Iteration 157/1000 | Loss: 0.00005485
Iteration 158/1000 | Loss: 0.00004029
Iteration 159/1000 | Loss: 0.00003442
Iteration 160/1000 | Loss: 0.00003202
Iteration 161/1000 | Loss: 0.00003095
Iteration 162/1000 | Loss: 0.00002983
Iteration 163/1000 | Loss: 0.00002915
Iteration 164/1000 | Loss: 0.00002875
Iteration 165/1000 | Loss: 0.00002827
Iteration 166/1000 | Loss: 0.00002793
Iteration 167/1000 | Loss: 0.00003065
Iteration 168/1000 | Loss: 0.00002780
Iteration 169/1000 | Loss: 0.00002746
Iteration 170/1000 | Loss: 0.00002729
Iteration 171/1000 | Loss: 0.00002711
Iteration 172/1000 | Loss: 0.00002693
Iteration 173/1000 | Loss: 0.00002681
Iteration 174/1000 | Loss: 0.00002664
Iteration 175/1000 | Loss: 0.00002659
Iteration 176/1000 | Loss: 0.00002642
Iteration 177/1000 | Loss: 0.00002640
Iteration 178/1000 | Loss: 0.00002639
Iteration 179/1000 | Loss: 0.00002639
Iteration 180/1000 | Loss: 0.00002639
Iteration 181/1000 | Loss: 0.00002639
Iteration 182/1000 | Loss: 0.00002638
Iteration 183/1000 | Loss: 0.00002638
Iteration 184/1000 | Loss: 0.00002637
Iteration 185/1000 | Loss: 0.00002636
Iteration 186/1000 | Loss: 0.00002636
Iteration 187/1000 | Loss: 0.00002636
Iteration 188/1000 | Loss: 0.00002635
Iteration 189/1000 | Loss: 0.00002630
Iteration 190/1000 | Loss: 0.00002630
Iteration 191/1000 | Loss: 0.00002630
Iteration 192/1000 | Loss: 0.00002630
Iteration 193/1000 | Loss: 0.00002629
Iteration 194/1000 | Loss: 0.00002629
Iteration 195/1000 | Loss: 0.00002629
Iteration 196/1000 | Loss: 0.00002628
Iteration 197/1000 | Loss: 0.00002628
Iteration 198/1000 | Loss: 0.00002628
Iteration 199/1000 | Loss: 0.00002627
Iteration 200/1000 | Loss: 0.00002627
Iteration 201/1000 | Loss: 0.00002627
Iteration 202/1000 | Loss: 0.00002627
Iteration 203/1000 | Loss: 0.00002627
Iteration 204/1000 | Loss: 0.00002626
Iteration 205/1000 | Loss: 0.00002626
Iteration 206/1000 | Loss: 0.00002626
Iteration 207/1000 | Loss: 0.00002625
Iteration 208/1000 | Loss: 0.00002625
Iteration 209/1000 | Loss: 0.00002624
Iteration 210/1000 | Loss: 0.00002624
Iteration 211/1000 | Loss: 0.00002624
Iteration 212/1000 | Loss: 0.00002623
Iteration 213/1000 | Loss: 0.00002622
Iteration 214/1000 | Loss: 0.00002620
Iteration 215/1000 | Loss: 0.00002620
Iteration 216/1000 | Loss: 0.00002619
Iteration 217/1000 | Loss: 0.00002619
Iteration 218/1000 | Loss: 0.00002619
Iteration 219/1000 | Loss: 0.00002619
Iteration 220/1000 | Loss: 0.00002619
Iteration 221/1000 | Loss: 0.00002619
Iteration 222/1000 | Loss: 0.00002619
Iteration 223/1000 | Loss: 0.00002619
Iteration 224/1000 | Loss: 0.00002619
Iteration 225/1000 | Loss: 0.00002619
Iteration 226/1000 | Loss: 0.00002619
Iteration 227/1000 | Loss: 0.00002619
Iteration 228/1000 | Loss: 0.00002618
Iteration 229/1000 | Loss: 0.00002618
Iteration 230/1000 | Loss: 0.00002618
Iteration 231/1000 | Loss: 0.00002618
Iteration 232/1000 | Loss: 0.00002617
Iteration 233/1000 | Loss: 0.00002617
Iteration 234/1000 | Loss: 0.00002617
Iteration 235/1000 | Loss: 0.00002617
Iteration 236/1000 | Loss: 0.00002617
Iteration 237/1000 | Loss: 0.00002616
Iteration 238/1000 | Loss: 0.00002616
Iteration 239/1000 | Loss: 0.00002616
Iteration 240/1000 | Loss: 0.00002615
Iteration 241/1000 | Loss: 0.00002615
Iteration 242/1000 | Loss: 0.00002613
Iteration 243/1000 | Loss: 0.00002612
Iteration 244/1000 | Loss: 0.00002612
Iteration 245/1000 | Loss: 0.00002612
Iteration 246/1000 | Loss: 0.00002612
Iteration 247/1000 | Loss: 0.00002612
Iteration 248/1000 | Loss: 0.00002611
Iteration 249/1000 | Loss: 0.00002611
Iteration 250/1000 | Loss: 0.00002611
Iteration 251/1000 | Loss: 0.00002610
Iteration 252/1000 | Loss: 0.00002610
Iteration 253/1000 | Loss: 0.00002610
Iteration 254/1000 | Loss: 0.00002609
Iteration 255/1000 | Loss: 0.00002609
Iteration 256/1000 | Loss: 0.00002609
Iteration 257/1000 | Loss: 0.00002608
Iteration 258/1000 | Loss: 0.00002608
Iteration 259/1000 | Loss: 0.00002607
Iteration 260/1000 | Loss: 0.00002607
Iteration 261/1000 | Loss: 0.00002607
Iteration 262/1000 | Loss: 0.00002606
Iteration 263/1000 | Loss: 0.00002606
Iteration 264/1000 | Loss: 0.00002605
Iteration 265/1000 | Loss: 0.00002605
Iteration 266/1000 | Loss: 0.00002603
Iteration 267/1000 | Loss: 0.00002603
Iteration 268/1000 | Loss: 0.00002603
Iteration 269/1000 | Loss: 0.00002602
Iteration 270/1000 | Loss: 0.00002602
Iteration 271/1000 | Loss: 0.00002602
Iteration 272/1000 | Loss: 0.00002602
Iteration 273/1000 | Loss: 0.00002602
Iteration 274/1000 | Loss: 0.00002602
Iteration 275/1000 | Loss: 0.00002602
Iteration 276/1000 | Loss: 0.00002602
Iteration 277/1000 | Loss: 0.00002601
Iteration 278/1000 | Loss: 0.00002601
Iteration 279/1000 | Loss: 0.00002601
Iteration 280/1000 | Loss: 0.00002601
Iteration 281/1000 | Loss: 0.00002601
Iteration 282/1000 | Loss: 0.00002601
Iteration 283/1000 | Loss: 0.00002601
Iteration 284/1000 | Loss: 0.00002601
Iteration 285/1000 | Loss: 0.00002601
Iteration 286/1000 | Loss: 0.00002601
Iteration 287/1000 | Loss: 0.00002601
Iteration 288/1000 | Loss: 0.00002600
Iteration 289/1000 | Loss: 0.00002600
Iteration 290/1000 | Loss: 0.00002600
Iteration 291/1000 | Loss: 0.00002600
Iteration 292/1000 | Loss: 0.00002600
Iteration 293/1000 | Loss: 0.00002600
Iteration 294/1000 | Loss: 0.00002600
Iteration 295/1000 | Loss: 0.00002600
Iteration 296/1000 | Loss: 0.00002600
Iteration 297/1000 | Loss: 0.00002600
Iteration 298/1000 | Loss: 0.00002600
Iteration 299/1000 | Loss: 0.00002600
Iteration 300/1000 | Loss: 0.00002600
Iteration 301/1000 | Loss: 0.00002600
Iteration 302/1000 | Loss: 0.00002600
Iteration 303/1000 | Loss: 0.00002599
Iteration 304/1000 | Loss: 0.00002599
Iteration 305/1000 | Loss: 0.00002599
Iteration 306/1000 | Loss: 0.00002599
Iteration 307/1000 | Loss: 0.00002599
Iteration 308/1000 | Loss: 0.00002599
Iteration 309/1000 | Loss: 0.00002599
Iteration 310/1000 | Loss: 0.00002598
Iteration 311/1000 | Loss: 0.00002598
Iteration 312/1000 | Loss: 0.00002598
Iteration 313/1000 | Loss: 0.00002598
Iteration 314/1000 | Loss: 0.00002598
Iteration 315/1000 | Loss: 0.00002598
Iteration 316/1000 | Loss: 0.00002598
Iteration 317/1000 | Loss: 0.00002598
Iteration 318/1000 | Loss: 0.00002598
Iteration 319/1000 | Loss: 0.00002598
Iteration 320/1000 | Loss: 0.00002598
Iteration 321/1000 | Loss: 0.00002598
Iteration 322/1000 | Loss: 0.00002598
Iteration 323/1000 | Loss: 0.00002598
Iteration 324/1000 | Loss: 0.00002597
Iteration 325/1000 | Loss: 0.00002597
Iteration 326/1000 | Loss: 0.00002597
Iteration 327/1000 | Loss: 0.00002597
Iteration 328/1000 | Loss: 0.00002597
Iteration 329/1000 | Loss: 0.00002597
Iteration 330/1000 | Loss: 0.00002597
Iteration 331/1000 | Loss: 0.00002597
Iteration 332/1000 | Loss: 0.00002597
Iteration 333/1000 | Loss: 0.00002597
Iteration 334/1000 | Loss: 0.00002597
Iteration 335/1000 | Loss: 0.00002597
Iteration 336/1000 | Loss: 0.00002597
Iteration 337/1000 | Loss: 0.00002597
Iteration 338/1000 | Loss: 0.00002597
Iteration 339/1000 | Loss: 0.00002597
Iteration 340/1000 | Loss: 0.00002597
Iteration 341/1000 | Loss: 0.00002597
Iteration 342/1000 | Loss: 0.00002597
Iteration 343/1000 | Loss: 0.00002597
Iteration 344/1000 | Loss: 0.00002597
Iteration 345/1000 | Loss: 0.00002597
Iteration 346/1000 | Loss: 0.00002597
Iteration 347/1000 | Loss: 0.00002597
Iteration 348/1000 | Loss: 0.00002597
Iteration 349/1000 | Loss: 0.00002597
Iteration 350/1000 | Loss: 0.00002597
Iteration 351/1000 | Loss: 0.00002597
Iteration 352/1000 | Loss: 0.00002597
Iteration 353/1000 | Loss: 0.00002597
Iteration 354/1000 | Loss: 0.00002597
Iteration 355/1000 | Loss: 0.00002597
Iteration 356/1000 | Loss: 0.00002597
Iteration 357/1000 | Loss: 0.00002597
Iteration 358/1000 | Loss: 0.00002597
Iteration 359/1000 | Loss: 0.00002597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 359. Stopping optimization.
Last 5 losses: [2.5971870854846202e-05, 2.5971870854846202e-05, 2.5971870854846202e-05, 2.5971870854846202e-05, 2.5971870854846202e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5971870854846202e-05

Optimization complete. Final v2v error: 4.134432792663574 mm

Highest mean error: 5.274432182312012 mm for frame 89

Lowest mean error: 3.257948637008667 mm for frame 34

Saving results

Total time: 352.20345640182495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496218
Iteration 2/25 | Loss: 0.00143899
Iteration 3/25 | Loss: 0.00112013
Iteration 4/25 | Loss: 0.00109525
Iteration 5/25 | Loss: 0.00108725
Iteration 6/25 | Loss: 0.00108498
Iteration 7/25 | Loss: 0.00108498
Iteration 8/25 | Loss: 0.00108498
Iteration 9/25 | Loss: 0.00108498
Iteration 10/25 | Loss: 0.00108498
Iteration 11/25 | Loss: 0.00108498
Iteration 12/25 | Loss: 0.00108498
Iteration 13/25 | Loss: 0.00108498
Iteration 14/25 | Loss: 0.00108498
Iteration 15/25 | Loss: 0.00108498
Iteration 16/25 | Loss: 0.00108498
Iteration 17/25 | Loss: 0.00108498
Iteration 18/25 | Loss: 0.00108498
Iteration 19/25 | Loss: 0.00108498
Iteration 20/25 | Loss: 0.00108498
Iteration 21/25 | Loss: 0.00108498
Iteration 22/25 | Loss: 0.00108498
Iteration 23/25 | Loss: 0.00108498
Iteration 24/25 | Loss: 0.00108498
Iteration 25/25 | Loss: 0.00108498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64310110
Iteration 2/25 | Loss: 0.00081391
Iteration 3/25 | Loss: 0.00081391
Iteration 4/25 | Loss: 0.00081391
Iteration 5/25 | Loss: 0.00081391
Iteration 6/25 | Loss: 0.00081391
Iteration 7/25 | Loss: 0.00081391
Iteration 8/25 | Loss: 0.00081391
Iteration 9/25 | Loss: 0.00081391
Iteration 10/25 | Loss: 0.00081391
Iteration 11/25 | Loss: 0.00081391
Iteration 12/25 | Loss: 0.00081391
Iteration 13/25 | Loss: 0.00081391
Iteration 14/25 | Loss: 0.00081391
Iteration 15/25 | Loss: 0.00081391
Iteration 16/25 | Loss: 0.00081391
Iteration 17/25 | Loss: 0.00081391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008139059063978493, 0.0008139059063978493, 0.0008139059063978493, 0.0008139059063978493, 0.0008139059063978493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139059063978493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081391
Iteration 2/1000 | Loss: 0.00003902
Iteration 3/1000 | Loss: 0.00002770
Iteration 4/1000 | Loss: 0.00002490
Iteration 5/1000 | Loss: 0.00002365
Iteration 6/1000 | Loss: 0.00002279
Iteration 7/1000 | Loss: 0.00002193
Iteration 8/1000 | Loss: 0.00002140
Iteration 9/1000 | Loss: 0.00002091
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002023
Iteration 12/1000 | Loss: 0.00001994
Iteration 13/1000 | Loss: 0.00001972
Iteration 14/1000 | Loss: 0.00001958
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001924
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001909
Iteration 19/1000 | Loss: 0.00001895
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001865
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001854
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001846
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001843
Iteration 50/1000 | Loss: 0.00001843
Iteration 51/1000 | Loss: 0.00001843
Iteration 52/1000 | Loss: 0.00001842
Iteration 53/1000 | Loss: 0.00001842
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001841
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001840
Iteration 66/1000 | Loss: 0.00001840
Iteration 67/1000 | Loss: 0.00001840
Iteration 68/1000 | Loss: 0.00001840
Iteration 69/1000 | Loss: 0.00001840
Iteration 70/1000 | Loss: 0.00001840
Iteration 71/1000 | Loss: 0.00001840
Iteration 72/1000 | Loss: 0.00001840
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.8404014554107562e-05, 1.8404014554107562e-05, 1.8404014554107562e-05, 1.8404014554107562e-05, 1.8404014554107562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8404014554107562e-05

Optimization complete. Final v2v error: 3.5602002143859863 mm

Highest mean error: 3.896695137023926 mm for frame 245

Lowest mean error: 3.4187121391296387 mm for frame 208

Saving results

Total time: 51.97891664505005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00776360
Iteration 2/25 | Loss: 0.00108813
Iteration 3/25 | Loss: 0.00098098
Iteration 4/25 | Loss: 0.00097215
Iteration 5/25 | Loss: 0.00097138
Iteration 6/25 | Loss: 0.00097138
Iteration 7/25 | Loss: 0.00097138
Iteration 8/25 | Loss: 0.00097138
Iteration 9/25 | Loss: 0.00097138
Iteration 10/25 | Loss: 0.00097138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000971380271948874, 0.000971380271948874, 0.000971380271948874, 0.000971380271948874, 0.000971380271948874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000971380271948874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38283598
Iteration 2/25 | Loss: 0.00061517
Iteration 3/25 | Loss: 0.00061517
Iteration 4/25 | Loss: 0.00061517
Iteration 5/25 | Loss: 0.00061517
Iteration 6/25 | Loss: 0.00061516
Iteration 7/25 | Loss: 0.00061516
Iteration 8/25 | Loss: 0.00061516
Iteration 9/25 | Loss: 0.00061516
Iteration 10/25 | Loss: 0.00061516
Iteration 11/25 | Loss: 0.00061516
Iteration 12/25 | Loss: 0.00061516
Iteration 13/25 | Loss: 0.00061516
Iteration 14/25 | Loss: 0.00061516
Iteration 15/25 | Loss: 0.00061516
Iteration 16/25 | Loss: 0.00061516
Iteration 17/25 | Loss: 0.00061516
Iteration 18/25 | Loss: 0.00061516
Iteration 19/25 | Loss: 0.00061516
Iteration 20/25 | Loss: 0.00061516
Iteration 21/25 | Loss: 0.00061516
Iteration 22/25 | Loss: 0.00061516
Iteration 23/25 | Loss: 0.00061516
Iteration 24/25 | Loss: 0.00061516
Iteration 25/25 | Loss: 0.00061516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061516
Iteration 2/1000 | Loss: 0.00002122
Iteration 3/1000 | Loss: 0.00001339
Iteration 4/1000 | Loss: 0.00001154
Iteration 5/1000 | Loss: 0.00001065
Iteration 6/1000 | Loss: 0.00001010
Iteration 7/1000 | Loss: 0.00000979
Iteration 8/1000 | Loss: 0.00000949
Iteration 9/1000 | Loss: 0.00000922
Iteration 10/1000 | Loss: 0.00000904
Iteration 11/1000 | Loss: 0.00000901
Iteration 12/1000 | Loss: 0.00000891
Iteration 13/1000 | Loss: 0.00000890
Iteration 14/1000 | Loss: 0.00000890
Iteration 15/1000 | Loss: 0.00000888
Iteration 16/1000 | Loss: 0.00000882
Iteration 17/1000 | Loss: 0.00000879
Iteration 18/1000 | Loss: 0.00000878
Iteration 19/1000 | Loss: 0.00000877
Iteration 20/1000 | Loss: 0.00000877
Iteration 21/1000 | Loss: 0.00000877
Iteration 22/1000 | Loss: 0.00000876
Iteration 23/1000 | Loss: 0.00000875
Iteration 24/1000 | Loss: 0.00000875
Iteration 25/1000 | Loss: 0.00000874
Iteration 26/1000 | Loss: 0.00000873
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000873
Iteration 29/1000 | Loss: 0.00000872
Iteration 30/1000 | Loss: 0.00000872
Iteration 31/1000 | Loss: 0.00000872
Iteration 32/1000 | Loss: 0.00000871
Iteration 33/1000 | Loss: 0.00000871
Iteration 34/1000 | Loss: 0.00000870
Iteration 35/1000 | Loss: 0.00000869
Iteration 36/1000 | Loss: 0.00000869
Iteration 37/1000 | Loss: 0.00000869
Iteration 38/1000 | Loss: 0.00000869
Iteration 39/1000 | Loss: 0.00000868
Iteration 40/1000 | Loss: 0.00000868
Iteration 41/1000 | Loss: 0.00000868
Iteration 42/1000 | Loss: 0.00000868
Iteration 43/1000 | Loss: 0.00000868
Iteration 44/1000 | Loss: 0.00000866
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000866
Iteration 47/1000 | Loss: 0.00000866
Iteration 48/1000 | Loss: 0.00000865
Iteration 49/1000 | Loss: 0.00000865
Iteration 50/1000 | Loss: 0.00000865
Iteration 51/1000 | Loss: 0.00000865
Iteration 52/1000 | Loss: 0.00000865
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000865
Iteration 55/1000 | Loss: 0.00000864
Iteration 56/1000 | Loss: 0.00000864
Iteration 57/1000 | Loss: 0.00000864
Iteration 58/1000 | Loss: 0.00000863
Iteration 59/1000 | Loss: 0.00000863
Iteration 60/1000 | Loss: 0.00000863
Iteration 61/1000 | Loss: 0.00000863
Iteration 62/1000 | Loss: 0.00000861
Iteration 63/1000 | Loss: 0.00000860
Iteration 64/1000 | Loss: 0.00000858
Iteration 65/1000 | Loss: 0.00000857
Iteration 66/1000 | Loss: 0.00000855
Iteration 67/1000 | Loss: 0.00000855
Iteration 68/1000 | Loss: 0.00000854
Iteration 69/1000 | Loss: 0.00000854
Iteration 70/1000 | Loss: 0.00000853
Iteration 71/1000 | Loss: 0.00000853
Iteration 72/1000 | Loss: 0.00000853
Iteration 73/1000 | Loss: 0.00000853
Iteration 74/1000 | Loss: 0.00000853
Iteration 75/1000 | Loss: 0.00000853
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000852
Iteration 78/1000 | Loss: 0.00000852
Iteration 79/1000 | Loss: 0.00000851
Iteration 80/1000 | Loss: 0.00000849
Iteration 81/1000 | Loss: 0.00000849
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000848
Iteration 85/1000 | Loss: 0.00000848
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000847
Iteration 88/1000 | Loss: 0.00000847
Iteration 89/1000 | Loss: 0.00000846
Iteration 90/1000 | Loss: 0.00000845
Iteration 91/1000 | Loss: 0.00000845
Iteration 92/1000 | Loss: 0.00000845
Iteration 93/1000 | Loss: 0.00000844
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000842
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000841
Iteration 99/1000 | Loss: 0.00000841
Iteration 100/1000 | Loss: 0.00000840
Iteration 101/1000 | Loss: 0.00000840
Iteration 102/1000 | Loss: 0.00000840
Iteration 103/1000 | Loss: 0.00000840
Iteration 104/1000 | Loss: 0.00000839
Iteration 105/1000 | Loss: 0.00000839
Iteration 106/1000 | Loss: 0.00000839
Iteration 107/1000 | Loss: 0.00000839
Iteration 108/1000 | Loss: 0.00000839
Iteration 109/1000 | Loss: 0.00000838
Iteration 110/1000 | Loss: 0.00000838
Iteration 111/1000 | Loss: 0.00000838
Iteration 112/1000 | Loss: 0.00000838
Iteration 113/1000 | Loss: 0.00000838
Iteration 114/1000 | Loss: 0.00000838
Iteration 115/1000 | Loss: 0.00000837
Iteration 116/1000 | Loss: 0.00000837
Iteration 117/1000 | Loss: 0.00000837
Iteration 118/1000 | Loss: 0.00000837
Iteration 119/1000 | Loss: 0.00000836
Iteration 120/1000 | Loss: 0.00000836
Iteration 121/1000 | Loss: 0.00000836
Iteration 122/1000 | Loss: 0.00000836
Iteration 123/1000 | Loss: 0.00000835
Iteration 124/1000 | Loss: 0.00000835
Iteration 125/1000 | Loss: 0.00000835
Iteration 126/1000 | Loss: 0.00000835
Iteration 127/1000 | Loss: 0.00000834
Iteration 128/1000 | Loss: 0.00000834
Iteration 129/1000 | Loss: 0.00000834
Iteration 130/1000 | Loss: 0.00000834
Iteration 131/1000 | Loss: 0.00000834
Iteration 132/1000 | Loss: 0.00000834
Iteration 133/1000 | Loss: 0.00000834
Iteration 134/1000 | Loss: 0.00000834
Iteration 135/1000 | Loss: 0.00000833
Iteration 136/1000 | Loss: 0.00000833
Iteration 137/1000 | Loss: 0.00000833
Iteration 138/1000 | Loss: 0.00000832
Iteration 139/1000 | Loss: 0.00000832
Iteration 140/1000 | Loss: 0.00000832
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000832
Iteration 145/1000 | Loss: 0.00000832
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000830
Iteration 152/1000 | Loss: 0.00000830
Iteration 153/1000 | Loss: 0.00000830
Iteration 154/1000 | Loss: 0.00000829
Iteration 155/1000 | Loss: 0.00000829
Iteration 156/1000 | Loss: 0.00000828
Iteration 157/1000 | Loss: 0.00000828
Iteration 158/1000 | Loss: 0.00000828
Iteration 159/1000 | Loss: 0.00000828
Iteration 160/1000 | Loss: 0.00000828
Iteration 161/1000 | Loss: 0.00000828
Iteration 162/1000 | Loss: 0.00000828
Iteration 163/1000 | Loss: 0.00000828
Iteration 164/1000 | Loss: 0.00000828
Iteration 165/1000 | Loss: 0.00000827
Iteration 166/1000 | Loss: 0.00000827
Iteration 167/1000 | Loss: 0.00000827
Iteration 168/1000 | Loss: 0.00000827
Iteration 169/1000 | Loss: 0.00000827
Iteration 170/1000 | Loss: 0.00000827
Iteration 171/1000 | Loss: 0.00000826
Iteration 172/1000 | Loss: 0.00000826
Iteration 173/1000 | Loss: 0.00000826
Iteration 174/1000 | Loss: 0.00000826
Iteration 175/1000 | Loss: 0.00000825
Iteration 176/1000 | Loss: 0.00000825
Iteration 177/1000 | Loss: 0.00000825
Iteration 178/1000 | Loss: 0.00000825
Iteration 179/1000 | Loss: 0.00000825
Iteration 180/1000 | Loss: 0.00000825
Iteration 181/1000 | Loss: 0.00000824
Iteration 182/1000 | Loss: 0.00000824
Iteration 183/1000 | Loss: 0.00000824
Iteration 184/1000 | Loss: 0.00000824
Iteration 185/1000 | Loss: 0.00000823
Iteration 186/1000 | Loss: 0.00000823
Iteration 187/1000 | Loss: 0.00000823
Iteration 188/1000 | Loss: 0.00000823
Iteration 189/1000 | Loss: 0.00000823
Iteration 190/1000 | Loss: 0.00000823
Iteration 191/1000 | Loss: 0.00000823
Iteration 192/1000 | Loss: 0.00000823
Iteration 193/1000 | Loss: 0.00000823
Iteration 194/1000 | Loss: 0.00000823
Iteration 195/1000 | Loss: 0.00000823
Iteration 196/1000 | Loss: 0.00000823
Iteration 197/1000 | Loss: 0.00000823
Iteration 198/1000 | Loss: 0.00000823
Iteration 199/1000 | Loss: 0.00000823
Iteration 200/1000 | Loss: 0.00000823
Iteration 201/1000 | Loss: 0.00000823
Iteration 202/1000 | Loss: 0.00000823
Iteration 203/1000 | Loss: 0.00000823
Iteration 204/1000 | Loss: 0.00000823
Iteration 205/1000 | Loss: 0.00000823
Iteration 206/1000 | Loss: 0.00000823
Iteration 207/1000 | Loss: 0.00000823
Iteration 208/1000 | Loss: 0.00000823
Iteration 209/1000 | Loss: 0.00000823
Iteration 210/1000 | Loss: 0.00000823
Iteration 211/1000 | Loss: 0.00000823
Iteration 212/1000 | Loss: 0.00000823
Iteration 213/1000 | Loss: 0.00000823
Iteration 214/1000 | Loss: 0.00000823
Iteration 215/1000 | Loss: 0.00000823
Iteration 216/1000 | Loss: 0.00000823
Iteration 217/1000 | Loss: 0.00000823
Iteration 218/1000 | Loss: 0.00000823
Iteration 219/1000 | Loss: 0.00000823
Iteration 220/1000 | Loss: 0.00000823
Iteration 221/1000 | Loss: 0.00000823
Iteration 222/1000 | Loss: 0.00000823
Iteration 223/1000 | Loss: 0.00000823
Iteration 224/1000 | Loss: 0.00000823
Iteration 225/1000 | Loss: 0.00000823
Iteration 226/1000 | Loss: 0.00000823
Iteration 227/1000 | Loss: 0.00000823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [8.225828423746862e-06, 8.225828423746862e-06, 8.225828423746862e-06, 8.225828423746862e-06, 8.225828423746862e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.225828423746862e-06

Optimization complete. Final v2v error: 2.4543542861938477 mm

Highest mean error: 2.6299805641174316 mm for frame 50

Lowest mean error: 2.2995729446411133 mm for frame 33

Saving results

Total time: 45.68406558036804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028749
Iteration 2/25 | Loss: 0.01028749
Iteration 3/25 | Loss: 0.01028749
Iteration 4/25 | Loss: 0.01028749
Iteration 5/25 | Loss: 0.01028749
Iteration 6/25 | Loss: 0.01028748
Iteration 7/25 | Loss: 0.01028748
Iteration 8/25 | Loss: 0.01028748
Iteration 9/25 | Loss: 0.01028748
Iteration 10/25 | Loss: 0.01028748
Iteration 11/25 | Loss: 0.01028748
Iteration 12/25 | Loss: 0.01028748
Iteration 13/25 | Loss: 0.01028747
Iteration 14/25 | Loss: 0.01028747
Iteration 15/25 | Loss: 0.01028747
Iteration 16/25 | Loss: 0.01028747
Iteration 17/25 | Loss: 0.01028747
Iteration 18/25 | Loss: 0.01028747
Iteration 19/25 | Loss: 0.01028747
Iteration 20/25 | Loss: 0.01028746
Iteration 21/25 | Loss: 0.01028746
Iteration 22/25 | Loss: 0.01028746
Iteration 23/25 | Loss: 0.01028746
Iteration 24/25 | Loss: 0.01028746
Iteration 25/25 | Loss: 0.01028746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47710323
Iteration 2/25 | Loss: 0.17051901
Iteration 3/25 | Loss: 0.17033163
Iteration 4/25 | Loss: 0.17032009
Iteration 5/25 | Loss: 0.17032005
Iteration 6/25 | Loss: 0.17032005
Iteration 7/25 | Loss: 0.17032005
Iteration 8/25 | Loss: 0.17032005
Iteration 9/25 | Loss: 0.17032005
Iteration 10/25 | Loss: 0.17032003
Iteration 11/25 | Loss: 0.17032003
Iteration 12/25 | Loss: 0.17032003
Iteration 13/25 | Loss: 0.17032003
Iteration 14/25 | Loss: 0.17032003
Iteration 15/25 | Loss: 0.17032003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.1703200340270996, 0.1703200340270996, 0.1703200340270996, 0.1703200340270996, 0.1703200340270996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1703200340270996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17032003
Iteration 2/1000 | Loss: 0.01493809
Iteration 3/1000 | Loss: 0.02413561
Iteration 4/1000 | Loss: 0.01054196
Iteration 5/1000 | Loss: 0.01109511
Iteration 6/1000 | Loss: 0.01439410
Iteration 7/1000 | Loss: 0.00354944
Iteration 8/1000 | Loss: 0.00276976
Iteration 9/1000 | Loss: 0.00277444
Iteration 10/1000 | Loss: 0.00121422
Iteration 11/1000 | Loss: 0.00093430
Iteration 12/1000 | Loss: 0.00118644
Iteration 13/1000 | Loss: 0.00013761
Iteration 14/1000 | Loss: 0.00011302
Iteration 15/1000 | Loss: 0.00009261
Iteration 16/1000 | Loss: 0.00073318
Iteration 17/1000 | Loss: 0.00010708
Iteration 18/1000 | Loss: 0.00009272
Iteration 19/1000 | Loss: 0.00007504
Iteration 20/1000 | Loss: 0.00006038
Iteration 21/1000 | Loss: 0.00007927
Iteration 22/1000 | Loss: 0.00008998
Iteration 23/1000 | Loss: 0.00007480
Iteration 24/1000 | Loss: 0.00006870
Iteration 25/1000 | Loss: 0.00008386
Iteration 26/1000 | Loss: 0.00088020
Iteration 27/1000 | Loss: 0.00101489
Iteration 28/1000 | Loss: 0.00011121
Iteration 29/1000 | Loss: 0.00006026
Iteration 30/1000 | Loss: 0.00005425
Iteration 31/1000 | Loss: 0.00005669
Iteration 32/1000 | Loss: 0.00005015
Iteration 33/1000 | Loss: 0.00005780
Iteration 34/1000 | Loss: 0.00005088
Iteration 35/1000 | Loss: 0.00004135
Iteration 36/1000 | Loss: 0.00005664
Iteration 37/1000 | Loss: 0.00022691
Iteration 38/1000 | Loss: 0.00008757
Iteration 39/1000 | Loss: 0.00007577
Iteration 40/1000 | Loss: 0.00006533
Iteration 41/1000 | Loss: 0.00003398
Iteration 42/1000 | Loss: 0.00002814
Iteration 43/1000 | Loss: 0.00003575
Iteration 44/1000 | Loss: 0.00003728
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00003317
Iteration 47/1000 | Loss: 0.00002078
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00003392
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00007918
Iteration 52/1000 | Loss: 0.00003200
Iteration 53/1000 | Loss: 0.00004206
Iteration 54/1000 | Loss: 0.00004229
Iteration 55/1000 | Loss: 0.00004164
Iteration 56/1000 | Loss: 0.00004368
Iteration 57/1000 | Loss: 0.00008378
Iteration 58/1000 | Loss: 0.00004173
Iteration 59/1000 | Loss: 0.00003540
Iteration 60/1000 | Loss: 0.00004704
Iteration 61/1000 | Loss: 0.00004345
Iteration 62/1000 | Loss: 0.00008931
Iteration 63/1000 | Loss: 0.00004984
Iteration 64/1000 | Loss: 0.00004126
Iteration 65/1000 | Loss: 0.00004309
Iteration 66/1000 | Loss: 0.00006037
Iteration 67/1000 | Loss: 0.00005024
Iteration 68/1000 | Loss: 0.00009790
Iteration 69/1000 | Loss: 0.00004909
Iteration 70/1000 | Loss: 0.00007181
Iteration 71/1000 | Loss: 0.00004586
Iteration 72/1000 | Loss: 0.00004521
Iteration 73/1000 | Loss: 0.00004450
Iteration 74/1000 | Loss: 0.00004540
Iteration 75/1000 | Loss: 0.00004502
Iteration 76/1000 | Loss: 0.00006565
Iteration 77/1000 | Loss: 0.00007431
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00001981
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00003023
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001641
Iteration 87/1000 | Loss: 0.00002992
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00003926
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00002283
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001580
Iteration 103/1000 | Loss: 0.00001580
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001576
Iteration 116/1000 | Loss: 0.00001575
Iteration 117/1000 | Loss: 0.00001575
Iteration 118/1000 | Loss: 0.00001575
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00004595
Iteration 124/1000 | Loss: 0.00001625
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001561
Iteration 128/1000 | Loss: 0.00001561
Iteration 129/1000 | Loss: 0.00001561
Iteration 130/1000 | Loss: 0.00001561
Iteration 131/1000 | Loss: 0.00001561
Iteration 132/1000 | Loss: 0.00001561
Iteration 133/1000 | Loss: 0.00001561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.561242970637977e-05, 1.561242970637977e-05, 1.561242970637977e-05, 1.561242970637977e-05, 1.561242970637977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.561242970637977e-05

Optimization complete. Final v2v error: 3.3130555152893066 mm

Highest mean error: 5.020005226135254 mm for frame 14

Lowest mean error: 2.8968966007232666 mm for frame 219

Saving results

Total time: 162.11937475204468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414404
Iteration 2/25 | Loss: 0.00112719
Iteration 3/25 | Loss: 0.00106148
Iteration 4/25 | Loss: 0.00105739
Iteration 5/25 | Loss: 0.00105739
Iteration 6/25 | Loss: 0.00105739
Iteration 7/25 | Loss: 0.00105739
Iteration 8/25 | Loss: 0.00105739
Iteration 9/25 | Loss: 0.00105739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0010573919862508774, 0.0010573919862508774, 0.0010573919862508774, 0.0010573919862508774, 0.0010573919862508774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010573919862508774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38137019
Iteration 2/25 | Loss: 0.00051303
Iteration 3/25 | Loss: 0.00051303
Iteration 4/25 | Loss: 0.00051302
Iteration 5/25 | Loss: 0.00051302
Iteration 6/25 | Loss: 0.00051302
Iteration 7/25 | Loss: 0.00051302
Iteration 8/25 | Loss: 0.00051302
Iteration 9/25 | Loss: 0.00051302
Iteration 10/25 | Loss: 0.00051302
Iteration 11/25 | Loss: 0.00051302
Iteration 12/25 | Loss: 0.00051302
Iteration 13/25 | Loss: 0.00051302
Iteration 14/25 | Loss: 0.00051302
Iteration 15/25 | Loss: 0.00051302
Iteration 16/25 | Loss: 0.00051302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005130228237248957, 0.0005130228237248957, 0.0005130228237248957, 0.0005130228237248957, 0.0005130228237248957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005130228237248957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051302
Iteration 2/1000 | Loss: 0.00002373
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001451
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001359
Iteration 10/1000 | Loss: 0.00001357
Iteration 11/1000 | Loss: 0.00001349
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001335
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001334
Iteration 19/1000 | Loss: 0.00001334
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001333
Iteration 23/1000 | Loss: 0.00001333
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001331
Iteration 28/1000 | Loss: 0.00001331
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [1.3313764611666556e-05, 1.3313764611666556e-05, 1.3313764611666556e-05, 1.3313764611666556e-05, 1.3313764611666556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3313764611666556e-05

Optimization complete. Final v2v error: 3.10719895362854 mm

Highest mean error: 3.1487300395965576 mm for frame 87

Lowest mean error: 3.076900005340576 mm for frame 55

Saving results

Total time: 24.029183626174927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030894
Iteration 2/25 | Loss: 0.01030893
Iteration 3/25 | Loss: 0.01030893
Iteration 4/25 | Loss: 0.00299632
Iteration 5/25 | Loss: 0.00195842
Iteration 6/25 | Loss: 0.00177727
Iteration 7/25 | Loss: 0.00162238
Iteration 8/25 | Loss: 0.00160014
Iteration 9/25 | Loss: 0.00152571
Iteration 10/25 | Loss: 0.00148527
Iteration 11/25 | Loss: 0.00144833
Iteration 12/25 | Loss: 0.00142283
Iteration 13/25 | Loss: 0.00141592
Iteration 14/25 | Loss: 0.00140496
Iteration 15/25 | Loss: 0.00139070
Iteration 16/25 | Loss: 0.00137706
Iteration 17/25 | Loss: 0.00136874
Iteration 18/25 | Loss: 0.00136462
Iteration 19/25 | Loss: 0.00136342
Iteration 20/25 | Loss: 0.00135851
Iteration 21/25 | Loss: 0.00135748
Iteration 22/25 | Loss: 0.00135521
Iteration 23/25 | Loss: 0.00135477
Iteration 24/25 | Loss: 0.00135452
Iteration 25/25 | Loss: 0.00135528

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32814026
Iteration 2/25 | Loss: 0.00526110
Iteration 3/25 | Loss: 0.00360651
Iteration 4/25 | Loss: 0.00360651
Iteration 5/25 | Loss: 0.00360651
Iteration 6/25 | Loss: 0.00360651
Iteration 7/25 | Loss: 0.00360651
Iteration 8/25 | Loss: 0.00360651
Iteration 9/25 | Loss: 0.00360651
Iteration 10/25 | Loss: 0.00360651
Iteration 11/25 | Loss: 0.00360651
Iteration 12/25 | Loss: 0.00360651
Iteration 13/25 | Loss: 0.00360651
Iteration 14/25 | Loss: 0.00360651
Iteration 15/25 | Loss: 0.00360651
Iteration 16/25 | Loss: 0.00360651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0036065084859728813, 0.0036065084859728813, 0.0036065084859728813, 0.0036065084859728813, 0.0036065084859728813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036065084859728813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360651
Iteration 2/1000 | Loss: 0.00124400
Iteration 3/1000 | Loss: 0.00098508
Iteration 4/1000 | Loss: 0.00215804
Iteration 5/1000 | Loss: 0.00068759
Iteration 6/1000 | Loss: 0.00046040
Iteration 7/1000 | Loss: 0.00066118
Iteration 8/1000 | Loss: 0.00038905
Iteration 9/1000 | Loss: 0.00047200
Iteration 10/1000 | Loss: 0.00164591
Iteration 11/1000 | Loss: 0.00231616
Iteration 12/1000 | Loss: 0.00155368
Iteration 13/1000 | Loss: 0.00121057
Iteration 14/1000 | Loss: 0.00059784
Iteration 15/1000 | Loss: 0.00078843
Iteration 16/1000 | Loss: 0.00024297
Iteration 17/1000 | Loss: 0.00101410
Iteration 18/1000 | Loss: 0.00095121
Iteration 19/1000 | Loss: 0.00054244
Iteration 20/1000 | Loss: 0.00042010
Iteration 21/1000 | Loss: 0.00079273
Iteration 22/1000 | Loss: 0.00066414
Iteration 23/1000 | Loss: 0.00084067
Iteration 24/1000 | Loss: 0.00098589
Iteration 25/1000 | Loss: 0.00059326
Iteration 26/1000 | Loss: 0.00091709
Iteration 27/1000 | Loss: 0.00109948
Iteration 28/1000 | Loss: 0.00082864
Iteration 29/1000 | Loss: 0.00159182
Iteration 30/1000 | Loss: 0.00073971
Iteration 31/1000 | Loss: 0.00095072
Iteration 32/1000 | Loss: 0.00052247
Iteration 33/1000 | Loss: 0.00048429
Iteration 34/1000 | Loss: 0.00053571
Iteration 35/1000 | Loss: 0.00138622
Iteration 36/1000 | Loss: 0.00170052
Iteration 37/1000 | Loss: 0.00126098
Iteration 38/1000 | Loss: 0.00148715
Iteration 39/1000 | Loss: 0.00114273
Iteration 40/1000 | Loss: 0.00080960
Iteration 41/1000 | Loss: 0.00171982
Iteration 42/1000 | Loss: 0.00128035
Iteration 43/1000 | Loss: 0.00066265
Iteration 44/1000 | Loss: 0.00180056
Iteration 45/1000 | Loss: 0.00116734
Iteration 46/1000 | Loss: 0.00139383
Iteration 47/1000 | Loss: 0.00058540
Iteration 48/1000 | Loss: 0.00021178
Iteration 49/1000 | Loss: 0.00032608
Iteration 50/1000 | Loss: 0.00046318
Iteration 51/1000 | Loss: 0.00037090
Iteration 52/1000 | Loss: 0.00016877
Iteration 53/1000 | Loss: 0.00056322
Iteration 54/1000 | Loss: 0.00027552
Iteration 55/1000 | Loss: 0.00031837
Iteration 56/1000 | Loss: 0.00054013
Iteration 57/1000 | Loss: 0.00037174
Iteration 58/1000 | Loss: 0.00029599
Iteration 59/1000 | Loss: 0.00013577
Iteration 60/1000 | Loss: 0.00020767
Iteration 61/1000 | Loss: 0.00020864
Iteration 62/1000 | Loss: 0.00030937
Iteration 63/1000 | Loss: 0.00033379
Iteration 64/1000 | Loss: 0.00060466
Iteration 65/1000 | Loss: 0.00097409
Iteration 66/1000 | Loss: 0.00128928
Iteration 67/1000 | Loss: 0.00044475
Iteration 68/1000 | Loss: 0.00031144
Iteration 69/1000 | Loss: 0.00020097
Iteration 70/1000 | Loss: 0.00021376
Iteration 71/1000 | Loss: 0.00011624
Iteration 72/1000 | Loss: 0.00011119
Iteration 73/1000 | Loss: 0.00010337
Iteration 74/1000 | Loss: 0.00016921
Iteration 75/1000 | Loss: 0.00010202
Iteration 76/1000 | Loss: 0.00010838
Iteration 77/1000 | Loss: 0.00010346
Iteration 78/1000 | Loss: 0.00010947
Iteration 79/1000 | Loss: 0.00019429
Iteration 80/1000 | Loss: 0.00080227
Iteration 81/1000 | Loss: 0.00037025
Iteration 82/1000 | Loss: 0.00022325
Iteration 83/1000 | Loss: 0.00021434
Iteration 84/1000 | Loss: 0.00015724
Iteration 85/1000 | Loss: 0.00013839
Iteration 86/1000 | Loss: 0.00009525
Iteration 87/1000 | Loss: 0.00009401
Iteration 88/1000 | Loss: 0.00025598
Iteration 89/1000 | Loss: 0.00018092
Iteration 90/1000 | Loss: 0.00009337
Iteration 91/1000 | Loss: 0.00009293
Iteration 92/1000 | Loss: 0.00019063
Iteration 93/1000 | Loss: 0.00038751
Iteration 94/1000 | Loss: 0.00021337
Iteration 95/1000 | Loss: 0.00034969
Iteration 96/1000 | Loss: 0.00044608
Iteration 97/1000 | Loss: 0.00051994
Iteration 98/1000 | Loss: 0.00009336
Iteration 99/1000 | Loss: 0.00021692
Iteration 100/1000 | Loss: 0.00016854
Iteration 101/1000 | Loss: 0.00009254
Iteration 102/1000 | Loss: 0.00012239
Iteration 103/1000 | Loss: 0.00010433
Iteration 104/1000 | Loss: 0.00011340
Iteration 105/1000 | Loss: 0.00009184
Iteration 106/1000 | Loss: 0.00009181
Iteration 107/1000 | Loss: 0.00021998
Iteration 108/1000 | Loss: 0.00081415
Iteration 109/1000 | Loss: 0.00009819
Iteration 110/1000 | Loss: 0.00013639
Iteration 111/1000 | Loss: 0.00009180
Iteration 112/1000 | Loss: 0.00012442
Iteration 113/1000 | Loss: 0.00009110
Iteration 114/1000 | Loss: 0.00009091
Iteration 115/1000 | Loss: 0.00009062
Iteration 116/1000 | Loss: 0.00009019
Iteration 117/1000 | Loss: 0.00024633
Iteration 118/1000 | Loss: 0.00009036
Iteration 119/1000 | Loss: 0.00008831
Iteration 120/1000 | Loss: 0.00026472
Iteration 121/1000 | Loss: 0.00011228
Iteration 122/1000 | Loss: 0.00017737
Iteration 123/1000 | Loss: 0.00012117
Iteration 124/1000 | Loss: 0.00015215
Iteration 125/1000 | Loss: 0.00008475
Iteration 126/1000 | Loss: 0.00008278
Iteration 127/1000 | Loss: 0.00008104
Iteration 128/1000 | Loss: 0.00007927
Iteration 129/1000 | Loss: 0.00037674
Iteration 130/1000 | Loss: 0.00007773
Iteration 131/1000 | Loss: 0.00027870
Iteration 132/1000 | Loss: 0.00116038
Iteration 133/1000 | Loss: 0.00121783
Iteration 134/1000 | Loss: 0.00089673
Iteration 135/1000 | Loss: 0.00060302
Iteration 136/1000 | Loss: 0.00084183
Iteration 137/1000 | Loss: 0.00013963
Iteration 138/1000 | Loss: 0.00013091
Iteration 139/1000 | Loss: 0.00033418
Iteration 140/1000 | Loss: 0.00005720
Iteration 141/1000 | Loss: 0.00022005
Iteration 142/1000 | Loss: 0.00012196
Iteration 143/1000 | Loss: 0.00009063
Iteration 144/1000 | Loss: 0.00004799
Iteration 145/1000 | Loss: 0.00014388
Iteration 146/1000 | Loss: 0.00003970
Iteration 147/1000 | Loss: 0.00019313
Iteration 148/1000 | Loss: 0.00003683
Iteration 149/1000 | Loss: 0.00003534
Iteration 150/1000 | Loss: 0.00003427
Iteration 151/1000 | Loss: 0.00003343
Iteration 152/1000 | Loss: 0.00003282
Iteration 153/1000 | Loss: 0.00015006
Iteration 154/1000 | Loss: 0.00003207
Iteration 155/1000 | Loss: 0.00020477
Iteration 156/1000 | Loss: 0.00006440
Iteration 157/1000 | Loss: 0.00004151
Iteration 158/1000 | Loss: 0.00003154
Iteration 159/1000 | Loss: 0.00019651
Iteration 160/1000 | Loss: 0.00008009
Iteration 161/1000 | Loss: 0.00003536
Iteration 162/1000 | Loss: 0.00019542
Iteration 163/1000 | Loss: 0.00005708
Iteration 164/1000 | Loss: 0.00003463
Iteration 165/1000 | Loss: 0.00019069
Iteration 166/1000 | Loss: 0.00004781
Iteration 167/1000 | Loss: 0.00017892
Iteration 168/1000 | Loss: 0.00003519
Iteration 169/1000 | Loss: 0.00012195
Iteration 170/1000 | Loss: 0.00013927
Iteration 171/1000 | Loss: 0.00015291
Iteration 172/1000 | Loss: 0.00015571
Iteration 173/1000 | Loss: 0.00026169
Iteration 174/1000 | Loss: 0.00015468
Iteration 175/1000 | Loss: 0.00037931
Iteration 176/1000 | Loss: 0.00022251
Iteration 177/1000 | Loss: 0.00007128
Iteration 178/1000 | Loss: 0.00003254
Iteration 179/1000 | Loss: 0.00020510
Iteration 180/1000 | Loss: 0.00016718
Iteration 181/1000 | Loss: 0.00018303
Iteration 182/1000 | Loss: 0.00024207
Iteration 183/1000 | Loss: 0.00023512
Iteration 184/1000 | Loss: 0.00016755
Iteration 185/1000 | Loss: 0.00018204
Iteration 186/1000 | Loss: 0.00027226
Iteration 187/1000 | Loss: 0.00026873
Iteration 188/1000 | Loss: 0.00015075
Iteration 189/1000 | Loss: 0.00011035
Iteration 190/1000 | Loss: 0.00004157
Iteration 191/1000 | Loss: 0.00003525
Iteration 192/1000 | Loss: 0.00006370
Iteration 193/1000 | Loss: 0.00006936
Iteration 194/1000 | Loss: 0.00003807
Iteration 195/1000 | Loss: 0.00005712
Iteration 196/1000 | Loss: 0.00003108
Iteration 197/1000 | Loss: 0.00002979
Iteration 198/1000 | Loss: 0.00014139
Iteration 199/1000 | Loss: 0.00003110
Iteration 200/1000 | Loss: 0.00002993
Iteration 201/1000 | Loss: 0.00002927
Iteration 202/1000 | Loss: 0.00002891
Iteration 203/1000 | Loss: 0.00002858
Iteration 204/1000 | Loss: 0.00002833
Iteration 205/1000 | Loss: 0.00002812
Iteration 206/1000 | Loss: 0.00016090
Iteration 207/1000 | Loss: 0.00016088
Iteration 208/1000 | Loss: 0.00010358
Iteration 209/1000 | Loss: 0.00015593
Iteration 210/1000 | Loss: 0.00004992
Iteration 211/1000 | Loss: 0.00002861
Iteration 212/1000 | Loss: 0.00002770
Iteration 213/1000 | Loss: 0.00002757
Iteration 214/1000 | Loss: 0.00002754
Iteration 215/1000 | Loss: 0.00002753
Iteration 216/1000 | Loss: 0.00002752
Iteration 217/1000 | Loss: 0.00002752
Iteration 218/1000 | Loss: 0.00002750
Iteration 219/1000 | Loss: 0.00002747
Iteration 220/1000 | Loss: 0.00002747
Iteration 221/1000 | Loss: 0.00002747
Iteration 222/1000 | Loss: 0.00002747
Iteration 223/1000 | Loss: 0.00002747
Iteration 224/1000 | Loss: 0.00002747
Iteration 225/1000 | Loss: 0.00002746
Iteration 226/1000 | Loss: 0.00002746
Iteration 227/1000 | Loss: 0.00002746
Iteration 228/1000 | Loss: 0.00002746
Iteration 229/1000 | Loss: 0.00002746
Iteration 230/1000 | Loss: 0.00002746
Iteration 231/1000 | Loss: 0.00002746
Iteration 232/1000 | Loss: 0.00002746
Iteration 233/1000 | Loss: 0.00002746
Iteration 234/1000 | Loss: 0.00002745
Iteration 235/1000 | Loss: 0.00002745
Iteration 236/1000 | Loss: 0.00002744
Iteration 237/1000 | Loss: 0.00002744
Iteration 238/1000 | Loss: 0.00002744
Iteration 239/1000 | Loss: 0.00002744
Iteration 240/1000 | Loss: 0.00002744
Iteration 241/1000 | Loss: 0.00002743
Iteration 242/1000 | Loss: 0.00002743
Iteration 243/1000 | Loss: 0.00002743
Iteration 244/1000 | Loss: 0.00002743
Iteration 245/1000 | Loss: 0.00002743
Iteration 246/1000 | Loss: 0.00002742
Iteration 247/1000 | Loss: 0.00002742
Iteration 248/1000 | Loss: 0.00002742
Iteration 249/1000 | Loss: 0.00002742
Iteration 250/1000 | Loss: 0.00002742
Iteration 251/1000 | Loss: 0.00002742
Iteration 252/1000 | Loss: 0.00002742
Iteration 253/1000 | Loss: 0.00002741
Iteration 254/1000 | Loss: 0.00002741
Iteration 255/1000 | Loss: 0.00002741
Iteration 256/1000 | Loss: 0.00002741
Iteration 257/1000 | Loss: 0.00002741
Iteration 258/1000 | Loss: 0.00002740
Iteration 259/1000 | Loss: 0.00002740
Iteration 260/1000 | Loss: 0.00002739
Iteration 261/1000 | Loss: 0.00002739
Iteration 262/1000 | Loss: 0.00002739
Iteration 263/1000 | Loss: 0.00002739
Iteration 264/1000 | Loss: 0.00002739
Iteration 265/1000 | Loss: 0.00002738
Iteration 266/1000 | Loss: 0.00002738
Iteration 267/1000 | Loss: 0.00002738
Iteration 268/1000 | Loss: 0.00002738
Iteration 269/1000 | Loss: 0.00002738
Iteration 270/1000 | Loss: 0.00002738
Iteration 271/1000 | Loss: 0.00002737
Iteration 272/1000 | Loss: 0.00002737
Iteration 273/1000 | Loss: 0.00002737
Iteration 274/1000 | Loss: 0.00002737
Iteration 275/1000 | Loss: 0.00002737
Iteration 276/1000 | Loss: 0.00002737
Iteration 277/1000 | Loss: 0.00002736
Iteration 278/1000 | Loss: 0.00002736
Iteration 279/1000 | Loss: 0.00002736
Iteration 280/1000 | Loss: 0.00002736
Iteration 281/1000 | Loss: 0.00002736
Iteration 282/1000 | Loss: 0.00002736
Iteration 283/1000 | Loss: 0.00002735
Iteration 284/1000 | Loss: 0.00002734
Iteration 285/1000 | Loss: 0.00002734
Iteration 286/1000 | Loss: 0.00002734
Iteration 287/1000 | Loss: 0.00002734
Iteration 288/1000 | Loss: 0.00002733
Iteration 289/1000 | Loss: 0.00002733
Iteration 290/1000 | Loss: 0.00002733
Iteration 291/1000 | Loss: 0.00002733
Iteration 292/1000 | Loss: 0.00002733
Iteration 293/1000 | Loss: 0.00002732
Iteration 294/1000 | Loss: 0.00002732
Iteration 295/1000 | Loss: 0.00002732
Iteration 296/1000 | Loss: 0.00002732
Iteration 297/1000 | Loss: 0.00002732
Iteration 298/1000 | Loss: 0.00002731
Iteration 299/1000 | Loss: 0.00002731
Iteration 300/1000 | Loss: 0.00002731
Iteration 301/1000 | Loss: 0.00002731
Iteration 302/1000 | Loss: 0.00002731
Iteration 303/1000 | Loss: 0.00002731
Iteration 304/1000 | Loss: 0.00002731
Iteration 305/1000 | Loss: 0.00002731
Iteration 306/1000 | Loss: 0.00002731
Iteration 307/1000 | Loss: 0.00002731
Iteration 308/1000 | Loss: 0.00002731
Iteration 309/1000 | Loss: 0.00002731
Iteration 310/1000 | Loss: 0.00002731
Iteration 311/1000 | Loss: 0.00002730
Iteration 312/1000 | Loss: 0.00002730
Iteration 313/1000 | Loss: 0.00002730
Iteration 314/1000 | Loss: 0.00002730
Iteration 315/1000 | Loss: 0.00002730
Iteration 316/1000 | Loss: 0.00002729
Iteration 317/1000 | Loss: 0.00002729
Iteration 318/1000 | Loss: 0.00002729
Iteration 319/1000 | Loss: 0.00002729
Iteration 320/1000 | Loss: 0.00002729
Iteration 321/1000 | Loss: 0.00002729
Iteration 322/1000 | Loss: 0.00002729
Iteration 323/1000 | Loss: 0.00002729
Iteration 324/1000 | Loss: 0.00002729
Iteration 325/1000 | Loss: 0.00002729
Iteration 326/1000 | Loss: 0.00002729
Iteration 327/1000 | Loss: 0.00002729
Iteration 328/1000 | Loss: 0.00002729
Iteration 329/1000 | Loss: 0.00002729
Iteration 330/1000 | Loss: 0.00002729
Iteration 331/1000 | Loss: 0.00002729
Iteration 332/1000 | Loss: 0.00002729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [2.7287527700536884e-05, 2.7287527700536884e-05, 2.7287527700536884e-05, 2.7287527700536884e-05, 2.7287527700536884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7287527700536884e-05

Optimization complete. Final v2v error: 3.374887228012085 mm

Highest mean error: 11.7224702835083 mm for frame 73

Lowest mean error: 2.7284371852874756 mm for frame 59

Saving results

Total time: 399.11789321899414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414645
Iteration 2/25 | Loss: 0.00111162
Iteration 3/25 | Loss: 0.00099920
Iteration 4/25 | Loss: 0.00099036
Iteration 5/25 | Loss: 0.00098769
Iteration 6/25 | Loss: 0.00098709
Iteration 7/25 | Loss: 0.00098709
Iteration 8/25 | Loss: 0.00098709
Iteration 9/25 | Loss: 0.00098709
Iteration 10/25 | Loss: 0.00098709
Iteration 11/25 | Loss: 0.00098709
Iteration 12/25 | Loss: 0.00098709
Iteration 13/25 | Loss: 0.00098709
Iteration 14/25 | Loss: 0.00098709
Iteration 15/25 | Loss: 0.00098709
Iteration 16/25 | Loss: 0.00098709
Iteration 17/25 | Loss: 0.00098709
Iteration 18/25 | Loss: 0.00098709
Iteration 19/25 | Loss: 0.00098709
Iteration 20/25 | Loss: 0.00098709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009870894718915224, 0.0009870894718915224, 0.0009870894718915224, 0.0009870894718915224, 0.0009870894718915224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009870894718915224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15559959
Iteration 2/25 | Loss: 0.00049567
Iteration 3/25 | Loss: 0.00049567
Iteration 4/25 | Loss: 0.00049567
Iteration 5/25 | Loss: 0.00049567
Iteration 6/25 | Loss: 0.00049567
Iteration 7/25 | Loss: 0.00049567
Iteration 8/25 | Loss: 0.00049567
Iteration 9/25 | Loss: 0.00049567
Iteration 10/25 | Loss: 0.00049567
Iteration 11/25 | Loss: 0.00049567
Iteration 12/25 | Loss: 0.00049567
Iteration 13/25 | Loss: 0.00049567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0004956656484864652, 0.0004956656484864652, 0.0004956656484864652, 0.0004956656484864652, 0.0004956656484864652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004956656484864652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049567
Iteration 2/1000 | Loss: 0.00003388
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001364
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001229
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001144
Iteration 10/1000 | Loss: 0.00001116
Iteration 11/1000 | Loss: 0.00001088
Iteration 12/1000 | Loss: 0.00001078
Iteration 13/1000 | Loss: 0.00001061
Iteration 14/1000 | Loss: 0.00001060
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001030
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001029
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001029
Iteration 30/1000 | Loss: 0.00001029
Iteration 31/1000 | Loss: 0.00001029
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001028
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001027
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001027
Iteration 45/1000 | Loss: 0.00001027
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001027
Iteration 49/1000 | Loss: 0.00001027
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001026
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001026
Iteration 54/1000 | Loss: 0.00001026
Iteration 55/1000 | Loss: 0.00001026
Iteration 56/1000 | Loss: 0.00001025
Iteration 57/1000 | Loss: 0.00001025
Iteration 58/1000 | Loss: 0.00001025
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001022
Iteration 68/1000 | Loss: 0.00001022
Iteration 69/1000 | Loss: 0.00001022
Iteration 70/1000 | Loss: 0.00001022
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001021
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001020
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001019
Iteration 98/1000 | Loss: 0.00001018
Iteration 99/1000 | Loss: 0.00001018
Iteration 100/1000 | Loss: 0.00001018
Iteration 101/1000 | Loss: 0.00001018
Iteration 102/1000 | Loss: 0.00001018
Iteration 103/1000 | Loss: 0.00001018
Iteration 104/1000 | Loss: 0.00001018
Iteration 105/1000 | Loss: 0.00001017
Iteration 106/1000 | Loss: 0.00001017
Iteration 107/1000 | Loss: 0.00001017
Iteration 108/1000 | Loss: 0.00001017
Iteration 109/1000 | Loss: 0.00001017
Iteration 110/1000 | Loss: 0.00001017
Iteration 111/1000 | Loss: 0.00001017
Iteration 112/1000 | Loss: 0.00001017
Iteration 113/1000 | Loss: 0.00001017
Iteration 114/1000 | Loss: 0.00001017
Iteration 115/1000 | Loss: 0.00001016
Iteration 116/1000 | Loss: 0.00001016
Iteration 117/1000 | Loss: 0.00001016
Iteration 118/1000 | Loss: 0.00001016
Iteration 119/1000 | Loss: 0.00001016
Iteration 120/1000 | Loss: 0.00001016
Iteration 121/1000 | Loss: 0.00001016
Iteration 122/1000 | Loss: 0.00001016
Iteration 123/1000 | Loss: 0.00001016
Iteration 124/1000 | Loss: 0.00001016
Iteration 125/1000 | Loss: 0.00001016
Iteration 126/1000 | Loss: 0.00001015
Iteration 127/1000 | Loss: 0.00001015
Iteration 128/1000 | Loss: 0.00001015
Iteration 129/1000 | Loss: 0.00001015
Iteration 130/1000 | Loss: 0.00001015
Iteration 131/1000 | Loss: 0.00001015
Iteration 132/1000 | Loss: 0.00001015
Iteration 133/1000 | Loss: 0.00001015
Iteration 134/1000 | Loss: 0.00001015
Iteration 135/1000 | Loss: 0.00001015
Iteration 136/1000 | Loss: 0.00001015
Iteration 137/1000 | Loss: 0.00001015
Iteration 138/1000 | Loss: 0.00001015
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.015361885947641e-05, 1.015361885947641e-05, 1.015361885947641e-05, 1.015361885947641e-05, 1.015361885947641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.015361885947641e-05

Optimization complete. Final v2v error: 2.7547614574432373 mm

Highest mean error: 2.794750690460205 mm for frame 14

Lowest mean error: 2.7378203868865967 mm for frame 119

Saving results

Total time: 34.295204877853394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838106
Iteration 2/25 | Loss: 0.00124129
Iteration 3/25 | Loss: 0.00106700
Iteration 4/25 | Loss: 0.00103264
Iteration 5/25 | Loss: 0.00102408
Iteration 6/25 | Loss: 0.00102286
Iteration 7/25 | Loss: 0.00102285
Iteration 8/25 | Loss: 0.00102285
Iteration 9/25 | Loss: 0.00102285
Iteration 10/25 | Loss: 0.00102285
Iteration 11/25 | Loss: 0.00102285
Iteration 12/25 | Loss: 0.00102285
Iteration 13/25 | Loss: 0.00102285
Iteration 14/25 | Loss: 0.00102285
Iteration 15/25 | Loss: 0.00102285
Iteration 16/25 | Loss: 0.00102285
Iteration 17/25 | Loss: 0.00102285
Iteration 18/25 | Loss: 0.00102285
Iteration 19/25 | Loss: 0.00102285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010228460887447, 0.0010228460887447, 0.0010228460887447, 0.0010228460887447, 0.0010228460887447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010228460887447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98438573
Iteration 2/25 | Loss: 0.00033977
Iteration 3/25 | Loss: 0.00033975
Iteration 4/25 | Loss: 0.00033975
Iteration 5/25 | Loss: 0.00033975
Iteration 6/25 | Loss: 0.00033975
Iteration 7/25 | Loss: 0.00033975
Iteration 8/25 | Loss: 0.00033975
Iteration 9/25 | Loss: 0.00033975
Iteration 10/25 | Loss: 0.00033975
Iteration 11/25 | Loss: 0.00033975
Iteration 12/25 | Loss: 0.00033975
Iteration 13/25 | Loss: 0.00033975
Iteration 14/25 | Loss: 0.00033975
Iteration 15/25 | Loss: 0.00033975
Iteration 16/25 | Loss: 0.00033975
Iteration 17/25 | Loss: 0.00033975
Iteration 18/25 | Loss: 0.00033975
Iteration 19/25 | Loss: 0.00033975
Iteration 20/25 | Loss: 0.00033975
Iteration 21/25 | Loss: 0.00033975
Iteration 22/25 | Loss: 0.00033975
Iteration 23/25 | Loss: 0.00033975
Iteration 24/25 | Loss: 0.00033975
Iteration 25/25 | Loss: 0.00033975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033975
Iteration 2/1000 | Loss: 0.00003425
Iteration 3/1000 | Loss: 0.00002727
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00002351
Iteration 6/1000 | Loss: 0.00002275
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002072
Iteration 10/1000 | Loss: 0.00002043
Iteration 11/1000 | Loss: 0.00002019
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00002001
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001994
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001980
Iteration 22/1000 | Loss: 0.00001979
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001979
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001977
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001973
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001972
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001971
Iteration 43/1000 | Loss: 0.00001971
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001970
Iteration 47/1000 | Loss: 0.00001970
Iteration 48/1000 | Loss: 0.00001970
Iteration 49/1000 | Loss: 0.00001970
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001969
Iteration 54/1000 | Loss: 0.00001969
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001968
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001968
Iteration 64/1000 | Loss: 0.00001968
Iteration 65/1000 | Loss: 0.00001968
Iteration 66/1000 | Loss: 0.00001968
Iteration 67/1000 | Loss: 0.00001968
Iteration 68/1000 | Loss: 0.00001968
Iteration 69/1000 | Loss: 0.00001968
Iteration 70/1000 | Loss: 0.00001968
Iteration 71/1000 | Loss: 0.00001968
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001966
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001966
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001965
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001964
Iteration 91/1000 | Loss: 0.00001964
Iteration 92/1000 | Loss: 0.00001964
Iteration 93/1000 | Loss: 0.00001964
Iteration 94/1000 | Loss: 0.00001964
Iteration 95/1000 | Loss: 0.00001963
Iteration 96/1000 | Loss: 0.00001963
Iteration 97/1000 | Loss: 0.00001963
Iteration 98/1000 | Loss: 0.00001963
Iteration 99/1000 | Loss: 0.00001963
Iteration 100/1000 | Loss: 0.00001963
Iteration 101/1000 | Loss: 0.00001963
Iteration 102/1000 | Loss: 0.00001963
Iteration 103/1000 | Loss: 0.00001963
Iteration 104/1000 | Loss: 0.00001963
Iteration 105/1000 | Loss: 0.00001963
Iteration 106/1000 | Loss: 0.00001963
Iteration 107/1000 | Loss: 0.00001963
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001963
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001962
Iteration 116/1000 | Loss: 0.00001962
Iteration 117/1000 | Loss: 0.00001962
Iteration 118/1000 | Loss: 0.00001962
Iteration 119/1000 | Loss: 0.00001962
Iteration 120/1000 | Loss: 0.00001962
Iteration 121/1000 | Loss: 0.00001962
Iteration 122/1000 | Loss: 0.00001962
Iteration 123/1000 | Loss: 0.00001962
Iteration 124/1000 | Loss: 0.00001962
Iteration 125/1000 | Loss: 0.00001962
Iteration 126/1000 | Loss: 0.00001962
Iteration 127/1000 | Loss: 0.00001962
Iteration 128/1000 | Loss: 0.00001962
Iteration 129/1000 | Loss: 0.00001962
Iteration 130/1000 | Loss: 0.00001962
Iteration 131/1000 | Loss: 0.00001962
Iteration 132/1000 | Loss: 0.00001961
Iteration 133/1000 | Loss: 0.00001961
Iteration 134/1000 | Loss: 0.00001961
Iteration 135/1000 | Loss: 0.00001961
Iteration 136/1000 | Loss: 0.00001961
Iteration 137/1000 | Loss: 0.00001961
Iteration 138/1000 | Loss: 0.00001961
Iteration 139/1000 | Loss: 0.00001961
Iteration 140/1000 | Loss: 0.00001961
Iteration 141/1000 | Loss: 0.00001961
Iteration 142/1000 | Loss: 0.00001961
Iteration 143/1000 | Loss: 0.00001961
Iteration 144/1000 | Loss: 0.00001961
Iteration 145/1000 | Loss: 0.00001961
Iteration 146/1000 | Loss: 0.00001960
Iteration 147/1000 | Loss: 0.00001960
Iteration 148/1000 | Loss: 0.00001960
Iteration 149/1000 | Loss: 0.00001960
Iteration 150/1000 | Loss: 0.00001960
Iteration 151/1000 | Loss: 0.00001960
Iteration 152/1000 | Loss: 0.00001960
Iteration 153/1000 | Loss: 0.00001960
Iteration 154/1000 | Loss: 0.00001960
Iteration 155/1000 | Loss: 0.00001960
Iteration 156/1000 | Loss: 0.00001960
Iteration 157/1000 | Loss: 0.00001960
Iteration 158/1000 | Loss: 0.00001960
Iteration 159/1000 | Loss: 0.00001960
Iteration 160/1000 | Loss: 0.00001960
Iteration 161/1000 | Loss: 0.00001960
Iteration 162/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9599232473410666e-05, 1.9599232473410666e-05, 1.9599232473410666e-05, 1.9599232473410666e-05, 1.9599232473410666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9599232473410666e-05

Optimization complete. Final v2v error: 3.8014936447143555 mm

Highest mean error: 3.9010822772979736 mm for frame 49

Lowest mean error: 3.6645636558532715 mm for frame 92

Saving results

Total time: 36.523069620132446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773643
Iteration 2/25 | Loss: 0.00108641
Iteration 3/25 | Loss: 0.00097987
Iteration 4/25 | Loss: 0.00097052
Iteration 5/25 | Loss: 0.00096841
Iteration 6/25 | Loss: 0.00096841
Iteration 7/25 | Loss: 0.00096841
Iteration 8/25 | Loss: 0.00096841
Iteration 9/25 | Loss: 0.00096841
Iteration 10/25 | Loss: 0.00096841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009684052201919258, 0.0009684052201919258, 0.0009684052201919258, 0.0009684052201919258, 0.0009684052201919258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009684052201919258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38307834
Iteration 2/25 | Loss: 0.00063153
Iteration 3/25 | Loss: 0.00063152
Iteration 4/25 | Loss: 0.00063152
Iteration 5/25 | Loss: 0.00063152
Iteration 6/25 | Loss: 0.00063152
Iteration 7/25 | Loss: 0.00063152
Iteration 8/25 | Loss: 0.00063152
Iteration 9/25 | Loss: 0.00063152
Iteration 10/25 | Loss: 0.00063152
Iteration 11/25 | Loss: 0.00063152
Iteration 12/25 | Loss: 0.00063152
Iteration 13/25 | Loss: 0.00063152
Iteration 14/25 | Loss: 0.00063152
Iteration 15/25 | Loss: 0.00063152
Iteration 16/25 | Loss: 0.00063152
Iteration 17/25 | Loss: 0.00063152
Iteration 18/25 | Loss: 0.00063152
Iteration 19/25 | Loss: 0.00063152
Iteration 20/25 | Loss: 0.00063152
Iteration 21/25 | Loss: 0.00063152
Iteration 22/25 | Loss: 0.00063152
Iteration 23/25 | Loss: 0.00063152
Iteration 24/25 | Loss: 0.00063152
Iteration 25/25 | Loss: 0.00063152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063152
Iteration 2/1000 | Loss: 0.00001906
Iteration 3/1000 | Loss: 0.00001302
Iteration 4/1000 | Loss: 0.00001132
Iteration 5/1000 | Loss: 0.00001032
Iteration 6/1000 | Loss: 0.00000982
Iteration 7/1000 | Loss: 0.00000951
Iteration 8/1000 | Loss: 0.00000917
Iteration 9/1000 | Loss: 0.00000898
Iteration 10/1000 | Loss: 0.00000897
Iteration 11/1000 | Loss: 0.00000893
Iteration 12/1000 | Loss: 0.00000885
Iteration 13/1000 | Loss: 0.00000876
Iteration 14/1000 | Loss: 0.00000871
Iteration 15/1000 | Loss: 0.00000871
Iteration 16/1000 | Loss: 0.00000870
Iteration 17/1000 | Loss: 0.00000870
Iteration 18/1000 | Loss: 0.00000866
Iteration 19/1000 | Loss: 0.00000866
Iteration 20/1000 | Loss: 0.00000866
Iteration 21/1000 | Loss: 0.00000866
Iteration 22/1000 | Loss: 0.00000865
Iteration 23/1000 | Loss: 0.00000865
Iteration 24/1000 | Loss: 0.00000865
Iteration 25/1000 | Loss: 0.00000865
Iteration 26/1000 | Loss: 0.00000865
Iteration 27/1000 | Loss: 0.00000864
Iteration 28/1000 | Loss: 0.00000864
Iteration 29/1000 | Loss: 0.00000863
Iteration 30/1000 | Loss: 0.00000863
Iteration 31/1000 | Loss: 0.00000862
Iteration 32/1000 | Loss: 0.00000862
Iteration 33/1000 | Loss: 0.00000862
Iteration 34/1000 | Loss: 0.00000861
Iteration 35/1000 | Loss: 0.00000860
Iteration 36/1000 | Loss: 0.00000859
Iteration 37/1000 | Loss: 0.00000859
Iteration 38/1000 | Loss: 0.00000858
Iteration 39/1000 | Loss: 0.00000858
Iteration 40/1000 | Loss: 0.00000858
Iteration 41/1000 | Loss: 0.00000858
Iteration 42/1000 | Loss: 0.00000858
Iteration 43/1000 | Loss: 0.00000858
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000857
Iteration 46/1000 | Loss: 0.00000857
Iteration 47/1000 | Loss: 0.00000856
Iteration 48/1000 | Loss: 0.00000855
Iteration 49/1000 | Loss: 0.00000855
Iteration 50/1000 | Loss: 0.00000855
Iteration 51/1000 | Loss: 0.00000855
Iteration 52/1000 | Loss: 0.00000854
Iteration 53/1000 | Loss: 0.00000854
Iteration 54/1000 | Loss: 0.00000852
Iteration 55/1000 | Loss: 0.00000851
Iteration 56/1000 | Loss: 0.00000850
Iteration 57/1000 | Loss: 0.00000850
Iteration 58/1000 | Loss: 0.00000849
Iteration 59/1000 | Loss: 0.00000849
Iteration 60/1000 | Loss: 0.00000849
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000847
Iteration 64/1000 | Loss: 0.00000847
Iteration 65/1000 | Loss: 0.00000847
Iteration 66/1000 | Loss: 0.00000846
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000845
Iteration 69/1000 | Loss: 0.00000845
Iteration 70/1000 | Loss: 0.00000843
Iteration 71/1000 | Loss: 0.00000843
Iteration 72/1000 | Loss: 0.00000843
Iteration 73/1000 | Loss: 0.00000843
Iteration 74/1000 | Loss: 0.00000843
Iteration 75/1000 | Loss: 0.00000842
Iteration 76/1000 | Loss: 0.00000842
Iteration 77/1000 | Loss: 0.00000842
Iteration 78/1000 | Loss: 0.00000842
Iteration 79/1000 | Loss: 0.00000842
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000841
Iteration 84/1000 | Loss: 0.00000841
Iteration 85/1000 | Loss: 0.00000840
Iteration 86/1000 | Loss: 0.00000840
Iteration 87/1000 | Loss: 0.00000840
Iteration 88/1000 | Loss: 0.00000839
Iteration 89/1000 | Loss: 0.00000839
Iteration 90/1000 | Loss: 0.00000839
Iteration 91/1000 | Loss: 0.00000838
Iteration 92/1000 | Loss: 0.00000838
Iteration 93/1000 | Loss: 0.00000838
Iteration 94/1000 | Loss: 0.00000837
Iteration 95/1000 | Loss: 0.00000837
Iteration 96/1000 | Loss: 0.00000837
Iteration 97/1000 | Loss: 0.00000836
Iteration 98/1000 | Loss: 0.00000835
Iteration 99/1000 | Loss: 0.00000835
Iteration 100/1000 | Loss: 0.00000835
Iteration 101/1000 | Loss: 0.00000835
Iteration 102/1000 | Loss: 0.00000835
Iteration 103/1000 | Loss: 0.00000835
Iteration 104/1000 | Loss: 0.00000835
Iteration 105/1000 | Loss: 0.00000835
Iteration 106/1000 | Loss: 0.00000835
Iteration 107/1000 | Loss: 0.00000834
Iteration 108/1000 | Loss: 0.00000834
Iteration 109/1000 | Loss: 0.00000834
Iteration 110/1000 | Loss: 0.00000834
Iteration 111/1000 | Loss: 0.00000833
Iteration 112/1000 | Loss: 0.00000833
Iteration 113/1000 | Loss: 0.00000833
Iteration 114/1000 | Loss: 0.00000833
Iteration 115/1000 | Loss: 0.00000833
Iteration 116/1000 | Loss: 0.00000832
Iteration 117/1000 | Loss: 0.00000832
Iteration 118/1000 | Loss: 0.00000832
Iteration 119/1000 | Loss: 0.00000831
Iteration 120/1000 | Loss: 0.00000831
Iteration 121/1000 | Loss: 0.00000831
Iteration 122/1000 | Loss: 0.00000831
Iteration 123/1000 | Loss: 0.00000831
Iteration 124/1000 | Loss: 0.00000830
Iteration 125/1000 | Loss: 0.00000830
Iteration 126/1000 | Loss: 0.00000830
Iteration 127/1000 | Loss: 0.00000830
Iteration 128/1000 | Loss: 0.00000830
Iteration 129/1000 | Loss: 0.00000830
Iteration 130/1000 | Loss: 0.00000830
Iteration 131/1000 | Loss: 0.00000830
Iteration 132/1000 | Loss: 0.00000830
Iteration 133/1000 | Loss: 0.00000830
Iteration 134/1000 | Loss: 0.00000830
Iteration 135/1000 | Loss: 0.00000830
Iteration 136/1000 | Loss: 0.00000830
Iteration 137/1000 | Loss: 0.00000830
Iteration 138/1000 | Loss: 0.00000830
Iteration 139/1000 | Loss: 0.00000830
Iteration 140/1000 | Loss: 0.00000830
Iteration 141/1000 | Loss: 0.00000830
Iteration 142/1000 | Loss: 0.00000830
Iteration 143/1000 | Loss: 0.00000830
Iteration 144/1000 | Loss: 0.00000830
Iteration 145/1000 | Loss: 0.00000830
Iteration 146/1000 | Loss: 0.00000830
Iteration 147/1000 | Loss: 0.00000830
Iteration 148/1000 | Loss: 0.00000830
Iteration 149/1000 | Loss: 0.00000830
Iteration 150/1000 | Loss: 0.00000830
Iteration 151/1000 | Loss: 0.00000830
Iteration 152/1000 | Loss: 0.00000830
Iteration 153/1000 | Loss: 0.00000830
Iteration 154/1000 | Loss: 0.00000830
Iteration 155/1000 | Loss: 0.00000830
Iteration 156/1000 | Loss: 0.00000830
Iteration 157/1000 | Loss: 0.00000830
Iteration 158/1000 | Loss: 0.00000830
Iteration 159/1000 | Loss: 0.00000830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [8.295908628497273e-06, 8.295908628497273e-06, 8.295908628497273e-06, 8.295908628497273e-06, 8.295908628497273e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.295908628497273e-06

Optimization complete. Final v2v error: 2.4740781784057617 mm

Highest mean error: 2.653341770172119 mm for frame 141

Lowest mean error: 2.3200061321258545 mm for frame 262

Saving results

Total time: 39.97849655151367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009181
Iteration 2/25 | Loss: 0.00230774
Iteration 3/25 | Loss: 0.00191777
Iteration 4/25 | Loss: 0.00171353
Iteration 5/25 | Loss: 0.00163330
Iteration 6/25 | Loss: 0.00141449
Iteration 7/25 | Loss: 0.00131423
Iteration 8/25 | Loss: 0.00121792
Iteration 9/25 | Loss: 0.00115313
Iteration 10/25 | Loss: 0.00114167
Iteration 11/25 | Loss: 0.00113943
Iteration 12/25 | Loss: 0.00113888
Iteration 13/25 | Loss: 0.00113884
Iteration 14/25 | Loss: 0.00113884
Iteration 15/25 | Loss: 0.00113884
Iteration 16/25 | Loss: 0.00113884
Iteration 17/25 | Loss: 0.00113884
Iteration 18/25 | Loss: 0.00113884
Iteration 19/25 | Loss: 0.00113884
Iteration 20/25 | Loss: 0.00113884
Iteration 21/25 | Loss: 0.00113884
Iteration 22/25 | Loss: 0.00113884
Iteration 23/25 | Loss: 0.00113884
Iteration 24/25 | Loss: 0.00113884
Iteration 25/25 | Loss: 0.00113884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36693549
Iteration 2/25 | Loss: 0.00056189
Iteration 3/25 | Loss: 0.00056189
Iteration 4/25 | Loss: 0.00056189
Iteration 5/25 | Loss: 0.00056189
Iteration 6/25 | Loss: 0.00056189
Iteration 7/25 | Loss: 0.00056189
Iteration 8/25 | Loss: 0.00056189
Iteration 9/25 | Loss: 0.00056189
Iteration 10/25 | Loss: 0.00056189
Iteration 11/25 | Loss: 0.00056189
Iteration 12/25 | Loss: 0.00056189
Iteration 13/25 | Loss: 0.00056189
Iteration 14/25 | Loss: 0.00056189
Iteration 15/25 | Loss: 0.00056189
Iteration 16/25 | Loss: 0.00056189
Iteration 17/25 | Loss: 0.00056189
Iteration 18/25 | Loss: 0.00056189
Iteration 19/25 | Loss: 0.00056189
Iteration 20/25 | Loss: 0.00056189
Iteration 21/25 | Loss: 0.00056189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005618853610940278, 0.0005618853610940278, 0.0005618853610940278, 0.0005618853610940278, 0.0005618853610940278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005618853610940278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056189
Iteration 2/1000 | Loss: 0.00004058
Iteration 3/1000 | Loss: 0.00002678
Iteration 4/1000 | Loss: 0.00002482
Iteration 5/1000 | Loss: 0.00002377
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002279
Iteration 8/1000 | Loss: 0.00002241
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002187
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002182
Iteration 20/1000 | Loss: 0.00002178
Iteration 21/1000 | Loss: 0.00002177
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002177
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002175
Iteration 26/1000 | Loss: 0.00002174
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002172
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002171
Iteration 32/1000 | Loss: 0.00002171
Iteration 33/1000 | Loss: 0.00002171
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002170
Iteration 36/1000 | Loss: 0.00002168
Iteration 37/1000 | Loss: 0.00002167
Iteration 38/1000 | Loss: 0.00002166
Iteration 39/1000 | Loss: 0.00002165
Iteration 40/1000 | Loss: 0.00002165
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002163
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002162
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002161
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002159
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002154
Iteration 55/1000 | Loss: 0.00002151
Iteration 56/1000 | Loss: 0.00002151
Iteration 57/1000 | Loss: 0.00002151
Iteration 58/1000 | Loss: 0.00002151
Iteration 59/1000 | Loss: 0.00002151
Iteration 60/1000 | Loss: 0.00002151
Iteration 61/1000 | Loss: 0.00002151
Iteration 62/1000 | Loss: 0.00002151
Iteration 63/1000 | Loss: 0.00002150
Iteration 64/1000 | Loss: 0.00002150
Iteration 65/1000 | Loss: 0.00002150
Iteration 66/1000 | Loss: 0.00002150
Iteration 67/1000 | Loss: 0.00002150
Iteration 68/1000 | Loss: 0.00002150
Iteration 69/1000 | Loss: 0.00002150
Iteration 70/1000 | Loss: 0.00002150
Iteration 71/1000 | Loss: 0.00002149
Iteration 72/1000 | Loss: 0.00002149
Iteration 73/1000 | Loss: 0.00002149
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002148
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002147
Iteration 82/1000 | Loss: 0.00002147
Iteration 83/1000 | Loss: 0.00002147
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002146
Iteration 89/1000 | Loss: 0.00002146
Iteration 90/1000 | Loss: 0.00002146
Iteration 91/1000 | Loss: 0.00002146
Iteration 92/1000 | Loss: 0.00002146
Iteration 93/1000 | Loss: 0.00002145
Iteration 94/1000 | Loss: 0.00002145
Iteration 95/1000 | Loss: 0.00002145
Iteration 96/1000 | Loss: 0.00002145
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002144
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002142
Iteration 104/1000 | Loss: 0.00002141
Iteration 105/1000 | Loss: 0.00002141
Iteration 106/1000 | Loss: 0.00002141
Iteration 107/1000 | Loss: 0.00002141
Iteration 108/1000 | Loss: 0.00002140
Iteration 109/1000 | Loss: 0.00002140
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002140
Iteration 112/1000 | Loss: 0.00002140
Iteration 113/1000 | Loss: 0.00002140
Iteration 114/1000 | Loss: 0.00002140
Iteration 115/1000 | Loss: 0.00002140
Iteration 116/1000 | Loss: 0.00002140
Iteration 117/1000 | Loss: 0.00002140
Iteration 118/1000 | Loss: 0.00002139
Iteration 119/1000 | Loss: 0.00002139
Iteration 120/1000 | Loss: 0.00002139
Iteration 121/1000 | Loss: 0.00002139
Iteration 122/1000 | Loss: 0.00002139
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002138
Iteration 126/1000 | Loss: 0.00002138
Iteration 127/1000 | Loss: 0.00002138
Iteration 128/1000 | Loss: 0.00002138
Iteration 129/1000 | Loss: 0.00002138
Iteration 130/1000 | Loss: 0.00002138
Iteration 131/1000 | Loss: 0.00002138
Iteration 132/1000 | Loss: 0.00002138
Iteration 133/1000 | Loss: 0.00002138
Iteration 134/1000 | Loss: 0.00002138
Iteration 135/1000 | Loss: 0.00002138
Iteration 136/1000 | Loss: 0.00002138
Iteration 137/1000 | Loss: 0.00002138
Iteration 138/1000 | Loss: 0.00002138
Iteration 139/1000 | Loss: 0.00002137
Iteration 140/1000 | Loss: 0.00002137
Iteration 141/1000 | Loss: 0.00002137
Iteration 142/1000 | Loss: 0.00002137
Iteration 143/1000 | Loss: 0.00002137
Iteration 144/1000 | Loss: 0.00002137
Iteration 145/1000 | Loss: 0.00002137
Iteration 146/1000 | Loss: 0.00002137
Iteration 147/1000 | Loss: 0.00002137
Iteration 148/1000 | Loss: 0.00002137
Iteration 149/1000 | Loss: 0.00002137
Iteration 150/1000 | Loss: 0.00002137
Iteration 151/1000 | Loss: 0.00002137
Iteration 152/1000 | Loss: 0.00002137
Iteration 153/1000 | Loss: 0.00002137
Iteration 154/1000 | Loss: 0.00002137
Iteration 155/1000 | Loss: 0.00002137
Iteration 156/1000 | Loss: 0.00002137
Iteration 157/1000 | Loss: 0.00002137
Iteration 158/1000 | Loss: 0.00002137
Iteration 159/1000 | Loss: 0.00002137
Iteration 160/1000 | Loss: 0.00002137
Iteration 161/1000 | Loss: 0.00002137
Iteration 162/1000 | Loss: 0.00002137
Iteration 163/1000 | Loss: 0.00002137
Iteration 164/1000 | Loss: 0.00002137
Iteration 165/1000 | Loss: 0.00002137
Iteration 166/1000 | Loss: 0.00002137
Iteration 167/1000 | Loss: 0.00002137
Iteration 168/1000 | Loss: 0.00002137
Iteration 169/1000 | Loss: 0.00002137
Iteration 170/1000 | Loss: 0.00002137
Iteration 171/1000 | Loss: 0.00002137
Iteration 172/1000 | Loss: 0.00002137
Iteration 173/1000 | Loss: 0.00002137
Iteration 174/1000 | Loss: 0.00002137
Iteration 175/1000 | Loss: 0.00002137
Iteration 176/1000 | Loss: 0.00002137
Iteration 177/1000 | Loss: 0.00002137
Iteration 178/1000 | Loss: 0.00002137
Iteration 179/1000 | Loss: 0.00002137
Iteration 180/1000 | Loss: 0.00002137
Iteration 181/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.1366027795011178e-05, 2.1366027795011178e-05, 2.1366027795011178e-05, 2.1366027795011178e-05, 2.1366027795011178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1366027795011178e-05

Optimization complete. Final v2v error: 3.9951581954956055 mm

Highest mean error: 4.114503383636475 mm for frame 0

Lowest mean error: 3.724860191345215 mm for frame 238

Saving results

Total time: 55.43154740333557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788300
Iteration 2/25 | Loss: 0.00144732
Iteration 3/25 | Loss: 0.00123407
Iteration 4/25 | Loss: 0.00118891
Iteration 5/25 | Loss: 0.00118981
Iteration 6/25 | Loss: 0.00115442
Iteration 7/25 | Loss: 0.00113215
Iteration 8/25 | Loss: 0.00111225
Iteration 9/25 | Loss: 0.00110092
Iteration 10/25 | Loss: 0.00108766
Iteration 11/25 | Loss: 0.00110822
Iteration 12/25 | Loss: 0.00108475
Iteration 13/25 | Loss: 0.00108215
Iteration 14/25 | Loss: 0.00108169
Iteration 15/25 | Loss: 0.00107474
Iteration 16/25 | Loss: 0.00107233
Iteration 17/25 | Loss: 0.00107413
Iteration 18/25 | Loss: 0.00107147
Iteration 19/25 | Loss: 0.00107371
Iteration 20/25 | Loss: 0.00107156
Iteration 21/25 | Loss: 0.00106689
Iteration 22/25 | Loss: 0.00107307
Iteration 23/25 | Loss: 0.00107065
Iteration 24/25 | Loss: 0.00106630
Iteration 25/25 | Loss: 0.00106110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23893094
Iteration 2/25 | Loss: 0.00096071
Iteration 3/25 | Loss: 0.00096069
Iteration 4/25 | Loss: 0.00096069
Iteration 5/25 | Loss: 0.00096068
Iteration 6/25 | Loss: 0.00096068
Iteration 7/25 | Loss: 0.00096068
Iteration 8/25 | Loss: 0.00096068
Iteration 9/25 | Loss: 0.00096068
Iteration 10/25 | Loss: 0.00096068
Iteration 11/25 | Loss: 0.00096068
Iteration 12/25 | Loss: 0.00096068
Iteration 13/25 | Loss: 0.00096068
Iteration 14/25 | Loss: 0.00096068
Iteration 15/25 | Loss: 0.00096068
Iteration 16/25 | Loss: 0.00096068
Iteration 17/25 | Loss: 0.00096068
Iteration 18/25 | Loss: 0.00096068
Iteration 19/25 | Loss: 0.00096068
Iteration 20/25 | Loss: 0.00096068
Iteration 21/25 | Loss: 0.00096068
Iteration 22/25 | Loss: 0.00096068
Iteration 23/25 | Loss: 0.00096068
Iteration 24/25 | Loss: 0.00096068
Iteration 25/25 | Loss: 0.00096068

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096068
Iteration 2/1000 | Loss: 0.00019303
Iteration 3/1000 | Loss: 0.00005034
Iteration 4/1000 | Loss: 0.00003494
Iteration 5/1000 | Loss: 0.00002884
Iteration 6/1000 | Loss: 0.00014569
Iteration 7/1000 | Loss: 0.00020053
Iteration 8/1000 | Loss: 0.00004081
Iteration 9/1000 | Loss: 0.00005724
Iteration 10/1000 | Loss: 0.00019651
Iteration 11/1000 | Loss: 0.00002868
Iteration 12/1000 | Loss: 0.00002421
Iteration 13/1000 | Loss: 0.00002229
Iteration 14/1000 | Loss: 0.00011444
Iteration 15/1000 | Loss: 0.00002090
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001835
Iteration 18/1000 | Loss: 0.00001766
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001636
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001565
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001499
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001489
Iteration 28/1000 | Loss: 0.00001489
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001481
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001477
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001465
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001458
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001457
Iteration 49/1000 | Loss: 0.00001457
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001456
Iteration 53/1000 | Loss: 0.00001456
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001455
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001452
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001448
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001438
Iteration 128/1000 | Loss: 0.00001438
Iteration 129/1000 | Loss: 0.00001438
Iteration 130/1000 | Loss: 0.00001438
Iteration 131/1000 | Loss: 0.00001438
Iteration 132/1000 | Loss: 0.00001438
Iteration 133/1000 | Loss: 0.00001438
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00001437
Iteration 140/1000 | Loss: 0.00001437
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001437
Iteration 143/1000 | Loss: 0.00001437
Iteration 144/1000 | Loss: 0.00001437
Iteration 145/1000 | Loss: 0.00001437
Iteration 146/1000 | Loss: 0.00001437
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001434
Iteration 166/1000 | Loss: 0.00001434
Iteration 167/1000 | Loss: 0.00001434
Iteration 168/1000 | Loss: 0.00001434
Iteration 169/1000 | Loss: 0.00001434
Iteration 170/1000 | Loss: 0.00001434
Iteration 171/1000 | Loss: 0.00001434
Iteration 172/1000 | Loss: 0.00001434
Iteration 173/1000 | Loss: 0.00001434
Iteration 174/1000 | Loss: 0.00001434
Iteration 175/1000 | Loss: 0.00001433
Iteration 176/1000 | Loss: 0.00001433
Iteration 177/1000 | Loss: 0.00001433
Iteration 178/1000 | Loss: 0.00001433
Iteration 179/1000 | Loss: 0.00001433
Iteration 180/1000 | Loss: 0.00001433
Iteration 181/1000 | Loss: 0.00001433
Iteration 182/1000 | Loss: 0.00001433
Iteration 183/1000 | Loss: 0.00001433
Iteration 184/1000 | Loss: 0.00001433
Iteration 185/1000 | Loss: 0.00001433
Iteration 186/1000 | Loss: 0.00001433
Iteration 187/1000 | Loss: 0.00001433
Iteration 188/1000 | Loss: 0.00001433
Iteration 189/1000 | Loss: 0.00001433
Iteration 190/1000 | Loss: 0.00001433
Iteration 191/1000 | Loss: 0.00001433
Iteration 192/1000 | Loss: 0.00001433
Iteration 193/1000 | Loss: 0.00001433
Iteration 194/1000 | Loss: 0.00001433
Iteration 195/1000 | Loss: 0.00001433
Iteration 196/1000 | Loss: 0.00001433
Iteration 197/1000 | Loss: 0.00001433
Iteration 198/1000 | Loss: 0.00001433
Iteration 199/1000 | Loss: 0.00001433
Iteration 200/1000 | Loss: 0.00001433
Iteration 201/1000 | Loss: 0.00001433
Iteration 202/1000 | Loss: 0.00001433
Iteration 203/1000 | Loss: 0.00001433
Iteration 204/1000 | Loss: 0.00001433
Iteration 205/1000 | Loss: 0.00001433
Iteration 206/1000 | Loss: 0.00001433
Iteration 207/1000 | Loss: 0.00001433
Iteration 208/1000 | Loss: 0.00001433
Iteration 209/1000 | Loss: 0.00001433
Iteration 210/1000 | Loss: 0.00001433
Iteration 211/1000 | Loss: 0.00001433
Iteration 212/1000 | Loss: 0.00001433
Iteration 213/1000 | Loss: 0.00001433
Iteration 214/1000 | Loss: 0.00001433
Iteration 215/1000 | Loss: 0.00001433
Iteration 216/1000 | Loss: 0.00001433
Iteration 217/1000 | Loss: 0.00001433
Iteration 218/1000 | Loss: 0.00001433
Iteration 219/1000 | Loss: 0.00001433
Iteration 220/1000 | Loss: 0.00001433
Iteration 221/1000 | Loss: 0.00001433
Iteration 222/1000 | Loss: 0.00001433
Iteration 223/1000 | Loss: 0.00001433
Iteration 224/1000 | Loss: 0.00001433
Iteration 225/1000 | Loss: 0.00001433
Iteration 226/1000 | Loss: 0.00001433
Iteration 227/1000 | Loss: 0.00001433
Iteration 228/1000 | Loss: 0.00001433
Iteration 229/1000 | Loss: 0.00001433
Iteration 230/1000 | Loss: 0.00001433
Iteration 231/1000 | Loss: 0.00001433
Iteration 232/1000 | Loss: 0.00001433
Iteration 233/1000 | Loss: 0.00001433
Iteration 234/1000 | Loss: 0.00001433
Iteration 235/1000 | Loss: 0.00001433
Iteration 236/1000 | Loss: 0.00001433
Iteration 237/1000 | Loss: 0.00001433
Iteration 238/1000 | Loss: 0.00001433
Iteration 239/1000 | Loss: 0.00001433
Iteration 240/1000 | Loss: 0.00001433
Iteration 241/1000 | Loss: 0.00001433
Iteration 242/1000 | Loss: 0.00001433
Iteration 243/1000 | Loss: 0.00001433
Iteration 244/1000 | Loss: 0.00001433
Iteration 245/1000 | Loss: 0.00001433
Iteration 246/1000 | Loss: 0.00001433
Iteration 247/1000 | Loss: 0.00001433
Iteration 248/1000 | Loss: 0.00001433
Iteration 249/1000 | Loss: 0.00001433
Iteration 250/1000 | Loss: 0.00001433
Iteration 251/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.432664521416882e-05, 1.432664521416882e-05, 1.432664521416882e-05, 1.432664521416882e-05, 1.432664521416882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.432664521416882e-05

Optimization complete. Final v2v error: 3.0944149494171143 mm

Highest mean error: 4.751280307769775 mm for frame 103

Lowest mean error: 2.3082735538482666 mm for frame 177

Saving results

Total time: 111.83832597732544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00212174
Iteration 2/25 | Loss: 0.00107597
Iteration 3/25 | Loss: 0.00097911
Iteration 4/25 | Loss: 0.00095798
Iteration 5/25 | Loss: 0.00095110
Iteration 6/25 | Loss: 0.00094938
Iteration 7/25 | Loss: 0.00094876
Iteration 8/25 | Loss: 0.00094876
Iteration 9/25 | Loss: 0.00094876
Iteration 10/25 | Loss: 0.00094876
Iteration 11/25 | Loss: 0.00094876
Iteration 12/25 | Loss: 0.00094876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009487576317042112, 0.0009487576317042112, 0.0009487576317042112, 0.0009487576317042112, 0.0009487576317042112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009487576317042112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33718061
Iteration 2/25 | Loss: 0.00117344
Iteration 3/25 | Loss: 0.00117344
Iteration 4/25 | Loss: 0.00117344
Iteration 5/25 | Loss: 0.00117344
Iteration 6/25 | Loss: 0.00117344
Iteration 7/25 | Loss: 0.00117344
Iteration 8/25 | Loss: 0.00117344
Iteration 9/25 | Loss: 0.00117344
Iteration 10/25 | Loss: 0.00117344
Iteration 11/25 | Loss: 0.00117344
Iteration 12/25 | Loss: 0.00117344
Iteration 13/25 | Loss: 0.00117344
Iteration 14/25 | Loss: 0.00117344
Iteration 15/25 | Loss: 0.00117344
Iteration 16/25 | Loss: 0.00117344
Iteration 17/25 | Loss: 0.00117344
Iteration 18/25 | Loss: 0.00117344
Iteration 19/25 | Loss: 0.00117344
Iteration 20/25 | Loss: 0.00117344
Iteration 21/25 | Loss: 0.00117344
Iteration 22/25 | Loss: 0.00117344
Iteration 23/25 | Loss: 0.00117344
Iteration 24/25 | Loss: 0.00117344
Iteration 25/25 | Loss: 0.00117344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117344
Iteration 2/1000 | Loss: 0.00004032
Iteration 3/1000 | Loss: 0.00002297
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001358
Iteration 6/1000 | Loss: 0.00001292
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001185
Iteration 9/1000 | Loss: 0.00001168
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001128
Iteration 13/1000 | Loss: 0.00001127
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001108
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001095
Iteration 22/1000 | Loss: 0.00001094
Iteration 23/1000 | Loss: 0.00001094
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001093
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001093
Iteration 28/1000 | Loss: 0.00001093
Iteration 29/1000 | Loss: 0.00001092
Iteration 30/1000 | Loss: 0.00001092
Iteration 31/1000 | Loss: 0.00001092
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001085
Iteration 54/1000 | Loss: 0.00001085
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001084
Iteration 58/1000 | Loss: 0.00001084
Iteration 59/1000 | Loss: 0.00001084
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001081
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001080
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001077
Iteration 79/1000 | Loss: 0.00001077
Iteration 80/1000 | Loss: 0.00001077
Iteration 81/1000 | Loss: 0.00001077
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001077
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001073
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001072
Iteration 99/1000 | Loss: 0.00001072
Iteration 100/1000 | Loss: 0.00001072
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001071
Iteration 104/1000 | Loss: 0.00001071
Iteration 105/1000 | Loss: 0.00001071
Iteration 106/1000 | Loss: 0.00001071
Iteration 107/1000 | Loss: 0.00001071
Iteration 108/1000 | Loss: 0.00001071
Iteration 109/1000 | Loss: 0.00001070
Iteration 110/1000 | Loss: 0.00001070
Iteration 111/1000 | Loss: 0.00001070
Iteration 112/1000 | Loss: 0.00001070
Iteration 113/1000 | Loss: 0.00001070
Iteration 114/1000 | Loss: 0.00001070
Iteration 115/1000 | Loss: 0.00001070
Iteration 116/1000 | Loss: 0.00001069
Iteration 117/1000 | Loss: 0.00001069
Iteration 118/1000 | Loss: 0.00001069
Iteration 119/1000 | Loss: 0.00001069
Iteration 120/1000 | Loss: 0.00001069
Iteration 121/1000 | Loss: 0.00001068
Iteration 122/1000 | Loss: 0.00001068
Iteration 123/1000 | Loss: 0.00001068
Iteration 124/1000 | Loss: 0.00001068
Iteration 125/1000 | Loss: 0.00001068
Iteration 126/1000 | Loss: 0.00001068
Iteration 127/1000 | Loss: 0.00001068
Iteration 128/1000 | Loss: 0.00001068
Iteration 129/1000 | Loss: 0.00001068
Iteration 130/1000 | Loss: 0.00001068
Iteration 131/1000 | Loss: 0.00001067
Iteration 132/1000 | Loss: 0.00001067
Iteration 133/1000 | Loss: 0.00001067
Iteration 134/1000 | Loss: 0.00001067
Iteration 135/1000 | Loss: 0.00001067
Iteration 136/1000 | Loss: 0.00001067
Iteration 137/1000 | Loss: 0.00001067
Iteration 138/1000 | Loss: 0.00001067
Iteration 139/1000 | Loss: 0.00001067
Iteration 140/1000 | Loss: 0.00001067
Iteration 141/1000 | Loss: 0.00001067
Iteration 142/1000 | Loss: 0.00001067
Iteration 143/1000 | Loss: 0.00001067
Iteration 144/1000 | Loss: 0.00001067
Iteration 145/1000 | Loss: 0.00001067
Iteration 146/1000 | Loss: 0.00001067
Iteration 147/1000 | Loss: 0.00001067
Iteration 148/1000 | Loss: 0.00001067
Iteration 149/1000 | Loss: 0.00001067
Iteration 150/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0665525223885197e-05, 1.0665525223885197e-05, 1.0665525223885197e-05, 1.0665525223885197e-05, 1.0665525223885197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0665525223885197e-05

Optimization complete. Final v2v error: 2.7789933681488037 mm

Highest mean error: 3.1271564960479736 mm for frame 48

Lowest mean error: 2.4688913822174072 mm for frame 120

Saving results

Total time: 36.587806701660156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579639
Iteration 2/25 | Loss: 0.00133794
Iteration 3/25 | Loss: 0.00108194
Iteration 4/25 | Loss: 0.00104960
Iteration 5/25 | Loss: 0.00104728
Iteration 6/25 | Loss: 0.00104658
Iteration 7/25 | Loss: 0.00104653
Iteration 8/25 | Loss: 0.00104653
Iteration 9/25 | Loss: 0.00104653
Iteration 10/25 | Loss: 0.00104653
Iteration 11/25 | Loss: 0.00104653
Iteration 12/25 | Loss: 0.00104653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010465307859703898, 0.0010465307859703898, 0.0010465307859703898, 0.0010465307859703898, 0.0010465307859703898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010465307859703898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33539414
Iteration 2/25 | Loss: 0.00049207
Iteration 3/25 | Loss: 0.00049203
Iteration 4/25 | Loss: 0.00049203
Iteration 5/25 | Loss: 0.00049203
Iteration 6/25 | Loss: 0.00049203
Iteration 7/25 | Loss: 0.00049203
Iteration 8/25 | Loss: 0.00049203
Iteration 9/25 | Loss: 0.00049203
Iteration 10/25 | Loss: 0.00049203
Iteration 11/25 | Loss: 0.00049203
Iteration 12/25 | Loss: 0.00049203
Iteration 13/25 | Loss: 0.00049203
Iteration 14/25 | Loss: 0.00049203
Iteration 15/25 | Loss: 0.00049203
Iteration 16/25 | Loss: 0.00049203
Iteration 17/25 | Loss: 0.00049203
Iteration 18/25 | Loss: 0.00049203
Iteration 19/25 | Loss: 0.00049203
Iteration 20/25 | Loss: 0.00049203
Iteration 21/25 | Loss: 0.00049203
Iteration 22/25 | Loss: 0.00049203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004920259234495461, 0.0004920259234495461, 0.0004920259234495461, 0.0004920259234495461, 0.0004920259234495461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004920259234495461

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049203
Iteration 2/1000 | Loss: 0.00004648
Iteration 3/1000 | Loss: 0.00002646
Iteration 4/1000 | Loss: 0.00002019
Iteration 5/1000 | Loss: 0.00001804
Iteration 6/1000 | Loss: 0.00001691
Iteration 7/1000 | Loss: 0.00001619
Iteration 8/1000 | Loss: 0.00001555
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001458
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001436
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001426
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001400
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001394
Iteration 20/1000 | Loss: 0.00001391
Iteration 21/1000 | Loss: 0.00001390
Iteration 22/1000 | Loss: 0.00001390
Iteration 23/1000 | Loss: 0.00001389
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001388
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001383
Iteration 34/1000 | Loss: 0.00001383
Iteration 35/1000 | Loss: 0.00001383
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001382
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001373
Iteration 57/1000 | Loss: 0.00001373
Iteration 58/1000 | Loss: 0.00001372
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001371
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001367
Iteration 73/1000 | Loss: 0.00001367
Iteration 74/1000 | Loss: 0.00001367
Iteration 75/1000 | Loss: 0.00001367
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001365
Iteration 90/1000 | Loss: 0.00001365
Iteration 91/1000 | Loss: 0.00001365
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001361
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001360
Iteration 112/1000 | Loss: 0.00001360
Iteration 113/1000 | Loss: 0.00001360
Iteration 114/1000 | Loss: 0.00001360
Iteration 115/1000 | Loss: 0.00001359
Iteration 116/1000 | Loss: 0.00001359
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001358
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001357
Iteration 124/1000 | Loss: 0.00001357
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001355
Iteration 139/1000 | Loss: 0.00001355
Iteration 140/1000 | Loss: 0.00001355
Iteration 141/1000 | Loss: 0.00001355
Iteration 142/1000 | Loss: 0.00001355
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001354
Iteration 146/1000 | Loss: 0.00001354
Iteration 147/1000 | Loss: 0.00001354
Iteration 148/1000 | Loss: 0.00001354
Iteration 149/1000 | Loss: 0.00001354
Iteration 150/1000 | Loss: 0.00001354
Iteration 151/1000 | Loss: 0.00001354
Iteration 152/1000 | Loss: 0.00001354
Iteration 153/1000 | Loss: 0.00001354
Iteration 154/1000 | Loss: 0.00001354
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001353
Iteration 157/1000 | Loss: 0.00001353
Iteration 158/1000 | Loss: 0.00001353
Iteration 159/1000 | Loss: 0.00001353
Iteration 160/1000 | Loss: 0.00001353
Iteration 161/1000 | Loss: 0.00001353
Iteration 162/1000 | Loss: 0.00001353
Iteration 163/1000 | Loss: 0.00001353
Iteration 164/1000 | Loss: 0.00001353
Iteration 165/1000 | Loss: 0.00001353
Iteration 166/1000 | Loss: 0.00001353
Iteration 167/1000 | Loss: 0.00001352
Iteration 168/1000 | Loss: 0.00001352
Iteration 169/1000 | Loss: 0.00001352
Iteration 170/1000 | Loss: 0.00001352
Iteration 171/1000 | Loss: 0.00001352
Iteration 172/1000 | Loss: 0.00001352
Iteration 173/1000 | Loss: 0.00001352
Iteration 174/1000 | Loss: 0.00001352
Iteration 175/1000 | Loss: 0.00001352
Iteration 176/1000 | Loss: 0.00001352
Iteration 177/1000 | Loss: 0.00001352
Iteration 178/1000 | Loss: 0.00001352
Iteration 179/1000 | Loss: 0.00001352
Iteration 180/1000 | Loss: 0.00001352
Iteration 181/1000 | Loss: 0.00001352
Iteration 182/1000 | Loss: 0.00001352
Iteration 183/1000 | Loss: 0.00001352
Iteration 184/1000 | Loss: 0.00001352
Iteration 185/1000 | Loss: 0.00001352
Iteration 186/1000 | Loss: 0.00001352
Iteration 187/1000 | Loss: 0.00001352
Iteration 188/1000 | Loss: 0.00001352
Iteration 189/1000 | Loss: 0.00001352
Iteration 190/1000 | Loss: 0.00001352
Iteration 191/1000 | Loss: 0.00001352
Iteration 192/1000 | Loss: 0.00001352
Iteration 193/1000 | Loss: 0.00001352
Iteration 194/1000 | Loss: 0.00001352
Iteration 195/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3517344086722005e-05, 1.3517344086722005e-05, 1.3517344086722005e-05, 1.3517344086722005e-05, 1.3517344086722005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3517344086722005e-05

Optimization complete. Final v2v error: 3.1119303703308105 mm

Highest mean error: 3.4154205322265625 mm for frame 69

Lowest mean error: 2.7252824306488037 mm for frame 16

Saving results

Total time: 42.27984952926636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819438
Iteration 2/25 | Loss: 0.00106406
Iteration 3/25 | Loss: 0.00097677
Iteration 4/25 | Loss: 0.00096736
Iteration 5/25 | Loss: 0.00096542
Iteration 6/25 | Loss: 0.00096542
Iteration 7/25 | Loss: 0.00096542
Iteration 8/25 | Loss: 0.00096542
Iteration 9/25 | Loss: 0.00096542
Iteration 10/25 | Loss: 0.00096542
Iteration 11/25 | Loss: 0.00096542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009654205059632659, 0.0009654205059632659, 0.0009654205059632659, 0.0009654205059632659, 0.0009654205059632659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009654205059632659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38130808
Iteration 2/25 | Loss: 0.00066839
Iteration 3/25 | Loss: 0.00066838
Iteration 4/25 | Loss: 0.00066838
Iteration 5/25 | Loss: 0.00066838
Iteration 6/25 | Loss: 0.00066838
Iteration 7/25 | Loss: 0.00066838
Iteration 8/25 | Loss: 0.00066838
Iteration 9/25 | Loss: 0.00066838
Iteration 10/25 | Loss: 0.00066838
Iteration 11/25 | Loss: 0.00066838
Iteration 12/25 | Loss: 0.00066838
Iteration 13/25 | Loss: 0.00066838
Iteration 14/25 | Loss: 0.00066838
Iteration 15/25 | Loss: 0.00066838
Iteration 16/25 | Loss: 0.00066838
Iteration 17/25 | Loss: 0.00066838
Iteration 18/25 | Loss: 0.00066838
Iteration 19/25 | Loss: 0.00066838
Iteration 20/25 | Loss: 0.00066838
Iteration 21/25 | Loss: 0.00066838
Iteration 22/25 | Loss: 0.00066838
Iteration 23/25 | Loss: 0.00066838
Iteration 24/25 | Loss: 0.00066838
Iteration 25/25 | Loss: 0.00066838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066838
Iteration 2/1000 | Loss: 0.00001664
Iteration 3/1000 | Loss: 0.00001181
Iteration 4/1000 | Loss: 0.00001063
Iteration 5/1000 | Loss: 0.00000973
Iteration 6/1000 | Loss: 0.00000932
Iteration 7/1000 | Loss: 0.00000891
Iteration 8/1000 | Loss: 0.00000874
Iteration 9/1000 | Loss: 0.00000862
Iteration 10/1000 | Loss: 0.00000861
Iteration 11/1000 | Loss: 0.00000839
Iteration 12/1000 | Loss: 0.00000836
Iteration 13/1000 | Loss: 0.00000835
Iteration 14/1000 | Loss: 0.00000834
Iteration 15/1000 | Loss: 0.00000833
Iteration 16/1000 | Loss: 0.00000833
Iteration 17/1000 | Loss: 0.00000831
Iteration 18/1000 | Loss: 0.00000830
Iteration 19/1000 | Loss: 0.00000829
Iteration 20/1000 | Loss: 0.00000829
Iteration 21/1000 | Loss: 0.00000828
Iteration 22/1000 | Loss: 0.00000828
Iteration 23/1000 | Loss: 0.00000828
Iteration 24/1000 | Loss: 0.00000827
Iteration 25/1000 | Loss: 0.00000826
Iteration 26/1000 | Loss: 0.00000826
Iteration 27/1000 | Loss: 0.00000826
Iteration 28/1000 | Loss: 0.00000820
Iteration 29/1000 | Loss: 0.00000819
Iteration 30/1000 | Loss: 0.00000815
Iteration 31/1000 | Loss: 0.00000815
Iteration 32/1000 | Loss: 0.00000815
Iteration 33/1000 | Loss: 0.00000814
Iteration 34/1000 | Loss: 0.00000813
Iteration 35/1000 | Loss: 0.00000812
Iteration 36/1000 | Loss: 0.00000812
Iteration 37/1000 | Loss: 0.00000811
Iteration 38/1000 | Loss: 0.00000811
Iteration 39/1000 | Loss: 0.00000810
Iteration 40/1000 | Loss: 0.00000810
Iteration 41/1000 | Loss: 0.00000809
Iteration 42/1000 | Loss: 0.00000809
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000807
Iteration 45/1000 | Loss: 0.00000807
Iteration 46/1000 | Loss: 0.00000807
Iteration 47/1000 | Loss: 0.00000806
Iteration 48/1000 | Loss: 0.00000806
Iteration 49/1000 | Loss: 0.00000806
Iteration 50/1000 | Loss: 0.00000805
Iteration 51/1000 | Loss: 0.00000805
Iteration 52/1000 | Loss: 0.00000805
Iteration 53/1000 | Loss: 0.00000804
Iteration 54/1000 | Loss: 0.00000803
Iteration 55/1000 | Loss: 0.00000803
Iteration 56/1000 | Loss: 0.00000803
Iteration 57/1000 | Loss: 0.00000803
Iteration 58/1000 | Loss: 0.00000802
Iteration 59/1000 | Loss: 0.00000802
Iteration 60/1000 | Loss: 0.00000802
Iteration 61/1000 | Loss: 0.00000802
Iteration 62/1000 | Loss: 0.00000802
Iteration 63/1000 | Loss: 0.00000802
Iteration 64/1000 | Loss: 0.00000801
Iteration 65/1000 | Loss: 0.00000801
Iteration 66/1000 | Loss: 0.00000800
Iteration 67/1000 | Loss: 0.00000800
Iteration 68/1000 | Loss: 0.00000800
Iteration 69/1000 | Loss: 0.00000800
Iteration 70/1000 | Loss: 0.00000800
Iteration 71/1000 | Loss: 0.00000800
Iteration 72/1000 | Loss: 0.00000800
Iteration 73/1000 | Loss: 0.00000799
Iteration 74/1000 | Loss: 0.00000799
Iteration 75/1000 | Loss: 0.00000799
Iteration 76/1000 | Loss: 0.00000799
Iteration 77/1000 | Loss: 0.00000799
Iteration 78/1000 | Loss: 0.00000799
Iteration 79/1000 | Loss: 0.00000799
Iteration 80/1000 | Loss: 0.00000799
Iteration 81/1000 | Loss: 0.00000799
Iteration 82/1000 | Loss: 0.00000798
Iteration 83/1000 | Loss: 0.00000798
Iteration 84/1000 | Loss: 0.00000797
Iteration 85/1000 | Loss: 0.00000796
Iteration 86/1000 | Loss: 0.00000796
Iteration 87/1000 | Loss: 0.00000796
Iteration 88/1000 | Loss: 0.00000793
Iteration 89/1000 | Loss: 0.00000792
Iteration 90/1000 | Loss: 0.00000791
Iteration 91/1000 | Loss: 0.00000791
Iteration 92/1000 | Loss: 0.00000790
Iteration 93/1000 | Loss: 0.00000790
Iteration 94/1000 | Loss: 0.00000790
Iteration 95/1000 | Loss: 0.00000789
Iteration 96/1000 | Loss: 0.00000789
Iteration 97/1000 | Loss: 0.00000787
Iteration 98/1000 | Loss: 0.00000787
Iteration 99/1000 | Loss: 0.00000787
Iteration 100/1000 | Loss: 0.00000787
Iteration 101/1000 | Loss: 0.00000787
Iteration 102/1000 | Loss: 0.00000787
Iteration 103/1000 | Loss: 0.00000787
Iteration 104/1000 | Loss: 0.00000786
Iteration 105/1000 | Loss: 0.00000786
Iteration 106/1000 | Loss: 0.00000786
Iteration 107/1000 | Loss: 0.00000786
Iteration 108/1000 | Loss: 0.00000786
Iteration 109/1000 | Loss: 0.00000786
Iteration 110/1000 | Loss: 0.00000785
Iteration 111/1000 | Loss: 0.00000784
Iteration 112/1000 | Loss: 0.00000784
Iteration 113/1000 | Loss: 0.00000784
Iteration 114/1000 | Loss: 0.00000784
Iteration 115/1000 | Loss: 0.00000783
Iteration 116/1000 | Loss: 0.00000783
Iteration 117/1000 | Loss: 0.00000783
Iteration 118/1000 | Loss: 0.00000783
Iteration 119/1000 | Loss: 0.00000783
Iteration 120/1000 | Loss: 0.00000783
Iteration 121/1000 | Loss: 0.00000783
Iteration 122/1000 | Loss: 0.00000783
Iteration 123/1000 | Loss: 0.00000783
Iteration 124/1000 | Loss: 0.00000783
Iteration 125/1000 | Loss: 0.00000783
Iteration 126/1000 | Loss: 0.00000782
Iteration 127/1000 | Loss: 0.00000782
Iteration 128/1000 | Loss: 0.00000782
Iteration 129/1000 | Loss: 0.00000782
Iteration 130/1000 | Loss: 0.00000781
Iteration 131/1000 | Loss: 0.00000781
Iteration 132/1000 | Loss: 0.00000781
Iteration 133/1000 | Loss: 0.00000781
Iteration 134/1000 | Loss: 0.00000781
Iteration 135/1000 | Loss: 0.00000781
Iteration 136/1000 | Loss: 0.00000781
Iteration 137/1000 | Loss: 0.00000781
Iteration 138/1000 | Loss: 0.00000781
Iteration 139/1000 | Loss: 0.00000781
Iteration 140/1000 | Loss: 0.00000780
Iteration 141/1000 | Loss: 0.00000780
Iteration 142/1000 | Loss: 0.00000780
Iteration 143/1000 | Loss: 0.00000780
Iteration 144/1000 | Loss: 0.00000779
Iteration 145/1000 | Loss: 0.00000779
Iteration 146/1000 | Loss: 0.00000779
Iteration 147/1000 | Loss: 0.00000779
Iteration 148/1000 | Loss: 0.00000779
Iteration 149/1000 | Loss: 0.00000779
Iteration 150/1000 | Loss: 0.00000779
Iteration 151/1000 | Loss: 0.00000779
Iteration 152/1000 | Loss: 0.00000779
Iteration 153/1000 | Loss: 0.00000779
Iteration 154/1000 | Loss: 0.00000779
Iteration 155/1000 | Loss: 0.00000779
Iteration 156/1000 | Loss: 0.00000779
Iteration 157/1000 | Loss: 0.00000779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [7.785927664372139e-06, 7.785927664372139e-06, 7.785927664372139e-06, 7.785927664372139e-06, 7.785927664372139e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.785927664372139e-06

Optimization complete. Final v2v error: 2.3719804286956787 mm

Highest mean error: 2.5397391319274902 mm for frame 57

Lowest mean error: 2.262200355529785 mm for frame 3

Saving results

Total time: 38.17966032028198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796322
Iteration 2/25 | Loss: 0.00306957
Iteration 3/25 | Loss: 0.00189608
Iteration 4/25 | Loss: 0.00168116
Iteration 5/25 | Loss: 0.00167552
Iteration 6/25 | Loss: 0.00151876
Iteration 7/25 | Loss: 0.00144778
Iteration 8/25 | Loss: 0.00136690
Iteration 9/25 | Loss: 0.00140301
Iteration 10/25 | Loss: 0.00136065
Iteration 11/25 | Loss: 0.00125795
Iteration 12/25 | Loss: 0.00124658
Iteration 13/25 | Loss: 0.00121621
Iteration 14/25 | Loss: 0.00120098
Iteration 15/25 | Loss: 0.00119755
Iteration 16/25 | Loss: 0.00119643
Iteration 17/25 | Loss: 0.00119244
Iteration 18/25 | Loss: 0.00119027
Iteration 19/25 | Loss: 0.00118901
Iteration 20/25 | Loss: 0.00119393
Iteration 21/25 | Loss: 0.00119291
Iteration 22/25 | Loss: 0.00118837
Iteration 23/25 | Loss: 0.00118714
Iteration 24/25 | Loss: 0.00119204
Iteration 25/25 | Loss: 0.00118941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82472110
Iteration 2/25 | Loss: 0.00125053
Iteration 3/25 | Loss: 0.00121498
Iteration 4/25 | Loss: 0.00114547
Iteration 5/25 | Loss: 0.00114547
Iteration 6/25 | Loss: 0.00114547
Iteration 7/25 | Loss: 0.00114547
Iteration 8/25 | Loss: 0.00114546
Iteration 9/25 | Loss: 0.00114546
Iteration 10/25 | Loss: 0.00114546
Iteration 11/25 | Loss: 0.00114546
Iteration 12/25 | Loss: 0.00114546
Iteration 13/25 | Loss: 0.00114546
Iteration 14/25 | Loss: 0.00114546
Iteration 15/25 | Loss: 0.00114546
Iteration 16/25 | Loss: 0.00114546
Iteration 17/25 | Loss: 0.00114546
Iteration 18/25 | Loss: 0.00114546
Iteration 19/25 | Loss: 0.00114546
Iteration 20/25 | Loss: 0.00114546
Iteration 21/25 | Loss: 0.00114546
Iteration 22/25 | Loss: 0.00114546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001145463320426643, 0.001145463320426643, 0.001145463320426643, 0.001145463320426643, 0.001145463320426643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001145463320426643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114546
Iteration 2/1000 | Loss: 0.00025173
Iteration 3/1000 | Loss: 0.00033336
Iteration 4/1000 | Loss: 0.00052856
Iteration 5/1000 | Loss: 0.00024787
Iteration 6/1000 | Loss: 0.00082183
Iteration 7/1000 | Loss: 0.00055230
Iteration 8/1000 | Loss: 0.00137190
Iteration 9/1000 | Loss: 0.00010186
Iteration 10/1000 | Loss: 0.00021056
Iteration 11/1000 | Loss: 0.00007261
Iteration 12/1000 | Loss: 0.00006924
Iteration 13/1000 | Loss: 0.00017974
Iteration 14/1000 | Loss: 0.00006670
Iteration 15/1000 | Loss: 0.00006561
Iteration 16/1000 | Loss: 0.00063530
Iteration 17/1000 | Loss: 0.00021624
Iteration 18/1000 | Loss: 0.00025716
Iteration 19/1000 | Loss: 0.00058408
Iteration 20/1000 | Loss: 0.00950466
Iteration 21/1000 | Loss: 0.00188085
Iteration 22/1000 | Loss: 0.00028713
Iteration 23/1000 | Loss: 0.00039777
Iteration 24/1000 | Loss: 0.00013299
Iteration 25/1000 | Loss: 0.00068269
Iteration 26/1000 | Loss: 0.00309527
Iteration 27/1000 | Loss: 0.00359367
Iteration 28/1000 | Loss: 0.00045554
Iteration 29/1000 | Loss: 0.00051443
Iteration 30/1000 | Loss: 0.00031481
Iteration 31/1000 | Loss: 0.00173710
Iteration 32/1000 | Loss: 0.00101180
Iteration 33/1000 | Loss: 0.00009876
Iteration 34/1000 | Loss: 0.00016206
Iteration 35/1000 | Loss: 0.00005286
Iteration 36/1000 | Loss: 0.00017147
Iteration 37/1000 | Loss: 0.00003575
Iteration 38/1000 | Loss: 0.00014329
Iteration 39/1000 | Loss: 0.00005443
Iteration 40/1000 | Loss: 0.00023099
Iteration 41/1000 | Loss: 0.00002536
Iteration 42/1000 | Loss: 0.00015127
Iteration 43/1000 | Loss: 0.00002242
Iteration 44/1000 | Loss: 0.00033131
Iteration 45/1000 | Loss: 0.00002058
Iteration 46/1000 | Loss: 0.00015234
Iteration 47/1000 | Loss: 0.00103137
Iteration 48/1000 | Loss: 0.00006742
Iteration 49/1000 | Loss: 0.00006162
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00007763
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00003608
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00088443
Iteration 57/1000 | Loss: 0.00004117
Iteration 58/1000 | Loss: 0.00006425
Iteration 59/1000 | Loss: 0.00004496
Iteration 60/1000 | Loss: 0.00001527
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001439
Iteration 69/1000 | Loss: 0.00001439
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001437
Iteration 72/1000 | Loss: 0.00001437
Iteration 73/1000 | Loss: 0.00001437
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001436
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001435
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001433
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001433
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00006039
Iteration 100/1000 | Loss: 0.00004613
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001422
Iteration 103/1000 | Loss: 0.00001421
Iteration 104/1000 | Loss: 0.00001421
Iteration 105/1000 | Loss: 0.00001420
Iteration 106/1000 | Loss: 0.00001420
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001419
Iteration 113/1000 | Loss: 0.00001419
Iteration 114/1000 | Loss: 0.00001419
Iteration 115/1000 | Loss: 0.00001419
Iteration 116/1000 | Loss: 0.00001419
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001418
Iteration 123/1000 | Loss: 0.00001418
Iteration 124/1000 | Loss: 0.00001418
Iteration 125/1000 | Loss: 0.00001418
Iteration 126/1000 | Loss: 0.00001418
Iteration 127/1000 | Loss: 0.00001418
Iteration 128/1000 | Loss: 0.00001418
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001418
Iteration 133/1000 | Loss: 0.00001418
Iteration 134/1000 | Loss: 0.00001418
Iteration 135/1000 | Loss: 0.00001418
Iteration 136/1000 | Loss: 0.00001418
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001418
Iteration 143/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4181313417793717e-05, 1.4181313417793717e-05, 1.4181313417793717e-05, 1.4181313417793717e-05, 1.4181313417793717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4181313417793717e-05

Optimization complete. Final v2v error: 3.114882469177246 mm

Highest mean error: 4.618103504180908 mm for frame 98

Lowest mean error: 2.7339048385620117 mm for frame 124

Saving results

Total time: 140.2618465423584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377310
Iteration 2/25 | Loss: 0.00116143
Iteration 3/25 | Loss: 0.00102665
Iteration 4/25 | Loss: 0.00100194
Iteration 5/25 | Loss: 0.00099465
Iteration 6/25 | Loss: 0.00099266
Iteration 7/25 | Loss: 0.00099242
Iteration 8/25 | Loss: 0.00099242
Iteration 9/25 | Loss: 0.00099242
Iteration 10/25 | Loss: 0.00099241
Iteration 11/25 | Loss: 0.00099241
Iteration 12/25 | Loss: 0.00099241
Iteration 13/25 | Loss: 0.00099241
Iteration 14/25 | Loss: 0.00099241
Iteration 15/25 | Loss: 0.00099241
Iteration 16/25 | Loss: 0.00099241
Iteration 17/25 | Loss: 0.00099241
Iteration 18/25 | Loss: 0.00099241
Iteration 19/25 | Loss: 0.00099241
Iteration 20/25 | Loss: 0.00099241
Iteration 21/25 | Loss: 0.00099241
Iteration 22/25 | Loss: 0.00099241
Iteration 23/25 | Loss: 0.00099241
Iteration 24/25 | Loss: 0.00099241
Iteration 25/25 | Loss: 0.00099241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33836019
Iteration 2/25 | Loss: 0.00072738
Iteration 3/25 | Loss: 0.00072738
Iteration 4/25 | Loss: 0.00072738
Iteration 5/25 | Loss: 0.00072738
Iteration 6/25 | Loss: 0.00072738
Iteration 7/25 | Loss: 0.00072738
Iteration 8/25 | Loss: 0.00072738
Iteration 9/25 | Loss: 0.00072738
Iteration 10/25 | Loss: 0.00072738
Iteration 11/25 | Loss: 0.00072738
Iteration 12/25 | Loss: 0.00072738
Iteration 13/25 | Loss: 0.00072738
Iteration 14/25 | Loss: 0.00072738
Iteration 15/25 | Loss: 0.00072738
Iteration 16/25 | Loss: 0.00072738
Iteration 17/25 | Loss: 0.00072738
Iteration 18/25 | Loss: 0.00072738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007273769588209689, 0.0007273769588209689, 0.0007273769588209689, 0.0007273769588209689, 0.0007273769588209689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007273769588209689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072738
Iteration 2/1000 | Loss: 0.00004553
Iteration 3/1000 | Loss: 0.00003021
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001733
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001618
Iteration 10/1000 | Loss: 0.00001584
Iteration 11/1000 | Loss: 0.00001560
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001507
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001495
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001481
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001470
Iteration 31/1000 | Loss: 0.00001469
Iteration 32/1000 | Loss: 0.00001469
Iteration 33/1000 | Loss: 0.00001469
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001455
Iteration 110/1000 | Loss: 0.00001455
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001453
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001453
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001452
Iteration 137/1000 | Loss: 0.00001452
Iteration 138/1000 | Loss: 0.00001452
Iteration 139/1000 | Loss: 0.00001452
Iteration 140/1000 | Loss: 0.00001452
Iteration 141/1000 | Loss: 0.00001452
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001451
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001451
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001449
Iteration 169/1000 | Loss: 0.00001449
Iteration 170/1000 | Loss: 0.00001449
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001449
Iteration 175/1000 | Loss: 0.00001449
Iteration 176/1000 | Loss: 0.00001449
Iteration 177/1000 | Loss: 0.00001449
Iteration 178/1000 | Loss: 0.00001448
Iteration 179/1000 | Loss: 0.00001448
Iteration 180/1000 | Loss: 0.00001448
Iteration 181/1000 | Loss: 0.00001448
Iteration 182/1000 | Loss: 0.00001448
Iteration 183/1000 | Loss: 0.00001448
Iteration 184/1000 | Loss: 0.00001448
Iteration 185/1000 | Loss: 0.00001448
Iteration 186/1000 | Loss: 0.00001448
Iteration 187/1000 | Loss: 0.00001448
Iteration 188/1000 | Loss: 0.00001448
Iteration 189/1000 | Loss: 0.00001448
Iteration 190/1000 | Loss: 0.00001448
Iteration 191/1000 | Loss: 0.00001448
Iteration 192/1000 | Loss: 0.00001448
Iteration 193/1000 | Loss: 0.00001448
Iteration 194/1000 | Loss: 0.00001448
Iteration 195/1000 | Loss: 0.00001448
Iteration 196/1000 | Loss: 0.00001448
Iteration 197/1000 | Loss: 0.00001448
Iteration 198/1000 | Loss: 0.00001448
Iteration 199/1000 | Loss: 0.00001448
Iteration 200/1000 | Loss: 0.00001448
Iteration 201/1000 | Loss: 0.00001448
Iteration 202/1000 | Loss: 0.00001448
Iteration 203/1000 | Loss: 0.00001448
Iteration 204/1000 | Loss: 0.00001448
Iteration 205/1000 | Loss: 0.00001448
Iteration 206/1000 | Loss: 0.00001448
Iteration 207/1000 | Loss: 0.00001448
Iteration 208/1000 | Loss: 0.00001448
Iteration 209/1000 | Loss: 0.00001448
Iteration 210/1000 | Loss: 0.00001448
Iteration 211/1000 | Loss: 0.00001448
Iteration 212/1000 | Loss: 0.00001448
Iteration 213/1000 | Loss: 0.00001448
Iteration 214/1000 | Loss: 0.00001448
Iteration 215/1000 | Loss: 0.00001448
Iteration 216/1000 | Loss: 0.00001448
Iteration 217/1000 | Loss: 0.00001448
Iteration 218/1000 | Loss: 0.00001448
Iteration 219/1000 | Loss: 0.00001448
Iteration 220/1000 | Loss: 0.00001448
Iteration 221/1000 | Loss: 0.00001448
Iteration 222/1000 | Loss: 0.00001448
Iteration 223/1000 | Loss: 0.00001448
Iteration 224/1000 | Loss: 0.00001448
Iteration 225/1000 | Loss: 0.00001448
Iteration 226/1000 | Loss: 0.00001448
Iteration 227/1000 | Loss: 0.00001448
Iteration 228/1000 | Loss: 0.00001448
Iteration 229/1000 | Loss: 0.00001448
Iteration 230/1000 | Loss: 0.00001448
Iteration 231/1000 | Loss: 0.00001448
Iteration 232/1000 | Loss: 0.00001448
Iteration 233/1000 | Loss: 0.00001448
Iteration 234/1000 | Loss: 0.00001448
Iteration 235/1000 | Loss: 0.00001448
Iteration 236/1000 | Loss: 0.00001448
Iteration 237/1000 | Loss: 0.00001448
Iteration 238/1000 | Loss: 0.00001448
Iteration 239/1000 | Loss: 0.00001448
Iteration 240/1000 | Loss: 0.00001448
Iteration 241/1000 | Loss: 0.00001448
Iteration 242/1000 | Loss: 0.00001448
Iteration 243/1000 | Loss: 0.00001448
Iteration 244/1000 | Loss: 0.00001448
Iteration 245/1000 | Loss: 0.00001448
Iteration 246/1000 | Loss: 0.00001448
Iteration 247/1000 | Loss: 0.00001448
Iteration 248/1000 | Loss: 0.00001448
Iteration 249/1000 | Loss: 0.00001448
Iteration 250/1000 | Loss: 0.00001448
Iteration 251/1000 | Loss: 0.00001448
Iteration 252/1000 | Loss: 0.00001448
Iteration 253/1000 | Loss: 0.00001448
Iteration 254/1000 | Loss: 0.00001448
Iteration 255/1000 | Loss: 0.00001448
Iteration 256/1000 | Loss: 0.00001448
Iteration 257/1000 | Loss: 0.00001448
Iteration 258/1000 | Loss: 0.00001448
Iteration 259/1000 | Loss: 0.00001448
Iteration 260/1000 | Loss: 0.00001448
Iteration 261/1000 | Loss: 0.00001448
Iteration 262/1000 | Loss: 0.00001448
Iteration 263/1000 | Loss: 0.00001448
Iteration 264/1000 | Loss: 0.00001448
Iteration 265/1000 | Loss: 0.00001448
Iteration 266/1000 | Loss: 0.00001448
Iteration 267/1000 | Loss: 0.00001448
Iteration 268/1000 | Loss: 0.00001448
Iteration 269/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.4477415788860526e-05, 1.4477415788860526e-05, 1.4477415788860526e-05, 1.4477415788860526e-05, 1.4477415788860526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4477415788860526e-05

Optimization complete. Final v2v error: 3.177762508392334 mm

Highest mean error: 3.8013086318969727 mm for frame 50

Lowest mean error: 2.4840140342712402 mm for frame 10

Saving results

Total time: 45.442116260528564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978873
Iteration 2/25 | Loss: 0.00170934
Iteration 3/25 | Loss: 0.00118859
Iteration 4/25 | Loss: 0.00111692
Iteration 5/25 | Loss: 0.00110268
Iteration 6/25 | Loss: 0.00109872
Iteration 7/25 | Loss: 0.00109860
Iteration 8/25 | Loss: 0.00109860
Iteration 9/25 | Loss: 0.00109860
Iteration 10/25 | Loss: 0.00109860
Iteration 11/25 | Loss: 0.00109860
Iteration 12/25 | Loss: 0.00109860
Iteration 13/25 | Loss: 0.00109860
Iteration 14/25 | Loss: 0.00109860
Iteration 15/25 | Loss: 0.00109860
Iteration 16/25 | Loss: 0.00109860
Iteration 17/25 | Loss: 0.00109860
Iteration 18/25 | Loss: 0.00109860
Iteration 19/25 | Loss: 0.00109860
Iteration 20/25 | Loss: 0.00109860
Iteration 21/25 | Loss: 0.00109860
Iteration 22/25 | Loss: 0.00109860
Iteration 23/25 | Loss: 0.00109860
Iteration 24/25 | Loss: 0.00109860
Iteration 25/25 | Loss: 0.00109860

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97858715
Iteration 2/25 | Loss: 0.00047971
Iteration 3/25 | Loss: 0.00047970
Iteration 4/25 | Loss: 0.00047970
Iteration 5/25 | Loss: 0.00047970
Iteration 6/25 | Loss: 0.00047970
Iteration 7/25 | Loss: 0.00047970
Iteration 8/25 | Loss: 0.00047970
Iteration 9/25 | Loss: 0.00047970
Iteration 10/25 | Loss: 0.00047970
Iteration 11/25 | Loss: 0.00047970
Iteration 12/25 | Loss: 0.00047970
Iteration 13/25 | Loss: 0.00047970
Iteration 14/25 | Loss: 0.00047970
Iteration 15/25 | Loss: 0.00047970
Iteration 16/25 | Loss: 0.00047970
Iteration 17/25 | Loss: 0.00047970
Iteration 18/25 | Loss: 0.00047970
Iteration 19/25 | Loss: 0.00047970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004796987632289529, 0.0004796987632289529, 0.0004796987632289529, 0.0004796987632289529, 0.0004796987632289529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004796987632289529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047970
Iteration 2/1000 | Loss: 0.00004454
Iteration 3/1000 | Loss: 0.00002691
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002157
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001953
Iteration 9/1000 | Loss: 0.00001930
Iteration 10/1000 | Loss: 0.00001910
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001891
Iteration 22/1000 | Loss: 0.00001891
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001888
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001888
Iteration 29/1000 | Loss: 0.00001888
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001887
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001887
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001887
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001887
Iteration 42/1000 | Loss: 0.00001886
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00001885
Iteration 45/1000 | Loss: 0.00001885
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001885
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001882
Iteration 51/1000 | Loss: 0.00001882
Iteration 52/1000 | Loss: 0.00001882
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001881
Iteration 57/1000 | Loss: 0.00001881
Iteration 58/1000 | Loss: 0.00001881
Iteration 59/1000 | Loss: 0.00001881
Iteration 60/1000 | Loss: 0.00001881
Iteration 61/1000 | Loss: 0.00001881
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001880
Iteration 64/1000 | Loss: 0.00001880
Iteration 65/1000 | Loss: 0.00001880
Iteration 66/1000 | Loss: 0.00001880
Iteration 67/1000 | Loss: 0.00001880
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001879
Iteration 70/1000 | Loss: 0.00001879
Iteration 71/1000 | Loss: 0.00001879
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001879
Iteration 74/1000 | Loss: 0.00001879
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001879
Iteration 79/1000 | Loss: 0.00001878
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001878
Iteration 85/1000 | Loss: 0.00001877
Iteration 86/1000 | Loss: 0.00001877
Iteration 87/1000 | Loss: 0.00001877
Iteration 88/1000 | Loss: 0.00001877
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001877
Iteration 92/1000 | Loss: 0.00001877
Iteration 93/1000 | Loss: 0.00001877
Iteration 94/1000 | Loss: 0.00001877
Iteration 95/1000 | Loss: 0.00001877
Iteration 96/1000 | Loss: 0.00001877
Iteration 97/1000 | Loss: 0.00001877
Iteration 98/1000 | Loss: 0.00001877
Iteration 99/1000 | Loss: 0.00001877
Iteration 100/1000 | Loss: 0.00001877
Iteration 101/1000 | Loss: 0.00001877
Iteration 102/1000 | Loss: 0.00001877
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001876
Iteration 105/1000 | Loss: 0.00001876
Iteration 106/1000 | Loss: 0.00001876
Iteration 107/1000 | Loss: 0.00001876
Iteration 108/1000 | Loss: 0.00001876
Iteration 109/1000 | Loss: 0.00001876
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001876
Iteration 112/1000 | Loss: 0.00001876
Iteration 113/1000 | Loss: 0.00001876
Iteration 114/1000 | Loss: 0.00001876
Iteration 115/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.876493297459092e-05, 1.876493297459092e-05, 1.876493297459092e-05, 1.876493297459092e-05, 1.876493297459092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.876493297459092e-05

Optimization complete. Final v2v error: 3.6851539611816406 mm

Highest mean error: 3.9860992431640625 mm for frame 117

Lowest mean error: 3.519580125808716 mm for frame 85

Saving results

Total time: 31.506567001342773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013295
Iteration 2/25 | Loss: 0.00253028
Iteration 3/25 | Loss: 0.00199936
Iteration 4/25 | Loss: 0.00189519
Iteration 5/25 | Loss: 0.00184798
Iteration 6/25 | Loss: 0.00183353
Iteration 7/25 | Loss: 0.00182503
Iteration 8/25 | Loss: 0.00182100
Iteration 9/25 | Loss: 0.00181905
Iteration 10/25 | Loss: 0.00181830
Iteration 11/25 | Loss: 0.00181809
Iteration 12/25 | Loss: 0.00181809
Iteration 13/25 | Loss: 0.00181809
Iteration 14/25 | Loss: 0.00181809
Iteration 15/25 | Loss: 0.00181809
Iteration 16/25 | Loss: 0.00181809
Iteration 17/25 | Loss: 0.00181809
Iteration 18/25 | Loss: 0.00181809
Iteration 19/25 | Loss: 0.00181809
Iteration 20/25 | Loss: 0.00181809
Iteration 21/25 | Loss: 0.00181809
Iteration 22/25 | Loss: 0.00181809
Iteration 23/25 | Loss: 0.00181809
Iteration 24/25 | Loss: 0.00181809
Iteration 25/25 | Loss: 0.00181809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35978711
Iteration 2/25 | Loss: 0.00402319
Iteration 3/25 | Loss: 0.00402319
Iteration 4/25 | Loss: 0.00402319
Iteration 5/25 | Loss: 0.00402319
Iteration 6/25 | Loss: 0.00402319
Iteration 7/25 | Loss: 0.00402319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0040231915190815926, 0.0040231915190815926, 0.0040231915190815926, 0.0040231915190815926, 0.0040231915190815926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0040231915190815926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00402319
Iteration 2/1000 | Loss: 0.00066798
Iteration 3/1000 | Loss: 0.00050138
Iteration 4/1000 | Loss: 0.00045166
Iteration 5/1000 | Loss: 0.00041069
Iteration 6/1000 | Loss: 0.00038037
Iteration 7/1000 | Loss: 0.00035626
Iteration 8/1000 | Loss: 0.00033335
Iteration 9/1000 | Loss: 0.00032181
Iteration 10/1000 | Loss: 0.02891045
Iteration 11/1000 | Loss: 0.00267102
Iteration 12/1000 | Loss: 0.00034340
Iteration 13/1000 | Loss: 0.00023917
Iteration 14/1000 | Loss: 0.00016711
Iteration 15/1000 | Loss: 0.00013715
Iteration 16/1000 | Loss: 0.00009294
Iteration 17/1000 | Loss: 0.00006760
Iteration 18/1000 | Loss: 0.00004819
Iteration 19/1000 | Loss: 0.00003742
Iteration 20/1000 | Loss: 0.00003177
Iteration 21/1000 | Loss: 0.00002694
Iteration 22/1000 | Loss: 0.00002410
Iteration 23/1000 | Loss: 0.00002178
Iteration 24/1000 | Loss: 0.00002005
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001466
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001442
Iteration 39/1000 | Loss: 0.00001439
Iteration 40/1000 | Loss: 0.00001438
Iteration 41/1000 | Loss: 0.00001438
Iteration 42/1000 | Loss: 0.00001438
Iteration 43/1000 | Loss: 0.00001437
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001435
Iteration 46/1000 | Loss: 0.00001435
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001432
Iteration 54/1000 | Loss: 0.00001432
Iteration 55/1000 | Loss: 0.00001432
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001431
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001429
Iteration 62/1000 | Loss: 0.00001429
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001428
Iteration 65/1000 | Loss: 0.00001428
Iteration 66/1000 | Loss: 0.00001428
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001427
Iteration 69/1000 | Loss: 0.00001427
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001425
Iteration 73/1000 | Loss: 0.00001425
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001425
Iteration 77/1000 | Loss: 0.00001425
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001423
Iteration 82/1000 | Loss: 0.00001423
Iteration 83/1000 | Loss: 0.00001422
Iteration 84/1000 | Loss: 0.00001422
Iteration 85/1000 | Loss: 0.00001422
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001420
Iteration 95/1000 | Loss: 0.00001420
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001419
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001419
Iteration 109/1000 | Loss: 0.00001419
Iteration 110/1000 | Loss: 0.00001419
Iteration 111/1000 | Loss: 0.00001419
Iteration 112/1000 | Loss: 0.00001418
Iteration 113/1000 | Loss: 0.00001418
Iteration 114/1000 | Loss: 0.00001418
Iteration 115/1000 | Loss: 0.00001418
Iteration 116/1000 | Loss: 0.00001418
Iteration 117/1000 | Loss: 0.00001418
Iteration 118/1000 | Loss: 0.00001418
Iteration 119/1000 | Loss: 0.00001418
Iteration 120/1000 | Loss: 0.00001418
Iteration 121/1000 | Loss: 0.00001418
Iteration 122/1000 | Loss: 0.00001418
Iteration 123/1000 | Loss: 0.00001418
Iteration 124/1000 | Loss: 0.00001418
Iteration 125/1000 | Loss: 0.00001418
Iteration 126/1000 | Loss: 0.00001418
Iteration 127/1000 | Loss: 0.00001418
Iteration 128/1000 | Loss: 0.00001418
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.417995554220397e-05, 1.417995554220397e-05, 1.417995554220397e-05, 1.417995554220397e-05, 1.417995554220397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.417995554220397e-05

Optimization complete. Final v2v error: 3.2010059356689453 mm

Highest mean error: 3.2490408420562744 mm for frame 46

Lowest mean error: 3.1531362533569336 mm for frame 6

Saving results

Total time: 67.99831461906433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798114
Iteration 2/25 | Loss: 0.00110386
Iteration 3/25 | Loss: 0.00098597
Iteration 4/25 | Loss: 0.00097556
Iteration 5/25 | Loss: 0.00097178
Iteration 6/25 | Loss: 0.00097166
Iteration 7/25 | Loss: 0.00097166
Iteration 8/25 | Loss: 0.00097166
Iteration 9/25 | Loss: 0.00097166
Iteration 10/25 | Loss: 0.00097166
Iteration 11/25 | Loss: 0.00097166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009716590866446495, 0.0009716590866446495, 0.0009716590866446495, 0.0009716590866446495, 0.0009716590866446495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009716590866446495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37351286
Iteration 2/25 | Loss: 0.00085889
Iteration 3/25 | Loss: 0.00085889
Iteration 4/25 | Loss: 0.00085889
Iteration 5/25 | Loss: 0.00085889
Iteration 6/25 | Loss: 0.00085889
Iteration 7/25 | Loss: 0.00085889
Iteration 8/25 | Loss: 0.00085889
Iteration 9/25 | Loss: 0.00085889
Iteration 10/25 | Loss: 0.00085889
Iteration 11/25 | Loss: 0.00085889
Iteration 12/25 | Loss: 0.00085889
Iteration 13/25 | Loss: 0.00085889
Iteration 14/25 | Loss: 0.00085889
Iteration 15/25 | Loss: 0.00085889
Iteration 16/25 | Loss: 0.00085889
Iteration 17/25 | Loss: 0.00085889
Iteration 18/25 | Loss: 0.00085889
Iteration 19/25 | Loss: 0.00085889
Iteration 20/25 | Loss: 0.00085889
Iteration 21/25 | Loss: 0.00085889
Iteration 22/25 | Loss: 0.00085889
Iteration 23/25 | Loss: 0.00085889
Iteration 24/25 | Loss: 0.00085889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008588851196691394, 0.0008588851196691394, 0.0008588851196691394, 0.0008588851196691394, 0.0008588851196691394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008588851196691394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085889
Iteration 2/1000 | Loss: 0.00003636
Iteration 3/1000 | Loss: 0.00002049
Iteration 4/1000 | Loss: 0.00001447
Iteration 5/1000 | Loss: 0.00001265
Iteration 6/1000 | Loss: 0.00001143
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001020
Iteration 9/1000 | Loss: 0.00000991
Iteration 10/1000 | Loss: 0.00000976
Iteration 11/1000 | Loss: 0.00000967
Iteration 12/1000 | Loss: 0.00000947
Iteration 13/1000 | Loss: 0.00000944
Iteration 14/1000 | Loss: 0.00000943
Iteration 15/1000 | Loss: 0.00000940
Iteration 16/1000 | Loss: 0.00000936
Iteration 17/1000 | Loss: 0.00000931
Iteration 18/1000 | Loss: 0.00000930
Iteration 19/1000 | Loss: 0.00000929
Iteration 20/1000 | Loss: 0.00000929
Iteration 21/1000 | Loss: 0.00000929
Iteration 22/1000 | Loss: 0.00000928
Iteration 23/1000 | Loss: 0.00000928
Iteration 24/1000 | Loss: 0.00000927
Iteration 25/1000 | Loss: 0.00000926
Iteration 26/1000 | Loss: 0.00000926
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000925
Iteration 29/1000 | Loss: 0.00000925
Iteration 30/1000 | Loss: 0.00000924
Iteration 31/1000 | Loss: 0.00000924
Iteration 32/1000 | Loss: 0.00000924
Iteration 33/1000 | Loss: 0.00000923
Iteration 34/1000 | Loss: 0.00000923
Iteration 35/1000 | Loss: 0.00000922
Iteration 36/1000 | Loss: 0.00000922
Iteration 37/1000 | Loss: 0.00000922
Iteration 38/1000 | Loss: 0.00000921
Iteration 39/1000 | Loss: 0.00000921
Iteration 40/1000 | Loss: 0.00000921
Iteration 41/1000 | Loss: 0.00000921
Iteration 42/1000 | Loss: 0.00000921
Iteration 43/1000 | Loss: 0.00000921
Iteration 44/1000 | Loss: 0.00000920
Iteration 45/1000 | Loss: 0.00000920
Iteration 46/1000 | Loss: 0.00000920
Iteration 47/1000 | Loss: 0.00000920
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000920
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000919
Iteration 58/1000 | Loss: 0.00000919
Iteration 59/1000 | Loss: 0.00000919
Iteration 60/1000 | Loss: 0.00000919
Iteration 61/1000 | Loss: 0.00000919
Iteration 62/1000 | Loss: 0.00000919
Iteration 63/1000 | Loss: 0.00000918
Iteration 64/1000 | Loss: 0.00000918
Iteration 65/1000 | Loss: 0.00000918
Iteration 66/1000 | Loss: 0.00000917
Iteration 67/1000 | Loss: 0.00000917
Iteration 68/1000 | Loss: 0.00000916
Iteration 69/1000 | Loss: 0.00000916
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000915
Iteration 73/1000 | Loss: 0.00000915
Iteration 74/1000 | Loss: 0.00000915
Iteration 75/1000 | Loss: 0.00000915
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000914
Iteration 80/1000 | Loss: 0.00000914
Iteration 81/1000 | Loss: 0.00000914
Iteration 82/1000 | Loss: 0.00000914
Iteration 83/1000 | Loss: 0.00000913
Iteration 84/1000 | Loss: 0.00000913
Iteration 85/1000 | Loss: 0.00000913
Iteration 86/1000 | Loss: 0.00000913
Iteration 87/1000 | Loss: 0.00000913
Iteration 88/1000 | Loss: 0.00000913
Iteration 89/1000 | Loss: 0.00000913
Iteration 90/1000 | Loss: 0.00000913
Iteration 91/1000 | Loss: 0.00000913
Iteration 92/1000 | Loss: 0.00000912
Iteration 93/1000 | Loss: 0.00000912
Iteration 94/1000 | Loss: 0.00000912
Iteration 95/1000 | Loss: 0.00000912
Iteration 96/1000 | Loss: 0.00000912
Iteration 97/1000 | Loss: 0.00000912
Iteration 98/1000 | Loss: 0.00000912
Iteration 99/1000 | Loss: 0.00000912
Iteration 100/1000 | Loss: 0.00000912
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000911
Iteration 108/1000 | Loss: 0.00000911
Iteration 109/1000 | Loss: 0.00000911
Iteration 110/1000 | Loss: 0.00000911
Iteration 111/1000 | Loss: 0.00000911
Iteration 112/1000 | Loss: 0.00000911
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000910
Iteration 119/1000 | Loss: 0.00000909
Iteration 120/1000 | Loss: 0.00000909
Iteration 121/1000 | Loss: 0.00000909
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000908
Iteration 125/1000 | Loss: 0.00000908
Iteration 126/1000 | Loss: 0.00000908
Iteration 127/1000 | Loss: 0.00000907
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000906
Iteration 134/1000 | Loss: 0.00000906
Iteration 135/1000 | Loss: 0.00000906
Iteration 136/1000 | Loss: 0.00000906
Iteration 137/1000 | Loss: 0.00000905
Iteration 138/1000 | Loss: 0.00000905
Iteration 139/1000 | Loss: 0.00000905
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000904
Iteration 142/1000 | Loss: 0.00000904
Iteration 143/1000 | Loss: 0.00000904
Iteration 144/1000 | Loss: 0.00000904
Iteration 145/1000 | Loss: 0.00000904
Iteration 146/1000 | Loss: 0.00000903
Iteration 147/1000 | Loss: 0.00000903
Iteration 148/1000 | Loss: 0.00000903
Iteration 149/1000 | Loss: 0.00000903
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000903
Iteration 152/1000 | Loss: 0.00000903
Iteration 153/1000 | Loss: 0.00000903
Iteration 154/1000 | Loss: 0.00000903
Iteration 155/1000 | Loss: 0.00000903
Iteration 156/1000 | Loss: 0.00000903
Iteration 157/1000 | Loss: 0.00000903
Iteration 158/1000 | Loss: 0.00000902
Iteration 159/1000 | Loss: 0.00000902
Iteration 160/1000 | Loss: 0.00000902
Iteration 161/1000 | Loss: 0.00000902
Iteration 162/1000 | Loss: 0.00000902
Iteration 163/1000 | Loss: 0.00000902
Iteration 164/1000 | Loss: 0.00000902
Iteration 165/1000 | Loss: 0.00000902
Iteration 166/1000 | Loss: 0.00000902
Iteration 167/1000 | Loss: 0.00000902
Iteration 168/1000 | Loss: 0.00000902
Iteration 169/1000 | Loss: 0.00000902
Iteration 170/1000 | Loss: 0.00000902
Iteration 171/1000 | Loss: 0.00000902
Iteration 172/1000 | Loss: 0.00000902
Iteration 173/1000 | Loss: 0.00000902
Iteration 174/1000 | Loss: 0.00000902
Iteration 175/1000 | Loss: 0.00000901
Iteration 176/1000 | Loss: 0.00000901
Iteration 177/1000 | Loss: 0.00000901
Iteration 178/1000 | Loss: 0.00000901
Iteration 179/1000 | Loss: 0.00000901
Iteration 180/1000 | Loss: 0.00000901
Iteration 181/1000 | Loss: 0.00000901
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Iteration 185/1000 | Loss: 0.00000901
Iteration 186/1000 | Loss: 0.00000901
Iteration 187/1000 | Loss: 0.00000901
Iteration 188/1000 | Loss: 0.00000901
Iteration 189/1000 | Loss: 0.00000901
Iteration 190/1000 | Loss: 0.00000901
Iteration 191/1000 | Loss: 0.00000901
Iteration 192/1000 | Loss: 0.00000901
Iteration 193/1000 | Loss: 0.00000900
Iteration 194/1000 | Loss: 0.00000900
Iteration 195/1000 | Loss: 0.00000900
Iteration 196/1000 | Loss: 0.00000900
Iteration 197/1000 | Loss: 0.00000900
Iteration 198/1000 | Loss: 0.00000900
Iteration 199/1000 | Loss: 0.00000900
Iteration 200/1000 | Loss: 0.00000900
Iteration 201/1000 | Loss: 0.00000899
Iteration 202/1000 | Loss: 0.00000899
Iteration 203/1000 | Loss: 0.00000899
Iteration 204/1000 | Loss: 0.00000899
Iteration 205/1000 | Loss: 0.00000899
Iteration 206/1000 | Loss: 0.00000899
Iteration 207/1000 | Loss: 0.00000899
Iteration 208/1000 | Loss: 0.00000899
Iteration 209/1000 | Loss: 0.00000899
Iteration 210/1000 | Loss: 0.00000899
Iteration 211/1000 | Loss: 0.00000899
Iteration 212/1000 | Loss: 0.00000899
Iteration 213/1000 | Loss: 0.00000899
Iteration 214/1000 | Loss: 0.00000899
Iteration 215/1000 | Loss: 0.00000898
Iteration 216/1000 | Loss: 0.00000898
Iteration 217/1000 | Loss: 0.00000898
Iteration 218/1000 | Loss: 0.00000898
Iteration 219/1000 | Loss: 0.00000898
Iteration 220/1000 | Loss: 0.00000898
Iteration 221/1000 | Loss: 0.00000898
Iteration 222/1000 | Loss: 0.00000898
Iteration 223/1000 | Loss: 0.00000898
Iteration 224/1000 | Loss: 0.00000898
Iteration 225/1000 | Loss: 0.00000898
Iteration 226/1000 | Loss: 0.00000898
Iteration 227/1000 | Loss: 0.00000898
Iteration 228/1000 | Loss: 0.00000897
Iteration 229/1000 | Loss: 0.00000897
Iteration 230/1000 | Loss: 0.00000897
Iteration 231/1000 | Loss: 0.00000897
Iteration 232/1000 | Loss: 0.00000897
Iteration 233/1000 | Loss: 0.00000897
Iteration 234/1000 | Loss: 0.00000897
Iteration 235/1000 | Loss: 0.00000897
Iteration 236/1000 | Loss: 0.00000897
Iteration 237/1000 | Loss: 0.00000897
Iteration 238/1000 | Loss: 0.00000897
Iteration 239/1000 | Loss: 0.00000896
Iteration 240/1000 | Loss: 0.00000896
Iteration 241/1000 | Loss: 0.00000896
Iteration 242/1000 | Loss: 0.00000896
Iteration 243/1000 | Loss: 0.00000896
Iteration 244/1000 | Loss: 0.00000896
Iteration 245/1000 | Loss: 0.00000896
Iteration 246/1000 | Loss: 0.00000896
Iteration 247/1000 | Loss: 0.00000896
Iteration 248/1000 | Loss: 0.00000896
Iteration 249/1000 | Loss: 0.00000896
Iteration 250/1000 | Loss: 0.00000896
Iteration 251/1000 | Loss: 0.00000896
Iteration 252/1000 | Loss: 0.00000896
Iteration 253/1000 | Loss: 0.00000896
Iteration 254/1000 | Loss: 0.00000896
Iteration 255/1000 | Loss: 0.00000895
Iteration 256/1000 | Loss: 0.00000895
Iteration 257/1000 | Loss: 0.00000895
Iteration 258/1000 | Loss: 0.00000895
Iteration 259/1000 | Loss: 0.00000895
Iteration 260/1000 | Loss: 0.00000895
Iteration 261/1000 | Loss: 0.00000895
Iteration 262/1000 | Loss: 0.00000895
Iteration 263/1000 | Loss: 0.00000895
Iteration 264/1000 | Loss: 0.00000895
Iteration 265/1000 | Loss: 0.00000895
Iteration 266/1000 | Loss: 0.00000895
Iteration 267/1000 | Loss: 0.00000895
Iteration 268/1000 | Loss: 0.00000895
Iteration 269/1000 | Loss: 0.00000895
Iteration 270/1000 | Loss: 0.00000895
Iteration 271/1000 | Loss: 0.00000895
Iteration 272/1000 | Loss: 0.00000895
Iteration 273/1000 | Loss: 0.00000895
Iteration 274/1000 | Loss: 0.00000894
Iteration 275/1000 | Loss: 0.00000894
Iteration 276/1000 | Loss: 0.00000894
Iteration 277/1000 | Loss: 0.00000894
Iteration 278/1000 | Loss: 0.00000894
Iteration 279/1000 | Loss: 0.00000894
Iteration 280/1000 | Loss: 0.00000894
Iteration 281/1000 | Loss: 0.00000894
Iteration 282/1000 | Loss: 0.00000894
Iteration 283/1000 | Loss: 0.00000894
Iteration 284/1000 | Loss: 0.00000894
Iteration 285/1000 | Loss: 0.00000894
Iteration 286/1000 | Loss: 0.00000894
Iteration 287/1000 | Loss: 0.00000894
Iteration 288/1000 | Loss: 0.00000894
Iteration 289/1000 | Loss: 0.00000894
Iteration 290/1000 | Loss: 0.00000894
Iteration 291/1000 | Loss: 0.00000894
Iteration 292/1000 | Loss: 0.00000894
Iteration 293/1000 | Loss: 0.00000894
Iteration 294/1000 | Loss: 0.00000893
Iteration 295/1000 | Loss: 0.00000893
Iteration 296/1000 | Loss: 0.00000893
Iteration 297/1000 | Loss: 0.00000893
Iteration 298/1000 | Loss: 0.00000893
Iteration 299/1000 | Loss: 0.00000893
Iteration 300/1000 | Loss: 0.00000893
Iteration 301/1000 | Loss: 0.00000893
Iteration 302/1000 | Loss: 0.00000893
Iteration 303/1000 | Loss: 0.00000893
Iteration 304/1000 | Loss: 0.00000893
Iteration 305/1000 | Loss: 0.00000893
Iteration 306/1000 | Loss: 0.00000893
Iteration 307/1000 | Loss: 0.00000893
Iteration 308/1000 | Loss: 0.00000893
Iteration 309/1000 | Loss: 0.00000893
Iteration 310/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [8.929406249080785e-06, 8.929406249080785e-06, 8.929406249080785e-06, 8.929406249080785e-06, 8.929406249080785e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.929406249080785e-06

Optimization complete. Final v2v error: 2.5697247982025146 mm

Highest mean error: 3.0852108001708984 mm for frame 239

Lowest mean error: 2.2832236289978027 mm for frame 173

Saving results

Total time: 50.861222982406616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363274
Iteration 2/25 | Loss: 0.00104363
Iteration 3/25 | Loss: 0.00097145
Iteration 4/25 | Loss: 0.00096505
Iteration 5/25 | Loss: 0.00096307
Iteration 6/25 | Loss: 0.00096246
Iteration 7/25 | Loss: 0.00096246
Iteration 8/25 | Loss: 0.00096246
Iteration 9/25 | Loss: 0.00096246
Iteration 10/25 | Loss: 0.00096246
Iteration 11/25 | Loss: 0.00096246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009624565136618912, 0.0009624565136618912, 0.0009624565136618912, 0.0009624565136618912, 0.0009624565136618912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009624565136618912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77183568
Iteration 2/25 | Loss: 0.00077354
Iteration 3/25 | Loss: 0.00077353
Iteration 4/25 | Loss: 0.00077353
Iteration 5/25 | Loss: 0.00077353
Iteration 6/25 | Loss: 0.00077353
Iteration 7/25 | Loss: 0.00077353
Iteration 8/25 | Loss: 0.00077353
Iteration 9/25 | Loss: 0.00077353
Iteration 10/25 | Loss: 0.00077353
Iteration 11/25 | Loss: 0.00077353
Iteration 12/25 | Loss: 0.00077353
Iteration 13/25 | Loss: 0.00077353
Iteration 14/25 | Loss: 0.00077353
Iteration 15/25 | Loss: 0.00077353
Iteration 16/25 | Loss: 0.00077353
Iteration 17/25 | Loss: 0.00077353
Iteration 18/25 | Loss: 0.00077353
Iteration 19/25 | Loss: 0.00077353
Iteration 20/25 | Loss: 0.00077353
Iteration 21/25 | Loss: 0.00077353
Iteration 22/25 | Loss: 0.00077353
Iteration 23/25 | Loss: 0.00077353
Iteration 24/25 | Loss: 0.00077353
Iteration 25/25 | Loss: 0.00077353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077353
Iteration 2/1000 | Loss: 0.00001762
Iteration 3/1000 | Loss: 0.00001098
Iteration 4/1000 | Loss: 0.00000949
Iteration 5/1000 | Loss: 0.00000882
Iteration 6/1000 | Loss: 0.00000833
Iteration 7/1000 | Loss: 0.00000830
Iteration 8/1000 | Loss: 0.00000797
Iteration 9/1000 | Loss: 0.00000782
Iteration 10/1000 | Loss: 0.00000767
Iteration 11/1000 | Loss: 0.00000766
Iteration 12/1000 | Loss: 0.00000763
Iteration 13/1000 | Loss: 0.00000760
Iteration 14/1000 | Loss: 0.00000758
Iteration 15/1000 | Loss: 0.00000755
Iteration 16/1000 | Loss: 0.00000755
Iteration 17/1000 | Loss: 0.00000753
Iteration 18/1000 | Loss: 0.00000751
Iteration 19/1000 | Loss: 0.00000751
Iteration 20/1000 | Loss: 0.00000751
Iteration 21/1000 | Loss: 0.00000750
Iteration 22/1000 | Loss: 0.00000750
Iteration 23/1000 | Loss: 0.00000750
Iteration 24/1000 | Loss: 0.00000750
Iteration 25/1000 | Loss: 0.00000749
Iteration 26/1000 | Loss: 0.00000749
Iteration 27/1000 | Loss: 0.00000748
Iteration 28/1000 | Loss: 0.00000748
Iteration 29/1000 | Loss: 0.00000747
Iteration 30/1000 | Loss: 0.00000747
Iteration 31/1000 | Loss: 0.00000746
Iteration 32/1000 | Loss: 0.00000745
Iteration 33/1000 | Loss: 0.00000745
Iteration 34/1000 | Loss: 0.00000744
Iteration 35/1000 | Loss: 0.00000744
Iteration 36/1000 | Loss: 0.00000743
Iteration 37/1000 | Loss: 0.00000743
Iteration 38/1000 | Loss: 0.00000740
Iteration 39/1000 | Loss: 0.00000740
Iteration 40/1000 | Loss: 0.00000740
Iteration 41/1000 | Loss: 0.00000740
Iteration 42/1000 | Loss: 0.00000740
Iteration 43/1000 | Loss: 0.00000740
Iteration 44/1000 | Loss: 0.00000739
Iteration 45/1000 | Loss: 0.00000737
Iteration 46/1000 | Loss: 0.00000737
Iteration 47/1000 | Loss: 0.00000737
Iteration 48/1000 | Loss: 0.00000737
Iteration 49/1000 | Loss: 0.00000737
Iteration 50/1000 | Loss: 0.00000737
Iteration 51/1000 | Loss: 0.00000737
Iteration 52/1000 | Loss: 0.00000737
Iteration 53/1000 | Loss: 0.00000737
Iteration 54/1000 | Loss: 0.00000737
Iteration 55/1000 | Loss: 0.00000736
Iteration 56/1000 | Loss: 0.00000736
Iteration 57/1000 | Loss: 0.00000736
Iteration 58/1000 | Loss: 0.00000736
Iteration 59/1000 | Loss: 0.00000736
Iteration 60/1000 | Loss: 0.00000736
Iteration 61/1000 | Loss: 0.00000736
Iteration 62/1000 | Loss: 0.00000735
Iteration 63/1000 | Loss: 0.00000734
Iteration 64/1000 | Loss: 0.00000734
Iteration 65/1000 | Loss: 0.00000734
Iteration 66/1000 | Loss: 0.00000733
Iteration 67/1000 | Loss: 0.00000733
Iteration 68/1000 | Loss: 0.00000732
Iteration 69/1000 | Loss: 0.00000732
Iteration 70/1000 | Loss: 0.00000732
Iteration 71/1000 | Loss: 0.00000732
Iteration 72/1000 | Loss: 0.00000731
Iteration 73/1000 | Loss: 0.00000731
Iteration 74/1000 | Loss: 0.00000731
Iteration 75/1000 | Loss: 0.00000731
Iteration 76/1000 | Loss: 0.00000731
Iteration 77/1000 | Loss: 0.00000730
Iteration 78/1000 | Loss: 0.00000730
Iteration 79/1000 | Loss: 0.00000730
Iteration 80/1000 | Loss: 0.00000730
Iteration 81/1000 | Loss: 0.00000730
Iteration 82/1000 | Loss: 0.00000730
Iteration 83/1000 | Loss: 0.00000730
Iteration 84/1000 | Loss: 0.00000730
Iteration 85/1000 | Loss: 0.00000730
Iteration 86/1000 | Loss: 0.00000729
Iteration 87/1000 | Loss: 0.00000729
Iteration 88/1000 | Loss: 0.00000729
Iteration 89/1000 | Loss: 0.00000729
Iteration 90/1000 | Loss: 0.00000729
Iteration 91/1000 | Loss: 0.00000729
Iteration 92/1000 | Loss: 0.00000728
Iteration 93/1000 | Loss: 0.00000728
Iteration 94/1000 | Loss: 0.00000728
Iteration 95/1000 | Loss: 0.00000727
Iteration 96/1000 | Loss: 0.00000727
Iteration 97/1000 | Loss: 0.00000727
Iteration 98/1000 | Loss: 0.00000727
Iteration 99/1000 | Loss: 0.00000726
Iteration 100/1000 | Loss: 0.00000726
Iteration 101/1000 | Loss: 0.00000726
Iteration 102/1000 | Loss: 0.00000726
Iteration 103/1000 | Loss: 0.00000726
Iteration 104/1000 | Loss: 0.00000726
Iteration 105/1000 | Loss: 0.00000726
Iteration 106/1000 | Loss: 0.00000725
Iteration 107/1000 | Loss: 0.00000725
Iteration 108/1000 | Loss: 0.00000725
Iteration 109/1000 | Loss: 0.00000724
Iteration 110/1000 | Loss: 0.00000724
Iteration 111/1000 | Loss: 0.00000724
Iteration 112/1000 | Loss: 0.00000724
Iteration 113/1000 | Loss: 0.00000724
Iteration 114/1000 | Loss: 0.00000724
Iteration 115/1000 | Loss: 0.00000723
Iteration 116/1000 | Loss: 0.00000723
Iteration 117/1000 | Loss: 0.00000723
Iteration 118/1000 | Loss: 0.00000723
Iteration 119/1000 | Loss: 0.00000723
Iteration 120/1000 | Loss: 0.00000723
Iteration 121/1000 | Loss: 0.00000723
Iteration 122/1000 | Loss: 0.00000723
Iteration 123/1000 | Loss: 0.00000723
Iteration 124/1000 | Loss: 0.00000723
Iteration 125/1000 | Loss: 0.00000723
Iteration 126/1000 | Loss: 0.00000723
Iteration 127/1000 | Loss: 0.00000723
Iteration 128/1000 | Loss: 0.00000723
Iteration 129/1000 | Loss: 0.00000723
Iteration 130/1000 | Loss: 0.00000723
Iteration 131/1000 | Loss: 0.00000723
Iteration 132/1000 | Loss: 0.00000723
Iteration 133/1000 | Loss: 0.00000723
Iteration 134/1000 | Loss: 0.00000723
Iteration 135/1000 | Loss: 0.00000723
Iteration 136/1000 | Loss: 0.00000723
Iteration 137/1000 | Loss: 0.00000723
Iteration 138/1000 | Loss: 0.00000723
Iteration 139/1000 | Loss: 0.00000723
Iteration 140/1000 | Loss: 0.00000723
Iteration 141/1000 | Loss: 0.00000723
Iteration 142/1000 | Loss: 0.00000723
Iteration 143/1000 | Loss: 0.00000723
Iteration 144/1000 | Loss: 0.00000723
Iteration 145/1000 | Loss: 0.00000723
Iteration 146/1000 | Loss: 0.00000723
Iteration 147/1000 | Loss: 0.00000723
Iteration 148/1000 | Loss: 0.00000723
Iteration 149/1000 | Loss: 0.00000723
Iteration 150/1000 | Loss: 0.00000723
Iteration 151/1000 | Loss: 0.00000723
Iteration 152/1000 | Loss: 0.00000723
Iteration 153/1000 | Loss: 0.00000723
Iteration 154/1000 | Loss: 0.00000723
Iteration 155/1000 | Loss: 0.00000723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [7.232962616399163e-06, 7.232962616399163e-06, 7.232962616399163e-06, 7.232962616399163e-06, 7.232962616399163e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.232962616399163e-06

Optimization complete. Final v2v error: 2.3179657459259033 mm

Highest mean error: 2.824354887008667 mm for frame 78

Lowest mean error: 2.212595224380493 mm for frame 114

Saving results

Total time: 32.11352229118347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857990
Iteration 2/25 | Loss: 0.00124050
Iteration 3/25 | Loss: 0.00105953
Iteration 4/25 | Loss: 0.00103642
Iteration 5/25 | Loss: 0.00102808
Iteration 6/25 | Loss: 0.00102942
Iteration 7/25 | Loss: 0.00102627
Iteration 8/25 | Loss: 0.00101212
Iteration 9/25 | Loss: 0.00101076
Iteration 10/25 | Loss: 0.00101051
Iteration 11/25 | Loss: 0.00101038
Iteration 12/25 | Loss: 0.00101012
Iteration 13/25 | Loss: 0.00100930
Iteration 14/25 | Loss: 0.00101447
Iteration 15/25 | Loss: 0.00100700
Iteration 16/25 | Loss: 0.00100544
Iteration 17/25 | Loss: 0.00100519
Iteration 18/25 | Loss: 0.00100508
Iteration 19/25 | Loss: 0.00100507
Iteration 20/25 | Loss: 0.00100507
Iteration 21/25 | Loss: 0.00100507
Iteration 22/25 | Loss: 0.00100506
Iteration 23/25 | Loss: 0.00100506
Iteration 24/25 | Loss: 0.00100506
Iteration 25/25 | Loss: 0.00100506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48451674
Iteration 2/25 | Loss: 0.00073093
Iteration 3/25 | Loss: 0.00073092
Iteration 4/25 | Loss: 0.00073092
Iteration 5/25 | Loss: 0.00073092
Iteration 6/25 | Loss: 0.00073092
Iteration 7/25 | Loss: 0.00073092
Iteration 8/25 | Loss: 0.00073092
Iteration 9/25 | Loss: 0.00073092
Iteration 10/25 | Loss: 0.00073092
Iteration 11/25 | Loss: 0.00073092
Iteration 12/25 | Loss: 0.00073092
Iteration 13/25 | Loss: 0.00073092
Iteration 14/25 | Loss: 0.00073092
Iteration 15/25 | Loss: 0.00073092
Iteration 16/25 | Loss: 0.00073092
Iteration 17/25 | Loss: 0.00073092
Iteration 18/25 | Loss: 0.00073092
Iteration 19/25 | Loss: 0.00073092
Iteration 20/25 | Loss: 0.00073092
Iteration 21/25 | Loss: 0.00073092
Iteration 22/25 | Loss: 0.00073092
Iteration 23/25 | Loss: 0.00073092
Iteration 24/25 | Loss: 0.00073092
Iteration 25/25 | Loss: 0.00073092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073092
Iteration 2/1000 | Loss: 0.00002326
Iteration 3/1000 | Loss: 0.00001591
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001256
Iteration 7/1000 | Loss: 0.00001210
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001172
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001122
Iteration 20/1000 | Loss: 0.00001121
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001120
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001114
Iteration 27/1000 | Loss: 0.00001111
Iteration 28/1000 | Loss: 0.00001110
Iteration 29/1000 | Loss: 0.00001110
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001108
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001108
Iteration 35/1000 | Loss: 0.00001107
Iteration 36/1000 | Loss: 0.00001107
Iteration 37/1000 | Loss: 0.00001106
Iteration 38/1000 | Loss: 0.00001106
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001105
Iteration 43/1000 | Loss: 0.00001105
Iteration 44/1000 | Loss: 0.00001104
Iteration 45/1000 | Loss: 0.00001104
Iteration 46/1000 | Loss: 0.00001104
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001103
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001103
Iteration 51/1000 | Loss: 0.00001103
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001103
Iteration 58/1000 | Loss: 0.00001102
Iteration 59/1000 | Loss: 0.00001102
Iteration 60/1000 | Loss: 0.00001102
Iteration 61/1000 | Loss: 0.00001102
Iteration 62/1000 | Loss: 0.00001102
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001102
Iteration 65/1000 | Loss: 0.00001102
Iteration 66/1000 | Loss: 0.00001102
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.1021135833289009e-05, 1.1021135833289009e-05, 1.1021135833289009e-05, 1.1021135833289009e-05, 1.1021135833289009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1021135833289009e-05

Optimization complete. Final v2v error: 2.7092833518981934 mm

Highest mean error: 3.9288182258605957 mm for frame 107

Lowest mean error: 2.2146270275115967 mm for frame 129

Saving results

Total time: 51.15819787979126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456898
Iteration 2/25 | Loss: 0.00143889
Iteration 3/25 | Loss: 0.00120538
Iteration 4/25 | Loss: 0.00116507
Iteration 5/25 | Loss: 0.00115370
Iteration 6/25 | Loss: 0.00114227
Iteration 7/25 | Loss: 0.00112311
Iteration 8/25 | Loss: 0.00110181
Iteration 9/25 | Loss: 0.00108521
Iteration 10/25 | Loss: 0.00107120
Iteration 11/25 | Loss: 0.00105208
Iteration 12/25 | Loss: 0.00104650
Iteration 13/25 | Loss: 0.00104007
Iteration 14/25 | Loss: 0.00103960
Iteration 15/25 | Loss: 0.00103855
Iteration 16/25 | Loss: 0.00103865
Iteration 17/25 | Loss: 0.00103804
Iteration 18/25 | Loss: 0.00103776
Iteration 19/25 | Loss: 0.00103801
Iteration 20/25 | Loss: 0.00103944
Iteration 21/25 | Loss: 0.00103794
Iteration 22/25 | Loss: 0.00103804
Iteration 23/25 | Loss: 0.00103825
Iteration 24/25 | Loss: 0.00103807
Iteration 25/25 | Loss: 0.00103878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41158772
Iteration 2/25 | Loss: 0.00062952
Iteration 3/25 | Loss: 0.00062951
Iteration 4/25 | Loss: 0.00062951
Iteration 5/25 | Loss: 0.00062951
Iteration 6/25 | Loss: 0.00062951
Iteration 7/25 | Loss: 0.00062951
Iteration 8/25 | Loss: 0.00062951
Iteration 9/25 | Loss: 0.00062951
Iteration 10/25 | Loss: 0.00062951
Iteration 11/25 | Loss: 0.00062951
Iteration 12/25 | Loss: 0.00062951
Iteration 13/25 | Loss: 0.00062951
Iteration 14/25 | Loss: 0.00062951
Iteration 15/25 | Loss: 0.00062951
Iteration 16/25 | Loss: 0.00062951
Iteration 17/25 | Loss: 0.00062951
Iteration 18/25 | Loss: 0.00062951
Iteration 19/25 | Loss: 0.00062951
Iteration 20/25 | Loss: 0.00062951
Iteration 21/25 | Loss: 0.00062951
Iteration 22/25 | Loss: 0.00062951
Iteration 23/25 | Loss: 0.00062951
Iteration 24/25 | Loss: 0.00062951
Iteration 25/25 | Loss: 0.00062951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062951
Iteration 2/1000 | Loss: 0.00004950
Iteration 3/1000 | Loss: 0.00005040
Iteration 4/1000 | Loss: 0.00001725
Iteration 5/1000 | Loss: 0.00003770
Iteration 6/1000 | Loss: 0.00002720
Iteration 7/1000 | Loss: 0.00003309
Iteration 8/1000 | Loss: 0.00003513
Iteration 9/1000 | Loss: 0.00003334
Iteration 10/1000 | Loss: 0.00003248
Iteration 11/1000 | Loss: 0.00003376
Iteration 12/1000 | Loss: 0.00003196
Iteration 13/1000 | Loss: 0.00003395
Iteration 14/1000 | Loss: 0.00003809
Iteration 15/1000 | Loss: 0.00003685
Iteration 16/1000 | Loss: 0.00002900
Iteration 17/1000 | Loss: 0.00003059
Iteration 18/1000 | Loss: 0.00002145
Iteration 19/1000 | Loss: 0.00003971
Iteration 20/1000 | Loss: 0.00003011
Iteration 21/1000 | Loss: 0.00002629
Iteration 22/1000 | Loss: 0.00004647
Iteration 23/1000 | Loss: 0.00003765
Iteration 24/1000 | Loss: 0.00004258
Iteration 25/1000 | Loss: 0.00003638
Iteration 26/1000 | Loss: 0.00004867
Iteration 27/1000 | Loss: 0.00004609
Iteration 28/1000 | Loss: 0.00005365
Iteration 29/1000 | Loss: 0.00002415
Iteration 30/1000 | Loss: 0.00003524
Iteration 31/1000 | Loss: 0.00003917
Iteration 32/1000 | Loss: 0.00005182
Iteration 33/1000 | Loss: 0.00002219
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001359
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001237
Iteration 40/1000 | Loss: 0.00001230
Iteration 41/1000 | Loss: 0.00001221
Iteration 42/1000 | Loss: 0.00001221
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001214
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001211
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001210
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001206
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001206
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001206
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001205
Iteration 73/1000 | Loss: 0.00001205
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001202
Iteration 105/1000 | Loss: 0.00001202
Iteration 106/1000 | Loss: 0.00001202
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001202
Iteration 112/1000 | Loss: 0.00001202
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001201
Iteration 115/1000 | Loss: 0.00001201
Iteration 116/1000 | Loss: 0.00001201
Iteration 117/1000 | Loss: 0.00001201
Iteration 118/1000 | Loss: 0.00001201
Iteration 119/1000 | Loss: 0.00001201
Iteration 120/1000 | Loss: 0.00001201
Iteration 121/1000 | Loss: 0.00001201
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001200
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001199
Iteration 133/1000 | Loss: 0.00001199
Iteration 134/1000 | Loss: 0.00001199
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001198
Iteration 149/1000 | Loss: 0.00001198
Iteration 150/1000 | Loss: 0.00001198
Iteration 151/1000 | Loss: 0.00001198
Iteration 152/1000 | Loss: 0.00001198
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001196
Iteration 164/1000 | Loss: 0.00001196
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001195
Iteration 169/1000 | Loss: 0.00001195
Iteration 170/1000 | Loss: 0.00001195
Iteration 171/1000 | Loss: 0.00001195
Iteration 172/1000 | Loss: 0.00001195
Iteration 173/1000 | Loss: 0.00001195
Iteration 174/1000 | Loss: 0.00001195
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Iteration 196/1000 | Loss: 0.00001195
Iteration 197/1000 | Loss: 0.00001195
Iteration 198/1000 | Loss: 0.00001195
Iteration 199/1000 | Loss: 0.00001195
Iteration 200/1000 | Loss: 0.00001195
Iteration 201/1000 | Loss: 0.00001195
Iteration 202/1000 | Loss: 0.00001195
Iteration 203/1000 | Loss: 0.00001195
Iteration 204/1000 | Loss: 0.00001195
Iteration 205/1000 | Loss: 0.00001195
Iteration 206/1000 | Loss: 0.00001195
Iteration 207/1000 | Loss: 0.00001195
Iteration 208/1000 | Loss: 0.00001195
Iteration 209/1000 | Loss: 0.00001195
Iteration 210/1000 | Loss: 0.00001195
Iteration 211/1000 | Loss: 0.00001195
Iteration 212/1000 | Loss: 0.00001195
Iteration 213/1000 | Loss: 0.00001195
Iteration 214/1000 | Loss: 0.00001195
Iteration 215/1000 | Loss: 0.00001195
Iteration 216/1000 | Loss: 0.00001195
Iteration 217/1000 | Loss: 0.00001195
Iteration 218/1000 | Loss: 0.00001195
Iteration 219/1000 | Loss: 0.00001195
Iteration 220/1000 | Loss: 0.00001195
Iteration 221/1000 | Loss: 0.00001195
Iteration 222/1000 | Loss: 0.00001195
Iteration 223/1000 | Loss: 0.00001195
Iteration 224/1000 | Loss: 0.00001195
Iteration 225/1000 | Loss: 0.00001195
Iteration 226/1000 | Loss: 0.00001195
Iteration 227/1000 | Loss: 0.00001195
Iteration 228/1000 | Loss: 0.00001195
Iteration 229/1000 | Loss: 0.00001195
Iteration 230/1000 | Loss: 0.00001195
Iteration 231/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1949573490710463e-05, 1.1949573490710463e-05, 1.1949573490710463e-05, 1.1949573490710463e-05, 1.1949573490710463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1949573490710463e-05

Optimization complete. Final v2v error: 2.960827350616455 mm

Highest mean error: 3.828822135925293 mm for frame 102

Lowest mean error: 2.8046071529388428 mm for frame 25

Saving results

Total time: 109.00934147834778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524101
Iteration 2/25 | Loss: 0.00127463
Iteration 3/25 | Loss: 0.00109520
Iteration 4/25 | Loss: 0.00104526
Iteration 5/25 | Loss: 0.00103329
Iteration 6/25 | Loss: 0.00104777
Iteration 7/25 | Loss: 0.00101797
Iteration 8/25 | Loss: 0.00102227
Iteration 9/25 | Loss: 0.00100708
Iteration 10/25 | Loss: 0.00099891
Iteration 11/25 | Loss: 0.00100307
Iteration 12/25 | Loss: 0.00099958
Iteration 13/25 | Loss: 0.00099837
Iteration 14/25 | Loss: 0.00099566
Iteration 15/25 | Loss: 0.00099490
Iteration 16/25 | Loss: 0.00099319
Iteration 17/25 | Loss: 0.00099318
Iteration 18/25 | Loss: 0.00099359
Iteration 19/25 | Loss: 0.00099254
Iteration 20/25 | Loss: 0.00099262
Iteration 21/25 | Loss: 0.00099230
Iteration 22/25 | Loss: 0.00099269
Iteration 23/25 | Loss: 0.00099584
Iteration 24/25 | Loss: 0.00099639
Iteration 25/25 | Loss: 0.00099584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43375707
Iteration 2/25 | Loss: 0.00100172
Iteration 3/25 | Loss: 0.00100172
Iteration 4/25 | Loss: 0.00100171
Iteration 5/25 | Loss: 0.00100171
Iteration 6/25 | Loss: 0.00100171
Iteration 7/25 | Loss: 0.00100171
Iteration 8/25 | Loss: 0.00100171
Iteration 9/25 | Loss: 0.00100171
Iteration 10/25 | Loss: 0.00100171
Iteration 11/25 | Loss: 0.00100171
Iteration 12/25 | Loss: 0.00100171
Iteration 13/25 | Loss: 0.00100171
Iteration 14/25 | Loss: 0.00100171
Iteration 15/25 | Loss: 0.00100171
Iteration 16/25 | Loss: 0.00100171
Iteration 17/25 | Loss: 0.00100171
Iteration 18/25 | Loss: 0.00100171
Iteration 19/25 | Loss: 0.00100171
Iteration 20/25 | Loss: 0.00100171
Iteration 21/25 | Loss: 0.00100171
Iteration 22/25 | Loss: 0.00100171
Iteration 23/25 | Loss: 0.00100171
Iteration 24/25 | Loss: 0.00100171
Iteration 25/25 | Loss: 0.00100171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100171
Iteration 2/1000 | Loss: 0.00003713
Iteration 3/1000 | Loss: 0.00003217
Iteration 4/1000 | Loss: 0.00003477
Iteration 5/1000 | Loss: 0.00003141
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00001386
Iteration 8/1000 | Loss: 0.00002499
Iteration 9/1000 | Loss: 0.00002535
Iteration 10/1000 | Loss: 0.00002549
Iteration 11/1000 | Loss: 0.00002700
Iteration 12/1000 | Loss: 0.00001630
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002697
Iteration 15/1000 | Loss: 0.00002755
Iteration 16/1000 | Loss: 0.00002748
Iteration 17/1000 | Loss: 0.00002694
Iteration 18/1000 | Loss: 0.00002738
Iteration 19/1000 | Loss: 0.00002507
Iteration 20/1000 | Loss: 0.00002712
Iteration 21/1000 | Loss: 0.00002555
Iteration 22/1000 | Loss: 0.00002764
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00002633
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00003986
Iteration 27/1000 | Loss: 0.00002640
Iteration 28/1000 | Loss: 0.00002591
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00003286
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00002711
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00000989
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000943
Iteration 39/1000 | Loss: 0.00000941
Iteration 40/1000 | Loss: 0.00000936
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000924
Iteration 43/1000 | Loss: 0.00000921
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000920
Iteration 46/1000 | Loss: 0.00000919
Iteration 47/1000 | Loss: 0.00000919
Iteration 48/1000 | Loss: 0.00000919
Iteration 49/1000 | Loss: 0.00000918
Iteration 50/1000 | Loss: 0.00000915
Iteration 51/1000 | Loss: 0.00000915
Iteration 52/1000 | Loss: 0.00000914
Iteration 53/1000 | Loss: 0.00000914
Iteration 54/1000 | Loss: 0.00000913
Iteration 55/1000 | Loss: 0.00000912
Iteration 56/1000 | Loss: 0.00000912
Iteration 57/1000 | Loss: 0.00000912
Iteration 58/1000 | Loss: 0.00000912
Iteration 59/1000 | Loss: 0.00000911
Iteration 60/1000 | Loss: 0.00000911
Iteration 61/1000 | Loss: 0.00000911
Iteration 62/1000 | Loss: 0.00000910
Iteration 63/1000 | Loss: 0.00000910
Iteration 64/1000 | Loss: 0.00000910
Iteration 65/1000 | Loss: 0.00000907
Iteration 66/1000 | Loss: 0.00000907
Iteration 67/1000 | Loss: 0.00000907
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000906
Iteration 71/1000 | Loss: 0.00000906
Iteration 72/1000 | Loss: 0.00000906
Iteration 73/1000 | Loss: 0.00000906
Iteration 74/1000 | Loss: 0.00000906
Iteration 75/1000 | Loss: 0.00000906
Iteration 76/1000 | Loss: 0.00000906
Iteration 77/1000 | Loss: 0.00000906
Iteration 78/1000 | Loss: 0.00000906
Iteration 79/1000 | Loss: 0.00000905
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000900
Iteration 82/1000 | Loss: 0.00000899
Iteration 83/1000 | Loss: 0.00000899
Iteration 84/1000 | Loss: 0.00000898
Iteration 85/1000 | Loss: 0.00000898
Iteration 86/1000 | Loss: 0.00000897
Iteration 87/1000 | Loss: 0.00000896
Iteration 88/1000 | Loss: 0.00000895
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000895
Iteration 91/1000 | Loss: 0.00000895
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000893
Iteration 96/1000 | Loss: 0.00000892
Iteration 97/1000 | Loss: 0.00000892
Iteration 98/1000 | Loss: 0.00000891
Iteration 99/1000 | Loss: 0.00000891
Iteration 100/1000 | Loss: 0.00000891
Iteration 101/1000 | Loss: 0.00000889
Iteration 102/1000 | Loss: 0.00000888
Iteration 103/1000 | Loss: 0.00000887
Iteration 104/1000 | Loss: 0.00000887
Iteration 105/1000 | Loss: 0.00000887
Iteration 106/1000 | Loss: 0.00000887
Iteration 107/1000 | Loss: 0.00000887
Iteration 108/1000 | Loss: 0.00000887
Iteration 109/1000 | Loss: 0.00000887
Iteration 110/1000 | Loss: 0.00000887
Iteration 111/1000 | Loss: 0.00000886
Iteration 112/1000 | Loss: 0.00000886
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00000886
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000885
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000885
Iteration 121/1000 | Loss: 0.00000885
Iteration 122/1000 | Loss: 0.00000885
Iteration 123/1000 | Loss: 0.00000885
Iteration 124/1000 | Loss: 0.00000885
Iteration 125/1000 | Loss: 0.00000885
Iteration 126/1000 | Loss: 0.00000885
Iteration 127/1000 | Loss: 0.00000885
Iteration 128/1000 | Loss: 0.00000885
Iteration 129/1000 | Loss: 0.00000885
Iteration 130/1000 | Loss: 0.00000884
Iteration 131/1000 | Loss: 0.00000884
Iteration 132/1000 | Loss: 0.00000884
Iteration 133/1000 | Loss: 0.00000884
Iteration 134/1000 | Loss: 0.00000884
Iteration 135/1000 | Loss: 0.00000884
Iteration 136/1000 | Loss: 0.00000884
Iteration 137/1000 | Loss: 0.00000884
Iteration 138/1000 | Loss: 0.00000883
Iteration 139/1000 | Loss: 0.00000883
Iteration 140/1000 | Loss: 0.00000883
Iteration 141/1000 | Loss: 0.00000883
Iteration 142/1000 | Loss: 0.00000883
Iteration 143/1000 | Loss: 0.00000883
Iteration 144/1000 | Loss: 0.00000883
Iteration 145/1000 | Loss: 0.00000883
Iteration 146/1000 | Loss: 0.00000882
Iteration 147/1000 | Loss: 0.00000882
Iteration 148/1000 | Loss: 0.00000882
Iteration 149/1000 | Loss: 0.00000882
Iteration 150/1000 | Loss: 0.00000882
Iteration 151/1000 | Loss: 0.00000882
Iteration 152/1000 | Loss: 0.00000882
Iteration 153/1000 | Loss: 0.00000882
Iteration 154/1000 | Loss: 0.00000882
Iteration 155/1000 | Loss: 0.00000882
Iteration 156/1000 | Loss: 0.00000882
Iteration 157/1000 | Loss: 0.00000882
Iteration 158/1000 | Loss: 0.00000882
Iteration 159/1000 | Loss: 0.00000882
Iteration 160/1000 | Loss: 0.00000882
Iteration 161/1000 | Loss: 0.00000881
Iteration 162/1000 | Loss: 0.00000881
Iteration 163/1000 | Loss: 0.00000881
Iteration 164/1000 | Loss: 0.00000881
Iteration 165/1000 | Loss: 0.00000881
Iteration 166/1000 | Loss: 0.00000881
Iteration 167/1000 | Loss: 0.00000881
Iteration 168/1000 | Loss: 0.00000881
Iteration 169/1000 | Loss: 0.00000881
Iteration 170/1000 | Loss: 0.00000881
Iteration 171/1000 | Loss: 0.00000881
Iteration 172/1000 | Loss: 0.00000881
Iteration 173/1000 | Loss: 0.00000881
Iteration 174/1000 | Loss: 0.00000881
Iteration 175/1000 | Loss: 0.00000881
Iteration 176/1000 | Loss: 0.00000881
Iteration 177/1000 | Loss: 0.00000881
Iteration 178/1000 | Loss: 0.00000881
Iteration 179/1000 | Loss: 0.00000881
Iteration 180/1000 | Loss: 0.00000881
Iteration 181/1000 | Loss: 0.00000881
Iteration 182/1000 | Loss: 0.00000880
Iteration 183/1000 | Loss: 0.00000880
Iteration 184/1000 | Loss: 0.00000880
Iteration 185/1000 | Loss: 0.00000880
Iteration 186/1000 | Loss: 0.00000880
Iteration 187/1000 | Loss: 0.00000880
Iteration 188/1000 | Loss: 0.00000880
Iteration 189/1000 | Loss: 0.00000880
Iteration 190/1000 | Loss: 0.00000880
Iteration 191/1000 | Loss: 0.00000880
Iteration 192/1000 | Loss: 0.00000880
Iteration 193/1000 | Loss: 0.00000880
Iteration 194/1000 | Loss: 0.00000880
Iteration 195/1000 | Loss: 0.00000880
Iteration 196/1000 | Loss: 0.00000880
Iteration 197/1000 | Loss: 0.00000880
Iteration 198/1000 | Loss: 0.00000880
Iteration 199/1000 | Loss: 0.00000880
Iteration 200/1000 | Loss: 0.00000880
Iteration 201/1000 | Loss: 0.00000879
Iteration 202/1000 | Loss: 0.00000879
Iteration 203/1000 | Loss: 0.00000879
Iteration 204/1000 | Loss: 0.00000879
Iteration 205/1000 | Loss: 0.00000879
Iteration 206/1000 | Loss: 0.00000879
Iteration 207/1000 | Loss: 0.00000879
Iteration 208/1000 | Loss: 0.00000879
Iteration 209/1000 | Loss: 0.00000879
Iteration 210/1000 | Loss: 0.00000879
Iteration 211/1000 | Loss: 0.00000879
Iteration 212/1000 | Loss: 0.00000879
Iteration 213/1000 | Loss: 0.00000879
Iteration 214/1000 | Loss: 0.00000879
Iteration 215/1000 | Loss: 0.00000878
Iteration 216/1000 | Loss: 0.00000878
Iteration 217/1000 | Loss: 0.00000878
Iteration 218/1000 | Loss: 0.00000878
Iteration 219/1000 | Loss: 0.00000878
Iteration 220/1000 | Loss: 0.00000878
Iteration 221/1000 | Loss: 0.00000878
Iteration 222/1000 | Loss: 0.00000878
Iteration 223/1000 | Loss: 0.00000878
Iteration 224/1000 | Loss: 0.00000878
Iteration 225/1000 | Loss: 0.00000878
Iteration 226/1000 | Loss: 0.00000878
Iteration 227/1000 | Loss: 0.00000878
Iteration 228/1000 | Loss: 0.00000878
Iteration 229/1000 | Loss: 0.00000878
Iteration 230/1000 | Loss: 0.00000878
Iteration 231/1000 | Loss: 0.00000878
Iteration 232/1000 | Loss: 0.00000878
Iteration 233/1000 | Loss: 0.00000878
Iteration 234/1000 | Loss: 0.00000878
Iteration 235/1000 | Loss: 0.00000878
Iteration 236/1000 | Loss: 0.00000878
Iteration 237/1000 | Loss: 0.00000878
Iteration 238/1000 | Loss: 0.00000878
Iteration 239/1000 | Loss: 0.00000878
Iteration 240/1000 | Loss: 0.00000878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [8.775787136983126e-06, 8.775787136983126e-06, 8.775787136983126e-06, 8.775787136983126e-06, 8.775787136983126e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.775787136983126e-06

Optimization complete. Final v2v error: 2.538785457611084 mm

Highest mean error: 3.5958986282348633 mm for frame 83

Lowest mean error: 2.2509121894836426 mm for frame 123

Saving results

Total time: 114.41427159309387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805346
Iteration 2/25 | Loss: 0.00134855
Iteration 3/25 | Loss: 0.00111515
Iteration 4/25 | Loss: 0.00109974
Iteration 5/25 | Loss: 0.00109890
Iteration 6/25 | Loss: 0.00109890
Iteration 7/25 | Loss: 0.00109890
Iteration 8/25 | Loss: 0.00109890
Iteration 9/25 | Loss: 0.00109890
Iteration 10/25 | Loss: 0.00109890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010989020811393857, 0.0010989020811393857, 0.0010989020811393857, 0.0010989020811393857, 0.0010989020811393857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010989020811393857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35317910
Iteration 2/25 | Loss: 0.00045413
Iteration 3/25 | Loss: 0.00045413
Iteration 4/25 | Loss: 0.00045413
Iteration 5/25 | Loss: 0.00045413
Iteration 6/25 | Loss: 0.00045413
Iteration 7/25 | Loss: 0.00045413
Iteration 8/25 | Loss: 0.00045412
Iteration 9/25 | Loss: 0.00045412
Iteration 10/25 | Loss: 0.00045412
Iteration 11/25 | Loss: 0.00045412
Iteration 12/25 | Loss: 0.00045412
Iteration 13/25 | Loss: 0.00045412
Iteration 14/25 | Loss: 0.00045412
Iteration 15/25 | Loss: 0.00045412
Iteration 16/25 | Loss: 0.00045412
Iteration 17/25 | Loss: 0.00045412
Iteration 18/25 | Loss: 0.00045412
Iteration 19/25 | Loss: 0.00045412
Iteration 20/25 | Loss: 0.00045412
Iteration 21/25 | Loss: 0.00045412
Iteration 22/25 | Loss: 0.00045412
Iteration 23/25 | Loss: 0.00045412
Iteration 24/25 | Loss: 0.00045412
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00045412444160319865, 0.00045412444160319865, 0.00045412444160319865, 0.00045412444160319865, 0.00045412444160319865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045412444160319865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045412
Iteration 2/1000 | Loss: 0.00002806
Iteration 3/1000 | Loss: 0.00001955
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001700
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001541
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001527
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001519
Iteration 17/1000 | Loss: 0.00001518
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001518
Iteration 20/1000 | Loss: 0.00001518
Iteration 21/1000 | Loss: 0.00001517
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001515
Iteration 30/1000 | Loss: 0.00001515
Iteration 31/1000 | Loss: 0.00001515
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001512
Iteration 34/1000 | Loss: 0.00001512
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001512
Iteration 39/1000 | Loss: 0.00001512
Iteration 40/1000 | Loss: 0.00001511
Iteration 41/1000 | Loss: 0.00001511
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001511
Iteration 45/1000 | Loss: 0.00001510
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001509
Iteration 51/1000 | Loss: 0.00001509
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001509
Iteration 56/1000 | Loss: 0.00001509
Iteration 57/1000 | Loss: 0.00001509
Iteration 58/1000 | Loss: 0.00001508
Iteration 59/1000 | Loss: 0.00001508
Iteration 60/1000 | Loss: 0.00001508
Iteration 61/1000 | Loss: 0.00001508
Iteration 62/1000 | Loss: 0.00001508
Iteration 63/1000 | Loss: 0.00001508
Iteration 64/1000 | Loss: 0.00001508
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001507
Iteration 75/1000 | Loss: 0.00001507
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001507
Iteration 79/1000 | Loss: 0.00001507
Iteration 80/1000 | Loss: 0.00001507
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001506
Iteration 96/1000 | Loss: 0.00001506
Iteration 97/1000 | Loss: 0.00001506
Iteration 98/1000 | Loss: 0.00001506
Iteration 99/1000 | Loss: 0.00001506
Iteration 100/1000 | Loss: 0.00001506
Iteration 101/1000 | Loss: 0.00001506
Iteration 102/1000 | Loss: 0.00001506
Iteration 103/1000 | Loss: 0.00001506
Iteration 104/1000 | Loss: 0.00001506
Iteration 105/1000 | Loss: 0.00001506
Iteration 106/1000 | Loss: 0.00001506
Iteration 107/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.5061901649460196e-05, 1.5061901649460196e-05, 1.5061901649460196e-05, 1.5061901649460196e-05, 1.5061901649460196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5061901649460196e-05

Optimization complete. Final v2v error: 3.2593777179718018 mm

Highest mean error: 3.6106998920440674 mm for frame 55

Lowest mean error: 3.115921974182129 mm for frame 165

Saving results

Total time: 29.824915885925293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548427
Iteration 2/25 | Loss: 0.00136376
Iteration 3/25 | Loss: 0.00116198
Iteration 4/25 | Loss: 0.00113785
Iteration 5/25 | Loss: 0.00113224
Iteration 6/25 | Loss: 0.00113204
Iteration 7/25 | Loss: 0.00113204
Iteration 8/25 | Loss: 0.00113204
Iteration 9/25 | Loss: 0.00113204
Iteration 10/25 | Loss: 0.00113204
Iteration 11/25 | Loss: 0.00113204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011320358607918024, 0.0011320358607918024, 0.0011320358607918024, 0.0011320358607918024, 0.0011320358607918024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011320358607918024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40065324
Iteration 2/25 | Loss: 0.00088758
Iteration 3/25 | Loss: 0.00088758
Iteration 4/25 | Loss: 0.00088758
Iteration 5/25 | Loss: 0.00088758
Iteration 6/25 | Loss: 0.00088758
Iteration 7/25 | Loss: 0.00088758
Iteration 8/25 | Loss: 0.00088758
Iteration 9/25 | Loss: 0.00088758
Iteration 10/25 | Loss: 0.00088758
Iteration 11/25 | Loss: 0.00088758
Iteration 12/25 | Loss: 0.00088758
Iteration 13/25 | Loss: 0.00088758
Iteration 14/25 | Loss: 0.00088758
Iteration 15/25 | Loss: 0.00088758
Iteration 16/25 | Loss: 0.00088758
Iteration 17/25 | Loss: 0.00088758
Iteration 18/25 | Loss: 0.00088758
Iteration 19/25 | Loss: 0.00088758
Iteration 20/25 | Loss: 0.00088758
Iteration 21/25 | Loss: 0.00088758
Iteration 22/25 | Loss: 0.00088758
Iteration 23/25 | Loss: 0.00088758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008875809144228697, 0.0008875809144228697, 0.0008875809144228697, 0.0008875809144228697, 0.0008875809144228697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008875809144228697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088758
Iteration 2/1000 | Loss: 0.00004893
Iteration 3/1000 | Loss: 0.00003105
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00002745
Iteration 6/1000 | Loss: 0.00002668
Iteration 7/1000 | Loss: 0.00002615
Iteration 8/1000 | Loss: 0.00002582
Iteration 9/1000 | Loss: 0.00002556
Iteration 10/1000 | Loss: 0.00002537
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002516
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002513
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00002513
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002512
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002511
Iteration 23/1000 | Loss: 0.00002511
Iteration 24/1000 | Loss: 0.00002511
Iteration 25/1000 | Loss: 0.00002511
Iteration 26/1000 | Loss: 0.00002511
Iteration 27/1000 | Loss: 0.00002511
Iteration 28/1000 | Loss: 0.00002510
Iteration 29/1000 | Loss: 0.00002510
Iteration 30/1000 | Loss: 0.00002510
Iteration 31/1000 | Loss: 0.00002509
Iteration 32/1000 | Loss: 0.00002508
Iteration 33/1000 | Loss: 0.00002508
Iteration 34/1000 | Loss: 0.00002508
Iteration 35/1000 | Loss: 0.00002508
Iteration 36/1000 | Loss: 0.00002508
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002507
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002507
Iteration 41/1000 | Loss: 0.00002507
Iteration 42/1000 | Loss: 0.00002507
Iteration 43/1000 | Loss: 0.00002507
Iteration 44/1000 | Loss: 0.00002507
Iteration 45/1000 | Loss: 0.00002507
Iteration 46/1000 | Loss: 0.00002507
Iteration 47/1000 | Loss: 0.00002506
Iteration 48/1000 | Loss: 0.00002506
Iteration 49/1000 | Loss: 0.00002505
Iteration 50/1000 | Loss: 0.00002505
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002504
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002502
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002502
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002500
Iteration 65/1000 | Loss: 0.00002500
Iteration 66/1000 | Loss: 0.00002500
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002498
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002497
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002497
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002497
Iteration 80/1000 | Loss: 0.00002497
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002496
Iteration 85/1000 | Loss: 0.00002496
Iteration 86/1000 | Loss: 0.00002496
Iteration 87/1000 | Loss: 0.00002496
Iteration 88/1000 | Loss: 0.00002496
Iteration 89/1000 | Loss: 0.00002496
Iteration 90/1000 | Loss: 0.00002496
Iteration 91/1000 | Loss: 0.00002496
Iteration 92/1000 | Loss: 0.00002496
Iteration 93/1000 | Loss: 0.00002496
Iteration 94/1000 | Loss: 0.00002496
Iteration 95/1000 | Loss: 0.00002496
Iteration 96/1000 | Loss: 0.00002496
Iteration 97/1000 | Loss: 0.00002496
Iteration 98/1000 | Loss: 0.00002496
Iteration 99/1000 | Loss: 0.00002496
Iteration 100/1000 | Loss: 0.00002496
Iteration 101/1000 | Loss: 0.00002496
Iteration 102/1000 | Loss: 0.00002496
Iteration 103/1000 | Loss: 0.00002496
Iteration 104/1000 | Loss: 0.00002496
Iteration 105/1000 | Loss: 0.00002496
Iteration 106/1000 | Loss: 0.00002496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.4961093004094437e-05, 2.4961093004094437e-05, 2.4961093004094437e-05, 2.4961093004094437e-05, 2.4961093004094437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4961093004094437e-05

Optimization complete. Final v2v error: 4.100286960601807 mm

Highest mean error: 4.762990474700928 mm for frame 88

Lowest mean error: 3.638244867324829 mm for frame 122

Saving results

Total time: 33.474282026290894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01083947
Iteration 2/25 | Loss: 0.00634304
Iteration 3/25 | Loss: 0.00381773
Iteration 4/25 | Loss: 0.00335096
Iteration 5/25 | Loss: 0.00257942
Iteration 6/25 | Loss: 0.00214294
Iteration 7/25 | Loss: 0.00180743
Iteration 8/25 | Loss: 0.00149217
Iteration 9/25 | Loss: 0.00139251
Iteration 10/25 | Loss: 0.00131374
Iteration 11/25 | Loss: 0.00127452
Iteration 12/25 | Loss: 0.00121946
Iteration 13/25 | Loss: 0.00123380
Iteration 14/25 | Loss: 0.00121127
Iteration 15/25 | Loss: 0.00120765
Iteration 16/25 | Loss: 0.00120971
Iteration 17/25 | Loss: 0.00121946
Iteration 18/25 | Loss: 0.00119785
Iteration 19/25 | Loss: 0.00119549
Iteration 20/25 | Loss: 0.00119275
Iteration 21/25 | Loss: 0.00119376
Iteration 22/25 | Loss: 0.00119249
Iteration 23/25 | Loss: 0.00119249
Iteration 24/25 | Loss: 0.00119248
Iteration 25/25 | Loss: 0.00119246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57020015
Iteration 2/25 | Loss: 0.00081204
Iteration 3/25 | Loss: 0.00081204
Iteration 4/25 | Loss: 0.00081204
Iteration 5/25 | Loss: 0.00081204
Iteration 6/25 | Loss: 0.00081204
Iteration 7/25 | Loss: 0.00081204
Iteration 8/25 | Loss: 0.00081204
Iteration 9/25 | Loss: 0.00081204
Iteration 10/25 | Loss: 0.00081204
Iteration 11/25 | Loss: 0.00081204
Iteration 12/25 | Loss: 0.00081204
Iteration 13/25 | Loss: 0.00081204
Iteration 14/25 | Loss: 0.00081204
Iteration 15/25 | Loss: 0.00081204
Iteration 16/25 | Loss: 0.00081204
Iteration 17/25 | Loss: 0.00081204
Iteration 18/25 | Loss: 0.00081204
Iteration 19/25 | Loss: 0.00081204
Iteration 20/25 | Loss: 0.00081204
Iteration 21/25 | Loss: 0.00081204
Iteration 22/25 | Loss: 0.00081204
Iteration 23/25 | Loss: 0.00081204
Iteration 24/25 | Loss: 0.00081204
Iteration 25/25 | Loss: 0.00081204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081204
Iteration 2/1000 | Loss: 0.00007231
Iteration 3/1000 | Loss: 0.00003438
Iteration 4/1000 | Loss: 0.00003028
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002805
Iteration 7/1000 | Loss: 0.00002746
Iteration 8/1000 | Loss: 0.00002707
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002652
Iteration 11/1000 | Loss: 0.00002622
Iteration 12/1000 | Loss: 0.00202643
Iteration 13/1000 | Loss: 0.00002944
Iteration 14/1000 | Loss: 0.00002547
Iteration 15/1000 | Loss: 0.00004756
Iteration 16/1000 | Loss: 0.00002263
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002695
Iteration 20/1000 | Loss: 0.00001946
Iteration 21/1000 | Loss: 0.00005617
Iteration 22/1000 | Loss: 0.00017896
Iteration 23/1000 | Loss: 0.00004874
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001762
Iteration 27/1000 | Loss: 0.00001743
Iteration 28/1000 | Loss: 0.00001741
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001726
Iteration 31/1000 | Loss: 0.00001725
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001720
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001714
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001713
Iteration 53/1000 | Loss: 0.00001713
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001705
Iteration 110/1000 | Loss: 0.00001705
Iteration 111/1000 | Loss: 0.00001705
Iteration 112/1000 | Loss: 0.00001705
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001705
Iteration 118/1000 | Loss: 0.00001705
Iteration 119/1000 | Loss: 0.00001705
Iteration 120/1000 | Loss: 0.00001705
Iteration 121/1000 | Loss: 0.00001705
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001704
Iteration 124/1000 | Loss: 0.00001704
Iteration 125/1000 | Loss: 0.00001704
Iteration 126/1000 | Loss: 0.00001704
Iteration 127/1000 | Loss: 0.00001704
Iteration 128/1000 | Loss: 0.00001704
Iteration 129/1000 | Loss: 0.00001703
Iteration 130/1000 | Loss: 0.00001703
Iteration 131/1000 | Loss: 0.00001703
Iteration 132/1000 | Loss: 0.00001703
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001703
Iteration 135/1000 | Loss: 0.00001703
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001702
Iteration 144/1000 | Loss: 0.00001702
Iteration 145/1000 | Loss: 0.00001702
Iteration 146/1000 | Loss: 0.00001702
Iteration 147/1000 | Loss: 0.00001702
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001701
Iteration 162/1000 | Loss: 0.00001701
Iteration 163/1000 | Loss: 0.00001701
Iteration 164/1000 | Loss: 0.00001701
Iteration 165/1000 | Loss: 0.00001701
Iteration 166/1000 | Loss: 0.00001701
Iteration 167/1000 | Loss: 0.00001701
Iteration 168/1000 | Loss: 0.00001701
Iteration 169/1000 | Loss: 0.00001701
Iteration 170/1000 | Loss: 0.00001701
Iteration 171/1000 | Loss: 0.00001701
Iteration 172/1000 | Loss: 0.00001701
Iteration 173/1000 | Loss: 0.00001701
Iteration 174/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.7012960597639903e-05, 1.7012960597639903e-05, 1.7012960597639903e-05, 1.7012960597639903e-05, 1.7012960597639903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7012960597639903e-05

Optimization complete. Final v2v error: 3.520251512527466 mm

Highest mean error: 3.763969659805298 mm for frame 134

Lowest mean error: 3.4128477573394775 mm for frame 44

Saving results

Total time: 97.34182977676392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825566
Iteration 2/25 | Loss: 0.00132906
Iteration 3/25 | Loss: 0.00110378
Iteration 4/25 | Loss: 0.00108303
Iteration 5/25 | Loss: 0.00108158
Iteration 6/25 | Loss: 0.00108158
Iteration 7/25 | Loss: 0.00108158
Iteration 8/25 | Loss: 0.00108158
Iteration 9/25 | Loss: 0.00108158
Iteration 10/25 | Loss: 0.00108158
Iteration 11/25 | Loss: 0.00108158
Iteration 12/25 | Loss: 0.00108158
Iteration 13/25 | Loss: 0.00108158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010815798304975033, 0.0010815798304975033, 0.0010815798304975033, 0.0010815798304975033, 0.0010815798304975033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010815798304975033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99074996
Iteration 2/25 | Loss: 0.00033987
Iteration 3/25 | Loss: 0.00033986
Iteration 4/25 | Loss: 0.00033986
Iteration 5/25 | Loss: 0.00033986
Iteration 6/25 | Loss: 0.00033986
Iteration 7/25 | Loss: 0.00033986
Iteration 8/25 | Loss: 0.00033986
Iteration 9/25 | Loss: 0.00033986
Iteration 10/25 | Loss: 0.00033986
Iteration 11/25 | Loss: 0.00033986
Iteration 12/25 | Loss: 0.00033986
Iteration 13/25 | Loss: 0.00033985
Iteration 14/25 | Loss: 0.00033985
Iteration 15/25 | Loss: 0.00033985
Iteration 16/25 | Loss: 0.00033985
Iteration 17/25 | Loss: 0.00033985
Iteration 18/25 | Loss: 0.00033985
Iteration 19/25 | Loss: 0.00033985
Iteration 20/25 | Loss: 0.00033985
Iteration 21/25 | Loss: 0.00033985
Iteration 22/25 | Loss: 0.00033985
Iteration 23/25 | Loss: 0.00033985
Iteration 24/25 | Loss: 0.00033985
Iteration 25/25 | Loss: 0.00033985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033985
Iteration 2/1000 | Loss: 0.00003196
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002165
Iteration 5/1000 | Loss: 0.00002061
Iteration 6/1000 | Loss: 0.00002006
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001884
Iteration 12/1000 | Loss: 0.00001884
Iteration 13/1000 | Loss: 0.00001883
Iteration 14/1000 | Loss: 0.00001882
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001876
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001872
Iteration 29/1000 | Loss: 0.00001872
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001861
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001861
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001860
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001859
Iteration 51/1000 | Loss: 0.00001859
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001853
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001849
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001848
Iteration 109/1000 | Loss: 0.00001848
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001844
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001844
Iteration 135/1000 | Loss: 0.00001844
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001843
Iteration 142/1000 | Loss: 0.00001843
Iteration 143/1000 | Loss: 0.00001843
Iteration 144/1000 | Loss: 0.00001843
Iteration 145/1000 | Loss: 0.00001842
Iteration 146/1000 | Loss: 0.00001842
Iteration 147/1000 | Loss: 0.00001842
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001841
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001839
Iteration 156/1000 | Loss: 0.00001839
Iteration 157/1000 | Loss: 0.00001838
Iteration 158/1000 | Loss: 0.00001838
Iteration 159/1000 | Loss: 0.00001838
Iteration 160/1000 | Loss: 0.00001837
Iteration 161/1000 | Loss: 0.00001837
Iteration 162/1000 | Loss: 0.00001837
Iteration 163/1000 | Loss: 0.00001837
Iteration 164/1000 | Loss: 0.00001837
Iteration 165/1000 | Loss: 0.00001837
Iteration 166/1000 | Loss: 0.00001837
Iteration 167/1000 | Loss: 0.00001837
Iteration 168/1000 | Loss: 0.00001837
Iteration 169/1000 | Loss: 0.00001837
Iteration 170/1000 | Loss: 0.00001837
Iteration 171/1000 | Loss: 0.00001837
Iteration 172/1000 | Loss: 0.00001837
Iteration 173/1000 | Loss: 0.00001837
Iteration 174/1000 | Loss: 0.00001837
Iteration 175/1000 | Loss: 0.00001837
Iteration 176/1000 | Loss: 0.00001837
Iteration 177/1000 | Loss: 0.00001837
Iteration 178/1000 | Loss: 0.00001837
Iteration 179/1000 | Loss: 0.00001837
Iteration 180/1000 | Loss: 0.00001837
Iteration 181/1000 | Loss: 0.00001837
Iteration 182/1000 | Loss: 0.00001837
Iteration 183/1000 | Loss: 0.00001836
Iteration 184/1000 | Loss: 0.00001836
Iteration 185/1000 | Loss: 0.00001836
Iteration 186/1000 | Loss: 0.00001836
Iteration 187/1000 | Loss: 0.00001836
Iteration 188/1000 | Loss: 0.00001836
Iteration 189/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.8364915376878344e-05, 1.8364915376878344e-05, 1.8364915376878344e-05, 1.8364915376878344e-05, 1.8364915376878344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8364915376878344e-05

Optimization complete. Final v2v error: 3.579586982727051 mm

Highest mean error: 3.68733286857605 mm for frame 119

Lowest mean error: 3.4612174034118652 mm for frame 95

Saving results

Total time: 35.31528186798096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849270
Iteration 2/25 | Loss: 0.00131272
Iteration 3/25 | Loss: 0.00108376
Iteration 4/25 | Loss: 0.00105954
Iteration 5/25 | Loss: 0.00105270
Iteration 6/25 | Loss: 0.00105172
Iteration 7/25 | Loss: 0.00105172
Iteration 8/25 | Loss: 0.00105172
Iteration 9/25 | Loss: 0.00105172
Iteration 10/25 | Loss: 0.00105172
Iteration 11/25 | Loss: 0.00105172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010517234914004803, 0.0010517234914004803, 0.0010517234914004803, 0.0010517234914004803, 0.0010517234914004803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010517234914004803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91834742
Iteration 2/25 | Loss: 0.00034273
Iteration 3/25 | Loss: 0.00034273
Iteration 4/25 | Loss: 0.00034272
Iteration 5/25 | Loss: 0.00034272
Iteration 6/25 | Loss: 0.00034272
Iteration 7/25 | Loss: 0.00034272
Iteration 8/25 | Loss: 0.00034272
Iteration 9/25 | Loss: 0.00034272
Iteration 10/25 | Loss: 0.00034272
Iteration 11/25 | Loss: 0.00034272
Iteration 12/25 | Loss: 0.00034272
Iteration 13/25 | Loss: 0.00034272
Iteration 14/25 | Loss: 0.00034272
Iteration 15/25 | Loss: 0.00034272
Iteration 16/25 | Loss: 0.00034272
Iteration 17/25 | Loss: 0.00034272
Iteration 18/25 | Loss: 0.00034272
Iteration 19/25 | Loss: 0.00034272
Iteration 20/25 | Loss: 0.00034272
Iteration 21/25 | Loss: 0.00034272
Iteration 22/25 | Loss: 0.00034272
Iteration 23/25 | Loss: 0.00034272
Iteration 24/25 | Loss: 0.00034272
Iteration 25/25 | Loss: 0.00034272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034272
Iteration 2/1000 | Loss: 0.00003737
Iteration 3/1000 | Loss: 0.00003008
Iteration 4/1000 | Loss: 0.00002748
Iteration 5/1000 | Loss: 0.00002633
Iteration 6/1000 | Loss: 0.00002544
Iteration 7/1000 | Loss: 0.00002510
Iteration 8/1000 | Loss: 0.00002486
Iteration 9/1000 | Loss: 0.00002467
Iteration 10/1000 | Loss: 0.00002464
Iteration 11/1000 | Loss: 0.00002458
Iteration 12/1000 | Loss: 0.00002447
Iteration 13/1000 | Loss: 0.00002446
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002429
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002429
Iteration 18/1000 | Loss: 0.00002429
Iteration 19/1000 | Loss: 0.00002429
Iteration 20/1000 | Loss: 0.00002429
Iteration 21/1000 | Loss: 0.00002428
Iteration 22/1000 | Loss: 0.00002428
Iteration 23/1000 | Loss: 0.00002428
Iteration 24/1000 | Loss: 0.00002428
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00002428
Iteration 29/1000 | Loss: 0.00002427
Iteration 30/1000 | Loss: 0.00002427
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002427
Iteration 33/1000 | Loss: 0.00002427
Iteration 34/1000 | Loss: 0.00002427
Iteration 35/1000 | Loss: 0.00002427
Iteration 36/1000 | Loss: 0.00002427
Iteration 37/1000 | Loss: 0.00002427
Iteration 38/1000 | Loss: 0.00002427
Iteration 39/1000 | Loss: 0.00002427
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002426
Iteration 42/1000 | Loss: 0.00002426
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002422
Iteration 48/1000 | Loss: 0.00002422
Iteration 49/1000 | Loss: 0.00002422
Iteration 50/1000 | Loss: 0.00002422
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002422
Iteration 53/1000 | Loss: 0.00002422
Iteration 54/1000 | Loss: 0.00002422
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002422
Iteration 57/1000 | Loss: 0.00002422
Iteration 58/1000 | Loss: 0.00002422
Iteration 59/1000 | Loss: 0.00002422
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002421
Iteration 65/1000 | Loss: 0.00002421
Iteration 66/1000 | Loss: 0.00002421
Iteration 67/1000 | Loss: 0.00002421
Iteration 68/1000 | Loss: 0.00002421
Iteration 69/1000 | Loss: 0.00002421
Iteration 70/1000 | Loss: 0.00002421
Iteration 71/1000 | Loss: 0.00002421
Iteration 72/1000 | Loss: 0.00002421
Iteration 73/1000 | Loss: 0.00002421
Iteration 74/1000 | Loss: 0.00002421
Iteration 75/1000 | Loss: 0.00002421
Iteration 76/1000 | Loss: 0.00002421
Iteration 77/1000 | Loss: 0.00002421
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002421
Iteration 80/1000 | Loss: 0.00002421
Iteration 81/1000 | Loss: 0.00002421
Iteration 82/1000 | Loss: 0.00002421
Iteration 83/1000 | Loss: 0.00002421
Iteration 84/1000 | Loss: 0.00002421
Iteration 85/1000 | Loss: 0.00002421
Iteration 86/1000 | Loss: 0.00002421
Iteration 87/1000 | Loss: 0.00002421
Iteration 88/1000 | Loss: 0.00002421
Iteration 89/1000 | Loss: 0.00002421
Iteration 90/1000 | Loss: 0.00002421
Iteration 91/1000 | Loss: 0.00002421
Iteration 92/1000 | Loss: 0.00002421
Iteration 93/1000 | Loss: 0.00002421
Iteration 94/1000 | Loss: 0.00002421
Iteration 95/1000 | Loss: 0.00002421
Iteration 96/1000 | Loss: 0.00002421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.4212153221014887e-05, 2.4212153221014887e-05, 2.4212153221014887e-05, 2.4212153221014887e-05, 2.4212153221014887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4212153221014887e-05

Optimization complete. Final v2v error: 4.146684646606445 mm

Highest mean error: 4.253168106079102 mm for frame 1

Lowest mean error: 4.0536208152771 mm for frame 111

Saving results

Total time: 28.690557718276978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473987
Iteration 2/25 | Loss: 0.00113048
Iteration 3/25 | Loss: 0.00102838
Iteration 4/25 | Loss: 0.00101681
Iteration 5/25 | Loss: 0.00101426
Iteration 6/25 | Loss: 0.00101426
Iteration 7/25 | Loss: 0.00101426
Iteration 8/25 | Loss: 0.00101426
Iteration 9/25 | Loss: 0.00101426
Iteration 10/25 | Loss: 0.00101426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010142613900825381, 0.0010142613900825381, 0.0010142613900825381, 0.0010142613900825381, 0.0010142613900825381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010142613900825381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70127392
Iteration 2/25 | Loss: 0.00065703
Iteration 3/25 | Loss: 0.00065702
Iteration 4/25 | Loss: 0.00065702
Iteration 5/25 | Loss: 0.00065702
Iteration 6/25 | Loss: 0.00065702
Iteration 7/25 | Loss: 0.00065702
Iteration 8/25 | Loss: 0.00065702
Iteration 9/25 | Loss: 0.00065702
Iteration 10/25 | Loss: 0.00065702
Iteration 11/25 | Loss: 0.00065702
Iteration 12/25 | Loss: 0.00065702
Iteration 13/25 | Loss: 0.00065702
Iteration 14/25 | Loss: 0.00065702
Iteration 15/25 | Loss: 0.00065702
Iteration 16/25 | Loss: 0.00065702
Iteration 17/25 | Loss: 0.00065702
Iteration 18/25 | Loss: 0.00065702
Iteration 19/25 | Loss: 0.00065702
Iteration 20/25 | Loss: 0.00065702
Iteration 21/25 | Loss: 0.00065702
Iteration 22/25 | Loss: 0.00065702
Iteration 23/25 | Loss: 0.00065702
Iteration 24/25 | Loss: 0.00065702
Iteration 25/25 | Loss: 0.00065702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065702
Iteration 2/1000 | Loss: 0.00001814
Iteration 3/1000 | Loss: 0.00001451
Iteration 4/1000 | Loss: 0.00001340
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001242
Iteration 7/1000 | Loss: 0.00001211
Iteration 8/1000 | Loss: 0.00001181
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001158
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001129
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001123
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001119
Iteration 32/1000 | Loss: 0.00001119
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001108
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001105
Iteration 40/1000 | Loss: 0.00001105
Iteration 41/1000 | Loss: 0.00001104
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001103
Iteration 44/1000 | Loss: 0.00001103
Iteration 45/1000 | Loss: 0.00001103
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001102
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001099
Iteration 54/1000 | Loss: 0.00001099
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001099
Iteration 57/1000 | Loss: 0.00001098
Iteration 58/1000 | Loss: 0.00001098
Iteration 59/1000 | Loss: 0.00001098
Iteration 60/1000 | Loss: 0.00001098
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001097
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001096
Iteration 66/1000 | Loss: 0.00001096
Iteration 67/1000 | Loss: 0.00001096
Iteration 68/1000 | Loss: 0.00001095
Iteration 69/1000 | Loss: 0.00001095
Iteration 70/1000 | Loss: 0.00001095
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001094
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001094
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001093
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001091
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001088
Iteration 90/1000 | Loss: 0.00001087
Iteration 91/1000 | Loss: 0.00001087
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001085
Iteration 96/1000 | Loss: 0.00001085
Iteration 97/1000 | Loss: 0.00001085
Iteration 98/1000 | Loss: 0.00001085
Iteration 99/1000 | Loss: 0.00001084
Iteration 100/1000 | Loss: 0.00001084
Iteration 101/1000 | Loss: 0.00001084
Iteration 102/1000 | Loss: 0.00001084
Iteration 103/1000 | Loss: 0.00001084
Iteration 104/1000 | Loss: 0.00001084
Iteration 105/1000 | Loss: 0.00001084
Iteration 106/1000 | Loss: 0.00001084
Iteration 107/1000 | Loss: 0.00001084
Iteration 108/1000 | Loss: 0.00001084
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001083
Iteration 111/1000 | Loss: 0.00001083
Iteration 112/1000 | Loss: 0.00001082
Iteration 113/1000 | Loss: 0.00001082
Iteration 114/1000 | Loss: 0.00001081
Iteration 115/1000 | Loss: 0.00001081
Iteration 116/1000 | Loss: 0.00001081
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001080
Iteration 121/1000 | Loss: 0.00001079
Iteration 122/1000 | Loss: 0.00001078
Iteration 123/1000 | Loss: 0.00001078
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001077
Iteration 128/1000 | Loss: 0.00001077
Iteration 129/1000 | Loss: 0.00001077
Iteration 130/1000 | Loss: 0.00001076
Iteration 131/1000 | Loss: 0.00001076
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001075
Iteration 138/1000 | Loss: 0.00001075
Iteration 139/1000 | Loss: 0.00001075
Iteration 140/1000 | Loss: 0.00001075
Iteration 141/1000 | Loss: 0.00001075
Iteration 142/1000 | Loss: 0.00001075
Iteration 143/1000 | Loss: 0.00001075
Iteration 144/1000 | Loss: 0.00001075
Iteration 145/1000 | Loss: 0.00001075
Iteration 146/1000 | Loss: 0.00001075
Iteration 147/1000 | Loss: 0.00001075
Iteration 148/1000 | Loss: 0.00001075
Iteration 149/1000 | Loss: 0.00001075
Iteration 150/1000 | Loss: 0.00001075
Iteration 151/1000 | Loss: 0.00001075
Iteration 152/1000 | Loss: 0.00001075
Iteration 153/1000 | Loss: 0.00001075
Iteration 154/1000 | Loss: 0.00001075
Iteration 155/1000 | Loss: 0.00001075
Iteration 156/1000 | Loss: 0.00001075
Iteration 157/1000 | Loss: 0.00001075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.0748471140686888e-05, 1.0748471140686888e-05, 1.0748471140686888e-05, 1.0748471140686888e-05, 1.0748471140686888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0748471140686888e-05

Optimization complete. Final v2v error: 2.8172733783721924 mm

Highest mean error: 3.029270887374878 mm for frame 88

Lowest mean error: 2.570394992828369 mm for frame 265

Saving results

Total time: 40.74476742744446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007590
Iteration 2/25 | Loss: 0.00254651
Iteration 3/25 | Loss: 0.00180676
Iteration 4/25 | Loss: 0.00167241
Iteration 5/25 | Loss: 0.00160196
Iteration 6/25 | Loss: 0.00159918
Iteration 7/25 | Loss: 0.00148517
Iteration 8/25 | Loss: 0.00142729
Iteration 9/25 | Loss: 0.00139862
Iteration 10/25 | Loss: 0.00137709
Iteration 11/25 | Loss: 0.00134841
Iteration 12/25 | Loss: 0.00133524
Iteration 13/25 | Loss: 0.00132358
Iteration 14/25 | Loss: 0.00132544
Iteration 15/25 | Loss: 0.00132058
Iteration 16/25 | Loss: 0.00131397
Iteration 17/25 | Loss: 0.00130613
Iteration 18/25 | Loss: 0.00130133
Iteration 19/25 | Loss: 0.00130017
Iteration 20/25 | Loss: 0.00129969
Iteration 21/25 | Loss: 0.00129843
Iteration 22/25 | Loss: 0.00129610
Iteration 23/25 | Loss: 0.00129892
Iteration 24/25 | Loss: 0.00129768
Iteration 25/25 | Loss: 0.00129951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33445561
Iteration 2/25 | Loss: 0.00378081
Iteration 3/25 | Loss: 0.00376808
Iteration 4/25 | Loss: 0.00376808
Iteration 5/25 | Loss: 0.00376808
Iteration 6/25 | Loss: 0.00376808
Iteration 7/25 | Loss: 0.00376808
Iteration 8/25 | Loss: 0.00376808
Iteration 9/25 | Loss: 0.00376808
Iteration 10/25 | Loss: 0.00376808
Iteration 11/25 | Loss: 0.00376808
Iteration 12/25 | Loss: 0.00376808
Iteration 13/25 | Loss: 0.00376808
Iteration 14/25 | Loss: 0.00376808
Iteration 15/25 | Loss: 0.00376808
Iteration 16/25 | Loss: 0.00376808
Iteration 17/25 | Loss: 0.00376808
Iteration 18/25 | Loss: 0.00376808
Iteration 19/25 | Loss: 0.00376808
Iteration 20/25 | Loss: 0.00376808
Iteration 21/25 | Loss: 0.00376808
Iteration 22/25 | Loss: 0.00376808
Iteration 23/25 | Loss: 0.00376808
Iteration 24/25 | Loss: 0.00376808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0037680789828300476, 0.0037680789828300476, 0.0037680789828300476, 0.0037680789828300476, 0.0037680789828300476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037680789828300476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00376808
Iteration 2/1000 | Loss: 0.00191881
Iteration 3/1000 | Loss: 0.00049891
Iteration 4/1000 | Loss: 0.00053842
Iteration 5/1000 | Loss: 0.00032053
Iteration 6/1000 | Loss: 0.00061837
Iteration 7/1000 | Loss: 0.00096942
Iteration 8/1000 | Loss: 0.00025141
Iteration 9/1000 | Loss: 0.00189569
Iteration 10/1000 | Loss: 0.00107867
Iteration 11/1000 | Loss: 0.00062883
Iteration 12/1000 | Loss: 0.00114206
Iteration 13/1000 | Loss: 0.00065985
Iteration 14/1000 | Loss: 0.00103177
Iteration 15/1000 | Loss: 0.00102728
Iteration 16/1000 | Loss: 0.00172319
Iteration 17/1000 | Loss: 0.00112486
Iteration 18/1000 | Loss: 0.00099845
Iteration 19/1000 | Loss: 0.00116402
Iteration 20/1000 | Loss: 0.00017380
Iteration 21/1000 | Loss: 0.00049252
Iteration 22/1000 | Loss: 0.00031739
Iteration 23/1000 | Loss: 0.00026648
Iteration 24/1000 | Loss: 0.00033677
Iteration 25/1000 | Loss: 0.00046397
Iteration 26/1000 | Loss: 0.00051160
Iteration 27/1000 | Loss: 0.00054991
Iteration 28/1000 | Loss: 0.00054010
Iteration 29/1000 | Loss: 0.00075614
Iteration 30/1000 | Loss: 0.00029887
Iteration 31/1000 | Loss: 0.00056056
Iteration 32/1000 | Loss: 0.00072557
Iteration 33/1000 | Loss: 0.00111530
Iteration 34/1000 | Loss: 0.00078021
Iteration 35/1000 | Loss: 0.00065562
Iteration 36/1000 | Loss: 0.00047704
Iteration 37/1000 | Loss: 0.00078211
Iteration 38/1000 | Loss: 0.00090909
Iteration 39/1000 | Loss: 0.00068091
Iteration 40/1000 | Loss: 0.00020305
Iteration 41/1000 | Loss: 0.00035196
Iteration 42/1000 | Loss: 0.00018200
Iteration 43/1000 | Loss: 0.00010555
Iteration 44/1000 | Loss: 0.00016499
Iteration 45/1000 | Loss: 0.00022336
Iteration 46/1000 | Loss: 0.00047979
Iteration 47/1000 | Loss: 0.00048056
Iteration 48/1000 | Loss: 0.00017870
Iteration 49/1000 | Loss: 0.00010100
Iteration 50/1000 | Loss: 0.00012367
Iteration 51/1000 | Loss: 0.00018292
Iteration 52/1000 | Loss: 0.00028377
Iteration 53/1000 | Loss: 0.00013411
Iteration 54/1000 | Loss: 0.00008796
Iteration 55/1000 | Loss: 0.00053764
Iteration 56/1000 | Loss: 0.00093842
Iteration 57/1000 | Loss: 0.00049681
Iteration 58/1000 | Loss: 0.00029075
Iteration 59/1000 | Loss: 0.00256287
Iteration 60/1000 | Loss: 0.00014555
Iteration 61/1000 | Loss: 0.00061935
Iteration 62/1000 | Loss: 0.00108954
Iteration 63/1000 | Loss: 0.00008182
Iteration 64/1000 | Loss: 0.00013922
Iteration 65/1000 | Loss: 0.00006073
Iteration 66/1000 | Loss: 0.00006531
Iteration 67/1000 | Loss: 0.00006359
Iteration 68/1000 | Loss: 0.00004562
Iteration 69/1000 | Loss: 0.00006364
Iteration 70/1000 | Loss: 0.00006174
Iteration 71/1000 | Loss: 0.00004265
Iteration 72/1000 | Loss: 0.00003962
Iteration 73/1000 | Loss: 0.00003838
Iteration 74/1000 | Loss: 0.00008940
Iteration 75/1000 | Loss: 0.00006576
Iteration 76/1000 | Loss: 0.00031530
Iteration 77/1000 | Loss: 0.00008747
Iteration 78/1000 | Loss: 0.00007444
Iteration 79/1000 | Loss: 0.00005760
Iteration 80/1000 | Loss: 0.00011581
Iteration 81/1000 | Loss: 0.00016172
Iteration 82/1000 | Loss: 0.00010897
Iteration 83/1000 | Loss: 0.00005684
Iteration 84/1000 | Loss: 0.00007908
Iteration 85/1000 | Loss: 0.00018123
Iteration 86/1000 | Loss: 0.00011609
Iteration 87/1000 | Loss: 0.00016913
Iteration 88/1000 | Loss: 0.00021482
Iteration 89/1000 | Loss: 0.00011775
Iteration 90/1000 | Loss: 0.00004027
Iteration 91/1000 | Loss: 0.00029209
Iteration 92/1000 | Loss: 0.00033803
Iteration 93/1000 | Loss: 0.00005440
Iteration 94/1000 | Loss: 0.00003654
Iteration 95/1000 | Loss: 0.00005193
Iteration 96/1000 | Loss: 0.00010317
Iteration 97/1000 | Loss: 0.00003741
Iteration 98/1000 | Loss: 0.00005772
Iteration 99/1000 | Loss: 0.00003391
Iteration 100/1000 | Loss: 0.00005810
Iteration 101/1000 | Loss: 0.00003268
Iteration 102/1000 | Loss: 0.00005786
Iteration 103/1000 | Loss: 0.00003214
Iteration 104/1000 | Loss: 0.00017796
Iteration 105/1000 | Loss: 0.00009266
Iteration 106/1000 | Loss: 0.00004319
Iteration 107/1000 | Loss: 0.00016857
Iteration 108/1000 | Loss: 0.00009365
Iteration 109/1000 | Loss: 0.00003681
Iteration 110/1000 | Loss: 0.00004653
Iteration 111/1000 | Loss: 0.00007495
Iteration 112/1000 | Loss: 0.00005534
Iteration 113/1000 | Loss: 0.00003358
Iteration 114/1000 | Loss: 0.00003168
Iteration 115/1000 | Loss: 0.00004884
Iteration 116/1000 | Loss: 0.00003116
Iteration 117/1000 | Loss: 0.00003223
Iteration 118/1000 | Loss: 0.00017027
Iteration 119/1000 | Loss: 0.00009022
Iteration 120/1000 | Loss: 0.00015257
Iteration 121/1000 | Loss: 0.00008904
Iteration 122/1000 | Loss: 0.00008799
Iteration 123/1000 | Loss: 0.00003584
Iteration 124/1000 | Loss: 0.00006511
Iteration 125/1000 | Loss: 0.00017730
Iteration 126/1000 | Loss: 0.00012199
Iteration 127/1000 | Loss: 0.00003596
Iteration 128/1000 | Loss: 0.00003241
Iteration 129/1000 | Loss: 0.00009470
Iteration 130/1000 | Loss: 0.00003316
Iteration 131/1000 | Loss: 0.00002989
Iteration 132/1000 | Loss: 0.00002985
Iteration 133/1000 | Loss: 0.00003695
Iteration 134/1000 | Loss: 0.00002954
Iteration 135/1000 | Loss: 0.00002947
Iteration 136/1000 | Loss: 0.00002947
Iteration 137/1000 | Loss: 0.00002947
Iteration 138/1000 | Loss: 0.00002947
Iteration 139/1000 | Loss: 0.00002946
Iteration 140/1000 | Loss: 0.00002946
Iteration 141/1000 | Loss: 0.00002946
Iteration 142/1000 | Loss: 0.00002946
Iteration 143/1000 | Loss: 0.00002946
Iteration 144/1000 | Loss: 0.00002946
Iteration 145/1000 | Loss: 0.00002946
Iteration 146/1000 | Loss: 0.00002946
Iteration 147/1000 | Loss: 0.00002945
Iteration 148/1000 | Loss: 0.00002945
Iteration 149/1000 | Loss: 0.00002945
Iteration 150/1000 | Loss: 0.00002945
Iteration 151/1000 | Loss: 0.00002944
Iteration 152/1000 | Loss: 0.00002944
Iteration 153/1000 | Loss: 0.00002944
Iteration 154/1000 | Loss: 0.00002944
Iteration 155/1000 | Loss: 0.00002943
Iteration 156/1000 | Loss: 0.00003721
Iteration 157/1000 | Loss: 0.00007708
Iteration 158/1000 | Loss: 0.00005858
Iteration 159/1000 | Loss: 0.00005590
Iteration 160/1000 | Loss: 0.00002951
Iteration 161/1000 | Loss: 0.00002934
Iteration 162/1000 | Loss: 0.00002934
Iteration 163/1000 | Loss: 0.00002934
Iteration 164/1000 | Loss: 0.00002934
Iteration 165/1000 | Loss: 0.00002934
Iteration 166/1000 | Loss: 0.00002934
Iteration 167/1000 | Loss: 0.00002934
Iteration 168/1000 | Loss: 0.00002933
Iteration 169/1000 | Loss: 0.00002933
Iteration 170/1000 | Loss: 0.00002932
Iteration 171/1000 | Loss: 0.00002931
Iteration 172/1000 | Loss: 0.00002931
Iteration 173/1000 | Loss: 0.00002930
Iteration 174/1000 | Loss: 0.00002930
Iteration 175/1000 | Loss: 0.00002929
Iteration 176/1000 | Loss: 0.00002929
Iteration 177/1000 | Loss: 0.00002928
Iteration 178/1000 | Loss: 0.00002928
Iteration 179/1000 | Loss: 0.00002928
Iteration 180/1000 | Loss: 0.00002928
Iteration 181/1000 | Loss: 0.00002928
Iteration 182/1000 | Loss: 0.00002927
Iteration 183/1000 | Loss: 0.00002927
Iteration 184/1000 | Loss: 0.00002927
Iteration 185/1000 | Loss: 0.00002927
Iteration 186/1000 | Loss: 0.00002927
Iteration 187/1000 | Loss: 0.00002927
Iteration 188/1000 | Loss: 0.00002927
Iteration 189/1000 | Loss: 0.00002926
Iteration 190/1000 | Loss: 0.00002926
Iteration 191/1000 | Loss: 0.00002926
Iteration 192/1000 | Loss: 0.00002926
Iteration 193/1000 | Loss: 0.00002926
Iteration 194/1000 | Loss: 0.00002925
Iteration 195/1000 | Loss: 0.00002925
Iteration 196/1000 | Loss: 0.00002925
Iteration 197/1000 | Loss: 0.00002925
Iteration 198/1000 | Loss: 0.00002925
Iteration 199/1000 | Loss: 0.00002925
Iteration 200/1000 | Loss: 0.00002924
Iteration 201/1000 | Loss: 0.00002924
Iteration 202/1000 | Loss: 0.00002924
Iteration 203/1000 | Loss: 0.00002924
Iteration 204/1000 | Loss: 0.00002924
Iteration 205/1000 | Loss: 0.00002924
Iteration 206/1000 | Loss: 0.00002923
Iteration 207/1000 | Loss: 0.00002923
Iteration 208/1000 | Loss: 0.00038029
Iteration 209/1000 | Loss: 0.00026156
Iteration 210/1000 | Loss: 0.00003214
Iteration 211/1000 | Loss: 0.00003061
Iteration 212/1000 | Loss: 0.00008851
Iteration 213/1000 | Loss: 0.00003232
Iteration 214/1000 | Loss: 0.00002858
Iteration 215/1000 | Loss: 0.00002776
Iteration 216/1000 | Loss: 0.00003004
Iteration 217/1000 | Loss: 0.00003071
Iteration 218/1000 | Loss: 0.00003000
Iteration 219/1000 | Loss: 0.00003153
Iteration 220/1000 | Loss: 0.00003041
Iteration 221/1000 | Loss: 0.00003234
Iteration 222/1000 | Loss: 0.00007110
Iteration 223/1000 | Loss: 0.00003939
Iteration 224/1000 | Loss: 0.00004140
Iteration 225/1000 | Loss: 0.00004161
Iteration 226/1000 | Loss: 0.00003110
Iteration 227/1000 | Loss: 0.00003070
Iteration 228/1000 | Loss: 0.00005454
Iteration 229/1000 | Loss: 0.00003036
Iteration 230/1000 | Loss: 0.00003727
Iteration 231/1000 | Loss: 0.00004617
Iteration 232/1000 | Loss: 0.00002977
Iteration 233/1000 | Loss: 0.00003027
Iteration 234/1000 | Loss: 0.00003080
Iteration 235/1000 | Loss: 0.00003161
Iteration 236/1000 | Loss: 0.00003068
Iteration 237/1000 | Loss: 0.00003019
Iteration 238/1000 | Loss: 0.00003022
Iteration 239/1000 | Loss: 0.00003268
Iteration 240/1000 | Loss: 0.00003152
Iteration 241/1000 | Loss: 0.00003035
Iteration 242/1000 | Loss: 0.00003088
Iteration 243/1000 | Loss: 0.00003272
Iteration 244/1000 | Loss: 0.00003062
Iteration 245/1000 | Loss: 0.00003059
Iteration 246/1000 | Loss: 0.00002990
Iteration 247/1000 | Loss: 0.00003379
Iteration 248/1000 | Loss: 0.00006177
Iteration 249/1000 | Loss: 0.00002956
Iteration 250/1000 | Loss: 0.00003236
Iteration 251/1000 | Loss: 0.00003012
Iteration 252/1000 | Loss: 0.00003296
Iteration 253/1000 | Loss: 0.00004470
Iteration 254/1000 | Loss: 0.00003076
Iteration 255/1000 | Loss: 0.00003394
Iteration 256/1000 | Loss: 0.00003004
Iteration 257/1000 | Loss: 0.00003016
Iteration 258/1000 | Loss: 0.00003964
Iteration 259/1000 | Loss: 0.00006875
Iteration 260/1000 | Loss: 0.00002762
Iteration 261/1000 | Loss: 0.00002702
Iteration 262/1000 | Loss: 0.00002913
Iteration 263/1000 | Loss: 0.00002698
Iteration 264/1000 | Loss: 0.00002694
Iteration 265/1000 | Loss: 0.00002691
Iteration 266/1000 | Loss: 0.00002691
Iteration 267/1000 | Loss: 0.00002690
Iteration 268/1000 | Loss: 0.00002690
Iteration 269/1000 | Loss: 0.00002690
Iteration 270/1000 | Loss: 0.00002689
Iteration 271/1000 | Loss: 0.00002689
Iteration 272/1000 | Loss: 0.00002689
Iteration 273/1000 | Loss: 0.00002689
Iteration 274/1000 | Loss: 0.00002689
Iteration 275/1000 | Loss: 0.00002689
Iteration 276/1000 | Loss: 0.00002689
Iteration 277/1000 | Loss: 0.00002689
Iteration 278/1000 | Loss: 0.00002688
Iteration 279/1000 | Loss: 0.00002688
Iteration 280/1000 | Loss: 0.00002688
Iteration 281/1000 | Loss: 0.00002688
Iteration 282/1000 | Loss: 0.00002688
Iteration 283/1000 | Loss: 0.00002688
Iteration 284/1000 | Loss: 0.00002688
Iteration 285/1000 | Loss: 0.00002688
Iteration 286/1000 | Loss: 0.00002688
Iteration 287/1000 | Loss: 0.00002688
Iteration 288/1000 | Loss: 0.00002687
Iteration 289/1000 | Loss: 0.00002687
Iteration 290/1000 | Loss: 0.00002687
Iteration 291/1000 | Loss: 0.00002687
Iteration 292/1000 | Loss: 0.00002687
Iteration 293/1000 | Loss: 0.00002687
Iteration 294/1000 | Loss: 0.00002687
Iteration 295/1000 | Loss: 0.00002687
Iteration 296/1000 | Loss: 0.00002687
Iteration 297/1000 | Loss: 0.00004456
Iteration 298/1000 | Loss: 0.00003187
Iteration 299/1000 | Loss: 0.00004574
Iteration 300/1000 | Loss: 0.00003013
Iteration 301/1000 | Loss: 0.00004168
Iteration 302/1000 | Loss: 0.00003652
Iteration 303/1000 | Loss: 0.00006060
Iteration 304/1000 | Loss: 0.00002695
Iteration 305/1000 | Loss: 0.00002689
Iteration 306/1000 | Loss: 0.00002689
Iteration 307/1000 | Loss: 0.00002688
Iteration 308/1000 | Loss: 0.00002687
Iteration 309/1000 | Loss: 0.00002687
Iteration 310/1000 | Loss: 0.00002687
Iteration 311/1000 | Loss: 0.00002687
Iteration 312/1000 | Loss: 0.00002687
Iteration 313/1000 | Loss: 0.00002687
Iteration 314/1000 | Loss: 0.00002686
Iteration 315/1000 | Loss: 0.00002686
Iteration 316/1000 | Loss: 0.00002686
Iteration 317/1000 | Loss: 0.00002686
Iteration 318/1000 | Loss: 0.00002686
Iteration 319/1000 | Loss: 0.00002686
Iteration 320/1000 | Loss: 0.00002686
Iteration 321/1000 | Loss: 0.00002686
Iteration 322/1000 | Loss: 0.00002686
Iteration 323/1000 | Loss: 0.00002686
Iteration 324/1000 | Loss: 0.00002686
Iteration 325/1000 | Loss: 0.00002686
Iteration 326/1000 | Loss: 0.00002686
Iteration 327/1000 | Loss: 0.00002686
Iteration 328/1000 | Loss: 0.00002686
Iteration 329/1000 | Loss: 0.00002686
Iteration 330/1000 | Loss: 0.00002686
Iteration 331/1000 | Loss: 0.00002686
Iteration 332/1000 | Loss: 0.00002686
Iteration 333/1000 | Loss: 0.00002686
Iteration 334/1000 | Loss: 0.00002686
Iteration 335/1000 | Loss: 0.00002686
Iteration 336/1000 | Loss: 0.00002686
Iteration 337/1000 | Loss: 0.00002686
Iteration 338/1000 | Loss: 0.00002686
Iteration 339/1000 | Loss: 0.00002686
Iteration 340/1000 | Loss: 0.00002686
Iteration 341/1000 | Loss: 0.00002686
Iteration 342/1000 | Loss: 0.00002686
Iteration 343/1000 | Loss: 0.00002686
Iteration 344/1000 | Loss: 0.00002686
Iteration 345/1000 | Loss: 0.00002686
Iteration 346/1000 | Loss: 0.00002686
Iteration 347/1000 | Loss: 0.00002686
Iteration 348/1000 | Loss: 0.00002686
Iteration 349/1000 | Loss: 0.00002686
Iteration 350/1000 | Loss: 0.00002686
Iteration 351/1000 | Loss: 0.00002686
Iteration 352/1000 | Loss: 0.00002686
Iteration 353/1000 | Loss: 0.00002686
Iteration 354/1000 | Loss: 0.00002686
Iteration 355/1000 | Loss: 0.00002686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [2.6855739633901976e-05, 2.6855739633901976e-05, 2.6855739633901976e-05, 2.6855739633901976e-05, 2.6855739633901976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6855739633901976e-05

Optimization complete. Final v2v error: 3.703866481781006 mm

Highest mean error: 11.49765682220459 mm for frame 42

Lowest mean error: 2.843665599822998 mm for frame 122

Saving results

Total time: 378.11283326148987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429978
Iteration 2/25 | Loss: 0.00104797
Iteration 3/25 | Loss: 0.00097855
Iteration 4/25 | Loss: 0.00096730
Iteration 5/25 | Loss: 0.00096376
Iteration 6/25 | Loss: 0.00096346
Iteration 7/25 | Loss: 0.00096346
Iteration 8/25 | Loss: 0.00096338
Iteration 9/25 | Loss: 0.00096338
Iteration 10/25 | Loss: 0.00096338
Iteration 11/25 | Loss: 0.00096338
Iteration 12/25 | Loss: 0.00096338
Iteration 13/25 | Loss: 0.00096338
Iteration 14/25 | Loss: 0.00096338
Iteration 15/25 | Loss: 0.00096338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009633769514039159, 0.0009633769514039159, 0.0009633769514039159, 0.0009633769514039159, 0.0009633769514039159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009633769514039159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64083004
Iteration 2/25 | Loss: 0.00071839
Iteration 3/25 | Loss: 0.00071839
Iteration 4/25 | Loss: 0.00071839
Iteration 5/25 | Loss: 0.00071839
Iteration 6/25 | Loss: 0.00071839
Iteration 7/25 | Loss: 0.00071839
Iteration 8/25 | Loss: 0.00071839
Iteration 9/25 | Loss: 0.00071839
Iteration 10/25 | Loss: 0.00071839
Iteration 11/25 | Loss: 0.00071839
Iteration 12/25 | Loss: 0.00071839
Iteration 13/25 | Loss: 0.00071839
Iteration 14/25 | Loss: 0.00071839
Iteration 15/25 | Loss: 0.00071839
Iteration 16/25 | Loss: 0.00071839
Iteration 17/25 | Loss: 0.00071839
Iteration 18/25 | Loss: 0.00071839
Iteration 19/25 | Loss: 0.00071839
Iteration 20/25 | Loss: 0.00071839
Iteration 21/25 | Loss: 0.00071839
Iteration 22/25 | Loss: 0.00071839
Iteration 23/25 | Loss: 0.00071839
Iteration 24/25 | Loss: 0.00071839
Iteration 25/25 | Loss: 0.00071839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071839
Iteration 2/1000 | Loss: 0.00001521
Iteration 3/1000 | Loss: 0.00001192
Iteration 4/1000 | Loss: 0.00001093
Iteration 5/1000 | Loss: 0.00001041
Iteration 6/1000 | Loss: 0.00001003
Iteration 7/1000 | Loss: 0.00000979
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000932
Iteration 10/1000 | Loss: 0.00000931
Iteration 11/1000 | Loss: 0.00000930
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000928
Iteration 15/1000 | Loss: 0.00000926
Iteration 16/1000 | Loss: 0.00000924
Iteration 17/1000 | Loss: 0.00000911
Iteration 18/1000 | Loss: 0.00000907
Iteration 19/1000 | Loss: 0.00000906
Iteration 20/1000 | Loss: 0.00000904
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000904
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000903
Iteration 25/1000 | Loss: 0.00000903
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000900
Iteration 28/1000 | Loss: 0.00000899
Iteration 29/1000 | Loss: 0.00000898
Iteration 30/1000 | Loss: 0.00000897
Iteration 31/1000 | Loss: 0.00000897
Iteration 32/1000 | Loss: 0.00000897
Iteration 33/1000 | Loss: 0.00000896
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000895
Iteration 36/1000 | Loss: 0.00000895
Iteration 37/1000 | Loss: 0.00000895
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000891
Iteration 40/1000 | Loss: 0.00000887
Iteration 41/1000 | Loss: 0.00000885
Iteration 42/1000 | Loss: 0.00000885
Iteration 43/1000 | Loss: 0.00000884
Iteration 44/1000 | Loss: 0.00000884
Iteration 45/1000 | Loss: 0.00000882
Iteration 46/1000 | Loss: 0.00000882
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000878
Iteration 49/1000 | Loss: 0.00000878
Iteration 50/1000 | Loss: 0.00000878
Iteration 51/1000 | Loss: 0.00000877
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000874
Iteration 58/1000 | Loss: 0.00000874
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000873
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000871
Iteration 64/1000 | Loss: 0.00000871
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000870
Iteration 69/1000 | Loss: 0.00000870
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000868
Iteration 76/1000 | Loss: 0.00000868
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000868
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000867
Iteration 82/1000 | Loss: 0.00000867
Iteration 83/1000 | Loss: 0.00000867
Iteration 84/1000 | Loss: 0.00000867
Iteration 85/1000 | Loss: 0.00000867
Iteration 86/1000 | Loss: 0.00000867
Iteration 87/1000 | Loss: 0.00000867
Iteration 88/1000 | Loss: 0.00000867
Iteration 89/1000 | Loss: 0.00000867
Iteration 90/1000 | Loss: 0.00000867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [8.668988812132739e-06, 8.668988812132739e-06, 8.668988812132739e-06, 8.668988812132739e-06, 8.668988812132739e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.668988812132739e-06

Optimization complete. Final v2v error: 2.5775108337402344 mm

Highest mean error: 2.9597835540771484 mm for frame 43

Lowest mean error: 2.351997137069702 mm for frame 27

Saving results

Total time: 34.992319107055664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401150
Iteration 2/25 | Loss: 0.00112071
Iteration 3/25 | Loss: 0.00099735
Iteration 4/25 | Loss: 0.00098514
Iteration 5/25 | Loss: 0.00098230
Iteration 6/25 | Loss: 0.00098170
Iteration 7/25 | Loss: 0.00098170
Iteration 8/25 | Loss: 0.00098170
Iteration 9/25 | Loss: 0.00098170
Iteration 10/25 | Loss: 0.00098170
Iteration 11/25 | Loss: 0.00098170
Iteration 12/25 | Loss: 0.00098170
Iteration 13/25 | Loss: 0.00098170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009816995589062572, 0.0009816995589062572, 0.0009816995589062572, 0.0009816995589062572, 0.0009816995589062572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009816995589062572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54036307
Iteration 2/25 | Loss: 0.00068426
Iteration 3/25 | Loss: 0.00068426
Iteration 4/25 | Loss: 0.00068426
Iteration 5/25 | Loss: 0.00068426
Iteration 6/25 | Loss: 0.00068426
Iteration 7/25 | Loss: 0.00068426
Iteration 8/25 | Loss: 0.00068426
Iteration 9/25 | Loss: 0.00068426
Iteration 10/25 | Loss: 0.00068425
Iteration 11/25 | Loss: 0.00068425
Iteration 12/25 | Loss: 0.00068425
Iteration 13/25 | Loss: 0.00068425
Iteration 14/25 | Loss: 0.00068425
Iteration 15/25 | Loss: 0.00068425
Iteration 16/25 | Loss: 0.00068425
Iteration 17/25 | Loss: 0.00068425
Iteration 18/25 | Loss: 0.00068425
Iteration 19/25 | Loss: 0.00068425
Iteration 20/25 | Loss: 0.00068425
Iteration 21/25 | Loss: 0.00068425
Iteration 22/25 | Loss: 0.00068425
Iteration 23/25 | Loss: 0.00068425
Iteration 24/25 | Loss: 0.00068425
Iteration 25/25 | Loss: 0.00068425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068425
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001144
Iteration 6/1000 | Loss: 0.00001067
Iteration 7/1000 | Loss: 0.00001013
Iteration 8/1000 | Loss: 0.00000978
Iteration 9/1000 | Loss: 0.00000959
Iteration 10/1000 | Loss: 0.00000950
Iteration 11/1000 | Loss: 0.00000932
Iteration 12/1000 | Loss: 0.00000928
Iteration 13/1000 | Loss: 0.00000923
Iteration 14/1000 | Loss: 0.00000920
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00000917
Iteration 17/1000 | Loss: 0.00000915
Iteration 18/1000 | Loss: 0.00000915
Iteration 19/1000 | Loss: 0.00000913
Iteration 20/1000 | Loss: 0.00000912
Iteration 21/1000 | Loss: 0.00000912
Iteration 22/1000 | Loss: 0.00000911
Iteration 23/1000 | Loss: 0.00000911
Iteration 24/1000 | Loss: 0.00000910
Iteration 25/1000 | Loss: 0.00000910
Iteration 26/1000 | Loss: 0.00000909
Iteration 27/1000 | Loss: 0.00000909
Iteration 28/1000 | Loss: 0.00000909
Iteration 29/1000 | Loss: 0.00000908
Iteration 30/1000 | Loss: 0.00000908
Iteration 31/1000 | Loss: 0.00000907
Iteration 32/1000 | Loss: 0.00000906
Iteration 33/1000 | Loss: 0.00000906
Iteration 34/1000 | Loss: 0.00000903
Iteration 35/1000 | Loss: 0.00000903
Iteration 36/1000 | Loss: 0.00000903
Iteration 37/1000 | Loss: 0.00000903
Iteration 38/1000 | Loss: 0.00000902
Iteration 39/1000 | Loss: 0.00000900
Iteration 40/1000 | Loss: 0.00000900
Iteration 41/1000 | Loss: 0.00000900
Iteration 42/1000 | Loss: 0.00000900
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000900
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000899
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000899
Iteration 55/1000 | Loss: 0.00000899
Iteration 56/1000 | Loss: 0.00000898
Iteration 57/1000 | Loss: 0.00000898
Iteration 58/1000 | Loss: 0.00000898
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000897
Iteration 61/1000 | Loss: 0.00000897
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000896
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000895
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000894
Iteration 73/1000 | Loss: 0.00000894
Iteration 74/1000 | Loss: 0.00000894
Iteration 75/1000 | Loss: 0.00000894
Iteration 76/1000 | Loss: 0.00000894
Iteration 77/1000 | Loss: 0.00000894
Iteration 78/1000 | Loss: 0.00000894
Iteration 79/1000 | Loss: 0.00000894
Iteration 80/1000 | Loss: 0.00000894
Iteration 81/1000 | Loss: 0.00000893
Iteration 82/1000 | Loss: 0.00000893
Iteration 83/1000 | Loss: 0.00000893
Iteration 84/1000 | Loss: 0.00000893
Iteration 85/1000 | Loss: 0.00000892
Iteration 86/1000 | Loss: 0.00000892
Iteration 87/1000 | Loss: 0.00000892
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000891
Iteration 90/1000 | Loss: 0.00000890
Iteration 91/1000 | Loss: 0.00000890
Iteration 92/1000 | Loss: 0.00000890
Iteration 93/1000 | Loss: 0.00000890
Iteration 94/1000 | Loss: 0.00000890
Iteration 95/1000 | Loss: 0.00000890
Iteration 96/1000 | Loss: 0.00000890
Iteration 97/1000 | Loss: 0.00000890
Iteration 98/1000 | Loss: 0.00000889
Iteration 99/1000 | Loss: 0.00000889
Iteration 100/1000 | Loss: 0.00000889
Iteration 101/1000 | Loss: 0.00000889
Iteration 102/1000 | Loss: 0.00000889
Iteration 103/1000 | Loss: 0.00000889
Iteration 104/1000 | Loss: 0.00000889
Iteration 105/1000 | Loss: 0.00000888
Iteration 106/1000 | Loss: 0.00000888
Iteration 107/1000 | Loss: 0.00000888
Iteration 108/1000 | Loss: 0.00000888
Iteration 109/1000 | Loss: 0.00000888
Iteration 110/1000 | Loss: 0.00000888
Iteration 111/1000 | Loss: 0.00000888
Iteration 112/1000 | Loss: 0.00000888
Iteration 113/1000 | Loss: 0.00000888
Iteration 114/1000 | Loss: 0.00000888
Iteration 115/1000 | Loss: 0.00000888
Iteration 116/1000 | Loss: 0.00000888
Iteration 117/1000 | Loss: 0.00000888
Iteration 118/1000 | Loss: 0.00000888
Iteration 119/1000 | Loss: 0.00000888
Iteration 120/1000 | Loss: 0.00000888
Iteration 121/1000 | Loss: 0.00000887
Iteration 122/1000 | Loss: 0.00000887
Iteration 123/1000 | Loss: 0.00000887
Iteration 124/1000 | Loss: 0.00000887
Iteration 125/1000 | Loss: 0.00000887
Iteration 126/1000 | Loss: 0.00000887
Iteration 127/1000 | Loss: 0.00000887
Iteration 128/1000 | Loss: 0.00000887
Iteration 129/1000 | Loss: 0.00000887
Iteration 130/1000 | Loss: 0.00000886
Iteration 131/1000 | Loss: 0.00000886
Iteration 132/1000 | Loss: 0.00000886
Iteration 133/1000 | Loss: 0.00000886
Iteration 134/1000 | Loss: 0.00000886
Iteration 135/1000 | Loss: 0.00000886
Iteration 136/1000 | Loss: 0.00000886
Iteration 137/1000 | Loss: 0.00000886
Iteration 138/1000 | Loss: 0.00000886
Iteration 139/1000 | Loss: 0.00000886
Iteration 140/1000 | Loss: 0.00000886
Iteration 141/1000 | Loss: 0.00000886
Iteration 142/1000 | Loss: 0.00000885
Iteration 143/1000 | Loss: 0.00000885
Iteration 144/1000 | Loss: 0.00000885
Iteration 145/1000 | Loss: 0.00000885
Iteration 146/1000 | Loss: 0.00000885
Iteration 147/1000 | Loss: 0.00000885
Iteration 148/1000 | Loss: 0.00000885
Iteration 149/1000 | Loss: 0.00000885
Iteration 150/1000 | Loss: 0.00000885
Iteration 151/1000 | Loss: 0.00000885
Iteration 152/1000 | Loss: 0.00000884
Iteration 153/1000 | Loss: 0.00000884
Iteration 154/1000 | Loss: 0.00000884
Iteration 155/1000 | Loss: 0.00000884
Iteration 156/1000 | Loss: 0.00000884
Iteration 157/1000 | Loss: 0.00000884
Iteration 158/1000 | Loss: 0.00000884
Iteration 159/1000 | Loss: 0.00000884
Iteration 160/1000 | Loss: 0.00000884
Iteration 161/1000 | Loss: 0.00000884
Iteration 162/1000 | Loss: 0.00000883
Iteration 163/1000 | Loss: 0.00000883
Iteration 164/1000 | Loss: 0.00000883
Iteration 165/1000 | Loss: 0.00000883
Iteration 166/1000 | Loss: 0.00000883
Iteration 167/1000 | Loss: 0.00000883
Iteration 168/1000 | Loss: 0.00000882
Iteration 169/1000 | Loss: 0.00000882
Iteration 170/1000 | Loss: 0.00000882
Iteration 171/1000 | Loss: 0.00000882
Iteration 172/1000 | Loss: 0.00000882
Iteration 173/1000 | Loss: 0.00000882
Iteration 174/1000 | Loss: 0.00000882
Iteration 175/1000 | Loss: 0.00000882
Iteration 176/1000 | Loss: 0.00000882
Iteration 177/1000 | Loss: 0.00000882
Iteration 178/1000 | Loss: 0.00000882
Iteration 179/1000 | Loss: 0.00000882
Iteration 180/1000 | Loss: 0.00000881
Iteration 181/1000 | Loss: 0.00000881
Iteration 182/1000 | Loss: 0.00000881
Iteration 183/1000 | Loss: 0.00000881
Iteration 184/1000 | Loss: 0.00000881
Iteration 185/1000 | Loss: 0.00000881
Iteration 186/1000 | Loss: 0.00000881
Iteration 187/1000 | Loss: 0.00000881
Iteration 188/1000 | Loss: 0.00000881
Iteration 189/1000 | Loss: 0.00000881
Iteration 190/1000 | Loss: 0.00000881
Iteration 191/1000 | Loss: 0.00000881
Iteration 192/1000 | Loss: 0.00000881
Iteration 193/1000 | Loss: 0.00000881
Iteration 194/1000 | Loss: 0.00000881
Iteration 195/1000 | Loss: 0.00000880
Iteration 196/1000 | Loss: 0.00000880
Iteration 197/1000 | Loss: 0.00000880
Iteration 198/1000 | Loss: 0.00000880
Iteration 199/1000 | Loss: 0.00000880
Iteration 200/1000 | Loss: 0.00000880
Iteration 201/1000 | Loss: 0.00000880
Iteration 202/1000 | Loss: 0.00000880
Iteration 203/1000 | Loss: 0.00000880
Iteration 204/1000 | Loss: 0.00000879
Iteration 205/1000 | Loss: 0.00000879
Iteration 206/1000 | Loss: 0.00000879
Iteration 207/1000 | Loss: 0.00000879
Iteration 208/1000 | Loss: 0.00000879
Iteration 209/1000 | Loss: 0.00000879
Iteration 210/1000 | Loss: 0.00000879
Iteration 211/1000 | Loss: 0.00000879
Iteration 212/1000 | Loss: 0.00000879
Iteration 213/1000 | Loss: 0.00000879
Iteration 214/1000 | Loss: 0.00000879
Iteration 215/1000 | Loss: 0.00000879
Iteration 216/1000 | Loss: 0.00000878
Iteration 217/1000 | Loss: 0.00000878
Iteration 218/1000 | Loss: 0.00000878
Iteration 219/1000 | Loss: 0.00000878
Iteration 220/1000 | Loss: 0.00000878
Iteration 221/1000 | Loss: 0.00000878
Iteration 222/1000 | Loss: 0.00000878
Iteration 223/1000 | Loss: 0.00000878
Iteration 224/1000 | Loss: 0.00000878
Iteration 225/1000 | Loss: 0.00000878
Iteration 226/1000 | Loss: 0.00000878
Iteration 227/1000 | Loss: 0.00000878
Iteration 228/1000 | Loss: 0.00000878
Iteration 229/1000 | Loss: 0.00000878
Iteration 230/1000 | Loss: 0.00000878
Iteration 231/1000 | Loss: 0.00000878
Iteration 232/1000 | Loss: 0.00000878
Iteration 233/1000 | Loss: 0.00000878
Iteration 234/1000 | Loss: 0.00000877
Iteration 235/1000 | Loss: 0.00000877
Iteration 236/1000 | Loss: 0.00000877
Iteration 237/1000 | Loss: 0.00000877
Iteration 238/1000 | Loss: 0.00000877
Iteration 239/1000 | Loss: 0.00000877
Iteration 240/1000 | Loss: 0.00000877
Iteration 241/1000 | Loss: 0.00000877
Iteration 242/1000 | Loss: 0.00000877
Iteration 243/1000 | Loss: 0.00000877
Iteration 244/1000 | Loss: 0.00000877
Iteration 245/1000 | Loss: 0.00000877
Iteration 246/1000 | Loss: 0.00000877
Iteration 247/1000 | Loss: 0.00000877
Iteration 248/1000 | Loss: 0.00000877
Iteration 249/1000 | Loss: 0.00000877
Iteration 250/1000 | Loss: 0.00000877
Iteration 251/1000 | Loss: 0.00000877
Iteration 252/1000 | Loss: 0.00000877
Iteration 253/1000 | Loss: 0.00000877
Iteration 254/1000 | Loss: 0.00000876
Iteration 255/1000 | Loss: 0.00000876
Iteration 256/1000 | Loss: 0.00000876
Iteration 257/1000 | Loss: 0.00000876
Iteration 258/1000 | Loss: 0.00000876
Iteration 259/1000 | Loss: 0.00000876
Iteration 260/1000 | Loss: 0.00000876
Iteration 261/1000 | Loss: 0.00000876
Iteration 262/1000 | Loss: 0.00000876
Iteration 263/1000 | Loss: 0.00000875
Iteration 264/1000 | Loss: 0.00000875
Iteration 265/1000 | Loss: 0.00000875
Iteration 266/1000 | Loss: 0.00000875
Iteration 267/1000 | Loss: 0.00000875
Iteration 268/1000 | Loss: 0.00000875
Iteration 269/1000 | Loss: 0.00000875
Iteration 270/1000 | Loss: 0.00000875
Iteration 271/1000 | Loss: 0.00000875
Iteration 272/1000 | Loss: 0.00000875
Iteration 273/1000 | Loss: 0.00000875
Iteration 274/1000 | Loss: 0.00000874
Iteration 275/1000 | Loss: 0.00000874
Iteration 276/1000 | Loss: 0.00000874
Iteration 277/1000 | Loss: 0.00000874
Iteration 278/1000 | Loss: 0.00000874
Iteration 279/1000 | Loss: 0.00000874
Iteration 280/1000 | Loss: 0.00000874
Iteration 281/1000 | Loss: 0.00000874
Iteration 282/1000 | Loss: 0.00000874
Iteration 283/1000 | Loss: 0.00000873
Iteration 284/1000 | Loss: 0.00000873
Iteration 285/1000 | Loss: 0.00000873
Iteration 286/1000 | Loss: 0.00000873
Iteration 287/1000 | Loss: 0.00000873
Iteration 288/1000 | Loss: 0.00000873
Iteration 289/1000 | Loss: 0.00000873
Iteration 290/1000 | Loss: 0.00000873
Iteration 291/1000 | Loss: 0.00000873
Iteration 292/1000 | Loss: 0.00000873
Iteration 293/1000 | Loss: 0.00000873
Iteration 294/1000 | Loss: 0.00000873
Iteration 295/1000 | Loss: 0.00000873
Iteration 296/1000 | Loss: 0.00000873
Iteration 297/1000 | Loss: 0.00000873
Iteration 298/1000 | Loss: 0.00000873
Iteration 299/1000 | Loss: 0.00000873
Iteration 300/1000 | Loss: 0.00000873
Iteration 301/1000 | Loss: 0.00000873
Iteration 302/1000 | Loss: 0.00000872
Iteration 303/1000 | Loss: 0.00000872
Iteration 304/1000 | Loss: 0.00000872
Iteration 305/1000 | Loss: 0.00000872
Iteration 306/1000 | Loss: 0.00000872
Iteration 307/1000 | Loss: 0.00000872
Iteration 308/1000 | Loss: 0.00000872
Iteration 309/1000 | Loss: 0.00000872
Iteration 310/1000 | Loss: 0.00000872
Iteration 311/1000 | Loss: 0.00000872
Iteration 312/1000 | Loss: 0.00000872
Iteration 313/1000 | Loss: 0.00000872
Iteration 314/1000 | Loss: 0.00000872
Iteration 315/1000 | Loss: 0.00000872
Iteration 316/1000 | Loss: 0.00000872
Iteration 317/1000 | Loss: 0.00000872
Iteration 318/1000 | Loss: 0.00000872
Iteration 319/1000 | Loss: 0.00000872
Iteration 320/1000 | Loss: 0.00000872
Iteration 321/1000 | Loss: 0.00000872
Iteration 322/1000 | Loss: 0.00000872
Iteration 323/1000 | Loss: 0.00000872
Iteration 324/1000 | Loss: 0.00000871
Iteration 325/1000 | Loss: 0.00000871
Iteration 326/1000 | Loss: 0.00000871
Iteration 327/1000 | Loss: 0.00000871
Iteration 328/1000 | Loss: 0.00000871
Iteration 329/1000 | Loss: 0.00000871
Iteration 330/1000 | Loss: 0.00000871
Iteration 331/1000 | Loss: 0.00000871
Iteration 332/1000 | Loss: 0.00000871
Iteration 333/1000 | Loss: 0.00000871
Iteration 334/1000 | Loss: 0.00000871
Iteration 335/1000 | Loss: 0.00000871
Iteration 336/1000 | Loss: 0.00000871
Iteration 337/1000 | Loss: 0.00000871
Iteration 338/1000 | Loss: 0.00000871
Iteration 339/1000 | Loss: 0.00000871
Iteration 340/1000 | Loss: 0.00000871
Iteration 341/1000 | Loss: 0.00000871
Iteration 342/1000 | Loss: 0.00000871
Iteration 343/1000 | Loss: 0.00000871
Iteration 344/1000 | Loss: 0.00000871
Iteration 345/1000 | Loss: 0.00000871
Iteration 346/1000 | Loss: 0.00000871
Iteration 347/1000 | Loss: 0.00000871
Iteration 348/1000 | Loss: 0.00000871
Iteration 349/1000 | Loss: 0.00000871
Iteration 350/1000 | Loss: 0.00000871
Iteration 351/1000 | Loss: 0.00000871
Iteration 352/1000 | Loss: 0.00000871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [8.707009328645654e-06, 8.707009328645654e-06, 8.707009328645654e-06, 8.707009328645654e-06, 8.707009328645654e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.707009328645654e-06

Optimization complete. Final v2v error: 2.4634459018707275 mm

Highest mean error: 3.7105064392089844 mm for frame 167

Lowest mean error: 2.1877501010894775 mm for frame 146

Saving results

Total time: 51.46482586860657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821610
Iteration 2/25 | Loss: 0.00110746
Iteration 3/25 | Loss: 0.00097619
Iteration 4/25 | Loss: 0.00096327
Iteration 5/25 | Loss: 0.00096098
Iteration 6/25 | Loss: 0.00096046
Iteration 7/25 | Loss: 0.00096046
Iteration 8/25 | Loss: 0.00096046
Iteration 9/25 | Loss: 0.00096046
Iteration 10/25 | Loss: 0.00096046
Iteration 11/25 | Loss: 0.00096046
Iteration 12/25 | Loss: 0.00096046
Iteration 13/25 | Loss: 0.00096046
Iteration 14/25 | Loss: 0.00096046
Iteration 15/25 | Loss: 0.00096046
Iteration 16/25 | Loss: 0.00096046
Iteration 17/25 | Loss: 0.00096046
Iteration 18/25 | Loss: 0.00096046
Iteration 19/25 | Loss: 0.00096046
Iteration 20/25 | Loss: 0.00096046
Iteration 21/25 | Loss: 0.00096046
Iteration 22/25 | Loss: 0.00096046
Iteration 23/25 | Loss: 0.00096046
Iteration 24/25 | Loss: 0.00096046
Iteration 25/25 | Loss: 0.00096046

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37742555
Iteration 2/25 | Loss: 0.00060020
Iteration 3/25 | Loss: 0.00060018
Iteration 4/25 | Loss: 0.00060018
Iteration 5/25 | Loss: 0.00060018
Iteration 6/25 | Loss: 0.00060018
Iteration 7/25 | Loss: 0.00060018
Iteration 8/25 | Loss: 0.00060018
Iteration 9/25 | Loss: 0.00060018
Iteration 10/25 | Loss: 0.00060018
Iteration 11/25 | Loss: 0.00060018
Iteration 12/25 | Loss: 0.00060018
Iteration 13/25 | Loss: 0.00060018
Iteration 14/25 | Loss: 0.00060018
Iteration 15/25 | Loss: 0.00060018
Iteration 16/25 | Loss: 0.00060018
Iteration 17/25 | Loss: 0.00060018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006001758156344295, 0.0006001758156344295, 0.0006001758156344295, 0.0006001758156344295, 0.0006001758156344295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006001758156344295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060018
Iteration 2/1000 | Loss: 0.00001609
Iteration 3/1000 | Loss: 0.00001072
Iteration 4/1000 | Loss: 0.00000917
Iteration 5/1000 | Loss: 0.00000836
Iteration 6/1000 | Loss: 0.00000794
Iteration 7/1000 | Loss: 0.00000751
Iteration 8/1000 | Loss: 0.00000736
Iteration 9/1000 | Loss: 0.00000735
Iteration 10/1000 | Loss: 0.00000734
Iteration 11/1000 | Loss: 0.00000734
Iteration 12/1000 | Loss: 0.00000733
Iteration 13/1000 | Loss: 0.00000732
Iteration 14/1000 | Loss: 0.00000732
Iteration 15/1000 | Loss: 0.00000730
Iteration 16/1000 | Loss: 0.00000729
Iteration 17/1000 | Loss: 0.00000728
Iteration 18/1000 | Loss: 0.00000728
Iteration 19/1000 | Loss: 0.00000727
Iteration 20/1000 | Loss: 0.00000726
Iteration 21/1000 | Loss: 0.00000723
Iteration 22/1000 | Loss: 0.00000719
Iteration 23/1000 | Loss: 0.00000717
Iteration 24/1000 | Loss: 0.00000707
Iteration 25/1000 | Loss: 0.00000701
Iteration 26/1000 | Loss: 0.00000700
Iteration 27/1000 | Loss: 0.00000699
Iteration 28/1000 | Loss: 0.00000699
Iteration 29/1000 | Loss: 0.00000698
Iteration 30/1000 | Loss: 0.00000697
Iteration 31/1000 | Loss: 0.00000697
Iteration 32/1000 | Loss: 0.00000697
Iteration 33/1000 | Loss: 0.00000696
Iteration 34/1000 | Loss: 0.00000696
Iteration 35/1000 | Loss: 0.00000695
Iteration 36/1000 | Loss: 0.00000695
Iteration 37/1000 | Loss: 0.00000695
Iteration 38/1000 | Loss: 0.00000692
Iteration 39/1000 | Loss: 0.00000692
Iteration 40/1000 | Loss: 0.00000692
Iteration 41/1000 | Loss: 0.00000692
Iteration 42/1000 | Loss: 0.00000692
Iteration 43/1000 | Loss: 0.00000691
Iteration 44/1000 | Loss: 0.00000691
Iteration 45/1000 | Loss: 0.00000691
Iteration 46/1000 | Loss: 0.00000691
Iteration 47/1000 | Loss: 0.00000691
Iteration 48/1000 | Loss: 0.00000691
Iteration 49/1000 | Loss: 0.00000691
Iteration 50/1000 | Loss: 0.00000691
Iteration 51/1000 | Loss: 0.00000691
Iteration 52/1000 | Loss: 0.00000690
Iteration 53/1000 | Loss: 0.00000688
Iteration 54/1000 | Loss: 0.00000688
Iteration 55/1000 | Loss: 0.00000687
Iteration 56/1000 | Loss: 0.00000686
Iteration 57/1000 | Loss: 0.00000686
Iteration 58/1000 | Loss: 0.00000685
Iteration 59/1000 | Loss: 0.00000684
Iteration 60/1000 | Loss: 0.00000684
Iteration 61/1000 | Loss: 0.00000684
Iteration 62/1000 | Loss: 0.00000683
Iteration 63/1000 | Loss: 0.00000683
Iteration 64/1000 | Loss: 0.00000683
Iteration 65/1000 | Loss: 0.00000683
Iteration 66/1000 | Loss: 0.00000683
Iteration 67/1000 | Loss: 0.00000682
Iteration 68/1000 | Loss: 0.00000682
Iteration 69/1000 | Loss: 0.00000682
Iteration 70/1000 | Loss: 0.00000681
Iteration 71/1000 | Loss: 0.00000681
Iteration 72/1000 | Loss: 0.00000681
Iteration 73/1000 | Loss: 0.00000681
Iteration 74/1000 | Loss: 0.00000681
Iteration 75/1000 | Loss: 0.00000681
Iteration 76/1000 | Loss: 0.00000681
Iteration 77/1000 | Loss: 0.00000681
Iteration 78/1000 | Loss: 0.00000681
Iteration 79/1000 | Loss: 0.00000681
Iteration 80/1000 | Loss: 0.00000680
Iteration 81/1000 | Loss: 0.00000680
Iteration 82/1000 | Loss: 0.00000680
Iteration 83/1000 | Loss: 0.00000680
Iteration 84/1000 | Loss: 0.00000680
Iteration 85/1000 | Loss: 0.00000680
Iteration 86/1000 | Loss: 0.00000680
Iteration 87/1000 | Loss: 0.00000680
Iteration 88/1000 | Loss: 0.00000680
Iteration 89/1000 | Loss: 0.00000679
Iteration 90/1000 | Loss: 0.00000679
Iteration 91/1000 | Loss: 0.00000679
Iteration 92/1000 | Loss: 0.00000679
Iteration 93/1000 | Loss: 0.00000679
Iteration 94/1000 | Loss: 0.00000679
Iteration 95/1000 | Loss: 0.00000679
Iteration 96/1000 | Loss: 0.00000678
Iteration 97/1000 | Loss: 0.00000678
Iteration 98/1000 | Loss: 0.00000678
Iteration 99/1000 | Loss: 0.00000678
Iteration 100/1000 | Loss: 0.00000678
Iteration 101/1000 | Loss: 0.00000678
Iteration 102/1000 | Loss: 0.00000678
Iteration 103/1000 | Loss: 0.00000678
Iteration 104/1000 | Loss: 0.00000678
Iteration 105/1000 | Loss: 0.00000678
Iteration 106/1000 | Loss: 0.00000678
Iteration 107/1000 | Loss: 0.00000678
Iteration 108/1000 | Loss: 0.00000678
Iteration 109/1000 | Loss: 0.00000677
Iteration 110/1000 | Loss: 0.00000677
Iteration 111/1000 | Loss: 0.00000677
Iteration 112/1000 | Loss: 0.00000677
Iteration 113/1000 | Loss: 0.00000677
Iteration 114/1000 | Loss: 0.00000677
Iteration 115/1000 | Loss: 0.00000677
Iteration 116/1000 | Loss: 0.00000677
Iteration 117/1000 | Loss: 0.00000677
Iteration 118/1000 | Loss: 0.00000677
Iteration 119/1000 | Loss: 0.00000677
Iteration 120/1000 | Loss: 0.00000676
Iteration 121/1000 | Loss: 0.00000676
Iteration 122/1000 | Loss: 0.00000676
Iteration 123/1000 | Loss: 0.00000676
Iteration 124/1000 | Loss: 0.00000676
Iteration 125/1000 | Loss: 0.00000676
Iteration 126/1000 | Loss: 0.00000676
Iteration 127/1000 | Loss: 0.00000676
Iteration 128/1000 | Loss: 0.00000676
Iteration 129/1000 | Loss: 0.00000676
Iteration 130/1000 | Loss: 0.00000676
Iteration 131/1000 | Loss: 0.00000676
Iteration 132/1000 | Loss: 0.00000676
Iteration 133/1000 | Loss: 0.00000676
Iteration 134/1000 | Loss: 0.00000676
Iteration 135/1000 | Loss: 0.00000676
Iteration 136/1000 | Loss: 0.00000676
Iteration 137/1000 | Loss: 0.00000676
Iteration 138/1000 | Loss: 0.00000675
Iteration 139/1000 | Loss: 0.00000675
Iteration 140/1000 | Loss: 0.00000675
Iteration 141/1000 | Loss: 0.00000675
Iteration 142/1000 | Loss: 0.00000675
Iteration 143/1000 | Loss: 0.00000675
Iteration 144/1000 | Loss: 0.00000675
Iteration 145/1000 | Loss: 0.00000675
Iteration 146/1000 | Loss: 0.00000675
Iteration 147/1000 | Loss: 0.00000675
Iteration 148/1000 | Loss: 0.00000675
Iteration 149/1000 | Loss: 0.00000675
Iteration 150/1000 | Loss: 0.00000675
Iteration 151/1000 | Loss: 0.00000675
Iteration 152/1000 | Loss: 0.00000674
Iteration 153/1000 | Loss: 0.00000674
Iteration 154/1000 | Loss: 0.00000674
Iteration 155/1000 | Loss: 0.00000674
Iteration 156/1000 | Loss: 0.00000674
Iteration 157/1000 | Loss: 0.00000674
Iteration 158/1000 | Loss: 0.00000674
Iteration 159/1000 | Loss: 0.00000674
Iteration 160/1000 | Loss: 0.00000674
Iteration 161/1000 | Loss: 0.00000674
Iteration 162/1000 | Loss: 0.00000674
Iteration 163/1000 | Loss: 0.00000674
Iteration 164/1000 | Loss: 0.00000674
Iteration 165/1000 | Loss: 0.00000674
Iteration 166/1000 | Loss: 0.00000674
Iteration 167/1000 | Loss: 0.00000674
Iteration 168/1000 | Loss: 0.00000674
Iteration 169/1000 | Loss: 0.00000674
Iteration 170/1000 | Loss: 0.00000674
Iteration 171/1000 | Loss: 0.00000673
Iteration 172/1000 | Loss: 0.00000673
Iteration 173/1000 | Loss: 0.00000673
Iteration 174/1000 | Loss: 0.00000673
Iteration 175/1000 | Loss: 0.00000673
Iteration 176/1000 | Loss: 0.00000673
Iteration 177/1000 | Loss: 0.00000673
Iteration 178/1000 | Loss: 0.00000673
Iteration 179/1000 | Loss: 0.00000673
Iteration 180/1000 | Loss: 0.00000673
Iteration 181/1000 | Loss: 0.00000673
Iteration 182/1000 | Loss: 0.00000673
Iteration 183/1000 | Loss: 0.00000673
Iteration 184/1000 | Loss: 0.00000673
Iteration 185/1000 | Loss: 0.00000673
Iteration 186/1000 | Loss: 0.00000673
Iteration 187/1000 | Loss: 0.00000673
Iteration 188/1000 | Loss: 0.00000672
Iteration 189/1000 | Loss: 0.00000672
Iteration 190/1000 | Loss: 0.00000672
Iteration 191/1000 | Loss: 0.00000672
Iteration 192/1000 | Loss: 0.00000672
Iteration 193/1000 | Loss: 0.00000672
Iteration 194/1000 | Loss: 0.00000672
Iteration 195/1000 | Loss: 0.00000672
Iteration 196/1000 | Loss: 0.00000672
Iteration 197/1000 | Loss: 0.00000672
Iteration 198/1000 | Loss: 0.00000672
Iteration 199/1000 | Loss: 0.00000672
Iteration 200/1000 | Loss: 0.00000672
Iteration 201/1000 | Loss: 0.00000671
Iteration 202/1000 | Loss: 0.00000671
Iteration 203/1000 | Loss: 0.00000671
Iteration 204/1000 | Loss: 0.00000671
Iteration 205/1000 | Loss: 0.00000671
Iteration 206/1000 | Loss: 0.00000671
Iteration 207/1000 | Loss: 0.00000671
Iteration 208/1000 | Loss: 0.00000671
Iteration 209/1000 | Loss: 0.00000671
Iteration 210/1000 | Loss: 0.00000671
Iteration 211/1000 | Loss: 0.00000671
Iteration 212/1000 | Loss: 0.00000671
Iteration 213/1000 | Loss: 0.00000671
Iteration 214/1000 | Loss: 0.00000670
Iteration 215/1000 | Loss: 0.00000670
Iteration 216/1000 | Loss: 0.00000670
Iteration 217/1000 | Loss: 0.00000670
Iteration 218/1000 | Loss: 0.00000670
Iteration 219/1000 | Loss: 0.00000670
Iteration 220/1000 | Loss: 0.00000670
Iteration 221/1000 | Loss: 0.00000670
Iteration 222/1000 | Loss: 0.00000670
Iteration 223/1000 | Loss: 0.00000670
Iteration 224/1000 | Loss: 0.00000670
Iteration 225/1000 | Loss: 0.00000670
Iteration 226/1000 | Loss: 0.00000670
Iteration 227/1000 | Loss: 0.00000669
Iteration 228/1000 | Loss: 0.00000669
Iteration 229/1000 | Loss: 0.00000669
Iteration 230/1000 | Loss: 0.00000669
Iteration 231/1000 | Loss: 0.00000669
Iteration 232/1000 | Loss: 0.00000669
Iteration 233/1000 | Loss: 0.00000669
Iteration 234/1000 | Loss: 0.00000669
Iteration 235/1000 | Loss: 0.00000669
Iteration 236/1000 | Loss: 0.00000669
Iteration 237/1000 | Loss: 0.00000669
Iteration 238/1000 | Loss: 0.00000669
Iteration 239/1000 | Loss: 0.00000669
Iteration 240/1000 | Loss: 0.00000669
Iteration 241/1000 | Loss: 0.00000669
Iteration 242/1000 | Loss: 0.00000669
Iteration 243/1000 | Loss: 0.00000669
Iteration 244/1000 | Loss: 0.00000669
Iteration 245/1000 | Loss: 0.00000669
Iteration 246/1000 | Loss: 0.00000668
Iteration 247/1000 | Loss: 0.00000668
Iteration 248/1000 | Loss: 0.00000668
Iteration 249/1000 | Loss: 0.00000668
Iteration 250/1000 | Loss: 0.00000668
Iteration 251/1000 | Loss: 0.00000668
Iteration 252/1000 | Loss: 0.00000668
Iteration 253/1000 | Loss: 0.00000668
Iteration 254/1000 | Loss: 0.00000668
Iteration 255/1000 | Loss: 0.00000668
Iteration 256/1000 | Loss: 0.00000668
Iteration 257/1000 | Loss: 0.00000668
Iteration 258/1000 | Loss: 0.00000668
Iteration 259/1000 | Loss: 0.00000668
Iteration 260/1000 | Loss: 0.00000668
Iteration 261/1000 | Loss: 0.00000668
Iteration 262/1000 | Loss: 0.00000668
Iteration 263/1000 | Loss: 0.00000667
Iteration 264/1000 | Loss: 0.00000667
Iteration 265/1000 | Loss: 0.00000667
Iteration 266/1000 | Loss: 0.00000667
Iteration 267/1000 | Loss: 0.00000667
Iteration 268/1000 | Loss: 0.00000667
Iteration 269/1000 | Loss: 0.00000667
Iteration 270/1000 | Loss: 0.00000667
Iteration 271/1000 | Loss: 0.00000667
Iteration 272/1000 | Loss: 0.00000666
Iteration 273/1000 | Loss: 0.00000666
Iteration 274/1000 | Loss: 0.00000666
Iteration 275/1000 | Loss: 0.00000666
Iteration 276/1000 | Loss: 0.00000666
Iteration 277/1000 | Loss: 0.00000666
Iteration 278/1000 | Loss: 0.00000666
Iteration 279/1000 | Loss: 0.00000666
Iteration 280/1000 | Loss: 0.00000666
Iteration 281/1000 | Loss: 0.00000666
Iteration 282/1000 | Loss: 0.00000666
Iteration 283/1000 | Loss: 0.00000666
Iteration 284/1000 | Loss: 0.00000666
Iteration 285/1000 | Loss: 0.00000666
Iteration 286/1000 | Loss: 0.00000666
Iteration 287/1000 | Loss: 0.00000666
Iteration 288/1000 | Loss: 0.00000666
Iteration 289/1000 | Loss: 0.00000666
Iteration 290/1000 | Loss: 0.00000665
Iteration 291/1000 | Loss: 0.00000665
Iteration 292/1000 | Loss: 0.00000665
Iteration 293/1000 | Loss: 0.00000665
Iteration 294/1000 | Loss: 0.00000665
Iteration 295/1000 | Loss: 0.00000665
Iteration 296/1000 | Loss: 0.00000665
Iteration 297/1000 | Loss: 0.00000665
Iteration 298/1000 | Loss: 0.00000665
Iteration 299/1000 | Loss: 0.00000665
Iteration 300/1000 | Loss: 0.00000665
Iteration 301/1000 | Loss: 0.00000665
Iteration 302/1000 | Loss: 0.00000665
Iteration 303/1000 | Loss: 0.00000665
Iteration 304/1000 | Loss: 0.00000665
Iteration 305/1000 | Loss: 0.00000665
Iteration 306/1000 | Loss: 0.00000665
Iteration 307/1000 | Loss: 0.00000665
Iteration 308/1000 | Loss: 0.00000665
Iteration 309/1000 | Loss: 0.00000665
Iteration 310/1000 | Loss: 0.00000665
Iteration 311/1000 | Loss: 0.00000665
Iteration 312/1000 | Loss: 0.00000665
Iteration 313/1000 | Loss: 0.00000665
Iteration 314/1000 | Loss: 0.00000665
Iteration 315/1000 | Loss: 0.00000665
Iteration 316/1000 | Loss: 0.00000665
Iteration 317/1000 | Loss: 0.00000665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [6.654228855040856e-06, 6.654228855040856e-06, 6.654228855040856e-06, 6.654228855040856e-06, 6.654228855040856e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.654228855040856e-06

Optimization complete. Final v2v error: 2.2268028259277344 mm

Highest mean error: 2.364212989807129 mm for frame 6

Lowest mean error: 2.125760555267334 mm for frame 107

Saving results

Total time: 39.96021270751953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936072
Iteration 2/25 | Loss: 0.00142546
Iteration 3/25 | Loss: 0.00129891
Iteration 4/25 | Loss: 0.00110382
Iteration 5/25 | Loss: 0.00108258
Iteration 6/25 | Loss: 0.00106831
Iteration 7/25 | Loss: 0.00106358
Iteration 8/25 | Loss: 0.00106217
Iteration 9/25 | Loss: 0.00105749
Iteration 10/25 | Loss: 0.00105418
Iteration 11/25 | Loss: 0.00105048
Iteration 12/25 | Loss: 0.00104966
Iteration 13/25 | Loss: 0.00104924
Iteration 14/25 | Loss: 0.00104919
Iteration 15/25 | Loss: 0.00104919
Iteration 16/25 | Loss: 0.00104919
Iteration 17/25 | Loss: 0.00104918
Iteration 18/25 | Loss: 0.00104918
Iteration 19/25 | Loss: 0.00104918
Iteration 20/25 | Loss: 0.00104918
Iteration 21/25 | Loss: 0.00104918
Iteration 22/25 | Loss: 0.00104918
Iteration 23/25 | Loss: 0.00104918
Iteration 24/25 | Loss: 0.00104918
Iteration 25/25 | Loss: 0.00104918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50080025
Iteration 2/25 | Loss: 0.00078571
Iteration 3/25 | Loss: 0.00073350
Iteration 4/25 | Loss: 0.00073350
Iteration 5/25 | Loss: 0.00073350
Iteration 6/25 | Loss: 0.00073350
Iteration 7/25 | Loss: 0.00073350
Iteration 8/25 | Loss: 0.00073350
Iteration 9/25 | Loss: 0.00073350
Iteration 10/25 | Loss: 0.00073350
Iteration 11/25 | Loss: 0.00073350
Iteration 12/25 | Loss: 0.00073350
Iteration 13/25 | Loss: 0.00073350
Iteration 14/25 | Loss: 0.00073350
Iteration 15/25 | Loss: 0.00073350
Iteration 16/25 | Loss: 0.00073350
Iteration 17/25 | Loss: 0.00073350
Iteration 18/25 | Loss: 0.00073350
Iteration 19/25 | Loss: 0.00073350
Iteration 20/25 | Loss: 0.00073350
Iteration 21/25 | Loss: 0.00073350
Iteration 22/25 | Loss: 0.00073350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007334977271966636, 0.0007334977271966636, 0.0007334977271966636, 0.0007334977271966636, 0.0007334977271966636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007334977271966636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073350
Iteration 2/1000 | Loss: 0.00007146
Iteration 3/1000 | Loss: 0.00010666
Iteration 4/1000 | Loss: 0.00002019
Iteration 5/1000 | Loss: 0.00001874
Iteration 6/1000 | Loss: 0.00001790
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00002906
Iteration 9/1000 | Loss: 0.00002586
Iteration 10/1000 | Loss: 0.00001761
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001607
Iteration 14/1000 | Loss: 0.00001603
Iteration 15/1000 | Loss: 0.00044337
Iteration 16/1000 | Loss: 0.00002822
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001971
Iteration 19/1000 | Loss: 0.00005901
Iteration 20/1000 | Loss: 0.00011466
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00006417
Iteration 23/1000 | Loss: 0.00004601
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00003266
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001440
Iteration 30/1000 | Loss: 0.00001428
Iteration 31/1000 | Loss: 0.00001425
Iteration 32/1000 | Loss: 0.00001425
Iteration 33/1000 | Loss: 0.00001425
Iteration 34/1000 | Loss: 0.00001424
Iteration 35/1000 | Loss: 0.00001424
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001422
Iteration 39/1000 | Loss: 0.00001422
Iteration 40/1000 | Loss: 0.00001421
Iteration 41/1000 | Loss: 0.00001420
Iteration 42/1000 | Loss: 0.00001419
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00002452
Iteration 45/1000 | Loss: 0.00001772
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001410
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001409
Iteration 57/1000 | Loss: 0.00001409
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001408
Iteration 60/1000 | Loss: 0.00001407
Iteration 61/1000 | Loss: 0.00001407
Iteration 62/1000 | Loss: 0.00001407
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001395
Iteration 65/1000 | Loss: 0.00001395
Iteration 66/1000 | Loss: 0.00001395
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001393
Iteration 69/1000 | Loss: 0.00001393
Iteration 70/1000 | Loss: 0.00001389
Iteration 71/1000 | Loss: 0.00001389
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001383
Iteration 92/1000 | Loss: 0.00001383
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00003493
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001380
Iteration 107/1000 | Loss: 0.00001380
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001380
Iteration 111/1000 | Loss: 0.00001380
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00001407
Iteration 114/1000 | Loss: 0.00002495
Iteration 115/1000 | Loss: 0.00001559
Iteration 116/1000 | Loss: 0.00001377
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001376
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001376
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001376
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001374
Iteration 129/1000 | Loss: 0.00001374
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001374
Iteration 136/1000 | Loss: 0.00001374
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001372
Iteration 155/1000 | Loss: 0.00001372
Iteration 156/1000 | Loss: 0.00001372
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001372
Iteration 159/1000 | Loss: 0.00001372
Iteration 160/1000 | Loss: 0.00001372
Iteration 161/1000 | Loss: 0.00001372
Iteration 162/1000 | Loss: 0.00001372
Iteration 163/1000 | Loss: 0.00001372
Iteration 164/1000 | Loss: 0.00001372
Iteration 165/1000 | Loss: 0.00001372
Iteration 166/1000 | Loss: 0.00001372
Iteration 167/1000 | Loss: 0.00001372
Iteration 168/1000 | Loss: 0.00001372
Iteration 169/1000 | Loss: 0.00001372
Iteration 170/1000 | Loss: 0.00001372
Iteration 171/1000 | Loss: 0.00001372
Iteration 172/1000 | Loss: 0.00001372
Iteration 173/1000 | Loss: 0.00001372
Iteration 174/1000 | Loss: 0.00001372
Iteration 175/1000 | Loss: 0.00001372
Iteration 176/1000 | Loss: 0.00001372
Iteration 177/1000 | Loss: 0.00001372
Iteration 178/1000 | Loss: 0.00001372
Iteration 179/1000 | Loss: 0.00001372
Iteration 180/1000 | Loss: 0.00001372
Iteration 181/1000 | Loss: 0.00001372
Iteration 182/1000 | Loss: 0.00001372
Iteration 183/1000 | Loss: 0.00001372
Iteration 184/1000 | Loss: 0.00001372
Iteration 185/1000 | Loss: 0.00001372
Iteration 186/1000 | Loss: 0.00001372
Iteration 187/1000 | Loss: 0.00001372
Iteration 188/1000 | Loss: 0.00001372
Iteration 189/1000 | Loss: 0.00001372
Iteration 190/1000 | Loss: 0.00001372
Iteration 191/1000 | Loss: 0.00001372
Iteration 192/1000 | Loss: 0.00001372
Iteration 193/1000 | Loss: 0.00001372
Iteration 194/1000 | Loss: 0.00001372
Iteration 195/1000 | Loss: 0.00001372
Iteration 196/1000 | Loss: 0.00001372
Iteration 197/1000 | Loss: 0.00001372
Iteration 198/1000 | Loss: 0.00001372
Iteration 199/1000 | Loss: 0.00001372
Iteration 200/1000 | Loss: 0.00001372
Iteration 201/1000 | Loss: 0.00001372
Iteration 202/1000 | Loss: 0.00001372
Iteration 203/1000 | Loss: 0.00001372
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001372
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.3720647075388115e-05, 1.3720647075388115e-05, 1.3720647075388115e-05, 1.3720647075388115e-05, 1.3720647075388115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3720647075388115e-05

Optimization complete. Final v2v error: 3.1249899864196777 mm

Highest mean error: 3.914970874786377 mm for frame 178

Lowest mean error: 2.5650947093963623 mm for frame 200

Saving results

Total time: 91.0729513168335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433085
Iteration 2/25 | Loss: 0.00110221
Iteration 3/25 | Loss: 0.00102183
Iteration 4/25 | Loss: 0.00101556
Iteration 5/25 | Loss: 0.00101307
Iteration 6/25 | Loss: 0.00101307
Iteration 7/25 | Loss: 0.00101307
Iteration 8/25 | Loss: 0.00101307
Iteration 9/25 | Loss: 0.00101307
Iteration 10/25 | Loss: 0.00101307
Iteration 11/25 | Loss: 0.00101307
Iteration 12/25 | Loss: 0.00101307
Iteration 13/25 | Loss: 0.00101307
Iteration 14/25 | Loss: 0.00101307
Iteration 15/25 | Loss: 0.00101307
Iteration 16/25 | Loss: 0.00101307
Iteration 17/25 | Loss: 0.00101307
Iteration 18/25 | Loss: 0.00101307
Iteration 19/25 | Loss: 0.00101307
Iteration 20/25 | Loss: 0.00101307
Iteration 21/25 | Loss: 0.00101307
Iteration 22/25 | Loss: 0.00101307
Iteration 23/25 | Loss: 0.00101307
Iteration 24/25 | Loss: 0.00101307
Iteration 25/25 | Loss: 0.00101307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40963924
Iteration 2/25 | Loss: 0.00079046
Iteration 3/25 | Loss: 0.00079046
Iteration 4/25 | Loss: 0.00079046
Iteration 5/25 | Loss: 0.00079046
Iteration 6/25 | Loss: 0.00079046
Iteration 7/25 | Loss: 0.00079046
Iteration 8/25 | Loss: 0.00079045
Iteration 9/25 | Loss: 0.00079045
Iteration 10/25 | Loss: 0.00079045
Iteration 11/25 | Loss: 0.00079045
Iteration 12/25 | Loss: 0.00079045
Iteration 13/25 | Loss: 0.00079045
Iteration 14/25 | Loss: 0.00079045
Iteration 15/25 | Loss: 0.00079045
Iteration 16/25 | Loss: 0.00079045
Iteration 17/25 | Loss: 0.00079045
Iteration 18/25 | Loss: 0.00079045
Iteration 19/25 | Loss: 0.00079045
Iteration 20/25 | Loss: 0.00079045
Iteration 21/25 | Loss: 0.00079045
Iteration 22/25 | Loss: 0.00079045
Iteration 23/25 | Loss: 0.00079045
Iteration 24/25 | Loss: 0.00079045
Iteration 25/25 | Loss: 0.00079045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079045
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001393
Iteration 4/1000 | Loss: 0.00001286
Iteration 5/1000 | Loss: 0.00001242
Iteration 6/1000 | Loss: 0.00001213
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001179
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001134
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001133
Iteration 24/1000 | Loss: 0.00001132
Iteration 25/1000 | Loss: 0.00001132
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001132
Iteration 28/1000 | Loss: 0.00001132
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001131
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001130
Iteration 34/1000 | Loss: 0.00001130
Iteration 35/1000 | Loss: 0.00001130
Iteration 36/1000 | Loss: 0.00001129
Iteration 37/1000 | Loss: 0.00001129
Iteration 38/1000 | Loss: 0.00001129
Iteration 39/1000 | Loss: 0.00001129
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001126
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001120
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001107
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001105
Iteration 101/1000 | Loss: 0.00001105
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001103
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001101
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001098
Iteration 127/1000 | Loss: 0.00001098
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001097
Iteration 130/1000 | Loss: 0.00001097
Iteration 131/1000 | Loss: 0.00001097
Iteration 132/1000 | Loss: 0.00001097
Iteration 133/1000 | Loss: 0.00001097
Iteration 134/1000 | Loss: 0.00001097
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001096
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001095
Iteration 142/1000 | Loss: 0.00001095
Iteration 143/1000 | Loss: 0.00001095
Iteration 144/1000 | Loss: 0.00001095
Iteration 145/1000 | Loss: 0.00001095
Iteration 146/1000 | Loss: 0.00001095
Iteration 147/1000 | Loss: 0.00001095
Iteration 148/1000 | Loss: 0.00001095
Iteration 149/1000 | Loss: 0.00001095
Iteration 150/1000 | Loss: 0.00001095
Iteration 151/1000 | Loss: 0.00001095
Iteration 152/1000 | Loss: 0.00001095
Iteration 153/1000 | Loss: 0.00001094
Iteration 154/1000 | Loss: 0.00001094
Iteration 155/1000 | Loss: 0.00001094
Iteration 156/1000 | Loss: 0.00001094
Iteration 157/1000 | Loss: 0.00001094
Iteration 158/1000 | Loss: 0.00001094
Iteration 159/1000 | Loss: 0.00001094
Iteration 160/1000 | Loss: 0.00001094
Iteration 161/1000 | Loss: 0.00001094
Iteration 162/1000 | Loss: 0.00001094
Iteration 163/1000 | Loss: 0.00001094
Iteration 164/1000 | Loss: 0.00001094
Iteration 165/1000 | Loss: 0.00001094
Iteration 166/1000 | Loss: 0.00001094
Iteration 167/1000 | Loss: 0.00001094
Iteration 168/1000 | Loss: 0.00001094
Iteration 169/1000 | Loss: 0.00001094
Iteration 170/1000 | Loss: 0.00001094
Iteration 171/1000 | Loss: 0.00001094
Iteration 172/1000 | Loss: 0.00001094
Iteration 173/1000 | Loss: 0.00001094
Iteration 174/1000 | Loss: 0.00001094
Iteration 175/1000 | Loss: 0.00001094
Iteration 176/1000 | Loss: 0.00001094
Iteration 177/1000 | Loss: 0.00001094
Iteration 178/1000 | Loss: 0.00001094
Iteration 179/1000 | Loss: 0.00001094
Iteration 180/1000 | Loss: 0.00001094
Iteration 181/1000 | Loss: 0.00001094
Iteration 182/1000 | Loss: 0.00001094
Iteration 183/1000 | Loss: 0.00001094
Iteration 184/1000 | Loss: 0.00001094
Iteration 185/1000 | Loss: 0.00001094
Iteration 186/1000 | Loss: 0.00001094
Iteration 187/1000 | Loss: 0.00001094
Iteration 188/1000 | Loss: 0.00001094
Iteration 189/1000 | Loss: 0.00001094
Iteration 190/1000 | Loss: 0.00001094
Iteration 191/1000 | Loss: 0.00001094
Iteration 192/1000 | Loss: 0.00001094
Iteration 193/1000 | Loss: 0.00001094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.0939262210740708e-05, 1.0939262210740708e-05, 1.0939262210740708e-05, 1.0939262210740708e-05, 1.0939262210740708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0939262210740708e-05

Optimization complete. Final v2v error: 2.741182565689087 mm

Highest mean error: 2.925457000732422 mm for frame 98

Lowest mean error: 2.4683518409729004 mm for frame 232

Saving results

Total time: 39.599976539611816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081528
Iteration 2/25 | Loss: 0.01081528
Iteration 3/25 | Loss: 0.01081527
Iteration 4/25 | Loss: 0.00391991
Iteration 5/25 | Loss: 0.00269511
Iteration 6/25 | Loss: 0.00243802
Iteration 7/25 | Loss: 0.00176116
Iteration 8/25 | Loss: 0.00164308
Iteration 9/25 | Loss: 0.00165837
Iteration 10/25 | Loss: 0.00166452
Iteration 11/25 | Loss: 0.00159032
Iteration 12/25 | Loss: 0.00155214
Iteration 13/25 | Loss: 0.00152393
Iteration 14/25 | Loss: 0.00152305
Iteration 15/25 | Loss: 0.00151991
Iteration 16/25 | Loss: 0.00151959
Iteration 17/25 | Loss: 0.00151954
Iteration 18/25 | Loss: 0.00151953
Iteration 19/25 | Loss: 0.00151953
Iteration 20/25 | Loss: 0.00151953
Iteration 21/25 | Loss: 0.00151953
Iteration 22/25 | Loss: 0.00151952
Iteration 23/25 | Loss: 0.00151952
Iteration 24/25 | Loss: 0.00151952
Iteration 25/25 | Loss: 0.00151952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65323031
Iteration 2/25 | Loss: 0.00523445
Iteration 3/25 | Loss: 0.00156807
Iteration 4/25 | Loss: 0.00156767
Iteration 5/25 | Loss: 0.00156767
Iteration 6/25 | Loss: 0.00156766
Iteration 7/25 | Loss: 0.00156766
Iteration 8/25 | Loss: 0.00156766
Iteration 9/25 | Loss: 0.00156766
Iteration 10/25 | Loss: 0.00156766
Iteration 11/25 | Loss: 0.00156766
Iteration 12/25 | Loss: 0.00156766
Iteration 13/25 | Loss: 0.00156766
Iteration 14/25 | Loss: 0.00156766
Iteration 15/25 | Loss: 0.00156766
Iteration 16/25 | Loss: 0.00156766
Iteration 17/25 | Loss: 0.00156766
Iteration 18/25 | Loss: 0.00156766
Iteration 19/25 | Loss: 0.00156766
Iteration 20/25 | Loss: 0.00156766
Iteration 21/25 | Loss: 0.00156766
Iteration 22/25 | Loss: 0.00156766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015676618786528707, 0.0015676618786528707, 0.0015676618786528707, 0.0015676618786528707, 0.0015676618786528707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015676618786528707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156766
Iteration 2/1000 | Loss: 0.00069734
Iteration 3/1000 | Loss: 0.00004267
Iteration 4/1000 | Loss: 0.00003528
Iteration 5/1000 | Loss: 0.00003297
Iteration 6/1000 | Loss: 0.00003204
Iteration 7/1000 | Loss: 0.00003125
Iteration 8/1000 | Loss: 0.00003061
Iteration 9/1000 | Loss: 0.00003025
Iteration 10/1000 | Loss: 0.00003264
Iteration 11/1000 | Loss: 0.00003050
Iteration 12/1000 | Loss: 0.00003519
Iteration 13/1000 | Loss: 0.00003433
Iteration 14/1000 | Loss: 0.00003522
Iteration 15/1000 | Loss: 0.00003152
Iteration 16/1000 | Loss: 0.00003006
Iteration 17/1000 | Loss: 0.00002941
Iteration 18/1000 | Loss: 0.00002897
Iteration 19/1000 | Loss: 0.00002875
Iteration 20/1000 | Loss: 0.00002849
Iteration 21/1000 | Loss: 0.00002823
Iteration 22/1000 | Loss: 0.00002809
Iteration 23/1000 | Loss: 0.00002808
Iteration 24/1000 | Loss: 0.00002808
Iteration 25/1000 | Loss: 0.00002808
Iteration 26/1000 | Loss: 0.00002807
Iteration 27/1000 | Loss: 0.00002805
Iteration 28/1000 | Loss: 0.00002805
Iteration 29/1000 | Loss: 0.00002805
Iteration 30/1000 | Loss: 0.00002805
Iteration 31/1000 | Loss: 0.00002805
Iteration 32/1000 | Loss: 0.00002805
Iteration 33/1000 | Loss: 0.00002804
Iteration 34/1000 | Loss: 0.00002804
Iteration 35/1000 | Loss: 0.00002804
Iteration 36/1000 | Loss: 0.00002804
Iteration 37/1000 | Loss: 0.00002804
Iteration 38/1000 | Loss: 0.00002804
Iteration 39/1000 | Loss: 0.00002804
Iteration 40/1000 | Loss: 0.00002804
Iteration 41/1000 | Loss: 0.00002803
Iteration 42/1000 | Loss: 0.00002803
Iteration 43/1000 | Loss: 0.00002802
Iteration 44/1000 | Loss: 0.00002802
Iteration 45/1000 | Loss: 0.00002802
Iteration 46/1000 | Loss: 0.00002802
Iteration 47/1000 | Loss: 0.00002802
Iteration 48/1000 | Loss: 0.00002802
Iteration 49/1000 | Loss: 0.00002802
Iteration 50/1000 | Loss: 0.00002802
Iteration 51/1000 | Loss: 0.00002801
Iteration 52/1000 | Loss: 0.00002801
Iteration 53/1000 | Loss: 0.00002801
Iteration 54/1000 | Loss: 0.00002801
Iteration 55/1000 | Loss: 0.00002801
Iteration 56/1000 | Loss: 0.00002801
Iteration 57/1000 | Loss: 0.00002801
Iteration 58/1000 | Loss: 0.00002801
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002800
Iteration 62/1000 | Loss: 0.00002800
Iteration 63/1000 | Loss: 0.00002800
Iteration 64/1000 | Loss: 0.00002800
Iteration 65/1000 | Loss: 0.00002800
Iteration 66/1000 | Loss: 0.00002799
Iteration 67/1000 | Loss: 0.00002799
Iteration 68/1000 | Loss: 0.00002799
Iteration 69/1000 | Loss: 0.00002799
Iteration 70/1000 | Loss: 0.00002798
Iteration 71/1000 | Loss: 0.00002798
Iteration 72/1000 | Loss: 0.00002798
Iteration 73/1000 | Loss: 0.00002798
Iteration 74/1000 | Loss: 0.00002798
Iteration 75/1000 | Loss: 0.00002797
Iteration 76/1000 | Loss: 0.00002797
Iteration 77/1000 | Loss: 0.00002797
Iteration 78/1000 | Loss: 0.00002796
Iteration 79/1000 | Loss: 0.00002796
Iteration 80/1000 | Loss: 0.00002796
Iteration 81/1000 | Loss: 0.00002796
Iteration 82/1000 | Loss: 0.00002796
Iteration 83/1000 | Loss: 0.00002795
Iteration 84/1000 | Loss: 0.00002795
Iteration 85/1000 | Loss: 0.00002795
Iteration 86/1000 | Loss: 0.00002795
Iteration 87/1000 | Loss: 0.00002795
Iteration 88/1000 | Loss: 0.00002795
Iteration 89/1000 | Loss: 0.00002795
Iteration 90/1000 | Loss: 0.00002795
Iteration 91/1000 | Loss: 0.00002794
Iteration 92/1000 | Loss: 0.00002794
Iteration 93/1000 | Loss: 0.00002794
Iteration 94/1000 | Loss: 0.00002794
Iteration 95/1000 | Loss: 0.00002794
Iteration 96/1000 | Loss: 0.00002792
Iteration 97/1000 | Loss: 0.00002792
Iteration 98/1000 | Loss: 0.00002792
Iteration 99/1000 | Loss: 0.00002791
Iteration 100/1000 | Loss: 0.00002790
Iteration 101/1000 | Loss: 0.00002790
Iteration 102/1000 | Loss: 0.00002790
Iteration 103/1000 | Loss: 0.00002790
Iteration 104/1000 | Loss: 0.00002790
Iteration 105/1000 | Loss: 0.00002790
Iteration 106/1000 | Loss: 0.00002790
Iteration 107/1000 | Loss: 0.00002789
Iteration 108/1000 | Loss: 0.00002789
Iteration 109/1000 | Loss: 0.00002788
Iteration 110/1000 | Loss: 0.00002788
Iteration 111/1000 | Loss: 0.00002787
Iteration 112/1000 | Loss: 0.00002787
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00002787
Iteration 115/1000 | Loss: 0.00002786
Iteration 116/1000 | Loss: 0.00002785
Iteration 117/1000 | Loss: 0.00002785
Iteration 118/1000 | Loss: 0.00002785
Iteration 119/1000 | Loss: 0.00002785
Iteration 120/1000 | Loss: 0.00002785
Iteration 121/1000 | Loss: 0.00002784
Iteration 122/1000 | Loss: 0.00002784
Iteration 123/1000 | Loss: 0.00002784
Iteration 124/1000 | Loss: 0.00002784
Iteration 125/1000 | Loss: 0.00002784
Iteration 126/1000 | Loss: 0.00002784
Iteration 127/1000 | Loss: 0.00002784
Iteration 128/1000 | Loss: 0.00002783
Iteration 129/1000 | Loss: 0.00002783
Iteration 130/1000 | Loss: 0.00002783
Iteration 131/1000 | Loss: 0.00002782
Iteration 132/1000 | Loss: 0.00002782
Iteration 133/1000 | Loss: 0.00002782
Iteration 134/1000 | Loss: 0.00002782
Iteration 135/1000 | Loss: 0.00002782
Iteration 136/1000 | Loss: 0.00002781
Iteration 137/1000 | Loss: 0.00002780
Iteration 138/1000 | Loss: 0.00002780
Iteration 139/1000 | Loss: 0.00002780
Iteration 140/1000 | Loss: 0.00002780
Iteration 141/1000 | Loss: 0.00002779
Iteration 142/1000 | Loss: 0.00002779
Iteration 143/1000 | Loss: 0.00002779
Iteration 144/1000 | Loss: 0.00002779
Iteration 145/1000 | Loss: 0.00002779
Iteration 146/1000 | Loss: 0.00002778
Iteration 147/1000 | Loss: 0.00002778
Iteration 148/1000 | Loss: 0.00002778
Iteration 149/1000 | Loss: 0.00002778
Iteration 150/1000 | Loss: 0.00002778
Iteration 151/1000 | Loss: 0.00002778
Iteration 152/1000 | Loss: 0.00002778
Iteration 153/1000 | Loss: 0.00002778
Iteration 154/1000 | Loss: 0.00002777
Iteration 155/1000 | Loss: 0.00002777
Iteration 156/1000 | Loss: 0.00002777
Iteration 157/1000 | Loss: 0.00002777
Iteration 158/1000 | Loss: 0.00002777
Iteration 159/1000 | Loss: 0.00002777
Iteration 160/1000 | Loss: 0.00002777
Iteration 161/1000 | Loss: 0.00002777
Iteration 162/1000 | Loss: 0.00002777
Iteration 163/1000 | Loss: 0.00002777
Iteration 164/1000 | Loss: 0.00002776
Iteration 165/1000 | Loss: 0.00002776
Iteration 166/1000 | Loss: 0.00002776
Iteration 167/1000 | Loss: 0.00002776
Iteration 168/1000 | Loss: 0.00002776
Iteration 169/1000 | Loss: 0.00002775
Iteration 170/1000 | Loss: 0.00002775
Iteration 171/1000 | Loss: 0.00002775
Iteration 172/1000 | Loss: 0.00002775
Iteration 173/1000 | Loss: 0.00002775
Iteration 174/1000 | Loss: 0.00002774
Iteration 175/1000 | Loss: 0.00002774
Iteration 176/1000 | Loss: 0.00002774
Iteration 177/1000 | Loss: 0.00002774
Iteration 178/1000 | Loss: 0.00002774
Iteration 179/1000 | Loss: 0.00002774
Iteration 180/1000 | Loss: 0.00002774
Iteration 181/1000 | Loss: 0.00002774
Iteration 182/1000 | Loss: 0.00002774
Iteration 183/1000 | Loss: 0.00002773
Iteration 184/1000 | Loss: 0.00002773
Iteration 185/1000 | Loss: 0.00002773
Iteration 186/1000 | Loss: 0.00002773
Iteration 187/1000 | Loss: 0.00002773
Iteration 188/1000 | Loss: 0.00002773
Iteration 189/1000 | Loss: 0.00002773
Iteration 190/1000 | Loss: 0.00002773
Iteration 191/1000 | Loss: 0.00002773
Iteration 192/1000 | Loss: 0.00002773
Iteration 193/1000 | Loss: 0.00002773
Iteration 194/1000 | Loss: 0.00002773
Iteration 195/1000 | Loss: 0.00002772
Iteration 196/1000 | Loss: 0.00002772
Iteration 197/1000 | Loss: 0.00002772
Iteration 198/1000 | Loss: 0.00002772
Iteration 199/1000 | Loss: 0.00002772
Iteration 200/1000 | Loss: 0.00002772
Iteration 201/1000 | Loss: 0.00002772
Iteration 202/1000 | Loss: 0.00002772
Iteration 203/1000 | Loss: 0.00002772
Iteration 204/1000 | Loss: 0.00002771
Iteration 205/1000 | Loss: 0.00002771
Iteration 206/1000 | Loss: 0.00002771
Iteration 207/1000 | Loss: 0.00002771
Iteration 208/1000 | Loss: 0.00002771
Iteration 209/1000 | Loss: 0.00002771
Iteration 210/1000 | Loss: 0.00002771
Iteration 211/1000 | Loss: 0.00002771
Iteration 212/1000 | Loss: 0.00002770
Iteration 213/1000 | Loss: 0.00002770
Iteration 214/1000 | Loss: 0.00002770
Iteration 215/1000 | Loss: 0.00002770
Iteration 216/1000 | Loss: 0.00002770
Iteration 217/1000 | Loss: 0.00002770
Iteration 218/1000 | Loss: 0.00002769
Iteration 219/1000 | Loss: 0.00002769
Iteration 220/1000 | Loss: 0.00002769
Iteration 221/1000 | Loss: 0.00002769
Iteration 222/1000 | Loss: 0.00002769
Iteration 223/1000 | Loss: 0.00002769
Iteration 224/1000 | Loss: 0.00002769
Iteration 225/1000 | Loss: 0.00002769
Iteration 226/1000 | Loss: 0.00002769
Iteration 227/1000 | Loss: 0.00002769
Iteration 228/1000 | Loss: 0.00002769
Iteration 229/1000 | Loss: 0.00002769
Iteration 230/1000 | Loss: 0.00002768
Iteration 231/1000 | Loss: 0.00002768
Iteration 232/1000 | Loss: 0.00002768
Iteration 233/1000 | Loss: 0.00002768
Iteration 234/1000 | Loss: 0.00002768
Iteration 235/1000 | Loss: 0.00002768
Iteration 236/1000 | Loss: 0.00002768
Iteration 237/1000 | Loss: 0.00002768
Iteration 238/1000 | Loss: 0.00002768
Iteration 239/1000 | Loss: 0.00002768
Iteration 240/1000 | Loss: 0.00002768
Iteration 241/1000 | Loss: 0.00002768
Iteration 242/1000 | Loss: 0.00002767
Iteration 243/1000 | Loss: 0.00002767
Iteration 244/1000 | Loss: 0.00002767
Iteration 245/1000 | Loss: 0.00002767
Iteration 246/1000 | Loss: 0.00002767
Iteration 247/1000 | Loss: 0.00002767
Iteration 248/1000 | Loss: 0.00002767
Iteration 249/1000 | Loss: 0.00002767
Iteration 250/1000 | Loss: 0.00002767
Iteration 251/1000 | Loss: 0.00002767
Iteration 252/1000 | Loss: 0.00002767
Iteration 253/1000 | Loss: 0.00002767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [2.767260230029933e-05, 2.767260230029933e-05, 2.767260230029933e-05, 2.767260230029933e-05, 2.767260230029933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.767260230029933e-05

Optimization complete. Final v2v error: 4.002617359161377 mm

Highest mean error: 9.621137619018555 mm for frame 82

Lowest mean error: 3.352020502090454 mm for frame 4

Saving results

Total time: 72.52984690666199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985061
Iteration 2/25 | Loss: 0.00219781
Iteration 3/25 | Loss: 0.00168569
Iteration 4/25 | Loss: 0.00153648
Iteration 5/25 | Loss: 0.00158102
Iteration 6/25 | Loss: 0.00145181
Iteration 7/25 | Loss: 0.00141323
Iteration 8/25 | Loss: 0.00138809
Iteration 9/25 | Loss: 0.00137447
Iteration 10/25 | Loss: 0.00137004
Iteration 11/25 | Loss: 0.00136917
Iteration 12/25 | Loss: 0.00136874
Iteration 13/25 | Loss: 0.00136854
Iteration 14/25 | Loss: 0.00136844
Iteration 15/25 | Loss: 0.00136838
Iteration 16/25 | Loss: 0.00136838
Iteration 17/25 | Loss: 0.00136838
Iteration 18/25 | Loss: 0.00136838
Iteration 19/25 | Loss: 0.00136837
Iteration 20/25 | Loss: 0.00136836
Iteration 21/25 | Loss: 0.00136829
Iteration 22/25 | Loss: 0.00136828
Iteration 23/25 | Loss: 0.00136827
Iteration 24/25 | Loss: 0.00136827
Iteration 25/25 | Loss: 0.00136826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34069836
Iteration 2/25 | Loss: 0.00228862
Iteration 3/25 | Loss: 0.00228862
Iteration 4/25 | Loss: 0.00228862
Iteration 5/25 | Loss: 0.00228862
Iteration 6/25 | Loss: 0.00228862
Iteration 7/25 | Loss: 0.00228862
Iteration 8/25 | Loss: 0.00228862
Iteration 9/25 | Loss: 0.00228862
Iteration 10/25 | Loss: 0.00228862
Iteration 11/25 | Loss: 0.00228862
Iteration 12/25 | Loss: 0.00228862
Iteration 13/25 | Loss: 0.00228862
Iteration 14/25 | Loss: 0.00228862
Iteration 15/25 | Loss: 0.00228862
Iteration 16/25 | Loss: 0.00228862
Iteration 17/25 | Loss: 0.00228862
Iteration 18/25 | Loss: 0.00228862
Iteration 19/25 | Loss: 0.00228862
Iteration 20/25 | Loss: 0.00228862
Iteration 21/25 | Loss: 0.00228862
Iteration 22/25 | Loss: 0.00228862
Iteration 23/25 | Loss: 0.00228862
Iteration 24/25 | Loss: 0.00228862
Iteration 25/25 | Loss: 0.00228862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228862
Iteration 2/1000 | Loss: 0.00030876
Iteration 3/1000 | Loss: 0.00023765
Iteration 4/1000 | Loss: 0.00021119
Iteration 5/1000 | Loss: 0.00019031
Iteration 6/1000 | Loss: 0.00018137
Iteration 7/1000 | Loss: 0.00017335
Iteration 8/1000 | Loss: 0.00016512
Iteration 9/1000 | Loss: 0.00015958
Iteration 10/1000 | Loss: 0.00040326
Iteration 11/1000 | Loss: 0.00030842
Iteration 12/1000 | Loss: 0.00042642
Iteration 13/1000 | Loss: 0.00078316
Iteration 14/1000 | Loss: 0.00321656
Iteration 15/1000 | Loss: 0.00622568
Iteration 16/1000 | Loss: 0.00052029
Iteration 17/1000 | Loss: 0.00027143
Iteration 18/1000 | Loss: 0.00018367
Iteration 19/1000 | Loss: 0.00012527
Iteration 20/1000 | Loss: 0.00008205
Iteration 21/1000 | Loss: 0.00005727
Iteration 22/1000 | Loss: 0.00004728
Iteration 23/1000 | Loss: 0.00004003
Iteration 24/1000 | Loss: 0.00003404
Iteration 25/1000 | Loss: 0.00002870
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002217
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001621
Iteration 32/1000 | Loss: 0.00001552
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001422
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001386
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001365
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001359
Iteration 64/1000 | Loss: 0.00001359
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001359
Iteration 67/1000 | Loss: 0.00001359
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001359
Iteration 83/1000 | Loss: 0.00001359
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.358904683002038e-05, 1.358904683002038e-05, 1.358904683002038e-05, 1.358904683002038e-05, 1.358904683002038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.358904683002038e-05

Optimization complete. Final v2v error: 3.146514415740967 mm

Highest mean error: 3.411041498184204 mm for frame 74

Lowest mean error: 2.791079521179199 mm for frame 134

Saving results

Total time: 83.65440797805786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049910
Iteration 2/25 | Loss: 0.00370677
Iteration 3/25 | Loss: 0.00244582
Iteration 4/25 | Loss: 0.00211042
Iteration 5/25 | Loss: 0.00185243
Iteration 6/25 | Loss: 0.00181198
Iteration 7/25 | Loss: 0.00163599
Iteration 8/25 | Loss: 0.00158956
Iteration 9/25 | Loss: 0.00149954
Iteration 10/25 | Loss: 0.00144522
Iteration 11/25 | Loss: 0.00143325
Iteration 12/25 | Loss: 0.00142000
Iteration 13/25 | Loss: 0.00141540
Iteration 14/25 | Loss: 0.00140424
Iteration 15/25 | Loss: 0.00141096
Iteration 16/25 | Loss: 0.00141012
Iteration 17/25 | Loss: 0.00138891
Iteration 18/25 | Loss: 0.00139435
Iteration 19/25 | Loss: 0.00138349
Iteration 20/25 | Loss: 0.00138058
Iteration 21/25 | Loss: 0.00137854
Iteration 22/25 | Loss: 0.00137902
Iteration 23/25 | Loss: 0.00138095
Iteration 24/25 | Loss: 0.00138202
Iteration 25/25 | Loss: 0.00138492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26087594
Iteration 2/25 | Loss: 0.00388940
Iteration 3/25 | Loss: 0.00328634
Iteration 4/25 | Loss: 0.00328634
Iteration 5/25 | Loss: 0.00328634
Iteration 6/25 | Loss: 0.00328634
Iteration 7/25 | Loss: 0.00328634
Iteration 8/25 | Loss: 0.00328634
Iteration 9/25 | Loss: 0.00328634
Iteration 10/25 | Loss: 0.00328634
Iteration 11/25 | Loss: 0.00328634
Iteration 12/25 | Loss: 0.00328634
Iteration 13/25 | Loss: 0.00328634
Iteration 14/25 | Loss: 0.00328634
Iteration 15/25 | Loss: 0.00328634
Iteration 16/25 | Loss: 0.00328633
Iteration 17/25 | Loss: 0.00328633
Iteration 18/25 | Loss: 0.00328633
Iteration 19/25 | Loss: 0.00328633
Iteration 20/25 | Loss: 0.00328633
Iteration 21/25 | Loss: 0.00328633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0032863346859812737, 0.0032863346859812737, 0.0032863346859812737, 0.0032863346859812737, 0.0032863346859812737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032863346859812737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00328633
Iteration 2/1000 | Loss: 0.01093930
Iteration 3/1000 | Loss: 0.00618855
Iteration 4/1000 | Loss: 0.00067306
Iteration 5/1000 | Loss: 0.00066294
Iteration 6/1000 | Loss: 0.00120172
Iteration 7/1000 | Loss: 0.00114936
Iteration 8/1000 | Loss: 0.00028663
Iteration 9/1000 | Loss: 0.00071061
Iteration 10/1000 | Loss: 0.00023719
Iteration 11/1000 | Loss: 0.00066650
Iteration 12/1000 | Loss: 0.00047211
Iteration 13/1000 | Loss: 0.00039349
Iteration 14/1000 | Loss: 0.00027466
Iteration 15/1000 | Loss: 0.00054431
Iteration 16/1000 | Loss: 0.00018911
Iteration 17/1000 | Loss: 0.00016900
Iteration 18/1000 | Loss: 0.00254164
Iteration 19/1000 | Loss: 0.00177981
Iteration 20/1000 | Loss: 0.00179285
Iteration 21/1000 | Loss: 0.00311007
Iteration 22/1000 | Loss: 0.00200208
Iteration 23/1000 | Loss: 0.00182393
Iteration 24/1000 | Loss: 0.00152880
Iteration 25/1000 | Loss: 0.00135030
Iteration 26/1000 | Loss: 0.00229233
Iteration 27/1000 | Loss: 0.00101103
Iteration 28/1000 | Loss: 0.00135410
Iteration 29/1000 | Loss: 0.00034286
Iteration 30/1000 | Loss: 0.00032799
Iteration 31/1000 | Loss: 0.00039597
Iteration 32/1000 | Loss: 0.00068602
Iteration 33/1000 | Loss: 0.00025418
Iteration 34/1000 | Loss: 0.00009835
Iteration 35/1000 | Loss: 0.00005361
Iteration 36/1000 | Loss: 0.00011450
Iteration 37/1000 | Loss: 0.00006087
Iteration 38/1000 | Loss: 0.00004632
Iteration 39/1000 | Loss: 0.00003496
Iteration 40/1000 | Loss: 0.00031277
Iteration 41/1000 | Loss: 0.00061296
Iteration 42/1000 | Loss: 0.00049670
Iteration 43/1000 | Loss: 0.00042041
Iteration 44/1000 | Loss: 0.00030898
Iteration 45/1000 | Loss: 0.00063593
Iteration 46/1000 | Loss: 0.00019239
Iteration 47/1000 | Loss: 0.00006359
Iteration 48/1000 | Loss: 0.00013653
Iteration 49/1000 | Loss: 0.00003998
Iteration 50/1000 | Loss: 0.00070247
Iteration 51/1000 | Loss: 0.00014422
Iteration 52/1000 | Loss: 0.00009962
Iteration 53/1000 | Loss: 0.00005848
Iteration 54/1000 | Loss: 0.00009725
Iteration 55/1000 | Loss: 0.00002251
Iteration 56/1000 | Loss: 0.00001974
Iteration 57/1000 | Loss: 0.00009824
Iteration 58/1000 | Loss: 0.00007216
Iteration 59/1000 | Loss: 0.00002148
Iteration 60/1000 | Loss: 0.00004538
Iteration 61/1000 | Loss: 0.00007094
Iteration 62/1000 | Loss: 0.00002768
Iteration 63/1000 | Loss: 0.00001578
Iteration 64/1000 | Loss: 0.00001555
Iteration 65/1000 | Loss: 0.00002503
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001494
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001483
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001473
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001472
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001471
Iteration 95/1000 | Loss: 0.00001471
Iteration 96/1000 | Loss: 0.00001471
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001470
Iteration 100/1000 | Loss: 0.00001470
Iteration 101/1000 | Loss: 0.00001470
Iteration 102/1000 | Loss: 0.00001470
Iteration 103/1000 | Loss: 0.00001470
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001469
Iteration 110/1000 | Loss: 0.00001469
Iteration 111/1000 | Loss: 0.00001469
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001466
Iteration 121/1000 | Loss: 0.00001466
Iteration 122/1000 | Loss: 0.00001466
Iteration 123/1000 | Loss: 0.00001465
Iteration 124/1000 | Loss: 0.00001465
Iteration 125/1000 | Loss: 0.00001464
Iteration 126/1000 | Loss: 0.00001464
Iteration 127/1000 | Loss: 0.00001464
Iteration 128/1000 | Loss: 0.00001464
Iteration 129/1000 | Loss: 0.00001464
Iteration 130/1000 | Loss: 0.00001464
Iteration 131/1000 | Loss: 0.00001464
Iteration 132/1000 | Loss: 0.00001464
Iteration 133/1000 | Loss: 0.00001464
Iteration 134/1000 | Loss: 0.00001464
Iteration 135/1000 | Loss: 0.00001463
Iteration 136/1000 | Loss: 0.00001463
Iteration 137/1000 | Loss: 0.00001463
Iteration 138/1000 | Loss: 0.00001463
Iteration 139/1000 | Loss: 0.00001463
Iteration 140/1000 | Loss: 0.00001463
Iteration 141/1000 | Loss: 0.00001463
Iteration 142/1000 | Loss: 0.00001463
Iteration 143/1000 | Loss: 0.00001463
Iteration 144/1000 | Loss: 0.00001463
Iteration 145/1000 | Loss: 0.00001463
Iteration 146/1000 | Loss: 0.00001463
Iteration 147/1000 | Loss: 0.00001462
Iteration 148/1000 | Loss: 0.00001462
Iteration 149/1000 | Loss: 0.00001462
Iteration 150/1000 | Loss: 0.00001462
Iteration 151/1000 | Loss: 0.00001462
Iteration 152/1000 | Loss: 0.00001462
Iteration 153/1000 | Loss: 0.00001462
Iteration 154/1000 | Loss: 0.00001462
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001461
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001460
Iteration 162/1000 | Loss: 0.00001460
Iteration 163/1000 | Loss: 0.00001460
Iteration 164/1000 | Loss: 0.00001460
Iteration 165/1000 | Loss: 0.00001460
Iteration 166/1000 | Loss: 0.00001460
Iteration 167/1000 | Loss: 0.00001460
Iteration 168/1000 | Loss: 0.00001460
Iteration 169/1000 | Loss: 0.00001460
Iteration 170/1000 | Loss: 0.00001460
Iteration 171/1000 | Loss: 0.00001459
Iteration 172/1000 | Loss: 0.00002448
Iteration 173/1000 | Loss: 0.00001460
Iteration 174/1000 | Loss: 0.00001459
Iteration 175/1000 | Loss: 0.00001459
Iteration 176/1000 | Loss: 0.00001458
Iteration 177/1000 | Loss: 0.00001458
Iteration 178/1000 | Loss: 0.00001458
Iteration 179/1000 | Loss: 0.00001458
Iteration 180/1000 | Loss: 0.00001458
Iteration 181/1000 | Loss: 0.00001458
Iteration 182/1000 | Loss: 0.00001458
Iteration 183/1000 | Loss: 0.00001458
Iteration 184/1000 | Loss: 0.00001457
Iteration 185/1000 | Loss: 0.00001457
Iteration 186/1000 | Loss: 0.00001457
Iteration 187/1000 | Loss: 0.00001457
Iteration 188/1000 | Loss: 0.00001457
Iteration 189/1000 | Loss: 0.00001457
Iteration 190/1000 | Loss: 0.00001457
Iteration 191/1000 | Loss: 0.00001457
Iteration 192/1000 | Loss: 0.00001457
Iteration 193/1000 | Loss: 0.00001457
Iteration 194/1000 | Loss: 0.00001457
Iteration 195/1000 | Loss: 0.00001457
Iteration 196/1000 | Loss: 0.00001457
Iteration 197/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.4571224710380193e-05, 1.4571224710380193e-05, 1.4571224710380193e-05, 1.4571224710380193e-05, 1.4571224710380193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4571224710380193e-05

Optimization complete. Final v2v error: 3.0724356174468994 mm

Highest mean error: 4.990137577056885 mm for frame 61

Lowest mean error: 2.4442880153656006 mm for frame 93

Saving results

Total time: 159.88135147094727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834408
Iteration 2/25 | Loss: 0.00147193
Iteration 3/25 | Loss: 0.00116756
Iteration 4/25 | Loss: 0.00114484
Iteration 5/25 | Loss: 0.00114207
Iteration 6/25 | Loss: 0.00114197
Iteration 7/25 | Loss: 0.00114197
Iteration 8/25 | Loss: 0.00114197
Iteration 9/25 | Loss: 0.00114197
Iteration 10/25 | Loss: 0.00114197
Iteration 11/25 | Loss: 0.00114197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001141972839832306, 0.001141972839832306, 0.001141972839832306, 0.001141972839832306, 0.001141972839832306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001141972839832306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65715659
Iteration 2/25 | Loss: 0.00087539
Iteration 3/25 | Loss: 0.00087538
Iteration 4/25 | Loss: 0.00087538
Iteration 5/25 | Loss: 0.00087538
Iteration 6/25 | Loss: 0.00087538
Iteration 7/25 | Loss: 0.00087538
Iteration 8/25 | Loss: 0.00087538
Iteration 9/25 | Loss: 0.00087538
Iteration 10/25 | Loss: 0.00087538
Iteration 11/25 | Loss: 0.00087538
Iteration 12/25 | Loss: 0.00087538
Iteration 13/25 | Loss: 0.00087538
Iteration 14/25 | Loss: 0.00087538
Iteration 15/25 | Loss: 0.00087538
Iteration 16/25 | Loss: 0.00087538
Iteration 17/25 | Loss: 0.00087538
Iteration 18/25 | Loss: 0.00087538
Iteration 19/25 | Loss: 0.00087538
Iteration 20/25 | Loss: 0.00087538
Iteration 21/25 | Loss: 0.00087538
Iteration 22/25 | Loss: 0.00087538
Iteration 23/25 | Loss: 0.00087538
Iteration 24/25 | Loss: 0.00087538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008753775036893785, 0.0008753775036893785, 0.0008753775036893785, 0.0008753775036893785, 0.0008753775036893785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008753775036893785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087538
Iteration 2/1000 | Loss: 0.00004696
Iteration 3/1000 | Loss: 0.00002199
Iteration 4/1000 | Loss: 0.00001830
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001547
Iteration 8/1000 | Loss: 0.00001522
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001467
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001445
Iteration 15/1000 | Loss: 0.00001443
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001439
Iteration 18/1000 | Loss: 0.00001439
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001436
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001436
Iteration 23/1000 | Loss: 0.00001435
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001435
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001435
Iteration 28/1000 | Loss: 0.00001434
Iteration 29/1000 | Loss: 0.00001434
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001431
Iteration 37/1000 | Loss: 0.00001431
Iteration 38/1000 | Loss: 0.00001430
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001429
Iteration 42/1000 | Loss: 0.00001429
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001428
Iteration 45/1000 | Loss: 0.00001428
Iteration 46/1000 | Loss: 0.00001428
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001427
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001426
Iteration 53/1000 | Loss: 0.00001426
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001425
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001424
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001424
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001422
Iteration 70/1000 | Loss: 0.00001422
Iteration 71/1000 | Loss: 0.00001422
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001422
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001421
Iteration 77/1000 | Loss: 0.00001421
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001420
Iteration 83/1000 | Loss: 0.00001420
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001420
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001418
Iteration 95/1000 | Loss: 0.00001418
Iteration 96/1000 | Loss: 0.00001418
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001418
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001416
Iteration 106/1000 | Loss: 0.00001416
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001415
Iteration 112/1000 | Loss: 0.00001415
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001414
Iteration 121/1000 | Loss: 0.00001414
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001413
Iteration 126/1000 | Loss: 0.00001413
Iteration 127/1000 | Loss: 0.00001413
Iteration 128/1000 | Loss: 0.00001413
Iteration 129/1000 | Loss: 0.00001413
Iteration 130/1000 | Loss: 0.00001413
Iteration 131/1000 | Loss: 0.00001413
Iteration 132/1000 | Loss: 0.00001413
Iteration 133/1000 | Loss: 0.00001413
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001413
Iteration 142/1000 | Loss: 0.00001413
Iteration 143/1000 | Loss: 0.00001412
Iteration 144/1000 | Loss: 0.00001412
Iteration 145/1000 | Loss: 0.00001412
Iteration 146/1000 | Loss: 0.00001412
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Iteration 150/1000 | Loss: 0.00001412
Iteration 151/1000 | Loss: 0.00001412
Iteration 152/1000 | Loss: 0.00001412
Iteration 153/1000 | Loss: 0.00001412
Iteration 154/1000 | Loss: 0.00001412
Iteration 155/1000 | Loss: 0.00001412
Iteration 156/1000 | Loss: 0.00001411
Iteration 157/1000 | Loss: 0.00001411
Iteration 158/1000 | Loss: 0.00001411
Iteration 159/1000 | Loss: 0.00001411
Iteration 160/1000 | Loss: 0.00001411
Iteration 161/1000 | Loss: 0.00001411
Iteration 162/1000 | Loss: 0.00001411
Iteration 163/1000 | Loss: 0.00001411
Iteration 164/1000 | Loss: 0.00001411
Iteration 165/1000 | Loss: 0.00001411
Iteration 166/1000 | Loss: 0.00001411
Iteration 167/1000 | Loss: 0.00001411
Iteration 168/1000 | Loss: 0.00001411
Iteration 169/1000 | Loss: 0.00001410
Iteration 170/1000 | Loss: 0.00001410
Iteration 171/1000 | Loss: 0.00001410
Iteration 172/1000 | Loss: 0.00001410
Iteration 173/1000 | Loss: 0.00001410
Iteration 174/1000 | Loss: 0.00001409
Iteration 175/1000 | Loss: 0.00001409
Iteration 176/1000 | Loss: 0.00001409
Iteration 177/1000 | Loss: 0.00001409
Iteration 178/1000 | Loss: 0.00001409
Iteration 179/1000 | Loss: 0.00001409
Iteration 180/1000 | Loss: 0.00001409
Iteration 181/1000 | Loss: 0.00001409
Iteration 182/1000 | Loss: 0.00001409
Iteration 183/1000 | Loss: 0.00001409
Iteration 184/1000 | Loss: 0.00001409
Iteration 185/1000 | Loss: 0.00001409
Iteration 186/1000 | Loss: 0.00001409
Iteration 187/1000 | Loss: 0.00001409
Iteration 188/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.4087430827203207e-05, 1.4087430827203207e-05, 1.4087430827203207e-05, 1.4087430827203207e-05, 1.4087430827203207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4087430827203207e-05

Optimization complete. Final v2v error: 3.2090423107147217 mm

Highest mean error: 3.43522572517395 mm for frame 95

Lowest mean error: 2.8816866874694824 mm for frame 12

Saving results

Total time: 41.690707206726074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588296
Iteration 2/25 | Loss: 0.00121325
Iteration 3/25 | Loss: 0.00111901
Iteration 4/25 | Loss: 0.00109526
Iteration 5/25 | Loss: 0.00108712
Iteration 6/25 | Loss: 0.00108537
Iteration 7/25 | Loss: 0.00108459
Iteration 8/25 | Loss: 0.00108458
Iteration 9/25 | Loss: 0.00108458
Iteration 10/25 | Loss: 0.00108458
Iteration 11/25 | Loss: 0.00108458
Iteration 12/25 | Loss: 0.00108458
Iteration 13/25 | Loss: 0.00108458
Iteration 14/25 | Loss: 0.00108458
Iteration 15/25 | Loss: 0.00108458
Iteration 16/25 | Loss: 0.00108458
Iteration 17/25 | Loss: 0.00108458
Iteration 18/25 | Loss: 0.00108458
Iteration 19/25 | Loss: 0.00108458
Iteration 20/25 | Loss: 0.00108458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010845784563571215, 0.0010845784563571215, 0.0010845784563571215, 0.0010845784563571215, 0.0010845784563571215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010845784563571215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03138554
Iteration 2/25 | Loss: 0.00119138
Iteration 3/25 | Loss: 0.00119129
Iteration 4/25 | Loss: 0.00119129
Iteration 5/25 | Loss: 0.00119129
Iteration 6/25 | Loss: 0.00119129
Iteration 7/25 | Loss: 0.00119129
Iteration 8/25 | Loss: 0.00119129
Iteration 9/25 | Loss: 0.00119129
Iteration 10/25 | Loss: 0.00119129
Iteration 11/25 | Loss: 0.00119129
Iteration 12/25 | Loss: 0.00119129
Iteration 13/25 | Loss: 0.00119129
Iteration 14/25 | Loss: 0.00119129
Iteration 15/25 | Loss: 0.00119129
Iteration 16/25 | Loss: 0.00119129
Iteration 17/25 | Loss: 0.00119129
Iteration 18/25 | Loss: 0.00119129
Iteration 19/25 | Loss: 0.00119129
Iteration 20/25 | Loss: 0.00119129
Iteration 21/25 | Loss: 0.00119129
Iteration 22/25 | Loss: 0.00119129
Iteration 23/25 | Loss: 0.00119129
Iteration 24/25 | Loss: 0.00119129
Iteration 25/25 | Loss: 0.00119129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119129
Iteration 2/1000 | Loss: 0.00008499
Iteration 3/1000 | Loss: 0.00004115
Iteration 4/1000 | Loss: 0.00003207
Iteration 5/1000 | Loss: 0.00002976
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002786
Iteration 8/1000 | Loss: 0.00002716
Iteration 9/1000 | Loss: 0.00002663
Iteration 10/1000 | Loss: 0.00002637
Iteration 11/1000 | Loss: 0.00002609
Iteration 12/1000 | Loss: 0.00002608
Iteration 13/1000 | Loss: 0.00002591
Iteration 14/1000 | Loss: 0.00002582
Iteration 15/1000 | Loss: 0.00002574
Iteration 16/1000 | Loss: 0.00002555
Iteration 17/1000 | Loss: 0.00002553
Iteration 18/1000 | Loss: 0.00002545
Iteration 19/1000 | Loss: 0.00002541
Iteration 20/1000 | Loss: 0.00002540
Iteration 21/1000 | Loss: 0.00002540
Iteration 22/1000 | Loss: 0.00002540
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002539
Iteration 25/1000 | Loss: 0.00002538
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002537
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002536
Iteration 30/1000 | Loss: 0.00002536
Iteration 31/1000 | Loss: 0.00002535
Iteration 32/1000 | Loss: 0.00002535
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002535
Iteration 36/1000 | Loss: 0.00002535
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002533
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002531
Iteration 42/1000 | Loss: 0.00002531
Iteration 43/1000 | Loss: 0.00002530
Iteration 44/1000 | Loss: 0.00002529
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00002528
Iteration 47/1000 | Loss: 0.00002528
Iteration 48/1000 | Loss: 0.00002528
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00002527
Iteration 51/1000 | Loss: 0.00002527
Iteration 52/1000 | Loss: 0.00002521
Iteration 53/1000 | Loss: 0.00002521
Iteration 54/1000 | Loss: 0.00002521
Iteration 55/1000 | Loss: 0.00002519
Iteration 56/1000 | Loss: 0.00002519
Iteration 57/1000 | Loss: 0.00002518
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002517
Iteration 62/1000 | Loss: 0.00002517
Iteration 63/1000 | Loss: 0.00002517
Iteration 64/1000 | Loss: 0.00002517
Iteration 65/1000 | Loss: 0.00002517
Iteration 66/1000 | Loss: 0.00002517
Iteration 67/1000 | Loss: 0.00002517
Iteration 68/1000 | Loss: 0.00002516
Iteration 69/1000 | Loss: 0.00002515
Iteration 70/1000 | Loss: 0.00002515
Iteration 71/1000 | Loss: 0.00002514
Iteration 72/1000 | Loss: 0.00002513
Iteration 73/1000 | Loss: 0.00002513
Iteration 74/1000 | Loss: 0.00002513
Iteration 75/1000 | Loss: 0.00002513
Iteration 76/1000 | Loss: 0.00002513
Iteration 77/1000 | Loss: 0.00002513
Iteration 78/1000 | Loss: 0.00002513
Iteration 79/1000 | Loss: 0.00002513
Iteration 80/1000 | Loss: 0.00002512
Iteration 81/1000 | Loss: 0.00002512
Iteration 82/1000 | Loss: 0.00002511
Iteration 83/1000 | Loss: 0.00002509
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002508
Iteration 86/1000 | Loss: 0.00002508
Iteration 87/1000 | Loss: 0.00002508
Iteration 88/1000 | Loss: 0.00002508
Iteration 89/1000 | Loss: 0.00002508
Iteration 90/1000 | Loss: 0.00002508
Iteration 91/1000 | Loss: 0.00002508
Iteration 92/1000 | Loss: 0.00002508
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002508
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002507
Iteration 98/1000 | Loss: 0.00002507
Iteration 99/1000 | Loss: 0.00002506
Iteration 100/1000 | Loss: 0.00002506
Iteration 101/1000 | Loss: 0.00002505
Iteration 102/1000 | Loss: 0.00002505
Iteration 103/1000 | Loss: 0.00002505
Iteration 104/1000 | Loss: 0.00002505
Iteration 105/1000 | Loss: 0.00002504
Iteration 106/1000 | Loss: 0.00002504
Iteration 107/1000 | Loss: 0.00002504
Iteration 108/1000 | Loss: 0.00002504
Iteration 109/1000 | Loss: 0.00002504
Iteration 110/1000 | Loss: 0.00002504
Iteration 111/1000 | Loss: 0.00002504
Iteration 112/1000 | Loss: 0.00002503
Iteration 113/1000 | Loss: 0.00002502
Iteration 114/1000 | Loss: 0.00002502
Iteration 115/1000 | Loss: 0.00002501
Iteration 116/1000 | Loss: 0.00002501
Iteration 117/1000 | Loss: 0.00002501
Iteration 118/1000 | Loss: 0.00002501
Iteration 119/1000 | Loss: 0.00002501
Iteration 120/1000 | Loss: 0.00002501
Iteration 121/1000 | Loss: 0.00002501
Iteration 122/1000 | Loss: 0.00002501
Iteration 123/1000 | Loss: 0.00002500
Iteration 124/1000 | Loss: 0.00002500
Iteration 125/1000 | Loss: 0.00002500
Iteration 126/1000 | Loss: 0.00002500
Iteration 127/1000 | Loss: 0.00002499
Iteration 128/1000 | Loss: 0.00002499
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002499
Iteration 131/1000 | Loss: 0.00002499
Iteration 132/1000 | Loss: 0.00002499
Iteration 133/1000 | Loss: 0.00002498
Iteration 134/1000 | Loss: 0.00002497
Iteration 135/1000 | Loss: 0.00002497
Iteration 136/1000 | Loss: 0.00002497
Iteration 137/1000 | Loss: 0.00002497
Iteration 138/1000 | Loss: 0.00002497
Iteration 139/1000 | Loss: 0.00002497
Iteration 140/1000 | Loss: 0.00002497
Iteration 141/1000 | Loss: 0.00002496
Iteration 142/1000 | Loss: 0.00002496
Iteration 143/1000 | Loss: 0.00002496
Iteration 144/1000 | Loss: 0.00002496
Iteration 145/1000 | Loss: 0.00002496
Iteration 146/1000 | Loss: 0.00002496
Iteration 147/1000 | Loss: 0.00002496
Iteration 148/1000 | Loss: 0.00002495
Iteration 149/1000 | Loss: 0.00002495
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00002495
Iteration 152/1000 | Loss: 0.00002495
Iteration 153/1000 | Loss: 0.00002495
Iteration 154/1000 | Loss: 0.00002494
Iteration 155/1000 | Loss: 0.00002494
Iteration 156/1000 | Loss: 0.00002494
Iteration 157/1000 | Loss: 0.00002494
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002493
Iteration 163/1000 | Loss: 0.00002493
Iteration 164/1000 | Loss: 0.00002493
Iteration 165/1000 | Loss: 0.00002493
Iteration 166/1000 | Loss: 0.00002493
Iteration 167/1000 | Loss: 0.00002492
Iteration 168/1000 | Loss: 0.00002492
Iteration 169/1000 | Loss: 0.00002492
Iteration 170/1000 | Loss: 0.00002492
Iteration 171/1000 | Loss: 0.00002492
Iteration 172/1000 | Loss: 0.00002492
Iteration 173/1000 | Loss: 0.00002492
Iteration 174/1000 | Loss: 0.00002492
Iteration 175/1000 | Loss: 0.00002492
Iteration 176/1000 | Loss: 0.00002492
Iteration 177/1000 | Loss: 0.00002492
Iteration 178/1000 | Loss: 0.00002491
Iteration 179/1000 | Loss: 0.00002491
Iteration 180/1000 | Loss: 0.00002491
Iteration 181/1000 | Loss: 0.00002491
Iteration 182/1000 | Loss: 0.00002491
Iteration 183/1000 | Loss: 0.00002491
Iteration 184/1000 | Loss: 0.00002491
Iteration 185/1000 | Loss: 0.00002491
Iteration 186/1000 | Loss: 0.00002491
Iteration 187/1000 | Loss: 0.00002491
Iteration 188/1000 | Loss: 0.00002491
Iteration 189/1000 | Loss: 0.00002490
Iteration 190/1000 | Loss: 0.00002490
Iteration 191/1000 | Loss: 0.00002490
Iteration 192/1000 | Loss: 0.00002490
Iteration 193/1000 | Loss: 0.00002490
Iteration 194/1000 | Loss: 0.00002490
Iteration 195/1000 | Loss: 0.00002490
Iteration 196/1000 | Loss: 0.00002490
Iteration 197/1000 | Loss: 0.00002490
Iteration 198/1000 | Loss: 0.00002490
Iteration 199/1000 | Loss: 0.00002490
Iteration 200/1000 | Loss: 0.00002490
Iteration 201/1000 | Loss: 0.00002490
Iteration 202/1000 | Loss: 0.00002490
Iteration 203/1000 | Loss: 0.00002490
Iteration 204/1000 | Loss: 0.00002490
Iteration 205/1000 | Loss: 0.00002490
Iteration 206/1000 | Loss: 0.00002490
Iteration 207/1000 | Loss: 0.00002490
Iteration 208/1000 | Loss: 0.00002490
Iteration 209/1000 | Loss: 0.00002490
Iteration 210/1000 | Loss: 0.00002490
Iteration 211/1000 | Loss: 0.00002489
Iteration 212/1000 | Loss: 0.00002489
Iteration 213/1000 | Loss: 0.00002489
Iteration 214/1000 | Loss: 0.00002489
Iteration 215/1000 | Loss: 0.00002489
Iteration 216/1000 | Loss: 0.00002489
Iteration 217/1000 | Loss: 0.00002489
Iteration 218/1000 | Loss: 0.00002489
Iteration 219/1000 | Loss: 0.00002489
Iteration 220/1000 | Loss: 0.00002489
Iteration 221/1000 | Loss: 0.00002489
Iteration 222/1000 | Loss: 0.00002489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.4894023226806894e-05, 2.4894023226806894e-05, 2.4894023226806894e-05, 2.4894023226806894e-05, 2.4894023226806894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4894023226806894e-05

Optimization complete. Final v2v error: 4.112399578094482 mm

Highest mean error: 4.4550886154174805 mm for frame 64

Lowest mean error: 3.755324125289917 mm for frame 9

Saving results

Total time: 47.58813142776489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868017
Iteration 2/25 | Loss: 0.00220135
Iteration 3/25 | Loss: 0.00161591
Iteration 4/25 | Loss: 0.00133352
Iteration 5/25 | Loss: 0.00128991
Iteration 6/25 | Loss: 0.00131374
Iteration 7/25 | Loss: 0.00129044
Iteration 8/25 | Loss: 0.00126090
Iteration 9/25 | Loss: 0.00124661
Iteration 10/25 | Loss: 0.00123903
Iteration 11/25 | Loss: 0.00123896
Iteration 12/25 | Loss: 0.00123651
Iteration 13/25 | Loss: 0.00123494
Iteration 14/25 | Loss: 0.00123458
Iteration 15/25 | Loss: 0.00123603
Iteration 16/25 | Loss: 0.00123384
Iteration 17/25 | Loss: 0.00123236
Iteration 18/25 | Loss: 0.00123049
Iteration 19/25 | Loss: 0.00123191
Iteration 20/25 | Loss: 0.00123016
Iteration 21/25 | Loss: 0.00123310
Iteration 22/25 | Loss: 0.00123048
Iteration 23/25 | Loss: 0.00123178
Iteration 24/25 | Loss: 0.00123390
Iteration 25/25 | Loss: 0.00123376

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14794087
Iteration 2/25 | Loss: 0.00058256
Iteration 3/25 | Loss: 0.00058256
Iteration 4/25 | Loss: 0.00058256
Iteration 5/25 | Loss: 0.00058256
Iteration 6/25 | Loss: 0.00058255
Iteration 7/25 | Loss: 0.00058255
Iteration 8/25 | Loss: 0.00058255
Iteration 9/25 | Loss: 0.00058255
Iteration 10/25 | Loss: 0.00058255
Iteration 11/25 | Loss: 0.00058255
Iteration 12/25 | Loss: 0.00058255
Iteration 13/25 | Loss: 0.00058255
Iteration 14/25 | Loss: 0.00058255
Iteration 15/25 | Loss: 0.00058255
Iteration 16/25 | Loss: 0.00058255
Iteration 17/25 | Loss: 0.00058255
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005825536209158599, 0.0005825536209158599, 0.0005825536209158599, 0.0005825536209158599, 0.0005825536209158599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005825536209158599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058255
Iteration 2/1000 | Loss: 0.00009551
Iteration 3/1000 | Loss: 0.00012171
Iteration 4/1000 | Loss: 0.00012880
Iteration 5/1000 | Loss: 0.00014034
Iteration 6/1000 | Loss: 0.00018005
Iteration 7/1000 | Loss: 0.00015520
Iteration 8/1000 | Loss: 0.00011065
Iteration 9/1000 | Loss: 0.00019869
Iteration 10/1000 | Loss: 0.00011038
Iteration 11/1000 | Loss: 0.00014727
Iteration 12/1000 | Loss: 0.00016314
Iteration 13/1000 | Loss: 0.00020405
Iteration 14/1000 | Loss: 0.00014785
Iteration 15/1000 | Loss: 0.00018472
Iteration 16/1000 | Loss: 0.00011431
Iteration 17/1000 | Loss: 0.00018301
Iteration 18/1000 | Loss: 0.00012662
Iteration 19/1000 | Loss: 0.00013960
Iteration 20/1000 | Loss: 0.00014501
Iteration 21/1000 | Loss: 0.00018894
Iteration 22/1000 | Loss: 0.00015281
Iteration 23/1000 | Loss: 0.00017666
Iteration 24/1000 | Loss: 0.00019836
Iteration 25/1000 | Loss: 0.00014401
Iteration 26/1000 | Loss: 0.00011861
Iteration 27/1000 | Loss: 0.00010956
Iteration 28/1000 | Loss: 0.00012969
Iteration 29/1000 | Loss: 0.00016241
Iteration 30/1000 | Loss: 0.00013708
Iteration 31/1000 | Loss: 0.00011020
Iteration 32/1000 | Loss: 0.00011537
Iteration 33/1000 | Loss: 0.00013300
Iteration 34/1000 | Loss: 0.00016005
Iteration 35/1000 | Loss: 0.00015250
Iteration 36/1000 | Loss: 0.00013252
Iteration 37/1000 | Loss: 0.00015818
Iteration 38/1000 | Loss: 0.00015988
Iteration 39/1000 | Loss: 0.00015887
Iteration 40/1000 | Loss: 0.00015639
Iteration 41/1000 | Loss: 0.00014803
Iteration 42/1000 | Loss: 0.00010208
Iteration 43/1000 | Loss: 0.00012266
Iteration 44/1000 | Loss: 0.00013151
Iteration 45/1000 | Loss: 0.00010477
Iteration 46/1000 | Loss: 0.00014671
Iteration 47/1000 | Loss: 0.00016161
Iteration 48/1000 | Loss: 0.00016041
Iteration 49/1000 | Loss: 0.00014787
Iteration 50/1000 | Loss: 0.00018269
Iteration 51/1000 | Loss: 0.00018525
Iteration 52/1000 | Loss: 0.00015981
Iteration 53/1000 | Loss: 0.00013320
Iteration 54/1000 | Loss: 0.00015660
Iteration 55/1000 | Loss: 0.00013249
Iteration 56/1000 | Loss: 0.00021087
Iteration 57/1000 | Loss: 0.00021362
Iteration 58/1000 | Loss: 0.00017015
Iteration 59/1000 | Loss: 0.00013186
Iteration 60/1000 | Loss: 0.00015346
Iteration 61/1000 | Loss: 0.00011580
Iteration 62/1000 | Loss: 0.00013865
Iteration 63/1000 | Loss: 0.00013450
Iteration 64/1000 | Loss: 0.00014493
Iteration 65/1000 | Loss: 0.00014552
Iteration 66/1000 | Loss: 0.00013428
Iteration 67/1000 | Loss: 0.00016356
Iteration 68/1000 | Loss: 0.00007297
Iteration 69/1000 | Loss: 0.00011948
Iteration 70/1000 | Loss: 0.00016363
Iteration 71/1000 | Loss: 0.00013309
Iteration 72/1000 | Loss: 0.00013657
Iteration 73/1000 | Loss: 0.00016321
Iteration 74/1000 | Loss: 0.00016585
Iteration 75/1000 | Loss: 0.00011537
Iteration 76/1000 | Loss: 0.00011257
Iteration 77/1000 | Loss: 0.00013632
Iteration 78/1000 | Loss: 0.00009202
Iteration 79/1000 | Loss: 0.00008384
Iteration 80/1000 | Loss: 0.00009738
Iteration 81/1000 | Loss: 0.00018047
Iteration 82/1000 | Loss: 0.00016243
Iteration 83/1000 | Loss: 0.00016620
Iteration 84/1000 | Loss: 0.00018375
Iteration 85/1000 | Loss: 0.00011683
Iteration 86/1000 | Loss: 0.00010324
Iteration 87/1000 | Loss: 0.00009427
Iteration 88/1000 | Loss: 0.00016679
Iteration 89/1000 | Loss: 0.00011914
Iteration 90/1000 | Loss: 0.00007250
Iteration 91/1000 | Loss: 0.00008198
Iteration 92/1000 | Loss: 0.00007310
Iteration 93/1000 | Loss: 0.00007979
Iteration 94/1000 | Loss: 0.00008106
Iteration 95/1000 | Loss: 0.00008573
Iteration 96/1000 | Loss: 0.00008627
Iteration 97/1000 | Loss: 0.00014654
Iteration 98/1000 | Loss: 0.00011916
Iteration 99/1000 | Loss: 0.00012368
Iteration 100/1000 | Loss: 0.00010762
Iteration 101/1000 | Loss: 0.00009431
Iteration 102/1000 | Loss: 0.00015243
Iteration 103/1000 | Loss: 0.00010192
Iteration 104/1000 | Loss: 0.00008427
Iteration 105/1000 | Loss: 0.00010555
Iteration 106/1000 | Loss: 0.00015731
Iteration 107/1000 | Loss: 0.00010355
Iteration 108/1000 | Loss: 0.00009917
Iteration 109/1000 | Loss: 0.00014309
Iteration 110/1000 | Loss: 0.00008716
Iteration 111/1000 | Loss: 0.00009875
Iteration 112/1000 | Loss: 0.00010189
Iteration 113/1000 | Loss: 0.00010458
Iteration 114/1000 | Loss: 0.00018259
Iteration 115/1000 | Loss: 0.00008681
Iteration 116/1000 | Loss: 0.00007908
Iteration 117/1000 | Loss: 0.00014026
Iteration 118/1000 | Loss: 0.00011950
Iteration 119/1000 | Loss: 0.00009234
Iteration 120/1000 | Loss: 0.00007097
Iteration 121/1000 | Loss: 0.00007947
Iteration 122/1000 | Loss: 0.00010475
Iteration 123/1000 | Loss: 0.00011536
Iteration 124/1000 | Loss: 0.00010301
Iteration 125/1000 | Loss: 0.00008405
Iteration 126/1000 | Loss: 0.00008800
Iteration 127/1000 | Loss: 0.00009186
Iteration 128/1000 | Loss: 0.00010286
Iteration 129/1000 | Loss: 0.00010144
Iteration 130/1000 | Loss: 0.00010752
Iteration 131/1000 | Loss: 0.00011541
Iteration 132/1000 | Loss: 0.00010099
Iteration 133/1000 | Loss: 0.00010524
Iteration 134/1000 | Loss: 0.00011451
Iteration 135/1000 | Loss: 0.00008140
Iteration 136/1000 | Loss: 0.00008865
Iteration 137/1000 | Loss: 0.00009381
Iteration 138/1000 | Loss: 0.00007390
Iteration 139/1000 | Loss: 0.00010372
Iteration 140/1000 | Loss: 0.00011206
Iteration 141/1000 | Loss: 0.00011736
Iteration 142/1000 | Loss: 0.00011551
Iteration 143/1000 | Loss: 0.00011516
Iteration 144/1000 | Loss: 0.00019928
Iteration 145/1000 | Loss: 0.00011894
Iteration 146/1000 | Loss: 0.00008298
Iteration 147/1000 | Loss: 0.00006583
Iteration 148/1000 | Loss: 0.00011213
Iteration 149/1000 | Loss: 0.00009201
Iteration 150/1000 | Loss: 0.00007405
Iteration 151/1000 | Loss: 0.00009509
Iteration 152/1000 | Loss: 0.00010406
Iteration 153/1000 | Loss: 0.00008517
Iteration 154/1000 | Loss: 0.00007902
Iteration 155/1000 | Loss: 0.00007860
Iteration 156/1000 | Loss: 0.00004648
Iteration 157/1000 | Loss: 0.00005919
Iteration 158/1000 | Loss: 0.00006046
Iteration 159/1000 | Loss: 0.00005197
Iteration 160/1000 | Loss: 0.00006646
Iteration 161/1000 | Loss: 0.00005378
Iteration 162/1000 | Loss: 0.00006056
Iteration 163/1000 | Loss: 0.00004863
Iteration 164/1000 | Loss: 0.00006549
Iteration 165/1000 | Loss: 0.00005369
Iteration 166/1000 | Loss: 0.00007517
Iteration 167/1000 | Loss: 0.00010380
Iteration 168/1000 | Loss: 0.00007221
Iteration 169/1000 | Loss: 0.00005026
Iteration 170/1000 | Loss: 0.00006300
Iteration 171/1000 | Loss: 0.00007529
Iteration 172/1000 | Loss: 0.00006390
Iteration 173/1000 | Loss: 0.00006311
Iteration 174/1000 | Loss: 0.00003936
Iteration 175/1000 | Loss: 0.00003293
Iteration 176/1000 | Loss: 0.00006172
Iteration 177/1000 | Loss: 0.00004450
Iteration 178/1000 | Loss: 0.00005847
Iteration 179/1000 | Loss: 0.00006374
Iteration 180/1000 | Loss: 0.00004945
Iteration 181/1000 | Loss: 0.00005322
Iteration 182/1000 | Loss: 0.00006869
Iteration 183/1000 | Loss: 0.00004778
Iteration 184/1000 | Loss: 0.00005934
Iteration 185/1000 | Loss: 0.00005337
Iteration 186/1000 | Loss: 0.00005512
Iteration 187/1000 | Loss: 0.00005968
Iteration 188/1000 | Loss: 0.00006410
Iteration 189/1000 | Loss: 0.00006466
Iteration 190/1000 | Loss: 0.00007724
Iteration 191/1000 | Loss: 0.00008122
Iteration 192/1000 | Loss: 0.00006667
Iteration 193/1000 | Loss: 0.00003867
Iteration 194/1000 | Loss: 0.00003285
Iteration 195/1000 | Loss: 0.00003287
Iteration 196/1000 | Loss: 0.00002777
Iteration 197/1000 | Loss: 0.00003397
Iteration 198/1000 | Loss: 0.00002706
Iteration 199/1000 | Loss: 0.00002450
Iteration 200/1000 | Loss: 0.00003149
Iteration 201/1000 | Loss: 0.00002995
Iteration 202/1000 | Loss: 0.00003082
Iteration 203/1000 | Loss: 0.00003987
Iteration 204/1000 | Loss: 0.00002631
Iteration 205/1000 | Loss: 0.00003434
Iteration 206/1000 | Loss: 0.00003276
Iteration 207/1000 | Loss: 0.00003030
Iteration 208/1000 | Loss: 0.00003927
Iteration 209/1000 | Loss: 0.00003209
Iteration 210/1000 | Loss: 0.00004791
Iteration 211/1000 | Loss: 0.00003211
Iteration 212/1000 | Loss: 0.00004630
Iteration 213/1000 | Loss: 0.00004565
Iteration 214/1000 | Loss: 0.00004392
Iteration 215/1000 | Loss: 0.00003592
Iteration 216/1000 | Loss: 0.00003340
Iteration 217/1000 | Loss: 0.00002700
Iteration 218/1000 | Loss: 0.00004110
Iteration 219/1000 | Loss: 0.00004122
Iteration 220/1000 | Loss: 0.00003949
Iteration 221/1000 | Loss: 0.00003464
Iteration 222/1000 | Loss: 0.00003335
Iteration 223/1000 | Loss: 0.00005201
Iteration 224/1000 | Loss: 0.00004378
Iteration 225/1000 | Loss: 0.00004982
Iteration 226/1000 | Loss: 0.00003472
Iteration 227/1000 | Loss: 0.00003679
Iteration 228/1000 | Loss: 0.00003654
Iteration 229/1000 | Loss: 0.00002647
Iteration 230/1000 | Loss: 0.00003716
Iteration 231/1000 | Loss: 0.00004293
Iteration 232/1000 | Loss: 0.00002926
Iteration 233/1000 | Loss: 0.00003713
Iteration 234/1000 | Loss: 0.00004659
Iteration 235/1000 | Loss: 0.00002524
Iteration 236/1000 | Loss: 0.00003679
Iteration 237/1000 | Loss: 0.00003098
Iteration 238/1000 | Loss: 0.00003690
Iteration 239/1000 | Loss: 0.00002527
Iteration 240/1000 | Loss: 0.00003051
Iteration 241/1000 | Loss: 0.00003415
Iteration 242/1000 | Loss: 0.00002537
Iteration 243/1000 | Loss: 0.00002914
Iteration 244/1000 | Loss: 0.00002246
Iteration 245/1000 | Loss: 0.00002745
Iteration 246/1000 | Loss: 0.00002853
Iteration 247/1000 | Loss: 0.00003193
Iteration 248/1000 | Loss: 0.00003275
Iteration 249/1000 | Loss: 0.00003219
Iteration 250/1000 | Loss: 0.00002706
Iteration 251/1000 | Loss: 0.00003268
Iteration 252/1000 | Loss: 0.00002750
Iteration 253/1000 | Loss: 0.00003162
Iteration 254/1000 | Loss: 0.00002893
Iteration 255/1000 | Loss: 0.00003173
Iteration 256/1000 | Loss: 0.00002447
Iteration 257/1000 | Loss: 0.00002691
Iteration 258/1000 | Loss: 0.00002995
Iteration 259/1000 | Loss: 0.00002611
Iteration 260/1000 | Loss: 0.00002001
Iteration 261/1000 | Loss: 0.00002072
Iteration 262/1000 | Loss: 0.00002891
Iteration 263/1000 | Loss: 0.00002592
Iteration 264/1000 | Loss: 0.00002711
Iteration 265/1000 | Loss: 0.00003810
Iteration 266/1000 | Loss: 0.00002809
Iteration 267/1000 | Loss: 0.00003056
Iteration 268/1000 | Loss: 0.00002673
Iteration 269/1000 | Loss: 0.00002548
Iteration 270/1000 | Loss: 0.00002471
Iteration 271/1000 | Loss: 0.00003893
Iteration 272/1000 | Loss: 0.00003323
Iteration 273/1000 | Loss: 0.00002780
Iteration 274/1000 | Loss: 0.00002066
Iteration 275/1000 | Loss: 0.00001993
Iteration 276/1000 | Loss: 0.00001960
Iteration 277/1000 | Loss: 0.00001934
Iteration 278/1000 | Loss: 0.00001911
Iteration 279/1000 | Loss: 0.00001900
Iteration 280/1000 | Loss: 0.00001896
Iteration 281/1000 | Loss: 0.00001891
Iteration 282/1000 | Loss: 0.00001889
Iteration 283/1000 | Loss: 0.00001886
Iteration 284/1000 | Loss: 0.00001886
Iteration 285/1000 | Loss: 0.00001877
Iteration 286/1000 | Loss: 0.00001875
Iteration 287/1000 | Loss: 0.00001860
Iteration 288/1000 | Loss: 0.00001855
Iteration 289/1000 | Loss: 0.00001855
Iteration 290/1000 | Loss: 0.00001855
Iteration 291/1000 | Loss: 0.00001855
Iteration 292/1000 | Loss: 0.00001855
Iteration 293/1000 | Loss: 0.00001855
Iteration 294/1000 | Loss: 0.00001854
Iteration 295/1000 | Loss: 0.00001854
Iteration 296/1000 | Loss: 0.00001854
Iteration 297/1000 | Loss: 0.00001852
Iteration 298/1000 | Loss: 0.00001849
Iteration 299/1000 | Loss: 0.00001846
Iteration 300/1000 | Loss: 0.00001845
Iteration 301/1000 | Loss: 0.00001845
Iteration 302/1000 | Loss: 0.00001843
Iteration 303/1000 | Loss: 0.00001843
Iteration 304/1000 | Loss: 0.00001842
Iteration 305/1000 | Loss: 0.00001842
Iteration 306/1000 | Loss: 0.00001842
Iteration 307/1000 | Loss: 0.00001842
Iteration 308/1000 | Loss: 0.00001842
Iteration 309/1000 | Loss: 0.00001842
Iteration 310/1000 | Loss: 0.00001841
Iteration 311/1000 | Loss: 0.00001841
Iteration 312/1000 | Loss: 0.00001841
Iteration 313/1000 | Loss: 0.00001841
Iteration 314/1000 | Loss: 0.00001841
Iteration 315/1000 | Loss: 0.00001841
Iteration 316/1000 | Loss: 0.00001841
Iteration 317/1000 | Loss: 0.00001840
Iteration 318/1000 | Loss: 0.00001840
Iteration 319/1000 | Loss: 0.00001840
Iteration 320/1000 | Loss: 0.00001839
Iteration 321/1000 | Loss: 0.00001839
Iteration 322/1000 | Loss: 0.00001839
Iteration 323/1000 | Loss: 0.00001839
Iteration 324/1000 | Loss: 0.00001839
Iteration 325/1000 | Loss: 0.00001839
Iteration 326/1000 | Loss: 0.00001838
Iteration 327/1000 | Loss: 0.00001838
Iteration 328/1000 | Loss: 0.00001838
Iteration 329/1000 | Loss: 0.00001838
Iteration 330/1000 | Loss: 0.00001838
Iteration 331/1000 | Loss: 0.00001838
Iteration 332/1000 | Loss: 0.00001837
Iteration 333/1000 | Loss: 0.00001837
Iteration 334/1000 | Loss: 0.00001837
Iteration 335/1000 | Loss: 0.00001837
Iteration 336/1000 | Loss: 0.00001837
Iteration 337/1000 | Loss: 0.00001837
Iteration 338/1000 | Loss: 0.00001837
Iteration 339/1000 | Loss: 0.00001837
Iteration 340/1000 | Loss: 0.00001837
Iteration 341/1000 | Loss: 0.00001837
Iteration 342/1000 | Loss: 0.00001837
Iteration 343/1000 | Loss: 0.00001837
Iteration 344/1000 | Loss: 0.00001837
Iteration 345/1000 | Loss: 0.00001837
Iteration 346/1000 | Loss: 0.00001837
Iteration 347/1000 | Loss: 0.00001837
Iteration 348/1000 | Loss: 0.00001837
Iteration 349/1000 | Loss: 0.00001837
Iteration 350/1000 | Loss: 0.00001837
Iteration 351/1000 | Loss: 0.00001837
Iteration 352/1000 | Loss: 0.00001837
Iteration 353/1000 | Loss: 0.00001837
Iteration 354/1000 | Loss: 0.00001837
Iteration 355/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [1.8368262317380868e-05, 1.8368262317380868e-05, 1.8368262317380868e-05, 1.8368262317380868e-05, 1.8368262317380868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8368262317380868e-05

Optimization complete. Final v2v error: 3.5642311573028564 mm

Highest mean error: 4.371277332305908 mm for frame 183

Lowest mean error: 3.3720622062683105 mm for frame 135

Saving results

Total time: 508.8331663608551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473220
Iteration 2/25 | Loss: 0.00112368
Iteration 3/25 | Loss: 0.00102691
Iteration 4/25 | Loss: 0.00101568
Iteration 5/25 | Loss: 0.00101313
Iteration 6/25 | Loss: 0.00101313
Iteration 7/25 | Loss: 0.00101313
Iteration 8/25 | Loss: 0.00101313
Iteration 9/25 | Loss: 0.00101313
Iteration 10/25 | Loss: 0.00101313
Iteration 11/25 | Loss: 0.00101313
Iteration 12/25 | Loss: 0.00101313
Iteration 13/25 | Loss: 0.00101313
Iteration 14/25 | Loss: 0.00101313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010131309973075986, 0.0010131309973075986, 0.0010131309973075986, 0.0010131309973075986, 0.0010131309973075986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010131309973075986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.92347956
Iteration 2/25 | Loss: 0.00065668
Iteration 3/25 | Loss: 0.00065668
Iteration 4/25 | Loss: 0.00065668
Iteration 5/25 | Loss: 0.00065668
Iteration 6/25 | Loss: 0.00065668
Iteration 7/25 | Loss: 0.00065668
Iteration 8/25 | Loss: 0.00065668
Iteration 9/25 | Loss: 0.00065668
Iteration 10/25 | Loss: 0.00065668
Iteration 11/25 | Loss: 0.00065668
Iteration 12/25 | Loss: 0.00065668
Iteration 13/25 | Loss: 0.00065668
Iteration 14/25 | Loss: 0.00065668
Iteration 15/25 | Loss: 0.00065668
Iteration 16/25 | Loss: 0.00065668
Iteration 17/25 | Loss: 0.00065668
Iteration 18/25 | Loss: 0.00065668
Iteration 19/25 | Loss: 0.00065668
Iteration 20/25 | Loss: 0.00065668
Iteration 21/25 | Loss: 0.00065668
Iteration 22/25 | Loss: 0.00065668
Iteration 23/25 | Loss: 0.00065668
Iteration 24/25 | Loss: 0.00065668
Iteration 25/25 | Loss: 0.00065668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065668
Iteration 2/1000 | Loss: 0.00001887
Iteration 3/1000 | Loss: 0.00001441
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001208
Iteration 8/1000 | Loss: 0.00001174
Iteration 9/1000 | Loss: 0.00001165
Iteration 10/1000 | Loss: 0.00001164
Iteration 11/1000 | Loss: 0.00001164
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001133
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001123
Iteration 23/1000 | Loss: 0.00001121
Iteration 24/1000 | Loss: 0.00001120
Iteration 25/1000 | Loss: 0.00001114
Iteration 26/1000 | Loss: 0.00001112
Iteration 27/1000 | Loss: 0.00001112
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001099
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001089
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001086
Iteration 60/1000 | Loss: 0.00001086
Iteration 61/1000 | Loss: 0.00001086
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001086
Iteration 65/1000 | Loss: 0.00001086
Iteration 66/1000 | Loss: 0.00001086
Iteration 67/1000 | Loss: 0.00001086
Iteration 68/1000 | Loss: 0.00001085
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001079
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001078
Iteration 99/1000 | Loss: 0.00001078
Iteration 100/1000 | Loss: 0.00001078
Iteration 101/1000 | Loss: 0.00001078
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001072
Iteration 130/1000 | Loss: 0.00001072
Iteration 131/1000 | Loss: 0.00001072
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001071
Iteration 141/1000 | Loss: 0.00001071
Iteration 142/1000 | Loss: 0.00001071
Iteration 143/1000 | Loss: 0.00001071
Iteration 144/1000 | Loss: 0.00001071
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.070844336936716e-05, 1.070844336936716e-05, 1.070844336936716e-05, 1.070844336936716e-05, 1.070844336936716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.070844336936716e-05

Optimization complete. Final v2v error: 2.81557559967041 mm

Highest mean error: 3.0269076824188232 mm for frame 80

Lowest mean error: 2.5571770668029785 mm for frame 261

Saving results

Total time: 41.25323438644409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972609
Iteration 2/25 | Loss: 0.00292358
Iteration 3/25 | Loss: 0.00193454
Iteration 4/25 | Loss: 0.00176549
Iteration 5/25 | Loss: 0.00171970
Iteration 6/25 | Loss: 0.00173865
Iteration 7/25 | Loss: 0.00167728
Iteration 8/25 | Loss: 0.00155030
Iteration 9/25 | Loss: 0.00147520
Iteration 10/25 | Loss: 0.00139258
Iteration 11/25 | Loss: 0.00135915
Iteration 12/25 | Loss: 0.00133912
Iteration 13/25 | Loss: 0.00132956
Iteration 14/25 | Loss: 0.00130595
Iteration 15/25 | Loss: 0.00129332
Iteration 16/25 | Loss: 0.00129588
Iteration 17/25 | Loss: 0.00129368
Iteration 18/25 | Loss: 0.00128845
Iteration 19/25 | Loss: 0.00128167
Iteration 20/25 | Loss: 0.00128056
Iteration 21/25 | Loss: 0.00128656
Iteration 22/25 | Loss: 0.00128278
Iteration 23/25 | Loss: 0.00128004
Iteration 24/25 | Loss: 0.00128099
Iteration 25/25 | Loss: 0.00127760

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36817598
Iteration 2/25 | Loss: 0.00244974
Iteration 3/25 | Loss: 0.00237384
Iteration 4/25 | Loss: 0.00237383
Iteration 5/25 | Loss: 0.00237382
Iteration 6/25 | Loss: 0.00237382
Iteration 7/25 | Loss: 0.00237382
Iteration 8/25 | Loss: 0.00237382
Iteration 9/25 | Loss: 0.00237382
Iteration 10/25 | Loss: 0.00237382
Iteration 11/25 | Loss: 0.00237382
Iteration 12/25 | Loss: 0.00237382
Iteration 13/25 | Loss: 0.00237382
Iteration 14/25 | Loss: 0.00237382
Iteration 15/25 | Loss: 0.00237382
Iteration 16/25 | Loss: 0.00237382
Iteration 17/25 | Loss: 0.00237382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002373821334913373, 0.002373821334913373, 0.002373821334913373, 0.002373821334913373, 0.002373821334913373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002373821334913373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237382
Iteration 2/1000 | Loss: 0.00044286
Iteration 3/1000 | Loss: 0.00041466
Iteration 4/1000 | Loss: 0.00098471
Iteration 5/1000 | Loss: 0.00029870
Iteration 6/1000 | Loss: 0.00029064
Iteration 7/1000 | Loss: 0.00116717
Iteration 8/1000 | Loss: 0.00149452
Iteration 9/1000 | Loss: 0.00033991
Iteration 10/1000 | Loss: 0.00177259
Iteration 11/1000 | Loss: 0.00092028
Iteration 12/1000 | Loss: 0.00095586
Iteration 13/1000 | Loss: 0.00018218
Iteration 14/1000 | Loss: 0.00013901
Iteration 15/1000 | Loss: 0.00052838
Iteration 16/1000 | Loss: 0.00064406
Iteration 17/1000 | Loss: 0.00062910
Iteration 18/1000 | Loss: 0.00019744
Iteration 19/1000 | Loss: 0.00034920
Iteration 20/1000 | Loss: 0.00013263
Iteration 21/1000 | Loss: 0.00020473
Iteration 22/1000 | Loss: 0.00012580
Iteration 23/1000 | Loss: 0.00016398
Iteration 24/1000 | Loss: 0.00010860
Iteration 25/1000 | Loss: 0.00062738
Iteration 26/1000 | Loss: 0.00704490
Iteration 27/1000 | Loss: 0.00522587
Iteration 28/1000 | Loss: 0.00128102
Iteration 29/1000 | Loss: 0.00480589
Iteration 30/1000 | Loss: 0.00398159
Iteration 31/1000 | Loss: 0.00384295
Iteration 32/1000 | Loss: 0.00193981
Iteration 33/1000 | Loss: 0.00414412
Iteration 34/1000 | Loss: 0.00249196
Iteration 35/1000 | Loss: 0.00157540
Iteration 36/1000 | Loss: 0.00130297
Iteration 37/1000 | Loss: 0.00162484
Iteration 38/1000 | Loss: 0.00012157
Iteration 39/1000 | Loss: 0.00009279
Iteration 40/1000 | Loss: 0.00097917
Iteration 41/1000 | Loss: 0.00098685
Iteration 42/1000 | Loss: 0.00017925
Iteration 43/1000 | Loss: 0.00050439
Iteration 44/1000 | Loss: 0.00042533
Iteration 45/1000 | Loss: 0.00090992
Iteration 46/1000 | Loss: 0.00004641
Iteration 47/1000 | Loss: 0.00005366
Iteration 48/1000 | Loss: 0.00020240
Iteration 49/1000 | Loss: 0.00020170
Iteration 50/1000 | Loss: 0.00003358
Iteration 51/1000 | Loss: 0.00018388
Iteration 52/1000 | Loss: 0.00018126
Iteration 53/1000 | Loss: 0.00002367
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00026251
Iteration 56/1000 | Loss: 0.00013205
Iteration 57/1000 | Loss: 0.00006819
Iteration 58/1000 | Loss: 0.00019039
Iteration 59/1000 | Loss: 0.00003165
Iteration 60/1000 | Loss: 0.00004551
Iteration 61/1000 | Loss: 0.00002868
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00002068
Iteration 64/1000 | Loss: 0.00003463
Iteration 65/1000 | Loss: 0.00005108
Iteration 66/1000 | Loss: 0.00012352
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001579
Iteration 69/1000 | Loss: 0.00003385
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00004268
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001418
Iteration 79/1000 | Loss: 0.00001418
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00001342
Iteration 93/1000 | Loss: 0.00001341
Iteration 94/1000 | Loss: 0.00001341
Iteration 95/1000 | Loss: 0.00001340
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001338
Iteration 98/1000 | Loss: 0.00001338
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001567
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001309
Iteration 113/1000 | Loss: 0.00001308
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00013229
Iteration 116/1000 | Loss: 0.00016982
Iteration 117/1000 | Loss: 0.00002666
Iteration 118/1000 | Loss: 0.00002141
Iteration 119/1000 | Loss: 0.00003582
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00003107
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00015974
Iteration 124/1000 | Loss: 0.00004817
Iteration 125/1000 | Loss: 0.00003465
Iteration 126/1000 | Loss: 0.00017430
Iteration 127/1000 | Loss: 0.00008471
Iteration 128/1000 | Loss: 0.00013096
Iteration 129/1000 | Loss: 0.00009343
Iteration 130/1000 | Loss: 0.00012693
Iteration 131/1000 | Loss: 0.00003501
Iteration 132/1000 | Loss: 0.00003525
Iteration 133/1000 | Loss: 0.00009814
Iteration 134/1000 | Loss: 0.00004428
Iteration 135/1000 | Loss: 0.00001673
Iteration 136/1000 | Loss: 0.00014346
Iteration 137/1000 | Loss: 0.00002301
Iteration 138/1000 | Loss: 0.00001706
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001964
Iteration 143/1000 | Loss: 0.00001330
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001174
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001173
Iteration 149/1000 | Loss: 0.00001173
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001172
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001172
Iteration 157/1000 | Loss: 0.00001172
Iteration 158/1000 | Loss: 0.00001172
Iteration 159/1000 | Loss: 0.00001171
Iteration 160/1000 | Loss: 0.00001171
Iteration 161/1000 | Loss: 0.00001171
Iteration 162/1000 | Loss: 0.00001171
Iteration 163/1000 | Loss: 0.00001171
Iteration 164/1000 | Loss: 0.00001170
Iteration 165/1000 | Loss: 0.00001170
Iteration 166/1000 | Loss: 0.00001170
Iteration 167/1000 | Loss: 0.00001202
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001164
Iteration 171/1000 | Loss: 0.00001164
Iteration 172/1000 | Loss: 0.00001164
Iteration 173/1000 | Loss: 0.00001163
Iteration 174/1000 | Loss: 0.00001163
Iteration 175/1000 | Loss: 0.00001163
Iteration 176/1000 | Loss: 0.00001163
Iteration 177/1000 | Loss: 0.00001163
Iteration 178/1000 | Loss: 0.00001163
Iteration 179/1000 | Loss: 0.00001163
Iteration 180/1000 | Loss: 0.00001163
Iteration 181/1000 | Loss: 0.00001163
Iteration 182/1000 | Loss: 0.00001163
Iteration 183/1000 | Loss: 0.00001163
Iteration 184/1000 | Loss: 0.00001163
Iteration 185/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.162985063274391e-05, 1.162985063274391e-05, 1.162985063274391e-05, 1.162985063274391e-05, 1.162985063274391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162985063274391e-05

Optimization complete. Final v2v error: 2.864616870880127 mm

Highest mean error: 5.463374614715576 mm for frame 224

Lowest mean error: 2.4447569847106934 mm for frame 233

Saving results

Total time: 237.58001852035522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604686
Iteration 2/25 | Loss: 0.00151315
Iteration 3/25 | Loss: 0.00120282
Iteration 4/25 | Loss: 0.00119166
Iteration 5/25 | Loss: 0.00118796
Iteration 6/25 | Loss: 0.00118702
Iteration 7/25 | Loss: 0.00118702
Iteration 8/25 | Loss: 0.00118702
Iteration 9/25 | Loss: 0.00118702
Iteration 10/25 | Loss: 0.00118702
Iteration 11/25 | Loss: 0.00118702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001187023939564824, 0.001187023939564824, 0.001187023939564824, 0.001187023939564824, 0.001187023939564824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001187023939564824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90558374
Iteration 2/25 | Loss: 0.00090956
Iteration 3/25 | Loss: 0.00090954
Iteration 4/25 | Loss: 0.00090954
Iteration 5/25 | Loss: 0.00090954
Iteration 6/25 | Loss: 0.00090954
Iteration 7/25 | Loss: 0.00090954
Iteration 8/25 | Loss: 0.00090954
Iteration 9/25 | Loss: 0.00090954
Iteration 10/25 | Loss: 0.00090954
Iteration 11/25 | Loss: 0.00090954
Iteration 12/25 | Loss: 0.00090954
Iteration 13/25 | Loss: 0.00090954
Iteration 14/25 | Loss: 0.00090954
Iteration 15/25 | Loss: 0.00090954
Iteration 16/25 | Loss: 0.00090954
Iteration 17/25 | Loss: 0.00090954
Iteration 18/25 | Loss: 0.00090954
Iteration 19/25 | Loss: 0.00090954
Iteration 20/25 | Loss: 0.00090954
Iteration 21/25 | Loss: 0.00090954
Iteration 22/25 | Loss: 0.00090954
Iteration 23/25 | Loss: 0.00090954
Iteration 24/25 | Loss: 0.00090954
Iteration 25/25 | Loss: 0.00090954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090954
Iteration 2/1000 | Loss: 0.00006089
Iteration 3/1000 | Loss: 0.00003805
Iteration 4/1000 | Loss: 0.00003338
Iteration 5/1000 | Loss: 0.00003163
Iteration 6/1000 | Loss: 0.00003069
Iteration 7/1000 | Loss: 0.00003014
Iteration 8/1000 | Loss: 0.00002954
Iteration 9/1000 | Loss: 0.00002917
Iteration 10/1000 | Loss: 0.00002883
Iteration 11/1000 | Loss: 0.00002855
Iteration 12/1000 | Loss: 0.00002824
Iteration 13/1000 | Loss: 0.00002797
Iteration 14/1000 | Loss: 0.00002771
Iteration 15/1000 | Loss: 0.00002752
Iteration 16/1000 | Loss: 0.00002733
Iteration 17/1000 | Loss: 0.00002718
Iteration 18/1000 | Loss: 0.00002715
Iteration 19/1000 | Loss: 0.00002706
Iteration 20/1000 | Loss: 0.00002698
Iteration 21/1000 | Loss: 0.00002698
Iteration 22/1000 | Loss: 0.00002696
Iteration 23/1000 | Loss: 0.00002694
Iteration 24/1000 | Loss: 0.00002693
Iteration 25/1000 | Loss: 0.00002689
Iteration 26/1000 | Loss: 0.00002688
Iteration 27/1000 | Loss: 0.00002687
Iteration 28/1000 | Loss: 0.00002686
Iteration 29/1000 | Loss: 0.00002686
Iteration 30/1000 | Loss: 0.00002685
Iteration 31/1000 | Loss: 0.00002685
Iteration 32/1000 | Loss: 0.00002685
Iteration 33/1000 | Loss: 0.00002684
Iteration 34/1000 | Loss: 0.00002684
Iteration 35/1000 | Loss: 0.00002684
Iteration 36/1000 | Loss: 0.00002684
Iteration 37/1000 | Loss: 0.00002684
Iteration 38/1000 | Loss: 0.00002683
Iteration 39/1000 | Loss: 0.00002683
Iteration 40/1000 | Loss: 0.00002682
Iteration 41/1000 | Loss: 0.00002681
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00002680
Iteration 44/1000 | Loss: 0.00002680
Iteration 45/1000 | Loss: 0.00002680
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002680
Iteration 48/1000 | Loss: 0.00002679
Iteration 49/1000 | Loss: 0.00002679
Iteration 50/1000 | Loss: 0.00002679
Iteration 51/1000 | Loss: 0.00002678
Iteration 52/1000 | Loss: 0.00002678
Iteration 53/1000 | Loss: 0.00002678
Iteration 54/1000 | Loss: 0.00002678
Iteration 55/1000 | Loss: 0.00002678
Iteration 56/1000 | Loss: 0.00002678
Iteration 57/1000 | Loss: 0.00002677
Iteration 58/1000 | Loss: 0.00002677
Iteration 59/1000 | Loss: 0.00002677
Iteration 60/1000 | Loss: 0.00002677
Iteration 61/1000 | Loss: 0.00002676
Iteration 62/1000 | Loss: 0.00002676
Iteration 63/1000 | Loss: 0.00002676
Iteration 64/1000 | Loss: 0.00002676
Iteration 65/1000 | Loss: 0.00002676
Iteration 66/1000 | Loss: 0.00002675
Iteration 67/1000 | Loss: 0.00002675
Iteration 68/1000 | Loss: 0.00002675
Iteration 69/1000 | Loss: 0.00002674
Iteration 70/1000 | Loss: 0.00002674
Iteration 71/1000 | Loss: 0.00002674
Iteration 72/1000 | Loss: 0.00002674
Iteration 73/1000 | Loss: 0.00002674
Iteration 74/1000 | Loss: 0.00002674
Iteration 75/1000 | Loss: 0.00002674
Iteration 76/1000 | Loss: 0.00002673
Iteration 77/1000 | Loss: 0.00002673
Iteration 78/1000 | Loss: 0.00002673
Iteration 79/1000 | Loss: 0.00002672
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002672
Iteration 82/1000 | Loss: 0.00002672
Iteration 83/1000 | Loss: 0.00002671
Iteration 84/1000 | Loss: 0.00002671
Iteration 85/1000 | Loss: 0.00002671
Iteration 86/1000 | Loss: 0.00002671
Iteration 87/1000 | Loss: 0.00002671
Iteration 88/1000 | Loss: 0.00002671
Iteration 89/1000 | Loss: 0.00002671
Iteration 90/1000 | Loss: 0.00002671
Iteration 91/1000 | Loss: 0.00002671
Iteration 92/1000 | Loss: 0.00002671
Iteration 93/1000 | Loss: 0.00002670
Iteration 94/1000 | Loss: 0.00002670
Iteration 95/1000 | Loss: 0.00002670
Iteration 96/1000 | Loss: 0.00002670
Iteration 97/1000 | Loss: 0.00002670
Iteration 98/1000 | Loss: 0.00002670
Iteration 99/1000 | Loss: 0.00002670
Iteration 100/1000 | Loss: 0.00002669
Iteration 101/1000 | Loss: 0.00002669
Iteration 102/1000 | Loss: 0.00002669
Iteration 103/1000 | Loss: 0.00002669
Iteration 104/1000 | Loss: 0.00002669
Iteration 105/1000 | Loss: 0.00002669
Iteration 106/1000 | Loss: 0.00002669
Iteration 107/1000 | Loss: 0.00002668
Iteration 108/1000 | Loss: 0.00002668
Iteration 109/1000 | Loss: 0.00002668
Iteration 110/1000 | Loss: 0.00002668
Iteration 111/1000 | Loss: 0.00002668
Iteration 112/1000 | Loss: 0.00002668
Iteration 113/1000 | Loss: 0.00002668
Iteration 114/1000 | Loss: 0.00002668
Iteration 115/1000 | Loss: 0.00002668
Iteration 116/1000 | Loss: 0.00002668
Iteration 117/1000 | Loss: 0.00002668
Iteration 118/1000 | Loss: 0.00002667
Iteration 119/1000 | Loss: 0.00002667
Iteration 120/1000 | Loss: 0.00002667
Iteration 121/1000 | Loss: 0.00002667
Iteration 122/1000 | Loss: 0.00002667
Iteration 123/1000 | Loss: 0.00002667
Iteration 124/1000 | Loss: 0.00002667
Iteration 125/1000 | Loss: 0.00002667
Iteration 126/1000 | Loss: 0.00002667
Iteration 127/1000 | Loss: 0.00002667
Iteration 128/1000 | Loss: 0.00002667
Iteration 129/1000 | Loss: 0.00002667
Iteration 130/1000 | Loss: 0.00002667
Iteration 131/1000 | Loss: 0.00002667
Iteration 132/1000 | Loss: 0.00002667
Iteration 133/1000 | Loss: 0.00002667
Iteration 134/1000 | Loss: 0.00002667
Iteration 135/1000 | Loss: 0.00002667
Iteration 136/1000 | Loss: 0.00002667
Iteration 137/1000 | Loss: 0.00002667
Iteration 138/1000 | Loss: 0.00002667
Iteration 139/1000 | Loss: 0.00002667
Iteration 140/1000 | Loss: 0.00002667
Iteration 141/1000 | Loss: 0.00002667
Iteration 142/1000 | Loss: 0.00002667
Iteration 143/1000 | Loss: 0.00002667
Iteration 144/1000 | Loss: 0.00002667
Iteration 145/1000 | Loss: 0.00002667
Iteration 146/1000 | Loss: 0.00002667
Iteration 147/1000 | Loss: 0.00002667
Iteration 148/1000 | Loss: 0.00002667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.667041371751111e-05, 2.667041371751111e-05, 2.667041371751111e-05, 2.667041371751111e-05, 2.667041371751111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.667041371751111e-05

Optimization complete. Final v2v error: 3.95332932472229 mm

Highest mean error: 4.833474636077881 mm for frame 97

Lowest mean error: 3.007723808288574 mm for frame 36

Saving results

Total time: 44.03706192970276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994725
Iteration 2/25 | Loss: 0.00994725
Iteration 3/25 | Loss: 0.00994725
Iteration 4/25 | Loss: 0.00306879
Iteration 5/25 | Loss: 0.00179072
Iteration 6/25 | Loss: 0.00146748
Iteration 7/25 | Loss: 0.00137920
Iteration 8/25 | Loss: 0.00137799
Iteration 9/25 | Loss: 0.00134666
Iteration 10/25 | Loss: 0.00128919
Iteration 11/25 | Loss: 0.00127212
Iteration 12/25 | Loss: 0.00125683
Iteration 13/25 | Loss: 0.00123309
Iteration 14/25 | Loss: 0.00122264
Iteration 15/25 | Loss: 0.00121989
Iteration 16/25 | Loss: 0.00122893
Iteration 17/25 | Loss: 0.00123046
Iteration 18/25 | Loss: 0.00121528
Iteration 19/25 | Loss: 0.00120886
Iteration 20/25 | Loss: 0.00121086
Iteration 21/25 | Loss: 0.00120666
Iteration 22/25 | Loss: 0.00120644
Iteration 23/25 | Loss: 0.00120558
Iteration 24/25 | Loss: 0.00120498
Iteration 25/25 | Loss: 0.00120527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35392094
Iteration 2/25 | Loss: 0.00191829
Iteration 3/25 | Loss: 0.00161801
Iteration 4/25 | Loss: 0.00161801
Iteration 5/25 | Loss: 0.00161801
Iteration 6/25 | Loss: 0.00161801
Iteration 7/25 | Loss: 0.00161801
Iteration 8/25 | Loss: 0.00161801
Iteration 9/25 | Loss: 0.00161801
Iteration 10/25 | Loss: 0.00161801
Iteration 11/25 | Loss: 0.00161801
Iteration 12/25 | Loss: 0.00161801
Iteration 13/25 | Loss: 0.00161801
Iteration 14/25 | Loss: 0.00161801
Iteration 15/25 | Loss: 0.00161801
Iteration 16/25 | Loss: 0.00161801
Iteration 17/25 | Loss: 0.00161801
Iteration 18/25 | Loss: 0.00161801
Iteration 19/25 | Loss: 0.00161801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016180051025003195, 0.0016180051025003195, 0.0016180051025003195, 0.0016180051025003195, 0.0016180051025003195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016180051025003195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161801
Iteration 2/1000 | Loss: 0.00044535
Iteration 3/1000 | Loss: 0.00076922
Iteration 4/1000 | Loss: 0.00055179
Iteration 5/1000 | Loss: 0.00033599
Iteration 6/1000 | Loss: 0.00027307
Iteration 7/1000 | Loss: 0.00034195
Iteration 8/1000 | Loss: 0.00027074
Iteration 9/1000 | Loss: 0.00010908
Iteration 10/1000 | Loss: 0.00010183
Iteration 11/1000 | Loss: 0.00015403
Iteration 12/1000 | Loss: 0.00010843
Iteration 13/1000 | Loss: 0.00007806
Iteration 14/1000 | Loss: 0.00007755
Iteration 15/1000 | Loss: 0.00023139
Iteration 16/1000 | Loss: 0.00016545
Iteration 17/1000 | Loss: 0.00014351
Iteration 18/1000 | Loss: 0.00051212
Iteration 19/1000 | Loss: 0.00027298
Iteration 20/1000 | Loss: 0.00090084
Iteration 21/1000 | Loss: 0.00164380
Iteration 22/1000 | Loss: 0.00114049
Iteration 23/1000 | Loss: 0.00114303
Iteration 24/1000 | Loss: 0.00199674
Iteration 25/1000 | Loss: 0.00021080
Iteration 26/1000 | Loss: 0.00019592
Iteration 27/1000 | Loss: 0.00009508
Iteration 28/1000 | Loss: 0.00023556
Iteration 29/1000 | Loss: 0.00007223
Iteration 30/1000 | Loss: 0.00005707
Iteration 31/1000 | Loss: 0.00012536
Iteration 32/1000 | Loss: 0.00011634
Iteration 33/1000 | Loss: 0.00005671
Iteration 34/1000 | Loss: 0.00003900
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00004846
Iteration 37/1000 | Loss: 0.00004538
Iteration 38/1000 | Loss: 0.00003772
Iteration 39/1000 | Loss: 0.00002697
Iteration 40/1000 | Loss: 0.00003514
Iteration 41/1000 | Loss: 0.00003880
Iteration 42/1000 | Loss: 0.00003903
Iteration 43/1000 | Loss: 0.00007356
Iteration 44/1000 | Loss: 0.00006564
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00003356
Iteration 47/1000 | Loss: 0.00002275
Iteration 48/1000 | Loss: 0.00002840
Iteration 49/1000 | Loss: 0.00002898
Iteration 50/1000 | Loss: 0.00003229
Iteration 51/1000 | Loss: 0.00003671
Iteration 52/1000 | Loss: 0.00003732
Iteration 53/1000 | Loss: 0.00003860
Iteration 54/1000 | Loss: 0.00002546
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001798
Iteration 57/1000 | Loss: 0.00002526
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00002699
Iteration 60/1000 | Loss: 0.00003046
Iteration 61/1000 | Loss: 0.00005043
Iteration 62/1000 | Loss: 0.00007755
Iteration 63/1000 | Loss: 0.00003012
Iteration 64/1000 | Loss: 0.00003244
Iteration 65/1000 | Loss: 0.00002308
Iteration 66/1000 | Loss: 0.00003782
Iteration 67/1000 | Loss: 0.00002974
Iteration 68/1000 | Loss: 0.00002401
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00005529
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00002910
Iteration 73/1000 | Loss: 0.00003063
Iteration 74/1000 | Loss: 0.00002618
Iteration 75/1000 | Loss: 0.00002270
Iteration 76/1000 | Loss: 0.00003508
Iteration 77/1000 | Loss: 0.00002797
Iteration 78/1000 | Loss: 0.00005884
Iteration 79/1000 | Loss: 0.00002106
Iteration 80/1000 | Loss: 0.00002831
Iteration 81/1000 | Loss: 0.00002892
Iteration 82/1000 | Loss: 0.00004765
Iteration 83/1000 | Loss: 0.00002680
Iteration 84/1000 | Loss: 0.00002567
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002755
Iteration 87/1000 | Loss: 0.00002754
Iteration 88/1000 | Loss: 0.00002751
Iteration 89/1000 | Loss: 0.00003807
Iteration 90/1000 | Loss: 0.00002815
Iteration 91/1000 | Loss: 0.00003419
Iteration 92/1000 | Loss: 0.00004556
Iteration 93/1000 | Loss: 0.00003347
Iteration 94/1000 | Loss: 0.00003385
Iteration 95/1000 | Loss: 0.00003608
Iteration 96/1000 | Loss: 0.00003709
Iteration 97/1000 | Loss: 0.00003307
Iteration 98/1000 | Loss: 0.00002434
Iteration 99/1000 | Loss: 0.00004764
Iteration 100/1000 | Loss: 0.00002456
Iteration 101/1000 | Loss: 0.00003120
Iteration 102/1000 | Loss: 0.00003174
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00003115
Iteration 105/1000 | Loss: 0.00004098
Iteration 106/1000 | Loss: 0.00004335
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001559
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001542
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001541
Iteration 123/1000 | Loss: 0.00001541
Iteration 124/1000 | Loss: 0.00001545
Iteration 125/1000 | Loss: 0.00004722
Iteration 126/1000 | Loss: 0.00004965
Iteration 127/1000 | Loss: 0.00003097
Iteration 128/1000 | Loss: 0.00001774
Iteration 129/1000 | Loss: 0.00001523
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00001715
Iteration 133/1000 | Loss: 0.00003399
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001514
Iteration 158/1000 | Loss: 0.00001513
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001701
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001510
Iteration 182/1000 | Loss: 0.00001510
Iteration 183/1000 | Loss: 0.00001510
Iteration 184/1000 | Loss: 0.00001510
Iteration 185/1000 | Loss: 0.00001510
Iteration 186/1000 | Loss: 0.00001550
Iteration 187/1000 | Loss: 0.00001509
Iteration 188/1000 | Loss: 0.00001509
Iteration 189/1000 | Loss: 0.00001509
Iteration 190/1000 | Loss: 0.00001509
Iteration 191/1000 | Loss: 0.00001509
Iteration 192/1000 | Loss: 0.00001509
Iteration 193/1000 | Loss: 0.00001509
Iteration 194/1000 | Loss: 0.00001509
Iteration 195/1000 | Loss: 0.00001509
Iteration 196/1000 | Loss: 0.00001508
Iteration 197/1000 | Loss: 0.00001508
Iteration 198/1000 | Loss: 0.00001508
Iteration 199/1000 | Loss: 0.00001508
Iteration 200/1000 | Loss: 0.00001508
Iteration 201/1000 | Loss: 0.00001507
Iteration 202/1000 | Loss: 0.00001507
Iteration 203/1000 | Loss: 0.00001521
Iteration 204/1000 | Loss: 0.00001521
Iteration 205/1000 | Loss: 0.00002566
Iteration 206/1000 | Loss: 0.00005421
Iteration 207/1000 | Loss: 0.00001539
Iteration 208/1000 | Loss: 0.00001502
Iteration 209/1000 | Loss: 0.00001502
Iteration 210/1000 | Loss: 0.00001502
Iteration 211/1000 | Loss: 0.00001502
Iteration 212/1000 | Loss: 0.00001502
Iteration 213/1000 | Loss: 0.00001502
Iteration 214/1000 | Loss: 0.00001502
Iteration 215/1000 | Loss: 0.00001502
Iteration 216/1000 | Loss: 0.00001525
Iteration 217/1000 | Loss: 0.00001502
Iteration 218/1000 | Loss: 0.00001502
Iteration 219/1000 | Loss: 0.00001502
Iteration 220/1000 | Loss: 0.00001502
Iteration 221/1000 | Loss: 0.00002361
Iteration 222/1000 | Loss: 0.00001502
Iteration 223/1000 | Loss: 0.00001502
Iteration 224/1000 | Loss: 0.00001502
Iteration 225/1000 | Loss: 0.00001502
Iteration 226/1000 | Loss: 0.00001502
Iteration 227/1000 | Loss: 0.00001502
Iteration 228/1000 | Loss: 0.00001502
Iteration 229/1000 | Loss: 0.00001502
Iteration 230/1000 | Loss: 0.00001502
Iteration 231/1000 | Loss: 0.00001502
Iteration 232/1000 | Loss: 0.00001502
Iteration 233/1000 | Loss: 0.00001501
Iteration 234/1000 | Loss: 0.00001501
Iteration 235/1000 | Loss: 0.00001501
Iteration 236/1000 | Loss: 0.00001501
Iteration 237/1000 | Loss: 0.00001501
Iteration 238/1000 | Loss: 0.00001501
Iteration 239/1000 | Loss: 0.00001501
Iteration 240/1000 | Loss: 0.00001501
Iteration 241/1000 | Loss: 0.00001501
Iteration 242/1000 | Loss: 0.00001501
Iteration 243/1000 | Loss: 0.00001501
Iteration 244/1000 | Loss: 0.00001501
Iteration 245/1000 | Loss: 0.00001501
Iteration 246/1000 | Loss: 0.00001501
Iteration 247/1000 | Loss: 0.00001501
Iteration 248/1000 | Loss: 0.00001501
Iteration 249/1000 | Loss: 0.00001500
Iteration 250/1000 | Loss: 0.00001500
Iteration 251/1000 | Loss: 0.00001500
Iteration 252/1000 | Loss: 0.00001500
Iteration 253/1000 | Loss: 0.00001500
Iteration 254/1000 | Loss: 0.00001500
Iteration 255/1000 | Loss: 0.00001500
Iteration 256/1000 | Loss: 0.00001500
Iteration 257/1000 | Loss: 0.00001500
Iteration 258/1000 | Loss: 0.00001500
Iteration 259/1000 | Loss: 0.00001500
Iteration 260/1000 | Loss: 0.00001500
Iteration 261/1000 | Loss: 0.00001500
Iteration 262/1000 | Loss: 0.00001500
Iteration 263/1000 | Loss: 0.00001500
Iteration 264/1000 | Loss: 0.00001500
Iteration 265/1000 | Loss: 0.00001500
Iteration 266/1000 | Loss: 0.00001500
Iteration 267/1000 | Loss: 0.00001500
Iteration 268/1000 | Loss: 0.00001500
Iteration 269/1000 | Loss: 0.00001500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.4998671758803539e-05, 1.4998671758803539e-05, 1.4998671758803539e-05, 1.4998671758803539e-05, 1.4998671758803539e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4998671758803539e-05

Optimization complete. Final v2v error: 3.0235581398010254 mm

Highest mean error: 5.28086519241333 mm for frame 230

Lowest mean error: 2.4947752952575684 mm for frame 135

Saving results

Total time: 245.47158002853394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590727
Iteration 2/25 | Loss: 0.00163176
Iteration 3/25 | Loss: 0.00123304
Iteration 4/25 | Loss: 0.00119622
Iteration 5/25 | Loss: 0.00119098
Iteration 6/25 | Loss: 0.00119040
Iteration 7/25 | Loss: 0.00119040
Iteration 8/25 | Loss: 0.00119040
Iteration 9/25 | Loss: 0.00119040
Iteration 10/25 | Loss: 0.00119040
Iteration 11/25 | Loss: 0.00119040
Iteration 12/25 | Loss: 0.00119040
Iteration 13/25 | Loss: 0.00119040
Iteration 14/25 | Loss: 0.00119040
Iteration 15/25 | Loss: 0.00119040
Iteration 16/25 | Loss: 0.00119040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011904046405106783, 0.0011904046405106783, 0.0011904046405106783, 0.0011904046405106783, 0.0011904046405106783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011904046405106783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35382640
Iteration 2/25 | Loss: 0.00066512
Iteration 3/25 | Loss: 0.00066510
Iteration 4/25 | Loss: 0.00066509
Iteration 5/25 | Loss: 0.00066509
Iteration 6/25 | Loss: 0.00066509
Iteration 7/25 | Loss: 0.00066509
Iteration 8/25 | Loss: 0.00066509
Iteration 9/25 | Loss: 0.00066509
Iteration 10/25 | Loss: 0.00066509
Iteration 11/25 | Loss: 0.00066509
Iteration 12/25 | Loss: 0.00066509
Iteration 13/25 | Loss: 0.00066509
Iteration 14/25 | Loss: 0.00066509
Iteration 15/25 | Loss: 0.00066509
Iteration 16/25 | Loss: 0.00066509
Iteration 17/25 | Loss: 0.00066509
Iteration 18/25 | Loss: 0.00066509
Iteration 19/25 | Loss: 0.00066509
Iteration 20/25 | Loss: 0.00066509
Iteration 21/25 | Loss: 0.00066509
Iteration 22/25 | Loss: 0.00066509
Iteration 23/25 | Loss: 0.00066509
Iteration 24/25 | Loss: 0.00066509
Iteration 25/25 | Loss: 0.00066509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006650926079601049, 0.0006650926079601049, 0.0006650926079601049, 0.0006650926079601049, 0.0006650926079601049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006650926079601049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066509
Iteration 2/1000 | Loss: 0.00004826
Iteration 3/1000 | Loss: 0.00002741
Iteration 4/1000 | Loss: 0.00002256
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001925
Iteration 8/1000 | Loss: 0.00001891
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001853
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001845
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00001840
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001821
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001814
Iteration 28/1000 | Loss: 0.00001814
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001809
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001808
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001800
Iteration 46/1000 | Loss: 0.00001799
Iteration 47/1000 | Loss: 0.00001798
Iteration 48/1000 | Loss: 0.00001797
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001796
Iteration 52/1000 | Loss: 0.00001796
Iteration 53/1000 | Loss: 0.00001796
Iteration 54/1000 | Loss: 0.00001795
Iteration 55/1000 | Loss: 0.00001795
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001794
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001791
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001790
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001789
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001787
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001785
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001779
Iteration 128/1000 | Loss: 0.00001779
Iteration 129/1000 | Loss: 0.00001779
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001779
Iteration 132/1000 | Loss: 0.00001779
Iteration 133/1000 | Loss: 0.00001779
Iteration 134/1000 | Loss: 0.00001779
Iteration 135/1000 | Loss: 0.00001778
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001777
Iteration 142/1000 | Loss: 0.00001777
Iteration 143/1000 | Loss: 0.00001777
Iteration 144/1000 | Loss: 0.00001777
Iteration 145/1000 | Loss: 0.00001777
Iteration 146/1000 | Loss: 0.00001777
Iteration 147/1000 | Loss: 0.00001777
Iteration 148/1000 | Loss: 0.00001777
Iteration 149/1000 | Loss: 0.00001777
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001777
Iteration 153/1000 | Loss: 0.00001777
Iteration 154/1000 | Loss: 0.00001776
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001776
Iteration 162/1000 | Loss: 0.00001776
Iteration 163/1000 | Loss: 0.00001775
Iteration 164/1000 | Loss: 0.00001775
Iteration 165/1000 | Loss: 0.00001775
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001775
Iteration 168/1000 | Loss: 0.00001775
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001775
Iteration 171/1000 | Loss: 0.00001774
Iteration 172/1000 | Loss: 0.00001774
Iteration 173/1000 | Loss: 0.00001774
Iteration 174/1000 | Loss: 0.00001774
Iteration 175/1000 | Loss: 0.00001774
Iteration 176/1000 | Loss: 0.00001774
Iteration 177/1000 | Loss: 0.00001774
Iteration 178/1000 | Loss: 0.00001774
Iteration 179/1000 | Loss: 0.00001774
Iteration 180/1000 | Loss: 0.00001774
Iteration 181/1000 | Loss: 0.00001774
Iteration 182/1000 | Loss: 0.00001773
Iteration 183/1000 | Loss: 0.00001773
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001773
Iteration 186/1000 | Loss: 0.00001773
Iteration 187/1000 | Loss: 0.00001773
Iteration 188/1000 | Loss: 0.00001773
Iteration 189/1000 | Loss: 0.00001773
Iteration 190/1000 | Loss: 0.00001773
Iteration 191/1000 | Loss: 0.00001773
Iteration 192/1000 | Loss: 0.00001773
Iteration 193/1000 | Loss: 0.00001773
Iteration 194/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.7732183550833724e-05, 1.7732183550833724e-05, 1.7732183550833724e-05, 1.7732183550833724e-05, 1.7732183550833724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7732183550833724e-05

Optimization complete. Final v2v error: 3.5883407592773438 mm

Highest mean error: 3.7154932022094727 mm for frame 83

Lowest mean error: 3.158029556274414 mm for frame 4

Saving results

Total time: 37.911460399627686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369806
Iteration 2/25 | Loss: 0.00104131
Iteration 3/25 | Loss: 0.00097630
Iteration 4/25 | Loss: 0.00096699
Iteration 5/25 | Loss: 0.00096381
Iteration 6/25 | Loss: 0.00096288
Iteration 7/25 | Loss: 0.00096288
Iteration 8/25 | Loss: 0.00096288
Iteration 9/25 | Loss: 0.00096288
Iteration 10/25 | Loss: 0.00096288
Iteration 11/25 | Loss: 0.00096288
Iteration 12/25 | Loss: 0.00096288
Iteration 13/25 | Loss: 0.00096288
Iteration 14/25 | Loss: 0.00096288
Iteration 15/25 | Loss: 0.00096288
Iteration 16/25 | Loss: 0.00096288
Iteration 17/25 | Loss: 0.00096288
Iteration 18/25 | Loss: 0.00096288
Iteration 19/25 | Loss: 0.00096288
Iteration 20/25 | Loss: 0.00096288
Iteration 21/25 | Loss: 0.00096288
Iteration 22/25 | Loss: 0.00096288
Iteration 23/25 | Loss: 0.00096288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009628800326026976, 0.0009628800326026976, 0.0009628800326026976, 0.0009628800326026976, 0.0009628800326026976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628800326026976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.95680761
Iteration 2/25 | Loss: 0.00079251
Iteration 3/25 | Loss: 0.00079251
Iteration 4/25 | Loss: 0.00079251
Iteration 5/25 | Loss: 0.00079251
Iteration 6/25 | Loss: 0.00079251
Iteration 7/25 | Loss: 0.00079251
Iteration 8/25 | Loss: 0.00079251
Iteration 9/25 | Loss: 0.00079251
Iteration 10/25 | Loss: 0.00079251
Iteration 11/25 | Loss: 0.00079251
Iteration 12/25 | Loss: 0.00079251
Iteration 13/25 | Loss: 0.00079251
Iteration 14/25 | Loss: 0.00079251
Iteration 15/25 | Loss: 0.00079251
Iteration 16/25 | Loss: 0.00079251
Iteration 17/25 | Loss: 0.00079251
Iteration 18/25 | Loss: 0.00079251
Iteration 19/25 | Loss: 0.00079251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007925064419396222, 0.0007925064419396222, 0.0007925064419396222, 0.0007925064419396222, 0.0007925064419396222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007925064419396222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079251
Iteration 2/1000 | Loss: 0.00001839
Iteration 3/1000 | Loss: 0.00001230
Iteration 4/1000 | Loss: 0.00001042
Iteration 5/1000 | Loss: 0.00000961
Iteration 6/1000 | Loss: 0.00000930
Iteration 7/1000 | Loss: 0.00000906
Iteration 8/1000 | Loss: 0.00000872
Iteration 9/1000 | Loss: 0.00000859
Iteration 10/1000 | Loss: 0.00000859
Iteration 11/1000 | Loss: 0.00000857
Iteration 12/1000 | Loss: 0.00000856
Iteration 13/1000 | Loss: 0.00000856
Iteration 14/1000 | Loss: 0.00000848
Iteration 15/1000 | Loss: 0.00000848
Iteration 16/1000 | Loss: 0.00000829
Iteration 17/1000 | Loss: 0.00000822
Iteration 18/1000 | Loss: 0.00000820
Iteration 19/1000 | Loss: 0.00000819
Iteration 20/1000 | Loss: 0.00000817
Iteration 21/1000 | Loss: 0.00000817
Iteration 22/1000 | Loss: 0.00000816
Iteration 23/1000 | Loss: 0.00000816
Iteration 24/1000 | Loss: 0.00000815
Iteration 25/1000 | Loss: 0.00000815
Iteration 26/1000 | Loss: 0.00000814
Iteration 27/1000 | Loss: 0.00000814
Iteration 28/1000 | Loss: 0.00000813
Iteration 29/1000 | Loss: 0.00000813
Iteration 30/1000 | Loss: 0.00000811
Iteration 31/1000 | Loss: 0.00000811
Iteration 32/1000 | Loss: 0.00000810
Iteration 33/1000 | Loss: 0.00000810
Iteration 34/1000 | Loss: 0.00000809
Iteration 35/1000 | Loss: 0.00000809
Iteration 36/1000 | Loss: 0.00000808
Iteration 37/1000 | Loss: 0.00000808
Iteration 38/1000 | Loss: 0.00000807
Iteration 39/1000 | Loss: 0.00000806
Iteration 40/1000 | Loss: 0.00000805
Iteration 41/1000 | Loss: 0.00000805
Iteration 42/1000 | Loss: 0.00000805
Iteration 43/1000 | Loss: 0.00000804
Iteration 44/1000 | Loss: 0.00000804
Iteration 45/1000 | Loss: 0.00000804
Iteration 46/1000 | Loss: 0.00000803
Iteration 47/1000 | Loss: 0.00000803
Iteration 48/1000 | Loss: 0.00000803
Iteration 49/1000 | Loss: 0.00000802
Iteration 50/1000 | Loss: 0.00000801
Iteration 51/1000 | Loss: 0.00000801
Iteration 52/1000 | Loss: 0.00000801
Iteration 53/1000 | Loss: 0.00000800
Iteration 54/1000 | Loss: 0.00000800
Iteration 55/1000 | Loss: 0.00000800
Iteration 56/1000 | Loss: 0.00000799
Iteration 57/1000 | Loss: 0.00000799
Iteration 58/1000 | Loss: 0.00000799
Iteration 59/1000 | Loss: 0.00000799
Iteration 60/1000 | Loss: 0.00000799
Iteration 61/1000 | Loss: 0.00000799
Iteration 62/1000 | Loss: 0.00000798
Iteration 63/1000 | Loss: 0.00000798
Iteration 64/1000 | Loss: 0.00000798
Iteration 65/1000 | Loss: 0.00000798
Iteration 66/1000 | Loss: 0.00000797
Iteration 67/1000 | Loss: 0.00000797
Iteration 68/1000 | Loss: 0.00000797
Iteration 69/1000 | Loss: 0.00000797
Iteration 70/1000 | Loss: 0.00000796
Iteration 71/1000 | Loss: 0.00000796
Iteration 72/1000 | Loss: 0.00000796
Iteration 73/1000 | Loss: 0.00000796
Iteration 74/1000 | Loss: 0.00000796
Iteration 75/1000 | Loss: 0.00000796
Iteration 76/1000 | Loss: 0.00000796
Iteration 77/1000 | Loss: 0.00000795
Iteration 78/1000 | Loss: 0.00000795
Iteration 79/1000 | Loss: 0.00000795
Iteration 80/1000 | Loss: 0.00000795
Iteration 81/1000 | Loss: 0.00000795
Iteration 82/1000 | Loss: 0.00000795
Iteration 83/1000 | Loss: 0.00000794
Iteration 84/1000 | Loss: 0.00000793
Iteration 85/1000 | Loss: 0.00000793
Iteration 86/1000 | Loss: 0.00000793
Iteration 87/1000 | Loss: 0.00000793
Iteration 88/1000 | Loss: 0.00000793
Iteration 89/1000 | Loss: 0.00000793
Iteration 90/1000 | Loss: 0.00000792
Iteration 91/1000 | Loss: 0.00000792
Iteration 92/1000 | Loss: 0.00000792
Iteration 93/1000 | Loss: 0.00000791
Iteration 94/1000 | Loss: 0.00000791
Iteration 95/1000 | Loss: 0.00000791
Iteration 96/1000 | Loss: 0.00000790
Iteration 97/1000 | Loss: 0.00000790
Iteration 98/1000 | Loss: 0.00000790
Iteration 99/1000 | Loss: 0.00000790
Iteration 100/1000 | Loss: 0.00000789
Iteration 101/1000 | Loss: 0.00000789
Iteration 102/1000 | Loss: 0.00000789
Iteration 103/1000 | Loss: 0.00000789
Iteration 104/1000 | Loss: 0.00000788
Iteration 105/1000 | Loss: 0.00000788
Iteration 106/1000 | Loss: 0.00000788
Iteration 107/1000 | Loss: 0.00000788
Iteration 108/1000 | Loss: 0.00000788
Iteration 109/1000 | Loss: 0.00000788
Iteration 110/1000 | Loss: 0.00000788
Iteration 111/1000 | Loss: 0.00000788
Iteration 112/1000 | Loss: 0.00000788
Iteration 113/1000 | Loss: 0.00000788
Iteration 114/1000 | Loss: 0.00000787
Iteration 115/1000 | Loss: 0.00000787
Iteration 116/1000 | Loss: 0.00000787
Iteration 117/1000 | Loss: 0.00000786
Iteration 118/1000 | Loss: 0.00000786
Iteration 119/1000 | Loss: 0.00000786
Iteration 120/1000 | Loss: 0.00000785
Iteration 121/1000 | Loss: 0.00000785
Iteration 122/1000 | Loss: 0.00000785
Iteration 123/1000 | Loss: 0.00000785
Iteration 124/1000 | Loss: 0.00000785
Iteration 125/1000 | Loss: 0.00000785
Iteration 126/1000 | Loss: 0.00000785
Iteration 127/1000 | Loss: 0.00000785
Iteration 128/1000 | Loss: 0.00000784
Iteration 129/1000 | Loss: 0.00000784
Iteration 130/1000 | Loss: 0.00000784
Iteration 131/1000 | Loss: 0.00000784
Iteration 132/1000 | Loss: 0.00000784
Iteration 133/1000 | Loss: 0.00000783
Iteration 134/1000 | Loss: 0.00000783
Iteration 135/1000 | Loss: 0.00000783
Iteration 136/1000 | Loss: 0.00000783
Iteration 137/1000 | Loss: 0.00000783
Iteration 138/1000 | Loss: 0.00000783
Iteration 139/1000 | Loss: 0.00000783
Iteration 140/1000 | Loss: 0.00000783
Iteration 141/1000 | Loss: 0.00000782
Iteration 142/1000 | Loss: 0.00000782
Iteration 143/1000 | Loss: 0.00000781
Iteration 144/1000 | Loss: 0.00000781
Iteration 145/1000 | Loss: 0.00000781
Iteration 146/1000 | Loss: 0.00000781
Iteration 147/1000 | Loss: 0.00000780
Iteration 148/1000 | Loss: 0.00000780
Iteration 149/1000 | Loss: 0.00000780
Iteration 150/1000 | Loss: 0.00000780
Iteration 151/1000 | Loss: 0.00000780
Iteration 152/1000 | Loss: 0.00000779
Iteration 153/1000 | Loss: 0.00000779
Iteration 154/1000 | Loss: 0.00000779
Iteration 155/1000 | Loss: 0.00000779
Iteration 156/1000 | Loss: 0.00000779
Iteration 157/1000 | Loss: 0.00000779
Iteration 158/1000 | Loss: 0.00000779
Iteration 159/1000 | Loss: 0.00000779
Iteration 160/1000 | Loss: 0.00000779
Iteration 161/1000 | Loss: 0.00000779
Iteration 162/1000 | Loss: 0.00000779
Iteration 163/1000 | Loss: 0.00000778
Iteration 164/1000 | Loss: 0.00000778
Iteration 165/1000 | Loss: 0.00000778
Iteration 166/1000 | Loss: 0.00000778
Iteration 167/1000 | Loss: 0.00000778
Iteration 168/1000 | Loss: 0.00000778
Iteration 169/1000 | Loss: 0.00000777
Iteration 170/1000 | Loss: 0.00000777
Iteration 171/1000 | Loss: 0.00000777
Iteration 172/1000 | Loss: 0.00000777
Iteration 173/1000 | Loss: 0.00000777
Iteration 174/1000 | Loss: 0.00000777
Iteration 175/1000 | Loss: 0.00000777
Iteration 176/1000 | Loss: 0.00000777
Iteration 177/1000 | Loss: 0.00000776
Iteration 178/1000 | Loss: 0.00000776
Iteration 179/1000 | Loss: 0.00000776
Iteration 180/1000 | Loss: 0.00000775
Iteration 181/1000 | Loss: 0.00000775
Iteration 182/1000 | Loss: 0.00000775
Iteration 183/1000 | Loss: 0.00000775
Iteration 184/1000 | Loss: 0.00000775
Iteration 185/1000 | Loss: 0.00000774
Iteration 186/1000 | Loss: 0.00000774
Iteration 187/1000 | Loss: 0.00000774
Iteration 188/1000 | Loss: 0.00000774
Iteration 189/1000 | Loss: 0.00000774
Iteration 190/1000 | Loss: 0.00000774
Iteration 191/1000 | Loss: 0.00000774
Iteration 192/1000 | Loss: 0.00000774
Iteration 193/1000 | Loss: 0.00000774
Iteration 194/1000 | Loss: 0.00000774
Iteration 195/1000 | Loss: 0.00000774
Iteration 196/1000 | Loss: 0.00000774
Iteration 197/1000 | Loss: 0.00000774
Iteration 198/1000 | Loss: 0.00000773
Iteration 199/1000 | Loss: 0.00000773
Iteration 200/1000 | Loss: 0.00000773
Iteration 201/1000 | Loss: 0.00000773
Iteration 202/1000 | Loss: 0.00000773
Iteration 203/1000 | Loss: 0.00000773
Iteration 204/1000 | Loss: 0.00000773
Iteration 205/1000 | Loss: 0.00000773
Iteration 206/1000 | Loss: 0.00000773
Iteration 207/1000 | Loss: 0.00000773
Iteration 208/1000 | Loss: 0.00000773
Iteration 209/1000 | Loss: 0.00000773
Iteration 210/1000 | Loss: 0.00000773
Iteration 211/1000 | Loss: 0.00000773
Iteration 212/1000 | Loss: 0.00000773
Iteration 213/1000 | Loss: 0.00000773
Iteration 214/1000 | Loss: 0.00000772
Iteration 215/1000 | Loss: 0.00000772
Iteration 216/1000 | Loss: 0.00000772
Iteration 217/1000 | Loss: 0.00000772
Iteration 218/1000 | Loss: 0.00000772
Iteration 219/1000 | Loss: 0.00000772
Iteration 220/1000 | Loss: 0.00000772
Iteration 221/1000 | Loss: 0.00000772
Iteration 222/1000 | Loss: 0.00000772
Iteration 223/1000 | Loss: 0.00000772
Iteration 224/1000 | Loss: 0.00000772
Iteration 225/1000 | Loss: 0.00000772
Iteration 226/1000 | Loss: 0.00000772
Iteration 227/1000 | Loss: 0.00000772
Iteration 228/1000 | Loss: 0.00000771
Iteration 229/1000 | Loss: 0.00000771
Iteration 230/1000 | Loss: 0.00000771
Iteration 231/1000 | Loss: 0.00000771
Iteration 232/1000 | Loss: 0.00000771
Iteration 233/1000 | Loss: 0.00000771
Iteration 234/1000 | Loss: 0.00000771
Iteration 235/1000 | Loss: 0.00000771
Iteration 236/1000 | Loss: 0.00000771
Iteration 237/1000 | Loss: 0.00000771
Iteration 238/1000 | Loss: 0.00000771
Iteration 239/1000 | Loss: 0.00000771
Iteration 240/1000 | Loss: 0.00000771
Iteration 241/1000 | Loss: 0.00000771
Iteration 242/1000 | Loss: 0.00000770
Iteration 243/1000 | Loss: 0.00000770
Iteration 244/1000 | Loss: 0.00000770
Iteration 245/1000 | Loss: 0.00000770
Iteration 246/1000 | Loss: 0.00000770
Iteration 247/1000 | Loss: 0.00000770
Iteration 248/1000 | Loss: 0.00000770
Iteration 249/1000 | Loss: 0.00000770
Iteration 250/1000 | Loss: 0.00000770
Iteration 251/1000 | Loss: 0.00000770
Iteration 252/1000 | Loss: 0.00000770
Iteration 253/1000 | Loss: 0.00000770
Iteration 254/1000 | Loss: 0.00000770
Iteration 255/1000 | Loss: 0.00000770
Iteration 256/1000 | Loss: 0.00000770
Iteration 257/1000 | Loss: 0.00000770
Iteration 258/1000 | Loss: 0.00000770
Iteration 259/1000 | Loss: 0.00000770
Iteration 260/1000 | Loss: 0.00000769
Iteration 261/1000 | Loss: 0.00000769
Iteration 262/1000 | Loss: 0.00000769
Iteration 263/1000 | Loss: 0.00000769
Iteration 264/1000 | Loss: 0.00000769
Iteration 265/1000 | Loss: 0.00000769
Iteration 266/1000 | Loss: 0.00000769
Iteration 267/1000 | Loss: 0.00000769
Iteration 268/1000 | Loss: 0.00000769
Iteration 269/1000 | Loss: 0.00000769
Iteration 270/1000 | Loss: 0.00000769
Iteration 271/1000 | Loss: 0.00000769
Iteration 272/1000 | Loss: 0.00000769
Iteration 273/1000 | Loss: 0.00000769
Iteration 274/1000 | Loss: 0.00000769
Iteration 275/1000 | Loss: 0.00000769
Iteration 276/1000 | Loss: 0.00000769
Iteration 277/1000 | Loss: 0.00000769
Iteration 278/1000 | Loss: 0.00000769
Iteration 279/1000 | Loss: 0.00000769
Iteration 280/1000 | Loss: 0.00000769
Iteration 281/1000 | Loss: 0.00000769
Iteration 282/1000 | Loss: 0.00000769
Iteration 283/1000 | Loss: 0.00000769
Iteration 284/1000 | Loss: 0.00000769
Iteration 285/1000 | Loss: 0.00000769
Iteration 286/1000 | Loss: 0.00000769
Iteration 287/1000 | Loss: 0.00000769
Iteration 288/1000 | Loss: 0.00000769
Iteration 289/1000 | Loss: 0.00000769
Iteration 290/1000 | Loss: 0.00000769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [7.691754944971763e-06, 7.691754944971763e-06, 7.691754944971763e-06, 7.691754944971763e-06, 7.691754944971763e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.691754944971763e-06

Optimization complete. Final v2v error: 2.399005889892578 mm

Highest mean error: 2.7026169300079346 mm for frame 96

Lowest mean error: 2.2430059909820557 mm for frame 27

Saving results

Total time: 43.35144853591919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495169
Iteration 2/25 | Loss: 0.00147036
Iteration 3/25 | Loss: 0.00125523
Iteration 4/25 | Loss: 0.00120985
Iteration 5/25 | Loss: 0.00120597
Iteration 6/25 | Loss: 0.00118739
Iteration 7/25 | Loss: 0.00117236
Iteration 8/25 | Loss: 0.00116120
Iteration 9/25 | Loss: 0.00115622
Iteration 10/25 | Loss: 0.00114014
Iteration 11/25 | Loss: 0.00112711
Iteration 12/25 | Loss: 0.00112534
Iteration 13/25 | Loss: 0.00112510
Iteration 14/25 | Loss: 0.00111509
Iteration 15/25 | Loss: 0.00111185
Iteration 16/25 | Loss: 0.00111617
Iteration 17/25 | Loss: 0.00111493
Iteration 18/25 | Loss: 0.00111103
Iteration 19/25 | Loss: 0.00110999
Iteration 20/25 | Loss: 0.00110973
Iteration 21/25 | Loss: 0.00110971
Iteration 22/25 | Loss: 0.00110971
Iteration 23/25 | Loss: 0.00110971
Iteration 24/25 | Loss: 0.00110971
Iteration 25/25 | Loss: 0.00110971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38022399
Iteration 2/25 | Loss: 0.00295358
Iteration 3/25 | Loss: 0.00295357
Iteration 4/25 | Loss: 0.00295357
Iteration 5/25 | Loss: 0.00295357
Iteration 6/25 | Loss: 0.00295357
Iteration 7/25 | Loss: 0.00295357
Iteration 8/25 | Loss: 0.00295357
Iteration 9/25 | Loss: 0.00295357
Iteration 10/25 | Loss: 0.00295357
Iteration 11/25 | Loss: 0.00295357
Iteration 12/25 | Loss: 0.00295357
Iteration 13/25 | Loss: 0.00295357
Iteration 14/25 | Loss: 0.00295357
Iteration 15/25 | Loss: 0.00295357
Iteration 16/25 | Loss: 0.00295357
Iteration 17/25 | Loss: 0.00295357
Iteration 18/25 | Loss: 0.00295357
Iteration 19/25 | Loss: 0.00295357
Iteration 20/25 | Loss: 0.00295357
Iteration 21/25 | Loss: 0.00295357
Iteration 22/25 | Loss: 0.00295357
Iteration 23/25 | Loss: 0.00295357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00295356591232121, 0.00295356591232121, 0.00295356591232121, 0.00295356591232121, 0.00295356591232121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00295356591232121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00295357
Iteration 2/1000 | Loss: 0.00020381
Iteration 3/1000 | Loss: 0.00012446
Iteration 4/1000 | Loss: 0.00009333
Iteration 5/1000 | Loss: 0.00008073
Iteration 6/1000 | Loss: 0.00007360
Iteration 7/1000 | Loss: 0.00006964
Iteration 8/1000 | Loss: 0.00006732
Iteration 9/1000 | Loss: 0.00006514
Iteration 10/1000 | Loss: 0.00006370
Iteration 11/1000 | Loss: 0.00006245
Iteration 12/1000 | Loss: 0.00006128
Iteration 13/1000 | Loss: 0.00006036
Iteration 14/1000 | Loss: 0.00005941
Iteration 15/1000 | Loss: 0.00028618
Iteration 16/1000 | Loss: 0.00006280
Iteration 17/1000 | Loss: 0.00006083
Iteration 18/1000 | Loss: 0.00005965
Iteration 19/1000 | Loss: 0.00005848
Iteration 20/1000 | Loss: 0.00005755
Iteration 21/1000 | Loss: 0.00032035
Iteration 22/1000 | Loss: 0.00035328
Iteration 23/1000 | Loss: 0.00053297
Iteration 24/1000 | Loss: 0.00008880
Iteration 25/1000 | Loss: 0.00007135
Iteration 26/1000 | Loss: 0.00006313
Iteration 27/1000 | Loss: 0.00005768
Iteration 28/1000 | Loss: 0.00005484
Iteration 29/1000 | Loss: 0.00005310
Iteration 30/1000 | Loss: 0.00005194
Iteration 31/1000 | Loss: 0.00005063
Iteration 32/1000 | Loss: 0.00005476
Iteration 33/1000 | Loss: 0.00004879
Iteration 34/1000 | Loss: 0.00004757
Iteration 35/1000 | Loss: 0.00004662
Iteration 36/1000 | Loss: 0.00004586
Iteration 37/1000 | Loss: 0.00029140
Iteration 38/1000 | Loss: 0.00017517
Iteration 39/1000 | Loss: 0.00029051
Iteration 40/1000 | Loss: 0.00019390
Iteration 41/1000 | Loss: 0.00005720
Iteration 42/1000 | Loss: 0.00025646
Iteration 43/1000 | Loss: 0.00005517
Iteration 44/1000 | Loss: 0.00005126
Iteration 45/1000 | Loss: 0.00004847
Iteration 46/1000 | Loss: 0.00004690
Iteration 47/1000 | Loss: 0.00004574
Iteration 48/1000 | Loss: 0.00004481
Iteration 49/1000 | Loss: 0.00004405
Iteration 50/1000 | Loss: 0.00004362
Iteration 51/1000 | Loss: 0.00004328
Iteration 52/1000 | Loss: 0.00004291
Iteration 53/1000 | Loss: 0.00004269
Iteration 54/1000 | Loss: 0.00004269
Iteration 55/1000 | Loss: 0.00004268
Iteration 56/1000 | Loss: 0.00004268
Iteration 57/1000 | Loss: 0.00004266
Iteration 58/1000 | Loss: 0.00004246
Iteration 59/1000 | Loss: 0.00004229
Iteration 60/1000 | Loss: 0.00004218
Iteration 61/1000 | Loss: 0.00004209
Iteration 62/1000 | Loss: 0.00004207
Iteration 63/1000 | Loss: 0.00004207
Iteration 64/1000 | Loss: 0.00004207
Iteration 65/1000 | Loss: 0.00004206
Iteration 66/1000 | Loss: 0.00004206
Iteration 67/1000 | Loss: 0.00004205
Iteration 68/1000 | Loss: 0.00004205
Iteration 69/1000 | Loss: 0.00004204
Iteration 70/1000 | Loss: 0.00004204
Iteration 71/1000 | Loss: 0.00004204
Iteration 72/1000 | Loss: 0.00004204
Iteration 73/1000 | Loss: 0.00004204
Iteration 74/1000 | Loss: 0.00004203
Iteration 75/1000 | Loss: 0.00004203
Iteration 76/1000 | Loss: 0.00004203
Iteration 77/1000 | Loss: 0.00004203
Iteration 78/1000 | Loss: 0.00004203
Iteration 79/1000 | Loss: 0.00004203
Iteration 80/1000 | Loss: 0.00004203
Iteration 81/1000 | Loss: 0.00004202
Iteration 82/1000 | Loss: 0.00004202
Iteration 83/1000 | Loss: 0.00004202
Iteration 84/1000 | Loss: 0.00004202
Iteration 85/1000 | Loss: 0.00004201
Iteration 86/1000 | Loss: 0.00004201
Iteration 87/1000 | Loss: 0.00004201
Iteration 88/1000 | Loss: 0.00004200
Iteration 89/1000 | Loss: 0.00004200
Iteration 90/1000 | Loss: 0.00004200
Iteration 91/1000 | Loss: 0.00004199
Iteration 92/1000 | Loss: 0.00004199
Iteration 93/1000 | Loss: 0.00004199
Iteration 94/1000 | Loss: 0.00004199
Iteration 95/1000 | Loss: 0.00004198
Iteration 96/1000 | Loss: 0.00004198
Iteration 97/1000 | Loss: 0.00004198
Iteration 98/1000 | Loss: 0.00004198
Iteration 99/1000 | Loss: 0.00004198
Iteration 100/1000 | Loss: 0.00004198
Iteration 101/1000 | Loss: 0.00004197
Iteration 102/1000 | Loss: 0.00004197
Iteration 103/1000 | Loss: 0.00004197
Iteration 104/1000 | Loss: 0.00004197
Iteration 105/1000 | Loss: 0.00004197
Iteration 106/1000 | Loss: 0.00004197
Iteration 107/1000 | Loss: 0.00004197
Iteration 108/1000 | Loss: 0.00004197
Iteration 109/1000 | Loss: 0.00004197
Iteration 110/1000 | Loss: 0.00004197
Iteration 111/1000 | Loss: 0.00004197
Iteration 112/1000 | Loss: 0.00004196
Iteration 113/1000 | Loss: 0.00004196
Iteration 114/1000 | Loss: 0.00004196
Iteration 115/1000 | Loss: 0.00004196
Iteration 116/1000 | Loss: 0.00004196
Iteration 117/1000 | Loss: 0.00004195
Iteration 118/1000 | Loss: 0.00004195
Iteration 119/1000 | Loss: 0.00004195
Iteration 120/1000 | Loss: 0.00004195
Iteration 121/1000 | Loss: 0.00004195
Iteration 122/1000 | Loss: 0.00004195
Iteration 123/1000 | Loss: 0.00004194
Iteration 124/1000 | Loss: 0.00004194
Iteration 125/1000 | Loss: 0.00004194
Iteration 126/1000 | Loss: 0.00004193
Iteration 127/1000 | Loss: 0.00004193
Iteration 128/1000 | Loss: 0.00004193
Iteration 129/1000 | Loss: 0.00004193
Iteration 130/1000 | Loss: 0.00004193
Iteration 131/1000 | Loss: 0.00004193
Iteration 132/1000 | Loss: 0.00004193
Iteration 133/1000 | Loss: 0.00004193
Iteration 134/1000 | Loss: 0.00004193
Iteration 135/1000 | Loss: 0.00004193
Iteration 136/1000 | Loss: 0.00004193
Iteration 137/1000 | Loss: 0.00004193
Iteration 138/1000 | Loss: 0.00004193
Iteration 139/1000 | Loss: 0.00004193
Iteration 140/1000 | Loss: 0.00004192
Iteration 141/1000 | Loss: 0.00004192
Iteration 142/1000 | Loss: 0.00004192
Iteration 143/1000 | Loss: 0.00004192
Iteration 144/1000 | Loss: 0.00004192
Iteration 145/1000 | Loss: 0.00004192
Iteration 146/1000 | Loss: 0.00004192
Iteration 147/1000 | Loss: 0.00004192
Iteration 148/1000 | Loss: 0.00004192
Iteration 149/1000 | Loss: 0.00004192
Iteration 150/1000 | Loss: 0.00004191
Iteration 151/1000 | Loss: 0.00004191
Iteration 152/1000 | Loss: 0.00004191
Iteration 153/1000 | Loss: 0.00004191
Iteration 154/1000 | Loss: 0.00004190
Iteration 155/1000 | Loss: 0.00004190
Iteration 156/1000 | Loss: 0.00004190
Iteration 157/1000 | Loss: 0.00004190
Iteration 158/1000 | Loss: 0.00004190
Iteration 159/1000 | Loss: 0.00004189
Iteration 160/1000 | Loss: 0.00004189
Iteration 161/1000 | Loss: 0.00004189
Iteration 162/1000 | Loss: 0.00004189
Iteration 163/1000 | Loss: 0.00004189
Iteration 164/1000 | Loss: 0.00004189
Iteration 165/1000 | Loss: 0.00004189
Iteration 166/1000 | Loss: 0.00004189
Iteration 167/1000 | Loss: 0.00004189
Iteration 168/1000 | Loss: 0.00004189
Iteration 169/1000 | Loss: 0.00004189
Iteration 170/1000 | Loss: 0.00004188
Iteration 171/1000 | Loss: 0.00004188
Iteration 172/1000 | Loss: 0.00004188
Iteration 173/1000 | Loss: 0.00004188
Iteration 174/1000 | Loss: 0.00004188
Iteration 175/1000 | Loss: 0.00004188
Iteration 176/1000 | Loss: 0.00004188
Iteration 177/1000 | Loss: 0.00004188
Iteration 178/1000 | Loss: 0.00004188
Iteration 179/1000 | Loss: 0.00004188
Iteration 180/1000 | Loss: 0.00004187
Iteration 181/1000 | Loss: 0.00004187
Iteration 182/1000 | Loss: 0.00004187
Iteration 183/1000 | Loss: 0.00004187
Iteration 184/1000 | Loss: 0.00004187
Iteration 185/1000 | Loss: 0.00004187
Iteration 186/1000 | Loss: 0.00004187
Iteration 187/1000 | Loss: 0.00004187
Iteration 188/1000 | Loss: 0.00004187
Iteration 189/1000 | Loss: 0.00004187
Iteration 190/1000 | Loss: 0.00004187
Iteration 191/1000 | Loss: 0.00004187
Iteration 192/1000 | Loss: 0.00004186
Iteration 193/1000 | Loss: 0.00004186
Iteration 194/1000 | Loss: 0.00004186
Iteration 195/1000 | Loss: 0.00004186
Iteration 196/1000 | Loss: 0.00004186
Iteration 197/1000 | Loss: 0.00004186
Iteration 198/1000 | Loss: 0.00004186
Iteration 199/1000 | Loss: 0.00004186
Iteration 200/1000 | Loss: 0.00004186
Iteration 201/1000 | Loss: 0.00004186
Iteration 202/1000 | Loss: 0.00004186
Iteration 203/1000 | Loss: 0.00004186
Iteration 204/1000 | Loss: 0.00004186
Iteration 205/1000 | Loss: 0.00004186
Iteration 206/1000 | Loss: 0.00004186
Iteration 207/1000 | Loss: 0.00004186
Iteration 208/1000 | Loss: 0.00004186
Iteration 209/1000 | Loss: 0.00004186
Iteration 210/1000 | Loss: 0.00004186
Iteration 211/1000 | Loss: 0.00004186
Iteration 212/1000 | Loss: 0.00004186
Iteration 213/1000 | Loss: 0.00004186
Iteration 214/1000 | Loss: 0.00004186
Iteration 215/1000 | Loss: 0.00004186
Iteration 216/1000 | Loss: 0.00004186
Iteration 217/1000 | Loss: 0.00004186
Iteration 218/1000 | Loss: 0.00004186
Iteration 219/1000 | Loss: 0.00004186
Iteration 220/1000 | Loss: 0.00004186
Iteration 221/1000 | Loss: 0.00004186
Iteration 222/1000 | Loss: 0.00004186
Iteration 223/1000 | Loss: 0.00004186
Iteration 224/1000 | Loss: 0.00004186
Iteration 225/1000 | Loss: 0.00004186
Iteration 226/1000 | Loss: 0.00004186
Iteration 227/1000 | Loss: 0.00004186
Iteration 228/1000 | Loss: 0.00004186
Iteration 229/1000 | Loss: 0.00004186
Iteration 230/1000 | Loss: 0.00004186
Iteration 231/1000 | Loss: 0.00004186
Iteration 232/1000 | Loss: 0.00004186
Iteration 233/1000 | Loss: 0.00004186
Iteration 234/1000 | Loss: 0.00004186
Iteration 235/1000 | Loss: 0.00004186
Iteration 236/1000 | Loss: 0.00004186
Iteration 237/1000 | Loss: 0.00004186
Iteration 238/1000 | Loss: 0.00004186
Iteration 239/1000 | Loss: 0.00004186
Iteration 240/1000 | Loss: 0.00004186
Iteration 241/1000 | Loss: 0.00004186
Iteration 242/1000 | Loss: 0.00004186
Iteration 243/1000 | Loss: 0.00004186
Iteration 244/1000 | Loss: 0.00004186
Iteration 245/1000 | Loss: 0.00004186
Iteration 246/1000 | Loss: 0.00004186
Iteration 247/1000 | Loss: 0.00004186
Iteration 248/1000 | Loss: 0.00004186
Iteration 249/1000 | Loss: 0.00004186
Iteration 250/1000 | Loss: 0.00004186
Iteration 251/1000 | Loss: 0.00004186
Iteration 252/1000 | Loss: 0.00004186
Iteration 253/1000 | Loss: 0.00004186
Iteration 254/1000 | Loss: 0.00004186
Iteration 255/1000 | Loss: 0.00004186
Iteration 256/1000 | Loss: 0.00004186
Iteration 257/1000 | Loss: 0.00004186
Iteration 258/1000 | Loss: 0.00004186
Iteration 259/1000 | Loss: 0.00004186
Iteration 260/1000 | Loss: 0.00004186
Iteration 261/1000 | Loss: 0.00004186
Iteration 262/1000 | Loss: 0.00004186
Iteration 263/1000 | Loss: 0.00004186
Iteration 264/1000 | Loss: 0.00004186
Iteration 265/1000 | Loss: 0.00004186
Iteration 266/1000 | Loss: 0.00004186
Iteration 267/1000 | Loss: 0.00004186
Iteration 268/1000 | Loss: 0.00004186
Iteration 269/1000 | Loss: 0.00004186
Iteration 270/1000 | Loss: 0.00004186
Iteration 271/1000 | Loss: 0.00004186
Iteration 272/1000 | Loss: 0.00004186
Iteration 273/1000 | Loss: 0.00004186
Iteration 274/1000 | Loss: 0.00004186
Iteration 275/1000 | Loss: 0.00004186
Iteration 276/1000 | Loss: 0.00004186
Iteration 277/1000 | Loss: 0.00004186
Iteration 278/1000 | Loss: 0.00004186
Iteration 279/1000 | Loss: 0.00004186
Iteration 280/1000 | Loss: 0.00004186
Iteration 281/1000 | Loss: 0.00004186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [4.1855400922941044e-05, 4.1855400922941044e-05, 4.1855400922941044e-05, 4.1855400922941044e-05, 4.1855400922941044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1855400922941044e-05

Optimization complete. Final v2v error: 3.9074370861053467 mm

Highest mean error: 11.509716033935547 mm for frame 107

Lowest mean error: 2.35798716545105 mm for frame 171

Saving results

Total time: 133.72977471351624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382928
Iteration 2/25 | Loss: 0.00110040
Iteration 3/25 | Loss: 0.00101197
Iteration 4/25 | Loss: 0.00100305
Iteration 5/25 | Loss: 0.00099841
Iteration 6/25 | Loss: 0.00099765
Iteration 7/25 | Loss: 0.00099765
Iteration 8/25 | Loss: 0.00099765
Iteration 9/25 | Loss: 0.00099765
Iteration 10/25 | Loss: 0.00099765
Iteration 11/25 | Loss: 0.00099765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009976524161174893, 0.0009976524161174893, 0.0009976524161174893, 0.0009976524161174893, 0.0009976524161174893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009976524161174893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50753808
Iteration 2/25 | Loss: 0.00061752
Iteration 3/25 | Loss: 0.00061750
Iteration 4/25 | Loss: 0.00061750
Iteration 5/25 | Loss: 0.00061750
Iteration 6/25 | Loss: 0.00061750
Iteration 7/25 | Loss: 0.00061750
Iteration 8/25 | Loss: 0.00061750
Iteration 9/25 | Loss: 0.00061750
Iteration 10/25 | Loss: 0.00061750
Iteration 11/25 | Loss: 0.00061750
Iteration 12/25 | Loss: 0.00061750
Iteration 13/25 | Loss: 0.00061750
Iteration 14/25 | Loss: 0.00061750
Iteration 15/25 | Loss: 0.00061750
Iteration 16/25 | Loss: 0.00061750
Iteration 17/25 | Loss: 0.00061750
Iteration 18/25 | Loss: 0.00061750
Iteration 19/25 | Loss: 0.00061750
Iteration 20/25 | Loss: 0.00061750
Iteration 21/25 | Loss: 0.00061750
Iteration 22/25 | Loss: 0.00061750
Iteration 23/25 | Loss: 0.00061750
Iteration 24/25 | Loss: 0.00061750
Iteration 25/25 | Loss: 0.00061750

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061750
Iteration 2/1000 | Loss: 0.00001995
Iteration 3/1000 | Loss: 0.00001283
Iteration 4/1000 | Loss: 0.00001155
Iteration 5/1000 | Loss: 0.00001104
Iteration 6/1000 | Loss: 0.00001044
Iteration 7/1000 | Loss: 0.00001020
Iteration 8/1000 | Loss: 0.00000996
Iteration 9/1000 | Loss: 0.00000973
Iteration 10/1000 | Loss: 0.00000973
Iteration 11/1000 | Loss: 0.00000970
Iteration 12/1000 | Loss: 0.00000963
Iteration 13/1000 | Loss: 0.00000962
Iteration 14/1000 | Loss: 0.00000961
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000946
Iteration 17/1000 | Loss: 0.00000937
Iteration 18/1000 | Loss: 0.00000936
Iteration 19/1000 | Loss: 0.00000935
Iteration 20/1000 | Loss: 0.00000935
Iteration 21/1000 | Loss: 0.00000934
Iteration 22/1000 | Loss: 0.00000934
Iteration 23/1000 | Loss: 0.00000933
Iteration 24/1000 | Loss: 0.00000933
Iteration 25/1000 | Loss: 0.00000933
Iteration 26/1000 | Loss: 0.00000932
Iteration 27/1000 | Loss: 0.00000932
Iteration 28/1000 | Loss: 0.00000932
Iteration 29/1000 | Loss: 0.00000932
Iteration 30/1000 | Loss: 0.00000932
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000932
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000931
Iteration 36/1000 | Loss: 0.00000931
Iteration 37/1000 | Loss: 0.00000931
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000930
Iteration 40/1000 | Loss: 0.00000930
Iteration 41/1000 | Loss: 0.00000929
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000928
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000928
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000927
Iteration 52/1000 | Loss: 0.00000926
Iteration 53/1000 | Loss: 0.00000926
Iteration 54/1000 | Loss: 0.00000925
Iteration 55/1000 | Loss: 0.00000925
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000924
Iteration 59/1000 | Loss: 0.00000924
Iteration 60/1000 | Loss: 0.00000924
Iteration 61/1000 | Loss: 0.00000924
Iteration 62/1000 | Loss: 0.00000924
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000923
Iteration 68/1000 | Loss: 0.00000923
Iteration 69/1000 | Loss: 0.00000923
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000923
Iteration 72/1000 | Loss: 0.00000923
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000922
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000922
Iteration 86/1000 | Loss: 0.00000922
Iteration 87/1000 | Loss: 0.00000922
Iteration 88/1000 | Loss: 0.00000922
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000921
Iteration 96/1000 | Loss: 0.00000921
Iteration 97/1000 | Loss: 0.00000921
Iteration 98/1000 | Loss: 0.00000921
Iteration 99/1000 | Loss: 0.00000921
Iteration 100/1000 | Loss: 0.00000921
Iteration 101/1000 | Loss: 0.00000921
Iteration 102/1000 | Loss: 0.00000921
Iteration 103/1000 | Loss: 0.00000921
Iteration 104/1000 | Loss: 0.00000921
Iteration 105/1000 | Loss: 0.00000921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [9.214358215103857e-06, 9.214358215103857e-06, 9.214358215103857e-06, 9.214358215103857e-06, 9.214358215103857e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.214358215103857e-06

Optimization complete. Final v2v error: 2.6231021881103516 mm

Highest mean error: 2.822122097015381 mm for frame 10

Lowest mean error: 2.467707872390747 mm for frame 101

Saving results

Total time: 35.50765824317932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fernanda_posed_010/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fernanda_posed_010/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556811
Iteration 2/25 | Loss: 0.00104613
Iteration 3/25 | Loss: 0.00097711
Iteration 4/25 | Loss: 0.00096862
Iteration 5/25 | Loss: 0.00096568
Iteration 6/25 | Loss: 0.00096499
Iteration 7/25 | Loss: 0.00096499
Iteration 8/25 | Loss: 0.00096499
Iteration 9/25 | Loss: 0.00096499
Iteration 10/25 | Loss: 0.00096499
Iteration 11/25 | Loss: 0.00096499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009649885469116271, 0.0009649885469116271, 0.0009649885469116271, 0.0009649885469116271, 0.0009649885469116271]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009649885469116271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74879766
Iteration 2/25 | Loss: 0.00072845
Iteration 3/25 | Loss: 0.00072844
Iteration 4/25 | Loss: 0.00072844
Iteration 5/25 | Loss: 0.00072844
Iteration 6/25 | Loss: 0.00072844
Iteration 7/25 | Loss: 0.00072844
Iteration 8/25 | Loss: 0.00072844
Iteration 9/25 | Loss: 0.00072844
Iteration 10/25 | Loss: 0.00072844
Iteration 11/25 | Loss: 0.00072844
Iteration 12/25 | Loss: 0.00072844
Iteration 13/25 | Loss: 0.00072844
Iteration 14/25 | Loss: 0.00072844
Iteration 15/25 | Loss: 0.00072844
Iteration 16/25 | Loss: 0.00072844
Iteration 17/25 | Loss: 0.00072844
Iteration 18/25 | Loss: 0.00072844
Iteration 19/25 | Loss: 0.00072844
Iteration 20/25 | Loss: 0.00072844
Iteration 21/25 | Loss: 0.00072844
Iteration 22/25 | Loss: 0.00072844
Iteration 23/25 | Loss: 0.00072844
Iteration 24/25 | Loss: 0.00072844
Iteration 25/25 | Loss: 0.00072844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072844
Iteration 2/1000 | Loss: 0.00001950
Iteration 3/1000 | Loss: 0.00001292
Iteration 4/1000 | Loss: 0.00001024
Iteration 5/1000 | Loss: 0.00000949
Iteration 6/1000 | Loss: 0.00000911
Iteration 7/1000 | Loss: 0.00000870
Iteration 8/1000 | Loss: 0.00000853
Iteration 9/1000 | Loss: 0.00000835
Iteration 10/1000 | Loss: 0.00000831
Iteration 11/1000 | Loss: 0.00000825
Iteration 12/1000 | Loss: 0.00000819
Iteration 13/1000 | Loss: 0.00000819
Iteration 14/1000 | Loss: 0.00000814
Iteration 15/1000 | Loss: 0.00000814
Iteration 16/1000 | Loss: 0.00000813
Iteration 17/1000 | Loss: 0.00000813
Iteration 18/1000 | Loss: 0.00000806
Iteration 19/1000 | Loss: 0.00000806
Iteration 20/1000 | Loss: 0.00000805
Iteration 21/1000 | Loss: 0.00000804
Iteration 22/1000 | Loss: 0.00000804
Iteration 23/1000 | Loss: 0.00000804
Iteration 24/1000 | Loss: 0.00000801
Iteration 25/1000 | Loss: 0.00000801
Iteration 26/1000 | Loss: 0.00000800
Iteration 27/1000 | Loss: 0.00000800
Iteration 28/1000 | Loss: 0.00000800
Iteration 29/1000 | Loss: 0.00000799
Iteration 30/1000 | Loss: 0.00000799
Iteration 31/1000 | Loss: 0.00000799
Iteration 32/1000 | Loss: 0.00000798
Iteration 33/1000 | Loss: 0.00000798
Iteration 34/1000 | Loss: 0.00000798
Iteration 35/1000 | Loss: 0.00000798
Iteration 36/1000 | Loss: 0.00000798
Iteration 37/1000 | Loss: 0.00000798
Iteration 38/1000 | Loss: 0.00000797
Iteration 39/1000 | Loss: 0.00000797
Iteration 40/1000 | Loss: 0.00000797
Iteration 41/1000 | Loss: 0.00000796
Iteration 42/1000 | Loss: 0.00000795
Iteration 43/1000 | Loss: 0.00000795
Iteration 44/1000 | Loss: 0.00000795
Iteration 45/1000 | Loss: 0.00000795
Iteration 46/1000 | Loss: 0.00000795
Iteration 47/1000 | Loss: 0.00000794
Iteration 48/1000 | Loss: 0.00000794
Iteration 49/1000 | Loss: 0.00000794
Iteration 50/1000 | Loss: 0.00000794
Iteration 51/1000 | Loss: 0.00000794
Iteration 52/1000 | Loss: 0.00000794
Iteration 53/1000 | Loss: 0.00000794
Iteration 54/1000 | Loss: 0.00000793
Iteration 55/1000 | Loss: 0.00000792
Iteration 56/1000 | Loss: 0.00000792
Iteration 57/1000 | Loss: 0.00000792
Iteration 58/1000 | Loss: 0.00000792
Iteration 59/1000 | Loss: 0.00000792
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000791
Iteration 63/1000 | Loss: 0.00000791
Iteration 64/1000 | Loss: 0.00000791
Iteration 65/1000 | Loss: 0.00000791
Iteration 66/1000 | Loss: 0.00000791
Iteration 67/1000 | Loss: 0.00000791
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000791
Iteration 70/1000 | Loss: 0.00000791
Iteration 71/1000 | Loss: 0.00000791
Iteration 72/1000 | Loss: 0.00000791
Iteration 73/1000 | Loss: 0.00000791
Iteration 74/1000 | Loss: 0.00000791
Iteration 75/1000 | Loss: 0.00000791
Iteration 76/1000 | Loss: 0.00000791
Iteration 77/1000 | Loss: 0.00000791
Iteration 78/1000 | Loss: 0.00000791
Iteration 79/1000 | Loss: 0.00000791
Iteration 80/1000 | Loss: 0.00000791
Iteration 81/1000 | Loss: 0.00000791
Iteration 82/1000 | Loss: 0.00000791
Iteration 83/1000 | Loss: 0.00000791
Iteration 84/1000 | Loss: 0.00000791
Iteration 85/1000 | Loss: 0.00000791
Iteration 86/1000 | Loss: 0.00000791
Iteration 87/1000 | Loss: 0.00000791
Iteration 88/1000 | Loss: 0.00000791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [7.910836757218931e-06, 7.910836757218931e-06, 7.910836757218931e-06, 7.910836757218931e-06, 7.910836757218931e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.910836757218931e-06

Optimization complete. Final v2v error: 2.4421544075012207 mm

Highest mean error: 2.8204216957092285 mm for frame 111

Lowest mean error: 2.2751989364624023 mm for frame 38

Saving results

Total time: 30.985044240951538
