Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=147, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8232-8287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873073
Iteration 2/25 | Loss: 0.00162836
Iteration 3/25 | Loss: 0.00133099
Iteration 4/25 | Loss: 0.00129219
Iteration 5/25 | Loss: 0.00128633
Iteration 6/25 | Loss: 0.00128530
Iteration 7/25 | Loss: 0.00128530
Iteration 8/25 | Loss: 0.00128530
Iteration 9/25 | Loss: 0.00128530
Iteration 10/25 | Loss: 0.00128530
Iteration 11/25 | Loss: 0.00128530
Iteration 12/25 | Loss: 0.00128530
Iteration 13/25 | Loss: 0.00128530
Iteration 14/25 | Loss: 0.00128530
Iteration 15/25 | Loss: 0.00128530
Iteration 16/25 | Loss: 0.00128530
Iteration 17/25 | Loss: 0.00128530
Iteration 18/25 | Loss: 0.00128530
Iteration 19/25 | Loss: 0.00128530
Iteration 20/25 | Loss: 0.00128530
Iteration 21/25 | Loss: 0.00128530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012853019870817661, 0.0012853019870817661, 0.0012853019870817661, 0.0012853019870817661, 0.0012853019870817661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012853019870817661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46992016
Iteration 2/25 | Loss: 0.00064305
Iteration 3/25 | Loss: 0.00064299
Iteration 4/25 | Loss: 0.00064299
Iteration 5/25 | Loss: 0.00064299
Iteration 6/25 | Loss: 0.00064299
Iteration 7/25 | Loss: 0.00064299
Iteration 8/25 | Loss: 0.00064299
Iteration 9/25 | Loss: 0.00064299
Iteration 10/25 | Loss: 0.00064299
Iteration 11/25 | Loss: 0.00064299
Iteration 12/25 | Loss: 0.00064299
Iteration 13/25 | Loss: 0.00064299
Iteration 14/25 | Loss: 0.00064299
Iteration 15/25 | Loss: 0.00064299
Iteration 16/25 | Loss: 0.00064299
Iteration 17/25 | Loss: 0.00064299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006429899949580431, 0.0006429899949580431, 0.0006429899949580431, 0.0006429899949580431, 0.0006429899949580431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006429899949580431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064299
Iteration 2/1000 | Loss: 0.00007313
Iteration 3/1000 | Loss: 0.00004052
Iteration 4/1000 | Loss: 0.00002989
Iteration 5/1000 | Loss: 0.00002708
Iteration 6/1000 | Loss: 0.00002544
Iteration 7/1000 | Loss: 0.00002429
Iteration 8/1000 | Loss: 0.00002349
Iteration 9/1000 | Loss: 0.00002285
Iteration 10/1000 | Loss: 0.00002226
Iteration 11/1000 | Loss: 0.00002180
Iteration 12/1000 | Loss: 0.00002151
Iteration 13/1000 | Loss: 0.00002135
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002112
Iteration 16/1000 | Loss: 0.00002108
Iteration 17/1000 | Loss: 0.00002106
Iteration 18/1000 | Loss: 0.00002106
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002097
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002094
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002093
Iteration 26/1000 | Loss: 0.00002093
Iteration 27/1000 | Loss: 0.00002093
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002092
Iteration 30/1000 | Loss: 0.00002091
Iteration 31/1000 | Loss: 0.00002091
Iteration 32/1000 | Loss: 0.00002090
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002089
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002089
Iteration 40/1000 | Loss: 0.00002089
Iteration 41/1000 | Loss: 0.00002089
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002088
Iteration 45/1000 | Loss: 0.00002088
Iteration 46/1000 | Loss: 0.00002087
Iteration 47/1000 | Loss: 0.00002087
Iteration 48/1000 | Loss: 0.00002087
Iteration 49/1000 | Loss: 0.00002087
Iteration 50/1000 | Loss: 0.00002087
Iteration 51/1000 | Loss: 0.00002087
Iteration 52/1000 | Loss: 0.00002086
Iteration 53/1000 | Loss: 0.00002086
Iteration 54/1000 | Loss: 0.00002086
Iteration 55/1000 | Loss: 0.00002085
Iteration 56/1000 | Loss: 0.00002085
Iteration 57/1000 | Loss: 0.00002085
Iteration 58/1000 | Loss: 0.00002085
Iteration 59/1000 | Loss: 0.00002085
Iteration 60/1000 | Loss: 0.00002085
Iteration 61/1000 | Loss: 0.00002085
Iteration 62/1000 | Loss: 0.00002085
Iteration 63/1000 | Loss: 0.00002085
Iteration 64/1000 | Loss: 0.00002085
Iteration 65/1000 | Loss: 0.00002084
Iteration 66/1000 | Loss: 0.00002084
Iteration 67/1000 | Loss: 0.00002084
Iteration 68/1000 | Loss: 0.00002084
Iteration 69/1000 | Loss: 0.00002084
Iteration 70/1000 | Loss: 0.00002084
Iteration 71/1000 | Loss: 0.00002084
Iteration 72/1000 | Loss: 0.00002084
Iteration 73/1000 | Loss: 0.00002083
Iteration 74/1000 | Loss: 0.00002083
Iteration 75/1000 | Loss: 0.00002083
Iteration 76/1000 | Loss: 0.00002083
Iteration 77/1000 | Loss: 0.00002083
Iteration 78/1000 | Loss: 0.00002083
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002082
Iteration 81/1000 | Loss: 0.00002082
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00002082
Iteration 84/1000 | Loss: 0.00002082
Iteration 85/1000 | Loss: 0.00002082
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002082
Iteration 88/1000 | Loss: 0.00002081
Iteration 89/1000 | Loss: 0.00002081
Iteration 90/1000 | Loss: 0.00002081
Iteration 91/1000 | Loss: 0.00002081
Iteration 92/1000 | Loss: 0.00002081
Iteration 93/1000 | Loss: 0.00002081
Iteration 94/1000 | Loss: 0.00002081
Iteration 95/1000 | Loss: 0.00002080
Iteration 96/1000 | Loss: 0.00002080
Iteration 97/1000 | Loss: 0.00002080
Iteration 98/1000 | Loss: 0.00002080
Iteration 99/1000 | Loss: 0.00002080
Iteration 100/1000 | Loss: 0.00002080
Iteration 101/1000 | Loss: 0.00002080
Iteration 102/1000 | Loss: 0.00002079
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002079
Iteration 105/1000 | Loss: 0.00002079
Iteration 106/1000 | Loss: 0.00002078
Iteration 107/1000 | Loss: 0.00002078
Iteration 108/1000 | Loss: 0.00002078
Iteration 109/1000 | Loss: 0.00002078
Iteration 110/1000 | Loss: 0.00002078
Iteration 111/1000 | Loss: 0.00002078
Iteration 112/1000 | Loss: 0.00002078
Iteration 113/1000 | Loss: 0.00002078
Iteration 114/1000 | Loss: 0.00002078
Iteration 115/1000 | Loss: 0.00002078
Iteration 116/1000 | Loss: 0.00002078
Iteration 117/1000 | Loss: 0.00002078
Iteration 118/1000 | Loss: 0.00002078
Iteration 119/1000 | Loss: 0.00002078
Iteration 120/1000 | Loss: 0.00002078
Iteration 121/1000 | Loss: 0.00002078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.0781955754500814e-05, 2.0781955754500814e-05, 2.0781955754500814e-05, 2.0781955754500814e-05, 2.0781955754500814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0781955754500814e-05

Optimization complete. Final v2v error: 4.020431041717529 mm

Highest mean error: 4.43596887588501 mm for frame 32

Lowest mean error: 3.6514697074890137 mm for frame 112

Saving results

Total time: 38.16865849494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909238
Iteration 2/25 | Loss: 0.00164258
Iteration 3/25 | Loss: 0.00127350
Iteration 4/25 | Loss: 0.00122603
Iteration 5/25 | Loss: 0.00120649
Iteration 6/25 | Loss: 0.00120885
Iteration 7/25 | Loss: 0.00118882
Iteration 8/25 | Loss: 0.00118428
Iteration 9/25 | Loss: 0.00118146
Iteration 10/25 | Loss: 0.00117901
Iteration 11/25 | Loss: 0.00117830
Iteration 12/25 | Loss: 0.00117799
Iteration 13/25 | Loss: 0.00117773
Iteration 14/25 | Loss: 0.00117751
Iteration 15/25 | Loss: 0.00117736
Iteration 16/25 | Loss: 0.00118006
Iteration 17/25 | Loss: 0.00117519
Iteration 18/25 | Loss: 0.00117431
Iteration 19/25 | Loss: 0.00117394
Iteration 20/25 | Loss: 0.00117387
Iteration 21/25 | Loss: 0.00117387
Iteration 22/25 | Loss: 0.00117387
Iteration 23/25 | Loss: 0.00117386
Iteration 24/25 | Loss: 0.00117386
Iteration 25/25 | Loss: 0.00117386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01251531
Iteration 2/25 | Loss: 0.00085985
Iteration 3/25 | Loss: 0.00085984
Iteration 4/25 | Loss: 0.00085984
Iteration 5/25 | Loss: 0.00085984
Iteration 6/25 | Loss: 0.00085984
Iteration 7/25 | Loss: 0.00085984
Iteration 8/25 | Loss: 0.00085984
Iteration 9/25 | Loss: 0.00085984
Iteration 10/25 | Loss: 0.00085984
Iteration 11/25 | Loss: 0.00085984
Iteration 12/25 | Loss: 0.00085984
Iteration 13/25 | Loss: 0.00085984
Iteration 14/25 | Loss: 0.00085984
Iteration 15/25 | Loss: 0.00085984
Iteration 16/25 | Loss: 0.00085984
Iteration 17/25 | Loss: 0.00085984
Iteration 18/25 | Loss: 0.00085984
Iteration 19/25 | Loss: 0.00085984
Iteration 20/25 | Loss: 0.00085984
Iteration 21/25 | Loss: 0.00085984
Iteration 22/25 | Loss: 0.00085984
Iteration 23/25 | Loss: 0.00085984
Iteration 24/25 | Loss: 0.00085984
Iteration 25/25 | Loss: 0.00085984

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085984
Iteration 2/1000 | Loss: 0.00003985
Iteration 3/1000 | Loss: 0.00002543
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002239
Iteration 6/1000 | Loss: 0.00002161
Iteration 7/1000 | Loss: 0.00002123
Iteration 8/1000 | Loss: 0.00002259
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002080
Iteration 11/1000 | Loss: 0.00002077
Iteration 12/1000 | Loss: 0.00002054
Iteration 13/1000 | Loss: 0.00039043
Iteration 14/1000 | Loss: 0.00002303
Iteration 15/1000 | Loss: 0.00002458
Iteration 16/1000 | Loss: 0.00001987
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00002144
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001918
Iteration 21/1000 | Loss: 0.00001872
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001871
Iteration 25/1000 | Loss: 0.00001870
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001867
Iteration 29/1000 | Loss: 0.00001866
Iteration 30/1000 | Loss: 0.00001860
Iteration 31/1000 | Loss: 0.00001860
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001854
Iteration 36/1000 | Loss: 0.00001851
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001849
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001847
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001844
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001842
Iteration 55/1000 | Loss: 0.00001842
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001841
Iteration 58/1000 | Loss: 0.00001841
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001840
Iteration 62/1000 | Loss: 0.00001840
Iteration 63/1000 | Loss: 0.00001840
Iteration 64/1000 | Loss: 0.00001840
Iteration 65/1000 | Loss: 0.00001839
Iteration 66/1000 | Loss: 0.00001839
Iteration 67/1000 | Loss: 0.00001839
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001839
Iteration 72/1000 | Loss: 0.00001838
Iteration 73/1000 | Loss: 0.00001838
Iteration 74/1000 | Loss: 0.00001838
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001838
Iteration 77/1000 | Loss: 0.00001838
Iteration 78/1000 | Loss: 0.00001838
Iteration 79/1000 | Loss: 0.00001838
Iteration 80/1000 | Loss: 0.00001837
Iteration 81/1000 | Loss: 0.00001837
Iteration 82/1000 | Loss: 0.00001837
Iteration 83/1000 | Loss: 0.00001836
Iteration 84/1000 | Loss: 0.00001836
Iteration 85/1000 | Loss: 0.00001836
Iteration 86/1000 | Loss: 0.00001836
Iteration 87/1000 | Loss: 0.00001836
Iteration 88/1000 | Loss: 0.00001836
Iteration 89/1000 | Loss: 0.00001836
Iteration 90/1000 | Loss: 0.00001836
Iteration 91/1000 | Loss: 0.00001835
Iteration 92/1000 | Loss: 0.00001835
Iteration 93/1000 | Loss: 0.00001835
Iteration 94/1000 | Loss: 0.00001835
Iteration 95/1000 | Loss: 0.00001835
Iteration 96/1000 | Loss: 0.00001835
Iteration 97/1000 | Loss: 0.00001835
Iteration 98/1000 | Loss: 0.00001834
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00001831
Iteration 102/1000 | Loss: 0.00001830
Iteration 103/1000 | Loss: 0.00001830
Iteration 104/1000 | Loss: 0.00001830
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001827
Iteration 119/1000 | Loss: 0.00001827
Iteration 120/1000 | Loss: 0.00001827
Iteration 121/1000 | Loss: 0.00001827
Iteration 122/1000 | Loss: 0.00001827
Iteration 123/1000 | Loss: 0.00001827
Iteration 124/1000 | Loss: 0.00001827
Iteration 125/1000 | Loss: 0.00001827
Iteration 126/1000 | Loss: 0.00001827
Iteration 127/1000 | Loss: 0.00001827
Iteration 128/1000 | Loss: 0.00001827
Iteration 129/1000 | Loss: 0.00001827
Iteration 130/1000 | Loss: 0.00001827
Iteration 131/1000 | Loss: 0.00001827
Iteration 132/1000 | Loss: 0.00001827
Iteration 133/1000 | Loss: 0.00001827
Iteration 134/1000 | Loss: 0.00001827
Iteration 135/1000 | Loss: 0.00001827
Iteration 136/1000 | Loss: 0.00001827
Iteration 137/1000 | Loss: 0.00001827
Iteration 138/1000 | Loss: 0.00001827
Iteration 139/1000 | Loss: 0.00001827
Iteration 140/1000 | Loss: 0.00001826
Iteration 141/1000 | Loss: 0.00001826
Iteration 142/1000 | Loss: 0.00001826
Iteration 143/1000 | Loss: 0.00001826
Iteration 144/1000 | Loss: 0.00001826
Iteration 145/1000 | Loss: 0.00001826
Iteration 146/1000 | Loss: 0.00001826
Iteration 147/1000 | Loss: 0.00001826
Iteration 148/1000 | Loss: 0.00001826
Iteration 149/1000 | Loss: 0.00001826
Iteration 150/1000 | Loss: 0.00001826
Iteration 151/1000 | Loss: 0.00001826
Iteration 152/1000 | Loss: 0.00001826
Iteration 153/1000 | Loss: 0.00001826
Iteration 154/1000 | Loss: 0.00001826
Iteration 155/1000 | Loss: 0.00001826
Iteration 156/1000 | Loss: 0.00001826
Iteration 157/1000 | Loss: 0.00001826
Iteration 158/1000 | Loss: 0.00001825
Iteration 159/1000 | Loss: 0.00001825
Iteration 160/1000 | Loss: 0.00001825
Iteration 161/1000 | Loss: 0.00001825
Iteration 162/1000 | Loss: 0.00001825
Iteration 163/1000 | Loss: 0.00001825
Iteration 164/1000 | Loss: 0.00001825
Iteration 165/1000 | Loss: 0.00001825
Iteration 166/1000 | Loss: 0.00001825
Iteration 167/1000 | Loss: 0.00001825
Iteration 168/1000 | Loss: 0.00001825
Iteration 169/1000 | Loss: 0.00001825
Iteration 170/1000 | Loss: 0.00001972
Iteration 171/1000 | Loss: 0.00001824
Iteration 172/1000 | Loss: 0.00001824
Iteration 173/1000 | Loss: 0.00001824
Iteration 174/1000 | Loss: 0.00001824
Iteration 175/1000 | Loss: 0.00001824
Iteration 176/1000 | Loss: 0.00001824
Iteration 177/1000 | Loss: 0.00001824
Iteration 178/1000 | Loss: 0.00001824
Iteration 179/1000 | Loss: 0.00001824
Iteration 180/1000 | Loss: 0.00001824
Iteration 181/1000 | Loss: 0.00001824
Iteration 182/1000 | Loss: 0.00001824
Iteration 183/1000 | Loss: 0.00001824
Iteration 184/1000 | Loss: 0.00001824
Iteration 185/1000 | Loss: 0.00001824
Iteration 186/1000 | Loss: 0.00001824
Iteration 187/1000 | Loss: 0.00001824
Iteration 188/1000 | Loss: 0.00001824
Iteration 189/1000 | Loss: 0.00001824
Iteration 190/1000 | Loss: 0.00001824
Iteration 191/1000 | Loss: 0.00001824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.8235945390188135e-05, 1.8235945390188135e-05, 1.8235945390188135e-05, 1.8235945390188135e-05, 1.8235945390188135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8235945390188135e-05

Optimization complete. Final v2v error: 3.661390781402588 mm

Highest mean error: 4.282003879547119 mm for frame 167

Lowest mean error: 3.2338740825653076 mm for frame 34

Saving results

Total time: 84.86687326431274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034766
Iteration 2/25 | Loss: 0.00191717
Iteration 3/25 | Loss: 0.00145937
Iteration 4/25 | Loss: 0.00131148
Iteration 5/25 | Loss: 0.00128524
Iteration 6/25 | Loss: 0.00128414
Iteration 7/25 | Loss: 0.00126550
Iteration 8/25 | Loss: 0.00125147
Iteration 9/25 | Loss: 0.00122537
Iteration 10/25 | Loss: 0.00121169
Iteration 11/25 | Loss: 0.00120323
Iteration 12/25 | Loss: 0.00119831
Iteration 13/25 | Loss: 0.00120098
Iteration 14/25 | Loss: 0.00121409
Iteration 15/25 | Loss: 0.00120321
Iteration 16/25 | Loss: 0.00119827
Iteration 17/25 | Loss: 0.00119202
Iteration 18/25 | Loss: 0.00118979
Iteration 19/25 | Loss: 0.00118953
Iteration 20/25 | Loss: 0.00118869
Iteration 21/25 | Loss: 0.00118843
Iteration 22/25 | Loss: 0.00118883
Iteration 23/25 | Loss: 0.00118882
Iteration 24/25 | Loss: 0.00118955
Iteration 25/25 | Loss: 0.00118924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49523759
Iteration 2/25 | Loss: 0.00082907
Iteration 3/25 | Loss: 0.00082907
Iteration 4/25 | Loss: 0.00082907
Iteration 5/25 | Loss: 0.00082907
Iteration 6/25 | Loss: 0.00082907
Iteration 7/25 | Loss: 0.00082907
Iteration 8/25 | Loss: 0.00082907
Iteration 9/25 | Loss: 0.00082907
Iteration 10/25 | Loss: 0.00082907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.000829070748295635, 0.000829070748295635, 0.000829070748295635, 0.000829070748295635, 0.000829070748295635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000829070748295635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082907
Iteration 2/1000 | Loss: 0.00034320
Iteration 3/1000 | Loss: 0.00010153
Iteration 4/1000 | Loss: 0.00008058
Iteration 5/1000 | Loss: 0.00007031
Iteration 6/1000 | Loss: 0.00006014
Iteration 7/1000 | Loss: 0.00007009
Iteration 8/1000 | Loss: 0.00005119
Iteration 9/1000 | Loss: 0.00006676
Iteration 10/1000 | Loss: 0.00006678
Iteration 11/1000 | Loss: 0.00005463
Iteration 12/1000 | Loss: 0.00027255
Iteration 13/1000 | Loss: 0.00205225
Iteration 14/1000 | Loss: 0.00065016
Iteration 15/1000 | Loss: 0.00007010
Iteration 16/1000 | Loss: 0.00005931
Iteration 17/1000 | Loss: 0.00003848
Iteration 18/1000 | Loss: 0.00004222
Iteration 19/1000 | Loss: 0.00003144
Iteration 20/1000 | Loss: 0.00004020
Iteration 21/1000 | Loss: 0.00003532
Iteration 22/1000 | Loss: 0.00002683
Iteration 23/1000 | Loss: 0.00002922
Iteration 24/1000 | Loss: 0.00002699
Iteration 25/1000 | Loss: 0.00003422
Iteration 26/1000 | Loss: 0.00003214
Iteration 27/1000 | Loss: 0.00003060
Iteration 28/1000 | Loss: 0.00003243
Iteration 29/1000 | Loss: 0.00004181
Iteration 30/1000 | Loss: 0.00003290
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002738
Iteration 33/1000 | Loss: 0.00003313
Iteration 34/1000 | Loss: 0.00003367
Iteration 35/1000 | Loss: 0.00003262
Iteration 36/1000 | Loss: 0.00002738
Iteration 37/1000 | Loss: 0.00002672
Iteration 38/1000 | Loss: 0.00002761
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002731
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002931
Iteration 43/1000 | Loss: 0.00002426
Iteration 44/1000 | Loss: 0.00003083
Iteration 45/1000 | Loss: 0.00002377
Iteration 46/1000 | Loss: 0.00003190
Iteration 47/1000 | Loss: 0.00003260
Iteration 48/1000 | Loss: 0.00003042
Iteration 49/1000 | Loss: 0.00002583
Iteration 50/1000 | Loss: 0.00003285
Iteration 51/1000 | Loss: 0.00002889
Iteration 52/1000 | Loss: 0.00003269
Iteration 53/1000 | Loss: 0.00002887
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00002842
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001769
Iteration 71/1000 | Loss: 0.00001769
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001766
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001760
Iteration 82/1000 | Loss: 0.00001760
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001757
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001756
Iteration 96/1000 | Loss: 0.00001756
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001754
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001753
Iteration 124/1000 | Loss: 0.00001753
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00001753
Iteration 127/1000 | Loss: 0.00001753
Iteration 128/1000 | Loss: 0.00001753
Iteration 129/1000 | Loss: 0.00001753
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001753
Iteration 132/1000 | Loss: 0.00001753
Iteration 133/1000 | Loss: 0.00001753
Iteration 134/1000 | Loss: 0.00001753
Iteration 135/1000 | Loss: 0.00001753
Iteration 136/1000 | Loss: 0.00001753
Iteration 137/1000 | Loss: 0.00001753
Iteration 138/1000 | Loss: 0.00001753
Iteration 139/1000 | Loss: 0.00001753
Iteration 140/1000 | Loss: 0.00001753
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00001753
Iteration 143/1000 | Loss: 0.00001753
Iteration 144/1000 | Loss: 0.00001753
Iteration 145/1000 | Loss: 0.00001753
Iteration 146/1000 | Loss: 0.00001753
Iteration 147/1000 | Loss: 0.00001753
Iteration 148/1000 | Loss: 0.00001753
Iteration 149/1000 | Loss: 0.00001753
Iteration 150/1000 | Loss: 0.00001753
Iteration 151/1000 | Loss: 0.00001753
Iteration 152/1000 | Loss: 0.00001753
Iteration 153/1000 | Loss: 0.00001753
Iteration 154/1000 | Loss: 0.00001753
Iteration 155/1000 | Loss: 0.00001753
Iteration 156/1000 | Loss: 0.00001753
Iteration 157/1000 | Loss: 0.00001753
Iteration 158/1000 | Loss: 0.00001753
Iteration 159/1000 | Loss: 0.00001753
Iteration 160/1000 | Loss: 0.00001753
Iteration 161/1000 | Loss: 0.00001753
Iteration 162/1000 | Loss: 0.00001753
Iteration 163/1000 | Loss: 0.00001753
Iteration 164/1000 | Loss: 0.00001753
Iteration 165/1000 | Loss: 0.00001753
Iteration 166/1000 | Loss: 0.00001753
Iteration 167/1000 | Loss: 0.00001753
Iteration 168/1000 | Loss: 0.00001753
Iteration 169/1000 | Loss: 0.00001753
Iteration 170/1000 | Loss: 0.00001753
Iteration 171/1000 | Loss: 0.00001753
Iteration 172/1000 | Loss: 0.00001753
Iteration 173/1000 | Loss: 0.00001753
Iteration 174/1000 | Loss: 0.00001753
Iteration 175/1000 | Loss: 0.00001753
Iteration 176/1000 | Loss: 0.00001753
Iteration 177/1000 | Loss: 0.00001753
Iteration 178/1000 | Loss: 0.00001753
Iteration 179/1000 | Loss: 0.00001753
Iteration 180/1000 | Loss: 0.00001753
Iteration 181/1000 | Loss: 0.00001753
Iteration 182/1000 | Loss: 0.00001753
Iteration 183/1000 | Loss: 0.00001753
Iteration 184/1000 | Loss: 0.00001753
Iteration 185/1000 | Loss: 0.00001753
Iteration 186/1000 | Loss: 0.00001753
Iteration 187/1000 | Loss: 0.00001753
Iteration 188/1000 | Loss: 0.00001753
Iteration 189/1000 | Loss: 0.00001753
Iteration 190/1000 | Loss: 0.00001753
Iteration 191/1000 | Loss: 0.00001753
Iteration 192/1000 | Loss: 0.00001753
Iteration 193/1000 | Loss: 0.00001753
Iteration 194/1000 | Loss: 0.00001753
Iteration 195/1000 | Loss: 0.00001753
Iteration 196/1000 | Loss: 0.00001753
Iteration 197/1000 | Loss: 0.00001753
Iteration 198/1000 | Loss: 0.00001753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.7534081052872352e-05, 1.7534081052872352e-05, 1.7534081052872352e-05, 1.7534081052872352e-05, 1.7534081052872352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7534081052872352e-05

Optimization complete. Final v2v error: 3.515777826309204 mm

Highest mean error: 8.82796573638916 mm for frame 55

Lowest mean error: 2.9520158767700195 mm for frame 0

Saving results

Total time: 160.27344751358032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908663
Iteration 2/25 | Loss: 0.00169572
Iteration 3/25 | Loss: 0.00130916
Iteration 4/25 | Loss: 0.00126579
Iteration 5/25 | Loss: 0.00125800
Iteration 6/25 | Loss: 0.00125671
Iteration 7/25 | Loss: 0.00125671
Iteration 8/25 | Loss: 0.00125671
Iteration 9/25 | Loss: 0.00125671
Iteration 10/25 | Loss: 0.00125671
Iteration 11/25 | Loss: 0.00125671
Iteration 12/25 | Loss: 0.00125671
Iteration 13/25 | Loss: 0.00125671
Iteration 14/25 | Loss: 0.00125671
Iteration 15/25 | Loss: 0.00125671
Iteration 16/25 | Loss: 0.00125671
Iteration 17/25 | Loss: 0.00125671
Iteration 18/25 | Loss: 0.00125671
Iteration 19/25 | Loss: 0.00125671
Iteration 20/25 | Loss: 0.00125671
Iteration 21/25 | Loss: 0.00125671
Iteration 22/25 | Loss: 0.00125671
Iteration 23/25 | Loss: 0.00125671
Iteration 24/25 | Loss: 0.00125671
Iteration 25/25 | Loss: 0.00125671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38733566
Iteration 2/25 | Loss: 0.00073727
Iteration 3/25 | Loss: 0.00073727
Iteration 4/25 | Loss: 0.00073727
Iteration 5/25 | Loss: 0.00073727
Iteration 6/25 | Loss: 0.00073727
Iteration 7/25 | Loss: 0.00073727
Iteration 8/25 | Loss: 0.00073727
Iteration 9/25 | Loss: 0.00073727
Iteration 10/25 | Loss: 0.00073727
Iteration 11/25 | Loss: 0.00073727
Iteration 12/25 | Loss: 0.00073727
Iteration 13/25 | Loss: 0.00073727
Iteration 14/25 | Loss: 0.00073727
Iteration 15/25 | Loss: 0.00073727
Iteration 16/25 | Loss: 0.00073727
Iteration 17/25 | Loss: 0.00073727
Iteration 18/25 | Loss: 0.00073727
Iteration 19/25 | Loss: 0.00073727
Iteration 20/25 | Loss: 0.00073727
Iteration 21/25 | Loss: 0.00073727
Iteration 22/25 | Loss: 0.00073727
Iteration 23/25 | Loss: 0.00073727
Iteration 24/25 | Loss: 0.00073727
Iteration 25/25 | Loss: 0.00073727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073727
Iteration 2/1000 | Loss: 0.00005393
Iteration 3/1000 | Loss: 0.00003473
Iteration 4/1000 | Loss: 0.00002971
Iteration 5/1000 | Loss: 0.00002758
Iteration 6/1000 | Loss: 0.00002596
Iteration 7/1000 | Loss: 0.00002489
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00002360
Iteration 10/1000 | Loss: 0.00002326
Iteration 11/1000 | Loss: 0.00002280
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002209
Iteration 14/1000 | Loss: 0.00002207
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002169
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002157
Iteration 20/1000 | Loss: 0.00002157
Iteration 21/1000 | Loss: 0.00002157
Iteration 22/1000 | Loss: 0.00002157
Iteration 23/1000 | Loss: 0.00002156
Iteration 24/1000 | Loss: 0.00002156
Iteration 25/1000 | Loss: 0.00002156
Iteration 26/1000 | Loss: 0.00002156
Iteration 27/1000 | Loss: 0.00002155
Iteration 28/1000 | Loss: 0.00002154
Iteration 29/1000 | Loss: 0.00002154
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00002153
Iteration 32/1000 | Loss: 0.00002153
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00002153
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002153
Iteration 39/1000 | Loss: 0.00002153
Iteration 40/1000 | Loss: 0.00002153
Iteration 41/1000 | Loss: 0.00002153
Iteration 42/1000 | Loss: 0.00002153
Iteration 43/1000 | Loss: 0.00002152
Iteration 44/1000 | Loss: 0.00002152
Iteration 45/1000 | Loss: 0.00002152
Iteration 46/1000 | Loss: 0.00002152
Iteration 47/1000 | Loss: 0.00002152
Iteration 48/1000 | Loss: 0.00002152
Iteration 49/1000 | Loss: 0.00002151
Iteration 50/1000 | Loss: 0.00002151
Iteration 51/1000 | Loss: 0.00002151
Iteration 52/1000 | Loss: 0.00002150
Iteration 53/1000 | Loss: 0.00002150
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002149
Iteration 61/1000 | Loss: 0.00002149
Iteration 62/1000 | Loss: 0.00002149
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00002149
Iteration 65/1000 | Loss: 0.00002149
Iteration 66/1000 | Loss: 0.00002149
Iteration 67/1000 | Loss: 0.00002149
Iteration 68/1000 | Loss: 0.00002148
Iteration 69/1000 | Loss: 0.00002148
Iteration 70/1000 | Loss: 0.00002148
Iteration 71/1000 | Loss: 0.00002148
Iteration 72/1000 | Loss: 0.00002148
Iteration 73/1000 | Loss: 0.00002148
Iteration 74/1000 | Loss: 0.00002148
Iteration 75/1000 | Loss: 0.00002148
Iteration 76/1000 | Loss: 0.00002147
Iteration 77/1000 | Loss: 0.00002147
Iteration 78/1000 | Loss: 0.00002147
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002147
Iteration 81/1000 | Loss: 0.00002146
Iteration 82/1000 | Loss: 0.00002146
Iteration 83/1000 | Loss: 0.00002146
Iteration 84/1000 | Loss: 0.00002146
Iteration 85/1000 | Loss: 0.00002146
Iteration 86/1000 | Loss: 0.00002146
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002145
Iteration 89/1000 | Loss: 0.00002145
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002143
Iteration 98/1000 | Loss: 0.00002143
Iteration 99/1000 | Loss: 0.00002143
Iteration 100/1000 | Loss: 0.00002143
Iteration 101/1000 | Loss: 0.00002143
Iteration 102/1000 | Loss: 0.00002143
Iteration 103/1000 | Loss: 0.00002143
Iteration 104/1000 | Loss: 0.00002143
Iteration 105/1000 | Loss: 0.00002143
Iteration 106/1000 | Loss: 0.00002143
Iteration 107/1000 | Loss: 0.00002143
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00002143
Iteration 114/1000 | Loss: 0.00002143
Iteration 115/1000 | Loss: 0.00002143
Iteration 116/1000 | Loss: 0.00002143
Iteration 117/1000 | Loss: 0.00002143
Iteration 118/1000 | Loss: 0.00002143
Iteration 119/1000 | Loss: 0.00002143
Iteration 120/1000 | Loss: 0.00002143
Iteration 121/1000 | Loss: 0.00002143
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002143
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.14279571082443e-05, 2.14279571082443e-05, 2.14279571082443e-05, 2.14279571082443e-05, 2.14279571082443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.14279571082443e-05

Optimization complete. Final v2v error: 3.9701857566833496 mm

Highest mean error: 4.203066825866699 mm for frame 176

Lowest mean error: 3.582016706466675 mm for frame 37

Saving results

Total time: 43.50089716911316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803740
Iteration 2/25 | Loss: 0.00209981
Iteration 3/25 | Loss: 0.00138060
Iteration 4/25 | Loss: 0.00127544
Iteration 5/25 | Loss: 0.00125264
Iteration 6/25 | Loss: 0.00123854
Iteration 7/25 | Loss: 0.00122527
Iteration 8/25 | Loss: 0.00122218
Iteration 9/25 | Loss: 0.00122137
Iteration 10/25 | Loss: 0.00122108
Iteration 11/25 | Loss: 0.00122093
Iteration 12/25 | Loss: 0.00122091
Iteration 13/25 | Loss: 0.00122091
Iteration 14/25 | Loss: 0.00122091
Iteration 15/25 | Loss: 0.00122091
Iteration 16/25 | Loss: 0.00122091
Iteration 17/25 | Loss: 0.00122091
Iteration 18/25 | Loss: 0.00122091
Iteration 19/25 | Loss: 0.00122091
Iteration 20/25 | Loss: 0.00122091
Iteration 21/25 | Loss: 0.00122091
Iteration 22/25 | Loss: 0.00122091
Iteration 23/25 | Loss: 0.00122091
Iteration 24/25 | Loss: 0.00122091
Iteration 25/25 | Loss: 0.00122091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17600799
Iteration 2/25 | Loss: 0.00069770
Iteration 3/25 | Loss: 0.00069766
Iteration 4/25 | Loss: 0.00069766
Iteration 5/25 | Loss: 0.00069766
Iteration 6/25 | Loss: 0.00069766
Iteration 7/25 | Loss: 0.00069766
Iteration 8/25 | Loss: 0.00069766
Iteration 9/25 | Loss: 0.00069766
Iteration 10/25 | Loss: 0.00069766
Iteration 11/25 | Loss: 0.00069766
Iteration 12/25 | Loss: 0.00069766
Iteration 13/25 | Loss: 0.00069766
Iteration 14/25 | Loss: 0.00069766
Iteration 15/25 | Loss: 0.00069766
Iteration 16/25 | Loss: 0.00069766
Iteration 17/25 | Loss: 0.00069766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006976556032896042, 0.0006976556032896042, 0.0006976556032896042, 0.0006976556032896042, 0.0006976556032896042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006976556032896042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069766
Iteration 2/1000 | Loss: 0.00004995
Iteration 3/1000 | Loss: 0.00003100
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002011
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001905
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001822
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001790
Iteration 16/1000 | Loss: 0.00001789
Iteration 17/1000 | Loss: 0.00001787
Iteration 18/1000 | Loss: 0.00001787
Iteration 19/1000 | Loss: 0.00001786
Iteration 20/1000 | Loss: 0.00001785
Iteration 21/1000 | Loss: 0.00001783
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001780
Iteration 29/1000 | Loss: 0.00001780
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001779
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00001779
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001779
Iteration 40/1000 | Loss: 0.00001779
Iteration 41/1000 | Loss: 0.00001779
Iteration 42/1000 | Loss: 0.00001779
Iteration 43/1000 | Loss: 0.00001779
Iteration 44/1000 | Loss: 0.00001779
Iteration 45/1000 | Loss: 0.00001778
Iteration 46/1000 | Loss: 0.00001778
Iteration 47/1000 | Loss: 0.00001778
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001777
Iteration 50/1000 | Loss: 0.00001777
Iteration 51/1000 | Loss: 0.00001777
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001775
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001774
Iteration 63/1000 | Loss: 0.00001774
Iteration 64/1000 | Loss: 0.00001774
Iteration 65/1000 | Loss: 0.00001774
Iteration 66/1000 | Loss: 0.00001774
Iteration 67/1000 | Loss: 0.00001774
Iteration 68/1000 | Loss: 0.00001774
Iteration 69/1000 | Loss: 0.00001774
Iteration 70/1000 | Loss: 0.00001774
Iteration 71/1000 | Loss: 0.00001773
Iteration 72/1000 | Loss: 0.00001773
Iteration 73/1000 | Loss: 0.00001773
Iteration 74/1000 | Loss: 0.00001773
Iteration 75/1000 | Loss: 0.00001773
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001773
Iteration 78/1000 | Loss: 0.00001773
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001772
Iteration 84/1000 | Loss: 0.00001772
Iteration 85/1000 | Loss: 0.00001772
Iteration 86/1000 | Loss: 0.00001772
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001771
Iteration 89/1000 | Loss: 0.00001771
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001771
Iteration 93/1000 | Loss: 0.00001771
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001770
Iteration 100/1000 | Loss: 0.00001770
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001769
Iteration 104/1000 | Loss: 0.00001769
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001767
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001766
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001765
Iteration 127/1000 | Loss: 0.00001765
Iteration 128/1000 | Loss: 0.00001765
Iteration 129/1000 | Loss: 0.00001765
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001764
Iteration 134/1000 | Loss: 0.00001764
Iteration 135/1000 | Loss: 0.00001764
Iteration 136/1000 | Loss: 0.00001764
Iteration 137/1000 | Loss: 0.00001764
Iteration 138/1000 | Loss: 0.00001764
Iteration 139/1000 | Loss: 0.00001764
Iteration 140/1000 | Loss: 0.00001764
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001764
Iteration 152/1000 | Loss: 0.00001764
Iteration 153/1000 | Loss: 0.00001764
Iteration 154/1000 | Loss: 0.00001764
Iteration 155/1000 | Loss: 0.00001764
Iteration 156/1000 | Loss: 0.00001764
Iteration 157/1000 | Loss: 0.00001764
Iteration 158/1000 | Loss: 0.00001764
Iteration 159/1000 | Loss: 0.00001764
Iteration 160/1000 | Loss: 0.00001764
Iteration 161/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05, 1.7638813005760312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7638813005760312e-05

Optimization complete. Final v2v error: 3.6000380516052246 mm

Highest mean error: 3.920466661453247 mm for frame 162

Lowest mean error: 3.128598213195801 mm for frame 229

Saving results

Total time: 51.814818382263184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820033
Iteration 2/25 | Loss: 0.00180442
Iteration 3/25 | Loss: 0.00131192
Iteration 4/25 | Loss: 0.00123632
Iteration 5/25 | Loss: 0.00121258
Iteration 6/25 | Loss: 0.00117680
Iteration 7/25 | Loss: 0.00117900
Iteration 8/25 | Loss: 0.00117452
Iteration 9/25 | Loss: 0.00116409
Iteration 10/25 | Loss: 0.00115687
Iteration 11/25 | Loss: 0.00115248
Iteration 12/25 | Loss: 0.00115122
Iteration 13/25 | Loss: 0.00114902
Iteration 14/25 | Loss: 0.00115010
Iteration 15/25 | Loss: 0.00115090
Iteration 16/25 | Loss: 0.00115049
Iteration 17/25 | Loss: 0.00114964
Iteration 18/25 | Loss: 0.00114927
Iteration 19/25 | Loss: 0.00114956
Iteration 20/25 | Loss: 0.00114899
Iteration 21/25 | Loss: 0.00114915
Iteration 22/25 | Loss: 0.00114871
Iteration 23/25 | Loss: 0.00115018
Iteration 24/25 | Loss: 0.00114896
Iteration 25/25 | Loss: 0.00115012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21704626
Iteration 2/25 | Loss: 0.00058908
Iteration 3/25 | Loss: 0.00058908
Iteration 4/25 | Loss: 0.00058908
Iteration 5/25 | Loss: 0.00058908
Iteration 6/25 | Loss: 0.00058908
Iteration 7/25 | Loss: 0.00058908
Iteration 8/25 | Loss: 0.00058908
Iteration 9/25 | Loss: 0.00058908
Iteration 10/25 | Loss: 0.00058908
Iteration 11/25 | Loss: 0.00058908
Iteration 12/25 | Loss: 0.00058908
Iteration 13/25 | Loss: 0.00058908
Iteration 14/25 | Loss: 0.00058908
Iteration 15/25 | Loss: 0.00058908
Iteration 16/25 | Loss: 0.00058908
Iteration 17/25 | Loss: 0.00058908
Iteration 18/25 | Loss: 0.00058908
Iteration 19/25 | Loss: 0.00058908
Iteration 20/25 | Loss: 0.00058908
Iteration 21/25 | Loss: 0.00058908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005890819593332708, 0.0005890819593332708, 0.0005890819593332708, 0.0005890819593332708, 0.0005890819593332708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005890819593332708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058908
Iteration 2/1000 | Loss: 0.00004902
Iteration 3/1000 | Loss: 0.00004390
Iteration 4/1000 | Loss: 0.00003844
Iteration 5/1000 | Loss: 0.00002438
Iteration 6/1000 | Loss: 0.00003441
Iteration 7/1000 | Loss: 0.00003379
Iteration 8/1000 | Loss: 0.00007922
Iteration 9/1000 | Loss: 0.00004824
Iteration 10/1000 | Loss: 0.00024355
Iteration 11/1000 | Loss: 0.00004517
Iteration 12/1000 | Loss: 0.00003547
Iteration 13/1000 | Loss: 0.00004204
Iteration 14/1000 | Loss: 0.00003498
Iteration 15/1000 | Loss: 0.00004027
Iteration 16/1000 | Loss: 0.00003456
Iteration 17/1000 | Loss: 0.00003740
Iteration 18/1000 | Loss: 0.00007090
Iteration 19/1000 | Loss: 0.00003244
Iteration 20/1000 | Loss: 0.00003095
Iteration 21/1000 | Loss: 0.00005719
Iteration 22/1000 | Loss: 0.00004496
Iteration 23/1000 | Loss: 0.00003176
Iteration 24/1000 | Loss: 0.00003024
Iteration 25/1000 | Loss: 0.00003761
Iteration 26/1000 | Loss: 0.00003568
Iteration 27/1000 | Loss: 0.00006035
Iteration 28/1000 | Loss: 0.00003607
Iteration 29/1000 | Loss: 0.00003339
Iteration 30/1000 | Loss: 0.00003072
Iteration 31/1000 | Loss: 0.00005645
Iteration 32/1000 | Loss: 0.00003468
Iteration 33/1000 | Loss: 0.00003479
Iteration 34/1000 | Loss: 0.00003217
Iteration 35/1000 | Loss: 0.00005468
Iteration 36/1000 | Loss: 0.00003475
Iteration 37/1000 | Loss: 0.00003635
Iteration 38/1000 | Loss: 0.00004598
Iteration 39/1000 | Loss: 0.00003802
Iteration 40/1000 | Loss: 0.00004352
Iteration 41/1000 | Loss: 0.00003551
Iteration 42/1000 | Loss: 0.00003331
Iteration 43/1000 | Loss: 0.00003870
Iteration 44/1000 | Loss: 0.00003279
Iteration 45/1000 | Loss: 0.00002936
Iteration 46/1000 | Loss: 0.00003603
Iteration 47/1000 | Loss: 0.00003730
Iteration 48/1000 | Loss: 0.00003325
Iteration 49/1000 | Loss: 0.00003188
Iteration 50/1000 | Loss: 0.00003135
Iteration 51/1000 | Loss: 0.00002486
Iteration 52/1000 | Loss: 0.00002446
Iteration 53/1000 | Loss: 0.00003607
Iteration 54/1000 | Loss: 0.00005295
Iteration 55/1000 | Loss: 0.00003129
Iteration 56/1000 | Loss: 0.00003035
Iteration 57/1000 | Loss: 0.00003011
Iteration 58/1000 | Loss: 0.00004472
Iteration 59/1000 | Loss: 0.00003890
Iteration 60/1000 | Loss: 0.00002784
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00005767
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001910
Iteration 68/1000 | Loss: 0.00001910
Iteration 69/1000 | Loss: 0.00001909
Iteration 70/1000 | Loss: 0.00001909
Iteration 71/1000 | Loss: 0.00001909
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001908
Iteration 75/1000 | Loss: 0.00001908
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001906
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001905
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001904
Iteration 101/1000 | Loss: 0.00001904
Iteration 102/1000 | Loss: 0.00001904
Iteration 103/1000 | Loss: 0.00001904
Iteration 104/1000 | Loss: 0.00001904
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001904
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001902
Iteration 122/1000 | Loss: 0.00001902
Iteration 123/1000 | Loss: 0.00001902
Iteration 124/1000 | Loss: 0.00001902
Iteration 125/1000 | Loss: 0.00001902
Iteration 126/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.9023389540961944e-05, 1.9023389540961944e-05, 1.9023389540961944e-05, 1.9023389540961944e-05, 1.9023389540961944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9023389540961944e-05

Optimization complete. Final v2v error: 3.754335641860962 mm

Highest mean error: 4.436202049255371 mm for frame 130

Lowest mean error: 3.4458065032958984 mm for frame 75

Saving results

Total time: 146.40408754348755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406961
Iteration 2/25 | Loss: 0.00118052
Iteration 3/25 | Loss: 0.00109266
Iteration 4/25 | Loss: 0.00108595
Iteration 5/25 | Loss: 0.00108386
Iteration 6/25 | Loss: 0.00108386
Iteration 7/25 | Loss: 0.00108386
Iteration 8/25 | Loss: 0.00108386
Iteration 9/25 | Loss: 0.00108386
Iteration 10/25 | Loss: 0.00108386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010838634334504604, 0.0010838634334504604, 0.0010838634334504604, 0.0010838634334504604, 0.0010838634334504604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010838634334504604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45228779
Iteration 2/25 | Loss: 0.00046683
Iteration 3/25 | Loss: 0.00046683
Iteration 4/25 | Loss: 0.00046683
Iteration 5/25 | Loss: 0.00046683
Iteration 6/25 | Loss: 0.00046683
Iteration 7/25 | Loss: 0.00046683
Iteration 8/25 | Loss: 0.00046683
Iteration 9/25 | Loss: 0.00046683
Iteration 10/25 | Loss: 0.00046683
Iteration 11/25 | Loss: 0.00046683
Iteration 12/25 | Loss: 0.00046683
Iteration 13/25 | Loss: 0.00046683
Iteration 14/25 | Loss: 0.00046683
Iteration 15/25 | Loss: 0.00046683
Iteration 16/25 | Loss: 0.00046683
Iteration 17/25 | Loss: 0.00046683
Iteration 18/25 | Loss: 0.00046683
Iteration 19/25 | Loss: 0.00046683
Iteration 20/25 | Loss: 0.00046683
Iteration 21/25 | Loss: 0.00046683
Iteration 22/25 | Loss: 0.00046683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00046682701213285327, 0.00046682701213285327, 0.00046682701213285327, 0.00046682701213285327, 0.00046682701213285327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046682701213285327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046683
Iteration 2/1000 | Loss: 0.00002217
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001447
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001276
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001243
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001240
Iteration 13/1000 | Loss: 0.00001240
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001231
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001224
Iteration 60/1000 | Loss: 0.00001223
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001223
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001223
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001223
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.2231413165864069e-05, 1.2231413165864069e-05, 1.2231413165864069e-05, 1.2231413165864069e-05, 1.2231413165864069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2231413165864069e-05

Optimization complete. Final v2v error: 3.022716999053955 mm

Highest mean error: 3.2500274181365967 mm for frame 212

Lowest mean error: 2.7451236248016357 mm for frame 266

Saving results

Total time: 26.618845462799072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475505
Iteration 2/25 | Loss: 0.00119732
Iteration 3/25 | Loss: 0.00111269
Iteration 4/25 | Loss: 0.00110121
Iteration 5/25 | Loss: 0.00109748
Iteration 6/25 | Loss: 0.00109658
Iteration 7/25 | Loss: 0.00109637
Iteration 8/25 | Loss: 0.00109637
Iteration 9/25 | Loss: 0.00109637
Iteration 10/25 | Loss: 0.00109637
Iteration 11/25 | Loss: 0.00109637
Iteration 12/25 | Loss: 0.00109637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010963688837364316, 0.0010963688837364316, 0.0010963688837364316, 0.0010963688837364316, 0.0010963688837364316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010963688837364316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59874749
Iteration 2/25 | Loss: 0.00052835
Iteration 3/25 | Loss: 0.00052834
Iteration 4/25 | Loss: 0.00052834
Iteration 5/25 | Loss: 0.00052834
Iteration 6/25 | Loss: 0.00052834
Iteration 7/25 | Loss: 0.00052834
Iteration 8/25 | Loss: 0.00052834
Iteration 9/25 | Loss: 0.00052834
Iteration 10/25 | Loss: 0.00052834
Iteration 11/25 | Loss: 0.00052834
Iteration 12/25 | Loss: 0.00052834
Iteration 13/25 | Loss: 0.00052834
Iteration 14/25 | Loss: 0.00052834
Iteration 15/25 | Loss: 0.00052834
Iteration 16/25 | Loss: 0.00052834
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005283406935632229, 0.0005283406935632229, 0.0005283406935632229, 0.0005283406935632229, 0.0005283406935632229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005283406935632229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052834
Iteration 2/1000 | Loss: 0.00003051
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001549
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001446
Iteration 8/1000 | Loss: 0.00001429
Iteration 9/1000 | Loss: 0.00001424
Iteration 10/1000 | Loss: 0.00001416
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001406
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001403
Iteration 18/1000 | Loss: 0.00001401
Iteration 19/1000 | Loss: 0.00001400
Iteration 20/1000 | Loss: 0.00001400
Iteration 21/1000 | Loss: 0.00001400
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001400
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001399
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001398
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001397
Iteration 31/1000 | Loss: 0.00001397
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001396
Iteration 46/1000 | Loss: 0.00001396
Iteration 47/1000 | Loss: 0.00001396
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001394
Iteration 52/1000 | Loss: 0.00001394
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001393
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001393
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001393
Iteration 63/1000 | Loss: 0.00001393
Iteration 64/1000 | Loss: 0.00001393
Iteration 65/1000 | Loss: 0.00001393
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001391
Iteration 71/1000 | Loss: 0.00001391
Iteration 72/1000 | Loss: 0.00001391
Iteration 73/1000 | Loss: 0.00001391
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001390
Iteration 81/1000 | Loss: 0.00001390
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001388
Iteration 101/1000 | Loss: 0.00001388
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001387
Iteration 115/1000 | Loss: 0.00001387
Iteration 116/1000 | Loss: 0.00001387
Iteration 117/1000 | Loss: 0.00001387
Iteration 118/1000 | Loss: 0.00001387
Iteration 119/1000 | Loss: 0.00001387
Iteration 120/1000 | Loss: 0.00001387
Iteration 121/1000 | Loss: 0.00001387
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001386
Iteration 140/1000 | Loss: 0.00001386
Iteration 141/1000 | Loss: 0.00001386
Iteration 142/1000 | Loss: 0.00001386
Iteration 143/1000 | Loss: 0.00001386
Iteration 144/1000 | Loss: 0.00001386
Iteration 145/1000 | Loss: 0.00001386
Iteration 146/1000 | Loss: 0.00001386
Iteration 147/1000 | Loss: 0.00001386
Iteration 148/1000 | Loss: 0.00001386
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001385
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.385466293868376e-05, 1.385466293868376e-05, 1.385466293868376e-05, 1.385466293868376e-05, 1.385466293868376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.385466293868376e-05

Optimization complete. Final v2v error: 3.220071315765381 mm

Highest mean error: 3.4899559020996094 mm for frame 99

Lowest mean error: 3.0157577991485596 mm for frame 17

Saving results

Total time: 32.07899498939514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842246
Iteration 2/25 | Loss: 0.00147610
Iteration 3/25 | Loss: 0.00121290
Iteration 4/25 | Loss: 0.00119326
Iteration 5/25 | Loss: 0.00119043
Iteration 6/25 | Loss: 0.00118971
Iteration 7/25 | Loss: 0.00118971
Iteration 8/25 | Loss: 0.00118971
Iteration 9/25 | Loss: 0.00118971
Iteration 10/25 | Loss: 0.00118971
Iteration 11/25 | Loss: 0.00118971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011897075455635786, 0.0011897075455635786, 0.0011897075455635786, 0.0011897075455635786, 0.0011897075455635786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011897075455635786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38349521
Iteration 2/25 | Loss: 0.00078776
Iteration 3/25 | Loss: 0.00078774
Iteration 4/25 | Loss: 0.00078774
Iteration 5/25 | Loss: 0.00078774
Iteration 6/25 | Loss: 0.00078774
Iteration 7/25 | Loss: 0.00078774
Iteration 8/25 | Loss: 0.00078774
Iteration 9/25 | Loss: 0.00078774
Iteration 10/25 | Loss: 0.00078774
Iteration 11/25 | Loss: 0.00078774
Iteration 12/25 | Loss: 0.00078774
Iteration 13/25 | Loss: 0.00078774
Iteration 14/25 | Loss: 0.00078774
Iteration 15/25 | Loss: 0.00078774
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007877390598878264, 0.0007877390598878264, 0.0007877390598878264, 0.0007877390598878264, 0.0007877390598878264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007877390598878264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078774
Iteration 2/1000 | Loss: 0.00003557
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001942
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001716
Iteration 9/1000 | Loss: 0.00001702
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001697
Iteration 13/1000 | Loss: 0.00001697
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00001684
Iteration 19/1000 | Loss: 0.00001682
Iteration 20/1000 | Loss: 0.00001677
Iteration 21/1000 | Loss: 0.00001677
Iteration 22/1000 | Loss: 0.00001677
Iteration 23/1000 | Loss: 0.00001677
Iteration 24/1000 | Loss: 0.00001677
Iteration 25/1000 | Loss: 0.00001677
Iteration 26/1000 | Loss: 0.00001677
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001675
Iteration 30/1000 | Loss: 0.00001675
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001669
Iteration 47/1000 | Loss: 0.00001669
Iteration 48/1000 | Loss: 0.00001669
Iteration 49/1000 | Loss: 0.00001669
Iteration 50/1000 | Loss: 0.00001669
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001669
Iteration 56/1000 | Loss: 0.00001669
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.66867994266795e-05, 1.66867994266795e-05, 1.66867994266795e-05, 1.66867994266795e-05, 1.66867994266795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.66867994266795e-05

Optimization complete. Final v2v error: 3.5389413833618164 mm

Highest mean error: 3.7556917667388916 mm for frame 77

Lowest mean error: 3.2964086532592773 mm for frame 8

Saving results

Total time: 27.1808762550354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082742
Iteration 2/25 | Loss: 0.00152634
Iteration 3/25 | Loss: 0.00124712
Iteration 4/25 | Loss: 0.00118638
Iteration 5/25 | Loss: 0.00118027
Iteration 6/25 | Loss: 0.00117942
Iteration 7/25 | Loss: 0.00117930
Iteration 8/25 | Loss: 0.00117930
Iteration 9/25 | Loss: 0.00117930
Iteration 10/25 | Loss: 0.00117930
Iteration 11/25 | Loss: 0.00117930
Iteration 12/25 | Loss: 0.00117930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001179297105409205, 0.001179297105409205, 0.001179297105409205, 0.001179297105409205, 0.001179297105409205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001179297105409205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45608902
Iteration 2/25 | Loss: 0.00051021
Iteration 3/25 | Loss: 0.00051021
Iteration 4/25 | Loss: 0.00051021
Iteration 5/25 | Loss: 0.00051021
Iteration 6/25 | Loss: 0.00051021
Iteration 7/25 | Loss: 0.00051021
Iteration 8/25 | Loss: 0.00051021
Iteration 9/25 | Loss: 0.00051021
Iteration 10/25 | Loss: 0.00051021
Iteration 11/25 | Loss: 0.00051021
Iteration 12/25 | Loss: 0.00051021
Iteration 13/25 | Loss: 0.00051021
Iteration 14/25 | Loss: 0.00051021
Iteration 15/25 | Loss: 0.00051021
Iteration 16/25 | Loss: 0.00051021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005102065042592585, 0.0005102065042592585, 0.0005102065042592585, 0.0005102065042592585, 0.0005102065042592585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005102065042592585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051021
Iteration 2/1000 | Loss: 0.00003753
Iteration 3/1000 | Loss: 0.00002521
Iteration 4/1000 | Loss: 0.00002356
Iteration 5/1000 | Loss: 0.00002257
Iteration 6/1000 | Loss: 0.00002193
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002158
Iteration 9/1000 | Loss: 0.00002130
Iteration 10/1000 | Loss: 0.00002112
Iteration 11/1000 | Loss: 0.00002093
Iteration 12/1000 | Loss: 0.00002086
Iteration 13/1000 | Loss: 0.00002077
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00002072
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002072
Iteration 19/1000 | Loss: 0.00002071
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002071
Iteration 23/1000 | Loss: 0.00002070
Iteration 24/1000 | Loss: 0.00002070
Iteration 25/1000 | Loss: 0.00002069
Iteration 26/1000 | Loss: 0.00002069
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002065
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002065
Iteration 31/1000 | Loss: 0.00002065
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00002065
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002064
Iteration 38/1000 | Loss: 0.00002064
Iteration 39/1000 | Loss: 0.00002064
Iteration 40/1000 | Loss: 0.00002063
Iteration 41/1000 | Loss: 0.00002063
Iteration 42/1000 | Loss: 0.00002063
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002060
Iteration 48/1000 | Loss: 0.00002060
Iteration 49/1000 | Loss: 0.00002060
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002059
Iteration 52/1000 | Loss: 0.00002059
Iteration 53/1000 | Loss: 0.00002059
Iteration 54/1000 | Loss: 0.00002059
Iteration 55/1000 | Loss: 0.00002059
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00002059
Iteration 58/1000 | Loss: 0.00002059
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002058
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002057
Iteration 66/1000 | Loss: 0.00002056
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002055
Iteration 69/1000 | Loss: 0.00002055
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002054
Iteration 77/1000 | Loss: 0.00002054
Iteration 78/1000 | Loss: 0.00002054
Iteration 79/1000 | Loss: 0.00002053
Iteration 80/1000 | Loss: 0.00002053
Iteration 81/1000 | Loss: 0.00002052
Iteration 82/1000 | Loss: 0.00002052
Iteration 83/1000 | Loss: 0.00002052
Iteration 84/1000 | Loss: 0.00002052
Iteration 85/1000 | Loss: 0.00002052
Iteration 86/1000 | Loss: 0.00002051
Iteration 87/1000 | Loss: 0.00002051
Iteration 88/1000 | Loss: 0.00002051
Iteration 89/1000 | Loss: 0.00002051
Iteration 90/1000 | Loss: 0.00002051
Iteration 91/1000 | Loss: 0.00002051
Iteration 92/1000 | Loss: 0.00002050
Iteration 93/1000 | Loss: 0.00002050
Iteration 94/1000 | Loss: 0.00002050
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002049
Iteration 97/1000 | Loss: 0.00002049
Iteration 98/1000 | Loss: 0.00002049
Iteration 99/1000 | Loss: 0.00002049
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002047
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002045
Iteration 121/1000 | Loss: 0.00002045
Iteration 122/1000 | Loss: 0.00002045
Iteration 123/1000 | Loss: 0.00002045
Iteration 124/1000 | Loss: 0.00002045
Iteration 125/1000 | Loss: 0.00002045
Iteration 126/1000 | Loss: 0.00002045
Iteration 127/1000 | Loss: 0.00002045
Iteration 128/1000 | Loss: 0.00002045
Iteration 129/1000 | Loss: 0.00002045
Iteration 130/1000 | Loss: 0.00002045
Iteration 131/1000 | Loss: 0.00002045
Iteration 132/1000 | Loss: 0.00002045
Iteration 133/1000 | Loss: 0.00002045
Iteration 134/1000 | Loss: 0.00002045
Iteration 135/1000 | Loss: 0.00002045
Iteration 136/1000 | Loss: 0.00002045
Iteration 137/1000 | Loss: 0.00002044
Iteration 138/1000 | Loss: 0.00002044
Iteration 139/1000 | Loss: 0.00002044
Iteration 140/1000 | Loss: 0.00002044
Iteration 141/1000 | Loss: 0.00002044
Iteration 142/1000 | Loss: 0.00002044
Iteration 143/1000 | Loss: 0.00002044
Iteration 144/1000 | Loss: 0.00002044
Iteration 145/1000 | Loss: 0.00002043
Iteration 146/1000 | Loss: 0.00002043
Iteration 147/1000 | Loss: 0.00002043
Iteration 148/1000 | Loss: 0.00002043
Iteration 149/1000 | Loss: 0.00002043
Iteration 150/1000 | Loss: 0.00002043
Iteration 151/1000 | Loss: 0.00002042
Iteration 152/1000 | Loss: 0.00002042
Iteration 153/1000 | Loss: 0.00002042
Iteration 154/1000 | Loss: 0.00002042
Iteration 155/1000 | Loss: 0.00002042
Iteration 156/1000 | Loss: 0.00002042
Iteration 157/1000 | Loss: 0.00002042
Iteration 158/1000 | Loss: 0.00002042
Iteration 159/1000 | Loss: 0.00002042
Iteration 160/1000 | Loss: 0.00002042
Iteration 161/1000 | Loss: 0.00002041
Iteration 162/1000 | Loss: 0.00002041
Iteration 163/1000 | Loss: 0.00002041
Iteration 164/1000 | Loss: 0.00002041
Iteration 165/1000 | Loss: 0.00002041
Iteration 166/1000 | Loss: 0.00002041
Iteration 167/1000 | Loss: 0.00002041
Iteration 168/1000 | Loss: 0.00002041
Iteration 169/1000 | Loss: 0.00002041
Iteration 170/1000 | Loss: 0.00002041
Iteration 171/1000 | Loss: 0.00002041
Iteration 172/1000 | Loss: 0.00002040
Iteration 173/1000 | Loss: 0.00002040
Iteration 174/1000 | Loss: 0.00002040
Iteration 175/1000 | Loss: 0.00002040
Iteration 176/1000 | Loss: 0.00002040
Iteration 177/1000 | Loss: 0.00002040
Iteration 178/1000 | Loss: 0.00002040
Iteration 179/1000 | Loss: 0.00002040
Iteration 180/1000 | Loss: 0.00002039
Iteration 181/1000 | Loss: 0.00002039
Iteration 182/1000 | Loss: 0.00002039
Iteration 183/1000 | Loss: 0.00002039
Iteration 184/1000 | Loss: 0.00002039
Iteration 185/1000 | Loss: 0.00002039
Iteration 186/1000 | Loss: 0.00002039
Iteration 187/1000 | Loss: 0.00002038
Iteration 188/1000 | Loss: 0.00002038
Iteration 189/1000 | Loss: 0.00002038
Iteration 190/1000 | Loss: 0.00002038
Iteration 191/1000 | Loss: 0.00002038
Iteration 192/1000 | Loss: 0.00002038
Iteration 193/1000 | Loss: 0.00002038
Iteration 194/1000 | Loss: 0.00002038
Iteration 195/1000 | Loss: 0.00002038
Iteration 196/1000 | Loss: 0.00002038
Iteration 197/1000 | Loss: 0.00002038
Iteration 198/1000 | Loss: 0.00002038
Iteration 199/1000 | Loss: 0.00002038
Iteration 200/1000 | Loss: 0.00002038
Iteration 201/1000 | Loss: 0.00002038
Iteration 202/1000 | Loss: 0.00002038
Iteration 203/1000 | Loss: 0.00002038
Iteration 204/1000 | Loss: 0.00002038
Iteration 205/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.0382874936331064e-05, 2.0382874936331064e-05, 2.0382874936331064e-05, 2.0382874936331064e-05, 2.0382874936331064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0382874936331064e-05

Optimization complete. Final v2v error: 3.840888500213623 mm

Highest mean error: 4.1016645431518555 mm for frame 23

Lowest mean error: 3.3387320041656494 mm for frame 2

Saving results

Total time: 38.12088108062744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044719
Iteration 2/25 | Loss: 0.01044719
Iteration 3/25 | Loss: 0.01044718
Iteration 4/25 | Loss: 0.00487657
Iteration 5/25 | Loss: 0.00338706
Iteration 6/25 | Loss: 0.00309217
Iteration 7/25 | Loss: 0.00249448
Iteration 8/25 | Loss: 0.00212147
Iteration 9/25 | Loss: 0.00198965
Iteration 10/25 | Loss: 0.00185432
Iteration 11/25 | Loss: 0.00177979
Iteration 12/25 | Loss: 0.00177365
Iteration 13/25 | Loss: 0.00172301
Iteration 14/25 | Loss: 0.00171614
Iteration 15/25 | Loss: 0.00169049
Iteration 16/25 | Loss: 0.00167580
Iteration 17/25 | Loss: 0.00166584
Iteration 18/25 | Loss: 0.00165972
Iteration 19/25 | Loss: 0.00166062
Iteration 20/25 | Loss: 0.00165796
Iteration 21/25 | Loss: 0.00165496
Iteration 22/25 | Loss: 0.00165403
Iteration 23/25 | Loss: 0.00165408
Iteration 24/25 | Loss: 0.00165414
Iteration 25/25 | Loss: 0.00165381

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39865279
Iteration 2/25 | Loss: 0.01784818
Iteration 3/25 | Loss: 0.00417368
Iteration 4/25 | Loss: 0.00417368
Iteration 5/25 | Loss: 0.00417368
Iteration 6/25 | Loss: 0.00417367
Iteration 7/25 | Loss: 0.00417367
Iteration 8/25 | Loss: 0.00417367
Iteration 9/25 | Loss: 0.00417367
Iteration 10/25 | Loss: 0.00417367
Iteration 11/25 | Loss: 0.00417367
Iteration 12/25 | Loss: 0.00417367
Iteration 13/25 | Loss: 0.00417367
Iteration 14/25 | Loss: 0.00417367
Iteration 15/25 | Loss: 0.00417367
Iteration 16/25 | Loss: 0.00417367
Iteration 17/25 | Loss: 0.00417367
Iteration 18/25 | Loss: 0.00417367
Iteration 19/25 | Loss: 0.00417367
Iteration 20/25 | Loss: 0.00417367
Iteration 21/25 | Loss: 0.00417367
Iteration 22/25 | Loss: 0.00417367
Iteration 23/25 | Loss: 0.00417367
Iteration 24/25 | Loss: 0.00417367
Iteration 25/25 | Loss: 0.00417367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417367
Iteration 2/1000 | Loss: 0.00394647
Iteration 3/1000 | Loss: 0.00424486
Iteration 4/1000 | Loss: 0.00199696
Iteration 5/1000 | Loss: 0.00106103
Iteration 6/1000 | Loss: 0.00049269
Iteration 7/1000 | Loss: 0.00302325
Iteration 8/1000 | Loss: 0.00066045
Iteration 9/1000 | Loss: 0.00161009
Iteration 10/1000 | Loss: 0.00103238
Iteration 11/1000 | Loss: 0.00374575
Iteration 12/1000 | Loss: 0.00088817
Iteration 13/1000 | Loss: 0.00077701
Iteration 14/1000 | Loss: 0.00048674
Iteration 15/1000 | Loss: 0.00048349
Iteration 16/1000 | Loss: 0.00032486
Iteration 17/1000 | Loss: 0.00057432
Iteration 18/1000 | Loss: 0.00619335
Iteration 19/1000 | Loss: 0.01391891
Iteration 20/1000 | Loss: 0.01655042
Iteration 21/1000 | Loss: 0.00413766
Iteration 22/1000 | Loss: 0.00503266
Iteration 23/1000 | Loss: 0.00130194
Iteration 24/1000 | Loss: 0.00047283
Iteration 25/1000 | Loss: 0.00058932
Iteration 26/1000 | Loss: 0.00084486
Iteration 27/1000 | Loss: 0.00080239
Iteration 28/1000 | Loss: 0.00027128
Iteration 29/1000 | Loss: 0.00024290
Iteration 30/1000 | Loss: 0.00012478
Iteration 31/1000 | Loss: 0.00064150
Iteration 32/1000 | Loss: 0.00024716
Iteration 33/1000 | Loss: 0.00010086
Iteration 34/1000 | Loss: 0.00029093
Iteration 35/1000 | Loss: 0.00083822
Iteration 36/1000 | Loss: 0.00022883
Iteration 37/1000 | Loss: 0.00031617
Iteration 38/1000 | Loss: 0.00035155
Iteration 39/1000 | Loss: 0.00050289
Iteration 40/1000 | Loss: 0.00074638
Iteration 41/1000 | Loss: 0.00051834
Iteration 42/1000 | Loss: 0.00055592
Iteration 43/1000 | Loss: 0.00314091
Iteration 44/1000 | Loss: 0.00219143
Iteration 45/1000 | Loss: 0.00285286
Iteration 46/1000 | Loss: 0.00185843
Iteration 47/1000 | Loss: 0.00019333
Iteration 48/1000 | Loss: 0.00091582
Iteration 49/1000 | Loss: 0.00008266
Iteration 50/1000 | Loss: 0.00009757
Iteration 51/1000 | Loss: 0.00005758
Iteration 52/1000 | Loss: 0.00005426
Iteration 53/1000 | Loss: 0.00016205
Iteration 54/1000 | Loss: 0.00059509
Iteration 55/1000 | Loss: 0.00091365
Iteration 56/1000 | Loss: 0.00015542
Iteration 57/1000 | Loss: 0.00007824
Iteration 58/1000 | Loss: 0.00005528
Iteration 59/1000 | Loss: 0.00053086
Iteration 60/1000 | Loss: 0.00005216
Iteration 61/1000 | Loss: 0.00005065
Iteration 62/1000 | Loss: 0.00015111
Iteration 63/1000 | Loss: 0.00026534
Iteration 64/1000 | Loss: 0.00005085
Iteration 65/1000 | Loss: 0.00004862
Iteration 66/1000 | Loss: 0.00004800
Iteration 67/1000 | Loss: 0.00004778
Iteration 68/1000 | Loss: 0.00004759
Iteration 69/1000 | Loss: 0.00004758
Iteration 70/1000 | Loss: 0.00004757
Iteration 71/1000 | Loss: 0.00004787
Iteration 72/1000 | Loss: 0.00004795
Iteration 73/1000 | Loss: 0.00004749
Iteration 74/1000 | Loss: 0.00004749
Iteration 75/1000 | Loss: 0.00004736
Iteration 76/1000 | Loss: 0.00004754
Iteration 77/1000 | Loss: 0.00004752
Iteration 78/1000 | Loss: 0.00004747
Iteration 79/1000 | Loss: 0.00004747
Iteration 80/1000 | Loss: 0.00004746
Iteration 81/1000 | Loss: 0.00004746
Iteration 82/1000 | Loss: 0.00004745
Iteration 83/1000 | Loss: 0.00004737
Iteration 84/1000 | Loss: 0.00004737
Iteration 85/1000 | Loss: 0.00004737
Iteration 86/1000 | Loss: 0.00004737
Iteration 87/1000 | Loss: 0.00004737
Iteration 88/1000 | Loss: 0.00004737
Iteration 89/1000 | Loss: 0.00004737
Iteration 90/1000 | Loss: 0.00004737
Iteration 91/1000 | Loss: 0.00004737
Iteration 92/1000 | Loss: 0.00004737
Iteration 93/1000 | Loss: 0.00004736
Iteration 94/1000 | Loss: 0.00004736
Iteration 95/1000 | Loss: 0.00004736
Iteration 96/1000 | Loss: 0.00004735
Iteration 97/1000 | Loss: 0.00004735
Iteration 98/1000 | Loss: 0.00004734
Iteration 99/1000 | Loss: 0.00004733
Iteration 100/1000 | Loss: 0.00004753
Iteration 101/1000 | Loss: 0.00004752
Iteration 102/1000 | Loss: 0.00004757
Iteration 103/1000 | Loss: 0.00004731
Iteration 104/1000 | Loss: 0.00004731
Iteration 105/1000 | Loss: 0.00004731
Iteration 106/1000 | Loss: 0.00004731
Iteration 107/1000 | Loss: 0.00004731
Iteration 108/1000 | Loss: 0.00004731
Iteration 109/1000 | Loss: 0.00004731
Iteration 110/1000 | Loss: 0.00004731
Iteration 111/1000 | Loss: 0.00004731
Iteration 112/1000 | Loss: 0.00004730
Iteration 113/1000 | Loss: 0.00004730
Iteration 114/1000 | Loss: 0.00004730
Iteration 115/1000 | Loss: 0.00004730
Iteration 116/1000 | Loss: 0.00024402
Iteration 117/1000 | Loss: 0.00105315
Iteration 118/1000 | Loss: 0.00011249
Iteration 119/1000 | Loss: 0.00005798
Iteration 120/1000 | Loss: 0.00004405
Iteration 121/1000 | Loss: 0.00003595
Iteration 122/1000 | Loss: 0.00002973
Iteration 123/1000 | Loss: 0.00002678
Iteration 124/1000 | Loss: 0.00002518
Iteration 125/1000 | Loss: 0.00002417
Iteration 126/1000 | Loss: 0.00002325
Iteration 127/1000 | Loss: 0.00002247
Iteration 128/1000 | Loss: 0.00002193
Iteration 129/1000 | Loss: 0.00002191
Iteration 130/1000 | Loss: 0.00002147
Iteration 131/1000 | Loss: 0.00002133
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002126
Iteration 134/1000 | Loss: 0.00002094
Iteration 135/1000 | Loss: 0.00002094
Iteration 136/1000 | Loss: 0.00002094
Iteration 137/1000 | Loss: 0.00002094
Iteration 138/1000 | Loss: 0.00002094
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002093
Iteration 144/1000 | Loss: 0.00002093
Iteration 145/1000 | Loss: 0.00002093
Iteration 146/1000 | Loss: 0.00002093
Iteration 147/1000 | Loss: 0.00002093
Iteration 148/1000 | Loss: 0.00002092
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002086
Iteration 151/1000 | Loss: 0.00002085
Iteration 152/1000 | Loss: 0.00002085
Iteration 153/1000 | Loss: 0.00002085
Iteration 154/1000 | Loss: 0.00002085
Iteration 155/1000 | Loss: 0.00002085
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002085
Iteration 158/1000 | Loss: 0.00002085
Iteration 159/1000 | Loss: 0.00002085
Iteration 160/1000 | Loss: 0.00002085
Iteration 161/1000 | Loss: 0.00002085
Iteration 162/1000 | Loss: 0.00002084
Iteration 163/1000 | Loss: 0.00002084
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002083
Iteration 168/1000 | Loss: 0.00002083
Iteration 169/1000 | Loss: 0.00002083
Iteration 170/1000 | Loss: 0.00002082
Iteration 171/1000 | Loss: 0.00002082
Iteration 172/1000 | Loss: 0.00002082
Iteration 173/1000 | Loss: 0.00002082
Iteration 174/1000 | Loss: 0.00002082
Iteration 175/1000 | Loss: 0.00002082
Iteration 176/1000 | Loss: 0.00002081
Iteration 177/1000 | Loss: 0.00002081
Iteration 178/1000 | Loss: 0.00002081
Iteration 179/1000 | Loss: 0.00002081
Iteration 180/1000 | Loss: 0.00002080
Iteration 181/1000 | Loss: 0.00002094
Iteration 182/1000 | Loss: 0.00002094
Iteration 183/1000 | Loss: 0.00002094
Iteration 184/1000 | Loss: 0.00002094
Iteration 185/1000 | Loss: 0.00002080
Iteration 186/1000 | Loss: 0.00002080
Iteration 187/1000 | Loss: 0.00002080
Iteration 188/1000 | Loss: 0.00002093
Iteration 189/1000 | Loss: 0.00002093
Iteration 190/1000 | Loss: 0.00002093
Iteration 191/1000 | Loss: 0.00002093
Iteration 192/1000 | Loss: 0.00002092
Iteration 193/1000 | Loss: 0.00002092
Iteration 194/1000 | Loss: 0.00002092
Iteration 195/1000 | Loss: 0.00002092
Iteration 196/1000 | Loss: 0.00002092
Iteration 197/1000 | Loss: 0.00002092
Iteration 198/1000 | Loss: 0.00002092
Iteration 199/1000 | Loss: 0.00002092
Iteration 200/1000 | Loss: 0.00002090
Iteration 201/1000 | Loss: 0.00002090
Iteration 202/1000 | Loss: 0.00002090
Iteration 203/1000 | Loss: 0.00002090
Iteration 204/1000 | Loss: 0.00002088
Iteration 205/1000 | Loss: 0.00002087
Iteration 206/1000 | Loss: 0.00002086
Iteration 207/1000 | Loss: 0.00002086
Iteration 208/1000 | Loss: 0.00002086
Iteration 209/1000 | Loss: 0.00002086
Iteration 210/1000 | Loss: 0.00002086
Iteration 211/1000 | Loss: 0.00002086
Iteration 212/1000 | Loss: 0.00002086
Iteration 213/1000 | Loss: 0.00002086
Iteration 214/1000 | Loss: 0.00002086
Iteration 215/1000 | Loss: 0.00002085
Iteration 216/1000 | Loss: 0.00002085
Iteration 217/1000 | Loss: 0.00002085
Iteration 218/1000 | Loss: 0.00002085
Iteration 219/1000 | Loss: 0.00002085
Iteration 220/1000 | Loss: 0.00002084
Iteration 221/1000 | Loss: 0.00002084
Iteration 222/1000 | Loss: 0.00002084
Iteration 223/1000 | Loss: 0.00002084
Iteration 224/1000 | Loss: 0.00002084
Iteration 225/1000 | Loss: 0.00002084
Iteration 226/1000 | Loss: 0.00002084
Iteration 227/1000 | Loss: 0.00002084
Iteration 228/1000 | Loss: 0.00002084
Iteration 229/1000 | Loss: 0.00002084
Iteration 230/1000 | Loss: 0.00002084
Iteration 231/1000 | Loss: 0.00002084
Iteration 232/1000 | Loss: 0.00002084
Iteration 233/1000 | Loss: 0.00002084
Iteration 234/1000 | Loss: 0.00002084
Iteration 235/1000 | Loss: 0.00002084
Iteration 236/1000 | Loss: 0.00002084
Iteration 237/1000 | Loss: 0.00002084
Iteration 238/1000 | Loss: 0.00002084
Iteration 239/1000 | Loss: 0.00002084
Iteration 240/1000 | Loss: 0.00002084
Iteration 241/1000 | Loss: 0.00002084
Iteration 242/1000 | Loss: 0.00002084
Iteration 243/1000 | Loss: 0.00002084
Iteration 244/1000 | Loss: 0.00002084
Iteration 245/1000 | Loss: 0.00002084
Iteration 246/1000 | Loss: 0.00002084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.0837127522099763e-05, 2.0837127522099763e-05, 2.0837127522099763e-05, 2.0837127522099763e-05, 2.0837127522099763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0837127522099763e-05

Optimization complete. Final v2v error: 3.342444658279419 mm

Highest mean error: 19.560773849487305 mm for frame 206

Lowest mean error: 2.9079244136810303 mm for frame 66

Saving results

Total time: 209.7777030467987
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614898
Iteration 2/25 | Loss: 0.00157370
Iteration 3/25 | Loss: 0.00129888
Iteration 4/25 | Loss: 0.00127412
Iteration 5/25 | Loss: 0.00126929
Iteration 6/25 | Loss: 0.00126887
Iteration 7/25 | Loss: 0.00126887
Iteration 8/25 | Loss: 0.00126887
Iteration 9/25 | Loss: 0.00126887
Iteration 10/25 | Loss: 0.00126887
Iteration 11/25 | Loss: 0.00126887
Iteration 12/25 | Loss: 0.00126887
Iteration 13/25 | Loss: 0.00126887
Iteration 14/25 | Loss: 0.00126887
Iteration 15/25 | Loss: 0.00126887
Iteration 16/25 | Loss: 0.00126887
Iteration 17/25 | Loss: 0.00126887
Iteration 18/25 | Loss: 0.00126887
Iteration 19/25 | Loss: 0.00126887
Iteration 20/25 | Loss: 0.00126887
Iteration 21/25 | Loss: 0.00126887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001268866122700274, 0.001268866122700274, 0.001268866122700274, 0.001268866122700274, 0.001268866122700274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268866122700274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04320741
Iteration 2/25 | Loss: 0.00084456
Iteration 3/25 | Loss: 0.00084456
Iteration 4/25 | Loss: 0.00084456
Iteration 5/25 | Loss: 0.00084456
Iteration 6/25 | Loss: 0.00084456
Iteration 7/25 | Loss: 0.00084456
Iteration 8/25 | Loss: 0.00084456
Iteration 9/25 | Loss: 0.00084456
Iteration 10/25 | Loss: 0.00084456
Iteration 11/25 | Loss: 0.00084456
Iteration 12/25 | Loss: 0.00084456
Iteration 13/25 | Loss: 0.00084456
Iteration 14/25 | Loss: 0.00084456
Iteration 15/25 | Loss: 0.00084456
Iteration 16/25 | Loss: 0.00084456
Iteration 17/25 | Loss: 0.00084456
Iteration 18/25 | Loss: 0.00084456
Iteration 19/25 | Loss: 0.00084456
Iteration 20/25 | Loss: 0.00084456
Iteration 21/25 | Loss: 0.00084456
Iteration 22/25 | Loss: 0.00084456
Iteration 23/25 | Loss: 0.00084456
Iteration 24/25 | Loss: 0.00084456
Iteration 25/25 | Loss: 0.00084456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084456
Iteration 2/1000 | Loss: 0.00005240
Iteration 3/1000 | Loss: 0.00003328
Iteration 4/1000 | Loss: 0.00002777
Iteration 5/1000 | Loss: 0.00002534
Iteration 6/1000 | Loss: 0.00002399
Iteration 7/1000 | Loss: 0.00002345
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002273
Iteration 10/1000 | Loss: 0.00002255
Iteration 11/1000 | Loss: 0.00002241
Iteration 12/1000 | Loss: 0.00002226
Iteration 13/1000 | Loss: 0.00002223
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00002216
Iteration 16/1000 | Loss: 0.00002211
Iteration 17/1000 | Loss: 0.00002210
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002208
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002205
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002201
Iteration 25/1000 | Loss: 0.00002200
Iteration 26/1000 | Loss: 0.00002198
Iteration 27/1000 | Loss: 0.00002198
Iteration 28/1000 | Loss: 0.00002198
Iteration 29/1000 | Loss: 0.00002198
Iteration 30/1000 | Loss: 0.00002198
Iteration 31/1000 | Loss: 0.00002198
Iteration 32/1000 | Loss: 0.00002198
Iteration 33/1000 | Loss: 0.00002198
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002197
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002193
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002191
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002188
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002185
Iteration 83/1000 | Loss: 0.00002185
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002184
Iteration 87/1000 | Loss: 0.00002184
Iteration 88/1000 | Loss: 0.00002184
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002184
Iteration 91/1000 | Loss: 0.00002183
Iteration 92/1000 | Loss: 0.00002183
Iteration 93/1000 | Loss: 0.00002183
Iteration 94/1000 | Loss: 0.00002183
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002182
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002181
Iteration 113/1000 | Loss: 0.00002181
Iteration 114/1000 | Loss: 0.00002181
Iteration 115/1000 | Loss: 0.00002181
Iteration 116/1000 | Loss: 0.00002181
Iteration 117/1000 | Loss: 0.00002181
Iteration 118/1000 | Loss: 0.00002181
Iteration 119/1000 | Loss: 0.00002181
Iteration 120/1000 | Loss: 0.00002181
Iteration 121/1000 | Loss: 0.00002181
Iteration 122/1000 | Loss: 0.00002181
Iteration 123/1000 | Loss: 0.00002181
Iteration 124/1000 | Loss: 0.00002181
Iteration 125/1000 | Loss: 0.00002181
Iteration 126/1000 | Loss: 0.00002181
Iteration 127/1000 | Loss: 0.00002181
Iteration 128/1000 | Loss: 0.00002181
Iteration 129/1000 | Loss: 0.00002181
Iteration 130/1000 | Loss: 0.00002181
Iteration 131/1000 | Loss: 0.00002181
Iteration 132/1000 | Loss: 0.00002181
Iteration 133/1000 | Loss: 0.00002181
Iteration 134/1000 | Loss: 0.00002181
Iteration 135/1000 | Loss: 0.00002181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.18080822378397e-05, 2.18080822378397e-05, 2.18080822378397e-05, 2.18080822378397e-05, 2.18080822378397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.18080822378397e-05

Optimization complete. Final v2v error: 4.00941801071167 mm

Highest mean error: 4.3184733390808105 mm for frame 93

Lowest mean error: 3.6917614936828613 mm for frame 12

Saving results

Total time: 35.74620032310486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01152326
Iteration 2/25 | Loss: 0.01152326
Iteration 3/25 | Loss: 0.01152326
Iteration 4/25 | Loss: 0.01152326
Iteration 5/25 | Loss: 0.01152325
Iteration 6/25 | Loss: 0.00415787
Iteration 7/25 | Loss: 0.00271674
Iteration 8/25 | Loss: 0.00235772
Iteration 9/25 | Loss: 0.00208030
Iteration 10/25 | Loss: 0.00199792
Iteration 11/25 | Loss: 0.00194595
Iteration 12/25 | Loss: 0.00187564
Iteration 13/25 | Loss: 0.00177102
Iteration 14/25 | Loss: 0.00170844
Iteration 15/25 | Loss: 0.00169616
Iteration 16/25 | Loss: 0.00169274
Iteration 17/25 | Loss: 0.00166203
Iteration 18/25 | Loss: 0.00164701
Iteration 19/25 | Loss: 0.00164186
Iteration 20/25 | Loss: 0.00164885
Iteration 21/25 | Loss: 0.00164063
Iteration 22/25 | Loss: 0.00162027
Iteration 23/25 | Loss: 0.00161896
Iteration 24/25 | Loss: 0.00162174
Iteration 25/25 | Loss: 0.00162353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32919025
Iteration 2/25 | Loss: 0.00452289
Iteration 3/25 | Loss: 0.00370456
Iteration 4/25 | Loss: 0.00370455
Iteration 5/25 | Loss: 0.00370455
Iteration 6/25 | Loss: 0.00370455
Iteration 7/25 | Loss: 0.00370455
Iteration 8/25 | Loss: 0.00370455
Iteration 9/25 | Loss: 0.00370455
Iteration 10/25 | Loss: 0.00370455
Iteration 11/25 | Loss: 0.00370455
Iteration 12/25 | Loss: 0.00370455
Iteration 13/25 | Loss: 0.00370455
Iteration 14/25 | Loss: 0.00370455
Iteration 15/25 | Loss: 0.00370455
Iteration 16/25 | Loss: 0.00370455
Iteration 17/25 | Loss: 0.00370455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0037045516073703766, 0.0037045516073703766, 0.0037045516073703766, 0.0037045516073703766, 0.0037045516073703766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037045516073703766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00370455
Iteration 2/1000 | Loss: 0.00210923
Iteration 3/1000 | Loss: 0.00119959
Iteration 4/1000 | Loss: 0.00125733
Iteration 5/1000 | Loss: 0.00211742
Iteration 6/1000 | Loss: 0.00125054
Iteration 7/1000 | Loss: 0.00289410
Iteration 8/1000 | Loss: 0.00168374
Iteration 9/1000 | Loss: 0.00127773
Iteration 10/1000 | Loss: 0.00232247
Iteration 11/1000 | Loss: 0.00180959
Iteration 12/1000 | Loss: 0.00080450
Iteration 13/1000 | Loss: 0.00160110
Iteration 14/1000 | Loss: 0.00069433
Iteration 15/1000 | Loss: 0.00061111
Iteration 16/1000 | Loss: 0.00075112
Iteration 17/1000 | Loss: 0.00078021
Iteration 18/1000 | Loss: 0.00105177
Iteration 19/1000 | Loss: 0.00090702
Iteration 20/1000 | Loss: 0.00425953
Iteration 21/1000 | Loss: 0.00101822
Iteration 22/1000 | Loss: 0.00048767
Iteration 23/1000 | Loss: 0.00095923
Iteration 24/1000 | Loss: 0.00147302
Iteration 25/1000 | Loss: 0.00128896
Iteration 26/1000 | Loss: 0.00077413
Iteration 27/1000 | Loss: 0.00153625
Iteration 28/1000 | Loss: 0.00093799
Iteration 29/1000 | Loss: 0.00099600
Iteration 30/1000 | Loss: 0.00060223
Iteration 31/1000 | Loss: 0.00072312
Iteration 32/1000 | Loss: 0.00102575
Iteration 33/1000 | Loss: 0.00075186
Iteration 34/1000 | Loss: 0.00061022
Iteration 35/1000 | Loss: 0.00050780
Iteration 36/1000 | Loss: 0.00075112
Iteration 37/1000 | Loss: 0.00174679
Iteration 38/1000 | Loss: 0.00076371
Iteration 39/1000 | Loss: 0.00155884
Iteration 40/1000 | Loss: 0.00102497
Iteration 41/1000 | Loss: 0.00112602
Iteration 42/1000 | Loss: 0.00145181
Iteration 43/1000 | Loss: 0.00101626
Iteration 44/1000 | Loss: 0.00098797
Iteration 45/1000 | Loss: 0.00040319
Iteration 46/1000 | Loss: 0.00044961
Iteration 47/1000 | Loss: 0.00048756
Iteration 48/1000 | Loss: 0.00064119
Iteration 49/1000 | Loss: 0.00050039
Iteration 50/1000 | Loss: 0.00051100
Iteration 51/1000 | Loss: 0.00052628
Iteration 52/1000 | Loss: 0.00027934
Iteration 53/1000 | Loss: 0.00037502
Iteration 54/1000 | Loss: 0.00030176
Iteration 55/1000 | Loss: 0.00034367
Iteration 56/1000 | Loss: 0.00028832
Iteration 57/1000 | Loss: 0.00038135
Iteration 58/1000 | Loss: 0.00103361
Iteration 59/1000 | Loss: 0.00056844
Iteration 60/1000 | Loss: 0.00075353
Iteration 61/1000 | Loss: 0.00053347
Iteration 62/1000 | Loss: 0.00060395
Iteration 63/1000 | Loss: 0.00040283
Iteration 64/1000 | Loss: 0.00065293
Iteration 65/1000 | Loss: 0.00040496
Iteration 66/1000 | Loss: 0.00041606
Iteration 67/1000 | Loss: 0.00027198
Iteration 68/1000 | Loss: 0.00037588
Iteration 69/1000 | Loss: 0.00071431
Iteration 70/1000 | Loss: 0.00073229
Iteration 71/1000 | Loss: 0.00031821
Iteration 72/1000 | Loss: 0.00039643
Iteration 73/1000 | Loss: 0.00041239
Iteration 74/1000 | Loss: 0.00077298
Iteration 75/1000 | Loss: 0.00072071
Iteration 76/1000 | Loss: 0.00043123
Iteration 77/1000 | Loss: 0.00086148
Iteration 78/1000 | Loss: 0.00106815
Iteration 79/1000 | Loss: 0.00082412
Iteration 80/1000 | Loss: 0.00041826
Iteration 81/1000 | Loss: 0.00043527
Iteration 82/1000 | Loss: 0.00042237
Iteration 83/1000 | Loss: 0.00023266
Iteration 84/1000 | Loss: 0.00026084
Iteration 85/1000 | Loss: 0.00031832
Iteration 86/1000 | Loss: 0.00035587
Iteration 87/1000 | Loss: 0.00018665
Iteration 88/1000 | Loss: 0.00039078
Iteration 89/1000 | Loss: 0.00031655
Iteration 90/1000 | Loss: 0.00030638
Iteration 91/1000 | Loss: 0.00044632
Iteration 92/1000 | Loss: 0.00029906
Iteration 93/1000 | Loss: 0.00019349
Iteration 94/1000 | Loss: 0.00030657
Iteration 95/1000 | Loss: 0.00022658
Iteration 96/1000 | Loss: 0.00024909
Iteration 97/1000 | Loss: 0.00118443
Iteration 98/1000 | Loss: 0.00092804
Iteration 99/1000 | Loss: 0.00050864
Iteration 100/1000 | Loss: 0.00050454
Iteration 101/1000 | Loss: 0.00050539
Iteration 102/1000 | Loss: 0.00086000
Iteration 103/1000 | Loss: 0.00080943
Iteration 104/1000 | Loss: 0.00026450
Iteration 105/1000 | Loss: 0.00036861
Iteration 106/1000 | Loss: 0.00025526
Iteration 107/1000 | Loss: 0.00083947
Iteration 108/1000 | Loss: 0.00110450
Iteration 109/1000 | Loss: 0.00105446
Iteration 110/1000 | Loss: 0.00165442
Iteration 111/1000 | Loss: 0.00083022
Iteration 112/1000 | Loss: 0.00066634
Iteration 113/1000 | Loss: 0.00075188
Iteration 114/1000 | Loss: 0.00061493
Iteration 115/1000 | Loss: 0.00149199
Iteration 116/1000 | Loss: 0.00090233
Iteration 117/1000 | Loss: 0.00025882
Iteration 118/1000 | Loss: 0.00037386
Iteration 119/1000 | Loss: 0.00036865
Iteration 120/1000 | Loss: 0.00042452
Iteration 121/1000 | Loss: 0.00031762
Iteration 122/1000 | Loss: 0.00034396
Iteration 123/1000 | Loss: 0.00021543
Iteration 124/1000 | Loss: 0.00017881
Iteration 125/1000 | Loss: 0.00018724
Iteration 126/1000 | Loss: 0.00045397
Iteration 127/1000 | Loss: 0.00032747
Iteration 128/1000 | Loss: 0.00019333
Iteration 129/1000 | Loss: 0.00017823
Iteration 130/1000 | Loss: 0.00017307
Iteration 131/1000 | Loss: 0.00017096
Iteration 132/1000 | Loss: 0.00016928
Iteration 133/1000 | Loss: 0.00097188
Iteration 134/1000 | Loss: 0.00063165
Iteration 135/1000 | Loss: 0.00018414
Iteration 136/1000 | Loss: 0.00017820
Iteration 137/1000 | Loss: 0.00017047
Iteration 138/1000 | Loss: 0.00048935
Iteration 139/1000 | Loss: 0.00020795
Iteration 140/1000 | Loss: 0.00018955
Iteration 141/1000 | Loss: 0.00017512
Iteration 142/1000 | Loss: 0.00017843
Iteration 143/1000 | Loss: 0.00016670
Iteration 144/1000 | Loss: 0.00017384
Iteration 145/1000 | Loss: 0.00018709
Iteration 146/1000 | Loss: 0.00016264
Iteration 147/1000 | Loss: 0.00017916
Iteration 148/1000 | Loss: 0.00017320
Iteration 149/1000 | Loss: 0.00017533
Iteration 150/1000 | Loss: 0.00017303
Iteration 151/1000 | Loss: 0.00016349
Iteration 152/1000 | Loss: 0.00016923
Iteration 153/1000 | Loss: 0.00017349
Iteration 154/1000 | Loss: 0.00016304
Iteration 155/1000 | Loss: 0.00016119
Iteration 156/1000 | Loss: 0.00016014
Iteration 157/1000 | Loss: 0.00043707
Iteration 158/1000 | Loss: 0.00066316
Iteration 159/1000 | Loss: 0.00056560
Iteration 160/1000 | Loss: 0.00056605
Iteration 161/1000 | Loss: 0.00042563
Iteration 162/1000 | Loss: 0.00029909
Iteration 163/1000 | Loss: 0.00019737
Iteration 164/1000 | Loss: 0.00016343
Iteration 165/1000 | Loss: 0.00019315
Iteration 166/1000 | Loss: 0.00015857
Iteration 167/1000 | Loss: 0.00015695
Iteration 168/1000 | Loss: 0.00015614
Iteration 169/1000 | Loss: 0.00015556
Iteration 170/1000 | Loss: 0.00015520
Iteration 171/1000 | Loss: 0.00015499
Iteration 172/1000 | Loss: 0.00015474
Iteration 173/1000 | Loss: 0.00015453
Iteration 174/1000 | Loss: 0.00015435
Iteration 175/1000 | Loss: 0.00015433
Iteration 176/1000 | Loss: 0.00015430
Iteration 177/1000 | Loss: 0.00015430
Iteration 178/1000 | Loss: 0.00015428
Iteration 179/1000 | Loss: 0.00015426
Iteration 180/1000 | Loss: 0.00015426
Iteration 181/1000 | Loss: 0.00015425
Iteration 182/1000 | Loss: 0.00015424
Iteration 183/1000 | Loss: 0.00015423
Iteration 184/1000 | Loss: 0.00015423
Iteration 185/1000 | Loss: 0.00015423
Iteration 186/1000 | Loss: 0.00015423
Iteration 187/1000 | Loss: 0.00015422
Iteration 188/1000 | Loss: 0.00015422
Iteration 189/1000 | Loss: 0.00015422
Iteration 190/1000 | Loss: 0.00015422
Iteration 191/1000 | Loss: 0.00015422
Iteration 192/1000 | Loss: 0.00015422
Iteration 193/1000 | Loss: 0.00015421
Iteration 194/1000 | Loss: 0.00015421
Iteration 195/1000 | Loss: 0.00015420
Iteration 196/1000 | Loss: 0.00015420
Iteration 197/1000 | Loss: 0.00015420
Iteration 198/1000 | Loss: 0.00015420
Iteration 199/1000 | Loss: 0.00015420
Iteration 200/1000 | Loss: 0.00015419
Iteration 201/1000 | Loss: 0.00015419
Iteration 202/1000 | Loss: 0.00015419
Iteration 203/1000 | Loss: 0.00015419
Iteration 204/1000 | Loss: 0.00015419
Iteration 205/1000 | Loss: 0.00015418
Iteration 206/1000 | Loss: 0.00015418
Iteration 207/1000 | Loss: 0.00015418
Iteration 208/1000 | Loss: 0.00015418
Iteration 209/1000 | Loss: 0.00015418
Iteration 210/1000 | Loss: 0.00015418
Iteration 211/1000 | Loss: 0.00015417
Iteration 212/1000 | Loss: 0.00015417
Iteration 213/1000 | Loss: 0.00015417
Iteration 214/1000 | Loss: 0.00015417
Iteration 215/1000 | Loss: 0.00015417
Iteration 216/1000 | Loss: 0.00015416
Iteration 217/1000 | Loss: 0.00015416
Iteration 218/1000 | Loss: 0.00015416
Iteration 219/1000 | Loss: 0.00015416
Iteration 220/1000 | Loss: 0.00015416
Iteration 221/1000 | Loss: 0.00015416
Iteration 222/1000 | Loss: 0.00015416
Iteration 223/1000 | Loss: 0.00015416
Iteration 224/1000 | Loss: 0.00015416
Iteration 225/1000 | Loss: 0.00015416
Iteration 226/1000 | Loss: 0.00015416
Iteration 227/1000 | Loss: 0.00015416
Iteration 228/1000 | Loss: 0.00015416
Iteration 229/1000 | Loss: 0.00015416
Iteration 230/1000 | Loss: 0.00015416
Iteration 231/1000 | Loss: 0.00015416
Iteration 232/1000 | Loss: 0.00015416
Iteration 233/1000 | Loss: 0.00015416
Iteration 234/1000 | Loss: 0.00015416
Iteration 235/1000 | Loss: 0.00015416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [0.00015415638335980475, 0.00015415638335980475, 0.00015415638335980475, 0.00015415638335980475, 0.00015415638335980475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015415638335980475

Optimization complete. Final v2v error: 6.378828048706055 mm

Highest mean error: 13.66472053527832 mm for frame 66

Lowest mean error: 3.593350410461426 mm for frame 105

Saving results

Total time: 305.4906997680664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103461
Iteration 2/25 | Loss: 0.00170095
Iteration 3/25 | Loss: 0.00129441
Iteration 4/25 | Loss: 0.00121071
Iteration 5/25 | Loss: 0.00118303
Iteration 6/25 | Loss: 0.00117330
Iteration 7/25 | Loss: 0.00114757
Iteration 8/25 | Loss: 0.00113288
Iteration 9/25 | Loss: 0.00111583
Iteration 10/25 | Loss: 0.00110904
Iteration 11/25 | Loss: 0.00111028
Iteration 12/25 | Loss: 0.00110431
Iteration 13/25 | Loss: 0.00110186
Iteration 14/25 | Loss: 0.00110755
Iteration 15/25 | Loss: 0.00109943
Iteration 16/25 | Loss: 0.00110186
Iteration 17/25 | Loss: 0.00109460
Iteration 18/25 | Loss: 0.00109105
Iteration 19/25 | Loss: 0.00108990
Iteration 20/25 | Loss: 0.00108960
Iteration 21/25 | Loss: 0.00109048
Iteration 22/25 | Loss: 0.00108976
Iteration 23/25 | Loss: 0.00108936
Iteration 24/25 | Loss: 0.00108931
Iteration 25/25 | Loss: 0.00108931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55245769
Iteration 2/25 | Loss: 0.00055661
Iteration 3/25 | Loss: 0.00055661
Iteration 4/25 | Loss: 0.00055661
Iteration 5/25 | Loss: 0.00055661
Iteration 6/25 | Loss: 0.00055661
Iteration 7/25 | Loss: 0.00055661
Iteration 8/25 | Loss: 0.00055661
Iteration 9/25 | Loss: 0.00055661
Iteration 10/25 | Loss: 0.00055661
Iteration 11/25 | Loss: 0.00055661
Iteration 12/25 | Loss: 0.00055661
Iteration 13/25 | Loss: 0.00055661
Iteration 14/25 | Loss: 0.00055661
Iteration 15/25 | Loss: 0.00055661
Iteration 16/25 | Loss: 0.00055661
Iteration 17/25 | Loss: 0.00055661
Iteration 18/25 | Loss: 0.00055661
Iteration 19/25 | Loss: 0.00055661
Iteration 20/25 | Loss: 0.00055661
Iteration 21/25 | Loss: 0.00055661
Iteration 22/25 | Loss: 0.00055661
Iteration 23/25 | Loss: 0.00055661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005566094187088311, 0.0005566094187088311, 0.0005566094187088311, 0.0005566094187088311, 0.0005566094187088311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005566094187088311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055661
Iteration 2/1000 | Loss: 0.00003513
Iteration 3/1000 | Loss: 0.00002431
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00048498
Iteration 10/1000 | Loss: 0.00133875
Iteration 11/1000 | Loss: 0.00020815
Iteration 12/1000 | Loss: 0.00037344
Iteration 13/1000 | Loss: 0.00002705
Iteration 14/1000 | Loss: 0.00060385
Iteration 15/1000 | Loss: 0.00115917
Iteration 16/1000 | Loss: 0.00008431
Iteration 17/1000 | Loss: 0.00104129
Iteration 18/1000 | Loss: 0.00008318
Iteration 19/1000 | Loss: 0.00002432
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001604
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001514
Iteration 27/1000 | Loss: 0.00001510
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001504
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001488
Iteration 40/1000 | Loss: 0.00001488
Iteration 41/1000 | Loss: 0.00001487
Iteration 42/1000 | Loss: 0.00001486
Iteration 43/1000 | Loss: 0.00001486
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001485
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001484
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001484
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001482
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001482
Iteration 64/1000 | Loss: 0.00001482
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001477
Iteration 80/1000 | Loss: 0.00001477
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001476
Iteration 87/1000 | Loss: 0.00001476
Iteration 88/1000 | Loss: 0.00001476
Iteration 89/1000 | Loss: 0.00001476
Iteration 90/1000 | Loss: 0.00001476
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Iteration 93/1000 | Loss: 0.00001475
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Iteration 150/1000 | Loss: 0.00001470
Iteration 151/1000 | Loss: 0.00001470
Iteration 152/1000 | Loss: 0.00001470
Iteration 153/1000 | Loss: 0.00001470
Iteration 154/1000 | Loss: 0.00001470
Iteration 155/1000 | Loss: 0.00001470
Iteration 156/1000 | Loss: 0.00001470
Iteration 157/1000 | Loss: 0.00001470
Iteration 158/1000 | Loss: 0.00001470
Iteration 159/1000 | Loss: 0.00001470
Iteration 160/1000 | Loss: 0.00001470
Iteration 161/1000 | Loss: 0.00001470
Iteration 162/1000 | Loss: 0.00001470
Iteration 163/1000 | Loss: 0.00001470
Iteration 164/1000 | Loss: 0.00001470
Iteration 165/1000 | Loss: 0.00001470
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001470
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001470
Iteration 177/1000 | Loss: 0.00001470
Iteration 178/1000 | Loss: 0.00001470
Iteration 179/1000 | Loss: 0.00001470
Iteration 180/1000 | Loss: 0.00001470
Iteration 181/1000 | Loss: 0.00001470
Iteration 182/1000 | Loss: 0.00001470
Iteration 183/1000 | Loss: 0.00001470
Iteration 184/1000 | Loss: 0.00001470
Iteration 185/1000 | Loss: 0.00001470
Iteration 186/1000 | Loss: 0.00001470
Iteration 187/1000 | Loss: 0.00001470
Iteration 188/1000 | Loss: 0.00001470
Iteration 189/1000 | Loss: 0.00001470
Iteration 190/1000 | Loss: 0.00001470
Iteration 191/1000 | Loss: 0.00001470
Iteration 192/1000 | Loss: 0.00001470
Iteration 193/1000 | Loss: 0.00001470
Iteration 194/1000 | Loss: 0.00001470
Iteration 195/1000 | Loss: 0.00001470
Iteration 196/1000 | Loss: 0.00001470
Iteration 197/1000 | Loss: 0.00001470
Iteration 198/1000 | Loss: 0.00001470
Iteration 199/1000 | Loss: 0.00001470
Iteration 200/1000 | Loss: 0.00001470
Iteration 201/1000 | Loss: 0.00001470
Iteration 202/1000 | Loss: 0.00001470
Iteration 203/1000 | Loss: 0.00001470
Iteration 204/1000 | Loss: 0.00001470
Iteration 205/1000 | Loss: 0.00001470
Iteration 206/1000 | Loss: 0.00001470
Iteration 207/1000 | Loss: 0.00001470
Iteration 208/1000 | Loss: 0.00001470
Iteration 209/1000 | Loss: 0.00001470
Iteration 210/1000 | Loss: 0.00001470
Iteration 211/1000 | Loss: 0.00001470
Iteration 212/1000 | Loss: 0.00001470
Iteration 213/1000 | Loss: 0.00001470
Iteration 214/1000 | Loss: 0.00001470
Iteration 215/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.4695186109747738e-05, 1.4695186109747738e-05, 1.4695186109747738e-05, 1.4695186109747738e-05, 1.4695186109747738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4695186109747738e-05

Optimization complete. Final v2v error: 3.2703990936279297 mm

Highest mean error: 9.435802459716797 mm for frame 19

Lowest mean error: 2.831381320953369 mm for frame 128

Saving results

Total time: 87.62127184867859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648756
Iteration 2/25 | Loss: 0.00124913
Iteration 3/25 | Loss: 0.00113260
Iteration 4/25 | Loss: 0.00111872
Iteration 5/25 | Loss: 0.00111505
Iteration 6/25 | Loss: 0.00111443
Iteration 7/25 | Loss: 0.00111443
Iteration 8/25 | Loss: 0.00111443
Iteration 9/25 | Loss: 0.00111443
Iteration 10/25 | Loss: 0.00111443
Iteration 11/25 | Loss: 0.00111443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001114428392611444, 0.001114428392611444, 0.001114428392611444, 0.001114428392611444, 0.001114428392611444]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001114428392611444

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.20688677
Iteration 2/25 | Loss: 0.00052704
Iteration 3/25 | Loss: 0.00052704
Iteration 4/25 | Loss: 0.00052704
Iteration 5/25 | Loss: 0.00052704
Iteration 6/25 | Loss: 0.00052704
Iteration 7/25 | Loss: 0.00052704
Iteration 8/25 | Loss: 0.00052704
Iteration 9/25 | Loss: 0.00052704
Iteration 10/25 | Loss: 0.00052704
Iteration 11/25 | Loss: 0.00052704
Iteration 12/25 | Loss: 0.00052704
Iteration 13/25 | Loss: 0.00052704
Iteration 14/25 | Loss: 0.00052704
Iteration 15/25 | Loss: 0.00052704
Iteration 16/25 | Loss: 0.00052704
Iteration 17/25 | Loss: 0.00052704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005270378896966577, 0.0005270378896966577, 0.0005270378896966577, 0.0005270378896966577, 0.0005270378896966577]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005270378896966577

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052704
Iteration 2/1000 | Loss: 0.00003375
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001690
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001585
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001580
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001570
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001567
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001566
Iteration 45/1000 | Loss: 0.00001566
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001566
Iteration 53/1000 | Loss: 0.00001566
Iteration 54/1000 | Loss: 0.00001565
Iteration 55/1000 | Loss: 0.00001565
Iteration 56/1000 | Loss: 0.00001565
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001563
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001560
Iteration 79/1000 | Loss: 0.00001560
Iteration 80/1000 | Loss: 0.00001560
Iteration 81/1000 | Loss: 0.00001560
Iteration 82/1000 | Loss: 0.00001559
Iteration 83/1000 | Loss: 0.00001559
Iteration 84/1000 | Loss: 0.00001559
Iteration 85/1000 | Loss: 0.00001559
Iteration 86/1000 | Loss: 0.00001558
Iteration 87/1000 | Loss: 0.00001558
Iteration 88/1000 | Loss: 0.00001558
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001558
Iteration 92/1000 | Loss: 0.00001558
Iteration 93/1000 | Loss: 0.00001558
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001558
Iteration 98/1000 | Loss: 0.00001558
Iteration 99/1000 | Loss: 0.00001558
Iteration 100/1000 | Loss: 0.00001558
Iteration 101/1000 | Loss: 0.00001558
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5575507859466597e-05, 1.5575507859466597e-05, 1.5575507859466597e-05, 1.5575507859466597e-05, 1.5575507859466597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5575507859466597e-05

Optimization complete. Final v2v error: 3.4187276363372803 mm

Highest mean error: 3.762728691101074 mm for frame 43

Lowest mean error: 3.0197935104370117 mm for frame 135

Saving results

Total time: 27.188135147094727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980762
Iteration 2/25 | Loss: 0.00187904
Iteration 3/25 | Loss: 0.00144413
Iteration 4/25 | Loss: 0.00137699
Iteration 5/25 | Loss: 0.00135851
Iteration 6/25 | Loss: 0.00136112
Iteration 7/25 | Loss: 0.00136053
Iteration 8/25 | Loss: 0.00134892
Iteration 9/25 | Loss: 0.00134045
Iteration 10/25 | Loss: 0.00133664
Iteration 11/25 | Loss: 0.00133487
Iteration 12/25 | Loss: 0.00135525
Iteration 13/25 | Loss: 0.00136990
Iteration 14/25 | Loss: 0.00131728
Iteration 15/25 | Loss: 0.00138099
Iteration 16/25 | Loss: 0.00144850
Iteration 17/25 | Loss: 0.00139370
Iteration 18/25 | Loss: 0.00133928
Iteration 19/25 | Loss: 0.00129402
Iteration 20/25 | Loss: 0.00129358
Iteration 21/25 | Loss: 0.00132639
Iteration 22/25 | Loss: 0.00128202
Iteration 23/25 | Loss: 0.00127511
Iteration 24/25 | Loss: 0.00127642
Iteration 25/25 | Loss: 0.00127629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48094583
Iteration 2/25 | Loss: 0.00106792
Iteration 3/25 | Loss: 0.00106774
Iteration 4/25 | Loss: 0.00106774
Iteration 5/25 | Loss: 0.00106774
Iteration 6/25 | Loss: 0.00106773
Iteration 7/25 | Loss: 0.00106773
Iteration 8/25 | Loss: 0.00106773
Iteration 9/25 | Loss: 0.00106773
Iteration 10/25 | Loss: 0.00106773
Iteration 11/25 | Loss: 0.00106773
Iteration 12/25 | Loss: 0.00106773
Iteration 13/25 | Loss: 0.00106773
Iteration 14/25 | Loss: 0.00106773
Iteration 15/25 | Loss: 0.00106773
Iteration 16/25 | Loss: 0.00106773
Iteration 17/25 | Loss: 0.00106773
Iteration 18/25 | Loss: 0.00106773
Iteration 19/25 | Loss: 0.00106773
Iteration 20/25 | Loss: 0.00106773
Iteration 21/25 | Loss: 0.00106773
Iteration 22/25 | Loss: 0.00106773
Iteration 23/25 | Loss: 0.00106773
Iteration 24/25 | Loss: 0.00106773
Iteration 25/25 | Loss: 0.00106773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106773
Iteration 2/1000 | Loss: 0.00826708
Iteration 3/1000 | Loss: 0.00029503
Iteration 4/1000 | Loss: 0.00009351
Iteration 5/1000 | Loss: 0.00027731
Iteration 6/1000 | Loss: 0.00025325
Iteration 7/1000 | Loss: 0.00023636
Iteration 8/1000 | Loss: 0.00005734
Iteration 9/1000 | Loss: 0.00022783
Iteration 10/1000 | Loss: 0.00016722
Iteration 11/1000 | Loss: 0.00024727
Iteration 12/1000 | Loss: 0.00008835
Iteration 13/1000 | Loss: 0.00021718
Iteration 14/1000 | Loss: 0.00017647
Iteration 15/1000 | Loss: 0.00029746
Iteration 16/1000 | Loss: 0.00016030
Iteration 17/1000 | Loss: 0.00028477
Iteration 18/1000 | Loss: 0.00005258
Iteration 19/1000 | Loss: 0.00004491
Iteration 20/1000 | Loss: 0.00004221
Iteration 21/1000 | Loss: 0.00004054
Iteration 22/1000 | Loss: 0.00003943
Iteration 23/1000 | Loss: 0.00003878
Iteration 24/1000 | Loss: 0.00171580
Iteration 25/1000 | Loss: 0.00087640
Iteration 26/1000 | Loss: 0.00004172
Iteration 27/1000 | Loss: 0.00003882
Iteration 28/1000 | Loss: 0.00199134
Iteration 29/1000 | Loss: 0.00302317
Iteration 30/1000 | Loss: 0.00236114
Iteration 31/1000 | Loss: 0.00371659
Iteration 32/1000 | Loss: 0.00228014
Iteration 33/1000 | Loss: 0.00310233
Iteration 34/1000 | Loss: 0.00006778
Iteration 35/1000 | Loss: 0.00156657
Iteration 36/1000 | Loss: 0.00260300
Iteration 37/1000 | Loss: 0.00073747
Iteration 38/1000 | Loss: 0.00165731
Iteration 39/1000 | Loss: 0.00099365
Iteration 40/1000 | Loss: 0.00148702
Iteration 41/1000 | Loss: 0.00140046
Iteration 42/1000 | Loss: 0.00187937
Iteration 43/1000 | Loss: 0.00274698
Iteration 44/1000 | Loss: 0.00168625
Iteration 45/1000 | Loss: 0.00236728
Iteration 46/1000 | Loss: 0.00106738
Iteration 47/1000 | Loss: 0.00146095
Iteration 48/1000 | Loss: 0.00143212
Iteration 49/1000 | Loss: 0.00292024
Iteration 50/1000 | Loss: 0.00161476
Iteration 51/1000 | Loss: 0.00182910
Iteration 52/1000 | Loss: 0.00158911
Iteration 53/1000 | Loss: 0.00244171
Iteration 54/1000 | Loss: 0.00252831
Iteration 55/1000 | Loss: 0.00341164
Iteration 56/1000 | Loss: 0.00413999
Iteration 57/1000 | Loss: 0.00171864
Iteration 58/1000 | Loss: 0.00056292
Iteration 59/1000 | Loss: 0.00028252
Iteration 60/1000 | Loss: 0.00009241
Iteration 61/1000 | Loss: 0.00040375
Iteration 62/1000 | Loss: 0.00010947
Iteration 63/1000 | Loss: 0.00013303
Iteration 64/1000 | Loss: 0.00020961
Iteration 65/1000 | Loss: 0.00020813
Iteration 66/1000 | Loss: 0.00020617
Iteration 67/1000 | Loss: 0.00052671
Iteration 68/1000 | Loss: 0.00006721
Iteration 69/1000 | Loss: 0.00004497
Iteration 70/1000 | Loss: 0.00003693
Iteration 71/1000 | Loss: 0.00003386
Iteration 72/1000 | Loss: 0.00003189
Iteration 73/1000 | Loss: 0.00003086
Iteration 74/1000 | Loss: 0.00002960
Iteration 75/1000 | Loss: 0.00002902
Iteration 76/1000 | Loss: 0.00002851
Iteration 77/1000 | Loss: 0.00002818
Iteration 78/1000 | Loss: 0.00002793
Iteration 79/1000 | Loss: 0.00002777
Iteration 80/1000 | Loss: 0.00002773
Iteration 81/1000 | Loss: 0.00002766
Iteration 82/1000 | Loss: 0.00002764
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002761
Iteration 87/1000 | Loss: 0.00002760
Iteration 88/1000 | Loss: 0.00002757
Iteration 89/1000 | Loss: 0.00002756
Iteration 90/1000 | Loss: 0.00002756
Iteration 91/1000 | Loss: 0.00002755
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002754
Iteration 95/1000 | Loss: 0.00002754
Iteration 96/1000 | Loss: 0.00002750
Iteration 97/1000 | Loss: 0.00002743
Iteration 98/1000 | Loss: 0.00002743
Iteration 99/1000 | Loss: 0.00002743
Iteration 100/1000 | Loss: 0.00002741
Iteration 101/1000 | Loss: 0.00002740
Iteration 102/1000 | Loss: 0.00002740
Iteration 103/1000 | Loss: 0.00002737
Iteration 104/1000 | Loss: 0.00002736
Iteration 105/1000 | Loss: 0.00002734
Iteration 106/1000 | Loss: 0.00002734
Iteration 107/1000 | Loss: 0.00002732
Iteration 108/1000 | Loss: 0.00002732
Iteration 109/1000 | Loss: 0.00002731
Iteration 110/1000 | Loss: 0.00002731
Iteration 111/1000 | Loss: 0.00002728
Iteration 112/1000 | Loss: 0.00002727
Iteration 113/1000 | Loss: 0.00002727
Iteration 114/1000 | Loss: 0.00002727
Iteration 115/1000 | Loss: 0.00002726
Iteration 116/1000 | Loss: 0.00002726
Iteration 117/1000 | Loss: 0.00002724
Iteration 118/1000 | Loss: 0.00002724
Iteration 119/1000 | Loss: 0.00002724
Iteration 120/1000 | Loss: 0.00002724
Iteration 121/1000 | Loss: 0.00002723
Iteration 122/1000 | Loss: 0.00002723
Iteration 123/1000 | Loss: 0.00002723
Iteration 124/1000 | Loss: 0.00002723
Iteration 125/1000 | Loss: 0.00002722
Iteration 126/1000 | Loss: 0.00002722
Iteration 127/1000 | Loss: 0.00002722
Iteration 128/1000 | Loss: 0.00002721
Iteration 129/1000 | Loss: 0.00002721
Iteration 130/1000 | Loss: 0.00002721
Iteration 131/1000 | Loss: 0.00002721
Iteration 132/1000 | Loss: 0.00002721
Iteration 133/1000 | Loss: 0.00002721
Iteration 134/1000 | Loss: 0.00002721
Iteration 135/1000 | Loss: 0.00002721
Iteration 136/1000 | Loss: 0.00002721
Iteration 137/1000 | Loss: 0.00002720
Iteration 138/1000 | Loss: 0.00002720
Iteration 139/1000 | Loss: 0.00002720
Iteration 140/1000 | Loss: 0.00002720
Iteration 141/1000 | Loss: 0.00002720
Iteration 142/1000 | Loss: 0.00002720
Iteration 143/1000 | Loss: 0.00002720
Iteration 144/1000 | Loss: 0.00002720
Iteration 145/1000 | Loss: 0.00002720
Iteration 146/1000 | Loss: 0.00002719
Iteration 147/1000 | Loss: 0.00002719
Iteration 148/1000 | Loss: 0.00002719
Iteration 149/1000 | Loss: 0.00002719
Iteration 150/1000 | Loss: 0.00002719
Iteration 151/1000 | Loss: 0.00002718
Iteration 152/1000 | Loss: 0.00002718
Iteration 153/1000 | Loss: 0.00002718
Iteration 154/1000 | Loss: 0.00002718
Iteration 155/1000 | Loss: 0.00002718
Iteration 156/1000 | Loss: 0.00002718
Iteration 157/1000 | Loss: 0.00002718
Iteration 158/1000 | Loss: 0.00002718
Iteration 159/1000 | Loss: 0.00002718
Iteration 160/1000 | Loss: 0.00002718
Iteration 161/1000 | Loss: 0.00002718
Iteration 162/1000 | Loss: 0.00002717
Iteration 163/1000 | Loss: 0.00002717
Iteration 164/1000 | Loss: 0.00002717
Iteration 165/1000 | Loss: 0.00002717
Iteration 166/1000 | Loss: 0.00002716
Iteration 167/1000 | Loss: 0.00002716
Iteration 168/1000 | Loss: 0.00002716
Iteration 169/1000 | Loss: 0.00002716
Iteration 170/1000 | Loss: 0.00002715
Iteration 171/1000 | Loss: 0.00002715
Iteration 172/1000 | Loss: 0.00002714
Iteration 173/1000 | Loss: 0.00002714
Iteration 174/1000 | Loss: 0.00002714
Iteration 175/1000 | Loss: 0.00002713
Iteration 176/1000 | Loss: 0.00002713
Iteration 177/1000 | Loss: 0.00002713
Iteration 178/1000 | Loss: 0.00002713
Iteration 179/1000 | Loss: 0.00002713
Iteration 180/1000 | Loss: 0.00002712
Iteration 181/1000 | Loss: 0.00002712
Iteration 182/1000 | Loss: 0.00002712
Iteration 183/1000 | Loss: 0.00002712
Iteration 184/1000 | Loss: 0.00002711
Iteration 185/1000 | Loss: 0.00002711
Iteration 186/1000 | Loss: 0.00002711
Iteration 187/1000 | Loss: 0.00002711
Iteration 188/1000 | Loss: 0.00002711
Iteration 189/1000 | Loss: 0.00002710
Iteration 190/1000 | Loss: 0.00002710
Iteration 191/1000 | Loss: 0.00002710
Iteration 192/1000 | Loss: 0.00002710
Iteration 193/1000 | Loss: 0.00002709
Iteration 194/1000 | Loss: 0.00002709
Iteration 195/1000 | Loss: 0.00002709
Iteration 196/1000 | Loss: 0.00002709
Iteration 197/1000 | Loss: 0.00002708
Iteration 198/1000 | Loss: 0.00002708
Iteration 199/1000 | Loss: 0.00002708
Iteration 200/1000 | Loss: 0.00002707
Iteration 201/1000 | Loss: 0.00002707
Iteration 202/1000 | Loss: 0.00002707
Iteration 203/1000 | Loss: 0.00002707
Iteration 204/1000 | Loss: 0.00002707
Iteration 205/1000 | Loss: 0.00002706
Iteration 206/1000 | Loss: 0.00002706
Iteration 207/1000 | Loss: 0.00002706
Iteration 208/1000 | Loss: 0.00002706
Iteration 209/1000 | Loss: 0.00002706
Iteration 210/1000 | Loss: 0.00002705
Iteration 211/1000 | Loss: 0.00002705
Iteration 212/1000 | Loss: 0.00002705
Iteration 213/1000 | Loss: 0.00002705
Iteration 214/1000 | Loss: 0.00002705
Iteration 215/1000 | Loss: 0.00002705
Iteration 216/1000 | Loss: 0.00002705
Iteration 217/1000 | Loss: 0.00002704
Iteration 218/1000 | Loss: 0.00002704
Iteration 219/1000 | Loss: 0.00002704
Iteration 220/1000 | Loss: 0.00002704
Iteration 221/1000 | Loss: 0.00002704
Iteration 222/1000 | Loss: 0.00002704
Iteration 223/1000 | Loss: 0.00002704
Iteration 224/1000 | Loss: 0.00002704
Iteration 225/1000 | Loss: 0.00002704
Iteration 226/1000 | Loss: 0.00002703
Iteration 227/1000 | Loss: 0.00002703
Iteration 228/1000 | Loss: 0.00002703
Iteration 229/1000 | Loss: 0.00002703
Iteration 230/1000 | Loss: 0.00002703
Iteration 231/1000 | Loss: 0.00002703
Iteration 232/1000 | Loss: 0.00002703
Iteration 233/1000 | Loss: 0.00002703
Iteration 234/1000 | Loss: 0.00002703
Iteration 235/1000 | Loss: 0.00002702
Iteration 236/1000 | Loss: 0.00002702
Iteration 237/1000 | Loss: 0.00002702
Iteration 238/1000 | Loss: 0.00002702
Iteration 239/1000 | Loss: 0.00002702
Iteration 240/1000 | Loss: 0.00002702
Iteration 241/1000 | Loss: 0.00002702
Iteration 242/1000 | Loss: 0.00002702
Iteration 243/1000 | Loss: 0.00002702
Iteration 244/1000 | Loss: 0.00002702
Iteration 245/1000 | Loss: 0.00002702
Iteration 246/1000 | Loss: 0.00002702
Iteration 247/1000 | Loss: 0.00002702
Iteration 248/1000 | Loss: 0.00002702
Iteration 249/1000 | Loss: 0.00002702
Iteration 250/1000 | Loss: 0.00002702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.7019017579732463e-05, 2.7019017579732463e-05, 2.7019017579732463e-05, 2.7019017579732463e-05, 2.7019017579732463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7019017579732463e-05

Optimization complete. Final v2v error: 4.294179916381836 mm

Highest mean error: 6.195074081420898 mm for frame 56

Lowest mean error: 3.374666690826416 mm for frame 141

Saving results

Total time: 174.39391255378723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565996
Iteration 2/25 | Loss: 0.00152835
Iteration 3/25 | Loss: 0.00124977
Iteration 4/25 | Loss: 0.00122115
Iteration 5/25 | Loss: 0.00121485
Iteration 6/25 | Loss: 0.00121350
Iteration 7/25 | Loss: 0.00121350
Iteration 8/25 | Loss: 0.00121350
Iteration 9/25 | Loss: 0.00121350
Iteration 10/25 | Loss: 0.00121350
Iteration 11/25 | Loss: 0.00121350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012135006254538894, 0.0012135006254538894, 0.0012135006254538894, 0.0012135006254538894, 0.0012135006254538894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012135006254538894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46307409
Iteration 2/25 | Loss: 0.00059867
Iteration 3/25 | Loss: 0.00059865
Iteration 4/25 | Loss: 0.00059865
Iteration 5/25 | Loss: 0.00059865
Iteration 6/25 | Loss: 0.00059865
Iteration 7/25 | Loss: 0.00059865
Iteration 8/25 | Loss: 0.00059865
Iteration 9/25 | Loss: 0.00059865
Iteration 10/25 | Loss: 0.00059865
Iteration 11/25 | Loss: 0.00059865
Iteration 12/25 | Loss: 0.00059865
Iteration 13/25 | Loss: 0.00059865
Iteration 14/25 | Loss: 0.00059865
Iteration 15/25 | Loss: 0.00059865
Iteration 16/25 | Loss: 0.00059865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005986497853882611, 0.0005986497853882611, 0.0005986497853882611, 0.0005986497853882611, 0.0005986497853882611]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005986497853882611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059865
Iteration 2/1000 | Loss: 0.00003832
Iteration 3/1000 | Loss: 0.00002844
Iteration 4/1000 | Loss: 0.00002586
Iteration 5/1000 | Loss: 0.00002455
Iteration 6/1000 | Loss: 0.00002366
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002232
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002196
Iteration 12/1000 | Loss: 0.00002191
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002186
Iteration 15/1000 | Loss: 0.00002186
Iteration 16/1000 | Loss: 0.00002185
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002183
Iteration 21/1000 | Loss: 0.00002183
Iteration 22/1000 | Loss: 0.00002183
Iteration 23/1000 | Loss: 0.00002182
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002181
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002181
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002181
Iteration 31/1000 | Loss: 0.00002181
Iteration 32/1000 | Loss: 0.00002181
Iteration 33/1000 | Loss: 0.00002180
Iteration 34/1000 | Loss: 0.00002180
Iteration 35/1000 | Loss: 0.00002180
Iteration 36/1000 | Loss: 0.00002179
Iteration 37/1000 | Loss: 0.00002179
Iteration 38/1000 | Loss: 0.00002179
Iteration 39/1000 | Loss: 0.00002179
Iteration 40/1000 | Loss: 0.00002178
Iteration 41/1000 | Loss: 0.00002178
Iteration 42/1000 | Loss: 0.00002178
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00002177
Iteration 45/1000 | Loss: 0.00002177
Iteration 46/1000 | Loss: 0.00002177
Iteration 47/1000 | Loss: 0.00002177
Iteration 48/1000 | Loss: 0.00002177
Iteration 49/1000 | Loss: 0.00002177
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002176
Iteration 53/1000 | Loss: 0.00002176
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002176
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002174
Iteration 65/1000 | Loss: 0.00002174
Iteration 66/1000 | Loss: 0.00002174
Iteration 67/1000 | Loss: 0.00002174
Iteration 68/1000 | Loss: 0.00002174
Iteration 69/1000 | Loss: 0.00002174
Iteration 70/1000 | Loss: 0.00002174
Iteration 71/1000 | Loss: 0.00002174
Iteration 72/1000 | Loss: 0.00002173
Iteration 73/1000 | Loss: 0.00002173
Iteration 74/1000 | Loss: 0.00002173
Iteration 75/1000 | Loss: 0.00002173
Iteration 76/1000 | Loss: 0.00002173
Iteration 77/1000 | Loss: 0.00002173
Iteration 78/1000 | Loss: 0.00002173
Iteration 79/1000 | Loss: 0.00002173
Iteration 80/1000 | Loss: 0.00002173
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002173
Iteration 87/1000 | Loss: 0.00002173
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002172
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002172
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002172
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002171
Iteration 122/1000 | Loss: 0.00002171
Iteration 123/1000 | Loss: 0.00002171
Iteration 124/1000 | Loss: 0.00002171
Iteration 125/1000 | Loss: 0.00002171
Iteration 126/1000 | Loss: 0.00002171
Iteration 127/1000 | Loss: 0.00002171
Iteration 128/1000 | Loss: 0.00002171
Iteration 129/1000 | Loss: 0.00002171
Iteration 130/1000 | Loss: 0.00002171
Iteration 131/1000 | Loss: 0.00002171
Iteration 132/1000 | Loss: 0.00002171
Iteration 133/1000 | Loss: 0.00002171
Iteration 134/1000 | Loss: 0.00002171
Iteration 135/1000 | Loss: 0.00002171
Iteration 136/1000 | Loss: 0.00002171
Iteration 137/1000 | Loss: 0.00002171
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002171
Iteration 141/1000 | Loss: 0.00002171
Iteration 142/1000 | Loss: 0.00002171
Iteration 143/1000 | Loss: 0.00002171
Iteration 144/1000 | Loss: 0.00002171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.1714953618356958e-05, 2.1714953618356958e-05, 2.1714953618356958e-05, 2.1714953618356958e-05, 2.1714953618356958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1714953618356958e-05

Optimization complete. Final v2v error: 4.041167259216309 mm

Highest mean error: 4.69448184967041 mm for frame 42

Lowest mean error: 3.5744123458862305 mm for frame 106

Saving results

Total time: 31.256299018859863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_us_2201/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_us_2201/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00657196
Iteration 2/25 | Loss: 0.00137068
Iteration 3/25 | Loss: 0.00115194
Iteration 4/25 | Loss: 0.00113369
Iteration 5/25 | Loss: 0.00113152
Iteration 6/25 | Loss: 0.00113111
Iteration 7/25 | Loss: 0.00113111
Iteration 8/25 | Loss: 0.00113111
Iteration 9/25 | Loss: 0.00113111
Iteration 10/25 | Loss: 0.00113111
Iteration 11/25 | Loss: 0.00113111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001131108496338129, 0.001131108496338129, 0.001131108496338129, 0.001131108496338129, 0.001131108496338129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001131108496338129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.22431421
Iteration 2/25 | Loss: 0.00064621
Iteration 3/25 | Loss: 0.00064614
Iteration 4/25 | Loss: 0.00064614
Iteration 5/25 | Loss: 0.00064614
Iteration 6/25 | Loss: 0.00064614
Iteration 7/25 | Loss: 0.00064614
Iteration 8/25 | Loss: 0.00064614
Iteration 9/25 | Loss: 0.00064614
Iteration 10/25 | Loss: 0.00064614
Iteration 11/25 | Loss: 0.00064614
Iteration 12/25 | Loss: 0.00064614
Iteration 13/25 | Loss: 0.00064614
Iteration 14/25 | Loss: 0.00064614
Iteration 15/25 | Loss: 0.00064614
Iteration 16/25 | Loss: 0.00064614
Iteration 17/25 | Loss: 0.00064614
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00064614147413522, 0.00064614147413522, 0.00064614147413522, 0.00064614147413522, 0.00064614147413522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00064614147413522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064614
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00002103
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001784
Iteration 6/1000 | Loss: 0.00001722
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001657
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001631
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001629
Iteration 17/1000 | Loss: 0.00001628
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001622
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001611
Iteration 29/1000 | Loss: 0.00001610
Iteration 30/1000 | Loss: 0.00001610
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001607
Iteration 34/1000 | Loss: 0.00001604
Iteration 35/1000 | Loss: 0.00001603
Iteration 36/1000 | Loss: 0.00001603
Iteration 37/1000 | Loss: 0.00001602
Iteration 38/1000 | Loss: 0.00001601
Iteration 39/1000 | Loss: 0.00001601
Iteration 40/1000 | Loss: 0.00001600
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001599
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001598
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001597
Iteration 51/1000 | Loss: 0.00001597
Iteration 52/1000 | Loss: 0.00001597
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001596
Iteration 55/1000 | Loss: 0.00001596
Iteration 56/1000 | Loss: 0.00001595
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001593
Iteration 61/1000 | Loss: 0.00001593
Iteration 62/1000 | Loss: 0.00001593
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001593
Iteration 65/1000 | Loss: 0.00001593
Iteration 66/1000 | Loss: 0.00001593
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001592
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001585
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001584
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001584
Iteration 101/1000 | Loss: 0.00001584
Iteration 102/1000 | Loss: 0.00001584
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001584
Iteration 131/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.583813900651876e-05, 1.583813900651876e-05, 1.583813900651876e-05, 1.583813900651876e-05, 1.583813900651876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.583813900651876e-05

Optimization complete. Final v2v error: 3.3557612895965576 mm

Highest mean error: 4.457451343536377 mm for frame 33

Lowest mean error: 2.939044952392578 mm for frame 140

Saving results

Total time: 34.37123203277588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014640
Iteration 2/25 | Loss: 0.00286576
Iteration 3/25 | Loss: 0.00213599
Iteration 4/25 | Loss: 0.00197393
Iteration 5/25 | Loss: 0.00183402
Iteration 6/25 | Loss: 0.00180126
Iteration 7/25 | Loss: 0.00176703
Iteration 8/25 | Loss: 0.00168690
Iteration 9/25 | Loss: 0.00167501
Iteration 10/25 | Loss: 0.00163909
Iteration 11/25 | Loss: 0.00163231
Iteration 12/25 | Loss: 0.00161105
Iteration 13/25 | Loss: 0.00159896
Iteration 14/25 | Loss: 0.00159582
Iteration 15/25 | Loss: 0.00157904
Iteration 16/25 | Loss: 0.00158092
Iteration 17/25 | Loss: 0.00157435
Iteration 18/25 | Loss: 0.00157304
Iteration 19/25 | Loss: 0.00156769
Iteration 20/25 | Loss: 0.00156625
Iteration 21/25 | Loss: 0.00156357
Iteration 22/25 | Loss: 0.00156342
Iteration 23/25 | Loss: 0.00156385
Iteration 24/25 | Loss: 0.00156043
Iteration 25/25 | Loss: 0.00155921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30692160
Iteration 2/25 | Loss: 0.00581860
Iteration 3/25 | Loss: 0.00393170
Iteration 4/25 | Loss: 0.00393169
Iteration 5/25 | Loss: 0.00393169
Iteration 6/25 | Loss: 0.00393169
Iteration 7/25 | Loss: 0.00393168
Iteration 8/25 | Loss: 0.00393168
Iteration 9/25 | Loss: 0.00393168
Iteration 10/25 | Loss: 0.00393168
Iteration 11/25 | Loss: 0.00393168
Iteration 12/25 | Loss: 0.00393168
Iteration 13/25 | Loss: 0.00393168
Iteration 14/25 | Loss: 0.00393168
Iteration 15/25 | Loss: 0.00393168
Iteration 16/25 | Loss: 0.00393168
Iteration 17/25 | Loss: 0.00393168
Iteration 18/25 | Loss: 0.00393168
Iteration 19/25 | Loss: 0.00393168
Iteration 20/25 | Loss: 0.00393168
Iteration 21/25 | Loss: 0.00393168
Iteration 22/25 | Loss: 0.00393168
Iteration 23/25 | Loss: 0.00393168
Iteration 24/25 | Loss: 0.00393168
Iteration 25/25 | Loss: 0.00393168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393168
Iteration 2/1000 | Loss: 0.00135068
Iteration 3/1000 | Loss: 0.00125004
Iteration 4/1000 | Loss: 0.00064383
Iteration 5/1000 | Loss: 0.00034474
Iteration 6/1000 | Loss: 0.00051344
Iteration 7/1000 | Loss: 0.00108066
Iteration 8/1000 | Loss: 0.00042423
Iteration 9/1000 | Loss: 0.00065179
Iteration 10/1000 | Loss: 0.00130391
Iteration 11/1000 | Loss: 0.00164126
Iteration 12/1000 | Loss: 0.00182032
Iteration 13/1000 | Loss: 0.00295408
Iteration 14/1000 | Loss: 0.00171661
Iteration 15/1000 | Loss: 0.00329404
Iteration 16/1000 | Loss: 0.00124909
Iteration 17/1000 | Loss: 0.00097121
Iteration 18/1000 | Loss: 0.00156097
Iteration 19/1000 | Loss: 0.00070321
Iteration 20/1000 | Loss: 0.00157078
Iteration 21/1000 | Loss: 0.00082756
Iteration 22/1000 | Loss: 0.00190860
Iteration 23/1000 | Loss: 0.00194316
Iteration 24/1000 | Loss: 0.00149470
Iteration 25/1000 | Loss: 0.00165498
Iteration 26/1000 | Loss: 0.00100090
Iteration 27/1000 | Loss: 0.00089289
Iteration 28/1000 | Loss: 0.00042118
Iteration 29/1000 | Loss: 0.00036804
Iteration 30/1000 | Loss: 0.00081199
Iteration 31/1000 | Loss: 0.00051129
Iteration 32/1000 | Loss: 0.00059720
Iteration 33/1000 | Loss: 0.00035981
Iteration 34/1000 | Loss: 0.00049230
Iteration 35/1000 | Loss: 0.00047679
Iteration 36/1000 | Loss: 0.00066093
Iteration 37/1000 | Loss: 0.00246582
Iteration 38/1000 | Loss: 0.00191600
Iteration 39/1000 | Loss: 0.00108847
Iteration 40/1000 | Loss: 0.00118113
Iteration 41/1000 | Loss: 0.00148139
Iteration 42/1000 | Loss: 0.00089838
Iteration 43/1000 | Loss: 0.00096913
Iteration 44/1000 | Loss: 0.00253274
Iteration 45/1000 | Loss: 0.00102936
Iteration 46/1000 | Loss: 0.00102400
Iteration 47/1000 | Loss: 0.00109182
Iteration 48/1000 | Loss: 0.00184940
Iteration 49/1000 | Loss: 0.00042069
Iteration 50/1000 | Loss: 0.00054853
Iteration 51/1000 | Loss: 0.00100258
Iteration 52/1000 | Loss: 0.00057804
Iteration 53/1000 | Loss: 0.00136396
Iteration 54/1000 | Loss: 0.00036379
Iteration 55/1000 | Loss: 0.00064760
Iteration 56/1000 | Loss: 0.00064230
Iteration 57/1000 | Loss: 0.00081162
Iteration 58/1000 | Loss: 0.00166648
Iteration 59/1000 | Loss: 0.00051853
Iteration 60/1000 | Loss: 0.00090986
Iteration 61/1000 | Loss: 0.00042919
Iteration 62/1000 | Loss: 0.00068997
Iteration 63/1000 | Loss: 0.00075978
Iteration 64/1000 | Loss: 0.00067546
Iteration 65/1000 | Loss: 0.00016200
Iteration 66/1000 | Loss: 0.00015470
Iteration 67/1000 | Loss: 0.00055620
Iteration 68/1000 | Loss: 0.00029345
Iteration 69/1000 | Loss: 0.00047516
Iteration 70/1000 | Loss: 0.00017142
Iteration 71/1000 | Loss: 0.00052511
Iteration 72/1000 | Loss: 0.00060252
Iteration 73/1000 | Loss: 0.00155357
Iteration 74/1000 | Loss: 0.00077691
Iteration 75/1000 | Loss: 0.00096251
Iteration 76/1000 | Loss: 0.00044404
Iteration 77/1000 | Loss: 0.00034927
Iteration 78/1000 | Loss: 0.00031655
Iteration 79/1000 | Loss: 0.00071262
Iteration 80/1000 | Loss: 0.00057826
Iteration 81/1000 | Loss: 0.00014535
Iteration 82/1000 | Loss: 0.00018890
Iteration 83/1000 | Loss: 0.00011017
Iteration 84/1000 | Loss: 0.00031519
Iteration 85/1000 | Loss: 0.00021296
Iteration 86/1000 | Loss: 0.00025636
Iteration 87/1000 | Loss: 0.00019392
Iteration 88/1000 | Loss: 0.00031198
Iteration 89/1000 | Loss: 0.00028265
Iteration 90/1000 | Loss: 0.00011692
Iteration 91/1000 | Loss: 0.00028696
Iteration 92/1000 | Loss: 0.00010386
Iteration 93/1000 | Loss: 0.00011976
Iteration 94/1000 | Loss: 0.00009640
Iteration 95/1000 | Loss: 0.00027485
Iteration 96/1000 | Loss: 0.00023038
Iteration 97/1000 | Loss: 0.00024588
Iteration 98/1000 | Loss: 0.00009953
Iteration 99/1000 | Loss: 0.00009408
Iteration 100/1000 | Loss: 0.00009513
Iteration 101/1000 | Loss: 0.00016344
Iteration 102/1000 | Loss: 0.00025261
Iteration 103/1000 | Loss: 0.00011291
Iteration 104/1000 | Loss: 0.00009172
Iteration 105/1000 | Loss: 0.00008759
Iteration 106/1000 | Loss: 0.00009243
Iteration 107/1000 | Loss: 0.00009009
Iteration 108/1000 | Loss: 0.00008659
Iteration 109/1000 | Loss: 0.00008404
Iteration 110/1000 | Loss: 0.00008334
Iteration 111/1000 | Loss: 0.00014630
Iteration 112/1000 | Loss: 0.00008264
Iteration 113/1000 | Loss: 0.00008216
Iteration 114/1000 | Loss: 0.00008174
Iteration 115/1000 | Loss: 0.00017136
Iteration 116/1000 | Loss: 0.00034873
Iteration 117/1000 | Loss: 0.00048389
Iteration 118/1000 | Loss: 0.00010569
Iteration 119/1000 | Loss: 0.00008119
Iteration 120/1000 | Loss: 0.00033095
Iteration 121/1000 | Loss: 0.00007960
Iteration 122/1000 | Loss: 0.00012343
Iteration 123/1000 | Loss: 0.00015558
Iteration 124/1000 | Loss: 0.00075583
Iteration 125/1000 | Loss: 0.00063772
Iteration 126/1000 | Loss: 0.00066812
Iteration 127/1000 | Loss: 0.00008443
Iteration 128/1000 | Loss: 0.00007799
Iteration 129/1000 | Loss: 0.00015039
Iteration 130/1000 | Loss: 0.00007515
Iteration 131/1000 | Loss: 0.00041934
Iteration 132/1000 | Loss: 0.00018057
Iteration 133/1000 | Loss: 0.00014545
Iteration 134/1000 | Loss: 0.00009869
Iteration 135/1000 | Loss: 0.00017554
Iteration 136/1000 | Loss: 0.00009909
Iteration 137/1000 | Loss: 0.00009406
Iteration 138/1000 | Loss: 0.00007144
Iteration 139/1000 | Loss: 0.00006989
Iteration 140/1000 | Loss: 0.00006859
Iteration 141/1000 | Loss: 0.00006775
Iteration 142/1000 | Loss: 0.00028716
Iteration 143/1000 | Loss: 0.00110864
Iteration 144/1000 | Loss: 0.00107495
Iteration 145/1000 | Loss: 0.00247706
Iteration 146/1000 | Loss: 0.00142647
Iteration 147/1000 | Loss: 0.00093094
Iteration 148/1000 | Loss: 0.00087171
Iteration 149/1000 | Loss: 0.00093836
Iteration 150/1000 | Loss: 0.00027119
Iteration 151/1000 | Loss: 0.00030028
Iteration 152/1000 | Loss: 0.00008810
Iteration 153/1000 | Loss: 0.00019818
Iteration 154/1000 | Loss: 0.00058446
Iteration 155/1000 | Loss: 0.00008959
Iteration 156/1000 | Loss: 0.00007277
Iteration 157/1000 | Loss: 0.00004879
Iteration 158/1000 | Loss: 0.00003987
Iteration 159/1000 | Loss: 0.00004585
Iteration 160/1000 | Loss: 0.00018593
Iteration 161/1000 | Loss: 0.00003082
Iteration 162/1000 | Loss: 0.00011771
Iteration 163/1000 | Loss: 0.00002535
Iteration 164/1000 | Loss: 0.00009760
Iteration 165/1000 | Loss: 0.00051005
Iteration 166/1000 | Loss: 0.00122788
Iteration 167/1000 | Loss: 0.00032635
Iteration 168/1000 | Loss: 0.00068694
Iteration 169/1000 | Loss: 0.00002682
Iteration 170/1000 | Loss: 0.00002101
Iteration 171/1000 | Loss: 0.00002040
Iteration 172/1000 | Loss: 0.00010497
Iteration 173/1000 | Loss: 0.00001979
Iteration 174/1000 | Loss: 0.00010605
Iteration 175/1000 | Loss: 0.00001941
Iteration 176/1000 | Loss: 0.00001901
Iteration 177/1000 | Loss: 0.00001892
Iteration 178/1000 | Loss: 0.00001891
Iteration 179/1000 | Loss: 0.00001866
Iteration 180/1000 | Loss: 0.00001859
Iteration 181/1000 | Loss: 0.00001845
Iteration 182/1000 | Loss: 0.00001843
Iteration 183/1000 | Loss: 0.00001831
Iteration 184/1000 | Loss: 0.00001828
Iteration 185/1000 | Loss: 0.00001824
Iteration 186/1000 | Loss: 0.00001824
Iteration 187/1000 | Loss: 0.00020176
Iteration 188/1000 | Loss: 0.00010035
Iteration 189/1000 | Loss: 0.00008320
Iteration 190/1000 | Loss: 0.00013147
Iteration 191/1000 | Loss: 0.00018609
Iteration 192/1000 | Loss: 0.00016379
Iteration 193/1000 | Loss: 0.00016073
Iteration 194/1000 | Loss: 0.00006887
Iteration 195/1000 | Loss: 0.00005074
Iteration 196/1000 | Loss: 0.00002571
Iteration 197/1000 | Loss: 0.00001927
Iteration 198/1000 | Loss: 0.00001878
Iteration 199/1000 | Loss: 0.00001845
Iteration 200/1000 | Loss: 0.00021897
Iteration 201/1000 | Loss: 0.00013666
Iteration 202/1000 | Loss: 0.00009374
Iteration 203/1000 | Loss: 0.00002459
Iteration 204/1000 | Loss: 0.00024261
Iteration 205/1000 | Loss: 0.00017883
Iteration 206/1000 | Loss: 0.00025263
Iteration 207/1000 | Loss: 0.00032622
Iteration 208/1000 | Loss: 0.00006148
Iteration 209/1000 | Loss: 0.00019793
Iteration 210/1000 | Loss: 0.00012735
Iteration 211/1000 | Loss: 0.00001946
Iteration 212/1000 | Loss: 0.00014963
Iteration 213/1000 | Loss: 0.00009870
Iteration 214/1000 | Loss: 0.00023011
Iteration 215/1000 | Loss: 0.00004414
Iteration 216/1000 | Loss: 0.00001961
Iteration 217/1000 | Loss: 0.00015775
Iteration 218/1000 | Loss: 0.00001832
Iteration 219/1000 | Loss: 0.00001781
Iteration 220/1000 | Loss: 0.00001745
Iteration 221/1000 | Loss: 0.00008516
Iteration 222/1000 | Loss: 0.00008902
Iteration 223/1000 | Loss: 0.00001858
Iteration 224/1000 | Loss: 0.00001702
Iteration 225/1000 | Loss: 0.00001701
Iteration 226/1000 | Loss: 0.00001952
Iteration 227/1000 | Loss: 0.00012252
Iteration 228/1000 | Loss: 0.00003550
Iteration 229/1000 | Loss: 0.00003441
Iteration 230/1000 | Loss: 0.00009655
Iteration 231/1000 | Loss: 0.00002406
Iteration 232/1000 | Loss: 0.00005262
Iteration 233/1000 | Loss: 0.00004309
Iteration 234/1000 | Loss: 0.00001732
Iteration 235/1000 | Loss: 0.00001709
Iteration 236/1000 | Loss: 0.00007266
Iteration 237/1000 | Loss: 0.00002524
Iteration 238/1000 | Loss: 0.00002233
Iteration 239/1000 | Loss: 0.00006800
Iteration 240/1000 | Loss: 0.00003170
Iteration 241/1000 | Loss: 0.00001705
Iteration 242/1000 | Loss: 0.00001688
Iteration 243/1000 | Loss: 0.00001685
Iteration 244/1000 | Loss: 0.00001680
Iteration 245/1000 | Loss: 0.00001674
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001669
Iteration 248/1000 | Loss: 0.00001660
Iteration 249/1000 | Loss: 0.00001660
Iteration 250/1000 | Loss: 0.00001659
Iteration 251/1000 | Loss: 0.00001659
Iteration 252/1000 | Loss: 0.00001658
Iteration 253/1000 | Loss: 0.00001657
Iteration 254/1000 | Loss: 0.00001657
Iteration 255/1000 | Loss: 0.00001657
Iteration 256/1000 | Loss: 0.00001657
Iteration 257/1000 | Loss: 0.00001656
Iteration 258/1000 | Loss: 0.00001656
Iteration 259/1000 | Loss: 0.00001656
Iteration 260/1000 | Loss: 0.00001656
Iteration 261/1000 | Loss: 0.00001655
Iteration 262/1000 | Loss: 0.00001655
Iteration 263/1000 | Loss: 0.00001655
Iteration 264/1000 | Loss: 0.00001655
Iteration 265/1000 | Loss: 0.00001655
Iteration 266/1000 | Loss: 0.00001655
Iteration 267/1000 | Loss: 0.00001654
Iteration 268/1000 | Loss: 0.00001654
Iteration 269/1000 | Loss: 0.00001654
Iteration 270/1000 | Loss: 0.00001654
Iteration 271/1000 | Loss: 0.00001654
Iteration 272/1000 | Loss: 0.00001654
Iteration 273/1000 | Loss: 0.00001654
Iteration 274/1000 | Loss: 0.00001654
Iteration 275/1000 | Loss: 0.00001654
Iteration 276/1000 | Loss: 0.00001653
Iteration 277/1000 | Loss: 0.00001653
Iteration 278/1000 | Loss: 0.00001653
Iteration 279/1000 | Loss: 0.00001653
Iteration 280/1000 | Loss: 0.00001653
Iteration 281/1000 | Loss: 0.00001653
Iteration 282/1000 | Loss: 0.00001653
Iteration 283/1000 | Loss: 0.00001653
Iteration 284/1000 | Loss: 0.00001653
Iteration 285/1000 | Loss: 0.00001653
Iteration 286/1000 | Loss: 0.00001653
Iteration 287/1000 | Loss: 0.00001653
Iteration 288/1000 | Loss: 0.00001653
Iteration 289/1000 | Loss: 0.00001653
Iteration 290/1000 | Loss: 0.00001652
Iteration 291/1000 | Loss: 0.00001652
Iteration 292/1000 | Loss: 0.00001652
Iteration 293/1000 | Loss: 0.00001652
Iteration 294/1000 | Loss: 0.00001652
Iteration 295/1000 | Loss: 0.00001652
Iteration 296/1000 | Loss: 0.00001652
Iteration 297/1000 | Loss: 0.00001652
Iteration 298/1000 | Loss: 0.00001652
Iteration 299/1000 | Loss: 0.00001652
Iteration 300/1000 | Loss: 0.00001652
Iteration 301/1000 | Loss: 0.00001652
Iteration 302/1000 | Loss: 0.00001652
Iteration 303/1000 | Loss: 0.00001652
Iteration 304/1000 | Loss: 0.00001652
Iteration 305/1000 | Loss: 0.00001652
Iteration 306/1000 | Loss: 0.00001651
Iteration 307/1000 | Loss: 0.00001651
Iteration 308/1000 | Loss: 0.00001651
Iteration 309/1000 | Loss: 0.00001651
Iteration 310/1000 | Loss: 0.00001651
Iteration 311/1000 | Loss: 0.00001651
Iteration 312/1000 | Loss: 0.00001651
Iteration 313/1000 | Loss: 0.00001651
Iteration 314/1000 | Loss: 0.00001651
Iteration 315/1000 | Loss: 0.00001651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.6513169612153433e-05, 1.6513169612153433e-05, 1.6513169612153433e-05, 1.6513169612153433e-05, 1.6513169612153433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6513169612153433e-05

Optimization complete. Final v2v error: 3.373645544052124 mm

Highest mean error: 10.608314514160156 mm for frame 188

Lowest mean error: 3.06256103515625 mm for frame 58

Saving results

Total time: 434.31309843063354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802061
Iteration 2/25 | Loss: 0.00157679
Iteration 3/25 | Loss: 0.00128420
Iteration 4/25 | Loss: 0.00125728
Iteration 5/25 | Loss: 0.00125316
Iteration 6/25 | Loss: 0.00125268
Iteration 7/25 | Loss: 0.00125268
Iteration 8/25 | Loss: 0.00125268
Iteration 9/25 | Loss: 0.00125268
Iteration 10/25 | Loss: 0.00125268
Iteration 11/25 | Loss: 0.00125268
Iteration 12/25 | Loss: 0.00125268
Iteration 13/25 | Loss: 0.00125268
Iteration 14/25 | Loss: 0.00125268
Iteration 15/25 | Loss: 0.00125268
Iteration 16/25 | Loss: 0.00125268
Iteration 17/25 | Loss: 0.00125268
Iteration 18/25 | Loss: 0.00125268
Iteration 19/25 | Loss: 0.00125268
Iteration 20/25 | Loss: 0.00125268
Iteration 21/25 | Loss: 0.00125268
Iteration 22/25 | Loss: 0.00125268
Iteration 23/25 | Loss: 0.00125268
Iteration 24/25 | Loss: 0.00125268
Iteration 25/25 | Loss: 0.00125268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28222632
Iteration 2/25 | Loss: 0.00079247
Iteration 3/25 | Loss: 0.00079246
Iteration 4/25 | Loss: 0.00079246
Iteration 5/25 | Loss: 0.00079246
Iteration 6/25 | Loss: 0.00079246
Iteration 7/25 | Loss: 0.00079246
Iteration 8/25 | Loss: 0.00079246
Iteration 9/25 | Loss: 0.00079246
Iteration 10/25 | Loss: 0.00079246
Iteration 11/25 | Loss: 0.00079246
Iteration 12/25 | Loss: 0.00079246
Iteration 13/25 | Loss: 0.00079246
Iteration 14/25 | Loss: 0.00079246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007924616220407188, 0.0007924616220407188, 0.0007924616220407188, 0.0007924616220407188, 0.0007924616220407188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007924616220407188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079246
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002029
Iteration 5/1000 | Loss: 0.00001945
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001784
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001657
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001588
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001579
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001571
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001568
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001566
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001553
Iteration 49/1000 | Loss: 0.00001553
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001542
Iteration 65/1000 | Loss: 0.00001541
Iteration 66/1000 | Loss: 0.00001541
Iteration 67/1000 | Loss: 0.00001540
Iteration 68/1000 | Loss: 0.00001540
Iteration 69/1000 | Loss: 0.00001539
Iteration 70/1000 | Loss: 0.00001539
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001536
Iteration 78/1000 | Loss: 0.00001535
Iteration 79/1000 | Loss: 0.00001535
Iteration 80/1000 | Loss: 0.00001535
Iteration 81/1000 | Loss: 0.00001535
Iteration 82/1000 | Loss: 0.00001535
Iteration 83/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.5351839465438388e-05, 1.5351839465438388e-05, 1.5351839465438388e-05, 1.5351839465438388e-05, 1.5351839465438388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5351839465438388e-05

Optimization complete. Final v2v error: 3.3162357807159424 mm

Highest mean error: 4.047450542449951 mm for frame 89

Lowest mean error: 2.897008180618286 mm for frame 53

Saving results

Total time: 42.978026390075684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481321
Iteration 2/25 | Loss: 0.00132119
Iteration 3/25 | Loss: 0.00124014
Iteration 4/25 | Loss: 0.00123004
Iteration 5/25 | Loss: 0.00122834
Iteration 6/25 | Loss: 0.00122834
Iteration 7/25 | Loss: 0.00122834
Iteration 8/25 | Loss: 0.00122834
Iteration 9/25 | Loss: 0.00122834
Iteration 10/25 | Loss: 0.00122834
Iteration 11/25 | Loss: 0.00122834
Iteration 12/25 | Loss: 0.00122834
Iteration 13/25 | Loss: 0.00122834
Iteration 14/25 | Loss: 0.00122834
Iteration 15/25 | Loss: 0.00122834
Iteration 16/25 | Loss: 0.00122834
Iteration 17/25 | Loss: 0.00122834
Iteration 18/25 | Loss: 0.00122834
Iteration 19/25 | Loss: 0.00122834
Iteration 20/25 | Loss: 0.00122834
Iteration 21/25 | Loss: 0.00122834
Iteration 22/25 | Loss: 0.00122834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012283411342650652, 0.0012283411342650652, 0.0012283411342650652, 0.0012283411342650652, 0.0012283411342650652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012283411342650652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.65741920
Iteration 2/25 | Loss: 0.00092675
Iteration 3/25 | Loss: 0.00092674
Iteration 4/25 | Loss: 0.00092674
Iteration 5/25 | Loss: 0.00092674
Iteration 6/25 | Loss: 0.00092674
Iteration 7/25 | Loss: 0.00092674
Iteration 8/25 | Loss: 0.00092674
Iteration 9/25 | Loss: 0.00092674
Iteration 10/25 | Loss: 0.00092674
Iteration 11/25 | Loss: 0.00092674
Iteration 12/25 | Loss: 0.00092674
Iteration 13/25 | Loss: 0.00092674
Iteration 14/25 | Loss: 0.00092674
Iteration 15/25 | Loss: 0.00092674
Iteration 16/25 | Loss: 0.00092674
Iteration 17/25 | Loss: 0.00092674
Iteration 18/25 | Loss: 0.00092674
Iteration 19/25 | Loss: 0.00092674
Iteration 20/25 | Loss: 0.00092674
Iteration 21/25 | Loss: 0.00092674
Iteration 22/25 | Loss: 0.00092674
Iteration 23/25 | Loss: 0.00092674
Iteration 24/25 | Loss: 0.00092674
Iteration 25/25 | Loss: 0.00092674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092674
Iteration 2/1000 | Loss: 0.00002231
Iteration 3/1000 | Loss: 0.00001810
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001454
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001404
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001377
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001349
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001338
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001325
Iteration 54/1000 | Loss: 0.00001325
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001324
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001322
Iteration 59/1000 | Loss: 0.00001321
Iteration 60/1000 | Loss: 0.00001321
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001320
Iteration 64/1000 | Loss: 0.00001320
Iteration 65/1000 | Loss: 0.00001320
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001319
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001317
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001315
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001313
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001312
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001312
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.312066706304904e-05, 1.312066706304904e-05, 1.312066706304904e-05, 1.312066706304904e-05, 1.312066706304904e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.312066706304904e-05

Optimization complete. Final v2v error: 3.098830461502075 mm

Highest mean error: 3.262354850769043 mm for frame 42

Lowest mean error: 2.894774913787842 mm for frame 266

Saving results

Total time: 41.88333582878113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999651
Iteration 2/25 | Loss: 0.00228329
Iteration 3/25 | Loss: 0.00207499
Iteration 4/25 | Loss: 0.00208680
Iteration 5/25 | Loss: 0.00176937
Iteration 6/25 | Loss: 0.00143446
Iteration 7/25 | Loss: 0.00140040
Iteration 8/25 | Loss: 0.00139838
Iteration 9/25 | Loss: 0.00139553
Iteration 10/25 | Loss: 0.00139457
Iteration 11/25 | Loss: 0.00139441
Iteration 12/25 | Loss: 0.00139440
Iteration 13/25 | Loss: 0.00139439
Iteration 14/25 | Loss: 0.00139439
Iteration 15/25 | Loss: 0.00139438
Iteration 16/25 | Loss: 0.00139438
Iteration 17/25 | Loss: 0.00139438
Iteration 18/25 | Loss: 0.00139438
Iteration 19/25 | Loss: 0.00139438
Iteration 20/25 | Loss: 0.00139438
Iteration 21/25 | Loss: 0.00139437
Iteration 22/25 | Loss: 0.00139437
Iteration 23/25 | Loss: 0.00139437
Iteration 24/25 | Loss: 0.00139437
Iteration 25/25 | Loss: 0.00139437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31048512
Iteration 2/25 | Loss: 0.00091575
Iteration 3/25 | Loss: 0.00091574
Iteration 4/25 | Loss: 0.00091574
Iteration 5/25 | Loss: 0.00091574
Iteration 6/25 | Loss: 0.00091574
Iteration 7/25 | Loss: 0.00091574
Iteration 8/25 | Loss: 0.00091574
Iteration 9/25 | Loss: 0.00091574
Iteration 10/25 | Loss: 0.00091574
Iteration 11/25 | Loss: 0.00091574
Iteration 12/25 | Loss: 0.00091574
Iteration 13/25 | Loss: 0.00091574
Iteration 14/25 | Loss: 0.00091574
Iteration 15/25 | Loss: 0.00091574
Iteration 16/25 | Loss: 0.00091574
Iteration 17/25 | Loss: 0.00091574
Iteration 18/25 | Loss: 0.00091574
Iteration 19/25 | Loss: 0.00091574
Iteration 20/25 | Loss: 0.00091574
Iteration 21/25 | Loss: 0.00091574
Iteration 22/25 | Loss: 0.00091574
Iteration 23/25 | Loss: 0.00091574
Iteration 24/25 | Loss: 0.00091574
Iteration 25/25 | Loss: 0.00091574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091574
Iteration 2/1000 | Loss: 0.00005331
Iteration 3/1000 | Loss: 0.00003917
Iteration 4/1000 | Loss: 0.00003685
Iteration 5/1000 | Loss: 0.00003551
Iteration 6/1000 | Loss: 0.00003477
Iteration 7/1000 | Loss: 0.00003429
Iteration 8/1000 | Loss: 0.00003392
Iteration 9/1000 | Loss: 0.00003362
Iteration 10/1000 | Loss: 0.00003341
Iteration 11/1000 | Loss: 0.00003334
Iteration 12/1000 | Loss: 0.00003409
Iteration 13/1000 | Loss: 0.00003336
Iteration 14/1000 | Loss: 0.00003314
Iteration 15/1000 | Loss: 0.00003291
Iteration 16/1000 | Loss: 0.00003268
Iteration 17/1000 | Loss: 0.00003247
Iteration 18/1000 | Loss: 0.00003241
Iteration 19/1000 | Loss: 0.00003210
Iteration 20/1000 | Loss: 0.00003207
Iteration 21/1000 | Loss: 0.00003193
Iteration 22/1000 | Loss: 0.00003191
Iteration 23/1000 | Loss: 0.00003190
Iteration 24/1000 | Loss: 0.00003188
Iteration 25/1000 | Loss: 0.00003187
Iteration 26/1000 | Loss: 0.00003179
Iteration 27/1000 | Loss: 0.00003176
Iteration 28/1000 | Loss: 0.00003171
Iteration 29/1000 | Loss: 0.00003165
Iteration 30/1000 | Loss: 0.00003165
Iteration 31/1000 | Loss: 0.00003162
Iteration 32/1000 | Loss: 0.00003162
Iteration 33/1000 | Loss: 0.00003162
Iteration 34/1000 | Loss: 0.00003161
Iteration 35/1000 | Loss: 0.00003161
Iteration 36/1000 | Loss: 0.00003161
Iteration 37/1000 | Loss: 0.00003161
Iteration 38/1000 | Loss: 0.00003161
Iteration 39/1000 | Loss: 0.00003161
Iteration 40/1000 | Loss: 0.00003160
Iteration 41/1000 | Loss: 0.00003160
Iteration 42/1000 | Loss: 0.00003160
Iteration 43/1000 | Loss: 0.00003158
Iteration 44/1000 | Loss: 0.00003157
Iteration 45/1000 | Loss: 0.00003157
Iteration 46/1000 | Loss: 0.00003157
Iteration 47/1000 | Loss: 0.00003157
Iteration 48/1000 | Loss: 0.00003157
Iteration 49/1000 | Loss: 0.00003157
Iteration 50/1000 | Loss: 0.00003157
Iteration 51/1000 | Loss: 0.00003156
Iteration 52/1000 | Loss: 0.00003156
Iteration 53/1000 | Loss: 0.00003156
Iteration 54/1000 | Loss: 0.00003155
Iteration 55/1000 | Loss: 0.00003155
Iteration 56/1000 | Loss: 0.00003155
Iteration 57/1000 | Loss: 0.00003154
Iteration 58/1000 | Loss: 0.00003154
Iteration 59/1000 | Loss: 0.00003154
Iteration 60/1000 | Loss: 0.00003154
Iteration 61/1000 | Loss: 0.00003154
Iteration 62/1000 | Loss: 0.00003154
Iteration 63/1000 | Loss: 0.00003154
Iteration 64/1000 | Loss: 0.00003154
Iteration 65/1000 | Loss: 0.00003154
Iteration 66/1000 | Loss: 0.00003154
Iteration 67/1000 | Loss: 0.00003154
Iteration 68/1000 | Loss: 0.00003154
Iteration 69/1000 | Loss: 0.00003153
Iteration 70/1000 | Loss: 0.00003153
Iteration 71/1000 | Loss: 0.00003153
Iteration 72/1000 | Loss: 0.00003153
Iteration 73/1000 | Loss: 0.00003153
Iteration 74/1000 | Loss: 0.00003153
Iteration 75/1000 | Loss: 0.00003153
Iteration 76/1000 | Loss: 0.00003153
Iteration 77/1000 | Loss: 0.00003153
Iteration 78/1000 | Loss: 0.00003153
Iteration 79/1000 | Loss: 0.00003152
Iteration 80/1000 | Loss: 0.00003152
Iteration 81/1000 | Loss: 0.00003152
Iteration 82/1000 | Loss: 0.00003152
Iteration 83/1000 | Loss: 0.00003152
Iteration 84/1000 | Loss: 0.00003152
Iteration 85/1000 | Loss: 0.00003152
Iteration 86/1000 | Loss: 0.00003152
Iteration 87/1000 | Loss: 0.00003152
Iteration 88/1000 | Loss: 0.00003151
Iteration 89/1000 | Loss: 0.00003151
Iteration 90/1000 | Loss: 0.00003151
Iteration 91/1000 | Loss: 0.00003151
Iteration 92/1000 | Loss: 0.00003151
Iteration 93/1000 | Loss: 0.00003150
Iteration 94/1000 | Loss: 0.00003150
Iteration 95/1000 | Loss: 0.00003150
Iteration 96/1000 | Loss: 0.00003150
Iteration 97/1000 | Loss: 0.00003150
Iteration 98/1000 | Loss: 0.00003150
Iteration 99/1000 | Loss: 0.00003150
Iteration 100/1000 | Loss: 0.00003149
Iteration 101/1000 | Loss: 0.00003149
Iteration 102/1000 | Loss: 0.00003149
Iteration 103/1000 | Loss: 0.00003149
Iteration 104/1000 | Loss: 0.00003149
Iteration 105/1000 | Loss: 0.00003148
Iteration 106/1000 | Loss: 0.00003148
Iteration 107/1000 | Loss: 0.00003148
Iteration 108/1000 | Loss: 0.00003148
Iteration 109/1000 | Loss: 0.00003148
Iteration 110/1000 | Loss: 0.00003148
Iteration 111/1000 | Loss: 0.00003148
Iteration 112/1000 | Loss: 0.00003147
Iteration 113/1000 | Loss: 0.00003147
Iteration 114/1000 | Loss: 0.00003147
Iteration 115/1000 | Loss: 0.00003147
Iteration 116/1000 | Loss: 0.00003147
Iteration 117/1000 | Loss: 0.00003147
Iteration 118/1000 | Loss: 0.00003147
Iteration 119/1000 | Loss: 0.00003147
Iteration 120/1000 | Loss: 0.00003147
Iteration 121/1000 | Loss: 0.00003147
Iteration 122/1000 | Loss: 0.00003146
Iteration 123/1000 | Loss: 0.00003146
Iteration 124/1000 | Loss: 0.00003146
Iteration 125/1000 | Loss: 0.00003146
Iteration 126/1000 | Loss: 0.00003146
Iteration 127/1000 | Loss: 0.00003146
Iteration 128/1000 | Loss: 0.00003146
Iteration 129/1000 | Loss: 0.00003146
Iteration 130/1000 | Loss: 0.00003146
Iteration 131/1000 | Loss: 0.00003146
Iteration 132/1000 | Loss: 0.00003145
Iteration 133/1000 | Loss: 0.00003145
Iteration 134/1000 | Loss: 0.00003145
Iteration 135/1000 | Loss: 0.00003145
Iteration 136/1000 | Loss: 0.00003145
Iteration 137/1000 | Loss: 0.00003145
Iteration 138/1000 | Loss: 0.00003145
Iteration 139/1000 | Loss: 0.00003144
Iteration 140/1000 | Loss: 0.00003144
Iteration 141/1000 | Loss: 0.00003144
Iteration 142/1000 | Loss: 0.00003144
Iteration 143/1000 | Loss: 0.00003144
Iteration 144/1000 | Loss: 0.00003144
Iteration 145/1000 | Loss: 0.00003144
Iteration 146/1000 | Loss: 0.00003144
Iteration 147/1000 | Loss: 0.00003144
Iteration 148/1000 | Loss: 0.00003144
Iteration 149/1000 | Loss: 0.00003144
Iteration 150/1000 | Loss: 0.00003143
Iteration 151/1000 | Loss: 0.00003143
Iteration 152/1000 | Loss: 0.00003143
Iteration 153/1000 | Loss: 0.00003143
Iteration 154/1000 | Loss: 0.00003143
Iteration 155/1000 | Loss: 0.00003143
Iteration 156/1000 | Loss: 0.00003143
Iteration 157/1000 | Loss: 0.00003143
Iteration 158/1000 | Loss: 0.00003143
Iteration 159/1000 | Loss: 0.00003143
Iteration 160/1000 | Loss: 0.00003143
Iteration 161/1000 | Loss: 0.00003143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.1432093237526715e-05, 3.1432093237526715e-05, 3.1432093237526715e-05, 3.1432093237526715e-05, 3.1432093237526715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1432093237526715e-05

Optimization complete. Final v2v error: 4.7802414894104 mm

Highest mean error: 5.11223030090332 mm for frame 178

Lowest mean error: 4.6239399909973145 mm for frame 156

Saving results

Total time: 68.29550909996033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769672
Iteration 2/25 | Loss: 0.00140552
Iteration 3/25 | Loss: 0.00124018
Iteration 4/25 | Loss: 0.00123037
Iteration 5/25 | Loss: 0.00122940
Iteration 6/25 | Loss: 0.00122940
Iteration 7/25 | Loss: 0.00122940
Iteration 8/25 | Loss: 0.00122940
Iteration 9/25 | Loss: 0.00122940
Iteration 10/25 | Loss: 0.00122940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012294021435081959, 0.0012294021435081959, 0.0012294021435081959, 0.0012294021435081959, 0.0012294021435081959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012294021435081959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34085655
Iteration 2/25 | Loss: 0.00100837
Iteration 3/25 | Loss: 0.00100837
Iteration 4/25 | Loss: 0.00100837
Iteration 5/25 | Loss: 0.00100837
Iteration 6/25 | Loss: 0.00100837
Iteration 7/25 | Loss: 0.00100837
Iteration 8/25 | Loss: 0.00100837
Iteration 9/25 | Loss: 0.00100837
Iteration 10/25 | Loss: 0.00100837
Iteration 11/25 | Loss: 0.00100837
Iteration 12/25 | Loss: 0.00100837
Iteration 13/25 | Loss: 0.00100837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001008367515169084, 0.001008367515169084, 0.001008367515169084, 0.001008367515169084, 0.001008367515169084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001008367515169084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100837
Iteration 2/1000 | Loss: 0.00003071
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001637
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001413
Iteration 7/1000 | Loss: 0.00001346
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001285
Iteration 10/1000 | Loss: 0.00001259
Iteration 11/1000 | Loss: 0.00001234
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001226
Iteration 16/1000 | Loss: 0.00001223
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001218
Iteration 19/1000 | Loss: 0.00001212
Iteration 20/1000 | Loss: 0.00001209
Iteration 21/1000 | Loss: 0.00001209
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001192
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001174
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001171
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001159
Iteration 63/1000 | Loss: 0.00001158
Iteration 64/1000 | Loss: 0.00001158
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001158
Iteration 67/1000 | Loss: 0.00001158
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001150
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001150
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001149
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001149
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Iteration 139/1000 | Loss: 0.00001144
Iteration 140/1000 | Loss: 0.00001144
Iteration 141/1000 | Loss: 0.00001144
Iteration 142/1000 | Loss: 0.00001144
Iteration 143/1000 | Loss: 0.00001144
Iteration 144/1000 | Loss: 0.00001144
Iteration 145/1000 | Loss: 0.00001144
Iteration 146/1000 | Loss: 0.00001144
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001143
Iteration 149/1000 | Loss: 0.00001143
Iteration 150/1000 | Loss: 0.00001143
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001143
Iteration 156/1000 | Loss: 0.00001143
Iteration 157/1000 | Loss: 0.00001143
Iteration 158/1000 | Loss: 0.00001143
Iteration 159/1000 | Loss: 0.00001143
Iteration 160/1000 | Loss: 0.00001143
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001143
Iteration 170/1000 | Loss: 0.00001143
Iteration 171/1000 | Loss: 0.00001143
Iteration 172/1000 | Loss: 0.00001143
Iteration 173/1000 | Loss: 0.00001143
Iteration 174/1000 | Loss: 0.00001143
Iteration 175/1000 | Loss: 0.00001143
Iteration 176/1000 | Loss: 0.00001143
Iteration 177/1000 | Loss: 0.00001143
Iteration 178/1000 | Loss: 0.00001143
Iteration 179/1000 | Loss: 0.00001143
Iteration 180/1000 | Loss: 0.00001143
Iteration 181/1000 | Loss: 0.00001143
Iteration 182/1000 | Loss: 0.00001143
Iteration 183/1000 | Loss: 0.00001143
Iteration 184/1000 | Loss: 0.00001143
Iteration 185/1000 | Loss: 0.00001143
Iteration 186/1000 | Loss: 0.00001143
Iteration 187/1000 | Loss: 0.00001143
Iteration 188/1000 | Loss: 0.00001143
Iteration 189/1000 | Loss: 0.00001143
Iteration 190/1000 | Loss: 0.00001143
Iteration 191/1000 | Loss: 0.00001143
Iteration 192/1000 | Loss: 0.00001143
Iteration 193/1000 | Loss: 0.00001143
Iteration 194/1000 | Loss: 0.00001143
Iteration 195/1000 | Loss: 0.00001143
Iteration 196/1000 | Loss: 0.00001143
Iteration 197/1000 | Loss: 0.00001143
Iteration 198/1000 | Loss: 0.00001143
Iteration 199/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1426856872276403e-05, 1.1426856872276403e-05, 1.1426856872276403e-05, 1.1426856872276403e-05, 1.1426856872276403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1426856872276403e-05

Optimization complete. Final v2v error: 2.8865256309509277 mm

Highest mean error: 3.2283565998077393 mm for frame 122

Lowest mean error: 2.6296133995056152 mm for frame 64

Saving results

Total time: 46.62027549743652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552158
Iteration 2/25 | Loss: 0.00150907
Iteration 3/25 | Loss: 0.00131887
Iteration 4/25 | Loss: 0.00130311
Iteration 5/25 | Loss: 0.00129809
Iteration 6/25 | Loss: 0.00129714
Iteration 7/25 | Loss: 0.00129714
Iteration 8/25 | Loss: 0.00129714
Iteration 9/25 | Loss: 0.00129714
Iteration 10/25 | Loss: 0.00129714
Iteration 11/25 | Loss: 0.00129714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012971368851140141, 0.0012971368851140141, 0.0012971368851140141, 0.0012971368851140141, 0.0012971368851140141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012971368851140141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37405920
Iteration 2/25 | Loss: 0.00114751
Iteration 3/25 | Loss: 0.00114751
Iteration 4/25 | Loss: 0.00114751
Iteration 5/25 | Loss: 0.00114751
Iteration 6/25 | Loss: 0.00114751
Iteration 7/25 | Loss: 0.00114751
Iteration 8/25 | Loss: 0.00114751
Iteration 9/25 | Loss: 0.00114751
Iteration 10/25 | Loss: 0.00114751
Iteration 11/25 | Loss: 0.00114751
Iteration 12/25 | Loss: 0.00114751
Iteration 13/25 | Loss: 0.00114751
Iteration 14/25 | Loss: 0.00114751
Iteration 15/25 | Loss: 0.00114751
Iteration 16/25 | Loss: 0.00114751
Iteration 17/25 | Loss: 0.00114751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011475092032924294, 0.0011475092032924294, 0.0011475092032924294, 0.0011475092032924294, 0.0011475092032924294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011475092032924294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114751
Iteration 2/1000 | Loss: 0.00003754
Iteration 3/1000 | Loss: 0.00002989
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002630
Iteration 7/1000 | Loss: 0.00002599
Iteration 8/1000 | Loss: 0.00002565
Iteration 9/1000 | Loss: 0.00002534
Iteration 10/1000 | Loss: 0.00002511
Iteration 11/1000 | Loss: 0.00002489
Iteration 12/1000 | Loss: 0.00002465
Iteration 13/1000 | Loss: 0.00002447
Iteration 14/1000 | Loss: 0.00002445
Iteration 15/1000 | Loss: 0.00002429
Iteration 16/1000 | Loss: 0.00002424
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002414
Iteration 19/1000 | Loss: 0.00002413
Iteration 20/1000 | Loss: 0.00002413
Iteration 21/1000 | Loss: 0.00002412
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002404
Iteration 24/1000 | Loss: 0.00002403
Iteration 25/1000 | Loss: 0.00002403
Iteration 26/1000 | Loss: 0.00002403
Iteration 27/1000 | Loss: 0.00002402
Iteration 28/1000 | Loss: 0.00002402
Iteration 29/1000 | Loss: 0.00002402
Iteration 30/1000 | Loss: 0.00002402
Iteration 31/1000 | Loss: 0.00002402
Iteration 32/1000 | Loss: 0.00002401
Iteration 33/1000 | Loss: 0.00002401
Iteration 34/1000 | Loss: 0.00002401
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002401
Iteration 37/1000 | Loss: 0.00002401
Iteration 38/1000 | Loss: 0.00002401
Iteration 39/1000 | Loss: 0.00002401
Iteration 40/1000 | Loss: 0.00002401
Iteration 41/1000 | Loss: 0.00002400
Iteration 42/1000 | Loss: 0.00002400
Iteration 43/1000 | Loss: 0.00002399
Iteration 44/1000 | Loss: 0.00002398
Iteration 45/1000 | Loss: 0.00002398
Iteration 46/1000 | Loss: 0.00002393
Iteration 47/1000 | Loss: 0.00002392
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002390
Iteration 50/1000 | Loss: 0.00002389
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002386
Iteration 60/1000 | Loss: 0.00002386
Iteration 61/1000 | Loss: 0.00002386
Iteration 62/1000 | Loss: 0.00002385
Iteration 63/1000 | Loss: 0.00002385
Iteration 64/1000 | Loss: 0.00002385
Iteration 65/1000 | Loss: 0.00002385
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002383
Iteration 69/1000 | Loss: 0.00002383
Iteration 70/1000 | Loss: 0.00002383
Iteration 71/1000 | Loss: 0.00002383
Iteration 72/1000 | Loss: 0.00002383
Iteration 73/1000 | Loss: 0.00002383
Iteration 74/1000 | Loss: 0.00002382
Iteration 75/1000 | Loss: 0.00002382
Iteration 76/1000 | Loss: 0.00002382
Iteration 77/1000 | Loss: 0.00002382
Iteration 78/1000 | Loss: 0.00002382
Iteration 79/1000 | Loss: 0.00002382
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002382
Iteration 82/1000 | Loss: 0.00002382
Iteration 83/1000 | Loss: 0.00002382
Iteration 84/1000 | Loss: 0.00002382
Iteration 85/1000 | Loss: 0.00002382
Iteration 86/1000 | Loss: 0.00002382
Iteration 87/1000 | Loss: 0.00002382
Iteration 88/1000 | Loss: 0.00002382
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002382
Iteration 92/1000 | Loss: 0.00002382
Iteration 93/1000 | Loss: 0.00002382
Iteration 94/1000 | Loss: 0.00002382
Iteration 95/1000 | Loss: 0.00002382
Iteration 96/1000 | Loss: 0.00002382
Iteration 97/1000 | Loss: 0.00002382
Iteration 98/1000 | Loss: 0.00002382
Iteration 99/1000 | Loss: 0.00002382
Iteration 100/1000 | Loss: 0.00002382
Iteration 101/1000 | Loss: 0.00002382
Iteration 102/1000 | Loss: 0.00002382
Iteration 103/1000 | Loss: 0.00002382
Iteration 104/1000 | Loss: 0.00002382
Iteration 105/1000 | Loss: 0.00002382
Iteration 106/1000 | Loss: 0.00002382
Iteration 107/1000 | Loss: 0.00002382
Iteration 108/1000 | Loss: 0.00002382
Iteration 109/1000 | Loss: 0.00002382
Iteration 110/1000 | Loss: 0.00002382
Iteration 111/1000 | Loss: 0.00002382
Iteration 112/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [2.38186457863776e-05, 2.38186457863776e-05, 2.38186457863776e-05, 2.38186457863776e-05, 2.38186457863776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.38186457863776e-05

Optimization complete. Final v2v error: 3.9705471992492676 mm

Highest mean error: 4.359687805175781 mm for frame 133

Lowest mean error: 3.5608725547790527 mm for frame 77

Saving results

Total time: 41.92724370956421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830883
Iteration 2/25 | Loss: 0.00144224
Iteration 3/25 | Loss: 0.00125386
Iteration 4/25 | Loss: 0.00122700
Iteration 5/25 | Loss: 0.00122324
Iteration 6/25 | Loss: 0.00122263
Iteration 7/25 | Loss: 0.00122263
Iteration 8/25 | Loss: 0.00122263
Iteration 9/25 | Loss: 0.00122263
Iteration 10/25 | Loss: 0.00122263
Iteration 11/25 | Loss: 0.00122263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001222630380652845, 0.001222630380652845, 0.001222630380652845, 0.001222630380652845, 0.001222630380652845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001222630380652845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35248363
Iteration 2/25 | Loss: 0.00100291
Iteration 3/25 | Loss: 0.00100291
Iteration 4/25 | Loss: 0.00100291
Iteration 5/25 | Loss: 0.00100291
Iteration 6/25 | Loss: 0.00100291
Iteration 7/25 | Loss: 0.00100291
Iteration 8/25 | Loss: 0.00100291
Iteration 9/25 | Loss: 0.00100291
Iteration 10/25 | Loss: 0.00100291
Iteration 11/25 | Loss: 0.00100291
Iteration 12/25 | Loss: 0.00100291
Iteration 13/25 | Loss: 0.00100291
Iteration 14/25 | Loss: 0.00100291
Iteration 15/25 | Loss: 0.00100291
Iteration 16/25 | Loss: 0.00100291
Iteration 17/25 | Loss: 0.00100291
Iteration 18/25 | Loss: 0.00100291
Iteration 19/25 | Loss: 0.00100291
Iteration 20/25 | Loss: 0.00100291
Iteration 21/25 | Loss: 0.00100291
Iteration 22/25 | Loss: 0.00100291
Iteration 23/25 | Loss: 0.00100291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010029063560068607, 0.0010029063560068607, 0.0010029063560068607, 0.0010029063560068607, 0.0010029063560068607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010029063560068607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100291
Iteration 2/1000 | Loss: 0.00002905
Iteration 3/1000 | Loss: 0.00001983
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001412
Iteration 9/1000 | Loss: 0.00001382
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001313
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001302
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001285
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001285
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001282
Iteration 43/1000 | Loss: 0.00001281
Iteration 44/1000 | Loss: 0.00001280
Iteration 45/1000 | Loss: 0.00001280
Iteration 46/1000 | Loss: 0.00001279
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001278
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001276
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001273
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001272
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001272
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001272
Iteration 78/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.2717685422103386e-05, 1.2717685422103386e-05, 1.2717685422103386e-05, 1.2717685422103386e-05, 1.2717685422103386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2717685422103386e-05

Optimization complete. Final v2v error: 3.0365078449249268 mm

Highest mean error: 3.9499764442443848 mm for frame 163

Lowest mean error: 2.6132895946502686 mm for frame 50

Saving results

Total time: 36.09025311470032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575587
Iteration 2/25 | Loss: 0.00146963
Iteration 3/25 | Loss: 0.00133025
Iteration 4/25 | Loss: 0.00130052
Iteration 5/25 | Loss: 0.00129068
Iteration 6/25 | Loss: 0.00128842
Iteration 7/25 | Loss: 0.00128717
Iteration 8/25 | Loss: 0.00129009
Iteration 9/25 | Loss: 0.00129092
Iteration 10/25 | Loss: 0.00128396
Iteration 11/25 | Loss: 0.00128268
Iteration 12/25 | Loss: 0.00128177
Iteration 13/25 | Loss: 0.00128128
Iteration 14/25 | Loss: 0.00128084
Iteration 15/25 | Loss: 0.00128037
Iteration 16/25 | Loss: 0.00128015
Iteration 17/25 | Loss: 0.00127994
Iteration 18/25 | Loss: 0.00127978
Iteration 19/25 | Loss: 0.00128368
Iteration 20/25 | Loss: 0.00128029
Iteration 21/25 | Loss: 0.00127944
Iteration 22/25 | Loss: 0.00127920
Iteration 23/25 | Loss: 0.00128359
Iteration 24/25 | Loss: 0.00128326
Iteration 25/25 | Loss: 0.00128347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16555762
Iteration 2/25 | Loss: 0.00096289
Iteration 3/25 | Loss: 0.00096287
Iteration 4/25 | Loss: 0.00096287
Iteration 5/25 | Loss: 0.00096287
Iteration 6/25 | Loss: 0.00096287
Iteration 7/25 | Loss: 0.00096287
Iteration 8/25 | Loss: 0.00096287
Iteration 9/25 | Loss: 0.00096287
Iteration 10/25 | Loss: 0.00096287
Iteration 11/25 | Loss: 0.00096287
Iteration 12/25 | Loss: 0.00096287
Iteration 13/25 | Loss: 0.00096287
Iteration 14/25 | Loss: 0.00096287
Iteration 15/25 | Loss: 0.00096287
Iteration 16/25 | Loss: 0.00096287
Iteration 17/25 | Loss: 0.00096287
Iteration 18/25 | Loss: 0.00096287
Iteration 19/25 | Loss: 0.00096287
Iteration 20/25 | Loss: 0.00096287
Iteration 21/25 | Loss: 0.00096287
Iteration 22/25 | Loss: 0.00096287
Iteration 23/25 | Loss: 0.00096287
Iteration 24/25 | Loss: 0.00096287
Iteration 25/25 | Loss: 0.00096287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096287
Iteration 2/1000 | Loss: 0.00005243
Iteration 3/1000 | Loss: 0.00003687
Iteration 4/1000 | Loss: 0.00002942
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002663
Iteration 7/1000 | Loss: 0.00002587
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002481
Iteration 10/1000 | Loss: 0.00002448
Iteration 11/1000 | Loss: 0.00002413
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002364
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002324
Iteration 17/1000 | Loss: 0.00002320
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002318
Iteration 20/1000 | Loss: 0.00002315
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002307
Iteration 23/1000 | Loss: 0.00002305
Iteration 24/1000 | Loss: 0.00002305
Iteration 25/1000 | Loss: 0.00002305
Iteration 26/1000 | Loss: 0.00002304
Iteration 27/1000 | Loss: 0.00002304
Iteration 28/1000 | Loss: 0.00002302
Iteration 29/1000 | Loss: 0.00002302
Iteration 30/1000 | Loss: 0.00002302
Iteration 31/1000 | Loss: 0.00002302
Iteration 32/1000 | Loss: 0.00002301
Iteration 33/1000 | Loss: 0.00002301
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002300
Iteration 36/1000 | Loss: 0.00002299
Iteration 37/1000 | Loss: 0.00002299
Iteration 38/1000 | Loss: 0.00002298
Iteration 39/1000 | Loss: 0.00002298
Iteration 40/1000 | Loss: 0.00002297
Iteration 41/1000 | Loss: 0.00002297
Iteration 42/1000 | Loss: 0.00002297
Iteration 43/1000 | Loss: 0.00002297
Iteration 44/1000 | Loss: 0.00002296
Iteration 45/1000 | Loss: 0.00002296
Iteration 46/1000 | Loss: 0.00002296
Iteration 47/1000 | Loss: 0.00002296
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002295
Iteration 50/1000 | Loss: 0.00002295
Iteration 51/1000 | Loss: 0.00002295
Iteration 52/1000 | Loss: 0.00002294
Iteration 53/1000 | Loss: 0.00002294
Iteration 54/1000 | Loss: 0.00002293
Iteration 55/1000 | Loss: 0.00002293
Iteration 56/1000 | Loss: 0.00002293
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002292
Iteration 59/1000 | Loss: 0.00002292
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002289
Iteration 66/1000 | Loss: 0.00002289
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002287
Iteration 72/1000 | Loss: 0.00002287
Iteration 73/1000 | Loss: 0.00002286
Iteration 74/1000 | Loss: 0.00002286
Iteration 75/1000 | Loss: 0.00002285
Iteration 76/1000 | Loss: 0.00002285
Iteration 77/1000 | Loss: 0.00002284
Iteration 78/1000 | Loss: 0.00002284
Iteration 79/1000 | Loss: 0.00002284
Iteration 80/1000 | Loss: 0.00002284
Iteration 81/1000 | Loss: 0.00002284
Iteration 82/1000 | Loss: 0.00002283
Iteration 83/1000 | Loss: 0.00002283
Iteration 84/1000 | Loss: 0.00002283
Iteration 85/1000 | Loss: 0.00002283
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002283
Iteration 88/1000 | Loss: 0.00002283
Iteration 89/1000 | Loss: 0.00002283
Iteration 90/1000 | Loss: 0.00002283
Iteration 91/1000 | Loss: 0.00002282
Iteration 92/1000 | Loss: 0.00002282
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002281
Iteration 95/1000 | Loss: 0.00002281
Iteration 96/1000 | Loss: 0.00002281
Iteration 97/1000 | Loss: 0.00002281
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002280
Iteration 100/1000 | Loss: 0.00002280
Iteration 101/1000 | Loss: 0.00002280
Iteration 102/1000 | Loss: 0.00002279
Iteration 103/1000 | Loss: 0.00002279
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Iteration 106/1000 | Loss: 0.00002278
Iteration 107/1000 | Loss: 0.00002278
Iteration 108/1000 | Loss: 0.00002278
Iteration 109/1000 | Loss: 0.00002278
Iteration 110/1000 | Loss: 0.00002278
Iteration 111/1000 | Loss: 0.00002278
Iteration 112/1000 | Loss: 0.00002278
Iteration 113/1000 | Loss: 0.00002278
Iteration 114/1000 | Loss: 0.00002278
Iteration 115/1000 | Loss: 0.00002278
Iteration 116/1000 | Loss: 0.00002278
Iteration 117/1000 | Loss: 0.00002278
Iteration 118/1000 | Loss: 0.00002277
Iteration 119/1000 | Loss: 0.00002277
Iteration 120/1000 | Loss: 0.00002277
Iteration 121/1000 | Loss: 0.00002277
Iteration 122/1000 | Loss: 0.00002277
Iteration 123/1000 | Loss: 0.00002277
Iteration 124/1000 | Loss: 0.00002277
Iteration 125/1000 | Loss: 0.00002277
Iteration 126/1000 | Loss: 0.00002277
Iteration 127/1000 | Loss: 0.00002277
Iteration 128/1000 | Loss: 0.00002277
Iteration 129/1000 | Loss: 0.00002277
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002276
Iteration 132/1000 | Loss: 0.00002276
Iteration 133/1000 | Loss: 0.00002276
Iteration 134/1000 | Loss: 0.00002276
Iteration 135/1000 | Loss: 0.00002276
Iteration 136/1000 | Loss: 0.00002276
Iteration 137/1000 | Loss: 0.00002275
Iteration 138/1000 | Loss: 0.00002275
Iteration 139/1000 | Loss: 0.00002275
Iteration 140/1000 | Loss: 0.00002275
Iteration 141/1000 | Loss: 0.00002275
Iteration 142/1000 | Loss: 0.00002275
Iteration 143/1000 | Loss: 0.00002275
Iteration 144/1000 | Loss: 0.00002275
Iteration 145/1000 | Loss: 0.00002275
Iteration 146/1000 | Loss: 0.00002275
Iteration 147/1000 | Loss: 0.00002275
Iteration 148/1000 | Loss: 0.00002275
Iteration 149/1000 | Loss: 0.00002275
Iteration 150/1000 | Loss: 0.00002275
Iteration 151/1000 | Loss: 0.00002275
Iteration 152/1000 | Loss: 0.00002274
Iteration 153/1000 | Loss: 0.00002274
Iteration 154/1000 | Loss: 0.00002274
Iteration 155/1000 | Loss: 0.00002274
Iteration 156/1000 | Loss: 0.00002274
Iteration 157/1000 | Loss: 0.00002274
Iteration 158/1000 | Loss: 0.00002274
Iteration 159/1000 | Loss: 0.00002274
Iteration 160/1000 | Loss: 0.00002274
Iteration 161/1000 | Loss: 0.00002274
Iteration 162/1000 | Loss: 0.00002274
Iteration 163/1000 | Loss: 0.00002274
Iteration 164/1000 | Loss: 0.00002274
Iteration 165/1000 | Loss: 0.00002274
Iteration 166/1000 | Loss: 0.00002274
Iteration 167/1000 | Loss: 0.00002274
Iteration 168/1000 | Loss: 0.00002274
Iteration 169/1000 | Loss: 0.00002274
Iteration 170/1000 | Loss: 0.00002274
Iteration 171/1000 | Loss: 0.00002274
Iteration 172/1000 | Loss: 0.00002274
Iteration 173/1000 | Loss: 0.00002274
Iteration 174/1000 | Loss: 0.00002274
Iteration 175/1000 | Loss: 0.00002274
Iteration 176/1000 | Loss: 0.00002274
Iteration 177/1000 | Loss: 0.00002274
Iteration 178/1000 | Loss: 0.00002274
Iteration 179/1000 | Loss: 0.00002274
Iteration 180/1000 | Loss: 0.00002274
Iteration 181/1000 | Loss: 0.00002274
Iteration 182/1000 | Loss: 0.00002274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.2736770915798843e-05, 2.2736770915798843e-05, 2.2736770915798843e-05, 2.2736770915798843e-05, 2.2736770915798843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2736770915798843e-05

Optimization complete. Final v2v error: 3.8825297355651855 mm

Highest mean error: 5.962530136108398 mm for frame 114

Lowest mean error: 3.25797176361084 mm for frame 0

Saving results

Total time: 82.24253439903259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609370
Iteration 2/25 | Loss: 0.00159473
Iteration 3/25 | Loss: 0.00145169
Iteration 4/25 | Loss: 0.00142902
Iteration 5/25 | Loss: 0.00142447
Iteration 6/25 | Loss: 0.00142410
Iteration 7/25 | Loss: 0.00142410
Iteration 8/25 | Loss: 0.00142410
Iteration 9/25 | Loss: 0.00142410
Iteration 10/25 | Loss: 0.00142410
Iteration 11/25 | Loss: 0.00142410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014240965247154236, 0.0014240965247154236, 0.0014240965247154236, 0.0014240965247154236, 0.0014240965247154236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014240965247154236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62975478
Iteration 2/25 | Loss: 0.00082900
Iteration 3/25 | Loss: 0.00082900
Iteration 4/25 | Loss: 0.00082900
Iteration 5/25 | Loss: 0.00082900
Iteration 6/25 | Loss: 0.00082900
Iteration 7/25 | Loss: 0.00082900
Iteration 8/25 | Loss: 0.00082900
Iteration 9/25 | Loss: 0.00082899
Iteration 10/25 | Loss: 0.00082899
Iteration 11/25 | Loss: 0.00082899
Iteration 12/25 | Loss: 0.00082899
Iteration 13/25 | Loss: 0.00082899
Iteration 14/25 | Loss: 0.00082899
Iteration 15/25 | Loss: 0.00082899
Iteration 16/25 | Loss: 0.00082899
Iteration 17/25 | Loss: 0.00082899
Iteration 18/25 | Loss: 0.00082899
Iteration 19/25 | Loss: 0.00082899
Iteration 20/25 | Loss: 0.00082899
Iteration 21/25 | Loss: 0.00082899
Iteration 22/25 | Loss: 0.00082899
Iteration 23/25 | Loss: 0.00082899
Iteration 24/25 | Loss: 0.00082899
Iteration 25/25 | Loss: 0.00082899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082899
Iteration 2/1000 | Loss: 0.00005207
Iteration 3/1000 | Loss: 0.00003748
Iteration 4/1000 | Loss: 0.00003508
Iteration 5/1000 | Loss: 0.00003413
Iteration 6/1000 | Loss: 0.00003336
Iteration 7/1000 | Loss: 0.00003275
Iteration 8/1000 | Loss: 0.00003238
Iteration 9/1000 | Loss: 0.00003214
Iteration 10/1000 | Loss: 0.00003187
Iteration 11/1000 | Loss: 0.00003167
Iteration 12/1000 | Loss: 0.00003145
Iteration 13/1000 | Loss: 0.00003131
Iteration 14/1000 | Loss: 0.00003122
Iteration 15/1000 | Loss: 0.00003119
Iteration 16/1000 | Loss: 0.00003119
Iteration 17/1000 | Loss: 0.00003119
Iteration 18/1000 | Loss: 0.00003119
Iteration 19/1000 | Loss: 0.00003119
Iteration 20/1000 | Loss: 0.00003119
Iteration 21/1000 | Loss: 0.00003118
Iteration 22/1000 | Loss: 0.00003118
Iteration 23/1000 | Loss: 0.00003110
Iteration 24/1000 | Loss: 0.00003106
Iteration 25/1000 | Loss: 0.00003106
Iteration 26/1000 | Loss: 0.00003106
Iteration 27/1000 | Loss: 0.00003106
Iteration 28/1000 | Loss: 0.00003105
Iteration 29/1000 | Loss: 0.00003105
Iteration 30/1000 | Loss: 0.00003105
Iteration 31/1000 | Loss: 0.00003103
Iteration 32/1000 | Loss: 0.00003102
Iteration 33/1000 | Loss: 0.00003102
Iteration 34/1000 | Loss: 0.00003102
Iteration 35/1000 | Loss: 0.00003102
Iteration 36/1000 | Loss: 0.00003102
Iteration 37/1000 | Loss: 0.00003102
Iteration 38/1000 | Loss: 0.00003102
Iteration 39/1000 | Loss: 0.00003102
Iteration 40/1000 | Loss: 0.00003102
Iteration 41/1000 | Loss: 0.00003102
Iteration 42/1000 | Loss: 0.00003101
Iteration 43/1000 | Loss: 0.00003101
Iteration 44/1000 | Loss: 0.00003101
Iteration 45/1000 | Loss: 0.00003101
Iteration 46/1000 | Loss: 0.00003099
Iteration 47/1000 | Loss: 0.00003098
Iteration 48/1000 | Loss: 0.00003097
Iteration 49/1000 | Loss: 0.00003097
Iteration 50/1000 | Loss: 0.00003097
Iteration 51/1000 | Loss: 0.00003097
Iteration 52/1000 | Loss: 0.00003096
Iteration 53/1000 | Loss: 0.00003096
Iteration 54/1000 | Loss: 0.00003096
Iteration 55/1000 | Loss: 0.00003096
Iteration 56/1000 | Loss: 0.00003095
Iteration 57/1000 | Loss: 0.00003095
Iteration 58/1000 | Loss: 0.00003095
Iteration 59/1000 | Loss: 0.00003095
Iteration 60/1000 | Loss: 0.00003095
Iteration 61/1000 | Loss: 0.00003095
Iteration 62/1000 | Loss: 0.00003094
Iteration 63/1000 | Loss: 0.00003094
Iteration 64/1000 | Loss: 0.00003094
Iteration 65/1000 | Loss: 0.00003094
Iteration 66/1000 | Loss: 0.00003094
Iteration 67/1000 | Loss: 0.00003094
Iteration 68/1000 | Loss: 0.00003094
Iteration 69/1000 | Loss: 0.00003094
Iteration 70/1000 | Loss: 0.00003094
Iteration 71/1000 | Loss: 0.00003094
Iteration 72/1000 | Loss: 0.00003094
Iteration 73/1000 | Loss: 0.00003094
Iteration 74/1000 | Loss: 0.00003094
Iteration 75/1000 | Loss: 0.00003094
Iteration 76/1000 | Loss: 0.00003094
Iteration 77/1000 | Loss: 0.00003094
Iteration 78/1000 | Loss: 0.00003094
Iteration 79/1000 | Loss: 0.00003094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.094410931225866e-05, 3.094410931225866e-05, 3.094410931225866e-05, 3.094410931225866e-05, 3.094410931225866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.094410931225866e-05

Optimization complete. Final v2v error: 4.650965690612793 mm

Highest mean error: 4.7541890144348145 mm for frame 0

Lowest mean error: 4.466004371643066 mm for frame 130

Saving results

Total time: 37.263675689697266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911723
Iteration 2/25 | Loss: 0.00150809
Iteration 3/25 | Loss: 0.00125869
Iteration 4/25 | Loss: 0.00123521
Iteration 5/25 | Loss: 0.00122922
Iteration 6/25 | Loss: 0.00122844
Iteration 7/25 | Loss: 0.00122844
Iteration 8/25 | Loss: 0.00122844
Iteration 9/25 | Loss: 0.00122844
Iteration 10/25 | Loss: 0.00122844
Iteration 11/25 | Loss: 0.00122844
Iteration 12/25 | Loss: 0.00122844
Iteration 13/25 | Loss: 0.00122844
Iteration 14/25 | Loss: 0.00122844
Iteration 15/25 | Loss: 0.00122844
Iteration 16/25 | Loss: 0.00122844
Iteration 17/25 | Loss: 0.00122844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012284426484256983, 0.0012284426484256983, 0.0012284426484256983, 0.0012284426484256983, 0.0012284426484256983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012284426484256983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47015738
Iteration 2/25 | Loss: 0.00100487
Iteration 3/25 | Loss: 0.00100487
Iteration 4/25 | Loss: 0.00100487
Iteration 5/25 | Loss: 0.00100486
Iteration 6/25 | Loss: 0.00100486
Iteration 7/25 | Loss: 0.00100486
Iteration 8/25 | Loss: 0.00100486
Iteration 9/25 | Loss: 0.00100486
Iteration 10/25 | Loss: 0.00100486
Iteration 11/25 | Loss: 0.00100486
Iteration 12/25 | Loss: 0.00100486
Iteration 13/25 | Loss: 0.00100486
Iteration 14/25 | Loss: 0.00100486
Iteration 15/25 | Loss: 0.00100486
Iteration 16/25 | Loss: 0.00100486
Iteration 17/25 | Loss: 0.00100486
Iteration 18/25 | Loss: 0.00100486
Iteration 19/25 | Loss: 0.00100486
Iteration 20/25 | Loss: 0.00100486
Iteration 21/25 | Loss: 0.00100486
Iteration 22/25 | Loss: 0.00100486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010048632975667715, 0.0010048632975667715, 0.0010048632975667715, 0.0010048632975667715, 0.0010048632975667715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010048632975667715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100486
Iteration 2/1000 | Loss: 0.00002734
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001841
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001629
Iteration 7/1000 | Loss: 0.00001589
Iteration 8/1000 | Loss: 0.00001551
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001473
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001460
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001441
Iteration 17/1000 | Loss: 0.00001434
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001431
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001428
Iteration 22/1000 | Loss: 0.00001428
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001425
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001415
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001408
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001406
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001405
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001405
Iteration 63/1000 | Loss: 0.00001405
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001404
Iteration 66/1000 | Loss: 0.00001404
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001400
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001398
Iteration 85/1000 | Loss: 0.00001398
Iteration 86/1000 | Loss: 0.00001397
Iteration 87/1000 | Loss: 0.00001397
Iteration 88/1000 | Loss: 0.00001397
Iteration 89/1000 | Loss: 0.00001397
Iteration 90/1000 | Loss: 0.00001397
Iteration 91/1000 | Loss: 0.00001396
Iteration 92/1000 | Loss: 0.00001396
Iteration 93/1000 | Loss: 0.00001396
Iteration 94/1000 | Loss: 0.00001396
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001395
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001390
Iteration 115/1000 | Loss: 0.00001390
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001389
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001386
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001384
Iteration 132/1000 | Loss: 0.00001384
Iteration 133/1000 | Loss: 0.00001384
Iteration 134/1000 | Loss: 0.00001384
Iteration 135/1000 | Loss: 0.00001384
Iteration 136/1000 | Loss: 0.00001383
Iteration 137/1000 | Loss: 0.00001383
Iteration 138/1000 | Loss: 0.00001383
Iteration 139/1000 | Loss: 0.00001383
Iteration 140/1000 | Loss: 0.00001383
Iteration 141/1000 | Loss: 0.00001383
Iteration 142/1000 | Loss: 0.00001383
Iteration 143/1000 | Loss: 0.00001383
Iteration 144/1000 | Loss: 0.00001383
Iteration 145/1000 | Loss: 0.00001383
Iteration 146/1000 | Loss: 0.00001383
Iteration 147/1000 | Loss: 0.00001383
Iteration 148/1000 | Loss: 0.00001383
Iteration 149/1000 | Loss: 0.00001383
Iteration 150/1000 | Loss: 0.00001383
Iteration 151/1000 | Loss: 0.00001383
Iteration 152/1000 | Loss: 0.00001383
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.3827580914949067e-05, 1.3827580914949067e-05, 1.3827580914949067e-05, 1.3827580914949067e-05, 1.3827580914949067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3827580914949067e-05

Optimization complete. Final v2v error: 3.139091968536377 mm

Highest mean error: 3.6880362033843994 mm for frame 1

Lowest mean error: 2.665855884552002 mm for frame 175

Saving results

Total time: 45.61097812652588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005090
Iteration 2/25 | Loss: 0.00219174
Iteration 3/25 | Loss: 0.00185444
Iteration 4/25 | Loss: 0.00181111
Iteration 5/25 | Loss: 0.00185553
Iteration 6/25 | Loss: 0.00153985
Iteration 7/25 | Loss: 0.00145748
Iteration 8/25 | Loss: 0.00142207
Iteration 9/25 | Loss: 0.00138236
Iteration 10/25 | Loss: 0.00136349
Iteration 11/25 | Loss: 0.00135887
Iteration 12/25 | Loss: 0.00135388
Iteration 13/25 | Loss: 0.00134778
Iteration 14/25 | Loss: 0.00134904
Iteration 15/25 | Loss: 0.00134484
Iteration 16/25 | Loss: 0.00134697
Iteration 17/25 | Loss: 0.00133856
Iteration 18/25 | Loss: 0.00133795
Iteration 19/25 | Loss: 0.00134048
Iteration 20/25 | Loss: 0.00134379
Iteration 21/25 | Loss: 0.00134326
Iteration 22/25 | Loss: 0.00134342
Iteration 23/25 | Loss: 0.00134337
Iteration 24/25 | Loss: 0.00134474
Iteration 25/25 | Loss: 0.00134712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31201661
Iteration 2/25 | Loss: 0.00120357
Iteration 3/25 | Loss: 0.00120357
Iteration 4/25 | Loss: 0.00120357
Iteration 5/25 | Loss: 0.00120357
Iteration 6/25 | Loss: 0.00120357
Iteration 7/25 | Loss: 0.00120357
Iteration 8/25 | Loss: 0.00120357
Iteration 9/25 | Loss: 0.00120357
Iteration 10/25 | Loss: 0.00120357
Iteration 11/25 | Loss: 0.00120357
Iteration 12/25 | Loss: 0.00120357
Iteration 13/25 | Loss: 0.00120357
Iteration 14/25 | Loss: 0.00120357
Iteration 15/25 | Loss: 0.00120357
Iteration 16/25 | Loss: 0.00120357
Iteration 17/25 | Loss: 0.00120357
Iteration 18/25 | Loss: 0.00120357
Iteration 19/25 | Loss: 0.00120357
Iteration 20/25 | Loss: 0.00120357
Iteration 21/25 | Loss: 0.00120357
Iteration 22/25 | Loss: 0.00120357
Iteration 23/25 | Loss: 0.00120357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012035652762278914, 0.0012035652762278914, 0.0012035652762278914, 0.0012035652762278914, 0.0012035652762278914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012035652762278914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120357
Iteration 2/1000 | Loss: 0.00023545
Iteration 3/1000 | Loss: 0.00019996
Iteration 4/1000 | Loss: 0.00027917
Iteration 5/1000 | Loss: 0.00030215
Iteration 6/1000 | Loss: 0.00030369
Iteration 7/1000 | Loss: 0.00026244
Iteration 8/1000 | Loss: 0.00032228
Iteration 9/1000 | Loss: 0.00026894
Iteration 10/1000 | Loss: 0.00029694
Iteration 11/1000 | Loss: 0.00027182
Iteration 12/1000 | Loss: 0.00028554
Iteration 13/1000 | Loss: 0.00031932
Iteration 14/1000 | Loss: 0.00028560
Iteration 15/1000 | Loss: 0.00027139
Iteration 16/1000 | Loss: 0.00024671
Iteration 17/1000 | Loss: 0.00024727
Iteration 18/1000 | Loss: 0.00022722
Iteration 19/1000 | Loss: 0.00022513
Iteration 20/1000 | Loss: 0.00019549
Iteration 21/1000 | Loss: 0.00020733
Iteration 22/1000 | Loss: 0.00018046
Iteration 23/1000 | Loss: 0.00020647
Iteration 24/1000 | Loss: 0.00021155
Iteration 25/1000 | Loss: 0.00018709
Iteration 26/1000 | Loss: 0.00021413
Iteration 27/1000 | Loss: 0.00021662
Iteration 28/1000 | Loss: 0.00020181
Iteration 29/1000 | Loss: 0.00021167
Iteration 30/1000 | Loss: 0.00021644
Iteration 31/1000 | Loss: 0.00016775
Iteration 32/1000 | Loss: 0.00018021
Iteration 33/1000 | Loss: 0.00017628
Iteration 34/1000 | Loss: 0.00014038
Iteration 35/1000 | Loss: 0.00019583
Iteration 36/1000 | Loss: 0.00019364
Iteration 37/1000 | Loss: 0.00014048
Iteration 38/1000 | Loss: 0.00014566
Iteration 39/1000 | Loss: 0.00018369
Iteration 40/1000 | Loss: 0.00019877
Iteration 41/1000 | Loss: 0.00020011
Iteration 42/1000 | Loss: 0.00020132
Iteration 43/1000 | Loss: 0.00020047
Iteration 44/1000 | Loss: 0.00020154
Iteration 45/1000 | Loss: 0.00020141
Iteration 46/1000 | Loss: 0.00019926
Iteration 47/1000 | Loss: 0.00022872
Iteration 48/1000 | Loss: 0.00021660
Iteration 49/1000 | Loss: 0.00021861
Iteration 50/1000 | Loss: 0.00021483
Iteration 51/1000 | Loss: 0.00022939
Iteration 52/1000 | Loss: 0.00018764
Iteration 53/1000 | Loss: 0.00017915
Iteration 54/1000 | Loss: 0.00022743
Iteration 55/1000 | Loss: 0.00023060
Iteration 56/1000 | Loss: 0.00028557
Iteration 57/1000 | Loss: 0.00020329
Iteration 58/1000 | Loss: 0.00021479
Iteration 59/1000 | Loss: 0.00019943
Iteration 60/1000 | Loss: 0.00021460
Iteration 61/1000 | Loss: 0.00020204
Iteration 62/1000 | Loss: 0.00023073
Iteration 63/1000 | Loss: 0.00016537
Iteration 64/1000 | Loss: 0.00022739
Iteration 65/1000 | Loss: 0.00022236
Iteration 66/1000 | Loss: 0.00022889
Iteration 67/1000 | Loss: 0.00017088
Iteration 68/1000 | Loss: 0.00015108
Iteration 69/1000 | Loss: 0.00014314
Iteration 70/1000 | Loss: 0.00012688
Iteration 71/1000 | Loss: 0.00014097
Iteration 72/1000 | Loss: 0.00015533
Iteration 73/1000 | Loss: 0.00014792
Iteration 74/1000 | Loss: 0.00014358
Iteration 75/1000 | Loss: 0.00016189
Iteration 76/1000 | Loss: 0.00014648
Iteration 77/1000 | Loss: 0.00013257
Iteration 78/1000 | Loss: 0.00014657
Iteration 79/1000 | Loss: 0.00014849
Iteration 80/1000 | Loss: 0.00014240
Iteration 81/1000 | Loss: 0.00015462
Iteration 82/1000 | Loss: 0.00016650
Iteration 83/1000 | Loss: 0.00016427
Iteration 84/1000 | Loss: 0.00016555
Iteration 85/1000 | Loss: 0.00014678
Iteration 86/1000 | Loss: 0.00016783
Iteration 87/1000 | Loss: 0.00016389
Iteration 88/1000 | Loss: 0.00016510
Iteration 89/1000 | Loss: 0.00016886
Iteration 90/1000 | Loss: 0.00013654
Iteration 91/1000 | Loss: 0.00016220
Iteration 92/1000 | Loss: 0.00017788
Iteration 93/1000 | Loss: 0.00019359
Iteration 94/1000 | Loss: 0.00017166
Iteration 95/1000 | Loss: 0.00021567
Iteration 96/1000 | Loss: 0.00018889
Iteration 97/1000 | Loss: 0.00014735
Iteration 98/1000 | Loss: 0.00016037
Iteration 99/1000 | Loss: 0.00017433
Iteration 100/1000 | Loss: 0.00016821
Iteration 101/1000 | Loss: 0.00013629
Iteration 102/1000 | Loss: 0.00012578
Iteration 103/1000 | Loss: 0.00013721
Iteration 104/1000 | Loss: 0.00018131
Iteration 105/1000 | Loss: 0.00016685
Iteration 106/1000 | Loss: 0.00015747
Iteration 107/1000 | Loss: 0.00015268
Iteration 108/1000 | Loss: 0.00015408
Iteration 109/1000 | Loss: 0.00015972
Iteration 110/1000 | Loss: 0.00011914
Iteration 111/1000 | Loss: 0.00008858
Iteration 112/1000 | Loss: 0.00015358
Iteration 113/1000 | Loss: 0.00014010
Iteration 114/1000 | Loss: 0.00013439
Iteration 115/1000 | Loss: 0.00016178
Iteration 116/1000 | Loss: 0.00012288
Iteration 117/1000 | Loss: 0.00011880
Iteration 118/1000 | Loss: 0.00013338
Iteration 119/1000 | Loss: 0.00017515
Iteration 120/1000 | Loss: 0.00011342
Iteration 121/1000 | Loss: 0.00016744
Iteration 122/1000 | Loss: 0.00016736
Iteration 123/1000 | Loss: 0.00012871
Iteration 124/1000 | Loss: 0.00017658
Iteration 125/1000 | Loss: 0.00016047
Iteration 126/1000 | Loss: 0.00011472
Iteration 127/1000 | Loss: 0.00010422
Iteration 128/1000 | Loss: 0.00013231
Iteration 129/1000 | Loss: 0.00014142
Iteration 130/1000 | Loss: 0.00011718
Iteration 131/1000 | Loss: 0.00012682
Iteration 132/1000 | Loss: 0.00013779
Iteration 133/1000 | Loss: 0.00018727
Iteration 134/1000 | Loss: 0.00017745
Iteration 135/1000 | Loss: 0.00013787
Iteration 136/1000 | Loss: 0.00015298
Iteration 137/1000 | Loss: 0.00015905
Iteration 138/1000 | Loss: 0.00013909
Iteration 139/1000 | Loss: 0.00014032
Iteration 140/1000 | Loss: 0.00016235
Iteration 141/1000 | Loss: 0.00012868
Iteration 142/1000 | Loss: 0.00010932
Iteration 143/1000 | Loss: 0.00007623
Iteration 144/1000 | Loss: 0.00005822
Iteration 145/1000 | Loss: 0.00006429
Iteration 146/1000 | Loss: 0.00007327
Iteration 147/1000 | Loss: 0.00006483
Iteration 148/1000 | Loss: 0.00006450
Iteration 149/1000 | Loss: 0.00005968
Iteration 150/1000 | Loss: 0.00005408
Iteration 151/1000 | Loss: 0.00004758
Iteration 152/1000 | Loss: 0.00004851
Iteration 153/1000 | Loss: 0.00004230
Iteration 154/1000 | Loss: 0.00004380
Iteration 155/1000 | Loss: 0.00003431
Iteration 156/1000 | Loss: 0.00007066
Iteration 157/1000 | Loss: 0.00005649
Iteration 158/1000 | Loss: 0.00007464
Iteration 159/1000 | Loss: 0.00005873
Iteration 160/1000 | Loss: 0.00006144
Iteration 161/1000 | Loss: 0.00006664
Iteration 162/1000 | Loss: 0.00007326
Iteration 163/1000 | Loss: 0.00006424
Iteration 164/1000 | Loss: 0.00006961
Iteration 165/1000 | Loss: 0.00008424
Iteration 166/1000 | Loss: 0.00008705
Iteration 167/1000 | Loss: 0.00008333
Iteration 168/1000 | Loss: 0.00008581
Iteration 169/1000 | Loss: 0.00008617
Iteration 170/1000 | Loss: 0.00007968
Iteration 171/1000 | Loss: 0.00024186
Iteration 172/1000 | Loss: 0.00007740
Iteration 173/1000 | Loss: 0.00006321
Iteration 174/1000 | Loss: 0.00004567
Iteration 175/1000 | Loss: 0.00005758
Iteration 176/1000 | Loss: 0.00005751
Iteration 177/1000 | Loss: 0.00003415
Iteration 178/1000 | Loss: 0.00010205
Iteration 179/1000 | Loss: 0.00026888
Iteration 180/1000 | Loss: 0.00032810
Iteration 181/1000 | Loss: 0.00040738
Iteration 182/1000 | Loss: 0.00011278
Iteration 183/1000 | Loss: 0.00007140
Iteration 184/1000 | Loss: 0.00012853
Iteration 185/1000 | Loss: 0.00020348
Iteration 186/1000 | Loss: 0.00017875
Iteration 187/1000 | Loss: 0.00016219
Iteration 188/1000 | Loss: 0.00010612
Iteration 189/1000 | Loss: 0.00017626
Iteration 190/1000 | Loss: 0.00009593
Iteration 191/1000 | Loss: 0.00007346
Iteration 192/1000 | Loss: 0.00007109
Iteration 193/1000 | Loss: 0.00007432
Iteration 194/1000 | Loss: 0.00006257
Iteration 195/1000 | Loss: 0.00007101
Iteration 196/1000 | Loss: 0.00003516
Iteration 197/1000 | Loss: 0.00002931
Iteration 198/1000 | Loss: 0.00002636
Iteration 199/1000 | Loss: 0.00005341
Iteration 200/1000 | Loss: 0.00004015
Iteration 201/1000 | Loss: 0.00004730
Iteration 202/1000 | Loss: 0.00029516
Iteration 203/1000 | Loss: 0.00006400
Iteration 204/1000 | Loss: 0.00004761
Iteration 205/1000 | Loss: 0.00008122
Iteration 206/1000 | Loss: 0.00007110
Iteration 207/1000 | Loss: 0.00008293
Iteration 208/1000 | Loss: 0.00007583
Iteration 209/1000 | Loss: 0.00007620
Iteration 210/1000 | Loss: 0.00007957
Iteration 211/1000 | Loss: 0.00005784
Iteration 212/1000 | Loss: 0.00008386
Iteration 213/1000 | Loss: 0.00005581
Iteration 214/1000 | Loss: 0.00003152
Iteration 215/1000 | Loss: 0.00005355
Iteration 216/1000 | Loss: 0.00002914
Iteration 217/1000 | Loss: 0.00004628
Iteration 218/1000 | Loss: 0.00004764
Iteration 219/1000 | Loss: 0.00002839
Iteration 220/1000 | Loss: 0.00005745
Iteration 221/1000 | Loss: 0.00004113
Iteration 222/1000 | Loss: 0.00006105
Iteration 223/1000 | Loss: 0.00003634
Iteration 224/1000 | Loss: 0.00007310
Iteration 225/1000 | Loss: 0.00006846
Iteration 226/1000 | Loss: 0.00006515
Iteration 227/1000 | Loss: 0.00007820
Iteration 228/1000 | Loss: 0.00003465
Iteration 229/1000 | Loss: 0.00005562
Iteration 230/1000 | Loss: 0.00005756
Iteration 231/1000 | Loss: 0.00004273
Iteration 232/1000 | Loss: 0.00004361
Iteration 233/1000 | Loss: 0.00003296
Iteration 234/1000 | Loss: 0.00004725
Iteration 235/1000 | Loss: 0.00004199
Iteration 236/1000 | Loss: 0.00006884
Iteration 237/1000 | Loss: 0.00006133
Iteration 238/1000 | Loss: 0.00004338
Iteration 239/1000 | Loss: 0.00004606
Iteration 240/1000 | Loss: 0.00004279
Iteration 241/1000 | Loss: 0.00002361
Iteration 242/1000 | Loss: 0.00004023
Iteration 243/1000 | Loss: 0.00002371
Iteration 244/1000 | Loss: 0.00003622
Iteration 245/1000 | Loss: 0.00002518
Iteration 246/1000 | Loss: 0.00003606
Iteration 247/1000 | Loss: 0.00003470
Iteration 248/1000 | Loss: 0.00004311
Iteration 249/1000 | Loss: 0.00004531
Iteration 250/1000 | Loss: 0.00002887
Iteration 251/1000 | Loss: 0.00006457
Iteration 252/1000 | Loss: 0.00004036
Iteration 253/1000 | Loss: 0.00004173
Iteration 254/1000 | Loss: 0.00004112
Iteration 255/1000 | Loss: 0.00004065
Iteration 256/1000 | Loss: 0.00005164
Iteration 257/1000 | Loss: 0.00005142
Iteration 258/1000 | Loss: 0.00006158
Iteration 259/1000 | Loss: 0.00004352
Iteration 260/1000 | Loss: 0.00005293
Iteration 261/1000 | Loss: 0.00004430
Iteration 262/1000 | Loss: 0.00005551
Iteration 263/1000 | Loss: 0.00002722
Iteration 264/1000 | Loss: 0.00002191
Iteration 265/1000 | Loss: 0.00002074
Iteration 266/1000 | Loss: 0.00002022
Iteration 267/1000 | Loss: 0.00003350
Iteration 268/1000 | Loss: 0.00002623
Iteration 269/1000 | Loss: 0.00001962
Iteration 270/1000 | Loss: 0.00001945
Iteration 271/1000 | Loss: 0.00001942
Iteration 272/1000 | Loss: 0.00001935
Iteration 273/1000 | Loss: 0.00001934
Iteration 274/1000 | Loss: 0.00001933
Iteration 275/1000 | Loss: 0.00001930
Iteration 276/1000 | Loss: 0.00001929
Iteration 277/1000 | Loss: 0.00001928
Iteration 278/1000 | Loss: 0.00001928
Iteration 279/1000 | Loss: 0.00001927
Iteration 280/1000 | Loss: 0.00003570
Iteration 281/1000 | Loss: 0.00003570
Iteration 282/1000 | Loss: 0.00004564
Iteration 283/1000 | Loss: 0.00003159
Iteration 284/1000 | Loss: 0.00002381
Iteration 285/1000 | Loss: 0.00001911
Iteration 286/1000 | Loss: 0.00001910
Iteration 287/1000 | Loss: 0.00001908
Iteration 288/1000 | Loss: 0.00001907
Iteration 289/1000 | Loss: 0.00001903
Iteration 290/1000 | Loss: 0.00001898
Iteration 291/1000 | Loss: 0.00004444
Iteration 292/1000 | Loss: 0.00002197
Iteration 293/1000 | Loss: 0.00002088
Iteration 294/1000 | Loss: 0.00002775
Iteration 295/1000 | Loss: 0.00003879
Iteration 296/1000 | Loss: 0.00006919
Iteration 297/1000 | Loss: 0.00004700
Iteration 298/1000 | Loss: 0.00007153
Iteration 299/1000 | Loss: 0.00005820
Iteration 300/1000 | Loss: 0.00005263
Iteration 301/1000 | Loss: 0.00003914
Iteration 302/1000 | Loss: 0.00005334
Iteration 303/1000 | Loss: 0.00005384
Iteration 304/1000 | Loss: 0.00006490
Iteration 305/1000 | Loss: 0.00007801
Iteration 306/1000 | Loss: 0.00004868
Iteration 307/1000 | Loss: 0.00004603
Iteration 308/1000 | Loss: 0.00004197
Iteration 309/1000 | Loss: 0.00006209
Iteration 310/1000 | Loss: 0.00007820
Iteration 311/1000 | Loss: 0.00004145
Iteration 312/1000 | Loss: 0.00003579
Iteration 313/1000 | Loss: 0.00003104
Iteration 314/1000 | Loss: 0.00003311
Iteration 315/1000 | Loss: 0.00003818
Iteration 316/1000 | Loss: 0.00004663
Iteration 317/1000 | Loss: 0.00005538
Iteration 318/1000 | Loss: 0.00004692
Iteration 319/1000 | Loss: 0.00005362
Iteration 320/1000 | Loss: 0.00002455
Iteration 321/1000 | Loss: 0.00002150
Iteration 322/1000 | Loss: 0.00002057
Iteration 323/1000 | Loss: 0.00001987
Iteration 324/1000 | Loss: 0.00001939
Iteration 325/1000 | Loss: 0.00001903
Iteration 326/1000 | Loss: 0.00001883
Iteration 327/1000 | Loss: 0.00001875
Iteration 328/1000 | Loss: 0.00001872
Iteration 329/1000 | Loss: 0.00001870
Iteration 330/1000 | Loss: 0.00001869
Iteration 331/1000 | Loss: 0.00001865
Iteration 332/1000 | Loss: 0.00001863
Iteration 333/1000 | Loss: 0.00001862
Iteration 334/1000 | Loss: 0.00001859
Iteration 335/1000 | Loss: 0.00001857
Iteration 336/1000 | Loss: 0.00001856
Iteration 337/1000 | Loss: 0.00001856
Iteration 338/1000 | Loss: 0.00001856
Iteration 339/1000 | Loss: 0.00001856
Iteration 340/1000 | Loss: 0.00001855
Iteration 341/1000 | Loss: 0.00001855
Iteration 342/1000 | Loss: 0.00001855
Iteration 343/1000 | Loss: 0.00001854
Iteration 344/1000 | Loss: 0.00001854
Iteration 345/1000 | Loss: 0.00001854
Iteration 346/1000 | Loss: 0.00001854
Iteration 347/1000 | Loss: 0.00001853
Iteration 348/1000 | Loss: 0.00001853
Iteration 349/1000 | Loss: 0.00001853
Iteration 350/1000 | Loss: 0.00001853
Iteration 351/1000 | Loss: 0.00001853
Iteration 352/1000 | Loss: 0.00001852
Iteration 353/1000 | Loss: 0.00001852
Iteration 354/1000 | Loss: 0.00001852
Iteration 355/1000 | Loss: 0.00001852
Iteration 356/1000 | Loss: 0.00001852
Iteration 357/1000 | Loss: 0.00001852
Iteration 358/1000 | Loss: 0.00001852
Iteration 359/1000 | Loss: 0.00001851
Iteration 360/1000 | Loss: 0.00001851
Iteration 361/1000 | Loss: 0.00001850
Iteration 362/1000 | Loss: 0.00001850
Iteration 363/1000 | Loss: 0.00001849
Iteration 364/1000 | Loss: 0.00001849
Iteration 365/1000 | Loss: 0.00001849
Iteration 366/1000 | Loss: 0.00001849
Iteration 367/1000 | Loss: 0.00001848
Iteration 368/1000 | Loss: 0.00001848
Iteration 369/1000 | Loss: 0.00001848
Iteration 370/1000 | Loss: 0.00001848
Iteration 371/1000 | Loss: 0.00001848
Iteration 372/1000 | Loss: 0.00001848
Iteration 373/1000 | Loss: 0.00001847
Iteration 374/1000 | Loss: 0.00001847
Iteration 375/1000 | Loss: 0.00001847
Iteration 376/1000 | Loss: 0.00001846
Iteration 377/1000 | Loss: 0.00001846
Iteration 378/1000 | Loss: 0.00001846
Iteration 379/1000 | Loss: 0.00001846
Iteration 380/1000 | Loss: 0.00001845
Iteration 381/1000 | Loss: 0.00001845
Iteration 382/1000 | Loss: 0.00001845
Iteration 383/1000 | Loss: 0.00001845
Iteration 384/1000 | Loss: 0.00001844
Iteration 385/1000 | Loss: 0.00001844
Iteration 386/1000 | Loss: 0.00001843
Iteration 387/1000 | Loss: 0.00001843
Iteration 388/1000 | Loss: 0.00001843
Iteration 389/1000 | Loss: 0.00001842
Iteration 390/1000 | Loss: 0.00001842
Iteration 391/1000 | Loss: 0.00001842
Iteration 392/1000 | Loss: 0.00001842
Iteration 393/1000 | Loss: 0.00001842
Iteration 394/1000 | Loss: 0.00001842
Iteration 395/1000 | Loss: 0.00001842
Iteration 396/1000 | Loss: 0.00001842
Iteration 397/1000 | Loss: 0.00001842
Iteration 398/1000 | Loss: 0.00001842
Iteration 399/1000 | Loss: 0.00001842
Iteration 400/1000 | Loss: 0.00001841
Iteration 401/1000 | Loss: 0.00001841
Iteration 402/1000 | Loss: 0.00001841
Iteration 403/1000 | Loss: 0.00001841
Iteration 404/1000 | Loss: 0.00001841
Iteration 405/1000 | Loss: 0.00001841
Iteration 406/1000 | Loss: 0.00001841
Iteration 407/1000 | Loss: 0.00001841
Iteration 408/1000 | Loss: 0.00001840
Iteration 409/1000 | Loss: 0.00001840
Iteration 410/1000 | Loss: 0.00001840
Iteration 411/1000 | Loss: 0.00001840
Iteration 412/1000 | Loss: 0.00001840
Iteration 413/1000 | Loss: 0.00001840
Iteration 414/1000 | Loss: 0.00001840
Iteration 415/1000 | Loss: 0.00001840
Iteration 416/1000 | Loss: 0.00001840
Iteration 417/1000 | Loss: 0.00001840
Iteration 418/1000 | Loss: 0.00001840
Iteration 419/1000 | Loss: 0.00001840
Iteration 420/1000 | Loss: 0.00001840
Iteration 421/1000 | Loss: 0.00001840
Iteration 422/1000 | Loss: 0.00001840
Iteration 423/1000 | Loss: 0.00001840
Iteration 424/1000 | Loss: 0.00001840
Iteration 425/1000 | Loss: 0.00001839
Iteration 426/1000 | Loss: 0.00001839
Iteration 427/1000 | Loss: 0.00001839
Iteration 428/1000 | Loss: 0.00001839
Iteration 429/1000 | Loss: 0.00001839
Iteration 430/1000 | Loss: 0.00001839
Iteration 431/1000 | Loss: 0.00001839
Iteration 432/1000 | Loss: 0.00001839
Iteration 433/1000 | Loss: 0.00001839
Iteration 434/1000 | Loss: 0.00001839
Iteration 435/1000 | Loss: 0.00001839
Iteration 436/1000 | Loss: 0.00001839
Iteration 437/1000 | Loss: 0.00001839
Iteration 438/1000 | Loss: 0.00001839
Iteration 439/1000 | Loss: 0.00001839
Iteration 440/1000 | Loss: 0.00001839
Iteration 441/1000 | Loss: 0.00001839
Iteration 442/1000 | Loss: 0.00001839
Iteration 443/1000 | Loss: 0.00001839
Iteration 444/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 444. Stopping optimization.
Last 5 losses: [1.8391390767646953e-05, 1.8391390767646953e-05, 1.8391390767646953e-05, 1.8391390767646953e-05, 1.8391390767646953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8391390767646953e-05

Optimization complete. Final v2v error: 3.5956740379333496 mm

Highest mean error: 3.9609930515289307 mm for frame 123

Lowest mean error: 3.4272983074188232 mm for frame 5

Saving results

Total time: 490.467333316803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741065
Iteration 2/25 | Loss: 0.00139874
Iteration 3/25 | Loss: 0.00125713
Iteration 4/25 | Loss: 0.00124584
Iteration 5/25 | Loss: 0.00124208
Iteration 6/25 | Loss: 0.00124152
Iteration 7/25 | Loss: 0.00124152
Iteration 8/25 | Loss: 0.00124152
Iteration 9/25 | Loss: 0.00124152
Iteration 10/25 | Loss: 0.00124152
Iteration 11/25 | Loss: 0.00124152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012415163218975067, 0.0012415163218975067, 0.0012415163218975067, 0.0012415163218975067, 0.0012415163218975067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012415163218975067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28091085
Iteration 2/25 | Loss: 0.00095023
Iteration 3/25 | Loss: 0.00095020
Iteration 4/25 | Loss: 0.00095020
Iteration 5/25 | Loss: 0.00095020
Iteration 6/25 | Loss: 0.00095020
Iteration 7/25 | Loss: 0.00095020
Iteration 8/25 | Loss: 0.00095020
Iteration 9/25 | Loss: 0.00095020
Iteration 10/25 | Loss: 0.00095020
Iteration 11/25 | Loss: 0.00095020
Iteration 12/25 | Loss: 0.00095020
Iteration 13/25 | Loss: 0.00095020
Iteration 14/25 | Loss: 0.00095020
Iteration 15/25 | Loss: 0.00095020
Iteration 16/25 | Loss: 0.00095020
Iteration 17/25 | Loss: 0.00095020
Iteration 18/25 | Loss: 0.00095020
Iteration 19/25 | Loss: 0.00095020
Iteration 20/25 | Loss: 0.00095020
Iteration 21/25 | Loss: 0.00095020
Iteration 22/25 | Loss: 0.00095020
Iteration 23/25 | Loss: 0.00095020
Iteration 24/25 | Loss: 0.00095020
Iteration 25/25 | Loss: 0.00095020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095020
Iteration 2/1000 | Loss: 0.00003114
Iteration 3/1000 | Loss: 0.00002250
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001828
Iteration 7/1000 | Loss: 0.00001782
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00001702
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001696
Iteration 14/1000 | Loss: 0.00001695
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001687
Iteration 17/1000 | Loss: 0.00001686
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001674
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001666
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001658
Iteration 26/1000 | Loss: 0.00001653
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001651
Iteration 29/1000 | Loss: 0.00001651
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001623
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001618
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001617
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001617
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001616
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001612
Iteration 90/1000 | Loss: 0.00001612
Iteration 91/1000 | Loss: 0.00001612
Iteration 92/1000 | Loss: 0.00001612
Iteration 93/1000 | Loss: 0.00001611
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001610
Iteration 104/1000 | Loss: 0.00001610
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001609
Iteration 113/1000 | Loss: 0.00001609
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001609
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001609
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001608
Iteration 122/1000 | Loss: 0.00001608
Iteration 123/1000 | Loss: 0.00001608
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001607
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001607
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001607
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001607
Iteration 144/1000 | Loss: 0.00001607
Iteration 145/1000 | Loss: 0.00001607
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001606
Iteration 149/1000 | Loss: 0.00001606
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001606
Iteration 155/1000 | Loss: 0.00001606
Iteration 156/1000 | Loss: 0.00001606
Iteration 157/1000 | Loss: 0.00001605
Iteration 158/1000 | Loss: 0.00001605
Iteration 159/1000 | Loss: 0.00001605
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001605
Iteration 162/1000 | Loss: 0.00001605
Iteration 163/1000 | Loss: 0.00001605
Iteration 164/1000 | Loss: 0.00001605
Iteration 165/1000 | Loss: 0.00001605
Iteration 166/1000 | Loss: 0.00001605
Iteration 167/1000 | Loss: 0.00001605
Iteration 168/1000 | Loss: 0.00001605
Iteration 169/1000 | Loss: 0.00001605
Iteration 170/1000 | Loss: 0.00001605
Iteration 171/1000 | Loss: 0.00001605
Iteration 172/1000 | Loss: 0.00001605
Iteration 173/1000 | Loss: 0.00001605
Iteration 174/1000 | Loss: 0.00001605
Iteration 175/1000 | Loss: 0.00001605
Iteration 176/1000 | Loss: 0.00001604
Iteration 177/1000 | Loss: 0.00001604
Iteration 178/1000 | Loss: 0.00001604
Iteration 179/1000 | Loss: 0.00001604
Iteration 180/1000 | Loss: 0.00001604
Iteration 181/1000 | Loss: 0.00001604
Iteration 182/1000 | Loss: 0.00001604
Iteration 183/1000 | Loss: 0.00001604
Iteration 184/1000 | Loss: 0.00001604
Iteration 185/1000 | Loss: 0.00001604
Iteration 186/1000 | Loss: 0.00001604
Iteration 187/1000 | Loss: 0.00001604
Iteration 188/1000 | Loss: 0.00001604
Iteration 189/1000 | Loss: 0.00001604
Iteration 190/1000 | Loss: 0.00001604
Iteration 191/1000 | Loss: 0.00001604
Iteration 192/1000 | Loss: 0.00001604
Iteration 193/1000 | Loss: 0.00001604
Iteration 194/1000 | Loss: 0.00001604
Iteration 195/1000 | Loss: 0.00001604
Iteration 196/1000 | Loss: 0.00001604
Iteration 197/1000 | Loss: 0.00001604
Iteration 198/1000 | Loss: 0.00001604
Iteration 199/1000 | Loss: 0.00001604
Iteration 200/1000 | Loss: 0.00001604
Iteration 201/1000 | Loss: 0.00001603
Iteration 202/1000 | Loss: 0.00001603
Iteration 203/1000 | Loss: 0.00001603
Iteration 204/1000 | Loss: 0.00001603
Iteration 205/1000 | Loss: 0.00001603
Iteration 206/1000 | Loss: 0.00001603
Iteration 207/1000 | Loss: 0.00001603
Iteration 208/1000 | Loss: 0.00001603
Iteration 209/1000 | Loss: 0.00001603
Iteration 210/1000 | Loss: 0.00001603
Iteration 211/1000 | Loss: 0.00001603
Iteration 212/1000 | Loss: 0.00001603
Iteration 213/1000 | Loss: 0.00001603
Iteration 214/1000 | Loss: 0.00001603
Iteration 215/1000 | Loss: 0.00001603
Iteration 216/1000 | Loss: 0.00001603
Iteration 217/1000 | Loss: 0.00001603
Iteration 218/1000 | Loss: 0.00001603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.6031348422984593e-05, 1.6031348422984593e-05, 1.6031348422984593e-05, 1.6031348422984593e-05, 1.6031348422984593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6031348422984593e-05

Optimization complete. Final v2v error: 3.4182891845703125 mm

Highest mean error: 3.759573459625244 mm for frame 4

Lowest mean error: 3.1282055377960205 mm for frame 239

Saving results

Total time: 49.36245322227478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825726
Iteration 2/25 | Loss: 0.00154135
Iteration 3/25 | Loss: 0.00134095
Iteration 4/25 | Loss: 0.00133033
Iteration 5/25 | Loss: 0.00132786
Iteration 6/25 | Loss: 0.00132765
Iteration 7/25 | Loss: 0.00132765
Iteration 8/25 | Loss: 0.00132765
Iteration 9/25 | Loss: 0.00132765
Iteration 10/25 | Loss: 0.00132765
Iteration 11/25 | Loss: 0.00132765
Iteration 12/25 | Loss: 0.00132765
Iteration 13/25 | Loss: 0.00132765
Iteration 14/25 | Loss: 0.00132765
Iteration 15/25 | Loss: 0.00132765
Iteration 16/25 | Loss: 0.00132765
Iteration 17/25 | Loss: 0.00132765
Iteration 18/25 | Loss: 0.00132765
Iteration 19/25 | Loss: 0.00132765
Iteration 20/25 | Loss: 0.00132765
Iteration 21/25 | Loss: 0.00132765
Iteration 22/25 | Loss: 0.00132765
Iteration 23/25 | Loss: 0.00132765
Iteration 24/25 | Loss: 0.00132765
Iteration 25/25 | Loss: 0.00132765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36284077
Iteration 2/25 | Loss: 0.00058773
Iteration 3/25 | Loss: 0.00058773
Iteration 4/25 | Loss: 0.00058772
Iteration 5/25 | Loss: 0.00058772
Iteration 6/25 | Loss: 0.00058772
Iteration 7/25 | Loss: 0.00058772
Iteration 8/25 | Loss: 0.00058772
Iteration 9/25 | Loss: 0.00058772
Iteration 10/25 | Loss: 0.00058772
Iteration 11/25 | Loss: 0.00058772
Iteration 12/25 | Loss: 0.00058772
Iteration 13/25 | Loss: 0.00058772
Iteration 14/25 | Loss: 0.00058772
Iteration 15/25 | Loss: 0.00058772
Iteration 16/25 | Loss: 0.00058772
Iteration 17/25 | Loss: 0.00058772
Iteration 18/25 | Loss: 0.00058772
Iteration 19/25 | Loss: 0.00058772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005877227522432804, 0.0005877227522432804, 0.0005877227522432804, 0.0005877227522432804, 0.0005877227522432804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005877227522432804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058772
Iteration 2/1000 | Loss: 0.00006122
Iteration 3/1000 | Loss: 0.00004662
Iteration 4/1000 | Loss: 0.00004458
Iteration 5/1000 | Loss: 0.00004290
Iteration 6/1000 | Loss: 0.00004183
Iteration 7/1000 | Loss: 0.00004097
Iteration 8/1000 | Loss: 0.00004037
Iteration 9/1000 | Loss: 0.00004002
Iteration 10/1000 | Loss: 0.00003975
Iteration 11/1000 | Loss: 0.00003954
Iteration 12/1000 | Loss: 0.00003948
Iteration 13/1000 | Loss: 0.00003933
Iteration 14/1000 | Loss: 0.00003920
Iteration 15/1000 | Loss: 0.00003920
Iteration 16/1000 | Loss: 0.00003919
Iteration 17/1000 | Loss: 0.00003918
Iteration 18/1000 | Loss: 0.00003918
Iteration 19/1000 | Loss: 0.00003917
Iteration 20/1000 | Loss: 0.00003917
Iteration 21/1000 | Loss: 0.00003917
Iteration 22/1000 | Loss: 0.00003917
Iteration 23/1000 | Loss: 0.00003917
Iteration 24/1000 | Loss: 0.00003916
Iteration 25/1000 | Loss: 0.00003916
Iteration 26/1000 | Loss: 0.00003916
Iteration 27/1000 | Loss: 0.00003916
Iteration 28/1000 | Loss: 0.00003915
Iteration 29/1000 | Loss: 0.00003915
Iteration 30/1000 | Loss: 0.00003915
Iteration 31/1000 | Loss: 0.00003914
Iteration 32/1000 | Loss: 0.00003914
Iteration 33/1000 | Loss: 0.00003914
Iteration 34/1000 | Loss: 0.00003914
Iteration 35/1000 | Loss: 0.00003914
Iteration 36/1000 | Loss: 0.00003914
Iteration 37/1000 | Loss: 0.00003913
Iteration 38/1000 | Loss: 0.00003912
Iteration 39/1000 | Loss: 0.00003912
Iteration 40/1000 | Loss: 0.00003911
Iteration 41/1000 | Loss: 0.00003911
Iteration 42/1000 | Loss: 0.00003910
Iteration 43/1000 | Loss: 0.00003909
Iteration 44/1000 | Loss: 0.00003909
Iteration 45/1000 | Loss: 0.00003909
Iteration 46/1000 | Loss: 0.00003909
Iteration 47/1000 | Loss: 0.00003908
Iteration 48/1000 | Loss: 0.00003908
Iteration 49/1000 | Loss: 0.00003908
Iteration 50/1000 | Loss: 0.00003908
Iteration 51/1000 | Loss: 0.00003908
Iteration 52/1000 | Loss: 0.00003907
Iteration 53/1000 | Loss: 0.00003907
Iteration 54/1000 | Loss: 0.00003907
Iteration 55/1000 | Loss: 0.00003906
Iteration 56/1000 | Loss: 0.00003906
Iteration 57/1000 | Loss: 0.00003906
Iteration 58/1000 | Loss: 0.00003906
Iteration 59/1000 | Loss: 0.00003906
Iteration 60/1000 | Loss: 0.00003906
Iteration 61/1000 | Loss: 0.00003906
Iteration 62/1000 | Loss: 0.00003906
Iteration 63/1000 | Loss: 0.00003906
Iteration 64/1000 | Loss: 0.00003906
Iteration 65/1000 | Loss: 0.00003905
Iteration 66/1000 | Loss: 0.00003905
Iteration 67/1000 | Loss: 0.00003905
Iteration 68/1000 | Loss: 0.00003904
Iteration 69/1000 | Loss: 0.00003904
Iteration 70/1000 | Loss: 0.00003904
Iteration 71/1000 | Loss: 0.00003904
Iteration 72/1000 | Loss: 0.00003904
Iteration 73/1000 | Loss: 0.00003904
Iteration 74/1000 | Loss: 0.00003904
Iteration 75/1000 | Loss: 0.00003904
Iteration 76/1000 | Loss: 0.00003904
Iteration 77/1000 | Loss: 0.00003904
Iteration 78/1000 | Loss: 0.00003904
Iteration 79/1000 | Loss: 0.00003903
Iteration 80/1000 | Loss: 0.00003902
Iteration 81/1000 | Loss: 0.00003902
Iteration 82/1000 | Loss: 0.00003902
Iteration 83/1000 | Loss: 0.00003901
Iteration 84/1000 | Loss: 0.00003901
Iteration 85/1000 | Loss: 0.00003901
Iteration 86/1000 | Loss: 0.00003900
Iteration 87/1000 | Loss: 0.00003900
Iteration 88/1000 | Loss: 0.00003900
Iteration 89/1000 | Loss: 0.00003900
Iteration 90/1000 | Loss: 0.00003899
Iteration 91/1000 | Loss: 0.00003899
Iteration 92/1000 | Loss: 0.00003899
Iteration 93/1000 | Loss: 0.00003899
Iteration 94/1000 | Loss: 0.00003899
Iteration 95/1000 | Loss: 0.00003899
Iteration 96/1000 | Loss: 0.00003899
Iteration 97/1000 | Loss: 0.00003899
Iteration 98/1000 | Loss: 0.00003899
Iteration 99/1000 | Loss: 0.00003899
Iteration 100/1000 | Loss: 0.00003898
Iteration 101/1000 | Loss: 0.00003898
Iteration 102/1000 | Loss: 0.00003898
Iteration 103/1000 | Loss: 0.00003898
Iteration 104/1000 | Loss: 0.00003898
Iteration 105/1000 | Loss: 0.00003898
Iteration 106/1000 | Loss: 0.00003898
Iteration 107/1000 | Loss: 0.00003898
Iteration 108/1000 | Loss: 0.00003898
Iteration 109/1000 | Loss: 0.00003898
Iteration 110/1000 | Loss: 0.00003898
Iteration 111/1000 | Loss: 0.00003898
Iteration 112/1000 | Loss: 0.00003898
Iteration 113/1000 | Loss: 0.00003897
Iteration 114/1000 | Loss: 0.00003897
Iteration 115/1000 | Loss: 0.00003897
Iteration 116/1000 | Loss: 0.00003897
Iteration 117/1000 | Loss: 0.00003897
Iteration 118/1000 | Loss: 0.00003896
Iteration 119/1000 | Loss: 0.00003896
Iteration 120/1000 | Loss: 0.00003896
Iteration 121/1000 | Loss: 0.00003896
Iteration 122/1000 | Loss: 0.00003896
Iteration 123/1000 | Loss: 0.00003896
Iteration 124/1000 | Loss: 0.00003896
Iteration 125/1000 | Loss: 0.00003896
Iteration 126/1000 | Loss: 0.00003896
Iteration 127/1000 | Loss: 0.00003896
Iteration 128/1000 | Loss: 0.00003896
Iteration 129/1000 | Loss: 0.00003895
Iteration 130/1000 | Loss: 0.00003895
Iteration 131/1000 | Loss: 0.00003895
Iteration 132/1000 | Loss: 0.00003895
Iteration 133/1000 | Loss: 0.00003895
Iteration 134/1000 | Loss: 0.00003894
Iteration 135/1000 | Loss: 0.00003894
Iteration 136/1000 | Loss: 0.00003894
Iteration 137/1000 | Loss: 0.00003894
Iteration 138/1000 | Loss: 0.00003894
Iteration 139/1000 | Loss: 0.00003893
Iteration 140/1000 | Loss: 0.00003893
Iteration 141/1000 | Loss: 0.00003893
Iteration 142/1000 | Loss: 0.00003893
Iteration 143/1000 | Loss: 0.00003893
Iteration 144/1000 | Loss: 0.00003893
Iteration 145/1000 | Loss: 0.00003892
Iteration 146/1000 | Loss: 0.00003892
Iteration 147/1000 | Loss: 0.00003892
Iteration 148/1000 | Loss: 0.00003892
Iteration 149/1000 | Loss: 0.00003892
Iteration 150/1000 | Loss: 0.00003891
Iteration 151/1000 | Loss: 0.00003891
Iteration 152/1000 | Loss: 0.00003891
Iteration 153/1000 | Loss: 0.00003891
Iteration 154/1000 | Loss: 0.00003891
Iteration 155/1000 | Loss: 0.00003891
Iteration 156/1000 | Loss: 0.00003891
Iteration 157/1000 | Loss: 0.00003891
Iteration 158/1000 | Loss: 0.00003891
Iteration 159/1000 | Loss: 0.00003891
Iteration 160/1000 | Loss: 0.00003891
Iteration 161/1000 | Loss: 0.00003891
Iteration 162/1000 | Loss: 0.00003891
Iteration 163/1000 | Loss: 0.00003891
Iteration 164/1000 | Loss: 0.00003891
Iteration 165/1000 | Loss: 0.00003891
Iteration 166/1000 | Loss: 0.00003891
Iteration 167/1000 | Loss: 0.00003891
Iteration 168/1000 | Loss: 0.00003891
Iteration 169/1000 | Loss: 0.00003891
Iteration 170/1000 | Loss: 0.00003891
Iteration 171/1000 | Loss: 0.00003891
Iteration 172/1000 | Loss: 0.00003891
Iteration 173/1000 | Loss: 0.00003891
Iteration 174/1000 | Loss: 0.00003891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.8906189729459584e-05, 3.8906189729459584e-05, 3.8906189729459584e-05, 3.8906189729459584e-05, 3.8906189729459584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8906189729459584e-05

Optimization complete. Final v2v error: 5.176769256591797 mm

Highest mean error: 5.626370429992676 mm for frame 18

Lowest mean error: 4.672922134399414 mm for frame 31

Saving results

Total time: 37.27760314941406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865420
Iteration 2/25 | Loss: 0.00160861
Iteration 3/25 | Loss: 0.00139334
Iteration 4/25 | Loss: 0.00136160
Iteration 5/25 | Loss: 0.00132582
Iteration 6/25 | Loss: 0.00132001
Iteration 7/25 | Loss: 0.00131318
Iteration 8/25 | Loss: 0.00131053
Iteration 9/25 | Loss: 0.00130878
Iteration 10/25 | Loss: 0.00130958
Iteration 11/25 | Loss: 0.00130548
Iteration 12/25 | Loss: 0.00130439
Iteration 13/25 | Loss: 0.00130411
Iteration 14/25 | Loss: 0.00130400
Iteration 15/25 | Loss: 0.00130399
Iteration 16/25 | Loss: 0.00130399
Iteration 17/25 | Loss: 0.00130399
Iteration 18/25 | Loss: 0.00130399
Iteration 19/25 | Loss: 0.00130398
Iteration 20/25 | Loss: 0.00130398
Iteration 21/25 | Loss: 0.00130398
Iteration 22/25 | Loss: 0.00130398
Iteration 23/25 | Loss: 0.00130398
Iteration 24/25 | Loss: 0.00130398
Iteration 25/25 | Loss: 0.00130398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43252993
Iteration 2/25 | Loss: 0.00086164
Iteration 3/25 | Loss: 0.00086161
Iteration 4/25 | Loss: 0.00086160
Iteration 5/25 | Loss: 0.00086160
Iteration 6/25 | Loss: 0.00086160
Iteration 7/25 | Loss: 0.00086160
Iteration 8/25 | Loss: 0.00086160
Iteration 9/25 | Loss: 0.00086160
Iteration 10/25 | Loss: 0.00086160
Iteration 11/25 | Loss: 0.00086160
Iteration 12/25 | Loss: 0.00086160
Iteration 13/25 | Loss: 0.00086160
Iteration 14/25 | Loss: 0.00086160
Iteration 15/25 | Loss: 0.00086160
Iteration 16/25 | Loss: 0.00086160
Iteration 17/25 | Loss: 0.00086160
Iteration 18/25 | Loss: 0.00086160
Iteration 19/25 | Loss: 0.00086160
Iteration 20/25 | Loss: 0.00086160
Iteration 21/25 | Loss: 0.00086160
Iteration 22/25 | Loss: 0.00086160
Iteration 23/25 | Loss: 0.00086160
Iteration 24/25 | Loss: 0.00086160
Iteration 25/25 | Loss: 0.00086160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008616006816737354, 0.0008616006816737354, 0.0008616006816737354, 0.0008616006816737354, 0.0008616006816737354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008616006816737354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086160
Iteration 2/1000 | Loss: 0.00003664
Iteration 3/1000 | Loss: 0.00002472
Iteration 4/1000 | Loss: 0.00002270
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002102
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002027
Iteration 9/1000 | Loss: 0.00002003
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00001962
Iteration 12/1000 | Loss: 0.00001950
Iteration 13/1000 | Loss: 0.00001942
Iteration 14/1000 | Loss: 0.00001928
Iteration 15/1000 | Loss: 0.00001924
Iteration 16/1000 | Loss: 0.00001920
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001918
Iteration 19/1000 | Loss: 0.00001917
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001917
Iteration 22/1000 | Loss: 0.00001917
Iteration 23/1000 | Loss: 0.00001916
Iteration 24/1000 | Loss: 0.00001916
Iteration 25/1000 | Loss: 0.00001915
Iteration 26/1000 | Loss: 0.00001914
Iteration 27/1000 | Loss: 0.00001913
Iteration 28/1000 | Loss: 0.00001913
Iteration 29/1000 | Loss: 0.00001913
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001912
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001908
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001907
Iteration 40/1000 | Loss: 0.00001907
Iteration 41/1000 | Loss: 0.00001906
Iteration 42/1000 | Loss: 0.00001906
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001905
Iteration 46/1000 | Loss: 0.00001905
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001904
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001904
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001903
Iteration 60/1000 | Loss: 0.00001903
Iteration 61/1000 | Loss: 0.00001903
Iteration 62/1000 | Loss: 0.00001902
Iteration 63/1000 | Loss: 0.00001902
Iteration 64/1000 | Loss: 0.00001902
Iteration 65/1000 | Loss: 0.00001902
Iteration 66/1000 | Loss: 0.00001901
Iteration 67/1000 | Loss: 0.00001901
Iteration 68/1000 | Loss: 0.00001901
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001900
Iteration 72/1000 | Loss: 0.00001900
Iteration 73/1000 | Loss: 0.00001900
Iteration 74/1000 | Loss: 0.00001900
Iteration 75/1000 | Loss: 0.00001900
Iteration 76/1000 | Loss: 0.00001900
Iteration 77/1000 | Loss: 0.00001900
Iteration 78/1000 | Loss: 0.00001900
Iteration 79/1000 | Loss: 0.00001900
Iteration 80/1000 | Loss: 0.00001900
Iteration 81/1000 | Loss: 0.00001900
Iteration 82/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.900094503071159e-05, 1.900094503071159e-05, 1.900094503071159e-05, 1.900094503071159e-05, 1.900094503071159e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.900094503071159e-05

Optimization complete. Final v2v error: 3.7294600009918213 mm

Highest mean error: 4.3702616691589355 mm for frame 12

Lowest mean error: 3.275096893310547 mm for frame 225

Saving results

Total time: 55.29024267196655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006545
Iteration 2/25 | Loss: 0.00222829
Iteration 3/25 | Loss: 0.00151845
Iteration 4/25 | Loss: 0.00137234
Iteration 5/25 | Loss: 0.00136027
Iteration 6/25 | Loss: 0.00130784
Iteration 7/25 | Loss: 0.00130680
Iteration 8/25 | Loss: 0.00128657
Iteration 9/25 | Loss: 0.00127890
Iteration 10/25 | Loss: 0.00128508
Iteration 11/25 | Loss: 0.00128658
Iteration 12/25 | Loss: 0.00127238
Iteration 13/25 | Loss: 0.00126211
Iteration 14/25 | Loss: 0.00125907
Iteration 15/25 | Loss: 0.00125975
Iteration 16/25 | Loss: 0.00126396
Iteration 17/25 | Loss: 0.00126481
Iteration 18/25 | Loss: 0.00126888
Iteration 19/25 | Loss: 0.00126547
Iteration 20/25 | Loss: 0.00125566
Iteration 21/25 | Loss: 0.00125259
Iteration 22/25 | Loss: 0.00125283
Iteration 23/25 | Loss: 0.00125031
Iteration 24/25 | Loss: 0.00124421
Iteration 25/25 | Loss: 0.00123769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28908420
Iteration 2/25 | Loss: 0.00162021
Iteration 3/25 | Loss: 0.00162021
Iteration 4/25 | Loss: 0.00162021
Iteration 5/25 | Loss: 0.00162021
Iteration 6/25 | Loss: 0.00162021
Iteration 7/25 | Loss: 0.00162021
Iteration 8/25 | Loss: 0.00162021
Iteration 9/25 | Loss: 0.00162021
Iteration 10/25 | Loss: 0.00162021
Iteration 11/25 | Loss: 0.00162020
Iteration 12/25 | Loss: 0.00162020
Iteration 13/25 | Loss: 0.00162020
Iteration 14/25 | Loss: 0.00162020
Iteration 15/25 | Loss: 0.00162020
Iteration 16/25 | Loss: 0.00162020
Iteration 17/25 | Loss: 0.00162020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016202046535909176, 0.0016202046535909176, 0.0016202046535909176, 0.0016202046535909176, 0.0016202046535909176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016202046535909176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162020
Iteration 2/1000 | Loss: 0.00026002
Iteration 3/1000 | Loss: 0.00045104
Iteration 4/1000 | Loss: 0.00026804
Iteration 5/1000 | Loss: 0.00012012
Iteration 6/1000 | Loss: 0.00024641
Iteration 7/1000 | Loss: 0.00026307
Iteration 8/1000 | Loss: 0.00030182
Iteration 9/1000 | Loss: 0.00032287
Iteration 10/1000 | Loss: 0.00029374
Iteration 11/1000 | Loss: 0.00067981
Iteration 12/1000 | Loss: 0.00085649
Iteration 13/1000 | Loss: 0.00065386
Iteration 14/1000 | Loss: 0.00115860
Iteration 15/1000 | Loss: 0.00284672
Iteration 16/1000 | Loss: 0.00138518
Iteration 17/1000 | Loss: 0.00106416
Iteration 18/1000 | Loss: 0.00042145
Iteration 19/1000 | Loss: 0.00058211
Iteration 20/1000 | Loss: 0.00187975
Iteration 21/1000 | Loss: 0.00125755
Iteration 22/1000 | Loss: 0.00083000
Iteration 23/1000 | Loss: 0.00057623
Iteration 24/1000 | Loss: 0.00075870
Iteration 25/1000 | Loss: 0.00101055
Iteration 26/1000 | Loss: 0.00183864
Iteration 27/1000 | Loss: 0.00171182
Iteration 28/1000 | Loss: 0.00252358
Iteration 29/1000 | Loss: 0.00164088
Iteration 30/1000 | Loss: 0.00229657
Iteration 31/1000 | Loss: 0.00216884
Iteration 32/1000 | Loss: 0.00225813
Iteration 33/1000 | Loss: 0.00120374
Iteration 34/1000 | Loss: 0.00153336
Iteration 35/1000 | Loss: 0.00112385
Iteration 36/1000 | Loss: 0.00100426
Iteration 37/1000 | Loss: 0.00123032
Iteration 38/1000 | Loss: 0.00018176
Iteration 39/1000 | Loss: 0.00019363
Iteration 40/1000 | Loss: 0.00041740
Iteration 41/1000 | Loss: 0.00042553
Iteration 42/1000 | Loss: 0.00048870
Iteration 43/1000 | Loss: 0.00149573
Iteration 44/1000 | Loss: 0.00106356
Iteration 45/1000 | Loss: 0.00143484
Iteration 46/1000 | Loss: 0.00201414
Iteration 47/1000 | Loss: 0.00154993
Iteration 48/1000 | Loss: 0.00183488
Iteration 49/1000 | Loss: 0.00080868
Iteration 50/1000 | Loss: 0.00132687
Iteration 51/1000 | Loss: 0.00062366
Iteration 52/1000 | Loss: 0.00432893
Iteration 53/1000 | Loss: 0.00135141
Iteration 54/1000 | Loss: 0.00105029
Iteration 55/1000 | Loss: 0.00068297
Iteration 56/1000 | Loss: 0.00077370
Iteration 57/1000 | Loss: 0.00031387
Iteration 58/1000 | Loss: 0.00032101
Iteration 59/1000 | Loss: 0.00043217
Iteration 60/1000 | Loss: 0.00073395
Iteration 61/1000 | Loss: 0.00043962
Iteration 62/1000 | Loss: 0.00057050
Iteration 63/1000 | Loss: 0.00061295
Iteration 64/1000 | Loss: 0.00071433
Iteration 65/1000 | Loss: 0.00071819
Iteration 66/1000 | Loss: 0.00078679
Iteration 67/1000 | Loss: 0.00069471
Iteration 68/1000 | Loss: 0.00048244
Iteration 69/1000 | Loss: 0.00072075
Iteration 70/1000 | Loss: 0.00041561
Iteration 71/1000 | Loss: 0.00032357
Iteration 72/1000 | Loss: 0.00100408
Iteration 73/1000 | Loss: 0.00096411
Iteration 74/1000 | Loss: 0.00044436
Iteration 75/1000 | Loss: 0.00059689
Iteration 76/1000 | Loss: 0.00047559
Iteration 77/1000 | Loss: 0.00058646
Iteration 78/1000 | Loss: 0.00057926
Iteration 79/1000 | Loss: 0.00016739
Iteration 80/1000 | Loss: 0.00054431
Iteration 81/1000 | Loss: 0.00047632
Iteration 82/1000 | Loss: 0.00061603
Iteration 83/1000 | Loss: 0.00061328
Iteration 84/1000 | Loss: 0.00030696
Iteration 85/1000 | Loss: 0.00071993
Iteration 86/1000 | Loss: 0.00053899
Iteration 87/1000 | Loss: 0.00076968
Iteration 88/1000 | Loss: 0.00054176
Iteration 89/1000 | Loss: 0.00079239
Iteration 90/1000 | Loss: 0.00014749
Iteration 91/1000 | Loss: 0.00026074
Iteration 92/1000 | Loss: 0.00016074
Iteration 93/1000 | Loss: 0.00030698
Iteration 94/1000 | Loss: 0.00045588
Iteration 95/1000 | Loss: 0.00038944
Iteration 96/1000 | Loss: 0.00019074
Iteration 97/1000 | Loss: 0.00028560
Iteration 98/1000 | Loss: 0.00013113
Iteration 99/1000 | Loss: 0.00026077
Iteration 100/1000 | Loss: 0.00033607
Iteration 101/1000 | Loss: 0.00035842
Iteration 102/1000 | Loss: 0.00023876
Iteration 103/1000 | Loss: 0.00036930
Iteration 104/1000 | Loss: 0.00027470
Iteration 105/1000 | Loss: 0.00050240
Iteration 106/1000 | Loss: 0.00050424
Iteration 107/1000 | Loss: 0.00032404
Iteration 108/1000 | Loss: 0.00039345
Iteration 109/1000 | Loss: 0.00037210
Iteration 110/1000 | Loss: 0.00041460
Iteration 111/1000 | Loss: 0.00025929
Iteration 112/1000 | Loss: 0.00062959
Iteration 113/1000 | Loss: 0.00045197
Iteration 114/1000 | Loss: 0.00048960
Iteration 115/1000 | Loss: 0.00084663
Iteration 116/1000 | Loss: 0.00047533
Iteration 117/1000 | Loss: 0.00040520
Iteration 118/1000 | Loss: 0.00032423
Iteration 119/1000 | Loss: 0.00052569
Iteration 120/1000 | Loss: 0.00064374
Iteration 121/1000 | Loss: 0.00060716
Iteration 122/1000 | Loss: 0.00035512
Iteration 123/1000 | Loss: 0.00032362
Iteration 124/1000 | Loss: 0.00033937
Iteration 125/1000 | Loss: 0.00041393
Iteration 126/1000 | Loss: 0.00051432
Iteration 127/1000 | Loss: 0.00042454
Iteration 128/1000 | Loss: 0.00004712
Iteration 129/1000 | Loss: 0.00024540
Iteration 130/1000 | Loss: 0.00028848
Iteration 131/1000 | Loss: 0.00024255
Iteration 132/1000 | Loss: 0.00015987
Iteration 133/1000 | Loss: 0.00015121
Iteration 134/1000 | Loss: 0.00034338
Iteration 135/1000 | Loss: 0.00059418
Iteration 136/1000 | Loss: 0.00034120
Iteration 137/1000 | Loss: 0.00010269
Iteration 138/1000 | Loss: 0.00038835
Iteration 139/1000 | Loss: 0.00010743
Iteration 140/1000 | Loss: 0.00035725
Iteration 141/1000 | Loss: 0.00022607
Iteration 142/1000 | Loss: 0.00025649
Iteration 143/1000 | Loss: 0.00013121
Iteration 144/1000 | Loss: 0.00052957
Iteration 145/1000 | Loss: 0.00042436
Iteration 146/1000 | Loss: 0.00025486
Iteration 147/1000 | Loss: 0.00035246
Iteration 148/1000 | Loss: 0.00031110
Iteration 149/1000 | Loss: 0.00058391
Iteration 150/1000 | Loss: 0.00035336
Iteration 151/1000 | Loss: 0.00048162
Iteration 152/1000 | Loss: 0.00052932
Iteration 153/1000 | Loss: 0.00042724
Iteration 154/1000 | Loss: 0.00045354
Iteration 155/1000 | Loss: 0.00033440
Iteration 156/1000 | Loss: 0.00061623
Iteration 157/1000 | Loss: 0.00041649
Iteration 158/1000 | Loss: 0.00054129
Iteration 159/1000 | Loss: 0.00130941
Iteration 160/1000 | Loss: 0.00038969
Iteration 161/1000 | Loss: 0.00035454
Iteration 162/1000 | Loss: 0.00043351
Iteration 163/1000 | Loss: 0.00041363
Iteration 164/1000 | Loss: 0.00062257
Iteration 165/1000 | Loss: 0.00029810
Iteration 166/1000 | Loss: 0.00044905
Iteration 167/1000 | Loss: 0.00005795
Iteration 168/1000 | Loss: 0.00003542
Iteration 169/1000 | Loss: 0.00002943
Iteration 170/1000 | Loss: 0.00047645
Iteration 171/1000 | Loss: 0.00002970
Iteration 172/1000 | Loss: 0.00002404
Iteration 173/1000 | Loss: 0.00002214
Iteration 174/1000 | Loss: 0.00002073
Iteration 175/1000 | Loss: 0.00001952
Iteration 176/1000 | Loss: 0.00001876
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001794
Iteration 179/1000 | Loss: 0.00001767
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001753
Iteration 182/1000 | Loss: 0.00001745
Iteration 183/1000 | Loss: 0.00001744
Iteration 184/1000 | Loss: 0.00001742
Iteration 185/1000 | Loss: 0.00001740
Iteration 186/1000 | Loss: 0.00001738
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001727
Iteration 189/1000 | Loss: 0.00001721
Iteration 190/1000 | Loss: 0.00001721
Iteration 191/1000 | Loss: 0.00001715
Iteration 192/1000 | Loss: 0.00001715
Iteration 193/1000 | Loss: 0.00001714
Iteration 194/1000 | Loss: 0.00001714
Iteration 195/1000 | Loss: 0.00001713
Iteration 196/1000 | Loss: 0.00001713
Iteration 197/1000 | Loss: 0.00001712
Iteration 198/1000 | Loss: 0.00001712
Iteration 199/1000 | Loss: 0.00001711
Iteration 200/1000 | Loss: 0.00001711
Iteration 201/1000 | Loss: 0.00001711
Iteration 202/1000 | Loss: 0.00001711
Iteration 203/1000 | Loss: 0.00001710
Iteration 204/1000 | Loss: 0.00001710
Iteration 205/1000 | Loss: 0.00001709
Iteration 206/1000 | Loss: 0.00001709
Iteration 207/1000 | Loss: 0.00001708
Iteration 208/1000 | Loss: 0.00001707
Iteration 209/1000 | Loss: 0.00001705
Iteration 210/1000 | Loss: 0.00001703
Iteration 211/1000 | Loss: 0.00001701
Iteration 212/1000 | Loss: 0.00001700
Iteration 213/1000 | Loss: 0.00001699
Iteration 214/1000 | Loss: 0.00001699
Iteration 215/1000 | Loss: 0.00001699
Iteration 216/1000 | Loss: 0.00001698
Iteration 217/1000 | Loss: 0.00001698
Iteration 218/1000 | Loss: 0.00001697
Iteration 219/1000 | Loss: 0.00001697
Iteration 220/1000 | Loss: 0.00001696
Iteration 221/1000 | Loss: 0.00001695
Iteration 222/1000 | Loss: 0.00001695
Iteration 223/1000 | Loss: 0.00001695
Iteration 224/1000 | Loss: 0.00001693
Iteration 225/1000 | Loss: 0.00001693
Iteration 226/1000 | Loss: 0.00001693
Iteration 227/1000 | Loss: 0.00001693
Iteration 228/1000 | Loss: 0.00001693
Iteration 229/1000 | Loss: 0.00001693
Iteration 230/1000 | Loss: 0.00001693
Iteration 231/1000 | Loss: 0.00001693
Iteration 232/1000 | Loss: 0.00001692
Iteration 233/1000 | Loss: 0.00001692
Iteration 234/1000 | Loss: 0.00001691
Iteration 235/1000 | Loss: 0.00001690
Iteration 236/1000 | Loss: 0.00001690
Iteration 237/1000 | Loss: 0.00001690
Iteration 238/1000 | Loss: 0.00001690
Iteration 239/1000 | Loss: 0.00001689
Iteration 240/1000 | Loss: 0.00001688
Iteration 241/1000 | Loss: 0.00001688
Iteration 242/1000 | Loss: 0.00001687
Iteration 243/1000 | Loss: 0.00001687
Iteration 244/1000 | Loss: 0.00001686
Iteration 245/1000 | Loss: 0.00001686
Iteration 246/1000 | Loss: 0.00001686
Iteration 247/1000 | Loss: 0.00001686
Iteration 248/1000 | Loss: 0.00001686
Iteration 249/1000 | Loss: 0.00001685
Iteration 250/1000 | Loss: 0.00001685
Iteration 251/1000 | Loss: 0.00001685
Iteration 252/1000 | Loss: 0.00001684
Iteration 253/1000 | Loss: 0.00001684
Iteration 254/1000 | Loss: 0.00001684
Iteration 255/1000 | Loss: 0.00001683
Iteration 256/1000 | Loss: 0.00001683
Iteration 257/1000 | Loss: 0.00001683
Iteration 258/1000 | Loss: 0.00001683
Iteration 259/1000 | Loss: 0.00001683
Iteration 260/1000 | Loss: 0.00001683
Iteration 261/1000 | Loss: 0.00001683
Iteration 262/1000 | Loss: 0.00001682
Iteration 263/1000 | Loss: 0.00001682
Iteration 264/1000 | Loss: 0.00001682
Iteration 265/1000 | Loss: 0.00001682
Iteration 266/1000 | Loss: 0.00001682
Iteration 267/1000 | Loss: 0.00001682
Iteration 268/1000 | Loss: 0.00001681
Iteration 269/1000 | Loss: 0.00001681
Iteration 270/1000 | Loss: 0.00001681
Iteration 271/1000 | Loss: 0.00001681
Iteration 272/1000 | Loss: 0.00001681
Iteration 273/1000 | Loss: 0.00001680
Iteration 274/1000 | Loss: 0.00001680
Iteration 275/1000 | Loss: 0.00001680
Iteration 276/1000 | Loss: 0.00001679
Iteration 277/1000 | Loss: 0.00001679
Iteration 278/1000 | Loss: 0.00001679
Iteration 279/1000 | Loss: 0.00001679
Iteration 280/1000 | Loss: 0.00001679
Iteration 281/1000 | Loss: 0.00001678
Iteration 282/1000 | Loss: 0.00001678
Iteration 283/1000 | Loss: 0.00001678
Iteration 284/1000 | Loss: 0.00001678
Iteration 285/1000 | Loss: 0.00001677
Iteration 286/1000 | Loss: 0.00001677
Iteration 287/1000 | Loss: 0.00001677
Iteration 288/1000 | Loss: 0.00001677
Iteration 289/1000 | Loss: 0.00001677
Iteration 290/1000 | Loss: 0.00001677
Iteration 291/1000 | Loss: 0.00001677
Iteration 292/1000 | Loss: 0.00001677
Iteration 293/1000 | Loss: 0.00001676
Iteration 294/1000 | Loss: 0.00001676
Iteration 295/1000 | Loss: 0.00001676
Iteration 296/1000 | Loss: 0.00001676
Iteration 297/1000 | Loss: 0.00001676
Iteration 298/1000 | Loss: 0.00001676
Iteration 299/1000 | Loss: 0.00001676
Iteration 300/1000 | Loss: 0.00001676
Iteration 301/1000 | Loss: 0.00001675
Iteration 302/1000 | Loss: 0.00001675
Iteration 303/1000 | Loss: 0.00001675
Iteration 304/1000 | Loss: 0.00001675
Iteration 305/1000 | Loss: 0.00001675
Iteration 306/1000 | Loss: 0.00001675
Iteration 307/1000 | Loss: 0.00001675
Iteration 308/1000 | Loss: 0.00001674
Iteration 309/1000 | Loss: 0.00001674
Iteration 310/1000 | Loss: 0.00001674
Iteration 311/1000 | Loss: 0.00001674
Iteration 312/1000 | Loss: 0.00001674
Iteration 313/1000 | Loss: 0.00001674
Iteration 314/1000 | Loss: 0.00001674
Iteration 315/1000 | Loss: 0.00001674
Iteration 316/1000 | Loss: 0.00001673
Iteration 317/1000 | Loss: 0.00001673
Iteration 318/1000 | Loss: 0.00001673
Iteration 319/1000 | Loss: 0.00001673
Iteration 320/1000 | Loss: 0.00001672
Iteration 321/1000 | Loss: 0.00001672
Iteration 322/1000 | Loss: 0.00001672
Iteration 323/1000 | Loss: 0.00001672
Iteration 324/1000 | Loss: 0.00001672
Iteration 325/1000 | Loss: 0.00001672
Iteration 326/1000 | Loss: 0.00001672
Iteration 327/1000 | Loss: 0.00001672
Iteration 328/1000 | Loss: 0.00001672
Iteration 329/1000 | Loss: 0.00001672
Iteration 330/1000 | Loss: 0.00001672
Iteration 331/1000 | Loss: 0.00001672
Iteration 332/1000 | Loss: 0.00001672
Iteration 333/1000 | Loss: 0.00001672
Iteration 334/1000 | Loss: 0.00001672
Iteration 335/1000 | Loss: 0.00001672
Iteration 336/1000 | Loss: 0.00001672
Iteration 337/1000 | Loss: 0.00001672
Iteration 338/1000 | Loss: 0.00001672
Iteration 339/1000 | Loss: 0.00001672
Iteration 340/1000 | Loss: 0.00001672
Iteration 341/1000 | Loss: 0.00001672
Iteration 342/1000 | Loss: 0.00001672
Iteration 343/1000 | Loss: 0.00001672
Iteration 344/1000 | Loss: 0.00001672
Iteration 345/1000 | Loss: 0.00001672
Iteration 346/1000 | Loss: 0.00001672
Iteration 347/1000 | Loss: 0.00001672
Iteration 348/1000 | Loss: 0.00001672
Iteration 349/1000 | Loss: 0.00001672
Iteration 350/1000 | Loss: 0.00001672
Iteration 351/1000 | Loss: 0.00001672
Iteration 352/1000 | Loss: 0.00001672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [1.671789141255431e-05, 1.671789141255431e-05, 1.671789141255431e-05, 1.671789141255431e-05, 1.671789141255431e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.671789141255431e-05

Optimization complete. Final v2v error: 3.4383976459503174 mm

Highest mean error: 5.341216087341309 mm for frame 233

Lowest mean error: 3.0402908325195312 mm for frame 44

Saving results

Total time: 354.8310556411743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395078
Iteration 2/25 | Loss: 0.00122901
Iteration 3/25 | Loss: 0.00117547
Iteration 4/25 | Loss: 0.00116793
Iteration 5/25 | Loss: 0.00116659
Iteration 6/25 | Loss: 0.00116659
Iteration 7/25 | Loss: 0.00116659
Iteration 8/25 | Loss: 0.00116659
Iteration 9/25 | Loss: 0.00116659
Iteration 10/25 | Loss: 0.00116659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011665893252938986, 0.0011665893252938986, 0.0011665893252938986, 0.0011665893252938986, 0.0011665893252938986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011665893252938986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40290332
Iteration 2/25 | Loss: 0.00095421
Iteration 3/25 | Loss: 0.00095421
Iteration 4/25 | Loss: 0.00095421
Iteration 5/25 | Loss: 0.00095421
Iteration 6/25 | Loss: 0.00095421
Iteration 7/25 | Loss: 0.00095421
Iteration 8/25 | Loss: 0.00095421
Iteration 9/25 | Loss: 0.00095421
Iteration 10/25 | Loss: 0.00095421
Iteration 11/25 | Loss: 0.00095421
Iteration 12/25 | Loss: 0.00095421
Iteration 13/25 | Loss: 0.00095421
Iteration 14/25 | Loss: 0.00095421
Iteration 15/25 | Loss: 0.00095421
Iteration 16/25 | Loss: 0.00095421
Iteration 17/25 | Loss: 0.00095421
Iteration 18/25 | Loss: 0.00095421
Iteration 19/25 | Loss: 0.00095421
Iteration 20/25 | Loss: 0.00095421
Iteration 21/25 | Loss: 0.00095421
Iteration 22/25 | Loss: 0.00095421
Iteration 23/25 | Loss: 0.00095421
Iteration 24/25 | Loss: 0.00095421
Iteration 25/25 | Loss: 0.00095421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095421
Iteration 2/1000 | Loss: 0.00001883
Iteration 3/1000 | Loss: 0.00001467
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001132
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001108
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001084
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001082
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001073
Iteration 19/1000 | Loss: 0.00001071
Iteration 20/1000 | Loss: 0.00001071
Iteration 21/1000 | Loss: 0.00001064
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001045
Iteration 24/1000 | Loss: 0.00001044
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001038
Iteration 29/1000 | Loss: 0.00001036
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001031
Iteration 40/1000 | Loss: 0.00001031
Iteration 41/1000 | Loss: 0.00001031
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001030
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001029
Iteration 46/1000 | Loss: 0.00001029
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001028
Iteration 51/1000 | Loss: 0.00001028
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001027
Iteration 55/1000 | Loss: 0.00001027
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001026
Iteration 58/1000 | Loss: 0.00001026
Iteration 59/1000 | Loss: 0.00001025
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001023
Iteration 66/1000 | Loss: 0.00001021
Iteration 67/1000 | Loss: 0.00001021
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001019
Iteration 72/1000 | Loss: 0.00001019
Iteration 73/1000 | Loss: 0.00001018
Iteration 74/1000 | Loss: 0.00001018
Iteration 75/1000 | Loss: 0.00001018
Iteration 76/1000 | Loss: 0.00001018
Iteration 77/1000 | Loss: 0.00001017
Iteration 78/1000 | Loss: 0.00001017
Iteration 79/1000 | Loss: 0.00001016
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001015
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001014
Iteration 84/1000 | Loss: 0.00001014
Iteration 85/1000 | Loss: 0.00001014
Iteration 86/1000 | Loss: 0.00001014
Iteration 87/1000 | Loss: 0.00001014
Iteration 88/1000 | Loss: 0.00001014
Iteration 89/1000 | Loss: 0.00001013
Iteration 90/1000 | Loss: 0.00001012
Iteration 91/1000 | Loss: 0.00001012
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001010
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001008
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001005
Iteration 101/1000 | Loss: 0.00001005
Iteration 102/1000 | Loss: 0.00001005
Iteration 103/1000 | Loss: 0.00001005
Iteration 104/1000 | Loss: 0.00001004
Iteration 105/1000 | Loss: 0.00001004
Iteration 106/1000 | Loss: 0.00001004
Iteration 107/1000 | Loss: 0.00001004
Iteration 108/1000 | Loss: 0.00001004
Iteration 109/1000 | Loss: 0.00001004
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001000
Iteration 125/1000 | Loss: 0.00001000
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000999
Iteration 138/1000 | Loss: 0.00000999
Iteration 139/1000 | Loss: 0.00000999
Iteration 140/1000 | Loss: 0.00000999
Iteration 141/1000 | Loss: 0.00000999
Iteration 142/1000 | Loss: 0.00000999
Iteration 143/1000 | Loss: 0.00000999
Iteration 144/1000 | Loss: 0.00000999
Iteration 145/1000 | Loss: 0.00000999
Iteration 146/1000 | Loss: 0.00000999
Iteration 147/1000 | Loss: 0.00000999
Iteration 148/1000 | Loss: 0.00000999
Iteration 149/1000 | Loss: 0.00000999
Iteration 150/1000 | Loss: 0.00000999
Iteration 151/1000 | Loss: 0.00000999
Iteration 152/1000 | Loss: 0.00000999
Iteration 153/1000 | Loss: 0.00000999
Iteration 154/1000 | Loss: 0.00000999
Iteration 155/1000 | Loss: 0.00000999
Iteration 156/1000 | Loss: 0.00000999
Iteration 157/1000 | Loss: 0.00000999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [9.988380952563602e-06, 9.988380952563602e-06, 9.988380952563602e-06, 9.988380952563602e-06, 9.988380952563602e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.988380952563602e-06

Optimization complete. Final v2v error: 2.7437658309936523 mm

Highest mean error: 2.9305098056793213 mm for frame 126

Lowest mean error: 2.639096736907959 mm for frame 0

Saving results

Total time: 42.04524278640747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640641
Iteration 2/25 | Loss: 0.00153596
Iteration 3/25 | Loss: 0.00134745
Iteration 4/25 | Loss: 0.00128383
Iteration 5/25 | Loss: 0.00127909
Iteration 6/25 | Loss: 0.00127289
Iteration 7/25 | Loss: 0.00127036
Iteration 8/25 | Loss: 0.00126732
Iteration 9/25 | Loss: 0.00126627
Iteration 10/25 | Loss: 0.00126598
Iteration 11/25 | Loss: 0.00126583
Iteration 12/25 | Loss: 0.00126581
Iteration 13/25 | Loss: 0.00126581
Iteration 14/25 | Loss: 0.00126581
Iteration 15/25 | Loss: 0.00126581
Iteration 16/25 | Loss: 0.00126581
Iteration 17/25 | Loss: 0.00126577
Iteration 18/25 | Loss: 0.00126576
Iteration 19/25 | Loss: 0.00126574
Iteration 20/25 | Loss: 0.00126574
Iteration 21/25 | Loss: 0.00126574
Iteration 22/25 | Loss: 0.00126574
Iteration 23/25 | Loss: 0.00126574
Iteration 24/25 | Loss: 0.00126574
Iteration 25/25 | Loss: 0.00126574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90939760
Iteration 2/25 | Loss: 0.00098869
Iteration 3/25 | Loss: 0.00098865
Iteration 4/25 | Loss: 0.00098865
Iteration 5/25 | Loss: 0.00098865
Iteration 6/25 | Loss: 0.00098865
Iteration 7/25 | Loss: 0.00098865
Iteration 8/25 | Loss: 0.00098865
Iteration 9/25 | Loss: 0.00098865
Iteration 10/25 | Loss: 0.00098865
Iteration 11/25 | Loss: 0.00098865
Iteration 12/25 | Loss: 0.00098865
Iteration 13/25 | Loss: 0.00098865
Iteration 14/25 | Loss: 0.00098865
Iteration 15/25 | Loss: 0.00098865
Iteration 16/25 | Loss: 0.00098865
Iteration 17/25 | Loss: 0.00098865
Iteration 18/25 | Loss: 0.00098865
Iteration 19/25 | Loss: 0.00098865
Iteration 20/25 | Loss: 0.00098865
Iteration 21/25 | Loss: 0.00098865
Iteration 22/25 | Loss: 0.00098865
Iteration 23/25 | Loss: 0.00098865
Iteration 24/25 | Loss: 0.00098865
Iteration 25/25 | Loss: 0.00098865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098865
Iteration 2/1000 | Loss: 0.00002979
Iteration 3/1000 | Loss: 0.00007372
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001718
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00009115
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001585
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001529
Iteration 14/1000 | Loss: 0.00001529
Iteration 15/1000 | Loss: 0.00001528
Iteration 16/1000 | Loss: 0.00001528
Iteration 17/1000 | Loss: 0.00001528
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001508
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001504
Iteration 26/1000 | Loss: 0.00001504
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001500
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00005945
Iteration 33/1000 | Loss: 0.00005945
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001488
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001473
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001472
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001467
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001446
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001446
Iteration 112/1000 | Loss: 0.00001446
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001445
Iteration 116/1000 | Loss: 0.00001445
Iteration 117/1000 | Loss: 0.00001445
Iteration 118/1000 | Loss: 0.00001445
Iteration 119/1000 | Loss: 0.00001445
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001445
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001445
Iteration 125/1000 | Loss: 0.00001445
Iteration 126/1000 | Loss: 0.00001445
Iteration 127/1000 | Loss: 0.00001445
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001445
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001444
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001443
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001443
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Iteration 148/1000 | Loss: 0.00001442
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001441
Iteration 152/1000 | Loss: 0.00001441
Iteration 153/1000 | Loss: 0.00001441
Iteration 154/1000 | Loss: 0.00001441
Iteration 155/1000 | Loss: 0.00001441
Iteration 156/1000 | Loss: 0.00001440
Iteration 157/1000 | Loss: 0.00001440
Iteration 158/1000 | Loss: 0.00001440
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001439
Iteration 168/1000 | Loss: 0.00001439
Iteration 169/1000 | Loss: 0.00001439
Iteration 170/1000 | Loss: 0.00001439
Iteration 171/1000 | Loss: 0.00001439
Iteration 172/1000 | Loss: 0.00001439
Iteration 173/1000 | Loss: 0.00001439
Iteration 174/1000 | Loss: 0.00001439
Iteration 175/1000 | Loss: 0.00001439
Iteration 176/1000 | Loss: 0.00001439
Iteration 177/1000 | Loss: 0.00001439
Iteration 178/1000 | Loss: 0.00001439
Iteration 179/1000 | Loss: 0.00001439
Iteration 180/1000 | Loss: 0.00001439
Iteration 181/1000 | Loss: 0.00001439
Iteration 182/1000 | Loss: 0.00001439
Iteration 183/1000 | Loss: 0.00001439
Iteration 184/1000 | Loss: 0.00001439
Iteration 185/1000 | Loss: 0.00001439
Iteration 186/1000 | Loss: 0.00001439
Iteration 187/1000 | Loss: 0.00001439
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00001439
Iteration 190/1000 | Loss: 0.00001439
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001439
Iteration 197/1000 | Loss: 0.00001439
Iteration 198/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.4391457625606563e-05, 1.4391457625606563e-05, 1.4391457625606563e-05, 1.4391457625606563e-05, 1.4391457625606563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4391457625606563e-05

Optimization complete. Final v2v error: 3.1433777809143066 mm

Highest mean error: 4.008224964141846 mm for frame 92

Lowest mean error: 2.696232318878174 mm for frame 45

Saving results

Total time: 71.75198888778687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402748
Iteration 2/25 | Loss: 0.00124823
Iteration 3/25 | Loss: 0.00117904
Iteration 4/25 | Loss: 0.00116787
Iteration 5/25 | Loss: 0.00116443
Iteration 6/25 | Loss: 0.00116422
Iteration 7/25 | Loss: 0.00116422
Iteration 8/25 | Loss: 0.00116422
Iteration 9/25 | Loss: 0.00116422
Iteration 10/25 | Loss: 0.00116422
Iteration 11/25 | Loss: 0.00116422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011642202734947205, 0.0011642202734947205, 0.0011642202734947205, 0.0011642202734947205, 0.0011642202734947205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011642202734947205

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93636918
Iteration 2/25 | Loss: 0.00090785
Iteration 3/25 | Loss: 0.00090784
Iteration 4/25 | Loss: 0.00090784
Iteration 5/25 | Loss: 0.00090784
Iteration 6/25 | Loss: 0.00090784
Iteration 7/25 | Loss: 0.00090784
Iteration 8/25 | Loss: 0.00090784
Iteration 9/25 | Loss: 0.00090784
Iteration 10/25 | Loss: 0.00090784
Iteration 11/25 | Loss: 0.00090784
Iteration 12/25 | Loss: 0.00090784
Iteration 13/25 | Loss: 0.00090784
Iteration 14/25 | Loss: 0.00090784
Iteration 15/25 | Loss: 0.00090784
Iteration 16/25 | Loss: 0.00090784
Iteration 17/25 | Loss: 0.00090784
Iteration 18/25 | Loss: 0.00090784
Iteration 19/25 | Loss: 0.00090784
Iteration 20/25 | Loss: 0.00090784
Iteration 21/25 | Loss: 0.00090784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009078388684429228, 0.0009078388684429228, 0.0009078388684429228, 0.0009078388684429228, 0.0009078388684429228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009078388684429228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090784
Iteration 2/1000 | Loss: 0.00002368
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001499
Iteration 5/1000 | Loss: 0.00001377
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001207
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001171
Iteration 12/1000 | Loss: 0.00001144
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001137
Iteration 15/1000 | Loss: 0.00001136
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001113
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001106
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001106
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001101
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001080
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001074
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001071
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001070
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001067
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001067
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001064
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001062
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001060
Iteration 83/1000 | Loss: 0.00001060
Iteration 84/1000 | Loss: 0.00001060
Iteration 85/1000 | Loss: 0.00001059
Iteration 86/1000 | Loss: 0.00001059
Iteration 87/1000 | Loss: 0.00001059
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001055
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001055
Iteration 100/1000 | Loss: 0.00001055
Iteration 101/1000 | Loss: 0.00001055
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001054
Iteration 105/1000 | Loss: 0.00001054
Iteration 106/1000 | Loss: 0.00001054
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001053
Iteration 109/1000 | Loss: 0.00001053
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001053
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001052
Iteration 120/1000 | Loss: 0.00001052
Iteration 121/1000 | Loss: 0.00001052
Iteration 122/1000 | Loss: 0.00001052
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001052
Iteration 126/1000 | Loss: 0.00001051
Iteration 127/1000 | Loss: 0.00001051
Iteration 128/1000 | Loss: 0.00001051
Iteration 129/1000 | Loss: 0.00001051
Iteration 130/1000 | Loss: 0.00001051
Iteration 131/1000 | Loss: 0.00001051
Iteration 132/1000 | Loss: 0.00001051
Iteration 133/1000 | Loss: 0.00001051
Iteration 134/1000 | Loss: 0.00001051
Iteration 135/1000 | Loss: 0.00001051
Iteration 136/1000 | Loss: 0.00001051
Iteration 137/1000 | Loss: 0.00001051
Iteration 138/1000 | Loss: 0.00001051
Iteration 139/1000 | Loss: 0.00001051
Iteration 140/1000 | Loss: 0.00001051
Iteration 141/1000 | Loss: 0.00001051
Iteration 142/1000 | Loss: 0.00001050
Iteration 143/1000 | Loss: 0.00001050
Iteration 144/1000 | Loss: 0.00001050
Iteration 145/1000 | Loss: 0.00001050
Iteration 146/1000 | Loss: 0.00001050
Iteration 147/1000 | Loss: 0.00001050
Iteration 148/1000 | Loss: 0.00001050
Iteration 149/1000 | Loss: 0.00001050
Iteration 150/1000 | Loss: 0.00001049
Iteration 151/1000 | Loss: 0.00001049
Iteration 152/1000 | Loss: 0.00001049
Iteration 153/1000 | Loss: 0.00001049
Iteration 154/1000 | Loss: 0.00001049
Iteration 155/1000 | Loss: 0.00001049
Iteration 156/1000 | Loss: 0.00001049
Iteration 157/1000 | Loss: 0.00001049
Iteration 158/1000 | Loss: 0.00001049
Iteration 159/1000 | Loss: 0.00001049
Iteration 160/1000 | Loss: 0.00001049
Iteration 161/1000 | Loss: 0.00001049
Iteration 162/1000 | Loss: 0.00001049
Iteration 163/1000 | Loss: 0.00001049
Iteration 164/1000 | Loss: 0.00001049
Iteration 165/1000 | Loss: 0.00001049
Iteration 166/1000 | Loss: 0.00001049
Iteration 167/1000 | Loss: 0.00001049
Iteration 168/1000 | Loss: 0.00001048
Iteration 169/1000 | Loss: 0.00001048
Iteration 170/1000 | Loss: 0.00001048
Iteration 171/1000 | Loss: 0.00001048
Iteration 172/1000 | Loss: 0.00001048
Iteration 173/1000 | Loss: 0.00001048
Iteration 174/1000 | Loss: 0.00001048
Iteration 175/1000 | Loss: 0.00001048
Iteration 176/1000 | Loss: 0.00001048
Iteration 177/1000 | Loss: 0.00001048
Iteration 178/1000 | Loss: 0.00001047
Iteration 179/1000 | Loss: 0.00001047
Iteration 180/1000 | Loss: 0.00001047
Iteration 181/1000 | Loss: 0.00001047
Iteration 182/1000 | Loss: 0.00001047
Iteration 183/1000 | Loss: 0.00001047
Iteration 184/1000 | Loss: 0.00001047
Iteration 185/1000 | Loss: 0.00001047
Iteration 186/1000 | Loss: 0.00001046
Iteration 187/1000 | Loss: 0.00001046
Iteration 188/1000 | Loss: 0.00001046
Iteration 189/1000 | Loss: 0.00001046
Iteration 190/1000 | Loss: 0.00001046
Iteration 191/1000 | Loss: 0.00001046
Iteration 192/1000 | Loss: 0.00001046
Iteration 193/1000 | Loss: 0.00001046
Iteration 194/1000 | Loss: 0.00001046
Iteration 195/1000 | Loss: 0.00001046
Iteration 196/1000 | Loss: 0.00001046
Iteration 197/1000 | Loss: 0.00001046
Iteration 198/1000 | Loss: 0.00001046
Iteration 199/1000 | Loss: 0.00001046
Iteration 200/1000 | Loss: 0.00001046
Iteration 201/1000 | Loss: 0.00001046
Iteration 202/1000 | Loss: 0.00001046
Iteration 203/1000 | Loss: 0.00001045
Iteration 204/1000 | Loss: 0.00001045
Iteration 205/1000 | Loss: 0.00001045
Iteration 206/1000 | Loss: 0.00001045
Iteration 207/1000 | Loss: 0.00001045
Iteration 208/1000 | Loss: 0.00001045
Iteration 209/1000 | Loss: 0.00001045
Iteration 210/1000 | Loss: 0.00001045
Iteration 211/1000 | Loss: 0.00001045
Iteration 212/1000 | Loss: 0.00001045
Iteration 213/1000 | Loss: 0.00001045
Iteration 214/1000 | Loss: 0.00001045
Iteration 215/1000 | Loss: 0.00001045
Iteration 216/1000 | Loss: 0.00001044
Iteration 217/1000 | Loss: 0.00001044
Iteration 218/1000 | Loss: 0.00001044
Iteration 219/1000 | Loss: 0.00001044
Iteration 220/1000 | Loss: 0.00001044
Iteration 221/1000 | Loss: 0.00001044
Iteration 222/1000 | Loss: 0.00001044
Iteration 223/1000 | Loss: 0.00001044
Iteration 224/1000 | Loss: 0.00001044
Iteration 225/1000 | Loss: 0.00001044
Iteration 226/1000 | Loss: 0.00001044
Iteration 227/1000 | Loss: 0.00001044
Iteration 228/1000 | Loss: 0.00001044
Iteration 229/1000 | Loss: 0.00001044
Iteration 230/1000 | Loss: 0.00001044
Iteration 231/1000 | Loss: 0.00001044
Iteration 232/1000 | Loss: 0.00001044
Iteration 233/1000 | Loss: 0.00001044
Iteration 234/1000 | Loss: 0.00001044
Iteration 235/1000 | Loss: 0.00001044
Iteration 236/1000 | Loss: 0.00001044
Iteration 237/1000 | Loss: 0.00001044
Iteration 238/1000 | Loss: 0.00001044
Iteration 239/1000 | Loss: 0.00001044
Iteration 240/1000 | Loss: 0.00001044
Iteration 241/1000 | Loss: 0.00001044
Iteration 242/1000 | Loss: 0.00001044
Iteration 243/1000 | Loss: 0.00001044
Iteration 244/1000 | Loss: 0.00001044
Iteration 245/1000 | Loss: 0.00001044
Iteration 246/1000 | Loss: 0.00001044
Iteration 247/1000 | Loss: 0.00001044
Iteration 248/1000 | Loss: 0.00001044
Iteration 249/1000 | Loss: 0.00001044
Iteration 250/1000 | Loss: 0.00001044
Iteration 251/1000 | Loss: 0.00001044
Iteration 252/1000 | Loss: 0.00001044
Iteration 253/1000 | Loss: 0.00001044
Iteration 254/1000 | Loss: 0.00001044
Iteration 255/1000 | Loss: 0.00001044
Iteration 256/1000 | Loss: 0.00001044
Iteration 257/1000 | Loss: 0.00001044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.0443177416163962e-05, 1.0443177416163962e-05, 1.0443177416163962e-05, 1.0443177416163962e-05, 1.0443177416163962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0443177416163962e-05

Optimization complete. Final v2v error: 2.792870044708252 mm

Highest mean error: 3.1601078510284424 mm for frame 90

Lowest mean error: 2.7011630535125732 mm for frame 135

Saving results

Total time: 43.63451862335205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823656
Iteration 2/25 | Loss: 0.00124222
Iteration 3/25 | Loss: 0.00119227
Iteration 4/25 | Loss: 0.00118499
Iteration 5/25 | Loss: 0.00118350
Iteration 6/25 | Loss: 0.00118331
Iteration 7/25 | Loss: 0.00118331
Iteration 8/25 | Loss: 0.00118331
Iteration 9/25 | Loss: 0.00118331
Iteration 10/25 | Loss: 0.00118331
Iteration 11/25 | Loss: 0.00118331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011833109892904758, 0.0011833109892904758, 0.0011833109892904758, 0.0011833109892904758, 0.0011833109892904758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011833109892904758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46429253
Iteration 2/25 | Loss: 0.00092720
Iteration 3/25 | Loss: 0.00092720
Iteration 4/25 | Loss: 0.00092720
Iteration 5/25 | Loss: 0.00092720
Iteration 6/25 | Loss: 0.00092720
Iteration 7/25 | Loss: 0.00092720
Iteration 8/25 | Loss: 0.00092720
Iteration 9/25 | Loss: 0.00092720
Iteration 10/25 | Loss: 0.00092720
Iteration 11/25 | Loss: 0.00092720
Iteration 12/25 | Loss: 0.00092720
Iteration 13/25 | Loss: 0.00092720
Iteration 14/25 | Loss: 0.00092720
Iteration 15/25 | Loss: 0.00092720
Iteration 16/25 | Loss: 0.00092720
Iteration 17/25 | Loss: 0.00092720
Iteration 18/25 | Loss: 0.00092720
Iteration 19/25 | Loss: 0.00092720
Iteration 20/25 | Loss: 0.00092720
Iteration 21/25 | Loss: 0.00092720
Iteration 22/25 | Loss: 0.00092720
Iteration 23/25 | Loss: 0.00092720
Iteration 24/25 | Loss: 0.00092720
Iteration 25/25 | Loss: 0.00092720

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092720
Iteration 2/1000 | Loss: 0.00002230
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001530
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001420
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001368
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001341
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001331
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001329
Iteration 27/1000 | Loss: 0.00001328
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001322
Iteration 33/1000 | Loss: 0.00001322
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001315
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001315
Iteration 39/1000 | Loss: 0.00001315
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001308
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001304
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001304
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001298
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001296
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001294
Iteration 89/1000 | Loss: 0.00001294
Iteration 90/1000 | Loss: 0.00001294
Iteration 91/1000 | Loss: 0.00001294
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001293
Iteration 94/1000 | Loss: 0.00001292
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001290
Iteration 98/1000 | Loss: 0.00001290
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001287
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001286
Iteration 108/1000 | Loss: 0.00001286
Iteration 109/1000 | Loss: 0.00001286
Iteration 110/1000 | Loss: 0.00001286
Iteration 111/1000 | Loss: 0.00001286
Iteration 112/1000 | Loss: 0.00001285
Iteration 113/1000 | Loss: 0.00001285
Iteration 114/1000 | Loss: 0.00001285
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001284
Iteration 121/1000 | Loss: 0.00001284
Iteration 122/1000 | Loss: 0.00001284
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001281
Iteration 136/1000 | Loss: 0.00001281
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001279
Iteration 146/1000 | Loss: 0.00001279
Iteration 147/1000 | Loss: 0.00001279
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001276
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Iteration 175/1000 | Loss: 0.00001276
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001275
Iteration 179/1000 | Loss: 0.00001275
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001275
Iteration 182/1000 | Loss: 0.00001275
Iteration 183/1000 | Loss: 0.00001275
Iteration 184/1000 | Loss: 0.00001275
Iteration 185/1000 | Loss: 0.00001275
Iteration 186/1000 | Loss: 0.00001275
Iteration 187/1000 | Loss: 0.00001275
Iteration 188/1000 | Loss: 0.00001274
Iteration 189/1000 | Loss: 0.00001274
Iteration 190/1000 | Loss: 0.00001274
Iteration 191/1000 | Loss: 0.00001274
Iteration 192/1000 | Loss: 0.00001274
Iteration 193/1000 | Loss: 0.00001274
Iteration 194/1000 | Loss: 0.00001274
Iteration 195/1000 | Loss: 0.00001274
Iteration 196/1000 | Loss: 0.00001274
Iteration 197/1000 | Loss: 0.00001274
Iteration 198/1000 | Loss: 0.00001274
Iteration 199/1000 | Loss: 0.00001274
Iteration 200/1000 | Loss: 0.00001274
Iteration 201/1000 | Loss: 0.00001274
Iteration 202/1000 | Loss: 0.00001274
Iteration 203/1000 | Loss: 0.00001274
Iteration 204/1000 | Loss: 0.00001274
Iteration 205/1000 | Loss: 0.00001274
Iteration 206/1000 | Loss: 0.00001273
Iteration 207/1000 | Loss: 0.00001273
Iteration 208/1000 | Loss: 0.00001273
Iteration 209/1000 | Loss: 0.00001273
Iteration 210/1000 | Loss: 0.00001273
Iteration 211/1000 | Loss: 0.00001273
Iteration 212/1000 | Loss: 0.00001273
Iteration 213/1000 | Loss: 0.00001273
Iteration 214/1000 | Loss: 0.00001273
Iteration 215/1000 | Loss: 0.00001273
Iteration 216/1000 | Loss: 0.00001273
Iteration 217/1000 | Loss: 0.00001273
Iteration 218/1000 | Loss: 0.00001273
Iteration 219/1000 | Loss: 0.00001273
Iteration 220/1000 | Loss: 0.00001273
Iteration 221/1000 | Loss: 0.00001273
Iteration 222/1000 | Loss: 0.00001273
Iteration 223/1000 | Loss: 0.00001273
Iteration 224/1000 | Loss: 0.00001273
Iteration 225/1000 | Loss: 0.00001273
Iteration 226/1000 | Loss: 0.00001273
Iteration 227/1000 | Loss: 0.00001273
Iteration 228/1000 | Loss: 0.00001273
Iteration 229/1000 | Loss: 0.00001273
Iteration 230/1000 | Loss: 0.00001273
Iteration 231/1000 | Loss: 0.00001273
Iteration 232/1000 | Loss: 0.00001273
Iteration 233/1000 | Loss: 0.00001273
Iteration 234/1000 | Loss: 0.00001273
Iteration 235/1000 | Loss: 0.00001273
Iteration 236/1000 | Loss: 0.00001273
Iteration 237/1000 | Loss: 0.00001273
Iteration 238/1000 | Loss: 0.00001273
Iteration 239/1000 | Loss: 0.00001273
Iteration 240/1000 | Loss: 0.00001273
Iteration 241/1000 | Loss: 0.00001273
Iteration 242/1000 | Loss: 0.00001273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.27269258882734e-05, 1.27269258882734e-05, 1.27269258882734e-05, 1.27269258882734e-05, 1.27269258882734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.27269258882734e-05

Optimization complete. Final v2v error: 3.0326995849609375 mm

Highest mean error: 3.6207499504089355 mm for frame 102

Lowest mean error: 2.7303435802459717 mm for frame 16

Saving results

Total time: 39.60581350326538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882489
Iteration 2/25 | Loss: 0.00201944
Iteration 3/25 | Loss: 0.00141727
Iteration 4/25 | Loss: 0.00135983
Iteration 5/25 | Loss: 0.00134450
Iteration 6/25 | Loss: 0.00129844
Iteration 7/25 | Loss: 0.00130461
Iteration 8/25 | Loss: 0.00125255
Iteration 9/25 | Loss: 0.00121709
Iteration 10/25 | Loss: 0.00121587
Iteration 11/25 | Loss: 0.00120920
Iteration 12/25 | Loss: 0.00119689
Iteration 13/25 | Loss: 0.00119530
Iteration 14/25 | Loss: 0.00119430
Iteration 15/25 | Loss: 0.00119607
Iteration 16/25 | Loss: 0.00119350
Iteration 17/25 | Loss: 0.00119340
Iteration 18/25 | Loss: 0.00119336
Iteration 19/25 | Loss: 0.00119336
Iteration 20/25 | Loss: 0.00119335
Iteration 21/25 | Loss: 0.00119335
Iteration 22/25 | Loss: 0.00119335
Iteration 23/25 | Loss: 0.00119335
Iteration 24/25 | Loss: 0.00119335
Iteration 25/25 | Loss: 0.00119335

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96688986
Iteration 2/25 | Loss: 0.00108239
Iteration 3/25 | Loss: 0.00099945
Iteration 4/25 | Loss: 0.00099945
Iteration 5/25 | Loss: 0.00099945
Iteration 6/25 | Loss: 0.00099945
Iteration 7/25 | Loss: 0.00099945
Iteration 8/25 | Loss: 0.00099945
Iteration 9/25 | Loss: 0.00099945
Iteration 10/25 | Loss: 0.00099945
Iteration 11/25 | Loss: 0.00099945
Iteration 12/25 | Loss: 0.00099945
Iteration 13/25 | Loss: 0.00099945
Iteration 14/25 | Loss: 0.00099945
Iteration 15/25 | Loss: 0.00099945
Iteration 16/25 | Loss: 0.00099945
Iteration 17/25 | Loss: 0.00099945
Iteration 18/25 | Loss: 0.00099945
Iteration 19/25 | Loss: 0.00099945
Iteration 20/25 | Loss: 0.00099945
Iteration 21/25 | Loss: 0.00099945
Iteration 22/25 | Loss: 0.00099945
Iteration 23/25 | Loss: 0.00099945
Iteration 24/25 | Loss: 0.00099945
Iteration 25/25 | Loss: 0.00099945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099945
Iteration 2/1000 | Loss: 0.00007320
Iteration 3/1000 | Loss: 0.00005146
Iteration 4/1000 | Loss: 0.00012181
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001583
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00007550
Iteration 9/1000 | Loss: 0.00004331
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00004231
Iteration 12/1000 | Loss: 0.00005157
Iteration 13/1000 | Loss: 0.00037540
Iteration 14/1000 | Loss: 0.00016827
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00006926
Iteration 18/1000 | Loss: 0.00034713
Iteration 19/1000 | Loss: 0.00004731
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001414
Iteration 22/1000 | Loss: 0.00004812
Iteration 23/1000 | Loss: 0.00001241
Iteration 24/1000 | Loss: 0.00002679
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001222
Iteration 33/1000 | Loss: 0.00001222
Iteration 34/1000 | Loss: 0.00001221
Iteration 35/1000 | Loss: 0.00001221
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00003601
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00003725
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001194
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001188
Iteration 75/1000 | Loss: 0.00001188
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00007145
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001181
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001179
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001178
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.1778378393501043e-05, 1.1778378393501043e-05, 1.1778378393501043e-05, 1.1778378393501043e-05, 1.1778378393501043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1778378393501043e-05

Optimization complete. Final v2v error: 2.9474034309387207 mm

Highest mean error: 3.7894504070281982 mm for frame 228

Lowest mean error: 2.6792893409729004 mm for frame 17

Saving results

Total time: 92.23463201522827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458297
Iteration 2/25 | Loss: 0.00137776
Iteration 3/25 | Loss: 0.00126929
Iteration 4/25 | Loss: 0.00125457
Iteration 5/25 | Loss: 0.00124956
Iteration 6/25 | Loss: 0.00124956
Iteration 7/25 | Loss: 0.00124956
Iteration 8/25 | Loss: 0.00124956
Iteration 9/25 | Loss: 0.00124956
Iteration 10/25 | Loss: 0.00124956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012495582923293114, 0.0012495582923293114, 0.0012495582923293114, 0.0012495582923293114, 0.0012495582923293114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012495582923293114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83061558
Iteration 2/25 | Loss: 0.00090932
Iteration 3/25 | Loss: 0.00090931
Iteration 4/25 | Loss: 0.00090931
Iteration 5/25 | Loss: 0.00090931
Iteration 6/25 | Loss: 0.00090931
Iteration 7/25 | Loss: 0.00090931
Iteration 8/25 | Loss: 0.00090931
Iteration 9/25 | Loss: 0.00090931
Iteration 10/25 | Loss: 0.00090931
Iteration 11/25 | Loss: 0.00090931
Iteration 12/25 | Loss: 0.00090931
Iteration 13/25 | Loss: 0.00090931
Iteration 14/25 | Loss: 0.00090931
Iteration 15/25 | Loss: 0.00090931
Iteration 16/25 | Loss: 0.00090931
Iteration 17/25 | Loss: 0.00090931
Iteration 18/25 | Loss: 0.00090931
Iteration 19/25 | Loss: 0.00090931
Iteration 20/25 | Loss: 0.00090931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009093116386793554, 0.0009093116386793554, 0.0009093116386793554, 0.0009093116386793554, 0.0009093116386793554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009093116386793554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090931
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00002246
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001810
Iteration 7/1000 | Loss: 0.00001745
Iteration 8/1000 | Loss: 0.00001713
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001654
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001612
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001590
Iteration 15/1000 | Loss: 0.00001588
Iteration 16/1000 | Loss: 0.00001582
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001556
Iteration 22/1000 | Loss: 0.00001554
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001553
Iteration 26/1000 | Loss: 0.00001552
Iteration 27/1000 | Loss: 0.00001550
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001544
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001529
Iteration 50/1000 | Loss: 0.00001529
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001528
Iteration 54/1000 | Loss: 0.00001528
Iteration 55/1000 | Loss: 0.00001528
Iteration 56/1000 | Loss: 0.00001525
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001521
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001519
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001517
Iteration 83/1000 | Loss: 0.00001517
Iteration 84/1000 | Loss: 0.00001517
Iteration 85/1000 | Loss: 0.00001517
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001516
Iteration 92/1000 | Loss: 0.00001516
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001515
Iteration 97/1000 | Loss: 0.00001515
Iteration 98/1000 | Loss: 0.00001515
Iteration 99/1000 | Loss: 0.00001515
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001514
Iteration 102/1000 | Loss: 0.00001514
Iteration 103/1000 | Loss: 0.00001514
Iteration 104/1000 | Loss: 0.00001514
Iteration 105/1000 | Loss: 0.00001514
Iteration 106/1000 | Loss: 0.00001514
Iteration 107/1000 | Loss: 0.00001514
Iteration 108/1000 | Loss: 0.00001514
Iteration 109/1000 | Loss: 0.00001514
Iteration 110/1000 | Loss: 0.00001514
Iteration 111/1000 | Loss: 0.00001514
Iteration 112/1000 | Loss: 0.00001514
Iteration 113/1000 | Loss: 0.00001514
Iteration 114/1000 | Loss: 0.00001514
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001513
Iteration 117/1000 | Loss: 0.00001513
Iteration 118/1000 | Loss: 0.00001513
Iteration 119/1000 | Loss: 0.00001513
Iteration 120/1000 | Loss: 0.00001513
Iteration 121/1000 | Loss: 0.00001513
Iteration 122/1000 | Loss: 0.00001513
Iteration 123/1000 | Loss: 0.00001513
Iteration 124/1000 | Loss: 0.00001513
Iteration 125/1000 | Loss: 0.00001512
Iteration 126/1000 | Loss: 0.00001512
Iteration 127/1000 | Loss: 0.00001512
Iteration 128/1000 | Loss: 0.00001512
Iteration 129/1000 | Loss: 0.00001512
Iteration 130/1000 | Loss: 0.00001512
Iteration 131/1000 | Loss: 0.00001512
Iteration 132/1000 | Loss: 0.00001512
Iteration 133/1000 | Loss: 0.00001512
Iteration 134/1000 | Loss: 0.00001511
Iteration 135/1000 | Loss: 0.00001511
Iteration 136/1000 | Loss: 0.00001511
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001510
Iteration 141/1000 | Loss: 0.00001510
Iteration 142/1000 | Loss: 0.00001510
Iteration 143/1000 | Loss: 0.00001510
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001509
Iteration 146/1000 | Loss: 0.00001509
Iteration 147/1000 | Loss: 0.00001509
Iteration 148/1000 | Loss: 0.00001509
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001507
Iteration 159/1000 | Loss: 0.00001507
Iteration 160/1000 | Loss: 0.00001507
Iteration 161/1000 | Loss: 0.00001507
Iteration 162/1000 | Loss: 0.00001507
Iteration 163/1000 | Loss: 0.00001507
Iteration 164/1000 | Loss: 0.00001507
Iteration 165/1000 | Loss: 0.00001507
Iteration 166/1000 | Loss: 0.00001507
Iteration 167/1000 | Loss: 0.00001507
Iteration 168/1000 | Loss: 0.00001507
Iteration 169/1000 | Loss: 0.00001507
Iteration 170/1000 | Loss: 0.00001507
Iteration 171/1000 | Loss: 0.00001507
Iteration 172/1000 | Loss: 0.00001506
Iteration 173/1000 | Loss: 0.00001506
Iteration 174/1000 | Loss: 0.00001506
Iteration 175/1000 | Loss: 0.00001506
Iteration 176/1000 | Loss: 0.00001506
Iteration 177/1000 | Loss: 0.00001506
Iteration 178/1000 | Loss: 0.00001506
Iteration 179/1000 | Loss: 0.00001506
Iteration 180/1000 | Loss: 0.00001506
Iteration 181/1000 | Loss: 0.00001506
Iteration 182/1000 | Loss: 0.00001505
Iteration 183/1000 | Loss: 0.00001505
Iteration 184/1000 | Loss: 0.00001505
Iteration 185/1000 | Loss: 0.00001505
Iteration 186/1000 | Loss: 0.00001505
Iteration 187/1000 | Loss: 0.00001505
Iteration 188/1000 | Loss: 0.00001505
Iteration 189/1000 | Loss: 0.00001504
Iteration 190/1000 | Loss: 0.00001504
Iteration 191/1000 | Loss: 0.00001504
Iteration 192/1000 | Loss: 0.00001504
Iteration 193/1000 | Loss: 0.00001504
Iteration 194/1000 | Loss: 0.00001504
Iteration 195/1000 | Loss: 0.00001504
Iteration 196/1000 | Loss: 0.00001504
Iteration 197/1000 | Loss: 0.00001504
Iteration 198/1000 | Loss: 0.00001504
Iteration 199/1000 | Loss: 0.00001504
Iteration 200/1000 | Loss: 0.00001504
Iteration 201/1000 | Loss: 0.00001504
Iteration 202/1000 | Loss: 0.00001504
Iteration 203/1000 | Loss: 0.00001504
Iteration 204/1000 | Loss: 0.00001504
Iteration 205/1000 | Loss: 0.00001504
Iteration 206/1000 | Loss: 0.00001504
Iteration 207/1000 | Loss: 0.00001504
Iteration 208/1000 | Loss: 0.00001504
Iteration 209/1000 | Loss: 0.00001504
Iteration 210/1000 | Loss: 0.00001504
Iteration 211/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.503768544353079e-05, 1.503768544353079e-05, 1.503768544353079e-05, 1.503768544353079e-05, 1.503768544353079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.503768544353079e-05

Optimization complete. Final v2v error: 3.284346580505371 mm

Highest mean error: 3.5857746601104736 mm for frame 246

Lowest mean error: 3.143599510192871 mm for frame 167

Saving results

Total time: 51.775583267211914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001552
Iteration 2/25 | Loss: 0.00178188
Iteration 3/25 | Loss: 0.00145396
Iteration 4/25 | Loss: 0.00143190
Iteration 5/25 | Loss: 0.00142410
Iteration 6/25 | Loss: 0.00142269
Iteration 7/25 | Loss: 0.00142269
Iteration 8/25 | Loss: 0.00142269
Iteration 9/25 | Loss: 0.00142269
Iteration 10/25 | Loss: 0.00142269
Iteration 11/25 | Loss: 0.00142269
Iteration 12/25 | Loss: 0.00142269
Iteration 13/25 | Loss: 0.00142269
Iteration 14/25 | Loss: 0.00142269
Iteration 15/25 | Loss: 0.00142269
Iteration 16/25 | Loss: 0.00142269
Iteration 17/25 | Loss: 0.00142269
Iteration 18/25 | Loss: 0.00142269
Iteration 19/25 | Loss: 0.00142269
Iteration 20/25 | Loss: 0.00142269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014226948842406273, 0.0014226948842406273, 0.0014226948842406273, 0.0014226948842406273, 0.0014226948842406273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014226948842406273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54352981
Iteration 2/25 | Loss: 0.00127121
Iteration 3/25 | Loss: 0.00127120
Iteration 4/25 | Loss: 0.00127120
Iteration 5/25 | Loss: 0.00127120
Iteration 6/25 | Loss: 0.00127120
Iteration 7/25 | Loss: 0.00127120
Iteration 8/25 | Loss: 0.00127120
Iteration 9/25 | Loss: 0.00127120
Iteration 10/25 | Loss: 0.00127120
Iteration 11/25 | Loss: 0.00127120
Iteration 12/25 | Loss: 0.00127120
Iteration 13/25 | Loss: 0.00127120
Iteration 14/25 | Loss: 0.00127120
Iteration 15/25 | Loss: 0.00127120
Iteration 16/25 | Loss: 0.00127120
Iteration 17/25 | Loss: 0.00127120
Iteration 18/25 | Loss: 0.00127120
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012712019961327314, 0.0012712019961327314, 0.0012712019961327314, 0.0012712019961327314, 0.0012712019961327314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012712019961327314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127120
Iteration 2/1000 | Loss: 0.00006437
Iteration 3/1000 | Loss: 0.00004491
Iteration 4/1000 | Loss: 0.00003820
Iteration 5/1000 | Loss: 0.00003567
Iteration 6/1000 | Loss: 0.00003450
Iteration 7/1000 | Loss: 0.00003373
Iteration 8/1000 | Loss: 0.00003294
Iteration 9/1000 | Loss: 0.00003221
Iteration 10/1000 | Loss: 0.00003188
Iteration 11/1000 | Loss: 0.00003154
Iteration 12/1000 | Loss: 0.00003122
Iteration 13/1000 | Loss: 0.00003096
Iteration 14/1000 | Loss: 0.00003068
Iteration 15/1000 | Loss: 0.00003039
Iteration 16/1000 | Loss: 0.00003018
Iteration 17/1000 | Loss: 0.00002997
Iteration 18/1000 | Loss: 0.00002973
Iteration 19/1000 | Loss: 0.00002955
Iteration 20/1000 | Loss: 0.00002947
Iteration 21/1000 | Loss: 0.00002946
Iteration 22/1000 | Loss: 0.00002944
Iteration 23/1000 | Loss: 0.00002939
Iteration 24/1000 | Loss: 0.00002938
Iteration 25/1000 | Loss: 0.00002938
Iteration 26/1000 | Loss: 0.00002937
Iteration 27/1000 | Loss: 0.00002937
Iteration 28/1000 | Loss: 0.00002932
Iteration 29/1000 | Loss: 0.00002930
Iteration 30/1000 | Loss: 0.00002929
Iteration 31/1000 | Loss: 0.00002929
Iteration 32/1000 | Loss: 0.00002928
Iteration 33/1000 | Loss: 0.00002923
Iteration 34/1000 | Loss: 0.00002922
Iteration 35/1000 | Loss: 0.00002921
Iteration 36/1000 | Loss: 0.00002920
Iteration 37/1000 | Loss: 0.00002913
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002911
Iteration 40/1000 | Loss: 0.00002909
Iteration 41/1000 | Loss: 0.00002908
Iteration 42/1000 | Loss: 0.00002908
Iteration 43/1000 | Loss: 0.00002907
Iteration 44/1000 | Loss: 0.00002907
Iteration 45/1000 | Loss: 0.00002907
Iteration 46/1000 | Loss: 0.00002907
Iteration 47/1000 | Loss: 0.00002907
Iteration 48/1000 | Loss: 0.00002907
Iteration 49/1000 | Loss: 0.00002906
Iteration 50/1000 | Loss: 0.00002906
Iteration 51/1000 | Loss: 0.00002906
Iteration 52/1000 | Loss: 0.00002906
Iteration 53/1000 | Loss: 0.00002906
Iteration 54/1000 | Loss: 0.00002906
Iteration 55/1000 | Loss: 0.00002906
Iteration 56/1000 | Loss: 0.00002906
Iteration 57/1000 | Loss: 0.00002905
Iteration 58/1000 | Loss: 0.00002905
Iteration 59/1000 | Loss: 0.00002905
Iteration 60/1000 | Loss: 0.00002905
Iteration 61/1000 | Loss: 0.00002905
Iteration 62/1000 | Loss: 0.00002905
Iteration 63/1000 | Loss: 0.00002904
Iteration 64/1000 | Loss: 0.00002903
Iteration 65/1000 | Loss: 0.00002903
Iteration 66/1000 | Loss: 0.00002903
Iteration 67/1000 | Loss: 0.00002903
Iteration 68/1000 | Loss: 0.00002903
Iteration 69/1000 | Loss: 0.00002903
Iteration 70/1000 | Loss: 0.00002903
Iteration 71/1000 | Loss: 0.00002902
Iteration 72/1000 | Loss: 0.00002902
Iteration 73/1000 | Loss: 0.00002902
Iteration 74/1000 | Loss: 0.00002902
Iteration 75/1000 | Loss: 0.00002902
Iteration 76/1000 | Loss: 0.00002902
Iteration 77/1000 | Loss: 0.00002902
Iteration 78/1000 | Loss: 0.00002902
Iteration 79/1000 | Loss: 0.00002902
Iteration 80/1000 | Loss: 0.00002902
Iteration 81/1000 | Loss: 0.00002902
Iteration 82/1000 | Loss: 0.00002902
Iteration 83/1000 | Loss: 0.00002901
Iteration 84/1000 | Loss: 0.00002901
Iteration 85/1000 | Loss: 0.00002901
Iteration 86/1000 | Loss: 0.00002901
Iteration 87/1000 | Loss: 0.00002900
Iteration 88/1000 | Loss: 0.00002900
Iteration 89/1000 | Loss: 0.00002900
Iteration 90/1000 | Loss: 0.00002900
Iteration 91/1000 | Loss: 0.00002900
Iteration 92/1000 | Loss: 0.00002900
Iteration 93/1000 | Loss: 0.00002900
Iteration 94/1000 | Loss: 0.00002900
Iteration 95/1000 | Loss: 0.00002900
Iteration 96/1000 | Loss: 0.00002900
Iteration 97/1000 | Loss: 0.00002900
Iteration 98/1000 | Loss: 0.00002900
Iteration 99/1000 | Loss: 0.00002900
Iteration 100/1000 | Loss: 0.00002900
Iteration 101/1000 | Loss: 0.00002900
Iteration 102/1000 | Loss: 0.00002900
Iteration 103/1000 | Loss: 0.00002900
Iteration 104/1000 | Loss: 0.00002900
Iteration 105/1000 | Loss: 0.00002899
Iteration 106/1000 | Loss: 0.00002899
Iteration 107/1000 | Loss: 0.00002899
Iteration 108/1000 | Loss: 0.00002899
Iteration 109/1000 | Loss: 0.00002899
Iteration 110/1000 | Loss: 0.00002899
Iteration 111/1000 | Loss: 0.00002899
Iteration 112/1000 | Loss: 0.00002899
Iteration 113/1000 | Loss: 0.00002899
Iteration 114/1000 | Loss: 0.00002899
Iteration 115/1000 | Loss: 0.00002899
Iteration 116/1000 | Loss: 0.00002899
Iteration 117/1000 | Loss: 0.00002899
Iteration 118/1000 | Loss: 0.00002899
Iteration 119/1000 | Loss: 0.00002899
Iteration 120/1000 | Loss: 0.00002899
Iteration 121/1000 | Loss: 0.00002899
Iteration 122/1000 | Loss: 0.00002899
Iteration 123/1000 | Loss: 0.00002899
Iteration 124/1000 | Loss: 0.00002899
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002899
Iteration 129/1000 | Loss: 0.00002899
Iteration 130/1000 | Loss: 0.00002899
Iteration 131/1000 | Loss: 0.00002899
Iteration 132/1000 | Loss: 0.00002899
Iteration 133/1000 | Loss: 0.00002899
Iteration 134/1000 | Loss: 0.00002899
Iteration 135/1000 | Loss: 0.00002899
Iteration 136/1000 | Loss: 0.00002899
Iteration 137/1000 | Loss: 0.00002899
Iteration 138/1000 | Loss: 0.00002899
Iteration 139/1000 | Loss: 0.00002899
Iteration 140/1000 | Loss: 0.00002899
Iteration 141/1000 | Loss: 0.00002899
Iteration 142/1000 | Loss: 0.00002899
Iteration 143/1000 | Loss: 0.00002899
Iteration 144/1000 | Loss: 0.00002899
Iteration 145/1000 | Loss: 0.00002899
Iteration 146/1000 | Loss: 0.00002899
Iteration 147/1000 | Loss: 0.00002899
Iteration 148/1000 | Loss: 0.00002899
Iteration 149/1000 | Loss: 0.00002899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.899371611420065e-05, 2.899371611420065e-05, 2.899371611420065e-05, 2.899371611420065e-05, 2.899371611420065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.899371611420065e-05

Optimization complete. Final v2v error: 4.541197776794434 mm

Highest mean error: 5.486974239349365 mm for frame 0

Lowest mean error: 3.9783716201782227 mm for frame 22

Saving results

Total time: 46.44961953163147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922107
Iteration 2/25 | Loss: 0.00367030
Iteration 3/25 | Loss: 0.00276290
Iteration 4/25 | Loss: 0.00254952
Iteration 5/25 | Loss: 0.00287693
Iteration 6/25 | Loss: 0.00218749
Iteration 7/25 | Loss: 0.00232409
Iteration 8/25 | Loss: 0.00198088
Iteration 9/25 | Loss: 0.00199436
Iteration 10/25 | Loss: 0.00191166
Iteration 11/25 | Loss: 0.00185946
Iteration 12/25 | Loss: 0.00183891
Iteration 13/25 | Loss: 0.00185382
Iteration 14/25 | Loss: 0.00183005
Iteration 15/25 | Loss: 0.00181245
Iteration 16/25 | Loss: 0.00177068
Iteration 17/25 | Loss: 0.00171779
Iteration 18/25 | Loss: 0.00169939
Iteration 19/25 | Loss: 0.00168992
Iteration 20/25 | Loss: 0.00171548
Iteration 21/25 | Loss: 0.00167450
Iteration 22/25 | Loss: 0.00165823
Iteration 23/25 | Loss: 0.00168501
Iteration 24/25 | Loss: 0.00168014
Iteration 25/25 | Loss: 0.00168259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90861487
Iteration 2/25 | Loss: 0.00683970
Iteration 3/25 | Loss: 0.00653767
Iteration 4/25 | Loss: 0.00653767
Iteration 5/25 | Loss: 0.00653767
Iteration 6/25 | Loss: 0.00653767
Iteration 7/25 | Loss: 0.00653767
Iteration 8/25 | Loss: 0.00653767
Iteration 9/25 | Loss: 0.00653767
Iteration 10/25 | Loss: 0.00653767
Iteration 11/25 | Loss: 0.00653767
Iteration 12/25 | Loss: 0.00653767
Iteration 13/25 | Loss: 0.00653767
Iteration 14/25 | Loss: 0.00653767
Iteration 15/25 | Loss: 0.00653767
Iteration 16/25 | Loss: 0.00653767
Iteration 17/25 | Loss: 0.00653767
Iteration 18/25 | Loss: 0.00653767
Iteration 19/25 | Loss: 0.00653767
Iteration 20/25 | Loss: 0.00653767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0065376656129956245, 0.0065376656129956245, 0.0065376656129956245, 0.0065376656129956245, 0.0065376656129956245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0065376656129956245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00653766
Iteration 2/1000 | Loss: 0.00345976
Iteration 3/1000 | Loss: 0.00287125
Iteration 4/1000 | Loss: 0.00438738
Iteration 5/1000 | Loss: 0.00149012
Iteration 6/1000 | Loss: 0.00337552
Iteration 7/1000 | Loss: 0.00219862
Iteration 8/1000 | Loss: 0.00241645
Iteration 9/1000 | Loss: 0.00661406
Iteration 10/1000 | Loss: 0.00338899
Iteration 11/1000 | Loss: 0.00528907
Iteration 12/1000 | Loss: 0.00231616
Iteration 13/1000 | Loss: 0.00139597
Iteration 14/1000 | Loss: 0.00663740
Iteration 15/1000 | Loss: 0.00421680
Iteration 16/1000 | Loss: 0.00238042
Iteration 17/1000 | Loss: 0.00427449
Iteration 18/1000 | Loss: 0.00225722
Iteration 19/1000 | Loss: 0.00173023
Iteration 20/1000 | Loss: 0.00241646
Iteration 21/1000 | Loss: 0.00210189
Iteration 22/1000 | Loss: 0.00200808
Iteration 23/1000 | Loss: 0.00438087
Iteration 24/1000 | Loss: 0.00228195
Iteration 25/1000 | Loss: 0.00238338
Iteration 26/1000 | Loss: 0.00167143
Iteration 27/1000 | Loss: 0.00318430
Iteration 28/1000 | Loss: 0.00136463
Iteration 29/1000 | Loss: 0.00127542
Iteration 30/1000 | Loss: 0.00082860
Iteration 31/1000 | Loss: 0.00478560
Iteration 32/1000 | Loss: 0.00096427
Iteration 33/1000 | Loss: 0.00209913
Iteration 34/1000 | Loss: 0.00120149
Iteration 35/1000 | Loss: 0.00114856
Iteration 36/1000 | Loss: 0.00075097
Iteration 37/1000 | Loss: 0.00526354
Iteration 38/1000 | Loss: 0.00156096
Iteration 39/1000 | Loss: 0.00177159
Iteration 40/1000 | Loss: 0.00143338
Iteration 41/1000 | Loss: 0.00159549
Iteration 42/1000 | Loss: 0.00197047
Iteration 43/1000 | Loss: 0.00138844
Iteration 44/1000 | Loss: 0.00145449
Iteration 45/1000 | Loss: 0.00099401
Iteration 46/1000 | Loss: 0.00173356
Iteration 47/1000 | Loss: 0.00185218
Iteration 48/1000 | Loss: 0.00083644
Iteration 49/1000 | Loss: 0.00218046
Iteration 50/1000 | Loss: 0.00212866
Iteration 51/1000 | Loss: 0.00184523
Iteration 52/1000 | Loss: 0.00166350
Iteration 53/1000 | Loss: 0.00135071
Iteration 54/1000 | Loss: 0.00178087
Iteration 55/1000 | Loss: 0.00306084
Iteration 56/1000 | Loss: 0.00196092
Iteration 57/1000 | Loss: 0.00201328
Iteration 58/1000 | Loss: 0.00174935
Iteration 59/1000 | Loss: 0.00195953
Iteration 60/1000 | Loss: 0.00290752
Iteration 61/1000 | Loss: 0.00196063
Iteration 62/1000 | Loss: 0.00286401
Iteration 63/1000 | Loss: 0.00307038
Iteration 64/1000 | Loss: 0.00167270
Iteration 65/1000 | Loss: 0.00128049
Iteration 66/1000 | Loss: 0.00072911
Iteration 67/1000 | Loss: 0.00068407
Iteration 68/1000 | Loss: 0.00065089
Iteration 69/1000 | Loss: 0.00063442
Iteration 70/1000 | Loss: 0.00075535
Iteration 71/1000 | Loss: 0.00052793
Iteration 72/1000 | Loss: 0.00360744
Iteration 73/1000 | Loss: 0.00074918
Iteration 74/1000 | Loss: 0.00057604
Iteration 75/1000 | Loss: 0.00275502
Iteration 76/1000 | Loss: 0.00145258
Iteration 77/1000 | Loss: 0.00052702
Iteration 78/1000 | Loss: 0.00056666
Iteration 79/1000 | Loss: 0.00036201
Iteration 80/1000 | Loss: 0.00220920
Iteration 81/1000 | Loss: 0.00055724
Iteration 82/1000 | Loss: 0.00118745
Iteration 83/1000 | Loss: 0.00096315
Iteration 84/1000 | Loss: 0.00039683
Iteration 85/1000 | Loss: 0.00050713
Iteration 86/1000 | Loss: 0.00077556
Iteration 87/1000 | Loss: 0.00061560
Iteration 88/1000 | Loss: 0.00135404
Iteration 89/1000 | Loss: 0.00070472
Iteration 90/1000 | Loss: 0.00041171
Iteration 91/1000 | Loss: 0.00035444
Iteration 92/1000 | Loss: 0.00036802
Iteration 93/1000 | Loss: 0.00032228
Iteration 94/1000 | Loss: 0.00075022
Iteration 95/1000 | Loss: 0.00062637
Iteration 96/1000 | Loss: 0.00047239
Iteration 97/1000 | Loss: 0.00032880
Iteration 98/1000 | Loss: 0.00035391
Iteration 99/1000 | Loss: 0.00056142
Iteration 100/1000 | Loss: 0.00044866
Iteration 101/1000 | Loss: 0.00040841
Iteration 102/1000 | Loss: 0.00033654
Iteration 103/1000 | Loss: 0.00022362
Iteration 104/1000 | Loss: 0.00050064
Iteration 105/1000 | Loss: 0.00023223
Iteration 106/1000 | Loss: 0.00016140
Iteration 107/1000 | Loss: 0.00014296
Iteration 108/1000 | Loss: 0.00077898
Iteration 109/1000 | Loss: 0.00041548
Iteration 110/1000 | Loss: 0.00070557
Iteration 111/1000 | Loss: 0.00042217
Iteration 112/1000 | Loss: 0.00070703
Iteration 113/1000 | Loss: 0.00061258
Iteration 114/1000 | Loss: 0.00031399
Iteration 115/1000 | Loss: 0.00014479
Iteration 116/1000 | Loss: 0.00013665
Iteration 117/1000 | Loss: 0.00178092
Iteration 118/1000 | Loss: 0.00102743
Iteration 119/1000 | Loss: 0.00058914
Iteration 120/1000 | Loss: 0.00029835
Iteration 121/1000 | Loss: 0.00228879
Iteration 122/1000 | Loss: 0.00050493
Iteration 123/1000 | Loss: 0.00062450
Iteration 124/1000 | Loss: 0.00033426
Iteration 125/1000 | Loss: 0.00028169
Iteration 126/1000 | Loss: 0.00030733
Iteration 127/1000 | Loss: 0.00027200
Iteration 128/1000 | Loss: 0.00043444
Iteration 129/1000 | Loss: 0.00358235
Iteration 130/1000 | Loss: 0.00110239
Iteration 131/1000 | Loss: 0.00169407
Iteration 132/1000 | Loss: 0.00205017
Iteration 133/1000 | Loss: 0.00141375
Iteration 134/1000 | Loss: 0.00095581
Iteration 135/1000 | Loss: 0.00148853
Iteration 136/1000 | Loss: 0.00075631
Iteration 137/1000 | Loss: 0.00163120
Iteration 138/1000 | Loss: 0.00038635
Iteration 139/1000 | Loss: 0.00024349
Iteration 140/1000 | Loss: 0.00019547
Iteration 141/1000 | Loss: 0.00052648
Iteration 142/1000 | Loss: 0.00042122
Iteration 143/1000 | Loss: 0.00168325
Iteration 144/1000 | Loss: 0.00024081
Iteration 145/1000 | Loss: 0.00012361
Iteration 146/1000 | Loss: 0.00011827
Iteration 147/1000 | Loss: 0.00011534
Iteration 148/1000 | Loss: 0.00103630
Iteration 149/1000 | Loss: 0.00196762
Iteration 150/1000 | Loss: 0.00103892
Iteration 151/1000 | Loss: 0.00173232
Iteration 152/1000 | Loss: 0.00013151
Iteration 153/1000 | Loss: 0.00023578
Iteration 154/1000 | Loss: 0.00011351
Iteration 155/1000 | Loss: 0.00010972
Iteration 156/1000 | Loss: 0.00010631
Iteration 157/1000 | Loss: 0.00044840
Iteration 158/1000 | Loss: 0.00020435
Iteration 159/1000 | Loss: 0.00029361
Iteration 160/1000 | Loss: 0.00010640
Iteration 161/1000 | Loss: 0.00010312
Iteration 162/1000 | Loss: 0.00010205
Iteration 163/1000 | Loss: 0.00047454
Iteration 164/1000 | Loss: 0.00129723
Iteration 165/1000 | Loss: 0.00030898
Iteration 166/1000 | Loss: 0.00049000
Iteration 167/1000 | Loss: 0.00010570
Iteration 168/1000 | Loss: 0.00018696
Iteration 169/1000 | Loss: 0.00010182
Iteration 170/1000 | Loss: 0.00018981
Iteration 171/1000 | Loss: 0.00032530
Iteration 172/1000 | Loss: 0.00042345
Iteration 173/1000 | Loss: 0.00132628
Iteration 174/1000 | Loss: 0.00208425
Iteration 175/1000 | Loss: 0.00077352
Iteration 176/1000 | Loss: 0.00027651
Iteration 177/1000 | Loss: 0.00023452
Iteration 178/1000 | Loss: 0.00032587
Iteration 179/1000 | Loss: 0.00024066
Iteration 180/1000 | Loss: 0.00016131
Iteration 181/1000 | Loss: 0.00009859
Iteration 182/1000 | Loss: 0.00102207
Iteration 183/1000 | Loss: 0.00042981
Iteration 184/1000 | Loss: 0.00019951
Iteration 185/1000 | Loss: 0.00010096
Iteration 186/1000 | Loss: 0.00009550
Iteration 187/1000 | Loss: 0.00009364
Iteration 188/1000 | Loss: 0.00021007
Iteration 189/1000 | Loss: 0.00009460
Iteration 190/1000 | Loss: 0.00009157
Iteration 191/1000 | Loss: 0.00009027
Iteration 192/1000 | Loss: 0.00024719
Iteration 193/1000 | Loss: 0.00021945
Iteration 194/1000 | Loss: 0.00009660
Iteration 195/1000 | Loss: 0.00008899
Iteration 196/1000 | Loss: 0.00008820
Iteration 197/1000 | Loss: 0.00024100
Iteration 198/1000 | Loss: 0.00023758
Iteration 199/1000 | Loss: 0.00010152
Iteration 200/1000 | Loss: 0.00009138
Iteration 201/1000 | Loss: 0.00010394
Iteration 202/1000 | Loss: 0.00009029
Iteration 203/1000 | Loss: 0.00014018
Iteration 204/1000 | Loss: 0.00009658
Iteration 205/1000 | Loss: 0.00008627
Iteration 206/1000 | Loss: 0.00038018
Iteration 207/1000 | Loss: 0.00014210
Iteration 208/1000 | Loss: 0.00014606
Iteration 209/1000 | Loss: 0.00013536
Iteration 210/1000 | Loss: 0.00015414
Iteration 211/1000 | Loss: 0.00009154
Iteration 212/1000 | Loss: 0.00010895
Iteration 213/1000 | Loss: 0.00009867
Iteration 214/1000 | Loss: 0.00009361
Iteration 215/1000 | Loss: 0.00008320
Iteration 216/1000 | Loss: 0.00020616
Iteration 217/1000 | Loss: 0.00031555
Iteration 218/1000 | Loss: 0.00039741
Iteration 219/1000 | Loss: 0.00010762
Iteration 220/1000 | Loss: 0.00008346
Iteration 221/1000 | Loss: 0.00021002
Iteration 222/1000 | Loss: 0.00055505
Iteration 223/1000 | Loss: 0.00008551
Iteration 224/1000 | Loss: 0.00008165
Iteration 225/1000 | Loss: 0.00018803
Iteration 226/1000 | Loss: 0.00008209
Iteration 227/1000 | Loss: 0.00008020
Iteration 228/1000 | Loss: 0.00007948
Iteration 229/1000 | Loss: 0.00007901
Iteration 230/1000 | Loss: 0.00007852
Iteration 231/1000 | Loss: 0.00007806
Iteration 232/1000 | Loss: 0.00036660
Iteration 233/1000 | Loss: 0.00057860
Iteration 234/1000 | Loss: 0.00007911
Iteration 235/1000 | Loss: 0.00007759
Iteration 236/1000 | Loss: 0.00007726
Iteration 237/1000 | Loss: 0.00007703
Iteration 238/1000 | Loss: 0.00007678
Iteration 239/1000 | Loss: 0.00007654
Iteration 240/1000 | Loss: 0.00051101
Iteration 241/1000 | Loss: 0.00084371
Iteration 242/1000 | Loss: 0.00033707
Iteration 243/1000 | Loss: 0.00007933
Iteration 244/1000 | Loss: 0.00007682
Iteration 245/1000 | Loss: 0.00007635
Iteration 246/1000 | Loss: 0.00007623
Iteration 247/1000 | Loss: 0.00007616
Iteration 248/1000 | Loss: 0.00007614
Iteration 249/1000 | Loss: 0.00007614
Iteration 250/1000 | Loss: 0.00007614
Iteration 251/1000 | Loss: 0.00007613
Iteration 252/1000 | Loss: 0.00007613
Iteration 253/1000 | Loss: 0.00007612
Iteration 254/1000 | Loss: 0.00007611
Iteration 255/1000 | Loss: 0.00007611
Iteration 256/1000 | Loss: 0.00007608
Iteration 257/1000 | Loss: 0.00007608
Iteration 258/1000 | Loss: 0.00007608
Iteration 259/1000 | Loss: 0.00007607
Iteration 260/1000 | Loss: 0.00007607
Iteration 261/1000 | Loss: 0.00007607
Iteration 262/1000 | Loss: 0.00007607
Iteration 263/1000 | Loss: 0.00007607
Iteration 264/1000 | Loss: 0.00007606
Iteration 265/1000 | Loss: 0.00007606
Iteration 266/1000 | Loss: 0.00007606
Iteration 267/1000 | Loss: 0.00007606
Iteration 268/1000 | Loss: 0.00007606
Iteration 269/1000 | Loss: 0.00007606
Iteration 270/1000 | Loss: 0.00007605
Iteration 271/1000 | Loss: 0.00007601
Iteration 272/1000 | Loss: 0.00019816
Iteration 273/1000 | Loss: 0.00019816
Iteration 274/1000 | Loss: 0.00055871
Iteration 275/1000 | Loss: 0.00030887
Iteration 276/1000 | Loss: 0.00021020
Iteration 277/1000 | Loss: 0.00008118
Iteration 278/1000 | Loss: 0.00007803
Iteration 279/1000 | Loss: 0.00007592
Iteration 280/1000 | Loss: 0.00007393
Iteration 281/1000 | Loss: 0.00007312
Iteration 282/1000 | Loss: 0.00007258
Iteration 283/1000 | Loss: 0.00007215
Iteration 284/1000 | Loss: 0.00007178
Iteration 285/1000 | Loss: 0.00007157
Iteration 286/1000 | Loss: 0.00007136
Iteration 287/1000 | Loss: 0.00007130
Iteration 288/1000 | Loss: 0.00007128
Iteration 289/1000 | Loss: 0.00007119
Iteration 290/1000 | Loss: 0.00007118
Iteration 291/1000 | Loss: 0.00007118
Iteration 292/1000 | Loss: 0.00007117
Iteration 293/1000 | Loss: 0.00007116
Iteration 294/1000 | Loss: 0.00007116
Iteration 295/1000 | Loss: 0.00007116
Iteration 296/1000 | Loss: 0.00007115
Iteration 297/1000 | Loss: 0.00007113
Iteration 298/1000 | Loss: 0.00007112
Iteration 299/1000 | Loss: 0.00007112
Iteration 300/1000 | Loss: 0.00007112
Iteration 301/1000 | Loss: 0.00007112
Iteration 302/1000 | Loss: 0.00007111
Iteration 303/1000 | Loss: 0.00007111
Iteration 304/1000 | Loss: 0.00007110
Iteration 305/1000 | Loss: 0.00007110
Iteration 306/1000 | Loss: 0.00007109
Iteration 307/1000 | Loss: 0.00007109
Iteration 308/1000 | Loss: 0.00007109
Iteration 309/1000 | Loss: 0.00007108
Iteration 310/1000 | Loss: 0.00007108
Iteration 311/1000 | Loss: 0.00007108
Iteration 312/1000 | Loss: 0.00007107
Iteration 313/1000 | Loss: 0.00007107
Iteration 314/1000 | Loss: 0.00007107
Iteration 315/1000 | Loss: 0.00007107
Iteration 316/1000 | Loss: 0.00007107
Iteration 317/1000 | Loss: 0.00007107
Iteration 318/1000 | Loss: 0.00007107
Iteration 319/1000 | Loss: 0.00007107
Iteration 320/1000 | Loss: 0.00007107
Iteration 321/1000 | Loss: 0.00007106
Iteration 322/1000 | Loss: 0.00007106
Iteration 323/1000 | Loss: 0.00007106
Iteration 324/1000 | Loss: 0.00007106
Iteration 325/1000 | Loss: 0.00007105
Iteration 326/1000 | Loss: 0.00007105
Iteration 327/1000 | Loss: 0.00007105
Iteration 328/1000 | Loss: 0.00007105
Iteration 329/1000 | Loss: 0.00007105
Iteration 330/1000 | Loss: 0.00007104
Iteration 331/1000 | Loss: 0.00007104
Iteration 332/1000 | Loss: 0.00007104
Iteration 333/1000 | Loss: 0.00007103
Iteration 334/1000 | Loss: 0.00007103
Iteration 335/1000 | Loss: 0.00007103
Iteration 336/1000 | Loss: 0.00007103
Iteration 337/1000 | Loss: 0.00007103
Iteration 338/1000 | Loss: 0.00007102
Iteration 339/1000 | Loss: 0.00007102
Iteration 340/1000 | Loss: 0.00007102
Iteration 341/1000 | Loss: 0.00007102
Iteration 342/1000 | Loss: 0.00007102
Iteration 343/1000 | Loss: 0.00007102
Iteration 344/1000 | Loss: 0.00007102
Iteration 345/1000 | Loss: 0.00007102
Iteration 346/1000 | Loss: 0.00007102
Iteration 347/1000 | Loss: 0.00007102
Iteration 348/1000 | Loss: 0.00007102
Iteration 349/1000 | Loss: 0.00007102
Iteration 350/1000 | Loss: 0.00007102
Iteration 351/1000 | Loss: 0.00007102
Iteration 352/1000 | Loss: 0.00007102
Iteration 353/1000 | Loss: 0.00007101
Iteration 354/1000 | Loss: 0.00007101
Iteration 355/1000 | Loss: 0.00007101
Iteration 356/1000 | Loss: 0.00007101
Iteration 357/1000 | Loss: 0.00007101
Iteration 358/1000 | Loss: 0.00007101
Iteration 359/1000 | Loss: 0.00007100
Iteration 360/1000 | Loss: 0.00007100
Iteration 361/1000 | Loss: 0.00007100
Iteration 362/1000 | Loss: 0.00007100
Iteration 363/1000 | Loss: 0.00007100
Iteration 364/1000 | Loss: 0.00007100
Iteration 365/1000 | Loss: 0.00007100
Iteration 366/1000 | Loss: 0.00007100
Iteration 367/1000 | Loss: 0.00007100
Iteration 368/1000 | Loss: 0.00007100
Iteration 369/1000 | Loss: 0.00007099
Iteration 370/1000 | Loss: 0.00007099
Iteration 371/1000 | Loss: 0.00007099
Iteration 372/1000 | Loss: 0.00007099
Iteration 373/1000 | Loss: 0.00007099
Iteration 374/1000 | Loss: 0.00007099
Iteration 375/1000 | Loss: 0.00007099
Iteration 376/1000 | Loss: 0.00007099
Iteration 377/1000 | Loss: 0.00007099
Iteration 378/1000 | Loss: 0.00007099
Iteration 379/1000 | Loss: 0.00007099
Iteration 380/1000 | Loss: 0.00007099
Iteration 381/1000 | Loss: 0.00007099
Iteration 382/1000 | Loss: 0.00007099
Iteration 383/1000 | Loss: 0.00007099
Iteration 384/1000 | Loss: 0.00007099
Iteration 385/1000 | Loss: 0.00007099
Iteration 386/1000 | Loss: 0.00007099
Iteration 387/1000 | Loss: 0.00007099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 387. Stopping optimization.
Last 5 losses: [7.098967034835368e-05, 7.098967034835368e-05, 7.098967034835368e-05, 7.098967034835368e-05, 7.098967034835368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.098967034835368e-05

Optimization complete. Final v2v error: 4.503989219665527 mm

Highest mean error: 13.482209205627441 mm for frame 76

Lowest mean error: 2.8176660537719727 mm for frame 154

Saving results

Total time: 486.803058385849
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393206
Iteration 2/25 | Loss: 0.00140247
Iteration 3/25 | Loss: 0.00123385
Iteration 4/25 | Loss: 0.00121046
Iteration 5/25 | Loss: 0.00120295
Iteration 6/25 | Loss: 0.00120139
Iteration 7/25 | Loss: 0.00120070
Iteration 8/25 | Loss: 0.00120070
Iteration 9/25 | Loss: 0.00120070
Iteration 10/25 | Loss: 0.00120070
Iteration 11/25 | Loss: 0.00120070
Iteration 12/25 | Loss: 0.00120070
Iteration 13/25 | Loss: 0.00120070
Iteration 14/25 | Loss: 0.00120070
Iteration 15/25 | Loss: 0.00120070
Iteration 16/25 | Loss: 0.00120070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012007046025246382, 0.0012007046025246382, 0.0012007046025246382, 0.0012007046025246382, 0.0012007046025246382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012007046025246382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44339633
Iteration 2/25 | Loss: 0.00132306
Iteration 3/25 | Loss: 0.00132306
Iteration 4/25 | Loss: 0.00132306
Iteration 5/25 | Loss: 0.00132306
Iteration 6/25 | Loss: 0.00132306
Iteration 7/25 | Loss: 0.00132306
Iteration 8/25 | Loss: 0.00132306
Iteration 9/25 | Loss: 0.00132306
Iteration 10/25 | Loss: 0.00132306
Iteration 11/25 | Loss: 0.00132306
Iteration 12/25 | Loss: 0.00132306
Iteration 13/25 | Loss: 0.00132306
Iteration 14/25 | Loss: 0.00132306
Iteration 15/25 | Loss: 0.00132306
Iteration 16/25 | Loss: 0.00132306
Iteration 17/25 | Loss: 0.00132306
Iteration 18/25 | Loss: 0.00132306
Iteration 19/25 | Loss: 0.00132306
Iteration 20/25 | Loss: 0.00132306
Iteration 21/25 | Loss: 0.00132306
Iteration 22/25 | Loss: 0.00132306
Iteration 23/25 | Loss: 0.00132306
Iteration 24/25 | Loss: 0.00132306
Iteration 25/25 | Loss: 0.00132306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132306
Iteration 2/1000 | Loss: 0.00003740
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002079
Iteration 5/1000 | Loss: 0.00001979
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001869
Iteration 8/1000 | Loss: 0.00001832
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001774
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001771
Iteration 16/1000 | Loss: 0.00001771
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001768
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001752
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001746
Iteration 35/1000 | Loss: 0.00001746
Iteration 36/1000 | Loss: 0.00001746
Iteration 37/1000 | Loss: 0.00001746
Iteration 38/1000 | Loss: 0.00001745
Iteration 39/1000 | Loss: 0.00001745
Iteration 40/1000 | Loss: 0.00001745
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001745
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001745
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001742
Iteration 57/1000 | Loss: 0.00001742
Iteration 58/1000 | Loss: 0.00001742
Iteration 59/1000 | Loss: 0.00001741
Iteration 60/1000 | Loss: 0.00001741
Iteration 61/1000 | Loss: 0.00001740
Iteration 62/1000 | Loss: 0.00001739
Iteration 63/1000 | Loss: 0.00001739
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001738
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001734
Iteration 75/1000 | Loss: 0.00001734
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001730
Iteration 95/1000 | Loss: 0.00001730
Iteration 96/1000 | Loss: 0.00001730
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001722
Iteration 111/1000 | Loss: 0.00001722
Iteration 112/1000 | Loss: 0.00001721
Iteration 113/1000 | Loss: 0.00001720
Iteration 114/1000 | Loss: 0.00001720
Iteration 115/1000 | Loss: 0.00001720
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001718
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001717
Iteration 122/1000 | Loss: 0.00001717
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001716
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001714
Iteration 131/1000 | Loss: 0.00001714
Iteration 132/1000 | Loss: 0.00001714
Iteration 133/1000 | Loss: 0.00001714
Iteration 134/1000 | Loss: 0.00001713
Iteration 135/1000 | Loss: 0.00001713
Iteration 136/1000 | Loss: 0.00001713
Iteration 137/1000 | Loss: 0.00001713
Iteration 138/1000 | Loss: 0.00001713
Iteration 139/1000 | Loss: 0.00001713
Iteration 140/1000 | Loss: 0.00001713
Iteration 141/1000 | Loss: 0.00001713
Iteration 142/1000 | Loss: 0.00001713
Iteration 143/1000 | Loss: 0.00001713
Iteration 144/1000 | Loss: 0.00001713
Iteration 145/1000 | Loss: 0.00001712
Iteration 146/1000 | Loss: 0.00001712
Iteration 147/1000 | Loss: 0.00001712
Iteration 148/1000 | Loss: 0.00001712
Iteration 149/1000 | Loss: 0.00001712
Iteration 150/1000 | Loss: 0.00001712
Iteration 151/1000 | Loss: 0.00001712
Iteration 152/1000 | Loss: 0.00001711
Iteration 153/1000 | Loss: 0.00001711
Iteration 154/1000 | Loss: 0.00001711
Iteration 155/1000 | Loss: 0.00001711
Iteration 156/1000 | Loss: 0.00001711
Iteration 157/1000 | Loss: 0.00001711
Iteration 158/1000 | Loss: 0.00001711
Iteration 159/1000 | Loss: 0.00001711
Iteration 160/1000 | Loss: 0.00001710
Iteration 161/1000 | Loss: 0.00001710
Iteration 162/1000 | Loss: 0.00001710
Iteration 163/1000 | Loss: 0.00001710
Iteration 164/1000 | Loss: 0.00001710
Iteration 165/1000 | Loss: 0.00001710
Iteration 166/1000 | Loss: 0.00001710
Iteration 167/1000 | Loss: 0.00001710
Iteration 168/1000 | Loss: 0.00001710
Iteration 169/1000 | Loss: 0.00001710
Iteration 170/1000 | Loss: 0.00001710
Iteration 171/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.7099786418839358e-05, 1.7099786418839358e-05, 1.7099786418839358e-05, 1.7099786418839358e-05, 1.7099786418839358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7099786418839358e-05

Optimization complete. Final v2v error: 3.4684128761291504 mm

Highest mean error: 3.852689027786255 mm for frame 110

Lowest mean error: 2.872283935546875 mm for frame 4

Saving results

Total time: 39.60939693450928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580582
Iteration 2/25 | Loss: 0.00138097
Iteration 3/25 | Loss: 0.00128177
Iteration 4/25 | Loss: 0.00125454
Iteration 5/25 | Loss: 0.00124416
Iteration 6/25 | Loss: 0.00124263
Iteration 7/25 | Loss: 0.00124263
Iteration 8/25 | Loss: 0.00124263
Iteration 9/25 | Loss: 0.00124263
Iteration 10/25 | Loss: 0.00124263
Iteration 11/25 | Loss: 0.00124263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001242628670297563, 0.001242628670297563, 0.001242628670297563, 0.001242628670297563, 0.001242628670297563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242628670297563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.66663551
Iteration 2/25 | Loss: 0.00112740
Iteration 3/25 | Loss: 0.00112740
Iteration 4/25 | Loss: 0.00112740
Iteration 5/25 | Loss: 0.00112740
Iteration 6/25 | Loss: 0.00112740
Iteration 7/25 | Loss: 0.00112740
Iteration 8/25 | Loss: 0.00112740
Iteration 9/25 | Loss: 0.00112740
Iteration 10/25 | Loss: 0.00112740
Iteration 11/25 | Loss: 0.00112740
Iteration 12/25 | Loss: 0.00112740
Iteration 13/25 | Loss: 0.00112740
Iteration 14/25 | Loss: 0.00112740
Iteration 15/25 | Loss: 0.00112740
Iteration 16/25 | Loss: 0.00112740
Iteration 17/25 | Loss: 0.00112740
Iteration 18/25 | Loss: 0.00112740
Iteration 19/25 | Loss: 0.00112740
Iteration 20/25 | Loss: 0.00112740
Iteration 21/25 | Loss: 0.00112740
Iteration 22/25 | Loss: 0.00112740
Iteration 23/25 | Loss: 0.00112740
Iteration 24/25 | Loss: 0.00112740
Iteration 25/25 | Loss: 0.00112740

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112740
Iteration 2/1000 | Loss: 0.00003274
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002320
Iteration 6/1000 | Loss: 0.00002269
Iteration 7/1000 | Loss: 0.00002209
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002130
Iteration 10/1000 | Loss: 0.00002101
Iteration 11/1000 | Loss: 0.00002068
Iteration 12/1000 | Loss: 0.00002045
Iteration 13/1000 | Loss: 0.00002039
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002015
Iteration 16/1000 | Loss: 0.00002012
Iteration 17/1000 | Loss: 0.00002008
Iteration 18/1000 | Loss: 0.00002008
Iteration 19/1000 | Loss: 0.00002007
Iteration 20/1000 | Loss: 0.00002007
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00002001
Iteration 23/1000 | Loss: 0.00002001
Iteration 24/1000 | Loss: 0.00002001
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002000
Iteration 27/1000 | Loss: 0.00001999
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001996
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001994
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001991
Iteration 36/1000 | Loss: 0.00001991
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001989
Iteration 41/1000 | Loss: 0.00001989
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001987
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001986
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001985
Iteration 52/1000 | Loss: 0.00001985
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001982
Iteration 61/1000 | Loss: 0.00001982
Iteration 62/1000 | Loss: 0.00001981
Iteration 63/1000 | Loss: 0.00001981
Iteration 64/1000 | Loss: 0.00001981
Iteration 65/1000 | Loss: 0.00001980
Iteration 66/1000 | Loss: 0.00001980
Iteration 67/1000 | Loss: 0.00001979
Iteration 68/1000 | Loss: 0.00001979
Iteration 69/1000 | Loss: 0.00001979
Iteration 70/1000 | Loss: 0.00001979
Iteration 71/1000 | Loss: 0.00001979
Iteration 72/1000 | Loss: 0.00001979
Iteration 73/1000 | Loss: 0.00001978
Iteration 74/1000 | Loss: 0.00001978
Iteration 75/1000 | Loss: 0.00001978
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001977
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001976
Iteration 82/1000 | Loss: 0.00001976
Iteration 83/1000 | Loss: 0.00001975
Iteration 84/1000 | Loss: 0.00001975
Iteration 85/1000 | Loss: 0.00001975
Iteration 86/1000 | Loss: 0.00001975
Iteration 87/1000 | Loss: 0.00001974
Iteration 88/1000 | Loss: 0.00001974
Iteration 89/1000 | Loss: 0.00001974
Iteration 90/1000 | Loss: 0.00001974
Iteration 91/1000 | Loss: 0.00001973
Iteration 92/1000 | Loss: 0.00001973
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001972
Iteration 97/1000 | Loss: 0.00001972
Iteration 98/1000 | Loss: 0.00001972
Iteration 99/1000 | Loss: 0.00001972
Iteration 100/1000 | Loss: 0.00001972
Iteration 101/1000 | Loss: 0.00001972
Iteration 102/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.9715675080078654e-05, 1.9715675080078654e-05, 1.9715675080078654e-05, 1.9715675080078654e-05, 1.9715675080078654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9715675080078654e-05

Optimization complete. Final v2v error: 3.7818403244018555 mm

Highest mean error: 4.254649639129639 mm for frame 163

Lowest mean error: 3.4245057106018066 mm for frame 29

Saving results

Total time: 42.71560001373291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533258
Iteration 2/25 | Loss: 0.00128447
Iteration 3/25 | Loss: 0.00120637
Iteration 4/25 | Loss: 0.00119360
Iteration 5/25 | Loss: 0.00118924
Iteration 6/25 | Loss: 0.00118876
Iteration 7/25 | Loss: 0.00118876
Iteration 8/25 | Loss: 0.00118876
Iteration 9/25 | Loss: 0.00118876
Iteration 10/25 | Loss: 0.00118876
Iteration 11/25 | Loss: 0.00118876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011887629516422749, 0.0011887629516422749, 0.0011887629516422749, 0.0011887629516422749, 0.0011887629516422749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011887629516422749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62448025
Iteration 2/25 | Loss: 0.00096508
Iteration 3/25 | Loss: 0.00096508
Iteration 4/25 | Loss: 0.00096508
Iteration 5/25 | Loss: 0.00096508
Iteration 6/25 | Loss: 0.00096508
Iteration 7/25 | Loss: 0.00096508
Iteration 8/25 | Loss: 0.00096507
Iteration 9/25 | Loss: 0.00096507
Iteration 10/25 | Loss: 0.00096507
Iteration 11/25 | Loss: 0.00096507
Iteration 12/25 | Loss: 0.00096507
Iteration 13/25 | Loss: 0.00096507
Iteration 14/25 | Loss: 0.00096507
Iteration 15/25 | Loss: 0.00096507
Iteration 16/25 | Loss: 0.00096507
Iteration 17/25 | Loss: 0.00096507
Iteration 18/25 | Loss: 0.00096507
Iteration 19/25 | Loss: 0.00096507
Iteration 20/25 | Loss: 0.00096507
Iteration 21/25 | Loss: 0.00096507
Iteration 22/25 | Loss: 0.00096507
Iteration 23/25 | Loss: 0.00096507
Iteration 24/25 | Loss: 0.00096507
Iteration 25/25 | Loss: 0.00096507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096507
Iteration 2/1000 | Loss: 0.00002475
Iteration 3/1000 | Loss: 0.00001764
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001290
Iteration 9/1000 | Loss: 0.00001256
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001175
Iteration 18/1000 | Loss: 0.00001173
Iteration 19/1000 | Loss: 0.00001172
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001166
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001164
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001155
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001149
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001146
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001141
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001136
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001135
Iteration 66/1000 | Loss: 0.00001135
Iteration 67/1000 | Loss: 0.00001135
Iteration 68/1000 | Loss: 0.00001135
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001132
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001132
Iteration 79/1000 | Loss: 0.00001132
Iteration 80/1000 | Loss: 0.00001132
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001129
Iteration 86/1000 | Loss: 0.00001129
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001129
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001124
Iteration 106/1000 | Loss: 0.00001124
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001124
Iteration 109/1000 | Loss: 0.00001124
Iteration 110/1000 | Loss: 0.00001124
Iteration 111/1000 | Loss: 0.00001124
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001123
Iteration 119/1000 | Loss: 0.00001123
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001122
Iteration 123/1000 | Loss: 0.00001122
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001122
Iteration 126/1000 | Loss: 0.00001122
Iteration 127/1000 | Loss: 0.00001122
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001122
Iteration 130/1000 | Loss: 0.00001122
Iteration 131/1000 | Loss: 0.00001122
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001121
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001121
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001120
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001119
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001118
Iteration 152/1000 | Loss: 0.00001118
Iteration 153/1000 | Loss: 0.00001118
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Iteration 162/1000 | Loss: 0.00001116
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Iteration 175/1000 | Loss: 0.00001115
Iteration 176/1000 | Loss: 0.00001115
Iteration 177/1000 | Loss: 0.00001115
Iteration 178/1000 | Loss: 0.00001115
Iteration 179/1000 | Loss: 0.00001115
Iteration 180/1000 | Loss: 0.00001115
Iteration 181/1000 | Loss: 0.00001115
Iteration 182/1000 | Loss: 0.00001115
Iteration 183/1000 | Loss: 0.00001115
Iteration 184/1000 | Loss: 0.00001115
Iteration 185/1000 | Loss: 0.00001115
Iteration 186/1000 | Loss: 0.00001115
Iteration 187/1000 | Loss: 0.00001115
Iteration 188/1000 | Loss: 0.00001115
Iteration 189/1000 | Loss: 0.00001115
Iteration 190/1000 | Loss: 0.00001115
Iteration 191/1000 | Loss: 0.00001115
Iteration 192/1000 | Loss: 0.00001115
Iteration 193/1000 | Loss: 0.00001115
Iteration 194/1000 | Loss: 0.00001115
Iteration 195/1000 | Loss: 0.00001115
Iteration 196/1000 | Loss: 0.00001115
Iteration 197/1000 | Loss: 0.00001115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.1146871656819712e-05, 1.1146871656819712e-05, 1.1146871656819712e-05, 1.1146871656819712e-05, 1.1146871656819712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1146871656819712e-05

Optimization complete. Final v2v error: 2.879621982574463 mm

Highest mean error: 3.2636539936065674 mm for frame 150

Lowest mean error: 2.6607985496520996 mm for frame 41

Saving results

Total time: 42.91639804840088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409735
Iteration 2/25 | Loss: 0.00126646
Iteration 3/25 | Loss: 0.00119345
Iteration 4/25 | Loss: 0.00118268
Iteration 5/25 | Loss: 0.00117853
Iteration 6/25 | Loss: 0.00117764
Iteration 7/25 | Loss: 0.00117764
Iteration 8/25 | Loss: 0.00117764
Iteration 9/25 | Loss: 0.00117764
Iteration 10/25 | Loss: 0.00117764
Iteration 11/25 | Loss: 0.00117764
Iteration 12/25 | Loss: 0.00117764
Iteration 13/25 | Loss: 0.00117764
Iteration 14/25 | Loss: 0.00117764
Iteration 15/25 | Loss: 0.00117764
Iteration 16/25 | Loss: 0.00117764
Iteration 17/25 | Loss: 0.00117764
Iteration 18/25 | Loss: 0.00117764
Iteration 19/25 | Loss: 0.00117764
Iteration 20/25 | Loss: 0.00117764
Iteration 21/25 | Loss: 0.00117764
Iteration 22/25 | Loss: 0.00117764
Iteration 23/25 | Loss: 0.00117764
Iteration 24/25 | Loss: 0.00117764
Iteration 25/25 | Loss: 0.00117764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83437872
Iteration 2/25 | Loss: 0.00105254
Iteration 3/25 | Loss: 0.00105254
Iteration 4/25 | Loss: 0.00105254
Iteration 5/25 | Loss: 0.00105254
Iteration 6/25 | Loss: 0.00105254
Iteration 7/25 | Loss: 0.00105254
Iteration 8/25 | Loss: 0.00105254
Iteration 9/25 | Loss: 0.00105254
Iteration 10/25 | Loss: 0.00105254
Iteration 11/25 | Loss: 0.00105254
Iteration 12/25 | Loss: 0.00105254
Iteration 13/25 | Loss: 0.00105254
Iteration 14/25 | Loss: 0.00105254
Iteration 15/25 | Loss: 0.00105254
Iteration 16/25 | Loss: 0.00105254
Iteration 17/25 | Loss: 0.00105254
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010525367688387632, 0.0010525367688387632, 0.0010525367688387632, 0.0010525367688387632, 0.0010525367688387632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010525367688387632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105254
Iteration 2/1000 | Loss: 0.00001745
Iteration 3/1000 | Loss: 0.00001346
Iteration 4/1000 | Loss: 0.00001216
Iteration 5/1000 | Loss: 0.00001151
Iteration 6/1000 | Loss: 0.00001110
Iteration 7/1000 | Loss: 0.00001088
Iteration 8/1000 | Loss: 0.00001065
Iteration 9/1000 | Loss: 0.00001036
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001009
Iteration 15/1000 | Loss: 0.00001000
Iteration 16/1000 | Loss: 0.00001000
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000992
Iteration 23/1000 | Loss: 0.00000991
Iteration 24/1000 | Loss: 0.00000991
Iteration 25/1000 | Loss: 0.00000991
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000987
Iteration 28/1000 | Loss: 0.00000987
Iteration 29/1000 | Loss: 0.00000986
Iteration 30/1000 | Loss: 0.00000985
Iteration 31/1000 | Loss: 0.00000984
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000978
Iteration 36/1000 | Loss: 0.00000974
Iteration 37/1000 | Loss: 0.00000974
Iteration 38/1000 | Loss: 0.00000972
Iteration 39/1000 | Loss: 0.00000968
Iteration 40/1000 | Loss: 0.00000968
Iteration 41/1000 | Loss: 0.00000967
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000967
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000963
Iteration 49/1000 | Loss: 0.00000963
Iteration 50/1000 | Loss: 0.00000963
Iteration 51/1000 | Loss: 0.00000963
Iteration 52/1000 | Loss: 0.00000963
Iteration 53/1000 | Loss: 0.00000962
Iteration 54/1000 | Loss: 0.00000962
Iteration 55/1000 | Loss: 0.00000962
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000961
Iteration 58/1000 | Loss: 0.00000961
Iteration 59/1000 | Loss: 0.00000961
Iteration 60/1000 | Loss: 0.00000960
Iteration 61/1000 | Loss: 0.00000959
Iteration 62/1000 | Loss: 0.00000959
Iteration 63/1000 | Loss: 0.00000959
Iteration 64/1000 | Loss: 0.00000959
Iteration 65/1000 | Loss: 0.00000959
Iteration 66/1000 | Loss: 0.00000959
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000958
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000957
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000955
Iteration 82/1000 | Loss: 0.00000955
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000952
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000951
Iteration 98/1000 | Loss: 0.00000951
Iteration 99/1000 | Loss: 0.00000951
Iteration 100/1000 | Loss: 0.00000951
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000949
Iteration 113/1000 | Loss: 0.00000949
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000948
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000948
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000948
Iteration 127/1000 | Loss: 0.00000948
Iteration 128/1000 | Loss: 0.00000948
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000947
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000947
Iteration 136/1000 | Loss: 0.00000947
Iteration 137/1000 | Loss: 0.00000947
Iteration 138/1000 | Loss: 0.00000946
Iteration 139/1000 | Loss: 0.00000946
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [9.464836693950929e-06, 9.464836693950929e-06, 9.464836693950929e-06, 9.464836693950929e-06, 9.464836693950929e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.464836693950929e-06

Optimization complete. Final v2v error: 2.667497396469116 mm

Highest mean error: 3.026247262954712 mm for frame 120

Lowest mean error: 2.4816348552703857 mm for frame 156

Saving results

Total time: 39.00674819946289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434692
Iteration 2/25 | Loss: 0.00137876
Iteration 3/25 | Loss: 0.00125262
Iteration 4/25 | Loss: 0.00124567
Iteration 5/25 | Loss: 0.00124467
Iteration 6/25 | Loss: 0.00124467
Iteration 7/25 | Loss: 0.00124467
Iteration 8/25 | Loss: 0.00124467
Iteration 9/25 | Loss: 0.00124467
Iteration 10/25 | Loss: 0.00124467
Iteration 11/25 | Loss: 0.00124467
Iteration 12/25 | Loss: 0.00124467
Iteration 13/25 | Loss: 0.00124467
Iteration 14/25 | Loss: 0.00124467
Iteration 15/25 | Loss: 0.00124467
Iteration 16/25 | Loss: 0.00124467
Iteration 17/25 | Loss: 0.00124467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012446742039173841, 0.0012446742039173841, 0.0012446742039173841, 0.0012446742039173841, 0.0012446742039173841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012446742039173841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.30335331
Iteration 2/25 | Loss: 0.00071165
Iteration 3/25 | Loss: 0.00071162
Iteration 4/25 | Loss: 0.00071162
Iteration 5/25 | Loss: 0.00071162
Iteration 6/25 | Loss: 0.00071162
Iteration 7/25 | Loss: 0.00071162
Iteration 8/25 | Loss: 0.00071162
Iteration 9/25 | Loss: 0.00071162
Iteration 10/25 | Loss: 0.00071162
Iteration 11/25 | Loss: 0.00071162
Iteration 12/25 | Loss: 0.00071162
Iteration 13/25 | Loss: 0.00071162
Iteration 14/25 | Loss: 0.00071162
Iteration 15/25 | Loss: 0.00071162
Iteration 16/25 | Loss: 0.00071162
Iteration 17/25 | Loss: 0.00071162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007116186898201704, 0.0007116186898201704, 0.0007116186898201704, 0.0007116186898201704, 0.0007116186898201704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007116186898201704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071162
Iteration 2/1000 | Loss: 0.00002649
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001884
Iteration 6/1000 | Loss: 0.00001805
Iteration 7/1000 | Loss: 0.00001753
Iteration 8/1000 | Loss: 0.00001705
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001615
Iteration 14/1000 | Loss: 0.00001610
Iteration 15/1000 | Loss: 0.00001598
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001594
Iteration 18/1000 | Loss: 0.00001594
Iteration 19/1000 | Loss: 0.00001592
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001590
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00001584
Iteration 33/1000 | Loss: 0.00001584
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001583
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001583
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001580
Iteration 45/1000 | Loss: 0.00001580
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001580
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001579
Iteration 51/1000 | Loss: 0.00001579
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001578
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001573
Iteration 78/1000 | Loss: 0.00001573
Iteration 79/1000 | Loss: 0.00001573
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001568
Iteration 114/1000 | Loss: 0.00001568
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001567
Iteration 120/1000 | Loss: 0.00001567
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001566
Iteration 124/1000 | Loss: 0.00001566
Iteration 125/1000 | Loss: 0.00001566
Iteration 126/1000 | Loss: 0.00001566
Iteration 127/1000 | Loss: 0.00001566
Iteration 128/1000 | Loss: 0.00001566
Iteration 129/1000 | Loss: 0.00001566
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001566
Iteration 132/1000 | Loss: 0.00001566
Iteration 133/1000 | Loss: 0.00001566
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001565
Iteration 138/1000 | Loss: 0.00001565
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001565
Iteration 141/1000 | Loss: 0.00001565
Iteration 142/1000 | Loss: 0.00001565
Iteration 143/1000 | Loss: 0.00001565
Iteration 144/1000 | Loss: 0.00001565
Iteration 145/1000 | Loss: 0.00001565
Iteration 146/1000 | Loss: 0.00001565
Iteration 147/1000 | Loss: 0.00001565
Iteration 148/1000 | Loss: 0.00001565
Iteration 149/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.5652047295589e-05, 1.5652047295589e-05, 1.5652047295589e-05, 1.5652047295589e-05, 1.5652047295589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5652047295589e-05

Optimization complete. Final v2v error: 3.3632090091705322 mm

Highest mean error: 3.7684128284454346 mm for frame 101

Lowest mean error: 3.065347194671631 mm for frame 130

Saving results

Total time: 37.88113355636597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753454
Iteration 2/25 | Loss: 0.00142053
Iteration 3/25 | Loss: 0.00129340
Iteration 4/25 | Loss: 0.00127511
Iteration 5/25 | Loss: 0.00126894
Iteration 6/25 | Loss: 0.00126755
Iteration 7/25 | Loss: 0.00126742
Iteration 8/25 | Loss: 0.00126742
Iteration 9/25 | Loss: 0.00126742
Iteration 10/25 | Loss: 0.00126742
Iteration 11/25 | Loss: 0.00126742
Iteration 12/25 | Loss: 0.00126742
Iteration 13/25 | Loss: 0.00126742
Iteration 14/25 | Loss: 0.00126742
Iteration 15/25 | Loss: 0.00126742
Iteration 16/25 | Loss: 0.00126742
Iteration 17/25 | Loss: 0.00126742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012674181489273906, 0.0012674181489273906, 0.0012674181489273906, 0.0012674181489273906, 0.0012674181489273906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012674181489273906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63463974
Iteration 2/25 | Loss: 0.00098380
Iteration 3/25 | Loss: 0.00098377
Iteration 4/25 | Loss: 0.00098377
Iteration 5/25 | Loss: 0.00098377
Iteration 6/25 | Loss: 0.00098377
Iteration 7/25 | Loss: 0.00098377
Iteration 8/25 | Loss: 0.00098377
Iteration 9/25 | Loss: 0.00098377
Iteration 10/25 | Loss: 0.00098377
Iteration 11/25 | Loss: 0.00098377
Iteration 12/25 | Loss: 0.00098377
Iteration 13/25 | Loss: 0.00098377
Iteration 14/25 | Loss: 0.00098377
Iteration 15/25 | Loss: 0.00098377
Iteration 16/25 | Loss: 0.00098377
Iteration 17/25 | Loss: 0.00098377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000983766745775938, 0.000983766745775938, 0.000983766745775938, 0.000983766745775938, 0.000983766745775938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000983766745775938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098377
Iteration 2/1000 | Loss: 0.00003982
Iteration 3/1000 | Loss: 0.00002267
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001677
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001644
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001640
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001638
Iteration 27/1000 | Loss: 0.00001638
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001629
Iteration 40/1000 | Loss: 0.00001629
Iteration 41/1000 | Loss: 0.00001629
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001628
Iteration 44/1000 | Loss: 0.00001628
Iteration 45/1000 | Loss: 0.00001628
Iteration 46/1000 | Loss: 0.00001627
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001627
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001626
Iteration 51/1000 | Loss: 0.00001626
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001625
Iteration 54/1000 | Loss: 0.00001625
Iteration 55/1000 | Loss: 0.00001625
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001625
Iteration 58/1000 | Loss: 0.00001625
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001624
Iteration 62/1000 | Loss: 0.00001624
Iteration 63/1000 | Loss: 0.00001624
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001624
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001622
Iteration 70/1000 | Loss: 0.00001622
Iteration 71/1000 | Loss: 0.00001622
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001621
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001620
Iteration 76/1000 | Loss: 0.00001620
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001619
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001615
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001615
Iteration 101/1000 | Loss: 0.00001615
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001614
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001612
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001611
Iteration 119/1000 | Loss: 0.00001611
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001609
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001609
Iteration 131/1000 | Loss: 0.00001609
Iteration 132/1000 | Loss: 0.00001609
Iteration 133/1000 | Loss: 0.00001609
Iteration 134/1000 | Loss: 0.00001609
Iteration 135/1000 | Loss: 0.00001609
Iteration 136/1000 | Loss: 0.00001609
Iteration 137/1000 | Loss: 0.00001609
Iteration 138/1000 | Loss: 0.00001609
Iteration 139/1000 | Loss: 0.00001609
Iteration 140/1000 | Loss: 0.00001609
Iteration 141/1000 | Loss: 0.00001609
Iteration 142/1000 | Loss: 0.00001609
Iteration 143/1000 | Loss: 0.00001609
Iteration 144/1000 | Loss: 0.00001609
Iteration 145/1000 | Loss: 0.00001609
Iteration 146/1000 | Loss: 0.00001609
Iteration 147/1000 | Loss: 0.00001609
Iteration 148/1000 | Loss: 0.00001609
Iteration 149/1000 | Loss: 0.00001609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.6088511983980425e-05, 1.6088511983980425e-05, 1.6088511983980425e-05, 1.6088511983980425e-05, 1.6088511983980425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6088511983980425e-05

Optimization complete. Final v2v error: 3.4319849014282227 mm

Highest mean error: 3.8366611003875732 mm for frame 48

Lowest mean error: 2.9690911769866943 mm for frame 6

Saving results

Total time: 37.75838613510132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063422
Iteration 2/25 | Loss: 0.01063422
Iteration 3/25 | Loss: 0.00340399
Iteration 4/25 | Loss: 0.00234684
Iteration 5/25 | Loss: 0.00193562
Iteration 6/25 | Loss: 0.00199977
Iteration 7/25 | Loss: 0.00188612
Iteration 8/25 | Loss: 0.00177439
Iteration 9/25 | Loss: 0.00170114
Iteration 10/25 | Loss: 0.00163335
Iteration 11/25 | Loss: 0.00160104
Iteration 12/25 | Loss: 0.00158093
Iteration 13/25 | Loss: 0.00155197
Iteration 14/25 | Loss: 0.00154357
Iteration 15/25 | Loss: 0.00152440
Iteration 16/25 | Loss: 0.00151831
Iteration 17/25 | Loss: 0.00154373
Iteration 18/25 | Loss: 0.00154081
Iteration 19/25 | Loss: 0.00151760
Iteration 20/25 | Loss: 0.00150545
Iteration 21/25 | Loss: 0.00150046
Iteration 22/25 | Loss: 0.00150269
Iteration 23/25 | Loss: 0.00149841
Iteration 24/25 | Loss: 0.00149642
Iteration 25/25 | Loss: 0.00149538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06775033
Iteration 2/25 | Loss: 0.00115601
Iteration 3/25 | Loss: 0.00102842
Iteration 4/25 | Loss: 0.00102842
Iteration 5/25 | Loss: 0.00102842
Iteration 6/25 | Loss: 0.00102842
Iteration 7/25 | Loss: 0.00102842
Iteration 8/25 | Loss: 0.00102842
Iteration 9/25 | Loss: 0.00102842
Iteration 10/25 | Loss: 0.00102842
Iteration 11/25 | Loss: 0.00102842
Iteration 12/25 | Loss: 0.00102842
Iteration 13/25 | Loss: 0.00102842
Iteration 14/25 | Loss: 0.00102842
Iteration 15/25 | Loss: 0.00102842
Iteration 16/25 | Loss: 0.00102842
Iteration 17/25 | Loss: 0.00102842
Iteration 18/25 | Loss: 0.00102842
Iteration 19/25 | Loss: 0.00102842
Iteration 20/25 | Loss: 0.00102842
Iteration 21/25 | Loss: 0.00102842
Iteration 22/25 | Loss: 0.00102842
Iteration 23/25 | Loss: 0.00102842
Iteration 24/25 | Loss: 0.00102842
Iteration 25/25 | Loss: 0.00102842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102842
Iteration 2/1000 | Loss: 0.00011761
Iteration 3/1000 | Loss: 0.00022472
Iteration 4/1000 | Loss: 0.00012153
Iteration 5/1000 | Loss: 0.00006678
Iteration 6/1000 | Loss: 0.00009025
Iteration 7/1000 | Loss: 0.00005308
Iteration 8/1000 | Loss: 0.00019805
Iteration 9/1000 | Loss: 0.00085122
Iteration 10/1000 | Loss: 0.00094118
Iteration 11/1000 | Loss: 0.00103198
Iteration 12/1000 | Loss: 0.00121354
Iteration 13/1000 | Loss: 0.00023241
Iteration 14/1000 | Loss: 0.00013597
Iteration 15/1000 | Loss: 0.00039311
Iteration 16/1000 | Loss: 0.00043231
Iteration 17/1000 | Loss: 0.00019822
Iteration 18/1000 | Loss: 0.00010774
Iteration 19/1000 | Loss: 0.00012256
Iteration 20/1000 | Loss: 0.00014661
Iteration 21/1000 | Loss: 0.00045247
Iteration 22/1000 | Loss: 0.00019130
Iteration 23/1000 | Loss: 0.00015853
Iteration 24/1000 | Loss: 0.00025279
Iteration 25/1000 | Loss: 0.00022161
Iteration 26/1000 | Loss: 0.00027890
Iteration 27/1000 | Loss: 0.00035838
Iteration 28/1000 | Loss: 0.00059019
Iteration 29/1000 | Loss: 0.00050242
Iteration 30/1000 | Loss: 0.00021266
Iteration 31/1000 | Loss: 0.00070658
Iteration 32/1000 | Loss: 0.00042875
Iteration 33/1000 | Loss: 0.00062059
Iteration 34/1000 | Loss: 0.00087218
Iteration 35/1000 | Loss: 0.00186378
Iteration 36/1000 | Loss: 0.00199607
Iteration 37/1000 | Loss: 0.00264605
Iteration 38/1000 | Loss: 0.00404330
Iteration 39/1000 | Loss: 0.00364855
Iteration 40/1000 | Loss: 0.00143410
Iteration 41/1000 | Loss: 0.00146956
Iteration 42/1000 | Loss: 0.00111768
Iteration 43/1000 | Loss: 0.00051784
Iteration 44/1000 | Loss: 0.00089102
Iteration 45/1000 | Loss: 0.00017926
Iteration 46/1000 | Loss: 0.00028921
Iteration 47/1000 | Loss: 0.00060197
Iteration 48/1000 | Loss: 0.00072145
Iteration 49/1000 | Loss: 0.00045829
Iteration 50/1000 | Loss: 0.00027391
Iteration 51/1000 | Loss: 0.00056250
Iteration 52/1000 | Loss: 0.00044227
Iteration 53/1000 | Loss: 0.00010962
Iteration 54/1000 | Loss: 0.00008053
Iteration 55/1000 | Loss: 0.00032456
Iteration 56/1000 | Loss: 0.00053853
Iteration 57/1000 | Loss: 0.00029781
Iteration 58/1000 | Loss: 0.00036047
Iteration 59/1000 | Loss: 0.00009209
Iteration 60/1000 | Loss: 0.00006827
Iteration 61/1000 | Loss: 0.00029818
Iteration 62/1000 | Loss: 0.00040366
Iteration 63/1000 | Loss: 0.00010168
Iteration 64/1000 | Loss: 0.00007511
Iteration 65/1000 | Loss: 0.00020730
Iteration 66/1000 | Loss: 0.00007804
Iteration 67/1000 | Loss: 0.00012594
Iteration 68/1000 | Loss: 0.00005951
Iteration 69/1000 | Loss: 0.00010546
Iteration 70/1000 | Loss: 0.00031315
Iteration 71/1000 | Loss: 0.00017610
Iteration 72/1000 | Loss: 0.00053762
Iteration 73/1000 | Loss: 0.00016688
Iteration 74/1000 | Loss: 0.00008678
Iteration 75/1000 | Loss: 0.00009609
Iteration 76/1000 | Loss: 0.00016308
Iteration 77/1000 | Loss: 0.00006118
Iteration 78/1000 | Loss: 0.00009365
Iteration 79/1000 | Loss: 0.00005607
Iteration 80/1000 | Loss: 0.00012742
Iteration 81/1000 | Loss: 0.00027657
Iteration 82/1000 | Loss: 0.00009953
Iteration 83/1000 | Loss: 0.00014687
Iteration 84/1000 | Loss: 0.00010901
Iteration 85/1000 | Loss: 0.00013214
Iteration 86/1000 | Loss: 0.00012808
Iteration 87/1000 | Loss: 0.00011482
Iteration 88/1000 | Loss: 0.00012598
Iteration 89/1000 | Loss: 0.00005277
Iteration 90/1000 | Loss: 0.00005996
Iteration 91/1000 | Loss: 0.00005039
Iteration 92/1000 | Loss: 0.00004945
Iteration 93/1000 | Loss: 0.00006798
Iteration 94/1000 | Loss: 0.00041657
Iteration 95/1000 | Loss: 0.00023215
Iteration 96/1000 | Loss: 0.00038456
Iteration 97/1000 | Loss: 0.00016180
Iteration 98/1000 | Loss: 0.00015271
Iteration 99/1000 | Loss: 0.00007344
Iteration 100/1000 | Loss: 0.00016978
Iteration 101/1000 | Loss: 0.00006822
Iteration 102/1000 | Loss: 0.00007331
Iteration 103/1000 | Loss: 0.00006571
Iteration 104/1000 | Loss: 0.00021157
Iteration 105/1000 | Loss: 0.00031813
Iteration 106/1000 | Loss: 0.00047411
Iteration 107/1000 | Loss: 0.00017379
Iteration 108/1000 | Loss: 0.00011884
Iteration 109/1000 | Loss: 0.00021305
Iteration 110/1000 | Loss: 0.00008695
Iteration 111/1000 | Loss: 0.00007273
Iteration 112/1000 | Loss: 0.00030063
Iteration 113/1000 | Loss: 0.00006928
Iteration 114/1000 | Loss: 0.00006499
Iteration 115/1000 | Loss: 0.00009016
Iteration 116/1000 | Loss: 0.00005802
Iteration 117/1000 | Loss: 0.00008615
Iteration 118/1000 | Loss: 0.00006365
Iteration 119/1000 | Loss: 0.00012010
Iteration 120/1000 | Loss: 0.00009546
Iteration 121/1000 | Loss: 0.00007242
Iteration 122/1000 | Loss: 0.00007929
Iteration 123/1000 | Loss: 0.00007422
Iteration 124/1000 | Loss: 0.00008724
Iteration 125/1000 | Loss: 0.00006227
Iteration 126/1000 | Loss: 0.00015092
Iteration 127/1000 | Loss: 0.00006630
Iteration 128/1000 | Loss: 0.00005065
Iteration 129/1000 | Loss: 0.00005922
Iteration 130/1000 | Loss: 0.00007239
Iteration 131/1000 | Loss: 0.00027868
Iteration 132/1000 | Loss: 0.00025287
Iteration 133/1000 | Loss: 0.00008766
Iteration 134/1000 | Loss: 0.00007280
Iteration 135/1000 | Loss: 0.00006687
Iteration 136/1000 | Loss: 0.00006816
Iteration 137/1000 | Loss: 0.00006643
Iteration 138/1000 | Loss: 0.00006363
Iteration 139/1000 | Loss: 0.00009266
Iteration 140/1000 | Loss: 0.00031235
Iteration 141/1000 | Loss: 0.00007577
Iteration 142/1000 | Loss: 0.00007946
Iteration 143/1000 | Loss: 0.00012771
Iteration 144/1000 | Loss: 0.00006738
Iteration 145/1000 | Loss: 0.00005560
Iteration 146/1000 | Loss: 0.00004637
Iteration 147/1000 | Loss: 0.00012581
Iteration 148/1000 | Loss: 0.00007359
Iteration 149/1000 | Loss: 0.00006147
Iteration 150/1000 | Loss: 0.00005163
Iteration 151/1000 | Loss: 0.00009719
Iteration 152/1000 | Loss: 0.00004688
Iteration 153/1000 | Loss: 0.00005223
Iteration 154/1000 | Loss: 0.00005150
Iteration 155/1000 | Loss: 0.00005086
Iteration 156/1000 | Loss: 0.00019826
Iteration 157/1000 | Loss: 0.00060100
Iteration 158/1000 | Loss: 0.00048918
Iteration 159/1000 | Loss: 0.00023839
Iteration 160/1000 | Loss: 0.00008726
Iteration 161/1000 | Loss: 0.00021710
Iteration 162/1000 | Loss: 0.00005136
Iteration 163/1000 | Loss: 0.00009603
Iteration 164/1000 | Loss: 0.00004935
Iteration 165/1000 | Loss: 0.00013474
Iteration 166/1000 | Loss: 0.00006304
Iteration 167/1000 | Loss: 0.00008609
Iteration 168/1000 | Loss: 0.00056895
Iteration 169/1000 | Loss: 0.00055357
Iteration 170/1000 | Loss: 0.00030074
Iteration 171/1000 | Loss: 0.00005468
Iteration 172/1000 | Loss: 0.00005351
Iteration 173/1000 | Loss: 0.00004557
Iteration 174/1000 | Loss: 0.00010170
Iteration 175/1000 | Loss: 0.00004916
Iteration 176/1000 | Loss: 0.00032646
Iteration 177/1000 | Loss: 0.00035051
Iteration 178/1000 | Loss: 0.00009896
Iteration 179/1000 | Loss: 0.00023956
Iteration 180/1000 | Loss: 0.00017896
Iteration 181/1000 | Loss: 0.00007693
Iteration 182/1000 | Loss: 0.00005837
Iteration 183/1000 | Loss: 0.00020728
Iteration 184/1000 | Loss: 0.00021341
Iteration 185/1000 | Loss: 0.00015012
Iteration 186/1000 | Loss: 0.00020337
Iteration 187/1000 | Loss: 0.00016261
Iteration 188/1000 | Loss: 0.00005896
Iteration 189/1000 | Loss: 0.00012680
Iteration 190/1000 | Loss: 0.00006108
Iteration 191/1000 | Loss: 0.00005176
Iteration 192/1000 | Loss: 0.00009246
Iteration 193/1000 | Loss: 0.00004570
Iteration 194/1000 | Loss: 0.00006408
Iteration 195/1000 | Loss: 0.00004393
Iteration 196/1000 | Loss: 0.00016365
Iteration 197/1000 | Loss: 0.00031459
Iteration 198/1000 | Loss: 0.00015424
Iteration 199/1000 | Loss: 0.00010327
Iteration 200/1000 | Loss: 0.00031062
Iteration 201/1000 | Loss: 0.00008935
Iteration 202/1000 | Loss: 0.00004299
Iteration 203/1000 | Loss: 0.00004158
Iteration 204/1000 | Loss: 0.00022795
Iteration 205/1000 | Loss: 0.00008342
Iteration 206/1000 | Loss: 0.00006218
Iteration 207/1000 | Loss: 0.00004418
Iteration 208/1000 | Loss: 0.00004149
Iteration 209/1000 | Loss: 0.00004040
Iteration 210/1000 | Loss: 0.00003959
Iteration 211/1000 | Loss: 0.00010719
Iteration 212/1000 | Loss: 0.00007869
Iteration 213/1000 | Loss: 0.00008324
Iteration 214/1000 | Loss: 0.00010328
Iteration 215/1000 | Loss: 0.00009844
Iteration 216/1000 | Loss: 0.00008215
Iteration 217/1000 | Loss: 0.00008918
Iteration 218/1000 | Loss: 0.00010119
Iteration 219/1000 | Loss: 0.00012427
Iteration 220/1000 | Loss: 0.00010688
Iteration 221/1000 | Loss: 0.00005286
Iteration 222/1000 | Loss: 0.00008471
Iteration 223/1000 | Loss: 0.00004413
Iteration 224/1000 | Loss: 0.00006906
Iteration 225/1000 | Loss: 0.00004192
Iteration 226/1000 | Loss: 0.00009472
Iteration 227/1000 | Loss: 0.00004227
Iteration 228/1000 | Loss: 0.00004272
Iteration 229/1000 | Loss: 0.00006751
Iteration 230/1000 | Loss: 0.00003933
Iteration 231/1000 | Loss: 0.00006840
Iteration 232/1000 | Loss: 0.00003850
Iteration 233/1000 | Loss: 0.00005838
Iteration 234/1000 | Loss: 0.00003825
Iteration 235/1000 | Loss: 0.00003801
Iteration 236/1000 | Loss: 0.00003788
Iteration 237/1000 | Loss: 0.00003772
Iteration 238/1000 | Loss: 0.00003771
Iteration 239/1000 | Loss: 0.00003763
Iteration 240/1000 | Loss: 0.00003761
Iteration 241/1000 | Loss: 0.00003759
Iteration 242/1000 | Loss: 0.00003759
Iteration 243/1000 | Loss: 0.00003759
Iteration 244/1000 | Loss: 0.00003759
Iteration 245/1000 | Loss: 0.00003759
Iteration 246/1000 | Loss: 0.00003759
Iteration 247/1000 | Loss: 0.00003759
Iteration 248/1000 | Loss: 0.00003759
Iteration 249/1000 | Loss: 0.00003759
Iteration 250/1000 | Loss: 0.00003759
Iteration 251/1000 | Loss: 0.00003759
Iteration 252/1000 | Loss: 0.00003758
Iteration 253/1000 | Loss: 0.00003758
Iteration 254/1000 | Loss: 0.00003758
Iteration 255/1000 | Loss: 0.00003758
Iteration 256/1000 | Loss: 0.00003758
Iteration 257/1000 | Loss: 0.00003758
Iteration 258/1000 | Loss: 0.00003758
Iteration 259/1000 | Loss: 0.00003758
Iteration 260/1000 | Loss: 0.00003758
Iteration 261/1000 | Loss: 0.00003757
Iteration 262/1000 | Loss: 0.00003756
Iteration 263/1000 | Loss: 0.00003756
Iteration 264/1000 | Loss: 0.00003756
Iteration 265/1000 | Loss: 0.00003756
Iteration 266/1000 | Loss: 0.00003756
Iteration 267/1000 | Loss: 0.00003755
Iteration 268/1000 | Loss: 0.00003755
Iteration 269/1000 | Loss: 0.00003755
Iteration 270/1000 | Loss: 0.00003755
Iteration 271/1000 | Loss: 0.00003755
Iteration 272/1000 | Loss: 0.00003755
Iteration 273/1000 | Loss: 0.00003754
Iteration 274/1000 | Loss: 0.00003754
Iteration 275/1000 | Loss: 0.00003753
Iteration 276/1000 | Loss: 0.00003753
Iteration 277/1000 | Loss: 0.00003753
Iteration 278/1000 | Loss: 0.00003753
Iteration 279/1000 | Loss: 0.00003753
Iteration 280/1000 | Loss: 0.00003753
Iteration 281/1000 | Loss: 0.00003753
Iteration 282/1000 | Loss: 0.00003753
Iteration 283/1000 | Loss: 0.00003753
Iteration 284/1000 | Loss: 0.00003753
Iteration 285/1000 | Loss: 0.00003753
Iteration 286/1000 | Loss: 0.00003752
Iteration 287/1000 | Loss: 0.00003752
Iteration 288/1000 | Loss: 0.00003752
Iteration 289/1000 | Loss: 0.00003751
Iteration 290/1000 | Loss: 0.00003751
Iteration 291/1000 | Loss: 0.00003751
Iteration 292/1000 | Loss: 0.00003751
Iteration 293/1000 | Loss: 0.00003751
Iteration 294/1000 | Loss: 0.00003750
Iteration 295/1000 | Loss: 0.00003750
Iteration 296/1000 | Loss: 0.00003750
Iteration 297/1000 | Loss: 0.00003750
Iteration 298/1000 | Loss: 0.00003749
Iteration 299/1000 | Loss: 0.00003748
Iteration 300/1000 | Loss: 0.00003747
Iteration 301/1000 | Loss: 0.00003747
Iteration 302/1000 | Loss: 0.00003746
Iteration 303/1000 | Loss: 0.00003746
Iteration 304/1000 | Loss: 0.00003743
Iteration 305/1000 | Loss: 0.00003743
Iteration 306/1000 | Loss: 0.00003743
Iteration 307/1000 | Loss: 0.00003743
Iteration 308/1000 | Loss: 0.00003743
Iteration 309/1000 | Loss: 0.00003743
Iteration 310/1000 | Loss: 0.00003743
Iteration 311/1000 | Loss: 0.00003743
Iteration 312/1000 | Loss: 0.00003742
Iteration 313/1000 | Loss: 0.00003742
Iteration 314/1000 | Loss: 0.00003742
Iteration 315/1000 | Loss: 0.00003742
Iteration 316/1000 | Loss: 0.00003742
Iteration 317/1000 | Loss: 0.00003742
Iteration 318/1000 | Loss: 0.00003742
Iteration 319/1000 | Loss: 0.00003742
Iteration 320/1000 | Loss: 0.00003742
Iteration 321/1000 | Loss: 0.00003742
Iteration 322/1000 | Loss: 0.00003742
Iteration 323/1000 | Loss: 0.00003742
Iteration 324/1000 | Loss: 0.00003742
Iteration 325/1000 | Loss: 0.00003742
Iteration 326/1000 | Loss: 0.00003742
Iteration 327/1000 | Loss: 0.00003742
Iteration 328/1000 | Loss: 0.00003742
Iteration 329/1000 | Loss: 0.00003742
Iteration 330/1000 | Loss: 0.00003742
Iteration 331/1000 | Loss: 0.00003742
Iteration 332/1000 | Loss: 0.00003742
Iteration 333/1000 | Loss: 0.00003742
Iteration 334/1000 | Loss: 0.00003742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 334. Stopping optimization.
Last 5 losses: [3.74213486793451e-05, 3.74213486793451e-05, 3.74213486793451e-05, 3.74213486793451e-05, 3.74213486793451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.74213486793451e-05

Optimization complete. Final v2v error: 4.818680286407471 mm

Highest mean error: 8.144784927368164 mm for frame 49

Lowest mean error: 3.8269717693328857 mm for frame 0

Saving results

Total time: 433.95529413223267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709109
Iteration 2/25 | Loss: 0.00173196
Iteration 3/25 | Loss: 0.00142635
Iteration 4/25 | Loss: 0.00125790
Iteration 5/25 | Loss: 0.00125753
Iteration 6/25 | Loss: 0.00123383
Iteration 7/25 | Loss: 0.00122453
Iteration 8/25 | Loss: 0.00122030
Iteration 9/25 | Loss: 0.00122386
Iteration 10/25 | Loss: 0.00121107
Iteration 11/25 | Loss: 0.00120921
Iteration 12/25 | Loss: 0.00120913
Iteration 13/25 | Loss: 0.00120913
Iteration 14/25 | Loss: 0.00120913
Iteration 15/25 | Loss: 0.00120913
Iteration 16/25 | Loss: 0.00120913
Iteration 17/25 | Loss: 0.00120913
Iteration 18/25 | Loss: 0.00120913
Iteration 19/25 | Loss: 0.00120913
Iteration 20/25 | Loss: 0.00120912
Iteration 21/25 | Loss: 0.00120912
Iteration 22/25 | Loss: 0.00120912
Iteration 23/25 | Loss: 0.00120912
Iteration 24/25 | Loss: 0.00120912
Iteration 25/25 | Loss: 0.00120912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.74772358
Iteration 2/25 | Loss: 0.00106966
Iteration 3/25 | Loss: 0.00106966
Iteration 4/25 | Loss: 0.00106966
Iteration 5/25 | Loss: 0.00106966
Iteration 6/25 | Loss: 0.00106966
Iteration 7/25 | Loss: 0.00106966
Iteration 8/25 | Loss: 0.00106966
Iteration 9/25 | Loss: 0.00106966
Iteration 10/25 | Loss: 0.00106966
Iteration 11/25 | Loss: 0.00106966
Iteration 12/25 | Loss: 0.00106966
Iteration 13/25 | Loss: 0.00106966
Iteration 14/25 | Loss: 0.00106966
Iteration 15/25 | Loss: 0.00106966
Iteration 16/25 | Loss: 0.00106966
Iteration 17/25 | Loss: 0.00106966
Iteration 18/25 | Loss: 0.00106966
Iteration 19/25 | Loss: 0.00106966
Iteration 20/25 | Loss: 0.00106965
Iteration 21/25 | Loss: 0.00106965
Iteration 22/25 | Loss: 0.00106965
Iteration 23/25 | Loss: 0.00106965
Iteration 24/25 | Loss: 0.00106965
Iteration 25/25 | Loss: 0.00106965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106965
Iteration 2/1000 | Loss: 0.00002196
Iteration 3/1000 | Loss: 0.00004136
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001547
Iteration 6/1000 | Loss: 0.00002369
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00002118
Iteration 9/1000 | Loss: 0.00002036
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001366
Iteration 13/1000 | Loss: 0.00002259
Iteration 14/1000 | Loss: 0.00001344
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001340
Iteration 18/1000 | Loss: 0.00001332
Iteration 19/1000 | Loss: 0.00001331
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001329
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001329
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001321
Iteration 26/1000 | Loss: 0.00002385
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001301
Iteration 34/1000 | Loss: 0.00001301
Iteration 35/1000 | Loss: 0.00001301
Iteration 36/1000 | Loss: 0.00001301
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001301
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001300
Iteration 45/1000 | Loss: 0.00001300
Iteration 46/1000 | Loss: 0.00001300
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001300
Iteration 52/1000 | Loss: 0.00001300
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001299
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001295
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001294
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001286
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001283
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001282
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001282
Iteration 119/1000 | Loss: 0.00001281
Iteration 120/1000 | Loss: 0.00001281
Iteration 121/1000 | Loss: 0.00001281
Iteration 122/1000 | Loss: 0.00001281
Iteration 123/1000 | Loss: 0.00001281
Iteration 124/1000 | Loss: 0.00001281
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001280
Iteration 128/1000 | Loss: 0.00001280
Iteration 129/1000 | Loss: 0.00001280
Iteration 130/1000 | Loss: 0.00001280
Iteration 131/1000 | Loss: 0.00001280
Iteration 132/1000 | Loss: 0.00001280
Iteration 133/1000 | Loss: 0.00001280
Iteration 134/1000 | Loss: 0.00001280
Iteration 135/1000 | Loss: 0.00001280
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001278
Iteration 141/1000 | Loss: 0.00001278
Iteration 142/1000 | Loss: 0.00001278
Iteration 143/1000 | Loss: 0.00001277
Iteration 144/1000 | Loss: 0.00001277
Iteration 145/1000 | Loss: 0.00001277
Iteration 146/1000 | Loss: 0.00001277
Iteration 147/1000 | Loss: 0.00001277
Iteration 148/1000 | Loss: 0.00001277
Iteration 149/1000 | Loss: 0.00001276
Iteration 150/1000 | Loss: 0.00001276
Iteration 151/1000 | Loss: 0.00001276
Iteration 152/1000 | Loss: 0.00001276
Iteration 153/1000 | Loss: 0.00001276
Iteration 154/1000 | Loss: 0.00001276
Iteration 155/1000 | Loss: 0.00001276
Iteration 156/1000 | Loss: 0.00001276
Iteration 157/1000 | Loss: 0.00001275
Iteration 158/1000 | Loss: 0.00001275
Iteration 159/1000 | Loss: 0.00001275
Iteration 160/1000 | Loss: 0.00001275
Iteration 161/1000 | Loss: 0.00001274
Iteration 162/1000 | Loss: 0.00001274
Iteration 163/1000 | Loss: 0.00001274
Iteration 164/1000 | Loss: 0.00001274
Iteration 165/1000 | Loss: 0.00001274
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001273
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001272
Iteration 177/1000 | Loss: 0.00001272
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001272
Iteration 182/1000 | Loss: 0.00001272
Iteration 183/1000 | Loss: 0.00001272
Iteration 184/1000 | Loss: 0.00001272
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001272
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001272
Iteration 193/1000 | Loss: 0.00001272
Iteration 194/1000 | Loss: 0.00001272
Iteration 195/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2719106962322257e-05, 1.2719106962322257e-05, 1.2719106962322257e-05, 1.2719106962322257e-05, 1.2719106962322257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2719106962322257e-05

Optimization complete. Final v2v error: 3.0235517024993896 mm

Highest mean error: 3.702091932296753 mm for frame 43

Lowest mean error: 2.7079739570617676 mm for frame 131

Saving results

Total time: 66.40333223342896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433307
Iteration 2/25 | Loss: 0.00142344
Iteration 3/25 | Loss: 0.00126731
Iteration 4/25 | Loss: 0.00125903
Iteration 5/25 | Loss: 0.00125731
Iteration 6/25 | Loss: 0.00125724
Iteration 7/25 | Loss: 0.00125724
Iteration 8/25 | Loss: 0.00125724
Iteration 9/25 | Loss: 0.00125724
Iteration 10/25 | Loss: 0.00125724
Iteration 11/25 | Loss: 0.00125724
Iteration 12/25 | Loss: 0.00125724
Iteration 13/25 | Loss: 0.00125724
Iteration 14/25 | Loss: 0.00125724
Iteration 15/25 | Loss: 0.00125724
Iteration 16/25 | Loss: 0.00125724
Iteration 17/25 | Loss: 0.00125724
Iteration 18/25 | Loss: 0.00125724
Iteration 19/25 | Loss: 0.00125724
Iteration 20/25 | Loss: 0.00125724
Iteration 21/25 | Loss: 0.00125724
Iteration 22/25 | Loss: 0.00125724
Iteration 23/25 | Loss: 0.00125724
Iteration 24/25 | Loss: 0.00125724
Iteration 25/25 | Loss: 0.00125724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.81626654
Iteration 2/25 | Loss: 0.00088441
Iteration 3/25 | Loss: 0.00088437
Iteration 4/25 | Loss: 0.00088437
Iteration 5/25 | Loss: 0.00088437
Iteration 6/25 | Loss: 0.00088437
Iteration 7/25 | Loss: 0.00088436
Iteration 8/25 | Loss: 0.00088436
Iteration 9/25 | Loss: 0.00088436
Iteration 10/25 | Loss: 0.00088436
Iteration 11/25 | Loss: 0.00088436
Iteration 12/25 | Loss: 0.00088436
Iteration 13/25 | Loss: 0.00088436
Iteration 14/25 | Loss: 0.00088436
Iteration 15/25 | Loss: 0.00088436
Iteration 16/25 | Loss: 0.00088436
Iteration 17/25 | Loss: 0.00088436
Iteration 18/25 | Loss: 0.00088436
Iteration 19/25 | Loss: 0.00088436
Iteration 20/25 | Loss: 0.00088436
Iteration 21/25 | Loss: 0.00088436
Iteration 22/25 | Loss: 0.00088436
Iteration 23/25 | Loss: 0.00088436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008843637770041823, 0.0008843637770041823, 0.0008843637770041823, 0.0008843637770041823, 0.0008843637770041823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008843637770041823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088436
Iteration 2/1000 | Loss: 0.00003450
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00001918
Iteration 5/1000 | Loss: 0.00001757
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001615
Iteration 8/1000 | Loss: 0.00001562
Iteration 9/1000 | Loss: 0.00001538
Iteration 10/1000 | Loss: 0.00001514
Iteration 11/1000 | Loss: 0.00001489
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001476
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001474
Iteration 16/1000 | Loss: 0.00001469
Iteration 17/1000 | Loss: 0.00001462
Iteration 18/1000 | Loss: 0.00001461
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001445
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001438
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001436
Iteration 28/1000 | Loss: 0.00001436
Iteration 29/1000 | Loss: 0.00001436
Iteration 30/1000 | Loss: 0.00001435
Iteration 31/1000 | Loss: 0.00001435
Iteration 32/1000 | Loss: 0.00001434
Iteration 33/1000 | Loss: 0.00001434
Iteration 34/1000 | Loss: 0.00001433
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001429
Iteration 41/1000 | Loss: 0.00001429
Iteration 42/1000 | Loss: 0.00001428
Iteration 43/1000 | Loss: 0.00001428
Iteration 44/1000 | Loss: 0.00001428
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001421
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001418
Iteration 57/1000 | Loss: 0.00001417
Iteration 58/1000 | Loss: 0.00001417
Iteration 59/1000 | Loss: 0.00001416
Iteration 60/1000 | Loss: 0.00001416
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001414
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001411
Iteration 80/1000 | Loss: 0.00001411
Iteration 81/1000 | Loss: 0.00001411
Iteration 82/1000 | Loss: 0.00001410
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001410
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001408
Iteration 90/1000 | Loss: 0.00001408
Iteration 91/1000 | Loss: 0.00001408
Iteration 92/1000 | Loss: 0.00001408
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001407
Iteration 96/1000 | Loss: 0.00001407
Iteration 97/1000 | Loss: 0.00001407
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001406
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001404
Iteration 105/1000 | Loss: 0.00001404
Iteration 106/1000 | Loss: 0.00001404
Iteration 107/1000 | Loss: 0.00001404
Iteration 108/1000 | Loss: 0.00001404
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001403
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001403
Iteration 114/1000 | Loss: 0.00001403
Iteration 115/1000 | Loss: 0.00001403
Iteration 116/1000 | Loss: 0.00001403
Iteration 117/1000 | Loss: 0.00001402
Iteration 118/1000 | Loss: 0.00001402
Iteration 119/1000 | Loss: 0.00001402
Iteration 120/1000 | Loss: 0.00001402
Iteration 121/1000 | Loss: 0.00001402
Iteration 122/1000 | Loss: 0.00001402
Iteration 123/1000 | Loss: 0.00001402
Iteration 124/1000 | Loss: 0.00001402
Iteration 125/1000 | Loss: 0.00001402
Iteration 126/1000 | Loss: 0.00001402
Iteration 127/1000 | Loss: 0.00001402
Iteration 128/1000 | Loss: 0.00001402
Iteration 129/1000 | Loss: 0.00001402
Iteration 130/1000 | Loss: 0.00001402
Iteration 131/1000 | Loss: 0.00001402
Iteration 132/1000 | Loss: 0.00001402
Iteration 133/1000 | Loss: 0.00001402
Iteration 134/1000 | Loss: 0.00001402
Iteration 135/1000 | Loss: 0.00001402
Iteration 136/1000 | Loss: 0.00001402
Iteration 137/1000 | Loss: 0.00001401
Iteration 138/1000 | Loss: 0.00001401
Iteration 139/1000 | Loss: 0.00001401
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001401
Iteration 143/1000 | Loss: 0.00001401
Iteration 144/1000 | Loss: 0.00001401
Iteration 145/1000 | Loss: 0.00001401
Iteration 146/1000 | Loss: 0.00001401
Iteration 147/1000 | Loss: 0.00001401
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001400
Iteration 150/1000 | Loss: 0.00001400
Iteration 151/1000 | Loss: 0.00001400
Iteration 152/1000 | Loss: 0.00001400
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001400
Iteration 162/1000 | Loss: 0.00001400
Iteration 163/1000 | Loss: 0.00001400
Iteration 164/1000 | Loss: 0.00001400
Iteration 165/1000 | Loss: 0.00001400
Iteration 166/1000 | Loss: 0.00001400
Iteration 167/1000 | Loss: 0.00001400
Iteration 168/1000 | Loss: 0.00001400
Iteration 169/1000 | Loss: 0.00001400
Iteration 170/1000 | Loss: 0.00001400
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4004775948706083e-05, 1.4004775948706083e-05, 1.4004775948706083e-05, 1.4004775948706083e-05, 1.4004775948706083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4004775948706083e-05

Optimization complete. Final v2v error: 3.1629996299743652 mm

Highest mean error: 3.7642366886138916 mm for frame 86

Lowest mean error: 2.828676462173462 mm for frame 6

Saving results

Total time: 39.62089228630066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640685
Iteration 2/25 | Loss: 0.00134659
Iteration 3/25 | Loss: 0.00126984
Iteration 4/25 | Loss: 0.00126371
Iteration 5/25 | Loss: 0.00126171
Iteration 6/25 | Loss: 0.00126171
Iteration 7/25 | Loss: 0.00126171
Iteration 8/25 | Loss: 0.00126171
Iteration 9/25 | Loss: 0.00126171
Iteration 10/25 | Loss: 0.00126171
Iteration 11/25 | Loss: 0.00126171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012617087922990322, 0.0012617087922990322, 0.0012617087922990322, 0.0012617087922990322, 0.0012617087922990322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012617087922990322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07048559
Iteration 2/25 | Loss: 0.00095316
Iteration 3/25 | Loss: 0.00095309
Iteration 4/25 | Loss: 0.00095309
Iteration 5/25 | Loss: 0.00095309
Iteration 6/25 | Loss: 0.00095309
Iteration 7/25 | Loss: 0.00095309
Iteration 8/25 | Loss: 0.00095309
Iteration 9/25 | Loss: 0.00095309
Iteration 10/25 | Loss: 0.00095309
Iteration 11/25 | Loss: 0.00095309
Iteration 12/25 | Loss: 0.00095309
Iteration 13/25 | Loss: 0.00095309
Iteration 14/25 | Loss: 0.00095309
Iteration 15/25 | Loss: 0.00095309
Iteration 16/25 | Loss: 0.00095309
Iteration 17/25 | Loss: 0.00095309
Iteration 18/25 | Loss: 0.00095309
Iteration 19/25 | Loss: 0.00095309
Iteration 20/25 | Loss: 0.00095309
Iteration 21/25 | Loss: 0.00095309
Iteration 22/25 | Loss: 0.00095309
Iteration 23/25 | Loss: 0.00095309
Iteration 24/25 | Loss: 0.00095309
Iteration 25/25 | Loss: 0.00095309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095309
Iteration 2/1000 | Loss: 0.00002451
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001790
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001538
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001491
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001487
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001482
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001474
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001472
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001470
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001464
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001462
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001460
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001456
Iteration 44/1000 | Loss: 0.00001456
Iteration 45/1000 | Loss: 0.00001455
Iteration 46/1000 | Loss: 0.00001455
Iteration 47/1000 | Loss: 0.00001454
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001447
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001445
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001443
Iteration 83/1000 | Loss: 0.00001443
Iteration 84/1000 | Loss: 0.00001443
Iteration 85/1000 | Loss: 0.00001443
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001441
Iteration 92/1000 | Loss: 0.00001441
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00001438
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4379432286659721e-05, 1.4379432286659721e-05, 1.4379432286659721e-05, 1.4379432286659721e-05, 1.4379432286659721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4379432286659721e-05

Optimization complete. Final v2v error: 3.20054292678833 mm

Highest mean error: 3.7891671657562256 mm for frame 140

Lowest mean error: 2.975184202194214 mm for frame 17

Saving results

Total time: 38.15226602554321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996050
Iteration 2/25 | Loss: 0.00271514
Iteration 3/25 | Loss: 0.00213236
Iteration 4/25 | Loss: 0.00190567
Iteration 5/25 | Loss: 0.00178338
Iteration 6/25 | Loss: 0.00167401
Iteration 7/25 | Loss: 0.00164200
Iteration 8/25 | Loss: 0.00154434
Iteration 9/25 | Loss: 0.00151803
Iteration 10/25 | Loss: 0.00149733
Iteration 11/25 | Loss: 0.00149166
Iteration 12/25 | Loss: 0.00146016
Iteration 13/25 | Loss: 0.00145176
Iteration 14/25 | Loss: 0.00144830
Iteration 15/25 | Loss: 0.00144863
Iteration 16/25 | Loss: 0.00144998
Iteration 17/25 | Loss: 0.00144875
Iteration 18/25 | Loss: 0.00144051
Iteration 19/25 | Loss: 0.00143788
Iteration 20/25 | Loss: 0.00143327
Iteration 21/25 | Loss: 0.00143487
Iteration 22/25 | Loss: 0.00143860
Iteration 23/25 | Loss: 0.00143907
Iteration 24/25 | Loss: 0.00143085
Iteration 25/25 | Loss: 0.00142998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53723741
Iteration 2/25 | Loss: 0.00219642
Iteration 3/25 | Loss: 0.00219642
Iteration 4/25 | Loss: 0.00219641
Iteration 5/25 | Loss: 0.00219641
Iteration 6/25 | Loss: 0.00219641
Iteration 7/25 | Loss: 0.00219641
Iteration 8/25 | Loss: 0.00219641
Iteration 9/25 | Loss: 0.00219641
Iteration 10/25 | Loss: 0.00219641
Iteration 11/25 | Loss: 0.00219641
Iteration 12/25 | Loss: 0.00219641
Iteration 13/25 | Loss: 0.00219641
Iteration 14/25 | Loss: 0.00219641
Iteration 15/25 | Loss: 0.00219641
Iteration 16/25 | Loss: 0.00219641
Iteration 17/25 | Loss: 0.00219641
Iteration 18/25 | Loss: 0.00219641
Iteration 19/25 | Loss: 0.00219641
Iteration 20/25 | Loss: 0.00219641
Iteration 21/25 | Loss: 0.00219641
Iteration 22/25 | Loss: 0.00219641
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0021964125335216522, 0.0021964125335216522, 0.0021964125335216522, 0.0021964125335216522, 0.0021964125335216522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021964125335216522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219641
Iteration 2/1000 | Loss: 0.00116765
Iteration 3/1000 | Loss: 0.00111981
Iteration 4/1000 | Loss: 0.00121782
Iteration 5/1000 | Loss: 0.00049339
Iteration 6/1000 | Loss: 0.00140633
Iteration 7/1000 | Loss: 0.00029121
Iteration 8/1000 | Loss: 0.00030794
Iteration 9/1000 | Loss: 0.00023082
Iteration 10/1000 | Loss: 0.00031590
Iteration 11/1000 | Loss: 0.00014536
Iteration 12/1000 | Loss: 0.00020025
Iteration 13/1000 | Loss: 0.00061056
Iteration 14/1000 | Loss: 0.00078136
Iteration 15/1000 | Loss: 0.00072258
Iteration 16/1000 | Loss: 0.00069223
Iteration 17/1000 | Loss: 0.00028186
Iteration 18/1000 | Loss: 0.00028482
Iteration 19/1000 | Loss: 0.00019857
Iteration 20/1000 | Loss: 0.00018739
Iteration 21/1000 | Loss: 0.00032652
Iteration 22/1000 | Loss: 0.00020199
Iteration 23/1000 | Loss: 0.00024280
Iteration 24/1000 | Loss: 0.00053303
Iteration 25/1000 | Loss: 0.00061791
Iteration 26/1000 | Loss: 0.00020901
Iteration 27/1000 | Loss: 0.00116687
Iteration 28/1000 | Loss: 0.00041325
Iteration 29/1000 | Loss: 0.00012265
Iteration 30/1000 | Loss: 0.00028612
Iteration 31/1000 | Loss: 0.00011703
Iteration 32/1000 | Loss: 0.00011476
Iteration 33/1000 | Loss: 0.00010722
Iteration 34/1000 | Loss: 0.00012312
Iteration 35/1000 | Loss: 0.00009420
Iteration 36/1000 | Loss: 0.00026594
Iteration 37/1000 | Loss: 0.00023036
Iteration 38/1000 | Loss: 0.00012923
Iteration 39/1000 | Loss: 0.00030752
Iteration 40/1000 | Loss: 0.00011020
Iteration 41/1000 | Loss: 0.00010822
Iteration 42/1000 | Loss: 0.00018724
Iteration 43/1000 | Loss: 0.00044554
Iteration 44/1000 | Loss: 0.00012739
Iteration 45/1000 | Loss: 0.00018428
Iteration 46/1000 | Loss: 0.00012118
Iteration 47/1000 | Loss: 0.00013930
Iteration 48/1000 | Loss: 0.00023390
Iteration 49/1000 | Loss: 0.00044287
Iteration 50/1000 | Loss: 0.00031415
Iteration 51/1000 | Loss: 0.00072448
Iteration 52/1000 | Loss: 0.00035878
Iteration 53/1000 | Loss: 0.00033759
Iteration 54/1000 | Loss: 0.00010377
Iteration 55/1000 | Loss: 0.00013017
Iteration 56/1000 | Loss: 0.00008807
Iteration 57/1000 | Loss: 0.00010998
Iteration 58/1000 | Loss: 0.00067071
Iteration 59/1000 | Loss: 0.00036039
Iteration 60/1000 | Loss: 0.00027571
Iteration 61/1000 | Loss: 0.00009945
Iteration 62/1000 | Loss: 0.00008764
Iteration 63/1000 | Loss: 0.00018212
Iteration 64/1000 | Loss: 0.00018720
Iteration 65/1000 | Loss: 0.00011115
Iteration 66/1000 | Loss: 0.00009930
Iteration 67/1000 | Loss: 0.00010088
Iteration 68/1000 | Loss: 0.00029429
Iteration 69/1000 | Loss: 0.00021329
Iteration 70/1000 | Loss: 0.00077348
Iteration 71/1000 | Loss: 0.00012529
Iteration 72/1000 | Loss: 0.00014151
Iteration 73/1000 | Loss: 0.00009546
Iteration 74/1000 | Loss: 0.00010951
Iteration 75/1000 | Loss: 0.00008345
Iteration 76/1000 | Loss: 0.00009279
Iteration 77/1000 | Loss: 0.00015713
Iteration 78/1000 | Loss: 0.00011566
Iteration 79/1000 | Loss: 0.00008700
Iteration 80/1000 | Loss: 0.00009642
Iteration 81/1000 | Loss: 0.00012247
Iteration 82/1000 | Loss: 0.00023148
Iteration 83/1000 | Loss: 0.00014533
Iteration 84/1000 | Loss: 0.00031592
Iteration 85/1000 | Loss: 0.00043825
Iteration 86/1000 | Loss: 0.00128369
Iteration 87/1000 | Loss: 0.00014211
Iteration 88/1000 | Loss: 0.00021239
Iteration 89/1000 | Loss: 0.00009088
Iteration 90/1000 | Loss: 0.00009316
Iteration 91/1000 | Loss: 0.00009583
Iteration 92/1000 | Loss: 0.00008478
Iteration 93/1000 | Loss: 0.00036163
Iteration 94/1000 | Loss: 0.00022784
Iteration 95/1000 | Loss: 0.00030557
Iteration 96/1000 | Loss: 0.00061558
Iteration 97/1000 | Loss: 0.00045634
Iteration 98/1000 | Loss: 0.00090053
Iteration 99/1000 | Loss: 0.00051487
Iteration 100/1000 | Loss: 0.00054526
Iteration 101/1000 | Loss: 0.00021375
Iteration 102/1000 | Loss: 0.00022234
Iteration 103/1000 | Loss: 0.00036908
Iteration 104/1000 | Loss: 0.00023951
Iteration 105/1000 | Loss: 0.00022351
Iteration 106/1000 | Loss: 0.00044607
Iteration 107/1000 | Loss: 0.00060523
Iteration 108/1000 | Loss: 0.00053431
Iteration 109/1000 | Loss: 0.00006891
Iteration 110/1000 | Loss: 0.00028263
Iteration 111/1000 | Loss: 0.00040266
Iteration 112/1000 | Loss: 0.00016357
Iteration 113/1000 | Loss: 0.00007225
Iteration 114/1000 | Loss: 0.00005577
Iteration 115/1000 | Loss: 0.00044199
Iteration 116/1000 | Loss: 0.00014006
Iteration 117/1000 | Loss: 0.00016944
Iteration 118/1000 | Loss: 0.00010387
Iteration 119/1000 | Loss: 0.00009968
Iteration 120/1000 | Loss: 0.00006031
Iteration 121/1000 | Loss: 0.00005006
Iteration 122/1000 | Loss: 0.00017032
Iteration 123/1000 | Loss: 0.00004676
Iteration 124/1000 | Loss: 0.00004499
Iteration 125/1000 | Loss: 0.00004790
Iteration 126/1000 | Loss: 0.00006091
Iteration 127/1000 | Loss: 0.00005187
Iteration 128/1000 | Loss: 0.00004999
Iteration 129/1000 | Loss: 0.00005335
Iteration 130/1000 | Loss: 0.00005137
Iteration 131/1000 | Loss: 0.00003925
Iteration 132/1000 | Loss: 0.00006234
Iteration 133/1000 | Loss: 0.00008407
Iteration 134/1000 | Loss: 0.00012166
Iteration 135/1000 | Loss: 0.00006338
Iteration 136/1000 | Loss: 0.00005044
Iteration 137/1000 | Loss: 0.00005346
Iteration 138/1000 | Loss: 0.00004857
Iteration 139/1000 | Loss: 0.00004670
Iteration 140/1000 | Loss: 0.00004542
Iteration 141/1000 | Loss: 0.00005203
Iteration 142/1000 | Loss: 0.00004397
Iteration 143/1000 | Loss: 0.00005469
Iteration 144/1000 | Loss: 0.00003739
Iteration 145/1000 | Loss: 0.00003786
Iteration 146/1000 | Loss: 0.00004772
Iteration 147/1000 | Loss: 0.00004706
Iteration 148/1000 | Loss: 0.00004695
Iteration 149/1000 | Loss: 0.00004276
Iteration 150/1000 | Loss: 0.00005585
Iteration 151/1000 | Loss: 0.00003589
Iteration 152/1000 | Loss: 0.00004602
Iteration 153/1000 | Loss: 0.00004496
Iteration 154/1000 | Loss: 0.00003245
Iteration 155/1000 | Loss: 0.00004004
Iteration 156/1000 | Loss: 0.00004648
Iteration 157/1000 | Loss: 0.00007417
Iteration 158/1000 | Loss: 0.00004121
Iteration 159/1000 | Loss: 0.00004583
Iteration 160/1000 | Loss: 0.00005403
Iteration 161/1000 | Loss: 0.00004781
Iteration 162/1000 | Loss: 0.00004627
Iteration 163/1000 | Loss: 0.00004663
Iteration 164/1000 | Loss: 0.00004682
Iteration 165/1000 | Loss: 0.00006992
Iteration 166/1000 | Loss: 0.00004884
Iteration 167/1000 | Loss: 0.00005118
Iteration 168/1000 | Loss: 0.00008062
Iteration 169/1000 | Loss: 0.00004515
Iteration 170/1000 | Loss: 0.00004629
Iteration 171/1000 | Loss: 0.00004684
Iteration 172/1000 | Loss: 0.00004665
Iteration 173/1000 | Loss: 0.00003803
Iteration 174/1000 | Loss: 0.00003874
Iteration 175/1000 | Loss: 0.00003778
Iteration 176/1000 | Loss: 0.00008327
Iteration 177/1000 | Loss: 0.00004679
Iteration 178/1000 | Loss: 0.00004645
Iteration 179/1000 | Loss: 0.00003906
Iteration 180/1000 | Loss: 0.00003970
Iteration 181/1000 | Loss: 0.00015446
Iteration 182/1000 | Loss: 0.00005322
Iteration 183/1000 | Loss: 0.00004265
Iteration 184/1000 | Loss: 0.00004654
Iteration 185/1000 | Loss: 0.00004450
Iteration 186/1000 | Loss: 0.00006022
Iteration 187/1000 | Loss: 0.00004443
Iteration 188/1000 | Loss: 0.00004668
Iteration 189/1000 | Loss: 0.00004816
Iteration 190/1000 | Loss: 0.00009561
Iteration 191/1000 | Loss: 0.00005524
Iteration 192/1000 | Loss: 0.00004856
Iteration 193/1000 | Loss: 0.00004678
Iteration 194/1000 | Loss: 0.00004037
Iteration 195/1000 | Loss: 0.00004725
Iteration 196/1000 | Loss: 0.00003991
Iteration 197/1000 | Loss: 0.00004602
Iteration 198/1000 | Loss: 0.00003911
Iteration 199/1000 | Loss: 0.00005007
Iteration 200/1000 | Loss: 0.00003937
Iteration 201/1000 | Loss: 0.00004624
Iteration 202/1000 | Loss: 0.00021794
Iteration 203/1000 | Loss: 0.00005768
Iteration 204/1000 | Loss: 0.00004594
Iteration 205/1000 | Loss: 0.00004104
Iteration 206/1000 | Loss: 0.00007402
Iteration 207/1000 | Loss: 0.00004955
Iteration 208/1000 | Loss: 0.00006604
Iteration 209/1000 | Loss: 0.00004995
Iteration 210/1000 | Loss: 0.00004983
Iteration 211/1000 | Loss: 0.00004538
Iteration 212/1000 | Loss: 0.00006678
Iteration 213/1000 | Loss: 0.00004535
Iteration 214/1000 | Loss: 0.00005699
Iteration 215/1000 | Loss: 0.00004664
Iteration 216/1000 | Loss: 0.00004946
Iteration 217/1000 | Loss: 0.00004659
Iteration 218/1000 | Loss: 0.00004949
Iteration 219/1000 | Loss: 0.00004640
Iteration 220/1000 | Loss: 0.00004811
Iteration 221/1000 | Loss: 0.00003656
Iteration 222/1000 | Loss: 0.00004160
Iteration 223/1000 | Loss: 0.00003605
Iteration 224/1000 | Loss: 0.00004243
Iteration 225/1000 | Loss: 0.00003657
Iteration 226/1000 | Loss: 0.00004365
Iteration 227/1000 | Loss: 0.00003662
Iteration 228/1000 | Loss: 0.00004240
Iteration 229/1000 | Loss: 0.00003637
Iteration 230/1000 | Loss: 0.00004403
Iteration 231/1000 | Loss: 0.00004051
Iteration 232/1000 | Loss: 0.00004449
Iteration 233/1000 | Loss: 0.00004984
Iteration 234/1000 | Loss: 0.00004335
Iteration 235/1000 | Loss: 0.00003675
Iteration 236/1000 | Loss: 0.00004118
Iteration 237/1000 | Loss: 0.00003756
Iteration 238/1000 | Loss: 0.00004052
Iteration 239/1000 | Loss: 0.00004593
Iteration 240/1000 | Loss: 0.00004545
Iteration 241/1000 | Loss: 0.00003983
Iteration 242/1000 | Loss: 0.00004370
Iteration 243/1000 | Loss: 0.00008163
Iteration 244/1000 | Loss: 0.00003714
Iteration 245/1000 | Loss: 0.00005524
Iteration 246/1000 | Loss: 0.00003803
Iteration 247/1000 | Loss: 0.00003875
Iteration 248/1000 | Loss: 0.00003759
Iteration 249/1000 | Loss: 0.00003799
Iteration 250/1000 | Loss: 0.00005119
Iteration 251/1000 | Loss: 0.00003839
Iteration 252/1000 | Loss: 0.00006721
Iteration 253/1000 | Loss: 0.00003945
Iteration 254/1000 | Loss: 0.00003430
Iteration 255/1000 | Loss: 0.00006271
Iteration 256/1000 | Loss: 0.00003173
Iteration 257/1000 | Loss: 0.00006882
Iteration 258/1000 | Loss: 0.00003107
Iteration 259/1000 | Loss: 0.00003072
Iteration 260/1000 | Loss: 0.00003039
Iteration 261/1000 | Loss: 0.00003039
Iteration 262/1000 | Loss: 0.00003005
Iteration 263/1000 | Loss: 0.00004575
Iteration 264/1000 | Loss: 0.00003077
Iteration 265/1000 | Loss: 0.00002999
Iteration 266/1000 | Loss: 0.00018478
Iteration 267/1000 | Loss: 0.00003589
Iteration 268/1000 | Loss: 0.00002940
Iteration 269/1000 | Loss: 0.00005017
Iteration 270/1000 | Loss: 0.00003183
Iteration 271/1000 | Loss: 0.00004232
Iteration 272/1000 | Loss: 0.00002958
Iteration 273/1000 | Loss: 0.00002926
Iteration 274/1000 | Loss: 0.00003683
Iteration 275/1000 | Loss: 0.00003819
Iteration 276/1000 | Loss: 0.00003774
Iteration 277/1000 | Loss: 0.00004597
Iteration 278/1000 | Loss: 0.00003582
Iteration 279/1000 | Loss: 0.00002943
Iteration 280/1000 | Loss: 0.00002930
Iteration 281/1000 | Loss: 0.00002926
Iteration 282/1000 | Loss: 0.00002922
Iteration 283/1000 | Loss: 0.00002921
Iteration 284/1000 | Loss: 0.00002897
Iteration 285/1000 | Loss: 0.00002889
Iteration 286/1000 | Loss: 0.00002886
Iteration 287/1000 | Loss: 0.00002884
Iteration 288/1000 | Loss: 0.00020093
Iteration 289/1000 | Loss: 0.00003013
Iteration 290/1000 | Loss: 0.00002866
Iteration 291/1000 | Loss: 0.00002844
Iteration 292/1000 | Loss: 0.00002842
Iteration 293/1000 | Loss: 0.00002840
Iteration 294/1000 | Loss: 0.00002840
Iteration 295/1000 | Loss: 0.00002839
Iteration 296/1000 | Loss: 0.00002838
Iteration 297/1000 | Loss: 0.00002837
Iteration 298/1000 | Loss: 0.00002835
Iteration 299/1000 | Loss: 0.00002834
Iteration 300/1000 | Loss: 0.00002834
Iteration 301/1000 | Loss: 0.00002833
Iteration 302/1000 | Loss: 0.00002831
Iteration 303/1000 | Loss: 0.00002828
Iteration 304/1000 | Loss: 0.00002828
Iteration 305/1000 | Loss: 0.00002828
Iteration 306/1000 | Loss: 0.00002828
Iteration 307/1000 | Loss: 0.00002828
Iteration 308/1000 | Loss: 0.00002828
Iteration 309/1000 | Loss: 0.00002828
Iteration 310/1000 | Loss: 0.00002828
Iteration 311/1000 | Loss: 0.00002827
Iteration 312/1000 | Loss: 0.00002827
Iteration 313/1000 | Loss: 0.00002826
Iteration 314/1000 | Loss: 0.00002826
Iteration 315/1000 | Loss: 0.00002823
Iteration 316/1000 | Loss: 0.00002823
Iteration 317/1000 | Loss: 0.00002822
Iteration 318/1000 | Loss: 0.00002822
Iteration 319/1000 | Loss: 0.00002822
Iteration 320/1000 | Loss: 0.00002821
Iteration 321/1000 | Loss: 0.00002821
Iteration 322/1000 | Loss: 0.00002821
Iteration 323/1000 | Loss: 0.00002821
Iteration 324/1000 | Loss: 0.00002821
Iteration 325/1000 | Loss: 0.00002821
Iteration 326/1000 | Loss: 0.00002820
Iteration 327/1000 | Loss: 0.00002820
Iteration 328/1000 | Loss: 0.00009712
Iteration 329/1000 | Loss: 0.00003123
Iteration 330/1000 | Loss: 0.00004206
Iteration 331/1000 | Loss: 0.00002836
Iteration 332/1000 | Loss: 0.00002818
Iteration 333/1000 | Loss: 0.00002817
Iteration 334/1000 | Loss: 0.00002817
Iteration 335/1000 | Loss: 0.00002815
Iteration 336/1000 | Loss: 0.00002815
Iteration 337/1000 | Loss: 0.00002815
Iteration 338/1000 | Loss: 0.00002814
Iteration 339/1000 | Loss: 0.00002814
Iteration 340/1000 | Loss: 0.00002814
Iteration 341/1000 | Loss: 0.00002813
Iteration 342/1000 | Loss: 0.00002813
Iteration 343/1000 | Loss: 0.00002811
Iteration 344/1000 | Loss: 0.00002811
Iteration 345/1000 | Loss: 0.00002810
Iteration 346/1000 | Loss: 0.00002809
Iteration 347/1000 | Loss: 0.00002809
Iteration 348/1000 | Loss: 0.00002808
Iteration 349/1000 | Loss: 0.00002808
Iteration 350/1000 | Loss: 0.00002806
Iteration 351/1000 | Loss: 0.00002806
Iteration 352/1000 | Loss: 0.00002806
Iteration 353/1000 | Loss: 0.00002806
Iteration 354/1000 | Loss: 0.00002806
Iteration 355/1000 | Loss: 0.00002805
Iteration 356/1000 | Loss: 0.00002805
Iteration 357/1000 | Loss: 0.00002805
Iteration 358/1000 | Loss: 0.00002803
Iteration 359/1000 | Loss: 0.00002803
Iteration 360/1000 | Loss: 0.00002800
Iteration 361/1000 | Loss: 0.00002800
Iteration 362/1000 | Loss: 0.00002799
Iteration 363/1000 | Loss: 0.00002799
Iteration 364/1000 | Loss: 0.00002799
Iteration 365/1000 | Loss: 0.00002798
Iteration 366/1000 | Loss: 0.00002798
Iteration 367/1000 | Loss: 0.00002798
Iteration 368/1000 | Loss: 0.00002798
Iteration 369/1000 | Loss: 0.00002798
Iteration 370/1000 | Loss: 0.00002798
Iteration 371/1000 | Loss: 0.00002797
Iteration 372/1000 | Loss: 0.00002797
Iteration 373/1000 | Loss: 0.00002797
Iteration 374/1000 | Loss: 0.00002797
Iteration 375/1000 | Loss: 0.00002797
Iteration 376/1000 | Loss: 0.00002797
Iteration 377/1000 | Loss: 0.00002797
Iteration 378/1000 | Loss: 0.00002797
Iteration 379/1000 | Loss: 0.00002797
Iteration 380/1000 | Loss: 0.00002797
Iteration 381/1000 | Loss: 0.00002796
Iteration 382/1000 | Loss: 0.00002796
Iteration 383/1000 | Loss: 0.00002796
Iteration 384/1000 | Loss: 0.00002796
Iteration 385/1000 | Loss: 0.00002796
Iteration 386/1000 | Loss: 0.00002796
Iteration 387/1000 | Loss: 0.00002796
Iteration 388/1000 | Loss: 0.00002796
Iteration 389/1000 | Loss: 0.00002796
Iteration 390/1000 | Loss: 0.00002796
Iteration 391/1000 | Loss: 0.00002795
Iteration 392/1000 | Loss: 0.00002795
Iteration 393/1000 | Loss: 0.00002795
Iteration 394/1000 | Loss: 0.00002795
Iteration 395/1000 | Loss: 0.00002795
Iteration 396/1000 | Loss: 0.00002795
Iteration 397/1000 | Loss: 0.00002795
Iteration 398/1000 | Loss: 0.00002795
Iteration 399/1000 | Loss: 0.00002795
Iteration 400/1000 | Loss: 0.00002795
Iteration 401/1000 | Loss: 0.00002795
Iteration 402/1000 | Loss: 0.00002795
Iteration 403/1000 | Loss: 0.00002795
Iteration 404/1000 | Loss: 0.00002794
Iteration 405/1000 | Loss: 0.00002794
Iteration 406/1000 | Loss: 0.00002794
Iteration 407/1000 | Loss: 0.00002794
Iteration 408/1000 | Loss: 0.00002794
Iteration 409/1000 | Loss: 0.00002794
Iteration 410/1000 | Loss: 0.00002794
Iteration 411/1000 | Loss: 0.00002794
Iteration 412/1000 | Loss: 0.00002794
Iteration 413/1000 | Loss: 0.00002794
Iteration 414/1000 | Loss: 0.00002794
Iteration 415/1000 | Loss: 0.00002794
Iteration 416/1000 | Loss: 0.00002794
Iteration 417/1000 | Loss: 0.00002794
Iteration 418/1000 | Loss: 0.00002793
Iteration 419/1000 | Loss: 0.00002793
Iteration 420/1000 | Loss: 0.00002793
Iteration 421/1000 | Loss: 0.00002793
Iteration 422/1000 | Loss: 0.00002793
Iteration 423/1000 | Loss: 0.00002793
Iteration 424/1000 | Loss: 0.00002793
Iteration 425/1000 | Loss: 0.00002793
Iteration 426/1000 | Loss: 0.00002793
Iteration 427/1000 | Loss: 0.00002793
Iteration 428/1000 | Loss: 0.00002793
Iteration 429/1000 | Loss: 0.00002793
Iteration 430/1000 | Loss: 0.00002793
Iteration 431/1000 | Loss: 0.00002793
Iteration 432/1000 | Loss: 0.00002793
Iteration 433/1000 | Loss: 0.00002793
Iteration 434/1000 | Loss: 0.00002793
Iteration 435/1000 | Loss: 0.00002792
Iteration 436/1000 | Loss: 0.00002792
Iteration 437/1000 | Loss: 0.00002792
Iteration 438/1000 | Loss: 0.00002792
Iteration 439/1000 | Loss: 0.00002792
Iteration 440/1000 | Loss: 0.00002792
Iteration 441/1000 | Loss: 0.00002792
Iteration 442/1000 | Loss: 0.00002792
Iteration 443/1000 | Loss: 0.00002792
Iteration 444/1000 | Loss: 0.00002792
Iteration 445/1000 | Loss: 0.00002792
Iteration 446/1000 | Loss: 0.00002792
Iteration 447/1000 | Loss: 0.00002792
Iteration 448/1000 | Loss: 0.00002792
Iteration 449/1000 | Loss: 0.00002792
Iteration 450/1000 | Loss: 0.00002792
Iteration 451/1000 | Loss: 0.00002792
Iteration 452/1000 | Loss: 0.00002792
Iteration 453/1000 | Loss: 0.00002792
Iteration 454/1000 | Loss: 0.00002792
Iteration 455/1000 | Loss: 0.00002792
Iteration 456/1000 | Loss: 0.00002792
Iteration 457/1000 | Loss: 0.00002791
Iteration 458/1000 | Loss: 0.00002791
Iteration 459/1000 | Loss: 0.00002791
Iteration 460/1000 | Loss: 0.00002791
Iteration 461/1000 | Loss: 0.00002791
Iteration 462/1000 | Loss: 0.00002791
Iteration 463/1000 | Loss: 0.00002791
Iteration 464/1000 | Loss: 0.00002791
Iteration 465/1000 | Loss: 0.00002791
Iteration 466/1000 | Loss: 0.00002791
Iteration 467/1000 | Loss: 0.00002791
Iteration 468/1000 | Loss: 0.00002791
Iteration 469/1000 | Loss: 0.00002791
Iteration 470/1000 | Loss: 0.00002791
Iteration 471/1000 | Loss: 0.00002791
Iteration 472/1000 | Loss: 0.00002791
Iteration 473/1000 | Loss: 0.00002791
Iteration 474/1000 | Loss: 0.00002791
Iteration 475/1000 | Loss: 0.00002791
Iteration 476/1000 | Loss: 0.00002791
Iteration 477/1000 | Loss: 0.00002791
Iteration 478/1000 | Loss: 0.00002791
Iteration 479/1000 | Loss: 0.00002791
Iteration 480/1000 | Loss: 0.00002791
Iteration 481/1000 | Loss: 0.00002791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 481. Stopping optimization.
Last 5 losses: [2.7914191377931274e-05, 2.7914191377931274e-05, 2.7914191377931274e-05, 2.7914191377931274e-05, 2.7914191377931274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7914191377931274e-05

Optimization complete. Final v2v error: 3.5150983333587646 mm

Highest mean error: 10.972121238708496 mm for frame 119

Lowest mean error: 2.66908597946167 mm for frame 22

Saving results

Total time: 530.2656111717224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919004
Iteration 2/25 | Loss: 0.00174863
Iteration 3/25 | Loss: 0.00150515
Iteration 4/25 | Loss: 0.00147091
Iteration 5/25 | Loss: 0.00146489
Iteration 6/25 | Loss: 0.00146456
Iteration 7/25 | Loss: 0.00146457
Iteration 8/25 | Loss: 0.00146457
Iteration 9/25 | Loss: 0.00146457
Iteration 10/25 | Loss: 0.00146457
Iteration 11/25 | Loss: 0.00146457
Iteration 12/25 | Loss: 0.00146457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0014645650517195463, 0.0014645650517195463, 0.0014645650517195463, 0.0014645650517195463, 0.0014645650517195463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014645650517195463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74884754
Iteration 2/25 | Loss: 0.00079049
Iteration 3/25 | Loss: 0.00079048
Iteration 4/25 | Loss: 0.00079048
Iteration 5/25 | Loss: 0.00079048
Iteration 6/25 | Loss: 0.00079048
Iteration 7/25 | Loss: 0.00079048
Iteration 8/25 | Loss: 0.00079048
Iteration 9/25 | Loss: 0.00079048
Iteration 10/25 | Loss: 0.00079048
Iteration 11/25 | Loss: 0.00079048
Iteration 12/25 | Loss: 0.00079048
Iteration 13/25 | Loss: 0.00079048
Iteration 14/25 | Loss: 0.00079048
Iteration 15/25 | Loss: 0.00079048
Iteration 16/25 | Loss: 0.00079048
Iteration 17/25 | Loss: 0.00079048
Iteration 18/25 | Loss: 0.00079048
Iteration 19/25 | Loss: 0.00079048
Iteration 20/25 | Loss: 0.00079048
Iteration 21/25 | Loss: 0.00079048
Iteration 22/25 | Loss: 0.00079048
Iteration 23/25 | Loss: 0.00079048
Iteration 24/25 | Loss: 0.00079048
Iteration 25/25 | Loss: 0.00079048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079048
Iteration 2/1000 | Loss: 0.00005851
Iteration 3/1000 | Loss: 0.00004256
Iteration 4/1000 | Loss: 0.00003821
Iteration 5/1000 | Loss: 0.00003690
Iteration 6/1000 | Loss: 0.00003611
Iteration 7/1000 | Loss: 0.00003559
Iteration 8/1000 | Loss: 0.00003520
Iteration 9/1000 | Loss: 0.00003496
Iteration 10/1000 | Loss: 0.00003477
Iteration 11/1000 | Loss: 0.00003469
Iteration 12/1000 | Loss: 0.00003456
Iteration 13/1000 | Loss: 0.00003439
Iteration 14/1000 | Loss: 0.00003424
Iteration 15/1000 | Loss: 0.00003410
Iteration 16/1000 | Loss: 0.00003403
Iteration 17/1000 | Loss: 0.00003400
Iteration 18/1000 | Loss: 0.00003395
Iteration 19/1000 | Loss: 0.00003391
Iteration 20/1000 | Loss: 0.00003391
Iteration 21/1000 | Loss: 0.00003391
Iteration 22/1000 | Loss: 0.00003391
Iteration 23/1000 | Loss: 0.00003391
Iteration 24/1000 | Loss: 0.00003391
Iteration 25/1000 | Loss: 0.00003391
Iteration 26/1000 | Loss: 0.00003391
Iteration 27/1000 | Loss: 0.00003391
Iteration 28/1000 | Loss: 0.00003391
Iteration 29/1000 | Loss: 0.00003390
Iteration 30/1000 | Loss: 0.00003389
Iteration 31/1000 | Loss: 0.00003388
Iteration 32/1000 | Loss: 0.00003385
Iteration 33/1000 | Loss: 0.00003383
Iteration 34/1000 | Loss: 0.00003380
Iteration 35/1000 | Loss: 0.00003376
Iteration 36/1000 | Loss: 0.00003376
Iteration 37/1000 | Loss: 0.00003375
Iteration 38/1000 | Loss: 0.00003375
Iteration 39/1000 | Loss: 0.00003373
Iteration 40/1000 | Loss: 0.00003373
Iteration 41/1000 | Loss: 0.00003373
Iteration 42/1000 | Loss: 0.00003373
Iteration 43/1000 | Loss: 0.00003372
Iteration 44/1000 | Loss: 0.00003371
Iteration 45/1000 | Loss: 0.00003371
Iteration 46/1000 | Loss: 0.00003371
Iteration 47/1000 | Loss: 0.00003370
Iteration 48/1000 | Loss: 0.00003370
Iteration 49/1000 | Loss: 0.00003370
Iteration 50/1000 | Loss: 0.00003369
Iteration 51/1000 | Loss: 0.00003369
Iteration 52/1000 | Loss: 0.00003369
Iteration 53/1000 | Loss: 0.00003369
Iteration 54/1000 | Loss: 0.00003368
Iteration 55/1000 | Loss: 0.00003368
Iteration 56/1000 | Loss: 0.00003368
Iteration 57/1000 | Loss: 0.00003367
Iteration 58/1000 | Loss: 0.00003367
Iteration 59/1000 | Loss: 0.00003367
Iteration 60/1000 | Loss: 0.00003367
Iteration 61/1000 | Loss: 0.00003367
Iteration 62/1000 | Loss: 0.00003367
Iteration 63/1000 | Loss: 0.00003367
Iteration 64/1000 | Loss: 0.00003367
Iteration 65/1000 | Loss: 0.00003366
Iteration 66/1000 | Loss: 0.00003366
Iteration 67/1000 | Loss: 0.00003366
Iteration 68/1000 | Loss: 0.00003365
Iteration 69/1000 | Loss: 0.00003365
Iteration 70/1000 | Loss: 0.00003365
Iteration 71/1000 | Loss: 0.00003365
Iteration 72/1000 | Loss: 0.00003365
Iteration 73/1000 | Loss: 0.00003365
Iteration 74/1000 | Loss: 0.00003365
Iteration 75/1000 | Loss: 0.00003365
Iteration 76/1000 | Loss: 0.00003365
Iteration 77/1000 | Loss: 0.00003365
Iteration 78/1000 | Loss: 0.00003365
Iteration 79/1000 | Loss: 0.00003365
Iteration 80/1000 | Loss: 0.00003365
Iteration 81/1000 | Loss: 0.00003365
Iteration 82/1000 | Loss: 0.00003365
Iteration 83/1000 | Loss: 0.00003365
Iteration 84/1000 | Loss: 0.00003364
Iteration 85/1000 | Loss: 0.00003364
Iteration 86/1000 | Loss: 0.00003364
Iteration 87/1000 | Loss: 0.00003364
Iteration 88/1000 | Loss: 0.00003363
Iteration 89/1000 | Loss: 0.00003363
Iteration 90/1000 | Loss: 0.00003363
Iteration 91/1000 | Loss: 0.00003363
Iteration 92/1000 | Loss: 0.00003363
Iteration 93/1000 | Loss: 0.00003363
Iteration 94/1000 | Loss: 0.00003363
Iteration 95/1000 | Loss: 0.00003362
Iteration 96/1000 | Loss: 0.00003362
Iteration 97/1000 | Loss: 0.00003362
Iteration 98/1000 | Loss: 0.00003362
Iteration 99/1000 | Loss: 0.00003362
Iteration 100/1000 | Loss: 0.00003362
Iteration 101/1000 | Loss: 0.00003362
Iteration 102/1000 | Loss: 0.00003362
Iteration 103/1000 | Loss: 0.00003362
Iteration 104/1000 | Loss: 0.00003362
Iteration 105/1000 | Loss: 0.00003362
Iteration 106/1000 | Loss: 0.00003361
Iteration 107/1000 | Loss: 0.00003361
Iteration 108/1000 | Loss: 0.00003361
Iteration 109/1000 | Loss: 0.00003361
Iteration 110/1000 | Loss: 0.00003361
Iteration 111/1000 | Loss: 0.00003360
Iteration 112/1000 | Loss: 0.00003360
Iteration 113/1000 | Loss: 0.00003360
Iteration 114/1000 | Loss: 0.00003360
Iteration 115/1000 | Loss: 0.00003360
Iteration 116/1000 | Loss: 0.00003360
Iteration 117/1000 | Loss: 0.00003360
Iteration 118/1000 | Loss: 0.00003360
Iteration 119/1000 | Loss: 0.00003359
Iteration 120/1000 | Loss: 0.00003359
Iteration 121/1000 | Loss: 0.00003359
Iteration 122/1000 | Loss: 0.00003359
Iteration 123/1000 | Loss: 0.00003359
Iteration 124/1000 | Loss: 0.00003359
Iteration 125/1000 | Loss: 0.00003359
Iteration 126/1000 | Loss: 0.00003359
Iteration 127/1000 | Loss: 0.00003358
Iteration 128/1000 | Loss: 0.00003358
Iteration 129/1000 | Loss: 0.00003358
Iteration 130/1000 | Loss: 0.00003358
Iteration 131/1000 | Loss: 0.00003358
Iteration 132/1000 | Loss: 0.00003358
Iteration 133/1000 | Loss: 0.00003358
Iteration 134/1000 | Loss: 0.00003358
Iteration 135/1000 | Loss: 0.00003357
Iteration 136/1000 | Loss: 0.00003357
Iteration 137/1000 | Loss: 0.00003357
Iteration 138/1000 | Loss: 0.00003357
Iteration 139/1000 | Loss: 0.00003357
Iteration 140/1000 | Loss: 0.00003357
Iteration 141/1000 | Loss: 0.00003357
Iteration 142/1000 | Loss: 0.00003357
Iteration 143/1000 | Loss: 0.00003357
Iteration 144/1000 | Loss: 0.00003357
Iteration 145/1000 | Loss: 0.00003356
Iteration 146/1000 | Loss: 0.00003356
Iteration 147/1000 | Loss: 0.00003356
Iteration 148/1000 | Loss: 0.00003356
Iteration 149/1000 | Loss: 0.00003356
Iteration 150/1000 | Loss: 0.00003356
Iteration 151/1000 | Loss: 0.00003356
Iteration 152/1000 | Loss: 0.00003356
Iteration 153/1000 | Loss: 0.00003356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.356382512720302e-05, 3.356382512720302e-05, 3.356382512720302e-05, 3.356382512720302e-05, 3.356382512720302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.356382512720302e-05

Optimization complete. Final v2v error: 4.770351886749268 mm

Highest mean error: 5.016007423400879 mm for frame 34

Lowest mean error: 4.384895324707031 mm for frame 0

Saving results

Total time: 40.9621148109436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755180
Iteration 2/25 | Loss: 0.00149205
Iteration 3/25 | Loss: 0.00130983
Iteration 4/25 | Loss: 0.00128815
Iteration 5/25 | Loss: 0.00128395
Iteration 6/25 | Loss: 0.00128336
Iteration 7/25 | Loss: 0.00128336
Iteration 8/25 | Loss: 0.00128336
Iteration 9/25 | Loss: 0.00128336
Iteration 10/25 | Loss: 0.00128336
Iteration 11/25 | Loss: 0.00128332
Iteration 12/25 | Loss: 0.00128332
Iteration 13/25 | Loss: 0.00128332
Iteration 14/25 | Loss: 0.00128332
Iteration 15/25 | Loss: 0.00128332
Iteration 16/25 | Loss: 0.00128332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001283318386413157, 0.001283318386413157, 0.001283318386413157, 0.001283318386413157, 0.001283318386413157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001283318386413157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07055473
Iteration 2/25 | Loss: 0.00099310
Iteration 3/25 | Loss: 0.00099309
Iteration 4/25 | Loss: 0.00099309
Iteration 5/25 | Loss: 0.00099309
Iteration 6/25 | Loss: 0.00099309
Iteration 7/25 | Loss: 0.00099309
Iteration 8/25 | Loss: 0.00099309
Iteration 9/25 | Loss: 0.00099309
Iteration 10/25 | Loss: 0.00099309
Iteration 11/25 | Loss: 0.00099309
Iteration 12/25 | Loss: 0.00099309
Iteration 13/25 | Loss: 0.00099309
Iteration 14/25 | Loss: 0.00099309
Iteration 15/25 | Loss: 0.00099309
Iteration 16/25 | Loss: 0.00099309
Iteration 17/25 | Loss: 0.00099309
Iteration 18/25 | Loss: 0.00099309
Iteration 19/25 | Loss: 0.00099309
Iteration 20/25 | Loss: 0.00099309
Iteration 21/25 | Loss: 0.00099309
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009930895175784826, 0.0009930895175784826, 0.0009930895175784826, 0.0009930895175784826, 0.0009930895175784826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009930895175784826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099309
Iteration 2/1000 | Loss: 0.00004472
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002293
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001944
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001892
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00001834
Iteration 17/1000 | Loss: 0.00001827
Iteration 18/1000 | Loss: 0.00001825
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001799
Iteration 23/1000 | Loss: 0.00001799
Iteration 24/1000 | Loss: 0.00001798
Iteration 25/1000 | Loss: 0.00001798
Iteration 26/1000 | Loss: 0.00001797
Iteration 27/1000 | Loss: 0.00001796
Iteration 28/1000 | Loss: 0.00001796
Iteration 29/1000 | Loss: 0.00001795
Iteration 30/1000 | Loss: 0.00001793
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Iteration 38/1000 | Loss: 0.00001786
Iteration 39/1000 | Loss: 0.00001786
Iteration 40/1000 | Loss: 0.00001785
Iteration 41/1000 | Loss: 0.00001783
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001778
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001777
Iteration 58/1000 | Loss: 0.00001776
Iteration 59/1000 | Loss: 0.00001776
Iteration 60/1000 | Loss: 0.00001775
Iteration 61/1000 | Loss: 0.00001775
Iteration 62/1000 | Loss: 0.00001775
Iteration 63/1000 | Loss: 0.00001775
Iteration 64/1000 | Loss: 0.00001775
Iteration 65/1000 | Loss: 0.00001775
Iteration 66/1000 | Loss: 0.00001775
Iteration 67/1000 | Loss: 0.00001775
Iteration 68/1000 | Loss: 0.00001775
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001774
Iteration 72/1000 | Loss: 0.00001774
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001773
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001772
Iteration 79/1000 | Loss: 0.00001772
Iteration 80/1000 | Loss: 0.00001772
Iteration 81/1000 | Loss: 0.00001772
Iteration 82/1000 | Loss: 0.00001772
Iteration 83/1000 | Loss: 0.00001771
Iteration 84/1000 | Loss: 0.00001771
Iteration 85/1000 | Loss: 0.00001771
Iteration 86/1000 | Loss: 0.00001771
Iteration 87/1000 | Loss: 0.00001771
Iteration 88/1000 | Loss: 0.00001770
Iteration 89/1000 | Loss: 0.00001770
Iteration 90/1000 | Loss: 0.00001770
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001769
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Iteration 108/1000 | Loss: 0.00001768
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001767
Iteration 112/1000 | Loss: 0.00001767
Iteration 113/1000 | Loss: 0.00001767
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001766
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001766
Iteration 119/1000 | Loss: 0.00001765
Iteration 120/1000 | Loss: 0.00001765
Iteration 121/1000 | Loss: 0.00001765
Iteration 122/1000 | Loss: 0.00001765
Iteration 123/1000 | Loss: 0.00001765
Iteration 124/1000 | Loss: 0.00001765
Iteration 125/1000 | Loss: 0.00001764
Iteration 126/1000 | Loss: 0.00001764
Iteration 127/1000 | Loss: 0.00001764
Iteration 128/1000 | Loss: 0.00001764
Iteration 129/1000 | Loss: 0.00001764
Iteration 130/1000 | Loss: 0.00001764
Iteration 131/1000 | Loss: 0.00001764
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001764
Iteration 134/1000 | Loss: 0.00001763
Iteration 135/1000 | Loss: 0.00001763
Iteration 136/1000 | Loss: 0.00001763
Iteration 137/1000 | Loss: 0.00001763
Iteration 138/1000 | Loss: 0.00001763
Iteration 139/1000 | Loss: 0.00001763
Iteration 140/1000 | Loss: 0.00001763
Iteration 141/1000 | Loss: 0.00001763
Iteration 142/1000 | Loss: 0.00001763
Iteration 143/1000 | Loss: 0.00001762
Iteration 144/1000 | Loss: 0.00001762
Iteration 145/1000 | Loss: 0.00001762
Iteration 146/1000 | Loss: 0.00001762
Iteration 147/1000 | Loss: 0.00001762
Iteration 148/1000 | Loss: 0.00001761
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001760
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001760
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001759
Iteration 157/1000 | Loss: 0.00001759
Iteration 158/1000 | Loss: 0.00001758
Iteration 159/1000 | Loss: 0.00001758
Iteration 160/1000 | Loss: 0.00001758
Iteration 161/1000 | Loss: 0.00001758
Iteration 162/1000 | Loss: 0.00001758
Iteration 163/1000 | Loss: 0.00001758
Iteration 164/1000 | Loss: 0.00001757
Iteration 165/1000 | Loss: 0.00001757
Iteration 166/1000 | Loss: 0.00001757
Iteration 167/1000 | Loss: 0.00001757
Iteration 168/1000 | Loss: 0.00001757
Iteration 169/1000 | Loss: 0.00001757
Iteration 170/1000 | Loss: 0.00001756
Iteration 171/1000 | Loss: 0.00001756
Iteration 172/1000 | Loss: 0.00001756
Iteration 173/1000 | Loss: 0.00001756
Iteration 174/1000 | Loss: 0.00001756
Iteration 175/1000 | Loss: 0.00001756
Iteration 176/1000 | Loss: 0.00001756
Iteration 177/1000 | Loss: 0.00001755
Iteration 178/1000 | Loss: 0.00001755
Iteration 179/1000 | Loss: 0.00001755
Iteration 180/1000 | Loss: 0.00001755
Iteration 181/1000 | Loss: 0.00001755
Iteration 182/1000 | Loss: 0.00001755
Iteration 183/1000 | Loss: 0.00001755
Iteration 184/1000 | Loss: 0.00001755
Iteration 185/1000 | Loss: 0.00001755
Iteration 186/1000 | Loss: 0.00001755
Iteration 187/1000 | Loss: 0.00001755
Iteration 188/1000 | Loss: 0.00001755
Iteration 189/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.7546522940392606e-05, 1.7546522940392606e-05, 1.7546522940392606e-05, 1.7546522940392606e-05, 1.7546522940392606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7546522940392606e-05

Optimization complete. Final v2v error: 3.4470267295837402 mm

Highest mean error: 4.911323547363281 mm for frame 161

Lowest mean error: 2.788890838623047 mm for frame 189

Saving results

Total time: 45.79530096054077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813928
Iteration 2/25 | Loss: 0.00135735
Iteration 3/25 | Loss: 0.00125288
Iteration 4/25 | Loss: 0.00123279
Iteration 5/25 | Loss: 0.00122785
Iteration 6/25 | Loss: 0.00122480
Iteration 7/25 | Loss: 0.00122314
Iteration 8/25 | Loss: 0.00122268
Iteration 9/25 | Loss: 0.00122248
Iteration 10/25 | Loss: 0.00122319
Iteration 11/25 | Loss: 0.00122256
Iteration 12/25 | Loss: 0.00122167
Iteration 13/25 | Loss: 0.00122142
Iteration 14/25 | Loss: 0.00122133
Iteration 15/25 | Loss: 0.00122133
Iteration 16/25 | Loss: 0.00122133
Iteration 17/25 | Loss: 0.00122132
Iteration 18/25 | Loss: 0.00122132
Iteration 19/25 | Loss: 0.00122132
Iteration 20/25 | Loss: 0.00122132
Iteration 21/25 | Loss: 0.00122132
Iteration 22/25 | Loss: 0.00122132
Iteration 23/25 | Loss: 0.00122132
Iteration 24/25 | Loss: 0.00122132
Iteration 25/25 | Loss: 0.00122132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.62169075
Iteration 2/25 | Loss: 0.00088125
Iteration 3/25 | Loss: 0.00088124
Iteration 4/25 | Loss: 0.00088124
Iteration 5/25 | Loss: 0.00088124
Iteration 6/25 | Loss: 0.00088124
Iteration 7/25 | Loss: 0.00088124
Iteration 8/25 | Loss: 0.00088124
Iteration 9/25 | Loss: 0.00088124
Iteration 10/25 | Loss: 0.00088124
Iteration 11/25 | Loss: 0.00088124
Iteration 12/25 | Loss: 0.00088123
Iteration 13/25 | Loss: 0.00088123
Iteration 14/25 | Loss: 0.00088123
Iteration 15/25 | Loss: 0.00088123
Iteration 16/25 | Loss: 0.00088123
Iteration 17/25 | Loss: 0.00088123
Iteration 18/25 | Loss: 0.00088123
Iteration 19/25 | Loss: 0.00088123
Iteration 20/25 | Loss: 0.00088123
Iteration 21/25 | Loss: 0.00088123
Iteration 22/25 | Loss: 0.00088123
Iteration 23/25 | Loss: 0.00088123
Iteration 24/25 | Loss: 0.00088123
Iteration 25/25 | Loss: 0.00088123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088123
Iteration 2/1000 | Loss: 0.00002610
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001819
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001531
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001518
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001504
Iteration 21/1000 | Loss: 0.00001503
Iteration 22/1000 | Loss: 0.00001502
Iteration 23/1000 | Loss: 0.00001501
Iteration 24/1000 | Loss: 0.00001501
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001494
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001491
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001487
Iteration 35/1000 | Loss: 0.00001486
Iteration 36/1000 | Loss: 0.00001486
Iteration 37/1000 | Loss: 0.00001485
Iteration 38/1000 | Loss: 0.00001485
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001484
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001482
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001476
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001474
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.4744042346137576e-05, 1.4744042346137576e-05, 1.4744042346137576e-05, 1.4744042346137576e-05, 1.4744042346137576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4744042346137576e-05

Optimization complete. Final v2v error: 3.26106595993042 mm

Highest mean error: 3.667691469192505 mm for frame 204

Lowest mean error: 2.968933343887329 mm for frame 107

Saving results

Total time: 54.37794589996338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_020/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_020/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532387
Iteration 2/25 | Loss: 0.00128079
Iteration 3/25 | Loss: 0.00122143
Iteration 4/25 | Loss: 0.00121282
Iteration 5/25 | Loss: 0.00120996
Iteration 6/25 | Loss: 0.00120978
Iteration 7/25 | Loss: 0.00120978
Iteration 8/25 | Loss: 0.00120978
Iteration 9/25 | Loss: 0.00120978
Iteration 10/25 | Loss: 0.00120978
Iteration 11/25 | Loss: 0.00120978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001209780341014266, 0.001209780341014266, 0.001209780341014266, 0.001209780341014266, 0.001209780341014266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209780341014266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.61148691
Iteration 2/25 | Loss: 0.00093432
Iteration 3/25 | Loss: 0.00093431
Iteration 4/25 | Loss: 0.00093431
Iteration 5/25 | Loss: 0.00093431
Iteration 6/25 | Loss: 0.00093430
Iteration 7/25 | Loss: 0.00093430
Iteration 8/25 | Loss: 0.00093430
Iteration 9/25 | Loss: 0.00093430
Iteration 10/25 | Loss: 0.00093430
Iteration 11/25 | Loss: 0.00093430
Iteration 12/25 | Loss: 0.00093430
Iteration 13/25 | Loss: 0.00093430
Iteration 14/25 | Loss: 0.00093430
Iteration 15/25 | Loss: 0.00093430
Iteration 16/25 | Loss: 0.00093430
Iteration 17/25 | Loss: 0.00093430
Iteration 18/25 | Loss: 0.00093430
Iteration 19/25 | Loss: 0.00093430
Iteration 20/25 | Loss: 0.00093430
Iteration 21/25 | Loss: 0.00093430
Iteration 22/25 | Loss: 0.00093430
Iteration 23/25 | Loss: 0.00093430
Iteration 24/25 | Loss: 0.00093430
Iteration 25/25 | Loss: 0.00093430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093430
Iteration 2/1000 | Loss: 0.00002653
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001573
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001403
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001338
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001315
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001298
Iteration 18/1000 | Loss: 0.00001297
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001289
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001286
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001285
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001284
Iteration 29/1000 | Loss: 0.00001284
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001283
Iteration 32/1000 | Loss: 0.00001280
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001279
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001270
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001260
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001257
Iteration 46/1000 | Loss: 0.00001257
Iteration 47/1000 | Loss: 0.00001256
Iteration 48/1000 | Loss: 0.00001256
Iteration 49/1000 | Loss: 0.00001256
Iteration 50/1000 | Loss: 0.00001256
Iteration 51/1000 | Loss: 0.00001256
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001253
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001250
Iteration 59/1000 | Loss: 0.00001250
Iteration 60/1000 | Loss: 0.00001250
Iteration 61/1000 | Loss: 0.00001249
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001248
Iteration 64/1000 | Loss: 0.00001248
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001245
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001244
Iteration 78/1000 | Loss: 0.00001244
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001237
Iteration 102/1000 | Loss: 0.00001237
Iteration 103/1000 | Loss: 0.00001237
Iteration 104/1000 | Loss: 0.00001237
Iteration 105/1000 | Loss: 0.00001237
Iteration 106/1000 | Loss: 0.00001237
Iteration 107/1000 | Loss: 0.00001237
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001236
Iteration 112/1000 | Loss: 0.00001236
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001235
Iteration 117/1000 | Loss: 0.00001235
Iteration 118/1000 | Loss: 0.00001235
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001234
Iteration 123/1000 | Loss: 0.00001234
Iteration 124/1000 | Loss: 0.00001234
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001233
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001231
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Iteration 145/1000 | Loss: 0.00001230
Iteration 146/1000 | Loss: 0.00001230
Iteration 147/1000 | Loss: 0.00001230
Iteration 148/1000 | Loss: 0.00001229
Iteration 149/1000 | Loss: 0.00001229
Iteration 150/1000 | Loss: 0.00001229
Iteration 151/1000 | Loss: 0.00001229
Iteration 152/1000 | Loss: 0.00001229
Iteration 153/1000 | Loss: 0.00001229
Iteration 154/1000 | Loss: 0.00001229
Iteration 155/1000 | Loss: 0.00001229
Iteration 156/1000 | Loss: 0.00001229
Iteration 157/1000 | Loss: 0.00001229
Iteration 158/1000 | Loss: 0.00001229
Iteration 159/1000 | Loss: 0.00001229
Iteration 160/1000 | Loss: 0.00001229
Iteration 161/1000 | Loss: 0.00001229
Iteration 162/1000 | Loss: 0.00001229
Iteration 163/1000 | Loss: 0.00001229
Iteration 164/1000 | Loss: 0.00001229
Iteration 165/1000 | Loss: 0.00001229
Iteration 166/1000 | Loss: 0.00001229
Iteration 167/1000 | Loss: 0.00001229
Iteration 168/1000 | Loss: 0.00001228
Iteration 169/1000 | Loss: 0.00001228
Iteration 170/1000 | Loss: 0.00001228
Iteration 171/1000 | Loss: 0.00001228
Iteration 172/1000 | Loss: 0.00001228
Iteration 173/1000 | Loss: 0.00001228
Iteration 174/1000 | Loss: 0.00001228
Iteration 175/1000 | Loss: 0.00001228
Iteration 176/1000 | Loss: 0.00001228
Iteration 177/1000 | Loss: 0.00001228
Iteration 178/1000 | Loss: 0.00001228
Iteration 179/1000 | Loss: 0.00001228
Iteration 180/1000 | Loss: 0.00001228
Iteration 181/1000 | Loss: 0.00001228
Iteration 182/1000 | Loss: 0.00001228
Iteration 183/1000 | Loss: 0.00001228
Iteration 184/1000 | Loss: 0.00001228
Iteration 185/1000 | Loss: 0.00001228
Iteration 186/1000 | Loss: 0.00001228
Iteration 187/1000 | Loss: 0.00001228
Iteration 188/1000 | Loss: 0.00001228
Iteration 189/1000 | Loss: 0.00001228
Iteration 190/1000 | Loss: 0.00001228
Iteration 191/1000 | Loss: 0.00001228
Iteration 192/1000 | Loss: 0.00001228
Iteration 193/1000 | Loss: 0.00001228
Iteration 194/1000 | Loss: 0.00001228
Iteration 195/1000 | Loss: 0.00001228
Iteration 196/1000 | Loss: 0.00001228
Iteration 197/1000 | Loss: 0.00001228
Iteration 198/1000 | Loss: 0.00001228
Iteration 199/1000 | Loss: 0.00001228
Iteration 200/1000 | Loss: 0.00001228
Iteration 201/1000 | Loss: 0.00001228
Iteration 202/1000 | Loss: 0.00001228
Iteration 203/1000 | Loss: 0.00001228
Iteration 204/1000 | Loss: 0.00001228
Iteration 205/1000 | Loss: 0.00001228
Iteration 206/1000 | Loss: 0.00001228
Iteration 207/1000 | Loss: 0.00001228
Iteration 208/1000 | Loss: 0.00001228
Iteration 209/1000 | Loss: 0.00001228
Iteration 210/1000 | Loss: 0.00001228
Iteration 211/1000 | Loss: 0.00001228
Iteration 212/1000 | Loss: 0.00001228
Iteration 213/1000 | Loss: 0.00001228
Iteration 214/1000 | Loss: 0.00001228
Iteration 215/1000 | Loss: 0.00001228
Iteration 216/1000 | Loss: 0.00001228
Iteration 217/1000 | Loss: 0.00001228
Iteration 218/1000 | Loss: 0.00001228
Iteration 219/1000 | Loss: 0.00001228
Iteration 220/1000 | Loss: 0.00001228
Iteration 221/1000 | Loss: 0.00001228
Iteration 222/1000 | Loss: 0.00001228
Iteration 223/1000 | Loss: 0.00001228
Iteration 224/1000 | Loss: 0.00001228
Iteration 225/1000 | Loss: 0.00001228
Iteration 226/1000 | Loss: 0.00001228
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001228
Iteration 229/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.2276375855435617e-05, 1.2276375855435617e-05, 1.2276375855435617e-05, 1.2276375855435617e-05, 1.2276375855435617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2276375855435617e-05

Optimization complete. Final v2v error: 2.9891557693481445 mm

Highest mean error: 3.67262864112854 mm for frame 66

Lowest mean error: 2.721067190170288 mm for frame 172

Saving results

Total time: 44.381972312927246
