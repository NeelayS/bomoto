Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=206, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11536-11591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958589
Iteration 2/25 | Loss: 0.00177200
Iteration 3/25 | Loss: 0.00154259
Iteration 4/25 | Loss: 0.00152838
Iteration 5/25 | Loss: 0.00152383
Iteration 6/25 | Loss: 0.00152378
Iteration 7/25 | Loss: 0.00152378
Iteration 8/25 | Loss: 0.00152378
Iteration 9/25 | Loss: 0.00152378
Iteration 10/25 | Loss: 0.00152378
Iteration 11/25 | Loss: 0.00152378
Iteration 12/25 | Loss: 0.00152378
Iteration 13/25 | Loss: 0.00152378
Iteration 14/25 | Loss: 0.00152378
Iteration 15/25 | Loss: 0.00152378
Iteration 16/25 | Loss: 0.00152378
Iteration 17/25 | Loss: 0.00152378
Iteration 18/25 | Loss: 0.00152378
Iteration 19/25 | Loss: 0.00152378
Iteration 20/25 | Loss: 0.00152378
Iteration 21/25 | Loss: 0.00152378
Iteration 22/25 | Loss: 0.00152378
Iteration 23/25 | Loss: 0.00152378
Iteration 24/25 | Loss: 0.00152378
Iteration 25/25 | Loss: 0.00152378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54100585
Iteration 2/25 | Loss: 0.00100185
Iteration 3/25 | Loss: 0.00100185
Iteration 4/25 | Loss: 0.00100185
Iteration 5/25 | Loss: 0.00100185
Iteration 6/25 | Loss: 0.00100185
Iteration 7/25 | Loss: 0.00100185
Iteration 8/25 | Loss: 0.00100185
Iteration 9/25 | Loss: 0.00100185
Iteration 10/25 | Loss: 0.00100185
Iteration 11/25 | Loss: 0.00100185
Iteration 12/25 | Loss: 0.00100185
Iteration 13/25 | Loss: 0.00100185
Iteration 14/25 | Loss: 0.00100185
Iteration 15/25 | Loss: 0.00100185
Iteration 16/25 | Loss: 0.00100185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001001847442239523, 0.001001847442239523, 0.001001847442239523, 0.001001847442239523, 0.001001847442239523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001001847442239523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100185
Iteration 2/1000 | Loss: 0.00006929
Iteration 3/1000 | Loss: 0.00004922
Iteration 4/1000 | Loss: 0.00004352
Iteration 5/1000 | Loss: 0.00004151
Iteration 6/1000 | Loss: 0.00003983
Iteration 7/1000 | Loss: 0.00003899
Iteration 8/1000 | Loss: 0.00003829
Iteration 9/1000 | Loss: 0.00003775
Iteration 10/1000 | Loss: 0.00003743
Iteration 11/1000 | Loss: 0.00003703
Iteration 12/1000 | Loss: 0.00003674
Iteration 13/1000 | Loss: 0.00003646
Iteration 14/1000 | Loss: 0.00003610
Iteration 15/1000 | Loss: 0.00003582
Iteration 16/1000 | Loss: 0.00003552
Iteration 17/1000 | Loss: 0.00003518
Iteration 18/1000 | Loss: 0.00003494
Iteration 19/1000 | Loss: 0.00003479
Iteration 20/1000 | Loss: 0.00003472
Iteration 21/1000 | Loss: 0.00003468
Iteration 22/1000 | Loss: 0.00003462
Iteration 23/1000 | Loss: 0.00003452
Iteration 24/1000 | Loss: 0.00003452
Iteration 25/1000 | Loss: 0.00003449
Iteration 26/1000 | Loss: 0.00003449
Iteration 27/1000 | Loss: 0.00003448
Iteration 28/1000 | Loss: 0.00003448
Iteration 29/1000 | Loss: 0.00003448
Iteration 30/1000 | Loss: 0.00003448
Iteration 31/1000 | Loss: 0.00003447
Iteration 32/1000 | Loss: 0.00003445
Iteration 33/1000 | Loss: 0.00003444
Iteration 34/1000 | Loss: 0.00003444
Iteration 35/1000 | Loss: 0.00003443
Iteration 36/1000 | Loss: 0.00003443
Iteration 37/1000 | Loss: 0.00003442
Iteration 38/1000 | Loss: 0.00003442
Iteration 39/1000 | Loss: 0.00003442
Iteration 40/1000 | Loss: 0.00003441
Iteration 41/1000 | Loss: 0.00003440
Iteration 42/1000 | Loss: 0.00003440
Iteration 43/1000 | Loss: 0.00003440
Iteration 44/1000 | Loss: 0.00003439
Iteration 45/1000 | Loss: 0.00003439
Iteration 46/1000 | Loss: 0.00003439
Iteration 47/1000 | Loss: 0.00003439
Iteration 48/1000 | Loss: 0.00003438
Iteration 49/1000 | Loss: 0.00003438
Iteration 50/1000 | Loss: 0.00003438
Iteration 51/1000 | Loss: 0.00003438
Iteration 52/1000 | Loss: 0.00003438
Iteration 53/1000 | Loss: 0.00003438
Iteration 54/1000 | Loss: 0.00003438
Iteration 55/1000 | Loss: 0.00003437
Iteration 56/1000 | Loss: 0.00003437
Iteration 57/1000 | Loss: 0.00003437
Iteration 58/1000 | Loss: 0.00003437
Iteration 59/1000 | Loss: 0.00003437
Iteration 60/1000 | Loss: 0.00003437
Iteration 61/1000 | Loss: 0.00003437
Iteration 62/1000 | Loss: 0.00003436
Iteration 63/1000 | Loss: 0.00003436
Iteration 64/1000 | Loss: 0.00003436
Iteration 65/1000 | Loss: 0.00003436
Iteration 66/1000 | Loss: 0.00003436
Iteration 67/1000 | Loss: 0.00003436
Iteration 68/1000 | Loss: 0.00003436
Iteration 69/1000 | Loss: 0.00003436
Iteration 70/1000 | Loss: 0.00003436
Iteration 71/1000 | Loss: 0.00003436
Iteration 72/1000 | Loss: 0.00003436
Iteration 73/1000 | Loss: 0.00003436
Iteration 74/1000 | Loss: 0.00003436
Iteration 75/1000 | Loss: 0.00003436
Iteration 76/1000 | Loss: 0.00003436
Iteration 77/1000 | Loss: 0.00003436
Iteration 78/1000 | Loss: 0.00003436
Iteration 79/1000 | Loss: 0.00003436
Iteration 80/1000 | Loss: 0.00003436
Iteration 81/1000 | Loss: 0.00003436
Iteration 82/1000 | Loss: 0.00003436
Iteration 83/1000 | Loss: 0.00003436
Iteration 84/1000 | Loss: 0.00003436
Iteration 85/1000 | Loss: 0.00003436
Iteration 86/1000 | Loss: 0.00003436
Iteration 87/1000 | Loss: 0.00003436
Iteration 88/1000 | Loss: 0.00003436
Iteration 89/1000 | Loss: 0.00003436
Iteration 90/1000 | Loss: 0.00003436
Iteration 91/1000 | Loss: 0.00003436
Iteration 92/1000 | Loss: 0.00003436
Iteration 93/1000 | Loss: 0.00003436
Iteration 94/1000 | Loss: 0.00003436
Iteration 95/1000 | Loss: 0.00003436
Iteration 96/1000 | Loss: 0.00003436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [3.435804683249444e-05, 3.435804683249444e-05, 3.435804683249444e-05, 3.435804683249444e-05, 3.435804683249444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.435804683249444e-05

Optimization complete. Final v2v error: 4.899433135986328 mm

Highest mean error: 5.7254862785339355 mm for frame 6

Lowest mean error: 4.567712306976318 mm for frame 60

Saving results

Total time: 46.04347777366638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790596
Iteration 2/25 | Loss: 0.00157555
Iteration 3/25 | Loss: 0.00142526
Iteration 4/25 | Loss: 0.00140156
Iteration 5/25 | Loss: 0.00139288
Iteration 6/25 | Loss: 0.00139030
Iteration 7/25 | Loss: 0.00138996
Iteration 8/25 | Loss: 0.00138996
Iteration 9/25 | Loss: 0.00138996
Iteration 10/25 | Loss: 0.00138996
Iteration 11/25 | Loss: 0.00138996
Iteration 12/25 | Loss: 0.00138996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013899566838517785, 0.0013899566838517785, 0.0013899566838517785, 0.0013899566838517785, 0.0013899566838517785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013899566838517785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11950994
Iteration 2/25 | Loss: 0.00113283
Iteration 3/25 | Loss: 0.00113277
Iteration 4/25 | Loss: 0.00113277
Iteration 5/25 | Loss: 0.00113277
Iteration 6/25 | Loss: 0.00113277
Iteration 7/25 | Loss: 0.00113277
Iteration 8/25 | Loss: 0.00113277
Iteration 9/25 | Loss: 0.00113276
Iteration 10/25 | Loss: 0.00113276
Iteration 11/25 | Loss: 0.00113276
Iteration 12/25 | Loss: 0.00113276
Iteration 13/25 | Loss: 0.00113276
Iteration 14/25 | Loss: 0.00113276
Iteration 15/25 | Loss: 0.00113276
Iteration 16/25 | Loss: 0.00113276
Iteration 17/25 | Loss: 0.00113276
Iteration 18/25 | Loss: 0.00113276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001132764620706439, 0.001132764620706439, 0.001132764620706439, 0.001132764620706439, 0.001132764620706439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132764620706439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113276
Iteration 2/1000 | Loss: 0.00007089
Iteration 3/1000 | Loss: 0.00003605
Iteration 4/1000 | Loss: 0.00002963
Iteration 5/1000 | Loss: 0.00002724
Iteration 6/1000 | Loss: 0.00002565
Iteration 7/1000 | Loss: 0.00002483
Iteration 8/1000 | Loss: 0.00002395
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002314
Iteration 11/1000 | Loss: 0.00002284
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002252
Iteration 14/1000 | Loss: 0.00002250
Iteration 15/1000 | Loss: 0.00002245
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002239
Iteration 18/1000 | Loss: 0.00002235
Iteration 19/1000 | Loss: 0.00002227
Iteration 20/1000 | Loss: 0.00002225
Iteration 21/1000 | Loss: 0.00002222
Iteration 22/1000 | Loss: 0.00002219
Iteration 23/1000 | Loss: 0.00002219
Iteration 24/1000 | Loss: 0.00002218
Iteration 25/1000 | Loss: 0.00002218
Iteration 26/1000 | Loss: 0.00002218
Iteration 27/1000 | Loss: 0.00002218
Iteration 28/1000 | Loss: 0.00002217
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002216
Iteration 32/1000 | Loss: 0.00002216
Iteration 33/1000 | Loss: 0.00002215
Iteration 34/1000 | Loss: 0.00002215
Iteration 35/1000 | Loss: 0.00002214
Iteration 36/1000 | Loss: 0.00002214
Iteration 37/1000 | Loss: 0.00002213
Iteration 38/1000 | Loss: 0.00002213
Iteration 39/1000 | Loss: 0.00002212
Iteration 40/1000 | Loss: 0.00002212
Iteration 41/1000 | Loss: 0.00002210
Iteration 42/1000 | Loss: 0.00002210
Iteration 43/1000 | Loss: 0.00002210
Iteration 44/1000 | Loss: 0.00002209
Iteration 45/1000 | Loss: 0.00002209
Iteration 46/1000 | Loss: 0.00002208
Iteration 47/1000 | Loss: 0.00002208
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002207
Iteration 51/1000 | Loss: 0.00002207
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002206
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002205
Iteration 59/1000 | Loss: 0.00002205
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00002203
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002203
Iteration 69/1000 | Loss: 0.00002202
Iteration 70/1000 | Loss: 0.00002202
Iteration 71/1000 | Loss: 0.00002202
Iteration 72/1000 | Loss: 0.00002202
Iteration 73/1000 | Loss: 0.00002202
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002201
Iteration 76/1000 | Loss: 0.00002201
Iteration 77/1000 | Loss: 0.00002201
Iteration 78/1000 | Loss: 0.00002201
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002198
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002197
Iteration 96/1000 | Loss: 0.00002197
Iteration 97/1000 | Loss: 0.00002197
Iteration 98/1000 | Loss: 0.00002197
Iteration 99/1000 | Loss: 0.00002197
Iteration 100/1000 | Loss: 0.00002197
Iteration 101/1000 | Loss: 0.00002196
Iteration 102/1000 | Loss: 0.00002196
Iteration 103/1000 | Loss: 0.00002196
Iteration 104/1000 | Loss: 0.00002196
Iteration 105/1000 | Loss: 0.00002196
Iteration 106/1000 | Loss: 0.00002196
Iteration 107/1000 | Loss: 0.00002196
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002196
Iteration 111/1000 | Loss: 0.00002196
Iteration 112/1000 | Loss: 0.00002196
Iteration 113/1000 | Loss: 0.00002196
Iteration 114/1000 | Loss: 0.00002195
Iteration 115/1000 | Loss: 0.00002195
Iteration 116/1000 | Loss: 0.00002195
Iteration 117/1000 | Loss: 0.00002195
Iteration 118/1000 | Loss: 0.00002195
Iteration 119/1000 | Loss: 0.00002195
Iteration 120/1000 | Loss: 0.00002195
Iteration 121/1000 | Loss: 0.00002195
Iteration 122/1000 | Loss: 0.00002195
Iteration 123/1000 | Loss: 0.00002195
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002194
Iteration 126/1000 | Loss: 0.00002194
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002194
Iteration 130/1000 | Loss: 0.00002194
Iteration 131/1000 | Loss: 0.00002194
Iteration 132/1000 | Loss: 0.00002194
Iteration 133/1000 | Loss: 0.00002194
Iteration 134/1000 | Loss: 0.00002194
Iteration 135/1000 | Loss: 0.00002193
Iteration 136/1000 | Loss: 0.00002193
Iteration 137/1000 | Loss: 0.00002193
Iteration 138/1000 | Loss: 0.00002193
Iteration 139/1000 | Loss: 0.00002193
Iteration 140/1000 | Loss: 0.00002193
Iteration 141/1000 | Loss: 0.00002192
Iteration 142/1000 | Loss: 0.00002192
Iteration 143/1000 | Loss: 0.00002192
Iteration 144/1000 | Loss: 0.00002192
Iteration 145/1000 | Loss: 0.00002192
Iteration 146/1000 | Loss: 0.00002192
Iteration 147/1000 | Loss: 0.00002192
Iteration 148/1000 | Loss: 0.00002192
Iteration 149/1000 | Loss: 0.00002192
Iteration 150/1000 | Loss: 0.00002192
Iteration 151/1000 | Loss: 0.00002192
Iteration 152/1000 | Loss: 0.00002192
Iteration 153/1000 | Loss: 0.00002191
Iteration 154/1000 | Loss: 0.00002191
Iteration 155/1000 | Loss: 0.00002191
Iteration 156/1000 | Loss: 0.00002191
Iteration 157/1000 | Loss: 0.00002190
Iteration 158/1000 | Loss: 0.00002190
Iteration 159/1000 | Loss: 0.00002190
Iteration 160/1000 | Loss: 0.00002190
Iteration 161/1000 | Loss: 0.00002190
Iteration 162/1000 | Loss: 0.00002190
Iteration 163/1000 | Loss: 0.00002190
Iteration 164/1000 | Loss: 0.00002190
Iteration 165/1000 | Loss: 0.00002190
Iteration 166/1000 | Loss: 0.00002189
Iteration 167/1000 | Loss: 0.00002189
Iteration 168/1000 | Loss: 0.00002189
Iteration 169/1000 | Loss: 0.00002189
Iteration 170/1000 | Loss: 0.00002189
Iteration 171/1000 | Loss: 0.00002189
Iteration 172/1000 | Loss: 0.00002189
Iteration 173/1000 | Loss: 0.00002188
Iteration 174/1000 | Loss: 0.00002188
Iteration 175/1000 | Loss: 0.00002188
Iteration 176/1000 | Loss: 0.00002188
Iteration 177/1000 | Loss: 0.00002188
Iteration 178/1000 | Loss: 0.00002188
Iteration 179/1000 | Loss: 0.00002188
Iteration 180/1000 | Loss: 0.00002188
Iteration 181/1000 | Loss: 0.00002188
Iteration 182/1000 | Loss: 0.00002188
Iteration 183/1000 | Loss: 0.00002188
Iteration 184/1000 | Loss: 0.00002188
Iteration 185/1000 | Loss: 0.00002188
Iteration 186/1000 | Loss: 0.00002188
Iteration 187/1000 | Loss: 0.00002188
Iteration 188/1000 | Loss: 0.00002187
Iteration 189/1000 | Loss: 0.00002187
Iteration 190/1000 | Loss: 0.00002187
Iteration 191/1000 | Loss: 0.00002187
Iteration 192/1000 | Loss: 0.00002187
Iteration 193/1000 | Loss: 0.00002186
Iteration 194/1000 | Loss: 0.00002186
Iteration 195/1000 | Loss: 0.00002186
Iteration 196/1000 | Loss: 0.00002186
Iteration 197/1000 | Loss: 0.00002186
Iteration 198/1000 | Loss: 0.00002186
Iteration 199/1000 | Loss: 0.00002186
Iteration 200/1000 | Loss: 0.00002186
Iteration 201/1000 | Loss: 0.00002186
Iteration 202/1000 | Loss: 0.00002186
Iteration 203/1000 | Loss: 0.00002186
Iteration 204/1000 | Loss: 0.00002186
Iteration 205/1000 | Loss: 0.00002186
Iteration 206/1000 | Loss: 0.00002186
Iteration 207/1000 | Loss: 0.00002186
Iteration 208/1000 | Loss: 0.00002185
Iteration 209/1000 | Loss: 0.00002185
Iteration 210/1000 | Loss: 0.00002185
Iteration 211/1000 | Loss: 0.00002185
Iteration 212/1000 | Loss: 0.00002185
Iteration 213/1000 | Loss: 0.00002185
Iteration 214/1000 | Loss: 0.00002185
Iteration 215/1000 | Loss: 0.00002185
Iteration 216/1000 | Loss: 0.00002185
Iteration 217/1000 | Loss: 0.00002185
Iteration 218/1000 | Loss: 0.00002185
Iteration 219/1000 | Loss: 0.00002185
Iteration 220/1000 | Loss: 0.00002185
Iteration 221/1000 | Loss: 0.00002185
Iteration 222/1000 | Loss: 0.00002185
Iteration 223/1000 | Loss: 0.00002185
Iteration 224/1000 | Loss: 0.00002185
Iteration 225/1000 | Loss: 0.00002185
Iteration 226/1000 | Loss: 0.00002185
Iteration 227/1000 | Loss: 0.00002185
Iteration 228/1000 | Loss: 0.00002184
Iteration 229/1000 | Loss: 0.00002184
Iteration 230/1000 | Loss: 0.00002184
Iteration 231/1000 | Loss: 0.00002184
Iteration 232/1000 | Loss: 0.00002184
Iteration 233/1000 | Loss: 0.00002184
Iteration 234/1000 | Loss: 0.00002184
Iteration 235/1000 | Loss: 0.00002184
Iteration 236/1000 | Loss: 0.00002184
Iteration 237/1000 | Loss: 0.00002184
Iteration 238/1000 | Loss: 0.00002184
Iteration 239/1000 | Loss: 0.00002184
Iteration 240/1000 | Loss: 0.00002184
Iteration 241/1000 | Loss: 0.00002184
Iteration 242/1000 | Loss: 0.00002184
Iteration 243/1000 | Loss: 0.00002184
Iteration 244/1000 | Loss: 0.00002183
Iteration 245/1000 | Loss: 0.00002183
Iteration 246/1000 | Loss: 0.00002183
Iteration 247/1000 | Loss: 0.00002183
Iteration 248/1000 | Loss: 0.00002183
Iteration 249/1000 | Loss: 0.00002183
Iteration 250/1000 | Loss: 0.00002183
Iteration 251/1000 | Loss: 0.00002183
Iteration 252/1000 | Loss: 0.00002183
Iteration 253/1000 | Loss: 0.00002183
Iteration 254/1000 | Loss: 0.00002183
Iteration 255/1000 | Loss: 0.00002183
Iteration 256/1000 | Loss: 0.00002183
Iteration 257/1000 | Loss: 0.00002183
Iteration 258/1000 | Loss: 0.00002183
Iteration 259/1000 | Loss: 0.00002183
Iteration 260/1000 | Loss: 0.00002183
Iteration 261/1000 | Loss: 0.00002183
Iteration 262/1000 | Loss: 0.00002183
Iteration 263/1000 | Loss: 0.00002183
Iteration 264/1000 | Loss: 0.00002183
Iteration 265/1000 | Loss: 0.00002183
Iteration 266/1000 | Loss: 0.00002183
Iteration 267/1000 | Loss: 0.00002183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.183330070693046e-05, 2.183330070693046e-05, 2.183330070693046e-05, 2.183330070693046e-05, 2.183330070693046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.183330070693046e-05

Optimization complete. Final v2v error: 3.962146043777466 mm

Highest mean error: 4.578952789306641 mm for frame 108

Lowest mean error: 3.443552017211914 mm for frame 214

Saving results

Total time: 55.456912994384766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979604
Iteration 2/25 | Loss: 0.00150624
Iteration 3/25 | Loss: 0.00138298
Iteration 4/25 | Loss: 0.00135638
Iteration 5/25 | Loss: 0.00135134
Iteration 6/25 | Loss: 0.00134480
Iteration 7/25 | Loss: 0.00134514
Iteration 8/25 | Loss: 0.00134118
Iteration 9/25 | Loss: 0.00134266
Iteration 10/25 | Loss: 0.00134077
Iteration 11/25 | Loss: 0.00134074
Iteration 12/25 | Loss: 0.00134073
Iteration 13/25 | Loss: 0.00134073
Iteration 14/25 | Loss: 0.00134073
Iteration 15/25 | Loss: 0.00134073
Iteration 16/25 | Loss: 0.00134073
Iteration 17/25 | Loss: 0.00134073
Iteration 18/25 | Loss: 0.00134072
Iteration 19/25 | Loss: 0.00134072
Iteration 20/25 | Loss: 0.00134072
Iteration 21/25 | Loss: 0.00134072
Iteration 22/25 | Loss: 0.00134072
Iteration 23/25 | Loss: 0.00134072
Iteration 24/25 | Loss: 0.00134072
Iteration 25/25 | Loss: 0.00134072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.13229036
Iteration 2/25 | Loss: 0.00110107
Iteration 3/25 | Loss: 0.00108320
Iteration 4/25 | Loss: 0.00108320
Iteration 5/25 | Loss: 0.00108319
Iteration 6/25 | Loss: 0.00108319
Iteration 7/25 | Loss: 0.00108319
Iteration 8/25 | Loss: 0.00108319
Iteration 9/25 | Loss: 0.00108319
Iteration 10/25 | Loss: 0.00108319
Iteration 11/25 | Loss: 0.00108319
Iteration 12/25 | Loss: 0.00108319
Iteration 13/25 | Loss: 0.00108319
Iteration 14/25 | Loss: 0.00108319
Iteration 15/25 | Loss: 0.00108319
Iteration 16/25 | Loss: 0.00108319
Iteration 17/25 | Loss: 0.00108319
Iteration 18/25 | Loss: 0.00108319
Iteration 19/25 | Loss: 0.00108319
Iteration 20/25 | Loss: 0.00108319
Iteration 21/25 | Loss: 0.00108319
Iteration 22/25 | Loss: 0.00108319
Iteration 23/25 | Loss: 0.00108319
Iteration 24/25 | Loss: 0.00108319
Iteration 25/25 | Loss: 0.00108319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108319
Iteration 2/1000 | Loss: 0.00005075
Iteration 3/1000 | Loss: 0.00004789
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001791
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001667
Iteration 9/1000 | Loss: 0.00001645
Iteration 10/1000 | Loss: 0.00001624
Iteration 11/1000 | Loss: 0.00001611
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001573
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001568
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001564
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001556
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001553
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001545
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001543
Iteration 36/1000 | Loss: 0.00001543
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001543
Iteration 39/1000 | Loss: 0.00001543
Iteration 40/1000 | Loss: 0.00001542
Iteration 41/1000 | Loss: 0.00001542
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001541
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001540
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001538
Iteration 53/1000 | Loss: 0.00001538
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001537
Iteration 56/1000 | Loss: 0.00001537
Iteration 57/1000 | Loss: 0.00001537
Iteration 58/1000 | Loss: 0.00001536
Iteration 59/1000 | Loss: 0.00001536
Iteration 60/1000 | Loss: 0.00001535
Iteration 61/1000 | Loss: 0.00001535
Iteration 62/1000 | Loss: 0.00001535
Iteration 63/1000 | Loss: 0.00001535
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001528
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001525
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001524
Iteration 99/1000 | Loss: 0.00001524
Iteration 100/1000 | Loss: 0.00001524
Iteration 101/1000 | Loss: 0.00001524
Iteration 102/1000 | Loss: 0.00001524
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001524
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001523
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001523
Iteration 126/1000 | Loss: 0.00001523
Iteration 127/1000 | Loss: 0.00001523
Iteration 128/1000 | Loss: 0.00001523
Iteration 129/1000 | Loss: 0.00001523
Iteration 130/1000 | Loss: 0.00001523
Iteration 131/1000 | Loss: 0.00001523
Iteration 132/1000 | Loss: 0.00001523
Iteration 133/1000 | Loss: 0.00001523
Iteration 134/1000 | Loss: 0.00001523
Iteration 135/1000 | Loss: 0.00001523
Iteration 136/1000 | Loss: 0.00001523
Iteration 137/1000 | Loss: 0.00001523
Iteration 138/1000 | Loss: 0.00001523
Iteration 139/1000 | Loss: 0.00001523
Iteration 140/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.5227955373120494e-05, 1.5227955373120494e-05, 1.5227955373120494e-05, 1.5227955373120494e-05, 1.5227955373120494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5227955373120494e-05

Optimization complete. Final v2v error: 3.290012836456299 mm

Highest mean error: 3.7715113162994385 mm for frame 172

Lowest mean error: 2.9858438968658447 mm for frame 138

Saving results

Total time: 51.918468713760376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438638
Iteration 2/25 | Loss: 0.00140498
Iteration 3/25 | Loss: 0.00131791
Iteration 4/25 | Loss: 0.00130524
Iteration 5/25 | Loss: 0.00130183
Iteration 6/25 | Loss: 0.00130097
Iteration 7/25 | Loss: 0.00130059
Iteration 8/25 | Loss: 0.00130055
Iteration 9/25 | Loss: 0.00130055
Iteration 10/25 | Loss: 0.00130055
Iteration 11/25 | Loss: 0.00130055
Iteration 12/25 | Loss: 0.00130055
Iteration 13/25 | Loss: 0.00130055
Iteration 14/25 | Loss: 0.00130055
Iteration 15/25 | Loss: 0.00130055
Iteration 16/25 | Loss: 0.00130055
Iteration 17/25 | Loss: 0.00130055
Iteration 18/25 | Loss: 0.00130055
Iteration 19/25 | Loss: 0.00130055
Iteration 20/25 | Loss: 0.00130055
Iteration 21/25 | Loss: 0.00130055
Iteration 22/25 | Loss: 0.00130055
Iteration 23/25 | Loss: 0.00130055
Iteration 24/25 | Loss: 0.00130055
Iteration 25/25 | Loss: 0.00130055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001300549483858049, 0.001300549483858049, 0.001300549483858049, 0.001300549483858049, 0.001300549483858049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001300549483858049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50412691
Iteration 2/25 | Loss: 0.00090368
Iteration 3/25 | Loss: 0.00090368
Iteration 4/25 | Loss: 0.00090368
Iteration 5/25 | Loss: 0.00090368
Iteration 6/25 | Loss: 0.00090368
Iteration 7/25 | Loss: 0.00090368
Iteration 8/25 | Loss: 0.00090368
Iteration 9/25 | Loss: 0.00090368
Iteration 10/25 | Loss: 0.00090368
Iteration 11/25 | Loss: 0.00090368
Iteration 12/25 | Loss: 0.00090368
Iteration 13/25 | Loss: 0.00090368
Iteration 14/25 | Loss: 0.00090368
Iteration 15/25 | Loss: 0.00090368
Iteration 16/25 | Loss: 0.00090368
Iteration 17/25 | Loss: 0.00090368
Iteration 18/25 | Loss: 0.00090368
Iteration 19/25 | Loss: 0.00090368
Iteration 20/25 | Loss: 0.00090368
Iteration 21/25 | Loss: 0.00090368
Iteration 22/25 | Loss: 0.00090368
Iteration 23/25 | Loss: 0.00090368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009036771371029317, 0.0009036771371029317, 0.0009036771371029317, 0.0009036771371029317, 0.0009036771371029317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009036771371029317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090368
Iteration 2/1000 | Loss: 0.00003313
Iteration 3/1000 | Loss: 0.00002397
Iteration 4/1000 | Loss: 0.00002104
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001940
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001798
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001726
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001708
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001683
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001678
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001674
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001669
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001655
Iteration 30/1000 | Loss: 0.00001654
Iteration 31/1000 | Loss: 0.00001654
Iteration 32/1000 | Loss: 0.00001653
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001651
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001642
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001636
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001636
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001636
Iteration 75/1000 | Loss: 0.00001636
Iteration 76/1000 | Loss: 0.00001636
Iteration 77/1000 | Loss: 0.00001636
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.636461092857644e-05, 1.636461092857644e-05, 1.636461092857644e-05, 1.636461092857644e-05, 1.636461092857644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.636461092857644e-05

Optimization complete. Final v2v error: 3.422028064727783 mm

Highest mean error: 4.72482442855835 mm for frame 47

Lowest mean error: 3.080400228500366 mm for frame 86

Saving results

Total time: 40.01636624336243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531715
Iteration 2/25 | Loss: 0.00149352
Iteration 3/25 | Loss: 0.00136315
Iteration 4/25 | Loss: 0.00133162
Iteration 5/25 | Loss: 0.00131946
Iteration 6/25 | Loss: 0.00132824
Iteration 7/25 | Loss: 0.00132599
Iteration 8/25 | Loss: 0.00131352
Iteration 9/25 | Loss: 0.00130893
Iteration 10/25 | Loss: 0.00129042
Iteration 11/25 | Loss: 0.00128498
Iteration 12/25 | Loss: 0.00128096
Iteration 13/25 | Loss: 0.00128017
Iteration 14/25 | Loss: 0.00127980
Iteration 15/25 | Loss: 0.00127975
Iteration 16/25 | Loss: 0.00127975
Iteration 17/25 | Loss: 0.00127975
Iteration 18/25 | Loss: 0.00127974
Iteration 19/25 | Loss: 0.00127974
Iteration 20/25 | Loss: 0.00127974
Iteration 21/25 | Loss: 0.00127974
Iteration 22/25 | Loss: 0.00127974
Iteration 23/25 | Loss: 0.00127974
Iteration 24/25 | Loss: 0.00127974
Iteration 25/25 | Loss: 0.00127974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45736146
Iteration 2/25 | Loss: 0.00094234
Iteration 3/25 | Loss: 0.00094234
Iteration 4/25 | Loss: 0.00094233
Iteration 5/25 | Loss: 0.00094233
Iteration 6/25 | Loss: 0.00094233
Iteration 7/25 | Loss: 0.00094233
Iteration 8/25 | Loss: 0.00094233
Iteration 9/25 | Loss: 0.00094233
Iteration 10/25 | Loss: 0.00094233
Iteration 11/25 | Loss: 0.00094233
Iteration 12/25 | Loss: 0.00094233
Iteration 13/25 | Loss: 0.00094233
Iteration 14/25 | Loss: 0.00094233
Iteration 15/25 | Loss: 0.00094233
Iteration 16/25 | Loss: 0.00094233
Iteration 17/25 | Loss: 0.00094233
Iteration 18/25 | Loss: 0.00094233
Iteration 19/25 | Loss: 0.00094233
Iteration 20/25 | Loss: 0.00094233
Iteration 21/25 | Loss: 0.00094233
Iteration 22/25 | Loss: 0.00094233
Iteration 23/25 | Loss: 0.00094233
Iteration 24/25 | Loss: 0.00094233
Iteration 25/25 | Loss: 0.00094233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094233
Iteration 2/1000 | Loss: 0.00003199
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001747
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001525
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001460
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00022979
Iteration 13/1000 | Loss: 0.00017264
Iteration 14/1000 | Loss: 0.00021538
Iteration 15/1000 | Loss: 0.00002448
Iteration 16/1000 | Loss: 0.00021846
Iteration 17/1000 | Loss: 0.00022944
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001672
Iteration 20/1000 | Loss: 0.00001513
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001398
Iteration 32/1000 | Loss: 0.00001397
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001387
Iteration 44/1000 | Loss: 0.00001387
Iteration 45/1000 | Loss: 0.00001386
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001375
Iteration 50/1000 | Loss: 0.00021137
Iteration 51/1000 | Loss: 0.00011555
Iteration 52/1000 | Loss: 0.00016476
Iteration 53/1000 | Loss: 0.00009180
Iteration 54/1000 | Loss: 0.00002644
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001645
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001391
Iteration 61/1000 | Loss: 0.00001355
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001329
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001321
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001305
Iteration 81/1000 | Loss: 0.00001305
Iteration 82/1000 | Loss: 0.00001305
Iteration 83/1000 | Loss: 0.00001305
Iteration 84/1000 | Loss: 0.00001305
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001305
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001305
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001303
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001300
Iteration 114/1000 | Loss: 0.00001300
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001298
Iteration 120/1000 | Loss: 0.00001298
Iteration 121/1000 | Loss: 0.00001298
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001297
Iteration 129/1000 | Loss: 0.00001297
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001297
Iteration 132/1000 | Loss: 0.00001297
Iteration 133/1000 | Loss: 0.00001296
Iteration 134/1000 | Loss: 0.00001296
Iteration 135/1000 | Loss: 0.00001296
Iteration 136/1000 | Loss: 0.00001296
Iteration 137/1000 | Loss: 0.00001296
Iteration 138/1000 | Loss: 0.00001296
Iteration 139/1000 | Loss: 0.00001295
Iteration 140/1000 | Loss: 0.00001295
Iteration 141/1000 | Loss: 0.00001295
Iteration 142/1000 | Loss: 0.00001295
Iteration 143/1000 | Loss: 0.00001295
Iteration 144/1000 | Loss: 0.00001295
Iteration 145/1000 | Loss: 0.00001294
Iteration 146/1000 | Loss: 0.00001294
Iteration 147/1000 | Loss: 0.00001294
Iteration 148/1000 | Loss: 0.00001293
Iteration 149/1000 | Loss: 0.00001293
Iteration 150/1000 | Loss: 0.00001292
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001291
Iteration 161/1000 | Loss: 0.00001291
Iteration 162/1000 | Loss: 0.00001291
Iteration 163/1000 | Loss: 0.00001291
Iteration 164/1000 | Loss: 0.00001291
Iteration 165/1000 | Loss: 0.00001291
Iteration 166/1000 | Loss: 0.00001291
Iteration 167/1000 | Loss: 0.00001291
Iteration 168/1000 | Loss: 0.00001291
Iteration 169/1000 | Loss: 0.00001291
Iteration 170/1000 | Loss: 0.00001290
Iteration 171/1000 | Loss: 0.00001290
Iteration 172/1000 | Loss: 0.00001290
Iteration 173/1000 | Loss: 0.00001290
Iteration 174/1000 | Loss: 0.00001290
Iteration 175/1000 | Loss: 0.00001290
Iteration 176/1000 | Loss: 0.00001289
Iteration 177/1000 | Loss: 0.00001289
Iteration 178/1000 | Loss: 0.00001289
Iteration 179/1000 | Loss: 0.00001288
Iteration 180/1000 | Loss: 0.00001288
Iteration 181/1000 | Loss: 0.00001288
Iteration 182/1000 | Loss: 0.00001287
Iteration 183/1000 | Loss: 0.00001287
Iteration 184/1000 | Loss: 0.00001287
Iteration 185/1000 | Loss: 0.00001287
Iteration 186/1000 | Loss: 0.00001287
Iteration 187/1000 | Loss: 0.00001287
Iteration 188/1000 | Loss: 0.00001286
Iteration 189/1000 | Loss: 0.00001286
Iteration 190/1000 | Loss: 0.00001286
Iteration 191/1000 | Loss: 0.00001286
Iteration 192/1000 | Loss: 0.00001286
Iteration 193/1000 | Loss: 0.00001285
Iteration 194/1000 | Loss: 0.00001285
Iteration 195/1000 | Loss: 0.00001285
Iteration 196/1000 | Loss: 0.00001285
Iteration 197/1000 | Loss: 0.00001285
Iteration 198/1000 | Loss: 0.00001285
Iteration 199/1000 | Loss: 0.00001285
Iteration 200/1000 | Loss: 0.00001285
Iteration 201/1000 | Loss: 0.00001285
Iteration 202/1000 | Loss: 0.00001285
Iteration 203/1000 | Loss: 0.00001285
Iteration 204/1000 | Loss: 0.00001284
Iteration 205/1000 | Loss: 0.00001284
Iteration 206/1000 | Loss: 0.00001284
Iteration 207/1000 | Loss: 0.00001284
Iteration 208/1000 | Loss: 0.00001284
Iteration 209/1000 | Loss: 0.00001284
Iteration 210/1000 | Loss: 0.00001284
Iteration 211/1000 | Loss: 0.00001284
Iteration 212/1000 | Loss: 0.00001284
Iteration 213/1000 | Loss: 0.00001283
Iteration 214/1000 | Loss: 0.00001283
Iteration 215/1000 | Loss: 0.00001283
Iteration 216/1000 | Loss: 0.00001283
Iteration 217/1000 | Loss: 0.00001283
Iteration 218/1000 | Loss: 0.00001283
Iteration 219/1000 | Loss: 0.00001283
Iteration 220/1000 | Loss: 0.00001283
Iteration 221/1000 | Loss: 0.00001283
Iteration 222/1000 | Loss: 0.00001282
Iteration 223/1000 | Loss: 0.00001282
Iteration 224/1000 | Loss: 0.00001282
Iteration 225/1000 | Loss: 0.00001281
Iteration 226/1000 | Loss: 0.00001281
Iteration 227/1000 | Loss: 0.00001281
Iteration 228/1000 | Loss: 0.00001280
Iteration 229/1000 | Loss: 0.00001280
Iteration 230/1000 | Loss: 0.00001280
Iteration 231/1000 | Loss: 0.00001280
Iteration 232/1000 | Loss: 0.00001280
Iteration 233/1000 | Loss: 0.00001280
Iteration 234/1000 | Loss: 0.00001280
Iteration 235/1000 | Loss: 0.00001280
Iteration 236/1000 | Loss: 0.00001280
Iteration 237/1000 | Loss: 0.00001280
Iteration 238/1000 | Loss: 0.00001280
Iteration 239/1000 | Loss: 0.00001280
Iteration 240/1000 | Loss: 0.00001280
Iteration 241/1000 | Loss: 0.00001280
Iteration 242/1000 | Loss: 0.00001280
Iteration 243/1000 | Loss: 0.00001280
Iteration 244/1000 | Loss: 0.00001280
Iteration 245/1000 | Loss: 0.00001279
Iteration 246/1000 | Loss: 0.00001279
Iteration 247/1000 | Loss: 0.00001279
Iteration 248/1000 | Loss: 0.00001279
Iteration 249/1000 | Loss: 0.00001279
Iteration 250/1000 | Loss: 0.00001279
Iteration 251/1000 | Loss: 0.00001279
Iteration 252/1000 | Loss: 0.00001279
Iteration 253/1000 | Loss: 0.00001279
Iteration 254/1000 | Loss: 0.00001279
Iteration 255/1000 | Loss: 0.00001279
Iteration 256/1000 | Loss: 0.00001279
Iteration 257/1000 | Loss: 0.00001278
Iteration 258/1000 | Loss: 0.00001278
Iteration 259/1000 | Loss: 0.00001278
Iteration 260/1000 | Loss: 0.00001278
Iteration 261/1000 | Loss: 0.00001278
Iteration 262/1000 | Loss: 0.00001278
Iteration 263/1000 | Loss: 0.00001278
Iteration 264/1000 | Loss: 0.00001278
Iteration 265/1000 | Loss: 0.00001278
Iteration 266/1000 | Loss: 0.00001278
Iteration 267/1000 | Loss: 0.00001278
Iteration 268/1000 | Loss: 0.00001277
Iteration 269/1000 | Loss: 0.00001277
Iteration 270/1000 | Loss: 0.00001277
Iteration 271/1000 | Loss: 0.00001277
Iteration 272/1000 | Loss: 0.00001277
Iteration 273/1000 | Loss: 0.00001277
Iteration 274/1000 | Loss: 0.00001277
Iteration 275/1000 | Loss: 0.00001277
Iteration 276/1000 | Loss: 0.00001277
Iteration 277/1000 | Loss: 0.00001276
Iteration 278/1000 | Loss: 0.00001276
Iteration 279/1000 | Loss: 0.00001276
Iteration 280/1000 | Loss: 0.00001276
Iteration 281/1000 | Loss: 0.00001276
Iteration 282/1000 | Loss: 0.00001276
Iteration 283/1000 | Loss: 0.00001275
Iteration 284/1000 | Loss: 0.00001275
Iteration 285/1000 | Loss: 0.00001275
Iteration 286/1000 | Loss: 0.00001275
Iteration 287/1000 | Loss: 0.00001275
Iteration 288/1000 | Loss: 0.00001274
Iteration 289/1000 | Loss: 0.00001274
Iteration 290/1000 | Loss: 0.00001274
Iteration 291/1000 | Loss: 0.00001274
Iteration 292/1000 | Loss: 0.00001274
Iteration 293/1000 | Loss: 0.00001274
Iteration 294/1000 | Loss: 0.00001274
Iteration 295/1000 | Loss: 0.00001274
Iteration 296/1000 | Loss: 0.00001274
Iteration 297/1000 | Loss: 0.00001274
Iteration 298/1000 | Loss: 0.00001274
Iteration 299/1000 | Loss: 0.00001274
Iteration 300/1000 | Loss: 0.00001274
Iteration 301/1000 | Loss: 0.00001274
Iteration 302/1000 | Loss: 0.00001273
Iteration 303/1000 | Loss: 0.00001273
Iteration 304/1000 | Loss: 0.00001273
Iteration 305/1000 | Loss: 0.00001273
Iteration 306/1000 | Loss: 0.00001273
Iteration 307/1000 | Loss: 0.00001273
Iteration 308/1000 | Loss: 0.00001273
Iteration 309/1000 | Loss: 0.00001273
Iteration 310/1000 | Loss: 0.00001273
Iteration 311/1000 | Loss: 0.00001273
Iteration 312/1000 | Loss: 0.00001273
Iteration 313/1000 | Loss: 0.00001273
Iteration 314/1000 | Loss: 0.00001273
Iteration 315/1000 | Loss: 0.00001273
Iteration 316/1000 | Loss: 0.00001273
Iteration 317/1000 | Loss: 0.00001272
Iteration 318/1000 | Loss: 0.00001272
Iteration 319/1000 | Loss: 0.00001272
Iteration 320/1000 | Loss: 0.00001272
Iteration 321/1000 | Loss: 0.00001272
Iteration 322/1000 | Loss: 0.00001272
Iteration 323/1000 | Loss: 0.00001272
Iteration 324/1000 | Loss: 0.00001272
Iteration 325/1000 | Loss: 0.00001272
Iteration 326/1000 | Loss: 0.00001272
Iteration 327/1000 | Loss: 0.00001272
Iteration 328/1000 | Loss: 0.00001272
Iteration 329/1000 | Loss: 0.00001272
Iteration 330/1000 | Loss: 0.00001272
Iteration 331/1000 | Loss: 0.00001271
Iteration 332/1000 | Loss: 0.00001271
Iteration 333/1000 | Loss: 0.00001271
Iteration 334/1000 | Loss: 0.00001271
Iteration 335/1000 | Loss: 0.00001271
Iteration 336/1000 | Loss: 0.00001271
Iteration 337/1000 | Loss: 0.00001271
Iteration 338/1000 | Loss: 0.00001271
Iteration 339/1000 | Loss: 0.00001271
Iteration 340/1000 | Loss: 0.00001271
Iteration 341/1000 | Loss: 0.00001271
Iteration 342/1000 | Loss: 0.00001270
Iteration 343/1000 | Loss: 0.00001270
Iteration 344/1000 | Loss: 0.00001270
Iteration 345/1000 | Loss: 0.00001270
Iteration 346/1000 | Loss: 0.00001270
Iteration 347/1000 | Loss: 0.00001270
Iteration 348/1000 | Loss: 0.00001270
Iteration 349/1000 | Loss: 0.00001270
Iteration 350/1000 | Loss: 0.00001270
Iteration 351/1000 | Loss: 0.00001270
Iteration 352/1000 | Loss: 0.00001270
Iteration 353/1000 | Loss: 0.00001270
Iteration 354/1000 | Loss: 0.00001270
Iteration 355/1000 | Loss: 0.00001270
Iteration 356/1000 | Loss: 0.00001270
Iteration 357/1000 | Loss: 0.00001270
Iteration 358/1000 | Loss: 0.00001269
Iteration 359/1000 | Loss: 0.00001269
Iteration 360/1000 | Loss: 0.00001269
Iteration 361/1000 | Loss: 0.00001269
Iteration 362/1000 | Loss: 0.00001269
Iteration 363/1000 | Loss: 0.00001269
Iteration 364/1000 | Loss: 0.00001269
Iteration 365/1000 | Loss: 0.00001269
Iteration 366/1000 | Loss: 0.00001269
Iteration 367/1000 | Loss: 0.00001269
Iteration 368/1000 | Loss: 0.00001269
Iteration 369/1000 | Loss: 0.00001269
Iteration 370/1000 | Loss: 0.00001269
Iteration 371/1000 | Loss: 0.00001269
Iteration 372/1000 | Loss: 0.00001269
Iteration 373/1000 | Loss: 0.00001268
Iteration 374/1000 | Loss: 0.00001268
Iteration 375/1000 | Loss: 0.00001268
Iteration 376/1000 | Loss: 0.00001268
Iteration 377/1000 | Loss: 0.00001268
Iteration 378/1000 | Loss: 0.00001268
Iteration 379/1000 | Loss: 0.00001268
Iteration 380/1000 | Loss: 0.00001268
Iteration 381/1000 | Loss: 0.00001267
Iteration 382/1000 | Loss: 0.00001267
Iteration 383/1000 | Loss: 0.00001267
Iteration 384/1000 | Loss: 0.00001267
Iteration 385/1000 | Loss: 0.00001267
Iteration 386/1000 | Loss: 0.00001267
Iteration 387/1000 | Loss: 0.00001267
Iteration 388/1000 | Loss: 0.00001267
Iteration 389/1000 | Loss: 0.00001267
Iteration 390/1000 | Loss: 0.00001267
Iteration 391/1000 | Loss: 0.00001267
Iteration 392/1000 | Loss: 0.00001267
Iteration 393/1000 | Loss: 0.00001267
Iteration 394/1000 | Loss: 0.00001266
Iteration 395/1000 | Loss: 0.00001266
Iteration 396/1000 | Loss: 0.00001266
Iteration 397/1000 | Loss: 0.00001266
Iteration 398/1000 | Loss: 0.00001266
Iteration 399/1000 | Loss: 0.00001266
Iteration 400/1000 | Loss: 0.00001266
Iteration 401/1000 | Loss: 0.00001266
Iteration 402/1000 | Loss: 0.00001266
Iteration 403/1000 | Loss: 0.00001266
Iteration 404/1000 | Loss: 0.00001266
Iteration 405/1000 | Loss: 0.00001266
Iteration 406/1000 | Loss: 0.00001266
Iteration 407/1000 | Loss: 0.00001266
Iteration 408/1000 | Loss: 0.00001266
Iteration 409/1000 | Loss: 0.00001266
Iteration 410/1000 | Loss: 0.00001266
Iteration 411/1000 | Loss: 0.00001266
Iteration 412/1000 | Loss: 0.00001265
Iteration 413/1000 | Loss: 0.00001265
Iteration 414/1000 | Loss: 0.00001265
Iteration 415/1000 | Loss: 0.00001265
Iteration 416/1000 | Loss: 0.00001265
Iteration 417/1000 | Loss: 0.00001265
Iteration 418/1000 | Loss: 0.00001265
Iteration 419/1000 | Loss: 0.00001265
Iteration 420/1000 | Loss: 0.00001265
Iteration 421/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 421. Stopping optimization.
Last 5 losses: [1.2654020792979281e-05, 1.2654020792979281e-05, 1.2654020792979281e-05, 1.2654020792979281e-05, 1.2654020792979281e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2654020792979281e-05

Optimization complete. Final v2v error: 3.0507357120513916 mm

Highest mean error: 4.1664018630981445 mm for frame 52

Lowest mean error: 2.7658963203430176 mm for frame 119

Saving results

Total time: 109.23143243789673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627316
Iteration 2/25 | Loss: 0.00135057
Iteration 3/25 | Loss: 0.00127986
Iteration 4/25 | Loss: 0.00127025
Iteration 5/25 | Loss: 0.00126654
Iteration 6/25 | Loss: 0.00126586
Iteration 7/25 | Loss: 0.00126586
Iteration 8/25 | Loss: 0.00126586
Iteration 9/25 | Loss: 0.00126586
Iteration 10/25 | Loss: 0.00126586
Iteration 11/25 | Loss: 0.00126586
Iteration 12/25 | Loss: 0.00126586
Iteration 13/25 | Loss: 0.00126586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012658642372116446, 0.0012658642372116446, 0.0012658642372116446, 0.0012658642372116446, 0.0012658642372116446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012658642372116446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44466615
Iteration 2/25 | Loss: 0.00086546
Iteration 3/25 | Loss: 0.00086546
Iteration 4/25 | Loss: 0.00086546
Iteration 5/25 | Loss: 0.00086545
Iteration 6/25 | Loss: 0.00086545
Iteration 7/25 | Loss: 0.00086545
Iteration 8/25 | Loss: 0.00086545
Iteration 9/25 | Loss: 0.00086545
Iteration 10/25 | Loss: 0.00086545
Iteration 11/25 | Loss: 0.00086545
Iteration 12/25 | Loss: 0.00086545
Iteration 13/25 | Loss: 0.00086545
Iteration 14/25 | Loss: 0.00086545
Iteration 15/25 | Loss: 0.00086545
Iteration 16/25 | Loss: 0.00086545
Iteration 17/25 | Loss: 0.00086545
Iteration 18/25 | Loss: 0.00086545
Iteration 19/25 | Loss: 0.00086545
Iteration 20/25 | Loss: 0.00086545
Iteration 21/25 | Loss: 0.00086545
Iteration 22/25 | Loss: 0.00086545
Iteration 23/25 | Loss: 0.00086545
Iteration 24/25 | Loss: 0.00086545
Iteration 25/25 | Loss: 0.00086545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086545
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001588
Iteration 4/1000 | Loss: 0.00001439
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001287
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001222
Iteration 11/1000 | Loss: 0.00001205
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001199
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001184
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001180
Iteration 22/1000 | Loss: 0.00001177
Iteration 23/1000 | Loss: 0.00001176
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001174
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001171
Iteration 34/1000 | Loss: 0.00001170
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001169
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001162
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001152
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001151
Iteration 93/1000 | Loss: 0.00001151
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001150
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001146
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001146
Iteration 126/1000 | Loss: 0.00001146
Iteration 127/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.1455925232439768e-05, 1.1455925232439768e-05, 1.1455925232439768e-05, 1.1455925232439768e-05, 1.1455925232439768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1455925232439768e-05

Optimization complete. Final v2v error: 2.8893635272979736 mm

Highest mean error: 3.5223424434661865 mm for frame 80

Lowest mean error: 2.7054240703582764 mm for frame 146

Saving results

Total time: 36.20610427856445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775034
Iteration 2/25 | Loss: 0.00134472
Iteration 3/25 | Loss: 0.00127881
Iteration 4/25 | Loss: 0.00126960
Iteration 5/25 | Loss: 0.00126747
Iteration 6/25 | Loss: 0.00126747
Iteration 7/25 | Loss: 0.00126747
Iteration 8/25 | Loss: 0.00126747
Iteration 9/25 | Loss: 0.00126747
Iteration 10/25 | Loss: 0.00126747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012674728641286492, 0.0012674728641286492, 0.0012674728641286492, 0.0012674728641286492, 0.0012674728641286492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012674728641286492

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39765918
Iteration 2/25 | Loss: 0.00081631
Iteration 3/25 | Loss: 0.00081630
Iteration 4/25 | Loss: 0.00081630
Iteration 5/25 | Loss: 0.00081630
Iteration 6/25 | Loss: 0.00081630
Iteration 7/25 | Loss: 0.00081630
Iteration 8/25 | Loss: 0.00081630
Iteration 9/25 | Loss: 0.00081630
Iteration 10/25 | Loss: 0.00081630
Iteration 11/25 | Loss: 0.00081630
Iteration 12/25 | Loss: 0.00081630
Iteration 13/25 | Loss: 0.00081630
Iteration 14/25 | Loss: 0.00081630
Iteration 15/25 | Loss: 0.00081630
Iteration 16/25 | Loss: 0.00081630
Iteration 17/25 | Loss: 0.00081630
Iteration 18/25 | Loss: 0.00081630
Iteration 19/25 | Loss: 0.00081630
Iteration 20/25 | Loss: 0.00081630
Iteration 21/25 | Loss: 0.00081630
Iteration 22/25 | Loss: 0.00081630
Iteration 23/25 | Loss: 0.00081630
Iteration 24/25 | Loss: 0.00081630
Iteration 25/25 | Loss: 0.00081630

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081630
Iteration 2/1000 | Loss: 0.00002791
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001343
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001290
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001261
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001246
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001228
Iteration 24/1000 | Loss: 0.00001227
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001221
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001203
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001202
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001199
Iteration 76/1000 | Loss: 0.00001199
Iteration 77/1000 | Loss: 0.00001198
Iteration 78/1000 | Loss: 0.00001198
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001195
Iteration 88/1000 | Loss: 0.00001195
Iteration 89/1000 | Loss: 0.00001195
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001185
Iteration 132/1000 | Loss: 0.00001185
Iteration 133/1000 | Loss: 0.00001185
Iteration 134/1000 | Loss: 0.00001185
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001185
Iteration 139/1000 | Loss: 0.00001185
Iteration 140/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.1854294825752731e-05, 1.1854294825752731e-05, 1.1854294825752731e-05, 1.1854294825752731e-05, 1.1854294825752731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1854294825752731e-05

Optimization complete. Final v2v error: 2.9424688816070557 mm

Highest mean error: 3.1665263175964355 mm for frame 75

Lowest mean error: 2.80275821685791 mm for frame 190

Saving results

Total time: 39.679216384887695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824278
Iteration 2/25 | Loss: 0.00148407
Iteration 3/25 | Loss: 0.00137322
Iteration 4/25 | Loss: 0.00134774
Iteration 5/25 | Loss: 0.00134120
Iteration 6/25 | Loss: 0.00134009
Iteration 7/25 | Loss: 0.00134009
Iteration 8/25 | Loss: 0.00134009
Iteration 9/25 | Loss: 0.00134009
Iteration 10/25 | Loss: 0.00134009
Iteration 11/25 | Loss: 0.00134009
Iteration 12/25 | Loss: 0.00134009
Iteration 13/25 | Loss: 0.00134009
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001340093556791544, 0.001340093556791544, 0.001340093556791544, 0.001340093556791544, 0.001340093556791544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001340093556791544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93656647
Iteration 2/25 | Loss: 0.00092537
Iteration 3/25 | Loss: 0.00092535
Iteration 4/25 | Loss: 0.00092535
Iteration 5/25 | Loss: 0.00092535
Iteration 6/25 | Loss: 0.00092535
Iteration 7/25 | Loss: 0.00092534
Iteration 8/25 | Loss: 0.00092534
Iteration 9/25 | Loss: 0.00092534
Iteration 10/25 | Loss: 0.00092534
Iteration 11/25 | Loss: 0.00092534
Iteration 12/25 | Loss: 0.00092534
Iteration 13/25 | Loss: 0.00092534
Iteration 14/25 | Loss: 0.00092534
Iteration 15/25 | Loss: 0.00092534
Iteration 16/25 | Loss: 0.00092534
Iteration 17/25 | Loss: 0.00092534
Iteration 18/25 | Loss: 0.00092534
Iteration 19/25 | Loss: 0.00092534
Iteration 20/25 | Loss: 0.00092534
Iteration 21/25 | Loss: 0.00092534
Iteration 22/25 | Loss: 0.00092534
Iteration 23/25 | Loss: 0.00092534
Iteration 24/25 | Loss: 0.00092534
Iteration 25/25 | Loss: 0.00092534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092534
Iteration 2/1000 | Loss: 0.00006176
Iteration 3/1000 | Loss: 0.00003871
Iteration 4/1000 | Loss: 0.00003145
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002826
Iteration 7/1000 | Loss: 0.00002733
Iteration 8/1000 | Loss: 0.00002666
Iteration 9/1000 | Loss: 0.00002603
Iteration 10/1000 | Loss: 0.00002558
Iteration 11/1000 | Loss: 0.00002530
Iteration 12/1000 | Loss: 0.00002507
Iteration 13/1000 | Loss: 0.00002493
Iteration 14/1000 | Loss: 0.00002474
Iteration 15/1000 | Loss: 0.00002465
Iteration 16/1000 | Loss: 0.00002447
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002435
Iteration 19/1000 | Loss: 0.00002434
Iteration 20/1000 | Loss: 0.00002432
Iteration 21/1000 | Loss: 0.00002432
Iteration 22/1000 | Loss: 0.00002431
Iteration 23/1000 | Loss: 0.00002430
Iteration 24/1000 | Loss: 0.00002429
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00002428
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002427
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002427
Iteration 33/1000 | Loss: 0.00002426
Iteration 34/1000 | Loss: 0.00002426
Iteration 35/1000 | Loss: 0.00002426
Iteration 36/1000 | Loss: 0.00002425
Iteration 37/1000 | Loss: 0.00002425
Iteration 38/1000 | Loss: 0.00002424
Iteration 39/1000 | Loss: 0.00002424
Iteration 40/1000 | Loss: 0.00002424
Iteration 41/1000 | Loss: 0.00002424
Iteration 42/1000 | Loss: 0.00002423
Iteration 43/1000 | Loss: 0.00002423
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00002423
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002422
Iteration 52/1000 | Loss: 0.00002421
Iteration 53/1000 | Loss: 0.00002421
Iteration 54/1000 | Loss: 0.00002421
Iteration 55/1000 | Loss: 0.00002420
Iteration 56/1000 | Loss: 0.00002420
Iteration 57/1000 | Loss: 0.00002420
Iteration 58/1000 | Loss: 0.00002419
Iteration 59/1000 | Loss: 0.00002418
Iteration 60/1000 | Loss: 0.00002418
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002417
Iteration 63/1000 | Loss: 0.00002417
Iteration 64/1000 | Loss: 0.00002417
Iteration 65/1000 | Loss: 0.00002416
Iteration 66/1000 | Loss: 0.00002416
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002415
Iteration 69/1000 | Loss: 0.00002415
Iteration 70/1000 | Loss: 0.00002415
Iteration 71/1000 | Loss: 0.00002415
Iteration 72/1000 | Loss: 0.00002415
Iteration 73/1000 | Loss: 0.00002415
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002413
Iteration 78/1000 | Loss: 0.00002413
Iteration 79/1000 | Loss: 0.00002413
Iteration 80/1000 | Loss: 0.00002413
Iteration 81/1000 | Loss: 0.00002413
Iteration 82/1000 | Loss: 0.00002413
Iteration 83/1000 | Loss: 0.00002412
Iteration 84/1000 | Loss: 0.00002412
Iteration 85/1000 | Loss: 0.00002412
Iteration 86/1000 | Loss: 0.00002412
Iteration 87/1000 | Loss: 0.00002411
Iteration 88/1000 | Loss: 0.00002411
Iteration 89/1000 | Loss: 0.00002411
Iteration 90/1000 | Loss: 0.00002411
Iteration 91/1000 | Loss: 0.00002410
Iteration 92/1000 | Loss: 0.00002410
Iteration 93/1000 | Loss: 0.00002410
Iteration 94/1000 | Loss: 0.00002410
Iteration 95/1000 | Loss: 0.00002410
Iteration 96/1000 | Loss: 0.00002410
Iteration 97/1000 | Loss: 0.00002410
Iteration 98/1000 | Loss: 0.00002410
Iteration 99/1000 | Loss: 0.00002410
Iteration 100/1000 | Loss: 0.00002410
Iteration 101/1000 | Loss: 0.00002410
Iteration 102/1000 | Loss: 0.00002410
Iteration 103/1000 | Loss: 0.00002410
Iteration 104/1000 | Loss: 0.00002410
Iteration 105/1000 | Loss: 0.00002410
Iteration 106/1000 | Loss: 0.00002410
Iteration 107/1000 | Loss: 0.00002410
Iteration 108/1000 | Loss: 0.00002410
Iteration 109/1000 | Loss: 0.00002410
Iteration 110/1000 | Loss: 0.00002410
Iteration 111/1000 | Loss: 0.00002410
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002410
Iteration 114/1000 | Loss: 0.00002410
Iteration 115/1000 | Loss: 0.00002410
Iteration 116/1000 | Loss: 0.00002410
Iteration 117/1000 | Loss: 0.00002410
Iteration 118/1000 | Loss: 0.00002410
Iteration 119/1000 | Loss: 0.00002410
Iteration 120/1000 | Loss: 0.00002410
Iteration 121/1000 | Loss: 0.00002410
Iteration 122/1000 | Loss: 0.00002410
Iteration 123/1000 | Loss: 0.00002410
Iteration 124/1000 | Loss: 0.00002410
Iteration 125/1000 | Loss: 0.00002410
Iteration 126/1000 | Loss: 0.00002410
Iteration 127/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.4096812921925448e-05, 2.4096812921925448e-05, 2.4096812921925448e-05, 2.4096812921925448e-05, 2.4096812921925448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4096812921925448e-05

Optimization complete. Final v2v error: 4.107237815856934 mm

Highest mean error: 4.55124044418335 mm for frame 51

Lowest mean error: 3.837984561920166 mm for frame 130

Saving results

Total time: 39.59576725959778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652047
Iteration 2/25 | Loss: 0.00169356
Iteration 3/25 | Loss: 0.00140916
Iteration 4/25 | Loss: 0.00136551
Iteration 5/25 | Loss: 0.00136007
Iteration 6/25 | Loss: 0.00134727
Iteration 7/25 | Loss: 0.00133972
Iteration 8/25 | Loss: 0.00133250
Iteration 9/25 | Loss: 0.00133210
Iteration 10/25 | Loss: 0.00133048
Iteration 11/25 | Loss: 0.00133038
Iteration 12/25 | Loss: 0.00133037
Iteration 13/25 | Loss: 0.00133033
Iteration 14/25 | Loss: 0.00133024
Iteration 15/25 | Loss: 0.00133482
Iteration 16/25 | Loss: 0.00133458
Iteration 17/25 | Loss: 0.00133535
Iteration 18/25 | Loss: 0.00133493
Iteration 19/25 | Loss: 0.00132820
Iteration 20/25 | Loss: 0.00132649
Iteration 21/25 | Loss: 0.00132638
Iteration 22/25 | Loss: 0.00132638
Iteration 23/25 | Loss: 0.00132638
Iteration 24/25 | Loss: 0.00132637
Iteration 25/25 | Loss: 0.00132637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02369237
Iteration 2/25 | Loss: 0.00103258
Iteration 3/25 | Loss: 0.00103237
Iteration 4/25 | Loss: 0.00103237
Iteration 5/25 | Loss: 0.00103237
Iteration 6/25 | Loss: 0.00103237
Iteration 7/25 | Loss: 0.00103237
Iteration 8/25 | Loss: 0.00103237
Iteration 9/25 | Loss: 0.00103237
Iteration 10/25 | Loss: 0.00103237
Iteration 11/25 | Loss: 0.00103237
Iteration 12/25 | Loss: 0.00103237
Iteration 13/25 | Loss: 0.00103237
Iteration 14/25 | Loss: 0.00103237
Iteration 15/25 | Loss: 0.00103237
Iteration 16/25 | Loss: 0.00103237
Iteration 17/25 | Loss: 0.00103237
Iteration 18/25 | Loss: 0.00103237
Iteration 19/25 | Loss: 0.00103237
Iteration 20/25 | Loss: 0.00103237
Iteration 21/25 | Loss: 0.00103237
Iteration 22/25 | Loss: 0.00103237
Iteration 23/25 | Loss: 0.00103237
Iteration 24/25 | Loss: 0.00103237
Iteration 25/25 | Loss: 0.00103237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103237
Iteration 2/1000 | Loss: 0.00007077
Iteration 3/1000 | Loss: 0.00004300
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003052
Iteration 6/1000 | Loss: 0.00002872
Iteration 7/1000 | Loss: 0.00002737
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002575
Iteration 10/1000 | Loss: 0.00002527
Iteration 11/1000 | Loss: 0.00002488
Iteration 12/1000 | Loss: 0.00002452
Iteration 13/1000 | Loss: 0.00002422
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002387
Iteration 17/1000 | Loss: 0.00002386
Iteration 18/1000 | Loss: 0.00002385
Iteration 19/1000 | Loss: 0.00002371
Iteration 20/1000 | Loss: 0.00002361
Iteration 21/1000 | Loss: 0.00002359
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00002340
Iteration 25/1000 | Loss: 0.00002336
Iteration 26/1000 | Loss: 0.00002336
Iteration 27/1000 | Loss: 0.00002332
Iteration 28/1000 | Loss: 0.00002331
Iteration 29/1000 | Loss: 0.00002329
Iteration 30/1000 | Loss: 0.00002328
Iteration 31/1000 | Loss: 0.00002328
Iteration 32/1000 | Loss: 0.00002326
Iteration 33/1000 | Loss: 0.00002324
Iteration 34/1000 | Loss: 0.00002323
Iteration 35/1000 | Loss: 0.00002323
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002322
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002318
Iteration 40/1000 | Loss: 0.00002317
Iteration 41/1000 | Loss: 0.00002317
Iteration 42/1000 | Loss: 0.00002316
Iteration 43/1000 | Loss: 0.00002316
Iteration 44/1000 | Loss: 0.00002316
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002315
Iteration 47/1000 | Loss: 0.00002315
Iteration 48/1000 | Loss: 0.00002315
Iteration 49/1000 | Loss: 0.00002315
Iteration 50/1000 | Loss: 0.00002315
Iteration 51/1000 | Loss: 0.00002315
Iteration 52/1000 | Loss: 0.00002315
Iteration 53/1000 | Loss: 0.00002315
Iteration 54/1000 | Loss: 0.00002315
Iteration 55/1000 | Loss: 0.00002315
Iteration 56/1000 | Loss: 0.00002315
Iteration 57/1000 | Loss: 0.00002314
Iteration 58/1000 | Loss: 0.00002314
Iteration 59/1000 | Loss: 0.00002314
Iteration 60/1000 | Loss: 0.00002314
Iteration 61/1000 | Loss: 0.00002314
Iteration 62/1000 | Loss: 0.00002314
Iteration 63/1000 | Loss: 0.00002313
Iteration 64/1000 | Loss: 0.00002312
Iteration 65/1000 | Loss: 0.00002311
Iteration 66/1000 | Loss: 0.00002311
Iteration 67/1000 | Loss: 0.00002311
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002308
Iteration 73/1000 | Loss: 0.00002308
Iteration 74/1000 | Loss: 0.00002308
Iteration 75/1000 | Loss: 0.00002308
Iteration 76/1000 | Loss: 0.00002308
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002308
Iteration 79/1000 | Loss: 0.00002308
Iteration 80/1000 | Loss: 0.00002308
Iteration 81/1000 | Loss: 0.00002308
Iteration 82/1000 | Loss: 0.00002308
Iteration 83/1000 | Loss: 0.00002307
Iteration 84/1000 | Loss: 0.00002307
Iteration 85/1000 | Loss: 0.00002307
Iteration 86/1000 | Loss: 0.00002307
Iteration 87/1000 | Loss: 0.00002307
Iteration 88/1000 | Loss: 0.00002307
Iteration 89/1000 | Loss: 0.00002307
Iteration 90/1000 | Loss: 0.00002305
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002303
Iteration 96/1000 | Loss: 0.00002303
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002302
Iteration 99/1000 | Loss: 0.00002302
Iteration 100/1000 | Loss: 0.00002302
Iteration 101/1000 | Loss: 0.00002301
Iteration 102/1000 | Loss: 0.00002301
Iteration 103/1000 | Loss: 0.00002300
Iteration 104/1000 | Loss: 0.00002300
Iteration 105/1000 | Loss: 0.00002299
Iteration 106/1000 | Loss: 0.00002299
Iteration 107/1000 | Loss: 0.00002299
Iteration 108/1000 | Loss: 0.00002299
Iteration 109/1000 | Loss: 0.00002299
Iteration 110/1000 | Loss: 0.00002299
Iteration 111/1000 | Loss: 0.00002298
Iteration 112/1000 | Loss: 0.00002298
Iteration 113/1000 | Loss: 0.00002298
Iteration 114/1000 | Loss: 0.00002298
Iteration 115/1000 | Loss: 0.00002298
Iteration 116/1000 | Loss: 0.00002297
Iteration 117/1000 | Loss: 0.00002297
Iteration 118/1000 | Loss: 0.00002297
Iteration 119/1000 | Loss: 0.00002296
Iteration 120/1000 | Loss: 0.00002296
Iteration 121/1000 | Loss: 0.00002295
Iteration 122/1000 | Loss: 0.00002295
Iteration 123/1000 | Loss: 0.00002295
Iteration 124/1000 | Loss: 0.00002295
Iteration 125/1000 | Loss: 0.00002294
Iteration 126/1000 | Loss: 0.00002294
Iteration 127/1000 | Loss: 0.00002294
Iteration 128/1000 | Loss: 0.00002294
Iteration 129/1000 | Loss: 0.00002294
Iteration 130/1000 | Loss: 0.00002293
Iteration 131/1000 | Loss: 0.00002293
Iteration 132/1000 | Loss: 0.00002293
Iteration 133/1000 | Loss: 0.00002292
Iteration 134/1000 | Loss: 0.00002292
Iteration 135/1000 | Loss: 0.00002292
Iteration 136/1000 | Loss: 0.00002291
Iteration 137/1000 | Loss: 0.00002291
Iteration 138/1000 | Loss: 0.00002291
Iteration 139/1000 | Loss: 0.00002290
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002290
Iteration 145/1000 | Loss: 0.00002290
Iteration 146/1000 | Loss: 0.00002290
Iteration 147/1000 | Loss: 0.00002290
Iteration 148/1000 | Loss: 0.00002290
Iteration 149/1000 | Loss: 0.00002290
Iteration 150/1000 | Loss: 0.00002290
Iteration 151/1000 | Loss: 0.00002289
Iteration 152/1000 | Loss: 0.00002289
Iteration 153/1000 | Loss: 0.00002289
Iteration 154/1000 | Loss: 0.00002289
Iteration 155/1000 | Loss: 0.00002289
Iteration 156/1000 | Loss: 0.00002289
Iteration 157/1000 | Loss: 0.00002289
Iteration 158/1000 | Loss: 0.00002288
Iteration 159/1000 | Loss: 0.00002288
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002288
Iteration 162/1000 | Loss: 0.00002288
Iteration 163/1000 | Loss: 0.00002288
Iteration 164/1000 | Loss: 0.00002287
Iteration 165/1000 | Loss: 0.00002287
Iteration 166/1000 | Loss: 0.00002287
Iteration 167/1000 | Loss: 0.00002287
Iteration 168/1000 | Loss: 0.00002286
Iteration 169/1000 | Loss: 0.00002286
Iteration 170/1000 | Loss: 0.00002286
Iteration 171/1000 | Loss: 0.00002286
Iteration 172/1000 | Loss: 0.00002286
Iteration 173/1000 | Loss: 0.00002286
Iteration 174/1000 | Loss: 0.00002286
Iteration 175/1000 | Loss: 0.00002286
Iteration 176/1000 | Loss: 0.00002286
Iteration 177/1000 | Loss: 0.00002285
Iteration 178/1000 | Loss: 0.00002285
Iteration 179/1000 | Loss: 0.00002285
Iteration 180/1000 | Loss: 0.00002285
Iteration 181/1000 | Loss: 0.00002285
Iteration 182/1000 | Loss: 0.00002285
Iteration 183/1000 | Loss: 0.00002285
Iteration 184/1000 | Loss: 0.00002285
Iteration 185/1000 | Loss: 0.00002285
Iteration 186/1000 | Loss: 0.00002285
Iteration 187/1000 | Loss: 0.00002285
Iteration 188/1000 | Loss: 0.00002285
Iteration 189/1000 | Loss: 0.00002285
Iteration 190/1000 | Loss: 0.00002284
Iteration 191/1000 | Loss: 0.00002284
Iteration 192/1000 | Loss: 0.00002284
Iteration 193/1000 | Loss: 0.00002284
Iteration 194/1000 | Loss: 0.00002284
Iteration 195/1000 | Loss: 0.00002284
Iteration 196/1000 | Loss: 0.00002284
Iteration 197/1000 | Loss: 0.00002284
Iteration 198/1000 | Loss: 0.00002283
Iteration 199/1000 | Loss: 0.00002283
Iteration 200/1000 | Loss: 0.00002283
Iteration 201/1000 | Loss: 0.00002283
Iteration 202/1000 | Loss: 0.00002282
Iteration 203/1000 | Loss: 0.00002282
Iteration 204/1000 | Loss: 0.00002282
Iteration 205/1000 | Loss: 0.00002282
Iteration 206/1000 | Loss: 0.00002282
Iteration 207/1000 | Loss: 0.00002282
Iteration 208/1000 | Loss: 0.00002282
Iteration 209/1000 | Loss: 0.00002282
Iteration 210/1000 | Loss: 0.00002282
Iteration 211/1000 | Loss: 0.00002282
Iteration 212/1000 | Loss: 0.00002281
Iteration 213/1000 | Loss: 0.00002281
Iteration 214/1000 | Loss: 0.00002281
Iteration 215/1000 | Loss: 0.00002281
Iteration 216/1000 | Loss: 0.00002280
Iteration 217/1000 | Loss: 0.00002280
Iteration 218/1000 | Loss: 0.00002280
Iteration 219/1000 | Loss: 0.00002280
Iteration 220/1000 | Loss: 0.00002280
Iteration 221/1000 | Loss: 0.00002280
Iteration 222/1000 | Loss: 0.00002280
Iteration 223/1000 | Loss: 0.00002280
Iteration 224/1000 | Loss: 0.00002280
Iteration 225/1000 | Loss: 0.00002280
Iteration 226/1000 | Loss: 0.00002280
Iteration 227/1000 | Loss: 0.00002279
Iteration 228/1000 | Loss: 0.00002279
Iteration 229/1000 | Loss: 0.00002279
Iteration 230/1000 | Loss: 0.00002279
Iteration 231/1000 | Loss: 0.00002279
Iteration 232/1000 | Loss: 0.00002279
Iteration 233/1000 | Loss: 0.00002279
Iteration 234/1000 | Loss: 0.00002279
Iteration 235/1000 | Loss: 0.00002278
Iteration 236/1000 | Loss: 0.00002278
Iteration 237/1000 | Loss: 0.00002278
Iteration 238/1000 | Loss: 0.00002278
Iteration 239/1000 | Loss: 0.00002278
Iteration 240/1000 | Loss: 0.00002278
Iteration 241/1000 | Loss: 0.00002278
Iteration 242/1000 | Loss: 0.00002278
Iteration 243/1000 | Loss: 0.00002278
Iteration 244/1000 | Loss: 0.00002278
Iteration 245/1000 | Loss: 0.00002278
Iteration 246/1000 | Loss: 0.00002278
Iteration 247/1000 | Loss: 0.00002278
Iteration 248/1000 | Loss: 0.00002278
Iteration 249/1000 | Loss: 0.00002278
Iteration 250/1000 | Loss: 0.00002278
Iteration 251/1000 | Loss: 0.00002278
Iteration 252/1000 | Loss: 0.00002278
Iteration 253/1000 | Loss: 0.00002278
Iteration 254/1000 | Loss: 0.00002278
Iteration 255/1000 | Loss: 0.00002278
Iteration 256/1000 | Loss: 0.00002278
Iteration 257/1000 | Loss: 0.00002278
Iteration 258/1000 | Loss: 0.00002278
Iteration 259/1000 | Loss: 0.00002278
Iteration 260/1000 | Loss: 0.00002278
Iteration 261/1000 | Loss: 0.00002278
Iteration 262/1000 | Loss: 0.00002278
Iteration 263/1000 | Loss: 0.00002278
Iteration 264/1000 | Loss: 0.00002278
Iteration 265/1000 | Loss: 0.00002278
Iteration 266/1000 | Loss: 0.00002278
Iteration 267/1000 | Loss: 0.00002278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.278169267810881e-05, 2.278169267810881e-05, 2.278169267810881e-05, 2.278169267810881e-05, 2.278169267810881e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.278169267810881e-05

Optimization complete. Final v2v error: 3.869234323501587 mm

Highest mean error: 6.245173454284668 mm for frame 120

Lowest mean error: 3.043423652648926 mm for frame 10

Saving results

Total time: 90.12437081336975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444474
Iteration 2/25 | Loss: 0.00137871
Iteration 3/25 | Loss: 0.00131230
Iteration 4/25 | Loss: 0.00129585
Iteration 5/25 | Loss: 0.00129045
Iteration 6/25 | Loss: 0.00128957
Iteration 7/25 | Loss: 0.00128957
Iteration 8/25 | Loss: 0.00128957
Iteration 9/25 | Loss: 0.00128957
Iteration 10/25 | Loss: 0.00128957
Iteration 11/25 | Loss: 0.00128957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012895733816549182, 0.0012895733816549182, 0.0012895733816549182, 0.0012895733816549182, 0.0012895733816549182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012895733816549182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47617435
Iteration 2/25 | Loss: 0.00086402
Iteration 3/25 | Loss: 0.00086402
Iteration 4/25 | Loss: 0.00086402
Iteration 5/25 | Loss: 0.00086402
Iteration 6/25 | Loss: 0.00086402
Iteration 7/25 | Loss: 0.00086402
Iteration 8/25 | Loss: 0.00086402
Iteration 9/25 | Loss: 0.00086402
Iteration 10/25 | Loss: 0.00086402
Iteration 11/25 | Loss: 0.00086402
Iteration 12/25 | Loss: 0.00086402
Iteration 13/25 | Loss: 0.00086402
Iteration 14/25 | Loss: 0.00086402
Iteration 15/25 | Loss: 0.00086402
Iteration 16/25 | Loss: 0.00086402
Iteration 17/25 | Loss: 0.00086402
Iteration 18/25 | Loss: 0.00086402
Iteration 19/25 | Loss: 0.00086402
Iteration 20/25 | Loss: 0.00086402
Iteration 21/25 | Loss: 0.00086402
Iteration 22/25 | Loss: 0.00086402
Iteration 23/25 | Loss: 0.00086402
Iteration 24/25 | Loss: 0.00086402
Iteration 25/25 | Loss: 0.00086402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086402
Iteration 2/1000 | Loss: 0.00003226
Iteration 3/1000 | Loss: 0.00002245
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001868
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001696
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001669
Iteration 13/1000 | Loss: 0.00001660
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001637
Iteration 17/1000 | Loss: 0.00001635
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001633
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001616
Iteration 29/1000 | Loss: 0.00001616
Iteration 30/1000 | Loss: 0.00001616
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001615
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001613
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001599
Iteration 56/1000 | Loss: 0.00001598
Iteration 57/1000 | Loss: 0.00001598
Iteration 58/1000 | Loss: 0.00001596
Iteration 59/1000 | Loss: 0.00001595
Iteration 60/1000 | Loss: 0.00001595
Iteration 61/1000 | Loss: 0.00001594
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001593
Iteration 64/1000 | Loss: 0.00001591
Iteration 65/1000 | Loss: 0.00001591
Iteration 66/1000 | Loss: 0.00001591
Iteration 67/1000 | Loss: 0.00001591
Iteration 68/1000 | Loss: 0.00001591
Iteration 69/1000 | Loss: 0.00001591
Iteration 70/1000 | Loss: 0.00001591
Iteration 71/1000 | Loss: 0.00001591
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001590
Iteration 79/1000 | Loss: 0.00001590
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001582
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001582
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001580
Iteration 118/1000 | Loss: 0.00001580
Iteration 119/1000 | Loss: 0.00001580
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001579
Iteration 128/1000 | Loss: 0.00001579
Iteration 129/1000 | Loss: 0.00001579
Iteration 130/1000 | Loss: 0.00001579
Iteration 131/1000 | Loss: 0.00001579
Iteration 132/1000 | Loss: 0.00001579
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001578
Iteration 140/1000 | Loss: 0.00001578
Iteration 141/1000 | Loss: 0.00001578
Iteration 142/1000 | Loss: 0.00001578
Iteration 143/1000 | Loss: 0.00001578
Iteration 144/1000 | Loss: 0.00001578
Iteration 145/1000 | Loss: 0.00001578
Iteration 146/1000 | Loss: 0.00001578
Iteration 147/1000 | Loss: 0.00001578
Iteration 148/1000 | Loss: 0.00001578
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001578
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001577
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001577
Iteration 160/1000 | Loss: 0.00001577
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001576
Iteration 176/1000 | Loss: 0.00001576
Iteration 177/1000 | Loss: 0.00001576
Iteration 178/1000 | Loss: 0.00001576
Iteration 179/1000 | Loss: 0.00001576
Iteration 180/1000 | Loss: 0.00001576
Iteration 181/1000 | Loss: 0.00001576
Iteration 182/1000 | Loss: 0.00001576
Iteration 183/1000 | Loss: 0.00001576
Iteration 184/1000 | Loss: 0.00001576
Iteration 185/1000 | Loss: 0.00001576
Iteration 186/1000 | Loss: 0.00001576
Iteration 187/1000 | Loss: 0.00001576
Iteration 188/1000 | Loss: 0.00001576
Iteration 189/1000 | Loss: 0.00001576
Iteration 190/1000 | Loss: 0.00001576
Iteration 191/1000 | Loss: 0.00001576
Iteration 192/1000 | Loss: 0.00001576
Iteration 193/1000 | Loss: 0.00001576
Iteration 194/1000 | Loss: 0.00001576
Iteration 195/1000 | Loss: 0.00001576
Iteration 196/1000 | Loss: 0.00001576
Iteration 197/1000 | Loss: 0.00001576
Iteration 198/1000 | Loss: 0.00001576
Iteration 199/1000 | Loss: 0.00001576
Iteration 200/1000 | Loss: 0.00001576
Iteration 201/1000 | Loss: 0.00001576
Iteration 202/1000 | Loss: 0.00001576
Iteration 203/1000 | Loss: 0.00001576
Iteration 204/1000 | Loss: 0.00001576
Iteration 205/1000 | Loss: 0.00001576
Iteration 206/1000 | Loss: 0.00001576
Iteration 207/1000 | Loss: 0.00001576
Iteration 208/1000 | Loss: 0.00001576
Iteration 209/1000 | Loss: 0.00001576
Iteration 210/1000 | Loss: 0.00001576
Iteration 211/1000 | Loss: 0.00001576
Iteration 212/1000 | Loss: 0.00001576
Iteration 213/1000 | Loss: 0.00001576
Iteration 214/1000 | Loss: 0.00001576
Iteration 215/1000 | Loss: 0.00001576
Iteration 216/1000 | Loss: 0.00001576
Iteration 217/1000 | Loss: 0.00001576
Iteration 218/1000 | Loss: 0.00001576
Iteration 219/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.5755787899252027e-05, 1.5755787899252027e-05, 1.5755787899252027e-05, 1.5755787899252027e-05, 1.5755787899252027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5755787899252027e-05

Optimization complete. Final v2v error: 3.3697948455810547 mm

Highest mean error: 3.8726119995117188 mm for frame 77

Lowest mean error: 3.2240235805511475 mm for frame 34

Saving results

Total time: 42.6529016494751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444450
Iteration 2/25 | Loss: 0.00137388
Iteration 3/25 | Loss: 0.00131705
Iteration 4/25 | Loss: 0.00130605
Iteration 5/25 | Loss: 0.00130247
Iteration 6/25 | Loss: 0.00130247
Iteration 7/25 | Loss: 0.00130247
Iteration 8/25 | Loss: 0.00130247
Iteration 9/25 | Loss: 0.00130247
Iteration 10/25 | Loss: 0.00130247
Iteration 11/25 | Loss: 0.00130247
Iteration 12/25 | Loss: 0.00130247
Iteration 13/25 | Loss: 0.00130247
Iteration 14/25 | Loss: 0.00130247
Iteration 15/25 | Loss: 0.00130247
Iteration 16/25 | Loss: 0.00130247
Iteration 17/25 | Loss: 0.00130247
Iteration 18/25 | Loss: 0.00130247
Iteration 19/25 | Loss: 0.00130247
Iteration 20/25 | Loss: 0.00130247
Iteration 21/25 | Loss: 0.00130247
Iteration 22/25 | Loss: 0.00130247
Iteration 23/25 | Loss: 0.00130247
Iteration 24/25 | Loss: 0.00130247
Iteration 25/25 | Loss: 0.00130247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39141059
Iteration 2/25 | Loss: 0.00106077
Iteration 3/25 | Loss: 0.00106077
Iteration 4/25 | Loss: 0.00106077
Iteration 5/25 | Loss: 0.00106077
Iteration 6/25 | Loss: 0.00106077
Iteration 7/25 | Loss: 0.00106077
Iteration 8/25 | Loss: 0.00106077
Iteration 9/25 | Loss: 0.00106077
Iteration 10/25 | Loss: 0.00106077
Iteration 11/25 | Loss: 0.00106077
Iteration 12/25 | Loss: 0.00106077
Iteration 13/25 | Loss: 0.00106077
Iteration 14/25 | Loss: 0.00106077
Iteration 15/25 | Loss: 0.00106077
Iteration 16/25 | Loss: 0.00106077
Iteration 17/25 | Loss: 0.00106077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010607673320919275, 0.0010607673320919275, 0.0010607673320919275, 0.0010607673320919275, 0.0010607673320919275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010607673320919275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106077
Iteration 2/1000 | Loss: 0.00003116
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001662
Iteration 6/1000 | Loss: 0.00001583
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001511
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001441
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001410
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001400
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001387
Iteration 22/1000 | Loss: 0.00001384
Iteration 23/1000 | Loss: 0.00001383
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001376
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001373
Iteration 42/1000 | Loss: 0.00001372
Iteration 43/1000 | Loss: 0.00001372
Iteration 44/1000 | Loss: 0.00001371
Iteration 45/1000 | Loss: 0.00001370
Iteration 46/1000 | Loss: 0.00001370
Iteration 47/1000 | Loss: 0.00001370
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001369
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001366
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001365
Iteration 59/1000 | Loss: 0.00001365
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001363
Iteration 65/1000 | Loss: 0.00001363
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001362
Iteration 69/1000 | Loss: 0.00001362
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001361
Iteration 72/1000 | Loss: 0.00001361
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001360
Iteration 75/1000 | Loss: 0.00001360
Iteration 76/1000 | Loss: 0.00001360
Iteration 77/1000 | Loss: 0.00001359
Iteration 78/1000 | Loss: 0.00001359
Iteration 79/1000 | Loss: 0.00001359
Iteration 80/1000 | Loss: 0.00001359
Iteration 81/1000 | Loss: 0.00001359
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001354
Iteration 98/1000 | Loss: 0.00001354
Iteration 99/1000 | Loss: 0.00001353
Iteration 100/1000 | Loss: 0.00001353
Iteration 101/1000 | Loss: 0.00001353
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001352
Iteration 104/1000 | Loss: 0.00001352
Iteration 105/1000 | Loss: 0.00001352
Iteration 106/1000 | Loss: 0.00001351
Iteration 107/1000 | Loss: 0.00001351
Iteration 108/1000 | Loss: 0.00001351
Iteration 109/1000 | Loss: 0.00001351
Iteration 110/1000 | Loss: 0.00001351
Iteration 111/1000 | Loss: 0.00001351
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001349
Iteration 129/1000 | Loss: 0.00001349
Iteration 130/1000 | Loss: 0.00001349
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001348
Iteration 133/1000 | Loss: 0.00001348
Iteration 134/1000 | Loss: 0.00001348
Iteration 135/1000 | Loss: 0.00001348
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001348
Iteration 138/1000 | Loss: 0.00001348
Iteration 139/1000 | Loss: 0.00001348
Iteration 140/1000 | Loss: 0.00001348
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001348
Iteration 150/1000 | Loss: 0.00001348
Iteration 151/1000 | Loss: 0.00001348
Iteration 152/1000 | Loss: 0.00001348
Iteration 153/1000 | Loss: 0.00001348
Iteration 154/1000 | Loss: 0.00001348
Iteration 155/1000 | Loss: 0.00001348
Iteration 156/1000 | Loss: 0.00001348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.3478029359248467e-05, 1.3478029359248467e-05, 1.3478029359248467e-05, 1.3478029359248467e-05, 1.3478029359248467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3478029359248467e-05

Optimization complete. Final v2v error: 3.148508071899414 mm

Highest mean error: 3.5264101028442383 mm for frame 47

Lowest mean error: 2.9467923641204834 mm for frame 73

Saving results

Total time: 44.29334783554077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777365
Iteration 2/25 | Loss: 0.00233300
Iteration 3/25 | Loss: 0.00189489
Iteration 4/25 | Loss: 0.00180240
Iteration 5/25 | Loss: 0.00171013
Iteration 6/25 | Loss: 0.00167577
Iteration 7/25 | Loss: 0.00163102
Iteration 8/25 | Loss: 0.00159212
Iteration 9/25 | Loss: 0.00155993
Iteration 10/25 | Loss: 0.00156749
Iteration 11/25 | Loss: 0.00155642
Iteration 12/25 | Loss: 0.00154991
Iteration 13/25 | Loss: 0.00155159
Iteration 14/25 | Loss: 0.00151162
Iteration 15/25 | Loss: 0.00151935
Iteration 16/25 | Loss: 0.00149540
Iteration 17/25 | Loss: 0.00148151
Iteration 18/25 | Loss: 0.00147313
Iteration 19/25 | Loss: 0.00146528
Iteration 20/25 | Loss: 0.00146367
Iteration 21/25 | Loss: 0.00146330
Iteration 22/25 | Loss: 0.00146316
Iteration 23/25 | Loss: 0.00146312
Iteration 24/25 | Loss: 0.00146311
Iteration 25/25 | Loss: 0.00146311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39671350
Iteration 2/25 | Loss: 0.00082812
Iteration 3/25 | Loss: 0.00082810
Iteration 4/25 | Loss: 0.00082810
Iteration 5/25 | Loss: 0.00082810
Iteration 6/25 | Loss: 0.00082810
Iteration 7/25 | Loss: 0.00082810
Iteration 8/25 | Loss: 0.00082810
Iteration 9/25 | Loss: 0.00082810
Iteration 10/25 | Loss: 0.00082810
Iteration 11/25 | Loss: 0.00082810
Iteration 12/25 | Loss: 0.00082810
Iteration 13/25 | Loss: 0.00082810
Iteration 14/25 | Loss: 0.00082810
Iteration 15/25 | Loss: 0.00082810
Iteration 16/25 | Loss: 0.00082810
Iteration 17/25 | Loss: 0.00082810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008281005430035293, 0.0008281005430035293, 0.0008281005430035293, 0.0008281005430035293, 0.0008281005430035293]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008281005430035293

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082810
Iteration 2/1000 | Loss: 0.00005314
Iteration 3/1000 | Loss: 0.00003703
Iteration 4/1000 | Loss: 0.00003372
Iteration 5/1000 | Loss: 0.00003269
Iteration 6/1000 | Loss: 0.00003180
Iteration 7/1000 | Loss: 0.00003123
Iteration 8/1000 | Loss: 0.00003086
Iteration 9/1000 | Loss: 0.00003060
Iteration 10/1000 | Loss: 0.00003032
Iteration 11/1000 | Loss: 0.00003031
Iteration 12/1000 | Loss: 0.00003029
Iteration 13/1000 | Loss: 0.00003011
Iteration 14/1000 | Loss: 0.00003008
Iteration 15/1000 | Loss: 0.00003006
Iteration 16/1000 | Loss: 0.00003005
Iteration 17/1000 | Loss: 0.00003005
Iteration 18/1000 | Loss: 0.00003005
Iteration 19/1000 | Loss: 0.00003004
Iteration 20/1000 | Loss: 0.00003002
Iteration 21/1000 | Loss: 0.00003002
Iteration 22/1000 | Loss: 0.00003001
Iteration 23/1000 | Loss: 0.00003001
Iteration 24/1000 | Loss: 0.00002997
Iteration 25/1000 | Loss: 0.00002986
Iteration 26/1000 | Loss: 0.00002981
Iteration 27/1000 | Loss: 0.00002977
Iteration 28/1000 | Loss: 0.00002976
Iteration 29/1000 | Loss: 0.00002976
Iteration 30/1000 | Loss: 0.00002976
Iteration 31/1000 | Loss: 0.00002974
Iteration 32/1000 | Loss: 0.00002973
Iteration 33/1000 | Loss: 0.00002973
Iteration 34/1000 | Loss: 0.00002973
Iteration 35/1000 | Loss: 0.00002973
Iteration 36/1000 | Loss: 0.00002972
Iteration 37/1000 | Loss: 0.00002972
Iteration 38/1000 | Loss: 0.00002972
Iteration 39/1000 | Loss: 0.00002972
Iteration 40/1000 | Loss: 0.00002972
Iteration 41/1000 | Loss: 0.00002972
Iteration 42/1000 | Loss: 0.00002972
Iteration 43/1000 | Loss: 0.00002972
Iteration 44/1000 | Loss: 0.00002972
Iteration 45/1000 | Loss: 0.00002972
Iteration 46/1000 | Loss: 0.00002971
Iteration 47/1000 | Loss: 0.00002971
Iteration 48/1000 | Loss: 0.00002971
Iteration 49/1000 | Loss: 0.00002971
Iteration 50/1000 | Loss: 0.00002971
Iteration 51/1000 | Loss: 0.00002971
Iteration 52/1000 | Loss: 0.00002971
Iteration 53/1000 | Loss: 0.00002970
Iteration 54/1000 | Loss: 0.00002970
Iteration 55/1000 | Loss: 0.00002969
Iteration 56/1000 | Loss: 0.00002969
Iteration 57/1000 | Loss: 0.00002969
Iteration 58/1000 | Loss: 0.00002969
Iteration 59/1000 | Loss: 0.00002969
Iteration 60/1000 | Loss: 0.00002969
Iteration 61/1000 | Loss: 0.00002969
Iteration 62/1000 | Loss: 0.00002969
Iteration 63/1000 | Loss: 0.00002969
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002968
Iteration 66/1000 | Loss: 0.00002968
Iteration 67/1000 | Loss: 0.00002968
Iteration 68/1000 | Loss: 0.00002968
Iteration 69/1000 | Loss: 0.00002968
Iteration 70/1000 | Loss: 0.00002968
Iteration 71/1000 | Loss: 0.00002968
Iteration 72/1000 | Loss: 0.00002968
Iteration 73/1000 | Loss: 0.00002968
Iteration 74/1000 | Loss: 0.00002967
Iteration 75/1000 | Loss: 0.00002967
Iteration 76/1000 | Loss: 0.00002967
Iteration 77/1000 | Loss: 0.00002967
Iteration 78/1000 | Loss: 0.00002967
Iteration 79/1000 | Loss: 0.00002967
Iteration 80/1000 | Loss: 0.00002967
Iteration 81/1000 | Loss: 0.00002967
Iteration 82/1000 | Loss: 0.00002967
Iteration 83/1000 | Loss: 0.00002967
Iteration 84/1000 | Loss: 0.00002967
Iteration 85/1000 | Loss: 0.00002967
Iteration 86/1000 | Loss: 0.00002967
Iteration 87/1000 | Loss: 0.00002967
Iteration 88/1000 | Loss: 0.00002967
Iteration 89/1000 | Loss: 0.00002967
Iteration 90/1000 | Loss: 0.00002967
Iteration 91/1000 | Loss: 0.00002967
Iteration 92/1000 | Loss: 0.00002967
Iteration 93/1000 | Loss: 0.00002967
Iteration 94/1000 | Loss: 0.00002967
Iteration 95/1000 | Loss: 0.00002967
Iteration 96/1000 | Loss: 0.00002967
Iteration 97/1000 | Loss: 0.00002967
Iteration 98/1000 | Loss: 0.00002967
Iteration 99/1000 | Loss: 0.00002967
Iteration 100/1000 | Loss: 0.00002967
Iteration 101/1000 | Loss: 0.00002967
Iteration 102/1000 | Loss: 0.00002967
Iteration 103/1000 | Loss: 0.00002967
Iteration 104/1000 | Loss: 0.00002967
Iteration 105/1000 | Loss: 0.00002967
Iteration 106/1000 | Loss: 0.00002967
Iteration 107/1000 | Loss: 0.00002967
Iteration 108/1000 | Loss: 0.00002967
Iteration 109/1000 | Loss: 0.00002967
Iteration 110/1000 | Loss: 0.00002967
Iteration 111/1000 | Loss: 0.00002967
Iteration 112/1000 | Loss: 0.00002967
Iteration 113/1000 | Loss: 0.00002967
Iteration 114/1000 | Loss: 0.00002967
Iteration 115/1000 | Loss: 0.00002967
Iteration 116/1000 | Loss: 0.00002967
Iteration 117/1000 | Loss: 0.00002967
Iteration 118/1000 | Loss: 0.00002967
Iteration 119/1000 | Loss: 0.00002967
Iteration 120/1000 | Loss: 0.00002967
Iteration 121/1000 | Loss: 0.00002967
Iteration 122/1000 | Loss: 0.00002967
Iteration 123/1000 | Loss: 0.00002967
Iteration 124/1000 | Loss: 0.00002967
Iteration 125/1000 | Loss: 0.00002967
Iteration 126/1000 | Loss: 0.00002967
Iteration 127/1000 | Loss: 0.00002967
Iteration 128/1000 | Loss: 0.00002967
Iteration 129/1000 | Loss: 0.00002967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.9673054086742923e-05, 2.9673054086742923e-05, 2.9673054086742923e-05, 2.9673054086742923e-05, 2.9673054086742923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9673054086742923e-05

Optimization complete. Final v2v error: 4.527349948883057 mm

Highest mean error: 4.748820781707764 mm for frame 6

Lowest mean error: 4.22487211227417 mm for frame 154

Saving results

Total time: 66.32958436012268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765275
Iteration 2/25 | Loss: 0.00217602
Iteration 3/25 | Loss: 0.00148988
Iteration 4/25 | Loss: 0.00133321
Iteration 5/25 | Loss: 0.00131964
Iteration 6/25 | Loss: 0.00131280
Iteration 7/25 | Loss: 0.00130717
Iteration 8/25 | Loss: 0.00130018
Iteration 9/25 | Loss: 0.00128326
Iteration 10/25 | Loss: 0.00126979
Iteration 11/25 | Loss: 0.00126725
Iteration 12/25 | Loss: 0.00126593
Iteration 13/25 | Loss: 0.00126545
Iteration 14/25 | Loss: 0.00126538
Iteration 15/25 | Loss: 0.00126538
Iteration 16/25 | Loss: 0.00126537
Iteration 17/25 | Loss: 0.00126537
Iteration 18/25 | Loss: 0.00126537
Iteration 19/25 | Loss: 0.00126537
Iteration 20/25 | Loss: 0.00126537
Iteration 21/25 | Loss: 0.00126537
Iteration 22/25 | Loss: 0.00126537
Iteration 23/25 | Loss: 0.00126537
Iteration 24/25 | Loss: 0.00126537
Iteration 25/25 | Loss: 0.00126537

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07592058
Iteration 2/25 | Loss: 0.00096961
Iteration 3/25 | Loss: 0.00096961
Iteration 4/25 | Loss: 0.00091167
Iteration 5/25 | Loss: 0.00091167
Iteration 6/25 | Loss: 0.00091167
Iteration 7/25 | Loss: 0.00091167
Iteration 8/25 | Loss: 0.00091167
Iteration 9/25 | Loss: 0.00091166
Iteration 10/25 | Loss: 0.00091166
Iteration 11/25 | Loss: 0.00091166
Iteration 12/25 | Loss: 0.00091166
Iteration 13/25 | Loss: 0.00091166
Iteration 14/25 | Loss: 0.00091166
Iteration 15/25 | Loss: 0.00091166
Iteration 16/25 | Loss: 0.00091166
Iteration 17/25 | Loss: 0.00091166
Iteration 18/25 | Loss: 0.00091166
Iteration 19/25 | Loss: 0.00091166
Iteration 20/25 | Loss: 0.00091166
Iteration 21/25 | Loss: 0.00091166
Iteration 22/25 | Loss: 0.00091166
Iteration 23/25 | Loss: 0.00091166
Iteration 24/25 | Loss: 0.00091166
Iteration 25/25 | Loss: 0.00091166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091166
Iteration 2/1000 | Loss: 0.00009858
Iteration 3/1000 | Loss: 0.00008548
Iteration 4/1000 | Loss: 0.00008730
Iteration 5/1000 | Loss: 0.00001531
Iteration 6/1000 | Loss: 0.00016636
Iteration 7/1000 | Loss: 0.00094571
Iteration 8/1000 | Loss: 0.00008646
Iteration 9/1000 | Loss: 0.00008379
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00001752
Iteration 12/1000 | Loss: 0.00007376
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001370
Iteration 15/1000 | Loss: 0.00005317
Iteration 16/1000 | Loss: 0.00001438
Iteration 17/1000 | Loss: 0.00001312
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00014626
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002549
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00020379
Iteration 24/1000 | Loss: 0.00012845
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00002789
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00017306
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00002920
Iteration 35/1000 | Loss: 0.00001406
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00002766
Iteration 40/1000 | Loss: 0.00005048
Iteration 41/1000 | Loss: 0.00012286
Iteration 42/1000 | Loss: 0.00003359
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001582
Iteration 47/1000 | Loss: 0.00001481
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00002713
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00002419
Iteration 69/1000 | Loss: 0.00003562
Iteration 70/1000 | Loss: 0.00001739
Iteration 71/1000 | Loss: 0.00001642
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00002334
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00003760
Iteration 76/1000 | Loss: 0.00001384
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001278
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002129
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001424
Iteration 85/1000 | Loss: 0.00007454
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001250
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00004725
Iteration 91/1000 | Loss: 0.00001272
Iteration 92/1000 | Loss: 0.00001705
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001218
Iteration 95/1000 | Loss: 0.00001218
Iteration 96/1000 | Loss: 0.00001218
Iteration 97/1000 | Loss: 0.00001218
Iteration 98/1000 | Loss: 0.00001218
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001217
Iteration 103/1000 | Loss: 0.00001217
Iteration 104/1000 | Loss: 0.00001217
Iteration 105/1000 | Loss: 0.00001217
Iteration 106/1000 | Loss: 0.00001217
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00004149
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001207
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00004739
Iteration 125/1000 | Loss: 0.00001330
Iteration 126/1000 | Loss: 0.00002400
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001201
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001200
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001200
Iteration 159/1000 | Loss: 0.00001200
Iteration 160/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.1998395166301634e-05, 1.1998395166301634e-05, 1.1998395166301634e-05, 1.1998395166301634e-05, 1.1998395166301634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1998395166301634e-05

Optimization complete. Final v2v error: 2.9839725494384766 mm

Highest mean error: 3.327354907989502 mm for frame 60

Lowest mean error: 2.8182003498077393 mm for frame 175

Saving results

Total time: 141.01401805877686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752888
Iteration 2/25 | Loss: 0.00148627
Iteration 3/25 | Loss: 0.00131237
Iteration 4/25 | Loss: 0.00128882
Iteration 5/25 | Loss: 0.00128347
Iteration 6/25 | Loss: 0.00128290
Iteration 7/25 | Loss: 0.00128290
Iteration 8/25 | Loss: 0.00128290
Iteration 9/25 | Loss: 0.00128290
Iteration 10/25 | Loss: 0.00128290
Iteration 11/25 | Loss: 0.00128290
Iteration 12/25 | Loss: 0.00128290
Iteration 13/25 | Loss: 0.00128290
Iteration 14/25 | Loss: 0.00128290
Iteration 15/25 | Loss: 0.00128290
Iteration 16/25 | Loss: 0.00128290
Iteration 17/25 | Loss: 0.00128290
Iteration 18/25 | Loss: 0.00128290
Iteration 19/25 | Loss: 0.00128290
Iteration 20/25 | Loss: 0.00128290
Iteration 21/25 | Loss: 0.00128290
Iteration 22/25 | Loss: 0.00128290
Iteration 23/25 | Loss: 0.00128290
Iteration 24/25 | Loss: 0.00128290
Iteration 25/25 | Loss: 0.00128290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36518312
Iteration 2/25 | Loss: 0.00068321
Iteration 3/25 | Loss: 0.00068317
Iteration 4/25 | Loss: 0.00068317
Iteration 5/25 | Loss: 0.00068317
Iteration 6/25 | Loss: 0.00068317
Iteration 7/25 | Loss: 0.00068317
Iteration 8/25 | Loss: 0.00068317
Iteration 9/25 | Loss: 0.00068317
Iteration 10/25 | Loss: 0.00068317
Iteration 11/25 | Loss: 0.00068317
Iteration 12/25 | Loss: 0.00068317
Iteration 13/25 | Loss: 0.00068317
Iteration 14/25 | Loss: 0.00068317
Iteration 15/25 | Loss: 0.00068317
Iteration 16/25 | Loss: 0.00068317
Iteration 17/25 | Loss: 0.00068317
Iteration 18/25 | Loss: 0.00068317
Iteration 19/25 | Loss: 0.00068317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006831677746959031, 0.0006831677746959031, 0.0006831677746959031, 0.0006831677746959031, 0.0006831677746959031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006831677746959031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068317
Iteration 2/1000 | Loss: 0.00003808
Iteration 3/1000 | Loss: 0.00002738
Iteration 4/1000 | Loss: 0.00002497
Iteration 5/1000 | Loss: 0.00002321
Iteration 6/1000 | Loss: 0.00002249
Iteration 7/1000 | Loss: 0.00002160
Iteration 8/1000 | Loss: 0.00002103
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002027
Iteration 11/1000 | Loss: 0.00002002
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001964
Iteration 14/1000 | Loss: 0.00001942
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001892
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001887
Iteration 21/1000 | Loss: 0.00001886
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001879
Iteration 24/1000 | Loss: 0.00001878
Iteration 25/1000 | Loss: 0.00001877
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001865
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001863
Iteration 49/1000 | Loss: 0.00001863
Iteration 50/1000 | Loss: 0.00001863
Iteration 51/1000 | Loss: 0.00001863
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001862
Iteration 54/1000 | Loss: 0.00001862
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001858
Iteration 67/1000 | Loss: 0.00001858
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001855
Iteration 82/1000 | Loss: 0.00001855
Iteration 83/1000 | Loss: 0.00001855
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001854
Iteration 86/1000 | Loss: 0.00001854
Iteration 87/1000 | Loss: 0.00001854
Iteration 88/1000 | Loss: 0.00001854
Iteration 89/1000 | Loss: 0.00001854
Iteration 90/1000 | Loss: 0.00001854
Iteration 91/1000 | Loss: 0.00001854
Iteration 92/1000 | Loss: 0.00001854
Iteration 93/1000 | Loss: 0.00001854
Iteration 94/1000 | Loss: 0.00001854
Iteration 95/1000 | Loss: 0.00001854
Iteration 96/1000 | Loss: 0.00001854
Iteration 97/1000 | Loss: 0.00001854
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001854
Iteration 106/1000 | Loss: 0.00001854
Iteration 107/1000 | Loss: 0.00001854
Iteration 108/1000 | Loss: 0.00001854
Iteration 109/1000 | Loss: 0.00001854
Iteration 110/1000 | Loss: 0.00001854
Iteration 111/1000 | Loss: 0.00001854
Iteration 112/1000 | Loss: 0.00001854
Iteration 113/1000 | Loss: 0.00001854
Iteration 114/1000 | Loss: 0.00001854
Iteration 115/1000 | Loss: 0.00001854
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.853615503932815e-05, 1.853615503932815e-05, 1.853615503932815e-05, 1.853615503932815e-05, 1.853615503932815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.853615503932815e-05

Optimization complete. Final v2v error: 3.679704189300537 mm

Highest mean error: 3.9905734062194824 mm for frame 141

Lowest mean error: 3.4155895709991455 mm for frame 169

Saving results

Total time: 46.50611090660095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007950
Iteration 2/25 | Loss: 0.00245792
Iteration 3/25 | Loss: 0.00181510
Iteration 4/25 | Loss: 0.00176887
Iteration 5/25 | Loss: 0.00171328
Iteration 6/25 | Loss: 0.00165118
Iteration 7/25 | Loss: 0.00164015
Iteration 8/25 | Loss: 0.00162624
Iteration 9/25 | Loss: 0.00170204
Iteration 10/25 | Loss: 0.00167810
Iteration 11/25 | Loss: 0.00158530
Iteration 12/25 | Loss: 0.00156071
Iteration 13/25 | Loss: 0.00154807
Iteration 14/25 | Loss: 0.00151927
Iteration 15/25 | Loss: 0.00150773
Iteration 16/25 | Loss: 0.00151003
Iteration 17/25 | Loss: 0.00148397
Iteration 18/25 | Loss: 0.00147584
Iteration 19/25 | Loss: 0.00146338
Iteration 20/25 | Loss: 0.00145424
Iteration 21/25 | Loss: 0.00144371
Iteration 22/25 | Loss: 0.00143717
Iteration 23/25 | Loss: 0.00143916
Iteration 24/25 | Loss: 0.00143568
Iteration 25/25 | Loss: 0.00143625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43289447
Iteration 2/25 | Loss: 0.00163355
Iteration 3/25 | Loss: 0.00162724
Iteration 4/25 | Loss: 0.00162724
Iteration 5/25 | Loss: 0.00162724
Iteration 6/25 | Loss: 0.00162724
Iteration 7/25 | Loss: 0.00162724
Iteration 8/25 | Loss: 0.00162724
Iteration 9/25 | Loss: 0.00162724
Iteration 10/25 | Loss: 0.00162724
Iteration 11/25 | Loss: 0.00162724
Iteration 12/25 | Loss: 0.00162724
Iteration 13/25 | Loss: 0.00162724
Iteration 14/25 | Loss: 0.00162724
Iteration 15/25 | Loss: 0.00162724
Iteration 16/25 | Loss: 0.00162724
Iteration 17/25 | Loss: 0.00162724
Iteration 18/25 | Loss: 0.00162724
Iteration 19/25 | Loss: 0.00162724
Iteration 20/25 | Loss: 0.00162724
Iteration 21/25 | Loss: 0.00162724
Iteration 22/25 | Loss: 0.00162724
Iteration 23/25 | Loss: 0.00162724
Iteration 24/25 | Loss: 0.00162724
Iteration 25/25 | Loss: 0.00162724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162724
Iteration 2/1000 | Loss: 0.00018619
Iteration 3/1000 | Loss: 0.00037067
Iteration 4/1000 | Loss: 0.00020403
Iteration 5/1000 | Loss: 0.00018650
Iteration 6/1000 | Loss: 0.00027532
Iteration 7/1000 | Loss: 0.00077535
Iteration 8/1000 | Loss: 0.00032701
Iteration 9/1000 | Loss: 0.00036988
Iteration 10/1000 | Loss: 0.00014856
Iteration 11/1000 | Loss: 0.00011533
Iteration 12/1000 | Loss: 0.00018374
Iteration 13/1000 | Loss: 0.00019971
Iteration 14/1000 | Loss: 0.00019909
Iteration 15/1000 | Loss: 0.00011542
Iteration 16/1000 | Loss: 0.00014882
Iteration 17/1000 | Loss: 0.00014945
Iteration 18/1000 | Loss: 0.00025618
Iteration 19/1000 | Loss: 0.00021474
Iteration 20/1000 | Loss: 0.00022283
Iteration 21/1000 | Loss: 0.00012030
Iteration 22/1000 | Loss: 0.00012542
Iteration 23/1000 | Loss: 0.00013837
Iteration 24/1000 | Loss: 0.00009744
Iteration 25/1000 | Loss: 0.00011468
Iteration 26/1000 | Loss: 0.00038537
Iteration 27/1000 | Loss: 0.00214625
Iteration 28/1000 | Loss: 0.00062974
Iteration 29/1000 | Loss: 0.00036101
Iteration 30/1000 | Loss: 0.00010369
Iteration 31/1000 | Loss: 0.00023779
Iteration 32/1000 | Loss: 0.00009222
Iteration 33/1000 | Loss: 0.00008584
Iteration 34/1000 | Loss: 0.00007271
Iteration 35/1000 | Loss: 0.00006977
Iteration 36/1000 | Loss: 0.00148411
Iteration 37/1000 | Loss: 0.00112628
Iteration 38/1000 | Loss: 0.00008485
Iteration 39/1000 | Loss: 0.00270415
Iteration 40/1000 | Loss: 0.00626847
Iteration 41/1000 | Loss: 0.00327890
Iteration 42/1000 | Loss: 0.00268320
Iteration 43/1000 | Loss: 0.00091481
Iteration 44/1000 | Loss: 0.00013699
Iteration 45/1000 | Loss: 0.00008963
Iteration 46/1000 | Loss: 0.00007864
Iteration 47/1000 | Loss: 0.00014896
Iteration 48/1000 | Loss: 0.00004284
Iteration 49/1000 | Loss: 0.00013228
Iteration 50/1000 | Loss: 0.00003061
Iteration 51/1000 | Loss: 0.00002687
Iteration 52/1000 | Loss: 0.00002351
Iteration 53/1000 | Loss: 0.00002120
Iteration 54/1000 | Loss: 0.00001985
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001467
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001465
Iteration 74/1000 | Loss: 0.00001465
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001463
Iteration 79/1000 | Loss: 0.00001462
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001450
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001446
Iteration 129/1000 | Loss: 0.00001446
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001445
Iteration 136/1000 | Loss: 0.00001445
Iteration 137/1000 | Loss: 0.00001445
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001444
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00001444
Iteration 143/1000 | Loss: 0.00001444
Iteration 144/1000 | Loss: 0.00001444
Iteration 145/1000 | Loss: 0.00001444
Iteration 146/1000 | Loss: 0.00001443
Iteration 147/1000 | Loss: 0.00001443
Iteration 148/1000 | Loss: 0.00001443
Iteration 149/1000 | Loss: 0.00001442
Iteration 150/1000 | Loss: 0.00001442
Iteration 151/1000 | Loss: 0.00001442
Iteration 152/1000 | Loss: 0.00001442
Iteration 153/1000 | Loss: 0.00001442
Iteration 154/1000 | Loss: 0.00001442
Iteration 155/1000 | Loss: 0.00001442
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001442
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001442
Iteration 162/1000 | Loss: 0.00001442
Iteration 163/1000 | Loss: 0.00001442
Iteration 164/1000 | Loss: 0.00001442
Iteration 165/1000 | Loss: 0.00001442
Iteration 166/1000 | Loss: 0.00001442
Iteration 167/1000 | Loss: 0.00001442
Iteration 168/1000 | Loss: 0.00001442
Iteration 169/1000 | Loss: 0.00001442
Iteration 170/1000 | Loss: 0.00001442
Iteration 171/1000 | Loss: 0.00001442
Iteration 172/1000 | Loss: 0.00001442
Iteration 173/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.4421484593185596e-05, 1.4421484593185596e-05, 1.4421484593185596e-05, 1.4421484593185596e-05, 1.4421484593185596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4421484593185596e-05

Optimization complete. Final v2v error: 3.2115395069122314 mm

Highest mean error: 3.6969797611236572 mm for frame 120

Lowest mean error: 2.8410274982452393 mm for frame 34

Saving results

Total time: 154.29458928108215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439850
Iteration 2/25 | Loss: 0.00144345
Iteration 3/25 | Loss: 0.00133665
Iteration 4/25 | Loss: 0.00132608
Iteration 5/25 | Loss: 0.00132356
Iteration 6/25 | Loss: 0.00132352
Iteration 7/25 | Loss: 0.00132352
Iteration 8/25 | Loss: 0.00132352
Iteration 9/25 | Loss: 0.00132352
Iteration 10/25 | Loss: 0.00132352
Iteration 11/25 | Loss: 0.00132352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013235219521448016, 0.0013235219521448016, 0.0013235219521448016, 0.0013235219521448016, 0.0013235219521448016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013235219521448016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38459265
Iteration 2/25 | Loss: 0.00099532
Iteration 3/25 | Loss: 0.00099532
Iteration 4/25 | Loss: 0.00099531
Iteration 5/25 | Loss: 0.00099531
Iteration 6/25 | Loss: 0.00099531
Iteration 7/25 | Loss: 0.00099531
Iteration 8/25 | Loss: 0.00099531
Iteration 9/25 | Loss: 0.00099531
Iteration 10/25 | Loss: 0.00099531
Iteration 11/25 | Loss: 0.00099531
Iteration 12/25 | Loss: 0.00099531
Iteration 13/25 | Loss: 0.00099531
Iteration 14/25 | Loss: 0.00099531
Iteration 15/25 | Loss: 0.00099531
Iteration 16/25 | Loss: 0.00099531
Iteration 17/25 | Loss: 0.00099531
Iteration 18/25 | Loss: 0.00099531
Iteration 19/25 | Loss: 0.00099531
Iteration 20/25 | Loss: 0.00099531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009953120024874806, 0.0009953120024874806, 0.0009953120024874806, 0.0009953120024874806, 0.0009953120024874806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009953120024874806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099531
Iteration 2/1000 | Loss: 0.00003131
Iteration 3/1000 | Loss: 0.00002090
Iteration 4/1000 | Loss: 0.00001856
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001701
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001659
Iteration 10/1000 | Loss: 0.00001638
Iteration 11/1000 | Loss: 0.00001624
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001607
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001579
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001576
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001559
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001547
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001544
Iteration 49/1000 | Loss: 0.00001544
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001532
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001518
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001516
Iteration 77/1000 | Loss: 0.00001515
Iteration 78/1000 | Loss: 0.00001515
Iteration 79/1000 | Loss: 0.00001515
Iteration 80/1000 | Loss: 0.00001515
Iteration 81/1000 | Loss: 0.00001515
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.5082840036484413e-05, 1.5082840036484413e-05, 1.5082840036484413e-05, 1.5082840036484413e-05, 1.5082840036484413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5082840036484413e-05

Optimization complete. Final v2v error: 3.227617025375366 mm

Highest mean error: 3.499783515930176 mm for frame 9

Lowest mean error: 3.007307767868042 mm for frame 140

Saving results

Total time: 42.18510627746582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988395
Iteration 2/25 | Loss: 0.00988395
Iteration 3/25 | Loss: 0.00525237
Iteration 4/25 | Loss: 0.00192721
Iteration 5/25 | Loss: 0.00157520
Iteration 6/25 | Loss: 0.00151788
Iteration 7/25 | Loss: 0.00146661
Iteration 8/25 | Loss: 0.00144091
Iteration 9/25 | Loss: 0.00140587
Iteration 10/25 | Loss: 0.00138233
Iteration 11/25 | Loss: 0.00137879
Iteration 12/25 | Loss: 0.00137671
Iteration 13/25 | Loss: 0.00138350
Iteration 14/25 | Loss: 0.00137427
Iteration 15/25 | Loss: 0.00136767
Iteration 16/25 | Loss: 0.00136946
Iteration 17/25 | Loss: 0.00136612
Iteration 18/25 | Loss: 0.00136595
Iteration 19/25 | Loss: 0.00137005
Iteration 20/25 | Loss: 0.00136824
Iteration 21/25 | Loss: 0.00136720
Iteration 22/25 | Loss: 0.00136444
Iteration 23/25 | Loss: 0.00136412
Iteration 24/25 | Loss: 0.00136446
Iteration 25/25 | Loss: 0.00136329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53216541
Iteration 2/25 | Loss: 0.00103853
Iteration 3/25 | Loss: 0.00098523
Iteration 4/25 | Loss: 0.00098523
Iteration 5/25 | Loss: 0.00098523
Iteration 6/25 | Loss: 0.00098523
Iteration 7/25 | Loss: 0.00098523
Iteration 8/25 | Loss: 0.00098523
Iteration 9/25 | Loss: 0.00098523
Iteration 10/25 | Loss: 0.00098523
Iteration 11/25 | Loss: 0.00098523
Iteration 12/25 | Loss: 0.00098523
Iteration 13/25 | Loss: 0.00098523
Iteration 14/25 | Loss: 0.00098523
Iteration 15/25 | Loss: 0.00098523
Iteration 16/25 | Loss: 0.00098523
Iteration 17/25 | Loss: 0.00098523
Iteration 18/25 | Loss: 0.00098523
Iteration 19/25 | Loss: 0.00098523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009852292714640498, 0.0009852292714640498, 0.0009852292714640498, 0.0009852292714640498, 0.0009852292714640498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009852292714640498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098523
Iteration 2/1000 | Loss: 0.00009268
Iteration 3/1000 | Loss: 0.00036205
Iteration 4/1000 | Loss: 0.00005557
Iteration 5/1000 | Loss: 0.00004066
Iteration 6/1000 | Loss: 0.00010434
Iteration 7/1000 | Loss: 0.00004000
Iteration 8/1000 | Loss: 0.00010795
Iteration 9/1000 | Loss: 0.00022794
Iteration 10/1000 | Loss: 0.00004592
Iteration 11/1000 | Loss: 0.00003999
Iteration 12/1000 | Loss: 0.00007592
Iteration 13/1000 | Loss: 0.00002374
Iteration 14/1000 | Loss: 0.00004561
Iteration 15/1000 | Loss: 0.00002305
Iteration 16/1000 | Loss: 0.00007148
Iteration 17/1000 | Loss: 0.00017978
Iteration 18/1000 | Loss: 0.00007494
Iteration 19/1000 | Loss: 0.00013492
Iteration 20/1000 | Loss: 0.00005183
Iteration 21/1000 | Loss: 0.00002168
Iteration 22/1000 | Loss: 0.00005388
Iteration 23/1000 | Loss: 0.00024910
Iteration 24/1000 | Loss: 0.00007429
Iteration 25/1000 | Loss: 0.00008125
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00002107
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002422
Iteration 30/1000 | Loss: 0.00002059
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002049
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002045
Iteration 35/1000 | Loss: 0.00002045
Iteration 36/1000 | Loss: 0.00002045
Iteration 37/1000 | Loss: 0.00002045
Iteration 38/1000 | Loss: 0.00002045
Iteration 39/1000 | Loss: 0.00002045
Iteration 40/1000 | Loss: 0.00002047
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002037
Iteration 45/1000 | Loss: 0.00002036
Iteration 46/1000 | Loss: 0.00002036
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002351
Iteration 51/1000 | Loss: 0.00002061
Iteration 52/1000 | Loss: 0.00002025
Iteration 53/1000 | Loss: 0.00002028
Iteration 54/1000 | Loss: 0.00002023
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002017
Iteration 61/1000 | Loss: 0.00005024
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002259
Iteration 64/1000 | Loss: 0.00002026
Iteration 65/1000 | Loss: 0.00002026
Iteration 66/1000 | Loss: 0.00037723
Iteration 67/1000 | Loss: 0.00003644
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002874
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00004683
Iteration 72/1000 | Loss: 0.00005145
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001954
Iteration 76/1000 | Loss: 0.00002321
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00013538
Iteration 79/1000 | Loss: 0.00004757
Iteration 80/1000 | Loss: 0.00002782
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001737
Iteration 88/1000 | Loss: 0.00001736
Iteration 89/1000 | Loss: 0.00001736
Iteration 90/1000 | Loss: 0.00001736
Iteration 91/1000 | Loss: 0.00001735
Iteration 92/1000 | Loss: 0.00001735
Iteration 93/1000 | Loss: 0.00001735
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001908
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001729
Iteration 99/1000 | Loss: 0.00001728
Iteration 100/1000 | Loss: 0.00001728
Iteration 101/1000 | Loss: 0.00001728
Iteration 102/1000 | Loss: 0.00001728
Iteration 103/1000 | Loss: 0.00001728
Iteration 104/1000 | Loss: 0.00001728
Iteration 105/1000 | Loss: 0.00001728
Iteration 106/1000 | Loss: 0.00001728
Iteration 107/1000 | Loss: 0.00001728
Iteration 108/1000 | Loss: 0.00001727
Iteration 109/1000 | Loss: 0.00001727
Iteration 110/1000 | Loss: 0.00001727
Iteration 111/1000 | Loss: 0.00001727
Iteration 112/1000 | Loss: 0.00001727
Iteration 113/1000 | Loss: 0.00001727
Iteration 114/1000 | Loss: 0.00001727
Iteration 115/1000 | Loss: 0.00001727
Iteration 116/1000 | Loss: 0.00001727
Iteration 117/1000 | Loss: 0.00001727
Iteration 118/1000 | Loss: 0.00001727
Iteration 119/1000 | Loss: 0.00001727
Iteration 120/1000 | Loss: 0.00001727
Iteration 121/1000 | Loss: 0.00001727
Iteration 122/1000 | Loss: 0.00001726
Iteration 123/1000 | Loss: 0.00001726
Iteration 124/1000 | Loss: 0.00001726
Iteration 125/1000 | Loss: 0.00001726
Iteration 126/1000 | Loss: 0.00001726
Iteration 127/1000 | Loss: 0.00001726
Iteration 128/1000 | Loss: 0.00001726
Iteration 129/1000 | Loss: 0.00001726
Iteration 130/1000 | Loss: 0.00001726
Iteration 131/1000 | Loss: 0.00001725
Iteration 132/1000 | Loss: 0.00001725
Iteration 133/1000 | Loss: 0.00001725
Iteration 134/1000 | Loss: 0.00001725
Iteration 135/1000 | Loss: 0.00001744
Iteration 136/1000 | Loss: 0.00001789
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001721
Iteration 139/1000 | Loss: 0.00001721
Iteration 140/1000 | Loss: 0.00001721
Iteration 141/1000 | Loss: 0.00001721
Iteration 142/1000 | Loss: 0.00001721
Iteration 143/1000 | Loss: 0.00001721
Iteration 144/1000 | Loss: 0.00001721
Iteration 145/1000 | Loss: 0.00001721
Iteration 146/1000 | Loss: 0.00001721
Iteration 147/1000 | Loss: 0.00001721
Iteration 148/1000 | Loss: 0.00001721
Iteration 149/1000 | Loss: 0.00001721
Iteration 150/1000 | Loss: 0.00001721
Iteration 151/1000 | Loss: 0.00001721
Iteration 152/1000 | Loss: 0.00001721
Iteration 153/1000 | Loss: 0.00001721
Iteration 154/1000 | Loss: 0.00001721
Iteration 155/1000 | Loss: 0.00001721
Iteration 156/1000 | Loss: 0.00001721
Iteration 157/1000 | Loss: 0.00001721
Iteration 158/1000 | Loss: 0.00001721
Iteration 159/1000 | Loss: 0.00001721
Iteration 160/1000 | Loss: 0.00001721
Iteration 161/1000 | Loss: 0.00001721
Iteration 162/1000 | Loss: 0.00001721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.7210510122822598e-05, 1.7210510122822598e-05, 1.7210510122822598e-05, 1.7210510122822598e-05, 1.7210510122822598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7210510122822598e-05

Optimization complete. Final v2v error: 3.4807286262512207 mm

Highest mean error: 5.2555975914001465 mm for frame 182

Lowest mean error: 3.0013070106506348 mm for frame 3

Saving results

Total time: 150.93389678001404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435363
Iteration 2/25 | Loss: 0.00143963
Iteration 3/25 | Loss: 0.00133664
Iteration 4/25 | Loss: 0.00132500
Iteration 5/25 | Loss: 0.00132339
Iteration 6/25 | Loss: 0.00132339
Iteration 7/25 | Loss: 0.00132339
Iteration 8/25 | Loss: 0.00132339
Iteration 9/25 | Loss: 0.00132339
Iteration 10/25 | Loss: 0.00132339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013233895879238844, 0.0013233895879238844, 0.0013233895879238844, 0.0013233895879238844, 0.0013233895879238844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013233895879238844

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38582158
Iteration 2/25 | Loss: 0.00074905
Iteration 3/25 | Loss: 0.00074905
Iteration 4/25 | Loss: 0.00074905
Iteration 5/25 | Loss: 0.00074905
Iteration 6/25 | Loss: 0.00074905
Iteration 7/25 | Loss: 0.00074905
Iteration 8/25 | Loss: 0.00074905
Iteration 9/25 | Loss: 0.00074905
Iteration 10/25 | Loss: 0.00074905
Iteration 11/25 | Loss: 0.00074905
Iteration 12/25 | Loss: 0.00074905
Iteration 13/25 | Loss: 0.00074905
Iteration 14/25 | Loss: 0.00074905
Iteration 15/25 | Loss: 0.00074905
Iteration 16/25 | Loss: 0.00074905
Iteration 17/25 | Loss: 0.00074905
Iteration 18/25 | Loss: 0.00074905
Iteration 19/25 | Loss: 0.00074905
Iteration 20/25 | Loss: 0.00074905
Iteration 21/25 | Loss: 0.00074905
Iteration 22/25 | Loss: 0.00074905
Iteration 23/25 | Loss: 0.00074905
Iteration 24/25 | Loss: 0.00074905
Iteration 25/25 | Loss: 0.00074905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074905
Iteration 2/1000 | Loss: 0.00003771
Iteration 3/1000 | Loss: 0.00002696
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002224
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00002013
Iteration 9/1000 | Loss: 0.00001973
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001914
Iteration 12/1000 | Loss: 0.00001905
Iteration 13/1000 | Loss: 0.00001902
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001882
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001879
Iteration 23/1000 | Loss: 0.00001875
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001830
Iteration 37/1000 | Loss: 0.00001827
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001823
Iteration 46/1000 | Loss: 0.00001823
Iteration 47/1000 | Loss: 0.00001823
Iteration 48/1000 | Loss: 0.00001823
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001821
Iteration 54/1000 | Loss: 0.00001821
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001820
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001813
Iteration 76/1000 | Loss: 0.00001812
Iteration 77/1000 | Loss: 0.00001811
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001810
Iteration 81/1000 | Loss: 0.00001809
Iteration 82/1000 | Loss: 0.00001809
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001807
Iteration 89/1000 | Loss: 0.00001807
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001802
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001802
Iteration 116/1000 | Loss: 0.00001802
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001800
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001799
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001797
Iteration 139/1000 | Loss: 0.00001797
Iteration 140/1000 | Loss: 0.00001797
Iteration 141/1000 | Loss: 0.00001797
Iteration 142/1000 | Loss: 0.00001797
Iteration 143/1000 | Loss: 0.00001797
Iteration 144/1000 | Loss: 0.00001797
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001796
Iteration 147/1000 | Loss: 0.00001796
Iteration 148/1000 | Loss: 0.00001796
Iteration 149/1000 | Loss: 0.00001796
Iteration 150/1000 | Loss: 0.00001796
Iteration 151/1000 | Loss: 0.00001796
Iteration 152/1000 | Loss: 0.00001796
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001796
Iteration 157/1000 | Loss: 0.00001796
Iteration 158/1000 | Loss: 0.00001795
Iteration 159/1000 | Loss: 0.00001795
Iteration 160/1000 | Loss: 0.00001795
Iteration 161/1000 | Loss: 0.00001795
Iteration 162/1000 | Loss: 0.00001795
Iteration 163/1000 | Loss: 0.00001795
Iteration 164/1000 | Loss: 0.00001795
Iteration 165/1000 | Loss: 0.00001795
Iteration 166/1000 | Loss: 0.00001795
Iteration 167/1000 | Loss: 0.00001795
Iteration 168/1000 | Loss: 0.00001795
Iteration 169/1000 | Loss: 0.00001795
Iteration 170/1000 | Loss: 0.00001795
Iteration 171/1000 | Loss: 0.00001794
Iteration 172/1000 | Loss: 0.00001794
Iteration 173/1000 | Loss: 0.00001794
Iteration 174/1000 | Loss: 0.00001794
Iteration 175/1000 | Loss: 0.00001794
Iteration 176/1000 | Loss: 0.00001794
Iteration 177/1000 | Loss: 0.00001794
Iteration 178/1000 | Loss: 0.00001794
Iteration 179/1000 | Loss: 0.00001794
Iteration 180/1000 | Loss: 0.00001794
Iteration 181/1000 | Loss: 0.00001794
Iteration 182/1000 | Loss: 0.00001794
Iteration 183/1000 | Loss: 0.00001793
Iteration 184/1000 | Loss: 0.00001793
Iteration 185/1000 | Loss: 0.00001793
Iteration 186/1000 | Loss: 0.00001793
Iteration 187/1000 | Loss: 0.00001793
Iteration 188/1000 | Loss: 0.00001793
Iteration 189/1000 | Loss: 0.00001793
Iteration 190/1000 | Loss: 0.00001793
Iteration 191/1000 | Loss: 0.00001793
Iteration 192/1000 | Loss: 0.00001793
Iteration 193/1000 | Loss: 0.00001793
Iteration 194/1000 | Loss: 0.00001793
Iteration 195/1000 | Loss: 0.00001793
Iteration 196/1000 | Loss: 0.00001793
Iteration 197/1000 | Loss: 0.00001793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.792777948139701e-05, 1.792777948139701e-05, 1.792777948139701e-05, 1.792777948139701e-05, 1.792777948139701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.792777948139701e-05

Optimization complete. Final v2v error: 3.635319232940674 mm

Highest mean error: 3.945568561553955 mm for frame 96

Lowest mean error: 3.307170867919922 mm for frame 18

Saving results

Total time: 46.79771637916565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392781
Iteration 2/25 | Loss: 0.00133637
Iteration 3/25 | Loss: 0.00126162
Iteration 4/25 | Loss: 0.00124961
Iteration 5/25 | Loss: 0.00124532
Iteration 6/25 | Loss: 0.00124485
Iteration 7/25 | Loss: 0.00124485
Iteration 8/25 | Loss: 0.00124485
Iteration 9/25 | Loss: 0.00124485
Iteration 10/25 | Loss: 0.00124485
Iteration 11/25 | Loss: 0.00124485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012448531342670321, 0.0012448531342670321, 0.0012448531342670321, 0.0012448531342670321, 0.0012448531342670321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012448531342670321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48701537
Iteration 2/25 | Loss: 0.00086066
Iteration 3/25 | Loss: 0.00086066
Iteration 4/25 | Loss: 0.00086066
Iteration 5/25 | Loss: 0.00086066
Iteration 6/25 | Loss: 0.00086066
Iteration 7/25 | Loss: 0.00086066
Iteration 8/25 | Loss: 0.00086066
Iteration 9/25 | Loss: 0.00086065
Iteration 10/25 | Loss: 0.00086065
Iteration 11/25 | Loss: 0.00086065
Iteration 12/25 | Loss: 0.00086065
Iteration 13/25 | Loss: 0.00086065
Iteration 14/25 | Loss: 0.00086065
Iteration 15/25 | Loss: 0.00086065
Iteration 16/25 | Loss: 0.00086065
Iteration 17/25 | Loss: 0.00086065
Iteration 18/25 | Loss: 0.00086065
Iteration 19/25 | Loss: 0.00086065
Iteration 20/25 | Loss: 0.00086065
Iteration 21/25 | Loss: 0.00086065
Iteration 22/25 | Loss: 0.00086065
Iteration 23/25 | Loss: 0.00086065
Iteration 24/25 | Loss: 0.00086065
Iteration 25/25 | Loss: 0.00086065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086065
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001415
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001197
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001143
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001105
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001097
Iteration 13/1000 | Loss: 0.00001096
Iteration 14/1000 | Loss: 0.00001089
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001083
Iteration 18/1000 | Loss: 0.00001082
Iteration 19/1000 | Loss: 0.00001082
Iteration 20/1000 | Loss: 0.00001082
Iteration 21/1000 | Loss: 0.00001081
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001078
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001073
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001072
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001070
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001070
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001069
Iteration 38/1000 | Loss: 0.00001067
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001067
Iteration 41/1000 | Loss: 0.00001067
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001066
Iteration 53/1000 | Loss: 0.00001066
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001066
Iteration 56/1000 | Loss: 0.00001066
Iteration 57/1000 | Loss: 0.00001065
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001055
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001054
Iteration 65/1000 | Loss: 0.00001054
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001053
Iteration 68/1000 | Loss: 0.00001053
Iteration 69/1000 | Loss: 0.00001052
Iteration 70/1000 | Loss: 0.00001052
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001050
Iteration 74/1000 | Loss: 0.00001050
Iteration 75/1000 | Loss: 0.00001049
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001046
Iteration 78/1000 | Loss: 0.00001046
Iteration 79/1000 | Loss: 0.00001045
Iteration 80/1000 | Loss: 0.00001045
Iteration 81/1000 | Loss: 0.00001045
Iteration 82/1000 | Loss: 0.00001045
Iteration 83/1000 | Loss: 0.00001045
Iteration 84/1000 | Loss: 0.00001045
Iteration 85/1000 | Loss: 0.00001044
Iteration 86/1000 | Loss: 0.00001044
Iteration 87/1000 | Loss: 0.00001044
Iteration 88/1000 | Loss: 0.00001044
Iteration 89/1000 | Loss: 0.00001044
Iteration 90/1000 | Loss: 0.00001044
Iteration 91/1000 | Loss: 0.00001043
Iteration 92/1000 | Loss: 0.00001043
Iteration 93/1000 | Loss: 0.00001043
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001043
Iteration 96/1000 | Loss: 0.00001042
Iteration 97/1000 | Loss: 0.00001042
Iteration 98/1000 | Loss: 0.00001042
Iteration 99/1000 | Loss: 0.00001042
Iteration 100/1000 | Loss: 0.00001042
Iteration 101/1000 | Loss: 0.00001041
Iteration 102/1000 | Loss: 0.00001041
Iteration 103/1000 | Loss: 0.00001041
Iteration 104/1000 | Loss: 0.00001041
Iteration 105/1000 | Loss: 0.00001041
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001040
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001039
Iteration 110/1000 | Loss: 0.00001039
Iteration 111/1000 | Loss: 0.00001039
Iteration 112/1000 | Loss: 0.00001038
Iteration 113/1000 | Loss: 0.00001038
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001037
Iteration 116/1000 | Loss: 0.00001037
Iteration 117/1000 | Loss: 0.00001037
Iteration 118/1000 | Loss: 0.00001037
Iteration 119/1000 | Loss: 0.00001037
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001036
Iteration 125/1000 | Loss: 0.00001036
Iteration 126/1000 | Loss: 0.00001036
Iteration 127/1000 | Loss: 0.00001036
Iteration 128/1000 | Loss: 0.00001036
Iteration 129/1000 | Loss: 0.00001036
Iteration 130/1000 | Loss: 0.00001036
Iteration 131/1000 | Loss: 0.00001035
Iteration 132/1000 | Loss: 0.00001035
Iteration 133/1000 | Loss: 0.00001035
Iteration 134/1000 | Loss: 0.00001035
Iteration 135/1000 | Loss: 0.00001035
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001034
Iteration 138/1000 | Loss: 0.00001034
Iteration 139/1000 | Loss: 0.00001034
Iteration 140/1000 | Loss: 0.00001034
Iteration 141/1000 | Loss: 0.00001034
Iteration 142/1000 | Loss: 0.00001034
Iteration 143/1000 | Loss: 0.00001034
Iteration 144/1000 | Loss: 0.00001034
Iteration 145/1000 | Loss: 0.00001034
Iteration 146/1000 | Loss: 0.00001034
Iteration 147/1000 | Loss: 0.00001034
Iteration 148/1000 | Loss: 0.00001034
Iteration 149/1000 | Loss: 0.00001033
Iteration 150/1000 | Loss: 0.00001033
Iteration 151/1000 | Loss: 0.00001033
Iteration 152/1000 | Loss: 0.00001033
Iteration 153/1000 | Loss: 0.00001033
Iteration 154/1000 | Loss: 0.00001033
Iteration 155/1000 | Loss: 0.00001033
Iteration 156/1000 | Loss: 0.00001032
Iteration 157/1000 | Loss: 0.00001032
Iteration 158/1000 | Loss: 0.00001032
Iteration 159/1000 | Loss: 0.00001032
Iteration 160/1000 | Loss: 0.00001032
Iteration 161/1000 | Loss: 0.00001031
Iteration 162/1000 | Loss: 0.00001031
Iteration 163/1000 | Loss: 0.00001031
Iteration 164/1000 | Loss: 0.00001031
Iteration 165/1000 | Loss: 0.00001031
Iteration 166/1000 | Loss: 0.00001031
Iteration 167/1000 | Loss: 0.00001031
Iteration 168/1000 | Loss: 0.00001031
Iteration 169/1000 | Loss: 0.00001031
Iteration 170/1000 | Loss: 0.00001030
Iteration 171/1000 | Loss: 0.00001030
Iteration 172/1000 | Loss: 0.00001030
Iteration 173/1000 | Loss: 0.00001030
Iteration 174/1000 | Loss: 0.00001030
Iteration 175/1000 | Loss: 0.00001030
Iteration 176/1000 | Loss: 0.00001030
Iteration 177/1000 | Loss: 0.00001030
Iteration 178/1000 | Loss: 0.00001030
Iteration 179/1000 | Loss: 0.00001030
Iteration 180/1000 | Loss: 0.00001030
Iteration 181/1000 | Loss: 0.00001030
Iteration 182/1000 | Loss: 0.00001029
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Iteration 187/1000 | Loss: 0.00001029
Iteration 188/1000 | Loss: 0.00001029
Iteration 189/1000 | Loss: 0.00001029
Iteration 190/1000 | Loss: 0.00001029
Iteration 191/1000 | Loss: 0.00001029
Iteration 192/1000 | Loss: 0.00001029
Iteration 193/1000 | Loss: 0.00001029
Iteration 194/1000 | Loss: 0.00001029
Iteration 195/1000 | Loss: 0.00001029
Iteration 196/1000 | Loss: 0.00001029
Iteration 197/1000 | Loss: 0.00001029
Iteration 198/1000 | Loss: 0.00001029
Iteration 199/1000 | Loss: 0.00001029
Iteration 200/1000 | Loss: 0.00001029
Iteration 201/1000 | Loss: 0.00001029
Iteration 202/1000 | Loss: 0.00001029
Iteration 203/1000 | Loss: 0.00001029
Iteration 204/1000 | Loss: 0.00001029
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001029
Iteration 207/1000 | Loss: 0.00001029
Iteration 208/1000 | Loss: 0.00001029
Iteration 209/1000 | Loss: 0.00001029
Iteration 210/1000 | Loss: 0.00001029
Iteration 211/1000 | Loss: 0.00001029
Iteration 212/1000 | Loss: 0.00001029
Iteration 213/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.0286999895470217e-05, 1.0286999895470217e-05, 1.0286999895470217e-05, 1.0286999895470217e-05, 1.0286999895470217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0286999895470217e-05

Optimization complete. Final v2v error: 2.77280330657959 mm

Highest mean error: 2.859998941421509 mm for frame 135

Lowest mean error: 2.7172281742095947 mm for frame 58

Saving results

Total time: 41.3801703453064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00653535
Iteration 2/25 | Loss: 0.00147336
Iteration 3/25 | Loss: 0.00135410
Iteration 4/25 | Loss: 0.00133492
Iteration 5/25 | Loss: 0.00132866
Iteration 6/25 | Loss: 0.00132847
Iteration 7/25 | Loss: 0.00132847
Iteration 8/25 | Loss: 0.00132847
Iteration 9/25 | Loss: 0.00132847
Iteration 10/25 | Loss: 0.00132847
Iteration 11/25 | Loss: 0.00132847
Iteration 12/25 | Loss: 0.00132847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001328474492765963, 0.001328474492765963, 0.001328474492765963, 0.001328474492765963, 0.001328474492765963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001328474492765963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39716601
Iteration 2/25 | Loss: 0.00087629
Iteration 3/25 | Loss: 0.00087628
Iteration 4/25 | Loss: 0.00087628
Iteration 5/25 | Loss: 0.00087628
Iteration 6/25 | Loss: 0.00087628
Iteration 7/25 | Loss: 0.00087628
Iteration 8/25 | Loss: 0.00087628
Iteration 9/25 | Loss: 0.00087628
Iteration 10/25 | Loss: 0.00087628
Iteration 11/25 | Loss: 0.00087628
Iteration 12/25 | Loss: 0.00087628
Iteration 13/25 | Loss: 0.00087628
Iteration 14/25 | Loss: 0.00087628
Iteration 15/25 | Loss: 0.00087628
Iteration 16/25 | Loss: 0.00087628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008762812940403819, 0.0008762812940403819, 0.0008762812940403819, 0.0008762812940403819, 0.0008762812940403819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008762812940403819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087628
Iteration 2/1000 | Loss: 0.00004365
Iteration 3/1000 | Loss: 0.00002839
Iteration 4/1000 | Loss: 0.00002636
Iteration 5/1000 | Loss: 0.00002577
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002454
Iteration 8/1000 | Loss: 0.00002421
Iteration 9/1000 | Loss: 0.00002384
Iteration 10/1000 | Loss: 0.00002353
Iteration 11/1000 | Loss: 0.00002329
Iteration 12/1000 | Loss: 0.00002312
Iteration 13/1000 | Loss: 0.00002296
Iteration 14/1000 | Loss: 0.00002286
Iteration 15/1000 | Loss: 0.00002278
Iteration 16/1000 | Loss: 0.00002273
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002269
Iteration 19/1000 | Loss: 0.00002269
Iteration 20/1000 | Loss: 0.00002267
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002267
Iteration 23/1000 | Loss: 0.00002266
Iteration 24/1000 | Loss: 0.00002266
Iteration 25/1000 | Loss: 0.00002266
Iteration 26/1000 | Loss: 0.00002265
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002265
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002264
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002264
Iteration 34/1000 | Loss: 0.00002263
Iteration 35/1000 | Loss: 0.00002263
Iteration 36/1000 | Loss: 0.00002263
Iteration 37/1000 | Loss: 0.00002263
Iteration 38/1000 | Loss: 0.00002263
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002263
Iteration 41/1000 | Loss: 0.00002263
Iteration 42/1000 | Loss: 0.00002262
Iteration 43/1000 | Loss: 0.00002262
Iteration 44/1000 | Loss: 0.00002262
Iteration 45/1000 | Loss: 0.00002262
Iteration 46/1000 | Loss: 0.00002262
Iteration 47/1000 | Loss: 0.00002262
Iteration 48/1000 | Loss: 0.00002262
Iteration 49/1000 | Loss: 0.00002261
Iteration 50/1000 | Loss: 0.00002261
Iteration 51/1000 | Loss: 0.00002261
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002261
Iteration 55/1000 | Loss: 0.00002261
Iteration 56/1000 | Loss: 0.00002261
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00002261
Iteration 59/1000 | Loss: 0.00002260
Iteration 60/1000 | Loss: 0.00002260
Iteration 61/1000 | Loss: 0.00002260
Iteration 62/1000 | Loss: 0.00002260
Iteration 63/1000 | Loss: 0.00002260
Iteration 64/1000 | Loss: 0.00002259
Iteration 65/1000 | Loss: 0.00002259
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002259
Iteration 69/1000 | Loss: 0.00002259
Iteration 70/1000 | Loss: 0.00002259
Iteration 71/1000 | Loss: 0.00002259
Iteration 72/1000 | Loss: 0.00002259
Iteration 73/1000 | Loss: 0.00002259
Iteration 74/1000 | Loss: 0.00002259
Iteration 75/1000 | Loss: 0.00002259
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00002258
Iteration 78/1000 | Loss: 0.00002258
Iteration 79/1000 | Loss: 0.00002258
Iteration 80/1000 | Loss: 0.00002257
Iteration 81/1000 | Loss: 0.00002257
Iteration 82/1000 | Loss: 0.00002257
Iteration 83/1000 | Loss: 0.00002257
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00002257
Iteration 86/1000 | Loss: 0.00002257
Iteration 87/1000 | Loss: 0.00002257
Iteration 88/1000 | Loss: 0.00002257
Iteration 89/1000 | Loss: 0.00002257
Iteration 90/1000 | Loss: 0.00002257
Iteration 91/1000 | Loss: 0.00002257
Iteration 92/1000 | Loss: 0.00002256
Iteration 93/1000 | Loss: 0.00002256
Iteration 94/1000 | Loss: 0.00002256
Iteration 95/1000 | Loss: 0.00002256
Iteration 96/1000 | Loss: 0.00002256
Iteration 97/1000 | Loss: 0.00002256
Iteration 98/1000 | Loss: 0.00002255
Iteration 99/1000 | Loss: 0.00002255
Iteration 100/1000 | Loss: 0.00002255
Iteration 101/1000 | Loss: 0.00002255
Iteration 102/1000 | Loss: 0.00002254
Iteration 103/1000 | Loss: 0.00002254
Iteration 104/1000 | Loss: 0.00002254
Iteration 105/1000 | Loss: 0.00002254
Iteration 106/1000 | Loss: 0.00002254
Iteration 107/1000 | Loss: 0.00002253
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002253
Iteration 110/1000 | Loss: 0.00002253
Iteration 111/1000 | Loss: 0.00002253
Iteration 112/1000 | Loss: 0.00002253
Iteration 113/1000 | Loss: 0.00002253
Iteration 114/1000 | Loss: 0.00002253
Iteration 115/1000 | Loss: 0.00002253
Iteration 116/1000 | Loss: 0.00002253
Iteration 117/1000 | Loss: 0.00002253
Iteration 118/1000 | Loss: 0.00002252
Iteration 119/1000 | Loss: 0.00002252
Iteration 120/1000 | Loss: 0.00002252
Iteration 121/1000 | Loss: 0.00002252
Iteration 122/1000 | Loss: 0.00002252
Iteration 123/1000 | Loss: 0.00002252
Iteration 124/1000 | Loss: 0.00002252
Iteration 125/1000 | Loss: 0.00002252
Iteration 126/1000 | Loss: 0.00002252
Iteration 127/1000 | Loss: 0.00002252
Iteration 128/1000 | Loss: 0.00002252
Iteration 129/1000 | Loss: 0.00002252
Iteration 130/1000 | Loss: 0.00002252
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002251
Iteration 133/1000 | Loss: 0.00002251
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002250
Iteration 149/1000 | Loss: 0.00002250
Iteration 150/1000 | Loss: 0.00002250
Iteration 151/1000 | Loss: 0.00002250
Iteration 152/1000 | Loss: 0.00002250
Iteration 153/1000 | Loss: 0.00002249
Iteration 154/1000 | Loss: 0.00002249
Iteration 155/1000 | Loss: 0.00002249
Iteration 156/1000 | Loss: 0.00002249
Iteration 157/1000 | Loss: 0.00002249
Iteration 158/1000 | Loss: 0.00002249
Iteration 159/1000 | Loss: 0.00002249
Iteration 160/1000 | Loss: 0.00002249
Iteration 161/1000 | Loss: 0.00002249
Iteration 162/1000 | Loss: 0.00002249
Iteration 163/1000 | Loss: 0.00002249
Iteration 164/1000 | Loss: 0.00002249
Iteration 165/1000 | Loss: 0.00002249
Iteration 166/1000 | Loss: 0.00002249
Iteration 167/1000 | Loss: 0.00002249
Iteration 168/1000 | Loss: 0.00002249
Iteration 169/1000 | Loss: 0.00002249
Iteration 170/1000 | Loss: 0.00002248
Iteration 171/1000 | Loss: 0.00002248
Iteration 172/1000 | Loss: 0.00002248
Iteration 173/1000 | Loss: 0.00002248
Iteration 174/1000 | Loss: 0.00002248
Iteration 175/1000 | Loss: 0.00002248
Iteration 176/1000 | Loss: 0.00002248
Iteration 177/1000 | Loss: 0.00002248
Iteration 178/1000 | Loss: 0.00002248
Iteration 179/1000 | Loss: 0.00002248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.248234704893548e-05, 2.248234704893548e-05, 2.248234704893548e-05, 2.248234704893548e-05, 2.248234704893548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.248234704893548e-05

Optimization complete. Final v2v error: 4.001972675323486 mm

Highest mean error: 4.387055397033691 mm for frame 217

Lowest mean error: 3.728532552719116 mm for frame 182

Saving results

Total time: 44.7093551158905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444817
Iteration 2/25 | Loss: 0.00144681
Iteration 3/25 | Loss: 0.00132807
Iteration 4/25 | Loss: 0.00131278
Iteration 5/25 | Loss: 0.00130886
Iteration 6/25 | Loss: 0.00130792
Iteration 7/25 | Loss: 0.00130768
Iteration 8/25 | Loss: 0.00130768
Iteration 9/25 | Loss: 0.00130768
Iteration 10/25 | Loss: 0.00130768
Iteration 11/25 | Loss: 0.00130768
Iteration 12/25 | Loss: 0.00130768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013076768955215812, 0.0013076768955215812, 0.0013076768955215812, 0.0013076768955215812, 0.0013076768955215812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013076768955215812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50759745
Iteration 2/25 | Loss: 0.00086354
Iteration 3/25 | Loss: 0.00086353
Iteration 4/25 | Loss: 0.00086353
Iteration 5/25 | Loss: 0.00086353
Iteration 6/25 | Loss: 0.00086353
Iteration 7/25 | Loss: 0.00086353
Iteration 8/25 | Loss: 0.00086353
Iteration 9/25 | Loss: 0.00086353
Iteration 10/25 | Loss: 0.00086353
Iteration 11/25 | Loss: 0.00086353
Iteration 12/25 | Loss: 0.00086353
Iteration 13/25 | Loss: 0.00086353
Iteration 14/25 | Loss: 0.00086353
Iteration 15/25 | Loss: 0.00086353
Iteration 16/25 | Loss: 0.00086353
Iteration 17/25 | Loss: 0.00086353
Iteration 18/25 | Loss: 0.00086353
Iteration 19/25 | Loss: 0.00086353
Iteration 20/25 | Loss: 0.00086353
Iteration 21/25 | Loss: 0.00086353
Iteration 22/25 | Loss: 0.00086353
Iteration 23/25 | Loss: 0.00086353
Iteration 24/25 | Loss: 0.00086353
Iteration 25/25 | Loss: 0.00086353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086353
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002082
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001840
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001738
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001638
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001620
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001602
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001594
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001593
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001591
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001590
Iteration 29/1000 | Loss: 0.00001590
Iteration 30/1000 | Loss: 0.00001589
Iteration 31/1000 | Loss: 0.00001589
Iteration 32/1000 | Loss: 0.00001589
Iteration 33/1000 | Loss: 0.00001588
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001587
Iteration 36/1000 | Loss: 0.00001587
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001586
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001584
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001583
Iteration 56/1000 | Loss: 0.00001583
Iteration 57/1000 | Loss: 0.00001582
Iteration 58/1000 | Loss: 0.00001582
Iteration 59/1000 | Loss: 0.00001582
Iteration 60/1000 | Loss: 0.00001582
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001580
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001579
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001579
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001578
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001577
Iteration 73/1000 | Loss: 0.00001577
Iteration 74/1000 | Loss: 0.00001576
Iteration 75/1000 | Loss: 0.00001576
Iteration 76/1000 | Loss: 0.00001576
Iteration 77/1000 | Loss: 0.00001576
Iteration 78/1000 | Loss: 0.00001575
Iteration 79/1000 | Loss: 0.00001575
Iteration 80/1000 | Loss: 0.00001575
Iteration 81/1000 | Loss: 0.00001575
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001574
Iteration 84/1000 | Loss: 0.00001574
Iteration 85/1000 | Loss: 0.00001573
Iteration 86/1000 | Loss: 0.00001573
Iteration 87/1000 | Loss: 0.00001573
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001572
Iteration 90/1000 | Loss: 0.00001572
Iteration 91/1000 | Loss: 0.00001572
Iteration 92/1000 | Loss: 0.00001572
Iteration 93/1000 | Loss: 0.00001572
Iteration 94/1000 | Loss: 0.00001572
Iteration 95/1000 | Loss: 0.00001571
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001571
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001570
Iteration 101/1000 | Loss: 0.00001570
Iteration 102/1000 | Loss: 0.00001570
Iteration 103/1000 | Loss: 0.00001570
Iteration 104/1000 | Loss: 0.00001570
Iteration 105/1000 | Loss: 0.00001570
Iteration 106/1000 | Loss: 0.00001570
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001565
Iteration 124/1000 | Loss: 0.00001565
Iteration 125/1000 | Loss: 0.00001565
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001564
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001562
Iteration 137/1000 | Loss: 0.00001562
Iteration 138/1000 | Loss: 0.00001562
Iteration 139/1000 | Loss: 0.00001562
Iteration 140/1000 | Loss: 0.00001562
Iteration 141/1000 | Loss: 0.00001562
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001562
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001562
Iteration 146/1000 | Loss: 0.00001561
Iteration 147/1000 | Loss: 0.00001561
Iteration 148/1000 | Loss: 0.00001561
Iteration 149/1000 | Loss: 0.00001561
Iteration 150/1000 | Loss: 0.00001561
Iteration 151/1000 | Loss: 0.00001561
Iteration 152/1000 | Loss: 0.00001561
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001561
Iteration 156/1000 | Loss: 0.00001561
Iteration 157/1000 | Loss: 0.00001560
Iteration 158/1000 | Loss: 0.00001560
Iteration 159/1000 | Loss: 0.00001560
Iteration 160/1000 | Loss: 0.00001560
Iteration 161/1000 | Loss: 0.00001560
Iteration 162/1000 | Loss: 0.00001560
Iteration 163/1000 | Loss: 0.00001560
Iteration 164/1000 | Loss: 0.00001560
Iteration 165/1000 | Loss: 0.00001560
Iteration 166/1000 | Loss: 0.00001560
Iteration 167/1000 | Loss: 0.00001560
Iteration 168/1000 | Loss: 0.00001560
Iteration 169/1000 | Loss: 0.00001560
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001560
Iteration 172/1000 | Loss: 0.00001560
Iteration 173/1000 | Loss: 0.00001560
Iteration 174/1000 | Loss: 0.00001560
Iteration 175/1000 | Loss: 0.00001560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.560186683491338e-05, 1.560186683491338e-05, 1.560186683491338e-05, 1.560186683491338e-05, 1.560186683491338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.560186683491338e-05

Optimization complete. Final v2v error: 3.3275270462036133 mm

Highest mean error: 4.1536030769348145 mm for frame 52

Lowest mean error: 2.9265801906585693 mm for frame 104

Saving results

Total time: 43.27291440963745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970681
Iteration 2/25 | Loss: 0.00970681
Iteration 3/25 | Loss: 0.00970680
Iteration 4/25 | Loss: 0.00970680
Iteration 5/25 | Loss: 0.00318056
Iteration 6/25 | Loss: 0.00206803
Iteration 7/25 | Loss: 0.00182155
Iteration 8/25 | Loss: 0.00184387
Iteration 9/25 | Loss: 0.00187115
Iteration 10/25 | Loss: 0.00174605
Iteration 11/25 | Loss: 0.00164567
Iteration 12/25 | Loss: 0.00159169
Iteration 13/25 | Loss: 0.00151509
Iteration 14/25 | Loss: 0.00146617
Iteration 15/25 | Loss: 0.00146075
Iteration 16/25 | Loss: 0.00146264
Iteration 17/25 | Loss: 0.00144078
Iteration 18/25 | Loss: 0.00141722
Iteration 19/25 | Loss: 0.00141135
Iteration 20/25 | Loss: 0.00140237
Iteration 21/25 | Loss: 0.00139257
Iteration 22/25 | Loss: 0.00138504
Iteration 23/25 | Loss: 0.00137509
Iteration 24/25 | Loss: 0.00136792
Iteration 25/25 | Loss: 0.00136322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37776899
Iteration 2/25 | Loss: 0.00121726
Iteration 3/25 | Loss: 0.00116265
Iteration 4/25 | Loss: 0.00116265
Iteration 5/25 | Loss: 0.00116265
Iteration 6/25 | Loss: 0.00116265
Iteration 7/25 | Loss: 0.00116265
Iteration 8/25 | Loss: 0.00116265
Iteration 9/25 | Loss: 0.00116265
Iteration 10/25 | Loss: 0.00116265
Iteration 11/25 | Loss: 0.00116265
Iteration 12/25 | Loss: 0.00116265
Iteration 13/25 | Loss: 0.00116265
Iteration 14/25 | Loss: 0.00116265
Iteration 15/25 | Loss: 0.00116265
Iteration 16/25 | Loss: 0.00116265
Iteration 17/25 | Loss: 0.00116265
Iteration 18/25 | Loss: 0.00116265
Iteration 19/25 | Loss: 0.00116265
Iteration 20/25 | Loss: 0.00116265
Iteration 21/25 | Loss: 0.00116265
Iteration 22/25 | Loss: 0.00116265
Iteration 23/25 | Loss: 0.00116265
Iteration 24/25 | Loss: 0.00116265
Iteration 25/25 | Loss: 0.00116265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011626469204202294, 0.0011626469204202294, 0.0011626469204202294, 0.0011626469204202294, 0.0011626469204202294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011626469204202294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116265
Iteration 2/1000 | Loss: 0.00126372
Iteration 3/1000 | Loss: 0.00012454
Iteration 4/1000 | Loss: 0.00008651
Iteration 5/1000 | Loss: 0.00019556
Iteration 6/1000 | Loss: 0.00020189
Iteration 7/1000 | Loss: 0.00016554
Iteration 8/1000 | Loss: 0.00016685
Iteration 9/1000 | Loss: 0.00048729
Iteration 10/1000 | Loss: 0.00031905
Iteration 11/1000 | Loss: 0.00024890
Iteration 12/1000 | Loss: 0.00010517
Iteration 13/1000 | Loss: 0.00010181
Iteration 14/1000 | Loss: 0.00006806
Iteration 15/1000 | Loss: 0.00018335
Iteration 16/1000 | Loss: 0.00007332
Iteration 17/1000 | Loss: 0.00006016
Iteration 18/1000 | Loss: 0.00015726
Iteration 19/1000 | Loss: 0.00024495
Iteration 20/1000 | Loss: 0.00014931
Iteration 21/1000 | Loss: 0.00019421
Iteration 22/1000 | Loss: 0.00015110
Iteration 23/1000 | Loss: 0.00012151
Iteration 24/1000 | Loss: 0.00008455
Iteration 25/1000 | Loss: 0.00009566
Iteration 26/1000 | Loss: 0.00006786
Iteration 27/1000 | Loss: 0.00005516
Iteration 28/1000 | Loss: 0.00016740
Iteration 29/1000 | Loss: 0.00025131
Iteration 30/1000 | Loss: 0.00051427
Iteration 31/1000 | Loss: 0.00058496
Iteration 32/1000 | Loss: 0.00013030
Iteration 33/1000 | Loss: 0.00022901
Iteration 34/1000 | Loss: 0.00018741
Iteration 35/1000 | Loss: 0.00012877
Iteration 36/1000 | Loss: 0.00021091
Iteration 37/1000 | Loss: 0.00012516
Iteration 38/1000 | Loss: 0.00021227
Iteration 39/1000 | Loss: 0.00009059
Iteration 40/1000 | Loss: 0.00005513
Iteration 41/1000 | Loss: 0.00005501
Iteration 42/1000 | Loss: 0.00004756
Iteration 43/1000 | Loss: 0.00005009
Iteration 44/1000 | Loss: 0.00007176
Iteration 45/1000 | Loss: 0.00016831
Iteration 46/1000 | Loss: 0.00040636
Iteration 47/1000 | Loss: 0.00030596
Iteration 48/1000 | Loss: 0.00010535
Iteration 49/1000 | Loss: 0.00012610
Iteration 50/1000 | Loss: 0.00012922
Iteration 51/1000 | Loss: 0.00005339
Iteration 52/1000 | Loss: 0.00004531
Iteration 53/1000 | Loss: 0.00014461
Iteration 54/1000 | Loss: 0.00010656
Iteration 55/1000 | Loss: 0.00010794
Iteration 56/1000 | Loss: 0.00014597
Iteration 57/1000 | Loss: 0.00027080
Iteration 58/1000 | Loss: 0.00020227
Iteration 59/1000 | Loss: 0.00023504
Iteration 60/1000 | Loss: 0.00010771
Iteration 61/1000 | Loss: 0.00007818
Iteration 62/1000 | Loss: 0.00011115
Iteration 63/1000 | Loss: 0.00025227
Iteration 64/1000 | Loss: 0.00038293
Iteration 65/1000 | Loss: 0.00030791
Iteration 66/1000 | Loss: 0.00023558
Iteration 67/1000 | Loss: 0.00022539
Iteration 68/1000 | Loss: 0.00007214
Iteration 69/1000 | Loss: 0.00018805
Iteration 70/1000 | Loss: 0.00081035
Iteration 71/1000 | Loss: 0.00007247
Iteration 72/1000 | Loss: 0.00007148
Iteration 73/1000 | Loss: 0.00018255
Iteration 74/1000 | Loss: 0.00004395
Iteration 75/1000 | Loss: 0.00015183
Iteration 76/1000 | Loss: 0.00013874
Iteration 77/1000 | Loss: 0.00006447
Iteration 78/1000 | Loss: 0.00006304
Iteration 79/1000 | Loss: 0.00005465
Iteration 80/1000 | Loss: 0.00005292
Iteration 81/1000 | Loss: 0.00005322
Iteration 82/1000 | Loss: 0.00005976
Iteration 83/1000 | Loss: 0.00005087
Iteration 84/1000 | Loss: 0.00005690
Iteration 85/1000 | Loss: 0.00020464
Iteration 86/1000 | Loss: 0.00019568
Iteration 87/1000 | Loss: 0.00004415
Iteration 88/1000 | Loss: 0.00004357
Iteration 89/1000 | Loss: 0.00003857
Iteration 90/1000 | Loss: 0.00003469
Iteration 91/1000 | Loss: 0.00003302
Iteration 92/1000 | Loss: 0.00002425
Iteration 93/1000 | Loss: 0.00002919
Iteration 94/1000 | Loss: 0.00002486
Iteration 95/1000 | Loss: 0.00002764
Iteration 96/1000 | Loss: 0.00002385
Iteration 97/1000 | Loss: 0.00003070
Iteration 98/1000 | Loss: 0.00003080
Iteration 99/1000 | Loss: 0.00002770
Iteration 100/1000 | Loss: 0.00003246
Iteration 101/1000 | Loss: 0.00003357
Iteration 102/1000 | Loss: 0.00003428
Iteration 103/1000 | Loss: 0.00003833
Iteration 104/1000 | Loss: 0.00003664
Iteration 105/1000 | Loss: 0.00004106
Iteration 106/1000 | Loss: 0.00006611
Iteration 107/1000 | Loss: 0.00004478
Iteration 108/1000 | Loss: 0.00007327
Iteration 109/1000 | Loss: 0.00004830
Iteration 110/1000 | Loss: 0.00003332
Iteration 111/1000 | Loss: 0.00003103
Iteration 112/1000 | Loss: 0.00003797
Iteration 113/1000 | Loss: 0.00003924
Iteration 114/1000 | Loss: 0.00003834
Iteration 115/1000 | Loss: 0.00003901
Iteration 116/1000 | Loss: 0.00002849
Iteration 117/1000 | Loss: 0.00002362
Iteration 118/1000 | Loss: 0.00002824
Iteration 119/1000 | Loss: 0.00003517
Iteration 120/1000 | Loss: 0.00003555
Iteration 121/1000 | Loss: 0.00002580
Iteration 122/1000 | Loss: 0.00002651
Iteration 123/1000 | Loss: 0.00024407
Iteration 124/1000 | Loss: 0.00003376
Iteration 125/1000 | Loss: 0.00003026
Iteration 126/1000 | Loss: 0.00003038
Iteration 127/1000 | Loss: 0.00003231
Iteration 128/1000 | Loss: 0.00002954
Iteration 129/1000 | Loss: 0.00003785
Iteration 130/1000 | Loss: 0.00003626
Iteration 131/1000 | Loss: 0.00003142
Iteration 132/1000 | Loss: 0.00002918
Iteration 133/1000 | Loss: 0.00003085
Iteration 134/1000 | Loss: 0.00002924
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00003171
Iteration 137/1000 | Loss: 0.00003062
Iteration 138/1000 | Loss: 0.00002926
Iteration 139/1000 | Loss: 0.00003231
Iteration 140/1000 | Loss: 0.00002957
Iteration 141/1000 | Loss: 0.00002670
Iteration 142/1000 | Loss: 0.00021816
Iteration 143/1000 | Loss: 0.00004439
Iteration 144/1000 | Loss: 0.00003084
Iteration 145/1000 | Loss: 0.00002421
Iteration 146/1000 | Loss: 0.00002813
Iteration 147/1000 | Loss: 0.00002224
Iteration 148/1000 | Loss: 0.00002977
Iteration 149/1000 | Loss: 0.00002932
Iteration 150/1000 | Loss: 0.00002171
Iteration 151/1000 | Loss: 0.00003028
Iteration 152/1000 | Loss: 0.00002109
Iteration 153/1000 | Loss: 0.00002726
Iteration 154/1000 | Loss: 0.00003094
Iteration 155/1000 | Loss: 0.00002864
Iteration 156/1000 | Loss: 0.00003348
Iteration 157/1000 | Loss: 0.00002913
Iteration 158/1000 | Loss: 0.00002957
Iteration 159/1000 | Loss: 0.00002069
Iteration 160/1000 | Loss: 0.00002279
Iteration 161/1000 | Loss: 0.00002279
Iteration 162/1000 | Loss: 0.00002078
Iteration 163/1000 | Loss: 0.00002443
Iteration 164/1000 | Loss: 0.00003108
Iteration 165/1000 | Loss: 0.00002436
Iteration 166/1000 | Loss: 0.00003159
Iteration 167/1000 | Loss: 0.00003148
Iteration 168/1000 | Loss: 0.00002732
Iteration 169/1000 | Loss: 0.00003492
Iteration 170/1000 | Loss: 0.00002689
Iteration 171/1000 | Loss: 0.00002918
Iteration 172/1000 | Loss: 0.00003936
Iteration 173/1000 | Loss: 0.00002460
Iteration 174/1000 | Loss: 0.00003060
Iteration 175/1000 | Loss: 0.00003509
Iteration 176/1000 | Loss: 0.00002435
Iteration 177/1000 | Loss: 0.00003428
Iteration 178/1000 | Loss: 0.00004135
Iteration 179/1000 | Loss: 0.00003387
Iteration 180/1000 | Loss: 0.00003469
Iteration 181/1000 | Loss: 0.00002296
Iteration 182/1000 | Loss: 0.00002943
Iteration 183/1000 | Loss: 0.00003835
Iteration 184/1000 | Loss: 0.00003013
Iteration 185/1000 | Loss: 0.00003271
Iteration 186/1000 | Loss: 0.00002729
Iteration 187/1000 | Loss: 0.00003209
Iteration 188/1000 | Loss: 0.00004105
Iteration 189/1000 | Loss: 0.00003384
Iteration 190/1000 | Loss: 0.00004146
Iteration 191/1000 | Loss: 0.00002576
Iteration 192/1000 | Loss: 0.00003664
Iteration 193/1000 | Loss: 0.00002365
Iteration 194/1000 | Loss: 0.00002739
Iteration 195/1000 | Loss: 0.00004311
Iteration 196/1000 | Loss: 0.00002701
Iteration 197/1000 | Loss: 0.00003815
Iteration 198/1000 | Loss: 0.00002769
Iteration 199/1000 | Loss: 0.00003044
Iteration 200/1000 | Loss: 0.00004204
Iteration 201/1000 | Loss: 0.00002638
Iteration 202/1000 | Loss: 0.00003414
Iteration 203/1000 | Loss: 0.00003515
Iteration 204/1000 | Loss: 0.00003587
Iteration 205/1000 | Loss: 0.00002609
Iteration 206/1000 | Loss: 0.00002897
Iteration 207/1000 | Loss: 0.00003294
Iteration 208/1000 | Loss: 0.00003388
Iteration 209/1000 | Loss: 0.00002890
Iteration 210/1000 | Loss: 0.00003342
Iteration 211/1000 | Loss: 0.00003473
Iteration 212/1000 | Loss: 0.00002185
Iteration 213/1000 | Loss: 0.00004103
Iteration 214/1000 | Loss: 0.00002094
Iteration 215/1000 | Loss: 0.00006022
Iteration 216/1000 | Loss: 0.00001910
Iteration 217/1000 | Loss: 0.00001880
Iteration 218/1000 | Loss: 0.00001843
Iteration 219/1000 | Loss: 0.00001822
Iteration 220/1000 | Loss: 0.00001806
Iteration 221/1000 | Loss: 0.00001802
Iteration 222/1000 | Loss: 0.00001801
Iteration 223/1000 | Loss: 0.00001796
Iteration 224/1000 | Loss: 0.00001795
Iteration 225/1000 | Loss: 0.00001794
Iteration 226/1000 | Loss: 0.00001793
Iteration 227/1000 | Loss: 0.00001793
Iteration 228/1000 | Loss: 0.00001792
Iteration 229/1000 | Loss: 0.00001792
Iteration 230/1000 | Loss: 0.00001792
Iteration 231/1000 | Loss: 0.00001791
Iteration 232/1000 | Loss: 0.00001791
Iteration 233/1000 | Loss: 0.00001790
Iteration 234/1000 | Loss: 0.00001790
Iteration 235/1000 | Loss: 0.00001789
Iteration 236/1000 | Loss: 0.00001788
Iteration 237/1000 | Loss: 0.00001787
Iteration 238/1000 | Loss: 0.00001787
Iteration 239/1000 | Loss: 0.00001786
Iteration 240/1000 | Loss: 0.00001786
Iteration 241/1000 | Loss: 0.00001785
Iteration 242/1000 | Loss: 0.00001785
Iteration 243/1000 | Loss: 0.00001785
Iteration 244/1000 | Loss: 0.00001784
Iteration 245/1000 | Loss: 0.00001780
Iteration 246/1000 | Loss: 0.00001780
Iteration 247/1000 | Loss: 0.00001777
Iteration 248/1000 | Loss: 0.00001777
Iteration 249/1000 | Loss: 0.00001777
Iteration 250/1000 | Loss: 0.00001776
Iteration 251/1000 | Loss: 0.00001776
Iteration 252/1000 | Loss: 0.00001776
Iteration 253/1000 | Loss: 0.00001776
Iteration 254/1000 | Loss: 0.00001776
Iteration 255/1000 | Loss: 0.00001776
Iteration 256/1000 | Loss: 0.00001775
Iteration 257/1000 | Loss: 0.00001775
Iteration 258/1000 | Loss: 0.00001775
Iteration 259/1000 | Loss: 0.00001775
Iteration 260/1000 | Loss: 0.00001775
Iteration 261/1000 | Loss: 0.00001774
Iteration 262/1000 | Loss: 0.00001774
Iteration 263/1000 | Loss: 0.00001774
Iteration 264/1000 | Loss: 0.00001774
Iteration 265/1000 | Loss: 0.00001774
Iteration 266/1000 | Loss: 0.00001773
Iteration 267/1000 | Loss: 0.00001773
Iteration 268/1000 | Loss: 0.00001773
Iteration 269/1000 | Loss: 0.00001773
Iteration 270/1000 | Loss: 0.00001772
Iteration 271/1000 | Loss: 0.00001772
Iteration 272/1000 | Loss: 0.00001772
Iteration 273/1000 | Loss: 0.00001772
Iteration 274/1000 | Loss: 0.00001772
Iteration 275/1000 | Loss: 0.00001772
Iteration 276/1000 | Loss: 0.00001771
Iteration 277/1000 | Loss: 0.00001771
Iteration 278/1000 | Loss: 0.00001771
Iteration 279/1000 | Loss: 0.00001771
Iteration 280/1000 | Loss: 0.00001771
Iteration 281/1000 | Loss: 0.00001771
Iteration 282/1000 | Loss: 0.00001771
Iteration 283/1000 | Loss: 0.00001771
Iteration 284/1000 | Loss: 0.00001770
Iteration 285/1000 | Loss: 0.00001770
Iteration 286/1000 | Loss: 0.00001770
Iteration 287/1000 | Loss: 0.00001770
Iteration 288/1000 | Loss: 0.00001770
Iteration 289/1000 | Loss: 0.00001770
Iteration 290/1000 | Loss: 0.00001770
Iteration 291/1000 | Loss: 0.00001770
Iteration 292/1000 | Loss: 0.00001770
Iteration 293/1000 | Loss: 0.00001770
Iteration 294/1000 | Loss: 0.00001769
Iteration 295/1000 | Loss: 0.00001769
Iteration 296/1000 | Loss: 0.00001769
Iteration 297/1000 | Loss: 0.00001769
Iteration 298/1000 | Loss: 0.00001769
Iteration 299/1000 | Loss: 0.00001769
Iteration 300/1000 | Loss: 0.00001769
Iteration 301/1000 | Loss: 0.00001769
Iteration 302/1000 | Loss: 0.00001769
Iteration 303/1000 | Loss: 0.00001769
Iteration 304/1000 | Loss: 0.00001768
Iteration 305/1000 | Loss: 0.00001768
Iteration 306/1000 | Loss: 0.00001768
Iteration 307/1000 | Loss: 0.00001768
Iteration 308/1000 | Loss: 0.00001768
Iteration 309/1000 | Loss: 0.00001768
Iteration 310/1000 | Loss: 0.00001768
Iteration 311/1000 | Loss: 0.00001768
Iteration 312/1000 | Loss: 0.00001768
Iteration 313/1000 | Loss: 0.00001768
Iteration 314/1000 | Loss: 0.00001767
Iteration 315/1000 | Loss: 0.00001767
Iteration 316/1000 | Loss: 0.00001767
Iteration 317/1000 | Loss: 0.00001767
Iteration 318/1000 | Loss: 0.00001766
Iteration 319/1000 | Loss: 0.00001766
Iteration 320/1000 | Loss: 0.00001766
Iteration 321/1000 | Loss: 0.00001766
Iteration 322/1000 | Loss: 0.00001766
Iteration 323/1000 | Loss: 0.00001766
Iteration 324/1000 | Loss: 0.00001766
Iteration 325/1000 | Loss: 0.00001766
Iteration 326/1000 | Loss: 0.00001766
Iteration 327/1000 | Loss: 0.00001766
Iteration 328/1000 | Loss: 0.00001766
Iteration 329/1000 | Loss: 0.00001766
Iteration 330/1000 | Loss: 0.00001765
Iteration 331/1000 | Loss: 0.00001765
Iteration 332/1000 | Loss: 0.00001765
Iteration 333/1000 | Loss: 0.00001765
Iteration 334/1000 | Loss: 0.00001765
Iteration 335/1000 | Loss: 0.00001765
Iteration 336/1000 | Loss: 0.00001765
Iteration 337/1000 | Loss: 0.00001765
Iteration 338/1000 | Loss: 0.00001765
Iteration 339/1000 | Loss: 0.00001765
Iteration 340/1000 | Loss: 0.00001764
Iteration 341/1000 | Loss: 0.00001764
Iteration 342/1000 | Loss: 0.00001764
Iteration 343/1000 | Loss: 0.00001764
Iteration 344/1000 | Loss: 0.00001764
Iteration 345/1000 | Loss: 0.00001764
Iteration 346/1000 | Loss: 0.00001764
Iteration 347/1000 | Loss: 0.00001764
Iteration 348/1000 | Loss: 0.00001764
Iteration 349/1000 | Loss: 0.00001764
Iteration 350/1000 | Loss: 0.00001764
Iteration 351/1000 | Loss: 0.00001764
Iteration 352/1000 | Loss: 0.00001764
Iteration 353/1000 | Loss: 0.00001764
Iteration 354/1000 | Loss: 0.00001764
Iteration 355/1000 | Loss: 0.00001764
Iteration 356/1000 | Loss: 0.00001764
Iteration 357/1000 | Loss: 0.00001763
Iteration 358/1000 | Loss: 0.00001763
Iteration 359/1000 | Loss: 0.00001763
Iteration 360/1000 | Loss: 0.00001763
Iteration 361/1000 | Loss: 0.00001763
Iteration 362/1000 | Loss: 0.00001763
Iteration 363/1000 | Loss: 0.00001763
Iteration 364/1000 | Loss: 0.00001763
Iteration 365/1000 | Loss: 0.00001763
Iteration 366/1000 | Loss: 0.00001763
Iteration 367/1000 | Loss: 0.00001763
Iteration 368/1000 | Loss: 0.00001763
Iteration 369/1000 | Loss: 0.00001763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 369. Stopping optimization.
Last 5 losses: [1.763264117471408e-05, 1.763264117471408e-05, 1.763264117471408e-05, 1.763264117471408e-05, 1.763264117471408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.763264117471408e-05

Optimization complete. Final v2v error: 3.4008352756500244 mm

Highest mean error: 5.172740936279297 mm for frame 98

Lowest mean error: 3.0883424282073975 mm for frame 86

Saving results

Total time: 421.46468353271484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789046
Iteration 2/25 | Loss: 0.00136268
Iteration 3/25 | Loss: 0.00128407
Iteration 4/25 | Loss: 0.00127143
Iteration 5/25 | Loss: 0.00126899
Iteration 6/25 | Loss: 0.00126899
Iteration 7/25 | Loss: 0.00126899
Iteration 8/25 | Loss: 0.00126899
Iteration 9/25 | Loss: 0.00126899
Iteration 10/25 | Loss: 0.00126899
Iteration 11/25 | Loss: 0.00126899
Iteration 12/25 | Loss: 0.00126899
Iteration 13/25 | Loss: 0.00126899
Iteration 14/25 | Loss: 0.00126899
Iteration 15/25 | Loss: 0.00126899
Iteration 16/25 | Loss: 0.00126899
Iteration 17/25 | Loss: 0.00126899
Iteration 18/25 | Loss: 0.00126899
Iteration 19/25 | Loss: 0.00126899
Iteration 20/25 | Loss: 0.00126899
Iteration 21/25 | Loss: 0.00126899
Iteration 22/25 | Loss: 0.00126899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012689926661550999, 0.0012689926661550999, 0.0012689926661550999, 0.0012689926661550999, 0.0012689926661550999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012689926661550999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44514787
Iteration 2/25 | Loss: 0.00081965
Iteration 3/25 | Loss: 0.00081964
Iteration 4/25 | Loss: 0.00081964
Iteration 5/25 | Loss: 0.00081964
Iteration 6/25 | Loss: 0.00081964
Iteration 7/25 | Loss: 0.00081964
Iteration 8/25 | Loss: 0.00081964
Iteration 9/25 | Loss: 0.00081964
Iteration 10/25 | Loss: 0.00081964
Iteration 11/25 | Loss: 0.00081964
Iteration 12/25 | Loss: 0.00081964
Iteration 13/25 | Loss: 0.00081964
Iteration 14/25 | Loss: 0.00081964
Iteration 15/25 | Loss: 0.00081964
Iteration 16/25 | Loss: 0.00081964
Iteration 17/25 | Loss: 0.00081964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008196356939151883, 0.0008196356939151883, 0.0008196356939151883, 0.0008196356939151883, 0.0008196356939151883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008196356939151883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081964
Iteration 2/1000 | Loss: 0.00002768
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001947
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001781
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001727
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001591
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001590
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001590
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001578
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001577
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001576
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001568
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001563
Iteration 69/1000 | Loss: 0.00001563
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001560
Iteration 75/1000 | Loss: 0.00001560
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001557
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001551
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001551
Iteration 109/1000 | Loss: 0.00001551
Iteration 110/1000 | Loss: 0.00001551
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001551
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.5507621355936863e-05, 1.5507621355936863e-05, 1.5507621355936863e-05, 1.5507621355936863e-05, 1.5507621355936863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5507621355936863e-05

Optimization complete. Final v2v error: 3.3908114433288574 mm

Highest mean error: 3.6819543838500977 mm for frame 155

Lowest mean error: 3.1987955570220947 mm for frame 65

Saving results

Total time: 42.003467321395874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595878
Iteration 2/25 | Loss: 0.00144069
Iteration 3/25 | Loss: 0.00130800
Iteration 4/25 | Loss: 0.00129449
Iteration 5/25 | Loss: 0.00129169
Iteration 6/25 | Loss: 0.00129169
Iteration 7/25 | Loss: 0.00129169
Iteration 8/25 | Loss: 0.00129169
Iteration 9/25 | Loss: 0.00129169
Iteration 10/25 | Loss: 0.00129169
Iteration 11/25 | Loss: 0.00129169
Iteration 12/25 | Loss: 0.00129169
Iteration 13/25 | Loss: 0.00129169
Iteration 14/25 | Loss: 0.00129169
Iteration 15/25 | Loss: 0.00129169
Iteration 16/25 | Loss: 0.00129169
Iteration 17/25 | Loss: 0.00129169
Iteration 18/25 | Loss: 0.00129169
Iteration 19/25 | Loss: 0.00129169
Iteration 20/25 | Loss: 0.00129169
Iteration 21/25 | Loss: 0.00129169
Iteration 22/25 | Loss: 0.00129169
Iteration 23/25 | Loss: 0.00129169
Iteration 24/25 | Loss: 0.00129169
Iteration 25/25 | Loss: 0.00129169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58823419
Iteration 2/25 | Loss: 0.00072246
Iteration 3/25 | Loss: 0.00072245
Iteration 4/25 | Loss: 0.00072245
Iteration 5/25 | Loss: 0.00072245
Iteration 6/25 | Loss: 0.00072245
Iteration 7/25 | Loss: 0.00072245
Iteration 8/25 | Loss: 0.00072245
Iteration 9/25 | Loss: 0.00072245
Iteration 10/25 | Loss: 0.00072245
Iteration 11/25 | Loss: 0.00072245
Iteration 12/25 | Loss: 0.00072245
Iteration 13/25 | Loss: 0.00072245
Iteration 14/25 | Loss: 0.00072245
Iteration 15/25 | Loss: 0.00072245
Iteration 16/25 | Loss: 0.00072245
Iteration 17/25 | Loss: 0.00072245
Iteration 18/25 | Loss: 0.00072245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007224477594718337, 0.0007224477594718337, 0.0007224477594718337, 0.0007224477594718337, 0.0007224477594718337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007224477594718337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072245
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00001941
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001668
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001599
Iteration 9/1000 | Loss: 0.00001591
Iteration 10/1000 | Loss: 0.00001590
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001580
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001534
Iteration 22/1000 | Loss: 0.00001519
Iteration 23/1000 | Loss: 0.00001518
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001512
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001500
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001497
Iteration 56/1000 | Loss: 0.00001497
Iteration 57/1000 | Loss: 0.00001497
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001495
Iteration 64/1000 | Loss: 0.00001495
Iteration 65/1000 | Loss: 0.00001494
Iteration 66/1000 | Loss: 0.00001494
Iteration 67/1000 | Loss: 0.00001494
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001493
Iteration 70/1000 | Loss: 0.00001493
Iteration 71/1000 | Loss: 0.00001493
Iteration 72/1000 | Loss: 0.00001493
Iteration 73/1000 | Loss: 0.00001493
Iteration 74/1000 | Loss: 0.00001492
Iteration 75/1000 | Loss: 0.00001492
Iteration 76/1000 | Loss: 0.00001492
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001491
Iteration 79/1000 | Loss: 0.00001491
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001489
Iteration 83/1000 | Loss: 0.00001489
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001486
Iteration 91/1000 | Loss: 0.00001486
Iteration 92/1000 | Loss: 0.00001486
Iteration 93/1000 | Loss: 0.00001486
Iteration 94/1000 | Loss: 0.00001486
Iteration 95/1000 | Loss: 0.00001486
Iteration 96/1000 | Loss: 0.00001486
Iteration 97/1000 | Loss: 0.00001486
Iteration 98/1000 | Loss: 0.00001486
Iteration 99/1000 | Loss: 0.00001486
Iteration 100/1000 | Loss: 0.00001485
Iteration 101/1000 | Loss: 0.00001485
Iteration 102/1000 | Loss: 0.00001485
Iteration 103/1000 | Loss: 0.00001485
Iteration 104/1000 | Loss: 0.00001485
Iteration 105/1000 | Loss: 0.00001485
Iteration 106/1000 | Loss: 0.00001485
Iteration 107/1000 | Loss: 0.00001485
Iteration 108/1000 | Loss: 0.00001484
Iteration 109/1000 | Loss: 0.00001484
Iteration 110/1000 | Loss: 0.00001484
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001482
Iteration 117/1000 | Loss: 0.00001482
Iteration 118/1000 | Loss: 0.00001482
Iteration 119/1000 | Loss: 0.00001482
Iteration 120/1000 | Loss: 0.00001482
Iteration 121/1000 | Loss: 0.00001482
Iteration 122/1000 | Loss: 0.00001481
Iteration 123/1000 | Loss: 0.00001481
Iteration 124/1000 | Loss: 0.00001481
Iteration 125/1000 | Loss: 0.00001481
Iteration 126/1000 | Loss: 0.00001481
Iteration 127/1000 | Loss: 0.00001480
Iteration 128/1000 | Loss: 0.00001480
Iteration 129/1000 | Loss: 0.00001480
Iteration 130/1000 | Loss: 0.00001480
Iteration 131/1000 | Loss: 0.00001480
Iteration 132/1000 | Loss: 0.00001480
Iteration 133/1000 | Loss: 0.00001480
Iteration 134/1000 | Loss: 0.00001480
Iteration 135/1000 | Loss: 0.00001479
Iteration 136/1000 | Loss: 0.00001479
Iteration 137/1000 | Loss: 0.00001479
Iteration 138/1000 | Loss: 0.00001479
Iteration 139/1000 | Loss: 0.00001479
Iteration 140/1000 | Loss: 0.00001479
Iteration 141/1000 | Loss: 0.00001479
Iteration 142/1000 | Loss: 0.00001479
Iteration 143/1000 | Loss: 0.00001479
Iteration 144/1000 | Loss: 0.00001479
Iteration 145/1000 | Loss: 0.00001479
Iteration 146/1000 | Loss: 0.00001479
Iteration 147/1000 | Loss: 0.00001479
Iteration 148/1000 | Loss: 0.00001479
Iteration 149/1000 | Loss: 0.00001479
Iteration 150/1000 | Loss: 0.00001479
Iteration 151/1000 | Loss: 0.00001479
Iteration 152/1000 | Loss: 0.00001479
Iteration 153/1000 | Loss: 0.00001479
Iteration 154/1000 | Loss: 0.00001479
Iteration 155/1000 | Loss: 0.00001479
Iteration 156/1000 | Loss: 0.00001479
Iteration 157/1000 | Loss: 0.00001479
Iteration 158/1000 | Loss: 0.00001479
Iteration 159/1000 | Loss: 0.00001479
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.4789202396059409e-05, 1.4789202396059409e-05, 1.4789202396059409e-05, 1.4789202396059409e-05, 1.4789202396059409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4789202396059409e-05

Optimization complete. Final v2v error: 3.2414090633392334 mm

Highest mean error: 3.4338905811309814 mm for frame 107

Lowest mean error: 3.053946018218994 mm for frame 73

Saving results

Total time: 43.111788272857666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591697
Iteration 2/25 | Loss: 0.00133113
Iteration 3/25 | Loss: 0.00126192
Iteration 4/25 | Loss: 0.00125266
Iteration 5/25 | Loss: 0.00124962
Iteration 6/25 | Loss: 0.00124938
Iteration 7/25 | Loss: 0.00124938
Iteration 8/25 | Loss: 0.00124938
Iteration 9/25 | Loss: 0.00124938
Iteration 10/25 | Loss: 0.00124938
Iteration 11/25 | Loss: 0.00124938
Iteration 12/25 | Loss: 0.00124938
Iteration 13/25 | Loss: 0.00124938
Iteration 14/25 | Loss: 0.00124938
Iteration 15/25 | Loss: 0.00124938
Iteration 16/25 | Loss: 0.00124938
Iteration 17/25 | Loss: 0.00124938
Iteration 18/25 | Loss: 0.00124938
Iteration 19/25 | Loss: 0.00124938
Iteration 20/25 | Loss: 0.00124938
Iteration 21/25 | Loss: 0.00124938
Iteration 22/25 | Loss: 0.00124938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012493765680119395, 0.0012493765680119395, 0.0012493765680119395, 0.0012493765680119395, 0.0012493765680119395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012493765680119395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.10026073
Iteration 2/25 | Loss: 0.00085688
Iteration 3/25 | Loss: 0.00085688
Iteration 4/25 | Loss: 0.00085688
Iteration 5/25 | Loss: 0.00085688
Iteration 6/25 | Loss: 0.00085688
Iteration 7/25 | Loss: 0.00085688
Iteration 8/25 | Loss: 0.00085688
Iteration 9/25 | Loss: 0.00085688
Iteration 10/25 | Loss: 0.00085688
Iteration 11/25 | Loss: 0.00085688
Iteration 12/25 | Loss: 0.00085688
Iteration 13/25 | Loss: 0.00085688
Iteration 14/25 | Loss: 0.00085688
Iteration 15/25 | Loss: 0.00085688
Iteration 16/25 | Loss: 0.00085688
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008568806806579232, 0.0008568806806579232, 0.0008568806806579232, 0.0008568806806579232, 0.0008568806806579232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008568806806579232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085688
Iteration 2/1000 | Loss: 0.00002449
Iteration 3/1000 | Loss: 0.00001791
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001435
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001373
Iteration 9/1000 | Loss: 0.00001340
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001319
Iteration 12/1000 | Loss: 0.00001316
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001273
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001267
Iteration 24/1000 | Loss: 0.00001266
Iteration 25/1000 | Loss: 0.00001265
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001240
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001232
Iteration 66/1000 | Loss: 0.00001231
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001230
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001224
Iteration 88/1000 | Loss: 0.00001224
Iteration 89/1000 | Loss: 0.00001224
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001219
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001219
Iteration 142/1000 | Loss: 0.00001219
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001217
Iteration 153/1000 | Loss: 0.00001217
Iteration 154/1000 | Loss: 0.00001217
Iteration 155/1000 | Loss: 0.00001217
Iteration 156/1000 | Loss: 0.00001217
Iteration 157/1000 | Loss: 0.00001217
Iteration 158/1000 | Loss: 0.00001216
Iteration 159/1000 | Loss: 0.00001216
Iteration 160/1000 | Loss: 0.00001216
Iteration 161/1000 | Loss: 0.00001216
Iteration 162/1000 | Loss: 0.00001216
Iteration 163/1000 | Loss: 0.00001216
Iteration 164/1000 | Loss: 0.00001216
Iteration 165/1000 | Loss: 0.00001216
Iteration 166/1000 | Loss: 0.00001216
Iteration 167/1000 | Loss: 0.00001215
Iteration 168/1000 | Loss: 0.00001215
Iteration 169/1000 | Loss: 0.00001215
Iteration 170/1000 | Loss: 0.00001215
Iteration 171/1000 | Loss: 0.00001215
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001214
Iteration 182/1000 | Loss: 0.00001214
Iteration 183/1000 | Loss: 0.00001214
Iteration 184/1000 | Loss: 0.00001213
Iteration 185/1000 | Loss: 0.00001213
Iteration 186/1000 | Loss: 0.00001213
Iteration 187/1000 | Loss: 0.00001213
Iteration 188/1000 | Loss: 0.00001212
Iteration 189/1000 | Loss: 0.00001212
Iteration 190/1000 | Loss: 0.00001212
Iteration 191/1000 | Loss: 0.00001212
Iteration 192/1000 | Loss: 0.00001212
Iteration 193/1000 | Loss: 0.00001212
Iteration 194/1000 | Loss: 0.00001212
Iteration 195/1000 | Loss: 0.00001211
Iteration 196/1000 | Loss: 0.00001211
Iteration 197/1000 | Loss: 0.00001211
Iteration 198/1000 | Loss: 0.00001211
Iteration 199/1000 | Loss: 0.00001211
Iteration 200/1000 | Loss: 0.00001211
Iteration 201/1000 | Loss: 0.00001211
Iteration 202/1000 | Loss: 0.00001211
Iteration 203/1000 | Loss: 0.00001211
Iteration 204/1000 | Loss: 0.00001211
Iteration 205/1000 | Loss: 0.00001210
Iteration 206/1000 | Loss: 0.00001210
Iteration 207/1000 | Loss: 0.00001210
Iteration 208/1000 | Loss: 0.00001210
Iteration 209/1000 | Loss: 0.00001210
Iteration 210/1000 | Loss: 0.00001210
Iteration 211/1000 | Loss: 0.00001210
Iteration 212/1000 | Loss: 0.00001210
Iteration 213/1000 | Loss: 0.00001210
Iteration 214/1000 | Loss: 0.00001210
Iteration 215/1000 | Loss: 0.00001210
Iteration 216/1000 | Loss: 0.00001210
Iteration 217/1000 | Loss: 0.00001210
Iteration 218/1000 | Loss: 0.00001210
Iteration 219/1000 | Loss: 0.00001210
Iteration 220/1000 | Loss: 0.00001210
Iteration 221/1000 | Loss: 0.00001210
Iteration 222/1000 | Loss: 0.00001210
Iteration 223/1000 | Loss: 0.00001210
Iteration 224/1000 | Loss: 0.00001210
Iteration 225/1000 | Loss: 0.00001210
Iteration 226/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.2104501365683973e-05, 1.2104501365683973e-05, 1.2104501365683973e-05, 1.2104501365683973e-05, 1.2104501365683973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2104501365683973e-05

Optimization complete. Final v2v error: 2.9933338165283203 mm

Highest mean error: 3.277902364730835 mm for frame 112

Lowest mean error: 2.8286349773406982 mm for frame 81

Saving results

Total time: 44.7001416683197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390426
Iteration 2/25 | Loss: 0.00140560
Iteration 3/25 | Loss: 0.00130407
Iteration 4/25 | Loss: 0.00128550
Iteration 5/25 | Loss: 0.00127943
Iteration 6/25 | Loss: 0.00127822
Iteration 7/25 | Loss: 0.00127810
Iteration 8/25 | Loss: 0.00127810
Iteration 9/25 | Loss: 0.00127810
Iteration 10/25 | Loss: 0.00127810
Iteration 11/25 | Loss: 0.00127810
Iteration 12/25 | Loss: 0.00127810
Iteration 13/25 | Loss: 0.00127810
Iteration 14/25 | Loss: 0.00127810
Iteration 15/25 | Loss: 0.00127810
Iteration 16/25 | Loss: 0.00127810
Iteration 17/25 | Loss: 0.00127810
Iteration 18/25 | Loss: 0.00127810
Iteration 19/25 | Loss: 0.00127810
Iteration 20/25 | Loss: 0.00127810
Iteration 21/25 | Loss: 0.00127810
Iteration 22/25 | Loss: 0.00127810
Iteration 23/25 | Loss: 0.00127810
Iteration 24/25 | Loss: 0.00127810
Iteration 25/25 | Loss: 0.00127810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36427200
Iteration 2/25 | Loss: 0.00081402
Iteration 3/25 | Loss: 0.00081402
Iteration 4/25 | Loss: 0.00081402
Iteration 5/25 | Loss: 0.00081402
Iteration 6/25 | Loss: 0.00081402
Iteration 7/25 | Loss: 0.00081402
Iteration 8/25 | Loss: 0.00081402
Iteration 9/25 | Loss: 0.00081402
Iteration 10/25 | Loss: 0.00081402
Iteration 11/25 | Loss: 0.00081402
Iteration 12/25 | Loss: 0.00081402
Iteration 13/25 | Loss: 0.00081402
Iteration 14/25 | Loss: 0.00081402
Iteration 15/25 | Loss: 0.00081402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008140168501995504, 0.0008140168501995504, 0.0008140168501995504, 0.0008140168501995504, 0.0008140168501995504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008140168501995504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081402
Iteration 2/1000 | Loss: 0.00005595
Iteration 3/1000 | Loss: 0.00003759
Iteration 4/1000 | Loss: 0.00002981
Iteration 5/1000 | Loss: 0.00002657
Iteration 6/1000 | Loss: 0.00002490
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00002162
Iteration 10/1000 | Loss: 0.00002113
Iteration 11/1000 | Loss: 0.00002081
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002011
Iteration 15/1000 | Loss: 0.00002007
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001973
Iteration 20/1000 | Loss: 0.00001971
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001950
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001950
Iteration 34/1000 | Loss: 0.00001950
Iteration 35/1000 | Loss: 0.00001949
Iteration 36/1000 | Loss: 0.00001949
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001948
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001948
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.945918302226346e-05, 1.945918302226346e-05, 1.945918302226346e-05, 1.945918302226346e-05, 1.945918302226346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945918302226346e-05

Optimization complete. Final v2v error: 3.737577438354492 mm

Highest mean error: 4.418720722198486 mm for frame 69

Lowest mean error: 3.281416416168213 mm for frame 83

Saving results

Total time: 39.54680013656616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879342
Iteration 2/25 | Loss: 0.00231269
Iteration 3/25 | Loss: 0.00150635
Iteration 4/25 | Loss: 0.00144536
Iteration 5/25 | Loss: 0.00141035
Iteration 6/25 | Loss: 0.00138608
Iteration 7/25 | Loss: 0.00134485
Iteration 8/25 | Loss: 0.00131827
Iteration 9/25 | Loss: 0.00129361
Iteration 10/25 | Loss: 0.00128526
Iteration 11/25 | Loss: 0.00128648
Iteration 12/25 | Loss: 0.00128564
Iteration 13/25 | Loss: 0.00127674
Iteration 14/25 | Loss: 0.00127591
Iteration 15/25 | Loss: 0.00127192
Iteration 16/25 | Loss: 0.00127656
Iteration 17/25 | Loss: 0.00127286
Iteration 18/25 | Loss: 0.00127067
Iteration 19/25 | Loss: 0.00127067
Iteration 20/25 | Loss: 0.00127067
Iteration 21/25 | Loss: 0.00127067
Iteration 22/25 | Loss: 0.00127067
Iteration 23/25 | Loss: 0.00127067
Iteration 24/25 | Loss: 0.00127067
Iteration 25/25 | Loss: 0.00127066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01006007
Iteration 2/25 | Loss: 0.00090110
Iteration 3/25 | Loss: 0.00088088
Iteration 4/25 | Loss: 0.00088088
Iteration 5/25 | Loss: 0.00088088
Iteration 6/25 | Loss: 0.00088088
Iteration 7/25 | Loss: 0.00088088
Iteration 8/25 | Loss: 0.00088088
Iteration 9/25 | Loss: 0.00088088
Iteration 10/25 | Loss: 0.00088088
Iteration 11/25 | Loss: 0.00088088
Iteration 12/25 | Loss: 0.00088088
Iteration 13/25 | Loss: 0.00088088
Iteration 14/25 | Loss: 0.00088088
Iteration 15/25 | Loss: 0.00088088
Iteration 16/25 | Loss: 0.00088088
Iteration 17/25 | Loss: 0.00088088
Iteration 18/25 | Loss: 0.00088088
Iteration 19/25 | Loss: 0.00088088
Iteration 20/25 | Loss: 0.00088088
Iteration 21/25 | Loss: 0.00088088
Iteration 22/25 | Loss: 0.00088088
Iteration 23/25 | Loss: 0.00088088
Iteration 24/25 | Loss: 0.00088088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008808792335912585, 0.0008808792335912585, 0.0008808792335912585, 0.0008808792335912585, 0.0008808792335912585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008808792335912585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088088
Iteration 2/1000 | Loss: 0.00007553
Iteration 3/1000 | Loss: 0.00011896
Iteration 4/1000 | Loss: 0.00009596
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00006065
Iteration 7/1000 | Loss: 0.00008314
Iteration 8/1000 | Loss: 0.00047691
Iteration 9/1000 | Loss: 0.00003761
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001542
Iteration 12/1000 | Loss: 0.00007889
Iteration 13/1000 | Loss: 0.00005134
Iteration 14/1000 | Loss: 0.00005006
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00004920
Iteration 22/1000 | Loss: 0.00001427
Iteration 23/1000 | Loss: 0.00001426
Iteration 24/1000 | Loss: 0.00004782
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001417
Iteration 28/1000 | Loss: 0.00001417
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001410
Iteration 42/1000 | Loss: 0.00001410
Iteration 43/1000 | Loss: 0.00001409
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00008103
Iteration 52/1000 | Loss: 0.00009134
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00003934
Iteration 83/1000 | Loss: 0.00011310
Iteration 84/1000 | Loss: 0.00001891
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001384
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00002933
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001381
Iteration 92/1000 | Loss: 0.00003916
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001377
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001375
Iteration 108/1000 | Loss: 0.00001375
Iteration 109/1000 | Loss: 0.00001374
Iteration 110/1000 | Loss: 0.00001374
Iteration 111/1000 | Loss: 0.00001373
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001371
Iteration 116/1000 | Loss: 0.00001371
Iteration 117/1000 | Loss: 0.00001371
Iteration 118/1000 | Loss: 0.00001371
Iteration 119/1000 | Loss: 0.00001371
Iteration 120/1000 | Loss: 0.00001371
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001370
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001370
Iteration 128/1000 | Loss: 0.00001370
Iteration 129/1000 | Loss: 0.00001370
Iteration 130/1000 | Loss: 0.00001370
Iteration 131/1000 | Loss: 0.00001370
Iteration 132/1000 | Loss: 0.00001370
Iteration 133/1000 | Loss: 0.00001370
Iteration 134/1000 | Loss: 0.00001370
Iteration 135/1000 | Loss: 0.00001370
Iteration 136/1000 | Loss: 0.00001370
Iteration 137/1000 | Loss: 0.00001370
Iteration 138/1000 | Loss: 0.00001370
Iteration 139/1000 | Loss: 0.00001370
Iteration 140/1000 | Loss: 0.00001370
Iteration 141/1000 | Loss: 0.00001370
Iteration 142/1000 | Loss: 0.00001370
Iteration 143/1000 | Loss: 0.00001370
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001370
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001370
Iteration 149/1000 | Loss: 0.00001370
Iteration 150/1000 | Loss: 0.00001370
Iteration 151/1000 | Loss: 0.00001370
Iteration 152/1000 | Loss: 0.00001370
Iteration 153/1000 | Loss: 0.00001370
Iteration 154/1000 | Loss: 0.00001370
Iteration 155/1000 | Loss: 0.00001370
Iteration 156/1000 | Loss: 0.00001370
Iteration 157/1000 | Loss: 0.00001370
Iteration 158/1000 | Loss: 0.00001370
Iteration 159/1000 | Loss: 0.00001370
Iteration 160/1000 | Loss: 0.00001370
Iteration 161/1000 | Loss: 0.00001370
Iteration 162/1000 | Loss: 0.00001370
Iteration 163/1000 | Loss: 0.00001370
Iteration 164/1000 | Loss: 0.00001370
Iteration 165/1000 | Loss: 0.00001370
Iteration 166/1000 | Loss: 0.00001370
Iteration 167/1000 | Loss: 0.00001370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3704117918678094e-05, 1.3704117918678094e-05, 1.3704117918678094e-05, 1.3704117918678094e-05, 1.3704117918678094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3704117918678094e-05

Optimization complete. Final v2v error: 3.1560394763946533 mm

Highest mean error: 3.5995776653289795 mm for frame 129

Lowest mean error: 2.7406163215637207 mm for frame 0

Saving results

Total time: 102.58437132835388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067646
Iteration 2/25 | Loss: 0.01067646
Iteration 3/25 | Loss: 0.00268812
Iteration 4/25 | Loss: 0.00174098
Iteration 5/25 | Loss: 0.00150621
Iteration 6/25 | Loss: 0.00148297
Iteration 7/25 | Loss: 0.00147699
Iteration 8/25 | Loss: 0.00151499
Iteration 9/25 | Loss: 0.00144327
Iteration 10/25 | Loss: 0.00142497
Iteration 11/25 | Loss: 0.00142192
Iteration 12/25 | Loss: 0.00141715
Iteration 13/25 | Loss: 0.00141913
Iteration 14/25 | Loss: 0.00142523
Iteration 15/25 | Loss: 0.00142151
Iteration 16/25 | Loss: 0.00142320
Iteration 17/25 | Loss: 0.00142219
Iteration 18/25 | Loss: 0.00142136
Iteration 19/25 | Loss: 0.00141922
Iteration 20/25 | Loss: 0.00142622
Iteration 21/25 | Loss: 0.00142301
Iteration 22/25 | Loss: 0.00141863
Iteration 23/25 | Loss: 0.00142040
Iteration 24/25 | Loss: 0.00141919
Iteration 25/25 | Loss: 0.00141484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43892515
Iteration 2/25 | Loss: 0.00124272
Iteration 3/25 | Loss: 0.00124271
Iteration 4/25 | Loss: 0.00124271
Iteration 5/25 | Loss: 0.00124271
Iteration 6/25 | Loss: 0.00124271
Iteration 7/25 | Loss: 0.00124271
Iteration 8/25 | Loss: 0.00124271
Iteration 9/25 | Loss: 0.00124271
Iteration 10/25 | Loss: 0.00124271
Iteration 11/25 | Loss: 0.00124271
Iteration 12/25 | Loss: 0.00124271
Iteration 13/25 | Loss: 0.00124271
Iteration 14/25 | Loss: 0.00124271
Iteration 15/25 | Loss: 0.00124271
Iteration 16/25 | Loss: 0.00124271
Iteration 17/25 | Loss: 0.00124271
Iteration 18/25 | Loss: 0.00124271
Iteration 19/25 | Loss: 0.00124271
Iteration 20/25 | Loss: 0.00124271
Iteration 21/25 | Loss: 0.00124271
Iteration 22/25 | Loss: 0.00124271
Iteration 23/25 | Loss: 0.00124271
Iteration 24/25 | Loss: 0.00124271
Iteration 25/25 | Loss: 0.00124271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124271
Iteration 2/1000 | Loss: 0.00037011
Iteration 3/1000 | Loss: 0.00031047
Iteration 4/1000 | Loss: 0.00012588
Iteration 5/1000 | Loss: 0.00018555
Iteration 6/1000 | Loss: 0.00013823
Iteration 7/1000 | Loss: 0.00012055
Iteration 8/1000 | Loss: 0.00003396
Iteration 9/1000 | Loss: 0.00011906
Iteration 10/1000 | Loss: 0.00016520
Iteration 11/1000 | Loss: 0.00011632
Iteration 12/1000 | Loss: 0.00003273
Iteration 13/1000 | Loss: 0.00008298
Iteration 14/1000 | Loss: 0.00064455
Iteration 15/1000 | Loss: 0.00048759
Iteration 16/1000 | Loss: 0.00016176
Iteration 17/1000 | Loss: 0.00023202
Iteration 18/1000 | Loss: 0.00030139
Iteration 19/1000 | Loss: 0.00003605
Iteration 20/1000 | Loss: 0.00035902
Iteration 21/1000 | Loss: 0.00071391
Iteration 22/1000 | Loss: 0.00043337
Iteration 23/1000 | Loss: 0.00010975
Iteration 24/1000 | Loss: 0.00005811
Iteration 25/1000 | Loss: 0.00003106
Iteration 26/1000 | Loss: 0.00033115
Iteration 27/1000 | Loss: 0.00037878
Iteration 28/1000 | Loss: 0.00005709
Iteration 29/1000 | Loss: 0.00003402
Iteration 30/1000 | Loss: 0.00004508
Iteration 31/1000 | Loss: 0.00004434
Iteration 32/1000 | Loss: 0.00010825
Iteration 33/1000 | Loss: 0.00080987
Iteration 34/1000 | Loss: 0.00015259
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002269
Iteration 37/1000 | Loss: 0.00013398
Iteration 38/1000 | Loss: 0.00005242
Iteration 39/1000 | Loss: 0.00002378
Iteration 40/1000 | Loss: 0.00015856
Iteration 41/1000 | Loss: 0.00004918
Iteration 42/1000 | Loss: 0.00021703
Iteration 43/1000 | Loss: 0.00034340
Iteration 44/1000 | Loss: 0.00005000
Iteration 45/1000 | Loss: 0.00013707
Iteration 46/1000 | Loss: 0.00004596
Iteration 47/1000 | Loss: 0.00011390
Iteration 48/1000 | Loss: 0.00004583
Iteration 49/1000 | Loss: 0.00002351
Iteration 50/1000 | Loss: 0.00002531
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00011069
Iteration 53/1000 | Loss: 0.00004482
Iteration 54/1000 | Loss: 0.00002322
Iteration 55/1000 | Loss: 0.00004373
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002113
Iteration 58/1000 | Loss: 0.00017646
Iteration 59/1000 | Loss: 0.00006329
Iteration 60/1000 | Loss: 0.00012037
Iteration 61/1000 | Loss: 0.00004358
Iteration 62/1000 | Loss: 0.00014847
Iteration 63/1000 | Loss: 0.00006113
Iteration 64/1000 | Loss: 0.00002824
Iteration 65/1000 | Loss: 0.00002229
Iteration 66/1000 | Loss: 0.00011439
Iteration 67/1000 | Loss: 0.00008404
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00011894
Iteration 70/1000 | Loss: 0.00008255
Iteration 71/1000 | Loss: 0.00005085
Iteration 72/1000 | Loss: 0.00021796
Iteration 73/1000 | Loss: 0.00008946
Iteration 74/1000 | Loss: 0.00020800
Iteration 75/1000 | Loss: 0.00022031
Iteration 76/1000 | Loss: 0.00015410
Iteration 77/1000 | Loss: 0.00008258
Iteration 78/1000 | Loss: 0.00003186
Iteration 79/1000 | Loss: 0.00021054
Iteration 80/1000 | Loss: 0.00006787
Iteration 81/1000 | Loss: 0.00024158
Iteration 82/1000 | Loss: 0.00011831
Iteration 83/1000 | Loss: 0.00002444
Iteration 84/1000 | Loss: 0.00002519
Iteration 85/1000 | Loss: 0.00002543
Iteration 86/1000 | Loss: 0.00002086
Iteration 87/1000 | Loss: 0.00002314
Iteration 88/1000 | Loss: 0.00002750
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002092
Iteration 91/1000 | Loss: 0.00002073
Iteration 92/1000 | Loss: 0.00002073
Iteration 93/1000 | Loss: 0.00002073
Iteration 94/1000 | Loss: 0.00002073
Iteration 95/1000 | Loss: 0.00002073
Iteration 96/1000 | Loss: 0.00002073
Iteration 97/1000 | Loss: 0.00002073
Iteration 98/1000 | Loss: 0.00002073
Iteration 99/1000 | Loss: 0.00002073
Iteration 100/1000 | Loss: 0.00002112
Iteration 101/1000 | Loss: 0.00002069
Iteration 102/1000 | Loss: 0.00002069
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002068
Iteration 105/1000 | Loss: 0.00002068
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00003688
Iteration 109/1000 | Loss: 0.00002204
Iteration 110/1000 | Loss: 0.00002838
Iteration 111/1000 | Loss: 0.00002062
Iteration 112/1000 | Loss: 0.00002062
Iteration 113/1000 | Loss: 0.00002062
Iteration 114/1000 | Loss: 0.00011965
Iteration 115/1000 | Loss: 0.00038285
Iteration 116/1000 | Loss: 0.00025336
Iteration 117/1000 | Loss: 0.00050955
Iteration 118/1000 | Loss: 0.00028469
Iteration 119/1000 | Loss: 0.00026716
Iteration 120/1000 | Loss: 0.00043933
Iteration 121/1000 | Loss: 0.00014163
Iteration 122/1000 | Loss: 0.00118534
Iteration 123/1000 | Loss: 0.00062752
Iteration 124/1000 | Loss: 0.00063180
Iteration 125/1000 | Loss: 0.00045867
Iteration 126/1000 | Loss: 0.00024133
Iteration 127/1000 | Loss: 0.00032139
Iteration 128/1000 | Loss: 0.00035400
Iteration 129/1000 | Loss: 0.00047532
Iteration 130/1000 | Loss: 0.00047837
Iteration 131/1000 | Loss: 0.00037103
Iteration 132/1000 | Loss: 0.00021644
Iteration 133/1000 | Loss: 0.00020142
Iteration 134/1000 | Loss: 0.00004650
Iteration 135/1000 | Loss: 0.00015650
Iteration 136/1000 | Loss: 0.00005890
Iteration 137/1000 | Loss: 0.00013676
Iteration 138/1000 | Loss: 0.00005475
Iteration 139/1000 | Loss: 0.00007127
Iteration 140/1000 | Loss: 0.00006291
Iteration 141/1000 | Loss: 0.00002593
Iteration 142/1000 | Loss: 0.00002121
Iteration 143/1000 | Loss: 0.00004353
Iteration 144/1000 | Loss: 0.00007004
Iteration 145/1000 | Loss: 0.00002106
Iteration 146/1000 | Loss: 0.00002077
Iteration 147/1000 | Loss: 0.00002070
Iteration 148/1000 | Loss: 0.00002069
Iteration 149/1000 | Loss: 0.00002068
Iteration 150/1000 | Loss: 0.00002067
Iteration 151/1000 | Loss: 0.00002067
Iteration 152/1000 | Loss: 0.00002066
Iteration 153/1000 | Loss: 0.00002066
Iteration 154/1000 | Loss: 0.00002066
Iteration 155/1000 | Loss: 0.00002066
Iteration 156/1000 | Loss: 0.00002066
Iteration 157/1000 | Loss: 0.00002065
Iteration 158/1000 | Loss: 0.00002065
Iteration 159/1000 | Loss: 0.00002065
Iteration 160/1000 | Loss: 0.00002065
Iteration 161/1000 | Loss: 0.00002065
Iteration 162/1000 | Loss: 0.00002065
Iteration 163/1000 | Loss: 0.00002065
Iteration 164/1000 | Loss: 0.00002065
Iteration 165/1000 | Loss: 0.00002065
Iteration 166/1000 | Loss: 0.00002065
Iteration 167/1000 | Loss: 0.00002074
Iteration 168/1000 | Loss: 0.00002064
Iteration 169/1000 | Loss: 0.00002061
Iteration 170/1000 | Loss: 0.00002061
Iteration 171/1000 | Loss: 0.00002061
Iteration 172/1000 | Loss: 0.00002060
Iteration 173/1000 | Loss: 0.00002060
Iteration 174/1000 | Loss: 0.00002059
Iteration 175/1000 | Loss: 0.00002058
Iteration 176/1000 | Loss: 0.00002057
Iteration 177/1000 | Loss: 0.00002057
Iteration 178/1000 | Loss: 0.00004703
Iteration 179/1000 | Loss: 0.00002094
Iteration 180/1000 | Loss: 0.00002055
Iteration 181/1000 | Loss: 0.00002052
Iteration 182/1000 | Loss: 0.00002052
Iteration 183/1000 | Loss: 0.00002052
Iteration 184/1000 | Loss: 0.00002052
Iteration 185/1000 | Loss: 0.00002052
Iteration 186/1000 | Loss: 0.00002052
Iteration 187/1000 | Loss: 0.00002052
Iteration 188/1000 | Loss: 0.00002052
Iteration 189/1000 | Loss: 0.00002052
Iteration 190/1000 | Loss: 0.00002052
Iteration 191/1000 | Loss: 0.00002052
Iteration 192/1000 | Loss: 0.00002052
Iteration 193/1000 | Loss: 0.00002052
Iteration 194/1000 | Loss: 0.00002052
Iteration 195/1000 | Loss: 0.00002189
Iteration 196/1000 | Loss: 0.00002048
Iteration 197/1000 | Loss: 0.00002048
Iteration 198/1000 | Loss: 0.00002048
Iteration 199/1000 | Loss: 0.00002048
Iteration 200/1000 | Loss: 0.00002048
Iteration 201/1000 | Loss: 0.00002048
Iteration 202/1000 | Loss: 0.00002048
Iteration 203/1000 | Loss: 0.00002047
Iteration 204/1000 | Loss: 0.00002047
Iteration 205/1000 | Loss: 0.00002047
Iteration 206/1000 | Loss: 0.00002047
Iteration 207/1000 | Loss: 0.00002047
Iteration 208/1000 | Loss: 0.00002047
Iteration 209/1000 | Loss: 0.00002047
Iteration 210/1000 | Loss: 0.00002047
Iteration 211/1000 | Loss: 0.00002138
Iteration 212/1000 | Loss: 0.00002138
Iteration 213/1000 | Loss: 0.00014263
Iteration 214/1000 | Loss: 0.00013268
Iteration 215/1000 | Loss: 0.00002355
Iteration 216/1000 | Loss: 0.00002727
Iteration 217/1000 | Loss: 0.00002284
Iteration 218/1000 | Loss: 0.00002052
Iteration 219/1000 | Loss: 0.00002051
Iteration 220/1000 | Loss: 0.00002051
Iteration 221/1000 | Loss: 0.00002049
Iteration 222/1000 | Loss: 0.00002049
Iteration 223/1000 | Loss: 0.00002049
Iteration 224/1000 | Loss: 0.00002048
Iteration 225/1000 | Loss: 0.00002048
Iteration 226/1000 | Loss: 0.00002048
Iteration 227/1000 | Loss: 0.00002048
Iteration 228/1000 | Loss: 0.00002048
Iteration 229/1000 | Loss: 0.00002048
Iteration 230/1000 | Loss: 0.00002048
Iteration 231/1000 | Loss: 0.00002048
Iteration 232/1000 | Loss: 0.00002876
Iteration 233/1000 | Loss: 0.00002512
Iteration 234/1000 | Loss: 0.00002047
Iteration 235/1000 | Loss: 0.00002047
Iteration 236/1000 | Loss: 0.00002047
Iteration 237/1000 | Loss: 0.00002046
Iteration 238/1000 | Loss: 0.00002046
Iteration 239/1000 | Loss: 0.00002046
Iteration 240/1000 | Loss: 0.00002046
Iteration 241/1000 | Loss: 0.00002046
Iteration 242/1000 | Loss: 0.00002045
Iteration 243/1000 | Loss: 0.00002045
Iteration 244/1000 | Loss: 0.00002045
Iteration 245/1000 | Loss: 0.00002045
Iteration 246/1000 | Loss: 0.00002045
Iteration 247/1000 | Loss: 0.00002045
Iteration 248/1000 | Loss: 0.00002045
Iteration 249/1000 | Loss: 0.00002044
Iteration 250/1000 | Loss: 0.00002044
Iteration 251/1000 | Loss: 0.00002044
Iteration 252/1000 | Loss: 0.00002044
Iteration 253/1000 | Loss: 0.00002043
Iteration 254/1000 | Loss: 0.00002043
Iteration 255/1000 | Loss: 0.00002043
Iteration 256/1000 | Loss: 0.00002043
Iteration 257/1000 | Loss: 0.00002043
Iteration 258/1000 | Loss: 0.00002043
Iteration 259/1000 | Loss: 0.00002043
Iteration 260/1000 | Loss: 0.00002043
Iteration 261/1000 | Loss: 0.00002043
Iteration 262/1000 | Loss: 0.00002043
Iteration 263/1000 | Loss: 0.00002043
Iteration 264/1000 | Loss: 0.00002043
Iteration 265/1000 | Loss: 0.00002043
Iteration 266/1000 | Loss: 0.00002043
Iteration 267/1000 | Loss: 0.00002043
Iteration 268/1000 | Loss: 0.00002042
Iteration 269/1000 | Loss: 0.00002042
Iteration 270/1000 | Loss: 0.00002042
Iteration 271/1000 | Loss: 0.00002042
Iteration 272/1000 | Loss: 0.00002042
Iteration 273/1000 | Loss: 0.00002042
Iteration 274/1000 | Loss: 0.00002041
Iteration 275/1000 | Loss: 0.00002041
Iteration 276/1000 | Loss: 0.00002041
Iteration 277/1000 | Loss: 0.00002041
Iteration 278/1000 | Loss: 0.00002041
Iteration 279/1000 | Loss: 0.00002041
Iteration 280/1000 | Loss: 0.00002041
Iteration 281/1000 | Loss: 0.00002041
Iteration 282/1000 | Loss: 0.00002040
Iteration 283/1000 | Loss: 0.00002040
Iteration 284/1000 | Loss: 0.00002040
Iteration 285/1000 | Loss: 0.00011761
Iteration 286/1000 | Loss: 0.00006030
Iteration 287/1000 | Loss: 0.00012288
Iteration 288/1000 | Loss: 0.00002118
Iteration 289/1000 | Loss: 0.00002058
Iteration 290/1000 | Loss: 0.00006384
Iteration 291/1000 | Loss: 0.00002401
Iteration 292/1000 | Loss: 0.00002047
Iteration 293/1000 | Loss: 0.00002047
Iteration 294/1000 | Loss: 0.00002046
Iteration 295/1000 | Loss: 0.00002046
Iteration 296/1000 | Loss: 0.00002045
Iteration 297/1000 | Loss: 0.00002045
Iteration 298/1000 | Loss: 0.00002349
Iteration 299/1000 | Loss: 0.00002043
Iteration 300/1000 | Loss: 0.00002043
Iteration 301/1000 | Loss: 0.00002043
Iteration 302/1000 | Loss: 0.00002043
Iteration 303/1000 | Loss: 0.00002043
Iteration 304/1000 | Loss: 0.00002043
Iteration 305/1000 | Loss: 0.00002043
Iteration 306/1000 | Loss: 0.00002042
Iteration 307/1000 | Loss: 0.00002042
Iteration 308/1000 | Loss: 0.00002042
Iteration 309/1000 | Loss: 0.00002042
Iteration 310/1000 | Loss: 0.00002042
Iteration 311/1000 | Loss: 0.00002042
Iteration 312/1000 | Loss: 0.00002042
Iteration 313/1000 | Loss: 0.00002042
Iteration 314/1000 | Loss: 0.00002042
Iteration 315/1000 | Loss: 0.00002042
Iteration 316/1000 | Loss: 0.00002042
Iteration 317/1000 | Loss: 0.00002042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [2.0424049580469728e-05, 2.0424049580469728e-05, 2.0424049580469728e-05, 2.0424049580469728e-05, 2.0424049580469728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0424049580469728e-05

Optimization complete. Final v2v error: 3.7376649379730225 mm

Highest mean error: 9.812673568725586 mm for frame 13

Lowest mean error: 3.2374935150146484 mm for frame 48

Saving results

Total time: 260.2897186279297
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840554
Iteration 2/25 | Loss: 0.00155707
Iteration 3/25 | Loss: 0.00136564
Iteration 4/25 | Loss: 0.00134540
Iteration 5/25 | Loss: 0.00134146
Iteration 6/25 | Loss: 0.00134113
Iteration 7/25 | Loss: 0.00134113
Iteration 8/25 | Loss: 0.00134113
Iteration 9/25 | Loss: 0.00134113
Iteration 10/25 | Loss: 0.00134113
Iteration 11/25 | Loss: 0.00134113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001341133494861424, 0.001341133494861424, 0.001341133494861424, 0.001341133494861424, 0.001341133494861424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001341133494861424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95605433
Iteration 2/25 | Loss: 0.00063322
Iteration 3/25 | Loss: 0.00063321
Iteration 4/25 | Loss: 0.00063321
Iteration 5/25 | Loss: 0.00063321
Iteration 6/25 | Loss: 0.00063321
Iteration 7/25 | Loss: 0.00063321
Iteration 8/25 | Loss: 0.00063321
Iteration 9/25 | Loss: 0.00063321
Iteration 10/25 | Loss: 0.00063321
Iteration 11/25 | Loss: 0.00063321
Iteration 12/25 | Loss: 0.00063321
Iteration 13/25 | Loss: 0.00063321
Iteration 14/25 | Loss: 0.00063321
Iteration 15/25 | Loss: 0.00063321
Iteration 16/25 | Loss: 0.00063321
Iteration 17/25 | Loss: 0.00063321
Iteration 18/25 | Loss: 0.00063321
Iteration 19/25 | Loss: 0.00063321
Iteration 20/25 | Loss: 0.00063321
Iteration 21/25 | Loss: 0.00063321
Iteration 22/25 | Loss: 0.00063321
Iteration 23/25 | Loss: 0.00063321
Iteration 24/25 | Loss: 0.00063321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006332102348096669, 0.0006332102348096669, 0.0006332102348096669, 0.0006332102348096669, 0.0006332102348096669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006332102348096669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063321
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00002776
Iteration 4/1000 | Loss: 0.00002629
Iteration 5/1000 | Loss: 0.00002501
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002306
Iteration 10/1000 | Loss: 0.00002273
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002245
Iteration 13/1000 | Loss: 0.00002224
Iteration 14/1000 | Loss: 0.00002222
Iteration 15/1000 | Loss: 0.00002215
Iteration 16/1000 | Loss: 0.00002206
Iteration 17/1000 | Loss: 0.00002203
Iteration 18/1000 | Loss: 0.00002202
Iteration 19/1000 | Loss: 0.00002202
Iteration 20/1000 | Loss: 0.00002202
Iteration 21/1000 | Loss: 0.00002201
Iteration 22/1000 | Loss: 0.00002201
Iteration 23/1000 | Loss: 0.00002198
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002191
Iteration 32/1000 | Loss: 0.00002191
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002191
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002191
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002191
Iteration 40/1000 | Loss: 0.00002191
Iteration 41/1000 | Loss: 0.00002191
Iteration 42/1000 | Loss: 0.00002190
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002189
Iteration 45/1000 | Loss: 0.00002189
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002188
Iteration 48/1000 | Loss: 0.00002188
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002187
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00002187
Iteration 53/1000 | Loss: 0.00002187
Iteration 54/1000 | Loss: 0.00002187
Iteration 55/1000 | Loss: 0.00002187
Iteration 56/1000 | Loss: 0.00002187
Iteration 57/1000 | Loss: 0.00002187
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002186
Iteration 62/1000 | Loss: 0.00002186
Iteration 63/1000 | Loss: 0.00002186
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002185
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002182
Iteration 75/1000 | Loss: 0.00002182
Iteration 76/1000 | Loss: 0.00002180
Iteration 77/1000 | Loss: 0.00002180
Iteration 78/1000 | Loss: 0.00002180
Iteration 79/1000 | Loss: 0.00002180
Iteration 80/1000 | Loss: 0.00002180
Iteration 81/1000 | Loss: 0.00002180
Iteration 82/1000 | Loss: 0.00002179
Iteration 83/1000 | Loss: 0.00002179
Iteration 84/1000 | Loss: 0.00002179
Iteration 85/1000 | Loss: 0.00002179
Iteration 86/1000 | Loss: 0.00002179
Iteration 87/1000 | Loss: 0.00002179
Iteration 88/1000 | Loss: 0.00002179
Iteration 89/1000 | Loss: 0.00002179
Iteration 90/1000 | Loss: 0.00002179
Iteration 91/1000 | Loss: 0.00002179
Iteration 92/1000 | Loss: 0.00002179
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002177
Iteration 99/1000 | Loss: 0.00002177
Iteration 100/1000 | Loss: 0.00002177
Iteration 101/1000 | Loss: 0.00002177
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00002176
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002175
Iteration 107/1000 | Loss: 0.00002175
Iteration 108/1000 | Loss: 0.00002175
Iteration 109/1000 | Loss: 0.00002175
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002175
Iteration 112/1000 | Loss: 0.00002175
Iteration 113/1000 | Loss: 0.00002175
Iteration 114/1000 | Loss: 0.00002175
Iteration 115/1000 | Loss: 0.00002175
Iteration 116/1000 | Loss: 0.00002175
Iteration 117/1000 | Loss: 0.00002175
Iteration 118/1000 | Loss: 0.00002174
Iteration 119/1000 | Loss: 0.00002174
Iteration 120/1000 | Loss: 0.00002174
Iteration 121/1000 | Loss: 0.00002174
Iteration 122/1000 | Loss: 0.00002174
Iteration 123/1000 | Loss: 0.00002174
Iteration 124/1000 | Loss: 0.00002174
Iteration 125/1000 | Loss: 0.00002174
Iteration 126/1000 | Loss: 0.00002174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.174387736886274e-05, 2.174387736886274e-05, 2.174387736886274e-05, 2.174387736886274e-05, 2.174387736886274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.174387736886274e-05

Optimization complete. Final v2v error: 3.887591600418091 mm

Highest mean error: 4.121022701263428 mm for frame 0

Lowest mean error: 3.7844340801239014 mm for frame 157

Saving results

Total time: 40.49948334693909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439436
Iteration 2/25 | Loss: 0.00137079
Iteration 3/25 | Loss: 0.00131394
Iteration 4/25 | Loss: 0.00130586
Iteration 5/25 | Loss: 0.00130402
Iteration 6/25 | Loss: 0.00130402
Iteration 7/25 | Loss: 0.00130402
Iteration 8/25 | Loss: 0.00130402
Iteration 9/25 | Loss: 0.00130402
Iteration 10/25 | Loss: 0.00130402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001304015633650124, 0.001304015633650124, 0.001304015633650124, 0.001304015633650124, 0.001304015633650124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001304015633650124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43712139
Iteration 2/25 | Loss: 0.00079474
Iteration 3/25 | Loss: 0.00079473
Iteration 4/25 | Loss: 0.00079473
Iteration 5/25 | Loss: 0.00079473
Iteration 6/25 | Loss: 0.00079473
Iteration 7/25 | Loss: 0.00079473
Iteration 8/25 | Loss: 0.00079473
Iteration 9/25 | Loss: 0.00079473
Iteration 10/25 | Loss: 0.00079473
Iteration 11/25 | Loss: 0.00079473
Iteration 12/25 | Loss: 0.00079473
Iteration 13/25 | Loss: 0.00079473
Iteration 14/25 | Loss: 0.00079473
Iteration 15/25 | Loss: 0.00079473
Iteration 16/25 | Loss: 0.00079473
Iteration 17/25 | Loss: 0.00079473
Iteration 18/25 | Loss: 0.00079473
Iteration 19/25 | Loss: 0.00079473
Iteration 20/25 | Loss: 0.00079473
Iteration 21/25 | Loss: 0.00079473
Iteration 22/25 | Loss: 0.00079473
Iteration 23/25 | Loss: 0.00079473
Iteration 24/25 | Loss: 0.00079473
Iteration 25/25 | Loss: 0.00079473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079473
Iteration 2/1000 | Loss: 0.00003140
Iteration 3/1000 | Loss: 0.00002380
Iteration 4/1000 | Loss: 0.00002229
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001968
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001835
Iteration 13/1000 | Loss: 0.00001823
Iteration 14/1000 | Loss: 0.00001823
Iteration 15/1000 | Loss: 0.00001804
Iteration 16/1000 | Loss: 0.00001786
Iteration 17/1000 | Loss: 0.00001779
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001775
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001740
Iteration 33/1000 | Loss: 0.00001738
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001736
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001735
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001732
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001728
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001727
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001725
Iteration 58/1000 | Loss: 0.00001725
Iteration 59/1000 | Loss: 0.00001725
Iteration 60/1000 | Loss: 0.00001725
Iteration 61/1000 | Loss: 0.00001725
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001724
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001721
Iteration 71/1000 | Loss: 0.00001721
Iteration 72/1000 | Loss: 0.00001721
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001721
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001720
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001719
Iteration 80/1000 | Loss: 0.00001719
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001718
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001716
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001715
Iteration 100/1000 | Loss: 0.00001715
Iteration 101/1000 | Loss: 0.00001715
Iteration 102/1000 | Loss: 0.00001715
Iteration 103/1000 | Loss: 0.00001715
Iteration 104/1000 | Loss: 0.00001715
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001714
Iteration 109/1000 | Loss: 0.00001714
Iteration 110/1000 | Loss: 0.00001714
Iteration 111/1000 | Loss: 0.00001714
Iteration 112/1000 | Loss: 0.00001714
Iteration 113/1000 | Loss: 0.00001714
Iteration 114/1000 | Loss: 0.00001714
Iteration 115/1000 | Loss: 0.00001714
Iteration 116/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.7140870113507845e-05, 1.7140870113507845e-05, 1.7140870113507845e-05, 1.7140870113507845e-05, 1.7140870113507845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7140870113507845e-05

Optimization complete. Final v2v error: 3.5173494815826416 mm

Highest mean error: 3.8510797023773193 mm for frame 70

Lowest mean error: 3.2224440574645996 mm for frame 85

Saving results

Total time: 44.483131885528564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810520
Iteration 2/25 | Loss: 0.00154930
Iteration 3/25 | Loss: 0.00132743
Iteration 4/25 | Loss: 0.00131115
Iteration 5/25 | Loss: 0.00130847
Iteration 6/25 | Loss: 0.00130818
Iteration 7/25 | Loss: 0.00130818
Iteration 8/25 | Loss: 0.00130818
Iteration 9/25 | Loss: 0.00130818
Iteration 10/25 | Loss: 0.00130818
Iteration 11/25 | Loss: 0.00130818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013081798097118735, 0.0013081798097118735, 0.0013081798097118735, 0.0013081798097118735, 0.0013081798097118735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013081798097118735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40241134
Iteration 2/25 | Loss: 0.00083150
Iteration 3/25 | Loss: 0.00083148
Iteration 4/25 | Loss: 0.00083148
Iteration 5/25 | Loss: 0.00083148
Iteration 6/25 | Loss: 0.00083148
Iteration 7/25 | Loss: 0.00083148
Iteration 8/25 | Loss: 0.00083148
Iteration 9/25 | Loss: 0.00083148
Iteration 10/25 | Loss: 0.00083148
Iteration 11/25 | Loss: 0.00083148
Iteration 12/25 | Loss: 0.00083148
Iteration 13/25 | Loss: 0.00083148
Iteration 14/25 | Loss: 0.00083148
Iteration 15/25 | Loss: 0.00083148
Iteration 16/25 | Loss: 0.00083148
Iteration 17/25 | Loss: 0.00083148
Iteration 18/25 | Loss: 0.00083148
Iteration 19/25 | Loss: 0.00083148
Iteration 20/25 | Loss: 0.00083148
Iteration 21/25 | Loss: 0.00083148
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008314769365824759, 0.0008314769365824759, 0.0008314769365824759, 0.0008314769365824759, 0.0008314769365824759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008314769365824759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083148
Iteration 2/1000 | Loss: 0.00002932
Iteration 3/1000 | Loss: 0.00002112
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001334
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001280
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001271
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001266
Iteration 17/1000 | Loss: 0.00001266
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001220
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001214
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001213
Iteration 40/1000 | Loss: 0.00001213
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001184
Iteration 134/1000 | Loss: 0.00001184
Iteration 135/1000 | Loss: 0.00001184
Iteration 136/1000 | Loss: 0.00001184
Iteration 137/1000 | Loss: 0.00001184
Iteration 138/1000 | Loss: 0.00001184
Iteration 139/1000 | Loss: 0.00001184
Iteration 140/1000 | Loss: 0.00001184
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001183
Iteration 147/1000 | Loss: 0.00001183
Iteration 148/1000 | Loss: 0.00001183
Iteration 149/1000 | Loss: 0.00001183
Iteration 150/1000 | Loss: 0.00001183
Iteration 151/1000 | Loss: 0.00001183
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001182
Iteration 160/1000 | Loss: 0.00001182
Iteration 161/1000 | Loss: 0.00001182
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001182
Iteration 165/1000 | Loss: 0.00001182
Iteration 166/1000 | Loss: 0.00001182
Iteration 167/1000 | Loss: 0.00001182
Iteration 168/1000 | Loss: 0.00001182
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001182
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001182
Iteration 174/1000 | Loss: 0.00001182
Iteration 175/1000 | Loss: 0.00001182
Iteration 176/1000 | Loss: 0.00001182
Iteration 177/1000 | Loss: 0.00001182
Iteration 178/1000 | Loss: 0.00001182
Iteration 179/1000 | Loss: 0.00001182
Iteration 180/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.1822490705526434e-05, 1.1822490705526434e-05, 1.1822490705526434e-05, 1.1822490705526434e-05, 1.1822490705526434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1822490705526434e-05

Optimization complete. Final v2v error: 2.9639222621917725 mm

Highest mean error: 3.3903303146362305 mm for frame 166

Lowest mean error: 2.796017646789551 mm for frame 81

Saving results

Total time: 41.90478563308716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00480279
Iteration 2/25 | Loss: 0.00141449
Iteration 3/25 | Loss: 0.00134204
Iteration 4/25 | Loss: 0.00132877
Iteration 5/25 | Loss: 0.00132362
Iteration 6/25 | Loss: 0.00132362
Iteration 7/25 | Loss: 0.00132362
Iteration 8/25 | Loss: 0.00132362
Iteration 9/25 | Loss: 0.00132362
Iteration 10/25 | Loss: 0.00132362
Iteration 11/25 | Loss: 0.00132362
Iteration 12/25 | Loss: 0.00132362
Iteration 13/25 | Loss: 0.00132362
Iteration 14/25 | Loss: 0.00132362
Iteration 15/25 | Loss: 0.00132362
Iteration 16/25 | Loss: 0.00132362
Iteration 17/25 | Loss: 0.00132362
Iteration 18/25 | Loss: 0.00132362
Iteration 19/25 | Loss: 0.00132362
Iteration 20/25 | Loss: 0.00132362
Iteration 21/25 | Loss: 0.00132362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001323622651398182, 0.001323622651398182, 0.001323622651398182, 0.001323622651398182, 0.001323622651398182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001323622651398182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78163409
Iteration 2/25 | Loss: 0.00079860
Iteration 3/25 | Loss: 0.00079860
Iteration 4/25 | Loss: 0.00079860
Iteration 5/25 | Loss: 0.00079860
Iteration 6/25 | Loss: 0.00079859
Iteration 7/25 | Loss: 0.00079859
Iteration 8/25 | Loss: 0.00079859
Iteration 9/25 | Loss: 0.00079859
Iteration 10/25 | Loss: 0.00079859
Iteration 11/25 | Loss: 0.00079859
Iteration 12/25 | Loss: 0.00079859
Iteration 13/25 | Loss: 0.00079859
Iteration 14/25 | Loss: 0.00079859
Iteration 15/25 | Loss: 0.00079859
Iteration 16/25 | Loss: 0.00079859
Iteration 17/25 | Loss: 0.00079859
Iteration 18/25 | Loss: 0.00079859
Iteration 19/25 | Loss: 0.00079859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007985940319485962, 0.0007985940319485962, 0.0007985940319485962, 0.0007985940319485962, 0.0007985940319485962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007985940319485962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079859
Iteration 2/1000 | Loss: 0.00004181
Iteration 3/1000 | Loss: 0.00003010
Iteration 4/1000 | Loss: 0.00002819
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002455
Iteration 9/1000 | Loss: 0.00002410
Iteration 10/1000 | Loss: 0.00002372
Iteration 11/1000 | Loss: 0.00002339
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002263
Iteration 15/1000 | Loss: 0.00002246
Iteration 16/1000 | Loss: 0.00002230
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002222
Iteration 19/1000 | Loss: 0.00002217
Iteration 20/1000 | Loss: 0.00002217
Iteration 21/1000 | Loss: 0.00002212
Iteration 22/1000 | Loss: 0.00002208
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002207
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002205
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002204
Iteration 31/1000 | Loss: 0.00002204
Iteration 32/1000 | Loss: 0.00002195
Iteration 33/1000 | Loss: 0.00002194
Iteration 34/1000 | Loss: 0.00002193
Iteration 35/1000 | Loss: 0.00002193
Iteration 36/1000 | Loss: 0.00002192
Iteration 37/1000 | Loss: 0.00002191
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002190
Iteration 40/1000 | Loss: 0.00002190
Iteration 41/1000 | Loss: 0.00002189
Iteration 42/1000 | Loss: 0.00002189
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002185
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002185
Iteration 49/1000 | Loss: 0.00002184
Iteration 50/1000 | Loss: 0.00002184
Iteration 51/1000 | Loss: 0.00002184
Iteration 52/1000 | Loss: 0.00002183
Iteration 53/1000 | Loss: 0.00002183
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002182
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002182
Iteration 64/1000 | Loss: 0.00002182
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002181
Iteration 68/1000 | Loss: 0.00002181
Iteration 69/1000 | Loss: 0.00002181
Iteration 70/1000 | Loss: 0.00002181
Iteration 71/1000 | Loss: 0.00002181
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00002180
Iteration 74/1000 | Loss: 0.00002180
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002177
Iteration 85/1000 | Loss: 0.00002177
Iteration 86/1000 | Loss: 0.00002177
Iteration 87/1000 | Loss: 0.00002177
Iteration 88/1000 | Loss: 0.00002176
Iteration 89/1000 | Loss: 0.00002176
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002172
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002171
Iteration 108/1000 | Loss: 0.00002171
Iteration 109/1000 | Loss: 0.00002171
Iteration 110/1000 | Loss: 0.00002171
Iteration 111/1000 | Loss: 0.00002170
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002169
Iteration 115/1000 | Loss: 0.00002168
Iteration 116/1000 | Loss: 0.00002168
Iteration 117/1000 | Loss: 0.00002167
Iteration 118/1000 | Loss: 0.00002167
Iteration 119/1000 | Loss: 0.00002167
Iteration 120/1000 | Loss: 0.00002167
Iteration 121/1000 | Loss: 0.00002167
Iteration 122/1000 | Loss: 0.00002166
Iteration 123/1000 | Loss: 0.00002166
Iteration 124/1000 | Loss: 0.00002166
Iteration 125/1000 | Loss: 0.00002166
Iteration 126/1000 | Loss: 0.00002165
Iteration 127/1000 | Loss: 0.00002165
Iteration 128/1000 | Loss: 0.00002165
Iteration 129/1000 | Loss: 0.00002165
Iteration 130/1000 | Loss: 0.00002165
Iteration 131/1000 | Loss: 0.00002165
Iteration 132/1000 | Loss: 0.00002165
Iteration 133/1000 | Loss: 0.00002165
Iteration 134/1000 | Loss: 0.00002165
Iteration 135/1000 | Loss: 0.00002165
Iteration 136/1000 | Loss: 0.00002164
Iteration 137/1000 | Loss: 0.00002164
Iteration 138/1000 | Loss: 0.00002163
Iteration 139/1000 | Loss: 0.00002163
Iteration 140/1000 | Loss: 0.00002163
Iteration 141/1000 | Loss: 0.00002162
Iteration 142/1000 | Loss: 0.00002162
Iteration 143/1000 | Loss: 0.00002162
Iteration 144/1000 | Loss: 0.00002162
Iteration 145/1000 | Loss: 0.00002161
Iteration 146/1000 | Loss: 0.00002161
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002159
Iteration 152/1000 | Loss: 0.00002159
Iteration 153/1000 | Loss: 0.00002159
Iteration 154/1000 | Loss: 0.00002158
Iteration 155/1000 | Loss: 0.00002158
Iteration 156/1000 | Loss: 0.00002158
Iteration 157/1000 | Loss: 0.00002158
Iteration 158/1000 | Loss: 0.00002158
Iteration 159/1000 | Loss: 0.00002158
Iteration 160/1000 | Loss: 0.00002158
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002157
Iteration 166/1000 | Loss: 0.00002157
Iteration 167/1000 | Loss: 0.00002157
Iteration 168/1000 | Loss: 0.00002157
Iteration 169/1000 | Loss: 0.00002157
Iteration 170/1000 | Loss: 0.00002157
Iteration 171/1000 | Loss: 0.00002156
Iteration 172/1000 | Loss: 0.00002156
Iteration 173/1000 | Loss: 0.00002156
Iteration 174/1000 | Loss: 0.00002155
Iteration 175/1000 | Loss: 0.00002155
Iteration 176/1000 | Loss: 0.00002155
Iteration 177/1000 | Loss: 0.00002155
Iteration 178/1000 | Loss: 0.00002155
Iteration 179/1000 | Loss: 0.00002154
Iteration 180/1000 | Loss: 0.00002154
Iteration 181/1000 | Loss: 0.00002154
Iteration 182/1000 | Loss: 0.00002154
Iteration 183/1000 | Loss: 0.00002154
Iteration 184/1000 | Loss: 0.00002154
Iteration 185/1000 | Loss: 0.00002154
Iteration 186/1000 | Loss: 0.00002154
Iteration 187/1000 | Loss: 0.00002154
Iteration 188/1000 | Loss: 0.00002154
Iteration 189/1000 | Loss: 0.00002154
Iteration 190/1000 | Loss: 0.00002154
Iteration 191/1000 | Loss: 0.00002154
Iteration 192/1000 | Loss: 0.00002154
Iteration 193/1000 | Loss: 0.00002153
Iteration 194/1000 | Loss: 0.00002153
Iteration 195/1000 | Loss: 0.00002152
Iteration 196/1000 | Loss: 0.00002152
Iteration 197/1000 | Loss: 0.00002152
Iteration 198/1000 | Loss: 0.00002151
Iteration 199/1000 | Loss: 0.00002151
Iteration 200/1000 | Loss: 0.00002151
Iteration 201/1000 | Loss: 0.00002151
Iteration 202/1000 | Loss: 0.00002150
Iteration 203/1000 | Loss: 0.00002150
Iteration 204/1000 | Loss: 0.00002150
Iteration 205/1000 | Loss: 0.00002150
Iteration 206/1000 | Loss: 0.00002150
Iteration 207/1000 | Loss: 0.00002149
Iteration 208/1000 | Loss: 0.00002149
Iteration 209/1000 | Loss: 0.00002149
Iteration 210/1000 | Loss: 0.00002149
Iteration 211/1000 | Loss: 0.00002149
Iteration 212/1000 | Loss: 0.00002148
Iteration 213/1000 | Loss: 0.00002148
Iteration 214/1000 | Loss: 0.00002148
Iteration 215/1000 | Loss: 0.00002148
Iteration 216/1000 | Loss: 0.00002148
Iteration 217/1000 | Loss: 0.00002148
Iteration 218/1000 | Loss: 0.00002147
Iteration 219/1000 | Loss: 0.00002147
Iteration 220/1000 | Loss: 0.00002147
Iteration 221/1000 | Loss: 0.00002147
Iteration 222/1000 | Loss: 0.00002147
Iteration 223/1000 | Loss: 0.00002147
Iteration 224/1000 | Loss: 0.00002147
Iteration 225/1000 | Loss: 0.00002147
Iteration 226/1000 | Loss: 0.00002147
Iteration 227/1000 | Loss: 0.00002147
Iteration 228/1000 | Loss: 0.00002147
Iteration 229/1000 | Loss: 0.00002147
Iteration 230/1000 | Loss: 0.00002147
Iteration 231/1000 | Loss: 0.00002146
Iteration 232/1000 | Loss: 0.00002146
Iteration 233/1000 | Loss: 0.00002146
Iteration 234/1000 | Loss: 0.00002146
Iteration 235/1000 | Loss: 0.00002146
Iteration 236/1000 | Loss: 0.00002146
Iteration 237/1000 | Loss: 0.00002146
Iteration 238/1000 | Loss: 0.00002146
Iteration 239/1000 | Loss: 0.00002146
Iteration 240/1000 | Loss: 0.00002146
Iteration 241/1000 | Loss: 0.00002146
Iteration 242/1000 | Loss: 0.00002146
Iteration 243/1000 | Loss: 0.00002146
Iteration 244/1000 | Loss: 0.00002146
Iteration 245/1000 | Loss: 0.00002146
Iteration 246/1000 | Loss: 0.00002146
Iteration 247/1000 | Loss: 0.00002146
Iteration 248/1000 | Loss: 0.00002146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.1459651179611683e-05, 2.1459651179611683e-05, 2.1459651179611683e-05, 2.1459651179611683e-05, 2.1459651179611683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1459651179611683e-05

Optimization complete. Final v2v error: 3.9581480026245117 mm

Highest mean error: 4.609400749206543 mm for frame 255

Lowest mean error: 3.8054306507110596 mm for frame 190

Saving results

Total time: 60.370179414749146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519367
Iteration 2/25 | Loss: 0.00175891
Iteration 3/25 | Loss: 0.00149871
Iteration 4/25 | Loss: 0.00145971
Iteration 5/25 | Loss: 0.00145312
Iteration 6/25 | Loss: 0.00145555
Iteration 7/25 | Loss: 0.00144605
Iteration 8/25 | Loss: 0.00143755
Iteration 9/25 | Loss: 0.00143546
Iteration 10/25 | Loss: 0.00143503
Iteration 11/25 | Loss: 0.00143489
Iteration 12/25 | Loss: 0.00143476
Iteration 13/25 | Loss: 0.00143780
Iteration 14/25 | Loss: 0.00143541
Iteration 15/25 | Loss: 0.00143393
Iteration 16/25 | Loss: 0.00143348
Iteration 17/25 | Loss: 0.00143341
Iteration 18/25 | Loss: 0.00143341
Iteration 19/25 | Loss: 0.00143341
Iteration 20/25 | Loss: 0.00143341
Iteration 21/25 | Loss: 0.00143340
Iteration 22/25 | Loss: 0.00143340
Iteration 23/25 | Loss: 0.00143340
Iteration 24/25 | Loss: 0.00143340
Iteration 25/25 | Loss: 0.00143340

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38199651
Iteration 2/25 | Loss: 0.00091884
Iteration 3/25 | Loss: 0.00091882
Iteration 4/25 | Loss: 0.00091882
Iteration 5/25 | Loss: 0.00091882
Iteration 6/25 | Loss: 0.00091882
Iteration 7/25 | Loss: 0.00091882
Iteration 8/25 | Loss: 0.00091882
Iteration 9/25 | Loss: 0.00091882
Iteration 10/25 | Loss: 0.00091882
Iteration 11/25 | Loss: 0.00091882
Iteration 12/25 | Loss: 0.00091882
Iteration 13/25 | Loss: 0.00091882
Iteration 14/25 | Loss: 0.00091882
Iteration 15/25 | Loss: 0.00091882
Iteration 16/25 | Loss: 0.00091882
Iteration 17/25 | Loss: 0.00091882
Iteration 18/25 | Loss: 0.00091882
Iteration 19/25 | Loss: 0.00091882
Iteration 20/25 | Loss: 0.00091882
Iteration 21/25 | Loss: 0.00091882
Iteration 22/25 | Loss: 0.00091882
Iteration 23/25 | Loss: 0.00091882
Iteration 24/25 | Loss: 0.00091882
Iteration 25/25 | Loss: 0.00091882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091882
Iteration 2/1000 | Loss: 0.00041558
Iteration 3/1000 | Loss: 0.00042790
Iteration 4/1000 | Loss: 0.00033191
Iteration 5/1000 | Loss: 0.00041128
Iteration 6/1000 | Loss: 0.00034802
Iteration 7/1000 | Loss: 0.00036141
Iteration 8/1000 | Loss: 0.00033175
Iteration 9/1000 | Loss: 0.00035859
Iteration 10/1000 | Loss: 0.00035818
Iteration 11/1000 | Loss: 0.00040323
Iteration 12/1000 | Loss: 0.00040978
Iteration 13/1000 | Loss: 0.00017622
Iteration 14/1000 | Loss: 0.00013573
Iteration 15/1000 | Loss: 0.00012493
Iteration 16/1000 | Loss: 0.00009796
Iteration 17/1000 | Loss: 0.00005194
Iteration 18/1000 | Loss: 0.00004911
Iteration 19/1000 | Loss: 0.00004647
Iteration 20/1000 | Loss: 0.00004474
Iteration 21/1000 | Loss: 0.00010981
Iteration 22/1000 | Loss: 0.00013104
Iteration 23/1000 | Loss: 0.00012174
Iteration 24/1000 | Loss: 0.00008463
Iteration 25/1000 | Loss: 0.00007877
Iteration 26/1000 | Loss: 0.00011886
Iteration 27/1000 | Loss: 0.00007227
Iteration 28/1000 | Loss: 0.00005168
Iteration 29/1000 | Loss: 0.00004764
Iteration 30/1000 | Loss: 0.00004664
Iteration 31/1000 | Loss: 0.00004505
Iteration 32/1000 | Loss: 0.00004362
Iteration 33/1000 | Loss: 0.00004315
Iteration 34/1000 | Loss: 0.00004263
Iteration 35/1000 | Loss: 0.00021896
Iteration 36/1000 | Loss: 0.00010913
Iteration 37/1000 | Loss: 0.00006094
Iteration 38/1000 | Loss: 0.00004606
Iteration 39/1000 | Loss: 0.00004298
Iteration 40/1000 | Loss: 0.00004239
Iteration 41/1000 | Loss: 0.00004204
Iteration 42/1000 | Loss: 0.00004171
Iteration 43/1000 | Loss: 0.00004161
Iteration 44/1000 | Loss: 0.00004143
Iteration 45/1000 | Loss: 0.00004136
Iteration 46/1000 | Loss: 0.00004136
Iteration 47/1000 | Loss: 0.00004136
Iteration 48/1000 | Loss: 0.00004136
Iteration 49/1000 | Loss: 0.00004135
Iteration 50/1000 | Loss: 0.00004135
Iteration 51/1000 | Loss: 0.00004134
Iteration 52/1000 | Loss: 0.00004134
Iteration 53/1000 | Loss: 0.00004134
Iteration 54/1000 | Loss: 0.00004134
Iteration 55/1000 | Loss: 0.00004133
Iteration 56/1000 | Loss: 0.00004133
Iteration 57/1000 | Loss: 0.00004132
Iteration 58/1000 | Loss: 0.00004132
Iteration 59/1000 | Loss: 0.00004132
Iteration 60/1000 | Loss: 0.00004131
Iteration 61/1000 | Loss: 0.00004131
Iteration 62/1000 | Loss: 0.00004131
Iteration 63/1000 | Loss: 0.00004131
Iteration 64/1000 | Loss: 0.00004131
Iteration 65/1000 | Loss: 0.00004131
Iteration 66/1000 | Loss: 0.00004131
Iteration 67/1000 | Loss: 0.00004131
Iteration 68/1000 | Loss: 0.00004131
Iteration 69/1000 | Loss: 0.00004130
Iteration 70/1000 | Loss: 0.00004130
Iteration 71/1000 | Loss: 0.00004130
Iteration 72/1000 | Loss: 0.00004130
Iteration 73/1000 | Loss: 0.00004130
Iteration 74/1000 | Loss: 0.00004130
Iteration 75/1000 | Loss: 0.00004130
Iteration 76/1000 | Loss: 0.00004130
Iteration 77/1000 | Loss: 0.00004130
Iteration 78/1000 | Loss: 0.00004130
Iteration 79/1000 | Loss: 0.00004130
Iteration 80/1000 | Loss: 0.00004130
Iteration 81/1000 | Loss: 0.00004129
Iteration 82/1000 | Loss: 0.00004129
Iteration 83/1000 | Loss: 0.00004129
Iteration 84/1000 | Loss: 0.00004129
Iteration 85/1000 | Loss: 0.00004129
Iteration 86/1000 | Loss: 0.00004129
Iteration 87/1000 | Loss: 0.00004129
Iteration 88/1000 | Loss: 0.00004129
Iteration 89/1000 | Loss: 0.00004129
Iteration 90/1000 | Loss: 0.00004129
Iteration 91/1000 | Loss: 0.00004129
Iteration 92/1000 | Loss: 0.00004128
Iteration 93/1000 | Loss: 0.00004128
Iteration 94/1000 | Loss: 0.00004128
Iteration 95/1000 | Loss: 0.00004128
Iteration 96/1000 | Loss: 0.00004128
Iteration 97/1000 | Loss: 0.00004128
Iteration 98/1000 | Loss: 0.00004128
Iteration 99/1000 | Loss: 0.00004128
Iteration 100/1000 | Loss: 0.00004128
Iteration 101/1000 | Loss: 0.00004128
Iteration 102/1000 | Loss: 0.00004128
Iteration 103/1000 | Loss: 0.00004128
Iteration 104/1000 | Loss: 0.00004128
Iteration 105/1000 | Loss: 0.00004128
Iteration 106/1000 | Loss: 0.00004128
Iteration 107/1000 | Loss: 0.00004128
Iteration 108/1000 | Loss: 0.00004128
Iteration 109/1000 | Loss: 0.00004128
Iteration 110/1000 | Loss: 0.00004127
Iteration 111/1000 | Loss: 0.00004127
Iteration 112/1000 | Loss: 0.00004127
Iteration 113/1000 | Loss: 0.00004127
Iteration 114/1000 | Loss: 0.00004127
Iteration 115/1000 | Loss: 0.00004127
Iteration 116/1000 | Loss: 0.00004127
Iteration 117/1000 | Loss: 0.00004127
Iteration 118/1000 | Loss: 0.00004127
Iteration 119/1000 | Loss: 0.00004127
Iteration 120/1000 | Loss: 0.00004126
Iteration 121/1000 | Loss: 0.00004126
Iteration 122/1000 | Loss: 0.00004126
Iteration 123/1000 | Loss: 0.00004126
Iteration 124/1000 | Loss: 0.00004126
Iteration 125/1000 | Loss: 0.00004126
Iteration 126/1000 | Loss: 0.00004126
Iteration 127/1000 | Loss: 0.00004126
Iteration 128/1000 | Loss: 0.00004126
Iteration 129/1000 | Loss: 0.00004126
Iteration 130/1000 | Loss: 0.00004126
Iteration 131/1000 | Loss: 0.00004126
Iteration 132/1000 | Loss: 0.00004126
Iteration 133/1000 | Loss: 0.00004126
Iteration 134/1000 | Loss: 0.00004126
Iteration 135/1000 | Loss: 0.00004126
Iteration 136/1000 | Loss: 0.00004126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [4.1257531847804785e-05, 4.1257531847804785e-05, 4.1257531847804785e-05, 4.1257531847804785e-05, 4.1257531847804785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1257531847804785e-05

Optimization complete. Final v2v error: 4.954394817352295 mm

Highest mean error: 6.371589183807373 mm for frame 166

Lowest mean error: 4.508644104003906 mm for frame 195

Saving results

Total time: 111.58706760406494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916146
Iteration 2/25 | Loss: 0.00208256
Iteration 3/25 | Loss: 0.00165006
Iteration 4/25 | Loss: 0.00158510
Iteration 5/25 | Loss: 0.00158015
Iteration 6/25 | Loss: 0.00156462
Iteration 7/25 | Loss: 0.00154688
Iteration 8/25 | Loss: 0.00153750
Iteration 9/25 | Loss: 0.00152553
Iteration 10/25 | Loss: 0.00152227
Iteration 11/25 | Loss: 0.00151935
Iteration 12/25 | Loss: 0.00151771
Iteration 13/25 | Loss: 0.00151399
Iteration 14/25 | Loss: 0.00151741
Iteration 15/25 | Loss: 0.00151350
Iteration 16/25 | Loss: 0.00151744
Iteration 17/25 | Loss: 0.00152008
Iteration 18/25 | Loss: 0.00151509
Iteration 19/25 | Loss: 0.00151216
Iteration 20/25 | Loss: 0.00150805
Iteration 21/25 | Loss: 0.00150814
Iteration 22/25 | Loss: 0.00150302
Iteration 23/25 | Loss: 0.00150658
Iteration 24/25 | Loss: 0.00150776
Iteration 25/25 | Loss: 0.00150569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37664270
Iteration 2/25 | Loss: 0.00307824
Iteration 3/25 | Loss: 0.00290479
Iteration 4/25 | Loss: 0.00290479
Iteration 5/25 | Loss: 0.00290479
Iteration 6/25 | Loss: 0.00290479
Iteration 7/25 | Loss: 0.00290479
Iteration 8/25 | Loss: 0.00290479
Iteration 9/25 | Loss: 0.00290479
Iteration 10/25 | Loss: 0.00290479
Iteration 11/25 | Loss: 0.00290479
Iteration 12/25 | Loss: 0.00290479
Iteration 13/25 | Loss: 0.00290479
Iteration 14/25 | Loss: 0.00290479
Iteration 15/25 | Loss: 0.00290479
Iteration 16/25 | Loss: 0.00290479
Iteration 17/25 | Loss: 0.00290479
Iteration 18/25 | Loss: 0.00290479
Iteration 19/25 | Loss: 0.00290479
Iteration 20/25 | Loss: 0.00290479
Iteration 21/25 | Loss: 0.00290479
Iteration 22/25 | Loss: 0.00290479
Iteration 23/25 | Loss: 0.00290479
Iteration 24/25 | Loss: 0.00290479
Iteration 25/25 | Loss: 0.00290479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290478
Iteration 2/1000 | Loss: 0.00044001
Iteration 3/1000 | Loss: 0.00048506
Iteration 4/1000 | Loss: 0.00234863
Iteration 5/1000 | Loss: 0.00050510
Iteration 6/1000 | Loss: 0.00194787
Iteration 7/1000 | Loss: 0.00359751
Iteration 8/1000 | Loss: 0.00078693
Iteration 9/1000 | Loss: 0.00108215
Iteration 10/1000 | Loss: 0.00013649
Iteration 11/1000 | Loss: 0.00009815
Iteration 12/1000 | Loss: 0.00063544
Iteration 13/1000 | Loss: 0.00007743
Iteration 14/1000 | Loss: 0.00352911
Iteration 15/1000 | Loss: 0.00067064
Iteration 16/1000 | Loss: 0.00035750
Iteration 17/1000 | Loss: 0.00005570
Iteration 18/1000 | Loss: 0.00004702
Iteration 19/1000 | Loss: 0.00004201
Iteration 20/1000 | Loss: 0.00043698
Iteration 21/1000 | Loss: 0.00004386
Iteration 22/1000 | Loss: 0.00003670
Iteration 23/1000 | Loss: 0.00003382
Iteration 24/1000 | Loss: 0.00003274
Iteration 25/1000 | Loss: 0.00003137
Iteration 26/1000 | Loss: 0.00003055
Iteration 27/1000 | Loss: 0.00003005
Iteration 28/1000 | Loss: 0.00067745
Iteration 29/1000 | Loss: 0.00003044
Iteration 30/1000 | Loss: 0.00002777
Iteration 31/1000 | Loss: 0.00002654
Iteration 32/1000 | Loss: 0.00085094
Iteration 33/1000 | Loss: 0.00003793
Iteration 34/1000 | Loss: 0.00002625
Iteration 35/1000 | Loss: 0.00002418
Iteration 36/1000 | Loss: 0.00002322
Iteration 37/1000 | Loss: 0.00002256
Iteration 38/1000 | Loss: 0.00002212
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002137
Iteration 42/1000 | Loss: 0.00002122
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002115
Iteration 45/1000 | Loss: 0.00002108
Iteration 46/1000 | Loss: 0.00002108
Iteration 47/1000 | Loss: 0.00002107
Iteration 48/1000 | Loss: 0.00002106
Iteration 49/1000 | Loss: 0.00002106
Iteration 50/1000 | Loss: 0.00002105
Iteration 51/1000 | Loss: 0.00002104
Iteration 52/1000 | Loss: 0.00002104
Iteration 53/1000 | Loss: 0.00002104
Iteration 54/1000 | Loss: 0.00002104
Iteration 55/1000 | Loss: 0.00002103
Iteration 56/1000 | Loss: 0.00002103
Iteration 57/1000 | Loss: 0.00002103
Iteration 58/1000 | Loss: 0.00002102
Iteration 59/1000 | Loss: 0.00002102
Iteration 60/1000 | Loss: 0.00002102
Iteration 61/1000 | Loss: 0.00002101
Iteration 62/1000 | Loss: 0.00002101
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002100
Iteration 66/1000 | Loss: 0.00002099
Iteration 67/1000 | Loss: 0.00002097
Iteration 68/1000 | Loss: 0.00002093
Iteration 69/1000 | Loss: 0.00002093
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002092
Iteration 72/1000 | Loss: 0.00002092
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002092
Iteration 75/1000 | Loss: 0.00002091
Iteration 76/1000 | Loss: 0.00002091
Iteration 77/1000 | Loss: 0.00002091
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002088
Iteration 88/1000 | Loss: 0.00002088
Iteration 89/1000 | Loss: 0.00002088
Iteration 90/1000 | Loss: 0.00002088
Iteration 91/1000 | Loss: 0.00002088
Iteration 92/1000 | Loss: 0.00002088
Iteration 93/1000 | Loss: 0.00002088
Iteration 94/1000 | Loss: 0.00002088
Iteration 95/1000 | Loss: 0.00002088
Iteration 96/1000 | Loss: 0.00002088
Iteration 97/1000 | Loss: 0.00002088
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002087
Iteration 101/1000 | Loss: 0.00002087
Iteration 102/1000 | Loss: 0.00002087
Iteration 103/1000 | Loss: 0.00002087
Iteration 104/1000 | Loss: 0.00002087
Iteration 105/1000 | Loss: 0.00002087
Iteration 106/1000 | Loss: 0.00002087
Iteration 107/1000 | Loss: 0.00002087
Iteration 108/1000 | Loss: 0.00002087
Iteration 109/1000 | Loss: 0.00002087
Iteration 110/1000 | Loss: 0.00002087
Iteration 111/1000 | Loss: 0.00002086
Iteration 112/1000 | Loss: 0.00002086
Iteration 113/1000 | Loss: 0.00002086
Iteration 114/1000 | Loss: 0.00002085
Iteration 115/1000 | Loss: 0.00002085
Iteration 116/1000 | Loss: 0.00002085
Iteration 117/1000 | Loss: 0.00002084
Iteration 118/1000 | Loss: 0.00002084
Iteration 119/1000 | Loss: 0.00002084
Iteration 120/1000 | Loss: 0.00002084
Iteration 121/1000 | Loss: 0.00002084
Iteration 122/1000 | Loss: 0.00002084
Iteration 123/1000 | Loss: 0.00002084
Iteration 124/1000 | Loss: 0.00002083
Iteration 125/1000 | Loss: 0.00002083
Iteration 126/1000 | Loss: 0.00002083
Iteration 127/1000 | Loss: 0.00002083
Iteration 128/1000 | Loss: 0.00002083
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002082
Iteration 132/1000 | Loss: 0.00002082
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002081
Iteration 135/1000 | Loss: 0.00002081
Iteration 136/1000 | Loss: 0.00002080
Iteration 137/1000 | Loss: 0.00002080
Iteration 138/1000 | Loss: 0.00002080
Iteration 139/1000 | Loss: 0.00002079
Iteration 140/1000 | Loss: 0.00002079
Iteration 141/1000 | Loss: 0.00002079
Iteration 142/1000 | Loss: 0.00002079
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002077
Iteration 148/1000 | Loss: 0.00002077
Iteration 149/1000 | Loss: 0.00002077
Iteration 150/1000 | Loss: 0.00002077
Iteration 151/1000 | Loss: 0.00002077
Iteration 152/1000 | Loss: 0.00002077
Iteration 153/1000 | Loss: 0.00002076
Iteration 154/1000 | Loss: 0.00002076
Iteration 155/1000 | Loss: 0.00002076
Iteration 156/1000 | Loss: 0.00002076
Iteration 157/1000 | Loss: 0.00002076
Iteration 158/1000 | Loss: 0.00002075
Iteration 159/1000 | Loss: 0.00002075
Iteration 160/1000 | Loss: 0.00002075
Iteration 161/1000 | Loss: 0.00002074
Iteration 162/1000 | Loss: 0.00002074
Iteration 163/1000 | Loss: 0.00002074
Iteration 164/1000 | Loss: 0.00002073
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002072
Iteration 168/1000 | Loss: 0.00002072
Iteration 169/1000 | Loss: 0.00002072
Iteration 170/1000 | Loss: 0.00002071
Iteration 171/1000 | Loss: 0.00002071
Iteration 172/1000 | Loss: 0.00002071
Iteration 173/1000 | Loss: 0.00002070
Iteration 174/1000 | Loss: 0.00002070
Iteration 175/1000 | Loss: 0.00002069
Iteration 176/1000 | Loss: 0.00002069
Iteration 177/1000 | Loss: 0.00002069
Iteration 178/1000 | Loss: 0.00002069
Iteration 179/1000 | Loss: 0.00002068
Iteration 180/1000 | Loss: 0.00002068
Iteration 181/1000 | Loss: 0.00002068
Iteration 182/1000 | Loss: 0.00002068
Iteration 183/1000 | Loss: 0.00002068
Iteration 184/1000 | Loss: 0.00002068
Iteration 185/1000 | Loss: 0.00002068
Iteration 186/1000 | Loss: 0.00002068
Iteration 187/1000 | Loss: 0.00002068
Iteration 188/1000 | Loss: 0.00002068
Iteration 189/1000 | Loss: 0.00002068
Iteration 190/1000 | Loss: 0.00002068
Iteration 191/1000 | Loss: 0.00002068
Iteration 192/1000 | Loss: 0.00002067
Iteration 193/1000 | Loss: 0.00002067
Iteration 194/1000 | Loss: 0.00002067
Iteration 195/1000 | Loss: 0.00002067
Iteration 196/1000 | Loss: 0.00002067
Iteration 197/1000 | Loss: 0.00002067
Iteration 198/1000 | Loss: 0.00002067
Iteration 199/1000 | Loss: 0.00002067
Iteration 200/1000 | Loss: 0.00002067
Iteration 201/1000 | Loss: 0.00002067
Iteration 202/1000 | Loss: 0.00002066
Iteration 203/1000 | Loss: 0.00002066
Iteration 204/1000 | Loss: 0.00002066
Iteration 205/1000 | Loss: 0.00002066
Iteration 206/1000 | Loss: 0.00002066
Iteration 207/1000 | Loss: 0.00002066
Iteration 208/1000 | Loss: 0.00002066
Iteration 209/1000 | Loss: 0.00002066
Iteration 210/1000 | Loss: 0.00002066
Iteration 211/1000 | Loss: 0.00002066
Iteration 212/1000 | Loss: 0.00002065
Iteration 213/1000 | Loss: 0.00002065
Iteration 214/1000 | Loss: 0.00002065
Iteration 215/1000 | Loss: 0.00002065
Iteration 216/1000 | Loss: 0.00002065
Iteration 217/1000 | Loss: 0.00002065
Iteration 218/1000 | Loss: 0.00002065
Iteration 219/1000 | Loss: 0.00002064
Iteration 220/1000 | Loss: 0.00002064
Iteration 221/1000 | Loss: 0.00002064
Iteration 222/1000 | Loss: 0.00002064
Iteration 223/1000 | Loss: 0.00002064
Iteration 224/1000 | Loss: 0.00002064
Iteration 225/1000 | Loss: 0.00002064
Iteration 226/1000 | Loss: 0.00002064
Iteration 227/1000 | Loss: 0.00002064
Iteration 228/1000 | Loss: 0.00002064
Iteration 229/1000 | Loss: 0.00002063
Iteration 230/1000 | Loss: 0.00002063
Iteration 231/1000 | Loss: 0.00002063
Iteration 232/1000 | Loss: 0.00002063
Iteration 233/1000 | Loss: 0.00002063
Iteration 234/1000 | Loss: 0.00002063
Iteration 235/1000 | Loss: 0.00002063
Iteration 236/1000 | Loss: 0.00002063
Iteration 237/1000 | Loss: 0.00002063
Iteration 238/1000 | Loss: 0.00002063
Iteration 239/1000 | Loss: 0.00002063
Iteration 240/1000 | Loss: 0.00002063
Iteration 241/1000 | Loss: 0.00002063
Iteration 242/1000 | Loss: 0.00002063
Iteration 243/1000 | Loss: 0.00002063
Iteration 244/1000 | Loss: 0.00002063
Iteration 245/1000 | Loss: 0.00002063
Iteration 246/1000 | Loss: 0.00002063
Iteration 247/1000 | Loss: 0.00002063
Iteration 248/1000 | Loss: 0.00002063
Iteration 249/1000 | Loss: 0.00002063
Iteration 250/1000 | Loss: 0.00002062
Iteration 251/1000 | Loss: 0.00002062
Iteration 252/1000 | Loss: 0.00002062
Iteration 253/1000 | Loss: 0.00002062
Iteration 254/1000 | Loss: 0.00002062
Iteration 255/1000 | Loss: 0.00002062
Iteration 256/1000 | Loss: 0.00002062
Iteration 257/1000 | Loss: 0.00002062
Iteration 258/1000 | Loss: 0.00002062
Iteration 259/1000 | Loss: 0.00002062
Iteration 260/1000 | Loss: 0.00002062
Iteration 261/1000 | Loss: 0.00002062
Iteration 262/1000 | Loss: 0.00002062
Iteration 263/1000 | Loss: 0.00002062
Iteration 264/1000 | Loss: 0.00002062
Iteration 265/1000 | Loss: 0.00002062
Iteration 266/1000 | Loss: 0.00002062
Iteration 267/1000 | Loss: 0.00002062
Iteration 268/1000 | Loss: 0.00002062
Iteration 269/1000 | Loss: 0.00002062
Iteration 270/1000 | Loss: 0.00002062
Iteration 271/1000 | Loss: 0.00002062
Iteration 272/1000 | Loss: 0.00002061
Iteration 273/1000 | Loss: 0.00002061
Iteration 274/1000 | Loss: 0.00002061
Iteration 275/1000 | Loss: 0.00002061
Iteration 276/1000 | Loss: 0.00002061
Iteration 277/1000 | Loss: 0.00002061
Iteration 278/1000 | Loss: 0.00002061
Iteration 279/1000 | Loss: 0.00002061
Iteration 280/1000 | Loss: 0.00002061
Iteration 281/1000 | Loss: 0.00002061
Iteration 282/1000 | Loss: 0.00002061
Iteration 283/1000 | Loss: 0.00002061
Iteration 284/1000 | Loss: 0.00002061
Iteration 285/1000 | Loss: 0.00002061
Iteration 286/1000 | Loss: 0.00002061
Iteration 287/1000 | Loss: 0.00002061
Iteration 288/1000 | Loss: 0.00002061
Iteration 289/1000 | Loss: 0.00002061
Iteration 290/1000 | Loss: 0.00002061
Iteration 291/1000 | Loss: 0.00002061
Iteration 292/1000 | Loss: 0.00002061
Iteration 293/1000 | Loss: 0.00002061
Iteration 294/1000 | Loss: 0.00002060
Iteration 295/1000 | Loss: 0.00002060
Iteration 296/1000 | Loss: 0.00002060
Iteration 297/1000 | Loss: 0.00002060
Iteration 298/1000 | Loss: 0.00002060
Iteration 299/1000 | Loss: 0.00002060
Iteration 300/1000 | Loss: 0.00002060
Iteration 301/1000 | Loss: 0.00002060
Iteration 302/1000 | Loss: 0.00002060
Iteration 303/1000 | Loss: 0.00002060
Iteration 304/1000 | Loss: 0.00002060
Iteration 305/1000 | Loss: 0.00002060
Iteration 306/1000 | Loss: 0.00002060
Iteration 307/1000 | Loss: 0.00002060
Iteration 308/1000 | Loss: 0.00002060
Iteration 309/1000 | Loss: 0.00002060
Iteration 310/1000 | Loss: 0.00002060
Iteration 311/1000 | Loss: 0.00002060
Iteration 312/1000 | Loss: 0.00002060
Iteration 313/1000 | Loss: 0.00002060
Iteration 314/1000 | Loss: 0.00002060
Iteration 315/1000 | Loss: 0.00002060
Iteration 316/1000 | Loss: 0.00002060
Iteration 317/1000 | Loss: 0.00002060
Iteration 318/1000 | Loss: 0.00002060
Iteration 319/1000 | Loss: 0.00002060
Iteration 320/1000 | Loss: 0.00002060
Iteration 321/1000 | Loss: 0.00002060
Iteration 322/1000 | Loss: 0.00002060
Iteration 323/1000 | Loss: 0.00002060
Iteration 324/1000 | Loss: 0.00002060
Iteration 325/1000 | Loss: 0.00002060
Iteration 326/1000 | Loss: 0.00002060
Iteration 327/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 327. Stopping optimization.
Last 5 losses: [2.059776488749776e-05, 2.059776488749776e-05, 2.059776488749776e-05, 2.059776488749776e-05, 2.059776488749776e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.059776488749776e-05

Optimization complete. Final v2v error: 3.4994056224823 mm

Highest mean error: 12.64960765838623 mm for frame 53

Lowest mean error: 2.8560383319854736 mm for frame 105

Saving results

Total time: 130.1518530845642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837484
Iteration 2/25 | Loss: 0.00135239
Iteration 3/25 | Loss: 0.00127854
Iteration 4/25 | Loss: 0.00126832
Iteration 5/25 | Loss: 0.00126450
Iteration 6/25 | Loss: 0.00126404
Iteration 7/25 | Loss: 0.00126404
Iteration 8/25 | Loss: 0.00126404
Iteration 9/25 | Loss: 0.00126404
Iteration 10/25 | Loss: 0.00126404
Iteration 11/25 | Loss: 0.00126404
Iteration 12/25 | Loss: 0.00126404
Iteration 13/25 | Loss: 0.00126404
Iteration 14/25 | Loss: 0.00126404
Iteration 15/25 | Loss: 0.00126404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001264043035916984, 0.001264043035916984, 0.001264043035916984, 0.001264043035916984, 0.001264043035916984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001264043035916984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72762394
Iteration 2/25 | Loss: 0.00090506
Iteration 3/25 | Loss: 0.00090506
Iteration 4/25 | Loss: 0.00090505
Iteration 5/25 | Loss: 0.00090505
Iteration 6/25 | Loss: 0.00090505
Iteration 7/25 | Loss: 0.00090505
Iteration 8/25 | Loss: 0.00090505
Iteration 9/25 | Loss: 0.00090505
Iteration 10/25 | Loss: 0.00090505
Iteration 11/25 | Loss: 0.00090505
Iteration 12/25 | Loss: 0.00090505
Iteration 13/25 | Loss: 0.00090505
Iteration 14/25 | Loss: 0.00090505
Iteration 15/25 | Loss: 0.00090505
Iteration 16/25 | Loss: 0.00090505
Iteration 17/25 | Loss: 0.00090505
Iteration 18/25 | Loss: 0.00090505
Iteration 19/25 | Loss: 0.00090505
Iteration 20/25 | Loss: 0.00090505
Iteration 21/25 | Loss: 0.00090505
Iteration 22/25 | Loss: 0.00090505
Iteration 23/25 | Loss: 0.00090505
Iteration 24/25 | Loss: 0.00090505
Iteration 25/25 | Loss: 0.00090505

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090505
Iteration 2/1000 | Loss: 0.00002246
Iteration 3/1000 | Loss: 0.00001682
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001298
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001242
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001200
Iteration 12/1000 | Loss: 0.00001198
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001183
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001178
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001171
Iteration 28/1000 | Loss: 0.00001171
Iteration 29/1000 | Loss: 0.00001170
Iteration 30/1000 | Loss: 0.00001169
Iteration 31/1000 | Loss: 0.00001169
Iteration 32/1000 | Loss: 0.00001169
Iteration 33/1000 | Loss: 0.00001165
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001161
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001155
Iteration 51/1000 | Loss: 0.00001155
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001154
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001153
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001151
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001147
Iteration 69/1000 | Loss: 0.00001147
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001144
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001142
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001140
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001137
Iteration 90/1000 | Loss: 0.00001137
Iteration 91/1000 | Loss: 0.00001137
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001134
Iteration 103/1000 | Loss: 0.00001134
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001127
Iteration 162/1000 | Loss: 0.00001127
Iteration 163/1000 | Loss: 0.00001127
Iteration 164/1000 | Loss: 0.00001127
Iteration 165/1000 | Loss: 0.00001127
Iteration 166/1000 | Loss: 0.00001127
Iteration 167/1000 | Loss: 0.00001127
Iteration 168/1000 | Loss: 0.00001127
Iteration 169/1000 | Loss: 0.00001127
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001126
Iteration 177/1000 | Loss: 0.00001126
Iteration 178/1000 | Loss: 0.00001126
Iteration 179/1000 | Loss: 0.00001126
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Iteration 191/1000 | Loss: 0.00001126
Iteration 192/1000 | Loss: 0.00001126
Iteration 193/1000 | Loss: 0.00001126
Iteration 194/1000 | Loss: 0.00001126
Iteration 195/1000 | Loss: 0.00001126
Iteration 196/1000 | Loss: 0.00001126
Iteration 197/1000 | Loss: 0.00001126
Iteration 198/1000 | Loss: 0.00001126
Iteration 199/1000 | Loss: 0.00001126
Iteration 200/1000 | Loss: 0.00001126
Iteration 201/1000 | Loss: 0.00001126
Iteration 202/1000 | Loss: 0.00001126
Iteration 203/1000 | Loss: 0.00001126
Iteration 204/1000 | Loss: 0.00001126
Iteration 205/1000 | Loss: 0.00001126
Iteration 206/1000 | Loss: 0.00001126
Iteration 207/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.1257649930485059e-05, 1.1257649930485059e-05, 1.1257649930485059e-05, 1.1257649930485059e-05, 1.1257649930485059e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1257649930485059e-05

Optimization complete. Final v2v error: 2.878612756729126 mm

Highest mean error: 3.317594528198242 mm for frame 91

Lowest mean error: 2.694969892501831 mm for frame 130

Saving results

Total time: 41.08982276916504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049642
Iteration 2/25 | Loss: 0.01049642
Iteration 3/25 | Loss: 0.00547460
Iteration 4/25 | Loss: 0.00378124
Iteration 5/25 | Loss: 0.00298360
Iteration 6/25 | Loss: 0.00261543
Iteration 7/25 | Loss: 0.00257318
Iteration 8/25 | Loss: 0.00228577
Iteration 9/25 | Loss: 0.00221365
Iteration 10/25 | Loss: 0.00201119
Iteration 11/25 | Loss: 0.00185232
Iteration 12/25 | Loss: 0.00175902
Iteration 13/25 | Loss: 0.00174501
Iteration 14/25 | Loss: 0.00172209
Iteration 15/25 | Loss: 0.00169482
Iteration 16/25 | Loss: 0.00164212
Iteration 17/25 | Loss: 0.00159990
Iteration 18/25 | Loss: 0.00157391
Iteration 19/25 | Loss: 0.00158861
Iteration 20/25 | Loss: 0.00158399
Iteration 21/25 | Loss: 0.00157555
Iteration 22/25 | Loss: 0.00155617
Iteration 23/25 | Loss: 0.00154108
Iteration 24/25 | Loss: 0.00153845
Iteration 25/25 | Loss: 0.00153697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58985865
Iteration 2/25 | Loss: 0.00144598
Iteration 3/25 | Loss: 0.00112969
Iteration 4/25 | Loss: 0.00112969
Iteration 5/25 | Loss: 0.00112969
Iteration 6/25 | Loss: 0.00112969
Iteration 7/25 | Loss: 0.00112969
Iteration 8/25 | Loss: 0.00112969
Iteration 9/25 | Loss: 0.00112969
Iteration 10/25 | Loss: 0.00112969
Iteration 11/25 | Loss: 0.00112969
Iteration 12/25 | Loss: 0.00112969
Iteration 13/25 | Loss: 0.00112969
Iteration 14/25 | Loss: 0.00112969
Iteration 15/25 | Loss: 0.00112969
Iteration 16/25 | Loss: 0.00112969
Iteration 17/25 | Loss: 0.00112969
Iteration 18/25 | Loss: 0.00112969
Iteration 19/25 | Loss: 0.00112969
Iteration 20/25 | Loss: 0.00112969
Iteration 21/25 | Loss: 0.00112969
Iteration 22/25 | Loss: 0.00112969
Iteration 23/25 | Loss: 0.00112969
Iteration 24/25 | Loss: 0.00112969
Iteration 25/25 | Loss: 0.00112969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112969
Iteration 2/1000 | Loss: 0.00153129
Iteration 3/1000 | Loss: 0.00135550
Iteration 4/1000 | Loss: 0.00130382
Iteration 5/1000 | Loss: 0.00152757
Iteration 6/1000 | Loss: 0.00107530
Iteration 7/1000 | Loss: 0.00055452
Iteration 8/1000 | Loss: 0.00048810
Iteration 9/1000 | Loss: 0.00048761
Iteration 10/1000 | Loss: 0.00090340
Iteration 11/1000 | Loss: 0.00049174
Iteration 12/1000 | Loss: 0.00033699
Iteration 13/1000 | Loss: 0.00045346
Iteration 14/1000 | Loss: 0.00056473
Iteration 15/1000 | Loss: 0.00029142
Iteration 16/1000 | Loss: 0.00020049
Iteration 17/1000 | Loss: 0.00008315
Iteration 18/1000 | Loss: 0.00007585
Iteration 19/1000 | Loss: 0.00034198
Iteration 20/1000 | Loss: 0.00030098
Iteration 21/1000 | Loss: 0.00007441
Iteration 22/1000 | Loss: 0.00006341
Iteration 23/1000 | Loss: 0.00014296
Iteration 24/1000 | Loss: 0.00006658
Iteration 25/1000 | Loss: 0.00031564
Iteration 26/1000 | Loss: 0.00006223
Iteration 27/1000 | Loss: 0.00006346
Iteration 28/1000 | Loss: 0.00006260
Iteration 29/1000 | Loss: 0.00066401
Iteration 30/1000 | Loss: 0.00048651
Iteration 31/1000 | Loss: 0.00014474
Iteration 32/1000 | Loss: 0.00024831
Iteration 33/1000 | Loss: 0.00012498
Iteration 34/1000 | Loss: 0.00070479
Iteration 35/1000 | Loss: 0.00035198
Iteration 36/1000 | Loss: 0.00022817
Iteration 37/1000 | Loss: 0.00012920
Iteration 38/1000 | Loss: 0.00037264
Iteration 39/1000 | Loss: 0.00030404
Iteration 40/1000 | Loss: 0.00068188
Iteration 41/1000 | Loss: 0.00027984
Iteration 42/1000 | Loss: 0.00028267
Iteration 43/1000 | Loss: 0.00006905
Iteration 44/1000 | Loss: 0.00023015
Iteration 45/1000 | Loss: 0.00016707
Iteration 46/1000 | Loss: 0.00021763
Iteration 47/1000 | Loss: 0.00021865
Iteration 48/1000 | Loss: 0.00018243
Iteration 49/1000 | Loss: 0.00012673
Iteration 50/1000 | Loss: 0.00016732
Iteration 51/1000 | Loss: 0.00014197
Iteration 52/1000 | Loss: 0.00017173
Iteration 53/1000 | Loss: 0.00009535
Iteration 54/1000 | Loss: 0.00005974
Iteration 55/1000 | Loss: 0.00008301
Iteration 56/1000 | Loss: 0.00006454
Iteration 57/1000 | Loss: 0.00004698
Iteration 58/1000 | Loss: 0.00005607
Iteration 59/1000 | Loss: 0.00006205
Iteration 60/1000 | Loss: 0.00041663
Iteration 61/1000 | Loss: 0.00024276
Iteration 62/1000 | Loss: 0.00008559
Iteration 63/1000 | Loss: 0.00007879
Iteration 64/1000 | Loss: 0.00007378
Iteration 65/1000 | Loss: 0.00006861
Iteration 66/1000 | Loss: 0.00005909
Iteration 67/1000 | Loss: 0.00049125
Iteration 68/1000 | Loss: 0.00046340
Iteration 69/1000 | Loss: 0.00007276
Iteration 70/1000 | Loss: 0.00007376
Iteration 71/1000 | Loss: 0.00035186
Iteration 72/1000 | Loss: 0.00027307
Iteration 73/1000 | Loss: 0.00006905
Iteration 74/1000 | Loss: 0.00007254
Iteration 75/1000 | Loss: 0.00005851
Iteration 76/1000 | Loss: 0.00007479
Iteration 77/1000 | Loss: 0.00005677
Iteration 78/1000 | Loss: 0.00063300
Iteration 79/1000 | Loss: 0.00022586
Iteration 80/1000 | Loss: 0.00005897
Iteration 81/1000 | Loss: 0.00044873
Iteration 82/1000 | Loss: 0.00019684
Iteration 83/1000 | Loss: 0.00006307
Iteration 84/1000 | Loss: 0.00005770
Iteration 85/1000 | Loss: 0.00042824
Iteration 86/1000 | Loss: 0.00030889
Iteration 87/1000 | Loss: 0.00124608
Iteration 88/1000 | Loss: 0.00072339
Iteration 89/1000 | Loss: 0.00030794
Iteration 90/1000 | Loss: 0.00032203
Iteration 91/1000 | Loss: 0.00022741
Iteration 92/1000 | Loss: 0.00022343
Iteration 93/1000 | Loss: 0.00022298
Iteration 94/1000 | Loss: 0.00005533
Iteration 95/1000 | Loss: 0.00005943
Iteration 96/1000 | Loss: 0.00005384
Iteration 97/1000 | Loss: 0.00005520
Iteration 98/1000 | Loss: 0.00095163
Iteration 99/1000 | Loss: 0.00017838
Iteration 100/1000 | Loss: 0.00023698
Iteration 101/1000 | Loss: 0.00019476
Iteration 102/1000 | Loss: 0.00008876
Iteration 103/1000 | Loss: 0.00031709
Iteration 104/1000 | Loss: 0.00022892
Iteration 105/1000 | Loss: 0.00026538
Iteration 106/1000 | Loss: 0.00012911
Iteration 107/1000 | Loss: 0.00006313
Iteration 108/1000 | Loss: 0.00006644
Iteration 109/1000 | Loss: 0.00005837
Iteration 110/1000 | Loss: 0.00005826
Iteration 111/1000 | Loss: 0.00006554
Iteration 112/1000 | Loss: 0.00018323
Iteration 113/1000 | Loss: 0.00077156
Iteration 114/1000 | Loss: 0.00023345
Iteration 115/1000 | Loss: 0.00006116
Iteration 116/1000 | Loss: 0.00004152
Iteration 117/1000 | Loss: 0.00006248
Iteration 118/1000 | Loss: 0.00059113
Iteration 119/1000 | Loss: 0.00014135
Iteration 120/1000 | Loss: 0.00029271
Iteration 121/1000 | Loss: 0.00020913
Iteration 122/1000 | Loss: 0.00023010
Iteration 123/1000 | Loss: 0.00004075
Iteration 124/1000 | Loss: 0.00003713
Iteration 125/1000 | Loss: 0.00015895
Iteration 126/1000 | Loss: 0.00031095
Iteration 127/1000 | Loss: 0.00031086
Iteration 128/1000 | Loss: 0.00010355
Iteration 129/1000 | Loss: 0.00004302
Iteration 130/1000 | Loss: 0.00013016
Iteration 131/1000 | Loss: 0.00010339
Iteration 132/1000 | Loss: 0.00003877
Iteration 133/1000 | Loss: 0.00003288
Iteration 134/1000 | Loss: 0.00003196
Iteration 135/1000 | Loss: 0.00003120
Iteration 136/1000 | Loss: 0.00003073
Iteration 137/1000 | Loss: 0.00003029
Iteration 138/1000 | Loss: 0.00002998
Iteration 139/1000 | Loss: 0.00002959
Iteration 140/1000 | Loss: 0.00002930
Iteration 141/1000 | Loss: 0.00002903
Iteration 142/1000 | Loss: 0.00002884
Iteration 143/1000 | Loss: 0.00002865
Iteration 144/1000 | Loss: 0.00030772
Iteration 145/1000 | Loss: 0.00003376
Iteration 146/1000 | Loss: 0.00003203
Iteration 147/1000 | Loss: 0.00003077
Iteration 148/1000 | Loss: 0.00040203
Iteration 149/1000 | Loss: 0.00035346
Iteration 150/1000 | Loss: 0.00009242
Iteration 151/1000 | Loss: 0.00023876
Iteration 152/1000 | Loss: 0.00003254
Iteration 153/1000 | Loss: 0.00003129
Iteration 154/1000 | Loss: 0.00003040
Iteration 155/1000 | Loss: 0.00002994
Iteration 156/1000 | Loss: 0.00002944
Iteration 157/1000 | Loss: 0.00019530
Iteration 158/1000 | Loss: 0.00041809
Iteration 159/1000 | Loss: 0.00019026
Iteration 160/1000 | Loss: 0.00026978
Iteration 161/1000 | Loss: 0.00017631
Iteration 162/1000 | Loss: 0.00020302
Iteration 163/1000 | Loss: 0.00026487
Iteration 164/1000 | Loss: 0.00014470
Iteration 165/1000 | Loss: 0.00016348
Iteration 166/1000 | Loss: 0.00026970
Iteration 167/1000 | Loss: 0.00019260
Iteration 168/1000 | Loss: 0.00031254
Iteration 169/1000 | Loss: 0.00028960
Iteration 170/1000 | Loss: 0.00013769
Iteration 171/1000 | Loss: 0.00023538
Iteration 172/1000 | Loss: 0.00011862
Iteration 173/1000 | Loss: 0.00006715
Iteration 174/1000 | Loss: 0.00024460
Iteration 175/1000 | Loss: 0.00018385
Iteration 176/1000 | Loss: 0.00005709
Iteration 177/1000 | Loss: 0.00003147
Iteration 178/1000 | Loss: 0.00003036
Iteration 179/1000 | Loss: 0.00002981
Iteration 180/1000 | Loss: 0.00002918
Iteration 181/1000 | Loss: 0.00017085
Iteration 182/1000 | Loss: 0.00024032
Iteration 183/1000 | Loss: 0.00016601
Iteration 184/1000 | Loss: 0.00023527
Iteration 185/1000 | Loss: 0.00013410
Iteration 186/1000 | Loss: 0.00003126
Iteration 187/1000 | Loss: 0.00002949
Iteration 188/1000 | Loss: 0.00002907
Iteration 189/1000 | Loss: 0.00017152
Iteration 190/1000 | Loss: 0.00008397
Iteration 191/1000 | Loss: 0.00002891
Iteration 192/1000 | Loss: 0.00018488
Iteration 193/1000 | Loss: 0.00009387
Iteration 194/1000 | Loss: 0.00003034
Iteration 195/1000 | Loss: 0.00016203
Iteration 196/1000 | Loss: 0.00009932
Iteration 197/1000 | Loss: 0.00011732
Iteration 198/1000 | Loss: 0.00009383
Iteration 199/1000 | Loss: 0.00013549
Iteration 200/1000 | Loss: 0.00003039
Iteration 201/1000 | Loss: 0.00030055
Iteration 202/1000 | Loss: 0.00017738
Iteration 203/1000 | Loss: 0.00009502
Iteration 204/1000 | Loss: 0.00003030
Iteration 205/1000 | Loss: 0.00002951
Iteration 206/1000 | Loss: 0.00002889
Iteration 207/1000 | Loss: 0.00002867
Iteration 208/1000 | Loss: 0.00035700
Iteration 209/1000 | Loss: 0.00021724
Iteration 210/1000 | Loss: 0.00015628
Iteration 211/1000 | Loss: 0.00003451
Iteration 212/1000 | Loss: 0.00019970
Iteration 213/1000 | Loss: 0.00004655
Iteration 214/1000 | Loss: 0.00003329
Iteration 215/1000 | Loss: 0.00003173
Iteration 216/1000 | Loss: 0.00003084
Iteration 217/1000 | Loss: 0.00003038
Iteration 218/1000 | Loss: 0.00019426
Iteration 219/1000 | Loss: 0.00003179
Iteration 220/1000 | Loss: 0.00003010
Iteration 221/1000 | Loss: 0.00002918
Iteration 222/1000 | Loss: 0.00002832
Iteration 223/1000 | Loss: 0.00002772
Iteration 224/1000 | Loss: 0.00002743
Iteration 225/1000 | Loss: 0.00002742
Iteration 226/1000 | Loss: 0.00002736
Iteration 227/1000 | Loss: 0.00002736
Iteration 228/1000 | Loss: 0.00002736
Iteration 229/1000 | Loss: 0.00002736
Iteration 230/1000 | Loss: 0.00002736
Iteration 231/1000 | Loss: 0.00002736
Iteration 232/1000 | Loss: 0.00002736
Iteration 233/1000 | Loss: 0.00002736
Iteration 234/1000 | Loss: 0.00002735
Iteration 235/1000 | Loss: 0.00002735
Iteration 236/1000 | Loss: 0.00002735
Iteration 237/1000 | Loss: 0.00002735
Iteration 238/1000 | Loss: 0.00002735
Iteration 239/1000 | Loss: 0.00002735
Iteration 240/1000 | Loss: 0.00002735
Iteration 241/1000 | Loss: 0.00002735
Iteration 242/1000 | Loss: 0.00002735
Iteration 243/1000 | Loss: 0.00002735
Iteration 244/1000 | Loss: 0.00002735
Iteration 245/1000 | Loss: 0.00002735
Iteration 246/1000 | Loss: 0.00002735
Iteration 247/1000 | Loss: 0.00002735
Iteration 248/1000 | Loss: 0.00002735
Iteration 249/1000 | Loss: 0.00002735
Iteration 250/1000 | Loss: 0.00002735
Iteration 251/1000 | Loss: 0.00002735
Iteration 252/1000 | Loss: 0.00002735
Iteration 253/1000 | Loss: 0.00002735
Iteration 254/1000 | Loss: 0.00002735
Iteration 255/1000 | Loss: 0.00002735
Iteration 256/1000 | Loss: 0.00002735
Iteration 257/1000 | Loss: 0.00002735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [2.734726331254933e-05, 2.734726331254933e-05, 2.734726331254933e-05, 2.734726331254933e-05, 2.734726331254933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.734726331254933e-05

Optimization complete. Final v2v error: 4.5372538566589355 mm

Highest mean error: 6.513508319854736 mm for frame 7

Lowest mean error: 4.260462284088135 mm for frame 9

Saving results

Total time: 365.56319403648376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818570
Iteration 2/25 | Loss: 0.00167384
Iteration 3/25 | Loss: 0.00146397
Iteration 4/25 | Loss: 0.00144979
Iteration 5/25 | Loss: 0.00144888
Iteration 6/25 | Loss: 0.00144888
Iteration 7/25 | Loss: 0.00144888
Iteration 8/25 | Loss: 0.00144888
Iteration 9/25 | Loss: 0.00144888
Iteration 10/25 | Loss: 0.00144888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014488796005025506, 0.0014488796005025506, 0.0014488796005025506, 0.0014488796005025506, 0.0014488796005025506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014488796005025506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88216162
Iteration 2/25 | Loss: 0.00109955
Iteration 3/25 | Loss: 0.00109952
Iteration 4/25 | Loss: 0.00109951
Iteration 5/25 | Loss: 0.00109951
Iteration 6/25 | Loss: 0.00109951
Iteration 7/25 | Loss: 0.00109951
Iteration 8/25 | Loss: 0.00109951
Iteration 9/25 | Loss: 0.00109951
Iteration 10/25 | Loss: 0.00109951
Iteration 11/25 | Loss: 0.00109951
Iteration 12/25 | Loss: 0.00109951
Iteration 13/25 | Loss: 0.00109951
Iteration 14/25 | Loss: 0.00109951
Iteration 15/25 | Loss: 0.00109951
Iteration 16/25 | Loss: 0.00109951
Iteration 17/25 | Loss: 0.00109951
Iteration 18/25 | Loss: 0.00109951
Iteration 19/25 | Loss: 0.00109951
Iteration 20/25 | Loss: 0.00109951
Iteration 21/25 | Loss: 0.00109951
Iteration 22/25 | Loss: 0.00109951
Iteration 23/25 | Loss: 0.00109951
Iteration 24/25 | Loss: 0.00109951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010995117481797934, 0.0010995117481797934, 0.0010995117481797934, 0.0010995117481797934, 0.0010995117481797934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010995117481797934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109951
Iteration 2/1000 | Loss: 0.00005303
Iteration 3/1000 | Loss: 0.00003064
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002326
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002115
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002074
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002052
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002034
Iteration 16/1000 | Loss: 0.00002034
Iteration 17/1000 | Loss: 0.00002028
Iteration 18/1000 | Loss: 0.00002026
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002024
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002023
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002023
Iteration 27/1000 | Loss: 0.00002022
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002020
Iteration 31/1000 | Loss: 0.00002020
Iteration 32/1000 | Loss: 0.00002019
Iteration 33/1000 | Loss: 0.00002019
Iteration 34/1000 | Loss: 0.00002019
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002019
Iteration 38/1000 | Loss: 0.00002019
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002017
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002013
Iteration 46/1000 | Loss: 0.00002006
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002002
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002002
Iteration 55/1000 | Loss: 0.00002002
Iteration 56/1000 | Loss: 0.00002002
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002001
Iteration 60/1000 | Loss: 0.00002001
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00002000
Iteration 63/1000 | Loss: 0.00001999
Iteration 64/1000 | Loss: 0.00001999
Iteration 65/1000 | Loss: 0.00001999
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001998
Iteration 69/1000 | Loss: 0.00001997
Iteration 70/1000 | Loss: 0.00001997
Iteration 71/1000 | Loss: 0.00001997
Iteration 72/1000 | Loss: 0.00001996
Iteration 73/1000 | Loss: 0.00001996
Iteration 74/1000 | Loss: 0.00001996
Iteration 75/1000 | Loss: 0.00001996
Iteration 76/1000 | Loss: 0.00001996
Iteration 77/1000 | Loss: 0.00001995
Iteration 78/1000 | Loss: 0.00001995
Iteration 79/1000 | Loss: 0.00001995
Iteration 80/1000 | Loss: 0.00001995
Iteration 81/1000 | Loss: 0.00001994
Iteration 82/1000 | Loss: 0.00001994
Iteration 83/1000 | Loss: 0.00001994
Iteration 84/1000 | Loss: 0.00001994
Iteration 85/1000 | Loss: 0.00001993
Iteration 86/1000 | Loss: 0.00001993
Iteration 87/1000 | Loss: 0.00001993
Iteration 88/1000 | Loss: 0.00001993
Iteration 89/1000 | Loss: 0.00001992
Iteration 90/1000 | Loss: 0.00001992
Iteration 91/1000 | Loss: 0.00001992
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001991
Iteration 101/1000 | Loss: 0.00001991
Iteration 102/1000 | Loss: 0.00001991
Iteration 103/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.9905775843653828e-05, 1.9905775843653828e-05, 1.9905775843653828e-05, 1.9905775843653828e-05, 1.9905775843653828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9905775843653828e-05

Optimization complete. Final v2v error: 3.718494415283203 mm

Highest mean error: 4.029909133911133 mm for frame 115

Lowest mean error: 3.4700262546539307 mm for frame 5

Saving results

Total time: 41.117583990097046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423731
Iteration 2/25 | Loss: 0.00133514
Iteration 3/25 | Loss: 0.00128691
Iteration 4/25 | Loss: 0.00127997
Iteration 5/25 | Loss: 0.00127742
Iteration 6/25 | Loss: 0.00127742
Iteration 7/25 | Loss: 0.00127742
Iteration 8/25 | Loss: 0.00127742
Iteration 9/25 | Loss: 0.00127742
Iteration 10/25 | Loss: 0.00127742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012774169445037842, 0.0012774169445037842, 0.0012774169445037842, 0.0012774169445037842, 0.0012774169445037842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012774169445037842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48214984
Iteration 2/25 | Loss: 0.00083663
Iteration 3/25 | Loss: 0.00083663
Iteration 4/25 | Loss: 0.00083663
Iteration 5/25 | Loss: 0.00083663
Iteration 6/25 | Loss: 0.00083663
Iteration 7/25 | Loss: 0.00083663
Iteration 8/25 | Loss: 0.00083663
Iteration 9/25 | Loss: 0.00083663
Iteration 10/25 | Loss: 0.00083663
Iteration 11/25 | Loss: 0.00083663
Iteration 12/25 | Loss: 0.00083663
Iteration 13/25 | Loss: 0.00083663
Iteration 14/25 | Loss: 0.00083663
Iteration 15/25 | Loss: 0.00083663
Iteration 16/25 | Loss: 0.00083663
Iteration 17/25 | Loss: 0.00083663
Iteration 18/25 | Loss: 0.00083663
Iteration 19/25 | Loss: 0.00083663
Iteration 20/25 | Loss: 0.00083663
Iteration 21/25 | Loss: 0.00083663
Iteration 22/25 | Loss: 0.00083663
Iteration 23/25 | Loss: 0.00083663
Iteration 24/25 | Loss: 0.00083663
Iteration 25/25 | Loss: 0.00083663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083663
Iteration 2/1000 | Loss: 0.00002251
Iteration 3/1000 | Loss: 0.00001728
Iteration 4/1000 | Loss: 0.00001613
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001458
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001370
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001360
Iteration 15/1000 | Loss: 0.00001357
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001339
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001332
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001330
Iteration 24/1000 | Loss: 0.00001330
Iteration 25/1000 | Loss: 0.00001329
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001321
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001316
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001314
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001311
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001311
Iteration 47/1000 | Loss: 0.00001310
Iteration 48/1000 | Loss: 0.00001310
Iteration 49/1000 | Loss: 0.00001310
Iteration 50/1000 | Loss: 0.00001310
Iteration 51/1000 | Loss: 0.00001310
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001309
Iteration 54/1000 | Loss: 0.00001309
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001304
Iteration 65/1000 | Loss: 0.00001303
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001302
Iteration 68/1000 | Loss: 0.00001302
Iteration 69/1000 | Loss: 0.00001302
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001301
Iteration 72/1000 | Loss: 0.00001301
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001295
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001294
Iteration 93/1000 | Loss: 0.00001294
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001293
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001291
Iteration 106/1000 | Loss: 0.00001291
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001285
Iteration 131/1000 | Loss: 0.00001285
Iteration 132/1000 | Loss: 0.00001285
Iteration 133/1000 | Loss: 0.00001285
Iteration 134/1000 | Loss: 0.00001285
Iteration 135/1000 | Loss: 0.00001285
Iteration 136/1000 | Loss: 0.00001285
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001285
Iteration 139/1000 | Loss: 0.00001285
Iteration 140/1000 | Loss: 0.00001285
Iteration 141/1000 | Loss: 0.00001285
Iteration 142/1000 | Loss: 0.00001285
Iteration 143/1000 | Loss: 0.00001285
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001285
Iteration 148/1000 | Loss: 0.00001285
Iteration 149/1000 | Loss: 0.00001285
Iteration 150/1000 | Loss: 0.00001285
Iteration 151/1000 | Loss: 0.00001285
Iteration 152/1000 | Loss: 0.00001285
Iteration 153/1000 | Loss: 0.00001285
Iteration 154/1000 | Loss: 0.00001285
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.2851183782913722e-05, 1.2851183782913722e-05, 1.2851183782913722e-05, 1.2851183782913722e-05, 1.2851183782913722e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2851183782913722e-05

Optimization complete. Final v2v error: 3.0363457202911377 mm

Highest mean error: 3.392853021621704 mm for frame 155

Lowest mean error: 2.8162498474121094 mm for frame 131

Saving results

Total time: 40.735339641571045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386663
Iteration 2/25 | Loss: 0.00139506
Iteration 3/25 | Loss: 0.00131344
Iteration 4/25 | Loss: 0.00129985
Iteration 5/25 | Loss: 0.00129453
Iteration 6/25 | Loss: 0.00129297
Iteration 7/25 | Loss: 0.00129232
Iteration 8/25 | Loss: 0.00129232
Iteration 9/25 | Loss: 0.00129232
Iteration 10/25 | Loss: 0.00129232
Iteration 11/25 | Loss: 0.00129232
Iteration 12/25 | Loss: 0.00129232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001292322063818574, 0.001292322063818574, 0.001292322063818574, 0.001292322063818574, 0.001292322063818574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001292322063818574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47854078
Iteration 2/25 | Loss: 0.00109690
Iteration 3/25 | Loss: 0.00109689
Iteration 4/25 | Loss: 0.00109689
Iteration 5/25 | Loss: 0.00109689
Iteration 6/25 | Loss: 0.00109689
Iteration 7/25 | Loss: 0.00109689
Iteration 8/25 | Loss: 0.00109689
Iteration 9/25 | Loss: 0.00109689
Iteration 10/25 | Loss: 0.00109689
Iteration 11/25 | Loss: 0.00109689
Iteration 12/25 | Loss: 0.00109689
Iteration 13/25 | Loss: 0.00109689
Iteration 14/25 | Loss: 0.00109689
Iteration 15/25 | Loss: 0.00109689
Iteration 16/25 | Loss: 0.00109689
Iteration 17/25 | Loss: 0.00109689
Iteration 18/25 | Loss: 0.00109689
Iteration 19/25 | Loss: 0.00109689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010968910064548254, 0.0010968910064548254, 0.0010968910064548254, 0.0010968910064548254, 0.0010968910064548254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010968910064548254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109689
Iteration 2/1000 | Loss: 0.00003403
Iteration 3/1000 | Loss: 0.00002207
Iteration 4/1000 | Loss: 0.00001993
Iteration 5/1000 | Loss: 0.00001895
Iteration 6/1000 | Loss: 0.00001837
Iteration 7/1000 | Loss: 0.00001779
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001722
Iteration 10/1000 | Loss: 0.00001721
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001666
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001644
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001639
Iteration 25/1000 | Loss: 0.00001639
Iteration 26/1000 | Loss: 0.00001639
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001636
Iteration 33/1000 | Loss: 0.00001636
Iteration 34/1000 | Loss: 0.00001635
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001625
Iteration 60/1000 | Loss: 0.00001625
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001624
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001622
Iteration 68/1000 | Loss: 0.00001622
Iteration 69/1000 | Loss: 0.00001621
Iteration 70/1000 | Loss: 0.00001621
Iteration 71/1000 | Loss: 0.00001621
Iteration 72/1000 | Loss: 0.00001620
Iteration 73/1000 | Loss: 0.00001620
Iteration 74/1000 | Loss: 0.00001619
Iteration 75/1000 | Loss: 0.00001619
Iteration 76/1000 | Loss: 0.00001619
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001618
Iteration 80/1000 | Loss: 0.00001618
Iteration 81/1000 | Loss: 0.00001617
Iteration 82/1000 | Loss: 0.00001617
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001616
Iteration 86/1000 | Loss: 0.00001616
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001610
Iteration 112/1000 | Loss: 0.00001610
Iteration 113/1000 | Loss: 0.00001610
Iteration 114/1000 | Loss: 0.00001610
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001610
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001609
Iteration 120/1000 | Loss: 0.00001609
Iteration 121/1000 | Loss: 0.00001609
Iteration 122/1000 | Loss: 0.00001609
Iteration 123/1000 | Loss: 0.00001609
Iteration 124/1000 | Loss: 0.00001608
Iteration 125/1000 | Loss: 0.00001608
Iteration 126/1000 | Loss: 0.00001608
Iteration 127/1000 | Loss: 0.00001608
Iteration 128/1000 | Loss: 0.00001608
Iteration 129/1000 | Loss: 0.00001608
Iteration 130/1000 | Loss: 0.00001607
Iteration 131/1000 | Loss: 0.00001607
Iteration 132/1000 | Loss: 0.00001607
Iteration 133/1000 | Loss: 0.00001607
Iteration 134/1000 | Loss: 0.00001607
Iteration 135/1000 | Loss: 0.00001607
Iteration 136/1000 | Loss: 0.00001607
Iteration 137/1000 | Loss: 0.00001607
Iteration 138/1000 | Loss: 0.00001607
Iteration 139/1000 | Loss: 0.00001607
Iteration 140/1000 | Loss: 0.00001607
Iteration 141/1000 | Loss: 0.00001607
Iteration 142/1000 | Loss: 0.00001607
Iteration 143/1000 | Loss: 0.00001606
Iteration 144/1000 | Loss: 0.00001606
Iteration 145/1000 | Loss: 0.00001606
Iteration 146/1000 | Loss: 0.00001606
Iteration 147/1000 | Loss: 0.00001606
Iteration 148/1000 | Loss: 0.00001606
Iteration 149/1000 | Loss: 0.00001606
Iteration 150/1000 | Loss: 0.00001606
Iteration 151/1000 | Loss: 0.00001606
Iteration 152/1000 | Loss: 0.00001606
Iteration 153/1000 | Loss: 0.00001606
Iteration 154/1000 | Loss: 0.00001606
Iteration 155/1000 | Loss: 0.00001606
Iteration 156/1000 | Loss: 0.00001606
Iteration 157/1000 | Loss: 0.00001606
Iteration 158/1000 | Loss: 0.00001605
Iteration 159/1000 | Loss: 0.00001605
Iteration 160/1000 | Loss: 0.00001605
Iteration 161/1000 | Loss: 0.00001605
Iteration 162/1000 | Loss: 0.00001605
Iteration 163/1000 | Loss: 0.00001605
Iteration 164/1000 | Loss: 0.00001605
Iteration 165/1000 | Loss: 0.00001605
Iteration 166/1000 | Loss: 0.00001605
Iteration 167/1000 | Loss: 0.00001605
Iteration 168/1000 | Loss: 0.00001605
Iteration 169/1000 | Loss: 0.00001605
Iteration 170/1000 | Loss: 0.00001605
Iteration 171/1000 | Loss: 0.00001605
Iteration 172/1000 | Loss: 0.00001605
Iteration 173/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.605323632247746e-05, 1.605323632247746e-05, 1.605323632247746e-05, 1.605323632247746e-05, 1.605323632247746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.605323632247746e-05

Optimization complete. Final v2v error: 3.3776464462280273 mm

Highest mean error: 3.960374355316162 mm for frame 144

Lowest mean error: 2.786773920059204 mm for frame 10

Saving results

Total time: 42.25790786743164
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598866
Iteration 2/25 | Loss: 0.00151129
Iteration 3/25 | Loss: 0.00139434
Iteration 4/25 | Loss: 0.00137054
Iteration 5/25 | Loss: 0.00136650
Iteration 6/25 | Loss: 0.00136479
Iteration 7/25 | Loss: 0.00136472
Iteration 8/25 | Loss: 0.00136356
Iteration 9/25 | Loss: 0.00136223
Iteration 10/25 | Loss: 0.00136610
Iteration 11/25 | Loss: 0.00136591
Iteration 12/25 | Loss: 0.00136546
Iteration 13/25 | Loss: 0.00136548
Iteration 14/25 | Loss: 0.00136782
Iteration 15/25 | Loss: 0.00136498
Iteration 16/25 | Loss: 0.00136719
Iteration 17/25 | Loss: 0.00136552
Iteration 18/25 | Loss: 0.00136644
Iteration 19/25 | Loss: 0.00136758
Iteration 20/25 | Loss: 0.00136540
Iteration 21/25 | Loss: 0.00136433
Iteration 22/25 | Loss: 0.00136753
Iteration 23/25 | Loss: 0.00136668
Iteration 24/25 | Loss: 0.00136620
Iteration 25/25 | Loss: 0.00136182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50556386
Iteration 2/25 | Loss: 0.00106737
Iteration 3/25 | Loss: 0.00106733
Iteration 4/25 | Loss: 0.00106733
Iteration 5/25 | Loss: 0.00106733
Iteration 6/25 | Loss: 0.00106733
Iteration 7/25 | Loss: 0.00106733
Iteration 8/25 | Loss: 0.00106733
Iteration 9/25 | Loss: 0.00106733
Iteration 10/25 | Loss: 0.00106733
Iteration 11/25 | Loss: 0.00106733
Iteration 12/25 | Loss: 0.00106733
Iteration 13/25 | Loss: 0.00106733
Iteration 14/25 | Loss: 0.00106733
Iteration 15/25 | Loss: 0.00106733
Iteration 16/25 | Loss: 0.00106733
Iteration 17/25 | Loss: 0.00106733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010673267534002662, 0.0010673267534002662, 0.0010673267534002662, 0.0010673267534002662, 0.0010673267534002662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010673267534002662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106733
Iteration 2/1000 | Loss: 0.00006223
Iteration 3/1000 | Loss: 0.00003812
Iteration 4/1000 | Loss: 0.00003242
Iteration 5/1000 | Loss: 0.00003018
Iteration 6/1000 | Loss: 0.00002815
Iteration 7/1000 | Loss: 0.00002702
Iteration 8/1000 | Loss: 0.00002617
Iteration 9/1000 | Loss: 0.00002557
Iteration 10/1000 | Loss: 0.00002516
Iteration 11/1000 | Loss: 0.00002476
Iteration 12/1000 | Loss: 0.00002448
Iteration 13/1000 | Loss: 0.00002427
Iteration 14/1000 | Loss: 0.00002412
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002399
Iteration 17/1000 | Loss: 0.00002396
Iteration 18/1000 | Loss: 0.00002388
Iteration 19/1000 | Loss: 0.00002381
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002375
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002371
Iteration 24/1000 | Loss: 0.00002370
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002368
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002367
Iteration 29/1000 | Loss: 0.00002366
Iteration 30/1000 | Loss: 0.00002366
Iteration 31/1000 | Loss: 0.00002365
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002361
Iteration 34/1000 | Loss: 0.00002360
Iteration 35/1000 | Loss: 0.00002359
Iteration 36/1000 | Loss: 0.00002358
Iteration 37/1000 | Loss: 0.00002358
Iteration 38/1000 | Loss: 0.00002358
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00002357
Iteration 41/1000 | Loss: 0.00002357
Iteration 42/1000 | Loss: 0.00002357
Iteration 43/1000 | Loss: 0.00002356
Iteration 44/1000 | Loss: 0.00002355
Iteration 45/1000 | Loss: 0.00002355
Iteration 46/1000 | Loss: 0.00002355
Iteration 47/1000 | Loss: 0.00002355
Iteration 48/1000 | Loss: 0.00002355
Iteration 49/1000 | Loss: 0.00002355
Iteration 50/1000 | Loss: 0.00002355
Iteration 51/1000 | Loss: 0.00002355
Iteration 52/1000 | Loss: 0.00002354
Iteration 53/1000 | Loss: 0.00002354
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002352
Iteration 57/1000 | Loss: 0.00002352
Iteration 58/1000 | Loss: 0.00002352
Iteration 59/1000 | Loss: 0.00002351
Iteration 60/1000 | Loss: 0.00002351
Iteration 61/1000 | Loss: 0.00002351
Iteration 62/1000 | Loss: 0.00002350
Iteration 63/1000 | Loss: 0.00002350
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002349
Iteration 67/1000 | Loss: 0.00002349
Iteration 68/1000 | Loss: 0.00002349
Iteration 69/1000 | Loss: 0.00002349
Iteration 70/1000 | Loss: 0.00002349
Iteration 71/1000 | Loss: 0.00002349
Iteration 72/1000 | Loss: 0.00002349
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002349
Iteration 75/1000 | Loss: 0.00002348
Iteration 76/1000 | Loss: 0.00002348
Iteration 77/1000 | Loss: 0.00002348
Iteration 78/1000 | Loss: 0.00002347
Iteration 79/1000 | Loss: 0.00002347
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002346
Iteration 82/1000 | Loss: 0.00002346
Iteration 83/1000 | Loss: 0.00002345
Iteration 84/1000 | Loss: 0.00002345
Iteration 85/1000 | Loss: 0.00002345
Iteration 86/1000 | Loss: 0.00002345
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002344
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002343
Iteration 103/1000 | Loss: 0.00002343
Iteration 104/1000 | Loss: 0.00002343
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002341
Iteration 114/1000 | Loss: 0.00002341
Iteration 115/1000 | Loss: 0.00002341
Iteration 116/1000 | Loss: 0.00002341
Iteration 117/1000 | Loss: 0.00002340
Iteration 118/1000 | Loss: 0.00002340
Iteration 119/1000 | Loss: 0.00002340
Iteration 120/1000 | Loss: 0.00002340
Iteration 121/1000 | Loss: 0.00002340
Iteration 122/1000 | Loss: 0.00002340
Iteration 123/1000 | Loss: 0.00002340
Iteration 124/1000 | Loss: 0.00002340
Iteration 125/1000 | Loss: 0.00002339
Iteration 126/1000 | Loss: 0.00002339
Iteration 127/1000 | Loss: 0.00002339
Iteration 128/1000 | Loss: 0.00002339
Iteration 129/1000 | Loss: 0.00002339
Iteration 130/1000 | Loss: 0.00002339
Iteration 131/1000 | Loss: 0.00002339
Iteration 132/1000 | Loss: 0.00002339
Iteration 133/1000 | Loss: 0.00002338
Iteration 134/1000 | Loss: 0.00002338
Iteration 135/1000 | Loss: 0.00002338
Iteration 136/1000 | Loss: 0.00002338
Iteration 137/1000 | Loss: 0.00002338
Iteration 138/1000 | Loss: 0.00002337
Iteration 139/1000 | Loss: 0.00002337
Iteration 140/1000 | Loss: 0.00002337
Iteration 141/1000 | Loss: 0.00002337
Iteration 142/1000 | Loss: 0.00002337
Iteration 143/1000 | Loss: 0.00002337
Iteration 144/1000 | Loss: 0.00002336
Iteration 145/1000 | Loss: 0.00002336
Iteration 146/1000 | Loss: 0.00002336
Iteration 147/1000 | Loss: 0.00002336
Iteration 148/1000 | Loss: 0.00002336
Iteration 149/1000 | Loss: 0.00002335
Iteration 150/1000 | Loss: 0.00002335
Iteration 151/1000 | Loss: 0.00002335
Iteration 152/1000 | Loss: 0.00002335
Iteration 153/1000 | Loss: 0.00002335
Iteration 154/1000 | Loss: 0.00002335
Iteration 155/1000 | Loss: 0.00002335
Iteration 156/1000 | Loss: 0.00002334
Iteration 157/1000 | Loss: 0.00002334
Iteration 158/1000 | Loss: 0.00002334
Iteration 159/1000 | Loss: 0.00002334
Iteration 160/1000 | Loss: 0.00002334
Iteration 161/1000 | Loss: 0.00002333
Iteration 162/1000 | Loss: 0.00002333
Iteration 163/1000 | Loss: 0.00002333
Iteration 164/1000 | Loss: 0.00002333
Iteration 165/1000 | Loss: 0.00002333
Iteration 166/1000 | Loss: 0.00002333
Iteration 167/1000 | Loss: 0.00002333
Iteration 168/1000 | Loss: 0.00002333
Iteration 169/1000 | Loss: 0.00002333
Iteration 170/1000 | Loss: 0.00002333
Iteration 171/1000 | Loss: 0.00002332
Iteration 172/1000 | Loss: 0.00002332
Iteration 173/1000 | Loss: 0.00002332
Iteration 174/1000 | Loss: 0.00002332
Iteration 175/1000 | Loss: 0.00002332
Iteration 176/1000 | Loss: 0.00002332
Iteration 177/1000 | Loss: 0.00002332
Iteration 178/1000 | Loss: 0.00002331
Iteration 179/1000 | Loss: 0.00002331
Iteration 180/1000 | Loss: 0.00002331
Iteration 181/1000 | Loss: 0.00002331
Iteration 182/1000 | Loss: 0.00002331
Iteration 183/1000 | Loss: 0.00002331
Iteration 184/1000 | Loss: 0.00002331
Iteration 185/1000 | Loss: 0.00002331
Iteration 186/1000 | Loss: 0.00002331
Iteration 187/1000 | Loss: 0.00002330
Iteration 188/1000 | Loss: 0.00002330
Iteration 189/1000 | Loss: 0.00002330
Iteration 190/1000 | Loss: 0.00002330
Iteration 191/1000 | Loss: 0.00002330
Iteration 192/1000 | Loss: 0.00002330
Iteration 193/1000 | Loss: 0.00002330
Iteration 194/1000 | Loss: 0.00002330
Iteration 195/1000 | Loss: 0.00002330
Iteration 196/1000 | Loss: 0.00002330
Iteration 197/1000 | Loss: 0.00002330
Iteration 198/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [2.329946801182814e-05, 2.329946801182814e-05, 2.329946801182814e-05, 2.329946801182814e-05, 2.329946801182814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.329946801182814e-05

Optimization complete. Final v2v error: 3.9535579681396484 mm

Highest mean error: 4.888833999633789 mm for frame 101

Lowest mean error: 3.078777313232422 mm for frame 197

Saving results

Total time: 96.18658971786499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027116
Iteration 2/25 | Loss: 0.01027116
Iteration 3/25 | Loss: 0.01027116
Iteration 4/25 | Loss: 0.01027116
Iteration 5/25 | Loss: 0.01027116
Iteration 6/25 | Loss: 0.01027115
Iteration 7/25 | Loss: 0.01027115
Iteration 8/25 | Loss: 0.01027115
Iteration 9/25 | Loss: 0.01027115
Iteration 10/25 | Loss: 0.01027115
Iteration 11/25 | Loss: 0.01027115
Iteration 12/25 | Loss: 0.01027115
Iteration 13/25 | Loss: 0.01027115
Iteration 14/25 | Loss: 0.01027115
Iteration 15/25 | Loss: 0.01027115
Iteration 16/25 | Loss: 0.01027115
Iteration 17/25 | Loss: 0.01027115
Iteration 18/25 | Loss: 0.01027114
Iteration 19/25 | Loss: 0.01027114
Iteration 20/25 | Loss: 0.01027114
Iteration 21/25 | Loss: 0.01027114
Iteration 22/25 | Loss: 0.01027114
Iteration 23/25 | Loss: 0.01027114
Iteration 24/25 | Loss: 0.01027114
Iteration 25/25 | Loss: 0.01027114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84386265
Iteration 2/25 | Loss: 0.06281723
Iteration 3/25 | Loss: 0.06279995
Iteration 4/25 | Loss: 0.06279993
Iteration 5/25 | Loss: 0.06279992
Iteration 6/25 | Loss: 0.06279992
Iteration 7/25 | Loss: 0.06279992
Iteration 8/25 | Loss: 0.06279992
Iteration 9/25 | Loss: 0.06279992
Iteration 10/25 | Loss: 0.06279992
Iteration 11/25 | Loss: 0.06279992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06279991567134857, 0.06279991567134857, 0.06279991567134857, 0.06279991567134857, 0.06279991567134857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06279991567134857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06279992
Iteration 2/1000 | Loss: 0.00405957
Iteration 3/1000 | Loss: 0.00057776
Iteration 4/1000 | Loss: 0.00026646
Iteration 5/1000 | Loss: 0.00015449
Iteration 6/1000 | Loss: 0.00010955
Iteration 7/1000 | Loss: 0.00008458
Iteration 8/1000 | Loss: 0.00007062
Iteration 9/1000 | Loss: 0.00006061
Iteration 10/1000 | Loss: 0.00005318
Iteration 11/1000 | Loss: 0.00004636
Iteration 12/1000 | Loss: 0.00004213
Iteration 13/1000 | Loss: 0.00003901
Iteration 14/1000 | Loss: 0.00003600
Iteration 15/1000 | Loss: 0.00003390
Iteration 16/1000 | Loss: 0.00003232
Iteration 17/1000 | Loss: 0.00003030
Iteration 18/1000 | Loss: 0.00002897
Iteration 19/1000 | Loss: 0.00002794
Iteration 20/1000 | Loss: 0.00002702
Iteration 21/1000 | Loss: 0.00002604
Iteration 22/1000 | Loss: 0.00002518
Iteration 23/1000 | Loss: 0.00002457
Iteration 24/1000 | Loss: 0.00002414
Iteration 25/1000 | Loss: 0.00002375
Iteration 26/1000 | Loss: 0.00002346
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00002282
Iteration 29/1000 | Loss: 0.00002260
Iteration 30/1000 | Loss: 0.00002250
Iteration 31/1000 | Loss: 0.00002230
Iteration 32/1000 | Loss: 0.00002229
Iteration 33/1000 | Loss: 0.00002227
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00002221
Iteration 36/1000 | Loss: 0.00002218
Iteration 37/1000 | Loss: 0.00002217
Iteration 38/1000 | Loss: 0.00002216
Iteration 39/1000 | Loss: 0.00002215
Iteration 40/1000 | Loss: 0.00002214
Iteration 41/1000 | Loss: 0.00002212
Iteration 42/1000 | Loss: 0.00002199
Iteration 43/1000 | Loss: 0.00002199
Iteration 44/1000 | Loss: 0.00002198
Iteration 45/1000 | Loss: 0.00002192
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002191
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002189
Iteration 50/1000 | Loss: 0.00002189
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00002187
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002184
Iteration 58/1000 | Loss: 0.00002184
Iteration 59/1000 | Loss: 0.00002183
Iteration 60/1000 | Loss: 0.00002183
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002177
Iteration 67/1000 | Loss: 0.00002177
Iteration 68/1000 | Loss: 0.00002177
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002174
Iteration 72/1000 | Loss: 0.00002174
Iteration 73/1000 | Loss: 0.00002173
Iteration 74/1000 | Loss: 0.00002173
Iteration 75/1000 | Loss: 0.00002173
Iteration 76/1000 | Loss: 0.00002173
Iteration 77/1000 | Loss: 0.00002172
Iteration 78/1000 | Loss: 0.00002172
Iteration 79/1000 | Loss: 0.00002172
Iteration 80/1000 | Loss: 0.00002171
Iteration 81/1000 | Loss: 0.00002171
Iteration 82/1000 | Loss: 0.00002171
Iteration 83/1000 | Loss: 0.00002171
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002169
Iteration 92/1000 | Loss: 0.00002169
Iteration 93/1000 | Loss: 0.00002168
Iteration 94/1000 | Loss: 0.00002168
Iteration 95/1000 | Loss: 0.00002168
Iteration 96/1000 | Loss: 0.00002168
Iteration 97/1000 | Loss: 0.00002168
Iteration 98/1000 | Loss: 0.00002168
Iteration 99/1000 | Loss: 0.00002168
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002168
Iteration 102/1000 | Loss: 0.00002168
Iteration 103/1000 | Loss: 0.00002168
Iteration 104/1000 | Loss: 0.00002168
Iteration 105/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.1683439626940526e-05, 2.1683439626940526e-05, 2.1683439626940526e-05, 2.1683439626940526e-05, 2.1683439626940526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1683439626940526e-05

Optimization complete. Final v2v error: 3.7915632724761963 mm

Highest mean error: 5.5710225105285645 mm for frame 49

Lowest mean error: 3.161377429962158 mm for frame 190

Saving results

Total time: 66.29524898529053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766292
Iteration 2/25 | Loss: 0.00157863
Iteration 3/25 | Loss: 0.00136183
Iteration 4/25 | Loss: 0.00132575
Iteration 5/25 | Loss: 0.00131716
Iteration 6/25 | Loss: 0.00133804
Iteration 7/25 | Loss: 0.00131248
Iteration 8/25 | Loss: 0.00130573
Iteration 9/25 | Loss: 0.00131330
Iteration 10/25 | Loss: 0.00130552
Iteration 11/25 | Loss: 0.00130295
Iteration 12/25 | Loss: 0.00130261
Iteration 13/25 | Loss: 0.00130259
Iteration 14/25 | Loss: 0.00130258
Iteration 15/25 | Loss: 0.00130258
Iteration 16/25 | Loss: 0.00130258
Iteration 17/25 | Loss: 0.00130258
Iteration 18/25 | Loss: 0.00130258
Iteration 19/25 | Loss: 0.00130258
Iteration 20/25 | Loss: 0.00130258
Iteration 21/25 | Loss: 0.00130258
Iteration 22/25 | Loss: 0.00130258
Iteration 23/25 | Loss: 0.00130258
Iteration 24/25 | Loss: 0.00130258
Iteration 25/25 | Loss: 0.00130258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39381874
Iteration 2/25 | Loss: 0.00068324
Iteration 3/25 | Loss: 0.00068323
Iteration 4/25 | Loss: 0.00068323
Iteration 5/25 | Loss: 0.00068323
Iteration 6/25 | Loss: 0.00068323
Iteration 7/25 | Loss: 0.00068323
Iteration 8/25 | Loss: 0.00068323
Iteration 9/25 | Loss: 0.00068323
Iteration 10/25 | Loss: 0.00068323
Iteration 11/25 | Loss: 0.00068323
Iteration 12/25 | Loss: 0.00068323
Iteration 13/25 | Loss: 0.00068323
Iteration 14/25 | Loss: 0.00068323
Iteration 15/25 | Loss: 0.00068323
Iteration 16/25 | Loss: 0.00068323
Iteration 17/25 | Loss: 0.00068323
Iteration 18/25 | Loss: 0.00068323
Iteration 19/25 | Loss: 0.00068323
Iteration 20/25 | Loss: 0.00068323
Iteration 21/25 | Loss: 0.00068323
Iteration 22/25 | Loss: 0.00068323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000683230406139046, 0.000683230406139046, 0.000683230406139046, 0.000683230406139046, 0.000683230406139046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000683230406139046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068323
Iteration 2/1000 | Loss: 0.00003401
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002227
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002026
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001912
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001814
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001803
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001775
Iteration 24/1000 | Loss: 0.00001774
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001771
Iteration 30/1000 | Loss: 0.00001771
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001770
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001770
Iteration 35/1000 | Loss: 0.00001770
Iteration 36/1000 | Loss: 0.00001769
Iteration 37/1000 | Loss: 0.00001769
Iteration 38/1000 | Loss: 0.00001769
Iteration 39/1000 | Loss: 0.00001767
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001766
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001761
Iteration 47/1000 | Loss: 0.00001761
Iteration 48/1000 | Loss: 0.00001761
Iteration 49/1000 | Loss: 0.00001761
Iteration 50/1000 | Loss: 0.00001761
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001760
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001758
Iteration 59/1000 | Loss: 0.00001758
Iteration 60/1000 | Loss: 0.00001758
Iteration 61/1000 | Loss: 0.00001757
Iteration 62/1000 | Loss: 0.00001757
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001755
Iteration 75/1000 | Loss: 0.00001754
Iteration 76/1000 | Loss: 0.00001754
Iteration 77/1000 | Loss: 0.00001754
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001753
Iteration 83/1000 | Loss: 0.00001753
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001753
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001750
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001748
Iteration 113/1000 | Loss: 0.00001748
Iteration 114/1000 | Loss: 0.00001747
Iteration 115/1000 | Loss: 0.00001747
Iteration 116/1000 | Loss: 0.00001747
Iteration 117/1000 | Loss: 0.00001747
Iteration 118/1000 | Loss: 0.00001747
Iteration 119/1000 | Loss: 0.00001747
Iteration 120/1000 | Loss: 0.00001746
Iteration 121/1000 | Loss: 0.00001746
Iteration 122/1000 | Loss: 0.00001746
Iteration 123/1000 | Loss: 0.00001746
Iteration 124/1000 | Loss: 0.00001746
Iteration 125/1000 | Loss: 0.00001745
Iteration 126/1000 | Loss: 0.00001745
Iteration 127/1000 | Loss: 0.00001745
Iteration 128/1000 | Loss: 0.00001744
Iteration 129/1000 | Loss: 0.00001744
Iteration 130/1000 | Loss: 0.00001744
Iteration 131/1000 | Loss: 0.00001743
Iteration 132/1000 | Loss: 0.00001743
Iteration 133/1000 | Loss: 0.00001743
Iteration 134/1000 | Loss: 0.00001743
Iteration 135/1000 | Loss: 0.00001743
Iteration 136/1000 | Loss: 0.00001743
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001742
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001742
Iteration 145/1000 | Loss: 0.00001742
Iteration 146/1000 | Loss: 0.00001742
Iteration 147/1000 | Loss: 0.00001742
Iteration 148/1000 | Loss: 0.00001741
Iteration 149/1000 | Loss: 0.00001741
Iteration 150/1000 | Loss: 0.00001741
Iteration 151/1000 | Loss: 0.00001741
Iteration 152/1000 | Loss: 0.00001741
Iteration 153/1000 | Loss: 0.00001741
Iteration 154/1000 | Loss: 0.00001741
Iteration 155/1000 | Loss: 0.00001741
Iteration 156/1000 | Loss: 0.00001741
Iteration 157/1000 | Loss: 0.00001741
Iteration 158/1000 | Loss: 0.00001741
Iteration 159/1000 | Loss: 0.00001741
Iteration 160/1000 | Loss: 0.00001741
Iteration 161/1000 | Loss: 0.00001741
Iteration 162/1000 | Loss: 0.00001741
Iteration 163/1000 | Loss: 0.00001741
Iteration 164/1000 | Loss: 0.00001741
Iteration 165/1000 | Loss: 0.00001741
Iteration 166/1000 | Loss: 0.00001741
Iteration 167/1000 | Loss: 0.00001741
Iteration 168/1000 | Loss: 0.00001741
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001741
Iteration 171/1000 | Loss: 0.00001741
Iteration 172/1000 | Loss: 0.00001741
Iteration 173/1000 | Loss: 0.00001741
Iteration 174/1000 | Loss: 0.00001741
Iteration 175/1000 | Loss: 0.00001741
Iteration 176/1000 | Loss: 0.00001741
Iteration 177/1000 | Loss: 0.00001741
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001741
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Iteration 184/1000 | Loss: 0.00001741
Iteration 185/1000 | Loss: 0.00001741
Iteration 186/1000 | Loss: 0.00001741
Iteration 187/1000 | Loss: 0.00001741
Iteration 188/1000 | Loss: 0.00001741
Iteration 189/1000 | Loss: 0.00001741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.7410386135452427e-05, 1.7410386135452427e-05, 1.7410386135452427e-05, 1.7410386135452427e-05, 1.7410386135452427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7410386135452427e-05

Optimization complete. Final v2v error: 3.494720697402954 mm

Highest mean error: 3.936635732650757 mm for frame 80

Lowest mean error: 3.2577593326568604 mm for frame 103

Saving results

Total time: 53.22536897659302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767264
Iteration 2/25 | Loss: 0.00145699
Iteration 3/25 | Loss: 0.00130988
Iteration 4/25 | Loss: 0.00129576
Iteration 5/25 | Loss: 0.00129364
Iteration 6/25 | Loss: 0.00129364
Iteration 7/25 | Loss: 0.00129364
Iteration 8/25 | Loss: 0.00129364
Iteration 9/25 | Loss: 0.00129364
Iteration 10/25 | Loss: 0.00129364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012936407001689076, 0.0012936407001689076, 0.0012936407001689076, 0.0012936407001689076, 0.0012936407001689076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012936407001689076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39389026
Iteration 2/25 | Loss: 0.00086060
Iteration 3/25 | Loss: 0.00086059
Iteration 4/25 | Loss: 0.00086059
Iteration 5/25 | Loss: 0.00086059
Iteration 6/25 | Loss: 0.00086059
Iteration 7/25 | Loss: 0.00086059
Iteration 8/25 | Loss: 0.00086059
Iteration 9/25 | Loss: 0.00086059
Iteration 10/25 | Loss: 0.00086059
Iteration 11/25 | Loss: 0.00086059
Iteration 12/25 | Loss: 0.00086059
Iteration 13/25 | Loss: 0.00086059
Iteration 14/25 | Loss: 0.00086059
Iteration 15/25 | Loss: 0.00086059
Iteration 16/25 | Loss: 0.00086059
Iteration 17/25 | Loss: 0.00086059
Iteration 18/25 | Loss: 0.00086059
Iteration 19/25 | Loss: 0.00086059
Iteration 20/25 | Loss: 0.00086059
Iteration 21/25 | Loss: 0.00086059
Iteration 22/25 | Loss: 0.00086059
Iteration 23/25 | Loss: 0.00086059
Iteration 24/25 | Loss: 0.00086059
Iteration 25/25 | Loss: 0.00086059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008605917100794613, 0.0008605917100794613, 0.0008605917100794613, 0.0008605917100794613, 0.0008605917100794613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008605917100794613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086059
Iteration 2/1000 | Loss: 0.00003035
Iteration 3/1000 | Loss: 0.00001898
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001499
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001383
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001301
Iteration 16/1000 | Loss: 0.00001301
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001301
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001300
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001296
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001283
Iteration 36/1000 | Loss: 0.00001282
Iteration 37/1000 | Loss: 0.00001281
Iteration 38/1000 | Loss: 0.00001281
Iteration 39/1000 | Loss: 0.00001281
Iteration 40/1000 | Loss: 0.00001280
Iteration 41/1000 | Loss: 0.00001280
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001278
Iteration 44/1000 | Loss: 0.00001275
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001272
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001270
Iteration 51/1000 | Loss: 0.00001270
Iteration 52/1000 | Loss: 0.00001269
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001262
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001261
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001261
Iteration 70/1000 | Loss: 0.00001261
Iteration 71/1000 | Loss: 0.00001260
Iteration 72/1000 | Loss: 0.00001260
Iteration 73/1000 | Loss: 0.00001260
Iteration 74/1000 | Loss: 0.00001259
Iteration 75/1000 | Loss: 0.00001259
Iteration 76/1000 | Loss: 0.00001259
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001256
Iteration 85/1000 | Loss: 0.00001256
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001255
Iteration 88/1000 | Loss: 0.00001255
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001254
Iteration 91/1000 | Loss: 0.00001254
Iteration 92/1000 | Loss: 0.00001253
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001247
Iteration 109/1000 | Loss: 0.00001247
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001246
Iteration 112/1000 | Loss: 0.00001246
Iteration 113/1000 | Loss: 0.00001246
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001245
Iteration 116/1000 | Loss: 0.00001245
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001243
Iteration 127/1000 | Loss: 0.00001243
Iteration 128/1000 | Loss: 0.00001243
Iteration 129/1000 | Loss: 0.00001243
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001242
Iteration 134/1000 | Loss: 0.00001242
Iteration 135/1000 | Loss: 0.00001242
Iteration 136/1000 | Loss: 0.00001242
Iteration 137/1000 | Loss: 0.00001242
Iteration 138/1000 | Loss: 0.00001242
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001241
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001240
Iteration 147/1000 | Loss: 0.00001240
Iteration 148/1000 | Loss: 0.00001240
Iteration 149/1000 | Loss: 0.00001240
Iteration 150/1000 | Loss: 0.00001240
Iteration 151/1000 | Loss: 0.00001240
Iteration 152/1000 | Loss: 0.00001240
Iteration 153/1000 | Loss: 0.00001239
Iteration 154/1000 | Loss: 0.00001239
Iteration 155/1000 | Loss: 0.00001239
Iteration 156/1000 | Loss: 0.00001239
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001238
Iteration 159/1000 | Loss: 0.00001238
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001237
Iteration 163/1000 | Loss: 0.00001237
Iteration 164/1000 | Loss: 0.00001237
Iteration 165/1000 | Loss: 0.00001237
Iteration 166/1000 | Loss: 0.00001237
Iteration 167/1000 | Loss: 0.00001237
Iteration 168/1000 | Loss: 0.00001237
Iteration 169/1000 | Loss: 0.00001237
Iteration 170/1000 | Loss: 0.00001237
Iteration 171/1000 | Loss: 0.00001237
Iteration 172/1000 | Loss: 0.00001237
Iteration 173/1000 | Loss: 0.00001237
Iteration 174/1000 | Loss: 0.00001237
Iteration 175/1000 | Loss: 0.00001237
Iteration 176/1000 | Loss: 0.00001236
Iteration 177/1000 | Loss: 0.00001236
Iteration 178/1000 | Loss: 0.00001236
Iteration 179/1000 | Loss: 0.00001236
Iteration 180/1000 | Loss: 0.00001236
Iteration 181/1000 | Loss: 0.00001236
Iteration 182/1000 | Loss: 0.00001236
Iteration 183/1000 | Loss: 0.00001236
Iteration 184/1000 | Loss: 0.00001236
Iteration 185/1000 | Loss: 0.00001236
Iteration 186/1000 | Loss: 0.00001236
Iteration 187/1000 | Loss: 0.00001236
Iteration 188/1000 | Loss: 0.00001236
Iteration 189/1000 | Loss: 0.00001236
Iteration 190/1000 | Loss: 0.00001236
Iteration 191/1000 | Loss: 0.00001235
Iteration 192/1000 | Loss: 0.00001235
Iteration 193/1000 | Loss: 0.00001235
Iteration 194/1000 | Loss: 0.00001235
Iteration 195/1000 | Loss: 0.00001235
Iteration 196/1000 | Loss: 0.00001235
Iteration 197/1000 | Loss: 0.00001235
Iteration 198/1000 | Loss: 0.00001235
Iteration 199/1000 | Loss: 0.00001235
Iteration 200/1000 | Loss: 0.00001235
Iteration 201/1000 | Loss: 0.00001235
Iteration 202/1000 | Loss: 0.00001234
Iteration 203/1000 | Loss: 0.00001234
Iteration 204/1000 | Loss: 0.00001234
Iteration 205/1000 | Loss: 0.00001234
Iteration 206/1000 | Loss: 0.00001234
Iteration 207/1000 | Loss: 0.00001234
Iteration 208/1000 | Loss: 0.00001234
Iteration 209/1000 | Loss: 0.00001234
Iteration 210/1000 | Loss: 0.00001234
Iteration 211/1000 | Loss: 0.00001233
Iteration 212/1000 | Loss: 0.00001233
Iteration 213/1000 | Loss: 0.00001233
Iteration 214/1000 | Loss: 0.00001233
Iteration 215/1000 | Loss: 0.00001233
Iteration 216/1000 | Loss: 0.00001233
Iteration 217/1000 | Loss: 0.00001233
Iteration 218/1000 | Loss: 0.00001232
Iteration 219/1000 | Loss: 0.00001232
Iteration 220/1000 | Loss: 0.00001232
Iteration 221/1000 | Loss: 0.00001232
Iteration 222/1000 | Loss: 0.00001232
Iteration 223/1000 | Loss: 0.00001232
Iteration 224/1000 | Loss: 0.00001232
Iteration 225/1000 | Loss: 0.00001232
Iteration 226/1000 | Loss: 0.00001232
Iteration 227/1000 | Loss: 0.00001232
Iteration 228/1000 | Loss: 0.00001232
Iteration 229/1000 | Loss: 0.00001232
Iteration 230/1000 | Loss: 0.00001232
Iteration 231/1000 | Loss: 0.00001232
Iteration 232/1000 | Loss: 0.00001231
Iteration 233/1000 | Loss: 0.00001231
Iteration 234/1000 | Loss: 0.00001231
Iteration 235/1000 | Loss: 0.00001231
Iteration 236/1000 | Loss: 0.00001231
Iteration 237/1000 | Loss: 0.00001231
Iteration 238/1000 | Loss: 0.00001231
Iteration 239/1000 | Loss: 0.00001231
Iteration 240/1000 | Loss: 0.00001231
Iteration 241/1000 | Loss: 0.00001231
Iteration 242/1000 | Loss: 0.00001231
Iteration 243/1000 | Loss: 0.00001230
Iteration 244/1000 | Loss: 0.00001230
Iteration 245/1000 | Loss: 0.00001230
Iteration 246/1000 | Loss: 0.00001230
Iteration 247/1000 | Loss: 0.00001230
Iteration 248/1000 | Loss: 0.00001230
Iteration 249/1000 | Loss: 0.00001230
Iteration 250/1000 | Loss: 0.00001230
Iteration 251/1000 | Loss: 0.00001230
Iteration 252/1000 | Loss: 0.00001230
Iteration 253/1000 | Loss: 0.00001230
Iteration 254/1000 | Loss: 0.00001230
Iteration 255/1000 | Loss: 0.00001230
Iteration 256/1000 | Loss: 0.00001230
Iteration 257/1000 | Loss: 0.00001230
Iteration 258/1000 | Loss: 0.00001230
Iteration 259/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.2302187315071933e-05, 1.2302187315071933e-05, 1.2302187315071933e-05, 1.2302187315071933e-05, 1.2302187315071933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2302187315071933e-05

Optimization complete. Final v2v error: 2.992173910140991 mm

Highest mean error: 3.3910105228424072 mm for frame 125

Lowest mean error: 2.8220739364624023 mm for frame 173

Saving results

Total time: 52.56239676475525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958848
Iteration 2/25 | Loss: 0.00161777
Iteration 3/25 | Loss: 0.00140582
Iteration 4/25 | Loss: 0.00137569
Iteration 5/25 | Loss: 0.00137254
Iteration 6/25 | Loss: 0.00137254
Iteration 7/25 | Loss: 0.00137254
Iteration 8/25 | Loss: 0.00137254
Iteration 9/25 | Loss: 0.00137254
Iteration 10/25 | Loss: 0.00137254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001372539671137929, 0.001372539671137929, 0.001372539671137929, 0.001372539671137929, 0.001372539671137929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001372539671137929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39758062
Iteration 2/25 | Loss: 0.00079078
Iteration 3/25 | Loss: 0.00079077
Iteration 4/25 | Loss: 0.00079077
Iteration 5/25 | Loss: 0.00079077
Iteration 6/25 | Loss: 0.00079077
Iteration 7/25 | Loss: 0.00079077
Iteration 8/25 | Loss: 0.00079077
Iteration 9/25 | Loss: 0.00079077
Iteration 10/25 | Loss: 0.00079077
Iteration 11/25 | Loss: 0.00079077
Iteration 12/25 | Loss: 0.00079077
Iteration 13/25 | Loss: 0.00079077
Iteration 14/25 | Loss: 0.00079077
Iteration 15/25 | Loss: 0.00079077
Iteration 16/25 | Loss: 0.00079077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007907713879831135, 0.0007907713879831135, 0.0007907713879831135, 0.0007907713879831135, 0.0007907713879831135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007907713879831135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079077
Iteration 2/1000 | Loss: 0.00003473
Iteration 3/1000 | Loss: 0.00002580
Iteration 4/1000 | Loss: 0.00002365
Iteration 5/1000 | Loss: 0.00002292
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002181
Iteration 8/1000 | Loss: 0.00002148
Iteration 9/1000 | Loss: 0.00002126
Iteration 10/1000 | Loss: 0.00002093
Iteration 11/1000 | Loss: 0.00002063
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002036
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00002033
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002027
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002017
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002016
Iteration 26/1000 | Loss: 0.00002015
Iteration 27/1000 | Loss: 0.00002015
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002005
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002003
Iteration 46/1000 | Loss: 0.00002003
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002003
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002002
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002001
Iteration 58/1000 | Loss: 0.00002001
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00002000
Iteration 61/1000 | Loss: 0.00002000
Iteration 62/1000 | Loss: 0.00001999
Iteration 63/1000 | Loss: 0.00001999
Iteration 64/1000 | Loss: 0.00001999
Iteration 65/1000 | Loss: 0.00001998
Iteration 66/1000 | Loss: 0.00001998
Iteration 67/1000 | Loss: 0.00001998
Iteration 68/1000 | Loss: 0.00001998
Iteration 69/1000 | Loss: 0.00001998
Iteration 70/1000 | Loss: 0.00001998
Iteration 71/1000 | Loss: 0.00001998
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00001997
Iteration 76/1000 | Loss: 0.00001997
Iteration 77/1000 | Loss: 0.00001997
Iteration 78/1000 | Loss: 0.00001997
Iteration 79/1000 | Loss: 0.00001997
Iteration 80/1000 | Loss: 0.00001997
Iteration 81/1000 | Loss: 0.00001996
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001996
Iteration 85/1000 | Loss: 0.00001996
Iteration 86/1000 | Loss: 0.00001996
Iteration 87/1000 | Loss: 0.00001996
Iteration 88/1000 | Loss: 0.00001996
Iteration 89/1000 | Loss: 0.00001996
Iteration 90/1000 | Loss: 0.00001996
Iteration 91/1000 | Loss: 0.00001995
Iteration 92/1000 | Loss: 0.00001995
Iteration 93/1000 | Loss: 0.00001995
Iteration 94/1000 | Loss: 0.00001995
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001995
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001994
Iteration 101/1000 | Loss: 0.00001994
Iteration 102/1000 | Loss: 0.00001993
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001993
Iteration 105/1000 | Loss: 0.00001993
Iteration 106/1000 | Loss: 0.00001993
Iteration 107/1000 | Loss: 0.00001993
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001992
Iteration 110/1000 | Loss: 0.00001992
Iteration 111/1000 | Loss: 0.00001991
Iteration 112/1000 | Loss: 0.00001991
Iteration 113/1000 | Loss: 0.00001991
Iteration 114/1000 | Loss: 0.00001991
Iteration 115/1000 | Loss: 0.00001990
Iteration 116/1000 | Loss: 0.00001990
Iteration 117/1000 | Loss: 0.00001990
Iteration 118/1000 | Loss: 0.00001990
Iteration 119/1000 | Loss: 0.00001990
Iteration 120/1000 | Loss: 0.00001989
Iteration 121/1000 | Loss: 0.00001989
Iteration 122/1000 | Loss: 0.00001989
Iteration 123/1000 | Loss: 0.00001989
Iteration 124/1000 | Loss: 0.00001989
Iteration 125/1000 | Loss: 0.00001989
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00001988
Iteration 133/1000 | Loss: 0.00001988
Iteration 134/1000 | Loss: 0.00001988
Iteration 135/1000 | Loss: 0.00001988
Iteration 136/1000 | Loss: 0.00001988
Iteration 137/1000 | Loss: 0.00001988
Iteration 138/1000 | Loss: 0.00001988
Iteration 139/1000 | Loss: 0.00001988
Iteration 140/1000 | Loss: 0.00001988
Iteration 141/1000 | Loss: 0.00001987
Iteration 142/1000 | Loss: 0.00001987
Iteration 143/1000 | Loss: 0.00001987
Iteration 144/1000 | Loss: 0.00001987
Iteration 145/1000 | Loss: 0.00001987
Iteration 146/1000 | Loss: 0.00001987
Iteration 147/1000 | Loss: 0.00001987
Iteration 148/1000 | Loss: 0.00001987
Iteration 149/1000 | Loss: 0.00001987
Iteration 150/1000 | Loss: 0.00001987
Iteration 151/1000 | Loss: 0.00001987
Iteration 152/1000 | Loss: 0.00001987
Iteration 153/1000 | Loss: 0.00001987
Iteration 154/1000 | Loss: 0.00001987
Iteration 155/1000 | Loss: 0.00001987
Iteration 156/1000 | Loss: 0.00001987
Iteration 157/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.986712231882848e-05, 1.986712231882848e-05, 1.986712231882848e-05, 1.986712231882848e-05, 1.986712231882848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.986712231882848e-05

Optimization complete. Final v2v error: 3.7671761512756348 mm

Highest mean error: 3.9165289402008057 mm for frame 113

Lowest mean error: 3.006208896636963 mm for frame 0

Saving results

Total time: 37.677395820617676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803673
Iteration 2/25 | Loss: 0.00197135
Iteration 3/25 | Loss: 0.00154713
Iteration 4/25 | Loss: 0.00148425
Iteration 5/25 | Loss: 0.00147773
Iteration 6/25 | Loss: 0.00147490
Iteration 7/25 | Loss: 0.00142856
Iteration 8/25 | Loss: 0.00141609
Iteration 9/25 | Loss: 0.00141676
Iteration 10/25 | Loss: 0.00143248
Iteration 11/25 | Loss: 0.00142679
Iteration 12/25 | Loss: 0.00141385
Iteration 13/25 | Loss: 0.00140560
Iteration 14/25 | Loss: 0.00140344
Iteration 15/25 | Loss: 0.00140295
Iteration 16/25 | Loss: 0.00140279
Iteration 17/25 | Loss: 0.00140278
Iteration 18/25 | Loss: 0.00140276
Iteration 19/25 | Loss: 0.00140276
Iteration 20/25 | Loss: 0.00140276
Iteration 21/25 | Loss: 0.00140276
Iteration 22/25 | Loss: 0.00140276
Iteration 23/25 | Loss: 0.00140276
Iteration 24/25 | Loss: 0.00140276
Iteration 25/25 | Loss: 0.00140276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37046075
Iteration 2/25 | Loss: 0.00078292
Iteration 3/25 | Loss: 0.00078292
Iteration 4/25 | Loss: 0.00078292
Iteration 5/25 | Loss: 0.00078292
Iteration 6/25 | Loss: 0.00078292
Iteration 7/25 | Loss: 0.00078292
Iteration 8/25 | Loss: 0.00078292
Iteration 9/25 | Loss: 0.00078292
Iteration 10/25 | Loss: 0.00078292
Iteration 11/25 | Loss: 0.00078292
Iteration 12/25 | Loss: 0.00078292
Iteration 13/25 | Loss: 0.00078292
Iteration 14/25 | Loss: 0.00078292
Iteration 15/25 | Loss: 0.00078292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007829171372577548, 0.0007829171372577548, 0.0007829171372577548, 0.0007829171372577548, 0.0007829171372577548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007829171372577548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078292
Iteration 2/1000 | Loss: 0.00585253
Iteration 3/1000 | Loss: 0.00076268
Iteration 4/1000 | Loss: 0.00020044
Iteration 5/1000 | Loss: 0.00012504
Iteration 6/1000 | Loss: 0.00009691
Iteration 7/1000 | Loss: 0.00008907
Iteration 8/1000 | Loss: 0.00485892
Iteration 9/1000 | Loss: 0.00042152
Iteration 10/1000 | Loss: 0.00071881
Iteration 11/1000 | Loss: 0.00094499
Iteration 12/1000 | Loss: 0.00022842
Iteration 13/1000 | Loss: 0.00018248
Iteration 14/1000 | Loss: 0.00037277
Iteration 15/1000 | Loss: 0.00017085
Iteration 16/1000 | Loss: 0.00016176
Iteration 17/1000 | Loss: 0.00016588
Iteration 18/1000 | Loss: 0.00033830
Iteration 19/1000 | Loss: 0.00032071
Iteration 20/1000 | Loss: 0.00015463
Iteration 21/1000 | Loss: 0.00014559
Iteration 22/1000 | Loss: 0.00013919
Iteration 23/1000 | Loss: 0.00035101
Iteration 24/1000 | Loss: 0.00033049
Iteration 25/1000 | Loss: 0.00021074
Iteration 26/1000 | Loss: 0.00025350
Iteration 27/1000 | Loss: 0.00016496
Iteration 28/1000 | Loss: 0.00015392
Iteration 29/1000 | Loss: 0.00015918
Iteration 30/1000 | Loss: 0.00028697
Iteration 31/1000 | Loss: 0.00020080
Iteration 32/1000 | Loss: 0.00024090
Iteration 33/1000 | Loss: 0.00021607
Iteration 34/1000 | Loss: 0.00023034
Iteration 35/1000 | Loss: 0.00014654
Iteration 36/1000 | Loss: 0.00012662
Iteration 37/1000 | Loss: 0.00014092
Iteration 38/1000 | Loss: 0.00014983
Iteration 39/1000 | Loss: 0.00010281
Iteration 40/1000 | Loss: 0.00009792
Iteration 41/1000 | Loss: 0.00015071
Iteration 42/1000 | Loss: 0.00033151
Iteration 43/1000 | Loss: 0.00008857
Iteration 44/1000 | Loss: 0.00008285
Iteration 45/1000 | Loss: 0.00009608
Iteration 46/1000 | Loss: 0.00019794
Iteration 47/1000 | Loss: 0.00105306
Iteration 48/1000 | Loss: 0.00013982
Iteration 49/1000 | Loss: 0.00008899
Iteration 50/1000 | Loss: 0.00093614
Iteration 51/1000 | Loss: 0.00068955
Iteration 52/1000 | Loss: 0.00031972
Iteration 53/1000 | Loss: 0.00011226
Iteration 54/1000 | Loss: 0.00008808
Iteration 55/1000 | Loss: 0.00007240
Iteration 56/1000 | Loss: 0.00022008
Iteration 57/1000 | Loss: 0.00007657
Iteration 58/1000 | Loss: 0.00005877
Iteration 59/1000 | Loss: 0.00036336
Iteration 60/1000 | Loss: 0.00020976
Iteration 61/1000 | Loss: 0.00015923
Iteration 62/1000 | Loss: 0.00006227
Iteration 63/1000 | Loss: 0.00050120
Iteration 64/1000 | Loss: 0.00022186
Iteration 65/1000 | Loss: 0.00016540
Iteration 66/1000 | Loss: 0.00010089
Iteration 67/1000 | Loss: 0.00009646
Iteration 68/1000 | Loss: 0.00005504
Iteration 69/1000 | Loss: 0.00004500
Iteration 70/1000 | Loss: 0.00004375
Iteration 71/1000 | Loss: 0.00018252
Iteration 72/1000 | Loss: 0.00018607
Iteration 73/1000 | Loss: 0.00004928
Iteration 74/1000 | Loss: 0.00004165
Iteration 75/1000 | Loss: 0.00003692
Iteration 76/1000 | Loss: 0.00003617
Iteration 77/1000 | Loss: 0.00003500
Iteration 78/1000 | Loss: 0.00011704
Iteration 79/1000 | Loss: 0.00014857
Iteration 80/1000 | Loss: 0.00004035
Iteration 81/1000 | Loss: 0.00003272
Iteration 82/1000 | Loss: 0.00002992
Iteration 83/1000 | Loss: 0.00002911
Iteration 84/1000 | Loss: 0.00002825
Iteration 85/1000 | Loss: 0.00002723
Iteration 86/1000 | Loss: 0.00002640
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002531
Iteration 89/1000 | Loss: 0.00002492
Iteration 90/1000 | Loss: 0.00002469
Iteration 91/1000 | Loss: 0.00002442
Iteration 92/1000 | Loss: 0.00002437
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002432
Iteration 95/1000 | Loss: 0.00002432
Iteration 96/1000 | Loss: 0.00002428
Iteration 97/1000 | Loss: 0.00002421
Iteration 98/1000 | Loss: 0.00002418
Iteration 99/1000 | Loss: 0.00002403
Iteration 100/1000 | Loss: 0.00002401
Iteration 101/1000 | Loss: 0.00002399
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002396
Iteration 105/1000 | Loss: 0.00002395
Iteration 106/1000 | Loss: 0.00002395
Iteration 107/1000 | Loss: 0.00002395
Iteration 108/1000 | Loss: 0.00002394
Iteration 109/1000 | Loss: 0.00002394
Iteration 110/1000 | Loss: 0.00002394
Iteration 111/1000 | Loss: 0.00002393
Iteration 112/1000 | Loss: 0.00002391
Iteration 113/1000 | Loss: 0.00002387
Iteration 114/1000 | Loss: 0.00002387
Iteration 115/1000 | Loss: 0.00002387
Iteration 116/1000 | Loss: 0.00002386
Iteration 117/1000 | Loss: 0.00002386
Iteration 118/1000 | Loss: 0.00002386
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002385
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002385
Iteration 124/1000 | Loss: 0.00002385
Iteration 125/1000 | Loss: 0.00002385
Iteration 126/1000 | Loss: 0.00002385
Iteration 127/1000 | Loss: 0.00002384
Iteration 128/1000 | Loss: 0.00002384
Iteration 129/1000 | Loss: 0.00002384
Iteration 130/1000 | Loss: 0.00002384
Iteration 131/1000 | Loss: 0.00002384
Iteration 132/1000 | Loss: 0.00002384
Iteration 133/1000 | Loss: 0.00002384
Iteration 134/1000 | Loss: 0.00002384
Iteration 135/1000 | Loss: 0.00002384
Iteration 136/1000 | Loss: 0.00002384
Iteration 137/1000 | Loss: 0.00002384
Iteration 138/1000 | Loss: 0.00002384
Iteration 139/1000 | Loss: 0.00002384
Iteration 140/1000 | Loss: 0.00002384
Iteration 141/1000 | Loss: 0.00002384
Iteration 142/1000 | Loss: 0.00002384
Iteration 143/1000 | Loss: 0.00002384
Iteration 144/1000 | Loss: 0.00002384
Iteration 145/1000 | Loss: 0.00002384
Iteration 146/1000 | Loss: 0.00002384
Iteration 147/1000 | Loss: 0.00002384
Iteration 148/1000 | Loss: 0.00002384
Iteration 149/1000 | Loss: 0.00002384
Iteration 150/1000 | Loss: 0.00002384
Iteration 151/1000 | Loss: 0.00002384
Iteration 152/1000 | Loss: 0.00002384
Iteration 153/1000 | Loss: 0.00002384
Iteration 154/1000 | Loss: 0.00002384
Iteration 155/1000 | Loss: 0.00002384
Iteration 156/1000 | Loss: 0.00002384
Iteration 157/1000 | Loss: 0.00002384
Iteration 158/1000 | Loss: 0.00002384
Iteration 159/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.38398315559607e-05, 2.38398315559607e-05, 2.38398315559607e-05, 2.38398315559607e-05, 2.38398315559607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.38398315559607e-05

Optimization complete. Final v2v error: 4.146319389343262 mm

Highest mean error: 5.058971405029297 mm for frame 216

Lowest mean error: 3.631291627883911 mm for frame 150

Saving results

Total time: 190.51374077796936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497303
Iteration 2/25 | Loss: 0.00141998
Iteration 3/25 | Loss: 0.00135190
Iteration 4/25 | Loss: 0.00134187
Iteration 5/25 | Loss: 0.00133904
Iteration 6/25 | Loss: 0.00133904
Iteration 7/25 | Loss: 0.00133904
Iteration 8/25 | Loss: 0.00133904
Iteration 9/25 | Loss: 0.00133904
Iteration 10/25 | Loss: 0.00133904
Iteration 11/25 | Loss: 0.00133904
Iteration 12/25 | Loss: 0.00133904
Iteration 13/25 | Loss: 0.00133904
Iteration 14/25 | Loss: 0.00133904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013390379026532173, 0.0013390379026532173, 0.0013390379026532173, 0.0013390379026532173, 0.0013390379026532173]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013390379026532173

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38399911
Iteration 2/25 | Loss: 0.00095713
Iteration 3/25 | Loss: 0.00095713
Iteration 4/25 | Loss: 0.00095713
Iteration 5/25 | Loss: 0.00095713
Iteration 6/25 | Loss: 0.00095713
Iteration 7/25 | Loss: 0.00095712
Iteration 8/25 | Loss: 0.00095712
Iteration 9/25 | Loss: 0.00095712
Iteration 10/25 | Loss: 0.00095712
Iteration 11/25 | Loss: 0.00095712
Iteration 12/25 | Loss: 0.00095712
Iteration 13/25 | Loss: 0.00095712
Iteration 14/25 | Loss: 0.00095712
Iteration 15/25 | Loss: 0.00095712
Iteration 16/25 | Loss: 0.00095712
Iteration 17/25 | Loss: 0.00095712
Iteration 18/25 | Loss: 0.00095712
Iteration 19/25 | Loss: 0.00095712
Iteration 20/25 | Loss: 0.00095712
Iteration 21/25 | Loss: 0.00095712
Iteration 22/25 | Loss: 0.00095712
Iteration 23/25 | Loss: 0.00095712
Iteration 24/25 | Loss: 0.00095712
Iteration 25/25 | Loss: 0.00095712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095712
Iteration 2/1000 | Loss: 0.00004701
Iteration 3/1000 | Loss: 0.00003367
Iteration 4/1000 | Loss: 0.00002923
Iteration 5/1000 | Loss: 0.00002797
Iteration 6/1000 | Loss: 0.00002716
Iteration 7/1000 | Loss: 0.00002651
Iteration 8/1000 | Loss: 0.00002593
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002527
Iteration 11/1000 | Loss: 0.00002523
Iteration 12/1000 | Loss: 0.00002505
Iteration 13/1000 | Loss: 0.00002496
Iteration 14/1000 | Loss: 0.00002476
Iteration 15/1000 | Loss: 0.00002461
Iteration 16/1000 | Loss: 0.00002456
Iteration 17/1000 | Loss: 0.00002451
Iteration 18/1000 | Loss: 0.00002447
Iteration 19/1000 | Loss: 0.00002446
Iteration 20/1000 | Loss: 0.00002446
Iteration 21/1000 | Loss: 0.00002438
Iteration 22/1000 | Loss: 0.00002433
Iteration 23/1000 | Loss: 0.00002431
Iteration 24/1000 | Loss: 0.00002430
Iteration 25/1000 | Loss: 0.00002429
Iteration 26/1000 | Loss: 0.00002428
Iteration 27/1000 | Loss: 0.00002427
Iteration 28/1000 | Loss: 0.00002425
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002421
Iteration 31/1000 | Loss: 0.00002420
Iteration 32/1000 | Loss: 0.00002418
Iteration 33/1000 | Loss: 0.00002416
Iteration 34/1000 | Loss: 0.00002416
Iteration 35/1000 | Loss: 0.00002415
Iteration 36/1000 | Loss: 0.00002413
Iteration 37/1000 | Loss: 0.00002413
Iteration 38/1000 | Loss: 0.00002412
Iteration 39/1000 | Loss: 0.00002411
Iteration 40/1000 | Loss: 0.00002411
Iteration 41/1000 | Loss: 0.00002410
Iteration 42/1000 | Loss: 0.00002407
Iteration 43/1000 | Loss: 0.00002406
Iteration 44/1000 | Loss: 0.00002404
Iteration 45/1000 | Loss: 0.00002403
Iteration 46/1000 | Loss: 0.00002403
Iteration 47/1000 | Loss: 0.00002402
Iteration 48/1000 | Loss: 0.00002402
Iteration 49/1000 | Loss: 0.00002401
Iteration 50/1000 | Loss: 0.00002400
Iteration 51/1000 | Loss: 0.00002399
Iteration 52/1000 | Loss: 0.00002399
Iteration 53/1000 | Loss: 0.00002399
Iteration 54/1000 | Loss: 0.00002398
Iteration 55/1000 | Loss: 0.00002398
Iteration 56/1000 | Loss: 0.00002398
Iteration 57/1000 | Loss: 0.00002398
Iteration 58/1000 | Loss: 0.00002397
Iteration 59/1000 | Loss: 0.00002397
Iteration 60/1000 | Loss: 0.00002397
Iteration 61/1000 | Loss: 0.00002396
Iteration 62/1000 | Loss: 0.00002396
Iteration 63/1000 | Loss: 0.00002396
Iteration 64/1000 | Loss: 0.00002395
Iteration 65/1000 | Loss: 0.00002395
Iteration 66/1000 | Loss: 0.00002395
Iteration 67/1000 | Loss: 0.00002395
Iteration 68/1000 | Loss: 0.00002395
Iteration 69/1000 | Loss: 0.00002394
Iteration 70/1000 | Loss: 0.00002394
Iteration 71/1000 | Loss: 0.00002394
Iteration 72/1000 | Loss: 0.00002393
Iteration 73/1000 | Loss: 0.00002393
Iteration 74/1000 | Loss: 0.00002392
Iteration 75/1000 | Loss: 0.00002392
Iteration 76/1000 | Loss: 0.00002392
Iteration 77/1000 | Loss: 0.00002391
Iteration 78/1000 | Loss: 0.00002391
Iteration 79/1000 | Loss: 0.00002391
Iteration 80/1000 | Loss: 0.00002390
Iteration 81/1000 | Loss: 0.00002390
Iteration 82/1000 | Loss: 0.00002389
Iteration 83/1000 | Loss: 0.00002389
Iteration 84/1000 | Loss: 0.00002389
Iteration 85/1000 | Loss: 0.00002389
Iteration 86/1000 | Loss: 0.00002389
Iteration 87/1000 | Loss: 0.00002389
Iteration 88/1000 | Loss: 0.00002389
Iteration 89/1000 | Loss: 0.00002389
Iteration 90/1000 | Loss: 0.00002389
Iteration 91/1000 | Loss: 0.00002389
Iteration 92/1000 | Loss: 0.00002388
Iteration 93/1000 | Loss: 0.00002388
Iteration 94/1000 | Loss: 0.00002388
Iteration 95/1000 | Loss: 0.00002388
Iteration 96/1000 | Loss: 0.00002388
Iteration 97/1000 | Loss: 0.00002388
Iteration 98/1000 | Loss: 0.00002388
Iteration 99/1000 | Loss: 0.00002388
Iteration 100/1000 | Loss: 0.00002388
Iteration 101/1000 | Loss: 0.00002387
Iteration 102/1000 | Loss: 0.00002387
Iteration 103/1000 | Loss: 0.00002387
Iteration 104/1000 | Loss: 0.00002387
Iteration 105/1000 | Loss: 0.00002386
Iteration 106/1000 | Loss: 0.00002386
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00002386
Iteration 109/1000 | Loss: 0.00002385
Iteration 110/1000 | Loss: 0.00002385
Iteration 111/1000 | Loss: 0.00002385
Iteration 112/1000 | Loss: 0.00002385
Iteration 113/1000 | Loss: 0.00002385
Iteration 114/1000 | Loss: 0.00002385
Iteration 115/1000 | Loss: 0.00002385
Iteration 116/1000 | Loss: 0.00002385
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002385
Iteration 119/1000 | Loss: 0.00002385
Iteration 120/1000 | Loss: 0.00002385
Iteration 121/1000 | Loss: 0.00002385
Iteration 122/1000 | Loss: 0.00002385
Iteration 123/1000 | Loss: 0.00002385
Iteration 124/1000 | Loss: 0.00002385
Iteration 125/1000 | Loss: 0.00002385
Iteration 126/1000 | Loss: 0.00002385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.384870094829239e-05, 2.384870094829239e-05, 2.384870094829239e-05, 2.384870094829239e-05, 2.384870094829239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.384870094829239e-05

Optimization complete. Final v2v error: 3.795907497406006 mm

Highest mean error: 4.789458274841309 mm for frame 92

Lowest mean error: 3.312612533569336 mm for frame 137

Saving results

Total time: 41.42945694923401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569484
Iteration 2/25 | Loss: 0.00151120
Iteration 3/25 | Loss: 0.00141639
Iteration 4/25 | Loss: 0.00140611
Iteration 5/25 | Loss: 0.00140207
Iteration 6/25 | Loss: 0.00140190
Iteration 7/25 | Loss: 0.00140190
Iteration 8/25 | Loss: 0.00140190
Iteration 9/25 | Loss: 0.00140190
Iteration 10/25 | Loss: 0.00140190
Iteration 11/25 | Loss: 0.00140190
Iteration 12/25 | Loss: 0.00140190
Iteration 13/25 | Loss: 0.00140190
Iteration 14/25 | Loss: 0.00140190
Iteration 15/25 | Loss: 0.00140190
Iteration 16/25 | Loss: 0.00140190
Iteration 17/25 | Loss: 0.00140190
Iteration 18/25 | Loss: 0.00140190
Iteration 19/25 | Loss: 0.00140190
Iteration 20/25 | Loss: 0.00140190
Iteration 21/25 | Loss: 0.00140190
Iteration 22/25 | Loss: 0.00140190
Iteration 23/25 | Loss: 0.00140190
Iteration 24/25 | Loss: 0.00140190
Iteration 25/25 | Loss: 0.00140190

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.18234730
Iteration 2/25 | Loss: 0.00095890
Iteration 3/25 | Loss: 0.00095890
Iteration 4/25 | Loss: 0.00095890
Iteration 5/25 | Loss: 0.00095890
Iteration 6/25 | Loss: 0.00095890
Iteration 7/25 | Loss: 0.00095890
Iteration 8/25 | Loss: 0.00095890
Iteration 9/25 | Loss: 0.00095890
Iteration 10/25 | Loss: 0.00095890
Iteration 11/25 | Loss: 0.00095890
Iteration 12/25 | Loss: 0.00095890
Iteration 13/25 | Loss: 0.00095890
Iteration 14/25 | Loss: 0.00095890
Iteration 15/25 | Loss: 0.00095890
Iteration 16/25 | Loss: 0.00095890
Iteration 17/25 | Loss: 0.00095890
Iteration 18/25 | Loss: 0.00095890
Iteration 19/25 | Loss: 0.00095890
Iteration 20/25 | Loss: 0.00095890
Iteration 21/25 | Loss: 0.00095890
Iteration 22/25 | Loss: 0.00095890
Iteration 23/25 | Loss: 0.00095890
Iteration 24/25 | Loss: 0.00095890
Iteration 25/25 | Loss: 0.00095890

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095890
Iteration 2/1000 | Loss: 0.00002886
Iteration 3/1000 | Loss: 0.00002237
Iteration 4/1000 | Loss: 0.00002113
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00002004
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001927
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001865
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001840
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001840
Iteration 26/1000 | Loss: 0.00001839
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001833
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001830
Iteration 35/1000 | Loss: 0.00001829
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001826
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001825
Iteration 44/1000 | Loss: 0.00001825
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001824
Iteration 53/1000 | Loss: 0.00001824
Iteration 54/1000 | Loss: 0.00001824
Iteration 55/1000 | Loss: 0.00001823
Iteration 56/1000 | Loss: 0.00001823
Iteration 57/1000 | Loss: 0.00001823
Iteration 58/1000 | Loss: 0.00001823
Iteration 59/1000 | Loss: 0.00001823
Iteration 60/1000 | Loss: 0.00001823
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001822
Iteration 63/1000 | Loss: 0.00001822
Iteration 64/1000 | Loss: 0.00001822
Iteration 65/1000 | Loss: 0.00001822
Iteration 66/1000 | Loss: 0.00001821
Iteration 67/1000 | Loss: 0.00001821
Iteration 68/1000 | Loss: 0.00001820
Iteration 69/1000 | Loss: 0.00001820
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001816
Iteration 75/1000 | Loss: 0.00001816
Iteration 76/1000 | Loss: 0.00001815
Iteration 77/1000 | Loss: 0.00001815
Iteration 78/1000 | Loss: 0.00001815
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001814
Iteration 83/1000 | Loss: 0.00001814
Iteration 84/1000 | Loss: 0.00001814
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001812
Iteration 87/1000 | Loss: 0.00001812
Iteration 88/1000 | Loss: 0.00001811
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001811
Iteration 91/1000 | Loss: 0.00001811
Iteration 92/1000 | Loss: 0.00001811
Iteration 93/1000 | Loss: 0.00001811
Iteration 94/1000 | Loss: 0.00001811
Iteration 95/1000 | Loss: 0.00001811
Iteration 96/1000 | Loss: 0.00001811
Iteration 97/1000 | Loss: 0.00001811
Iteration 98/1000 | Loss: 0.00001811
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001810
Iteration 101/1000 | Loss: 0.00001810
Iteration 102/1000 | Loss: 0.00001810
Iteration 103/1000 | Loss: 0.00001809
Iteration 104/1000 | Loss: 0.00001809
Iteration 105/1000 | Loss: 0.00001809
Iteration 106/1000 | Loss: 0.00001809
Iteration 107/1000 | Loss: 0.00001809
Iteration 108/1000 | Loss: 0.00001809
Iteration 109/1000 | Loss: 0.00001809
Iteration 110/1000 | Loss: 0.00001809
Iteration 111/1000 | Loss: 0.00001809
Iteration 112/1000 | Loss: 0.00001808
Iteration 113/1000 | Loss: 0.00001808
Iteration 114/1000 | Loss: 0.00001808
Iteration 115/1000 | Loss: 0.00001808
Iteration 116/1000 | Loss: 0.00001808
Iteration 117/1000 | Loss: 0.00001808
Iteration 118/1000 | Loss: 0.00001808
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001808
Iteration 121/1000 | Loss: 0.00001808
Iteration 122/1000 | Loss: 0.00001808
Iteration 123/1000 | Loss: 0.00001808
Iteration 124/1000 | Loss: 0.00001807
Iteration 125/1000 | Loss: 0.00001807
Iteration 126/1000 | Loss: 0.00001807
Iteration 127/1000 | Loss: 0.00001807
Iteration 128/1000 | Loss: 0.00001807
Iteration 129/1000 | Loss: 0.00001807
Iteration 130/1000 | Loss: 0.00001807
Iteration 131/1000 | Loss: 0.00001807
Iteration 132/1000 | Loss: 0.00001807
Iteration 133/1000 | Loss: 0.00001807
Iteration 134/1000 | Loss: 0.00001807
Iteration 135/1000 | Loss: 0.00001807
Iteration 136/1000 | Loss: 0.00001807
Iteration 137/1000 | Loss: 0.00001807
Iteration 138/1000 | Loss: 0.00001807
Iteration 139/1000 | Loss: 0.00001807
Iteration 140/1000 | Loss: 0.00001807
Iteration 141/1000 | Loss: 0.00001807
Iteration 142/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.8066879420075566e-05, 1.8066879420075566e-05, 1.8066879420075566e-05, 1.8066879420075566e-05, 1.8066879420075566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8066879420075566e-05

Optimization complete. Final v2v error: 3.5765597820281982 mm

Highest mean error: 3.7275543212890625 mm for frame 204

Lowest mean error: 3.4183528423309326 mm for frame 212

Saving results

Total time: 45.099056243896484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596646
Iteration 2/25 | Loss: 0.00145635
Iteration 3/25 | Loss: 0.00139271
Iteration 4/25 | Loss: 0.00138681
Iteration 5/25 | Loss: 0.00138566
Iteration 6/25 | Loss: 0.00138566
Iteration 7/25 | Loss: 0.00138566
Iteration 8/25 | Loss: 0.00138566
Iteration 9/25 | Loss: 0.00138566
Iteration 10/25 | Loss: 0.00138566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013856579316779971, 0.0013856579316779971, 0.0013856579316779971, 0.0013856579316779971, 0.0013856579316779971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013856579316779971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40654099
Iteration 2/25 | Loss: 0.00089364
Iteration 3/25 | Loss: 0.00089360
Iteration 4/25 | Loss: 0.00089360
Iteration 5/25 | Loss: 0.00089360
Iteration 6/25 | Loss: 0.00089360
Iteration 7/25 | Loss: 0.00089360
Iteration 8/25 | Loss: 0.00089360
Iteration 9/25 | Loss: 0.00089360
Iteration 10/25 | Loss: 0.00089360
Iteration 11/25 | Loss: 0.00089360
Iteration 12/25 | Loss: 0.00089360
Iteration 13/25 | Loss: 0.00089360
Iteration 14/25 | Loss: 0.00089360
Iteration 15/25 | Loss: 0.00089360
Iteration 16/25 | Loss: 0.00089360
Iteration 17/25 | Loss: 0.00089360
Iteration 18/25 | Loss: 0.00089360
Iteration 19/25 | Loss: 0.00089360
Iteration 20/25 | Loss: 0.00089360
Iteration 21/25 | Loss: 0.00089360
Iteration 22/25 | Loss: 0.00089360
Iteration 23/25 | Loss: 0.00089360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008936004596762359, 0.0008936004596762359, 0.0008936004596762359, 0.0008936004596762359, 0.0008936004596762359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008936004596762359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089360
Iteration 2/1000 | Loss: 0.00004393
Iteration 3/1000 | Loss: 0.00002818
Iteration 4/1000 | Loss: 0.00002450
Iteration 5/1000 | Loss: 0.00002319
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002173
Iteration 8/1000 | Loss: 0.00002140
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00002013
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001989
Iteration 17/1000 | Loss: 0.00001983
Iteration 18/1000 | Loss: 0.00001980
Iteration 19/1000 | Loss: 0.00001975
Iteration 20/1000 | Loss: 0.00001967
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001965
Iteration 23/1000 | Loss: 0.00001964
Iteration 24/1000 | Loss: 0.00001964
Iteration 25/1000 | Loss: 0.00001963
Iteration 26/1000 | Loss: 0.00001963
Iteration 27/1000 | Loss: 0.00001962
Iteration 28/1000 | Loss: 0.00001962
Iteration 29/1000 | Loss: 0.00001962
Iteration 30/1000 | Loss: 0.00001961
Iteration 31/1000 | Loss: 0.00001961
Iteration 32/1000 | Loss: 0.00001960
Iteration 33/1000 | Loss: 0.00001960
Iteration 34/1000 | Loss: 0.00001960
Iteration 35/1000 | Loss: 0.00001958
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001956
Iteration 39/1000 | Loss: 0.00001956
Iteration 40/1000 | Loss: 0.00001956
Iteration 41/1000 | Loss: 0.00001956
Iteration 42/1000 | Loss: 0.00001956
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001955
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001952
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001951
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001948
Iteration 58/1000 | Loss: 0.00001948
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001946
Iteration 64/1000 | Loss: 0.00001946
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001945
Iteration 69/1000 | Loss: 0.00001945
Iteration 70/1000 | Loss: 0.00001945
Iteration 71/1000 | Loss: 0.00001945
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001944
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001942
Iteration 77/1000 | Loss: 0.00001942
Iteration 78/1000 | Loss: 0.00001942
Iteration 79/1000 | Loss: 0.00001941
Iteration 80/1000 | Loss: 0.00001941
Iteration 81/1000 | Loss: 0.00001941
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001940
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001939
Iteration 93/1000 | Loss: 0.00001939
Iteration 94/1000 | Loss: 0.00001938
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00001938
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001938
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001937
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001937
Iteration 109/1000 | Loss: 0.00001937
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001936
Iteration 115/1000 | Loss: 0.00001935
Iteration 116/1000 | Loss: 0.00001935
Iteration 117/1000 | Loss: 0.00001935
Iteration 118/1000 | Loss: 0.00001935
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001935
Iteration 121/1000 | Loss: 0.00001934
Iteration 122/1000 | Loss: 0.00001934
Iteration 123/1000 | Loss: 0.00001934
Iteration 124/1000 | Loss: 0.00001934
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001934
Iteration 127/1000 | Loss: 0.00001934
Iteration 128/1000 | Loss: 0.00001934
Iteration 129/1000 | Loss: 0.00001933
Iteration 130/1000 | Loss: 0.00001933
Iteration 131/1000 | Loss: 0.00001933
Iteration 132/1000 | Loss: 0.00001933
Iteration 133/1000 | Loss: 0.00001933
Iteration 134/1000 | Loss: 0.00001933
Iteration 135/1000 | Loss: 0.00001933
Iteration 136/1000 | Loss: 0.00001933
Iteration 137/1000 | Loss: 0.00001933
Iteration 138/1000 | Loss: 0.00001933
Iteration 139/1000 | Loss: 0.00001932
Iteration 140/1000 | Loss: 0.00001932
Iteration 141/1000 | Loss: 0.00001932
Iteration 142/1000 | Loss: 0.00001932
Iteration 143/1000 | Loss: 0.00001932
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001932
Iteration 146/1000 | Loss: 0.00001932
Iteration 147/1000 | Loss: 0.00001932
Iteration 148/1000 | Loss: 0.00001932
Iteration 149/1000 | Loss: 0.00001932
Iteration 150/1000 | Loss: 0.00001932
Iteration 151/1000 | Loss: 0.00001931
Iteration 152/1000 | Loss: 0.00001931
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.931312908709515e-05, 1.931312908709515e-05, 1.931312908709515e-05, 1.931312908709515e-05, 1.931312908709515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.931312908709515e-05

Optimization complete. Final v2v error: 3.6651344299316406 mm

Highest mean error: 4.050648212432861 mm for frame 157

Lowest mean error: 3.179011344909668 mm for frame 239

Saving results

Total time: 47.253253698349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786148
Iteration 2/25 | Loss: 0.00154466
Iteration 3/25 | Loss: 0.00132033
Iteration 4/25 | Loss: 0.00130606
Iteration 5/25 | Loss: 0.00130075
Iteration 6/25 | Loss: 0.00130015
Iteration 7/25 | Loss: 0.00130015
Iteration 8/25 | Loss: 0.00130015
Iteration 9/25 | Loss: 0.00130015
Iteration 10/25 | Loss: 0.00130015
Iteration 11/25 | Loss: 0.00130015
Iteration 12/25 | Loss: 0.00130015
Iteration 13/25 | Loss: 0.00130015
Iteration 14/25 | Loss: 0.00130015
Iteration 15/25 | Loss: 0.00130015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001300151809118688, 0.001300151809118688, 0.001300151809118688, 0.001300151809118688, 0.001300151809118688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001300151809118688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14451540
Iteration 2/25 | Loss: 0.00089849
Iteration 3/25 | Loss: 0.00089849
Iteration 4/25 | Loss: 0.00089849
Iteration 5/25 | Loss: 0.00089849
Iteration 6/25 | Loss: 0.00089849
Iteration 7/25 | Loss: 0.00089849
Iteration 8/25 | Loss: 0.00089849
Iteration 9/25 | Loss: 0.00089849
Iteration 10/25 | Loss: 0.00089849
Iteration 11/25 | Loss: 0.00089849
Iteration 12/25 | Loss: 0.00089849
Iteration 13/25 | Loss: 0.00089849
Iteration 14/25 | Loss: 0.00089849
Iteration 15/25 | Loss: 0.00089849
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008984867599792778, 0.0008984867599792778, 0.0008984867599792778, 0.0008984867599792778, 0.0008984867599792778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008984867599792778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089849
Iteration 2/1000 | Loss: 0.00005189
Iteration 3/1000 | Loss: 0.00003376
Iteration 4/1000 | Loss: 0.00002681
Iteration 5/1000 | Loss: 0.00002420
Iteration 6/1000 | Loss: 0.00002247
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001857
Iteration 12/1000 | Loss: 0.00001829
Iteration 13/1000 | Loss: 0.00001808
Iteration 14/1000 | Loss: 0.00001794
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001732
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001706
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001706
Iteration 27/1000 | Loss: 0.00001706
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001699
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001696
Iteration 39/1000 | Loss: 0.00001696
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001695
Iteration 42/1000 | Loss: 0.00001695
Iteration 43/1000 | Loss: 0.00001694
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001693
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001692
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001691
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001686
Iteration 63/1000 | Loss: 0.00001686
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001681
Iteration 76/1000 | Loss: 0.00001681
Iteration 77/1000 | Loss: 0.00001681
Iteration 78/1000 | Loss: 0.00001681
Iteration 79/1000 | Loss: 0.00001681
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001679
Iteration 89/1000 | Loss: 0.00001679
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001679
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001678
Iteration 108/1000 | Loss: 0.00001678
Iteration 109/1000 | Loss: 0.00001677
Iteration 110/1000 | Loss: 0.00001677
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001677
Iteration 117/1000 | Loss: 0.00001677
Iteration 118/1000 | Loss: 0.00001677
Iteration 119/1000 | Loss: 0.00001677
Iteration 120/1000 | Loss: 0.00001677
Iteration 121/1000 | Loss: 0.00001677
Iteration 122/1000 | Loss: 0.00001677
Iteration 123/1000 | Loss: 0.00001676
Iteration 124/1000 | Loss: 0.00001676
Iteration 125/1000 | Loss: 0.00001676
Iteration 126/1000 | Loss: 0.00001676
Iteration 127/1000 | Loss: 0.00001676
Iteration 128/1000 | Loss: 0.00001676
Iteration 129/1000 | Loss: 0.00001676
Iteration 130/1000 | Loss: 0.00001676
Iteration 131/1000 | Loss: 0.00001676
Iteration 132/1000 | Loss: 0.00001676
Iteration 133/1000 | Loss: 0.00001676
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.675979365245439e-05, 1.675979365245439e-05, 1.675979365245439e-05, 1.675979365245439e-05, 1.675979365245439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.675979365245439e-05

Optimization complete. Final v2v error: 3.3946356773376465 mm

Highest mean error: 4.463335990905762 mm for frame 82

Lowest mean error: 2.764176607131958 mm for frame 197

Saving results

Total time: 51.51134705543518
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771372
Iteration 2/25 | Loss: 0.00177813
Iteration 3/25 | Loss: 0.00156245
Iteration 4/25 | Loss: 0.00145885
Iteration 5/25 | Loss: 0.00142740
Iteration 6/25 | Loss: 0.00139397
Iteration 7/25 | Loss: 0.00136413
Iteration 8/25 | Loss: 0.00137647
Iteration 9/25 | Loss: 0.00134911
Iteration 10/25 | Loss: 0.00132548
Iteration 11/25 | Loss: 0.00131662
Iteration 12/25 | Loss: 0.00131362
Iteration 13/25 | Loss: 0.00131321
Iteration 14/25 | Loss: 0.00131269
Iteration 15/25 | Loss: 0.00131292
Iteration 16/25 | Loss: 0.00131064
Iteration 17/25 | Loss: 0.00130950
Iteration 18/25 | Loss: 0.00130931
Iteration 19/25 | Loss: 0.00130929
Iteration 20/25 | Loss: 0.00130929
Iteration 21/25 | Loss: 0.00130929
Iteration 22/25 | Loss: 0.00130928
Iteration 23/25 | Loss: 0.00130928
Iteration 24/25 | Loss: 0.00130928
Iteration 25/25 | Loss: 0.00130928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87055635
Iteration 2/25 | Loss: 0.00097304
Iteration 3/25 | Loss: 0.00097304
Iteration 4/25 | Loss: 0.00097304
Iteration 5/25 | Loss: 0.00097304
Iteration 6/25 | Loss: 0.00097304
Iteration 7/25 | Loss: 0.00097304
Iteration 8/25 | Loss: 0.00097304
Iteration 9/25 | Loss: 0.00097304
Iteration 10/25 | Loss: 0.00097304
Iteration 11/25 | Loss: 0.00097304
Iteration 12/25 | Loss: 0.00097304
Iteration 13/25 | Loss: 0.00097304
Iteration 14/25 | Loss: 0.00097304
Iteration 15/25 | Loss: 0.00097304
Iteration 16/25 | Loss: 0.00097304
Iteration 17/25 | Loss: 0.00097304
Iteration 18/25 | Loss: 0.00097304
Iteration 19/25 | Loss: 0.00097304
Iteration 20/25 | Loss: 0.00097304
Iteration 21/25 | Loss: 0.00097304
Iteration 22/25 | Loss: 0.00097304
Iteration 23/25 | Loss: 0.00097304
Iteration 24/25 | Loss: 0.00097304
Iteration 25/25 | Loss: 0.00097304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097304
Iteration 2/1000 | Loss: 0.00005074
Iteration 3/1000 | Loss: 0.00010586
Iteration 4/1000 | Loss: 0.00047591
Iteration 5/1000 | Loss: 0.00003124
Iteration 6/1000 | Loss: 0.00002705
Iteration 7/1000 | Loss: 0.00002498
Iteration 8/1000 | Loss: 0.00012824
Iteration 9/1000 | Loss: 0.00002356
Iteration 10/1000 | Loss: 0.00002259
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002123
Iteration 14/1000 | Loss: 0.00002087
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002036
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002012
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00002005
Iteration 24/1000 | Loss: 0.00002004
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00002003
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001998
Iteration 30/1000 | Loss: 0.00008117
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001984
Iteration 33/1000 | Loss: 0.00001979
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001978
Iteration 36/1000 | Loss: 0.00001978
Iteration 37/1000 | Loss: 0.00001977
Iteration 38/1000 | Loss: 0.00001977
Iteration 39/1000 | Loss: 0.00001977
Iteration 40/1000 | Loss: 0.00001976
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001975
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001974
Iteration 45/1000 | Loss: 0.00001974
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001971
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001970
Iteration 56/1000 | Loss: 0.00001970
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001968
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001967
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001967
Iteration 70/1000 | Loss: 0.00001966
Iteration 71/1000 | Loss: 0.00001966
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001964
Iteration 84/1000 | Loss: 0.00001964
Iteration 85/1000 | Loss: 0.00001964
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001958
Iteration 121/1000 | Loss: 0.00001958
Iteration 122/1000 | Loss: 0.00001958
Iteration 123/1000 | Loss: 0.00001958
Iteration 124/1000 | Loss: 0.00001958
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001957
Iteration 131/1000 | Loss: 0.00001957
Iteration 132/1000 | Loss: 0.00001957
Iteration 133/1000 | Loss: 0.00001957
Iteration 134/1000 | Loss: 0.00001957
Iteration 135/1000 | Loss: 0.00001957
Iteration 136/1000 | Loss: 0.00001956
Iteration 137/1000 | Loss: 0.00001956
Iteration 138/1000 | Loss: 0.00001956
Iteration 139/1000 | Loss: 0.00001955
Iteration 140/1000 | Loss: 0.00001955
Iteration 141/1000 | Loss: 0.00001955
Iteration 142/1000 | Loss: 0.00001955
Iteration 143/1000 | Loss: 0.00001954
Iteration 144/1000 | Loss: 0.00001954
Iteration 145/1000 | Loss: 0.00001954
Iteration 146/1000 | Loss: 0.00001954
Iteration 147/1000 | Loss: 0.00001953
Iteration 148/1000 | Loss: 0.00001953
Iteration 149/1000 | Loss: 0.00001953
Iteration 150/1000 | Loss: 0.00001953
Iteration 151/1000 | Loss: 0.00001953
Iteration 152/1000 | Loss: 0.00001953
Iteration 153/1000 | Loss: 0.00001953
Iteration 154/1000 | Loss: 0.00001952
Iteration 155/1000 | Loss: 0.00001952
Iteration 156/1000 | Loss: 0.00001952
Iteration 157/1000 | Loss: 0.00001952
Iteration 158/1000 | Loss: 0.00001952
Iteration 159/1000 | Loss: 0.00001952
Iteration 160/1000 | Loss: 0.00001952
Iteration 161/1000 | Loss: 0.00001952
Iteration 162/1000 | Loss: 0.00001951
Iteration 163/1000 | Loss: 0.00001951
Iteration 164/1000 | Loss: 0.00001951
Iteration 165/1000 | Loss: 0.00001951
Iteration 166/1000 | Loss: 0.00001951
Iteration 167/1000 | Loss: 0.00001951
Iteration 168/1000 | Loss: 0.00001951
Iteration 169/1000 | Loss: 0.00001951
Iteration 170/1000 | Loss: 0.00001951
Iteration 171/1000 | Loss: 0.00001951
Iteration 172/1000 | Loss: 0.00001950
Iteration 173/1000 | Loss: 0.00001950
Iteration 174/1000 | Loss: 0.00001950
Iteration 175/1000 | Loss: 0.00001949
Iteration 176/1000 | Loss: 0.00001949
Iteration 177/1000 | Loss: 0.00001949
Iteration 178/1000 | Loss: 0.00001949
Iteration 179/1000 | Loss: 0.00001948
Iteration 180/1000 | Loss: 0.00001948
Iteration 181/1000 | Loss: 0.00001948
Iteration 182/1000 | Loss: 0.00001948
Iteration 183/1000 | Loss: 0.00001948
Iteration 184/1000 | Loss: 0.00001948
Iteration 185/1000 | Loss: 0.00001948
Iteration 186/1000 | Loss: 0.00001948
Iteration 187/1000 | Loss: 0.00001947
Iteration 188/1000 | Loss: 0.00001947
Iteration 189/1000 | Loss: 0.00001947
Iteration 190/1000 | Loss: 0.00001947
Iteration 191/1000 | Loss: 0.00001946
Iteration 192/1000 | Loss: 0.00001946
Iteration 193/1000 | Loss: 0.00001946
Iteration 194/1000 | Loss: 0.00001946
Iteration 195/1000 | Loss: 0.00001946
Iteration 196/1000 | Loss: 0.00001946
Iteration 197/1000 | Loss: 0.00001945
Iteration 198/1000 | Loss: 0.00001945
Iteration 199/1000 | Loss: 0.00001945
Iteration 200/1000 | Loss: 0.00001945
Iteration 201/1000 | Loss: 0.00001945
Iteration 202/1000 | Loss: 0.00001945
Iteration 203/1000 | Loss: 0.00001945
Iteration 204/1000 | Loss: 0.00001945
Iteration 205/1000 | Loss: 0.00001945
Iteration 206/1000 | Loss: 0.00001945
Iteration 207/1000 | Loss: 0.00001945
Iteration 208/1000 | Loss: 0.00001945
Iteration 209/1000 | Loss: 0.00001945
Iteration 210/1000 | Loss: 0.00001944
Iteration 211/1000 | Loss: 0.00001944
Iteration 212/1000 | Loss: 0.00001944
Iteration 213/1000 | Loss: 0.00001944
Iteration 214/1000 | Loss: 0.00001944
Iteration 215/1000 | Loss: 0.00001944
Iteration 216/1000 | Loss: 0.00001944
Iteration 217/1000 | Loss: 0.00001944
Iteration 218/1000 | Loss: 0.00001943
Iteration 219/1000 | Loss: 0.00001943
Iteration 220/1000 | Loss: 0.00001943
Iteration 221/1000 | Loss: 0.00001943
Iteration 222/1000 | Loss: 0.00001943
Iteration 223/1000 | Loss: 0.00001943
Iteration 224/1000 | Loss: 0.00001943
Iteration 225/1000 | Loss: 0.00001943
Iteration 226/1000 | Loss: 0.00001943
Iteration 227/1000 | Loss: 0.00001942
Iteration 228/1000 | Loss: 0.00001942
Iteration 229/1000 | Loss: 0.00001942
Iteration 230/1000 | Loss: 0.00001942
Iteration 231/1000 | Loss: 0.00001942
Iteration 232/1000 | Loss: 0.00001942
Iteration 233/1000 | Loss: 0.00001941
Iteration 234/1000 | Loss: 0.00001941
Iteration 235/1000 | Loss: 0.00001941
Iteration 236/1000 | Loss: 0.00001941
Iteration 237/1000 | Loss: 0.00001941
Iteration 238/1000 | Loss: 0.00001941
Iteration 239/1000 | Loss: 0.00001941
Iteration 240/1000 | Loss: 0.00001941
Iteration 241/1000 | Loss: 0.00001941
Iteration 242/1000 | Loss: 0.00001941
Iteration 243/1000 | Loss: 0.00001941
Iteration 244/1000 | Loss: 0.00001941
Iteration 245/1000 | Loss: 0.00001941
Iteration 246/1000 | Loss: 0.00001940
Iteration 247/1000 | Loss: 0.00001940
Iteration 248/1000 | Loss: 0.00001940
Iteration 249/1000 | Loss: 0.00001940
Iteration 250/1000 | Loss: 0.00001940
Iteration 251/1000 | Loss: 0.00001940
Iteration 252/1000 | Loss: 0.00001940
Iteration 253/1000 | Loss: 0.00001940
Iteration 254/1000 | Loss: 0.00001940
Iteration 255/1000 | Loss: 0.00001940
Iteration 256/1000 | Loss: 0.00001940
Iteration 257/1000 | Loss: 0.00001940
Iteration 258/1000 | Loss: 0.00001940
Iteration 259/1000 | Loss: 0.00001940
Iteration 260/1000 | Loss: 0.00001940
Iteration 261/1000 | Loss: 0.00001940
Iteration 262/1000 | Loss: 0.00001940
Iteration 263/1000 | Loss: 0.00001940
Iteration 264/1000 | Loss: 0.00001940
Iteration 265/1000 | Loss: 0.00001940
Iteration 266/1000 | Loss: 0.00001940
Iteration 267/1000 | Loss: 0.00001940
Iteration 268/1000 | Loss: 0.00001940
Iteration 269/1000 | Loss: 0.00001940
Iteration 270/1000 | Loss: 0.00001940
Iteration 271/1000 | Loss: 0.00001940
Iteration 272/1000 | Loss: 0.00001940
Iteration 273/1000 | Loss: 0.00001940
Iteration 274/1000 | Loss: 0.00001940
Iteration 275/1000 | Loss: 0.00001940
Iteration 276/1000 | Loss: 0.00001940
Iteration 277/1000 | Loss: 0.00001940
Iteration 278/1000 | Loss: 0.00001940
Iteration 279/1000 | Loss: 0.00001940
Iteration 280/1000 | Loss: 0.00001940
Iteration 281/1000 | Loss: 0.00001940
Iteration 282/1000 | Loss: 0.00001940
Iteration 283/1000 | Loss: 0.00001940
Iteration 284/1000 | Loss: 0.00001940
Iteration 285/1000 | Loss: 0.00001940
Iteration 286/1000 | Loss: 0.00001940
Iteration 287/1000 | Loss: 0.00001940
Iteration 288/1000 | Loss: 0.00001940
Iteration 289/1000 | Loss: 0.00001940
Iteration 290/1000 | Loss: 0.00001940
Iteration 291/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.939628600666765e-05, 1.939628600666765e-05, 1.939628600666765e-05, 1.939628600666765e-05, 1.939628600666765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.939628600666765e-05

Optimization complete. Final v2v error: 3.717186212539673 mm

Highest mean error: 5.510459899902344 mm for frame 168

Lowest mean error: 3.046891212463379 mm for frame 0

Saving results

Total time: 91.51492166519165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835815
Iteration 2/25 | Loss: 0.00185008
Iteration 3/25 | Loss: 0.00145188
Iteration 4/25 | Loss: 0.00141409
Iteration 5/25 | Loss: 0.00141414
Iteration 6/25 | Loss: 0.00139952
Iteration 7/25 | Loss: 0.00140584
Iteration 8/25 | Loss: 0.00139672
Iteration 9/25 | Loss: 0.00139121
Iteration 10/25 | Loss: 0.00138960
Iteration 11/25 | Loss: 0.00138899
Iteration 12/25 | Loss: 0.00139693
Iteration 13/25 | Loss: 0.00139369
Iteration 14/25 | Loss: 0.00139045
Iteration 15/25 | Loss: 0.00138371
Iteration 16/25 | Loss: 0.00137995
Iteration 17/25 | Loss: 0.00137939
Iteration 18/25 | Loss: 0.00137892
Iteration 19/25 | Loss: 0.00137868
Iteration 20/25 | Loss: 0.00137855
Iteration 21/25 | Loss: 0.00137852
Iteration 22/25 | Loss: 0.00137852
Iteration 23/25 | Loss: 0.00137852
Iteration 24/25 | Loss: 0.00137852
Iteration 25/25 | Loss: 0.00137848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66535258
Iteration 2/25 | Loss: 0.00114372
Iteration 3/25 | Loss: 0.00099069
Iteration 4/25 | Loss: 0.00099069
Iteration 5/25 | Loss: 0.00099069
Iteration 6/25 | Loss: 0.00099069
Iteration 7/25 | Loss: 0.00099068
Iteration 8/25 | Loss: 0.00099068
Iteration 9/25 | Loss: 0.00099068
Iteration 10/25 | Loss: 0.00099068
Iteration 11/25 | Loss: 0.00099068
Iteration 12/25 | Loss: 0.00099068
Iteration 13/25 | Loss: 0.00099068
Iteration 14/25 | Loss: 0.00099068
Iteration 15/25 | Loss: 0.00099068
Iteration 16/25 | Loss: 0.00099068
Iteration 17/25 | Loss: 0.00099068
Iteration 18/25 | Loss: 0.00099068
Iteration 19/25 | Loss: 0.00099068
Iteration 20/25 | Loss: 0.00099068
Iteration 21/25 | Loss: 0.00099068
Iteration 22/25 | Loss: 0.00099068
Iteration 23/25 | Loss: 0.00099068
Iteration 24/25 | Loss: 0.00099068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009906835621222854, 0.0009906835621222854, 0.0009906835621222854, 0.0009906835621222854, 0.0009906835621222854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009906835621222854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099068
Iteration 2/1000 | Loss: 0.00006400
Iteration 3/1000 | Loss: 0.00004401
Iteration 4/1000 | Loss: 0.00003764
Iteration 5/1000 | Loss: 0.00003508
Iteration 6/1000 | Loss: 0.00003271
Iteration 7/1000 | Loss: 0.00003126
Iteration 8/1000 | Loss: 0.00011478
Iteration 9/1000 | Loss: 0.00023912
Iteration 10/1000 | Loss: 0.00003824
Iteration 11/1000 | Loss: 0.00003192
Iteration 12/1000 | Loss: 0.00002788
Iteration 13/1000 | Loss: 0.00002595
Iteration 14/1000 | Loss: 0.00002442
Iteration 15/1000 | Loss: 0.00002354
Iteration 16/1000 | Loss: 0.00002294
Iteration 17/1000 | Loss: 0.00002260
Iteration 18/1000 | Loss: 0.00002231
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002190
Iteration 21/1000 | Loss: 0.00002170
Iteration 22/1000 | Loss: 0.00002152
Iteration 23/1000 | Loss: 0.00002152
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002140
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00002136
Iteration 30/1000 | Loss: 0.00002134
Iteration 31/1000 | Loss: 0.00002134
Iteration 32/1000 | Loss: 0.00002133
Iteration 33/1000 | Loss: 0.00002133
Iteration 34/1000 | Loss: 0.00002133
Iteration 35/1000 | Loss: 0.00002132
Iteration 36/1000 | Loss: 0.00002132
Iteration 37/1000 | Loss: 0.00002131
Iteration 38/1000 | Loss: 0.00002131
Iteration 39/1000 | Loss: 0.00002131
Iteration 40/1000 | Loss: 0.00002131
Iteration 41/1000 | Loss: 0.00002130
Iteration 42/1000 | Loss: 0.00002130
Iteration 43/1000 | Loss: 0.00002130
Iteration 44/1000 | Loss: 0.00002130
Iteration 45/1000 | Loss: 0.00002130
Iteration 46/1000 | Loss: 0.00002130
Iteration 47/1000 | Loss: 0.00002129
Iteration 48/1000 | Loss: 0.00002129
Iteration 49/1000 | Loss: 0.00002129
Iteration 50/1000 | Loss: 0.00002129
Iteration 51/1000 | Loss: 0.00002129
Iteration 52/1000 | Loss: 0.00002129
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002129
Iteration 58/1000 | Loss: 0.00002128
Iteration 59/1000 | Loss: 0.00002128
Iteration 60/1000 | Loss: 0.00002128
Iteration 61/1000 | Loss: 0.00002128
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002127
Iteration 65/1000 | Loss: 0.00002127
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002125
Iteration 69/1000 | Loss: 0.00002125
Iteration 70/1000 | Loss: 0.00002125
Iteration 71/1000 | Loss: 0.00002124
Iteration 72/1000 | Loss: 0.00002124
Iteration 73/1000 | Loss: 0.00002124
Iteration 74/1000 | Loss: 0.00002123
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002123
Iteration 77/1000 | Loss: 0.00002122
Iteration 78/1000 | Loss: 0.00002122
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002122
Iteration 81/1000 | Loss: 0.00002122
Iteration 82/1000 | Loss: 0.00002122
Iteration 83/1000 | Loss: 0.00002121
Iteration 84/1000 | Loss: 0.00002121
Iteration 85/1000 | Loss: 0.00002121
Iteration 86/1000 | Loss: 0.00002121
Iteration 87/1000 | Loss: 0.00002121
Iteration 88/1000 | Loss: 0.00002121
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002120
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00002120
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002119
Iteration 101/1000 | Loss: 0.00002119
Iteration 102/1000 | Loss: 0.00002119
Iteration 103/1000 | Loss: 0.00002119
Iteration 104/1000 | Loss: 0.00002119
Iteration 105/1000 | Loss: 0.00002119
Iteration 106/1000 | Loss: 0.00002119
Iteration 107/1000 | Loss: 0.00002119
Iteration 108/1000 | Loss: 0.00002118
Iteration 109/1000 | Loss: 0.00002118
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002117
Iteration 113/1000 | Loss: 0.00002117
Iteration 114/1000 | Loss: 0.00002116
Iteration 115/1000 | Loss: 0.00002116
Iteration 116/1000 | Loss: 0.00002116
Iteration 117/1000 | Loss: 0.00002115
Iteration 118/1000 | Loss: 0.00002115
Iteration 119/1000 | Loss: 0.00002115
Iteration 120/1000 | Loss: 0.00002114
Iteration 121/1000 | Loss: 0.00002114
Iteration 122/1000 | Loss: 0.00002114
Iteration 123/1000 | Loss: 0.00002114
Iteration 124/1000 | Loss: 0.00002113
Iteration 125/1000 | Loss: 0.00002113
Iteration 126/1000 | Loss: 0.00002113
Iteration 127/1000 | Loss: 0.00002113
Iteration 128/1000 | Loss: 0.00002113
Iteration 129/1000 | Loss: 0.00002113
Iteration 130/1000 | Loss: 0.00002113
Iteration 131/1000 | Loss: 0.00002113
Iteration 132/1000 | Loss: 0.00002113
Iteration 133/1000 | Loss: 0.00002113
Iteration 134/1000 | Loss: 0.00002113
Iteration 135/1000 | Loss: 0.00002113
Iteration 136/1000 | Loss: 0.00002113
Iteration 137/1000 | Loss: 0.00002113
Iteration 138/1000 | Loss: 0.00002113
Iteration 139/1000 | Loss: 0.00002113
Iteration 140/1000 | Loss: 0.00002113
Iteration 141/1000 | Loss: 0.00002112
Iteration 142/1000 | Loss: 0.00002112
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002112
Iteration 145/1000 | Loss: 0.00002112
Iteration 146/1000 | Loss: 0.00002112
Iteration 147/1000 | Loss: 0.00002112
Iteration 148/1000 | Loss: 0.00002112
Iteration 149/1000 | Loss: 0.00002112
Iteration 150/1000 | Loss: 0.00002112
Iteration 151/1000 | Loss: 0.00002112
Iteration 152/1000 | Loss: 0.00002112
Iteration 153/1000 | Loss: 0.00002111
Iteration 154/1000 | Loss: 0.00002111
Iteration 155/1000 | Loss: 0.00002111
Iteration 156/1000 | Loss: 0.00002111
Iteration 157/1000 | Loss: 0.00002111
Iteration 158/1000 | Loss: 0.00002111
Iteration 159/1000 | Loss: 0.00002111
Iteration 160/1000 | Loss: 0.00002110
Iteration 161/1000 | Loss: 0.00002110
Iteration 162/1000 | Loss: 0.00002110
Iteration 163/1000 | Loss: 0.00002110
Iteration 164/1000 | Loss: 0.00002110
Iteration 165/1000 | Loss: 0.00002110
Iteration 166/1000 | Loss: 0.00002110
Iteration 167/1000 | Loss: 0.00002110
Iteration 168/1000 | Loss: 0.00002110
Iteration 169/1000 | Loss: 0.00002110
Iteration 170/1000 | Loss: 0.00002110
Iteration 171/1000 | Loss: 0.00002109
Iteration 172/1000 | Loss: 0.00002109
Iteration 173/1000 | Loss: 0.00002109
Iteration 174/1000 | Loss: 0.00002109
Iteration 175/1000 | Loss: 0.00002109
Iteration 176/1000 | Loss: 0.00002109
Iteration 177/1000 | Loss: 0.00002109
Iteration 178/1000 | Loss: 0.00002109
Iteration 179/1000 | Loss: 0.00002109
Iteration 180/1000 | Loss: 0.00002109
Iteration 181/1000 | Loss: 0.00002109
Iteration 182/1000 | Loss: 0.00002109
Iteration 183/1000 | Loss: 0.00002109
Iteration 184/1000 | Loss: 0.00002109
Iteration 185/1000 | Loss: 0.00002109
Iteration 186/1000 | Loss: 0.00002109
Iteration 187/1000 | Loss: 0.00002109
Iteration 188/1000 | Loss: 0.00002109
Iteration 189/1000 | Loss: 0.00002108
Iteration 190/1000 | Loss: 0.00002108
Iteration 191/1000 | Loss: 0.00002108
Iteration 192/1000 | Loss: 0.00002108
Iteration 193/1000 | Loss: 0.00002108
Iteration 194/1000 | Loss: 0.00002108
Iteration 195/1000 | Loss: 0.00002108
Iteration 196/1000 | Loss: 0.00002108
Iteration 197/1000 | Loss: 0.00002108
Iteration 198/1000 | Loss: 0.00002108
Iteration 199/1000 | Loss: 0.00002108
Iteration 200/1000 | Loss: 0.00002108
Iteration 201/1000 | Loss: 0.00002108
Iteration 202/1000 | Loss: 0.00002108
Iteration 203/1000 | Loss: 0.00002107
Iteration 204/1000 | Loss: 0.00002107
Iteration 205/1000 | Loss: 0.00002107
Iteration 206/1000 | Loss: 0.00002107
Iteration 207/1000 | Loss: 0.00002107
Iteration 208/1000 | Loss: 0.00002107
Iteration 209/1000 | Loss: 0.00002107
Iteration 210/1000 | Loss: 0.00002107
Iteration 211/1000 | Loss: 0.00002107
Iteration 212/1000 | Loss: 0.00002107
Iteration 213/1000 | Loss: 0.00002107
Iteration 214/1000 | Loss: 0.00002107
Iteration 215/1000 | Loss: 0.00002107
Iteration 216/1000 | Loss: 0.00002107
Iteration 217/1000 | Loss: 0.00002107
Iteration 218/1000 | Loss: 0.00002107
Iteration 219/1000 | Loss: 0.00002107
Iteration 220/1000 | Loss: 0.00002107
Iteration 221/1000 | Loss: 0.00002107
Iteration 222/1000 | Loss: 0.00002107
Iteration 223/1000 | Loss: 0.00002107
Iteration 224/1000 | Loss: 0.00002107
Iteration 225/1000 | Loss: 0.00002107
Iteration 226/1000 | Loss: 0.00002107
Iteration 227/1000 | Loss: 0.00002107
Iteration 228/1000 | Loss: 0.00002107
Iteration 229/1000 | Loss: 0.00002107
Iteration 230/1000 | Loss: 0.00002107
Iteration 231/1000 | Loss: 0.00002107
Iteration 232/1000 | Loss: 0.00002107
Iteration 233/1000 | Loss: 0.00002107
Iteration 234/1000 | Loss: 0.00002107
Iteration 235/1000 | Loss: 0.00002107
Iteration 236/1000 | Loss: 0.00002107
Iteration 237/1000 | Loss: 0.00002107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.1065203327452764e-05, 2.1065203327452764e-05, 2.1065203327452764e-05, 2.1065203327452764e-05, 2.1065203327452764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1065203327452764e-05

Optimization complete. Final v2v error: 3.782785177230835 mm

Highest mean error: 4.537841320037842 mm for frame 222

Lowest mean error: 3.0454039573669434 mm for frame 130

Saving results

Total time: 94.69325923919678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000522
Iteration 2/25 | Loss: 0.00169324
Iteration 3/25 | Loss: 0.00149248
Iteration 4/25 | Loss: 0.00143628
Iteration 5/25 | Loss: 0.00142549
Iteration 6/25 | Loss: 0.00140448
Iteration 7/25 | Loss: 0.00139507
Iteration 8/25 | Loss: 0.00140146
Iteration 9/25 | Loss: 0.00138797
Iteration 10/25 | Loss: 0.00138965
Iteration 11/25 | Loss: 0.00138494
Iteration 12/25 | Loss: 0.00138559
Iteration 13/25 | Loss: 0.00136469
Iteration 14/25 | Loss: 0.00135040
Iteration 15/25 | Loss: 0.00134820
Iteration 16/25 | Loss: 0.00137114
Iteration 17/25 | Loss: 0.00133245
Iteration 18/25 | Loss: 0.00132736
Iteration 19/25 | Loss: 0.00134148
Iteration 20/25 | Loss: 0.00132395
Iteration 21/25 | Loss: 0.00131973
Iteration 22/25 | Loss: 0.00132062
Iteration 23/25 | Loss: 0.00131971
Iteration 24/25 | Loss: 0.00131936
Iteration 25/25 | Loss: 0.00131936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.99077845
Iteration 2/25 | Loss: 0.00112599
Iteration 3/25 | Loss: 0.00112599
Iteration 4/25 | Loss: 0.00112599
Iteration 5/25 | Loss: 0.00112599
Iteration 6/25 | Loss: 0.00112599
Iteration 7/25 | Loss: 0.00112599
Iteration 8/25 | Loss: 0.00112599
Iteration 9/25 | Loss: 0.00112599
Iteration 10/25 | Loss: 0.00112599
Iteration 11/25 | Loss: 0.00112599
Iteration 12/25 | Loss: 0.00112599
Iteration 13/25 | Loss: 0.00112599
Iteration 14/25 | Loss: 0.00112599
Iteration 15/25 | Loss: 0.00112599
Iteration 16/25 | Loss: 0.00112599
Iteration 17/25 | Loss: 0.00112599
Iteration 18/25 | Loss: 0.00112599
Iteration 19/25 | Loss: 0.00112599
Iteration 20/25 | Loss: 0.00112599
Iteration 21/25 | Loss: 0.00112599
Iteration 22/25 | Loss: 0.00112599
Iteration 23/25 | Loss: 0.00112599
Iteration 24/25 | Loss: 0.00112599
Iteration 25/25 | Loss: 0.00112599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112599
Iteration 2/1000 | Loss: 0.00004359
Iteration 3/1000 | Loss: 0.00002861
Iteration 4/1000 | Loss: 0.00001836
Iteration 5/1000 | Loss: 0.00009950
Iteration 6/1000 | Loss: 0.00004354
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001633
Iteration 11/1000 | Loss: 0.00003512
Iteration 12/1000 | Loss: 0.00001670
Iteration 13/1000 | Loss: 0.00001588
Iteration 14/1000 | Loss: 0.00001574
Iteration 15/1000 | Loss: 0.00002035
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001548
Iteration 18/1000 | Loss: 0.00011394
Iteration 19/1000 | Loss: 0.00001548
Iteration 20/1000 | Loss: 0.00003783
Iteration 21/1000 | Loss: 0.00009415
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001516
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001516
Iteration 27/1000 | Loss: 0.00001516
Iteration 28/1000 | Loss: 0.00001515
Iteration 29/1000 | Loss: 0.00001513
Iteration 30/1000 | Loss: 0.00001513
Iteration 31/1000 | Loss: 0.00001513
Iteration 32/1000 | Loss: 0.00001513
Iteration 33/1000 | Loss: 0.00001513
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001513
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001513
Iteration 38/1000 | Loss: 0.00001513
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001512
Iteration 44/1000 | Loss: 0.00001512
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001507
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00003912
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001501
Iteration 58/1000 | Loss: 0.00001501
Iteration 59/1000 | Loss: 0.00001500
Iteration 60/1000 | Loss: 0.00001500
Iteration 61/1000 | Loss: 0.00001500
Iteration 62/1000 | Loss: 0.00001500
Iteration 63/1000 | Loss: 0.00001500
Iteration 64/1000 | Loss: 0.00001499
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001499
Iteration 67/1000 | Loss: 0.00001499
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001497
Iteration 78/1000 | Loss: 0.00001497
Iteration 79/1000 | Loss: 0.00001497
Iteration 80/1000 | Loss: 0.00001497
Iteration 81/1000 | Loss: 0.00001497
Iteration 82/1000 | Loss: 0.00001496
Iteration 83/1000 | Loss: 0.00001496
Iteration 84/1000 | Loss: 0.00001496
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001495
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001495
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001495
Iteration 93/1000 | Loss: 0.00001495
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001494
Iteration 96/1000 | Loss: 0.00002948
Iteration 97/1000 | Loss: 0.00001835
Iteration 98/1000 | Loss: 0.00001888
Iteration 99/1000 | Loss: 0.00001493
Iteration 100/1000 | Loss: 0.00001492
Iteration 101/1000 | Loss: 0.00001492
Iteration 102/1000 | Loss: 0.00001492
Iteration 103/1000 | Loss: 0.00001492
Iteration 104/1000 | Loss: 0.00001492
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001491
Iteration 114/1000 | Loss: 0.00001491
Iteration 115/1000 | Loss: 0.00001491
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001489
Iteration 128/1000 | Loss: 0.00001489
Iteration 129/1000 | Loss: 0.00001487
Iteration 130/1000 | Loss: 0.00001487
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001486
Iteration 134/1000 | Loss: 0.00001486
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001484
Iteration 142/1000 | Loss: 0.00001484
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001484
Iteration 147/1000 | Loss: 0.00001484
Iteration 148/1000 | Loss: 0.00001484
Iteration 149/1000 | Loss: 0.00001484
Iteration 150/1000 | Loss: 0.00001484
Iteration 151/1000 | Loss: 0.00001484
Iteration 152/1000 | Loss: 0.00001484
Iteration 153/1000 | Loss: 0.00001484
Iteration 154/1000 | Loss: 0.00001484
Iteration 155/1000 | Loss: 0.00001484
Iteration 156/1000 | Loss: 0.00001484
Iteration 157/1000 | Loss: 0.00001484
Iteration 158/1000 | Loss: 0.00001484
Iteration 159/1000 | Loss: 0.00001484
Iteration 160/1000 | Loss: 0.00001484
Iteration 161/1000 | Loss: 0.00001484
Iteration 162/1000 | Loss: 0.00001484
Iteration 163/1000 | Loss: 0.00001484
Iteration 164/1000 | Loss: 0.00001484
Iteration 165/1000 | Loss: 0.00001484
Iteration 166/1000 | Loss: 0.00001484
Iteration 167/1000 | Loss: 0.00001484
Iteration 168/1000 | Loss: 0.00001484
Iteration 169/1000 | Loss: 0.00001484
Iteration 170/1000 | Loss: 0.00001484
Iteration 171/1000 | Loss: 0.00001484
Iteration 172/1000 | Loss: 0.00001484
Iteration 173/1000 | Loss: 0.00001484
Iteration 174/1000 | Loss: 0.00001484
Iteration 175/1000 | Loss: 0.00001484
Iteration 176/1000 | Loss: 0.00001484
Iteration 177/1000 | Loss: 0.00001484
Iteration 178/1000 | Loss: 0.00001484
Iteration 179/1000 | Loss: 0.00001484
Iteration 180/1000 | Loss: 0.00001484
Iteration 181/1000 | Loss: 0.00001484
Iteration 182/1000 | Loss: 0.00001484
Iteration 183/1000 | Loss: 0.00001484
Iteration 184/1000 | Loss: 0.00001484
Iteration 185/1000 | Loss: 0.00001484
Iteration 186/1000 | Loss: 0.00001484
Iteration 187/1000 | Loss: 0.00001484
Iteration 188/1000 | Loss: 0.00001484
Iteration 189/1000 | Loss: 0.00001484
Iteration 190/1000 | Loss: 0.00001484
Iteration 191/1000 | Loss: 0.00001484
Iteration 192/1000 | Loss: 0.00001484
Iteration 193/1000 | Loss: 0.00001484
Iteration 194/1000 | Loss: 0.00001484
Iteration 195/1000 | Loss: 0.00001484
Iteration 196/1000 | Loss: 0.00001484
Iteration 197/1000 | Loss: 0.00001484
Iteration 198/1000 | Loss: 0.00001484
Iteration 199/1000 | Loss: 0.00001484
Iteration 200/1000 | Loss: 0.00001484
Iteration 201/1000 | Loss: 0.00001484
Iteration 202/1000 | Loss: 0.00001484
Iteration 203/1000 | Loss: 0.00001484
Iteration 204/1000 | Loss: 0.00001484
Iteration 205/1000 | Loss: 0.00001484
Iteration 206/1000 | Loss: 0.00001484
Iteration 207/1000 | Loss: 0.00001484
Iteration 208/1000 | Loss: 0.00001484
Iteration 209/1000 | Loss: 0.00001484
Iteration 210/1000 | Loss: 0.00001484
Iteration 211/1000 | Loss: 0.00001484
Iteration 212/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.483616233599605e-05, 1.483616233599605e-05, 1.483616233599605e-05, 1.483616233599605e-05, 1.483616233599605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.483616233599605e-05

Optimization complete. Final v2v error: 3.2459492683410645 mm

Highest mean error: 4.21014928817749 mm for frame 140

Lowest mean error: 2.9593777656555176 mm for frame 235

Saving results

Total time: 102.99310064315796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845248
Iteration 2/25 | Loss: 0.00138970
Iteration 3/25 | Loss: 0.00129286
Iteration 4/25 | Loss: 0.00128256
Iteration 5/25 | Loss: 0.00127901
Iteration 6/25 | Loss: 0.00127846
Iteration 7/25 | Loss: 0.00127846
Iteration 8/25 | Loss: 0.00127846
Iteration 9/25 | Loss: 0.00127846
Iteration 10/25 | Loss: 0.00127846
Iteration 11/25 | Loss: 0.00127846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012784601422026753, 0.0012784601422026753, 0.0012784601422026753, 0.0012784601422026753, 0.0012784601422026753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012784601422026753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04485679
Iteration 2/25 | Loss: 0.00093991
Iteration 3/25 | Loss: 0.00093991
Iteration 4/25 | Loss: 0.00093991
Iteration 5/25 | Loss: 0.00093991
Iteration 6/25 | Loss: 0.00093991
Iteration 7/25 | Loss: 0.00093991
Iteration 8/25 | Loss: 0.00093991
Iteration 9/25 | Loss: 0.00093991
Iteration 10/25 | Loss: 0.00093991
Iteration 11/25 | Loss: 0.00093991
Iteration 12/25 | Loss: 0.00093990
Iteration 13/25 | Loss: 0.00093990
Iteration 14/25 | Loss: 0.00093990
Iteration 15/25 | Loss: 0.00093990
Iteration 16/25 | Loss: 0.00093990
Iteration 17/25 | Loss: 0.00093990
Iteration 18/25 | Loss: 0.00093990
Iteration 19/25 | Loss: 0.00093990
Iteration 20/25 | Loss: 0.00093990
Iteration 21/25 | Loss: 0.00093990
Iteration 22/25 | Loss: 0.00093990
Iteration 23/25 | Loss: 0.00093990
Iteration 24/25 | Loss: 0.00093990
Iteration 25/25 | Loss: 0.00093990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093990
Iteration 2/1000 | Loss: 0.00002589
Iteration 3/1000 | Loss: 0.00001795
Iteration 4/1000 | Loss: 0.00001596
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001415
Iteration 8/1000 | Loss: 0.00001376
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001270
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001270
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001267
Iteration 32/1000 | Loss: 0.00001266
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001265
Iteration 36/1000 | Loss: 0.00001265
Iteration 37/1000 | Loss: 0.00001265
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001256
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001255
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001251
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001250
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001250
Iteration 80/1000 | Loss: 0.00001250
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.250116474693641e-05, 1.250116474693641e-05, 1.250116474693641e-05, 1.250116474693641e-05, 1.250116474693641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.250116474693641e-05

Optimization complete. Final v2v error: 3.01943302154541 mm

Highest mean error: 3.5794870853424072 mm for frame 52

Lowest mean error: 2.7950010299682617 mm for frame 99

Saving results

Total time: 33.7335307598114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856828
Iteration 2/25 | Loss: 0.00144086
Iteration 3/25 | Loss: 0.00134848
Iteration 4/25 | Loss: 0.00133225
Iteration 5/25 | Loss: 0.00132789
Iteration 6/25 | Loss: 0.00132705
Iteration 7/25 | Loss: 0.00132700
Iteration 8/25 | Loss: 0.00132700
Iteration 9/25 | Loss: 0.00132700
Iteration 10/25 | Loss: 0.00132700
Iteration 11/25 | Loss: 0.00132700
Iteration 12/25 | Loss: 0.00132700
Iteration 13/25 | Loss: 0.00132700
Iteration 14/25 | Loss: 0.00132700
Iteration 15/25 | Loss: 0.00132700
Iteration 16/25 | Loss: 0.00132700
Iteration 17/25 | Loss: 0.00132700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013269956689327955, 0.0013269956689327955, 0.0013269956689327955, 0.0013269956689327955, 0.0013269956689327955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013269956689327955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60446811
Iteration 2/25 | Loss: 0.00101472
Iteration 3/25 | Loss: 0.00101469
Iteration 4/25 | Loss: 0.00101469
Iteration 5/25 | Loss: 0.00101469
Iteration 6/25 | Loss: 0.00101469
Iteration 7/25 | Loss: 0.00101469
Iteration 8/25 | Loss: 0.00101469
Iteration 9/25 | Loss: 0.00101469
Iteration 10/25 | Loss: 0.00101469
Iteration 11/25 | Loss: 0.00101469
Iteration 12/25 | Loss: 0.00101469
Iteration 13/25 | Loss: 0.00101469
Iteration 14/25 | Loss: 0.00101469
Iteration 15/25 | Loss: 0.00101469
Iteration 16/25 | Loss: 0.00101469
Iteration 17/25 | Loss: 0.00101469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010146892163902521, 0.0010146892163902521, 0.0010146892163902521, 0.0010146892163902521, 0.0010146892163902521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010146892163902521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101469
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00002369
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001961
Iteration 6/1000 | Loss: 0.00001874
Iteration 7/1000 | Loss: 0.00001836
Iteration 8/1000 | Loss: 0.00001788
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001736
Iteration 11/1000 | Loss: 0.00001709
Iteration 12/1000 | Loss: 0.00001693
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001687
Iteration 15/1000 | Loss: 0.00001686
Iteration 16/1000 | Loss: 0.00001684
Iteration 17/1000 | Loss: 0.00001683
Iteration 18/1000 | Loss: 0.00001674
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001655
Iteration 22/1000 | Loss: 0.00001654
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001641
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001640
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001631
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001629
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001622
Iteration 75/1000 | Loss: 0.00001622
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001621
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001620
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001617
Iteration 88/1000 | Loss: 0.00001617
Iteration 89/1000 | Loss: 0.00001617
Iteration 90/1000 | Loss: 0.00001616
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001616
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001614
Iteration 96/1000 | Loss: 0.00001614
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Iteration 100/1000 | Loss: 0.00001613
Iteration 101/1000 | Loss: 0.00001613
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001612
Iteration 106/1000 | Loss: 0.00001612
Iteration 107/1000 | Loss: 0.00001612
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001612
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001611
Iteration 121/1000 | Loss: 0.00001611
Iteration 122/1000 | Loss: 0.00001611
Iteration 123/1000 | Loss: 0.00001611
Iteration 124/1000 | Loss: 0.00001611
Iteration 125/1000 | Loss: 0.00001611
Iteration 126/1000 | Loss: 0.00001611
Iteration 127/1000 | Loss: 0.00001611
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Iteration 140/1000 | Loss: 0.00001610
Iteration 141/1000 | Loss: 0.00001610
Iteration 142/1000 | Loss: 0.00001610
Iteration 143/1000 | Loss: 0.00001610
Iteration 144/1000 | Loss: 0.00001610
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.609892387932632e-05, 1.609892387932632e-05, 1.609892387932632e-05, 1.609892387932632e-05, 1.609892387932632e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.609892387932632e-05

Optimization complete. Final v2v error: 3.3832647800445557 mm

Highest mean error: 3.7515416145324707 mm for frame 32

Lowest mean error: 3.183457851409912 mm for frame 55

Saving results

Total time: 40.10171890258789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781225
Iteration 2/25 | Loss: 0.00149788
Iteration 3/25 | Loss: 0.00133192
Iteration 4/25 | Loss: 0.00131321
Iteration 5/25 | Loss: 0.00130892
Iteration 6/25 | Loss: 0.00130796
Iteration 7/25 | Loss: 0.00130796
Iteration 8/25 | Loss: 0.00130796
Iteration 9/25 | Loss: 0.00130796
Iteration 10/25 | Loss: 0.00130796
Iteration 11/25 | Loss: 0.00130796
Iteration 12/25 | Loss: 0.00130796
Iteration 13/25 | Loss: 0.00130796
Iteration 14/25 | Loss: 0.00130796
Iteration 15/25 | Loss: 0.00130796
Iteration 16/25 | Loss: 0.00130796
Iteration 17/25 | Loss: 0.00130796
Iteration 18/25 | Loss: 0.00130796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001307957456447184, 0.001307957456447184, 0.001307957456447184, 0.001307957456447184, 0.001307957456447184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001307957456447184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39635038
Iteration 2/25 | Loss: 0.00081853
Iteration 3/25 | Loss: 0.00081853
Iteration 4/25 | Loss: 0.00081853
Iteration 5/25 | Loss: 0.00081853
Iteration 6/25 | Loss: 0.00081853
Iteration 7/25 | Loss: 0.00081853
Iteration 8/25 | Loss: 0.00081853
Iteration 9/25 | Loss: 0.00081852
Iteration 10/25 | Loss: 0.00081852
Iteration 11/25 | Loss: 0.00081852
Iteration 12/25 | Loss: 0.00081852
Iteration 13/25 | Loss: 0.00081852
Iteration 14/25 | Loss: 0.00081852
Iteration 15/25 | Loss: 0.00081852
Iteration 16/25 | Loss: 0.00081852
Iteration 17/25 | Loss: 0.00081852
Iteration 18/25 | Loss: 0.00081852
Iteration 19/25 | Loss: 0.00081852
Iteration 20/25 | Loss: 0.00081852
Iteration 21/25 | Loss: 0.00081852
Iteration 22/25 | Loss: 0.00081852
Iteration 23/25 | Loss: 0.00081852
Iteration 24/25 | Loss: 0.00081852
Iteration 25/25 | Loss: 0.00081852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081852
Iteration 2/1000 | Loss: 0.00004166
Iteration 3/1000 | Loss: 0.00002984
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002183
Iteration 6/1000 | Loss: 0.00002088
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001831
Iteration 11/1000 | Loss: 0.00001802
Iteration 12/1000 | Loss: 0.00001775
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001740
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001737
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001719
Iteration 33/1000 | Loss: 0.00001719
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001711
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00001703
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001702
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001701
Iteration 125/1000 | Loss: 0.00001701
Iteration 126/1000 | Loss: 0.00001701
Iteration 127/1000 | Loss: 0.00001700
Iteration 128/1000 | Loss: 0.00001700
Iteration 129/1000 | Loss: 0.00001700
Iteration 130/1000 | Loss: 0.00001700
Iteration 131/1000 | Loss: 0.00001699
Iteration 132/1000 | Loss: 0.00001699
Iteration 133/1000 | Loss: 0.00001699
Iteration 134/1000 | Loss: 0.00001699
Iteration 135/1000 | Loss: 0.00001699
Iteration 136/1000 | Loss: 0.00001699
Iteration 137/1000 | Loss: 0.00001699
Iteration 138/1000 | Loss: 0.00001699
Iteration 139/1000 | Loss: 0.00001699
Iteration 140/1000 | Loss: 0.00001699
Iteration 141/1000 | Loss: 0.00001699
Iteration 142/1000 | Loss: 0.00001699
Iteration 143/1000 | Loss: 0.00001699
Iteration 144/1000 | Loss: 0.00001699
Iteration 145/1000 | Loss: 0.00001699
Iteration 146/1000 | Loss: 0.00001699
Iteration 147/1000 | Loss: 0.00001699
Iteration 148/1000 | Loss: 0.00001699
Iteration 149/1000 | Loss: 0.00001699
Iteration 150/1000 | Loss: 0.00001699
Iteration 151/1000 | Loss: 0.00001699
Iteration 152/1000 | Loss: 0.00001699
Iteration 153/1000 | Loss: 0.00001699
Iteration 154/1000 | Loss: 0.00001699
Iteration 155/1000 | Loss: 0.00001699
Iteration 156/1000 | Loss: 0.00001699
Iteration 157/1000 | Loss: 0.00001699
Iteration 158/1000 | Loss: 0.00001699
Iteration 159/1000 | Loss: 0.00001699
Iteration 160/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.6987918570521288e-05, 1.6987918570521288e-05, 1.6987918570521288e-05, 1.6987918570521288e-05, 1.6987918570521288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6987918570521288e-05

Optimization complete. Final v2v error: 3.4782073497772217 mm

Highest mean error: 4.208826541900635 mm for frame 89

Lowest mean error: 3.0755577087402344 mm for frame 56

Saving results

Total time: 39.14507746696472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_025/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_025/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878027
Iteration 2/25 | Loss: 0.00149306
Iteration 3/25 | Loss: 0.00138818
Iteration 4/25 | Loss: 0.00137284
Iteration 5/25 | Loss: 0.00136855
Iteration 6/25 | Loss: 0.00136851
Iteration 7/25 | Loss: 0.00136851
Iteration 8/25 | Loss: 0.00136851
Iteration 9/25 | Loss: 0.00136851
Iteration 10/25 | Loss: 0.00136851
Iteration 11/25 | Loss: 0.00136851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013685120502486825, 0.0013685120502486825, 0.0013685120502486825, 0.0013685120502486825, 0.0013685120502486825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013685120502486825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33060896
Iteration 2/25 | Loss: 0.00083714
Iteration 3/25 | Loss: 0.00083705
Iteration 4/25 | Loss: 0.00083705
Iteration 5/25 | Loss: 0.00083705
Iteration 6/25 | Loss: 0.00083705
Iteration 7/25 | Loss: 0.00083705
Iteration 8/25 | Loss: 0.00083705
Iteration 9/25 | Loss: 0.00083705
Iteration 10/25 | Loss: 0.00083705
Iteration 11/25 | Loss: 0.00083705
Iteration 12/25 | Loss: 0.00083705
Iteration 13/25 | Loss: 0.00083705
Iteration 14/25 | Loss: 0.00083705
Iteration 15/25 | Loss: 0.00083705
Iteration 16/25 | Loss: 0.00083705
Iteration 17/25 | Loss: 0.00083705
Iteration 18/25 | Loss: 0.00083705
Iteration 19/25 | Loss: 0.00083705
Iteration 20/25 | Loss: 0.00083705
Iteration 21/25 | Loss: 0.00083705
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008370502036996186, 0.0008370502036996186, 0.0008370502036996186, 0.0008370502036996186, 0.0008370502036996186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008370502036996186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083705
Iteration 2/1000 | Loss: 0.00005971
Iteration 3/1000 | Loss: 0.00004469
Iteration 4/1000 | Loss: 0.00004127
Iteration 5/1000 | Loss: 0.00003955
Iteration 6/1000 | Loss: 0.00003847
Iteration 7/1000 | Loss: 0.00003783
Iteration 8/1000 | Loss: 0.00003717
Iteration 9/1000 | Loss: 0.00003660
Iteration 10/1000 | Loss: 0.00003603
Iteration 11/1000 | Loss: 0.00003566
Iteration 12/1000 | Loss: 0.00003523
Iteration 13/1000 | Loss: 0.00003490
Iteration 14/1000 | Loss: 0.00003467
Iteration 15/1000 | Loss: 0.00003450
Iteration 16/1000 | Loss: 0.00003438
Iteration 17/1000 | Loss: 0.00003436
Iteration 18/1000 | Loss: 0.00003436
Iteration 19/1000 | Loss: 0.00003436
Iteration 20/1000 | Loss: 0.00003436
Iteration 21/1000 | Loss: 0.00003434
Iteration 22/1000 | Loss: 0.00003432
Iteration 23/1000 | Loss: 0.00003431
Iteration 24/1000 | Loss: 0.00003431
Iteration 25/1000 | Loss: 0.00003430
Iteration 26/1000 | Loss: 0.00003430
Iteration 27/1000 | Loss: 0.00003429
Iteration 28/1000 | Loss: 0.00003428
Iteration 29/1000 | Loss: 0.00003428
Iteration 30/1000 | Loss: 0.00003427
Iteration 31/1000 | Loss: 0.00003427
Iteration 32/1000 | Loss: 0.00003427
Iteration 33/1000 | Loss: 0.00003426
Iteration 34/1000 | Loss: 0.00003426
Iteration 35/1000 | Loss: 0.00003426
Iteration 36/1000 | Loss: 0.00003425
Iteration 37/1000 | Loss: 0.00003425
Iteration 38/1000 | Loss: 0.00003424
Iteration 39/1000 | Loss: 0.00003424
Iteration 40/1000 | Loss: 0.00003419
Iteration 41/1000 | Loss: 0.00003417
Iteration 42/1000 | Loss: 0.00003417
Iteration 43/1000 | Loss: 0.00003416
Iteration 44/1000 | Loss: 0.00003416
Iteration 45/1000 | Loss: 0.00003416
Iteration 46/1000 | Loss: 0.00003413
Iteration 47/1000 | Loss: 0.00003412
Iteration 48/1000 | Loss: 0.00003408
Iteration 49/1000 | Loss: 0.00003407
Iteration 50/1000 | Loss: 0.00003405
Iteration 51/1000 | Loss: 0.00003404
Iteration 52/1000 | Loss: 0.00003404
Iteration 53/1000 | Loss: 0.00003404
Iteration 54/1000 | Loss: 0.00003404
Iteration 55/1000 | Loss: 0.00003404
Iteration 56/1000 | Loss: 0.00003404
Iteration 57/1000 | Loss: 0.00003403
Iteration 58/1000 | Loss: 0.00003403
Iteration 59/1000 | Loss: 0.00003403
Iteration 60/1000 | Loss: 0.00003403
Iteration 61/1000 | Loss: 0.00003403
Iteration 62/1000 | Loss: 0.00003403
Iteration 63/1000 | Loss: 0.00003402
Iteration 64/1000 | Loss: 0.00003402
Iteration 65/1000 | Loss: 0.00003400
Iteration 66/1000 | Loss: 0.00003400
Iteration 67/1000 | Loss: 0.00003400
Iteration 68/1000 | Loss: 0.00003400
Iteration 69/1000 | Loss: 0.00003399
Iteration 70/1000 | Loss: 0.00003399
Iteration 71/1000 | Loss: 0.00003398
Iteration 72/1000 | Loss: 0.00003397
Iteration 73/1000 | Loss: 0.00003397
Iteration 74/1000 | Loss: 0.00003397
Iteration 75/1000 | Loss: 0.00003397
Iteration 76/1000 | Loss: 0.00003397
Iteration 77/1000 | Loss: 0.00003397
Iteration 78/1000 | Loss: 0.00003397
Iteration 79/1000 | Loss: 0.00003397
Iteration 80/1000 | Loss: 0.00003396
Iteration 81/1000 | Loss: 0.00003396
Iteration 82/1000 | Loss: 0.00003396
Iteration 83/1000 | Loss: 0.00003396
Iteration 84/1000 | Loss: 0.00003396
Iteration 85/1000 | Loss: 0.00003395
Iteration 86/1000 | Loss: 0.00003394
Iteration 87/1000 | Loss: 0.00003394
Iteration 88/1000 | Loss: 0.00003394
Iteration 89/1000 | Loss: 0.00003394
Iteration 90/1000 | Loss: 0.00003394
Iteration 91/1000 | Loss: 0.00003394
Iteration 92/1000 | Loss: 0.00003393
Iteration 93/1000 | Loss: 0.00003393
Iteration 94/1000 | Loss: 0.00003393
Iteration 95/1000 | Loss: 0.00003393
Iteration 96/1000 | Loss: 0.00003393
Iteration 97/1000 | Loss: 0.00003393
Iteration 98/1000 | Loss: 0.00003393
Iteration 99/1000 | Loss: 0.00003393
Iteration 100/1000 | Loss: 0.00003393
Iteration 101/1000 | Loss: 0.00003393
Iteration 102/1000 | Loss: 0.00003393
Iteration 103/1000 | Loss: 0.00003393
Iteration 104/1000 | Loss: 0.00003392
Iteration 105/1000 | Loss: 0.00003392
Iteration 106/1000 | Loss: 0.00003392
Iteration 107/1000 | Loss: 0.00003392
Iteration 108/1000 | Loss: 0.00003392
Iteration 109/1000 | Loss: 0.00003392
Iteration 110/1000 | Loss: 0.00003392
Iteration 111/1000 | Loss: 0.00003392
Iteration 112/1000 | Loss: 0.00003392
Iteration 113/1000 | Loss: 0.00003392
Iteration 114/1000 | Loss: 0.00003392
Iteration 115/1000 | Loss: 0.00003392
Iteration 116/1000 | Loss: 0.00003392
Iteration 117/1000 | Loss: 0.00003392
Iteration 118/1000 | Loss: 0.00003392
Iteration 119/1000 | Loss: 0.00003391
Iteration 120/1000 | Loss: 0.00003391
Iteration 121/1000 | Loss: 0.00003391
Iteration 122/1000 | Loss: 0.00003391
Iteration 123/1000 | Loss: 0.00003391
Iteration 124/1000 | Loss: 0.00003391
Iteration 125/1000 | Loss: 0.00003391
Iteration 126/1000 | Loss: 0.00003391
Iteration 127/1000 | Loss: 0.00003391
Iteration 128/1000 | Loss: 0.00003391
Iteration 129/1000 | Loss: 0.00003391
Iteration 130/1000 | Loss: 0.00003391
Iteration 131/1000 | Loss: 0.00003391
Iteration 132/1000 | Loss: 0.00003391
Iteration 133/1000 | Loss: 0.00003391
Iteration 134/1000 | Loss: 0.00003391
Iteration 135/1000 | Loss: 0.00003391
Iteration 136/1000 | Loss: 0.00003391
Iteration 137/1000 | Loss: 0.00003391
Iteration 138/1000 | Loss: 0.00003391
Iteration 139/1000 | Loss: 0.00003391
Iteration 140/1000 | Loss: 0.00003391
Iteration 141/1000 | Loss: 0.00003391
Iteration 142/1000 | Loss: 0.00003391
Iteration 143/1000 | Loss: 0.00003391
Iteration 144/1000 | Loss: 0.00003391
Iteration 145/1000 | Loss: 0.00003391
Iteration 146/1000 | Loss: 0.00003391
Iteration 147/1000 | Loss: 0.00003391
Iteration 148/1000 | Loss: 0.00003391
Iteration 149/1000 | Loss: 0.00003391
Iteration 150/1000 | Loss: 0.00003391
Iteration 151/1000 | Loss: 0.00003391
Iteration 152/1000 | Loss: 0.00003391
Iteration 153/1000 | Loss: 0.00003391
Iteration 154/1000 | Loss: 0.00003391
Iteration 155/1000 | Loss: 0.00003391
Iteration 156/1000 | Loss: 0.00003391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.390797064639628e-05, 3.390797064639628e-05, 3.390797064639628e-05, 3.390797064639628e-05, 3.390797064639628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.390797064639628e-05

Optimization complete. Final v2v error: 4.763070583343506 mm

Highest mean error: 4.967363357543945 mm for frame 7

Lowest mean error: 4.6302642822265625 mm for frame 141

Saving results

Total time: 47.824098348617554
