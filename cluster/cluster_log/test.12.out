Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=12, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 672-727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037156
Iteration 2/25 | Loss: 0.00231967
Iteration 3/25 | Loss: 0.00175183
Iteration 4/25 | Loss: 0.00155670
Iteration 5/25 | Loss: 0.00140801
Iteration 6/25 | Loss: 0.00135063
Iteration 7/25 | Loss: 0.00134218
Iteration 8/25 | Loss: 0.00135582
Iteration 9/25 | Loss: 0.00132623
Iteration 10/25 | Loss: 0.00131606
Iteration 11/25 | Loss: 0.00133285
Iteration 12/25 | Loss: 0.00131397
Iteration 13/25 | Loss: 0.00128451
Iteration 14/25 | Loss: 0.00129050
Iteration 15/25 | Loss: 0.00128486
Iteration 16/25 | Loss: 0.00128175
Iteration 17/25 | Loss: 0.00128589
Iteration 18/25 | Loss: 0.00128016
Iteration 19/25 | Loss: 0.00127220
Iteration 20/25 | Loss: 0.00127271
Iteration 21/25 | Loss: 0.00126669
Iteration 22/25 | Loss: 0.00126637
Iteration 23/25 | Loss: 0.00127197
Iteration 24/25 | Loss: 0.00126581
Iteration 25/25 | Loss: 0.00126544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26884031
Iteration 2/25 | Loss: 0.00297495
Iteration 3/25 | Loss: 0.00285885
Iteration 4/25 | Loss: 0.00285885
Iteration 5/25 | Loss: 0.00285885
Iteration 6/25 | Loss: 0.00285885
Iteration 7/25 | Loss: 0.00285885
Iteration 8/25 | Loss: 0.00285885
Iteration 9/25 | Loss: 0.00285885
Iteration 10/25 | Loss: 0.00285885
Iteration 11/25 | Loss: 0.00285885
Iteration 12/25 | Loss: 0.00285885
Iteration 13/25 | Loss: 0.00285885
Iteration 14/25 | Loss: 0.00285885
Iteration 15/25 | Loss: 0.00285885
Iteration 16/25 | Loss: 0.00285885
Iteration 17/25 | Loss: 0.00285885
Iteration 18/25 | Loss: 0.00285885
Iteration 19/25 | Loss: 0.00285885
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002858845517039299, 0.002858845517039299, 0.002858845517039299, 0.002858845517039299, 0.002858845517039299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002858845517039299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285885
Iteration 2/1000 | Loss: 0.00028530
Iteration 3/1000 | Loss: 0.00064204
Iteration 4/1000 | Loss: 0.00085698
Iteration 5/1000 | Loss: 0.00008378
Iteration 6/1000 | Loss: 0.00022006
Iteration 7/1000 | Loss: 0.00024579
Iteration 8/1000 | Loss: 0.00018678
Iteration 9/1000 | Loss: 0.00047451
Iteration 10/1000 | Loss: 0.00093753
Iteration 11/1000 | Loss: 0.00014751
Iteration 12/1000 | Loss: 0.00020112
Iteration 13/1000 | Loss: 0.00015333
Iteration 14/1000 | Loss: 0.00008481
Iteration 15/1000 | Loss: 0.00006253
Iteration 16/1000 | Loss: 0.00010915
Iteration 17/1000 | Loss: 0.00029151
Iteration 18/1000 | Loss: 0.00632985
Iteration 19/1000 | Loss: 0.00138557
Iteration 20/1000 | Loss: 0.00014951
Iteration 21/1000 | Loss: 0.00008449
Iteration 22/1000 | Loss: 0.00024717
Iteration 23/1000 | Loss: 0.00012784
Iteration 24/1000 | Loss: 0.00017742
Iteration 25/1000 | Loss: 0.00051238
Iteration 26/1000 | Loss: 0.00011183
Iteration 27/1000 | Loss: 0.00022057
Iteration 28/1000 | Loss: 0.00029702
Iteration 29/1000 | Loss: 0.00002805
Iteration 30/1000 | Loss: 0.00011584
Iteration 31/1000 | Loss: 0.00008134
Iteration 32/1000 | Loss: 0.00002373
Iteration 33/1000 | Loss: 0.00015923
Iteration 34/1000 | Loss: 0.00153171
Iteration 35/1000 | Loss: 0.00014963
Iteration 36/1000 | Loss: 0.00002471
Iteration 37/1000 | Loss: 0.00006678
Iteration 38/1000 | Loss: 0.00027539
Iteration 39/1000 | Loss: 0.00020081
Iteration 40/1000 | Loss: 0.00037345
Iteration 41/1000 | Loss: 0.00002201
Iteration 42/1000 | Loss: 0.00005203
Iteration 43/1000 | Loss: 0.00014769
Iteration 44/1000 | Loss: 0.00003724
Iteration 45/1000 | Loss: 0.00018558
Iteration 46/1000 | Loss: 0.00004493
Iteration 47/1000 | Loss: 0.00010001
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00001877
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00008213
Iteration 53/1000 | Loss: 0.00030828
Iteration 54/1000 | Loss: 0.00029978
Iteration 55/1000 | Loss: 0.00003977
Iteration 56/1000 | Loss: 0.00002314
Iteration 57/1000 | Loss: 0.00021627
Iteration 58/1000 | Loss: 0.00042020
Iteration 59/1000 | Loss: 0.00001854
Iteration 60/1000 | Loss: 0.00005513
Iteration 61/1000 | Loss: 0.00062024
Iteration 62/1000 | Loss: 0.00195146
Iteration 63/1000 | Loss: 0.00221921
Iteration 64/1000 | Loss: 0.00350120
Iteration 65/1000 | Loss: 0.00228846
Iteration 66/1000 | Loss: 0.00040177
Iteration 67/1000 | Loss: 0.00004276
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00001901
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00008049
Iteration 72/1000 | Loss: 0.00025069
Iteration 73/1000 | Loss: 0.00013077
Iteration 74/1000 | Loss: 0.00003362
Iteration 75/1000 | Loss: 0.00003797
Iteration 76/1000 | Loss: 0.00002166
Iteration 77/1000 | Loss: 0.00005237
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00001793
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00003601
Iteration 82/1000 | Loss: 0.00003017
Iteration 83/1000 | Loss: 0.00001779
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001777
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00003360
Iteration 89/1000 | Loss: 0.00002783
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00001778
Iteration 92/1000 | Loss: 0.00001777
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001776
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001773
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001773
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001767
Iteration 120/1000 | Loss: 0.00001767
Iteration 121/1000 | Loss: 0.00001767
Iteration 122/1000 | Loss: 0.00001767
Iteration 123/1000 | Loss: 0.00001767
Iteration 124/1000 | Loss: 0.00001767
Iteration 125/1000 | Loss: 0.00001767
Iteration 126/1000 | Loss: 0.00001767
Iteration 127/1000 | Loss: 0.00001767
Iteration 128/1000 | Loss: 0.00001767
Iteration 129/1000 | Loss: 0.00001767
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001766
Iteration 135/1000 | Loss: 0.00001766
Iteration 136/1000 | Loss: 0.00001765
Iteration 137/1000 | Loss: 0.00001765
Iteration 138/1000 | Loss: 0.00001765
Iteration 139/1000 | Loss: 0.00001765
Iteration 140/1000 | Loss: 0.00001765
Iteration 141/1000 | Loss: 0.00001764
Iteration 142/1000 | Loss: 0.00001764
Iteration 143/1000 | Loss: 0.00001764
Iteration 144/1000 | Loss: 0.00001764
Iteration 145/1000 | Loss: 0.00001764
Iteration 146/1000 | Loss: 0.00001764
Iteration 147/1000 | Loss: 0.00001764
Iteration 148/1000 | Loss: 0.00001764
Iteration 149/1000 | Loss: 0.00001764
Iteration 150/1000 | Loss: 0.00001764
Iteration 151/1000 | Loss: 0.00001763
Iteration 152/1000 | Loss: 0.00001763
Iteration 153/1000 | Loss: 0.00001763
Iteration 154/1000 | Loss: 0.00001763
Iteration 155/1000 | Loss: 0.00001763
Iteration 156/1000 | Loss: 0.00001763
Iteration 157/1000 | Loss: 0.00001763
Iteration 158/1000 | Loss: 0.00001763
Iteration 159/1000 | Loss: 0.00001763
Iteration 160/1000 | Loss: 0.00001763
Iteration 161/1000 | Loss: 0.00001763
Iteration 162/1000 | Loss: 0.00001763
Iteration 163/1000 | Loss: 0.00001763
Iteration 164/1000 | Loss: 0.00001762
Iteration 165/1000 | Loss: 0.00001762
Iteration 166/1000 | Loss: 0.00001762
Iteration 167/1000 | Loss: 0.00001762
Iteration 168/1000 | Loss: 0.00001762
Iteration 169/1000 | Loss: 0.00001762
Iteration 170/1000 | Loss: 0.00001762
Iteration 171/1000 | Loss: 0.00001762
Iteration 172/1000 | Loss: 0.00001762
Iteration 173/1000 | Loss: 0.00001762
Iteration 174/1000 | Loss: 0.00001762
Iteration 175/1000 | Loss: 0.00001762
Iteration 176/1000 | Loss: 0.00001762
Iteration 177/1000 | Loss: 0.00001762
Iteration 178/1000 | Loss: 0.00001762
Iteration 179/1000 | Loss: 0.00001762
Iteration 180/1000 | Loss: 0.00001762
Iteration 181/1000 | Loss: 0.00001762
Iteration 182/1000 | Loss: 0.00001762
Iteration 183/1000 | Loss: 0.00001762
Iteration 184/1000 | Loss: 0.00001762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.7620237485971302e-05, 1.7620237485971302e-05, 1.7620237485971302e-05, 1.7620237485971302e-05, 1.7620237485971302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7620237485971302e-05

Optimization complete. Final v2v error: 3.470881700515747 mm

Highest mean error: 9.050993919372559 mm for frame 6

Lowest mean error: 2.892012596130371 mm for frame 1

Saving results

Total time: 167.00138139724731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402559
Iteration 2/25 | Loss: 0.00116352
Iteration 3/25 | Loss: 0.00108469
Iteration 4/25 | Loss: 0.00106542
Iteration 5/25 | Loss: 0.00105972
Iteration 6/25 | Loss: 0.00105738
Iteration 7/25 | Loss: 0.00105735
Iteration 8/25 | Loss: 0.00105735
Iteration 9/25 | Loss: 0.00105735
Iteration 10/25 | Loss: 0.00105735
Iteration 11/25 | Loss: 0.00105735
Iteration 12/25 | Loss: 0.00105735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010573536856099963, 0.0010573536856099963, 0.0010573536856099963, 0.0010573536856099963, 0.0010573536856099963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010573536856099963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29098344
Iteration 2/25 | Loss: 0.00211592
Iteration 3/25 | Loss: 0.00211592
Iteration 4/25 | Loss: 0.00211592
Iteration 5/25 | Loss: 0.00211592
Iteration 6/25 | Loss: 0.00211592
Iteration 7/25 | Loss: 0.00211592
Iteration 8/25 | Loss: 0.00211592
Iteration 9/25 | Loss: 0.00211592
Iteration 10/25 | Loss: 0.00211592
Iteration 11/25 | Loss: 0.00211592
Iteration 12/25 | Loss: 0.00211592
Iteration 13/25 | Loss: 0.00211592
Iteration 14/25 | Loss: 0.00211592
Iteration 15/25 | Loss: 0.00211592
Iteration 16/25 | Loss: 0.00211592
Iteration 17/25 | Loss: 0.00211592
Iteration 18/25 | Loss: 0.00211592
Iteration 19/25 | Loss: 0.00211592
Iteration 20/25 | Loss: 0.00211592
Iteration 21/25 | Loss: 0.00211592
Iteration 22/25 | Loss: 0.00211592
Iteration 23/25 | Loss: 0.00211592
Iteration 24/25 | Loss: 0.00211592
Iteration 25/25 | Loss: 0.00211592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211592
Iteration 2/1000 | Loss: 0.00003979
Iteration 3/1000 | Loss: 0.00001664
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001297
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001157
Iteration 8/1000 | Loss: 0.00001140
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001083
Iteration 11/1000 | Loss: 0.00001070
Iteration 12/1000 | Loss: 0.00001070
Iteration 13/1000 | Loss: 0.00001057
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001052
Iteration 16/1000 | Loss: 0.00001052
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001051
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001044
Iteration 33/1000 | Loss: 0.00001044
Iteration 34/1000 | Loss: 0.00001043
Iteration 35/1000 | Loss: 0.00001042
Iteration 36/1000 | Loss: 0.00001042
Iteration 37/1000 | Loss: 0.00001042
Iteration 38/1000 | Loss: 0.00001042
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001041
Iteration 41/1000 | Loss: 0.00001040
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001038
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001036
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001035
Iteration 50/1000 | Loss: 0.00001034
Iteration 51/1000 | Loss: 0.00001034
Iteration 52/1000 | Loss: 0.00001034
Iteration 53/1000 | Loss: 0.00001033
Iteration 54/1000 | Loss: 0.00001033
Iteration 55/1000 | Loss: 0.00001032
Iteration 56/1000 | Loss: 0.00001032
Iteration 57/1000 | Loss: 0.00001032
Iteration 58/1000 | Loss: 0.00001032
Iteration 59/1000 | Loss: 0.00001031
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001028
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001028
Iteration 75/1000 | Loss: 0.00001028
Iteration 76/1000 | Loss: 0.00001028
Iteration 77/1000 | Loss: 0.00001028
Iteration 78/1000 | Loss: 0.00001028
Iteration 79/1000 | Loss: 0.00001028
Iteration 80/1000 | Loss: 0.00001028
Iteration 81/1000 | Loss: 0.00001028
Iteration 82/1000 | Loss: 0.00001028
Iteration 83/1000 | Loss: 0.00001028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.0280545211571734e-05, 1.0280545211571734e-05, 1.0280545211571734e-05, 1.0280545211571734e-05, 1.0280545211571734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0280545211571734e-05

Optimization complete. Final v2v error: 2.726824998855591 mm

Highest mean error: 2.9385859966278076 mm for frame 114

Lowest mean error: 2.4180045127868652 mm for frame 134

Saving results

Total time: 30.67963171005249
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_28_it_4404/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_28_it_4404/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405637
Iteration 2/25 | Loss: 0.00119076
Iteration 3/25 | Loss: 0.00108247
Iteration 4/25 | Loss: 0.00106965
Iteration 5/25 | Loss: 0.00106636
Iteration 6/25 | Loss: 0.00106636
Iteration 7/25 | Loss: 0.00106636
Iteration 8/25 | Loss: 0.00106636
Iteration 9/25 | Loss: 0.00106636
Iteration 10/25 | Loss: 0.00106636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001066356897354126, 0.001066356897354126, 0.001066356897354126, 0.001066356897354126, 0.001066356897354126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066356897354126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48640835
Iteration 2/25 | Loss: 0.00206299
Iteration 3/25 | Loss: 0.00206299
Iteration 4/25 | Loss: 0.00206298
Iteration 5/25 | Loss: 0.00206298
Iteration 6/25 | Loss: 0.00206298
Iteration 7/25 | Loss: 0.00206298
Iteration 8/25 | Loss: 0.00206298
Iteration 9/25 | Loss: 0.00206298
Iteration 10/25 | Loss: 0.00206298
Iteration 11/25 | Loss: 0.00206298
Iteration 12/25 | Loss: 0.00206298
Iteration 13/25 | Loss: 0.00206298
Iteration 14/25 | Loss: 0.00206298
Iteration 15/25 | Loss: 0.00206298
Iteration 16/25 | Loss: 0.00206298
Iteration 17/25 | Loss: 0.00206298
Iteration 18/25 | Loss: 0.00206298
Iteration 19/25 | Loss: 0.00206298
Iteration 20/25 | Loss: 0.00206298
Iteration 21/25 | Loss: 0.00206298
Iteration 22/25 | Loss: 0.00206298
Iteration 23/25 | Loss: 0.00206298
Iteration 24/25 | Loss: 0.00206298
Iteration 25/25 | Loss: 0.00206298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206298
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00001581
Iteration 4/1000 | Loss: 0.00001117
Iteration 5/1000 | Loss: 0.00000964
Iteration 6/1000 | Loss: 0.00000882
Iteration 7/1000 | Loss: 0.00000846
Iteration 8/1000 | Loss: 0.00000825
Iteration 9/1000 | Loss: 0.00000798
Iteration 10/1000 | Loss: 0.00000775
Iteration 11/1000 | Loss: 0.00000769
Iteration 12/1000 | Loss: 0.00000758
Iteration 13/1000 | Loss: 0.00000756
Iteration 14/1000 | Loss: 0.00000756
Iteration 15/1000 | Loss: 0.00000755
Iteration 16/1000 | Loss: 0.00000755
Iteration 17/1000 | Loss: 0.00000754
Iteration 18/1000 | Loss: 0.00000754
Iteration 19/1000 | Loss: 0.00000753
Iteration 20/1000 | Loss: 0.00000753
Iteration 21/1000 | Loss: 0.00000752
Iteration 22/1000 | Loss: 0.00000752
Iteration 23/1000 | Loss: 0.00000752
Iteration 24/1000 | Loss: 0.00000751
Iteration 25/1000 | Loss: 0.00000751
Iteration 26/1000 | Loss: 0.00000750
Iteration 27/1000 | Loss: 0.00000750
Iteration 28/1000 | Loss: 0.00000749
Iteration 29/1000 | Loss: 0.00000749
Iteration 30/1000 | Loss: 0.00000748
Iteration 31/1000 | Loss: 0.00000747
Iteration 32/1000 | Loss: 0.00000744
Iteration 33/1000 | Loss: 0.00000744
Iteration 34/1000 | Loss: 0.00000741
Iteration 35/1000 | Loss: 0.00000741
Iteration 36/1000 | Loss: 0.00000740
Iteration 37/1000 | Loss: 0.00000739
Iteration 38/1000 | Loss: 0.00000739
Iteration 39/1000 | Loss: 0.00000738
Iteration 40/1000 | Loss: 0.00000737
Iteration 41/1000 | Loss: 0.00000736
Iteration 42/1000 | Loss: 0.00000736
Iteration 43/1000 | Loss: 0.00000736
Iteration 44/1000 | Loss: 0.00000735
Iteration 45/1000 | Loss: 0.00000735
Iteration 46/1000 | Loss: 0.00000734
Iteration 47/1000 | Loss: 0.00000734
Iteration 48/1000 | Loss: 0.00000733
Iteration 49/1000 | Loss: 0.00000733
Iteration 50/1000 | Loss: 0.00000732
Iteration 51/1000 | Loss: 0.00000732
Iteration 52/1000 | Loss: 0.00000732
Iteration 53/1000 | Loss: 0.00000732
Iteration 54/1000 | Loss: 0.00000732
Iteration 55/1000 | Loss: 0.00000732
Iteration 56/1000 | Loss: 0.00000732
Iteration 57/1000 | Loss: 0.00000731
Iteration 58/1000 | Loss: 0.00000731
Iteration 59/1000 | Loss: 0.00000731
Iteration 60/1000 | Loss: 0.00000731
Iteration 61/1000 | Loss: 0.00000730
Iteration 62/1000 | Loss: 0.00000730
Iteration 63/1000 | Loss: 0.00000729
Iteration 64/1000 | Loss: 0.00000728
Iteration 65/1000 | Loss: 0.00000728
Iteration 66/1000 | Loss: 0.00000728
Iteration 67/1000 | Loss: 0.00000727
Iteration 68/1000 | Loss: 0.00000727
Iteration 69/1000 | Loss: 0.00000727
Iteration 70/1000 | Loss: 0.00000727
Iteration 71/1000 | Loss: 0.00000726
Iteration 72/1000 | Loss: 0.00000726
Iteration 73/1000 | Loss: 0.00000726
Iteration 74/1000 | Loss: 0.00000726
Iteration 75/1000 | Loss: 0.00000726
Iteration 76/1000 | Loss: 0.00000725
Iteration 77/1000 | Loss: 0.00000725
Iteration 78/1000 | Loss: 0.00000725
Iteration 79/1000 | Loss: 0.00000725
Iteration 80/1000 | Loss: 0.00000725
Iteration 81/1000 | Loss: 0.00000725
Iteration 82/1000 | Loss: 0.00000725
Iteration 83/1000 | Loss: 0.00000725
Iteration 84/1000 | Loss: 0.00000724
Iteration 85/1000 | Loss: 0.00000724
Iteration 86/1000 | Loss: 0.00000724
Iteration 87/1000 | Loss: 0.00000724
Iteration 88/1000 | Loss: 0.00000724
Iteration 89/1000 | Loss: 0.00000724
Iteration 90/1000 | Loss: 0.00000723
Iteration 91/1000 | Loss: 0.00000723
Iteration 92/1000 | Loss: 0.00000723
Iteration 93/1000 | Loss: 0.00000723
Iteration 94/1000 | Loss: 0.00000723
Iteration 95/1000 | Loss: 0.00000722
Iteration 96/1000 | Loss: 0.00000722
Iteration 97/1000 | Loss: 0.00000722
Iteration 98/1000 | Loss: 0.00000722
Iteration 99/1000 | Loss: 0.00000722
Iteration 100/1000 | Loss: 0.00000722
Iteration 101/1000 | Loss: 0.00000721
Iteration 102/1000 | Loss: 0.00000721
Iteration 103/1000 | Loss: 0.00000721
Iteration 104/1000 | Loss: 0.00000721
Iteration 105/1000 | Loss: 0.00000721
Iteration 106/1000 | Loss: 0.00000721
Iteration 107/1000 | Loss: 0.00000721
Iteration 108/1000 | Loss: 0.00000721
Iteration 109/1000 | Loss: 0.00000721
Iteration 110/1000 | Loss: 0.00000720
Iteration 111/1000 | Loss: 0.00000720
Iteration 112/1000 | Loss: 0.00000720
Iteration 113/1000 | Loss: 0.00000720
Iteration 114/1000 | Loss: 0.00000720
Iteration 115/1000 | Loss: 0.00000720
Iteration 116/1000 | Loss: 0.00000720
Iteration 117/1000 | Loss: 0.00000719
Iteration 118/1000 | Loss: 0.00000719
Iteration 119/1000 | Loss: 0.00000719
Iteration 120/1000 | Loss: 0.00000719
Iteration 121/1000 | Loss: 0.00000719
Iteration 122/1000 | Loss: 0.00000719
Iteration 123/1000 | Loss: 0.00000719
Iteration 124/1000 | Loss: 0.00000719
Iteration 125/1000 | Loss: 0.00000719
Iteration 126/1000 | Loss: 0.00000719
Iteration 127/1000 | Loss: 0.00000719
Iteration 128/1000 | Loss: 0.00000719
Iteration 129/1000 | Loss: 0.00000719
Iteration 130/1000 | Loss: 0.00000719
Iteration 131/1000 | Loss: 0.00000719
Iteration 132/1000 | Loss: 0.00000718
Iteration 133/1000 | Loss: 0.00000718
Iteration 134/1000 | Loss: 0.00000718
Iteration 135/1000 | Loss: 0.00000718
Iteration 136/1000 | Loss: 0.00000718
Iteration 137/1000 | Loss: 0.00000718
Iteration 138/1000 | Loss: 0.00000718
Iteration 139/1000 | Loss: 0.00000718
Iteration 140/1000 | Loss: 0.00000718
Iteration 141/1000 | Loss: 0.00000718
Iteration 142/1000 | Loss: 0.00000718
Iteration 143/1000 | Loss: 0.00000718
Iteration 144/1000 | Loss: 0.00000718
Iteration 145/1000 | Loss: 0.00000718
Iteration 146/1000 | Loss: 0.00000718
Iteration 147/1000 | Loss: 0.00000718
Iteration 148/1000 | Loss: 0.00000718
Iteration 149/1000 | Loss: 0.00000718
Iteration 150/1000 | Loss: 0.00000718
Iteration 151/1000 | Loss: 0.00000718
Iteration 152/1000 | Loss: 0.00000718
Iteration 153/1000 | Loss: 0.00000718
Iteration 154/1000 | Loss: 0.00000718
Iteration 155/1000 | Loss: 0.00000718
Iteration 156/1000 | Loss: 0.00000718
Iteration 157/1000 | Loss: 0.00000718
Iteration 158/1000 | Loss: 0.00000718
Iteration 159/1000 | Loss: 0.00000718
Iteration 160/1000 | Loss: 0.00000718
Iteration 161/1000 | Loss: 0.00000718
Iteration 162/1000 | Loss: 0.00000718
Iteration 163/1000 | Loss: 0.00000718
Iteration 164/1000 | Loss: 0.00000718
Iteration 165/1000 | Loss: 0.00000718
Iteration 166/1000 | Loss: 0.00000718
Iteration 167/1000 | Loss: 0.00000718
Iteration 168/1000 | Loss: 0.00000718
Iteration 169/1000 | Loss: 0.00000718
Iteration 170/1000 | Loss: 0.00000718
Iteration 171/1000 | Loss: 0.00000718
Iteration 172/1000 | Loss: 0.00000718
Iteration 173/1000 | Loss: 0.00000718
Iteration 174/1000 | Loss: 0.00000718
Iteration 175/1000 | Loss: 0.00000718
Iteration 176/1000 | Loss: 0.00000718
Iteration 177/1000 | Loss: 0.00000718
Iteration 178/1000 | Loss: 0.00000718
Iteration 179/1000 | Loss: 0.00000718
Iteration 180/1000 | Loss: 0.00000718
Iteration 181/1000 | Loss: 0.00000718
Iteration 182/1000 | Loss: 0.00000718
Iteration 183/1000 | Loss: 0.00000718
Iteration 184/1000 | Loss: 0.00000718
Iteration 185/1000 | Loss: 0.00000718
Iteration 186/1000 | Loss: 0.00000718
Iteration 187/1000 | Loss: 0.00000718
Iteration 188/1000 | Loss: 0.00000718
Iteration 189/1000 | Loss: 0.00000718
Iteration 190/1000 | Loss: 0.00000718
Iteration 191/1000 | Loss: 0.00000718
Iteration 192/1000 | Loss: 0.00000718
Iteration 193/1000 | Loss: 0.00000718
Iteration 194/1000 | Loss: 0.00000718
Iteration 195/1000 | Loss: 0.00000718
Iteration 196/1000 | Loss: 0.00000718
Iteration 197/1000 | Loss: 0.00000718
Iteration 198/1000 | Loss: 0.00000718
Iteration 199/1000 | Loss: 0.00000718
Iteration 200/1000 | Loss: 0.00000718
Iteration 201/1000 | Loss: 0.00000718
Iteration 202/1000 | Loss: 0.00000718
Iteration 203/1000 | Loss: 0.00000718
Iteration 204/1000 | Loss: 0.00000718
Iteration 205/1000 | Loss: 0.00000718
Iteration 206/1000 | Loss: 0.00000718
Iteration 207/1000 | Loss: 0.00000718
Iteration 208/1000 | Loss: 0.00000718
Iteration 209/1000 | Loss: 0.00000718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [7.178995929280063e-06, 7.178995929280063e-06, 7.178995929280063e-06, 7.178995929280063e-06, 7.178995929280063e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.178995929280063e-06

Optimization complete. Final v2v error: 2.2997806072235107 mm

Highest mean error: 2.671874761581421 mm for frame 114

Lowest mean error: 2.0872035026550293 mm for frame 164

Saving results

Total time: 39.13170003890991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872574
Iteration 2/25 | Loss: 0.00133705
Iteration 3/25 | Loss: 0.00120916
Iteration 4/25 | Loss: 0.00120433
Iteration 5/25 | Loss: 0.00120356
Iteration 6/25 | Loss: 0.00120356
Iteration 7/25 | Loss: 0.00120356
Iteration 8/25 | Loss: 0.00120356
Iteration 9/25 | Loss: 0.00120356
Iteration 10/25 | Loss: 0.00120356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001203560852445662, 0.001203560852445662, 0.001203560852445662, 0.001203560852445662, 0.001203560852445662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203560852445662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27133203
Iteration 2/25 | Loss: 0.00131604
Iteration 3/25 | Loss: 0.00131604
Iteration 4/25 | Loss: 0.00131604
Iteration 5/25 | Loss: 0.00131604
Iteration 6/25 | Loss: 0.00131604
Iteration 7/25 | Loss: 0.00131604
Iteration 8/25 | Loss: 0.00131604
Iteration 9/25 | Loss: 0.00131604
Iteration 10/25 | Loss: 0.00131604
Iteration 11/25 | Loss: 0.00131604
Iteration 12/25 | Loss: 0.00131604
Iteration 13/25 | Loss: 0.00131604
Iteration 14/25 | Loss: 0.00131604
Iteration 15/25 | Loss: 0.00131604
Iteration 16/25 | Loss: 0.00131604
Iteration 17/25 | Loss: 0.00131604
Iteration 18/25 | Loss: 0.00131604
Iteration 19/25 | Loss: 0.00131604
Iteration 20/25 | Loss: 0.00131604
Iteration 21/25 | Loss: 0.00131604
Iteration 22/25 | Loss: 0.00131604
Iteration 23/25 | Loss: 0.00131604
Iteration 24/25 | Loss: 0.00131604
Iteration 25/25 | Loss: 0.00131604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013160367961972952, 0.0013160367961972952, 0.0013160367961972952, 0.0013160367961972952, 0.0013160367961972952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013160367961972952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131604
Iteration 2/1000 | Loss: 0.00004716
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002101
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001652
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00001584
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001555
Iteration 13/1000 | Loss: 0.00001555
Iteration 14/1000 | Loss: 0.00001551
Iteration 15/1000 | Loss: 0.00001547
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001543
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001538
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001538
Iteration 33/1000 | Loss: 0.00001538
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001538
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001537
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001537
Iteration 49/1000 | Loss: 0.00001537
Iteration 50/1000 | Loss: 0.00001536
Iteration 51/1000 | Loss: 0.00001536
Iteration 52/1000 | Loss: 0.00001536
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001535
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001534
Iteration 63/1000 | Loss: 0.00001534
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001531
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001531
Iteration 79/1000 | Loss: 0.00001531
Iteration 80/1000 | Loss: 0.00001531
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001530
Iteration 84/1000 | Loss: 0.00001530
Iteration 85/1000 | Loss: 0.00001530
Iteration 86/1000 | Loss: 0.00001530
Iteration 87/1000 | Loss: 0.00001530
Iteration 88/1000 | Loss: 0.00001530
Iteration 89/1000 | Loss: 0.00001530
Iteration 90/1000 | Loss: 0.00001530
Iteration 91/1000 | Loss: 0.00001529
Iteration 92/1000 | Loss: 0.00001529
Iteration 93/1000 | Loss: 0.00001529
Iteration 94/1000 | Loss: 0.00001529
Iteration 95/1000 | Loss: 0.00001529
Iteration 96/1000 | Loss: 0.00001529
Iteration 97/1000 | Loss: 0.00001529
Iteration 98/1000 | Loss: 0.00001529
Iteration 99/1000 | Loss: 0.00001529
Iteration 100/1000 | Loss: 0.00001529
Iteration 101/1000 | Loss: 0.00001529
Iteration 102/1000 | Loss: 0.00001528
Iteration 103/1000 | Loss: 0.00001528
Iteration 104/1000 | Loss: 0.00001528
Iteration 105/1000 | Loss: 0.00001528
Iteration 106/1000 | Loss: 0.00001528
Iteration 107/1000 | Loss: 0.00001528
Iteration 108/1000 | Loss: 0.00001528
Iteration 109/1000 | Loss: 0.00001528
Iteration 110/1000 | Loss: 0.00001528
Iteration 111/1000 | Loss: 0.00001527
Iteration 112/1000 | Loss: 0.00001527
Iteration 113/1000 | Loss: 0.00001527
Iteration 114/1000 | Loss: 0.00001527
Iteration 115/1000 | Loss: 0.00001527
Iteration 116/1000 | Loss: 0.00001527
Iteration 117/1000 | Loss: 0.00001527
Iteration 118/1000 | Loss: 0.00001527
Iteration 119/1000 | Loss: 0.00001527
Iteration 120/1000 | Loss: 0.00001527
Iteration 121/1000 | Loss: 0.00001527
Iteration 122/1000 | Loss: 0.00001527
Iteration 123/1000 | Loss: 0.00001527
Iteration 124/1000 | Loss: 0.00001527
Iteration 125/1000 | Loss: 0.00001527
Iteration 126/1000 | Loss: 0.00001527
Iteration 127/1000 | Loss: 0.00001527
Iteration 128/1000 | Loss: 0.00001526
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001526
Iteration 133/1000 | Loss: 0.00001526
Iteration 134/1000 | Loss: 0.00001526
Iteration 135/1000 | Loss: 0.00001526
Iteration 136/1000 | Loss: 0.00001526
Iteration 137/1000 | Loss: 0.00001526
Iteration 138/1000 | Loss: 0.00001526
Iteration 139/1000 | Loss: 0.00001526
Iteration 140/1000 | Loss: 0.00001526
Iteration 141/1000 | Loss: 0.00001526
Iteration 142/1000 | Loss: 0.00001526
Iteration 143/1000 | Loss: 0.00001526
Iteration 144/1000 | Loss: 0.00001526
Iteration 145/1000 | Loss: 0.00001526
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001525
Iteration 155/1000 | Loss: 0.00001525
Iteration 156/1000 | Loss: 0.00001525
Iteration 157/1000 | Loss: 0.00001525
Iteration 158/1000 | Loss: 0.00001525
Iteration 159/1000 | Loss: 0.00001525
Iteration 160/1000 | Loss: 0.00001525
Iteration 161/1000 | Loss: 0.00001525
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001524
Iteration 164/1000 | Loss: 0.00001524
Iteration 165/1000 | Loss: 0.00001524
Iteration 166/1000 | Loss: 0.00001524
Iteration 167/1000 | Loss: 0.00001524
Iteration 168/1000 | Loss: 0.00001524
Iteration 169/1000 | Loss: 0.00001524
Iteration 170/1000 | Loss: 0.00001524
Iteration 171/1000 | Loss: 0.00001524
Iteration 172/1000 | Loss: 0.00001523
Iteration 173/1000 | Loss: 0.00001523
Iteration 174/1000 | Loss: 0.00001523
Iteration 175/1000 | Loss: 0.00001523
Iteration 176/1000 | Loss: 0.00001523
Iteration 177/1000 | Loss: 0.00001523
Iteration 178/1000 | Loss: 0.00001523
Iteration 179/1000 | Loss: 0.00001522
Iteration 180/1000 | Loss: 0.00001522
Iteration 181/1000 | Loss: 0.00001522
Iteration 182/1000 | Loss: 0.00001522
Iteration 183/1000 | Loss: 0.00001521
Iteration 184/1000 | Loss: 0.00001521
Iteration 185/1000 | Loss: 0.00001521
Iteration 186/1000 | Loss: 0.00001521
Iteration 187/1000 | Loss: 0.00001521
Iteration 188/1000 | Loss: 0.00001521
Iteration 189/1000 | Loss: 0.00001521
Iteration 190/1000 | Loss: 0.00001521
Iteration 191/1000 | Loss: 0.00001521
Iteration 192/1000 | Loss: 0.00001520
Iteration 193/1000 | Loss: 0.00001520
Iteration 194/1000 | Loss: 0.00001520
Iteration 195/1000 | Loss: 0.00001520
Iteration 196/1000 | Loss: 0.00001520
Iteration 197/1000 | Loss: 0.00001520
Iteration 198/1000 | Loss: 0.00001520
Iteration 199/1000 | Loss: 0.00001520
Iteration 200/1000 | Loss: 0.00001520
Iteration 201/1000 | Loss: 0.00001520
Iteration 202/1000 | Loss: 0.00001520
Iteration 203/1000 | Loss: 0.00001520
Iteration 204/1000 | Loss: 0.00001520
Iteration 205/1000 | Loss: 0.00001520
Iteration 206/1000 | Loss: 0.00001520
Iteration 207/1000 | Loss: 0.00001520
Iteration 208/1000 | Loss: 0.00001520
Iteration 209/1000 | Loss: 0.00001520
Iteration 210/1000 | Loss: 0.00001520
Iteration 211/1000 | Loss: 0.00001520
Iteration 212/1000 | Loss: 0.00001520
Iteration 213/1000 | Loss: 0.00001520
Iteration 214/1000 | Loss: 0.00001520
Iteration 215/1000 | Loss: 0.00001520
Iteration 216/1000 | Loss: 0.00001520
Iteration 217/1000 | Loss: 0.00001520
Iteration 218/1000 | Loss: 0.00001520
Iteration 219/1000 | Loss: 0.00001520
Iteration 220/1000 | Loss: 0.00001520
Iteration 221/1000 | Loss: 0.00001520
Iteration 222/1000 | Loss: 0.00001520
Iteration 223/1000 | Loss: 0.00001520
Iteration 224/1000 | Loss: 0.00001520
Iteration 225/1000 | Loss: 0.00001520
Iteration 226/1000 | Loss: 0.00001520
Iteration 227/1000 | Loss: 0.00001520
Iteration 228/1000 | Loss: 0.00001520
Iteration 229/1000 | Loss: 0.00001520
Iteration 230/1000 | Loss: 0.00001520
Iteration 231/1000 | Loss: 0.00001520
Iteration 232/1000 | Loss: 0.00001520
Iteration 233/1000 | Loss: 0.00001520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.5197601896943524e-05, 1.5197601896943524e-05, 1.5197601896943524e-05, 1.5197601896943524e-05, 1.5197601896943524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5197601896943524e-05

Optimization complete. Final v2v error: 3.3440473079681396 mm

Highest mean error: 4.02721643447876 mm for frame 69

Lowest mean error: 2.9952199459075928 mm for frame 47

Saving results

Total time: 36.32798099517822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025253
Iteration 2/25 | Loss: 0.00182982
Iteration 3/25 | Loss: 0.00142708
Iteration 4/25 | Loss: 0.00139934
Iteration 5/25 | Loss: 0.00138992
Iteration 6/25 | Loss: 0.00138856
Iteration 7/25 | Loss: 0.00138856
Iteration 8/25 | Loss: 0.00138856
Iteration 9/25 | Loss: 0.00138856
Iteration 10/25 | Loss: 0.00138856
Iteration 11/25 | Loss: 0.00138856
Iteration 12/25 | Loss: 0.00138856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013885637745261192, 0.0013885637745261192, 0.0013885637745261192, 0.0013885637745261192, 0.0013885637745261192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013885637745261192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79788989
Iteration 2/25 | Loss: 0.00119671
Iteration 3/25 | Loss: 0.00119671
Iteration 4/25 | Loss: 0.00119671
Iteration 5/25 | Loss: 0.00119671
Iteration 6/25 | Loss: 0.00119671
Iteration 7/25 | Loss: 0.00119670
Iteration 8/25 | Loss: 0.00119670
Iteration 9/25 | Loss: 0.00119670
Iteration 10/25 | Loss: 0.00119670
Iteration 11/25 | Loss: 0.00119670
Iteration 12/25 | Loss: 0.00119670
Iteration 13/25 | Loss: 0.00119670
Iteration 14/25 | Loss: 0.00119670
Iteration 15/25 | Loss: 0.00119670
Iteration 16/25 | Loss: 0.00119670
Iteration 17/25 | Loss: 0.00119670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011967038735747337, 0.0011967038735747337, 0.0011967038735747337, 0.0011967038735747337, 0.0011967038735747337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011967038735747337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119670
Iteration 2/1000 | Loss: 0.00007625
Iteration 3/1000 | Loss: 0.00006213
Iteration 4/1000 | Loss: 0.00005655
Iteration 5/1000 | Loss: 0.00005323
Iteration 6/1000 | Loss: 0.00005117
Iteration 7/1000 | Loss: 0.00005031
Iteration 8/1000 | Loss: 0.00004965
Iteration 9/1000 | Loss: 0.00004900
Iteration 10/1000 | Loss: 0.00004845
Iteration 11/1000 | Loss: 0.00004802
Iteration 12/1000 | Loss: 0.00004772
Iteration 13/1000 | Loss: 0.00004737
Iteration 14/1000 | Loss: 0.00004705
Iteration 15/1000 | Loss: 0.00004677
Iteration 16/1000 | Loss: 0.00004650
Iteration 17/1000 | Loss: 0.00004627
Iteration 18/1000 | Loss: 0.00004601
Iteration 19/1000 | Loss: 0.00004579
Iteration 20/1000 | Loss: 0.00004564
Iteration 21/1000 | Loss: 0.00004557
Iteration 22/1000 | Loss: 0.00004557
Iteration 23/1000 | Loss: 0.00004554
Iteration 24/1000 | Loss: 0.00004554
Iteration 25/1000 | Loss: 0.00004551
Iteration 26/1000 | Loss: 0.00004551
Iteration 27/1000 | Loss: 0.00004551
Iteration 28/1000 | Loss: 0.00004550
Iteration 29/1000 | Loss: 0.00004550
Iteration 30/1000 | Loss: 0.00004550
Iteration 31/1000 | Loss: 0.00004550
Iteration 32/1000 | Loss: 0.00004550
Iteration 33/1000 | Loss: 0.00004549
Iteration 34/1000 | Loss: 0.00004549
Iteration 35/1000 | Loss: 0.00004549
Iteration 36/1000 | Loss: 0.00004549
Iteration 37/1000 | Loss: 0.00004549
Iteration 38/1000 | Loss: 0.00004549
Iteration 39/1000 | Loss: 0.00004548
Iteration 40/1000 | Loss: 0.00004548
Iteration 41/1000 | Loss: 0.00004547
Iteration 42/1000 | Loss: 0.00004547
Iteration 43/1000 | Loss: 0.00004546
Iteration 44/1000 | Loss: 0.00004545
Iteration 45/1000 | Loss: 0.00004545
Iteration 46/1000 | Loss: 0.00004545
Iteration 47/1000 | Loss: 0.00004545
Iteration 48/1000 | Loss: 0.00004545
Iteration 49/1000 | Loss: 0.00004545
Iteration 50/1000 | Loss: 0.00004544
Iteration 51/1000 | Loss: 0.00004544
Iteration 52/1000 | Loss: 0.00004544
Iteration 53/1000 | Loss: 0.00004544
Iteration 54/1000 | Loss: 0.00004543
Iteration 55/1000 | Loss: 0.00004543
Iteration 56/1000 | Loss: 0.00004543
Iteration 57/1000 | Loss: 0.00004543
Iteration 58/1000 | Loss: 0.00004543
Iteration 59/1000 | Loss: 0.00004543
Iteration 60/1000 | Loss: 0.00004543
Iteration 61/1000 | Loss: 0.00004542
Iteration 62/1000 | Loss: 0.00004542
Iteration 63/1000 | Loss: 0.00004542
Iteration 64/1000 | Loss: 0.00004542
Iteration 65/1000 | Loss: 0.00004542
Iteration 66/1000 | Loss: 0.00004542
Iteration 67/1000 | Loss: 0.00004542
Iteration 68/1000 | Loss: 0.00004542
Iteration 69/1000 | Loss: 0.00004542
Iteration 70/1000 | Loss: 0.00004542
Iteration 71/1000 | Loss: 0.00004542
Iteration 72/1000 | Loss: 0.00004541
Iteration 73/1000 | Loss: 0.00004541
Iteration 74/1000 | Loss: 0.00004541
Iteration 75/1000 | Loss: 0.00004541
Iteration 76/1000 | Loss: 0.00004541
Iteration 77/1000 | Loss: 0.00004541
Iteration 78/1000 | Loss: 0.00004541
Iteration 79/1000 | Loss: 0.00004540
Iteration 80/1000 | Loss: 0.00004540
Iteration 81/1000 | Loss: 0.00004540
Iteration 82/1000 | Loss: 0.00004540
Iteration 83/1000 | Loss: 0.00004539
Iteration 84/1000 | Loss: 0.00004539
Iteration 85/1000 | Loss: 0.00004539
Iteration 86/1000 | Loss: 0.00004539
Iteration 87/1000 | Loss: 0.00004539
Iteration 88/1000 | Loss: 0.00004539
Iteration 89/1000 | Loss: 0.00004539
Iteration 90/1000 | Loss: 0.00004539
Iteration 91/1000 | Loss: 0.00004539
Iteration 92/1000 | Loss: 0.00004539
Iteration 93/1000 | Loss: 0.00004538
Iteration 94/1000 | Loss: 0.00004538
Iteration 95/1000 | Loss: 0.00004538
Iteration 96/1000 | Loss: 0.00004538
Iteration 97/1000 | Loss: 0.00004538
Iteration 98/1000 | Loss: 0.00004538
Iteration 99/1000 | Loss: 0.00004538
Iteration 100/1000 | Loss: 0.00004538
Iteration 101/1000 | Loss: 0.00004538
Iteration 102/1000 | Loss: 0.00004537
Iteration 103/1000 | Loss: 0.00004537
Iteration 104/1000 | Loss: 0.00004537
Iteration 105/1000 | Loss: 0.00004537
Iteration 106/1000 | Loss: 0.00004537
Iteration 107/1000 | Loss: 0.00004537
Iteration 108/1000 | Loss: 0.00004537
Iteration 109/1000 | Loss: 0.00004537
Iteration 110/1000 | Loss: 0.00004536
Iteration 111/1000 | Loss: 0.00004536
Iteration 112/1000 | Loss: 0.00004536
Iteration 113/1000 | Loss: 0.00004536
Iteration 114/1000 | Loss: 0.00004536
Iteration 115/1000 | Loss: 0.00004536
Iteration 116/1000 | Loss: 0.00004536
Iteration 117/1000 | Loss: 0.00004536
Iteration 118/1000 | Loss: 0.00004536
Iteration 119/1000 | Loss: 0.00004536
Iteration 120/1000 | Loss: 0.00004536
Iteration 121/1000 | Loss: 0.00004536
Iteration 122/1000 | Loss: 0.00004536
Iteration 123/1000 | Loss: 0.00004536
Iteration 124/1000 | Loss: 0.00004535
Iteration 125/1000 | Loss: 0.00004535
Iteration 126/1000 | Loss: 0.00004535
Iteration 127/1000 | Loss: 0.00004535
Iteration 128/1000 | Loss: 0.00004534
Iteration 129/1000 | Loss: 0.00004534
Iteration 130/1000 | Loss: 0.00004533
Iteration 131/1000 | Loss: 0.00004533
Iteration 132/1000 | Loss: 0.00004533
Iteration 133/1000 | Loss: 0.00004533
Iteration 134/1000 | Loss: 0.00004533
Iteration 135/1000 | Loss: 0.00004533
Iteration 136/1000 | Loss: 0.00004533
Iteration 137/1000 | Loss: 0.00004533
Iteration 138/1000 | Loss: 0.00004533
Iteration 139/1000 | Loss: 0.00004532
Iteration 140/1000 | Loss: 0.00004532
Iteration 141/1000 | Loss: 0.00004532
Iteration 142/1000 | Loss: 0.00004532
Iteration 143/1000 | Loss: 0.00004532
Iteration 144/1000 | Loss: 0.00004532
Iteration 145/1000 | Loss: 0.00004532
Iteration 146/1000 | Loss: 0.00004532
Iteration 147/1000 | Loss: 0.00004531
Iteration 148/1000 | Loss: 0.00004531
Iteration 149/1000 | Loss: 0.00004531
Iteration 150/1000 | Loss: 0.00004531
Iteration 151/1000 | Loss: 0.00004531
Iteration 152/1000 | Loss: 0.00004530
Iteration 153/1000 | Loss: 0.00004530
Iteration 154/1000 | Loss: 0.00004530
Iteration 155/1000 | Loss: 0.00004530
Iteration 156/1000 | Loss: 0.00004530
Iteration 157/1000 | Loss: 0.00004530
Iteration 158/1000 | Loss: 0.00004530
Iteration 159/1000 | Loss: 0.00004530
Iteration 160/1000 | Loss: 0.00004530
Iteration 161/1000 | Loss: 0.00004530
Iteration 162/1000 | Loss: 0.00004530
Iteration 163/1000 | Loss: 0.00004529
Iteration 164/1000 | Loss: 0.00004529
Iteration 165/1000 | Loss: 0.00004529
Iteration 166/1000 | Loss: 0.00004529
Iteration 167/1000 | Loss: 0.00004529
Iteration 168/1000 | Loss: 0.00004529
Iteration 169/1000 | Loss: 0.00004529
Iteration 170/1000 | Loss: 0.00004529
Iteration 171/1000 | Loss: 0.00004529
Iteration 172/1000 | Loss: 0.00004529
Iteration 173/1000 | Loss: 0.00004528
Iteration 174/1000 | Loss: 0.00004528
Iteration 175/1000 | Loss: 0.00004528
Iteration 176/1000 | Loss: 0.00004528
Iteration 177/1000 | Loss: 0.00004528
Iteration 178/1000 | Loss: 0.00004528
Iteration 179/1000 | Loss: 0.00004528
Iteration 180/1000 | Loss: 0.00004528
Iteration 181/1000 | Loss: 0.00004528
Iteration 182/1000 | Loss: 0.00004528
Iteration 183/1000 | Loss: 0.00004528
Iteration 184/1000 | Loss: 0.00004528
Iteration 185/1000 | Loss: 0.00004528
Iteration 186/1000 | Loss: 0.00004527
Iteration 187/1000 | Loss: 0.00004527
Iteration 188/1000 | Loss: 0.00004527
Iteration 189/1000 | Loss: 0.00004527
Iteration 190/1000 | Loss: 0.00004527
Iteration 191/1000 | Loss: 0.00004527
Iteration 192/1000 | Loss: 0.00004527
Iteration 193/1000 | Loss: 0.00004527
Iteration 194/1000 | Loss: 0.00004527
Iteration 195/1000 | Loss: 0.00004527
Iteration 196/1000 | Loss: 0.00004527
Iteration 197/1000 | Loss: 0.00004527
Iteration 198/1000 | Loss: 0.00004526
Iteration 199/1000 | Loss: 0.00004526
Iteration 200/1000 | Loss: 0.00004526
Iteration 201/1000 | Loss: 0.00004526
Iteration 202/1000 | Loss: 0.00004526
Iteration 203/1000 | Loss: 0.00004526
Iteration 204/1000 | Loss: 0.00004526
Iteration 205/1000 | Loss: 0.00004526
Iteration 206/1000 | Loss: 0.00004526
Iteration 207/1000 | Loss: 0.00004526
Iteration 208/1000 | Loss: 0.00004526
Iteration 209/1000 | Loss: 0.00004526
Iteration 210/1000 | Loss: 0.00004526
Iteration 211/1000 | Loss: 0.00004526
Iteration 212/1000 | Loss: 0.00004526
Iteration 213/1000 | Loss: 0.00004526
Iteration 214/1000 | Loss: 0.00004526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [4.5257449528435245e-05, 4.5257449528435245e-05, 4.5257449528435245e-05, 4.5257449528435245e-05, 4.5257449528435245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5257449528435245e-05

Optimization complete. Final v2v error: 5.499801158905029 mm

Highest mean error: 6.095517635345459 mm for frame 98

Lowest mean error: 4.355062484741211 mm for frame 50

Saving results

Total time: 57.72747611999512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881801
Iteration 2/25 | Loss: 0.00149301
Iteration 3/25 | Loss: 0.00126163
Iteration 4/25 | Loss: 0.00122334
Iteration 5/25 | Loss: 0.00121899
Iteration 6/25 | Loss: 0.00121859
Iteration 7/25 | Loss: 0.00121859
Iteration 8/25 | Loss: 0.00121859
Iteration 9/25 | Loss: 0.00121859
Iteration 10/25 | Loss: 0.00121859
Iteration 11/25 | Loss: 0.00121859
Iteration 12/25 | Loss: 0.00121859
Iteration 13/25 | Loss: 0.00121859
Iteration 14/25 | Loss: 0.00121859
Iteration 15/25 | Loss: 0.00121859
Iteration 16/25 | Loss: 0.00121859
Iteration 17/25 | Loss: 0.00121859
Iteration 18/25 | Loss: 0.00121859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012185884406790137, 0.0012185884406790137, 0.0012185884406790137, 0.0012185884406790137, 0.0012185884406790137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012185884406790137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89682966
Iteration 2/25 | Loss: 0.00069727
Iteration 3/25 | Loss: 0.00069726
Iteration 4/25 | Loss: 0.00069726
Iteration 5/25 | Loss: 0.00069726
Iteration 6/25 | Loss: 0.00069726
Iteration 7/25 | Loss: 0.00069726
Iteration 8/25 | Loss: 0.00069726
Iteration 9/25 | Loss: 0.00069726
Iteration 10/25 | Loss: 0.00069726
Iteration 11/25 | Loss: 0.00069726
Iteration 12/25 | Loss: 0.00069726
Iteration 13/25 | Loss: 0.00069726
Iteration 14/25 | Loss: 0.00069726
Iteration 15/25 | Loss: 0.00069726
Iteration 16/25 | Loss: 0.00069726
Iteration 17/25 | Loss: 0.00069726
Iteration 18/25 | Loss: 0.00069726
Iteration 19/25 | Loss: 0.00069726
Iteration 20/25 | Loss: 0.00069726
Iteration 21/25 | Loss: 0.00069726
Iteration 22/25 | Loss: 0.00069726
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006972579867579043, 0.0006972579867579043, 0.0006972579867579043, 0.0006972579867579043, 0.0006972579867579043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006972579867579043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069726
Iteration 2/1000 | Loss: 0.00003793
Iteration 3/1000 | Loss: 0.00002855
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00002054
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001982
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001980
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001979
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001973
Iteration 23/1000 | Loss: 0.00001973
Iteration 24/1000 | Loss: 0.00001972
Iteration 25/1000 | Loss: 0.00001972
Iteration 26/1000 | Loss: 0.00001972
Iteration 27/1000 | Loss: 0.00001972
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001969
Iteration 36/1000 | Loss: 0.00001969
Iteration 37/1000 | Loss: 0.00001969
Iteration 38/1000 | Loss: 0.00001969
Iteration 39/1000 | Loss: 0.00001968
Iteration 40/1000 | Loss: 0.00001968
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001967
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001966
Iteration 45/1000 | Loss: 0.00001966
Iteration 46/1000 | Loss: 0.00001966
Iteration 47/1000 | Loss: 0.00001966
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001965
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001964
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001963
Iteration 57/1000 | Loss: 0.00001963
Iteration 58/1000 | Loss: 0.00001963
Iteration 59/1000 | Loss: 0.00001963
Iteration 60/1000 | Loss: 0.00001963
Iteration 61/1000 | Loss: 0.00001963
Iteration 62/1000 | Loss: 0.00001963
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00001963
Iteration 65/1000 | Loss: 0.00001963
Iteration 66/1000 | Loss: 0.00001962
Iteration 67/1000 | Loss: 0.00001962
Iteration 68/1000 | Loss: 0.00001962
Iteration 69/1000 | Loss: 0.00001962
Iteration 70/1000 | Loss: 0.00001962
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001962
Iteration 73/1000 | Loss: 0.00001962
Iteration 74/1000 | Loss: 0.00001962
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001962
Iteration 78/1000 | Loss: 0.00001962
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001962
Iteration 101/1000 | Loss: 0.00001962
Iteration 102/1000 | Loss: 0.00001962
Iteration 103/1000 | Loss: 0.00001962
Iteration 104/1000 | Loss: 0.00001962
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001962
Iteration 108/1000 | Loss: 0.00001962
Iteration 109/1000 | Loss: 0.00001962
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001962
Iteration 113/1000 | Loss: 0.00001962
Iteration 114/1000 | Loss: 0.00001962
Iteration 115/1000 | Loss: 0.00001962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.9621740648290142e-05, 1.9621740648290142e-05, 1.9621740648290142e-05, 1.9621740648290142e-05, 1.9621740648290142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9621740648290142e-05

Optimization complete. Final v2v error: 3.7814698219299316 mm

Highest mean error: 4.074136257171631 mm for frame 149

Lowest mean error: 3.558433771133423 mm for frame 98

Saving results

Total time: 28.469590187072754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589393
Iteration 2/25 | Loss: 0.00133263
Iteration 3/25 | Loss: 0.00122759
Iteration 4/25 | Loss: 0.00121350
Iteration 5/25 | Loss: 0.00120866
Iteration 6/25 | Loss: 0.00120757
Iteration 7/25 | Loss: 0.00120749
Iteration 8/25 | Loss: 0.00120749
Iteration 9/25 | Loss: 0.00120749
Iteration 10/25 | Loss: 0.00120749
Iteration 11/25 | Loss: 0.00120749
Iteration 12/25 | Loss: 0.00120749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012074902188032866, 0.0012074902188032866, 0.0012074902188032866, 0.0012074902188032866, 0.0012074902188032866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012074902188032866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39893186
Iteration 2/25 | Loss: 0.00135285
Iteration 3/25 | Loss: 0.00135285
Iteration 4/25 | Loss: 0.00135285
Iteration 5/25 | Loss: 0.00135285
Iteration 6/25 | Loss: 0.00135284
Iteration 7/25 | Loss: 0.00135284
Iteration 8/25 | Loss: 0.00135284
Iteration 9/25 | Loss: 0.00135284
Iteration 10/25 | Loss: 0.00135284
Iteration 11/25 | Loss: 0.00135284
Iteration 12/25 | Loss: 0.00135284
Iteration 13/25 | Loss: 0.00135284
Iteration 14/25 | Loss: 0.00135284
Iteration 15/25 | Loss: 0.00135284
Iteration 16/25 | Loss: 0.00135284
Iteration 17/25 | Loss: 0.00135284
Iteration 18/25 | Loss: 0.00135284
Iteration 19/25 | Loss: 0.00135284
Iteration 20/25 | Loss: 0.00135284
Iteration 21/25 | Loss: 0.00135284
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013528431300073862, 0.0013528431300073862, 0.0013528431300073862, 0.0013528431300073862, 0.0013528431300073862]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013528431300073862

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135284
Iteration 2/1000 | Loss: 0.00004514
Iteration 3/1000 | Loss: 0.00002346
Iteration 4/1000 | Loss: 0.00002059
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001808
Iteration 7/1000 | Loss: 0.00001759
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001680
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001611
Iteration 18/1000 | Loss: 0.00001611
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001608
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001607
Iteration 23/1000 | Loss: 0.00001607
Iteration 24/1000 | Loss: 0.00001607
Iteration 25/1000 | Loss: 0.00001607
Iteration 26/1000 | Loss: 0.00001607
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001607
Iteration 29/1000 | Loss: 0.00001607
Iteration 30/1000 | Loss: 0.00001607
Iteration 31/1000 | Loss: 0.00001607
Iteration 32/1000 | Loss: 0.00001607
Iteration 33/1000 | Loss: 0.00001606
Iteration 34/1000 | Loss: 0.00001606
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001604
Iteration 38/1000 | Loss: 0.00001604
Iteration 39/1000 | Loss: 0.00001603
Iteration 40/1000 | Loss: 0.00001603
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001603
Iteration 43/1000 | Loss: 0.00001602
Iteration 44/1000 | Loss: 0.00001602
Iteration 45/1000 | Loss: 0.00001602
Iteration 46/1000 | Loss: 0.00001602
Iteration 47/1000 | Loss: 0.00001602
Iteration 48/1000 | Loss: 0.00001602
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001600
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001600
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001600
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001596
Iteration 78/1000 | Loss: 0.00001596
Iteration 79/1000 | Loss: 0.00001596
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001595
Iteration 89/1000 | Loss: 0.00001595
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001594
Iteration 102/1000 | Loss: 0.00001594
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.59390147018712e-05, 1.59390147018712e-05, 1.59390147018712e-05, 1.59390147018712e-05, 1.59390147018712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.59390147018712e-05

Optimization complete. Final v2v error: 3.41999888420105 mm

Highest mean error: 3.705023765563965 mm for frame 30

Lowest mean error: 3.2064578533172607 mm for frame 82

Saving results

Total time: 31.563767433166504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613810
Iteration 2/25 | Loss: 0.00129300
Iteration 3/25 | Loss: 0.00116711
Iteration 4/25 | Loss: 0.00115974
Iteration 5/25 | Loss: 0.00115876
Iteration 6/25 | Loss: 0.00115876
Iteration 7/25 | Loss: 0.00115876
Iteration 8/25 | Loss: 0.00115876
Iteration 9/25 | Loss: 0.00115876
Iteration 10/25 | Loss: 0.00115876
Iteration 11/25 | Loss: 0.00115876
Iteration 12/25 | Loss: 0.00115876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011587617918848991, 0.0011587617918848991, 0.0011587617918848991, 0.0011587617918848991, 0.0011587617918848991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011587617918848991

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44460320
Iteration 2/25 | Loss: 0.00097476
Iteration 3/25 | Loss: 0.00097476
Iteration 4/25 | Loss: 0.00097476
Iteration 5/25 | Loss: 0.00097476
Iteration 6/25 | Loss: 0.00097476
Iteration 7/25 | Loss: 0.00097476
Iteration 8/25 | Loss: 0.00097476
Iteration 9/25 | Loss: 0.00097476
Iteration 10/25 | Loss: 0.00097476
Iteration 11/25 | Loss: 0.00097476
Iteration 12/25 | Loss: 0.00097476
Iteration 13/25 | Loss: 0.00097476
Iteration 14/25 | Loss: 0.00097476
Iteration 15/25 | Loss: 0.00097476
Iteration 16/25 | Loss: 0.00097476
Iteration 17/25 | Loss: 0.00097476
Iteration 18/25 | Loss: 0.00097476
Iteration 19/25 | Loss: 0.00097476
Iteration 20/25 | Loss: 0.00097476
Iteration 21/25 | Loss: 0.00097476
Iteration 22/25 | Loss: 0.00097476
Iteration 23/25 | Loss: 0.00097476
Iteration 24/25 | Loss: 0.00097476
Iteration 25/25 | Loss: 0.00097476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097476
Iteration 2/1000 | Loss: 0.00003294
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001644
Iteration 6/1000 | Loss: 0.00001580
Iteration 7/1000 | Loss: 0.00001534
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001482
Iteration 10/1000 | Loss: 0.00001478
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001464
Iteration 13/1000 | Loss: 0.00001463
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001455
Iteration 21/1000 | Loss: 0.00001455
Iteration 22/1000 | Loss: 0.00001454
Iteration 23/1000 | Loss: 0.00001454
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001454
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001453
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001452
Iteration 31/1000 | Loss: 0.00001452
Iteration 32/1000 | Loss: 0.00001452
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001451
Iteration 37/1000 | Loss: 0.00001451
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001446
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001442
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001441
Iteration 67/1000 | Loss: 0.00001441
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001436
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001435
Iteration 85/1000 | Loss: 0.00001435
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001430
Iteration 95/1000 | Loss: 0.00001430
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001426
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001425
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001425
Iteration 138/1000 | Loss: 0.00001425
Iteration 139/1000 | Loss: 0.00001425
Iteration 140/1000 | Loss: 0.00001425
Iteration 141/1000 | Loss: 0.00001425
Iteration 142/1000 | Loss: 0.00001425
Iteration 143/1000 | Loss: 0.00001425
Iteration 144/1000 | Loss: 0.00001425
Iteration 145/1000 | Loss: 0.00001425
Iteration 146/1000 | Loss: 0.00001425
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001425
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001425
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001425
Iteration 155/1000 | Loss: 0.00001425
Iteration 156/1000 | Loss: 0.00001425
Iteration 157/1000 | Loss: 0.00001425
Iteration 158/1000 | Loss: 0.00001425
Iteration 159/1000 | Loss: 0.00001425
Iteration 160/1000 | Loss: 0.00001425
Iteration 161/1000 | Loss: 0.00001425
Iteration 162/1000 | Loss: 0.00001425
Iteration 163/1000 | Loss: 0.00001425
Iteration 164/1000 | Loss: 0.00001425
Iteration 165/1000 | Loss: 0.00001425
Iteration 166/1000 | Loss: 0.00001425
Iteration 167/1000 | Loss: 0.00001425
Iteration 168/1000 | Loss: 0.00001425
Iteration 169/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4247255421651062e-05, 1.4247255421651062e-05, 1.4247255421651062e-05, 1.4247255421651062e-05, 1.4247255421651062e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4247255421651062e-05

Optimization complete. Final v2v error: 3.1654763221740723 mm

Highest mean error: 3.462010622024536 mm for frame 24

Lowest mean error: 2.946715831756592 mm for frame 103

Saving results

Total time: 36.37549448013306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896101
Iteration 2/25 | Loss: 0.00144287
Iteration 3/25 | Loss: 0.00127795
Iteration 4/25 | Loss: 0.00125889
Iteration 5/25 | Loss: 0.00125400
Iteration 6/25 | Loss: 0.00125263
Iteration 7/25 | Loss: 0.00125235
Iteration 8/25 | Loss: 0.00125235
Iteration 9/25 | Loss: 0.00125235
Iteration 10/25 | Loss: 0.00125235
Iteration 11/25 | Loss: 0.00125235
Iteration 12/25 | Loss: 0.00125235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012523530749604106, 0.0012523530749604106, 0.0012523530749604106, 0.0012523530749604106, 0.0012523530749604106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012523530749604106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27921653
Iteration 2/25 | Loss: 0.00231654
Iteration 3/25 | Loss: 0.00231653
Iteration 4/25 | Loss: 0.00231653
Iteration 5/25 | Loss: 0.00231653
Iteration 6/25 | Loss: 0.00231653
Iteration 7/25 | Loss: 0.00231653
Iteration 8/25 | Loss: 0.00231653
Iteration 9/25 | Loss: 0.00231653
Iteration 10/25 | Loss: 0.00231653
Iteration 11/25 | Loss: 0.00231653
Iteration 12/25 | Loss: 0.00231653
Iteration 13/25 | Loss: 0.00231653
Iteration 14/25 | Loss: 0.00231653
Iteration 15/25 | Loss: 0.00231653
Iteration 16/25 | Loss: 0.00231653
Iteration 17/25 | Loss: 0.00231653
Iteration 18/25 | Loss: 0.00231653
Iteration 19/25 | Loss: 0.00231653
Iteration 20/25 | Loss: 0.00231653
Iteration 21/25 | Loss: 0.00231653
Iteration 22/25 | Loss: 0.00231653
Iteration 23/25 | Loss: 0.00231653
Iteration 24/25 | Loss: 0.00231653
Iteration 25/25 | Loss: 0.00231653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231653
Iteration 2/1000 | Loss: 0.00027853
Iteration 3/1000 | Loss: 0.00021024
Iteration 4/1000 | Loss: 0.00017080
Iteration 5/1000 | Loss: 0.00012822
Iteration 6/1000 | Loss: 0.00018781
Iteration 7/1000 | Loss: 0.00008765
Iteration 8/1000 | Loss: 0.00007590
Iteration 9/1000 | Loss: 0.00007008
Iteration 10/1000 | Loss: 0.00006674
Iteration 11/1000 | Loss: 0.00006462
Iteration 12/1000 | Loss: 0.00006193
Iteration 13/1000 | Loss: 0.00005979
Iteration 14/1000 | Loss: 0.00005871
Iteration 15/1000 | Loss: 0.00005784
Iteration 16/1000 | Loss: 0.00005715
Iteration 17/1000 | Loss: 0.00005660
Iteration 18/1000 | Loss: 0.00005611
Iteration 19/1000 | Loss: 0.00005571
Iteration 20/1000 | Loss: 0.00005534
Iteration 21/1000 | Loss: 0.00005507
Iteration 22/1000 | Loss: 0.00005486
Iteration 23/1000 | Loss: 0.00005462
Iteration 24/1000 | Loss: 0.00005448
Iteration 25/1000 | Loss: 0.00005444
Iteration 26/1000 | Loss: 0.00005434
Iteration 27/1000 | Loss: 0.00005431
Iteration 28/1000 | Loss: 0.00005429
Iteration 29/1000 | Loss: 0.00005425
Iteration 30/1000 | Loss: 0.00005424
Iteration 31/1000 | Loss: 0.00005421
Iteration 32/1000 | Loss: 0.00005420
Iteration 33/1000 | Loss: 0.00005418
Iteration 34/1000 | Loss: 0.00005417
Iteration 35/1000 | Loss: 0.00005417
Iteration 36/1000 | Loss: 0.00005416
Iteration 37/1000 | Loss: 0.00005415
Iteration 38/1000 | Loss: 0.00005415
Iteration 39/1000 | Loss: 0.00005414
Iteration 40/1000 | Loss: 0.00005414
Iteration 41/1000 | Loss: 0.00005413
Iteration 42/1000 | Loss: 0.00005413
Iteration 43/1000 | Loss: 0.00005413
Iteration 44/1000 | Loss: 0.00005412
Iteration 45/1000 | Loss: 0.00005412
Iteration 46/1000 | Loss: 0.00005412
Iteration 47/1000 | Loss: 0.00005412
Iteration 48/1000 | Loss: 0.00005411
Iteration 49/1000 | Loss: 0.00005411
Iteration 50/1000 | Loss: 0.00005411
Iteration 51/1000 | Loss: 0.00005410
Iteration 52/1000 | Loss: 0.00005410
Iteration 53/1000 | Loss: 0.00005410
Iteration 54/1000 | Loss: 0.00005410
Iteration 55/1000 | Loss: 0.00005410
Iteration 56/1000 | Loss: 0.00005410
Iteration 57/1000 | Loss: 0.00005409
Iteration 58/1000 | Loss: 0.00005409
Iteration 59/1000 | Loss: 0.00005409
Iteration 60/1000 | Loss: 0.00005409
Iteration 61/1000 | Loss: 0.00005408
Iteration 62/1000 | Loss: 0.00005408
Iteration 63/1000 | Loss: 0.00005407
Iteration 64/1000 | Loss: 0.00005407
Iteration 65/1000 | Loss: 0.00005407
Iteration 66/1000 | Loss: 0.00005406
Iteration 67/1000 | Loss: 0.00005406
Iteration 68/1000 | Loss: 0.00005406
Iteration 69/1000 | Loss: 0.00005406
Iteration 70/1000 | Loss: 0.00005406
Iteration 71/1000 | Loss: 0.00005405
Iteration 72/1000 | Loss: 0.00005405
Iteration 73/1000 | Loss: 0.00005405
Iteration 74/1000 | Loss: 0.00005405
Iteration 75/1000 | Loss: 0.00005405
Iteration 76/1000 | Loss: 0.00005405
Iteration 77/1000 | Loss: 0.00005404
Iteration 78/1000 | Loss: 0.00005404
Iteration 79/1000 | Loss: 0.00005404
Iteration 80/1000 | Loss: 0.00005404
Iteration 81/1000 | Loss: 0.00005404
Iteration 82/1000 | Loss: 0.00005404
Iteration 83/1000 | Loss: 0.00005403
Iteration 84/1000 | Loss: 0.00005403
Iteration 85/1000 | Loss: 0.00005403
Iteration 86/1000 | Loss: 0.00005403
Iteration 87/1000 | Loss: 0.00005403
Iteration 88/1000 | Loss: 0.00005402
Iteration 89/1000 | Loss: 0.00005402
Iteration 90/1000 | Loss: 0.00005402
Iteration 91/1000 | Loss: 0.00005402
Iteration 92/1000 | Loss: 0.00005401
Iteration 93/1000 | Loss: 0.00005401
Iteration 94/1000 | Loss: 0.00005401
Iteration 95/1000 | Loss: 0.00005400
Iteration 96/1000 | Loss: 0.00005400
Iteration 97/1000 | Loss: 0.00005400
Iteration 98/1000 | Loss: 0.00005399
Iteration 99/1000 | Loss: 0.00005399
Iteration 100/1000 | Loss: 0.00005399
Iteration 101/1000 | Loss: 0.00005398
Iteration 102/1000 | Loss: 0.00005398
Iteration 103/1000 | Loss: 0.00005398
Iteration 104/1000 | Loss: 0.00005398
Iteration 105/1000 | Loss: 0.00005398
Iteration 106/1000 | Loss: 0.00005398
Iteration 107/1000 | Loss: 0.00005397
Iteration 108/1000 | Loss: 0.00005397
Iteration 109/1000 | Loss: 0.00005397
Iteration 110/1000 | Loss: 0.00005397
Iteration 111/1000 | Loss: 0.00005397
Iteration 112/1000 | Loss: 0.00005397
Iteration 113/1000 | Loss: 0.00005397
Iteration 114/1000 | Loss: 0.00005397
Iteration 115/1000 | Loss: 0.00005396
Iteration 116/1000 | Loss: 0.00005396
Iteration 117/1000 | Loss: 0.00005396
Iteration 118/1000 | Loss: 0.00005396
Iteration 119/1000 | Loss: 0.00005395
Iteration 120/1000 | Loss: 0.00005395
Iteration 121/1000 | Loss: 0.00005395
Iteration 122/1000 | Loss: 0.00005395
Iteration 123/1000 | Loss: 0.00005395
Iteration 124/1000 | Loss: 0.00005395
Iteration 125/1000 | Loss: 0.00005394
Iteration 126/1000 | Loss: 0.00005394
Iteration 127/1000 | Loss: 0.00005394
Iteration 128/1000 | Loss: 0.00005394
Iteration 129/1000 | Loss: 0.00005394
Iteration 130/1000 | Loss: 0.00005394
Iteration 131/1000 | Loss: 0.00005394
Iteration 132/1000 | Loss: 0.00005394
Iteration 133/1000 | Loss: 0.00005394
Iteration 134/1000 | Loss: 0.00005394
Iteration 135/1000 | Loss: 0.00005394
Iteration 136/1000 | Loss: 0.00005394
Iteration 137/1000 | Loss: 0.00005394
Iteration 138/1000 | Loss: 0.00005394
Iteration 139/1000 | Loss: 0.00005394
Iteration 140/1000 | Loss: 0.00005394
Iteration 141/1000 | Loss: 0.00005394
Iteration 142/1000 | Loss: 0.00005394
Iteration 143/1000 | Loss: 0.00005394
Iteration 144/1000 | Loss: 0.00005394
Iteration 145/1000 | Loss: 0.00005394
Iteration 146/1000 | Loss: 0.00005394
Iteration 147/1000 | Loss: 0.00005394
Iteration 148/1000 | Loss: 0.00005394
Iteration 149/1000 | Loss: 0.00005394
Iteration 150/1000 | Loss: 0.00005394
Iteration 151/1000 | Loss: 0.00005394
Iteration 152/1000 | Loss: 0.00005394
Iteration 153/1000 | Loss: 0.00005394
Iteration 154/1000 | Loss: 0.00005394
Iteration 155/1000 | Loss: 0.00005394
Iteration 156/1000 | Loss: 0.00005394
Iteration 157/1000 | Loss: 0.00005394
Iteration 158/1000 | Loss: 0.00005394
Iteration 159/1000 | Loss: 0.00005394
Iteration 160/1000 | Loss: 0.00005394
Iteration 161/1000 | Loss: 0.00005394
Iteration 162/1000 | Loss: 0.00005394
Iteration 163/1000 | Loss: 0.00005394
Iteration 164/1000 | Loss: 0.00005394
Iteration 165/1000 | Loss: 0.00005394
Iteration 166/1000 | Loss: 0.00005394
Iteration 167/1000 | Loss: 0.00005394
Iteration 168/1000 | Loss: 0.00005394
Iteration 169/1000 | Loss: 0.00005394
Iteration 170/1000 | Loss: 0.00005394
Iteration 171/1000 | Loss: 0.00005394
Iteration 172/1000 | Loss: 0.00005394
Iteration 173/1000 | Loss: 0.00005394
Iteration 174/1000 | Loss: 0.00005394
Iteration 175/1000 | Loss: 0.00005394
Iteration 176/1000 | Loss: 0.00005394
Iteration 177/1000 | Loss: 0.00005394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [5.3938074415782467e-05, 5.3938074415782467e-05, 5.3938074415782467e-05, 5.3938074415782467e-05, 5.3938074415782467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.3938074415782467e-05

Optimization complete. Final v2v error: 3.8524320125579834 mm

Highest mean error: 13.4537935256958 mm for frame 115

Lowest mean error: 2.9260919094085693 mm for frame 64

Saving results

Total time: 64.16519498825073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106320
Iteration 2/25 | Loss: 0.00161655
Iteration 3/25 | Loss: 0.00125571
Iteration 4/25 | Loss: 0.00122576
Iteration 5/25 | Loss: 0.00122114
Iteration 6/25 | Loss: 0.00122021
Iteration 7/25 | Loss: 0.00122021
Iteration 8/25 | Loss: 0.00122021
Iteration 9/25 | Loss: 0.00122021
Iteration 10/25 | Loss: 0.00122021
Iteration 11/25 | Loss: 0.00122021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012202137149870396, 0.0012202137149870396, 0.0012202137149870396, 0.0012202137149870396, 0.0012202137149870396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012202137149870396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46518648
Iteration 2/25 | Loss: 0.00127127
Iteration 3/25 | Loss: 0.00127127
Iteration 4/25 | Loss: 0.00127126
Iteration 5/25 | Loss: 0.00127126
Iteration 6/25 | Loss: 0.00127126
Iteration 7/25 | Loss: 0.00127126
Iteration 8/25 | Loss: 0.00127126
Iteration 9/25 | Loss: 0.00127126
Iteration 10/25 | Loss: 0.00127126
Iteration 11/25 | Loss: 0.00127126
Iteration 12/25 | Loss: 0.00127126
Iteration 13/25 | Loss: 0.00127126
Iteration 14/25 | Loss: 0.00127126
Iteration 15/25 | Loss: 0.00127126
Iteration 16/25 | Loss: 0.00127126
Iteration 17/25 | Loss: 0.00127126
Iteration 18/25 | Loss: 0.00127126
Iteration 19/25 | Loss: 0.00127126
Iteration 20/25 | Loss: 0.00127126
Iteration 21/25 | Loss: 0.00127126
Iteration 22/25 | Loss: 0.00127126
Iteration 23/25 | Loss: 0.00127126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012712635798379779, 0.0012712635798379779, 0.0012712635798379779, 0.0012712635798379779, 0.0012712635798379779]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012712635798379779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127126
Iteration 2/1000 | Loss: 0.00004584
Iteration 3/1000 | Loss: 0.00002740
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001726
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001663
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001652
Iteration 14/1000 | Loss: 0.00001651
Iteration 15/1000 | Loss: 0.00001650
Iteration 16/1000 | Loss: 0.00001647
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001644
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001631
Iteration 21/1000 | Loss: 0.00001630
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001627
Iteration 24/1000 | Loss: 0.00001626
Iteration 25/1000 | Loss: 0.00001626
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001624
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001620
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001615
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.6081125068012625e-05, 1.6081125068012625e-05, 1.6081125068012625e-05, 1.6081125068012625e-05, 1.6081125068012625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6081125068012625e-05

Optimization complete. Final v2v error: 3.412905216217041 mm

Highest mean error: 3.6742963790893555 mm for frame 61

Lowest mean error: 2.9980974197387695 mm for frame 24

Saving results

Total time: 29.314924240112305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525644
Iteration 2/25 | Loss: 0.00143239
Iteration 3/25 | Loss: 0.00129139
Iteration 4/25 | Loss: 0.00126972
Iteration 5/25 | Loss: 0.00126309
Iteration 6/25 | Loss: 0.00126119
Iteration 7/25 | Loss: 0.00126060
Iteration 8/25 | Loss: 0.00126048
Iteration 9/25 | Loss: 0.00126048
Iteration 10/25 | Loss: 0.00126048
Iteration 11/25 | Loss: 0.00126048
Iteration 12/25 | Loss: 0.00126048
Iteration 13/25 | Loss: 0.00126048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012604767689481378, 0.0012604767689481378, 0.0012604767689481378, 0.0012604767689481378, 0.0012604767689481378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012604767689481378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28130388
Iteration 2/25 | Loss: 0.00192783
Iteration 3/25 | Loss: 0.00192783
Iteration 4/25 | Loss: 0.00192783
Iteration 5/25 | Loss: 0.00192783
Iteration 6/25 | Loss: 0.00192783
Iteration 7/25 | Loss: 0.00192783
Iteration 8/25 | Loss: 0.00192783
Iteration 9/25 | Loss: 0.00192783
Iteration 10/25 | Loss: 0.00192783
Iteration 11/25 | Loss: 0.00192783
Iteration 12/25 | Loss: 0.00192783
Iteration 13/25 | Loss: 0.00192783
Iteration 14/25 | Loss: 0.00192783
Iteration 15/25 | Loss: 0.00192783
Iteration 16/25 | Loss: 0.00192783
Iteration 17/25 | Loss: 0.00192783
Iteration 18/25 | Loss: 0.00192783
Iteration 19/25 | Loss: 0.00192783
Iteration 20/25 | Loss: 0.00192783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019278251565992832, 0.0019278251565992832, 0.0019278251565992832, 0.0019278251565992832, 0.0019278251565992832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019278251565992832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192782
Iteration 2/1000 | Loss: 0.00008013
Iteration 3/1000 | Loss: 0.00004607
Iteration 4/1000 | Loss: 0.00003376
Iteration 5/1000 | Loss: 0.00002833
Iteration 6/1000 | Loss: 0.00002607
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002364
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002273
Iteration 11/1000 | Loss: 0.00002263
Iteration 12/1000 | Loss: 0.00002234
Iteration 13/1000 | Loss: 0.00002209
Iteration 14/1000 | Loss: 0.00002190
Iteration 15/1000 | Loss: 0.00002184
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002158
Iteration 18/1000 | Loss: 0.00002157
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002149
Iteration 21/1000 | Loss: 0.00002149
Iteration 22/1000 | Loss: 0.00002147
Iteration 23/1000 | Loss: 0.00002147
Iteration 24/1000 | Loss: 0.00002147
Iteration 25/1000 | Loss: 0.00002145
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002144
Iteration 29/1000 | Loss: 0.00002143
Iteration 30/1000 | Loss: 0.00002142
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002140
Iteration 33/1000 | Loss: 0.00002140
Iteration 34/1000 | Loss: 0.00002140
Iteration 35/1000 | Loss: 0.00002140
Iteration 36/1000 | Loss: 0.00002140
Iteration 37/1000 | Loss: 0.00002139
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00002138
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00002138
Iteration 42/1000 | Loss: 0.00002137
Iteration 43/1000 | Loss: 0.00002137
Iteration 44/1000 | Loss: 0.00002137
Iteration 45/1000 | Loss: 0.00002136
Iteration 46/1000 | Loss: 0.00002136
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002136
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002135
Iteration 52/1000 | Loss: 0.00002135
Iteration 53/1000 | Loss: 0.00002135
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002132
Iteration 62/1000 | Loss: 0.00002132
Iteration 63/1000 | Loss: 0.00002131
Iteration 64/1000 | Loss: 0.00002130
Iteration 65/1000 | Loss: 0.00002130
Iteration 66/1000 | Loss: 0.00002129
Iteration 67/1000 | Loss: 0.00002129
Iteration 68/1000 | Loss: 0.00002129
Iteration 69/1000 | Loss: 0.00002129
Iteration 70/1000 | Loss: 0.00002128
Iteration 71/1000 | Loss: 0.00002128
Iteration 72/1000 | Loss: 0.00002128
Iteration 73/1000 | Loss: 0.00002127
Iteration 74/1000 | Loss: 0.00002127
Iteration 75/1000 | Loss: 0.00002127
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002125
Iteration 79/1000 | Loss: 0.00002125
Iteration 80/1000 | Loss: 0.00002125
Iteration 81/1000 | Loss: 0.00002124
Iteration 82/1000 | Loss: 0.00002124
Iteration 83/1000 | Loss: 0.00002124
Iteration 84/1000 | Loss: 0.00002124
Iteration 85/1000 | Loss: 0.00002123
Iteration 86/1000 | Loss: 0.00002123
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002122
Iteration 89/1000 | Loss: 0.00002122
Iteration 90/1000 | Loss: 0.00002122
Iteration 91/1000 | Loss: 0.00002122
Iteration 92/1000 | Loss: 0.00002122
Iteration 93/1000 | Loss: 0.00002122
Iteration 94/1000 | Loss: 0.00002122
Iteration 95/1000 | Loss: 0.00002121
Iteration 96/1000 | Loss: 0.00002121
Iteration 97/1000 | Loss: 0.00002121
Iteration 98/1000 | Loss: 0.00002121
Iteration 99/1000 | Loss: 0.00002121
Iteration 100/1000 | Loss: 0.00002121
Iteration 101/1000 | Loss: 0.00002120
Iteration 102/1000 | Loss: 0.00002120
Iteration 103/1000 | Loss: 0.00002120
Iteration 104/1000 | Loss: 0.00002120
Iteration 105/1000 | Loss: 0.00002120
Iteration 106/1000 | Loss: 0.00002120
Iteration 107/1000 | Loss: 0.00002120
Iteration 108/1000 | Loss: 0.00002120
Iteration 109/1000 | Loss: 0.00002120
Iteration 110/1000 | Loss: 0.00002120
Iteration 111/1000 | Loss: 0.00002119
Iteration 112/1000 | Loss: 0.00002119
Iteration 113/1000 | Loss: 0.00002119
Iteration 114/1000 | Loss: 0.00002119
Iteration 115/1000 | Loss: 0.00002119
Iteration 116/1000 | Loss: 0.00002119
Iteration 117/1000 | Loss: 0.00002119
Iteration 118/1000 | Loss: 0.00002119
Iteration 119/1000 | Loss: 0.00002119
Iteration 120/1000 | Loss: 0.00002119
Iteration 121/1000 | Loss: 0.00002119
Iteration 122/1000 | Loss: 0.00002119
Iteration 123/1000 | Loss: 0.00002119
Iteration 124/1000 | Loss: 0.00002119
Iteration 125/1000 | Loss: 0.00002119
Iteration 126/1000 | Loss: 0.00002119
Iteration 127/1000 | Loss: 0.00002119
Iteration 128/1000 | Loss: 0.00002119
Iteration 129/1000 | Loss: 0.00002119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.1189855033298954e-05, 2.1189855033298954e-05, 2.1189855033298954e-05, 2.1189855033298954e-05, 2.1189855033298954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1189855033298954e-05

Optimization complete. Final v2v error: 3.852080821990967 mm

Highest mean error: 4.623811721801758 mm for frame 93

Lowest mean error: 3.0904057025909424 mm for frame 49

Saving results

Total time: 41.216670513153076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114472
Iteration 2/25 | Loss: 0.00205117
Iteration 3/25 | Loss: 0.00149216
Iteration 4/25 | Loss: 0.00141084
Iteration 5/25 | Loss: 0.00139060
Iteration 6/25 | Loss: 0.00138499
Iteration 7/25 | Loss: 0.00138038
Iteration 8/25 | Loss: 0.00138167
Iteration 9/25 | Loss: 0.00138091
Iteration 10/25 | Loss: 0.00137161
Iteration 11/25 | Loss: 0.00137275
Iteration 12/25 | Loss: 0.00136932
Iteration 13/25 | Loss: 0.00137152
Iteration 14/25 | Loss: 0.00137159
Iteration 15/25 | Loss: 0.00136987
Iteration 16/25 | Loss: 0.00136900
Iteration 17/25 | Loss: 0.00136767
Iteration 18/25 | Loss: 0.00136685
Iteration 19/25 | Loss: 0.00136707
Iteration 20/25 | Loss: 0.00136549
Iteration 21/25 | Loss: 0.00136546
Iteration 22/25 | Loss: 0.00136512
Iteration 23/25 | Loss: 0.00136434
Iteration 24/25 | Loss: 0.00136575
Iteration 25/25 | Loss: 0.00136515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76330817
Iteration 2/25 | Loss: 0.00148994
Iteration 3/25 | Loss: 0.00148981
Iteration 4/25 | Loss: 0.00148981
Iteration 5/25 | Loss: 0.00148981
Iteration 6/25 | Loss: 0.00148981
Iteration 7/25 | Loss: 0.00148981
Iteration 8/25 | Loss: 0.00148981
Iteration 9/25 | Loss: 0.00148981
Iteration 10/25 | Loss: 0.00148981
Iteration 11/25 | Loss: 0.00148981
Iteration 12/25 | Loss: 0.00148981
Iteration 13/25 | Loss: 0.00148981
Iteration 14/25 | Loss: 0.00148981
Iteration 15/25 | Loss: 0.00148981
Iteration 16/25 | Loss: 0.00148981
Iteration 17/25 | Loss: 0.00148981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001489809132181108, 0.001489809132181108, 0.001489809132181108, 0.001489809132181108, 0.001489809132181108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001489809132181108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148981
Iteration 2/1000 | Loss: 0.00011449
Iteration 3/1000 | Loss: 0.00007468
Iteration 4/1000 | Loss: 0.00005376
Iteration 5/1000 | Loss: 0.00005888
Iteration 6/1000 | Loss: 0.00005293
Iteration 7/1000 | Loss: 0.00004632
Iteration 8/1000 | Loss: 0.00004542
Iteration 9/1000 | Loss: 0.00005191
Iteration 10/1000 | Loss: 0.00005044
Iteration 11/1000 | Loss: 0.00004233
Iteration 12/1000 | Loss: 0.00005101
Iteration 13/1000 | Loss: 0.00005189
Iteration 14/1000 | Loss: 0.00005029
Iteration 15/1000 | Loss: 0.00004879
Iteration 16/1000 | Loss: 0.00005184
Iteration 17/1000 | Loss: 0.00006016
Iteration 18/1000 | Loss: 0.00004500
Iteration 19/1000 | Loss: 0.00004240
Iteration 20/1000 | Loss: 0.00004055
Iteration 21/1000 | Loss: 0.00003958
Iteration 22/1000 | Loss: 0.00003914
Iteration 23/1000 | Loss: 0.00003895
Iteration 24/1000 | Loss: 0.00003885
Iteration 25/1000 | Loss: 0.00003870
Iteration 26/1000 | Loss: 0.00003860
Iteration 27/1000 | Loss: 0.00003854
Iteration 28/1000 | Loss: 0.00003847
Iteration 29/1000 | Loss: 0.00003840
Iteration 30/1000 | Loss: 0.00003840
Iteration 31/1000 | Loss: 0.00003839
Iteration 32/1000 | Loss: 0.00003838
Iteration 33/1000 | Loss: 0.00003837
Iteration 34/1000 | Loss: 0.00003828
Iteration 35/1000 | Loss: 0.00003827
Iteration 36/1000 | Loss: 0.00003825
Iteration 37/1000 | Loss: 0.00003822
Iteration 38/1000 | Loss: 0.00003818
Iteration 39/1000 | Loss: 0.00003817
Iteration 40/1000 | Loss: 0.00003816
Iteration 41/1000 | Loss: 0.00003816
Iteration 42/1000 | Loss: 0.00003816
Iteration 43/1000 | Loss: 0.00003815
Iteration 44/1000 | Loss: 0.00003815
Iteration 45/1000 | Loss: 0.00003814
Iteration 46/1000 | Loss: 0.00003813
Iteration 47/1000 | Loss: 0.00003813
Iteration 48/1000 | Loss: 0.00003813
Iteration 49/1000 | Loss: 0.00003813
Iteration 50/1000 | Loss: 0.00003812
Iteration 51/1000 | Loss: 0.00003812
Iteration 52/1000 | Loss: 0.00003812
Iteration 53/1000 | Loss: 0.00003812
Iteration 54/1000 | Loss: 0.00003812
Iteration 55/1000 | Loss: 0.00003812
Iteration 56/1000 | Loss: 0.00003812
Iteration 57/1000 | Loss: 0.00003812
Iteration 58/1000 | Loss: 0.00003812
Iteration 59/1000 | Loss: 0.00003812
Iteration 60/1000 | Loss: 0.00003812
Iteration 61/1000 | Loss: 0.00003811
Iteration 62/1000 | Loss: 0.00003811
Iteration 63/1000 | Loss: 0.00003811
Iteration 64/1000 | Loss: 0.00003811
Iteration 65/1000 | Loss: 0.00003811
Iteration 66/1000 | Loss: 0.00003810
Iteration 67/1000 | Loss: 0.00003810
Iteration 68/1000 | Loss: 0.00003810
Iteration 69/1000 | Loss: 0.00003810
Iteration 70/1000 | Loss: 0.00003810
Iteration 71/1000 | Loss: 0.00003809
Iteration 72/1000 | Loss: 0.00003809
Iteration 73/1000 | Loss: 0.00003809
Iteration 74/1000 | Loss: 0.00003808
Iteration 75/1000 | Loss: 0.00003808
Iteration 76/1000 | Loss: 0.00003808
Iteration 77/1000 | Loss: 0.00003808
Iteration 78/1000 | Loss: 0.00003807
Iteration 79/1000 | Loss: 0.00003807
Iteration 80/1000 | Loss: 0.00003807
Iteration 81/1000 | Loss: 0.00003806
Iteration 82/1000 | Loss: 0.00003806
Iteration 83/1000 | Loss: 0.00003806
Iteration 84/1000 | Loss: 0.00003806
Iteration 85/1000 | Loss: 0.00003806
Iteration 86/1000 | Loss: 0.00003805
Iteration 87/1000 | Loss: 0.00003805
Iteration 88/1000 | Loss: 0.00003805
Iteration 89/1000 | Loss: 0.00003805
Iteration 90/1000 | Loss: 0.00003805
Iteration 91/1000 | Loss: 0.00003805
Iteration 92/1000 | Loss: 0.00003805
Iteration 93/1000 | Loss: 0.00003805
Iteration 94/1000 | Loss: 0.00003805
Iteration 95/1000 | Loss: 0.00003805
Iteration 96/1000 | Loss: 0.00003805
Iteration 97/1000 | Loss: 0.00003804
Iteration 98/1000 | Loss: 0.00003804
Iteration 99/1000 | Loss: 0.00003804
Iteration 100/1000 | Loss: 0.00003804
Iteration 101/1000 | Loss: 0.00003804
Iteration 102/1000 | Loss: 0.00003804
Iteration 103/1000 | Loss: 0.00003804
Iteration 104/1000 | Loss: 0.00003804
Iteration 105/1000 | Loss: 0.00003804
Iteration 106/1000 | Loss: 0.00003804
Iteration 107/1000 | Loss: 0.00003804
Iteration 108/1000 | Loss: 0.00003804
Iteration 109/1000 | Loss: 0.00003803
Iteration 110/1000 | Loss: 0.00003803
Iteration 111/1000 | Loss: 0.00003803
Iteration 112/1000 | Loss: 0.00003803
Iteration 113/1000 | Loss: 0.00003803
Iteration 114/1000 | Loss: 0.00003803
Iteration 115/1000 | Loss: 0.00003803
Iteration 116/1000 | Loss: 0.00003803
Iteration 117/1000 | Loss: 0.00003803
Iteration 118/1000 | Loss: 0.00003803
Iteration 119/1000 | Loss: 0.00003803
Iteration 120/1000 | Loss: 0.00003802
Iteration 121/1000 | Loss: 0.00003802
Iteration 122/1000 | Loss: 0.00003802
Iteration 123/1000 | Loss: 0.00003802
Iteration 124/1000 | Loss: 0.00003802
Iteration 125/1000 | Loss: 0.00003802
Iteration 126/1000 | Loss: 0.00003801
Iteration 127/1000 | Loss: 0.00003801
Iteration 128/1000 | Loss: 0.00003801
Iteration 129/1000 | Loss: 0.00003801
Iteration 130/1000 | Loss: 0.00003801
Iteration 131/1000 | Loss: 0.00003801
Iteration 132/1000 | Loss: 0.00003801
Iteration 133/1000 | Loss: 0.00003801
Iteration 134/1000 | Loss: 0.00003801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.8012931327102706e-05, 3.8012931327102706e-05, 3.8012931327102706e-05, 3.8012931327102706e-05, 3.8012931327102706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8012931327102706e-05

Optimization complete. Final v2v error: 5.122532844543457 mm

Highest mean error: 6.2195000648498535 mm for frame 212

Lowest mean error: 3.969900369644165 mm for frame 67

Saving results

Total time: 105.4201169013977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456647
Iteration 2/25 | Loss: 0.00143313
Iteration 3/25 | Loss: 0.00123176
Iteration 4/25 | Loss: 0.00120710
Iteration 5/25 | Loss: 0.00120256
Iteration 6/25 | Loss: 0.00120101
Iteration 7/25 | Loss: 0.00120099
Iteration 8/25 | Loss: 0.00120099
Iteration 9/25 | Loss: 0.00120099
Iteration 10/25 | Loss: 0.00120099
Iteration 11/25 | Loss: 0.00120099
Iteration 12/25 | Loss: 0.00120099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012009911006316543, 0.0012009911006316543, 0.0012009911006316543, 0.0012009911006316543, 0.0012009911006316543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012009911006316543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26333809
Iteration 2/25 | Loss: 0.00124103
Iteration 3/25 | Loss: 0.00124103
Iteration 4/25 | Loss: 0.00124103
Iteration 5/25 | Loss: 0.00124103
Iteration 6/25 | Loss: 0.00124103
Iteration 7/25 | Loss: 0.00124103
Iteration 8/25 | Loss: 0.00124103
Iteration 9/25 | Loss: 0.00124103
Iteration 10/25 | Loss: 0.00124103
Iteration 11/25 | Loss: 0.00124103
Iteration 12/25 | Loss: 0.00124103
Iteration 13/25 | Loss: 0.00124103
Iteration 14/25 | Loss: 0.00124103
Iteration 15/25 | Loss: 0.00124103
Iteration 16/25 | Loss: 0.00124103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012410287745296955, 0.0012410287745296955, 0.0012410287745296955, 0.0012410287745296955, 0.0012410287745296955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012410287745296955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124103
Iteration 2/1000 | Loss: 0.00004603
Iteration 3/1000 | Loss: 0.00002605
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001593
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001520
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001490
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001487
Iteration 29/1000 | Loss: 0.00001487
Iteration 30/1000 | Loss: 0.00001487
Iteration 31/1000 | Loss: 0.00001487
Iteration 32/1000 | Loss: 0.00001486
Iteration 33/1000 | Loss: 0.00001486
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001482
Iteration 42/1000 | Loss: 0.00001482
Iteration 43/1000 | Loss: 0.00001481
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001479
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001478
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001478
Iteration 58/1000 | Loss: 0.00001477
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001477
Iteration 61/1000 | Loss: 0.00001477
Iteration 62/1000 | Loss: 0.00001477
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001476
Iteration 65/1000 | Loss: 0.00001476
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001476
Iteration 70/1000 | Loss: 0.00001476
Iteration 71/1000 | Loss: 0.00001476
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001475
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001475
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001475
Iteration 87/1000 | Loss: 0.00001475
Iteration 88/1000 | Loss: 0.00001475
Iteration 89/1000 | Loss: 0.00001475
Iteration 90/1000 | Loss: 0.00001475
Iteration 91/1000 | Loss: 0.00001475
Iteration 92/1000 | Loss: 0.00001475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.4752198694623075e-05, 1.4752198694623075e-05, 1.4752198694623075e-05, 1.4752198694623075e-05, 1.4752198694623075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4752198694623075e-05

Optimization complete. Final v2v error: 3.283599853515625 mm

Highest mean error: 3.780412197113037 mm for frame 169

Lowest mean error: 2.6563689708709717 mm for frame 6

Saving results

Total time: 36.79560875892639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964604
Iteration 2/25 | Loss: 0.00314215
Iteration 3/25 | Loss: 0.00229830
Iteration 4/25 | Loss: 0.00187606
Iteration 5/25 | Loss: 0.00170409
Iteration 6/25 | Loss: 0.00168470
Iteration 7/25 | Loss: 0.00164118
Iteration 8/25 | Loss: 0.00160834
Iteration 9/25 | Loss: 0.00156285
Iteration 10/25 | Loss: 0.00156150
Iteration 11/25 | Loss: 0.00152088
Iteration 12/25 | Loss: 0.00149822
Iteration 13/25 | Loss: 0.00148782
Iteration 14/25 | Loss: 0.00149644
Iteration 15/25 | Loss: 0.00148132
Iteration 16/25 | Loss: 0.00148003
Iteration 17/25 | Loss: 0.00148223
Iteration 18/25 | Loss: 0.00147736
Iteration 19/25 | Loss: 0.00147619
Iteration 20/25 | Loss: 0.00147564
Iteration 21/25 | Loss: 0.00147559
Iteration 22/25 | Loss: 0.00147577
Iteration 23/25 | Loss: 0.00147514
Iteration 24/25 | Loss: 0.00147506
Iteration 25/25 | Loss: 0.00147498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.19737005
Iteration 2/25 | Loss: 0.00464904
Iteration 3/25 | Loss: 0.00389325
Iteration 4/25 | Loss: 0.00389325
Iteration 5/25 | Loss: 0.00389325
Iteration 6/25 | Loss: 0.00389325
Iteration 7/25 | Loss: 0.00389324
Iteration 8/25 | Loss: 0.00389324
Iteration 9/25 | Loss: 0.00389324
Iteration 10/25 | Loss: 0.00389324
Iteration 11/25 | Loss: 0.00389324
Iteration 12/25 | Loss: 0.00389324
Iteration 13/25 | Loss: 0.00389324
Iteration 14/25 | Loss: 0.00389324
Iteration 15/25 | Loss: 0.00389324
Iteration 16/25 | Loss: 0.00389324
Iteration 17/25 | Loss: 0.00389324
Iteration 18/25 | Loss: 0.00389324
Iteration 19/25 | Loss: 0.00389324
Iteration 20/25 | Loss: 0.00389324
Iteration 21/25 | Loss: 0.00389324
Iteration 22/25 | Loss: 0.00389324
Iteration 23/25 | Loss: 0.00389324
Iteration 24/25 | Loss: 0.00389324
Iteration 25/25 | Loss: 0.00389324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389324
Iteration 2/1000 | Loss: 0.00076449
Iteration 3/1000 | Loss: 0.00058172
Iteration 4/1000 | Loss: 0.00219833
Iteration 5/1000 | Loss: 0.00543088
Iteration 6/1000 | Loss: 0.00105851
Iteration 7/1000 | Loss: 0.00080423
Iteration 8/1000 | Loss: 0.00228166
Iteration 9/1000 | Loss: 0.00194614
Iteration 10/1000 | Loss: 0.00250474
Iteration 11/1000 | Loss: 0.00186554
Iteration 12/1000 | Loss: 0.00324781
Iteration 13/1000 | Loss: 0.00336742
Iteration 14/1000 | Loss: 0.00106172
Iteration 15/1000 | Loss: 0.00357286
Iteration 16/1000 | Loss: 0.00390551
Iteration 17/1000 | Loss: 0.00073194
Iteration 18/1000 | Loss: 0.00279332
Iteration 19/1000 | Loss: 0.00276777
Iteration 20/1000 | Loss: 0.00415593
Iteration 21/1000 | Loss: 0.00104280
Iteration 22/1000 | Loss: 0.00030457
Iteration 23/1000 | Loss: 0.00380323
Iteration 24/1000 | Loss: 0.00198708
Iteration 25/1000 | Loss: 0.00288808
Iteration 26/1000 | Loss: 0.00084418
Iteration 27/1000 | Loss: 0.00100090
Iteration 28/1000 | Loss: 0.00088756
Iteration 29/1000 | Loss: 0.00019545
Iteration 30/1000 | Loss: 0.00043694
Iteration 31/1000 | Loss: 0.00049737
Iteration 32/1000 | Loss: 0.00016429
Iteration 33/1000 | Loss: 0.00106993
Iteration 34/1000 | Loss: 0.00056839
Iteration 35/1000 | Loss: 0.00037165
Iteration 36/1000 | Loss: 0.00031258
Iteration 37/1000 | Loss: 0.00099904
Iteration 38/1000 | Loss: 0.00118389
Iteration 39/1000 | Loss: 0.00048875
Iteration 40/1000 | Loss: 0.00037113
Iteration 41/1000 | Loss: 0.00045225
Iteration 42/1000 | Loss: 0.00034402
Iteration 43/1000 | Loss: 0.00067248
Iteration 44/1000 | Loss: 0.00144027
Iteration 45/1000 | Loss: 0.00238322
Iteration 46/1000 | Loss: 0.00150080
Iteration 47/1000 | Loss: 0.00266323
Iteration 48/1000 | Loss: 0.00112597
Iteration 49/1000 | Loss: 0.00152105
Iteration 50/1000 | Loss: 0.00391508
Iteration 51/1000 | Loss: 0.00148938
Iteration 52/1000 | Loss: 0.00190650
Iteration 53/1000 | Loss: 0.00180680
Iteration 54/1000 | Loss: 0.00287893
Iteration 55/1000 | Loss: 0.00179828
Iteration 56/1000 | Loss: 0.00245574
Iteration 57/1000 | Loss: 0.00233525
Iteration 58/1000 | Loss: 0.00292202
Iteration 59/1000 | Loss: 0.00058018
Iteration 60/1000 | Loss: 0.00111904
Iteration 61/1000 | Loss: 0.00214767
Iteration 62/1000 | Loss: 0.00179503
Iteration 63/1000 | Loss: 0.00130120
Iteration 64/1000 | Loss: 0.00134912
Iteration 65/1000 | Loss: 0.00168791
Iteration 66/1000 | Loss: 0.00135107
Iteration 67/1000 | Loss: 0.00102639
Iteration 68/1000 | Loss: 0.00099649
Iteration 69/1000 | Loss: 0.00083836
Iteration 70/1000 | Loss: 0.00046720
Iteration 71/1000 | Loss: 0.00064754
Iteration 72/1000 | Loss: 0.00056376
Iteration 73/1000 | Loss: 0.00083042
Iteration 74/1000 | Loss: 0.00099767
Iteration 75/1000 | Loss: 0.00064700
Iteration 76/1000 | Loss: 0.00223942
Iteration 77/1000 | Loss: 0.00087058
Iteration 78/1000 | Loss: 0.00198891
Iteration 79/1000 | Loss: 0.00060480
Iteration 80/1000 | Loss: 0.00062616
Iteration 81/1000 | Loss: 0.00140243
Iteration 82/1000 | Loss: 0.00157914
Iteration 83/1000 | Loss: 0.00075643
Iteration 84/1000 | Loss: 0.00110696
Iteration 85/1000 | Loss: 0.00133183
Iteration 86/1000 | Loss: 0.00096816
Iteration 87/1000 | Loss: 0.00037827
Iteration 88/1000 | Loss: 0.00057164
Iteration 89/1000 | Loss: 0.00076155
Iteration 90/1000 | Loss: 0.00059052
Iteration 91/1000 | Loss: 0.00061318
Iteration 92/1000 | Loss: 0.00147515
Iteration 93/1000 | Loss: 0.00089718
Iteration 94/1000 | Loss: 0.00118744
Iteration 95/1000 | Loss: 0.00196424
Iteration 96/1000 | Loss: 0.00136884
Iteration 97/1000 | Loss: 0.00101566
Iteration 98/1000 | Loss: 0.00147548
Iteration 99/1000 | Loss: 0.00105743
Iteration 100/1000 | Loss: 0.00234872
Iteration 101/1000 | Loss: 0.00187687
Iteration 102/1000 | Loss: 0.00349347
Iteration 103/1000 | Loss: 0.00450316
Iteration 104/1000 | Loss: 0.00243432
Iteration 105/1000 | Loss: 0.00248246
Iteration 106/1000 | Loss: 0.00240393
Iteration 107/1000 | Loss: 0.00222495
Iteration 108/1000 | Loss: 0.00135892
Iteration 109/1000 | Loss: 0.00222871
Iteration 110/1000 | Loss: 0.00059917
Iteration 111/1000 | Loss: 0.00161905
Iteration 112/1000 | Loss: 0.00159999
Iteration 113/1000 | Loss: 0.00139546
Iteration 114/1000 | Loss: 0.00055007
Iteration 115/1000 | Loss: 0.00029003
Iteration 116/1000 | Loss: 0.00093488
Iteration 117/1000 | Loss: 0.00168611
Iteration 118/1000 | Loss: 0.00100122
Iteration 119/1000 | Loss: 0.00070307
Iteration 120/1000 | Loss: 0.00183448
Iteration 121/1000 | Loss: 0.00121462
Iteration 122/1000 | Loss: 0.00138397
Iteration 123/1000 | Loss: 0.00107910
Iteration 124/1000 | Loss: 0.00231144
Iteration 125/1000 | Loss: 0.00033809
Iteration 126/1000 | Loss: 0.00064586
Iteration 127/1000 | Loss: 0.00149303
Iteration 128/1000 | Loss: 0.00294335
Iteration 129/1000 | Loss: 0.00300281
Iteration 130/1000 | Loss: 0.00264427
Iteration 131/1000 | Loss: 0.00093723
Iteration 132/1000 | Loss: 0.00205573
Iteration 133/1000 | Loss: 0.00134598
Iteration 134/1000 | Loss: 0.00223381
Iteration 135/1000 | Loss: 0.00078715
Iteration 136/1000 | Loss: 0.00363626
Iteration 137/1000 | Loss: 0.00129773
Iteration 138/1000 | Loss: 0.00204573
Iteration 139/1000 | Loss: 0.00534712
Iteration 140/1000 | Loss: 0.00099233
Iteration 141/1000 | Loss: 0.00133187
Iteration 142/1000 | Loss: 0.00060315
Iteration 143/1000 | Loss: 0.00122182
Iteration 144/1000 | Loss: 0.00087452
Iteration 145/1000 | Loss: 0.00051925
Iteration 146/1000 | Loss: 0.00047551
Iteration 147/1000 | Loss: 0.00182130
Iteration 148/1000 | Loss: 0.00056910
Iteration 149/1000 | Loss: 0.00098226
Iteration 150/1000 | Loss: 0.00035393
Iteration 151/1000 | Loss: 0.00069147
Iteration 152/1000 | Loss: 0.00036111
Iteration 153/1000 | Loss: 0.00079449
Iteration 154/1000 | Loss: 0.00010985
Iteration 155/1000 | Loss: 0.00075462
Iteration 156/1000 | Loss: 0.00063544
Iteration 157/1000 | Loss: 0.00051493
Iteration 158/1000 | Loss: 0.00182238
Iteration 159/1000 | Loss: 0.00060536
Iteration 160/1000 | Loss: 0.00023642
Iteration 161/1000 | Loss: 0.00049108
Iteration 162/1000 | Loss: 0.00030253
Iteration 163/1000 | Loss: 0.00200677
Iteration 164/1000 | Loss: 0.00071009
Iteration 165/1000 | Loss: 0.00057556
Iteration 166/1000 | Loss: 0.00068659
Iteration 167/1000 | Loss: 0.00120238
Iteration 168/1000 | Loss: 0.00086561
Iteration 169/1000 | Loss: 0.00026129
Iteration 170/1000 | Loss: 0.00017850
Iteration 171/1000 | Loss: 0.00004755
Iteration 172/1000 | Loss: 0.00004046
Iteration 173/1000 | Loss: 0.00032422
Iteration 174/1000 | Loss: 0.00032738
Iteration 175/1000 | Loss: 0.00025336
Iteration 176/1000 | Loss: 0.00052958
Iteration 177/1000 | Loss: 0.00012773
Iteration 178/1000 | Loss: 0.00018412
Iteration 179/1000 | Loss: 0.00031441
Iteration 180/1000 | Loss: 0.00006346
Iteration 181/1000 | Loss: 0.00003553
Iteration 182/1000 | Loss: 0.00041025
Iteration 183/1000 | Loss: 0.00013435
Iteration 184/1000 | Loss: 0.00033930
Iteration 185/1000 | Loss: 0.00007557
Iteration 186/1000 | Loss: 0.00003045
Iteration 187/1000 | Loss: 0.00002879
Iteration 188/1000 | Loss: 0.00002835
Iteration 189/1000 | Loss: 0.00002737
Iteration 190/1000 | Loss: 0.00003278
Iteration 191/1000 | Loss: 0.00006343
Iteration 192/1000 | Loss: 0.00002633
Iteration 193/1000 | Loss: 0.00002588
Iteration 194/1000 | Loss: 0.00002551
Iteration 195/1000 | Loss: 0.00002525
Iteration 196/1000 | Loss: 0.00003481
Iteration 197/1000 | Loss: 0.00002498
Iteration 198/1000 | Loss: 0.00002477
Iteration 199/1000 | Loss: 0.00002474
Iteration 200/1000 | Loss: 0.00048463
Iteration 201/1000 | Loss: 0.00012380
Iteration 202/1000 | Loss: 0.00002559
Iteration 203/1000 | Loss: 0.00045697
Iteration 204/1000 | Loss: 0.00011807
Iteration 205/1000 | Loss: 0.00002589
Iteration 206/1000 | Loss: 0.00002475
Iteration 207/1000 | Loss: 0.00088128
Iteration 208/1000 | Loss: 0.00016279
Iteration 209/1000 | Loss: 0.00013233
Iteration 210/1000 | Loss: 0.00002859
Iteration 211/1000 | Loss: 0.00031540
Iteration 212/1000 | Loss: 0.00010781
Iteration 213/1000 | Loss: 0.00002715
Iteration 214/1000 | Loss: 0.00006029
Iteration 215/1000 | Loss: 0.00027935
Iteration 216/1000 | Loss: 0.00013247
Iteration 217/1000 | Loss: 0.00002523
Iteration 218/1000 | Loss: 0.00002499
Iteration 219/1000 | Loss: 0.00002480
Iteration 220/1000 | Loss: 0.00002469
Iteration 221/1000 | Loss: 0.00002468
Iteration 222/1000 | Loss: 0.00002468
Iteration 223/1000 | Loss: 0.00002467
Iteration 224/1000 | Loss: 0.00002464
Iteration 225/1000 | Loss: 0.00002450
Iteration 226/1000 | Loss: 0.00002446
Iteration 227/1000 | Loss: 0.00002441
Iteration 228/1000 | Loss: 0.00002441
Iteration 229/1000 | Loss: 0.00002440
Iteration 230/1000 | Loss: 0.00002439
Iteration 231/1000 | Loss: 0.00002439
Iteration 232/1000 | Loss: 0.00051295
Iteration 233/1000 | Loss: 0.00056067
Iteration 234/1000 | Loss: 0.00042625
Iteration 235/1000 | Loss: 0.00093325
Iteration 236/1000 | Loss: 0.00037297
Iteration 237/1000 | Loss: 0.00091300
Iteration 238/1000 | Loss: 0.00024781
Iteration 239/1000 | Loss: 0.00029647
Iteration 240/1000 | Loss: 0.00049803
Iteration 241/1000 | Loss: 0.00016676
Iteration 242/1000 | Loss: 0.00023014
Iteration 243/1000 | Loss: 0.00030995
Iteration 244/1000 | Loss: 0.00006443
Iteration 245/1000 | Loss: 0.00010225
Iteration 246/1000 | Loss: 0.00002811
Iteration 247/1000 | Loss: 0.00002714
Iteration 248/1000 | Loss: 0.00003899
Iteration 249/1000 | Loss: 0.00002798
Iteration 250/1000 | Loss: 0.00002757
Iteration 251/1000 | Loss: 0.00002577
Iteration 252/1000 | Loss: 0.00002550
Iteration 253/1000 | Loss: 0.00002547
Iteration 254/1000 | Loss: 0.00002542
Iteration 255/1000 | Loss: 0.00002541
Iteration 256/1000 | Loss: 0.00002524
Iteration 257/1000 | Loss: 0.00002514
Iteration 258/1000 | Loss: 0.00002498
Iteration 259/1000 | Loss: 0.00002497
Iteration 260/1000 | Loss: 0.00002489
Iteration 261/1000 | Loss: 0.00002487
Iteration 262/1000 | Loss: 0.00002486
Iteration 263/1000 | Loss: 0.00002480
Iteration 264/1000 | Loss: 0.00002479
Iteration 265/1000 | Loss: 0.00002473
Iteration 266/1000 | Loss: 0.00002472
Iteration 267/1000 | Loss: 0.00002471
Iteration 268/1000 | Loss: 0.00002467
Iteration 269/1000 | Loss: 0.00002457
Iteration 270/1000 | Loss: 0.00002453
Iteration 271/1000 | Loss: 0.00027009
Iteration 272/1000 | Loss: 0.00055617
Iteration 273/1000 | Loss: 0.00007690
Iteration 274/1000 | Loss: 0.00056007
Iteration 275/1000 | Loss: 0.00011992
Iteration 276/1000 | Loss: 0.00002543
Iteration 277/1000 | Loss: 0.00026196
Iteration 278/1000 | Loss: 0.00003865
Iteration 279/1000 | Loss: 0.00038167
Iteration 280/1000 | Loss: 0.00022336
Iteration 281/1000 | Loss: 0.00030907
Iteration 282/1000 | Loss: 0.00003280
Iteration 283/1000 | Loss: 0.00002618
Iteration 284/1000 | Loss: 0.00002329
Iteration 285/1000 | Loss: 0.00002917
Iteration 286/1000 | Loss: 0.00002203
Iteration 287/1000 | Loss: 0.00003466
Iteration 288/1000 | Loss: 0.00002053
Iteration 289/1000 | Loss: 0.00002562
Iteration 290/1000 | Loss: 0.00002010
Iteration 291/1000 | Loss: 0.00002384
Iteration 292/1000 | Loss: 0.00004463
Iteration 293/1000 | Loss: 0.00002193
Iteration 294/1000 | Loss: 0.00002296
Iteration 295/1000 | Loss: 0.00001989
Iteration 296/1000 | Loss: 0.00001978
Iteration 297/1000 | Loss: 0.00001978
Iteration 298/1000 | Loss: 0.00001977
Iteration 299/1000 | Loss: 0.00001977
Iteration 300/1000 | Loss: 0.00001977
Iteration 301/1000 | Loss: 0.00001977
Iteration 302/1000 | Loss: 0.00001977
Iteration 303/1000 | Loss: 0.00001976
Iteration 304/1000 | Loss: 0.00001976
Iteration 305/1000 | Loss: 0.00001976
Iteration 306/1000 | Loss: 0.00001975
Iteration 307/1000 | Loss: 0.00001975
Iteration 308/1000 | Loss: 0.00001974
Iteration 309/1000 | Loss: 0.00001974
Iteration 310/1000 | Loss: 0.00001974
Iteration 311/1000 | Loss: 0.00001974
Iteration 312/1000 | Loss: 0.00001974
Iteration 313/1000 | Loss: 0.00001973
Iteration 314/1000 | Loss: 0.00001973
Iteration 315/1000 | Loss: 0.00001972
Iteration 316/1000 | Loss: 0.00001972
Iteration 317/1000 | Loss: 0.00001971
Iteration 318/1000 | Loss: 0.00001971
Iteration 319/1000 | Loss: 0.00001971
Iteration 320/1000 | Loss: 0.00001970
Iteration 321/1000 | Loss: 0.00001970
Iteration 322/1000 | Loss: 0.00001970
Iteration 323/1000 | Loss: 0.00001970
Iteration 324/1000 | Loss: 0.00001970
Iteration 325/1000 | Loss: 0.00001970
Iteration 326/1000 | Loss: 0.00001970
Iteration 327/1000 | Loss: 0.00001969
Iteration 328/1000 | Loss: 0.00001969
Iteration 329/1000 | Loss: 0.00001969
Iteration 330/1000 | Loss: 0.00001968
Iteration 331/1000 | Loss: 0.00001968
Iteration 332/1000 | Loss: 0.00001968
Iteration 333/1000 | Loss: 0.00001967
Iteration 334/1000 | Loss: 0.00001967
Iteration 335/1000 | Loss: 0.00001967
Iteration 336/1000 | Loss: 0.00001966
Iteration 337/1000 | Loss: 0.00001966
Iteration 338/1000 | Loss: 0.00001965
Iteration 339/1000 | Loss: 0.00001965
Iteration 340/1000 | Loss: 0.00001965
Iteration 341/1000 | Loss: 0.00001965
Iteration 342/1000 | Loss: 0.00002476
Iteration 343/1000 | Loss: 0.00001964
Iteration 344/1000 | Loss: 0.00001964
Iteration 345/1000 | Loss: 0.00001964
Iteration 346/1000 | Loss: 0.00001963
Iteration 347/1000 | Loss: 0.00001963
Iteration 348/1000 | Loss: 0.00001963
Iteration 349/1000 | Loss: 0.00001963
Iteration 350/1000 | Loss: 0.00001963
Iteration 351/1000 | Loss: 0.00001963
Iteration 352/1000 | Loss: 0.00001963
Iteration 353/1000 | Loss: 0.00001963
Iteration 354/1000 | Loss: 0.00001962
Iteration 355/1000 | Loss: 0.00001961
Iteration 356/1000 | Loss: 0.00002150
Iteration 357/1000 | Loss: 0.00002005
Iteration 358/1000 | Loss: 0.00001959
Iteration 359/1000 | Loss: 0.00001957
Iteration 360/1000 | Loss: 0.00001956
Iteration 361/1000 | Loss: 0.00001956
Iteration 362/1000 | Loss: 0.00001955
Iteration 363/1000 | Loss: 0.00001955
Iteration 364/1000 | Loss: 0.00001954
Iteration 365/1000 | Loss: 0.00001954
Iteration 366/1000 | Loss: 0.00001953
Iteration 367/1000 | Loss: 0.00001953
Iteration 368/1000 | Loss: 0.00001953
Iteration 369/1000 | Loss: 0.00001953
Iteration 370/1000 | Loss: 0.00001953
Iteration 371/1000 | Loss: 0.00001953
Iteration 372/1000 | Loss: 0.00001953
Iteration 373/1000 | Loss: 0.00001953
Iteration 374/1000 | Loss: 0.00001953
Iteration 375/1000 | Loss: 0.00001953
Iteration 376/1000 | Loss: 0.00001953
Iteration 377/1000 | Loss: 0.00001953
Iteration 378/1000 | Loss: 0.00001953
Iteration 379/1000 | Loss: 0.00001953
Iteration 380/1000 | Loss: 0.00001953
Iteration 381/1000 | Loss: 0.00001953
Iteration 382/1000 | Loss: 0.00001953
Iteration 383/1000 | Loss: 0.00001953
Iteration 384/1000 | Loss: 0.00001953
Iteration 385/1000 | Loss: 0.00001953
Iteration 386/1000 | Loss: 0.00001953
Iteration 387/1000 | Loss: 0.00001952
Iteration 388/1000 | Loss: 0.00001952
Iteration 389/1000 | Loss: 0.00001952
Iteration 390/1000 | Loss: 0.00001952
Iteration 391/1000 | Loss: 0.00001952
Iteration 392/1000 | Loss: 0.00001952
Iteration 393/1000 | Loss: 0.00001952
Iteration 394/1000 | Loss: 0.00001952
Iteration 395/1000 | Loss: 0.00001952
Iteration 396/1000 | Loss: 0.00001952
Iteration 397/1000 | Loss: 0.00001952
Iteration 398/1000 | Loss: 0.00001952
Iteration 399/1000 | Loss: 0.00001952
Iteration 400/1000 | Loss: 0.00001952
Iteration 401/1000 | Loss: 0.00001952
Iteration 402/1000 | Loss: 0.00001952
Iteration 403/1000 | Loss: 0.00001952
Iteration 404/1000 | Loss: 0.00001952
Iteration 405/1000 | Loss: 0.00001952
Iteration 406/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [1.952453749254346e-05, 1.952453749254346e-05, 1.952453749254346e-05, 1.952453749254346e-05, 1.952453749254346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.952453749254346e-05

Optimization complete. Final v2v error: 3.6299362182617188 mm

Highest mean error: 9.149391174316406 mm for frame 7

Lowest mean error: 2.8352086544036865 mm for frame 11

Saving results

Total time: 460.80381774902344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419278
Iteration 2/25 | Loss: 0.00131122
Iteration 3/25 | Loss: 0.00116586
Iteration 4/25 | Loss: 0.00115225
Iteration 5/25 | Loss: 0.00115087
Iteration 6/25 | Loss: 0.00115087
Iteration 7/25 | Loss: 0.00115087
Iteration 8/25 | Loss: 0.00115087
Iteration 9/25 | Loss: 0.00115087
Iteration 10/25 | Loss: 0.00115087
Iteration 11/25 | Loss: 0.00115087
Iteration 12/25 | Loss: 0.00115087
Iteration 13/25 | Loss: 0.00115087
Iteration 14/25 | Loss: 0.00115087
Iteration 15/25 | Loss: 0.00115087
Iteration 16/25 | Loss: 0.00115087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011508744210004807, 0.0011508744210004807, 0.0011508744210004807, 0.0011508744210004807, 0.0011508744210004807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011508744210004807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28041792
Iteration 2/25 | Loss: 0.00112656
Iteration 3/25 | Loss: 0.00112656
Iteration 4/25 | Loss: 0.00112655
Iteration 5/25 | Loss: 0.00112655
Iteration 6/25 | Loss: 0.00112655
Iteration 7/25 | Loss: 0.00112655
Iteration 8/25 | Loss: 0.00112655
Iteration 9/25 | Loss: 0.00112655
Iteration 10/25 | Loss: 0.00112655
Iteration 11/25 | Loss: 0.00112655
Iteration 12/25 | Loss: 0.00112655
Iteration 13/25 | Loss: 0.00112655
Iteration 14/25 | Loss: 0.00112655
Iteration 15/25 | Loss: 0.00112655
Iteration 16/25 | Loss: 0.00112655
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011265528155490756, 0.0011265528155490756, 0.0011265528155490756, 0.0011265528155490756, 0.0011265528155490756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265528155490756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112655
Iteration 2/1000 | Loss: 0.00003113
Iteration 3/1000 | Loss: 0.00001964
Iteration 4/1000 | Loss: 0.00001681
Iteration 5/1000 | Loss: 0.00001567
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001437
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001377
Iteration 10/1000 | Loss: 0.00001358
Iteration 11/1000 | Loss: 0.00001354
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001344
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001340
Iteration 20/1000 | Loss: 0.00001340
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001333
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001327
Iteration 48/1000 | Loss: 0.00001327
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001326
Iteration 52/1000 | Loss: 0.00001326
Iteration 53/1000 | Loss: 0.00001326
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001325
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001324
Iteration 63/1000 | Loss: 0.00001324
Iteration 64/1000 | Loss: 0.00001324
Iteration 65/1000 | Loss: 0.00001324
Iteration 66/1000 | Loss: 0.00001324
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001323
Iteration 70/1000 | Loss: 0.00001323
Iteration 71/1000 | Loss: 0.00001323
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001322
Iteration 74/1000 | Loss: 0.00001322
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001321
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001319
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001318
Iteration 90/1000 | Loss: 0.00001318
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001318
Iteration 93/1000 | Loss: 0.00001318
Iteration 94/1000 | Loss: 0.00001318
Iteration 95/1000 | Loss: 0.00001318
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001318
Iteration 99/1000 | Loss: 0.00001318
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001314
Iteration 133/1000 | Loss: 0.00001314
Iteration 134/1000 | Loss: 0.00001314
Iteration 135/1000 | Loss: 0.00001314
Iteration 136/1000 | Loss: 0.00001314
Iteration 137/1000 | Loss: 0.00001314
Iteration 138/1000 | Loss: 0.00001314
Iteration 139/1000 | Loss: 0.00001314
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001313
Iteration 149/1000 | Loss: 0.00001313
Iteration 150/1000 | Loss: 0.00001313
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001313
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001312
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00001312
Iteration 169/1000 | Loss: 0.00001312
Iteration 170/1000 | Loss: 0.00001312
Iteration 171/1000 | Loss: 0.00001312
Iteration 172/1000 | Loss: 0.00001312
Iteration 173/1000 | Loss: 0.00001312
Iteration 174/1000 | Loss: 0.00001312
Iteration 175/1000 | Loss: 0.00001312
Iteration 176/1000 | Loss: 0.00001312
Iteration 177/1000 | Loss: 0.00001312
Iteration 178/1000 | Loss: 0.00001312
Iteration 179/1000 | Loss: 0.00001312
Iteration 180/1000 | Loss: 0.00001312
Iteration 181/1000 | Loss: 0.00001312
Iteration 182/1000 | Loss: 0.00001312
Iteration 183/1000 | Loss: 0.00001312
Iteration 184/1000 | Loss: 0.00001312
Iteration 185/1000 | Loss: 0.00001312
Iteration 186/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.3123565622663591e-05, 1.3123565622663591e-05, 1.3123565622663591e-05, 1.3123565622663591e-05, 1.3123565622663591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3123565622663591e-05

Optimization complete. Final v2v error: 3.057339668273926 mm

Highest mean error: 4.1511664390563965 mm for frame 56

Lowest mean error: 2.7437517642974854 mm for frame 157

Saving results

Total time: 32.60330080986023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869503
Iteration 2/25 | Loss: 0.00157667
Iteration 3/25 | Loss: 0.00129775
Iteration 4/25 | Loss: 0.00125615
Iteration 5/25 | Loss: 0.00125621
Iteration 6/25 | Loss: 0.00124861
Iteration 7/25 | Loss: 0.00124651
Iteration 8/25 | Loss: 0.00124584
Iteration 9/25 | Loss: 0.00123842
Iteration 10/25 | Loss: 0.00123379
Iteration 11/25 | Loss: 0.00123350
Iteration 12/25 | Loss: 0.00123086
Iteration 13/25 | Loss: 0.00122463
Iteration 14/25 | Loss: 0.00122339
Iteration 15/25 | Loss: 0.00122306
Iteration 16/25 | Loss: 0.00122289
Iteration 17/25 | Loss: 0.00122282
Iteration 18/25 | Loss: 0.00122282
Iteration 19/25 | Loss: 0.00122281
Iteration 20/25 | Loss: 0.00122281
Iteration 21/25 | Loss: 0.00122281
Iteration 22/25 | Loss: 0.00122277
Iteration 23/25 | Loss: 0.00122277
Iteration 24/25 | Loss: 0.00122277
Iteration 25/25 | Loss: 0.00122277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31727695
Iteration 2/25 | Loss: 0.00125021
Iteration 3/25 | Loss: 0.00125020
Iteration 4/25 | Loss: 0.00125020
Iteration 5/25 | Loss: 0.00125020
Iteration 6/25 | Loss: 0.00125020
Iteration 7/25 | Loss: 0.00125020
Iteration 8/25 | Loss: 0.00125020
Iteration 9/25 | Loss: 0.00125020
Iteration 10/25 | Loss: 0.00125020
Iteration 11/25 | Loss: 0.00125020
Iteration 12/25 | Loss: 0.00125020
Iteration 13/25 | Loss: 0.00125020
Iteration 14/25 | Loss: 0.00125020
Iteration 15/25 | Loss: 0.00125020
Iteration 16/25 | Loss: 0.00125020
Iteration 17/25 | Loss: 0.00125020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001250196946784854, 0.001250196946784854, 0.001250196946784854, 0.001250196946784854, 0.001250196946784854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001250196946784854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125020
Iteration 2/1000 | Loss: 0.00006626
Iteration 3/1000 | Loss: 0.00004699
Iteration 4/1000 | Loss: 0.00004229
Iteration 5/1000 | Loss: 0.00003831
Iteration 6/1000 | Loss: 0.00003600
Iteration 7/1000 | Loss: 0.00003432
Iteration 8/1000 | Loss: 0.00038720
Iteration 9/1000 | Loss: 0.00015138
Iteration 10/1000 | Loss: 0.00034014
Iteration 11/1000 | Loss: 0.00040723
Iteration 12/1000 | Loss: 0.00028124
Iteration 13/1000 | Loss: 0.00056743
Iteration 14/1000 | Loss: 0.00019060
Iteration 15/1000 | Loss: 0.00003282
Iteration 16/1000 | Loss: 0.00094056
Iteration 17/1000 | Loss: 0.00025214
Iteration 18/1000 | Loss: 0.00003286
Iteration 19/1000 | Loss: 0.00053441
Iteration 20/1000 | Loss: 0.00022834
Iteration 21/1000 | Loss: 0.00004476
Iteration 22/1000 | Loss: 0.00007760
Iteration 23/1000 | Loss: 0.00003859
Iteration 24/1000 | Loss: 0.00032138
Iteration 25/1000 | Loss: 0.00014113
Iteration 26/1000 | Loss: 0.00030324
Iteration 27/1000 | Loss: 0.00014687
Iteration 28/1000 | Loss: 0.00034760
Iteration 29/1000 | Loss: 0.00003722
Iteration 30/1000 | Loss: 0.00003359
Iteration 31/1000 | Loss: 0.00003182
Iteration 32/1000 | Loss: 0.00046179
Iteration 33/1000 | Loss: 0.00060372
Iteration 34/1000 | Loss: 0.00080687
Iteration 35/1000 | Loss: 0.00016072
Iteration 36/1000 | Loss: 0.00045914
Iteration 37/1000 | Loss: 0.00008484
Iteration 38/1000 | Loss: 0.00003222
Iteration 39/1000 | Loss: 0.00002871
Iteration 40/1000 | Loss: 0.00002607
Iteration 41/1000 | Loss: 0.00012550
Iteration 42/1000 | Loss: 0.00003159
Iteration 43/1000 | Loss: 0.00002468
Iteration 44/1000 | Loss: 0.00002338
Iteration 45/1000 | Loss: 0.00002211
Iteration 46/1000 | Loss: 0.00002155
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00002066
Iteration 50/1000 | Loss: 0.00002051
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002026
Iteration 54/1000 | Loss: 0.00002018
Iteration 55/1000 | Loss: 0.00002009
Iteration 56/1000 | Loss: 0.00002009
Iteration 57/1000 | Loss: 0.00002008
Iteration 58/1000 | Loss: 0.00002007
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002004
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002000
Iteration 67/1000 | Loss: 0.00002000
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00001999
Iteration 72/1000 | Loss: 0.00001998
Iteration 73/1000 | Loss: 0.00001998
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00001998
Iteration 76/1000 | Loss: 0.00001998
Iteration 77/1000 | Loss: 0.00001998
Iteration 78/1000 | Loss: 0.00001998
Iteration 79/1000 | Loss: 0.00001998
Iteration 80/1000 | Loss: 0.00001997
Iteration 81/1000 | Loss: 0.00001997
Iteration 82/1000 | Loss: 0.00001996
Iteration 83/1000 | Loss: 0.00001996
Iteration 84/1000 | Loss: 0.00001995
Iteration 85/1000 | Loss: 0.00001995
Iteration 86/1000 | Loss: 0.00001995
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001995
Iteration 89/1000 | Loss: 0.00001995
Iteration 90/1000 | Loss: 0.00001994
Iteration 91/1000 | Loss: 0.00001993
Iteration 92/1000 | Loss: 0.00001993
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001983
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001982
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001973
Iteration 103/1000 | Loss: 0.00001972
Iteration 104/1000 | Loss: 0.00018055
Iteration 105/1000 | Loss: 0.00002334
Iteration 106/1000 | Loss: 0.00012199
Iteration 107/1000 | Loss: 0.00002677
Iteration 108/1000 | Loss: 0.00001985
Iteration 109/1000 | Loss: 0.00001958
Iteration 110/1000 | Loss: 0.00001957
Iteration 111/1000 | Loss: 0.00016755
Iteration 112/1000 | Loss: 0.00037682
Iteration 113/1000 | Loss: 0.00004710
Iteration 114/1000 | Loss: 0.00003445
Iteration 115/1000 | Loss: 0.00003133
Iteration 116/1000 | Loss: 0.00002914
Iteration 117/1000 | Loss: 0.00002741
Iteration 118/1000 | Loss: 0.00002605
Iteration 119/1000 | Loss: 0.00002494
Iteration 120/1000 | Loss: 0.00002415
Iteration 121/1000 | Loss: 0.00002368
Iteration 122/1000 | Loss: 0.00049057
Iteration 123/1000 | Loss: 0.00003713
Iteration 124/1000 | Loss: 0.00002676
Iteration 125/1000 | Loss: 0.00002384
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002132
Iteration 128/1000 | Loss: 0.00002071
Iteration 129/1000 | Loss: 0.00002013
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001987
Iteration 132/1000 | Loss: 0.00001985
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001972
Iteration 138/1000 | Loss: 0.00001971
Iteration 139/1000 | Loss: 0.00001971
Iteration 140/1000 | Loss: 0.00001970
Iteration 141/1000 | Loss: 0.00001969
Iteration 142/1000 | Loss: 0.00001968
Iteration 143/1000 | Loss: 0.00001968
Iteration 144/1000 | Loss: 0.00001967
Iteration 145/1000 | Loss: 0.00001966
Iteration 146/1000 | Loss: 0.00001966
Iteration 147/1000 | Loss: 0.00001965
Iteration 148/1000 | Loss: 0.00001965
Iteration 149/1000 | Loss: 0.00001964
Iteration 150/1000 | Loss: 0.00001950
Iteration 151/1000 | Loss: 0.00001931
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001869
Iteration 154/1000 | Loss: 0.00023663
Iteration 155/1000 | Loss: 0.00009279
Iteration 156/1000 | Loss: 0.00020506
Iteration 157/1000 | Loss: 0.00002247
Iteration 158/1000 | Loss: 0.00001890
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001743
Iteration 163/1000 | Loss: 0.00001732
Iteration 164/1000 | Loss: 0.00001724
Iteration 165/1000 | Loss: 0.00001724
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001721
Iteration 172/1000 | Loss: 0.00001720
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001720
Iteration 183/1000 | Loss: 0.00001720
Iteration 184/1000 | Loss: 0.00001720
Iteration 185/1000 | Loss: 0.00001719
Iteration 186/1000 | Loss: 0.00001719
Iteration 187/1000 | Loss: 0.00001719
Iteration 188/1000 | Loss: 0.00001719
Iteration 189/1000 | Loss: 0.00001719
Iteration 190/1000 | Loss: 0.00001719
Iteration 191/1000 | Loss: 0.00001719
Iteration 192/1000 | Loss: 0.00001719
Iteration 193/1000 | Loss: 0.00001718
Iteration 194/1000 | Loss: 0.00001718
Iteration 195/1000 | Loss: 0.00001718
Iteration 196/1000 | Loss: 0.00001718
Iteration 197/1000 | Loss: 0.00001718
Iteration 198/1000 | Loss: 0.00001718
Iteration 199/1000 | Loss: 0.00001718
Iteration 200/1000 | Loss: 0.00001718
Iteration 201/1000 | Loss: 0.00001718
Iteration 202/1000 | Loss: 0.00001718
Iteration 203/1000 | Loss: 0.00001718
Iteration 204/1000 | Loss: 0.00001718
Iteration 205/1000 | Loss: 0.00001718
Iteration 206/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.717675149848219e-05, 1.717675149848219e-05, 1.717675149848219e-05, 1.717675149848219e-05, 1.717675149848219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.717675149848219e-05

Optimization complete. Final v2v error: 3.511229991912842 mm

Highest mean error: 5.154017925262451 mm for frame 170

Lowest mean error: 2.9437177181243896 mm for frame 142

Saving results

Total time: 192.32727098464966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037760
Iteration 2/25 | Loss: 0.00208487
Iteration 3/25 | Loss: 0.00217888
Iteration 4/25 | Loss: 0.00149326
Iteration 5/25 | Loss: 0.00147240
Iteration 6/25 | Loss: 0.00137080
Iteration 7/25 | Loss: 0.00135740
Iteration 8/25 | Loss: 0.00137628
Iteration 9/25 | Loss: 0.00138886
Iteration 10/25 | Loss: 0.00132453
Iteration 11/25 | Loss: 0.00131102
Iteration 12/25 | Loss: 0.00130778
Iteration 13/25 | Loss: 0.00130448
Iteration 14/25 | Loss: 0.00133132
Iteration 15/25 | Loss: 0.00132882
Iteration 16/25 | Loss: 0.00132380
Iteration 17/25 | Loss: 0.00132157
Iteration 18/25 | Loss: 0.00131789
Iteration 19/25 | Loss: 0.00131868
Iteration 20/25 | Loss: 0.00132368
Iteration 21/25 | Loss: 0.00132428
Iteration 22/25 | Loss: 0.00133291
Iteration 23/25 | Loss: 0.00132109
Iteration 24/25 | Loss: 0.00132975
Iteration 25/25 | Loss: 0.00131140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97789806
Iteration 2/25 | Loss: 0.00101824
Iteration 3/25 | Loss: 0.00101824
Iteration 4/25 | Loss: 0.00101824
Iteration 5/25 | Loss: 0.00101824
Iteration 6/25 | Loss: 0.00101824
Iteration 7/25 | Loss: 0.00101824
Iteration 8/25 | Loss: 0.00101824
Iteration 9/25 | Loss: 0.00101824
Iteration 10/25 | Loss: 0.00101824
Iteration 11/25 | Loss: 0.00101824
Iteration 12/25 | Loss: 0.00101824
Iteration 13/25 | Loss: 0.00101824
Iteration 14/25 | Loss: 0.00101824
Iteration 15/25 | Loss: 0.00101824
Iteration 16/25 | Loss: 0.00101824
Iteration 17/25 | Loss: 0.00101824
Iteration 18/25 | Loss: 0.00101824
Iteration 19/25 | Loss: 0.00101824
Iteration 20/25 | Loss: 0.00101824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010182360420003533, 0.0010182360420003533, 0.0010182360420003533, 0.0010182360420003533, 0.0010182360420003533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010182360420003533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101824
Iteration 2/1000 | Loss: 0.00084990
Iteration 3/1000 | Loss: 0.00105646
Iteration 4/1000 | Loss: 0.00079126
Iteration 5/1000 | Loss: 0.00111777
Iteration 6/1000 | Loss: 0.00098552
Iteration 7/1000 | Loss: 0.00127840
Iteration 8/1000 | Loss: 0.00015746
Iteration 9/1000 | Loss: 0.00138892
Iteration 10/1000 | Loss: 0.00101580
Iteration 11/1000 | Loss: 0.00100888
Iteration 12/1000 | Loss: 0.00019509
Iteration 13/1000 | Loss: 0.00013111
Iteration 14/1000 | Loss: 0.00042293
Iteration 15/1000 | Loss: 0.00009052
Iteration 16/1000 | Loss: 0.00007200
Iteration 17/1000 | Loss: 0.00088282
Iteration 18/1000 | Loss: 0.00109177
Iteration 19/1000 | Loss: 0.00048353
Iteration 20/1000 | Loss: 0.00011396
Iteration 21/1000 | Loss: 0.00057187
Iteration 22/1000 | Loss: 0.00062951
Iteration 23/1000 | Loss: 0.00085998
Iteration 24/1000 | Loss: 0.00159099
Iteration 25/1000 | Loss: 0.00140334
Iteration 26/1000 | Loss: 0.00110044
Iteration 27/1000 | Loss: 0.00008957
Iteration 28/1000 | Loss: 0.00006426
Iteration 29/1000 | Loss: 0.00005767
Iteration 30/1000 | Loss: 0.00005112
Iteration 31/1000 | Loss: 0.00004898
Iteration 32/1000 | Loss: 0.00185267
Iteration 33/1000 | Loss: 0.00085945
Iteration 34/1000 | Loss: 0.00127542
Iteration 35/1000 | Loss: 0.00093705
Iteration 36/1000 | Loss: 0.00139698
Iteration 37/1000 | Loss: 0.00209544
Iteration 38/1000 | Loss: 0.00013840
Iteration 39/1000 | Loss: 0.00198119
Iteration 40/1000 | Loss: 0.00009568
Iteration 41/1000 | Loss: 0.00005679
Iteration 42/1000 | Loss: 0.00004661
Iteration 43/1000 | Loss: 0.00004393
Iteration 44/1000 | Loss: 0.00004270
Iteration 45/1000 | Loss: 0.00004149
Iteration 46/1000 | Loss: 0.00004086
Iteration 47/1000 | Loss: 0.00004036
Iteration 48/1000 | Loss: 0.00003994
Iteration 49/1000 | Loss: 0.00003959
Iteration 50/1000 | Loss: 0.00003943
Iteration 51/1000 | Loss: 0.00003918
Iteration 52/1000 | Loss: 0.00003888
Iteration 53/1000 | Loss: 0.00003866
Iteration 54/1000 | Loss: 0.00003852
Iteration 55/1000 | Loss: 0.00003850
Iteration 56/1000 | Loss: 0.00003845
Iteration 57/1000 | Loss: 0.00003841
Iteration 58/1000 | Loss: 0.00003838
Iteration 59/1000 | Loss: 0.00003838
Iteration 60/1000 | Loss: 0.00003831
Iteration 61/1000 | Loss: 0.00003830
Iteration 62/1000 | Loss: 0.00003829
Iteration 63/1000 | Loss: 0.00003828
Iteration 64/1000 | Loss: 0.00003828
Iteration 65/1000 | Loss: 0.00003828
Iteration 66/1000 | Loss: 0.00003828
Iteration 67/1000 | Loss: 0.00003828
Iteration 68/1000 | Loss: 0.00003827
Iteration 69/1000 | Loss: 0.00003827
Iteration 70/1000 | Loss: 0.00003827
Iteration 71/1000 | Loss: 0.00003827
Iteration 72/1000 | Loss: 0.00003827
Iteration 73/1000 | Loss: 0.00003827
Iteration 74/1000 | Loss: 0.00003827
Iteration 75/1000 | Loss: 0.00003827
Iteration 76/1000 | Loss: 0.00003826
Iteration 77/1000 | Loss: 0.00003826
Iteration 78/1000 | Loss: 0.00003826
Iteration 79/1000 | Loss: 0.00003825
Iteration 80/1000 | Loss: 0.00003825
Iteration 81/1000 | Loss: 0.00003825
Iteration 82/1000 | Loss: 0.00003824
Iteration 83/1000 | Loss: 0.00003824
Iteration 84/1000 | Loss: 0.00003824
Iteration 85/1000 | Loss: 0.00003824
Iteration 86/1000 | Loss: 0.00003824
Iteration 87/1000 | Loss: 0.00003824
Iteration 88/1000 | Loss: 0.00003824
Iteration 89/1000 | Loss: 0.00003824
Iteration 90/1000 | Loss: 0.00003824
Iteration 91/1000 | Loss: 0.00003824
Iteration 92/1000 | Loss: 0.00003824
Iteration 93/1000 | Loss: 0.00003824
Iteration 94/1000 | Loss: 0.00003824
Iteration 95/1000 | Loss: 0.00003824
Iteration 96/1000 | Loss: 0.00003823
Iteration 97/1000 | Loss: 0.00003823
Iteration 98/1000 | Loss: 0.00003822
Iteration 99/1000 | Loss: 0.00003821
Iteration 100/1000 | Loss: 0.00003821
Iteration 101/1000 | Loss: 0.00003821
Iteration 102/1000 | Loss: 0.00003819
Iteration 103/1000 | Loss: 0.00003819
Iteration 104/1000 | Loss: 0.00003817
Iteration 105/1000 | Loss: 0.00003813
Iteration 106/1000 | Loss: 0.00003812
Iteration 107/1000 | Loss: 0.00003812
Iteration 108/1000 | Loss: 0.00003812
Iteration 109/1000 | Loss: 0.00003812
Iteration 110/1000 | Loss: 0.00003812
Iteration 111/1000 | Loss: 0.00003811
Iteration 112/1000 | Loss: 0.00003807
Iteration 113/1000 | Loss: 0.00003805
Iteration 114/1000 | Loss: 0.00003804
Iteration 115/1000 | Loss: 0.00003804
Iteration 116/1000 | Loss: 0.00003804
Iteration 117/1000 | Loss: 0.00003803
Iteration 118/1000 | Loss: 0.00003803
Iteration 119/1000 | Loss: 0.00003802
Iteration 120/1000 | Loss: 0.00003802
Iteration 121/1000 | Loss: 0.00003801
Iteration 122/1000 | Loss: 0.00003801
Iteration 123/1000 | Loss: 0.00003801
Iteration 124/1000 | Loss: 0.00003800
Iteration 125/1000 | Loss: 0.00003800
Iteration 126/1000 | Loss: 0.00003800
Iteration 127/1000 | Loss: 0.00003800
Iteration 128/1000 | Loss: 0.00003800
Iteration 129/1000 | Loss: 0.00003800
Iteration 130/1000 | Loss: 0.00003800
Iteration 131/1000 | Loss: 0.00003800
Iteration 132/1000 | Loss: 0.00003800
Iteration 133/1000 | Loss: 0.00003798
Iteration 134/1000 | Loss: 0.00003798
Iteration 135/1000 | Loss: 0.00003798
Iteration 136/1000 | Loss: 0.00003797
Iteration 137/1000 | Loss: 0.00003797
Iteration 138/1000 | Loss: 0.00003797
Iteration 139/1000 | Loss: 0.00003797
Iteration 140/1000 | Loss: 0.00003797
Iteration 141/1000 | Loss: 0.00003796
Iteration 142/1000 | Loss: 0.00003796
Iteration 143/1000 | Loss: 0.00003796
Iteration 144/1000 | Loss: 0.00003796
Iteration 145/1000 | Loss: 0.00003796
Iteration 146/1000 | Loss: 0.00003796
Iteration 147/1000 | Loss: 0.00003796
Iteration 148/1000 | Loss: 0.00003796
Iteration 149/1000 | Loss: 0.00003796
Iteration 150/1000 | Loss: 0.00003796
Iteration 151/1000 | Loss: 0.00003795
Iteration 152/1000 | Loss: 0.00003795
Iteration 153/1000 | Loss: 0.00003795
Iteration 154/1000 | Loss: 0.00003795
Iteration 155/1000 | Loss: 0.00003794
Iteration 156/1000 | Loss: 0.00003794
Iteration 157/1000 | Loss: 0.00003794
Iteration 158/1000 | Loss: 0.00003794
Iteration 159/1000 | Loss: 0.00003794
Iteration 160/1000 | Loss: 0.00003794
Iteration 161/1000 | Loss: 0.00003794
Iteration 162/1000 | Loss: 0.00003794
Iteration 163/1000 | Loss: 0.00003794
Iteration 164/1000 | Loss: 0.00003794
Iteration 165/1000 | Loss: 0.00003794
Iteration 166/1000 | Loss: 0.00003794
Iteration 167/1000 | Loss: 0.00003793
Iteration 168/1000 | Loss: 0.00003793
Iteration 169/1000 | Loss: 0.00003793
Iteration 170/1000 | Loss: 0.00003792
Iteration 171/1000 | Loss: 0.00003792
Iteration 172/1000 | Loss: 0.00003791
Iteration 173/1000 | Loss: 0.00003791
Iteration 174/1000 | Loss: 0.00003791
Iteration 175/1000 | Loss: 0.00003790
Iteration 176/1000 | Loss: 0.00003790
Iteration 177/1000 | Loss: 0.00003790
Iteration 178/1000 | Loss: 0.00003790
Iteration 179/1000 | Loss: 0.00003790
Iteration 180/1000 | Loss: 0.00003790
Iteration 181/1000 | Loss: 0.00003790
Iteration 182/1000 | Loss: 0.00003789
Iteration 183/1000 | Loss: 0.00003789
Iteration 184/1000 | Loss: 0.00003789
Iteration 185/1000 | Loss: 0.00003788
Iteration 186/1000 | Loss: 0.00003788
Iteration 187/1000 | Loss: 0.00003788
Iteration 188/1000 | Loss: 0.00003788
Iteration 189/1000 | Loss: 0.00003788
Iteration 190/1000 | Loss: 0.00003788
Iteration 191/1000 | Loss: 0.00003787
Iteration 192/1000 | Loss: 0.00003787
Iteration 193/1000 | Loss: 0.00003787
Iteration 194/1000 | Loss: 0.00003787
Iteration 195/1000 | Loss: 0.00003787
Iteration 196/1000 | Loss: 0.00003787
Iteration 197/1000 | Loss: 0.00003787
Iteration 198/1000 | Loss: 0.00003787
Iteration 199/1000 | Loss: 0.00003786
Iteration 200/1000 | Loss: 0.00003786
Iteration 201/1000 | Loss: 0.00003786
Iteration 202/1000 | Loss: 0.00003786
Iteration 203/1000 | Loss: 0.00003786
Iteration 204/1000 | Loss: 0.00003786
Iteration 205/1000 | Loss: 0.00003786
Iteration 206/1000 | Loss: 0.00003786
Iteration 207/1000 | Loss: 0.00003786
Iteration 208/1000 | Loss: 0.00003786
Iteration 209/1000 | Loss: 0.00003785
Iteration 210/1000 | Loss: 0.00003785
Iteration 211/1000 | Loss: 0.00003785
Iteration 212/1000 | Loss: 0.00003785
Iteration 213/1000 | Loss: 0.00003785
Iteration 214/1000 | Loss: 0.00003784
Iteration 215/1000 | Loss: 0.00003784
Iteration 216/1000 | Loss: 0.00003784
Iteration 217/1000 | Loss: 0.00003784
Iteration 218/1000 | Loss: 0.00003783
Iteration 219/1000 | Loss: 0.00003783
Iteration 220/1000 | Loss: 0.00003783
Iteration 221/1000 | Loss: 0.00003782
Iteration 222/1000 | Loss: 0.00003782
Iteration 223/1000 | Loss: 0.00003782
Iteration 224/1000 | Loss: 0.00003782
Iteration 225/1000 | Loss: 0.00003782
Iteration 226/1000 | Loss: 0.00003782
Iteration 227/1000 | Loss: 0.00003781
Iteration 228/1000 | Loss: 0.00003781
Iteration 229/1000 | Loss: 0.00003781
Iteration 230/1000 | Loss: 0.00003781
Iteration 231/1000 | Loss: 0.00003781
Iteration 232/1000 | Loss: 0.00003781
Iteration 233/1000 | Loss: 0.00003780
Iteration 234/1000 | Loss: 0.00003780
Iteration 235/1000 | Loss: 0.00003780
Iteration 236/1000 | Loss: 0.00003780
Iteration 237/1000 | Loss: 0.00003780
Iteration 238/1000 | Loss: 0.00003780
Iteration 239/1000 | Loss: 0.00003780
Iteration 240/1000 | Loss: 0.00003780
Iteration 241/1000 | Loss: 0.00003780
Iteration 242/1000 | Loss: 0.00003780
Iteration 243/1000 | Loss: 0.00003780
Iteration 244/1000 | Loss: 0.00003780
Iteration 245/1000 | Loss: 0.00003779
Iteration 246/1000 | Loss: 0.00003779
Iteration 247/1000 | Loss: 0.00003779
Iteration 248/1000 | Loss: 0.00003779
Iteration 249/1000 | Loss: 0.00003779
Iteration 250/1000 | Loss: 0.00003779
Iteration 251/1000 | Loss: 0.00003779
Iteration 252/1000 | Loss: 0.00003779
Iteration 253/1000 | Loss: 0.00003779
Iteration 254/1000 | Loss: 0.00003779
Iteration 255/1000 | Loss: 0.00003779
Iteration 256/1000 | Loss: 0.00003778
Iteration 257/1000 | Loss: 0.00003778
Iteration 258/1000 | Loss: 0.00003778
Iteration 259/1000 | Loss: 0.00003778
Iteration 260/1000 | Loss: 0.00003778
Iteration 261/1000 | Loss: 0.00003778
Iteration 262/1000 | Loss: 0.00003778
Iteration 263/1000 | Loss: 0.00003778
Iteration 264/1000 | Loss: 0.00003778
Iteration 265/1000 | Loss: 0.00003778
Iteration 266/1000 | Loss: 0.00003778
Iteration 267/1000 | Loss: 0.00003778
Iteration 268/1000 | Loss: 0.00003778
Iteration 269/1000 | Loss: 0.00003778
Iteration 270/1000 | Loss: 0.00003778
Iteration 271/1000 | Loss: 0.00003777
Iteration 272/1000 | Loss: 0.00003777
Iteration 273/1000 | Loss: 0.00003777
Iteration 274/1000 | Loss: 0.00003777
Iteration 275/1000 | Loss: 0.00003777
Iteration 276/1000 | Loss: 0.00003777
Iteration 277/1000 | Loss: 0.00003777
Iteration 278/1000 | Loss: 0.00003777
Iteration 279/1000 | Loss: 0.00003777
Iteration 280/1000 | Loss: 0.00003777
Iteration 281/1000 | Loss: 0.00003777
Iteration 282/1000 | Loss: 0.00003777
Iteration 283/1000 | Loss: 0.00003777
Iteration 284/1000 | Loss: 0.00003777
Iteration 285/1000 | Loss: 0.00003777
Iteration 286/1000 | Loss: 0.00003776
Iteration 287/1000 | Loss: 0.00003776
Iteration 288/1000 | Loss: 0.00003776
Iteration 289/1000 | Loss: 0.00003776
Iteration 290/1000 | Loss: 0.00003776
Iteration 291/1000 | Loss: 0.00003776
Iteration 292/1000 | Loss: 0.00003776
Iteration 293/1000 | Loss: 0.00003776
Iteration 294/1000 | Loss: 0.00003776
Iteration 295/1000 | Loss: 0.00003776
Iteration 296/1000 | Loss: 0.00003776
Iteration 297/1000 | Loss: 0.00003776
Iteration 298/1000 | Loss: 0.00003776
Iteration 299/1000 | Loss: 0.00003776
Iteration 300/1000 | Loss: 0.00003776
Iteration 301/1000 | Loss: 0.00003775
Iteration 302/1000 | Loss: 0.00003775
Iteration 303/1000 | Loss: 0.00003775
Iteration 304/1000 | Loss: 0.00003775
Iteration 305/1000 | Loss: 0.00003775
Iteration 306/1000 | Loss: 0.00003775
Iteration 307/1000 | Loss: 0.00003775
Iteration 308/1000 | Loss: 0.00003775
Iteration 309/1000 | Loss: 0.00003775
Iteration 310/1000 | Loss: 0.00003775
Iteration 311/1000 | Loss: 0.00003775
Iteration 312/1000 | Loss: 0.00003775
Iteration 313/1000 | Loss: 0.00003775
Iteration 314/1000 | Loss: 0.00003775
Iteration 315/1000 | Loss: 0.00003775
Iteration 316/1000 | Loss: 0.00003775
Iteration 317/1000 | Loss: 0.00003775
Iteration 318/1000 | Loss: 0.00003775
Iteration 319/1000 | Loss: 0.00003775
Iteration 320/1000 | Loss: 0.00003775
Iteration 321/1000 | Loss: 0.00003775
Iteration 322/1000 | Loss: 0.00003775
Iteration 323/1000 | Loss: 0.00003775
Iteration 324/1000 | Loss: 0.00003775
Iteration 325/1000 | Loss: 0.00003775
Iteration 326/1000 | Loss: 0.00003775
Iteration 327/1000 | Loss: 0.00003775
Iteration 328/1000 | Loss: 0.00003775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [3.774852302740328e-05, 3.774852302740328e-05, 3.774852302740328e-05, 3.774852302740328e-05, 3.774852302740328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.774852302740328e-05

Optimization complete. Final v2v error: 4.996877670288086 mm

Highest mean error: 6.217580318450928 mm for frame 166

Lowest mean error: 3.689868688583374 mm for frame 7

Saving results

Total time: 158.32696628570557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109005
Iteration 2/25 | Loss: 0.01109005
Iteration 3/25 | Loss: 0.01109005
Iteration 4/25 | Loss: 0.00310479
Iteration 5/25 | Loss: 0.00215784
Iteration 6/25 | Loss: 0.00157620
Iteration 7/25 | Loss: 0.00150524
Iteration 8/25 | Loss: 0.00145298
Iteration 9/25 | Loss: 0.00142128
Iteration 10/25 | Loss: 0.00139569
Iteration 11/25 | Loss: 0.00138270
Iteration 12/25 | Loss: 0.00136809
Iteration 13/25 | Loss: 0.00137141
Iteration 14/25 | Loss: 0.00136623
Iteration 15/25 | Loss: 0.00136068
Iteration 16/25 | Loss: 0.00134957
Iteration 17/25 | Loss: 0.00134385
Iteration 18/25 | Loss: 0.00133775
Iteration 19/25 | Loss: 0.00133502
Iteration 20/25 | Loss: 0.00133095
Iteration 21/25 | Loss: 0.00133011
Iteration 22/25 | Loss: 0.00132991
Iteration 23/25 | Loss: 0.00132985
Iteration 24/25 | Loss: 0.00132985
Iteration 25/25 | Loss: 0.00132984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23335445
Iteration 2/25 | Loss: 0.00518961
Iteration 3/25 | Loss: 0.00252396
Iteration 4/25 | Loss: 0.00252396
Iteration 5/25 | Loss: 0.00252396
Iteration 6/25 | Loss: 0.00252396
Iteration 7/25 | Loss: 0.00252395
Iteration 8/25 | Loss: 0.00252395
Iteration 9/25 | Loss: 0.00252395
Iteration 10/25 | Loss: 0.00252395
Iteration 11/25 | Loss: 0.00252395
Iteration 12/25 | Loss: 0.00252395
Iteration 13/25 | Loss: 0.00252395
Iteration 14/25 | Loss: 0.00252395
Iteration 15/25 | Loss: 0.00252395
Iteration 16/25 | Loss: 0.00252395
Iteration 17/25 | Loss: 0.00252395
Iteration 18/25 | Loss: 0.00252395
Iteration 19/25 | Loss: 0.00252395
Iteration 20/25 | Loss: 0.00252395
Iteration 21/25 | Loss: 0.00252395
Iteration 22/25 | Loss: 0.00252395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0025239516980946064, 0.0025239516980946064, 0.0025239516980946064, 0.0025239516980946064, 0.0025239516980946064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025239516980946064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252395
Iteration 2/1000 | Loss: 0.00032207
Iteration 3/1000 | Loss: 0.00040780
Iteration 4/1000 | Loss: 0.00064904
Iteration 5/1000 | Loss: 0.00037639
Iteration 6/1000 | Loss: 0.00046293
Iteration 7/1000 | Loss: 0.00038760
Iteration 8/1000 | Loss: 0.00133803
Iteration 9/1000 | Loss: 0.00034078
Iteration 10/1000 | Loss: 0.00013746
Iteration 11/1000 | Loss: 0.00130300
Iteration 12/1000 | Loss: 0.00015795
Iteration 13/1000 | Loss: 0.00030415
Iteration 14/1000 | Loss: 0.00036798
Iteration 15/1000 | Loss: 0.00019778
Iteration 16/1000 | Loss: 0.00010007
Iteration 17/1000 | Loss: 0.00036554
Iteration 18/1000 | Loss: 0.00014950
Iteration 19/1000 | Loss: 0.00008678
Iteration 20/1000 | Loss: 0.00042432
Iteration 21/1000 | Loss: 0.00033220
Iteration 22/1000 | Loss: 0.00009332
Iteration 23/1000 | Loss: 0.00008642
Iteration 24/1000 | Loss: 0.00023050
Iteration 25/1000 | Loss: 0.00008812
Iteration 26/1000 | Loss: 0.00007999
Iteration 27/1000 | Loss: 0.00088833
Iteration 28/1000 | Loss: 0.00743654
Iteration 29/1000 | Loss: 0.00303411
Iteration 30/1000 | Loss: 0.00261313
Iteration 31/1000 | Loss: 0.00429405
Iteration 32/1000 | Loss: 0.00329233
Iteration 33/1000 | Loss: 0.00080223
Iteration 34/1000 | Loss: 0.00123883
Iteration 35/1000 | Loss: 0.00186312
Iteration 36/1000 | Loss: 0.00074410
Iteration 37/1000 | Loss: 0.00042116
Iteration 38/1000 | Loss: 0.00072669
Iteration 39/1000 | Loss: 0.00090405
Iteration 40/1000 | Loss: 0.00085294
Iteration 41/1000 | Loss: 0.00065815
Iteration 42/1000 | Loss: 0.00059530
Iteration 43/1000 | Loss: 0.00104734
Iteration 44/1000 | Loss: 0.00052744
Iteration 45/1000 | Loss: 0.00072958
Iteration 46/1000 | Loss: 0.00031924
Iteration 47/1000 | Loss: 0.00229958
Iteration 48/1000 | Loss: 0.00128184
Iteration 49/1000 | Loss: 0.00063314
Iteration 50/1000 | Loss: 0.00051107
Iteration 51/1000 | Loss: 0.00146274
Iteration 52/1000 | Loss: 0.00069333
Iteration 53/1000 | Loss: 0.00048636
Iteration 54/1000 | Loss: 0.00053368
Iteration 55/1000 | Loss: 0.00072508
Iteration 56/1000 | Loss: 0.00067480
Iteration 57/1000 | Loss: 0.00035289
Iteration 58/1000 | Loss: 0.00025127
Iteration 59/1000 | Loss: 0.00069694
Iteration 60/1000 | Loss: 0.00030465
Iteration 61/1000 | Loss: 0.00070622
Iteration 62/1000 | Loss: 0.00106673
Iteration 63/1000 | Loss: 0.00133390
Iteration 64/1000 | Loss: 0.00083852
Iteration 65/1000 | Loss: 0.00071837
Iteration 66/1000 | Loss: 0.00091638
Iteration 67/1000 | Loss: 0.00124728
Iteration 68/1000 | Loss: 0.00077100
Iteration 69/1000 | Loss: 0.00060974
Iteration 70/1000 | Loss: 0.00059024
Iteration 71/1000 | Loss: 0.00066436
Iteration 72/1000 | Loss: 0.00062681
Iteration 73/1000 | Loss: 0.00102476
Iteration 74/1000 | Loss: 0.00075760
Iteration 75/1000 | Loss: 0.00032497
Iteration 76/1000 | Loss: 0.00007269
Iteration 77/1000 | Loss: 0.00066861
Iteration 78/1000 | Loss: 0.00033384
Iteration 79/1000 | Loss: 0.00114573
Iteration 80/1000 | Loss: 0.00052003
Iteration 81/1000 | Loss: 0.00042208
Iteration 82/1000 | Loss: 0.00086971
Iteration 83/1000 | Loss: 0.00082413
Iteration 84/1000 | Loss: 0.00047047
Iteration 85/1000 | Loss: 0.00098842
Iteration 86/1000 | Loss: 0.00030826
Iteration 87/1000 | Loss: 0.00072449
Iteration 88/1000 | Loss: 0.00106282
Iteration 89/1000 | Loss: 0.00077045
Iteration 90/1000 | Loss: 0.00039981
Iteration 91/1000 | Loss: 0.00037629
Iteration 92/1000 | Loss: 0.00129603
Iteration 93/1000 | Loss: 0.00078657
Iteration 94/1000 | Loss: 0.00089135
Iteration 95/1000 | Loss: 0.00112423
Iteration 96/1000 | Loss: 0.00092813
Iteration 97/1000 | Loss: 0.00073162
Iteration 98/1000 | Loss: 0.00033766
Iteration 99/1000 | Loss: 0.00072597
Iteration 100/1000 | Loss: 0.00050029
Iteration 101/1000 | Loss: 0.00063662
Iteration 102/1000 | Loss: 0.00098938
Iteration 103/1000 | Loss: 0.00055516
Iteration 104/1000 | Loss: 0.00089369
Iteration 105/1000 | Loss: 0.00154519
Iteration 106/1000 | Loss: 0.00120470
Iteration 107/1000 | Loss: 0.00081068
Iteration 108/1000 | Loss: 0.00030039
Iteration 109/1000 | Loss: 0.00026554
Iteration 110/1000 | Loss: 0.00079727
Iteration 111/1000 | Loss: 0.00055489
Iteration 112/1000 | Loss: 0.00071541
Iteration 113/1000 | Loss: 0.00078949
Iteration 114/1000 | Loss: 0.00066600
Iteration 115/1000 | Loss: 0.00030392
Iteration 116/1000 | Loss: 0.00048734
Iteration 117/1000 | Loss: 0.00052304
Iteration 118/1000 | Loss: 0.00125834
Iteration 119/1000 | Loss: 0.00076740
Iteration 120/1000 | Loss: 0.00068675
Iteration 121/1000 | Loss: 0.00066711
Iteration 122/1000 | Loss: 0.00083409
Iteration 123/1000 | Loss: 0.00077263
Iteration 124/1000 | Loss: 0.00074475
Iteration 125/1000 | Loss: 0.00061018
Iteration 126/1000 | Loss: 0.00035846
Iteration 127/1000 | Loss: 0.00109664
Iteration 128/1000 | Loss: 0.00067243
Iteration 129/1000 | Loss: 0.00038970
Iteration 130/1000 | Loss: 0.00042588
Iteration 131/1000 | Loss: 0.00006137
Iteration 132/1000 | Loss: 0.00030486
Iteration 133/1000 | Loss: 0.00024764
Iteration 134/1000 | Loss: 0.00030769
Iteration 135/1000 | Loss: 0.00046293
Iteration 136/1000 | Loss: 0.00020237
Iteration 137/1000 | Loss: 0.00005421
Iteration 138/1000 | Loss: 0.00024940
Iteration 139/1000 | Loss: 0.00034156
Iteration 140/1000 | Loss: 0.00059549
Iteration 141/1000 | Loss: 0.00062105
Iteration 142/1000 | Loss: 0.00052411
Iteration 143/1000 | Loss: 0.00033668
Iteration 144/1000 | Loss: 0.00101383
Iteration 145/1000 | Loss: 0.00088546
Iteration 146/1000 | Loss: 0.00066342
Iteration 147/1000 | Loss: 0.00134695
Iteration 148/1000 | Loss: 0.00118948
Iteration 149/1000 | Loss: 0.00078549
Iteration 150/1000 | Loss: 0.00032987
Iteration 151/1000 | Loss: 0.00029171
Iteration 152/1000 | Loss: 0.00025555
Iteration 153/1000 | Loss: 0.00033384
Iteration 154/1000 | Loss: 0.00060274
Iteration 155/1000 | Loss: 0.00061918
Iteration 156/1000 | Loss: 0.00053341
Iteration 157/1000 | Loss: 0.00019420
Iteration 158/1000 | Loss: 0.00056694
Iteration 159/1000 | Loss: 0.00037522
Iteration 160/1000 | Loss: 0.00029096
Iteration 161/1000 | Loss: 0.00026602
Iteration 162/1000 | Loss: 0.00018460
Iteration 163/1000 | Loss: 0.00040704
Iteration 164/1000 | Loss: 0.00053340
Iteration 165/1000 | Loss: 0.00043085
Iteration 166/1000 | Loss: 0.00029934
Iteration 167/1000 | Loss: 0.00039349
Iteration 168/1000 | Loss: 0.00077989
Iteration 169/1000 | Loss: 0.00072326
Iteration 170/1000 | Loss: 0.00040968
Iteration 171/1000 | Loss: 0.00051025
Iteration 172/1000 | Loss: 0.00104593
Iteration 173/1000 | Loss: 0.00075237
Iteration 174/1000 | Loss: 0.00056620
Iteration 175/1000 | Loss: 0.00024672
Iteration 176/1000 | Loss: 0.00036130
Iteration 177/1000 | Loss: 0.00036755
Iteration 178/1000 | Loss: 0.00054911
Iteration 179/1000 | Loss: 0.00010060
Iteration 180/1000 | Loss: 0.00051811
Iteration 181/1000 | Loss: 0.00031006
Iteration 182/1000 | Loss: 0.00027625
Iteration 183/1000 | Loss: 0.00004625
Iteration 184/1000 | Loss: 0.00011048
Iteration 185/1000 | Loss: 0.00004709
Iteration 186/1000 | Loss: 0.00005266
Iteration 187/1000 | Loss: 0.00046018
Iteration 188/1000 | Loss: 0.00013442
Iteration 189/1000 | Loss: 0.00008468
Iteration 190/1000 | Loss: 0.00004075
Iteration 191/1000 | Loss: 0.00020765
Iteration 192/1000 | Loss: 0.00008419
Iteration 193/1000 | Loss: 0.00008974
Iteration 194/1000 | Loss: 0.00034019
Iteration 195/1000 | Loss: 0.00011948
Iteration 196/1000 | Loss: 0.00046474
Iteration 197/1000 | Loss: 0.00008943
Iteration 198/1000 | Loss: 0.00003690
Iteration 199/1000 | Loss: 0.00003287
Iteration 200/1000 | Loss: 0.00017338
Iteration 201/1000 | Loss: 0.00069259
Iteration 202/1000 | Loss: 0.00046299
Iteration 203/1000 | Loss: 0.00067420
Iteration 204/1000 | Loss: 0.00065295
Iteration 205/1000 | Loss: 0.00018518
Iteration 206/1000 | Loss: 0.00003942
Iteration 207/1000 | Loss: 0.00035416
Iteration 208/1000 | Loss: 0.00051196
Iteration 209/1000 | Loss: 0.00008433
Iteration 210/1000 | Loss: 0.00023717
Iteration 211/1000 | Loss: 0.00022004
Iteration 212/1000 | Loss: 0.00069030
Iteration 213/1000 | Loss: 0.00076919
Iteration 214/1000 | Loss: 0.00035168
Iteration 215/1000 | Loss: 0.00061932
Iteration 216/1000 | Loss: 0.00024538
Iteration 217/1000 | Loss: 0.00013455
Iteration 218/1000 | Loss: 0.00024976
Iteration 219/1000 | Loss: 0.00037159
Iteration 220/1000 | Loss: 0.00006992
Iteration 221/1000 | Loss: 0.00021459
Iteration 222/1000 | Loss: 0.00012465
Iteration 223/1000 | Loss: 0.00012194
Iteration 224/1000 | Loss: 0.00026104
Iteration 225/1000 | Loss: 0.00033247
Iteration 226/1000 | Loss: 0.00029479
Iteration 227/1000 | Loss: 0.00032949
Iteration 228/1000 | Loss: 0.00016281
Iteration 229/1000 | Loss: 0.00035177
Iteration 230/1000 | Loss: 0.00021497
Iteration 231/1000 | Loss: 0.00011968
Iteration 232/1000 | Loss: 0.00013790
Iteration 233/1000 | Loss: 0.00148171
Iteration 234/1000 | Loss: 0.00060050
Iteration 235/1000 | Loss: 0.00051565
Iteration 236/1000 | Loss: 0.00004778
Iteration 237/1000 | Loss: 0.00003561
Iteration 238/1000 | Loss: 0.00032116
Iteration 239/1000 | Loss: 0.00067964
Iteration 240/1000 | Loss: 0.00004970
Iteration 241/1000 | Loss: 0.00003162
Iteration 242/1000 | Loss: 0.00002628
Iteration 243/1000 | Loss: 0.00002407
Iteration 244/1000 | Loss: 0.00002205
Iteration 245/1000 | Loss: 0.00002100
Iteration 246/1000 | Loss: 0.00012757
Iteration 247/1000 | Loss: 0.00002640
Iteration 248/1000 | Loss: 0.00002364
Iteration 249/1000 | Loss: 0.00002136
Iteration 250/1000 | Loss: 0.00022388
Iteration 251/1000 | Loss: 0.00004030
Iteration 252/1000 | Loss: 0.00041133
Iteration 253/1000 | Loss: 0.00049170
Iteration 254/1000 | Loss: 0.00003384
Iteration 255/1000 | Loss: 0.00002261
Iteration 256/1000 | Loss: 0.00002077
Iteration 257/1000 | Loss: 0.00002010
Iteration 258/1000 | Loss: 0.00001976
Iteration 259/1000 | Loss: 0.00001945
Iteration 260/1000 | Loss: 0.00001904
Iteration 261/1000 | Loss: 0.00001880
Iteration 262/1000 | Loss: 0.00001878
Iteration 263/1000 | Loss: 0.00001872
Iteration 264/1000 | Loss: 0.00001856
Iteration 265/1000 | Loss: 0.00001848
Iteration 266/1000 | Loss: 0.00001824
Iteration 267/1000 | Loss: 0.00001794
Iteration 268/1000 | Loss: 0.00001785
Iteration 269/1000 | Loss: 0.00001776
Iteration 270/1000 | Loss: 0.00001767
Iteration 271/1000 | Loss: 0.00001763
Iteration 272/1000 | Loss: 0.00001762
Iteration 273/1000 | Loss: 0.00001760
Iteration 274/1000 | Loss: 0.00001760
Iteration 275/1000 | Loss: 0.00001759
Iteration 276/1000 | Loss: 0.00001758
Iteration 277/1000 | Loss: 0.00001758
Iteration 278/1000 | Loss: 0.00001758
Iteration 279/1000 | Loss: 0.00001757
Iteration 280/1000 | Loss: 0.00001757
Iteration 281/1000 | Loss: 0.00001756
Iteration 282/1000 | Loss: 0.00001756
Iteration 283/1000 | Loss: 0.00001756
Iteration 284/1000 | Loss: 0.00001768
Iteration 285/1000 | Loss: 0.00001767
Iteration 286/1000 | Loss: 0.00001734
Iteration 287/1000 | Loss: 0.00001724
Iteration 288/1000 | Loss: 0.00001723
Iteration 289/1000 | Loss: 0.00001721
Iteration 290/1000 | Loss: 0.00001720
Iteration 291/1000 | Loss: 0.00001719
Iteration 292/1000 | Loss: 0.00001719
Iteration 293/1000 | Loss: 0.00001719
Iteration 294/1000 | Loss: 0.00001715
Iteration 295/1000 | Loss: 0.00001712
Iteration 296/1000 | Loss: 0.00001711
Iteration 297/1000 | Loss: 0.00001711
Iteration 298/1000 | Loss: 0.00001710
Iteration 299/1000 | Loss: 0.00001708
Iteration 300/1000 | Loss: 0.00001707
Iteration 301/1000 | Loss: 0.00001706
Iteration 302/1000 | Loss: 0.00001705
Iteration 303/1000 | Loss: 0.00001705
Iteration 304/1000 | Loss: 0.00001705
Iteration 305/1000 | Loss: 0.00001705
Iteration 306/1000 | Loss: 0.00001705
Iteration 307/1000 | Loss: 0.00001705
Iteration 308/1000 | Loss: 0.00001704
Iteration 309/1000 | Loss: 0.00001704
Iteration 310/1000 | Loss: 0.00001703
Iteration 311/1000 | Loss: 0.00001702
Iteration 312/1000 | Loss: 0.00001702
Iteration 313/1000 | Loss: 0.00001702
Iteration 314/1000 | Loss: 0.00001701
Iteration 315/1000 | Loss: 0.00001701
Iteration 316/1000 | Loss: 0.00001700
Iteration 317/1000 | Loss: 0.00001700
Iteration 318/1000 | Loss: 0.00001700
Iteration 319/1000 | Loss: 0.00001699
Iteration 320/1000 | Loss: 0.00001699
Iteration 321/1000 | Loss: 0.00001698
Iteration 322/1000 | Loss: 0.00001698
Iteration 323/1000 | Loss: 0.00001698
Iteration 324/1000 | Loss: 0.00001698
Iteration 325/1000 | Loss: 0.00001698
Iteration 326/1000 | Loss: 0.00001697
Iteration 327/1000 | Loss: 0.00001697
Iteration 328/1000 | Loss: 0.00001697
Iteration 329/1000 | Loss: 0.00001697
Iteration 330/1000 | Loss: 0.00001697
Iteration 331/1000 | Loss: 0.00001697
Iteration 332/1000 | Loss: 0.00001697
Iteration 333/1000 | Loss: 0.00001697
Iteration 334/1000 | Loss: 0.00001697
Iteration 335/1000 | Loss: 0.00001697
Iteration 336/1000 | Loss: 0.00001697
Iteration 337/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 337. Stopping optimization.
Last 5 losses: [1.6971063814708032e-05, 1.6971063814708032e-05, 1.6971063814708032e-05, 1.6971063814708032e-05, 1.6971063814708032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6971063814708032e-05

Optimization complete. Final v2v error: 3.2686567306518555 mm

Highest mean error: 17.104808807373047 mm for frame 37

Lowest mean error: 2.7920851707458496 mm for frame 226

Saving results

Total time: 474.64337635040283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851246
Iteration 2/25 | Loss: 0.00171328
Iteration 3/25 | Loss: 0.00139352
Iteration 4/25 | Loss: 0.00136495
Iteration 5/25 | Loss: 0.00135592
Iteration 6/25 | Loss: 0.00135350
Iteration 7/25 | Loss: 0.00135302
Iteration 8/25 | Loss: 0.00135302
Iteration 9/25 | Loss: 0.00135302
Iteration 10/25 | Loss: 0.00135302
Iteration 11/25 | Loss: 0.00135302
Iteration 12/25 | Loss: 0.00135302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001353019499219954, 0.001353019499219954, 0.001353019499219954, 0.001353019499219954, 0.001353019499219954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001353019499219954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12930298
Iteration 2/25 | Loss: 0.00168559
Iteration 3/25 | Loss: 0.00168559
Iteration 4/25 | Loss: 0.00168559
Iteration 5/25 | Loss: 0.00168559
Iteration 6/25 | Loss: 0.00168559
Iteration 7/25 | Loss: 0.00168559
Iteration 8/25 | Loss: 0.00168559
Iteration 9/25 | Loss: 0.00168559
Iteration 10/25 | Loss: 0.00168559
Iteration 11/25 | Loss: 0.00168559
Iteration 12/25 | Loss: 0.00168559
Iteration 13/25 | Loss: 0.00168559
Iteration 14/25 | Loss: 0.00168559
Iteration 15/25 | Loss: 0.00168559
Iteration 16/25 | Loss: 0.00168559
Iteration 17/25 | Loss: 0.00168559
Iteration 18/25 | Loss: 0.00168559
Iteration 19/25 | Loss: 0.00168559
Iteration 20/25 | Loss: 0.00168559
Iteration 21/25 | Loss: 0.00168559
Iteration 22/25 | Loss: 0.00168559
Iteration 23/25 | Loss: 0.00168559
Iteration 24/25 | Loss: 0.00168559
Iteration 25/25 | Loss: 0.00168559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168559
Iteration 2/1000 | Loss: 0.00009492
Iteration 3/1000 | Loss: 0.00005878
Iteration 4/1000 | Loss: 0.00004649
Iteration 5/1000 | Loss: 0.00004278
Iteration 6/1000 | Loss: 0.00003961
Iteration 7/1000 | Loss: 0.00003747
Iteration 8/1000 | Loss: 0.00003634
Iteration 9/1000 | Loss: 0.00003555
Iteration 10/1000 | Loss: 0.00003492
Iteration 11/1000 | Loss: 0.00003408
Iteration 12/1000 | Loss: 0.00003343
Iteration 13/1000 | Loss: 0.00003292
Iteration 14/1000 | Loss: 0.00003258
Iteration 15/1000 | Loss: 0.00003230
Iteration 16/1000 | Loss: 0.00003208
Iteration 17/1000 | Loss: 0.00003196
Iteration 18/1000 | Loss: 0.00003196
Iteration 19/1000 | Loss: 0.00003193
Iteration 20/1000 | Loss: 0.00003190
Iteration 21/1000 | Loss: 0.00003190
Iteration 22/1000 | Loss: 0.00003188
Iteration 23/1000 | Loss: 0.00003187
Iteration 24/1000 | Loss: 0.00003187
Iteration 25/1000 | Loss: 0.00003183
Iteration 26/1000 | Loss: 0.00003179
Iteration 27/1000 | Loss: 0.00003177
Iteration 28/1000 | Loss: 0.00003177
Iteration 29/1000 | Loss: 0.00003176
Iteration 30/1000 | Loss: 0.00003176
Iteration 31/1000 | Loss: 0.00003176
Iteration 32/1000 | Loss: 0.00003174
Iteration 33/1000 | Loss: 0.00003174
Iteration 34/1000 | Loss: 0.00003173
Iteration 35/1000 | Loss: 0.00003172
Iteration 36/1000 | Loss: 0.00003172
Iteration 37/1000 | Loss: 0.00003172
Iteration 38/1000 | Loss: 0.00003172
Iteration 39/1000 | Loss: 0.00003170
Iteration 40/1000 | Loss: 0.00003170
Iteration 41/1000 | Loss: 0.00003170
Iteration 42/1000 | Loss: 0.00003170
Iteration 43/1000 | Loss: 0.00003170
Iteration 44/1000 | Loss: 0.00003170
Iteration 45/1000 | Loss: 0.00003170
Iteration 46/1000 | Loss: 0.00003170
Iteration 47/1000 | Loss: 0.00003170
Iteration 48/1000 | Loss: 0.00003169
Iteration 49/1000 | Loss: 0.00003169
Iteration 50/1000 | Loss: 0.00003169
Iteration 51/1000 | Loss: 0.00003169
Iteration 52/1000 | Loss: 0.00003169
Iteration 53/1000 | Loss: 0.00003169
Iteration 54/1000 | Loss: 0.00003167
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003167
Iteration 58/1000 | Loss: 0.00003167
Iteration 59/1000 | Loss: 0.00003167
Iteration 60/1000 | Loss: 0.00003167
Iteration 61/1000 | Loss: 0.00003167
Iteration 62/1000 | Loss: 0.00003166
Iteration 63/1000 | Loss: 0.00003166
Iteration 64/1000 | Loss: 0.00003166
Iteration 65/1000 | Loss: 0.00003166
Iteration 66/1000 | Loss: 0.00003166
Iteration 67/1000 | Loss: 0.00003166
Iteration 68/1000 | Loss: 0.00003166
Iteration 69/1000 | Loss: 0.00003166
Iteration 70/1000 | Loss: 0.00003166
Iteration 71/1000 | Loss: 0.00003166
Iteration 72/1000 | Loss: 0.00003166
Iteration 73/1000 | Loss: 0.00003166
Iteration 74/1000 | Loss: 0.00003165
Iteration 75/1000 | Loss: 0.00003165
Iteration 76/1000 | Loss: 0.00003165
Iteration 77/1000 | Loss: 0.00003165
Iteration 78/1000 | Loss: 0.00003165
Iteration 79/1000 | Loss: 0.00003165
Iteration 80/1000 | Loss: 0.00003165
Iteration 81/1000 | Loss: 0.00003163
Iteration 82/1000 | Loss: 0.00003163
Iteration 83/1000 | Loss: 0.00003163
Iteration 84/1000 | Loss: 0.00003163
Iteration 85/1000 | Loss: 0.00003163
Iteration 86/1000 | Loss: 0.00003163
Iteration 87/1000 | Loss: 0.00003163
Iteration 88/1000 | Loss: 0.00003163
Iteration 89/1000 | Loss: 0.00003163
Iteration 90/1000 | Loss: 0.00003162
Iteration 91/1000 | Loss: 0.00003162
Iteration 92/1000 | Loss: 0.00003162
Iteration 93/1000 | Loss: 0.00003162
Iteration 94/1000 | Loss: 0.00003161
Iteration 95/1000 | Loss: 0.00003161
Iteration 96/1000 | Loss: 0.00003157
Iteration 97/1000 | Loss: 0.00003156
Iteration 98/1000 | Loss: 0.00003155
Iteration 99/1000 | Loss: 0.00003155
Iteration 100/1000 | Loss: 0.00003155
Iteration 101/1000 | Loss: 0.00003154
Iteration 102/1000 | Loss: 0.00003153
Iteration 103/1000 | Loss: 0.00003153
Iteration 104/1000 | Loss: 0.00003153
Iteration 105/1000 | Loss: 0.00003153
Iteration 106/1000 | Loss: 0.00003152
Iteration 107/1000 | Loss: 0.00003151
Iteration 108/1000 | Loss: 0.00003151
Iteration 109/1000 | Loss: 0.00003150
Iteration 110/1000 | Loss: 0.00003150
Iteration 111/1000 | Loss: 0.00003150
Iteration 112/1000 | Loss: 0.00003149
Iteration 113/1000 | Loss: 0.00003148
Iteration 114/1000 | Loss: 0.00003148
Iteration 115/1000 | Loss: 0.00003148
Iteration 116/1000 | Loss: 0.00003148
Iteration 117/1000 | Loss: 0.00003148
Iteration 118/1000 | Loss: 0.00003148
Iteration 119/1000 | Loss: 0.00003148
Iteration 120/1000 | Loss: 0.00003148
Iteration 121/1000 | Loss: 0.00003147
Iteration 122/1000 | Loss: 0.00003147
Iteration 123/1000 | Loss: 0.00003147
Iteration 124/1000 | Loss: 0.00003146
Iteration 125/1000 | Loss: 0.00003146
Iteration 126/1000 | Loss: 0.00003146
Iteration 127/1000 | Loss: 0.00003146
Iteration 128/1000 | Loss: 0.00003146
Iteration 129/1000 | Loss: 0.00003145
Iteration 130/1000 | Loss: 0.00003145
Iteration 131/1000 | Loss: 0.00003145
Iteration 132/1000 | Loss: 0.00003144
Iteration 133/1000 | Loss: 0.00003144
Iteration 134/1000 | Loss: 0.00003144
Iteration 135/1000 | Loss: 0.00003144
Iteration 136/1000 | Loss: 0.00003144
Iteration 137/1000 | Loss: 0.00003143
Iteration 138/1000 | Loss: 0.00003143
Iteration 139/1000 | Loss: 0.00003143
Iteration 140/1000 | Loss: 0.00003143
Iteration 141/1000 | Loss: 0.00003143
Iteration 142/1000 | Loss: 0.00003143
Iteration 143/1000 | Loss: 0.00003143
Iteration 144/1000 | Loss: 0.00003143
Iteration 145/1000 | Loss: 0.00003143
Iteration 146/1000 | Loss: 0.00003143
Iteration 147/1000 | Loss: 0.00003142
Iteration 148/1000 | Loss: 0.00003142
Iteration 149/1000 | Loss: 0.00003142
Iteration 150/1000 | Loss: 0.00003142
Iteration 151/1000 | Loss: 0.00003142
Iteration 152/1000 | Loss: 0.00003142
Iteration 153/1000 | Loss: 0.00003142
Iteration 154/1000 | Loss: 0.00003142
Iteration 155/1000 | Loss: 0.00003142
Iteration 156/1000 | Loss: 0.00003142
Iteration 157/1000 | Loss: 0.00003142
Iteration 158/1000 | Loss: 0.00003142
Iteration 159/1000 | Loss: 0.00003141
Iteration 160/1000 | Loss: 0.00003140
Iteration 161/1000 | Loss: 0.00003140
Iteration 162/1000 | Loss: 0.00003140
Iteration 163/1000 | Loss: 0.00003140
Iteration 164/1000 | Loss: 0.00003140
Iteration 165/1000 | Loss: 0.00003140
Iteration 166/1000 | Loss: 0.00003140
Iteration 167/1000 | Loss: 0.00003140
Iteration 168/1000 | Loss: 0.00003140
Iteration 169/1000 | Loss: 0.00003140
Iteration 170/1000 | Loss: 0.00003140
Iteration 171/1000 | Loss: 0.00003140
Iteration 172/1000 | Loss: 0.00003139
Iteration 173/1000 | Loss: 0.00003139
Iteration 174/1000 | Loss: 0.00003139
Iteration 175/1000 | Loss: 0.00003139
Iteration 176/1000 | Loss: 0.00003139
Iteration 177/1000 | Loss: 0.00003139
Iteration 178/1000 | Loss: 0.00003139
Iteration 179/1000 | Loss: 0.00003139
Iteration 180/1000 | Loss: 0.00003139
Iteration 181/1000 | Loss: 0.00003138
Iteration 182/1000 | Loss: 0.00003138
Iteration 183/1000 | Loss: 0.00003138
Iteration 184/1000 | Loss: 0.00003138
Iteration 185/1000 | Loss: 0.00003138
Iteration 186/1000 | Loss: 0.00003138
Iteration 187/1000 | Loss: 0.00003138
Iteration 188/1000 | Loss: 0.00003138
Iteration 189/1000 | Loss: 0.00003138
Iteration 190/1000 | Loss: 0.00003137
Iteration 191/1000 | Loss: 0.00003137
Iteration 192/1000 | Loss: 0.00003137
Iteration 193/1000 | Loss: 0.00003137
Iteration 194/1000 | Loss: 0.00003137
Iteration 195/1000 | Loss: 0.00003137
Iteration 196/1000 | Loss: 0.00003137
Iteration 197/1000 | Loss: 0.00003136
Iteration 198/1000 | Loss: 0.00003136
Iteration 199/1000 | Loss: 0.00003136
Iteration 200/1000 | Loss: 0.00003136
Iteration 201/1000 | Loss: 0.00003136
Iteration 202/1000 | Loss: 0.00003135
Iteration 203/1000 | Loss: 0.00003135
Iteration 204/1000 | Loss: 0.00003135
Iteration 205/1000 | Loss: 0.00003135
Iteration 206/1000 | Loss: 0.00003135
Iteration 207/1000 | Loss: 0.00003135
Iteration 208/1000 | Loss: 0.00003135
Iteration 209/1000 | Loss: 0.00003134
Iteration 210/1000 | Loss: 0.00003134
Iteration 211/1000 | Loss: 0.00003134
Iteration 212/1000 | Loss: 0.00003134
Iteration 213/1000 | Loss: 0.00003134
Iteration 214/1000 | Loss: 0.00003134
Iteration 215/1000 | Loss: 0.00003134
Iteration 216/1000 | Loss: 0.00003133
Iteration 217/1000 | Loss: 0.00003133
Iteration 218/1000 | Loss: 0.00003133
Iteration 219/1000 | Loss: 0.00003133
Iteration 220/1000 | Loss: 0.00003133
Iteration 221/1000 | Loss: 0.00003133
Iteration 222/1000 | Loss: 0.00003133
Iteration 223/1000 | Loss: 0.00003132
Iteration 224/1000 | Loss: 0.00003132
Iteration 225/1000 | Loss: 0.00003132
Iteration 226/1000 | Loss: 0.00003132
Iteration 227/1000 | Loss: 0.00003132
Iteration 228/1000 | Loss: 0.00003132
Iteration 229/1000 | Loss: 0.00003132
Iteration 230/1000 | Loss: 0.00003132
Iteration 231/1000 | Loss: 0.00003132
Iteration 232/1000 | Loss: 0.00003132
Iteration 233/1000 | Loss: 0.00003132
Iteration 234/1000 | Loss: 0.00003132
Iteration 235/1000 | Loss: 0.00003131
Iteration 236/1000 | Loss: 0.00003131
Iteration 237/1000 | Loss: 0.00003131
Iteration 238/1000 | Loss: 0.00003131
Iteration 239/1000 | Loss: 0.00003131
Iteration 240/1000 | Loss: 0.00003130
Iteration 241/1000 | Loss: 0.00003130
Iteration 242/1000 | Loss: 0.00003130
Iteration 243/1000 | Loss: 0.00003130
Iteration 244/1000 | Loss: 0.00003130
Iteration 245/1000 | Loss: 0.00003130
Iteration 246/1000 | Loss: 0.00003129
Iteration 247/1000 | Loss: 0.00003129
Iteration 248/1000 | Loss: 0.00003129
Iteration 249/1000 | Loss: 0.00003129
Iteration 250/1000 | Loss: 0.00003128
Iteration 251/1000 | Loss: 0.00003128
Iteration 252/1000 | Loss: 0.00003128
Iteration 253/1000 | Loss: 0.00003128
Iteration 254/1000 | Loss: 0.00003128
Iteration 255/1000 | Loss: 0.00003128
Iteration 256/1000 | Loss: 0.00003128
Iteration 257/1000 | Loss: 0.00003128
Iteration 258/1000 | Loss: 0.00003128
Iteration 259/1000 | Loss: 0.00003128
Iteration 260/1000 | Loss: 0.00003128
Iteration 261/1000 | Loss: 0.00003128
Iteration 262/1000 | Loss: 0.00003128
Iteration 263/1000 | Loss: 0.00003127
Iteration 264/1000 | Loss: 0.00003127
Iteration 265/1000 | Loss: 0.00003127
Iteration 266/1000 | Loss: 0.00003127
Iteration 267/1000 | Loss: 0.00003127
Iteration 268/1000 | Loss: 0.00003127
Iteration 269/1000 | Loss: 0.00003127
Iteration 270/1000 | Loss: 0.00003127
Iteration 271/1000 | Loss: 0.00003127
Iteration 272/1000 | Loss: 0.00003127
Iteration 273/1000 | Loss: 0.00003126
Iteration 274/1000 | Loss: 0.00003126
Iteration 275/1000 | Loss: 0.00003126
Iteration 276/1000 | Loss: 0.00003126
Iteration 277/1000 | Loss: 0.00003126
Iteration 278/1000 | Loss: 0.00003126
Iteration 279/1000 | Loss: 0.00003126
Iteration 280/1000 | Loss: 0.00003126
Iteration 281/1000 | Loss: 0.00003125
Iteration 282/1000 | Loss: 0.00003125
Iteration 283/1000 | Loss: 0.00003125
Iteration 284/1000 | Loss: 0.00003125
Iteration 285/1000 | Loss: 0.00003125
Iteration 286/1000 | Loss: 0.00003125
Iteration 287/1000 | Loss: 0.00003125
Iteration 288/1000 | Loss: 0.00003125
Iteration 289/1000 | Loss: 0.00003124
Iteration 290/1000 | Loss: 0.00003124
Iteration 291/1000 | Loss: 0.00003124
Iteration 292/1000 | Loss: 0.00003124
Iteration 293/1000 | Loss: 0.00003124
Iteration 294/1000 | Loss: 0.00003124
Iteration 295/1000 | Loss: 0.00003124
Iteration 296/1000 | Loss: 0.00003124
Iteration 297/1000 | Loss: 0.00003124
Iteration 298/1000 | Loss: 0.00003124
Iteration 299/1000 | Loss: 0.00003123
Iteration 300/1000 | Loss: 0.00003123
Iteration 301/1000 | Loss: 0.00003123
Iteration 302/1000 | Loss: 0.00003123
Iteration 303/1000 | Loss: 0.00003123
Iteration 304/1000 | Loss: 0.00003123
Iteration 305/1000 | Loss: 0.00003123
Iteration 306/1000 | Loss: 0.00003123
Iteration 307/1000 | Loss: 0.00003122
Iteration 308/1000 | Loss: 0.00003122
Iteration 309/1000 | Loss: 0.00003122
Iteration 310/1000 | Loss: 0.00003122
Iteration 311/1000 | Loss: 0.00003122
Iteration 312/1000 | Loss: 0.00003122
Iteration 313/1000 | Loss: 0.00003122
Iteration 314/1000 | Loss: 0.00003121
Iteration 315/1000 | Loss: 0.00003121
Iteration 316/1000 | Loss: 0.00003121
Iteration 317/1000 | Loss: 0.00003121
Iteration 318/1000 | Loss: 0.00003121
Iteration 319/1000 | Loss: 0.00003121
Iteration 320/1000 | Loss: 0.00003121
Iteration 321/1000 | Loss: 0.00003121
Iteration 322/1000 | Loss: 0.00003121
Iteration 323/1000 | Loss: 0.00003121
Iteration 324/1000 | Loss: 0.00003121
Iteration 325/1000 | Loss: 0.00003121
Iteration 326/1000 | Loss: 0.00003121
Iteration 327/1000 | Loss: 0.00003121
Iteration 328/1000 | Loss: 0.00003120
Iteration 329/1000 | Loss: 0.00003120
Iteration 330/1000 | Loss: 0.00003120
Iteration 331/1000 | Loss: 0.00003120
Iteration 332/1000 | Loss: 0.00003120
Iteration 333/1000 | Loss: 0.00003120
Iteration 334/1000 | Loss: 0.00003120
Iteration 335/1000 | Loss: 0.00003120
Iteration 336/1000 | Loss: 0.00003120
Iteration 337/1000 | Loss: 0.00003120
Iteration 338/1000 | Loss: 0.00003120
Iteration 339/1000 | Loss: 0.00003120
Iteration 340/1000 | Loss: 0.00003120
Iteration 341/1000 | Loss: 0.00003120
Iteration 342/1000 | Loss: 0.00003119
Iteration 343/1000 | Loss: 0.00003119
Iteration 344/1000 | Loss: 0.00003119
Iteration 345/1000 | Loss: 0.00003119
Iteration 346/1000 | Loss: 0.00003119
Iteration 347/1000 | Loss: 0.00003119
Iteration 348/1000 | Loss: 0.00003119
Iteration 349/1000 | Loss: 0.00003119
Iteration 350/1000 | Loss: 0.00003119
Iteration 351/1000 | Loss: 0.00003119
Iteration 352/1000 | Loss: 0.00003119
Iteration 353/1000 | Loss: 0.00003119
Iteration 354/1000 | Loss: 0.00003119
Iteration 355/1000 | Loss: 0.00003119
Iteration 356/1000 | Loss: 0.00003119
Iteration 357/1000 | Loss: 0.00003119
Iteration 358/1000 | Loss: 0.00003119
Iteration 359/1000 | Loss: 0.00003118
Iteration 360/1000 | Loss: 0.00003118
Iteration 361/1000 | Loss: 0.00003118
Iteration 362/1000 | Loss: 0.00003118
Iteration 363/1000 | Loss: 0.00003118
Iteration 364/1000 | Loss: 0.00003118
Iteration 365/1000 | Loss: 0.00003118
Iteration 366/1000 | Loss: 0.00003118
Iteration 367/1000 | Loss: 0.00003118
Iteration 368/1000 | Loss: 0.00003118
Iteration 369/1000 | Loss: 0.00003118
Iteration 370/1000 | Loss: 0.00003118
Iteration 371/1000 | Loss: 0.00003118
Iteration 372/1000 | Loss: 0.00003118
Iteration 373/1000 | Loss: 0.00003118
Iteration 374/1000 | Loss: 0.00003118
Iteration 375/1000 | Loss: 0.00003118
Iteration 376/1000 | Loss: 0.00003117
Iteration 377/1000 | Loss: 0.00003117
Iteration 378/1000 | Loss: 0.00003117
Iteration 379/1000 | Loss: 0.00003117
Iteration 380/1000 | Loss: 0.00003117
Iteration 381/1000 | Loss: 0.00003117
Iteration 382/1000 | Loss: 0.00003117
Iteration 383/1000 | Loss: 0.00003117
Iteration 384/1000 | Loss: 0.00003117
Iteration 385/1000 | Loss: 0.00003117
Iteration 386/1000 | Loss: 0.00003117
Iteration 387/1000 | Loss: 0.00003117
Iteration 388/1000 | Loss: 0.00003117
Iteration 389/1000 | Loss: 0.00003117
Iteration 390/1000 | Loss: 0.00003117
Iteration 391/1000 | Loss: 0.00003117
Iteration 392/1000 | Loss: 0.00003117
Iteration 393/1000 | Loss: 0.00003117
Iteration 394/1000 | Loss: 0.00003117
Iteration 395/1000 | Loss: 0.00003117
Iteration 396/1000 | Loss: 0.00003117
Iteration 397/1000 | Loss: 0.00003117
Iteration 398/1000 | Loss: 0.00003117
Iteration 399/1000 | Loss: 0.00003117
Iteration 400/1000 | Loss: 0.00003117
Iteration 401/1000 | Loss: 0.00003117
Iteration 402/1000 | Loss: 0.00003117
Iteration 403/1000 | Loss: 0.00003117
Iteration 404/1000 | Loss: 0.00003117
Iteration 405/1000 | Loss: 0.00003117
Iteration 406/1000 | Loss: 0.00003117
Iteration 407/1000 | Loss: 0.00003117
Iteration 408/1000 | Loss: 0.00003117
Iteration 409/1000 | Loss: 0.00003117
Iteration 410/1000 | Loss: 0.00003117
Iteration 411/1000 | Loss: 0.00003117
Iteration 412/1000 | Loss: 0.00003117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 412. Stopping optimization.
Last 5 losses: [3.116764492006041e-05, 3.116764492006041e-05, 3.116764492006041e-05, 3.116764492006041e-05, 3.116764492006041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.116764492006041e-05

Optimization complete. Final v2v error: 4.429509162902832 mm

Highest mean error: 5.846724510192871 mm for frame 82

Lowest mean error: 3.650834560394287 mm for frame 0

Saving results

Total time: 62.487005949020386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089029
Iteration 2/25 | Loss: 0.00208259
Iteration 3/25 | Loss: 0.00155070
Iteration 4/25 | Loss: 0.00140090
Iteration 5/25 | Loss: 0.00131089
Iteration 6/25 | Loss: 0.00126372
Iteration 7/25 | Loss: 0.00124193
Iteration 8/25 | Loss: 0.00125814
Iteration 9/25 | Loss: 0.00123946
Iteration 10/25 | Loss: 0.00120504
Iteration 11/25 | Loss: 0.00120874
Iteration 12/25 | Loss: 0.00120112
Iteration 13/25 | Loss: 0.00119828
Iteration 14/25 | Loss: 0.00119281
Iteration 15/25 | Loss: 0.00119928
Iteration 16/25 | Loss: 0.00120176
Iteration 17/25 | Loss: 0.00119841
Iteration 18/25 | Loss: 0.00119945
Iteration 19/25 | Loss: 0.00119863
Iteration 20/25 | Loss: 0.00119780
Iteration 21/25 | Loss: 0.00119488
Iteration 22/25 | Loss: 0.00119036
Iteration 23/25 | Loss: 0.00119266
Iteration 24/25 | Loss: 0.00119340
Iteration 25/25 | Loss: 0.00119306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30280030
Iteration 2/25 | Loss: 0.00163277
Iteration 3/25 | Loss: 0.00158413
Iteration 4/25 | Loss: 0.00158413
Iteration 5/25 | Loss: 0.00158413
Iteration 6/25 | Loss: 0.00158413
Iteration 7/25 | Loss: 0.00158413
Iteration 8/25 | Loss: 0.00158413
Iteration 9/25 | Loss: 0.00158413
Iteration 10/25 | Loss: 0.00158413
Iteration 11/25 | Loss: 0.00158413
Iteration 12/25 | Loss: 0.00158413
Iteration 13/25 | Loss: 0.00158413
Iteration 14/25 | Loss: 0.00158413
Iteration 15/25 | Loss: 0.00158413
Iteration 16/25 | Loss: 0.00158413
Iteration 17/25 | Loss: 0.00158413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015841253334656358, 0.0015841253334656358, 0.0015841253334656358, 0.0015841253334656358, 0.0015841253334656358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015841253334656358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158413
Iteration 2/1000 | Loss: 0.00044433
Iteration 3/1000 | Loss: 0.00010312
Iteration 4/1000 | Loss: 0.00010373
Iteration 5/1000 | Loss: 0.00021310
Iteration 6/1000 | Loss: 0.00025628
Iteration 7/1000 | Loss: 0.00025556
Iteration 8/1000 | Loss: 0.00113327
Iteration 9/1000 | Loss: 0.00041996
Iteration 10/1000 | Loss: 0.00052068
Iteration 11/1000 | Loss: 0.00033259
Iteration 12/1000 | Loss: 0.00015265
Iteration 13/1000 | Loss: 0.00043432
Iteration 14/1000 | Loss: 0.00034755
Iteration 15/1000 | Loss: 0.00034681
Iteration 16/1000 | Loss: 0.00036096
Iteration 17/1000 | Loss: 0.00035229
Iteration 18/1000 | Loss: 0.00035806
Iteration 19/1000 | Loss: 0.00042828
Iteration 20/1000 | Loss: 0.00033453
Iteration 21/1000 | Loss: 0.00029518
Iteration 22/1000 | Loss: 0.00020174
Iteration 23/1000 | Loss: 0.00007050
Iteration 24/1000 | Loss: 0.00018827
Iteration 25/1000 | Loss: 0.00057934
Iteration 26/1000 | Loss: 0.00025699
Iteration 27/1000 | Loss: 0.00032420
Iteration 28/1000 | Loss: 0.00043214
Iteration 29/1000 | Loss: 0.00035506
Iteration 30/1000 | Loss: 0.00016068
Iteration 31/1000 | Loss: 0.00010690
Iteration 32/1000 | Loss: 0.00004888
Iteration 33/1000 | Loss: 0.00023628
Iteration 34/1000 | Loss: 0.00018059
Iteration 35/1000 | Loss: 0.00021864
Iteration 36/1000 | Loss: 0.00035359
Iteration 37/1000 | Loss: 0.00024592
Iteration 38/1000 | Loss: 0.00027608
Iteration 39/1000 | Loss: 0.00020700
Iteration 40/1000 | Loss: 0.00018152
Iteration 41/1000 | Loss: 0.00029794
Iteration 42/1000 | Loss: 0.00036094
Iteration 43/1000 | Loss: 0.00012086
Iteration 44/1000 | Loss: 0.00008300
Iteration 45/1000 | Loss: 0.00022144
Iteration 46/1000 | Loss: 0.00035667
Iteration 47/1000 | Loss: 0.00039047
Iteration 48/1000 | Loss: 0.00043052
Iteration 49/1000 | Loss: 0.00032297
Iteration 50/1000 | Loss: 0.00027593
Iteration 51/1000 | Loss: 0.00060225
Iteration 52/1000 | Loss: 0.00032049
Iteration 53/1000 | Loss: 0.00031412
Iteration 54/1000 | Loss: 0.00031566
Iteration 55/1000 | Loss: 0.00031549
Iteration 56/1000 | Loss: 0.00041838
Iteration 57/1000 | Loss: 0.00024807
Iteration 58/1000 | Loss: 0.00029020
Iteration 59/1000 | Loss: 0.00033082
Iteration 60/1000 | Loss: 0.00026055
Iteration 61/1000 | Loss: 0.00030053
Iteration 62/1000 | Loss: 0.00021603
Iteration 63/1000 | Loss: 0.00035993
Iteration 64/1000 | Loss: 0.00112084
Iteration 65/1000 | Loss: 0.00047473
Iteration 66/1000 | Loss: 0.00066124
Iteration 67/1000 | Loss: 0.00042577
Iteration 68/1000 | Loss: 0.00016738
Iteration 69/1000 | Loss: 0.00026722
Iteration 70/1000 | Loss: 0.00022673
Iteration 71/1000 | Loss: 0.00047986
Iteration 72/1000 | Loss: 0.00025013
Iteration 73/1000 | Loss: 0.00006403
Iteration 74/1000 | Loss: 0.00008802
Iteration 75/1000 | Loss: 0.00018167
Iteration 76/1000 | Loss: 0.00029268
Iteration 77/1000 | Loss: 0.00081260
Iteration 78/1000 | Loss: 0.00090441
Iteration 79/1000 | Loss: 0.00013230
Iteration 80/1000 | Loss: 0.00044999
Iteration 81/1000 | Loss: 0.00009872
Iteration 82/1000 | Loss: 0.00026860
Iteration 83/1000 | Loss: 0.00015417
Iteration 84/1000 | Loss: 0.00014870
Iteration 85/1000 | Loss: 0.00010739
Iteration 86/1000 | Loss: 0.00006568
Iteration 87/1000 | Loss: 0.00022913
Iteration 88/1000 | Loss: 0.00014401
Iteration 89/1000 | Loss: 0.00018773
Iteration 90/1000 | Loss: 0.00037817
Iteration 91/1000 | Loss: 0.00036852
Iteration 92/1000 | Loss: 0.00032764
Iteration 93/1000 | Loss: 0.00025959
Iteration 94/1000 | Loss: 0.00020038
Iteration 95/1000 | Loss: 0.00021406
Iteration 96/1000 | Loss: 0.00020631
Iteration 97/1000 | Loss: 0.00039052
Iteration 98/1000 | Loss: 0.00047074
Iteration 99/1000 | Loss: 0.00037423
Iteration 100/1000 | Loss: 0.00027081
Iteration 101/1000 | Loss: 0.00011471
Iteration 102/1000 | Loss: 0.00020995
Iteration 103/1000 | Loss: 0.00006071
Iteration 104/1000 | Loss: 0.00005457
Iteration 105/1000 | Loss: 0.00005486
Iteration 106/1000 | Loss: 0.00005270
Iteration 107/1000 | Loss: 0.00020851
Iteration 108/1000 | Loss: 0.00016486
Iteration 109/1000 | Loss: 0.00015445
Iteration 110/1000 | Loss: 0.00009152
Iteration 111/1000 | Loss: 0.00016165
Iteration 112/1000 | Loss: 0.00017381
Iteration 113/1000 | Loss: 0.00015023
Iteration 114/1000 | Loss: 0.00015582
Iteration 115/1000 | Loss: 0.00030477
Iteration 116/1000 | Loss: 0.00032851
Iteration 117/1000 | Loss: 0.00018895
Iteration 118/1000 | Loss: 0.00014491
Iteration 119/1000 | Loss: 0.00005324
Iteration 120/1000 | Loss: 0.00019529
Iteration 121/1000 | Loss: 0.00008935
Iteration 122/1000 | Loss: 0.00011329
Iteration 123/1000 | Loss: 0.00016925
Iteration 124/1000 | Loss: 0.00003944
Iteration 125/1000 | Loss: 0.00002574
Iteration 126/1000 | Loss: 0.00007620
Iteration 127/1000 | Loss: 0.00002134
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00005317
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00003282
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00004782
Iteration 138/1000 | Loss: 0.00001406
Iteration 139/1000 | Loss: 0.00001402
Iteration 140/1000 | Loss: 0.00001401
Iteration 141/1000 | Loss: 0.00001401
Iteration 142/1000 | Loss: 0.00001392
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001378
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001377
Iteration 148/1000 | Loss: 0.00001377
Iteration 149/1000 | Loss: 0.00001376
Iteration 150/1000 | Loss: 0.00001376
Iteration 151/1000 | Loss: 0.00001376
Iteration 152/1000 | Loss: 0.00001374
Iteration 153/1000 | Loss: 0.00001374
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001373
Iteration 157/1000 | Loss: 0.00001373
Iteration 158/1000 | Loss: 0.00001373
Iteration 159/1000 | Loss: 0.00001373
Iteration 160/1000 | Loss: 0.00001373
Iteration 161/1000 | Loss: 0.00001372
Iteration 162/1000 | Loss: 0.00001372
Iteration 163/1000 | Loss: 0.00001370
Iteration 164/1000 | Loss: 0.00001370
Iteration 165/1000 | Loss: 0.00001370
Iteration 166/1000 | Loss: 0.00001369
Iteration 167/1000 | Loss: 0.00001369
Iteration 168/1000 | Loss: 0.00001369
Iteration 169/1000 | Loss: 0.00001369
Iteration 170/1000 | Loss: 0.00001369
Iteration 171/1000 | Loss: 0.00001369
Iteration 172/1000 | Loss: 0.00001369
Iteration 173/1000 | Loss: 0.00001369
Iteration 174/1000 | Loss: 0.00001369
Iteration 175/1000 | Loss: 0.00001369
Iteration 176/1000 | Loss: 0.00001369
Iteration 177/1000 | Loss: 0.00001369
Iteration 178/1000 | Loss: 0.00001369
Iteration 179/1000 | Loss: 0.00001368
Iteration 180/1000 | Loss: 0.00001368
Iteration 181/1000 | Loss: 0.00001368
Iteration 182/1000 | Loss: 0.00001368
Iteration 183/1000 | Loss: 0.00001368
Iteration 184/1000 | Loss: 0.00001368
Iteration 185/1000 | Loss: 0.00001367
Iteration 186/1000 | Loss: 0.00001367
Iteration 187/1000 | Loss: 0.00001367
Iteration 188/1000 | Loss: 0.00001367
Iteration 189/1000 | Loss: 0.00001367
Iteration 190/1000 | Loss: 0.00001367
Iteration 191/1000 | Loss: 0.00001367
Iteration 192/1000 | Loss: 0.00001367
Iteration 193/1000 | Loss: 0.00001367
Iteration 194/1000 | Loss: 0.00001367
Iteration 195/1000 | Loss: 0.00001367
Iteration 196/1000 | Loss: 0.00001367
Iteration 197/1000 | Loss: 0.00001367
Iteration 198/1000 | Loss: 0.00001367
Iteration 199/1000 | Loss: 0.00001367
Iteration 200/1000 | Loss: 0.00001367
Iteration 201/1000 | Loss: 0.00001367
Iteration 202/1000 | Loss: 0.00001367
Iteration 203/1000 | Loss: 0.00001367
Iteration 204/1000 | Loss: 0.00001367
Iteration 205/1000 | Loss: 0.00001367
Iteration 206/1000 | Loss: 0.00001367
Iteration 207/1000 | Loss: 0.00001367
Iteration 208/1000 | Loss: 0.00001367
Iteration 209/1000 | Loss: 0.00001367
Iteration 210/1000 | Loss: 0.00001367
Iteration 211/1000 | Loss: 0.00001367
Iteration 212/1000 | Loss: 0.00001367
Iteration 213/1000 | Loss: 0.00001367
Iteration 214/1000 | Loss: 0.00001367
Iteration 215/1000 | Loss: 0.00001367
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.3669480722455774e-05, 1.3669480722455774e-05, 1.3669480722455774e-05, 1.3669480722455774e-05, 1.3669480722455774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3669480722455774e-05

Optimization complete. Final v2v error: 3.121460199356079 mm

Highest mean error: 4.031687259674072 mm for frame 22

Lowest mean error: 2.810307741165161 mm for frame 1

Saving results

Total time: 232.466712474823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422387
Iteration 2/25 | Loss: 0.00132387
Iteration 3/25 | Loss: 0.00123056
Iteration 4/25 | Loss: 0.00122149
Iteration 5/25 | Loss: 0.00121881
Iteration 6/25 | Loss: 0.00121794
Iteration 7/25 | Loss: 0.00121794
Iteration 8/25 | Loss: 0.00121794
Iteration 9/25 | Loss: 0.00121794
Iteration 10/25 | Loss: 0.00121794
Iteration 11/25 | Loss: 0.00121794
Iteration 12/25 | Loss: 0.00121794
Iteration 13/25 | Loss: 0.00121794
Iteration 14/25 | Loss: 0.00121794
Iteration 15/25 | Loss: 0.00121794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012179408222436905, 0.0012179408222436905, 0.0012179408222436905, 0.0012179408222436905, 0.0012179408222436905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012179408222436905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27592838
Iteration 2/25 | Loss: 0.00127209
Iteration 3/25 | Loss: 0.00127209
Iteration 4/25 | Loss: 0.00127209
Iteration 5/25 | Loss: 0.00127209
Iteration 6/25 | Loss: 0.00127209
Iteration 7/25 | Loss: 0.00127209
Iteration 8/25 | Loss: 0.00127209
Iteration 9/25 | Loss: 0.00127209
Iteration 10/25 | Loss: 0.00127208
Iteration 11/25 | Loss: 0.00127208
Iteration 12/25 | Loss: 0.00127208
Iteration 13/25 | Loss: 0.00127208
Iteration 14/25 | Loss: 0.00127208
Iteration 15/25 | Loss: 0.00127208
Iteration 16/25 | Loss: 0.00127208
Iteration 17/25 | Loss: 0.00127208
Iteration 18/25 | Loss: 0.00127208
Iteration 19/25 | Loss: 0.00127208
Iteration 20/25 | Loss: 0.00127208
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012720846571028233, 0.0012720846571028233, 0.0012720846571028233, 0.0012720846571028233, 0.0012720846571028233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012720846571028233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127208
Iteration 2/1000 | Loss: 0.00003734
Iteration 3/1000 | Loss: 0.00002245
Iteration 4/1000 | Loss: 0.00001969
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001731
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001625
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001573
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001555
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001545
Iteration 15/1000 | Loss: 0.00001544
Iteration 16/1000 | Loss: 0.00001543
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001539
Iteration 20/1000 | Loss: 0.00001539
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001539
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001536
Iteration 29/1000 | Loss: 0.00001536
Iteration 30/1000 | Loss: 0.00001536
Iteration 31/1000 | Loss: 0.00001536
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001535
Iteration 37/1000 | Loss: 0.00001535
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001534
Iteration 40/1000 | Loss: 0.00001534
Iteration 41/1000 | Loss: 0.00001534
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001529
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001528
Iteration 59/1000 | Loss: 0.00001528
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001524
Iteration 82/1000 | Loss: 0.00001524
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001522
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001522
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001521
Iteration 113/1000 | Loss: 0.00001521
Iteration 114/1000 | Loss: 0.00001521
Iteration 115/1000 | Loss: 0.00001521
Iteration 116/1000 | Loss: 0.00001521
Iteration 117/1000 | Loss: 0.00001521
Iteration 118/1000 | Loss: 0.00001521
Iteration 119/1000 | Loss: 0.00001521
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001521
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Iteration 136/1000 | Loss: 0.00001521
Iteration 137/1000 | Loss: 0.00001521
Iteration 138/1000 | Loss: 0.00001521
Iteration 139/1000 | Loss: 0.00001521
Iteration 140/1000 | Loss: 0.00001521
Iteration 141/1000 | Loss: 0.00001521
Iteration 142/1000 | Loss: 0.00001521
Iteration 143/1000 | Loss: 0.00001521
Iteration 144/1000 | Loss: 0.00001521
Iteration 145/1000 | Loss: 0.00001521
Iteration 146/1000 | Loss: 0.00001521
Iteration 147/1000 | Loss: 0.00001521
Iteration 148/1000 | Loss: 0.00001521
Iteration 149/1000 | Loss: 0.00001521
Iteration 150/1000 | Loss: 0.00001521
Iteration 151/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.5210449419100769e-05, 1.5210449419100769e-05, 1.5210449419100769e-05, 1.5210449419100769e-05, 1.5210449419100769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5210449419100769e-05

Optimization complete. Final v2v error: 3.4266772270202637 mm

Highest mean error: 3.7674336433410645 mm for frame 77

Lowest mean error: 3.1274049282073975 mm for frame 29

Saving results

Total time: 32.07956528663635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071585
Iteration 2/25 | Loss: 0.00134707
Iteration 3/25 | Loss: 0.00121326
Iteration 4/25 | Loss: 0.00120584
Iteration 5/25 | Loss: 0.00119121
Iteration 6/25 | Loss: 0.00118866
Iteration 7/25 | Loss: 0.00118810
Iteration 8/25 | Loss: 0.00118798
Iteration 9/25 | Loss: 0.00118797
Iteration 10/25 | Loss: 0.00118797
Iteration 11/25 | Loss: 0.00118796
Iteration 12/25 | Loss: 0.00118796
Iteration 13/25 | Loss: 0.00118796
Iteration 14/25 | Loss: 0.00118796
Iteration 15/25 | Loss: 0.00118796
Iteration 16/25 | Loss: 0.00118796
Iteration 17/25 | Loss: 0.00118796
Iteration 18/25 | Loss: 0.00118795
Iteration 19/25 | Loss: 0.00118795
Iteration 20/25 | Loss: 0.00118795
Iteration 21/25 | Loss: 0.00118795
Iteration 22/25 | Loss: 0.00118795
Iteration 23/25 | Loss: 0.00118795
Iteration 24/25 | Loss: 0.00118795
Iteration 25/25 | Loss: 0.00118795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.89065981
Iteration 2/25 | Loss: 0.00112737
Iteration 3/25 | Loss: 0.00112737
Iteration 4/25 | Loss: 0.00112737
Iteration 5/25 | Loss: 0.00112737
Iteration 6/25 | Loss: 0.00112737
Iteration 7/25 | Loss: 0.00112737
Iteration 8/25 | Loss: 0.00112737
Iteration 9/25 | Loss: 0.00112737
Iteration 10/25 | Loss: 0.00112737
Iteration 11/25 | Loss: 0.00112737
Iteration 12/25 | Loss: 0.00112737
Iteration 13/25 | Loss: 0.00112737
Iteration 14/25 | Loss: 0.00112737
Iteration 15/25 | Loss: 0.00112737
Iteration 16/25 | Loss: 0.00112737
Iteration 17/25 | Loss: 0.00112737
Iteration 18/25 | Loss: 0.00112737
Iteration 19/25 | Loss: 0.00112737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011273669078946114, 0.0011273669078946114, 0.0011273669078946114, 0.0011273669078946114, 0.0011273669078946114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011273669078946114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112737
Iteration 2/1000 | Loss: 0.00003364
Iteration 3/1000 | Loss: 0.00002488
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002102
Iteration 6/1000 | Loss: 0.00002022
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001896
Iteration 9/1000 | Loss: 0.00001866
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001842
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001834
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001827
Iteration 21/1000 | Loss: 0.00001826
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001823
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001822
Iteration 33/1000 | Loss: 0.00001822
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001822
Iteration 40/1000 | Loss: 0.00001822
Iteration 41/1000 | Loss: 0.00001821
Iteration 42/1000 | Loss: 0.00001821
Iteration 43/1000 | Loss: 0.00001821
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001821
Iteration 46/1000 | Loss: 0.00001821
Iteration 47/1000 | Loss: 0.00001820
Iteration 48/1000 | Loss: 0.00001820
Iteration 49/1000 | Loss: 0.00001820
Iteration 50/1000 | Loss: 0.00001820
Iteration 51/1000 | Loss: 0.00001820
Iteration 52/1000 | Loss: 0.00001820
Iteration 53/1000 | Loss: 0.00001820
Iteration 54/1000 | Loss: 0.00001820
Iteration 55/1000 | Loss: 0.00001820
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001819
Iteration 63/1000 | Loss: 0.00001819
Iteration 64/1000 | Loss: 0.00001819
Iteration 65/1000 | Loss: 0.00001819
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001818
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001818
Iteration 74/1000 | Loss: 0.00001818
Iteration 75/1000 | Loss: 0.00001818
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001817
Iteration 90/1000 | Loss: 0.00001817
Iteration 91/1000 | Loss: 0.00001817
Iteration 92/1000 | Loss: 0.00001817
Iteration 93/1000 | Loss: 0.00001817
Iteration 94/1000 | Loss: 0.00001817
Iteration 95/1000 | Loss: 0.00001817
Iteration 96/1000 | Loss: 0.00001817
Iteration 97/1000 | Loss: 0.00001817
Iteration 98/1000 | Loss: 0.00001817
Iteration 99/1000 | Loss: 0.00001817
Iteration 100/1000 | Loss: 0.00001817
Iteration 101/1000 | Loss: 0.00001817
Iteration 102/1000 | Loss: 0.00001817
Iteration 103/1000 | Loss: 0.00001817
Iteration 104/1000 | Loss: 0.00001817
Iteration 105/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.8172087948187254e-05, 1.8172087948187254e-05, 1.8172087948187254e-05, 1.8172087948187254e-05, 1.8172087948187254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8172087948187254e-05

Optimization complete. Final v2v error: 3.5689053535461426 mm

Highest mean error: 3.873749256134033 mm for frame 50

Lowest mean error: 3.1446473598480225 mm for frame 149

Saving results

Total time: 33.574084758758545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848654
Iteration 2/25 | Loss: 0.00154267
Iteration 3/25 | Loss: 0.00127026
Iteration 4/25 | Loss: 0.00123667
Iteration 5/25 | Loss: 0.00123066
Iteration 6/25 | Loss: 0.00122932
Iteration 7/25 | Loss: 0.00122932
Iteration 8/25 | Loss: 0.00122932
Iteration 9/25 | Loss: 0.00122932
Iteration 10/25 | Loss: 0.00122932
Iteration 11/25 | Loss: 0.00122932
Iteration 12/25 | Loss: 0.00122932
Iteration 13/25 | Loss: 0.00122932
Iteration 14/25 | Loss: 0.00122932
Iteration 15/25 | Loss: 0.00122932
Iteration 16/25 | Loss: 0.00122932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012293205363675952, 0.0012293205363675952, 0.0012293205363675952, 0.0012293205363675952, 0.0012293205363675952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012293205363675952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20735693
Iteration 2/25 | Loss: 0.00133679
Iteration 3/25 | Loss: 0.00133678
Iteration 4/25 | Loss: 0.00133678
Iteration 5/25 | Loss: 0.00133678
Iteration 6/25 | Loss: 0.00133678
Iteration 7/25 | Loss: 0.00133678
Iteration 8/25 | Loss: 0.00133678
Iteration 9/25 | Loss: 0.00133678
Iteration 10/25 | Loss: 0.00133678
Iteration 11/25 | Loss: 0.00133678
Iteration 12/25 | Loss: 0.00133678
Iteration 13/25 | Loss: 0.00133678
Iteration 14/25 | Loss: 0.00133678
Iteration 15/25 | Loss: 0.00133678
Iteration 16/25 | Loss: 0.00133678
Iteration 17/25 | Loss: 0.00133678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013367785140872002, 0.0013367785140872002, 0.0013367785140872002, 0.0013367785140872002, 0.0013367785140872002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013367785140872002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133678
Iteration 2/1000 | Loss: 0.00005723
Iteration 3/1000 | Loss: 0.00002919
Iteration 4/1000 | Loss: 0.00002236
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001784
Iteration 7/1000 | Loss: 0.00001666
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001518
Iteration 12/1000 | Loss: 0.00001491
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001457
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001451
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001450
Iteration 24/1000 | Loss: 0.00001449
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001449
Iteration 27/1000 | Loss: 0.00001448
Iteration 28/1000 | Loss: 0.00001448
Iteration 29/1000 | Loss: 0.00001447
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001445
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001442
Iteration 41/1000 | Loss: 0.00001442
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001438
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001438
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001433
Iteration 75/1000 | Loss: 0.00001433
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001430
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001429
Iteration 100/1000 | Loss: 0.00001429
Iteration 101/1000 | Loss: 0.00001429
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001428
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001426
Iteration 134/1000 | Loss: 0.00001426
Iteration 135/1000 | Loss: 0.00001426
Iteration 136/1000 | Loss: 0.00001426
Iteration 137/1000 | Loss: 0.00001426
Iteration 138/1000 | Loss: 0.00001426
Iteration 139/1000 | Loss: 0.00001426
Iteration 140/1000 | Loss: 0.00001426
Iteration 141/1000 | Loss: 0.00001426
Iteration 142/1000 | Loss: 0.00001426
Iteration 143/1000 | Loss: 0.00001425
Iteration 144/1000 | Loss: 0.00001425
Iteration 145/1000 | Loss: 0.00001425
Iteration 146/1000 | Loss: 0.00001425
Iteration 147/1000 | Loss: 0.00001425
Iteration 148/1000 | Loss: 0.00001425
Iteration 149/1000 | Loss: 0.00001425
Iteration 150/1000 | Loss: 0.00001425
Iteration 151/1000 | Loss: 0.00001425
Iteration 152/1000 | Loss: 0.00001425
Iteration 153/1000 | Loss: 0.00001425
Iteration 154/1000 | Loss: 0.00001424
Iteration 155/1000 | Loss: 0.00001424
Iteration 156/1000 | Loss: 0.00001424
Iteration 157/1000 | Loss: 0.00001424
Iteration 158/1000 | Loss: 0.00001424
Iteration 159/1000 | Loss: 0.00001424
Iteration 160/1000 | Loss: 0.00001424
Iteration 161/1000 | Loss: 0.00001424
Iteration 162/1000 | Loss: 0.00001424
Iteration 163/1000 | Loss: 0.00001424
Iteration 164/1000 | Loss: 0.00001424
Iteration 165/1000 | Loss: 0.00001424
Iteration 166/1000 | Loss: 0.00001424
Iteration 167/1000 | Loss: 0.00001424
Iteration 168/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.4240521522879135e-05, 1.4240521522879135e-05, 1.4240521522879135e-05, 1.4240521522879135e-05, 1.4240521522879135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4240521522879135e-05

Optimization complete. Final v2v error: 3.267970323562622 mm

Highest mean error: 3.6246612071990967 mm for frame 161

Lowest mean error: 2.8334503173828125 mm for frame 201

Saving results

Total time: 43.09794354438782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080184
Iteration 2/25 | Loss: 0.00191883
Iteration 3/25 | Loss: 0.00138998
Iteration 4/25 | Loss: 0.00132883
Iteration 5/25 | Loss: 0.00130804
Iteration 6/25 | Loss: 0.00130412
Iteration 7/25 | Loss: 0.00130334
Iteration 8/25 | Loss: 0.00130303
Iteration 9/25 | Loss: 0.00130293
Iteration 10/25 | Loss: 0.00130292
Iteration 11/25 | Loss: 0.00130292
Iteration 12/25 | Loss: 0.00130292
Iteration 13/25 | Loss: 0.00130292
Iteration 14/25 | Loss: 0.00130292
Iteration 15/25 | Loss: 0.00130292
Iteration 16/25 | Loss: 0.00130292
Iteration 17/25 | Loss: 0.00130292
Iteration 18/25 | Loss: 0.00130292
Iteration 19/25 | Loss: 0.00130291
Iteration 20/25 | Loss: 0.00130291
Iteration 21/25 | Loss: 0.00130291
Iteration 22/25 | Loss: 0.00130291
Iteration 23/25 | Loss: 0.00130291
Iteration 24/25 | Loss: 0.00130291
Iteration 25/25 | Loss: 0.00130291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95155936
Iteration 2/25 | Loss: 0.00111998
Iteration 3/25 | Loss: 0.00111998
Iteration 4/25 | Loss: 0.00111998
Iteration 5/25 | Loss: 0.00111998
Iteration 6/25 | Loss: 0.00111998
Iteration 7/25 | Loss: 0.00111998
Iteration 8/25 | Loss: 0.00111998
Iteration 9/25 | Loss: 0.00111998
Iteration 10/25 | Loss: 0.00111998
Iteration 11/25 | Loss: 0.00111998
Iteration 12/25 | Loss: 0.00111998
Iteration 13/25 | Loss: 0.00111998
Iteration 14/25 | Loss: 0.00111998
Iteration 15/25 | Loss: 0.00111998
Iteration 16/25 | Loss: 0.00111998
Iteration 17/25 | Loss: 0.00111998
Iteration 18/25 | Loss: 0.00111998
Iteration 19/25 | Loss: 0.00111998
Iteration 20/25 | Loss: 0.00111998
Iteration 21/25 | Loss: 0.00111998
Iteration 22/25 | Loss: 0.00111998
Iteration 23/25 | Loss: 0.00111998
Iteration 24/25 | Loss: 0.00111998
Iteration 25/25 | Loss: 0.00111998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111998
Iteration 2/1000 | Loss: 0.00010436
Iteration 3/1000 | Loss: 0.00005887
Iteration 4/1000 | Loss: 0.00004788
Iteration 5/1000 | Loss: 0.00004374
Iteration 6/1000 | Loss: 0.00004101
Iteration 7/1000 | Loss: 0.00003941
Iteration 8/1000 | Loss: 0.00003855
Iteration 9/1000 | Loss: 0.00003806
Iteration 10/1000 | Loss: 0.00003767
Iteration 11/1000 | Loss: 0.00003711
Iteration 12/1000 | Loss: 0.00003664
Iteration 13/1000 | Loss: 0.00003625
Iteration 14/1000 | Loss: 0.00003590
Iteration 15/1000 | Loss: 0.00003564
Iteration 16/1000 | Loss: 0.00003541
Iteration 17/1000 | Loss: 0.00003521
Iteration 18/1000 | Loss: 0.00003502
Iteration 19/1000 | Loss: 0.00003488
Iteration 20/1000 | Loss: 0.00003471
Iteration 21/1000 | Loss: 0.00003460
Iteration 22/1000 | Loss: 0.00003454
Iteration 23/1000 | Loss: 0.00003446
Iteration 24/1000 | Loss: 0.00003441
Iteration 25/1000 | Loss: 0.00003441
Iteration 26/1000 | Loss: 0.00003436
Iteration 27/1000 | Loss: 0.00003435
Iteration 28/1000 | Loss: 0.00003432
Iteration 29/1000 | Loss: 0.00003428
Iteration 30/1000 | Loss: 0.00003428
Iteration 31/1000 | Loss: 0.00003428
Iteration 32/1000 | Loss: 0.00003425
Iteration 33/1000 | Loss: 0.00003425
Iteration 34/1000 | Loss: 0.00003419
Iteration 35/1000 | Loss: 0.00003419
Iteration 36/1000 | Loss: 0.00003419
Iteration 37/1000 | Loss: 0.00003419
Iteration 38/1000 | Loss: 0.00003418
Iteration 39/1000 | Loss: 0.00003418
Iteration 40/1000 | Loss: 0.00003418
Iteration 41/1000 | Loss: 0.00003417
Iteration 42/1000 | Loss: 0.00003417
Iteration 43/1000 | Loss: 0.00003417
Iteration 44/1000 | Loss: 0.00003417
Iteration 45/1000 | Loss: 0.00003417
Iteration 46/1000 | Loss: 0.00003417
Iteration 47/1000 | Loss: 0.00003416
Iteration 48/1000 | Loss: 0.00003416
Iteration 49/1000 | Loss: 0.00003416
Iteration 50/1000 | Loss: 0.00003415
Iteration 51/1000 | Loss: 0.00003415
Iteration 52/1000 | Loss: 0.00003415
Iteration 53/1000 | Loss: 0.00003415
Iteration 54/1000 | Loss: 0.00003415
Iteration 55/1000 | Loss: 0.00003414
Iteration 56/1000 | Loss: 0.00003414
Iteration 57/1000 | Loss: 0.00003414
Iteration 58/1000 | Loss: 0.00003414
Iteration 59/1000 | Loss: 0.00003414
Iteration 60/1000 | Loss: 0.00003414
Iteration 61/1000 | Loss: 0.00003413
Iteration 62/1000 | Loss: 0.00003412
Iteration 63/1000 | Loss: 0.00003411
Iteration 64/1000 | Loss: 0.00003411
Iteration 65/1000 | Loss: 0.00003411
Iteration 66/1000 | Loss: 0.00003410
Iteration 67/1000 | Loss: 0.00003410
Iteration 68/1000 | Loss: 0.00003410
Iteration 69/1000 | Loss: 0.00003410
Iteration 70/1000 | Loss: 0.00003409
Iteration 71/1000 | Loss: 0.00003409
Iteration 72/1000 | Loss: 0.00003409
Iteration 73/1000 | Loss: 0.00003408
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003408
Iteration 76/1000 | Loss: 0.00003408
Iteration 77/1000 | Loss: 0.00003407
Iteration 78/1000 | Loss: 0.00003407
Iteration 79/1000 | Loss: 0.00003407
Iteration 80/1000 | Loss: 0.00003407
Iteration 81/1000 | Loss: 0.00003407
Iteration 82/1000 | Loss: 0.00003407
Iteration 83/1000 | Loss: 0.00003407
Iteration 84/1000 | Loss: 0.00003407
Iteration 85/1000 | Loss: 0.00003407
Iteration 86/1000 | Loss: 0.00003407
Iteration 87/1000 | Loss: 0.00003406
Iteration 88/1000 | Loss: 0.00003406
Iteration 89/1000 | Loss: 0.00003406
Iteration 90/1000 | Loss: 0.00003406
Iteration 91/1000 | Loss: 0.00003406
Iteration 92/1000 | Loss: 0.00003406
Iteration 93/1000 | Loss: 0.00003406
Iteration 94/1000 | Loss: 0.00003405
Iteration 95/1000 | Loss: 0.00003405
Iteration 96/1000 | Loss: 0.00003405
Iteration 97/1000 | Loss: 0.00003405
Iteration 98/1000 | Loss: 0.00003405
Iteration 99/1000 | Loss: 0.00003405
Iteration 100/1000 | Loss: 0.00003405
Iteration 101/1000 | Loss: 0.00003405
Iteration 102/1000 | Loss: 0.00003405
Iteration 103/1000 | Loss: 0.00003405
Iteration 104/1000 | Loss: 0.00003405
Iteration 105/1000 | Loss: 0.00003405
Iteration 106/1000 | Loss: 0.00003405
Iteration 107/1000 | Loss: 0.00003405
Iteration 108/1000 | Loss: 0.00003405
Iteration 109/1000 | Loss: 0.00003405
Iteration 110/1000 | Loss: 0.00003405
Iteration 111/1000 | Loss: 0.00003405
Iteration 112/1000 | Loss: 0.00003405
Iteration 113/1000 | Loss: 0.00003405
Iteration 114/1000 | Loss: 0.00003405
Iteration 115/1000 | Loss: 0.00003405
Iteration 116/1000 | Loss: 0.00003405
Iteration 117/1000 | Loss: 0.00003405
Iteration 118/1000 | Loss: 0.00003405
Iteration 119/1000 | Loss: 0.00003405
Iteration 120/1000 | Loss: 0.00003405
Iteration 121/1000 | Loss: 0.00003405
Iteration 122/1000 | Loss: 0.00003405
Iteration 123/1000 | Loss: 0.00003405
Iteration 124/1000 | Loss: 0.00003405
Iteration 125/1000 | Loss: 0.00003405
Iteration 126/1000 | Loss: 0.00003405
Iteration 127/1000 | Loss: 0.00003405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [3.4048789530061185e-05, 3.4048789530061185e-05, 3.4048789530061185e-05, 3.4048789530061185e-05, 3.4048789530061185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4048789530061185e-05

Optimization complete. Final v2v error: 4.607421398162842 mm

Highest mean error: 5.674406051635742 mm for frame 80

Lowest mean error: 4.034666538238525 mm for frame 61

Saving results

Total time: 56.506317377090454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928634
Iteration 2/25 | Loss: 0.00133197
Iteration 3/25 | Loss: 0.00124662
Iteration 4/25 | Loss: 0.00123356
Iteration 5/25 | Loss: 0.00122981
Iteration 6/25 | Loss: 0.00122898
Iteration 7/25 | Loss: 0.00122898
Iteration 8/25 | Loss: 0.00122898
Iteration 9/25 | Loss: 0.00122898
Iteration 10/25 | Loss: 0.00122898
Iteration 11/25 | Loss: 0.00122898
Iteration 12/25 | Loss: 0.00122898
Iteration 13/25 | Loss: 0.00122898
Iteration 14/25 | Loss: 0.00122898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012289808364585042, 0.0012289808364585042, 0.0012289808364585042, 0.0012289808364585042, 0.0012289808364585042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012289808364585042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25654757
Iteration 2/25 | Loss: 0.00111435
Iteration 3/25 | Loss: 0.00111428
Iteration 4/25 | Loss: 0.00111427
Iteration 5/25 | Loss: 0.00111427
Iteration 6/25 | Loss: 0.00111427
Iteration 7/25 | Loss: 0.00111427
Iteration 8/25 | Loss: 0.00111427
Iteration 9/25 | Loss: 0.00111427
Iteration 10/25 | Loss: 0.00111427
Iteration 11/25 | Loss: 0.00111427
Iteration 12/25 | Loss: 0.00111427
Iteration 13/25 | Loss: 0.00111427
Iteration 14/25 | Loss: 0.00111427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001114272978156805, 0.001114272978156805, 0.001114272978156805, 0.001114272978156805, 0.001114272978156805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001114272978156805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111427
Iteration 2/1000 | Loss: 0.00002448
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001463
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001366
Iteration 9/1000 | Loss: 0.00001365
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001325
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001301
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001300
Iteration 26/1000 | Loss: 0.00001300
Iteration 27/1000 | Loss: 0.00001300
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001295
Iteration 41/1000 | Loss: 0.00001295
Iteration 42/1000 | Loss: 0.00001295
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001293
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001292
Iteration 57/1000 | Loss: 0.00001292
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001288
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001286
Iteration 104/1000 | Loss: 0.00001286
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001286
Iteration 107/1000 | Loss: 0.00001286
Iteration 108/1000 | Loss: 0.00001286
Iteration 109/1000 | Loss: 0.00001286
Iteration 110/1000 | Loss: 0.00001286
Iteration 111/1000 | Loss: 0.00001286
Iteration 112/1000 | Loss: 0.00001286
Iteration 113/1000 | Loss: 0.00001286
Iteration 114/1000 | Loss: 0.00001286
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001286
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001286
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001285
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001285
Iteration 131/1000 | Loss: 0.00001285
Iteration 132/1000 | Loss: 0.00001285
Iteration 133/1000 | Loss: 0.00001285
Iteration 134/1000 | Loss: 0.00001285
Iteration 135/1000 | Loss: 0.00001285
Iteration 136/1000 | Loss: 0.00001285
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001285
Iteration 139/1000 | Loss: 0.00001285
Iteration 140/1000 | Loss: 0.00001284
Iteration 141/1000 | Loss: 0.00001284
Iteration 142/1000 | Loss: 0.00001284
Iteration 143/1000 | Loss: 0.00001284
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001284
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001283
Iteration 158/1000 | Loss: 0.00001283
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001283
Iteration 163/1000 | Loss: 0.00001283
Iteration 164/1000 | Loss: 0.00001283
Iteration 165/1000 | Loss: 0.00001283
Iteration 166/1000 | Loss: 0.00001283
Iteration 167/1000 | Loss: 0.00001283
Iteration 168/1000 | Loss: 0.00001283
Iteration 169/1000 | Loss: 0.00001283
Iteration 170/1000 | Loss: 0.00001283
Iteration 171/1000 | Loss: 0.00001283
Iteration 172/1000 | Loss: 0.00001283
Iteration 173/1000 | Loss: 0.00001283
Iteration 174/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2826520105591044e-05, 1.2826520105591044e-05, 1.2826520105591044e-05, 1.2826520105591044e-05, 1.2826520105591044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2826520105591044e-05

Optimization complete. Final v2v error: 3.1230199337005615 mm

Highest mean error: 3.3852319717407227 mm for frame 207

Lowest mean error: 2.9223194122314453 mm for frame 59

Saving results

Total time: 37.7042121887207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066009
Iteration 2/25 | Loss: 0.00199765
Iteration 3/25 | Loss: 0.00176651
Iteration 4/25 | Loss: 0.00147135
Iteration 5/25 | Loss: 0.00133053
Iteration 6/25 | Loss: 0.00138529
Iteration 7/25 | Loss: 0.00147293
Iteration 8/25 | Loss: 0.00143027
Iteration 9/25 | Loss: 0.00129481
Iteration 10/25 | Loss: 0.00129529
Iteration 11/25 | Loss: 0.00127296
Iteration 12/25 | Loss: 0.00126048
Iteration 13/25 | Loss: 0.00129419
Iteration 14/25 | Loss: 0.00124956
Iteration 15/25 | Loss: 0.00123656
Iteration 16/25 | Loss: 0.00123436
Iteration 17/25 | Loss: 0.00123720
Iteration 18/25 | Loss: 0.00123552
Iteration 19/25 | Loss: 0.00123905
Iteration 20/25 | Loss: 0.00123170
Iteration 21/25 | Loss: 0.00123130
Iteration 22/25 | Loss: 0.00123281
Iteration 23/25 | Loss: 0.00123251
Iteration 24/25 | Loss: 0.00123310
Iteration 25/25 | Loss: 0.00122821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28465021
Iteration 2/25 | Loss: 0.00186699
Iteration 3/25 | Loss: 0.00186699
Iteration 4/25 | Loss: 0.00186699
Iteration 5/25 | Loss: 0.00186699
Iteration 6/25 | Loss: 0.00186699
Iteration 7/25 | Loss: 0.00186699
Iteration 8/25 | Loss: 0.00186699
Iteration 9/25 | Loss: 0.00186699
Iteration 10/25 | Loss: 0.00186699
Iteration 11/25 | Loss: 0.00186699
Iteration 12/25 | Loss: 0.00186699
Iteration 13/25 | Loss: 0.00186699
Iteration 14/25 | Loss: 0.00186699
Iteration 15/25 | Loss: 0.00186699
Iteration 16/25 | Loss: 0.00186699
Iteration 17/25 | Loss: 0.00186699
Iteration 18/25 | Loss: 0.00186699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018669896526262164, 0.0018669896526262164, 0.0018669896526262164, 0.0018669896526262164, 0.0018669896526262164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018669896526262164

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186699
Iteration 2/1000 | Loss: 0.00024143
Iteration 3/1000 | Loss: 0.00234115
Iteration 4/1000 | Loss: 0.00028610
Iteration 5/1000 | Loss: 0.00095860
Iteration 6/1000 | Loss: 0.00194452
Iteration 7/1000 | Loss: 0.00151855
Iteration 8/1000 | Loss: 0.00107785
Iteration 9/1000 | Loss: 0.00093523
Iteration 10/1000 | Loss: 0.00323819
Iteration 11/1000 | Loss: 0.00079731
Iteration 12/1000 | Loss: 0.00099762
Iteration 13/1000 | Loss: 0.00091718
Iteration 14/1000 | Loss: 0.00127006
Iteration 15/1000 | Loss: 0.00109266
Iteration 16/1000 | Loss: 0.00089763
Iteration 17/1000 | Loss: 0.00077443
Iteration 18/1000 | Loss: 0.00135462
Iteration 19/1000 | Loss: 0.00072397
Iteration 20/1000 | Loss: 0.00073532
Iteration 21/1000 | Loss: 0.00118510
Iteration 22/1000 | Loss: 0.00091625
Iteration 23/1000 | Loss: 0.00104240
Iteration 24/1000 | Loss: 0.00094061
Iteration 25/1000 | Loss: 0.00261027
Iteration 26/1000 | Loss: 0.00107492
Iteration 27/1000 | Loss: 0.00080776
Iteration 28/1000 | Loss: 0.00074781
Iteration 29/1000 | Loss: 0.00066738
Iteration 30/1000 | Loss: 0.00073808
Iteration 31/1000 | Loss: 0.00106579
Iteration 32/1000 | Loss: 0.00085405
Iteration 33/1000 | Loss: 0.00173411
Iteration 34/1000 | Loss: 0.00084938
Iteration 35/1000 | Loss: 0.00149263
Iteration 36/1000 | Loss: 0.00144622
Iteration 37/1000 | Loss: 0.00106376
Iteration 38/1000 | Loss: 0.00129081
Iteration 39/1000 | Loss: 0.00186018
Iteration 40/1000 | Loss: 0.00085276
Iteration 41/1000 | Loss: 0.00062178
Iteration 42/1000 | Loss: 0.00034135
Iteration 43/1000 | Loss: 0.00060483
Iteration 44/1000 | Loss: 0.00031193
Iteration 45/1000 | Loss: 0.00044268
Iteration 46/1000 | Loss: 0.00026314
Iteration 47/1000 | Loss: 0.00013615
Iteration 48/1000 | Loss: 0.00021124
Iteration 49/1000 | Loss: 0.00021730
Iteration 50/1000 | Loss: 0.00018154
Iteration 51/1000 | Loss: 0.00032955
Iteration 52/1000 | Loss: 0.00033015
Iteration 53/1000 | Loss: 0.00035348
Iteration 54/1000 | Loss: 0.00023039
Iteration 55/1000 | Loss: 0.00019631
Iteration 56/1000 | Loss: 0.00022417
Iteration 57/1000 | Loss: 0.00015782
Iteration 58/1000 | Loss: 0.00030496
Iteration 59/1000 | Loss: 0.00024501
Iteration 60/1000 | Loss: 0.00029627
Iteration 61/1000 | Loss: 0.00038521
Iteration 62/1000 | Loss: 0.00022187
Iteration 63/1000 | Loss: 0.00047214
Iteration 64/1000 | Loss: 0.00018520
Iteration 65/1000 | Loss: 0.00009476
Iteration 66/1000 | Loss: 0.00023378
Iteration 67/1000 | Loss: 0.00010918
Iteration 68/1000 | Loss: 0.00014757
Iteration 69/1000 | Loss: 0.00015591
Iteration 70/1000 | Loss: 0.00006712
Iteration 71/1000 | Loss: 0.00018209
Iteration 72/1000 | Loss: 0.00021448
Iteration 73/1000 | Loss: 0.00014844
Iteration 74/1000 | Loss: 0.00006240
Iteration 75/1000 | Loss: 0.00008321
Iteration 76/1000 | Loss: 0.00017087
Iteration 77/1000 | Loss: 0.00022242
Iteration 78/1000 | Loss: 0.00017185
Iteration 79/1000 | Loss: 0.00015311
Iteration 80/1000 | Loss: 0.00017696
Iteration 81/1000 | Loss: 0.00016524
Iteration 82/1000 | Loss: 0.00016347
Iteration 83/1000 | Loss: 0.00014544
Iteration 84/1000 | Loss: 0.00015307
Iteration 85/1000 | Loss: 0.00015318
Iteration 86/1000 | Loss: 0.00013397
Iteration 87/1000 | Loss: 0.00015201
Iteration 88/1000 | Loss: 0.00004738
Iteration 89/1000 | Loss: 0.00003580
Iteration 90/1000 | Loss: 0.00003362
Iteration 91/1000 | Loss: 0.00026327
Iteration 92/1000 | Loss: 0.00014977
Iteration 93/1000 | Loss: 0.00018930
Iteration 94/1000 | Loss: 0.00015112
Iteration 95/1000 | Loss: 0.00004131
Iteration 96/1000 | Loss: 0.00003602
Iteration 97/1000 | Loss: 0.00018698
Iteration 98/1000 | Loss: 0.00004188
Iteration 99/1000 | Loss: 0.00010075
Iteration 100/1000 | Loss: 0.00003889
Iteration 101/1000 | Loss: 0.00011760
Iteration 102/1000 | Loss: 0.00004115
Iteration 103/1000 | Loss: 0.00005943
Iteration 104/1000 | Loss: 0.00006784
Iteration 105/1000 | Loss: 0.00005481
Iteration 106/1000 | Loss: 0.00004064
Iteration 107/1000 | Loss: 0.00005214
Iteration 108/1000 | Loss: 0.00003623
Iteration 109/1000 | Loss: 0.00004348
Iteration 110/1000 | Loss: 0.00004549
Iteration 111/1000 | Loss: 0.00004085
Iteration 112/1000 | Loss: 0.00003539
Iteration 113/1000 | Loss: 0.00004885
Iteration 114/1000 | Loss: 0.00004896
Iteration 115/1000 | Loss: 0.00004864
Iteration 116/1000 | Loss: 0.00006939
Iteration 117/1000 | Loss: 0.00004259
Iteration 118/1000 | Loss: 0.00005509
Iteration 119/1000 | Loss: 0.00004026
Iteration 120/1000 | Loss: 0.00005431
Iteration 121/1000 | Loss: 0.00006237
Iteration 122/1000 | Loss: 0.00006501
Iteration 123/1000 | Loss: 0.00006119
Iteration 124/1000 | Loss: 0.00005876
Iteration 125/1000 | Loss: 0.00005191
Iteration 126/1000 | Loss: 0.00004773
Iteration 127/1000 | Loss: 0.00005480
Iteration 128/1000 | Loss: 0.00005407
Iteration 129/1000 | Loss: 0.00005115
Iteration 130/1000 | Loss: 0.00004906
Iteration 131/1000 | Loss: 0.00005078
Iteration 132/1000 | Loss: 0.00005930
Iteration 133/1000 | Loss: 0.00006085
Iteration 134/1000 | Loss: 0.00004643
Iteration 135/1000 | Loss: 0.00004702
Iteration 136/1000 | Loss: 0.00005689
Iteration 137/1000 | Loss: 0.00004031
Iteration 138/1000 | Loss: 0.00019048
Iteration 139/1000 | Loss: 0.00004505
Iteration 140/1000 | Loss: 0.00003384
Iteration 141/1000 | Loss: 0.00002846
Iteration 142/1000 | Loss: 0.00002638
Iteration 143/1000 | Loss: 0.00002561
Iteration 144/1000 | Loss: 0.00002503
Iteration 145/1000 | Loss: 0.00002573
Iteration 146/1000 | Loss: 0.00003683
Iteration 147/1000 | Loss: 0.00004254
Iteration 148/1000 | Loss: 0.00002861
Iteration 149/1000 | Loss: 0.00003579
Iteration 150/1000 | Loss: 0.00003543
Iteration 151/1000 | Loss: 0.00003470
Iteration 152/1000 | Loss: 0.00003950
Iteration 153/1000 | Loss: 0.00004035
Iteration 154/1000 | Loss: 0.00003701
Iteration 155/1000 | Loss: 0.00003745
Iteration 156/1000 | Loss: 0.00004630
Iteration 157/1000 | Loss: 0.00003711
Iteration 158/1000 | Loss: 0.00003169
Iteration 159/1000 | Loss: 0.00003678
Iteration 160/1000 | Loss: 0.00003915
Iteration 161/1000 | Loss: 0.00003617
Iteration 162/1000 | Loss: 0.00003747
Iteration 163/1000 | Loss: 0.00003913
Iteration 164/1000 | Loss: 0.00004285
Iteration 165/1000 | Loss: 0.00004074
Iteration 166/1000 | Loss: 0.00003619
Iteration 167/1000 | Loss: 0.00004785
Iteration 168/1000 | Loss: 0.00004705
Iteration 169/1000 | Loss: 0.00004881
Iteration 170/1000 | Loss: 0.00004351
Iteration 171/1000 | Loss: 0.00004220
Iteration 172/1000 | Loss: 0.00004599
Iteration 173/1000 | Loss: 0.00004602
Iteration 174/1000 | Loss: 0.00003815
Iteration 175/1000 | Loss: 0.00003081
Iteration 176/1000 | Loss: 0.00004920
Iteration 177/1000 | Loss: 0.00003612
Iteration 178/1000 | Loss: 0.00003168
Iteration 179/1000 | Loss: 0.00004388
Iteration 180/1000 | Loss: 0.00004344
Iteration 181/1000 | Loss: 0.00004091
Iteration 182/1000 | Loss: 0.00003773
Iteration 183/1000 | Loss: 0.00004155
Iteration 184/1000 | Loss: 0.00004337
Iteration 185/1000 | Loss: 0.00004210
Iteration 186/1000 | Loss: 0.00004515
Iteration 187/1000 | Loss: 0.00003251
Iteration 188/1000 | Loss: 0.00004439
Iteration 189/1000 | Loss: 0.00002812
Iteration 190/1000 | Loss: 0.00003059
Iteration 191/1000 | Loss: 0.00002778
Iteration 192/1000 | Loss: 0.00002990
Iteration 193/1000 | Loss: 0.00002592
Iteration 194/1000 | Loss: 0.00002698
Iteration 195/1000 | Loss: 0.00003109
Iteration 196/1000 | Loss: 0.00003760
Iteration 197/1000 | Loss: 0.00005443
Iteration 198/1000 | Loss: 0.00003251
Iteration 199/1000 | Loss: 0.00003098
Iteration 200/1000 | Loss: 0.00003867
Iteration 201/1000 | Loss: 0.00004154
Iteration 202/1000 | Loss: 0.00003214
Iteration 203/1000 | Loss: 0.00002623
Iteration 204/1000 | Loss: 0.00003762
Iteration 205/1000 | Loss: 0.00006036
Iteration 206/1000 | Loss: 0.00004910
Iteration 207/1000 | Loss: 0.00004079
Iteration 208/1000 | Loss: 0.00003769
Iteration 209/1000 | Loss: 0.00006166
Iteration 210/1000 | Loss: 0.00003467
Iteration 211/1000 | Loss: 0.00005572
Iteration 212/1000 | Loss: 0.00003315
Iteration 213/1000 | Loss: 0.00003787
Iteration 214/1000 | Loss: 0.00003900
Iteration 215/1000 | Loss: 0.00003948
Iteration 216/1000 | Loss: 0.00003854
Iteration 217/1000 | Loss: 0.00003912
Iteration 218/1000 | Loss: 0.00005204
Iteration 219/1000 | Loss: 0.00003796
Iteration 220/1000 | Loss: 0.00003569
Iteration 221/1000 | Loss: 0.00003131
Iteration 222/1000 | Loss: 0.00004589
Iteration 223/1000 | Loss: 0.00003965
Iteration 224/1000 | Loss: 0.00004756
Iteration 225/1000 | Loss: 0.00003006
Iteration 226/1000 | Loss: 0.00002417
Iteration 227/1000 | Loss: 0.00002233
Iteration 228/1000 | Loss: 0.00002103
Iteration 229/1000 | Loss: 0.00002044
Iteration 230/1000 | Loss: 0.00002010
Iteration 231/1000 | Loss: 0.00001986
Iteration 232/1000 | Loss: 0.00001961
Iteration 233/1000 | Loss: 0.00001958
Iteration 234/1000 | Loss: 0.00001934
Iteration 235/1000 | Loss: 0.00001931
Iteration 236/1000 | Loss: 0.00001923
Iteration 237/1000 | Loss: 0.00001916
Iteration 238/1000 | Loss: 0.00001913
Iteration 239/1000 | Loss: 0.00001910
Iteration 240/1000 | Loss: 0.00001908
Iteration 241/1000 | Loss: 0.00001908
Iteration 242/1000 | Loss: 0.00001907
Iteration 243/1000 | Loss: 0.00001905
Iteration 244/1000 | Loss: 0.00001904
Iteration 245/1000 | Loss: 0.00001903
Iteration 246/1000 | Loss: 0.00001903
Iteration 247/1000 | Loss: 0.00001903
Iteration 248/1000 | Loss: 0.00001902
Iteration 249/1000 | Loss: 0.00001901
Iteration 250/1000 | Loss: 0.00001901
Iteration 251/1000 | Loss: 0.00001901
Iteration 252/1000 | Loss: 0.00001901
Iteration 253/1000 | Loss: 0.00001900
Iteration 254/1000 | Loss: 0.00001900
Iteration 255/1000 | Loss: 0.00001900
Iteration 256/1000 | Loss: 0.00001900
Iteration 257/1000 | Loss: 0.00001900
Iteration 258/1000 | Loss: 0.00001900
Iteration 259/1000 | Loss: 0.00001900
Iteration 260/1000 | Loss: 0.00001900
Iteration 261/1000 | Loss: 0.00001900
Iteration 262/1000 | Loss: 0.00001900
Iteration 263/1000 | Loss: 0.00001900
Iteration 264/1000 | Loss: 0.00001900
Iteration 265/1000 | Loss: 0.00001900
Iteration 266/1000 | Loss: 0.00001899
Iteration 267/1000 | Loss: 0.00001899
Iteration 268/1000 | Loss: 0.00001899
Iteration 269/1000 | Loss: 0.00001899
Iteration 270/1000 | Loss: 0.00001899
Iteration 271/1000 | Loss: 0.00001898
Iteration 272/1000 | Loss: 0.00001898
Iteration 273/1000 | Loss: 0.00001898
Iteration 274/1000 | Loss: 0.00001898
Iteration 275/1000 | Loss: 0.00001898
Iteration 276/1000 | Loss: 0.00001898
Iteration 277/1000 | Loss: 0.00001898
Iteration 278/1000 | Loss: 0.00001898
Iteration 279/1000 | Loss: 0.00001897
Iteration 280/1000 | Loss: 0.00001897
Iteration 281/1000 | Loss: 0.00001897
Iteration 282/1000 | Loss: 0.00001896
Iteration 283/1000 | Loss: 0.00001896
Iteration 284/1000 | Loss: 0.00001896
Iteration 285/1000 | Loss: 0.00001896
Iteration 286/1000 | Loss: 0.00001895
Iteration 287/1000 | Loss: 0.00001895
Iteration 288/1000 | Loss: 0.00001895
Iteration 289/1000 | Loss: 0.00001895
Iteration 290/1000 | Loss: 0.00001895
Iteration 291/1000 | Loss: 0.00001895
Iteration 292/1000 | Loss: 0.00001895
Iteration 293/1000 | Loss: 0.00001895
Iteration 294/1000 | Loss: 0.00001895
Iteration 295/1000 | Loss: 0.00001895
Iteration 296/1000 | Loss: 0.00001895
Iteration 297/1000 | Loss: 0.00001895
Iteration 298/1000 | Loss: 0.00001895
Iteration 299/1000 | Loss: 0.00001895
Iteration 300/1000 | Loss: 0.00001895
Iteration 301/1000 | Loss: 0.00001895
Iteration 302/1000 | Loss: 0.00001895
Iteration 303/1000 | Loss: 0.00001895
Iteration 304/1000 | Loss: 0.00001895
Iteration 305/1000 | Loss: 0.00001895
Iteration 306/1000 | Loss: 0.00001895
Iteration 307/1000 | Loss: 0.00001895
Iteration 308/1000 | Loss: 0.00001895
Iteration 309/1000 | Loss: 0.00001895
Iteration 310/1000 | Loss: 0.00001895
Iteration 311/1000 | Loss: 0.00001895
Iteration 312/1000 | Loss: 0.00001895
Iteration 313/1000 | Loss: 0.00001895
Iteration 314/1000 | Loss: 0.00001895
Iteration 315/1000 | Loss: 0.00001895
Iteration 316/1000 | Loss: 0.00001895
Iteration 317/1000 | Loss: 0.00001895
Iteration 318/1000 | Loss: 0.00001895
Iteration 319/1000 | Loss: 0.00001895
Iteration 320/1000 | Loss: 0.00001895
Iteration 321/1000 | Loss: 0.00001895
Iteration 322/1000 | Loss: 0.00001895
Iteration 323/1000 | Loss: 0.00001895
Iteration 324/1000 | Loss: 0.00001895
Iteration 325/1000 | Loss: 0.00001895
Iteration 326/1000 | Loss: 0.00001895
Iteration 327/1000 | Loss: 0.00001895
Iteration 328/1000 | Loss: 0.00001895
Iteration 329/1000 | Loss: 0.00001895
Iteration 330/1000 | Loss: 0.00001895
Iteration 331/1000 | Loss: 0.00001895
Iteration 332/1000 | Loss: 0.00001895
Iteration 333/1000 | Loss: 0.00001895
Iteration 334/1000 | Loss: 0.00001895
Iteration 335/1000 | Loss: 0.00001895
Iteration 336/1000 | Loss: 0.00001895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [1.8946164345834404e-05, 1.8946164345834404e-05, 1.8946164345834404e-05, 1.8946164345834404e-05, 1.8946164345834404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8946164345834404e-05

Optimization complete. Final v2v error: 3.662907600402832 mm

Highest mean error: 9.322580337524414 mm for frame 86

Lowest mean error: 3.0467653274536133 mm for frame 20

Saving results

Total time: 361.9655909538269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456847
Iteration 2/25 | Loss: 0.00129216
Iteration 3/25 | Loss: 0.00121550
Iteration 4/25 | Loss: 0.00120250
Iteration 5/25 | Loss: 0.00119844
Iteration 6/25 | Loss: 0.00119748
Iteration 7/25 | Loss: 0.00119748
Iteration 8/25 | Loss: 0.00119748
Iteration 9/25 | Loss: 0.00119748
Iteration 10/25 | Loss: 0.00119748
Iteration 11/25 | Loss: 0.00119748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011974789667874575, 0.0011974789667874575, 0.0011974789667874575, 0.0011974789667874575, 0.0011974789667874575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011974789667874575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27157640
Iteration 2/25 | Loss: 0.00120437
Iteration 3/25 | Loss: 0.00120437
Iteration 4/25 | Loss: 0.00120437
Iteration 5/25 | Loss: 0.00120437
Iteration 6/25 | Loss: 0.00120437
Iteration 7/25 | Loss: 0.00120437
Iteration 8/25 | Loss: 0.00120437
Iteration 9/25 | Loss: 0.00120437
Iteration 10/25 | Loss: 0.00120437
Iteration 11/25 | Loss: 0.00120437
Iteration 12/25 | Loss: 0.00120437
Iteration 13/25 | Loss: 0.00120437
Iteration 14/25 | Loss: 0.00120437
Iteration 15/25 | Loss: 0.00120437
Iteration 16/25 | Loss: 0.00120437
Iteration 17/25 | Loss: 0.00120437
Iteration 18/25 | Loss: 0.00120437
Iteration 19/25 | Loss: 0.00120437
Iteration 20/25 | Loss: 0.00120437
Iteration 21/25 | Loss: 0.00120437
Iteration 22/25 | Loss: 0.00120437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012043672613799572, 0.0012043672613799572, 0.0012043672613799572, 0.0012043672613799572, 0.0012043672613799572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012043672613799572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120437
Iteration 2/1000 | Loss: 0.00004050
Iteration 3/1000 | Loss: 0.00002542
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00002022
Iteration 7/1000 | Loss: 0.00001984
Iteration 8/1000 | Loss: 0.00001952
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001895
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001880
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001879
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001878
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001873
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001872
Iteration 61/1000 | Loss: 0.00001872
Iteration 62/1000 | Loss: 0.00001871
Iteration 63/1000 | Loss: 0.00001871
Iteration 64/1000 | Loss: 0.00001871
Iteration 65/1000 | Loss: 0.00001871
Iteration 66/1000 | Loss: 0.00001870
Iteration 67/1000 | Loss: 0.00001870
Iteration 68/1000 | Loss: 0.00001870
Iteration 69/1000 | Loss: 0.00001869
Iteration 70/1000 | Loss: 0.00001869
Iteration 71/1000 | Loss: 0.00001869
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001865
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001865
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001862
Iteration 94/1000 | Loss: 0.00001862
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001861
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00001861
Iteration 121/1000 | Loss: 0.00001861
Iteration 122/1000 | Loss: 0.00001861
Iteration 123/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.8614546206663363e-05, 1.8614546206663363e-05, 1.8614546206663363e-05, 1.8614546206663363e-05, 1.8614546206663363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8614546206663363e-05

Optimization complete. Final v2v error: 3.649348020553589 mm

Highest mean error: 3.916727304458618 mm for frame 87

Lowest mean error: 3.338428020477295 mm for frame 0

Saving results

Total time: 32.212942361831665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_1262/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_1262/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823436
Iteration 2/25 | Loss: 0.00174522
Iteration 3/25 | Loss: 0.00138745
Iteration 4/25 | Loss: 0.00134119
Iteration 5/25 | Loss: 0.00135264
Iteration 6/25 | Loss: 0.00130234
Iteration 7/25 | Loss: 0.00131046
Iteration 8/25 | Loss: 0.00133206
Iteration 9/25 | Loss: 0.00130994
Iteration 10/25 | Loss: 0.00127632
Iteration 11/25 | Loss: 0.00127676
Iteration 12/25 | Loss: 0.00127107
Iteration 13/25 | Loss: 0.00126878
Iteration 14/25 | Loss: 0.00126647
Iteration 15/25 | Loss: 0.00126621
Iteration 16/25 | Loss: 0.00126605
Iteration 17/25 | Loss: 0.00126589
Iteration 18/25 | Loss: 0.00126580
Iteration 19/25 | Loss: 0.00126576
Iteration 20/25 | Loss: 0.00126568
Iteration 21/25 | Loss: 0.00126565
Iteration 22/25 | Loss: 0.00126565
Iteration 23/25 | Loss: 0.00126565
Iteration 24/25 | Loss: 0.00126565
Iteration 25/25 | Loss: 0.00126564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42228413
Iteration 2/25 | Loss: 0.00085374
Iteration 3/25 | Loss: 0.00085370
Iteration 4/25 | Loss: 0.00085370
Iteration 5/25 | Loss: 0.00085370
Iteration 6/25 | Loss: 0.00085370
Iteration 7/25 | Loss: 0.00085370
Iteration 8/25 | Loss: 0.00085370
Iteration 9/25 | Loss: 0.00085370
Iteration 10/25 | Loss: 0.00085370
Iteration 11/25 | Loss: 0.00085370
Iteration 12/25 | Loss: 0.00085370
Iteration 13/25 | Loss: 0.00085370
Iteration 14/25 | Loss: 0.00085370
Iteration 15/25 | Loss: 0.00085370
Iteration 16/25 | Loss: 0.00085370
Iteration 17/25 | Loss: 0.00085370
Iteration 18/25 | Loss: 0.00085370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008536987588740885, 0.0008536987588740885, 0.0008536987588740885, 0.0008536987588740885, 0.0008536987588740885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008536987588740885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085370
Iteration 2/1000 | Loss: 0.00003842
Iteration 3/1000 | Loss: 0.00002429
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001862
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00014545
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001572
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001559
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001558
Iteration 18/1000 | Loss: 0.00001556
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001554
Iteration 21/1000 | Loss: 0.00001552
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001548
Iteration 26/1000 | Loss: 0.00001548
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001547
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001544
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001534
Iteration 46/1000 | Loss: 0.00001534
Iteration 47/1000 | Loss: 0.00001534
Iteration 48/1000 | Loss: 0.00001534
Iteration 49/1000 | Loss: 0.00001533
Iteration 50/1000 | Loss: 0.00001533
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001532
Iteration 53/1000 | Loss: 0.00001532
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001529
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001527
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001523
Iteration 76/1000 | Loss: 0.00001523
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001519
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001518
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.5161804185481742e-05, 1.5161804185481742e-05, 1.5161804185481742e-05, 1.5161804185481742e-05, 1.5161804185481742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5161804185481742e-05

Optimization complete. Final v2v error: 3.396315097808838 mm

Highest mean error: 4.374139785766602 mm for frame 10

Lowest mean error: 3.178084373474121 mm for frame 2

Saving results

Total time: 70.84237170219421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841884
Iteration 2/25 | Loss: 0.00141673
Iteration 3/25 | Loss: 0.00109307
Iteration 4/25 | Loss: 0.00107064
Iteration 5/25 | Loss: 0.00106391
Iteration 6/25 | Loss: 0.00106188
Iteration 7/25 | Loss: 0.00106188
Iteration 8/25 | Loss: 0.00106188
Iteration 9/25 | Loss: 0.00106188
Iteration 10/25 | Loss: 0.00106188
Iteration 11/25 | Loss: 0.00106188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010618842206895351, 0.0010618842206895351, 0.0010618842206895351, 0.0010618842206895351, 0.0010618842206895351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010618842206895351

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07807481
Iteration 2/25 | Loss: 0.00134404
Iteration 3/25 | Loss: 0.00134402
Iteration 4/25 | Loss: 0.00134401
Iteration 5/25 | Loss: 0.00134401
Iteration 6/25 | Loss: 0.00134401
Iteration 7/25 | Loss: 0.00134401
Iteration 8/25 | Loss: 0.00134401
Iteration 9/25 | Loss: 0.00134401
Iteration 10/25 | Loss: 0.00134401
Iteration 11/25 | Loss: 0.00134401
Iteration 12/25 | Loss: 0.00134401
Iteration 13/25 | Loss: 0.00134401
Iteration 14/25 | Loss: 0.00134401
Iteration 15/25 | Loss: 0.00134401
Iteration 16/25 | Loss: 0.00134401
Iteration 17/25 | Loss: 0.00134401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013440123293548822, 0.0013440123293548822, 0.0013440123293548822, 0.0013440123293548822, 0.0013440123293548822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013440123293548822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134401
Iteration 2/1000 | Loss: 0.00005061
Iteration 3/1000 | Loss: 0.00003131
Iteration 4/1000 | Loss: 0.00002473
Iteration 5/1000 | Loss: 0.00002238
Iteration 6/1000 | Loss: 0.00002114
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00001972
Iteration 9/1000 | Loss: 0.00001919
Iteration 10/1000 | Loss: 0.00001888
Iteration 11/1000 | Loss: 0.00001851
Iteration 12/1000 | Loss: 0.00001830
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001765
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001765
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001764
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001764
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001764
Iteration 32/1000 | Loss: 0.00001764
Iteration 33/1000 | Loss: 0.00001764
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001762
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001760
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001759
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001750
Iteration 67/1000 | Loss: 0.00001750
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001748
Iteration 77/1000 | Loss: 0.00001748
Iteration 78/1000 | Loss: 0.00001747
Iteration 79/1000 | Loss: 0.00001747
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001747
Iteration 82/1000 | Loss: 0.00001747
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001747
Iteration 86/1000 | Loss: 0.00001747
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001746
Iteration 91/1000 | Loss: 0.00001745
Iteration 92/1000 | Loss: 0.00001745
Iteration 93/1000 | Loss: 0.00001745
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001741
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001738
Iteration 102/1000 | Loss: 0.00001738
Iteration 103/1000 | Loss: 0.00001738
Iteration 104/1000 | Loss: 0.00001738
Iteration 105/1000 | Loss: 0.00001738
Iteration 106/1000 | Loss: 0.00001738
Iteration 107/1000 | Loss: 0.00001738
Iteration 108/1000 | Loss: 0.00001737
Iteration 109/1000 | Loss: 0.00001736
Iteration 110/1000 | Loss: 0.00001736
Iteration 111/1000 | Loss: 0.00001736
Iteration 112/1000 | Loss: 0.00001736
Iteration 113/1000 | Loss: 0.00001735
Iteration 114/1000 | Loss: 0.00001734
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001733
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001730
Iteration 130/1000 | Loss: 0.00001730
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001730
Iteration 137/1000 | Loss: 0.00001729
Iteration 138/1000 | Loss: 0.00001729
Iteration 139/1000 | Loss: 0.00001729
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001729
Iteration 144/1000 | Loss: 0.00001729
Iteration 145/1000 | Loss: 0.00001729
Iteration 146/1000 | Loss: 0.00001728
Iteration 147/1000 | Loss: 0.00001728
Iteration 148/1000 | Loss: 0.00001728
Iteration 149/1000 | Loss: 0.00001728
Iteration 150/1000 | Loss: 0.00001728
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Iteration 155/1000 | Loss: 0.00001727
Iteration 156/1000 | Loss: 0.00001727
Iteration 157/1000 | Loss: 0.00001727
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001727
Iteration 160/1000 | Loss: 0.00001727
Iteration 161/1000 | Loss: 0.00001727
Iteration 162/1000 | Loss: 0.00001727
Iteration 163/1000 | Loss: 0.00001727
Iteration 164/1000 | Loss: 0.00001727
Iteration 165/1000 | Loss: 0.00001727
Iteration 166/1000 | Loss: 0.00001727
Iteration 167/1000 | Loss: 0.00001727
Iteration 168/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.726912523736246e-05, 1.726912523736246e-05, 1.726912523736246e-05, 1.726912523736246e-05, 1.726912523736246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.726912523736246e-05

Optimization complete. Final v2v error: 3.2903316020965576 mm

Highest mean error: 4.571165561676025 mm for frame 83

Lowest mean error: 2.7102432250976562 mm for frame 200

Saving results

Total time: 49.32169723510742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856830
Iteration 2/25 | Loss: 0.00117320
Iteration 3/25 | Loss: 0.00102048
Iteration 4/25 | Loss: 0.00100421
Iteration 5/25 | Loss: 0.00100133
Iteration 6/25 | Loss: 0.00100066
Iteration 7/25 | Loss: 0.00100066
Iteration 8/25 | Loss: 0.00100066
Iteration 9/25 | Loss: 0.00100066
Iteration 10/25 | Loss: 0.00100066
Iteration 11/25 | Loss: 0.00100066
Iteration 12/25 | Loss: 0.00100066
Iteration 13/25 | Loss: 0.00100066
Iteration 14/25 | Loss: 0.00100066
Iteration 15/25 | Loss: 0.00100066
Iteration 16/25 | Loss: 0.00100066
Iteration 17/25 | Loss: 0.00100066
Iteration 18/25 | Loss: 0.00100066
Iteration 19/25 | Loss: 0.00100066
Iteration 20/25 | Loss: 0.00100066
Iteration 21/25 | Loss: 0.00100066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010006631491705775, 0.0010006631491705775, 0.0010006631491705775, 0.0010006631491705775, 0.0010006631491705775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010006631491705775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25955784
Iteration 2/25 | Loss: 0.00136645
Iteration 3/25 | Loss: 0.00136644
Iteration 4/25 | Loss: 0.00136644
Iteration 5/25 | Loss: 0.00136644
Iteration 6/25 | Loss: 0.00136644
Iteration 7/25 | Loss: 0.00136644
Iteration 8/25 | Loss: 0.00136644
Iteration 9/25 | Loss: 0.00136644
Iteration 10/25 | Loss: 0.00136644
Iteration 11/25 | Loss: 0.00136644
Iteration 12/25 | Loss: 0.00136644
Iteration 13/25 | Loss: 0.00136644
Iteration 14/25 | Loss: 0.00136644
Iteration 15/25 | Loss: 0.00136644
Iteration 16/25 | Loss: 0.00136644
Iteration 17/25 | Loss: 0.00136644
Iteration 18/25 | Loss: 0.00136644
Iteration 19/25 | Loss: 0.00136644
Iteration 20/25 | Loss: 0.00136644
Iteration 21/25 | Loss: 0.00136644
Iteration 22/25 | Loss: 0.00136644
Iteration 23/25 | Loss: 0.00136644
Iteration 24/25 | Loss: 0.00136644
Iteration 25/25 | Loss: 0.00136644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136644
Iteration 2/1000 | Loss: 0.00003103
Iteration 3/1000 | Loss: 0.00001375
Iteration 4/1000 | Loss: 0.00001173
Iteration 5/1000 | Loss: 0.00001088
Iteration 6/1000 | Loss: 0.00001036
Iteration 7/1000 | Loss: 0.00000994
Iteration 8/1000 | Loss: 0.00000969
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000961
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000952
Iteration 13/1000 | Loss: 0.00000946
Iteration 14/1000 | Loss: 0.00000933
Iteration 15/1000 | Loss: 0.00000929
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000925
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000924
Iteration 20/1000 | Loss: 0.00000923
Iteration 21/1000 | Loss: 0.00000923
Iteration 22/1000 | Loss: 0.00000922
Iteration 23/1000 | Loss: 0.00000921
Iteration 24/1000 | Loss: 0.00000921
Iteration 25/1000 | Loss: 0.00000920
Iteration 26/1000 | Loss: 0.00000920
Iteration 27/1000 | Loss: 0.00000919
Iteration 28/1000 | Loss: 0.00000919
Iteration 29/1000 | Loss: 0.00000918
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000917
Iteration 32/1000 | Loss: 0.00000916
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000915
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000914
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000913
Iteration 42/1000 | Loss: 0.00000912
Iteration 43/1000 | Loss: 0.00000912
Iteration 44/1000 | Loss: 0.00000911
Iteration 45/1000 | Loss: 0.00000911
Iteration 46/1000 | Loss: 0.00000911
Iteration 47/1000 | Loss: 0.00000911
Iteration 48/1000 | Loss: 0.00000910
Iteration 49/1000 | Loss: 0.00000910
Iteration 50/1000 | Loss: 0.00000910
Iteration 51/1000 | Loss: 0.00000909
Iteration 52/1000 | Loss: 0.00000909
Iteration 53/1000 | Loss: 0.00000909
Iteration 54/1000 | Loss: 0.00000908
Iteration 55/1000 | Loss: 0.00000907
Iteration 56/1000 | Loss: 0.00000907
Iteration 57/1000 | Loss: 0.00000907
Iteration 58/1000 | Loss: 0.00000907
Iteration 59/1000 | Loss: 0.00000906
Iteration 60/1000 | Loss: 0.00000906
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000905
Iteration 68/1000 | Loss: 0.00000905
Iteration 69/1000 | Loss: 0.00000904
Iteration 70/1000 | Loss: 0.00000904
Iteration 71/1000 | Loss: 0.00000904
Iteration 72/1000 | Loss: 0.00000903
Iteration 73/1000 | Loss: 0.00000903
Iteration 74/1000 | Loss: 0.00000903
Iteration 75/1000 | Loss: 0.00000903
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000902
Iteration 78/1000 | Loss: 0.00000902
Iteration 79/1000 | Loss: 0.00000902
Iteration 80/1000 | Loss: 0.00000902
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000901
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000901
Iteration 89/1000 | Loss: 0.00000901
Iteration 90/1000 | Loss: 0.00000901
Iteration 91/1000 | Loss: 0.00000901
Iteration 92/1000 | Loss: 0.00000901
Iteration 93/1000 | Loss: 0.00000901
Iteration 94/1000 | Loss: 0.00000901
Iteration 95/1000 | Loss: 0.00000901
Iteration 96/1000 | Loss: 0.00000900
Iteration 97/1000 | Loss: 0.00000900
Iteration 98/1000 | Loss: 0.00000900
Iteration 99/1000 | Loss: 0.00000900
Iteration 100/1000 | Loss: 0.00000900
Iteration 101/1000 | Loss: 0.00000900
Iteration 102/1000 | Loss: 0.00000900
Iteration 103/1000 | Loss: 0.00000900
Iteration 104/1000 | Loss: 0.00000900
Iteration 105/1000 | Loss: 0.00000900
Iteration 106/1000 | Loss: 0.00000900
Iteration 107/1000 | Loss: 0.00000900
Iteration 108/1000 | Loss: 0.00000900
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000899
Iteration 113/1000 | Loss: 0.00000899
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000898
Iteration 119/1000 | Loss: 0.00000898
Iteration 120/1000 | Loss: 0.00000898
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000898
Iteration 125/1000 | Loss: 0.00000898
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [8.983543011709116e-06, 8.983543011709116e-06, 8.983543011709116e-06, 8.983543011709116e-06, 8.983543011709116e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.983543011709116e-06

Optimization complete. Final v2v error: 2.4886088371276855 mm

Highest mean error: 2.7317099571228027 mm for frame 91

Lowest mean error: 2.2442750930786133 mm for frame 134

Saving results

Total time: 30.797613620758057
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047429
Iteration 2/25 | Loss: 0.00167181
Iteration 3/25 | Loss: 0.00130263
Iteration 4/25 | Loss: 0.00126023
Iteration 5/25 | Loss: 0.00125247
Iteration 6/25 | Loss: 0.00123190
Iteration 7/25 | Loss: 0.00121752
Iteration 8/25 | Loss: 0.00121146
Iteration 9/25 | Loss: 0.00120066
Iteration 10/25 | Loss: 0.00119056
Iteration 11/25 | Loss: 0.00118886
Iteration 12/25 | Loss: 0.00119148
Iteration 13/25 | Loss: 0.00118839
Iteration 14/25 | Loss: 0.00118454
Iteration 15/25 | Loss: 0.00118409
Iteration 16/25 | Loss: 0.00118199
Iteration 17/25 | Loss: 0.00117908
Iteration 18/25 | Loss: 0.00117836
Iteration 19/25 | Loss: 0.00117809
Iteration 20/25 | Loss: 0.00117796
Iteration 21/25 | Loss: 0.00117789
Iteration 22/25 | Loss: 0.00117788
Iteration 23/25 | Loss: 0.00117787
Iteration 24/25 | Loss: 0.00117781
Iteration 25/25 | Loss: 0.00117780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68383133
Iteration 2/25 | Loss: 0.00221139
Iteration 3/25 | Loss: 0.00211212
Iteration 4/25 | Loss: 0.00211212
Iteration 5/25 | Loss: 0.00211212
Iteration 6/25 | Loss: 0.00211212
Iteration 7/25 | Loss: 0.00211212
Iteration 8/25 | Loss: 0.00211212
Iteration 9/25 | Loss: 0.00211212
Iteration 10/25 | Loss: 0.00211212
Iteration 11/25 | Loss: 0.00211212
Iteration 12/25 | Loss: 0.00211212
Iteration 13/25 | Loss: 0.00211212
Iteration 14/25 | Loss: 0.00211212
Iteration 15/25 | Loss: 0.00211212
Iteration 16/25 | Loss: 0.00211212
Iteration 17/25 | Loss: 0.00211212
Iteration 18/25 | Loss: 0.00211212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002112117363139987, 0.002112117363139987, 0.002112117363139987, 0.002112117363139987, 0.002112117363139987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002112117363139987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211212
Iteration 2/1000 | Loss: 0.00069001
Iteration 3/1000 | Loss: 0.00017087
Iteration 4/1000 | Loss: 0.00013179
Iteration 5/1000 | Loss: 0.00011453
Iteration 6/1000 | Loss: 0.00032708
Iteration 7/1000 | Loss: 0.00106973
Iteration 8/1000 | Loss: 0.00097351
Iteration 9/1000 | Loss: 0.00052127
Iteration 10/1000 | Loss: 0.00010899
Iteration 11/1000 | Loss: 0.00049144
Iteration 12/1000 | Loss: 0.00040854
Iteration 13/1000 | Loss: 0.00055297
Iteration 14/1000 | Loss: 0.00034775
Iteration 15/1000 | Loss: 0.00023753
Iteration 16/1000 | Loss: 0.00040067
Iteration 17/1000 | Loss: 0.00011295
Iteration 18/1000 | Loss: 0.00031919
Iteration 19/1000 | Loss: 0.00007904
Iteration 20/1000 | Loss: 0.00007340
Iteration 21/1000 | Loss: 0.00026731
Iteration 22/1000 | Loss: 0.00010288
Iteration 23/1000 | Loss: 0.00023000
Iteration 24/1000 | Loss: 0.00073073
Iteration 25/1000 | Loss: 0.00027645
Iteration 26/1000 | Loss: 0.00015352
Iteration 27/1000 | Loss: 0.00073477
Iteration 28/1000 | Loss: 0.00014134
Iteration 29/1000 | Loss: 0.00035700
Iteration 30/1000 | Loss: 0.00052758
Iteration 31/1000 | Loss: 0.00018307
Iteration 32/1000 | Loss: 0.00030548
Iteration 33/1000 | Loss: 0.00077121
Iteration 34/1000 | Loss: 0.00020534
Iteration 35/1000 | Loss: 0.00015745
Iteration 36/1000 | Loss: 0.00007009
Iteration 37/1000 | Loss: 0.00033416
Iteration 38/1000 | Loss: 0.00035936
Iteration 39/1000 | Loss: 0.00007816
Iteration 40/1000 | Loss: 0.00006735
Iteration 41/1000 | Loss: 0.00056634
Iteration 42/1000 | Loss: 0.00067667
Iteration 43/1000 | Loss: 0.00038094
Iteration 44/1000 | Loss: 0.00023610
Iteration 45/1000 | Loss: 0.00006616
Iteration 46/1000 | Loss: 0.00016605
Iteration 47/1000 | Loss: 0.00014016
Iteration 48/1000 | Loss: 0.00014780
Iteration 49/1000 | Loss: 0.00016098
Iteration 50/1000 | Loss: 0.00005757
Iteration 51/1000 | Loss: 0.00005542
Iteration 52/1000 | Loss: 0.00005416
Iteration 53/1000 | Loss: 0.00005273
Iteration 54/1000 | Loss: 0.00005148
Iteration 55/1000 | Loss: 0.00105293
Iteration 56/1000 | Loss: 0.00340084
Iteration 57/1000 | Loss: 0.00053520
Iteration 58/1000 | Loss: 0.00024186
Iteration 59/1000 | Loss: 0.00023105
Iteration 60/1000 | Loss: 0.00013549
Iteration 61/1000 | Loss: 0.00006662
Iteration 62/1000 | Loss: 0.00019513
Iteration 63/1000 | Loss: 0.00006620
Iteration 64/1000 | Loss: 0.00008769
Iteration 65/1000 | Loss: 0.00005027
Iteration 66/1000 | Loss: 0.00007264
Iteration 67/1000 | Loss: 0.00004423
Iteration 68/1000 | Loss: 0.00014734
Iteration 69/1000 | Loss: 0.00027913
Iteration 70/1000 | Loss: 0.00028158
Iteration 71/1000 | Loss: 0.00024757
Iteration 72/1000 | Loss: 0.00022053
Iteration 73/1000 | Loss: 0.00008309
Iteration 74/1000 | Loss: 0.00007616
Iteration 75/1000 | Loss: 0.00007570
Iteration 76/1000 | Loss: 0.00007345
Iteration 77/1000 | Loss: 0.00013600
Iteration 78/1000 | Loss: 0.00007130
Iteration 79/1000 | Loss: 0.00007614
Iteration 80/1000 | Loss: 0.00003866
Iteration 81/1000 | Loss: 0.00003626
Iteration 82/1000 | Loss: 0.00003330
Iteration 83/1000 | Loss: 0.00013997
Iteration 84/1000 | Loss: 0.00026871
Iteration 85/1000 | Loss: 0.00042201
Iteration 86/1000 | Loss: 0.00017654
Iteration 87/1000 | Loss: 0.00022785
Iteration 88/1000 | Loss: 0.00024968
Iteration 89/1000 | Loss: 0.00043876
Iteration 90/1000 | Loss: 0.00004656
Iteration 91/1000 | Loss: 0.00003657
Iteration 92/1000 | Loss: 0.00003324
Iteration 93/1000 | Loss: 0.00003045
Iteration 94/1000 | Loss: 0.00002927
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002836
Iteration 97/1000 | Loss: 0.00002829
Iteration 98/1000 | Loss: 0.00002818
Iteration 99/1000 | Loss: 0.00002808
Iteration 100/1000 | Loss: 0.00002786
Iteration 101/1000 | Loss: 0.00002780
Iteration 102/1000 | Loss: 0.00002780
Iteration 103/1000 | Loss: 0.00002779
Iteration 104/1000 | Loss: 0.00002779
Iteration 105/1000 | Loss: 0.00002778
Iteration 106/1000 | Loss: 0.00002778
Iteration 107/1000 | Loss: 0.00002777
Iteration 108/1000 | Loss: 0.00002765
Iteration 109/1000 | Loss: 0.00002761
Iteration 110/1000 | Loss: 0.00002761
Iteration 111/1000 | Loss: 0.00002760
Iteration 112/1000 | Loss: 0.00002760
Iteration 113/1000 | Loss: 0.00002760
Iteration 114/1000 | Loss: 0.00002759
Iteration 115/1000 | Loss: 0.00002759
Iteration 116/1000 | Loss: 0.00002758
Iteration 117/1000 | Loss: 0.00002758
Iteration 118/1000 | Loss: 0.00002757
Iteration 119/1000 | Loss: 0.00002757
Iteration 120/1000 | Loss: 0.00002757
Iteration 121/1000 | Loss: 0.00002756
Iteration 122/1000 | Loss: 0.00002756
Iteration 123/1000 | Loss: 0.00002756
Iteration 124/1000 | Loss: 0.00002756
Iteration 125/1000 | Loss: 0.00002755
Iteration 126/1000 | Loss: 0.00002755
Iteration 127/1000 | Loss: 0.00002754
Iteration 128/1000 | Loss: 0.00002754
Iteration 129/1000 | Loss: 0.00002754
Iteration 130/1000 | Loss: 0.00002753
Iteration 131/1000 | Loss: 0.00002753
Iteration 132/1000 | Loss: 0.00002752
Iteration 133/1000 | Loss: 0.00002751
Iteration 134/1000 | Loss: 0.00002751
Iteration 135/1000 | Loss: 0.00002751
Iteration 136/1000 | Loss: 0.00002751
Iteration 137/1000 | Loss: 0.00002751
Iteration 138/1000 | Loss: 0.00002750
Iteration 139/1000 | Loss: 0.00002750
Iteration 140/1000 | Loss: 0.00002750
Iteration 141/1000 | Loss: 0.00002750
Iteration 142/1000 | Loss: 0.00002750
Iteration 143/1000 | Loss: 0.00002750
Iteration 144/1000 | Loss: 0.00002750
Iteration 145/1000 | Loss: 0.00002749
Iteration 146/1000 | Loss: 0.00002749
Iteration 147/1000 | Loss: 0.00002749
Iteration 148/1000 | Loss: 0.00002748
Iteration 149/1000 | Loss: 0.00002748
Iteration 150/1000 | Loss: 0.00002748
Iteration 151/1000 | Loss: 0.00002748
Iteration 152/1000 | Loss: 0.00002748
Iteration 153/1000 | Loss: 0.00002748
Iteration 154/1000 | Loss: 0.00002748
Iteration 155/1000 | Loss: 0.00002748
Iteration 156/1000 | Loss: 0.00002748
Iteration 157/1000 | Loss: 0.00002747
Iteration 158/1000 | Loss: 0.00002747
Iteration 159/1000 | Loss: 0.00002747
Iteration 160/1000 | Loss: 0.00002747
Iteration 161/1000 | Loss: 0.00002746
Iteration 162/1000 | Loss: 0.00002746
Iteration 163/1000 | Loss: 0.00002746
Iteration 164/1000 | Loss: 0.00002746
Iteration 165/1000 | Loss: 0.00002746
Iteration 166/1000 | Loss: 0.00002745
Iteration 167/1000 | Loss: 0.00002745
Iteration 168/1000 | Loss: 0.00002745
Iteration 169/1000 | Loss: 0.00002744
Iteration 170/1000 | Loss: 0.00002744
Iteration 171/1000 | Loss: 0.00002744
Iteration 172/1000 | Loss: 0.00002743
Iteration 173/1000 | Loss: 0.00002743
Iteration 174/1000 | Loss: 0.00002743
Iteration 175/1000 | Loss: 0.00002743
Iteration 176/1000 | Loss: 0.00002743
Iteration 177/1000 | Loss: 0.00002743
Iteration 178/1000 | Loss: 0.00002743
Iteration 179/1000 | Loss: 0.00002743
Iteration 180/1000 | Loss: 0.00002743
Iteration 181/1000 | Loss: 0.00002743
Iteration 182/1000 | Loss: 0.00002743
Iteration 183/1000 | Loss: 0.00002742
Iteration 184/1000 | Loss: 0.00002742
Iteration 185/1000 | Loss: 0.00002742
Iteration 186/1000 | Loss: 0.00002742
Iteration 187/1000 | Loss: 0.00002742
Iteration 188/1000 | Loss: 0.00002742
Iteration 189/1000 | Loss: 0.00002742
Iteration 190/1000 | Loss: 0.00002742
Iteration 191/1000 | Loss: 0.00002742
Iteration 192/1000 | Loss: 0.00002742
Iteration 193/1000 | Loss: 0.00002742
Iteration 194/1000 | Loss: 0.00002742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.7423917345004156e-05, 2.7423917345004156e-05, 2.7423917345004156e-05, 2.7423917345004156e-05, 2.7423917345004156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7423917345004156e-05

Optimization complete. Final v2v error: 3.9236507415771484 mm

Highest mean error: 13.44462776184082 mm for frame 149

Lowest mean error: 3.0895774364471436 mm for frame 14

Saving results

Total time: 208.95746898651123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620472
Iteration 2/25 | Loss: 0.00171993
Iteration 3/25 | Loss: 0.00127753
Iteration 4/25 | Loss: 0.00123424
Iteration 5/25 | Loss: 0.00122868
Iteration 6/25 | Loss: 0.00122805
Iteration 7/25 | Loss: 0.00122805
Iteration 8/25 | Loss: 0.00122805
Iteration 9/25 | Loss: 0.00122805
Iteration 10/25 | Loss: 0.00122805
Iteration 11/25 | Loss: 0.00122805
Iteration 12/25 | Loss: 0.00122805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012280464870855212, 0.0012280464870855212, 0.0012280464870855212, 0.0012280464870855212, 0.0012280464870855212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012280464870855212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22827280
Iteration 2/25 | Loss: 0.00108581
Iteration 3/25 | Loss: 0.00108578
Iteration 4/25 | Loss: 0.00108578
Iteration 5/25 | Loss: 0.00108578
Iteration 6/25 | Loss: 0.00108578
Iteration 7/25 | Loss: 0.00108578
Iteration 8/25 | Loss: 0.00108578
Iteration 9/25 | Loss: 0.00108578
Iteration 10/25 | Loss: 0.00108577
Iteration 11/25 | Loss: 0.00108577
Iteration 12/25 | Loss: 0.00108577
Iteration 13/25 | Loss: 0.00108577
Iteration 14/25 | Loss: 0.00108577
Iteration 15/25 | Loss: 0.00108577
Iteration 16/25 | Loss: 0.00108577
Iteration 17/25 | Loss: 0.00108577
Iteration 18/25 | Loss: 0.00108577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010857745073735714, 0.0010857745073735714, 0.0010857745073735714, 0.0010857745073735714, 0.0010857745073735714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010857745073735714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108577
Iteration 2/1000 | Loss: 0.00006719
Iteration 3/1000 | Loss: 0.00003680
Iteration 4/1000 | Loss: 0.00002690
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002181
Iteration 7/1000 | Loss: 0.00002039
Iteration 8/1000 | Loss: 0.00001970
Iteration 9/1000 | Loss: 0.00001892
Iteration 10/1000 | Loss: 0.00001852
Iteration 11/1000 | Loss: 0.00001819
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001782
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001762
Iteration 18/1000 | Loss: 0.00001762
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001761
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001758
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001752
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001746
Iteration 57/1000 | Loss: 0.00001746
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001743
Iteration 77/1000 | Loss: 0.00001743
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001743
Iteration 84/1000 | Loss: 0.00001743
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001743
Iteration 87/1000 | Loss: 0.00001743
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001742
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001742
Iteration 112/1000 | Loss: 0.00001742
Iteration 113/1000 | Loss: 0.00001742
Iteration 114/1000 | Loss: 0.00001742
Iteration 115/1000 | Loss: 0.00001742
Iteration 116/1000 | Loss: 0.00001742
Iteration 117/1000 | Loss: 0.00001742
Iteration 118/1000 | Loss: 0.00001742
Iteration 119/1000 | Loss: 0.00001742
Iteration 120/1000 | Loss: 0.00001742
Iteration 121/1000 | Loss: 0.00001742
Iteration 122/1000 | Loss: 0.00001742
Iteration 123/1000 | Loss: 0.00001742
Iteration 124/1000 | Loss: 0.00001742
Iteration 125/1000 | Loss: 0.00001742
Iteration 126/1000 | Loss: 0.00001742
Iteration 127/1000 | Loss: 0.00001742
Iteration 128/1000 | Loss: 0.00001742
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001742
Iteration 133/1000 | Loss: 0.00001742
Iteration 134/1000 | Loss: 0.00001742
Iteration 135/1000 | Loss: 0.00001742
Iteration 136/1000 | Loss: 0.00001742
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001742
Iteration 140/1000 | Loss: 0.00001742
Iteration 141/1000 | Loss: 0.00001742
Iteration 142/1000 | Loss: 0.00001742
Iteration 143/1000 | Loss: 0.00001742
Iteration 144/1000 | Loss: 0.00001742
Iteration 145/1000 | Loss: 0.00001742
Iteration 146/1000 | Loss: 0.00001742
Iteration 147/1000 | Loss: 0.00001742
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.7423131794203073e-05, 1.7423131794203073e-05, 1.7423131794203073e-05, 1.7423131794203073e-05, 1.7423131794203073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7423131794203073e-05

Optimization complete. Final v2v error: 3.574324607849121 mm

Highest mean error: 3.755798578262329 mm for frame 20

Lowest mean error: 3.3655872344970703 mm for frame 5

Saving results

Total time: 34.21495223045349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629645
Iteration 2/25 | Loss: 0.00115920
Iteration 3/25 | Loss: 0.00105450
Iteration 4/25 | Loss: 0.00104615
Iteration 5/25 | Loss: 0.00104572
Iteration 6/25 | Loss: 0.00104572
Iteration 7/25 | Loss: 0.00104572
Iteration 8/25 | Loss: 0.00104572
Iteration 9/25 | Loss: 0.00104572
Iteration 10/25 | Loss: 0.00104572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010457215830683708, 0.0010457215830683708, 0.0010457215830683708, 0.0010457215830683708, 0.0010457215830683708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010457215830683708

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.59450340
Iteration 2/25 | Loss: 0.00132194
Iteration 3/25 | Loss: 0.00132191
Iteration 4/25 | Loss: 0.00132191
Iteration 5/25 | Loss: 0.00132191
Iteration 6/25 | Loss: 0.00132191
Iteration 7/25 | Loss: 0.00132191
Iteration 8/25 | Loss: 0.00132191
Iteration 9/25 | Loss: 0.00132191
Iteration 10/25 | Loss: 0.00132191
Iteration 11/25 | Loss: 0.00132191
Iteration 12/25 | Loss: 0.00132191
Iteration 13/25 | Loss: 0.00132191
Iteration 14/25 | Loss: 0.00132191
Iteration 15/25 | Loss: 0.00132191
Iteration 16/25 | Loss: 0.00132191
Iteration 17/25 | Loss: 0.00132191
Iteration 18/25 | Loss: 0.00132191
Iteration 19/25 | Loss: 0.00132191
Iteration 20/25 | Loss: 0.00132191
Iteration 21/25 | Loss: 0.00132191
Iteration 22/25 | Loss: 0.00132191
Iteration 23/25 | Loss: 0.00132191
Iteration 24/25 | Loss: 0.00132191
Iteration 25/25 | Loss: 0.00132191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132191
Iteration 2/1000 | Loss: 0.00002861
Iteration 3/1000 | Loss: 0.00001669
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001342
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001240
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001203
Iteration 10/1000 | Loss: 0.00001170
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001146
Iteration 13/1000 | Loss: 0.00001141
Iteration 14/1000 | Loss: 0.00001138
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001132
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001129
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001124
Iteration 27/1000 | Loss: 0.00001123
Iteration 28/1000 | Loss: 0.00001122
Iteration 29/1000 | Loss: 0.00001122
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001119
Iteration 35/1000 | Loss: 0.00001118
Iteration 36/1000 | Loss: 0.00001118
Iteration 37/1000 | Loss: 0.00001118
Iteration 38/1000 | Loss: 0.00001118
Iteration 39/1000 | Loss: 0.00001117
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001117
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001116
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001115
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001113
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001111
Iteration 56/1000 | Loss: 0.00001111
Iteration 57/1000 | Loss: 0.00001111
Iteration 58/1000 | Loss: 0.00001110
Iteration 59/1000 | Loss: 0.00001110
Iteration 60/1000 | Loss: 0.00001110
Iteration 61/1000 | Loss: 0.00001110
Iteration 62/1000 | Loss: 0.00001110
Iteration 63/1000 | Loss: 0.00001109
Iteration 64/1000 | Loss: 0.00001109
Iteration 65/1000 | Loss: 0.00001108
Iteration 66/1000 | Loss: 0.00001108
Iteration 67/1000 | Loss: 0.00001108
Iteration 68/1000 | Loss: 0.00001108
Iteration 69/1000 | Loss: 0.00001108
Iteration 70/1000 | Loss: 0.00001107
Iteration 71/1000 | Loss: 0.00001107
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001107
Iteration 74/1000 | Loss: 0.00001107
Iteration 75/1000 | Loss: 0.00001107
Iteration 76/1000 | Loss: 0.00001107
Iteration 77/1000 | Loss: 0.00001107
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001107
Iteration 80/1000 | Loss: 0.00001107
Iteration 81/1000 | Loss: 0.00001106
Iteration 82/1000 | Loss: 0.00001106
Iteration 83/1000 | Loss: 0.00001106
Iteration 84/1000 | Loss: 0.00001106
Iteration 85/1000 | Loss: 0.00001106
Iteration 86/1000 | Loss: 0.00001106
Iteration 87/1000 | Loss: 0.00001106
Iteration 88/1000 | Loss: 0.00001106
Iteration 89/1000 | Loss: 0.00001106
Iteration 90/1000 | Loss: 0.00001106
Iteration 91/1000 | Loss: 0.00001106
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001105
Iteration 96/1000 | Loss: 0.00001105
Iteration 97/1000 | Loss: 0.00001105
Iteration 98/1000 | Loss: 0.00001105
Iteration 99/1000 | Loss: 0.00001105
Iteration 100/1000 | Loss: 0.00001105
Iteration 101/1000 | Loss: 0.00001105
Iteration 102/1000 | Loss: 0.00001105
Iteration 103/1000 | Loss: 0.00001105
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001105
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Iteration 148/1000 | Loss: 0.00001105
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001105
Iteration 151/1000 | Loss: 0.00001105
Iteration 152/1000 | Loss: 0.00001105
Iteration 153/1000 | Loss: 0.00001105
Iteration 154/1000 | Loss: 0.00001105
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001105
Iteration 177/1000 | Loss: 0.00001105
Iteration 178/1000 | Loss: 0.00001105
Iteration 179/1000 | Loss: 0.00001105
Iteration 180/1000 | Loss: 0.00001105
Iteration 181/1000 | Loss: 0.00001105
Iteration 182/1000 | Loss: 0.00001105
Iteration 183/1000 | Loss: 0.00001105
Iteration 184/1000 | Loss: 0.00001105
Iteration 185/1000 | Loss: 0.00001105
Iteration 186/1000 | Loss: 0.00001105
Iteration 187/1000 | Loss: 0.00001105
Iteration 188/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1054184142267331e-05, 1.1054184142267331e-05, 1.1054184142267331e-05, 1.1054184142267331e-05, 1.1054184142267331e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1054184142267331e-05

Optimization complete. Final v2v error: 2.8236284255981445 mm

Highest mean error: 3.231966733932495 mm for frame 54

Lowest mean error: 2.3096365928649902 mm for frame 16

Saving results

Total time: 35.56140375137329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076737
Iteration 2/25 | Loss: 0.00158693
Iteration 3/25 | Loss: 0.00123489
Iteration 4/25 | Loss: 0.00120192
Iteration 5/25 | Loss: 0.00119665
Iteration 6/25 | Loss: 0.00119584
Iteration 7/25 | Loss: 0.00119584
Iteration 8/25 | Loss: 0.00119584
Iteration 9/25 | Loss: 0.00119584
Iteration 10/25 | Loss: 0.00119584
Iteration 11/25 | Loss: 0.00119584
Iteration 12/25 | Loss: 0.00119584
Iteration 13/25 | Loss: 0.00119584
Iteration 14/25 | Loss: 0.00119584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001195840653963387, 0.001195840653963387, 0.001195840653963387, 0.001195840653963387, 0.001195840653963387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195840653963387

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53556967
Iteration 2/25 | Loss: 0.00115816
Iteration 3/25 | Loss: 0.00115815
Iteration 4/25 | Loss: 0.00115814
Iteration 5/25 | Loss: 0.00115814
Iteration 6/25 | Loss: 0.00115814
Iteration 7/25 | Loss: 0.00115814
Iteration 8/25 | Loss: 0.00115814
Iteration 9/25 | Loss: 0.00115814
Iteration 10/25 | Loss: 0.00115814
Iteration 11/25 | Loss: 0.00115814
Iteration 12/25 | Loss: 0.00115814
Iteration 13/25 | Loss: 0.00115814
Iteration 14/25 | Loss: 0.00115814
Iteration 15/25 | Loss: 0.00115814
Iteration 16/25 | Loss: 0.00115814
Iteration 17/25 | Loss: 0.00115814
Iteration 18/25 | Loss: 0.00115814
Iteration 19/25 | Loss: 0.00115814
Iteration 20/25 | Loss: 0.00115814
Iteration 21/25 | Loss: 0.00115814
Iteration 22/25 | Loss: 0.00115814
Iteration 23/25 | Loss: 0.00115814
Iteration 24/25 | Loss: 0.00115814
Iteration 25/25 | Loss: 0.00115814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011581412982195616, 0.0011581412982195616, 0.0011581412982195616, 0.0011581412982195616, 0.0011581412982195616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011581412982195616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115814
Iteration 2/1000 | Loss: 0.00008984
Iteration 3/1000 | Loss: 0.00004741
Iteration 4/1000 | Loss: 0.00003368
Iteration 5/1000 | Loss: 0.00003026
Iteration 6/1000 | Loss: 0.00002890
Iteration 7/1000 | Loss: 0.00002804
Iteration 8/1000 | Loss: 0.00002737
Iteration 9/1000 | Loss: 0.00002669
Iteration 10/1000 | Loss: 0.00002609
Iteration 11/1000 | Loss: 0.00002567
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002503
Iteration 15/1000 | Loss: 0.00002496
Iteration 16/1000 | Loss: 0.00002490
Iteration 17/1000 | Loss: 0.00002484
Iteration 18/1000 | Loss: 0.00002479
Iteration 19/1000 | Loss: 0.00002479
Iteration 20/1000 | Loss: 0.00002477
Iteration 21/1000 | Loss: 0.00002477
Iteration 22/1000 | Loss: 0.00002477
Iteration 23/1000 | Loss: 0.00002477
Iteration 24/1000 | Loss: 0.00002477
Iteration 25/1000 | Loss: 0.00002477
Iteration 26/1000 | Loss: 0.00002477
Iteration 27/1000 | Loss: 0.00002476
Iteration 28/1000 | Loss: 0.00002476
Iteration 29/1000 | Loss: 0.00002476
Iteration 30/1000 | Loss: 0.00002476
Iteration 31/1000 | Loss: 0.00002476
Iteration 32/1000 | Loss: 0.00002476
Iteration 33/1000 | Loss: 0.00002475
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002475
Iteration 37/1000 | Loss: 0.00002474
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002473
Iteration 40/1000 | Loss: 0.00002473
Iteration 41/1000 | Loss: 0.00002472
Iteration 42/1000 | Loss: 0.00002472
Iteration 43/1000 | Loss: 0.00002472
Iteration 44/1000 | Loss: 0.00002471
Iteration 45/1000 | Loss: 0.00002471
Iteration 46/1000 | Loss: 0.00002471
Iteration 47/1000 | Loss: 0.00002470
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002469
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002468
Iteration 54/1000 | Loss: 0.00002467
Iteration 55/1000 | Loss: 0.00002467
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002466
Iteration 58/1000 | Loss: 0.00002466
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002465
Iteration 62/1000 | Loss: 0.00002465
Iteration 63/1000 | Loss: 0.00002464
Iteration 64/1000 | Loss: 0.00002464
Iteration 65/1000 | Loss: 0.00002463
Iteration 66/1000 | Loss: 0.00002463
Iteration 67/1000 | Loss: 0.00002463
Iteration 68/1000 | Loss: 0.00002462
Iteration 69/1000 | Loss: 0.00002462
Iteration 70/1000 | Loss: 0.00002462
Iteration 71/1000 | Loss: 0.00002462
Iteration 72/1000 | Loss: 0.00002462
Iteration 73/1000 | Loss: 0.00002461
Iteration 74/1000 | Loss: 0.00002461
Iteration 75/1000 | Loss: 0.00002461
Iteration 76/1000 | Loss: 0.00002460
Iteration 77/1000 | Loss: 0.00002460
Iteration 78/1000 | Loss: 0.00002460
Iteration 79/1000 | Loss: 0.00002460
Iteration 80/1000 | Loss: 0.00002460
Iteration 81/1000 | Loss: 0.00002459
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002459
Iteration 84/1000 | Loss: 0.00002459
Iteration 85/1000 | Loss: 0.00002459
Iteration 86/1000 | Loss: 0.00002459
Iteration 87/1000 | Loss: 0.00002459
Iteration 88/1000 | Loss: 0.00002458
Iteration 89/1000 | Loss: 0.00002458
Iteration 90/1000 | Loss: 0.00002458
Iteration 91/1000 | Loss: 0.00002458
Iteration 92/1000 | Loss: 0.00002458
Iteration 93/1000 | Loss: 0.00002458
Iteration 94/1000 | Loss: 0.00002457
Iteration 95/1000 | Loss: 0.00002457
Iteration 96/1000 | Loss: 0.00002457
Iteration 97/1000 | Loss: 0.00002457
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002456
Iteration 100/1000 | Loss: 0.00002456
Iteration 101/1000 | Loss: 0.00002456
Iteration 102/1000 | Loss: 0.00002456
Iteration 103/1000 | Loss: 0.00002456
Iteration 104/1000 | Loss: 0.00002456
Iteration 105/1000 | Loss: 0.00002456
Iteration 106/1000 | Loss: 0.00002456
Iteration 107/1000 | Loss: 0.00002456
Iteration 108/1000 | Loss: 0.00002456
Iteration 109/1000 | Loss: 0.00002456
Iteration 110/1000 | Loss: 0.00002455
Iteration 111/1000 | Loss: 0.00002455
Iteration 112/1000 | Loss: 0.00002455
Iteration 113/1000 | Loss: 0.00002455
Iteration 114/1000 | Loss: 0.00002455
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002455
Iteration 117/1000 | Loss: 0.00002455
Iteration 118/1000 | Loss: 0.00002455
Iteration 119/1000 | Loss: 0.00002454
Iteration 120/1000 | Loss: 0.00002454
Iteration 121/1000 | Loss: 0.00002454
Iteration 122/1000 | Loss: 0.00002454
Iteration 123/1000 | Loss: 0.00002454
Iteration 124/1000 | Loss: 0.00002454
Iteration 125/1000 | Loss: 0.00002454
Iteration 126/1000 | Loss: 0.00002454
Iteration 127/1000 | Loss: 0.00002454
Iteration 128/1000 | Loss: 0.00002454
Iteration 129/1000 | Loss: 0.00002454
Iteration 130/1000 | Loss: 0.00002454
Iteration 131/1000 | Loss: 0.00002454
Iteration 132/1000 | Loss: 0.00002454
Iteration 133/1000 | Loss: 0.00002454
Iteration 134/1000 | Loss: 0.00002454
Iteration 135/1000 | Loss: 0.00002454
Iteration 136/1000 | Loss: 0.00002454
Iteration 137/1000 | Loss: 0.00002454
Iteration 138/1000 | Loss: 0.00002454
Iteration 139/1000 | Loss: 0.00002454
Iteration 140/1000 | Loss: 0.00002454
Iteration 141/1000 | Loss: 0.00002454
Iteration 142/1000 | Loss: 0.00002454
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002454
Iteration 145/1000 | Loss: 0.00002454
Iteration 146/1000 | Loss: 0.00002454
Iteration 147/1000 | Loss: 0.00002454
Iteration 148/1000 | Loss: 0.00002454
Iteration 149/1000 | Loss: 0.00002454
Iteration 150/1000 | Loss: 0.00002454
Iteration 151/1000 | Loss: 0.00002454
Iteration 152/1000 | Loss: 0.00002454
Iteration 153/1000 | Loss: 0.00002454
Iteration 154/1000 | Loss: 0.00002454
Iteration 155/1000 | Loss: 0.00002454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.4543020117562264e-05, 2.4543020117562264e-05, 2.4543020117562264e-05, 2.4543020117562264e-05, 2.4543020117562264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4543020117562264e-05

Optimization complete. Final v2v error: 4.078624725341797 mm

Highest mean error: 4.976135730743408 mm for frame 194

Lowest mean error: 3.511584520339966 mm for frame 98

Saving results

Total time: 43.39444828033447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427211
Iteration 2/25 | Loss: 0.00126555
Iteration 3/25 | Loss: 0.00109858
Iteration 4/25 | Loss: 0.00107877
Iteration 5/25 | Loss: 0.00107536
Iteration 6/25 | Loss: 0.00107439
Iteration 7/25 | Loss: 0.00107439
Iteration 8/25 | Loss: 0.00107439
Iteration 9/25 | Loss: 0.00107439
Iteration 10/25 | Loss: 0.00107439
Iteration 11/25 | Loss: 0.00107439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010743943275883794, 0.0010743943275883794, 0.0010743943275883794, 0.0010743943275883794, 0.0010743943275883794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010743943275883794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48594248
Iteration 2/25 | Loss: 0.00132878
Iteration 3/25 | Loss: 0.00132878
Iteration 4/25 | Loss: 0.00132878
Iteration 5/25 | Loss: 0.00132878
Iteration 6/25 | Loss: 0.00132878
Iteration 7/25 | Loss: 0.00132878
Iteration 8/25 | Loss: 0.00132878
Iteration 9/25 | Loss: 0.00132878
Iteration 10/25 | Loss: 0.00132878
Iteration 11/25 | Loss: 0.00132878
Iteration 12/25 | Loss: 0.00132878
Iteration 13/25 | Loss: 0.00132878
Iteration 14/25 | Loss: 0.00132878
Iteration 15/25 | Loss: 0.00132878
Iteration 16/25 | Loss: 0.00132878
Iteration 17/25 | Loss: 0.00132878
Iteration 18/25 | Loss: 0.00132878
Iteration 19/25 | Loss: 0.00132878
Iteration 20/25 | Loss: 0.00132878
Iteration 21/25 | Loss: 0.00132878
Iteration 22/25 | Loss: 0.00132878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013287763576954603, 0.0013287763576954603, 0.0013287763576954603, 0.0013287763576954603, 0.0013287763576954603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013287763576954603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132878
Iteration 2/1000 | Loss: 0.00003355
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001344
Iteration 6/1000 | Loss: 0.00001287
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001150
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001138
Iteration 20/1000 | Loss: 0.00001136
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001134
Iteration 24/1000 | Loss: 0.00001134
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001127
Iteration 32/1000 | Loss: 0.00001126
Iteration 33/1000 | Loss: 0.00001126
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001122
Iteration 41/1000 | Loss: 0.00001122
Iteration 42/1000 | Loss: 0.00001121
Iteration 43/1000 | Loss: 0.00001121
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001120
Iteration 46/1000 | Loss: 0.00001120
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001120
Iteration 49/1000 | Loss: 0.00001120
Iteration 50/1000 | Loss: 0.00001120
Iteration 51/1000 | Loss: 0.00001120
Iteration 52/1000 | Loss: 0.00001119
Iteration 53/1000 | Loss: 0.00001119
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001118
Iteration 57/1000 | Loss: 0.00001118
Iteration 58/1000 | Loss: 0.00001118
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001116
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001116
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001115
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001115
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001114
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001113
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.1122153409814928e-05, 1.1122153409814928e-05, 1.1122153409814928e-05, 1.1122153409814928e-05, 1.1122153409814928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1122153409814928e-05

Optimization complete. Final v2v error: 2.8142781257629395 mm

Highest mean error: 3.834394693374634 mm for frame 117

Lowest mean error: 2.1814262866973877 mm for frame 238

Saving results

Total time: 37.252453327178955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103255
Iteration 2/25 | Loss: 0.00165251
Iteration 3/25 | Loss: 0.00128974
Iteration 4/25 | Loss: 0.00123611
Iteration 5/25 | Loss: 0.00119062
Iteration 6/25 | Loss: 0.00118970
Iteration 7/25 | Loss: 0.00118105
Iteration 8/25 | Loss: 0.00118016
Iteration 9/25 | Loss: 0.00116326
Iteration 10/25 | Loss: 0.00115922
Iteration 11/25 | Loss: 0.00115990
Iteration 12/25 | Loss: 0.00115866
Iteration 13/25 | Loss: 0.00115937
Iteration 14/25 | Loss: 0.00115884
Iteration 15/25 | Loss: 0.00115774
Iteration 16/25 | Loss: 0.00116002
Iteration 17/25 | Loss: 0.00116009
Iteration 18/25 | Loss: 0.00115806
Iteration 19/25 | Loss: 0.00115945
Iteration 20/25 | Loss: 0.00115841
Iteration 21/25 | Loss: 0.00115717
Iteration 22/25 | Loss: 0.00115731
Iteration 23/25 | Loss: 0.00115774
Iteration 24/25 | Loss: 0.00115892
Iteration 25/25 | Loss: 0.00115898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.98719692
Iteration 2/25 | Loss: 0.02065600
Iteration 3/25 | Loss: 0.01356920
Iteration 4/25 | Loss: 0.00827053
Iteration 5/25 | Loss: 0.01291488
Iteration 6/25 | Loss: 0.00191519
Iteration 7/25 | Loss: 0.00191519
Iteration 8/25 | Loss: 0.00191519
Iteration 9/25 | Loss: 0.00191519
Iteration 10/25 | Loss: 0.00191519
Iteration 11/25 | Loss: 0.00191519
Iteration 12/25 | Loss: 0.00191519
Iteration 13/25 | Loss: 0.00191519
Iteration 14/25 | Loss: 0.00191519
Iteration 15/25 | Loss: 0.00191519
Iteration 16/25 | Loss: 0.00191519
Iteration 17/25 | Loss: 0.00191519
Iteration 18/25 | Loss: 0.00191519
Iteration 19/25 | Loss: 0.00191519
Iteration 20/25 | Loss: 0.00191519
Iteration 21/25 | Loss: 0.00191519
Iteration 22/25 | Loss: 0.00191519
Iteration 23/25 | Loss: 0.00191519
Iteration 24/25 | Loss: 0.00191519
Iteration 25/25 | Loss: 0.00191519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191519
Iteration 2/1000 | Loss: 0.00010567
Iteration 3/1000 | Loss: 0.00028589
Iteration 4/1000 | Loss: 0.00017401
Iteration 5/1000 | Loss: 0.00005807
Iteration 6/1000 | Loss: 0.00015410
Iteration 7/1000 | Loss: 0.00004843
Iteration 8/1000 | Loss: 0.00012884
Iteration 9/1000 | Loss: 0.00015668
Iteration 10/1000 | Loss: 0.00017861
Iteration 11/1000 | Loss: 0.00013686
Iteration 12/1000 | Loss: 0.00013403
Iteration 13/1000 | Loss: 0.00013125
Iteration 14/1000 | Loss: 0.00014740
Iteration 15/1000 | Loss: 0.00013264
Iteration 16/1000 | Loss: 0.00015006
Iteration 17/1000 | Loss: 0.00012697
Iteration 18/1000 | Loss: 0.00017369
Iteration 19/1000 | Loss: 0.00012417
Iteration 20/1000 | Loss: 0.00019140
Iteration 21/1000 | Loss: 0.00009063
Iteration 22/1000 | Loss: 0.00010326
Iteration 23/1000 | Loss: 0.00009492
Iteration 24/1000 | Loss: 0.00011387
Iteration 25/1000 | Loss: 0.00008766
Iteration 26/1000 | Loss: 0.00011266
Iteration 27/1000 | Loss: 0.00008741
Iteration 28/1000 | Loss: 0.00015314
Iteration 29/1000 | Loss: 0.00011490
Iteration 30/1000 | Loss: 0.00008636
Iteration 31/1000 | Loss: 0.00024929
Iteration 32/1000 | Loss: 0.00012157
Iteration 33/1000 | Loss: 0.00014575
Iteration 34/1000 | Loss: 0.00012110
Iteration 35/1000 | Loss: 0.00013788
Iteration 36/1000 | Loss: 0.00004415
Iteration 37/1000 | Loss: 0.00010900
Iteration 38/1000 | Loss: 0.00016323
Iteration 39/1000 | Loss: 0.00012062
Iteration 40/1000 | Loss: 0.00015787
Iteration 41/1000 | Loss: 0.00012691
Iteration 42/1000 | Loss: 0.00030119
Iteration 43/1000 | Loss: 0.00016985
Iteration 44/1000 | Loss: 0.00008682
Iteration 45/1000 | Loss: 0.00026904
Iteration 46/1000 | Loss: 0.00005410
Iteration 47/1000 | Loss: 0.00004158
Iteration 48/1000 | Loss: 0.00003996
Iteration 49/1000 | Loss: 0.00003877
Iteration 50/1000 | Loss: 0.00003816
Iteration 51/1000 | Loss: 0.00003765
Iteration 52/1000 | Loss: 0.00003738
Iteration 53/1000 | Loss: 0.00003702
Iteration 54/1000 | Loss: 0.00003676
Iteration 55/1000 | Loss: 0.00003652
Iteration 56/1000 | Loss: 0.00003631
Iteration 57/1000 | Loss: 0.00003623
Iteration 58/1000 | Loss: 0.00003617
Iteration 59/1000 | Loss: 0.00003613
Iteration 60/1000 | Loss: 0.00003608
Iteration 61/1000 | Loss: 0.00003595
Iteration 62/1000 | Loss: 0.00003595
Iteration 63/1000 | Loss: 0.00003594
Iteration 64/1000 | Loss: 0.00003593
Iteration 65/1000 | Loss: 0.00003593
Iteration 66/1000 | Loss: 0.00003593
Iteration 67/1000 | Loss: 0.00003593
Iteration 68/1000 | Loss: 0.00003592
Iteration 69/1000 | Loss: 0.00003590
Iteration 70/1000 | Loss: 0.00003585
Iteration 71/1000 | Loss: 0.00003580
Iteration 72/1000 | Loss: 0.00003580
Iteration 73/1000 | Loss: 0.00003580
Iteration 74/1000 | Loss: 0.00003580
Iteration 75/1000 | Loss: 0.00003579
Iteration 76/1000 | Loss: 0.00003579
Iteration 77/1000 | Loss: 0.00003579
Iteration 78/1000 | Loss: 0.00003579
Iteration 79/1000 | Loss: 0.00003579
Iteration 80/1000 | Loss: 0.00003579
Iteration 81/1000 | Loss: 0.00003578
Iteration 82/1000 | Loss: 0.00003573
Iteration 83/1000 | Loss: 0.00003572
Iteration 84/1000 | Loss: 0.00003572
Iteration 85/1000 | Loss: 0.00003569
Iteration 86/1000 | Loss: 0.00003569
Iteration 87/1000 | Loss: 0.00003568
Iteration 88/1000 | Loss: 0.00003567
Iteration 89/1000 | Loss: 0.00003567
Iteration 90/1000 | Loss: 0.00003566
Iteration 91/1000 | Loss: 0.00003566
Iteration 92/1000 | Loss: 0.00003566
Iteration 93/1000 | Loss: 0.00003566
Iteration 94/1000 | Loss: 0.00003565
Iteration 95/1000 | Loss: 0.00003565
Iteration 96/1000 | Loss: 0.00003565
Iteration 97/1000 | Loss: 0.00003565
Iteration 98/1000 | Loss: 0.00003565
Iteration 99/1000 | Loss: 0.00003564
Iteration 100/1000 | Loss: 0.00003564
Iteration 101/1000 | Loss: 0.00003564
Iteration 102/1000 | Loss: 0.00003564
Iteration 103/1000 | Loss: 0.00003563
Iteration 104/1000 | Loss: 0.00003563
Iteration 105/1000 | Loss: 0.00003563
Iteration 106/1000 | Loss: 0.00003563
Iteration 107/1000 | Loss: 0.00003562
Iteration 108/1000 | Loss: 0.00003562
Iteration 109/1000 | Loss: 0.00003562
Iteration 110/1000 | Loss: 0.00003562
Iteration 111/1000 | Loss: 0.00003562
Iteration 112/1000 | Loss: 0.00003562
Iteration 113/1000 | Loss: 0.00003562
Iteration 114/1000 | Loss: 0.00003561
Iteration 115/1000 | Loss: 0.00003561
Iteration 116/1000 | Loss: 0.00003561
Iteration 117/1000 | Loss: 0.00003560
Iteration 118/1000 | Loss: 0.00003560
Iteration 119/1000 | Loss: 0.00003560
Iteration 120/1000 | Loss: 0.00003559
Iteration 121/1000 | Loss: 0.00003559
Iteration 122/1000 | Loss: 0.00003559
Iteration 123/1000 | Loss: 0.00003558
Iteration 124/1000 | Loss: 0.00003557
Iteration 125/1000 | Loss: 0.00003556
Iteration 126/1000 | Loss: 0.00003556
Iteration 127/1000 | Loss: 0.00003556
Iteration 128/1000 | Loss: 0.00003555
Iteration 129/1000 | Loss: 0.00003555
Iteration 130/1000 | Loss: 0.00003554
Iteration 131/1000 | Loss: 0.00003554
Iteration 132/1000 | Loss: 0.00003554
Iteration 133/1000 | Loss: 0.00003554
Iteration 134/1000 | Loss: 0.00003553
Iteration 135/1000 | Loss: 0.00003553
Iteration 136/1000 | Loss: 0.00003553
Iteration 137/1000 | Loss: 0.00003552
Iteration 138/1000 | Loss: 0.00003552
Iteration 139/1000 | Loss: 0.00003552
Iteration 140/1000 | Loss: 0.00003551
Iteration 141/1000 | Loss: 0.00003550
Iteration 142/1000 | Loss: 0.00003550
Iteration 143/1000 | Loss: 0.00003550
Iteration 144/1000 | Loss: 0.00003550
Iteration 145/1000 | Loss: 0.00003549
Iteration 146/1000 | Loss: 0.00003549
Iteration 147/1000 | Loss: 0.00003549
Iteration 148/1000 | Loss: 0.00003549
Iteration 149/1000 | Loss: 0.00003548
Iteration 150/1000 | Loss: 0.00003548
Iteration 151/1000 | Loss: 0.00003547
Iteration 152/1000 | Loss: 0.00003547
Iteration 153/1000 | Loss: 0.00003547
Iteration 154/1000 | Loss: 0.00003546
Iteration 155/1000 | Loss: 0.00003546
Iteration 156/1000 | Loss: 0.00003546
Iteration 157/1000 | Loss: 0.00003545
Iteration 158/1000 | Loss: 0.00003545
Iteration 159/1000 | Loss: 0.00003545
Iteration 160/1000 | Loss: 0.00003544
Iteration 161/1000 | Loss: 0.00003544
Iteration 162/1000 | Loss: 0.00003544
Iteration 163/1000 | Loss: 0.00003544
Iteration 164/1000 | Loss: 0.00003544
Iteration 165/1000 | Loss: 0.00003544
Iteration 166/1000 | Loss: 0.00003544
Iteration 167/1000 | Loss: 0.00003544
Iteration 168/1000 | Loss: 0.00003544
Iteration 169/1000 | Loss: 0.00003544
Iteration 170/1000 | Loss: 0.00003544
Iteration 171/1000 | Loss: 0.00003544
Iteration 172/1000 | Loss: 0.00003543
Iteration 173/1000 | Loss: 0.00003543
Iteration 174/1000 | Loss: 0.00003543
Iteration 175/1000 | Loss: 0.00003543
Iteration 176/1000 | Loss: 0.00003543
Iteration 177/1000 | Loss: 0.00003543
Iteration 178/1000 | Loss: 0.00003543
Iteration 179/1000 | Loss: 0.00003543
Iteration 180/1000 | Loss: 0.00003543
Iteration 181/1000 | Loss: 0.00003543
Iteration 182/1000 | Loss: 0.00003543
Iteration 183/1000 | Loss: 0.00003543
Iteration 184/1000 | Loss: 0.00003543
Iteration 185/1000 | Loss: 0.00003542
Iteration 186/1000 | Loss: 0.00003542
Iteration 187/1000 | Loss: 0.00003542
Iteration 188/1000 | Loss: 0.00003542
Iteration 189/1000 | Loss: 0.00003542
Iteration 190/1000 | Loss: 0.00003542
Iteration 191/1000 | Loss: 0.00003542
Iteration 192/1000 | Loss: 0.00003541
Iteration 193/1000 | Loss: 0.00003541
Iteration 194/1000 | Loss: 0.00003541
Iteration 195/1000 | Loss: 0.00003541
Iteration 196/1000 | Loss: 0.00003541
Iteration 197/1000 | Loss: 0.00003541
Iteration 198/1000 | Loss: 0.00003541
Iteration 199/1000 | Loss: 0.00003541
Iteration 200/1000 | Loss: 0.00003540
Iteration 201/1000 | Loss: 0.00003540
Iteration 202/1000 | Loss: 0.00003540
Iteration 203/1000 | Loss: 0.00003540
Iteration 204/1000 | Loss: 0.00003540
Iteration 205/1000 | Loss: 0.00003540
Iteration 206/1000 | Loss: 0.00003540
Iteration 207/1000 | Loss: 0.00003540
Iteration 208/1000 | Loss: 0.00003540
Iteration 209/1000 | Loss: 0.00003539
Iteration 210/1000 | Loss: 0.00003539
Iteration 211/1000 | Loss: 0.00003539
Iteration 212/1000 | Loss: 0.00003539
Iteration 213/1000 | Loss: 0.00003539
Iteration 214/1000 | Loss: 0.00003539
Iteration 215/1000 | Loss: 0.00003539
Iteration 216/1000 | Loss: 0.00003539
Iteration 217/1000 | Loss: 0.00003539
Iteration 218/1000 | Loss: 0.00003539
Iteration 219/1000 | Loss: 0.00003539
Iteration 220/1000 | Loss: 0.00003539
Iteration 221/1000 | Loss: 0.00003539
Iteration 222/1000 | Loss: 0.00003538
Iteration 223/1000 | Loss: 0.00003538
Iteration 224/1000 | Loss: 0.00003538
Iteration 225/1000 | Loss: 0.00003538
Iteration 226/1000 | Loss: 0.00003538
Iteration 227/1000 | Loss: 0.00003538
Iteration 228/1000 | Loss: 0.00003538
Iteration 229/1000 | Loss: 0.00003537
Iteration 230/1000 | Loss: 0.00003537
Iteration 231/1000 | Loss: 0.00003537
Iteration 232/1000 | Loss: 0.00003537
Iteration 233/1000 | Loss: 0.00003537
Iteration 234/1000 | Loss: 0.00003537
Iteration 235/1000 | Loss: 0.00003536
Iteration 236/1000 | Loss: 0.00003536
Iteration 237/1000 | Loss: 0.00003536
Iteration 238/1000 | Loss: 0.00003536
Iteration 239/1000 | Loss: 0.00003536
Iteration 240/1000 | Loss: 0.00003536
Iteration 241/1000 | Loss: 0.00003536
Iteration 242/1000 | Loss: 0.00003536
Iteration 243/1000 | Loss: 0.00003536
Iteration 244/1000 | Loss: 0.00003535
Iteration 245/1000 | Loss: 0.00003535
Iteration 246/1000 | Loss: 0.00003535
Iteration 247/1000 | Loss: 0.00003535
Iteration 248/1000 | Loss: 0.00003535
Iteration 249/1000 | Loss: 0.00003535
Iteration 250/1000 | Loss: 0.00003535
Iteration 251/1000 | Loss: 0.00003535
Iteration 252/1000 | Loss: 0.00003534
Iteration 253/1000 | Loss: 0.00003534
Iteration 254/1000 | Loss: 0.00003534
Iteration 255/1000 | Loss: 0.00003534
Iteration 256/1000 | Loss: 0.00003534
Iteration 257/1000 | Loss: 0.00003533
Iteration 258/1000 | Loss: 0.00003533
Iteration 259/1000 | Loss: 0.00003533
Iteration 260/1000 | Loss: 0.00003533
Iteration 261/1000 | Loss: 0.00003533
Iteration 262/1000 | Loss: 0.00003533
Iteration 263/1000 | Loss: 0.00003533
Iteration 264/1000 | Loss: 0.00003532
Iteration 265/1000 | Loss: 0.00003532
Iteration 266/1000 | Loss: 0.00003532
Iteration 267/1000 | Loss: 0.00003532
Iteration 268/1000 | Loss: 0.00003531
Iteration 269/1000 | Loss: 0.00003531
Iteration 270/1000 | Loss: 0.00003531
Iteration 271/1000 | Loss: 0.00003531
Iteration 272/1000 | Loss: 0.00003531
Iteration 273/1000 | Loss: 0.00003531
Iteration 274/1000 | Loss: 0.00003531
Iteration 275/1000 | Loss: 0.00003531
Iteration 276/1000 | Loss: 0.00003531
Iteration 277/1000 | Loss: 0.00003531
Iteration 278/1000 | Loss: 0.00003531
Iteration 279/1000 | Loss: 0.00003531
Iteration 280/1000 | Loss: 0.00003531
Iteration 281/1000 | Loss: 0.00003531
Iteration 282/1000 | Loss: 0.00003530
Iteration 283/1000 | Loss: 0.00003530
Iteration 284/1000 | Loss: 0.00003530
Iteration 285/1000 | Loss: 0.00003530
Iteration 286/1000 | Loss: 0.00003530
Iteration 287/1000 | Loss: 0.00003530
Iteration 288/1000 | Loss: 0.00003530
Iteration 289/1000 | Loss: 0.00003530
Iteration 290/1000 | Loss: 0.00003530
Iteration 291/1000 | Loss: 0.00003530
Iteration 292/1000 | Loss: 0.00003529
Iteration 293/1000 | Loss: 0.00003529
Iteration 294/1000 | Loss: 0.00003529
Iteration 295/1000 | Loss: 0.00003529
Iteration 296/1000 | Loss: 0.00003529
Iteration 297/1000 | Loss: 0.00003529
Iteration 298/1000 | Loss: 0.00003529
Iteration 299/1000 | Loss: 0.00003529
Iteration 300/1000 | Loss: 0.00003529
Iteration 301/1000 | Loss: 0.00003528
Iteration 302/1000 | Loss: 0.00003528
Iteration 303/1000 | Loss: 0.00003528
Iteration 304/1000 | Loss: 0.00003528
Iteration 305/1000 | Loss: 0.00003528
Iteration 306/1000 | Loss: 0.00003528
Iteration 307/1000 | Loss: 0.00003528
Iteration 308/1000 | Loss: 0.00003528
Iteration 309/1000 | Loss: 0.00003528
Iteration 310/1000 | Loss: 0.00003528
Iteration 311/1000 | Loss: 0.00003528
Iteration 312/1000 | Loss: 0.00003528
Iteration 313/1000 | Loss: 0.00003528
Iteration 314/1000 | Loss: 0.00003527
Iteration 315/1000 | Loss: 0.00003527
Iteration 316/1000 | Loss: 0.00003527
Iteration 317/1000 | Loss: 0.00003527
Iteration 318/1000 | Loss: 0.00003527
Iteration 319/1000 | Loss: 0.00003527
Iteration 320/1000 | Loss: 0.00003527
Iteration 321/1000 | Loss: 0.00003527
Iteration 322/1000 | Loss: 0.00003527
Iteration 323/1000 | Loss: 0.00003527
Iteration 324/1000 | Loss: 0.00003527
Iteration 325/1000 | Loss: 0.00003527
Iteration 326/1000 | Loss: 0.00003527
Iteration 327/1000 | Loss: 0.00003527
Iteration 328/1000 | Loss: 0.00003527
Iteration 329/1000 | Loss: 0.00003527
Iteration 330/1000 | Loss: 0.00003527
Iteration 331/1000 | Loss: 0.00003526
Iteration 332/1000 | Loss: 0.00003526
Iteration 333/1000 | Loss: 0.00003526
Iteration 334/1000 | Loss: 0.00003526
Iteration 335/1000 | Loss: 0.00003526
Iteration 336/1000 | Loss: 0.00003526
Iteration 337/1000 | Loss: 0.00003526
Iteration 338/1000 | Loss: 0.00003526
Iteration 339/1000 | Loss: 0.00003526
Iteration 340/1000 | Loss: 0.00003526
Iteration 341/1000 | Loss: 0.00003526
Iteration 342/1000 | Loss: 0.00003526
Iteration 343/1000 | Loss: 0.00003526
Iteration 344/1000 | Loss: 0.00003526
Iteration 345/1000 | Loss: 0.00003526
Iteration 346/1000 | Loss: 0.00003526
Iteration 347/1000 | Loss: 0.00003526
Iteration 348/1000 | Loss: 0.00003526
Iteration 349/1000 | Loss: 0.00003526
Iteration 350/1000 | Loss: 0.00003526
Iteration 351/1000 | Loss: 0.00003526
Iteration 352/1000 | Loss: 0.00003526
Iteration 353/1000 | Loss: 0.00003526
Iteration 354/1000 | Loss: 0.00003526
Iteration 355/1000 | Loss: 0.00003526
Iteration 356/1000 | Loss: 0.00003526
Iteration 357/1000 | Loss: 0.00003526
Iteration 358/1000 | Loss: 0.00003526
Iteration 359/1000 | Loss: 0.00003526
Iteration 360/1000 | Loss: 0.00003526
Iteration 361/1000 | Loss: 0.00003526
Iteration 362/1000 | Loss: 0.00003526
Iteration 363/1000 | Loss: 0.00003526
Iteration 364/1000 | Loss: 0.00003526
Iteration 365/1000 | Loss: 0.00003526
Iteration 366/1000 | Loss: 0.00003526
Iteration 367/1000 | Loss: 0.00003526
Iteration 368/1000 | Loss: 0.00003526
Iteration 369/1000 | Loss: 0.00003526
Iteration 370/1000 | Loss: 0.00003526
Iteration 371/1000 | Loss: 0.00003526
Iteration 372/1000 | Loss: 0.00003526
Iteration 373/1000 | Loss: 0.00003526
Iteration 374/1000 | Loss: 0.00003526
Iteration 375/1000 | Loss: 0.00003526
Iteration 376/1000 | Loss: 0.00003526
Iteration 377/1000 | Loss: 0.00003526
Iteration 378/1000 | Loss: 0.00003526
Iteration 379/1000 | Loss: 0.00003526
Iteration 380/1000 | Loss: 0.00003526
Iteration 381/1000 | Loss: 0.00003526
Iteration 382/1000 | Loss: 0.00003526
Iteration 383/1000 | Loss: 0.00003526
Iteration 384/1000 | Loss: 0.00003526
Iteration 385/1000 | Loss: 0.00003526
Iteration 386/1000 | Loss: 0.00003526
Iteration 387/1000 | Loss: 0.00003526
Iteration 388/1000 | Loss: 0.00003526
Iteration 389/1000 | Loss: 0.00003526
Iteration 390/1000 | Loss: 0.00003526
Iteration 391/1000 | Loss: 0.00003526
Iteration 392/1000 | Loss: 0.00003526
Iteration 393/1000 | Loss: 0.00003526
Iteration 394/1000 | Loss: 0.00003526
Iteration 395/1000 | Loss: 0.00003526
Iteration 396/1000 | Loss: 0.00003526
Iteration 397/1000 | Loss: 0.00003526
Iteration 398/1000 | Loss: 0.00003526
Iteration 399/1000 | Loss: 0.00003526
Iteration 400/1000 | Loss: 0.00003526
Iteration 401/1000 | Loss: 0.00003526
Iteration 402/1000 | Loss: 0.00003526
Iteration 403/1000 | Loss: 0.00003526
Iteration 404/1000 | Loss: 0.00003526
Iteration 405/1000 | Loss: 0.00003526
Iteration 406/1000 | Loss: 0.00003526
Iteration 407/1000 | Loss: 0.00003526
Iteration 408/1000 | Loss: 0.00003526
Iteration 409/1000 | Loss: 0.00003526
Iteration 410/1000 | Loss: 0.00003526
Iteration 411/1000 | Loss: 0.00003526
Iteration 412/1000 | Loss: 0.00003526
Iteration 413/1000 | Loss: 0.00003526
Iteration 414/1000 | Loss: 0.00003526
Iteration 415/1000 | Loss: 0.00003526
Iteration 416/1000 | Loss: 0.00003526
Iteration 417/1000 | Loss: 0.00003526
Iteration 418/1000 | Loss: 0.00003526
Iteration 419/1000 | Loss: 0.00003526
Iteration 420/1000 | Loss: 0.00003526
Iteration 421/1000 | Loss: 0.00003526
Iteration 422/1000 | Loss: 0.00003526
Iteration 423/1000 | Loss: 0.00003526
Iteration 424/1000 | Loss: 0.00003526
Iteration 425/1000 | Loss: 0.00003526
Iteration 426/1000 | Loss: 0.00003526
Iteration 427/1000 | Loss: 0.00003526
Iteration 428/1000 | Loss: 0.00003526
Iteration 429/1000 | Loss: 0.00003526
Iteration 430/1000 | Loss: 0.00003526
Iteration 431/1000 | Loss: 0.00003526
Iteration 432/1000 | Loss: 0.00003526
Iteration 433/1000 | Loss: 0.00003526
Iteration 434/1000 | Loss: 0.00003526
Iteration 435/1000 | Loss: 0.00003526
Iteration 436/1000 | Loss: 0.00003526
Iteration 437/1000 | Loss: 0.00003526
Iteration 438/1000 | Loss: 0.00003526
Iteration 439/1000 | Loss: 0.00003526
Iteration 440/1000 | Loss: 0.00003526
Iteration 441/1000 | Loss: 0.00003526
Iteration 442/1000 | Loss: 0.00003526
Iteration 443/1000 | Loss: 0.00003526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 443. Stopping optimization.
Last 5 losses: [3.5256747651146725e-05, 3.5256747651146725e-05, 3.5256747651146725e-05, 3.5256747651146725e-05, 3.5256747651146725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5256747651146725e-05

Optimization complete. Final v2v error: 4.6937174797058105 mm

Highest mean error: 10.45368480682373 mm for frame 134

Lowest mean error: 2.841853380203247 mm for frame 141

Saving results

Total time: 152.55846428871155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438146
Iteration 2/25 | Loss: 0.00127111
Iteration 3/25 | Loss: 0.00108671
Iteration 4/25 | Loss: 0.00107552
Iteration 5/25 | Loss: 0.00107307
Iteration 6/25 | Loss: 0.00107296
Iteration 7/25 | Loss: 0.00107296
Iteration 8/25 | Loss: 0.00107296
Iteration 9/25 | Loss: 0.00107296
Iteration 10/25 | Loss: 0.00107296
Iteration 11/25 | Loss: 0.00107296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001072957064025104, 0.001072957064025104, 0.001072957064025104, 0.001072957064025104, 0.001072957064025104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001072957064025104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22199130
Iteration 2/25 | Loss: 0.00141625
Iteration 3/25 | Loss: 0.00141624
Iteration 4/25 | Loss: 0.00141624
Iteration 5/25 | Loss: 0.00141624
Iteration 6/25 | Loss: 0.00141624
Iteration 7/25 | Loss: 0.00141624
Iteration 8/25 | Loss: 0.00141624
Iteration 9/25 | Loss: 0.00141624
Iteration 10/25 | Loss: 0.00141624
Iteration 11/25 | Loss: 0.00141624
Iteration 12/25 | Loss: 0.00141624
Iteration 13/25 | Loss: 0.00141624
Iteration 14/25 | Loss: 0.00141624
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014162411680445075, 0.0014162411680445075, 0.0014162411680445075, 0.0014162411680445075, 0.0014162411680445075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014162411680445075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141624
Iteration 2/1000 | Loss: 0.00005932
Iteration 3/1000 | Loss: 0.00003265
Iteration 4/1000 | Loss: 0.00001956
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001486
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001242
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001233
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001231
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001229
Iteration 22/1000 | Loss: 0.00001224
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001216
Iteration 32/1000 | Loss: 0.00001216
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001214
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001211
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001211
Iteration 42/1000 | Loss: 0.00001211
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001200
Iteration 73/1000 | Loss: 0.00001200
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001198
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001197
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001197
Iteration 95/1000 | Loss: 0.00001197
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001197
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001194
Iteration 129/1000 | Loss: 0.00001194
Iteration 130/1000 | Loss: 0.00001194
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001193
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001193
Iteration 158/1000 | Loss: 0.00001193
Iteration 159/1000 | Loss: 0.00001193
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001193
Iteration 162/1000 | Loss: 0.00001193
Iteration 163/1000 | Loss: 0.00001193
Iteration 164/1000 | Loss: 0.00001193
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001193
Iteration 169/1000 | Loss: 0.00001193
Iteration 170/1000 | Loss: 0.00001193
Iteration 171/1000 | Loss: 0.00001193
Iteration 172/1000 | Loss: 0.00001193
Iteration 173/1000 | Loss: 0.00001193
Iteration 174/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.193121715914458e-05, 1.193121715914458e-05, 1.193121715914458e-05, 1.193121715914458e-05, 1.193121715914458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.193121715914458e-05

Optimization complete. Final v2v error: 2.890474319458008 mm

Highest mean error: 3.975276470184326 mm for frame 239

Lowest mean error: 2.4929630756378174 mm for frame 202

Saving results

Total time: 45.05939841270447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938474
Iteration 2/25 | Loss: 0.00183535
Iteration 3/25 | Loss: 0.00130014
Iteration 4/25 | Loss: 0.00122765
Iteration 5/25 | Loss: 0.00117762
Iteration 6/25 | Loss: 0.00118853
Iteration 7/25 | Loss: 0.00114514
Iteration 8/25 | Loss: 0.00110300
Iteration 9/25 | Loss: 0.00107608
Iteration 10/25 | Loss: 0.00106203
Iteration 11/25 | Loss: 0.00105734
Iteration 12/25 | Loss: 0.00106371
Iteration 13/25 | Loss: 0.00105642
Iteration 14/25 | Loss: 0.00105621
Iteration 15/25 | Loss: 0.00105620
Iteration 16/25 | Loss: 0.00105620
Iteration 17/25 | Loss: 0.00105620
Iteration 18/25 | Loss: 0.00105620
Iteration 19/25 | Loss: 0.00105620
Iteration 20/25 | Loss: 0.00105620
Iteration 21/25 | Loss: 0.00105620
Iteration 22/25 | Loss: 0.00105620
Iteration 23/25 | Loss: 0.00105619
Iteration 24/25 | Loss: 0.00105619
Iteration 25/25 | Loss: 0.00105619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30105388
Iteration 2/25 | Loss: 0.00133883
Iteration 3/25 | Loss: 0.00133882
Iteration 4/25 | Loss: 0.00133882
Iteration 5/25 | Loss: 0.00133882
Iteration 6/25 | Loss: 0.00133882
Iteration 7/25 | Loss: 0.00133882
Iteration 8/25 | Loss: 0.00133882
Iteration 9/25 | Loss: 0.00133882
Iteration 10/25 | Loss: 0.00133882
Iteration 11/25 | Loss: 0.00133882
Iteration 12/25 | Loss: 0.00133882
Iteration 13/25 | Loss: 0.00133882
Iteration 14/25 | Loss: 0.00133882
Iteration 15/25 | Loss: 0.00133882
Iteration 16/25 | Loss: 0.00133882
Iteration 17/25 | Loss: 0.00133882
Iteration 18/25 | Loss: 0.00133882
Iteration 19/25 | Loss: 0.00133882
Iteration 20/25 | Loss: 0.00133882
Iteration 21/25 | Loss: 0.00133882
Iteration 22/25 | Loss: 0.00133882
Iteration 23/25 | Loss: 0.00133882
Iteration 24/25 | Loss: 0.00133882
Iteration 25/25 | Loss: 0.00133882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133882
Iteration 2/1000 | Loss: 0.00004994
Iteration 3/1000 | Loss: 0.00002550
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00001888
Iteration 6/1000 | Loss: 0.00001792
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001675
Iteration 9/1000 | Loss: 0.00001634
Iteration 10/1000 | Loss: 0.00001606
Iteration 11/1000 | Loss: 0.00055849
Iteration 12/1000 | Loss: 0.00001910
Iteration 13/1000 | Loss: 0.00001550
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001361
Iteration 16/1000 | Loss: 0.00001306
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00001258
Iteration 26/1000 | Loss: 0.00001258
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001233
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001232
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001232
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001232
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001229
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001228
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001226
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001226
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001225
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001224
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001223
Iteration 94/1000 | Loss: 0.00001223
Iteration 95/1000 | Loss: 0.00001223
Iteration 96/1000 | Loss: 0.00001223
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001222
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001222
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001221
Iteration 122/1000 | Loss: 0.00001221
Iteration 123/1000 | Loss: 0.00001221
Iteration 124/1000 | Loss: 0.00001221
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001220
Iteration 133/1000 | Loss: 0.00001220
Iteration 134/1000 | Loss: 0.00001220
Iteration 135/1000 | Loss: 0.00001220
Iteration 136/1000 | Loss: 0.00001220
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001220
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001219
Iteration 142/1000 | Loss: 0.00001219
Iteration 143/1000 | Loss: 0.00001219
Iteration 144/1000 | Loss: 0.00001219
Iteration 145/1000 | Loss: 0.00001219
Iteration 146/1000 | Loss: 0.00001219
Iteration 147/1000 | Loss: 0.00001219
Iteration 148/1000 | Loss: 0.00001219
Iteration 149/1000 | Loss: 0.00001219
Iteration 150/1000 | Loss: 0.00001219
Iteration 151/1000 | Loss: 0.00001219
Iteration 152/1000 | Loss: 0.00001219
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001218
Iteration 159/1000 | Loss: 0.00001218
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Iteration 164/1000 | Loss: 0.00001218
Iteration 165/1000 | Loss: 0.00001218
Iteration 166/1000 | Loss: 0.00001218
Iteration 167/1000 | Loss: 0.00001218
Iteration 168/1000 | Loss: 0.00001218
Iteration 169/1000 | Loss: 0.00001218
Iteration 170/1000 | Loss: 0.00001218
Iteration 171/1000 | Loss: 0.00001218
Iteration 172/1000 | Loss: 0.00001218
Iteration 173/1000 | Loss: 0.00001218
Iteration 174/1000 | Loss: 0.00001218
Iteration 175/1000 | Loss: 0.00001218
Iteration 176/1000 | Loss: 0.00001218
Iteration 177/1000 | Loss: 0.00001218
Iteration 178/1000 | Loss: 0.00001218
Iteration 179/1000 | Loss: 0.00001218
Iteration 180/1000 | Loss: 0.00001218
Iteration 181/1000 | Loss: 0.00001218
Iteration 182/1000 | Loss: 0.00001218
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001217
Iteration 185/1000 | Loss: 0.00001217
Iteration 186/1000 | Loss: 0.00001217
Iteration 187/1000 | Loss: 0.00001217
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001217
Iteration 191/1000 | Loss: 0.00001217
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001217
Iteration 194/1000 | Loss: 0.00001217
Iteration 195/1000 | Loss: 0.00001217
Iteration 196/1000 | Loss: 0.00001217
Iteration 197/1000 | Loss: 0.00001217
Iteration 198/1000 | Loss: 0.00001217
Iteration 199/1000 | Loss: 0.00001217
Iteration 200/1000 | Loss: 0.00001217
Iteration 201/1000 | Loss: 0.00001217
Iteration 202/1000 | Loss: 0.00001216
Iteration 203/1000 | Loss: 0.00001216
Iteration 204/1000 | Loss: 0.00001216
Iteration 205/1000 | Loss: 0.00001216
Iteration 206/1000 | Loss: 0.00001216
Iteration 207/1000 | Loss: 0.00001216
Iteration 208/1000 | Loss: 0.00001216
Iteration 209/1000 | Loss: 0.00001216
Iteration 210/1000 | Loss: 0.00001216
Iteration 211/1000 | Loss: 0.00001216
Iteration 212/1000 | Loss: 0.00001216
Iteration 213/1000 | Loss: 0.00001216
Iteration 214/1000 | Loss: 0.00001216
Iteration 215/1000 | Loss: 0.00001216
Iteration 216/1000 | Loss: 0.00001216
Iteration 217/1000 | Loss: 0.00001216
Iteration 218/1000 | Loss: 0.00001216
Iteration 219/1000 | Loss: 0.00001216
Iteration 220/1000 | Loss: 0.00001216
Iteration 221/1000 | Loss: 0.00001216
Iteration 222/1000 | Loss: 0.00001216
Iteration 223/1000 | Loss: 0.00001216
Iteration 224/1000 | Loss: 0.00001216
Iteration 225/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.2162327038822696e-05, 1.2162327038822696e-05, 1.2162327038822696e-05, 1.2162327038822696e-05, 1.2162327038822696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2162327038822696e-05

Optimization complete. Final v2v error: 3.001866102218628 mm

Highest mean error: 3.7130300998687744 mm for frame 76

Lowest mean error: 2.5945727825164795 mm for frame 0

Saving results

Total time: 61.148160457611084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869299
Iteration 2/25 | Loss: 0.00137358
Iteration 3/25 | Loss: 0.00112740
Iteration 4/25 | Loss: 0.00109499
Iteration 5/25 | Loss: 0.00108911
Iteration 6/25 | Loss: 0.00108821
Iteration 7/25 | Loss: 0.00108821
Iteration 8/25 | Loss: 0.00108821
Iteration 9/25 | Loss: 0.00108821
Iteration 10/25 | Loss: 0.00108821
Iteration 11/25 | Loss: 0.00108821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010882144561037421, 0.0010882144561037421, 0.0010882144561037421, 0.0010882144561037421, 0.0010882144561037421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010882144561037421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88966960
Iteration 2/25 | Loss: 0.00077470
Iteration 3/25 | Loss: 0.00077469
Iteration 4/25 | Loss: 0.00077469
Iteration 5/25 | Loss: 0.00077469
Iteration 6/25 | Loss: 0.00077469
Iteration 7/25 | Loss: 0.00077469
Iteration 8/25 | Loss: 0.00077469
Iteration 9/25 | Loss: 0.00077469
Iteration 10/25 | Loss: 0.00077469
Iteration 11/25 | Loss: 0.00077469
Iteration 12/25 | Loss: 0.00077469
Iteration 13/25 | Loss: 0.00077469
Iteration 14/25 | Loss: 0.00077469
Iteration 15/25 | Loss: 0.00077469
Iteration 16/25 | Loss: 0.00077469
Iteration 17/25 | Loss: 0.00077469
Iteration 18/25 | Loss: 0.00077469
Iteration 19/25 | Loss: 0.00077469
Iteration 20/25 | Loss: 0.00077469
Iteration 21/25 | Loss: 0.00077469
Iteration 22/25 | Loss: 0.00077469
Iteration 23/25 | Loss: 0.00077469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007746872724965215, 0.0007746872724965215, 0.0007746872724965215, 0.0007746872724965215, 0.0007746872724965215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007746872724965215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077469
Iteration 2/1000 | Loss: 0.00005423
Iteration 3/1000 | Loss: 0.00003805
Iteration 4/1000 | Loss: 0.00003091
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00002685
Iteration 7/1000 | Loss: 0.00002559
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002384
Iteration 10/1000 | Loss: 0.00002358
Iteration 11/1000 | Loss: 0.00002328
Iteration 12/1000 | Loss: 0.00002310
Iteration 13/1000 | Loss: 0.00002294
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002286
Iteration 16/1000 | Loss: 0.00002286
Iteration 17/1000 | Loss: 0.00002286
Iteration 18/1000 | Loss: 0.00002286
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00002286
Iteration 21/1000 | Loss: 0.00002286
Iteration 22/1000 | Loss: 0.00002286
Iteration 23/1000 | Loss: 0.00002286
Iteration 24/1000 | Loss: 0.00002286
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002286
Iteration 28/1000 | Loss: 0.00002284
Iteration 29/1000 | Loss: 0.00002284
Iteration 30/1000 | Loss: 0.00002283
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002283
Iteration 33/1000 | Loss: 0.00002282
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002282
Iteration 39/1000 | Loss: 0.00002281
Iteration 40/1000 | Loss: 0.00002281
Iteration 41/1000 | Loss: 0.00002281
Iteration 42/1000 | Loss: 0.00002281
Iteration 43/1000 | Loss: 0.00002281
Iteration 44/1000 | Loss: 0.00002281
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002280
Iteration 48/1000 | Loss: 0.00002280
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002280
Iteration 51/1000 | Loss: 0.00002280
Iteration 52/1000 | Loss: 0.00002280
Iteration 53/1000 | Loss: 0.00002280
Iteration 54/1000 | Loss: 0.00002280
Iteration 55/1000 | Loss: 0.00002280
Iteration 56/1000 | Loss: 0.00002280
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002280
Iteration 60/1000 | Loss: 0.00002280
Iteration 61/1000 | Loss: 0.00002280
Iteration 62/1000 | Loss: 0.00002280
Iteration 63/1000 | Loss: 0.00002280
Iteration 64/1000 | Loss: 0.00002280
Iteration 65/1000 | Loss: 0.00002280
Iteration 66/1000 | Loss: 0.00002280
Iteration 67/1000 | Loss: 0.00002280
Iteration 68/1000 | Loss: 0.00002280
Iteration 69/1000 | Loss: 0.00002280
Iteration 70/1000 | Loss: 0.00002280
Iteration 71/1000 | Loss: 0.00002280
Iteration 72/1000 | Loss: 0.00002280
Iteration 73/1000 | Loss: 0.00002280
Iteration 74/1000 | Loss: 0.00002280
Iteration 75/1000 | Loss: 0.00002280
Iteration 76/1000 | Loss: 0.00002280
Iteration 77/1000 | Loss: 0.00002280
Iteration 78/1000 | Loss: 0.00002280
Iteration 79/1000 | Loss: 0.00002280
Iteration 80/1000 | Loss: 0.00002280
Iteration 81/1000 | Loss: 0.00002280
Iteration 82/1000 | Loss: 0.00002280
Iteration 83/1000 | Loss: 0.00002280
Iteration 84/1000 | Loss: 0.00002280
Iteration 85/1000 | Loss: 0.00002280
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002280
Iteration 91/1000 | Loss: 0.00002280
Iteration 92/1000 | Loss: 0.00002280
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002280
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002280
Iteration 100/1000 | Loss: 0.00002280
Iteration 101/1000 | Loss: 0.00002280
Iteration 102/1000 | Loss: 0.00002280
Iteration 103/1000 | Loss: 0.00002280
Iteration 104/1000 | Loss: 0.00002280
Iteration 105/1000 | Loss: 0.00002280
Iteration 106/1000 | Loss: 0.00002280
Iteration 107/1000 | Loss: 0.00002280
Iteration 108/1000 | Loss: 0.00002280
Iteration 109/1000 | Loss: 0.00002280
Iteration 110/1000 | Loss: 0.00002280
Iteration 111/1000 | Loss: 0.00002280
Iteration 112/1000 | Loss: 0.00002280
Iteration 113/1000 | Loss: 0.00002280
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002280
Iteration 118/1000 | Loss: 0.00002280
Iteration 119/1000 | Loss: 0.00002280
Iteration 120/1000 | Loss: 0.00002280
Iteration 121/1000 | Loss: 0.00002280
Iteration 122/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.2803640604251996e-05, 2.2803640604251996e-05, 2.2803640604251996e-05, 2.2803640604251996e-05, 2.2803640604251996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2803640604251996e-05

Optimization complete. Final v2v error: 3.9697933197021484 mm

Highest mean error: 4.2176690101623535 mm for frame 149

Lowest mean error: 3.706181049346924 mm for frame 92

Saving results

Total time: 31.13399362564087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386096
Iteration 2/25 | Loss: 0.00116532
Iteration 3/25 | Loss: 0.00106697
Iteration 4/25 | Loss: 0.00105661
Iteration 5/25 | Loss: 0.00105274
Iteration 6/25 | Loss: 0.00105216
Iteration 7/25 | Loss: 0.00105216
Iteration 8/25 | Loss: 0.00105216
Iteration 9/25 | Loss: 0.00105216
Iteration 10/25 | Loss: 0.00105216
Iteration 11/25 | Loss: 0.00105216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010521593503654003, 0.0010521593503654003, 0.0010521593503654003, 0.0010521593503654003, 0.0010521593503654003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010521593503654003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52124798
Iteration 2/25 | Loss: 0.00128725
Iteration 3/25 | Loss: 0.00128724
Iteration 4/25 | Loss: 0.00128724
Iteration 5/25 | Loss: 0.00128724
Iteration 6/25 | Loss: 0.00128724
Iteration 7/25 | Loss: 0.00128724
Iteration 8/25 | Loss: 0.00128724
Iteration 9/25 | Loss: 0.00128724
Iteration 10/25 | Loss: 0.00128724
Iteration 11/25 | Loss: 0.00128724
Iteration 12/25 | Loss: 0.00128724
Iteration 13/25 | Loss: 0.00128724
Iteration 14/25 | Loss: 0.00128724
Iteration 15/25 | Loss: 0.00128724
Iteration 16/25 | Loss: 0.00128724
Iteration 17/25 | Loss: 0.00128724
Iteration 18/25 | Loss: 0.00128724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012872377410531044, 0.0012872377410531044, 0.0012872377410531044, 0.0012872377410531044, 0.0012872377410531044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012872377410531044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128724
Iteration 2/1000 | Loss: 0.00003724
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001209
Iteration 7/1000 | Loss: 0.00001157
Iteration 8/1000 | Loss: 0.00001126
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001058
Iteration 15/1000 | Loss: 0.00001057
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001052
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001050
Iteration 25/1000 | Loss: 0.00001050
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001050
Iteration 30/1000 | Loss: 0.00001050
Iteration 31/1000 | Loss: 0.00001050
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001050
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001050
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 39. Stopping optimization.
Last 5 losses: [1.0495038623048458e-05, 1.0495038623048458e-05, 1.0495038623048458e-05, 1.0495038623048458e-05, 1.0495038623048458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0495038623048458e-05

Optimization complete. Final v2v error: 2.7538609504699707 mm

Highest mean error: 2.9783191680908203 mm for frame 148

Lowest mean error: 2.4938015937805176 mm for frame 233

Saving results

Total time: 28.81014370918274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126627
Iteration 2/25 | Loss: 0.01126627
Iteration 3/25 | Loss: 0.00310596
Iteration 4/25 | Loss: 0.00167444
Iteration 5/25 | Loss: 0.00146744
Iteration 6/25 | Loss: 0.00142135
Iteration 7/25 | Loss: 0.00133010
Iteration 8/25 | Loss: 0.00124734
Iteration 9/25 | Loss: 0.00118771
Iteration 10/25 | Loss: 0.00115350
Iteration 11/25 | Loss: 0.00114249
Iteration 12/25 | Loss: 0.00112898
Iteration 13/25 | Loss: 0.00112015
Iteration 14/25 | Loss: 0.00111418
Iteration 15/25 | Loss: 0.00111095
Iteration 16/25 | Loss: 0.00110174
Iteration 17/25 | Loss: 0.00109635
Iteration 18/25 | Loss: 0.00109421
Iteration 19/25 | Loss: 0.00109454
Iteration 20/25 | Loss: 0.00109251
Iteration 21/25 | Loss: 0.00109218
Iteration 22/25 | Loss: 0.00109064
Iteration 23/25 | Loss: 0.00108896
Iteration 24/25 | Loss: 0.00108847
Iteration 25/25 | Loss: 0.00108760

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29257250
Iteration 2/25 | Loss: 0.00149925
Iteration 3/25 | Loss: 0.00149925
Iteration 4/25 | Loss: 0.00149925
Iteration 5/25 | Loss: 0.00149925
Iteration 6/25 | Loss: 0.00149925
Iteration 7/25 | Loss: 0.00149925
Iteration 8/25 | Loss: 0.00149925
Iteration 9/25 | Loss: 0.00149925
Iteration 10/25 | Loss: 0.00149925
Iteration 11/25 | Loss: 0.00149925
Iteration 12/25 | Loss: 0.00149925
Iteration 13/25 | Loss: 0.00149925
Iteration 14/25 | Loss: 0.00149925
Iteration 15/25 | Loss: 0.00149925
Iteration 16/25 | Loss: 0.00149925
Iteration 17/25 | Loss: 0.00149925
Iteration 18/25 | Loss: 0.00149925
Iteration 19/25 | Loss: 0.00149925
Iteration 20/25 | Loss: 0.00149925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001499245292507112, 0.001499245292507112, 0.001499245292507112, 0.001499245292507112, 0.001499245292507112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001499245292507112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149925
Iteration 2/1000 | Loss: 0.00005273
Iteration 3/1000 | Loss: 0.00004461
Iteration 4/1000 | Loss: 0.00002939
Iteration 5/1000 | Loss: 0.00004243
Iteration 6/1000 | Loss: 0.00003086
Iteration 7/1000 | Loss: 0.00002818
Iteration 8/1000 | Loss: 0.00003606
Iteration 9/1000 | Loss: 0.00003136
Iteration 10/1000 | Loss: 0.00003602
Iteration 11/1000 | Loss: 0.00003522
Iteration 12/1000 | Loss: 0.00004075
Iteration 13/1000 | Loss: 0.00003454
Iteration 14/1000 | Loss: 0.00003469
Iteration 15/1000 | Loss: 0.00003245
Iteration 16/1000 | Loss: 0.00003446
Iteration 17/1000 | Loss: 0.00003182
Iteration 18/1000 | Loss: 0.00002850
Iteration 19/1000 | Loss: 0.00003991
Iteration 20/1000 | Loss: 0.00003217
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00039429
Iteration 23/1000 | Loss: 0.00002549
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00001984
Iteration 26/1000 | Loss: 0.00001911
Iteration 27/1000 | Loss: 0.00001862
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001805
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001786
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001779
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001771
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001755
Iteration 39/1000 | Loss: 0.00001753
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001752
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00018056
Iteration 48/1000 | Loss: 0.00002266
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00001966
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001825
Iteration 55/1000 | Loss: 0.00018252
Iteration 56/1000 | Loss: 0.00016599
Iteration 57/1000 | Loss: 0.00020092
Iteration 58/1000 | Loss: 0.00002688
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00001955
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001693
Iteration 71/1000 | Loss: 0.00001692
Iteration 72/1000 | Loss: 0.00001692
Iteration 73/1000 | Loss: 0.00001691
Iteration 74/1000 | Loss: 0.00001691
Iteration 75/1000 | Loss: 0.00001690
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001683
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001683
Iteration 84/1000 | Loss: 0.00001683
Iteration 85/1000 | Loss: 0.00001683
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001680
Iteration 91/1000 | Loss: 0.00001680
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001676
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001675
Iteration 113/1000 | Loss: 0.00001675
Iteration 114/1000 | Loss: 0.00001675
Iteration 115/1000 | Loss: 0.00001675
Iteration 116/1000 | Loss: 0.00001675
Iteration 117/1000 | Loss: 0.00001675
Iteration 118/1000 | Loss: 0.00001675
Iteration 119/1000 | Loss: 0.00001674
Iteration 120/1000 | Loss: 0.00001674
Iteration 121/1000 | Loss: 0.00001674
Iteration 122/1000 | Loss: 0.00001674
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001673
Iteration 125/1000 | Loss: 0.00001673
Iteration 126/1000 | Loss: 0.00001673
Iteration 127/1000 | Loss: 0.00001672
Iteration 128/1000 | Loss: 0.00001672
Iteration 129/1000 | Loss: 0.00001672
Iteration 130/1000 | Loss: 0.00001672
Iteration 131/1000 | Loss: 0.00001672
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001671
Iteration 135/1000 | Loss: 0.00001671
Iteration 136/1000 | Loss: 0.00001671
Iteration 137/1000 | Loss: 0.00001671
Iteration 138/1000 | Loss: 0.00001671
Iteration 139/1000 | Loss: 0.00001671
Iteration 140/1000 | Loss: 0.00001671
Iteration 141/1000 | Loss: 0.00001671
Iteration 142/1000 | Loss: 0.00001671
Iteration 143/1000 | Loss: 0.00001671
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001671
Iteration 146/1000 | Loss: 0.00001671
Iteration 147/1000 | Loss: 0.00001671
Iteration 148/1000 | Loss: 0.00001671
Iteration 149/1000 | Loss: 0.00001671
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001670
Iteration 152/1000 | Loss: 0.00001670
Iteration 153/1000 | Loss: 0.00001670
Iteration 154/1000 | Loss: 0.00001670
Iteration 155/1000 | Loss: 0.00001670
Iteration 156/1000 | Loss: 0.00001670
Iteration 157/1000 | Loss: 0.00001670
Iteration 158/1000 | Loss: 0.00001670
Iteration 159/1000 | Loss: 0.00001670
Iteration 160/1000 | Loss: 0.00001670
Iteration 161/1000 | Loss: 0.00001670
Iteration 162/1000 | Loss: 0.00001670
Iteration 163/1000 | Loss: 0.00001670
Iteration 164/1000 | Loss: 0.00001669
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001669
Iteration 167/1000 | Loss: 0.00001669
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001669
Iteration 170/1000 | Loss: 0.00001669
Iteration 171/1000 | Loss: 0.00001669
Iteration 172/1000 | Loss: 0.00001669
Iteration 173/1000 | Loss: 0.00001669
Iteration 174/1000 | Loss: 0.00001669
Iteration 175/1000 | Loss: 0.00001669
Iteration 176/1000 | Loss: 0.00001669
Iteration 177/1000 | Loss: 0.00001669
Iteration 178/1000 | Loss: 0.00001669
Iteration 179/1000 | Loss: 0.00001669
Iteration 180/1000 | Loss: 0.00001669
Iteration 181/1000 | Loss: 0.00001669
Iteration 182/1000 | Loss: 0.00001669
Iteration 183/1000 | Loss: 0.00001669
Iteration 184/1000 | Loss: 0.00001669
Iteration 185/1000 | Loss: 0.00001669
Iteration 186/1000 | Loss: 0.00001669
Iteration 187/1000 | Loss: 0.00001669
Iteration 188/1000 | Loss: 0.00001669
Iteration 189/1000 | Loss: 0.00001669
Iteration 190/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.6692403733031824e-05, 1.6692403733031824e-05, 1.6692403733031824e-05, 1.6692403733031824e-05, 1.6692403733031824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6692403733031824e-05

Optimization complete. Final v2v error: 3.4913690090179443 mm

Highest mean error: 5.334838390350342 mm for frame 81

Lowest mean error: 3.0951244831085205 mm for frame 42

Saving results

Total time: 127.43701958656311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472788
Iteration 2/25 | Loss: 0.00126630
Iteration 3/25 | Loss: 0.00108132
Iteration 4/25 | Loss: 0.00106864
Iteration 5/25 | Loss: 0.00106321
Iteration 6/25 | Loss: 0.00106213
Iteration 7/25 | Loss: 0.00106213
Iteration 8/25 | Loss: 0.00106213
Iteration 9/25 | Loss: 0.00106213
Iteration 10/25 | Loss: 0.00106213
Iteration 11/25 | Loss: 0.00106213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010621275287121534, 0.0010621275287121534, 0.0010621275287121534, 0.0010621275287121534, 0.0010621275287121534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010621275287121534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.73998690
Iteration 2/25 | Loss: 0.00125702
Iteration 3/25 | Loss: 0.00125702
Iteration 4/25 | Loss: 0.00125702
Iteration 5/25 | Loss: 0.00125702
Iteration 6/25 | Loss: 0.00125702
Iteration 7/25 | Loss: 0.00125702
Iteration 8/25 | Loss: 0.00125702
Iteration 9/25 | Loss: 0.00125702
Iteration 10/25 | Loss: 0.00125702
Iteration 11/25 | Loss: 0.00125702
Iteration 12/25 | Loss: 0.00125702
Iteration 13/25 | Loss: 0.00125702
Iteration 14/25 | Loss: 0.00125702
Iteration 15/25 | Loss: 0.00125702
Iteration 16/25 | Loss: 0.00125702
Iteration 17/25 | Loss: 0.00125702
Iteration 18/25 | Loss: 0.00125702
Iteration 19/25 | Loss: 0.00125702
Iteration 20/25 | Loss: 0.00125702
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012570155085995793, 0.0012570155085995793, 0.0012570155085995793, 0.0012570155085995793, 0.0012570155085995793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012570155085995793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125702
Iteration 2/1000 | Loss: 0.00005725
Iteration 3/1000 | Loss: 0.00003497
Iteration 4/1000 | Loss: 0.00003022
Iteration 5/1000 | Loss: 0.00002851
Iteration 6/1000 | Loss: 0.00002756
Iteration 7/1000 | Loss: 0.00002680
Iteration 8/1000 | Loss: 0.00002624
Iteration 9/1000 | Loss: 0.00002597
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002564
Iteration 12/1000 | Loss: 0.00002550
Iteration 13/1000 | Loss: 0.00002533
Iteration 14/1000 | Loss: 0.00002512
Iteration 15/1000 | Loss: 0.00002496
Iteration 16/1000 | Loss: 0.00002489
Iteration 17/1000 | Loss: 0.00002475
Iteration 18/1000 | Loss: 0.00002458
Iteration 19/1000 | Loss: 0.00002445
Iteration 20/1000 | Loss: 0.00002431
Iteration 21/1000 | Loss: 0.00002424
Iteration 22/1000 | Loss: 0.00002405
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002393
Iteration 25/1000 | Loss: 0.00002385
Iteration 26/1000 | Loss: 0.00002383
Iteration 27/1000 | Loss: 0.00002380
Iteration 28/1000 | Loss: 0.00002369
Iteration 29/1000 | Loss: 0.00002369
Iteration 30/1000 | Loss: 0.00002366
Iteration 31/1000 | Loss: 0.00002366
Iteration 32/1000 | Loss: 0.00002365
Iteration 33/1000 | Loss: 0.00002365
Iteration 34/1000 | Loss: 0.00002365
Iteration 35/1000 | Loss: 0.00002364
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002364
Iteration 38/1000 | Loss: 0.00002364
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002364
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002364
Iteration 43/1000 | Loss: 0.00002364
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002362
Iteration 46/1000 | Loss: 0.00002362
Iteration 47/1000 | Loss: 0.00002362
Iteration 48/1000 | Loss: 0.00002362
Iteration 49/1000 | Loss: 0.00002362
Iteration 50/1000 | Loss: 0.00002361
Iteration 51/1000 | Loss: 0.00002361
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002360
Iteration 55/1000 | Loss: 0.00002360
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002360
Iteration 58/1000 | Loss: 0.00002358
Iteration 59/1000 | Loss: 0.00002358
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002354
Iteration 62/1000 | Loss: 0.00002354
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002354
Iteration 66/1000 | Loss: 0.00002354
Iteration 67/1000 | Loss: 0.00002354
Iteration 68/1000 | Loss: 0.00002354
Iteration 69/1000 | Loss: 0.00002354
Iteration 70/1000 | Loss: 0.00002353
Iteration 71/1000 | Loss: 0.00002353
Iteration 72/1000 | Loss: 0.00002353
Iteration 73/1000 | Loss: 0.00002353
Iteration 74/1000 | Loss: 0.00002353
Iteration 75/1000 | Loss: 0.00002353
Iteration 76/1000 | Loss: 0.00002353
Iteration 77/1000 | Loss: 0.00002353
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00002352
Iteration 80/1000 | Loss: 0.00002352
Iteration 81/1000 | Loss: 0.00002351
Iteration 82/1000 | Loss: 0.00002351
Iteration 83/1000 | Loss: 0.00002350
Iteration 84/1000 | Loss: 0.00002350
Iteration 85/1000 | Loss: 0.00002348
Iteration 86/1000 | Loss: 0.00002345
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00002342
Iteration 91/1000 | Loss: 0.00002342
Iteration 92/1000 | Loss: 0.00002342
Iteration 93/1000 | Loss: 0.00002341
Iteration 94/1000 | Loss: 0.00002341
Iteration 95/1000 | Loss: 0.00002341
Iteration 96/1000 | Loss: 0.00002341
Iteration 97/1000 | Loss: 0.00002340
Iteration 98/1000 | Loss: 0.00002340
Iteration 99/1000 | Loss: 0.00002340
Iteration 100/1000 | Loss: 0.00002339
Iteration 101/1000 | Loss: 0.00002339
Iteration 102/1000 | Loss: 0.00002339
Iteration 103/1000 | Loss: 0.00002339
Iteration 104/1000 | Loss: 0.00002339
Iteration 105/1000 | Loss: 0.00002339
Iteration 106/1000 | Loss: 0.00002339
Iteration 107/1000 | Loss: 0.00002338
Iteration 108/1000 | Loss: 0.00002338
Iteration 109/1000 | Loss: 0.00002338
Iteration 110/1000 | Loss: 0.00002338
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002337
Iteration 117/1000 | Loss: 0.00002337
Iteration 118/1000 | Loss: 0.00002337
Iteration 119/1000 | Loss: 0.00002337
Iteration 120/1000 | Loss: 0.00002337
Iteration 121/1000 | Loss: 0.00002336
Iteration 122/1000 | Loss: 0.00002336
Iteration 123/1000 | Loss: 0.00002336
Iteration 124/1000 | Loss: 0.00002336
Iteration 125/1000 | Loss: 0.00002336
Iteration 126/1000 | Loss: 0.00002335
Iteration 127/1000 | Loss: 0.00002335
Iteration 128/1000 | Loss: 0.00002335
Iteration 129/1000 | Loss: 0.00002335
Iteration 130/1000 | Loss: 0.00002335
Iteration 131/1000 | Loss: 0.00002335
Iteration 132/1000 | Loss: 0.00002335
Iteration 133/1000 | Loss: 0.00002335
Iteration 134/1000 | Loss: 0.00002335
Iteration 135/1000 | Loss: 0.00002335
Iteration 136/1000 | Loss: 0.00002334
Iteration 137/1000 | Loss: 0.00002334
Iteration 138/1000 | Loss: 0.00002334
Iteration 139/1000 | Loss: 0.00002333
Iteration 140/1000 | Loss: 0.00002333
Iteration 141/1000 | Loss: 0.00002333
Iteration 142/1000 | Loss: 0.00002333
Iteration 143/1000 | Loss: 0.00002332
Iteration 144/1000 | Loss: 0.00002332
Iteration 145/1000 | Loss: 0.00002332
Iteration 146/1000 | Loss: 0.00002332
Iteration 147/1000 | Loss: 0.00002332
Iteration 148/1000 | Loss: 0.00002332
Iteration 149/1000 | Loss: 0.00002332
Iteration 150/1000 | Loss: 0.00002332
Iteration 151/1000 | Loss: 0.00002332
Iteration 152/1000 | Loss: 0.00002332
Iteration 153/1000 | Loss: 0.00002331
Iteration 154/1000 | Loss: 0.00002331
Iteration 155/1000 | Loss: 0.00002331
Iteration 156/1000 | Loss: 0.00002331
Iteration 157/1000 | Loss: 0.00002331
Iteration 158/1000 | Loss: 0.00002331
Iteration 159/1000 | Loss: 0.00002331
Iteration 160/1000 | Loss: 0.00002331
Iteration 161/1000 | Loss: 0.00002330
Iteration 162/1000 | Loss: 0.00002330
Iteration 163/1000 | Loss: 0.00002330
Iteration 164/1000 | Loss: 0.00002330
Iteration 165/1000 | Loss: 0.00002330
Iteration 166/1000 | Loss: 0.00002330
Iteration 167/1000 | Loss: 0.00002330
Iteration 168/1000 | Loss: 0.00002329
Iteration 169/1000 | Loss: 0.00002329
Iteration 170/1000 | Loss: 0.00002329
Iteration 171/1000 | Loss: 0.00002329
Iteration 172/1000 | Loss: 0.00002329
Iteration 173/1000 | Loss: 0.00002329
Iteration 174/1000 | Loss: 0.00002329
Iteration 175/1000 | Loss: 0.00002329
Iteration 176/1000 | Loss: 0.00002329
Iteration 177/1000 | Loss: 0.00002328
Iteration 178/1000 | Loss: 0.00002328
Iteration 179/1000 | Loss: 0.00002328
Iteration 180/1000 | Loss: 0.00002328
Iteration 181/1000 | Loss: 0.00002328
Iteration 182/1000 | Loss: 0.00002328
Iteration 183/1000 | Loss: 0.00002328
Iteration 184/1000 | Loss: 0.00002328
Iteration 185/1000 | Loss: 0.00002327
Iteration 186/1000 | Loss: 0.00002327
Iteration 187/1000 | Loss: 0.00002327
Iteration 188/1000 | Loss: 0.00002327
Iteration 189/1000 | Loss: 0.00002327
Iteration 190/1000 | Loss: 0.00002327
Iteration 191/1000 | Loss: 0.00002327
Iteration 192/1000 | Loss: 0.00002327
Iteration 193/1000 | Loss: 0.00002327
Iteration 194/1000 | Loss: 0.00002327
Iteration 195/1000 | Loss: 0.00002327
Iteration 196/1000 | Loss: 0.00002327
Iteration 197/1000 | Loss: 0.00002327
Iteration 198/1000 | Loss: 0.00002327
Iteration 199/1000 | Loss: 0.00002327
Iteration 200/1000 | Loss: 0.00002327
Iteration 201/1000 | Loss: 0.00002327
Iteration 202/1000 | Loss: 0.00002327
Iteration 203/1000 | Loss: 0.00002327
Iteration 204/1000 | Loss: 0.00002327
Iteration 205/1000 | Loss: 0.00002327
Iteration 206/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.32666679949034e-05, 2.32666679949034e-05, 2.32666679949034e-05, 2.32666679949034e-05, 2.32666679949034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.32666679949034e-05

Optimization complete. Final v2v error: 3.907242774963379 mm

Highest mean error: 4.343188762664795 mm for frame 255

Lowest mean error: 3.7688686847686768 mm for frame 112

Saving results

Total time: 61.205437421798706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028197
Iteration 2/25 | Loss: 0.00380403
Iteration 3/25 | Loss: 0.00226204
Iteration 4/25 | Loss: 0.00198427
Iteration 5/25 | Loss: 0.00181746
Iteration 6/25 | Loss: 0.00172782
Iteration 7/25 | Loss: 0.00168879
Iteration 8/25 | Loss: 0.00167032
Iteration 9/25 | Loss: 0.00165061
Iteration 10/25 | Loss: 0.00164279
Iteration 11/25 | Loss: 0.00164743
Iteration 12/25 | Loss: 0.00162869
Iteration 13/25 | Loss: 0.00162201
Iteration 14/25 | Loss: 0.00161695
Iteration 15/25 | Loss: 0.00161549
Iteration 16/25 | Loss: 0.00161408
Iteration 17/25 | Loss: 0.00161305
Iteration 18/25 | Loss: 0.00161355
Iteration 19/25 | Loss: 0.00161156
Iteration 20/25 | Loss: 0.00161102
Iteration 21/25 | Loss: 0.00161064
Iteration 22/25 | Loss: 0.00161030
Iteration 23/25 | Loss: 0.00161105
Iteration 24/25 | Loss: 0.00161020
Iteration 25/25 | Loss: 0.00161085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23323357
Iteration 2/25 | Loss: 0.00872835
Iteration 3/25 | Loss: 0.00842056
Iteration 4/25 | Loss: 0.00842056
Iteration 5/25 | Loss: 0.00842056
Iteration 6/25 | Loss: 0.00842056
Iteration 7/25 | Loss: 0.00842056
Iteration 8/25 | Loss: 0.00842056
Iteration 9/25 | Loss: 0.00842056
Iteration 10/25 | Loss: 0.00842056
Iteration 11/25 | Loss: 0.00842056
Iteration 12/25 | Loss: 0.00842056
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.008420557714998722, 0.008420557714998722, 0.008420557714998722, 0.008420557714998722, 0.008420557714998722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008420557714998722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00842056
Iteration 2/1000 | Loss: 0.00129288
Iteration 3/1000 | Loss: 0.00321685
Iteration 4/1000 | Loss: 0.00838173
Iteration 5/1000 | Loss: 0.00205010
Iteration 6/1000 | Loss: 0.00297282
Iteration 7/1000 | Loss: 0.00223667
Iteration 8/1000 | Loss: 0.00291147
Iteration 9/1000 | Loss: 0.00483625
Iteration 10/1000 | Loss: 0.00096819
Iteration 11/1000 | Loss: 0.00145769
Iteration 12/1000 | Loss: 0.00189550
Iteration 13/1000 | Loss: 0.00151606
Iteration 14/1000 | Loss: 0.00059736
Iteration 15/1000 | Loss: 0.00045443
Iteration 16/1000 | Loss: 0.00075772
Iteration 17/1000 | Loss: 0.00111029
Iteration 18/1000 | Loss: 0.00053728
Iteration 19/1000 | Loss: 0.00112174
Iteration 20/1000 | Loss: 0.00055556
Iteration 21/1000 | Loss: 0.00042425
Iteration 22/1000 | Loss: 0.00260021
Iteration 23/1000 | Loss: 0.00040002
Iteration 24/1000 | Loss: 0.00051773
Iteration 25/1000 | Loss: 0.00044615
Iteration 26/1000 | Loss: 0.00046261
Iteration 27/1000 | Loss: 0.00030257
Iteration 28/1000 | Loss: 0.00061966
Iteration 29/1000 | Loss: 0.00026846
Iteration 30/1000 | Loss: 0.00054619
Iteration 31/1000 | Loss: 0.00048592
Iteration 32/1000 | Loss: 0.00266190
Iteration 33/1000 | Loss: 0.00050907
Iteration 34/1000 | Loss: 0.00086918
Iteration 35/1000 | Loss: 0.00048402
Iteration 36/1000 | Loss: 0.00037042
Iteration 37/1000 | Loss: 0.00051939
Iteration 38/1000 | Loss: 0.00045436
Iteration 39/1000 | Loss: 0.00057571
Iteration 40/1000 | Loss: 0.00023393
Iteration 41/1000 | Loss: 0.00021325
Iteration 42/1000 | Loss: 0.00022610
Iteration 43/1000 | Loss: 0.00073250
Iteration 44/1000 | Loss: 0.00035515
Iteration 45/1000 | Loss: 0.00050060
Iteration 46/1000 | Loss: 0.00080278
Iteration 47/1000 | Loss: 0.00144722
Iteration 48/1000 | Loss: 0.00245022
Iteration 49/1000 | Loss: 0.00111626
Iteration 50/1000 | Loss: 0.00171101
Iteration 51/1000 | Loss: 0.00019617
Iteration 52/1000 | Loss: 0.00019697
Iteration 53/1000 | Loss: 0.00019430
Iteration 54/1000 | Loss: 0.00060416
Iteration 55/1000 | Loss: 0.00019073
Iteration 56/1000 | Loss: 0.00060754
Iteration 57/1000 | Loss: 0.00117648
Iteration 58/1000 | Loss: 0.00058731
Iteration 59/1000 | Loss: 0.00021344
Iteration 60/1000 | Loss: 0.00019978
Iteration 61/1000 | Loss: 0.00030645
Iteration 62/1000 | Loss: 0.00019521
Iteration 63/1000 | Loss: 0.00021535
Iteration 64/1000 | Loss: 0.00018139
Iteration 65/1000 | Loss: 0.00023334
Iteration 66/1000 | Loss: 0.00017599
Iteration 67/1000 | Loss: 0.00034762
Iteration 68/1000 | Loss: 0.00018306
Iteration 69/1000 | Loss: 0.00017748
Iteration 70/1000 | Loss: 0.00035122
Iteration 71/1000 | Loss: 0.00019243
Iteration 72/1000 | Loss: 0.00022094
Iteration 73/1000 | Loss: 0.00021590
Iteration 74/1000 | Loss: 0.00020438
Iteration 75/1000 | Loss: 0.00017180
Iteration 76/1000 | Loss: 0.00037466
Iteration 77/1000 | Loss: 0.00064693
Iteration 78/1000 | Loss: 0.00076894
Iteration 79/1000 | Loss: 0.00034867
Iteration 80/1000 | Loss: 0.00043408
Iteration 81/1000 | Loss: 0.00025282
Iteration 82/1000 | Loss: 0.00030814
Iteration 83/1000 | Loss: 0.00027419
Iteration 84/1000 | Loss: 0.00019353
Iteration 85/1000 | Loss: 0.00018247
Iteration 86/1000 | Loss: 0.00017934
Iteration 87/1000 | Loss: 0.00042986
Iteration 88/1000 | Loss: 0.00059245
Iteration 89/1000 | Loss: 0.00044316
Iteration 90/1000 | Loss: 0.00037847
Iteration 91/1000 | Loss: 0.00021822
Iteration 92/1000 | Loss: 0.00019813
Iteration 93/1000 | Loss: 0.00017282
Iteration 94/1000 | Loss: 0.00017215
Iteration 95/1000 | Loss: 0.00017710
Iteration 96/1000 | Loss: 0.00055797
Iteration 97/1000 | Loss: 0.00119523
Iteration 98/1000 | Loss: 0.00049350
Iteration 99/1000 | Loss: 0.00066508
Iteration 100/1000 | Loss: 0.00043232
Iteration 101/1000 | Loss: 0.00017400
Iteration 102/1000 | Loss: 0.00022532
Iteration 103/1000 | Loss: 0.00016887
Iteration 104/1000 | Loss: 0.00017565
Iteration 105/1000 | Loss: 0.00017153
Iteration 106/1000 | Loss: 0.00015810
Iteration 107/1000 | Loss: 0.00048575
Iteration 108/1000 | Loss: 0.00129490
Iteration 109/1000 | Loss: 0.00103812
Iteration 110/1000 | Loss: 0.00041220
Iteration 111/1000 | Loss: 0.00028141
Iteration 112/1000 | Loss: 0.00026528
Iteration 113/1000 | Loss: 0.00028883
Iteration 114/1000 | Loss: 0.00020006
Iteration 115/1000 | Loss: 0.00021542
Iteration 116/1000 | Loss: 0.00035632
Iteration 117/1000 | Loss: 0.00028146
Iteration 118/1000 | Loss: 0.00036271
Iteration 119/1000 | Loss: 0.00040072
Iteration 120/1000 | Loss: 0.00020276
Iteration 121/1000 | Loss: 0.00061428
Iteration 122/1000 | Loss: 0.00030032
Iteration 123/1000 | Loss: 0.00039903
Iteration 124/1000 | Loss: 0.00016308
Iteration 125/1000 | Loss: 0.00018151
Iteration 126/1000 | Loss: 0.00015584
Iteration 127/1000 | Loss: 0.00015407
Iteration 128/1000 | Loss: 0.00018579
Iteration 129/1000 | Loss: 0.00065999
Iteration 130/1000 | Loss: 0.00065992
Iteration 131/1000 | Loss: 0.00090123
Iteration 132/1000 | Loss: 0.00067247
Iteration 133/1000 | Loss: 0.00038587
Iteration 134/1000 | Loss: 0.00052088
Iteration 135/1000 | Loss: 0.00041726
Iteration 136/1000 | Loss: 0.00025516
Iteration 137/1000 | Loss: 0.00015491
Iteration 138/1000 | Loss: 0.00015494
Iteration 139/1000 | Loss: 0.00015674
Iteration 140/1000 | Loss: 0.00039161
Iteration 141/1000 | Loss: 0.00079335
Iteration 142/1000 | Loss: 0.00104891
Iteration 143/1000 | Loss: 0.00127534
Iteration 144/1000 | Loss: 0.00149595
Iteration 145/1000 | Loss: 0.00058895
Iteration 146/1000 | Loss: 0.00090946
Iteration 147/1000 | Loss: 0.00052350
Iteration 148/1000 | Loss: 0.00073195
Iteration 149/1000 | Loss: 0.00021627
Iteration 150/1000 | Loss: 0.00024621
Iteration 151/1000 | Loss: 0.00018206
Iteration 152/1000 | Loss: 0.00014806
Iteration 153/1000 | Loss: 0.00014860
Iteration 154/1000 | Loss: 0.00024884
Iteration 155/1000 | Loss: 0.00027892
Iteration 156/1000 | Loss: 0.00074179
Iteration 157/1000 | Loss: 0.00027786
Iteration 158/1000 | Loss: 0.00025480
Iteration 159/1000 | Loss: 0.00023865
Iteration 160/1000 | Loss: 0.00021241
Iteration 161/1000 | Loss: 0.00051079
Iteration 162/1000 | Loss: 0.00038668
Iteration 163/1000 | Loss: 0.00017116
Iteration 164/1000 | Loss: 0.00015563
Iteration 165/1000 | Loss: 0.00014938
Iteration 166/1000 | Loss: 0.00018162
Iteration 167/1000 | Loss: 0.00016443
Iteration 168/1000 | Loss: 0.00081588
Iteration 169/1000 | Loss: 0.00035550
Iteration 170/1000 | Loss: 0.00029410
Iteration 171/1000 | Loss: 0.00014273
Iteration 172/1000 | Loss: 0.00014936
Iteration 173/1000 | Loss: 0.00013538
Iteration 174/1000 | Loss: 0.00087294
Iteration 175/1000 | Loss: 0.00029467
Iteration 176/1000 | Loss: 0.00015738
Iteration 177/1000 | Loss: 0.00019085
Iteration 178/1000 | Loss: 0.00013797
Iteration 179/1000 | Loss: 0.00014005
Iteration 180/1000 | Loss: 0.00014755
Iteration 181/1000 | Loss: 0.00012857
Iteration 182/1000 | Loss: 0.00038416
Iteration 183/1000 | Loss: 0.00014035
Iteration 184/1000 | Loss: 0.00013188
Iteration 185/1000 | Loss: 0.00035591
Iteration 186/1000 | Loss: 0.00017868
Iteration 187/1000 | Loss: 0.00013180
Iteration 188/1000 | Loss: 0.00042996
Iteration 189/1000 | Loss: 0.00014845
Iteration 190/1000 | Loss: 0.00013847
Iteration 191/1000 | Loss: 0.00013175
Iteration 192/1000 | Loss: 0.00012663
Iteration 193/1000 | Loss: 0.00043928
Iteration 194/1000 | Loss: 0.00041479
Iteration 195/1000 | Loss: 0.00014193
Iteration 196/1000 | Loss: 0.00016233
Iteration 197/1000 | Loss: 0.00012671
Iteration 198/1000 | Loss: 0.00039100
Iteration 199/1000 | Loss: 0.00040300
Iteration 200/1000 | Loss: 0.00035602
Iteration 201/1000 | Loss: 0.00013214
Iteration 202/1000 | Loss: 0.00038305
Iteration 203/1000 | Loss: 0.00032821
Iteration 204/1000 | Loss: 0.00036075
Iteration 205/1000 | Loss: 0.00021734
Iteration 206/1000 | Loss: 0.00025350
Iteration 207/1000 | Loss: 0.00012012
Iteration 208/1000 | Loss: 0.00037424
Iteration 209/1000 | Loss: 0.00013616
Iteration 210/1000 | Loss: 0.00020145
Iteration 211/1000 | Loss: 0.00012305
Iteration 212/1000 | Loss: 0.00012380
Iteration 213/1000 | Loss: 0.00012838
Iteration 214/1000 | Loss: 0.00012798
Iteration 215/1000 | Loss: 0.00011317
Iteration 216/1000 | Loss: 0.00060371
Iteration 217/1000 | Loss: 0.00047089
Iteration 218/1000 | Loss: 0.00055827
Iteration 219/1000 | Loss: 0.00011577
Iteration 220/1000 | Loss: 0.00011718
Iteration 221/1000 | Loss: 0.00011250
Iteration 222/1000 | Loss: 0.00011591
Iteration 223/1000 | Loss: 0.00011543
Iteration 224/1000 | Loss: 0.00011470
Iteration 225/1000 | Loss: 0.00011240
Iteration 226/1000 | Loss: 0.00011813
Iteration 227/1000 | Loss: 0.00011846
Iteration 228/1000 | Loss: 0.00011143
Iteration 229/1000 | Loss: 0.00010929
Iteration 230/1000 | Loss: 0.00011076
Iteration 231/1000 | Loss: 0.00010948
Iteration 232/1000 | Loss: 0.00010948
Iteration 233/1000 | Loss: 0.00011089
Iteration 234/1000 | Loss: 0.00010888
Iteration 235/1000 | Loss: 0.00010888
Iteration 236/1000 | Loss: 0.00010903
Iteration 237/1000 | Loss: 0.00010887
Iteration 238/1000 | Loss: 0.00010887
Iteration 239/1000 | Loss: 0.00010887
Iteration 240/1000 | Loss: 0.00010887
Iteration 241/1000 | Loss: 0.00010887
Iteration 242/1000 | Loss: 0.00010887
Iteration 243/1000 | Loss: 0.00010887
Iteration 244/1000 | Loss: 0.00010887
Iteration 245/1000 | Loss: 0.00010887
Iteration 246/1000 | Loss: 0.00010887
Iteration 247/1000 | Loss: 0.00010887
Iteration 248/1000 | Loss: 0.00010887
Iteration 249/1000 | Loss: 0.00010887
Iteration 250/1000 | Loss: 0.00010887
Iteration 251/1000 | Loss: 0.00010887
Iteration 252/1000 | Loss: 0.00010887
Iteration 253/1000 | Loss: 0.00010887
Iteration 254/1000 | Loss: 0.00010887
Iteration 255/1000 | Loss: 0.00010887
Iteration 256/1000 | Loss: 0.00010887
Iteration 257/1000 | Loss: 0.00010887
Iteration 258/1000 | Loss: 0.00010887
Iteration 259/1000 | Loss: 0.00010887
Iteration 260/1000 | Loss: 0.00010887
Iteration 261/1000 | Loss: 0.00010887
Iteration 262/1000 | Loss: 0.00010887
Iteration 263/1000 | Loss: 0.00010887
Iteration 264/1000 | Loss: 0.00010887
Iteration 265/1000 | Loss: 0.00010887
Iteration 266/1000 | Loss: 0.00010887
Iteration 267/1000 | Loss: 0.00010887
Iteration 268/1000 | Loss: 0.00010887
Iteration 269/1000 | Loss: 0.00010887
Iteration 270/1000 | Loss: 0.00010887
Iteration 271/1000 | Loss: 0.00010887
Iteration 272/1000 | Loss: 0.00010887
Iteration 273/1000 | Loss: 0.00010887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [0.0001088691788027063, 0.0001088691788027063, 0.0001088691788027063, 0.0001088691788027063, 0.0001088691788027063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001088691788027063

Optimization complete. Final v2v error: 4.689399242401123 mm

Highest mean error: 12.50683879852295 mm for frame 208

Lowest mean error: 2.6863179206848145 mm for frame 182

Saving results

Total time: 414.3006896972656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402729
Iteration 2/25 | Loss: 0.00121003
Iteration 3/25 | Loss: 0.00106833
Iteration 4/25 | Loss: 0.00105024
Iteration 5/25 | Loss: 0.00104759
Iteration 6/25 | Loss: 0.00104693
Iteration 7/25 | Loss: 0.00104693
Iteration 8/25 | Loss: 0.00104693
Iteration 9/25 | Loss: 0.00104693
Iteration 10/25 | Loss: 0.00104693
Iteration 11/25 | Loss: 0.00104693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010469331173226237, 0.0010469331173226237, 0.0010469331173226237, 0.0010469331173226237, 0.0010469331173226237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010469331173226237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24420476
Iteration 2/25 | Loss: 0.00131647
Iteration 3/25 | Loss: 0.00131646
Iteration 4/25 | Loss: 0.00131646
Iteration 5/25 | Loss: 0.00131646
Iteration 6/25 | Loss: 0.00131646
Iteration 7/25 | Loss: 0.00131646
Iteration 8/25 | Loss: 0.00131646
Iteration 9/25 | Loss: 0.00131646
Iteration 10/25 | Loss: 0.00131646
Iteration 11/25 | Loss: 0.00131646
Iteration 12/25 | Loss: 0.00131646
Iteration 13/25 | Loss: 0.00131646
Iteration 14/25 | Loss: 0.00131646
Iteration 15/25 | Loss: 0.00131646
Iteration 16/25 | Loss: 0.00131646
Iteration 17/25 | Loss: 0.00131646
Iteration 18/25 | Loss: 0.00131646
Iteration 19/25 | Loss: 0.00131646
Iteration 20/25 | Loss: 0.00131646
Iteration 21/25 | Loss: 0.00131646
Iteration 22/25 | Loss: 0.00131646
Iteration 23/25 | Loss: 0.00131646
Iteration 24/25 | Loss: 0.00131646
Iteration 25/25 | Loss: 0.00131646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131646
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00001602
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001221
Iteration 7/1000 | Loss: 0.00001173
Iteration 8/1000 | Loss: 0.00001141
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001107
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001087
Iteration 14/1000 | Loss: 0.00001087
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001085
Iteration 17/1000 | Loss: 0.00001085
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001084
Iteration 20/1000 | Loss: 0.00001084
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001082
Iteration 24/1000 | Loss: 0.00001079
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001079
Iteration 29/1000 | Loss: 0.00001079
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001079
Iteration 32/1000 | Loss: 0.00001077
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001076
Iteration 35/1000 | Loss: 0.00001076
Iteration 36/1000 | Loss: 0.00001076
Iteration 37/1000 | Loss: 0.00001076
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001074
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001073
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001072
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001071
Iteration 58/1000 | Loss: 0.00001071
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001070
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001070
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001069
Iteration 71/1000 | Loss: 0.00001069
Iteration 72/1000 | Loss: 0.00001069
Iteration 73/1000 | Loss: 0.00001069
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001069
Iteration 80/1000 | Loss: 0.00001069
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001068
Iteration 86/1000 | Loss: 0.00001068
Iteration 87/1000 | Loss: 0.00001068
Iteration 88/1000 | Loss: 0.00001068
Iteration 89/1000 | Loss: 0.00001068
Iteration 90/1000 | Loss: 0.00001068
Iteration 91/1000 | Loss: 0.00001068
Iteration 92/1000 | Loss: 0.00001067
Iteration 93/1000 | Loss: 0.00001067
Iteration 94/1000 | Loss: 0.00001067
Iteration 95/1000 | Loss: 0.00001067
Iteration 96/1000 | Loss: 0.00001067
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001067
Iteration 102/1000 | Loss: 0.00001067
Iteration 103/1000 | Loss: 0.00001067
Iteration 104/1000 | Loss: 0.00001067
Iteration 105/1000 | Loss: 0.00001067
Iteration 106/1000 | Loss: 0.00001067
Iteration 107/1000 | Loss: 0.00001067
Iteration 108/1000 | Loss: 0.00001067
Iteration 109/1000 | Loss: 0.00001067
Iteration 110/1000 | Loss: 0.00001066
Iteration 111/1000 | Loss: 0.00001066
Iteration 112/1000 | Loss: 0.00001066
Iteration 113/1000 | Loss: 0.00001066
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001066
Iteration 116/1000 | Loss: 0.00001066
Iteration 117/1000 | Loss: 0.00001066
Iteration 118/1000 | Loss: 0.00001066
Iteration 119/1000 | Loss: 0.00001066
Iteration 120/1000 | Loss: 0.00001066
Iteration 121/1000 | Loss: 0.00001066
Iteration 122/1000 | Loss: 0.00001065
Iteration 123/1000 | Loss: 0.00001065
Iteration 124/1000 | Loss: 0.00001065
Iteration 125/1000 | Loss: 0.00001065
Iteration 126/1000 | Loss: 0.00001065
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001064
Iteration 129/1000 | Loss: 0.00001064
Iteration 130/1000 | Loss: 0.00001064
Iteration 131/1000 | Loss: 0.00001064
Iteration 132/1000 | Loss: 0.00001064
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Iteration 139/1000 | Loss: 0.00001063
Iteration 140/1000 | Loss: 0.00001063
Iteration 141/1000 | Loss: 0.00001063
Iteration 142/1000 | Loss: 0.00001063
Iteration 143/1000 | Loss: 0.00001063
Iteration 144/1000 | Loss: 0.00001063
Iteration 145/1000 | Loss: 0.00001063
Iteration 146/1000 | Loss: 0.00001063
Iteration 147/1000 | Loss: 0.00001063
Iteration 148/1000 | Loss: 0.00001063
Iteration 149/1000 | Loss: 0.00001063
Iteration 150/1000 | Loss: 0.00001063
Iteration 151/1000 | Loss: 0.00001063
Iteration 152/1000 | Loss: 0.00001063
Iteration 153/1000 | Loss: 0.00001063
Iteration 154/1000 | Loss: 0.00001063
Iteration 155/1000 | Loss: 0.00001062
Iteration 156/1000 | Loss: 0.00001062
Iteration 157/1000 | Loss: 0.00001062
Iteration 158/1000 | Loss: 0.00001062
Iteration 159/1000 | Loss: 0.00001062
Iteration 160/1000 | Loss: 0.00001062
Iteration 161/1000 | Loss: 0.00001062
Iteration 162/1000 | Loss: 0.00001062
Iteration 163/1000 | Loss: 0.00001062
Iteration 164/1000 | Loss: 0.00001062
Iteration 165/1000 | Loss: 0.00001062
Iteration 166/1000 | Loss: 0.00001062
Iteration 167/1000 | Loss: 0.00001062
Iteration 168/1000 | Loss: 0.00001061
Iteration 169/1000 | Loss: 0.00001061
Iteration 170/1000 | Loss: 0.00001061
Iteration 171/1000 | Loss: 0.00001061
Iteration 172/1000 | Loss: 0.00001061
Iteration 173/1000 | Loss: 0.00001061
Iteration 174/1000 | Loss: 0.00001061
Iteration 175/1000 | Loss: 0.00001061
Iteration 176/1000 | Loss: 0.00001061
Iteration 177/1000 | Loss: 0.00001061
Iteration 178/1000 | Loss: 0.00001061
Iteration 179/1000 | Loss: 0.00001061
Iteration 180/1000 | Loss: 0.00001061
Iteration 181/1000 | Loss: 0.00001060
Iteration 182/1000 | Loss: 0.00001060
Iteration 183/1000 | Loss: 0.00001060
Iteration 184/1000 | Loss: 0.00001060
Iteration 185/1000 | Loss: 0.00001060
Iteration 186/1000 | Loss: 0.00001060
Iteration 187/1000 | Loss: 0.00001060
Iteration 188/1000 | Loss: 0.00001059
Iteration 189/1000 | Loss: 0.00001059
Iteration 190/1000 | Loss: 0.00001059
Iteration 191/1000 | Loss: 0.00001059
Iteration 192/1000 | Loss: 0.00001059
Iteration 193/1000 | Loss: 0.00001058
Iteration 194/1000 | Loss: 0.00001058
Iteration 195/1000 | Loss: 0.00001058
Iteration 196/1000 | Loss: 0.00001058
Iteration 197/1000 | Loss: 0.00001057
Iteration 198/1000 | Loss: 0.00001057
Iteration 199/1000 | Loss: 0.00001057
Iteration 200/1000 | Loss: 0.00001057
Iteration 201/1000 | Loss: 0.00001057
Iteration 202/1000 | Loss: 0.00001057
Iteration 203/1000 | Loss: 0.00001057
Iteration 204/1000 | Loss: 0.00001057
Iteration 205/1000 | Loss: 0.00001057
Iteration 206/1000 | Loss: 0.00001057
Iteration 207/1000 | Loss: 0.00001057
Iteration 208/1000 | Loss: 0.00001057
Iteration 209/1000 | Loss: 0.00001057
Iteration 210/1000 | Loss: 0.00001056
Iteration 211/1000 | Loss: 0.00001056
Iteration 212/1000 | Loss: 0.00001056
Iteration 213/1000 | Loss: 0.00001056
Iteration 214/1000 | Loss: 0.00001056
Iteration 215/1000 | Loss: 0.00001056
Iteration 216/1000 | Loss: 0.00001056
Iteration 217/1000 | Loss: 0.00001056
Iteration 218/1000 | Loss: 0.00001056
Iteration 219/1000 | Loss: 0.00001056
Iteration 220/1000 | Loss: 0.00001056
Iteration 221/1000 | Loss: 0.00001056
Iteration 222/1000 | Loss: 0.00001056
Iteration 223/1000 | Loss: 0.00001056
Iteration 224/1000 | Loss: 0.00001056
Iteration 225/1000 | Loss: 0.00001056
Iteration 226/1000 | Loss: 0.00001056
Iteration 227/1000 | Loss: 0.00001056
Iteration 228/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.0563180694589391e-05, 1.0563180694589391e-05, 1.0563180694589391e-05, 1.0563180694589391e-05, 1.0563180694589391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0563180694589391e-05

Optimization complete. Final v2v error: 2.763410806655884 mm

Highest mean error: 3.162160634994507 mm for frame 85

Lowest mean error: 2.242121934890747 mm for frame 0

Saving results

Total time: 36.490079402923584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108174
Iteration 2/25 | Loss: 0.00281654
Iteration 3/25 | Loss: 0.00188858
Iteration 4/25 | Loss: 0.00166967
Iteration 5/25 | Loss: 0.00164035
Iteration 6/25 | Loss: 0.00156180
Iteration 7/25 | Loss: 0.00152315
Iteration 8/25 | Loss: 0.00150303
Iteration 9/25 | Loss: 0.00149170
Iteration 10/25 | Loss: 0.00148992
Iteration 11/25 | Loss: 0.00148703
Iteration 12/25 | Loss: 0.00147616
Iteration 13/25 | Loss: 0.00147084
Iteration 14/25 | Loss: 0.00146517
Iteration 15/25 | Loss: 0.00146209
Iteration 16/25 | Loss: 0.00146428
Iteration 17/25 | Loss: 0.00146232
Iteration 18/25 | Loss: 0.00146320
Iteration 19/25 | Loss: 0.00146083
Iteration 20/25 | Loss: 0.00146104
Iteration 21/25 | Loss: 0.00146169
Iteration 22/25 | Loss: 0.00146076
Iteration 23/25 | Loss: 0.00146116
Iteration 24/25 | Loss: 0.00146055
Iteration 25/25 | Loss: 0.00146119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.12813902
Iteration 2/25 | Loss: 0.00396744
Iteration 3/25 | Loss: 0.00396744
Iteration 4/25 | Loss: 0.00396744
Iteration 5/25 | Loss: 0.00396744
Iteration 6/25 | Loss: 0.00396744
Iteration 7/25 | Loss: 0.00396744
Iteration 8/25 | Loss: 0.00396744
Iteration 9/25 | Loss: 0.00396744
Iteration 10/25 | Loss: 0.00396744
Iteration 11/25 | Loss: 0.00396744
Iteration 12/25 | Loss: 0.00396744
Iteration 13/25 | Loss: 0.00396744
Iteration 14/25 | Loss: 0.00396744
Iteration 15/25 | Loss: 0.00396744
Iteration 16/25 | Loss: 0.00396744
Iteration 17/25 | Loss: 0.00396744
Iteration 18/25 | Loss: 0.00396744
Iteration 19/25 | Loss: 0.00396744
Iteration 20/25 | Loss: 0.00396744
Iteration 21/25 | Loss: 0.00396744
Iteration 22/25 | Loss: 0.00396744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0039674355648458, 0.0039674355648458, 0.0039674355648458, 0.0039674355648458, 0.0039674355648458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039674355648458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00396744
Iteration 2/1000 | Loss: 0.00058585
Iteration 3/1000 | Loss: 0.00272722
Iteration 4/1000 | Loss: 0.00208736
Iteration 5/1000 | Loss: 0.00119954
Iteration 6/1000 | Loss: 0.00095364
Iteration 7/1000 | Loss: 0.00093950
Iteration 8/1000 | Loss: 0.00038488
Iteration 9/1000 | Loss: 0.00069736
Iteration 10/1000 | Loss: 0.00093744
Iteration 11/1000 | Loss: 0.00060216
Iteration 12/1000 | Loss: 0.00058572
Iteration 13/1000 | Loss: 0.00021482
Iteration 14/1000 | Loss: 0.00020674
Iteration 15/1000 | Loss: 0.00026301
Iteration 16/1000 | Loss: 0.00186426
Iteration 17/1000 | Loss: 0.00030044
Iteration 18/1000 | Loss: 0.00021336
Iteration 19/1000 | Loss: 0.00087957
Iteration 20/1000 | Loss: 0.00214518
Iteration 21/1000 | Loss: 0.00140130
Iteration 22/1000 | Loss: 0.00061565
Iteration 23/1000 | Loss: 0.00146847
Iteration 24/1000 | Loss: 0.00072601
Iteration 25/1000 | Loss: 0.00158608
Iteration 26/1000 | Loss: 0.00150869
Iteration 27/1000 | Loss: 0.00167555
Iteration 28/1000 | Loss: 0.00133469
Iteration 29/1000 | Loss: 0.00132497
Iteration 30/1000 | Loss: 0.00115174
Iteration 31/1000 | Loss: 0.00079938
Iteration 32/1000 | Loss: 0.00077884
Iteration 33/1000 | Loss: 0.00075567
Iteration 34/1000 | Loss: 0.00076628
Iteration 35/1000 | Loss: 0.00063730
Iteration 36/1000 | Loss: 0.00170222
Iteration 37/1000 | Loss: 0.00111030
Iteration 38/1000 | Loss: 0.00076361
Iteration 39/1000 | Loss: 0.00065716
Iteration 40/1000 | Loss: 0.00095260
Iteration 41/1000 | Loss: 0.00142705
Iteration 42/1000 | Loss: 0.00144818
Iteration 43/1000 | Loss: 0.00256578
Iteration 44/1000 | Loss: 0.00189714
Iteration 45/1000 | Loss: 0.00147929
Iteration 46/1000 | Loss: 0.00208518
Iteration 47/1000 | Loss: 0.00247280
Iteration 48/1000 | Loss: 0.00201656
Iteration 49/1000 | Loss: 0.00212285
Iteration 50/1000 | Loss: 0.00225170
Iteration 51/1000 | Loss: 0.00176207
Iteration 52/1000 | Loss: 0.00144205
Iteration 53/1000 | Loss: 0.00260888
Iteration 54/1000 | Loss: 0.00208657
Iteration 55/1000 | Loss: 0.00289836
Iteration 56/1000 | Loss: 0.00235933
Iteration 57/1000 | Loss: 0.00178914
Iteration 58/1000 | Loss: 0.00220725
Iteration 59/1000 | Loss: 0.00189500
Iteration 60/1000 | Loss: 0.00170016
Iteration 61/1000 | Loss: 0.00150912
Iteration 62/1000 | Loss: 0.00158416
Iteration 63/1000 | Loss: 0.00197304
Iteration 64/1000 | Loss: 0.00187420
Iteration 65/1000 | Loss: 0.00363692
Iteration 66/1000 | Loss: 0.00282097
Iteration 67/1000 | Loss: 0.00244910
Iteration 68/1000 | Loss: 0.00200289
Iteration 69/1000 | Loss: 0.00330608
Iteration 70/1000 | Loss: 0.00287372
Iteration 71/1000 | Loss: 0.00224544
Iteration 72/1000 | Loss: 0.00177830
Iteration 73/1000 | Loss: 0.00178757
Iteration 74/1000 | Loss: 0.00234369
Iteration 75/1000 | Loss: 0.00163387
Iteration 76/1000 | Loss: 0.00293002
Iteration 77/1000 | Loss: 0.00202510
Iteration 78/1000 | Loss: 0.00238854
Iteration 79/1000 | Loss: 0.00217571
Iteration 80/1000 | Loss: 0.00240247
Iteration 81/1000 | Loss: 0.00201043
Iteration 82/1000 | Loss: 0.00251088
Iteration 83/1000 | Loss: 0.00220805
Iteration 84/1000 | Loss: 0.00282898
Iteration 85/1000 | Loss: 0.00235878
Iteration 86/1000 | Loss: 0.00181507
Iteration 87/1000 | Loss: 0.00141665
Iteration 88/1000 | Loss: 0.00139448
Iteration 89/1000 | Loss: 0.00219009
Iteration 90/1000 | Loss: 0.00094567
Iteration 91/1000 | Loss: 0.00129546
Iteration 92/1000 | Loss: 0.00078796
Iteration 93/1000 | Loss: 0.00094192
Iteration 94/1000 | Loss: 0.00177885
Iteration 95/1000 | Loss: 0.00138145
Iteration 96/1000 | Loss: 0.00076830
Iteration 97/1000 | Loss: 0.00158568
Iteration 98/1000 | Loss: 0.00108947
Iteration 99/1000 | Loss: 0.00185132
Iteration 100/1000 | Loss: 0.00141821
Iteration 101/1000 | Loss: 0.00089135
Iteration 102/1000 | Loss: 0.00173108
Iteration 103/1000 | Loss: 0.00073860
Iteration 104/1000 | Loss: 0.00058845
Iteration 105/1000 | Loss: 0.00061452
Iteration 106/1000 | Loss: 0.00126377
Iteration 107/1000 | Loss: 0.00071168
Iteration 108/1000 | Loss: 0.00049730
Iteration 109/1000 | Loss: 0.00042046
Iteration 110/1000 | Loss: 0.00018172
Iteration 111/1000 | Loss: 0.00065023
Iteration 112/1000 | Loss: 0.00068490
Iteration 113/1000 | Loss: 0.00025071
Iteration 114/1000 | Loss: 0.00064806
Iteration 115/1000 | Loss: 0.00049346
Iteration 116/1000 | Loss: 0.00062111
Iteration 117/1000 | Loss: 0.00137300
Iteration 118/1000 | Loss: 0.00026432
Iteration 119/1000 | Loss: 0.00059295
Iteration 120/1000 | Loss: 0.00050165
Iteration 121/1000 | Loss: 0.00061099
Iteration 122/1000 | Loss: 0.00045723
Iteration 123/1000 | Loss: 0.00070566
Iteration 124/1000 | Loss: 0.00067380
Iteration 125/1000 | Loss: 0.00059921
Iteration 126/1000 | Loss: 0.00076380
Iteration 127/1000 | Loss: 0.00057759
Iteration 128/1000 | Loss: 0.00070282
Iteration 129/1000 | Loss: 0.00042396
Iteration 130/1000 | Loss: 0.00080248
Iteration 131/1000 | Loss: 0.00073228
Iteration 132/1000 | Loss: 0.00080524
Iteration 133/1000 | Loss: 0.00060337
Iteration 134/1000 | Loss: 0.00050252
Iteration 135/1000 | Loss: 0.00089449
Iteration 136/1000 | Loss: 0.00055248
Iteration 137/1000 | Loss: 0.00024900
Iteration 138/1000 | Loss: 0.00062509
Iteration 139/1000 | Loss: 0.00047386
Iteration 140/1000 | Loss: 0.00008749
Iteration 141/1000 | Loss: 0.00029782
Iteration 142/1000 | Loss: 0.00019213
Iteration 143/1000 | Loss: 0.00007823
Iteration 144/1000 | Loss: 0.00008564
Iteration 145/1000 | Loss: 0.00025619
Iteration 146/1000 | Loss: 0.00013261
Iteration 147/1000 | Loss: 0.00067621
Iteration 148/1000 | Loss: 0.00046687
Iteration 149/1000 | Loss: 0.00008289
Iteration 150/1000 | Loss: 0.00007050
Iteration 151/1000 | Loss: 0.00085804
Iteration 152/1000 | Loss: 0.00015238
Iteration 153/1000 | Loss: 0.00009785
Iteration 154/1000 | Loss: 0.00007218
Iteration 155/1000 | Loss: 0.00008646
Iteration 156/1000 | Loss: 0.00007901
Iteration 157/1000 | Loss: 0.00006598
Iteration 158/1000 | Loss: 0.00006200
Iteration 159/1000 | Loss: 0.00008268
Iteration 160/1000 | Loss: 0.00006498
Iteration 161/1000 | Loss: 0.00007279
Iteration 162/1000 | Loss: 0.00009374
Iteration 163/1000 | Loss: 0.00020873
Iteration 164/1000 | Loss: 0.00095936
Iteration 165/1000 | Loss: 0.00094815
Iteration 166/1000 | Loss: 0.00056675
Iteration 167/1000 | Loss: 0.00029515
Iteration 168/1000 | Loss: 0.00042988
Iteration 169/1000 | Loss: 0.00013233
Iteration 170/1000 | Loss: 0.00041966
Iteration 171/1000 | Loss: 0.00052848
Iteration 172/1000 | Loss: 0.00054218
Iteration 173/1000 | Loss: 0.00106345
Iteration 174/1000 | Loss: 0.00057701
Iteration 175/1000 | Loss: 0.00048135
Iteration 176/1000 | Loss: 0.00040976
Iteration 177/1000 | Loss: 0.00007313
Iteration 178/1000 | Loss: 0.00006350
Iteration 179/1000 | Loss: 0.00041574
Iteration 180/1000 | Loss: 0.00043490
Iteration 181/1000 | Loss: 0.00035927
Iteration 182/1000 | Loss: 0.00045436
Iteration 183/1000 | Loss: 0.00026830
Iteration 184/1000 | Loss: 0.00146217
Iteration 185/1000 | Loss: 0.00011598
Iteration 186/1000 | Loss: 0.00007851
Iteration 187/1000 | Loss: 0.00006986
Iteration 188/1000 | Loss: 0.00006358
Iteration 189/1000 | Loss: 0.00103350
Iteration 190/1000 | Loss: 0.00078381
Iteration 191/1000 | Loss: 0.00092473
Iteration 192/1000 | Loss: 0.00038199
Iteration 193/1000 | Loss: 0.00076890
Iteration 194/1000 | Loss: 0.00022182
Iteration 195/1000 | Loss: 0.00006467
Iteration 196/1000 | Loss: 0.00059815
Iteration 197/1000 | Loss: 0.00045844
Iteration 198/1000 | Loss: 0.00005795
Iteration 199/1000 | Loss: 0.00005584
Iteration 200/1000 | Loss: 0.00005474
Iteration 201/1000 | Loss: 0.00005380
Iteration 202/1000 | Loss: 0.00005326
Iteration 203/1000 | Loss: 0.00005297
Iteration 204/1000 | Loss: 0.00061574
Iteration 205/1000 | Loss: 0.00152258
Iteration 206/1000 | Loss: 0.00014271
Iteration 207/1000 | Loss: 0.00062430
Iteration 208/1000 | Loss: 0.00016716
Iteration 209/1000 | Loss: 0.00041844
Iteration 210/1000 | Loss: 0.00010899
Iteration 211/1000 | Loss: 0.00005401
Iteration 212/1000 | Loss: 0.00038834
Iteration 213/1000 | Loss: 0.00019742
Iteration 214/1000 | Loss: 0.00005361
Iteration 215/1000 | Loss: 0.00005250
Iteration 216/1000 | Loss: 0.00005218
Iteration 217/1000 | Loss: 0.00043286
Iteration 218/1000 | Loss: 0.00035363
Iteration 219/1000 | Loss: 0.00028635
Iteration 220/1000 | Loss: 0.00029066
Iteration 221/1000 | Loss: 0.00007344
Iteration 222/1000 | Loss: 0.00005599
Iteration 223/1000 | Loss: 0.00005228
Iteration 224/1000 | Loss: 0.00004981
Iteration 225/1000 | Loss: 0.00004863
Iteration 226/1000 | Loss: 0.00004803
Iteration 227/1000 | Loss: 0.00004773
Iteration 228/1000 | Loss: 0.00004769
Iteration 229/1000 | Loss: 0.00004745
Iteration 230/1000 | Loss: 0.00004740
Iteration 231/1000 | Loss: 0.00004736
Iteration 232/1000 | Loss: 0.00004735
Iteration 233/1000 | Loss: 0.00004735
Iteration 234/1000 | Loss: 0.00004735
Iteration 235/1000 | Loss: 0.00004734
Iteration 236/1000 | Loss: 0.00004734
Iteration 237/1000 | Loss: 0.00004733
Iteration 238/1000 | Loss: 0.00004733
Iteration 239/1000 | Loss: 0.00004730
Iteration 240/1000 | Loss: 0.00004728
Iteration 241/1000 | Loss: 0.00004728
Iteration 242/1000 | Loss: 0.00004727
Iteration 243/1000 | Loss: 0.00004727
Iteration 244/1000 | Loss: 0.00004727
Iteration 245/1000 | Loss: 0.00004727
Iteration 246/1000 | Loss: 0.00004727
Iteration 247/1000 | Loss: 0.00004727
Iteration 248/1000 | Loss: 0.00004727
Iteration 249/1000 | Loss: 0.00004727
Iteration 250/1000 | Loss: 0.00004727
Iteration 251/1000 | Loss: 0.00004726
Iteration 252/1000 | Loss: 0.00004726
Iteration 253/1000 | Loss: 0.00004726
Iteration 254/1000 | Loss: 0.00004726
Iteration 255/1000 | Loss: 0.00004726
Iteration 256/1000 | Loss: 0.00004725
Iteration 257/1000 | Loss: 0.00004725
Iteration 258/1000 | Loss: 0.00004725
Iteration 259/1000 | Loss: 0.00004725
Iteration 260/1000 | Loss: 0.00004725
Iteration 261/1000 | Loss: 0.00004725
Iteration 262/1000 | Loss: 0.00004724
Iteration 263/1000 | Loss: 0.00004724
Iteration 264/1000 | Loss: 0.00004724
Iteration 265/1000 | Loss: 0.00004724
Iteration 266/1000 | Loss: 0.00004724
Iteration 267/1000 | Loss: 0.00004724
Iteration 268/1000 | Loss: 0.00004724
Iteration 269/1000 | Loss: 0.00004724
Iteration 270/1000 | Loss: 0.00004723
Iteration 271/1000 | Loss: 0.00004723
Iteration 272/1000 | Loss: 0.00004723
Iteration 273/1000 | Loss: 0.00004723
Iteration 274/1000 | Loss: 0.00004723
Iteration 275/1000 | Loss: 0.00004723
Iteration 276/1000 | Loss: 0.00004723
Iteration 277/1000 | Loss: 0.00004723
Iteration 278/1000 | Loss: 0.00004722
Iteration 279/1000 | Loss: 0.00004722
Iteration 280/1000 | Loss: 0.00004722
Iteration 281/1000 | Loss: 0.00004722
Iteration 282/1000 | Loss: 0.00004722
Iteration 283/1000 | Loss: 0.00004721
Iteration 284/1000 | Loss: 0.00004721
Iteration 285/1000 | Loss: 0.00004721
Iteration 286/1000 | Loss: 0.00004721
Iteration 287/1000 | Loss: 0.00004721
Iteration 288/1000 | Loss: 0.00004721
Iteration 289/1000 | Loss: 0.00004721
Iteration 290/1000 | Loss: 0.00004721
Iteration 291/1000 | Loss: 0.00004721
Iteration 292/1000 | Loss: 0.00004720
Iteration 293/1000 | Loss: 0.00004720
Iteration 294/1000 | Loss: 0.00004720
Iteration 295/1000 | Loss: 0.00004720
Iteration 296/1000 | Loss: 0.00004720
Iteration 297/1000 | Loss: 0.00004720
Iteration 298/1000 | Loss: 0.00004720
Iteration 299/1000 | Loss: 0.00004720
Iteration 300/1000 | Loss: 0.00004719
Iteration 301/1000 | Loss: 0.00004719
Iteration 302/1000 | Loss: 0.00004719
Iteration 303/1000 | Loss: 0.00004719
Iteration 304/1000 | Loss: 0.00004719
Iteration 305/1000 | Loss: 0.00004719
Iteration 306/1000 | Loss: 0.00004719
Iteration 307/1000 | Loss: 0.00004719
Iteration 308/1000 | Loss: 0.00004719
Iteration 309/1000 | Loss: 0.00004719
Iteration 310/1000 | Loss: 0.00004719
Iteration 311/1000 | Loss: 0.00004718
Iteration 312/1000 | Loss: 0.00004718
Iteration 313/1000 | Loss: 0.00004718
Iteration 314/1000 | Loss: 0.00004718
Iteration 315/1000 | Loss: 0.00004718
Iteration 316/1000 | Loss: 0.00004718
Iteration 317/1000 | Loss: 0.00004718
Iteration 318/1000 | Loss: 0.00004718
Iteration 319/1000 | Loss: 0.00004718
Iteration 320/1000 | Loss: 0.00004718
Iteration 321/1000 | Loss: 0.00004718
Iteration 322/1000 | Loss: 0.00004718
Iteration 323/1000 | Loss: 0.00004718
Iteration 324/1000 | Loss: 0.00004718
Iteration 325/1000 | Loss: 0.00004718
Iteration 326/1000 | Loss: 0.00004718
Iteration 327/1000 | Loss: 0.00004718
Iteration 328/1000 | Loss: 0.00004718
Iteration 329/1000 | Loss: 0.00004718
Iteration 330/1000 | Loss: 0.00004718
Iteration 331/1000 | Loss: 0.00004718
Iteration 332/1000 | Loss: 0.00004718
Iteration 333/1000 | Loss: 0.00004718
Iteration 334/1000 | Loss: 0.00004718
Iteration 335/1000 | Loss: 0.00004718
Iteration 336/1000 | Loss: 0.00004718
Iteration 337/1000 | Loss: 0.00004718
Iteration 338/1000 | Loss: 0.00004718
Iteration 339/1000 | Loss: 0.00004718
Iteration 340/1000 | Loss: 0.00004718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [4.7178084059851244e-05, 4.7178084059851244e-05, 4.7178084059851244e-05, 4.7178084059851244e-05, 4.7178084059851244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7178084059851244e-05

Optimization complete. Final v2v error: 4.360095500946045 mm

Highest mean error: 20.452476501464844 mm for frame 64

Lowest mean error: 3.144408941268921 mm for frame 134

Saving results

Total time: 361.60665559768677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896112
Iteration 2/25 | Loss: 0.00119698
Iteration 3/25 | Loss: 0.00108508
Iteration 4/25 | Loss: 0.00106528
Iteration 5/25 | Loss: 0.00105792
Iteration 6/25 | Loss: 0.00105578
Iteration 7/25 | Loss: 0.00105552
Iteration 8/25 | Loss: 0.00105552
Iteration 9/25 | Loss: 0.00105552
Iteration 10/25 | Loss: 0.00105552
Iteration 11/25 | Loss: 0.00105552
Iteration 12/25 | Loss: 0.00105552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010555160697549582, 0.0010555160697549582, 0.0010555160697549582, 0.0010555160697549582, 0.0010555160697549582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010555160697549582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92005193
Iteration 2/25 | Loss: 0.00147701
Iteration 3/25 | Loss: 0.00147701
Iteration 4/25 | Loss: 0.00147701
Iteration 5/25 | Loss: 0.00147701
Iteration 6/25 | Loss: 0.00147700
Iteration 7/25 | Loss: 0.00147700
Iteration 8/25 | Loss: 0.00147700
Iteration 9/25 | Loss: 0.00147700
Iteration 10/25 | Loss: 0.00147700
Iteration 11/25 | Loss: 0.00147700
Iteration 12/25 | Loss: 0.00147700
Iteration 13/25 | Loss: 0.00147700
Iteration 14/25 | Loss: 0.00147700
Iteration 15/25 | Loss: 0.00147700
Iteration 16/25 | Loss: 0.00147700
Iteration 17/25 | Loss: 0.00147700
Iteration 18/25 | Loss: 0.00147700
Iteration 19/25 | Loss: 0.00147700
Iteration 20/25 | Loss: 0.00147700
Iteration 21/25 | Loss: 0.00147700
Iteration 22/25 | Loss: 0.00147700
Iteration 23/25 | Loss: 0.00147700
Iteration 24/25 | Loss: 0.00147700
Iteration 25/25 | Loss: 0.00147700

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147700
Iteration 2/1000 | Loss: 0.00003198
Iteration 3/1000 | Loss: 0.00001764
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001403
Iteration 6/1000 | Loss: 0.00001343
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001213
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001204
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001200
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001190
Iteration 125/1000 | Loss: 0.00001190
Iteration 126/1000 | Loss: 0.00001190
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001190
Iteration 137/1000 | Loss: 0.00001190
Iteration 138/1000 | Loss: 0.00001190
Iteration 139/1000 | Loss: 0.00001190
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001190
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001190
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1898384400410578e-05, 1.1898384400410578e-05, 1.1898384400410578e-05, 1.1898384400410578e-05, 1.1898384400410578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1898384400410578e-05

Optimization complete. Final v2v error: 2.9479031562805176 mm

Highest mean error: 3.520871162414551 mm for frame 71

Lowest mean error: 2.5960421562194824 mm for frame 0

Saving results

Total time: 33.696539878845215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649660
Iteration 2/25 | Loss: 0.00114464
Iteration 3/25 | Loss: 0.00105503
Iteration 4/25 | Loss: 0.00103894
Iteration 5/25 | Loss: 0.00103317
Iteration 6/25 | Loss: 0.00103157
Iteration 7/25 | Loss: 0.00103157
Iteration 8/25 | Loss: 0.00103157
Iteration 9/25 | Loss: 0.00103157
Iteration 10/25 | Loss: 0.00103157
Iteration 11/25 | Loss: 0.00103157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010315714171156287, 0.0010315714171156287, 0.0010315714171156287, 0.0010315714171156287, 0.0010315714171156287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010315714171156287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82605910
Iteration 2/25 | Loss: 0.00139792
Iteration 3/25 | Loss: 0.00139792
Iteration 4/25 | Loss: 0.00139792
Iteration 5/25 | Loss: 0.00139792
Iteration 6/25 | Loss: 0.00139792
Iteration 7/25 | Loss: 0.00139792
Iteration 8/25 | Loss: 0.00139792
Iteration 9/25 | Loss: 0.00139792
Iteration 10/25 | Loss: 0.00139792
Iteration 11/25 | Loss: 0.00139792
Iteration 12/25 | Loss: 0.00139791
Iteration 13/25 | Loss: 0.00139791
Iteration 14/25 | Loss: 0.00139791
Iteration 15/25 | Loss: 0.00139791
Iteration 16/25 | Loss: 0.00139791
Iteration 17/25 | Loss: 0.00139791
Iteration 18/25 | Loss: 0.00139791
Iteration 19/25 | Loss: 0.00139791
Iteration 20/25 | Loss: 0.00139791
Iteration 21/25 | Loss: 0.00139791
Iteration 22/25 | Loss: 0.00139791
Iteration 23/25 | Loss: 0.00139791
Iteration 24/25 | Loss: 0.00139791
Iteration 25/25 | Loss: 0.00139791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139791
Iteration 2/1000 | Loss: 0.00003697
Iteration 3/1000 | Loss: 0.00002231
Iteration 4/1000 | Loss: 0.00001724
Iteration 5/1000 | Loss: 0.00001544
Iteration 6/1000 | Loss: 0.00001452
Iteration 7/1000 | Loss: 0.00001396
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001250
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001237
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001229
Iteration 86/1000 | Loss: 0.00001229
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001229
Iteration 91/1000 | Loss: 0.00001229
Iteration 92/1000 | Loss: 0.00001229
Iteration 93/1000 | Loss: 0.00001229
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.2294689440750517e-05, 1.2294689440750517e-05, 1.2294689440750517e-05, 1.2294689440750517e-05, 1.2294689440750517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2294689440750517e-05

Optimization complete. Final v2v error: 2.9784719944000244 mm

Highest mean error: 3.3096749782562256 mm for frame 77

Lowest mean error: 2.61814284324646 mm for frame 1

Saving results

Total time: 32.9115891456604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863592
Iteration 2/25 | Loss: 0.00147136
Iteration 3/25 | Loss: 0.00112996
Iteration 4/25 | Loss: 0.00106383
Iteration 5/25 | Loss: 0.00105438
Iteration 6/25 | Loss: 0.00105155
Iteration 7/25 | Loss: 0.00105037
Iteration 8/25 | Loss: 0.00104952
Iteration 9/25 | Loss: 0.00104874
Iteration 10/25 | Loss: 0.00105131
Iteration 11/25 | Loss: 0.00105317
Iteration 12/25 | Loss: 0.00105478
Iteration 13/25 | Loss: 0.00105397
Iteration 14/25 | Loss: 0.00105235
Iteration 15/25 | Loss: 0.00105397
Iteration 16/25 | Loss: 0.00105291
Iteration 17/25 | Loss: 0.00105301
Iteration 18/25 | Loss: 0.00105137
Iteration 19/25 | Loss: 0.00105483
Iteration 20/25 | Loss: 0.00105371
Iteration 21/25 | Loss: 0.00105380
Iteration 22/25 | Loss: 0.00105426
Iteration 23/25 | Loss: 0.00105285
Iteration 24/25 | Loss: 0.00105263
Iteration 25/25 | Loss: 0.00105119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24405837
Iteration 2/25 | Loss: 0.00137441
Iteration 3/25 | Loss: 0.00137441
Iteration 4/25 | Loss: 0.00137441
Iteration 5/25 | Loss: 0.00137441
Iteration 6/25 | Loss: 0.00137441
Iteration 7/25 | Loss: 0.00137441
Iteration 8/25 | Loss: 0.00137441
Iteration 9/25 | Loss: 0.00137441
Iteration 10/25 | Loss: 0.00137441
Iteration 11/25 | Loss: 0.00137441
Iteration 12/25 | Loss: 0.00137441
Iteration 13/25 | Loss: 0.00137441
Iteration 14/25 | Loss: 0.00137441
Iteration 15/25 | Loss: 0.00137441
Iteration 16/25 | Loss: 0.00137441
Iteration 17/25 | Loss: 0.00137441
Iteration 18/25 | Loss: 0.00137441
Iteration 19/25 | Loss: 0.00137441
Iteration 20/25 | Loss: 0.00137441
Iteration 21/25 | Loss: 0.00137441
Iteration 22/25 | Loss: 0.00137441
Iteration 23/25 | Loss: 0.00137441
Iteration 24/25 | Loss: 0.00137441
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013744072057306767, 0.0013744072057306767, 0.0013744072057306767, 0.0013744072057306767, 0.0013744072057306767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013744072057306767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137441
Iteration 2/1000 | Loss: 0.00005822
Iteration 3/1000 | Loss: 0.00004831
Iteration 4/1000 | Loss: 0.00005577
Iteration 5/1000 | Loss: 0.00003217
Iteration 6/1000 | Loss: 0.00005728
Iteration 7/1000 | Loss: 0.00006192
Iteration 8/1000 | Loss: 0.00005945
Iteration 9/1000 | Loss: 0.00003235
Iteration 10/1000 | Loss: 0.00004209
Iteration 11/1000 | Loss: 0.00004987
Iteration 12/1000 | Loss: 0.00006371
Iteration 13/1000 | Loss: 0.00004690
Iteration 14/1000 | Loss: 0.00005489
Iteration 15/1000 | Loss: 0.00006600
Iteration 16/1000 | Loss: 0.00006419
Iteration 17/1000 | Loss: 0.00004463
Iteration 18/1000 | Loss: 0.00005803
Iteration 19/1000 | Loss: 0.00005496
Iteration 20/1000 | Loss: 0.00006939
Iteration 21/1000 | Loss: 0.00006007
Iteration 22/1000 | Loss: 0.00003664
Iteration 23/1000 | Loss: 0.00004800
Iteration 24/1000 | Loss: 0.00005340
Iteration 25/1000 | Loss: 0.00005329
Iteration 26/1000 | Loss: 0.00007632
Iteration 27/1000 | Loss: 0.00008616
Iteration 28/1000 | Loss: 0.00004864
Iteration 29/1000 | Loss: 0.00003401
Iteration 30/1000 | Loss: 0.00002529
Iteration 31/1000 | Loss: 0.00004883
Iteration 32/1000 | Loss: 0.00003859
Iteration 33/1000 | Loss: 0.00003370
Iteration 34/1000 | Loss: 0.00003389
Iteration 35/1000 | Loss: 0.00003274
Iteration 36/1000 | Loss: 0.00003036
Iteration 37/1000 | Loss: 0.00003070
Iteration 38/1000 | Loss: 0.00003201
Iteration 39/1000 | Loss: 0.00003804
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00002258
Iteration 42/1000 | Loss: 0.00002870
Iteration 43/1000 | Loss: 0.00002634
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002852
Iteration 46/1000 | Loss: 0.00002105
Iteration 47/1000 | Loss: 0.00001923
Iteration 48/1000 | Loss: 0.00003099
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00001725
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00002859
Iteration 53/1000 | Loss: 0.00001543
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001966
Iteration 56/1000 | Loss: 0.00002586
Iteration 57/1000 | Loss: 0.00002959
Iteration 58/1000 | Loss: 0.00002644
Iteration 59/1000 | Loss: 0.00002871
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00003351
Iteration 62/1000 | Loss: 0.00002316
Iteration 63/1000 | Loss: 0.00001391
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00003414
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001109
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001018
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001016
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001016
Iteration 78/1000 | Loss: 0.00001016
Iteration 79/1000 | Loss: 0.00001015
Iteration 80/1000 | Loss: 0.00001015
Iteration 81/1000 | Loss: 0.00001015
Iteration 82/1000 | Loss: 0.00001015
Iteration 83/1000 | Loss: 0.00001015
Iteration 84/1000 | Loss: 0.00001014
Iteration 85/1000 | Loss: 0.00001014
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001013
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001012
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001009
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001008
Iteration 97/1000 | Loss: 0.00001007
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001006
Iteration 101/1000 | Loss: 0.00001005
Iteration 102/1000 | Loss: 0.00001004
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001002
Iteration 109/1000 | Loss: 0.00001002
Iteration 110/1000 | Loss: 0.00001002
Iteration 111/1000 | Loss: 0.00001002
Iteration 112/1000 | Loss: 0.00001002
Iteration 113/1000 | Loss: 0.00001002
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001001
Iteration 117/1000 | Loss: 0.00001000
Iteration 118/1000 | Loss: 0.00001000
Iteration 119/1000 | Loss: 0.00001000
Iteration 120/1000 | Loss: 0.00000999
Iteration 121/1000 | Loss: 0.00000999
Iteration 122/1000 | Loss: 0.00000999
Iteration 123/1000 | Loss: 0.00000998
Iteration 124/1000 | Loss: 0.00000998
Iteration 125/1000 | Loss: 0.00000998
Iteration 126/1000 | Loss: 0.00000998
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000991
Iteration 139/1000 | Loss: 0.00000991
Iteration 140/1000 | Loss: 0.00000990
Iteration 141/1000 | Loss: 0.00000990
Iteration 142/1000 | Loss: 0.00000990
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000984
Iteration 158/1000 | Loss: 0.00000984
Iteration 159/1000 | Loss: 0.00000984
Iteration 160/1000 | Loss: 0.00000983
Iteration 161/1000 | Loss: 0.00000983
Iteration 162/1000 | Loss: 0.00000983
Iteration 163/1000 | Loss: 0.00000983
Iteration 164/1000 | Loss: 0.00000983
Iteration 165/1000 | Loss: 0.00000983
Iteration 166/1000 | Loss: 0.00000983
Iteration 167/1000 | Loss: 0.00000983
Iteration 168/1000 | Loss: 0.00000983
Iteration 169/1000 | Loss: 0.00000983
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000982
Iteration 172/1000 | Loss: 0.00000982
Iteration 173/1000 | Loss: 0.00000982
Iteration 174/1000 | Loss: 0.00000982
Iteration 175/1000 | Loss: 0.00000982
Iteration 176/1000 | Loss: 0.00000981
Iteration 177/1000 | Loss: 0.00000981
Iteration 178/1000 | Loss: 0.00000981
Iteration 179/1000 | Loss: 0.00000980
Iteration 180/1000 | Loss: 0.00000980
Iteration 181/1000 | Loss: 0.00000980
Iteration 182/1000 | Loss: 0.00000980
Iteration 183/1000 | Loss: 0.00000980
Iteration 184/1000 | Loss: 0.00000979
Iteration 185/1000 | Loss: 0.00000979
Iteration 186/1000 | Loss: 0.00000979
Iteration 187/1000 | Loss: 0.00000979
Iteration 188/1000 | Loss: 0.00000978
Iteration 189/1000 | Loss: 0.00000978
Iteration 190/1000 | Loss: 0.00000978
Iteration 191/1000 | Loss: 0.00000978
Iteration 192/1000 | Loss: 0.00000977
Iteration 193/1000 | Loss: 0.00000977
Iteration 194/1000 | Loss: 0.00000977
Iteration 195/1000 | Loss: 0.00000977
Iteration 196/1000 | Loss: 0.00000977
Iteration 197/1000 | Loss: 0.00000977
Iteration 198/1000 | Loss: 0.00000977
Iteration 199/1000 | Loss: 0.00000977
Iteration 200/1000 | Loss: 0.00000976
Iteration 201/1000 | Loss: 0.00000976
Iteration 202/1000 | Loss: 0.00000976
Iteration 203/1000 | Loss: 0.00000976
Iteration 204/1000 | Loss: 0.00000976
Iteration 205/1000 | Loss: 0.00000976
Iteration 206/1000 | Loss: 0.00000976
Iteration 207/1000 | Loss: 0.00000976
Iteration 208/1000 | Loss: 0.00000976
Iteration 209/1000 | Loss: 0.00000976
Iteration 210/1000 | Loss: 0.00000976
Iteration 211/1000 | Loss: 0.00000976
Iteration 212/1000 | Loss: 0.00000976
Iteration 213/1000 | Loss: 0.00000976
Iteration 214/1000 | Loss: 0.00000976
Iteration 215/1000 | Loss: 0.00000976
Iteration 216/1000 | Loss: 0.00000975
Iteration 217/1000 | Loss: 0.00000975
Iteration 218/1000 | Loss: 0.00000975
Iteration 219/1000 | Loss: 0.00000975
Iteration 220/1000 | Loss: 0.00000975
Iteration 221/1000 | Loss: 0.00000975
Iteration 222/1000 | Loss: 0.00000975
Iteration 223/1000 | Loss: 0.00000975
Iteration 224/1000 | Loss: 0.00000975
Iteration 225/1000 | Loss: 0.00000975
Iteration 226/1000 | Loss: 0.00000975
Iteration 227/1000 | Loss: 0.00000975
Iteration 228/1000 | Loss: 0.00000975
Iteration 229/1000 | Loss: 0.00000975
Iteration 230/1000 | Loss: 0.00000974
Iteration 231/1000 | Loss: 0.00000974
Iteration 232/1000 | Loss: 0.00000974
Iteration 233/1000 | Loss: 0.00000974
Iteration 234/1000 | Loss: 0.00000974
Iteration 235/1000 | Loss: 0.00000974
Iteration 236/1000 | Loss: 0.00000974
Iteration 237/1000 | Loss: 0.00000974
Iteration 238/1000 | Loss: 0.00000974
Iteration 239/1000 | Loss: 0.00000974
Iteration 240/1000 | Loss: 0.00000974
Iteration 241/1000 | Loss: 0.00000974
Iteration 242/1000 | Loss: 0.00000974
Iteration 243/1000 | Loss: 0.00000973
Iteration 244/1000 | Loss: 0.00000973
Iteration 245/1000 | Loss: 0.00000973
Iteration 246/1000 | Loss: 0.00000973
Iteration 247/1000 | Loss: 0.00000973
Iteration 248/1000 | Loss: 0.00000973
Iteration 249/1000 | Loss: 0.00000973
Iteration 250/1000 | Loss: 0.00000973
Iteration 251/1000 | Loss: 0.00000973
Iteration 252/1000 | Loss: 0.00000973
Iteration 253/1000 | Loss: 0.00000973
Iteration 254/1000 | Loss: 0.00000973
Iteration 255/1000 | Loss: 0.00000973
Iteration 256/1000 | Loss: 0.00000973
Iteration 257/1000 | Loss: 0.00000973
Iteration 258/1000 | Loss: 0.00000973
Iteration 259/1000 | Loss: 0.00000972
Iteration 260/1000 | Loss: 0.00000972
Iteration 261/1000 | Loss: 0.00000972
Iteration 262/1000 | Loss: 0.00000972
Iteration 263/1000 | Loss: 0.00000972
Iteration 264/1000 | Loss: 0.00000972
Iteration 265/1000 | Loss: 0.00000972
Iteration 266/1000 | Loss: 0.00000972
Iteration 267/1000 | Loss: 0.00000972
Iteration 268/1000 | Loss: 0.00000972
Iteration 269/1000 | Loss: 0.00000972
Iteration 270/1000 | Loss: 0.00000972
Iteration 271/1000 | Loss: 0.00000972
Iteration 272/1000 | Loss: 0.00000972
Iteration 273/1000 | Loss: 0.00000972
Iteration 274/1000 | Loss: 0.00000972
Iteration 275/1000 | Loss: 0.00000972
Iteration 276/1000 | Loss: 0.00000972
Iteration 277/1000 | Loss: 0.00000971
Iteration 278/1000 | Loss: 0.00000971
Iteration 279/1000 | Loss: 0.00000971
Iteration 280/1000 | Loss: 0.00000971
Iteration 281/1000 | Loss: 0.00000971
Iteration 282/1000 | Loss: 0.00000971
Iteration 283/1000 | Loss: 0.00000971
Iteration 284/1000 | Loss: 0.00000971
Iteration 285/1000 | Loss: 0.00000971
Iteration 286/1000 | Loss: 0.00000971
Iteration 287/1000 | Loss: 0.00000971
Iteration 288/1000 | Loss: 0.00000971
Iteration 289/1000 | Loss: 0.00000971
Iteration 290/1000 | Loss: 0.00000971
Iteration 291/1000 | Loss: 0.00000971
Iteration 292/1000 | Loss: 0.00000971
Iteration 293/1000 | Loss: 0.00000971
Iteration 294/1000 | Loss: 0.00000971
Iteration 295/1000 | Loss: 0.00000971
Iteration 296/1000 | Loss: 0.00000971
Iteration 297/1000 | Loss: 0.00000971
Iteration 298/1000 | Loss: 0.00000971
Iteration 299/1000 | Loss: 0.00000971
Iteration 300/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [9.70858491200488e-06, 9.70858491200488e-06, 9.70858491200488e-06, 9.70858491200488e-06, 9.70858491200488e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.70858491200488e-06

Optimization complete. Final v2v error: 2.5968730449676514 mm

Highest mean error: 3.664180040359497 mm for frame 144

Lowest mean error: 2.242870807647705 mm for frame 3

Saving results

Total time: 180.98014950752258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061075
Iteration 2/25 | Loss: 0.00232890
Iteration 3/25 | Loss: 0.00163580
Iteration 4/25 | Loss: 0.00147170
Iteration 5/25 | Loss: 0.00137042
Iteration 6/25 | Loss: 0.00125723
Iteration 7/25 | Loss: 0.00120966
Iteration 8/25 | Loss: 0.00117464
Iteration 9/25 | Loss: 0.00115516
Iteration 10/25 | Loss: 0.00114375
Iteration 11/25 | Loss: 0.00114162
Iteration 12/25 | Loss: 0.00113510
Iteration 13/25 | Loss: 0.00113416
Iteration 14/25 | Loss: 0.00113225
Iteration 15/25 | Loss: 0.00113211
Iteration 16/25 | Loss: 0.00112896
Iteration 17/25 | Loss: 0.00112927
Iteration 18/25 | Loss: 0.00112951
Iteration 19/25 | Loss: 0.00112776
Iteration 20/25 | Loss: 0.00112806
Iteration 21/25 | Loss: 0.00113029
Iteration 22/25 | Loss: 0.00112950
Iteration 23/25 | Loss: 0.00113016
Iteration 24/25 | Loss: 0.00112724
Iteration 25/25 | Loss: 0.00112580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40276587
Iteration 2/25 | Loss: 0.00164809
Iteration 3/25 | Loss: 0.00160160
Iteration 4/25 | Loss: 0.00160160
Iteration 5/25 | Loss: 0.00160160
Iteration 6/25 | Loss: 0.00160160
Iteration 7/25 | Loss: 0.00160160
Iteration 8/25 | Loss: 0.00160160
Iteration 9/25 | Loss: 0.00160160
Iteration 10/25 | Loss: 0.00160160
Iteration 11/25 | Loss: 0.00160160
Iteration 12/25 | Loss: 0.00160160
Iteration 13/25 | Loss: 0.00160160
Iteration 14/25 | Loss: 0.00160160
Iteration 15/25 | Loss: 0.00160160
Iteration 16/25 | Loss: 0.00160160
Iteration 17/25 | Loss: 0.00160160
Iteration 18/25 | Loss: 0.00160160
Iteration 19/25 | Loss: 0.00160160
Iteration 20/25 | Loss: 0.00160160
Iteration 21/25 | Loss: 0.00160160
Iteration 22/25 | Loss: 0.00160160
Iteration 23/25 | Loss: 0.00160160
Iteration 24/25 | Loss: 0.00160160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001601595082320273, 0.001601595082320273, 0.001601595082320273, 0.001601595082320273, 0.001601595082320273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001601595082320273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160160
Iteration 2/1000 | Loss: 0.00031049
Iteration 3/1000 | Loss: 0.00016141
Iteration 4/1000 | Loss: 0.00012216
Iteration 5/1000 | Loss: 0.00012354
Iteration 6/1000 | Loss: 0.00017082
Iteration 7/1000 | Loss: 0.00020128
Iteration 8/1000 | Loss: 0.00021458
Iteration 9/1000 | Loss: 0.00007680
Iteration 10/1000 | Loss: 0.00021410
Iteration 11/1000 | Loss: 0.00012182
Iteration 12/1000 | Loss: 0.00011608
Iteration 13/1000 | Loss: 0.00010870
Iteration 14/1000 | Loss: 0.00003481
Iteration 15/1000 | Loss: 0.00003487
Iteration 16/1000 | Loss: 0.00004078
Iteration 17/1000 | Loss: 0.00002713
Iteration 18/1000 | Loss: 0.00002642
Iteration 19/1000 | Loss: 0.00039549
Iteration 20/1000 | Loss: 0.00004614
Iteration 21/1000 | Loss: 0.00002606
Iteration 22/1000 | Loss: 0.00041418
Iteration 23/1000 | Loss: 0.00006129
Iteration 24/1000 | Loss: 0.00002946
Iteration 25/1000 | Loss: 0.00006108
Iteration 26/1000 | Loss: 0.00002734
Iteration 27/1000 | Loss: 0.00017124
Iteration 28/1000 | Loss: 0.00025138
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00002532
Iteration 31/1000 | Loss: 0.00002486
Iteration 32/1000 | Loss: 0.00019681
Iteration 33/1000 | Loss: 0.00015438
Iteration 34/1000 | Loss: 0.00014669
Iteration 35/1000 | Loss: 0.00011874
Iteration 36/1000 | Loss: 0.00015006
Iteration 37/1000 | Loss: 0.00005875
Iteration 38/1000 | Loss: 0.00010757
Iteration 39/1000 | Loss: 0.00014522
Iteration 40/1000 | Loss: 0.00006047
Iteration 41/1000 | Loss: 0.00002633
Iteration 42/1000 | Loss: 0.00002237
Iteration 43/1000 | Loss: 0.00002725
Iteration 44/1000 | Loss: 0.00002289
Iteration 45/1000 | Loss: 0.00002631
Iteration 46/1000 | Loss: 0.00002086
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001798
Iteration 51/1000 | Loss: 0.00001882
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001751
Iteration 54/1000 | Loss: 0.00002996
Iteration 55/1000 | Loss: 0.00002190
Iteration 56/1000 | Loss: 0.00037845
Iteration 57/1000 | Loss: 0.00002262
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001739
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001482
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001418
Iteration 67/1000 | Loss: 0.00001416
Iteration 68/1000 | Loss: 0.00001415
Iteration 69/1000 | Loss: 0.00001415
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001405
Iteration 74/1000 | Loss: 0.00001405
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001404
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001398
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001397
Iteration 97/1000 | Loss: 0.00001397
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001396
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001632
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001418
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00002266
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001370
Iteration 117/1000 | Loss: 0.00001370
Iteration 118/1000 | Loss: 0.00001370
Iteration 119/1000 | Loss: 0.00001370
Iteration 120/1000 | Loss: 0.00001370
Iteration 121/1000 | Loss: 0.00001369
Iteration 122/1000 | Loss: 0.00001369
Iteration 123/1000 | Loss: 0.00001369
Iteration 124/1000 | Loss: 0.00001369
Iteration 125/1000 | Loss: 0.00001369
Iteration 126/1000 | Loss: 0.00001369
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001369
Iteration 138/1000 | Loss: 0.00001369
Iteration 139/1000 | Loss: 0.00001369
Iteration 140/1000 | Loss: 0.00001369
Iteration 141/1000 | Loss: 0.00001369
Iteration 142/1000 | Loss: 0.00001369
Iteration 143/1000 | Loss: 0.00001368
Iteration 144/1000 | Loss: 0.00001368
Iteration 145/1000 | Loss: 0.00001368
Iteration 146/1000 | Loss: 0.00001368
Iteration 147/1000 | Loss: 0.00001368
Iteration 148/1000 | Loss: 0.00001368
Iteration 149/1000 | Loss: 0.00001368
Iteration 150/1000 | Loss: 0.00001368
Iteration 151/1000 | Loss: 0.00001368
Iteration 152/1000 | Loss: 0.00001368
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001368
Iteration 165/1000 | Loss: 0.00001368
Iteration 166/1000 | Loss: 0.00001368
Iteration 167/1000 | Loss: 0.00001368
Iteration 168/1000 | Loss: 0.00001368
Iteration 169/1000 | Loss: 0.00001368
Iteration 170/1000 | Loss: 0.00001368
Iteration 171/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.3682958524441347e-05, 1.3682958524441347e-05, 1.3682958524441347e-05, 1.3682958524441347e-05, 1.3682958524441347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3682958524441347e-05

Optimization complete. Final v2v error: 3.1782753467559814 mm

Highest mean error: 9.198110580444336 mm for frame 29

Lowest mean error: 2.662881374359131 mm for frame 17

Saving results

Total time: 176.86110305786133
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078164
Iteration 2/25 | Loss: 0.01078164
Iteration 3/25 | Loss: 0.01078164
Iteration 4/25 | Loss: 0.01078164
Iteration 5/25 | Loss: 0.01078164
Iteration 6/25 | Loss: 0.01078164
Iteration 7/25 | Loss: 0.01078164
Iteration 8/25 | Loss: 0.01078164
Iteration 9/25 | Loss: 0.01078164
Iteration 10/25 | Loss: 0.01078164
Iteration 11/25 | Loss: 0.01078163
Iteration 12/25 | Loss: 0.01078163
Iteration 13/25 | Loss: 0.01078163
Iteration 14/25 | Loss: 0.01078163
Iteration 15/25 | Loss: 0.01078163
Iteration 16/25 | Loss: 0.01078163
Iteration 17/25 | Loss: 0.01078163
Iteration 18/25 | Loss: 0.01078163
Iteration 19/25 | Loss: 0.01078163
Iteration 20/25 | Loss: 0.01078163
Iteration 21/25 | Loss: 0.01078163
Iteration 22/25 | Loss: 0.01078163
Iteration 23/25 | Loss: 0.01078162
Iteration 24/25 | Loss: 0.01078162
Iteration 25/25 | Loss: 0.01078162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87446451
Iteration 2/25 | Loss: 0.05914924
Iteration 3/25 | Loss: 0.05910328
Iteration 4/25 | Loss: 0.05897435
Iteration 5/25 | Loss: 0.05897434
Iteration 6/25 | Loss: 0.05897433
Iteration 7/25 | Loss: 0.05897433
Iteration 8/25 | Loss: 0.05897431
Iteration 9/25 | Loss: 0.05897431
Iteration 10/25 | Loss: 0.05897431
Iteration 11/25 | Loss: 0.05897431
Iteration 12/25 | Loss: 0.05897431
Iteration 13/25 | Loss: 0.05897431
Iteration 14/25 | Loss: 0.05897431
Iteration 15/25 | Loss: 0.05897431
Iteration 16/25 | Loss: 0.05897431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.058974314481019974, 0.058974314481019974, 0.058974314481019974, 0.058974314481019974, 0.058974314481019974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.058974314481019974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05897431
Iteration 2/1000 | Loss: 0.00481860
Iteration 3/1000 | Loss: 0.00337211
Iteration 4/1000 | Loss: 0.00197013
Iteration 5/1000 | Loss: 0.00188586
Iteration 6/1000 | Loss: 0.00805395
Iteration 7/1000 | Loss: 0.00149441
Iteration 8/1000 | Loss: 0.00344100
Iteration 9/1000 | Loss: 0.00031016
Iteration 10/1000 | Loss: 0.00036783
Iteration 11/1000 | Loss: 0.00079209
Iteration 12/1000 | Loss: 0.00050010
Iteration 13/1000 | Loss: 0.00149714
Iteration 14/1000 | Loss: 0.00017142
Iteration 15/1000 | Loss: 0.00021519
Iteration 16/1000 | Loss: 0.00024763
Iteration 17/1000 | Loss: 0.00020233
Iteration 18/1000 | Loss: 0.00022944
Iteration 19/1000 | Loss: 0.00019794
Iteration 20/1000 | Loss: 0.00062836
Iteration 21/1000 | Loss: 0.00020216
Iteration 22/1000 | Loss: 0.00006341
Iteration 23/1000 | Loss: 0.00006979
Iteration 24/1000 | Loss: 0.00021088
Iteration 25/1000 | Loss: 0.00051322
Iteration 26/1000 | Loss: 0.00029499
Iteration 27/1000 | Loss: 0.00037254
Iteration 28/1000 | Loss: 0.00076755
Iteration 29/1000 | Loss: 0.00012893
Iteration 30/1000 | Loss: 0.00012443
Iteration 31/1000 | Loss: 0.00009618
Iteration 32/1000 | Loss: 0.00010407
Iteration 33/1000 | Loss: 0.00006823
Iteration 34/1000 | Loss: 0.00007387
Iteration 35/1000 | Loss: 0.00008356
Iteration 36/1000 | Loss: 0.00004289
Iteration 37/1000 | Loss: 0.00032688
Iteration 38/1000 | Loss: 0.00004333
Iteration 39/1000 | Loss: 0.00013787
Iteration 40/1000 | Loss: 0.00007100
Iteration 41/1000 | Loss: 0.00003875
Iteration 42/1000 | Loss: 0.00018831
Iteration 43/1000 | Loss: 0.00011330
Iteration 44/1000 | Loss: 0.00004491
Iteration 45/1000 | Loss: 0.00005489
Iteration 46/1000 | Loss: 0.00011255
Iteration 47/1000 | Loss: 0.00004409
Iteration 48/1000 | Loss: 0.00003617
Iteration 49/1000 | Loss: 0.00012140
Iteration 50/1000 | Loss: 0.00022916
Iteration 51/1000 | Loss: 0.00015629
Iteration 52/1000 | Loss: 0.00009507
Iteration 53/1000 | Loss: 0.00009377
Iteration 54/1000 | Loss: 0.00016425
Iteration 55/1000 | Loss: 0.00003758
Iteration 56/1000 | Loss: 0.00003656
Iteration 57/1000 | Loss: 0.00003511
Iteration 58/1000 | Loss: 0.00004054
Iteration 59/1000 | Loss: 0.00008678
Iteration 60/1000 | Loss: 0.00003782
Iteration 61/1000 | Loss: 0.00003631
Iteration 62/1000 | Loss: 0.00003654
Iteration 63/1000 | Loss: 0.00003442
Iteration 64/1000 | Loss: 0.00003768
Iteration 65/1000 | Loss: 0.00003206
Iteration 66/1000 | Loss: 0.00003202
Iteration 67/1000 | Loss: 0.00003636
Iteration 68/1000 | Loss: 0.00003137
Iteration 69/1000 | Loss: 0.00003133
Iteration 70/1000 | Loss: 0.00003133
Iteration 71/1000 | Loss: 0.00003133
Iteration 72/1000 | Loss: 0.00003133
Iteration 73/1000 | Loss: 0.00003133
Iteration 74/1000 | Loss: 0.00003133
Iteration 75/1000 | Loss: 0.00003133
Iteration 76/1000 | Loss: 0.00003133
Iteration 77/1000 | Loss: 0.00003133
Iteration 78/1000 | Loss: 0.00003133
Iteration 79/1000 | Loss: 0.00003133
Iteration 80/1000 | Loss: 0.00003133
Iteration 81/1000 | Loss: 0.00003133
Iteration 82/1000 | Loss: 0.00003133
Iteration 83/1000 | Loss: 0.00003133
Iteration 84/1000 | Loss: 0.00003133
Iteration 85/1000 | Loss: 0.00003133
Iteration 86/1000 | Loss: 0.00003133
Iteration 87/1000 | Loss: 0.00003133
Iteration 88/1000 | Loss: 0.00003133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.133081554551609e-05, 3.133081554551609e-05, 3.133081554551609e-05, 3.133081554551609e-05, 3.133081554551609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.133081554551609e-05

Optimization complete. Final v2v error: 3.3151190280914307 mm

Highest mean error: 20.889507293701172 mm for frame 67

Lowest mean error: 2.4691851139068604 mm for frame 3

Saving results

Total time: 113.95428705215454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840267
Iteration 2/25 | Loss: 0.00115344
Iteration 3/25 | Loss: 0.00102237
Iteration 4/25 | Loss: 0.00100964
Iteration 5/25 | Loss: 0.00100695
Iteration 6/25 | Loss: 0.00100660
Iteration 7/25 | Loss: 0.00100660
Iteration 8/25 | Loss: 0.00100660
Iteration 9/25 | Loss: 0.00100660
Iteration 10/25 | Loss: 0.00100660
Iteration 11/25 | Loss: 0.00100660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010066046379506588, 0.0010066046379506588, 0.0010066046379506588, 0.0010066046379506588, 0.0010066046379506588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010066046379506588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26343989
Iteration 2/25 | Loss: 0.00137223
Iteration 3/25 | Loss: 0.00137222
Iteration 4/25 | Loss: 0.00137222
Iteration 5/25 | Loss: 0.00137222
Iteration 6/25 | Loss: 0.00137222
Iteration 7/25 | Loss: 0.00137222
Iteration 8/25 | Loss: 0.00137222
Iteration 9/25 | Loss: 0.00137222
Iteration 10/25 | Loss: 0.00137222
Iteration 11/25 | Loss: 0.00137222
Iteration 12/25 | Loss: 0.00137222
Iteration 13/25 | Loss: 0.00137222
Iteration 14/25 | Loss: 0.00137222
Iteration 15/25 | Loss: 0.00137222
Iteration 16/25 | Loss: 0.00137222
Iteration 17/25 | Loss: 0.00137222
Iteration 18/25 | Loss: 0.00137222
Iteration 19/25 | Loss: 0.00137222
Iteration 20/25 | Loss: 0.00137222
Iteration 21/25 | Loss: 0.00137222
Iteration 22/25 | Loss: 0.00137222
Iteration 23/25 | Loss: 0.00137222
Iteration 24/25 | Loss: 0.00137222
Iteration 25/25 | Loss: 0.00137222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137222
Iteration 2/1000 | Loss: 0.00002865
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001350
Iteration 5/1000 | Loss: 0.00001232
Iteration 6/1000 | Loss: 0.00001175
Iteration 7/1000 | Loss: 0.00001125
Iteration 8/1000 | Loss: 0.00001102
Iteration 9/1000 | Loss: 0.00001096
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001069
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001050
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00001048
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001047
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001045
Iteration 21/1000 | Loss: 0.00001044
Iteration 22/1000 | Loss: 0.00001044
Iteration 23/1000 | Loss: 0.00001043
Iteration 24/1000 | Loss: 0.00001042
Iteration 25/1000 | Loss: 0.00001039
Iteration 26/1000 | Loss: 0.00001037
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001034
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001031
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001030
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001027
Iteration 39/1000 | Loss: 0.00001026
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001025
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001024
Iteration 45/1000 | Loss: 0.00001023
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001022
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001020
Iteration 52/1000 | Loss: 0.00001020
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001019
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001017
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001013
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001012
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001010
Iteration 96/1000 | Loss: 0.00001010
Iteration 97/1000 | Loss: 0.00001010
Iteration 98/1000 | Loss: 0.00001010
Iteration 99/1000 | Loss: 0.00001010
Iteration 100/1000 | Loss: 0.00001010
Iteration 101/1000 | Loss: 0.00001010
Iteration 102/1000 | Loss: 0.00001010
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001010
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001010
Iteration 107/1000 | Loss: 0.00001010
Iteration 108/1000 | Loss: 0.00001009
Iteration 109/1000 | Loss: 0.00001009
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001009
Iteration 113/1000 | Loss: 0.00001009
Iteration 114/1000 | Loss: 0.00001008
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001008
Iteration 119/1000 | Loss: 0.00001008
Iteration 120/1000 | Loss: 0.00001008
Iteration 121/1000 | Loss: 0.00001008
Iteration 122/1000 | Loss: 0.00001008
Iteration 123/1000 | Loss: 0.00001008
Iteration 124/1000 | Loss: 0.00001008
Iteration 125/1000 | Loss: 0.00001008
Iteration 126/1000 | Loss: 0.00001007
Iteration 127/1000 | Loss: 0.00001007
Iteration 128/1000 | Loss: 0.00001007
Iteration 129/1000 | Loss: 0.00001007
Iteration 130/1000 | Loss: 0.00001007
Iteration 131/1000 | Loss: 0.00001007
Iteration 132/1000 | Loss: 0.00001007
Iteration 133/1000 | Loss: 0.00001007
Iteration 134/1000 | Loss: 0.00001007
Iteration 135/1000 | Loss: 0.00001007
Iteration 136/1000 | Loss: 0.00001007
Iteration 137/1000 | Loss: 0.00001007
Iteration 138/1000 | Loss: 0.00001007
Iteration 139/1000 | Loss: 0.00001007
Iteration 140/1000 | Loss: 0.00001007
Iteration 141/1000 | Loss: 0.00001006
Iteration 142/1000 | Loss: 0.00001006
Iteration 143/1000 | Loss: 0.00001006
Iteration 144/1000 | Loss: 0.00001006
Iteration 145/1000 | Loss: 0.00001006
Iteration 146/1000 | Loss: 0.00001006
Iteration 147/1000 | Loss: 0.00001006
Iteration 148/1000 | Loss: 0.00001006
Iteration 149/1000 | Loss: 0.00001006
Iteration 150/1000 | Loss: 0.00001006
Iteration 151/1000 | Loss: 0.00001006
Iteration 152/1000 | Loss: 0.00001006
Iteration 153/1000 | Loss: 0.00001006
Iteration 154/1000 | Loss: 0.00001006
Iteration 155/1000 | Loss: 0.00001006
Iteration 156/1000 | Loss: 0.00001005
Iteration 157/1000 | Loss: 0.00001005
Iteration 158/1000 | Loss: 0.00001005
Iteration 159/1000 | Loss: 0.00001005
Iteration 160/1000 | Loss: 0.00001005
Iteration 161/1000 | Loss: 0.00001005
Iteration 162/1000 | Loss: 0.00001005
Iteration 163/1000 | Loss: 0.00001005
Iteration 164/1000 | Loss: 0.00001005
Iteration 165/1000 | Loss: 0.00001005
Iteration 166/1000 | Loss: 0.00001005
Iteration 167/1000 | Loss: 0.00001005
Iteration 168/1000 | Loss: 0.00001005
Iteration 169/1000 | Loss: 0.00001005
Iteration 170/1000 | Loss: 0.00001005
Iteration 171/1000 | Loss: 0.00001005
Iteration 172/1000 | Loss: 0.00001005
Iteration 173/1000 | Loss: 0.00001005
Iteration 174/1000 | Loss: 0.00001005
Iteration 175/1000 | Loss: 0.00001005
Iteration 176/1000 | Loss: 0.00001005
Iteration 177/1000 | Loss: 0.00001005
Iteration 178/1000 | Loss: 0.00001005
Iteration 179/1000 | Loss: 0.00001005
Iteration 180/1000 | Loss: 0.00001004
Iteration 181/1000 | Loss: 0.00001004
Iteration 182/1000 | Loss: 0.00001004
Iteration 183/1000 | Loss: 0.00001004
Iteration 184/1000 | Loss: 0.00001004
Iteration 185/1000 | Loss: 0.00001004
Iteration 186/1000 | Loss: 0.00001004
Iteration 187/1000 | Loss: 0.00001004
Iteration 188/1000 | Loss: 0.00001004
Iteration 189/1000 | Loss: 0.00001004
Iteration 190/1000 | Loss: 0.00001004
Iteration 191/1000 | Loss: 0.00001004
Iteration 192/1000 | Loss: 0.00001004
Iteration 193/1000 | Loss: 0.00001004
Iteration 194/1000 | Loss: 0.00001004
Iteration 195/1000 | Loss: 0.00001004
Iteration 196/1000 | Loss: 0.00001004
Iteration 197/1000 | Loss: 0.00001004
Iteration 198/1000 | Loss: 0.00001004
Iteration 199/1000 | Loss: 0.00001004
Iteration 200/1000 | Loss: 0.00001004
Iteration 201/1000 | Loss: 0.00001004
Iteration 202/1000 | Loss: 0.00001004
Iteration 203/1000 | Loss: 0.00001004
Iteration 204/1000 | Loss: 0.00001004
Iteration 205/1000 | Loss: 0.00001004
Iteration 206/1000 | Loss: 0.00001004
Iteration 207/1000 | Loss: 0.00001004
Iteration 208/1000 | Loss: 0.00001004
Iteration 209/1000 | Loss: 0.00001004
Iteration 210/1000 | Loss: 0.00001004
Iteration 211/1000 | Loss: 0.00001004
Iteration 212/1000 | Loss: 0.00001004
Iteration 213/1000 | Loss: 0.00001004
Iteration 214/1000 | Loss: 0.00001004
Iteration 215/1000 | Loss: 0.00001004
Iteration 216/1000 | Loss: 0.00001004
Iteration 217/1000 | Loss: 0.00001004
Iteration 218/1000 | Loss: 0.00001004
Iteration 219/1000 | Loss: 0.00001004
Iteration 220/1000 | Loss: 0.00001004
Iteration 221/1000 | Loss: 0.00001004
Iteration 222/1000 | Loss: 0.00001004
Iteration 223/1000 | Loss: 0.00001004
Iteration 224/1000 | Loss: 0.00001004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.0044811460829806e-05, 1.0044811460829806e-05, 1.0044811460829806e-05, 1.0044811460829806e-05, 1.0044811460829806e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0044811460829806e-05

Optimization complete. Final v2v error: 2.6411023139953613 mm

Highest mean error: 2.892214298248291 mm for frame 82

Lowest mean error: 2.318253517150879 mm for frame 159

Saving results

Total time: 36.751198053359985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035347
Iteration 2/25 | Loss: 0.00333536
Iteration 3/25 | Loss: 0.00183433
Iteration 4/25 | Loss: 0.00162793
Iteration 5/25 | Loss: 0.00156232
Iteration 6/25 | Loss: 0.00151964
Iteration 7/25 | Loss: 0.00150267
Iteration 8/25 | Loss: 0.00148853
Iteration 9/25 | Loss: 0.00146055
Iteration 10/25 | Loss: 0.00144882
Iteration 11/25 | Loss: 0.00143744
Iteration 12/25 | Loss: 0.00143435
Iteration 13/25 | Loss: 0.00142952
Iteration 14/25 | Loss: 0.00141845
Iteration 15/25 | Loss: 0.00141661
Iteration 16/25 | Loss: 0.00141594
Iteration 17/25 | Loss: 0.00141864
Iteration 18/25 | Loss: 0.00141490
Iteration 19/25 | Loss: 0.00141385
Iteration 20/25 | Loss: 0.00141363
Iteration 21/25 | Loss: 0.00141441
Iteration 22/25 | Loss: 0.00141389
Iteration 23/25 | Loss: 0.00141464
Iteration 24/25 | Loss: 0.00141388
Iteration 25/25 | Loss: 0.00141443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20217085
Iteration 2/25 | Loss: 0.00396193
Iteration 3/25 | Loss: 0.00396193
Iteration 4/25 | Loss: 0.00396192
Iteration 5/25 | Loss: 0.00396192
Iteration 6/25 | Loss: 0.00396192
Iteration 7/25 | Loss: 0.00396192
Iteration 8/25 | Loss: 0.00396192
Iteration 9/25 | Loss: 0.00396192
Iteration 10/25 | Loss: 0.00396192
Iteration 11/25 | Loss: 0.00396192
Iteration 12/25 | Loss: 0.00396192
Iteration 13/25 | Loss: 0.00396192
Iteration 14/25 | Loss: 0.00396192
Iteration 15/25 | Loss: 0.00396192
Iteration 16/25 | Loss: 0.00396192
Iteration 17/25 | Loss: 0.00396192
Iteration 18/25 | Loss: 0.00396192
Iteration 19/25 | Loss: 0.00396192
Iteration 20/25 | Loss: 0.00396192
Iteration 21/25 | Loss: 0.00396192
Iteration 22/25 | Loss: 0.00396192
Iteration 23/25 | Loss: 0.00396192
Iteration 24/25 | Loss: 0.00396192
Iteration 25/25 | Loss: 0.00396192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00396192
Iteration 2/1000 | Loss: 0.00050937
Iteration 3/1000 | Loss: 0.00038156
Iteration 4/1000 | Loss: 0.00032073
Iteration 5/1000 | Loss: 0.00051788
Iteration 6/1000 | Loss: 0.00142062
Iteration 7/1000 | Loss: 0.00027404
Iteration 8/1000 | Loss: 0.00033617
Iteration 9/1000 | Loss: 0.00046022
Iteration 10/1000 | Loss: 0.00022874
Iteration 11/1000 | Loss: 0.00023194
Iteration 12/1000 | Loss: 0.00045613
Iteration 13/1000 | Loss: 0.00026044
Iteration 14/1000 | Loss: 0.00027095
Iteration 15/1000 | Loss: 0.00025755
Iteration 16/1000 | Loss: 0.00034035
Iteration 17/1000 | Loss: 0.00039402
Iteration 18/1000 | Loss: 0.00113578
Iteration 19/1000 | Loss: 0.00344771
Iteration 20/1000 | Loss: 0.01141884
Iteration 21/1000 | Loss: 0.00982791
Iteration 22/1000 | Loss: 0.00731022
Iteration 23/1000 | Loss: 0.00987970
Iteration 24/1000 | Loss: 0.00445432
Iteration 25/1000 | Loss: 0.00464528
Iteration 26/1000 | Loss: 0.00219085
Iteration 27/1000 | Loss: 0.00147715
Iteration 28/1000 | Loss: 0.00143781
Iteration 29/1000 | Loss: 0.00092316
Iteration 30/1000 | Loss: 0.00126928
Iteration 31/1000 | Loss: 0.00121945
Iteration 32/1000 | Loss: 0.00193976
Iteration 33/1000 | Loss: 0.00158671
Iteration 34/1000 | Loss: 0.00153865
Iteration 35/1000 | Loss: 0.00135842
Iteration 36/1000 | Loss: 0.00154434
Iteration 37/1000 | Loss: 0.00212370
Iteration 38/1000 | Loss: 0.00164946
Iteration 39/1000 | Loss: 0.00108821
Iteration 40/1000 | Loss: 0.00124341
Iteration 41/1000 | Loss: 0.00121196
Iteration 42/1000 | Loss: 0.00140215
Iteration 43/1000 | Loss: 0.00123442
Iteration 44/1000 | Loss: 0.00120033
Iteration 45/1000 | Loss: 0.00135343
Iteration 46/1000 | Loss: 0.00132365
Iteration 47/1000 | Loss: 0.00148076
Iteration 48/1000 | Loss: 0.00133718
Iteration 49/1000 | Loss: 0.00182979
Iteration 50/1000 | Loss: 0.00144516
Iteration 51/1000 | Loss: 0.00198253
Iteration 52/1000 | Loss: 0.00117947
Iteration 53/1000 | Loss: 0.00139391
Iteration 54/1000 | Loss: 0.00107770
Iteration 55/1000 | Loss: 0.00240193
Iteration 56/1000 | Loss: 0.00160160
Iteration 57/1000 | Loss: 0.00139765
Iteration 58/1000 | Loss: 0.00128979
Iteration 59/1000 | Loss: 0.00078764
Iteration 60/1000 | Loss: 0.00080925
Iteration 61/1000 | Loss: 0.00060064
Iteration 62/1000 | Loss: 0.00087864
Iteration 63/1000 | Loss: 0.00050729
Iteration 64/1000 | Loss: 0.00071370
Iteration 65/1000 | Loss: 0.00079063
Iteration 66/1000 | Loss: 0.00060330
Iteration 67/1000 | Loss: 0.00060760
Iteration 68/1000 | Loss: 0.00056194
Iteration 69/1000 | Loss: 0.00075385
Iteration 70/1000 | Loss: 0.00107065
Iteration 71/1000 | Loss: 0.00068932
Iteration 72/1000 | Loss: 0.00121871
Iteration 73/1000 | Loss: 0.00280977
Iteration 74/1000 | Loss: 0.00139759
Iteration 75/1000 | Loss: 0.00149666
Iteration 76/1000 | Loss: 0.00092558
Iteration 77/1000 | Loss: 0.00080845
Iteration 78/1000 | Loss: 0.00115624
Iteration 79/1000 | Loss: 0.00125917
Iteration 80/1000 | Loss: 0.00052070
Iteration 81/1000 | Loss: 0.00106469
Iteration 82/1000 | Loss: 0.00069036
Iteration 83/1000 | Loss: 0.00052061
Iteration 84/1000 | Loss: 0.00061974
Iteration 85/1000 | Loss: 0.00071429
Iteration 86/1000 | Loss: 0.00061752
Iteration 87/1000 | Loss: 0.00041424
Iteration 88/1000 | Loss: 0.00046566
Iteration 89/1000 | Loss: 0.00031205
Iteration 90/1000 | Loss: 0.00039572
Iteration 91/1000 | Loss: 0.00041972
Iteration 92/1000 | Loss: 0.00025855
Iteration 93/1000 | Loss: 0.00025017
Iteration 94/1000 | Loss: 0.00015312
Iteration 95/1000 | Loss: 0.00032676
Iteration 96/1000 | Loss: 0.00074430
Iteration 97/1000 | Loss: 0.00047081
Iteration 98/1000 | Loss: 0.00019766
Iteration 99/1000 | Loss: 0.00029125
Iteration 100/1000 | Loss: 0.00019166
Iteration 101/1000 | Loss: 0.00027846
Iteration 102/1000 | Loss: 0.00016359
Iteration 103/1000 | Loss: 0.00035951
Iteration 104/1000 | Loss: 0.00036549
Iteration 105/1000 | Loss: 0.00020735
Iteration 106/1000 | Loss: 0.00016395
Iteration 107/1000 | Loss: 0.00015796
Iteration 108/1000 | Loss: 0.00018759
Iteration 109/1000 | Loss: 0.00029247
Iteration 110/1000 | Loss: 0.00039213
Iteration 111/1000 | Loss: 0.00021091
Iteration 112/1000 | Loss: 0.00015575
Iteration 113/1000 | Loss: 0.00012977
Iteration 114/1000 | Loss: 0.00041924
Iteration 115/1000 | Loss: 0.00024279
Iteration 116/1000 | Loss: 0.00058665
Iteration 117/1000 | Loss: 0.00025318
Iteration 118/1000 | Loss: 0.00025001
Iteration 119/1000 | Loss: 0.00041296
Iteration 120/1000 | Loss: 0.00020655
Iteration 121/1000 | Loss: 0.00011864
Iteration 122/1000 | Loss: 0.00011611
Iteration 123/1000 | Loss: 0.00011395
Iteration 124/1000 | Loss: 0.00036401
Iteration 125/1000 | Loss: 0.00025006
Iteration 126/1000 | Loss: 0.00052307
Iteration 127/1000 | Loss: 0.00053007
Iteration 128/1000 | Loss: 0.00044146
Iteration 129/1000 | Loss: 0.00019972
Iteration 130/1000 | Loss: 0.00060648
Iteration 131/1000 | Loss: 0.00034605
Iteration 132/1000 | Loss: 0.00019707
Iteration 133/1000 | Loss: 0.00041129
Iteration 134/1000 | Loss: 0.00012324
Iteration 135/1000 | Loss: 0.00029493
Iteration 136/1000 | Loss: 0.00019811
Iteration 137/1000 | Loss: 0.00036801
Iteration 138/1000 | Loss: 0.00023337
Iteration 139/1000 | Loss: 0.00041874
Iteration 140/1000 | Loss: 0.00044947
Iteration 141/1000 | Loss: 0.00065976
Iteration 142/1000 | Loss: 0.00038323
Iteration 143/1000 | Loss: 0.00036256
Iteration 144/1000 | Loss: 0.00033973
Iteration 145/1000 | Loss: 0.00019942
Iteration 146/1000 | Loss: 0.00013522
Iteration 147/1000 | Loss: 0.00013086
Iteration 148/1000 | Loss: 0.00015012
Iteration 149/1000 | Loss: 0.00011367
Iteration 150/1000 | Loss: 0.00012617
Iteration 151/1000 | Loss: 0.00011178
Iteration 152/1000 | Loss: 0.00020503
Iteration 153/1000 | Loss: 0.00036294
Iteration 154/1000 | Loss: 0.00021830
Iteration 155/1000 | Loss: 0.00022181
Iteration 156/1000 | Loss: 0.00033959
Iteration 157/1000 | Loss: 0.00024876
Iteration 158/1000 | Loss: 0.00013827
Iteration 159/1000 | Loss: 0.00017877
Iteration 160/1000 | Loss: 0.00011224
Iteration 161/1000 | Loss: 0.00016576
Iteration 162/1000 | Loss: 0.00011296
Iteration 163/1000 | Loss: 0.00015776
Iteration 164/1000 | Loss: 0.00025839
Iteration 165/1000 | Loss: 0.00015859
Iteration 166/1000 | Loss: 0.00014185
Iteration 167/1000 | Loss: 0.00022395
Iteration 168/1000 | Loss: 0.00033514
Iteration 169/1000 | Loss: 0.00020133
Iteration 170/1000 | Loss: 0.00036785
Iteration 171/1000 | Loss: 0.00018439
Iteration 172/1000 | Loss: 0.00032782
Iteration 173/1000 | Loss: 0.00028043
Iteration 174/1000 | Loss: 0.00011946
Iteration 175/1000 | Loss: 0.00062382
Iteration 176/1000 | Loss: 0.00077015
Iteration 177/1000 | Loss: 0.00030390
Iteration 178/1000 | Loss: 0.00107385
Iteration 179/1000 | Loss: 0.00057020
Iteration 180/1000 | Loss: 0.00065014
Iteration 181/1000 | Loss: 0.00058043
Iteration 182/1000 | Loss: 0.00032175
Iteration 183/1000 | Loss: 0.00043043
Iteration 184/1000 | Loss: 0.00036139
Iteration 185/1000 | Loss: 0.00042114
Iteration 186/1000 | Loss: 0.00050397
Iteration 187/1000 | Loss: 0.00029327
Iteration 188/1000 | Loss: 0.00031747
Iteration 189/1000 | Loss: 0.00012750
Iteration 190/1000 | Loss: 0.00037312
Iteration 191/1000 | Loss: 0.00013540
Iteration 192/1000 | Loss: 0.00041000
Iteration 193/1000 | Loss: 0.00043778
Iteration 194/1000 | Loss: 0.00025723
Iteration 195/1000 | Loss: 0.00012202
Iteration 196/1000 | Loss: 0.00027338
Iteration 197/1000 | Loss: 0.00021732
Iteration 198/1000 | Loss: 0.00034453
Iteration 199/1000 | Loss: 0.00038504
Iteration 200/1000 | Loss: 0.00027478
Iteration 201/1000 | Loss: 0.00012787
Iteration 202/1000 | Loss: 0.00015457
Iteration 203/1000 | Loss: 0.00030701
Iteration 204/1000 | Loss: 0.00045742
Iteration 205/1000 | Loss: 0.00038511
Iteration 206/1000 | Loss: 0.00081822
Iteration 207/1000 | Loss: 0.00026710
Iteration 208/1000 | Loss: 0.00028571
Iteration 209/1000 | Loss: 0.00020603
Iteration 210/1000 | Loss: 0.00022481
Iteration 211/1000 | Loss: 0.00019741
Iteration 212/1000 | Loss: 0.00050041
Iteration 213/1000 | Loss: 0.00024319
Iteration 214/1000 | Loss: 0.00048114
Iteration 215/1000 | Loss: 0.00032375
Iteration 216/1000 | Loss: 0.00044707
Iteration 217/1000 | Loss: 0.00033531
Iteration 218/1000 | Loss: 0.00057881
Iteration 219/1000 | Loss: 0.00065705
Iteration 220/1000 | Loss: 0.00070982
Iteration 221/1000 | Loss: 0.00050998
Iteration 222/1000 | Loss: 0.00120064
Iteration 223/1000 | Loss: 0.00056394
Iteration 224/1000 | Loss: 0.00026318
Iteration 225/1000 | Loss: 0.00027684
Iteration 226/1000 | Loss: 0.00042522
Iteration 227/1000 | Loss: 0.00029478
Iteration 228/1000 | Loss: 0.00026325
Iteration 229/1000 | Loss: 0.00024469
Iteration 230/1000 | Loss: 0.00023843
Iteration 231/1000 | Loss: 0.00016065
Iteration 232/1000 | Loss: 0.00036670
Iteration 233/1000 | Loss: 0.00017824
Iteration 234/1000 | Loss: 0.00027977
Iteration 235/1000 | Loss: 0.00039920
Iteration 236/1000 | Loss: 0.00045599
Iteration 237/1000 | Loss: 0.00036278
Iteration 238/1000 | Loss: 0.00012775
Iteration 239/1000 | Loss: 0.00023949
Iteration 240/1000 | Loss: 0.00025592
Iteration 241/1000 | Loss: 0.00015643
Iteration 242/1000 | Loss: 0.00018613
Iteration 243/1000 | Loss: 0.00014515
Iteration 244/1000 | Loss: 0.00040304
Iteration 245/1000 | Loss: 0.00018487
Iteration 246/1000 | Loss: 0.00059510
Iteration 247/1000 | Loss: 0.00030108
Iteration 248/1000 | Loss: 0.00029226
Iteration 249/1000 | Loss: 0.00022180
Iteration 250/1000 | Loss: 0.00028686
Iteration 251/1000 | Loss: 0.00034098
Iteration 252/1000 | Loss: 0.00020509
Iteration 253/1000 | Loss: 0.00012487
Iteration 254/1000 | Loss: 0.00010468
Iteration 255/1000 | Loss: 0.00037133
Iteration 256/1000 | Loss: 0.00037382
Iteration 257/1000 | Loss: 0.00014444
Iteration 258/1000 | Loss: 0.00012405
Iteration 259/1000 | Loss: 0.00027496
Iteration 260/1000 | Loss: 0.00017544
Iteration 261/1000 | Loss: 0.00009894
Iteration 262/1000 | Loss: 0.00017679
Iteration 263/1000 | Loss: 0.00074938
Iteration 264/1000 | Loss: 0.00032709
Iteration 265/1000 | Loss: 0.00033452
Iteration 266/1000 | Loss: 0.00016638
Iteration 267/1000 | Loss: 0.00022377
Iteration 268/1000 | Loss: 0.00016826
Iteration 269/1000 | Loss: 0.00018844
Iteration 270/1000 | Loss: 0.00014106
Iteration 271/1000 | Loss: 0.00016036
Iteration 272/1000 | Loss: 0.00013714
Iteration 273/1000 | Loss: 0.00058763
Iteration 274/1000 | Loss: 0.00046210
Iteration 275/1000 | Loss: 0.00021387
Iteration 276/1000 | Loss: 0.00024103
Iteration 277/1000 | Loss: 0.00028210
Iteration 278/1000 | Loss: 0.00047669
Iteration 279/1000 | Loss: 0.00040014
Iteration 280/1000 | Loss: 0.00015458
Iteration 281/1000 | Loss: 0.00014018
Iteration 282/1000 | Loss: 0.00018465
Iteration 283/1000 | Loss: 0.00026010
Iteration 284/1000 | Loss: 0.00023210
Iteration 285/1000 | Loss: 0.00029021
Iteration 286/1000 | Loss: 0.00034322
Iteration 287/1000 | Loss: 0.00080157
Iteration 288/1000 | Loss: 0.00040785
Iteration 289/1000 | Loss: 0.00024738
Iteration 290/1000 | Loss: 0.00054810
Iteration 291/1000 | Loss: 0.00056253
Iteration 292/1000 | Loss: 0.00037818
Iteration 293/1000 | Loss: 0.00034355
Iteration 294/1000 | Loss: 0.00057327
Iteration 295/1000 | Loss: 0.00046550
Iteration 296/1000 | Loss: 0.00046612
Iteration 297/1000 | Loss: 0.00046355
Iteration 298/1000 | Loss: 0.00041628
Iteration 299/1000 | Loss: 0.00032952
Iteration 300/1000 | Loss: 0.00063308
Iteration 301/1000 | Loss: 0.00043165
Iteration 302/1000 | Loss: 0.00040116
Iteration 303/1000 | Loss: 0.00036211
Iteration 304/1000 | Loss: 0.00089908
Iteration 305/1000 | Loss: 0.00053246
Iteration 306/1000 | Loss: 0.00040220
Iteration 307/1000 | Loss: 0.00040646
Iteration 308/1000 | Loss: 0.00030081
Iteration 309/1000 | Loss: 0.00034402
Iteration 310/1000 | Loss: 0.00029404
Iteration 311/1000 | Loss: 0.00027790
Iteration 312/1000 | Loss: 0.00029686
Iteration 313/1000 | Loss: 0.00035779
Iteration 314/1000 | Loss: 0.00022993
Iteration 315/1000 | Loss: 0.00029988
Iteration 316/1000 | Loss: 0.00090718
Iteration 317/1000 | Loss: 0.00044737
Iteration 318/1000 | Loss: 0.00026721
Iteration 319/1000 | Loss: 0.00013788
Iteration 320/1000 | Loss: 0.00009289
Iteration 321/1000 | Loss: 0.00008787
Iteration 322/1000 | Loss: 0.00065805
Iteration 323/1000 | Loss: 0.00017095
Iteration 324/1000 | Loss: 0.00011768
Iteration 325/1000 | Loss: 0.00009250
Iteration 326/1000 | Loss: 0.00013378
Iteration 327/1000 | Loss: 0.00023664
Iteration 328/1000 | Loss: 0.00052664
Iteration 329/1000 | Loss: 0.00058238
Iteration 330/1000 | Loss: 0.00050228
Iteration 331/1000 | Loss: 0.00040518
Iteration 332/1000 | Loss: 0.00037715
Iteration 333/1000 | Loss: 0.00038846
Iteration 334/1000 | Loss: 0.00049263
Iteration 335/1000 | Loss: 0.00092361
Iteration 336/1000 | Loss: 0.00041499
Iteration 337/1000 | Loss: 0.00025523
Iteration 338/1000 | Loss: 0.00018094
Iteration 339/1000 | Loss: 0.00042433
Iteration 340/1000 | Loss: 0.00025697
Iteration 341/1000 | Loss: 0.00039982
Iteration 342/1000 | Loss: 0.00031363
Iteration 343/1000 | Loss: 0.00023024
Iteration 344/1000 | Loss: 0.00015058
Iteration 345/1000 | Loss: 0.00022041
Iteration 346/1000 | Loss: 0.00029144
Iteration 347/1000 | Loss: 0.00019792
Iteration 348/1000 | Loss: 0.00028637
Iteration 349/1000 | Loss: 0.00017460
Iteration 350/1000 | Loss: 0.00018406
Iteration 351/1000 | Loss: 0.00013742
Iteration 352/1000 | Loss: 0.00016643
Iteration 353/1000 | Loss: 0.00037019
Iteration 354/1000 | Loss: 0.00024219
Iteration 355/1000 | Loss: 0.00011670
Iteration 356/1000 | Loss: 0.00023961
Iteration 357/1000 | Loss: 0.00063392
Iteration 358/1000 | Loss: 0.00055471
Iteration 359/1000 | Loss: 0.00031831
Iteration 360/1000 | Loss: 0.00036537
Iteration 361/1000 | Loss: 0.00032597
Iteration 362/1000 | Loss: 0.00014632
Iteration 363/1000 | Loss: 0.00031349
Iteration 364/1000 | Loss: 0.00021467
Iteration 365/1000 | Loss: 0.00022518
Iteration 366/1000 | Loss: 0.00071886
Iteration 367/1000 | Loss: 0.00025955
Iteration 368/1000 | Loss: 0.00020033
Iteration 369/1000 | Loss: 0.00026293
Iteration 370/1000 | Loss: 0.00020087
Iteration 371/1000 | Loss: 0.00021683
Iteration 372/1000 | Loss: 0.00026684
Iteration 373/1000 | Loss: 0.00048879
Iteration 374/1000 | Loss: 0.00031417
Iteration 375/1000 | Loss: 0.00026062
Iteration 376/1000 | Loss: 0.00024858
Iteration 377/1000 | Loss: 0.00010653
Iteration 378/1000 | Loss: 0.00007476
Iteration 379/1000 | Loss: 0.00010912
Iteration 380/1000 | Loss: 0.00021168
Iteration 381/1000 | Loss: 0.00017625
Iteration 382/1000 | Loss: 0.00017972
Iteration 383/1000 | Loss: 0.00018361
Iteration 384/1000 | Loss: 0.00033630
Iteration 385/1000 | Loss: 0.00034049
Iteration 386/1000 | Loss: 0.00030825
Iteration 387/1000 | Loss: 0.00023392
Iteration 388/1000 | Loss: 0.00033078
Iteration 389/1000 | Loss: 0.00014175
Iteration 390/1000 | Loss: 0.00029539
Iteration 391/1000 | Loss: 0.00020664
Iteration 392/1000 | Loss: 0.00018374
Iteration 393/1000 | Loss: 0.00015296
Iteration 394/1000 | Loss: 0.00020018
Iteration 395/1000 | Loss: 0.00016289
Iteration 396/1000 | Loss: 0.00021901
Iteration 397/1000 | Loss: 0.00023924
Iteration 398/1000 | Loss: 0.00029641
Iteration 399/1000 | Loss: 0.00019695
Iteration 400/1000 | Loss: 0.00035484
Iteration 401/1000 | Loss: 0.00037602
Iteration 402/1000 | Loss: 0.00011958
Iteration 403/1000 | Loss: 0.00019717
Iteration 404/1000 | Loss: 0.00038783
Iteration 405/1000 | Loss: 0.00025251
Iteration 406/1000 | Loss: 0.00032835
Iteration 407/1000 | Loss: 0.00014831
Iteration 408/1000 | Loss: 0.00023198
Iteration 409/1000 | Loss: 0.00019805
Iteration 410/1000 | Loss: 0.00030674
Iteration 411/1000 | Loss: 0.00020368
Iteration 412/1000 | Loss: 0.00020277
Iteration 413/1000 | Loss: 0.00038152
Iteration 414/1000 | Loss: 0.00027809
Iteration 415/1000 | Loss: 0.00024159
Iteration 416/1000 | Loss: 0.00023710
Iteration 417/1000 | Loss: 0.00008939
Iteration 418/1000 | Loss: 0.00007737
Iteration 419/1000 | Loss: 0.00015215
Iteration 420/1000 | Loss: 0.00007314
Iteration 421/1000 | Loss: 0.00011718
Iteration 422/1000 | Loss: 0.00006301
Iteration 423/1000 | Loss: 0.00005866
Iteration 424/1000 | Loss: 0.00006163
Iteration 425/1000 | Loss: 0.00006073
Iteration 426/1000 | Loss: 0.00005653
Iteration 427/1000 | Loss: 0.00005469
Iteration 428/1000 | Loss: 0.00014915
Iteration 429/1000 | Loss: 0.00036519
Iteration 430/1000 | Loss: 0.00006870
Iteration 431/1000 | Loss: 0.00005741
Iteration 432/1000 | Loss: 0.00005583
Iteration 433/1000 | Loss: 0.00005442
Iteration 434/1000 | Loss: 0.00005369
Iteration 435/1000 | Loss: 0.00005250
Iteration 436/1000 | Loss: 0.00005218
Iteration 437/1000 | Loss: 0.00020385
Iteration 438/1000 | Loss: 0.00011923
Iteration 439/1000 | Loss: 0.00017628
Iteration 440/1000 | Loss: 0.00013511
Iteration 441/1000 | Loss: 0.00005201
Iteration 442/1000 | Loss: 0.00017803
Iteration 443/1000 | Loss: 0.00009626
Iteration 444/1000 | Loss: 0.00019032
Iteration 445/1000 | Loss: 0.00020185
Iteration 446/1000 | Loss: 0.00005219
Iteration 447/1000 | Loss: 0.00018605
Iteration 448/1000 | Loss: 0.00007485
Iteration 449/1000 | Loss: 0.00005654
Iteration 450/1000 | Loss: 0.00027934
Iteration 451/1000 | Loss: 0.00005455
Iteration 452/1000 | Loss: 0.00005278
Iteration 453/1000 | Loss: 0.00012093
Iteration 454/1000 | Loss: 0.00009841
Iteration 455/1000 | Loss: 0.00005133
Iteration 456/1000 | Loss: 0.00016743
Iteration 457/1000 | Loss: 0.00018233
Iteration 458/1000 | Loss: 0.00014220
Iteration 459/1000 | Loss: 0.00007582
Iteration 460/1000 | Loss: 0.00006480
Iteration 461/1000 | Loss: 0.00014925
Iteration 462/1000 | Loss: 0.00011296
Iteration 463/1000 | Loss: 0.00005525
Iteration 464/1000 | Loss: 0.00021418
Iteration 465/1000 | Loss: 0.00016493
Iteration 466/1000 | Loss: 0.00020804
Iteration 467/1000 | Loss: 0.00020695
Iteration 468/1000 | Loss: 0.00007124
Iteration 469/1000 | Loss: 0.00005723
Iteration 470/1000 | Loss: 0.00005407
Iteration 471/1000 | Loss: 0.00005263
Iteration 472/1000 | Loss: 0.00005159
Iteration 473/1000 | Loss: 0.00005072
Iteration 474/1000 | Loss: 0.00005013
Iteration 475/1000 | Loss: 0.00005022
Iteration 476/1000 | Loss: 0.00013139
Iteration 477/1000 | Loss: 0.00006740
Iteration 478/1000 | Loss: 0.00012121
Iteration 479/1000 | Loss: 0.00018636
Iteration 480/1000 | Loss: 0.00021048
Iteration 481/1000 | Loss: 0.00025215
Iteration 482/1000 | Loss: 0.00029079
Iteration 483/1000 | Loss: 0.00023068
Iteration 484/1000 | Loss: 0.00020608
Iteration 485/1000 | Loss: 0.00009878
Iteration 486/1000 | Loss: 0.00014051
Iteration 487/1000 | Loss: 0.00016466
Iteration 488/1000 | Loss: 0.00012178
Iteration 489/1000 | Loss: 0.00017100
Iteration 490/1000 | Loss: 0.00013239
Iteration 491/1000 | Loss: 0.00005902
Iteration 492/1000 | Loss: 0.00005253
Iteration 493/1000 | Loss: 0.00004996
Iteration 494/1000 | Loss: 0.00004896
Iteration 495/1000 | Loss: 0.00004729
Iteration 496/1000 | Loss: 0.00004657
Iteration 497/1000 | Loss: 0.00004606
Iteration 498/1000 | Loss: 0.00004590
Iteration 499/1000 | Loss: 0.00017272
Iteration 500/1000 | Loss: 0.00025524
Iteration 501/1000 | Loss: 0.00016613
Iteration 502/1000 | Loss: 0.00058898
Iteration 503/1000 | Loss: 0.00014290
Iteration 504/1000 | Loss: 0.00028434
Iteration 505/1000 | Loss: 0.00015633
Iteration 506/1000 | Loss: 0.00028081
Iteration 507/1000 | Loss: 0.00016474
Iteration 508/1000 | Loss: 0.00005649
Iteration 509/1000 | Loss: 0.00017389
Iteration 510/1000 | Loss: 0.00007081
Iteration 511/1000 | Loss: 0.00004716
Iteration 512/1000 | Loss: 0.00009617
Iteration 513/1000 | Loss: 0.00007978
Iteration 514/1000 | Loss: 0.00004613
Iteration 515/1000 | Loss: 0.00004588
Iteration 516/1000 | Loss: 0.00004593
Iteration 517/1000 | Loss: 0.00004557
Iteration 518/1000 | Loss: 0.00004548
Iteration 519/1000 | Loss: 0.00004523
Iteration 520/1000 | Loss: 0.00004513
Iteration 521/1000 | Loss: 0.00031613
Iteration 522/1000 | Loss: 0.00017845
Iteration 523/1000 | Loss: 0.00008914
Iteration 524/1000 | Loss: 0.00006415
Iteration 525/1000 | Loss: 0.00028005
Iteration 526/1000 | Loss: 0.00019220
Iteration 527/1000 | Loss: 0.00006102
Iteration 528/1000 | Loss: 0.00028328
Iteration 529/1000 | Loss: 0.00024823
Iteration 530/1000 | Loss: 0.00010711
Iteration 531/1000 | Loss: 0.00029353
Iteration 532/1000 | Loss: 0.00006706
Iteration 533/1000 | Loss: 0.00005146
Iteration 534/1000 | Loss: 0.00004880
Iteration 535/1000 | Loss: 0.00004756
Iteration 536/1000 | Loss: 0.00004603
Iteration 537/1000 | Loss: 0.00004510
Iteration 538/1000 | Loss: 0.00004408
Iteration 539/1000 | Loss: 0.00004399
Iteration 540/1000 | Loss: 0.00004339
Iteration 541/1000 | Loss: 0.00004324
Iteration 542/1000 | Loss: 0.00004318
Iteration 543/1000 | Loss: 0.00004294
Iteration 544/1000 | Loss: 0.00004293
Iteration 545/1000 | Loss: 0.00004293
Iteration 546/1000 | Loss: 0.00004293
Iteration 547/1000 | Loss: 0.00004293
Iteration 548/1000 | Loss: 0.00004293
Iteration 549/1000 | Loss: 0.00004289
Iteration 550/1000 | Loss: 0.00004289
Iteration 551/1000 | Loss: 0.00004297
Iteration 552/1000 | Loss: 0.00004284
Iteration 553/1000 | Loss: 0.00004283
Iteration 554/1000 | Loss: 0.00004283
Iteration 555/1000 | Loss: 0.00004282
Iteration 556/1000 | Loss: 0.00004282
Iteration 557/1000 | Loss: 0.00004281
Iteration 558/1000 | Loss: 0.00004280
Iteration 559/1000 | Loss: 0.00004280
Iteration 560/1000 | Loss: 0.00004279
Iteration 561/1000 | Loss: 0.00004279
Iteration 562/1000 | Loss: 0.00004279
Iteration 563/1000 | Loss: 0.00004278
Iteration 564/1000 | Loss: 0.00009309
Iteration 565/1000 | Loss: 0.00006031
Iteration 566/1000 | Loss: 0.00008269
Iteration 567/1000 | Loss: 0.00006208
Iteration 568/1000 | Loss: 0.00006816
Iteration 569/1000 | Loss: 0.00010824
Iteration 570/1000 | Loss: 0.00013151
Iteration 571/1000 | Loss: 0.00010873
Iteration 572/1000 | Loss: 0.00013743
Iteration 573/1000 | Loss: 0.00013571
Iteration 574/1000 | Loss: 0.00013518
Iteration 575/1000 | Loss: 0.00014653
Iteration 576/1000 | Loss: 0.00016854
Iteration 577/1000 | Loss: 0.00007488
Iteration 578/1000 | Loss: 0.00006523
Iteration 579/1000 | Loss: 0.00010518
Iteration 580/1000 | Loss: 0.00012598
Iteration 581/1000 | Loss: 0.00010981
Iteration 582/1000 | Loss: 0.00012419
Iteration 583/1000 | Loss: 0.00005270
Iteration 584/1000 | Loss: 0.00019421
Iteration 585/1000 | Loss: 0.00005272
Iteration 586/1000 | Loss: 0.00016483
Iteration 587/1000 | Loss: 0.00019078
Iteration 588/1000 | Loss: 0.00015278
Iteration 589/1000 | Loss: 0.00010106
Iteration 590/1000 | Loss: 0.00011847
Iteration 591/1000 | Loss: 0.00006798
Iteration 592/1000 | Loss: 0.00004383
Iteration 593/1000 | Loss: 0.00004248
Iteration 594/1000 | Loss: 0.00004162
Iteration 595/1000 | Loss: 0.00004100
Iteration 596/1000 | Loss: 0.00004080
Iteration 597/1000 | Loss: 0.00004065
Iteration 598/1000 | Loss: 0.00004056
Iteration 599/1000 | Loss: 0.00004074
Iteration 600/1000 | Loss: 0.00004061
Iteration 601/1000 | Loss: 0.00004049
Iteration 602/1000 | Loss: 0.00004047
Iteration 603/1000 | Loss: 0.00004047
Iteration 604/1000 | Loss: 0.00004047
Iteration 605/1000 | Loss: 0.00004046
Iteration 606/1000 | Loss: 0.00004046
Iteration 607/1000 | Loss: 0.00004046
Iteration 608/1000 | Loss: 0.00004045
Iteration 609/1000 | Loss: 0.00004045
Iteration 610/1000 | Loss: 0.00004045
Iteration 611/1000 | Loss: 0.00004045
Iteration 612/1000 | Loss: 0.00004045
Iteration 613/1000 | Loss: 0.00004045
Iteration 614/1000 | Loss: 0.00004045
Iteration 615/1000 | Loss: 0.00004045
Iteration 616/1000 | Loss: 0.00004045
Iteration 617/1000 | Loss: 0.00004044
Iteration 618/1000 | Loss: 0.00004044
Iteration 619/1000 | Loss: 0.00004044
Iteration 620/1000 | Loss: 0.00004044
Iteration 621/1000 | Loss: 0.00004044
Iteration 622/1000 | Loss: 0.00004043
Iteration 623/1000 | Loss: 0.00004043
Iteration 624/1000 | Loss: 0.00004042
Iteration 625/1000 | Loss: 0.00004042
Iteration 626/1000 | Loss: 0.00004042
Iteration 627/1000 | Loss: 0.00004052
Iteration 628/1000 | Loss: 0.00004048
Iteration 629/1000 | Loss: 0.00004048
Iteration 630/1000 | Loss: 0.00004036
Iteration 631/1000 | Loss: 0.00004035
Iteration 632/1000 | Loss: 0.00004035
Iteration 633/1000 | Loss: 0.00004034
Iteration 634/1000 | Loss: 0.00004034
Iteration 635/1000 | Loss: 0.00004034
Iteration 636/1000 | Loss: 0.00004034
Iteration 637/1000 | Loss: 0.00004034
Iteration 638/1000 | Loss: 0.00004034
Iteration 639/1000 | Loss: 0.00004034
Iteration 640/1000 | Loss: 0.00004034
Iteration 641/1000 | Loss: 0.00004034
Iteration 642/1000 | Loss: 0.00004034
Iteration 643/1000 | Loss: 0.00004034
Iteration 644/1000 | Loss: 0.00004034
Iteration 645/1000 | Loss: 0.00004034
Iteration 646/1000 | Loss: 0.00004034
Iteration 647/1000 | Loss: 0.00004034
Iteration 648/1000 | Loss: 0.00004034
Iteration 649/1000 | Loss: 0.00004034
Iteration 650/1000 | Loss: 0.00004034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 650. Stopping optimization.
Last 5 losses: [4.0339684346690774e-05, 4.0339684346690774e-05, 4.0339684346690774e-05, 4.0339684346690774e-05, 4.0339684346690774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0339684346690774e-05

Optimization complete. Final v2v error: 3.537790298461914 mm

Highest mean error: 12.503198623657227 mm for frame 206

Lowest mean error: 2.697660446166992 mm for frame 10

Saving results

Total time: 959.3995206356049
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_34_nl_6330/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_34_nl_6330/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358842
Iteration 2/25 | Loss: 0.00117816
Iteration 3/25 | Loss: 0.00109254
Iteration 4/25 | Loss: 0.00107589
Iteration 5/25 | Loss: 0.00106963
Iteration 6/25 | Loss: 0.00106792
Iteration 7/25 | Loss: 0.00106792
Iteration 8/25 | Loss: 0.00106792
Iteration 9/25 | Loss: 0.00106792
Iteration 10/25 | Loss: 0.00106792
Iteration 11/25 | Loss: 0.00106792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010679219849407673, 0.0010679219849407673, 0.0010679219849407673, 0.0010679219849407673, 0.0010679219849407673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010679219849407673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20043957
Iteration 2/25 | Loss: 0.00204020
Iteration 3/25 | Loss: 0.00204020
Iteration 4/25 | Loss: 0.00204020
Iteration 5/25 | Loss: 0.00204020
Iteration 6/25 | Loss: 0.00204020
Iteration 7/25 | Loss: 0.00204020
Iteration 8/25 | Loss: 0.00204020
Iteration 9/25 | Loss: 0.00204020
Iteration 10/25 | Loss: 0.00204020
Iteration 11/25 | Loss: 0.00204019
Iteration 12/25 | Loss: 0.00204019
Iteration 13/25 | Loss: 0.00204019
Iteration 14/25 | Loss: 0.00204019
Iteration 15/25 | Loss: 0.00204019
Iteration 16/25 | Loss: 0.00204019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020401948131620884, 0.0020401948131620884, 0.0020401948131620884, 0.0020401948131620884, 0.0020401948131620884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020401948131620884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00204019
Iteration 2/1000 | Loss: 0.00005008
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00001926
Iteration 5/1000 | Loss: 0.00001757
Iteration 6/1000 | Loss: 0.00001660
Iteration 7/1000 | Loss: 0.00001603
Iteration 8/1000 | Loss: 0.00001572
Iteration 9/1000 | Loss: 0.00001542
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001486
Iteration 14/1000 | Loss: 0.00001476
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001469
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001464
Iteration 33/1000 | Loss: 0.00001463
Iteration 34/1000 | Loss: 0.00001463
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001462
Iteration 37/1000 | Loss: 0.00001462
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001462
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001461
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001458
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001457
Iteration 67/1000 | Loss: 0.00001457
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001450
Iteration 108/1000 | Loss: 0.00001450
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001449
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001448
Iteration 119/1000 | Loss: 0.00001448
Iteration 120/1000 | Loss: 0.00001448
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001448
Iteration 123/1000 | Loss: 0.00001448
Iteration 124/1000 | Loss: 0.00001448
Iteration 125/1000 | Loss: 0.00001448
Iteration 126/1000 | Loss: 0.00001448
Iteration 127/1000 | Loss: 0.00001448
Iteration 128/1000 | Loss: 0.00001448
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.447566319257021e-05, 1.447566319257021e-05, 1.447566319257021e-05, 1.447566319257021e-05, 1.447566319257021e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.447566319257021e-05

Optimization complete. Final v2v error: 3.174541473388672 mm

Highest mean error: 3.6461400985717773 mm for frame 144

Lowest mean error: 2.6271727085113525 mm for frame 179

Saving results

Total time: 39.84858441352844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646758
Iteration 2/25 | Loss: 0.00243909
Iteration 3/25 | Loss: 0.00227748
Iteration 4/25 | Loss: 0.00226442
Iteration 5/25 | Loss: 0.00226038
Iteration 6/25 | Loss: 0.00225970
Iteration 7/25 | Loss: 0.00225970
Iteration 8/25 | Loss: 0.00225970
Iteration 9/25 | Loss: 0.00225970
Iteration 10/25 | Loss: 0.00225970
Iteration 11/25 | Loss: 0.00225970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0022596963681280613, 0.0022596963681280613, 0.0022596963681280613, 0.0022596963681280613, 0.0022596963681280613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022596963681280613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84522134
Iteration 2/25 | Loss: 0.00259771
Iteration 3/25 | Loss: 0.00259771
Iteration 4/25 | Loss: 0.00259771
Iteration 5/25 | Loss: 0.00259771
Iteration 6/25 | Loss: 0.00259771
Iteration 7/25 | Loss: 0.00259771
Iteration 8/25 | Loss: 0.00259771
Iteration 9/25 | Loss: 0.00259771
Iteration 10/25 | Loss: 0.00259771
Iteration 11/25 | Loss: 0.00259771
Iteration 12/25 | Loss: 0.00259771
Iteration 13/25 | Loss: 0.00259771
Iteration 14/25 | Loss: 0.00259771
Iteration 15/25 | Loss: 0.00259771
Iteration 16/25 | Loss: 0.00259771
Iteration 17/25 | Loss: 0.00259771
Iteration 18/25 | Loss: 0.00259771
Iteration 19/25 | Loss: 0.00259771
Iteration 20/25 | Loss: 0.00259771
Iteration 21/25 | Loss: 0.00259771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0025977070908993483, 0.0025977070908993483, 0.0025977070908993483, 0.0025977070908993483, 0.0025977070908993483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025977070908993483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259771
Iteration 2/1000 | Loss: 0.00010474
Iteration 3/1000 | Loss: 0.00007050
Iteration 4/1000 | Loss: 0.00006079
Iteration 5/1000 | Loss: 0.00005644
Iteration 6/1000 | Loss: 0.00005414
Iteration 7/1000 | Loss: 0.00005290
Iteration 8/1000 | Loss: 0.00005209
Iteration 9/1000 | Loss: 0.00005136
Iteration 10/1000 | Loss: 0.00005090
Iteration 11/1000 | Loss: 0.00005047
Iteration 12/1000 | Loss: 0.00005018
Iteration 13/1000 | Loss: 0.00004995
Iteration 14/1000 | Loss: 0.00004974
Iteration 15/1000 | Loss: 0.00004963
Iteration 16/1000 | Loss: 0.00004962
Iteration 17/1000 | Loss: 0.00004962
Iteration 18/1000 | Loss: 0.00004960
Iteration 19/1000 | Loss: 0.00004959
Iteration 20/1000 | Loss: 0.00004959
Iteration 21/1000 | Loss: 0.00004958
Iteration 22/1000 | Loss: 0.00004958
Iteration 23/1000 | Loss: 0.00004956
Iteration 24/1000 | Loss: 0.00004955
Iteration 25/1000 | Loss: 0.00004955
Iteration 26/1000 | Loss: 0.00004954
Iteration 27/1000 | Loss: 0.00004954
Iteration 28/1000 | Loss: 0.00004953
Iteration 29/1000 | Loss: 0.00004953
Iteration 30/1000 | Loss: 0.00004953
Iteration 31/1000 | Loss: 0.00004952
Iteration 32/1000 | Loss: 0.00004952
Iteration 33/1000 | Loss: 0.00004951
Iteration 34/1000 | Loss: 0.00004951
Iteration 35/1000 | Loss: 0.00004950
Iteration 36/1000 | Loss: 0.00004950
Iteration 37/1000 | Loss: 0.00004950
Iteration 38/1000 | Loss: 0.00004949
Iteration 39/1000 | Loss: 0.00004949
Iteration 40/1000 | Loss: 0.00004949
Iteration 41/1000 | Loss: 0.00004949
Iteration 42/1000 | Loss: 0.00004949
Iteration 43/1000 | Loss: 0.00004949
Iteration 44/1000 | Loss: 0.00004948
Iteration 45/1000 | Loss: 0.00004948
Iteration 46/1000 | Loss: 0.00004947
Iteration 47/1000 | Loss: 0.00004947
Iteration 48/1000 | Loss: 0.00004947
Iteration 49/1000 | Loss: 0.00004947
Iteration 50/1000 | Loss: 0.00004947
Iteration 51/1000 | Loss: 0.00004947
Iteration 52/1000 | Loss: 0.00004947
Iteration 53/1000 | Loss: 0.00004946
Iteration 54/1000 | Loss: 0.00004946
Iteration 55/1000 | Loss: 0.00004946
Iteration 56/1000 | Loss: 0.00004946
Iteration 57/1000 | Loss: 0.00004946
Iteration 58/1000 | Loss: 0.00004945
Iteration 59/1000 | Loss: 0.00004945
Iteration 60/1000 | Loss: 0.00004945
Iteration 61/1000 | Loss: 0.00004944
Iteration 62/1000 | Loss: 0.00004944
Iteration 63/1000 | Loss: 0.00004944
Iteration 64/1000 | Loss: 0.00004944
Iteration 65/1000 | Loss: 0.00004944
Iteration 66/1000 | Loss: 0.00004944
Iteration 67/1000 | Loss: 0.00004944
Iteration 68/1000 | Loss: 0.00004944
Iteration 69/1000 | Loss: 0.00004944
Iteration 70/1000 | Loss: 0.00004944
Iteration 71/1000 | Loss: 0.00004944
Iteration 72/1000 | Loss: 0.00004943
Iteration 73/1000 | Loss: 0.00004943
Iteration 74/1000 | Loss: 0.00004942
Iteration 75/1000 | Loss: 0.00004942
Iteration 76/1000 | Loss: 0.00004941
Iteration 77/1000 | Loss: 0.00004941
Iteration 78/1000 | Loss: 0.00004941
Iteration 79/1000 | Loss: 0.00004941
Iteration 80/1000 | Loss: 0.00004941
Iteration 81/1000 | Loss: 0.00004940
Iteration 82/1000 | Loss: 0.00004940
Iteration 83/1000 | Loss: 0.00004940
Iteration 84/1000 | Loss: 0.00004940
Iteration 85/1000 | Loss: 0.00004940
Iteration 86/1000 | Loss: 0.00004940
Iteration 87/1000 | Loss: 0.00004940
Iteration 88/1000 | Loss: 0.00004940
Iteration 89/1000 | Loss: 0.00004939
Iteration 90/1000 | Loss: 0.00004939
Iteration 91/1000 | Loss: 0.00004939
Iteration 92/1000 | Loss: 0.00004938
Iteration 93/1000 | Loss: 0.00004938
Iteration 94/1000 | Loss: 0.00004938
Iteration 95/1000 | Loss: 0.00004938
Iteration 96/1000 | Loss: 0.00004938
Iteration 97/1000 | Loss: 0.00004938
Iteration 98/1000 | Loss: 0.00004938
Iteration 99/1000 | Loss: 0.00004938
Iteration 100/1000 | Loss: 0.00004938
Iteration 101/1000 | Loss: 0.00004938
Iteration 102/1000 | Loss: 0.00004937
Iteration 103/1000 | Loss: 0.00004937
Iteration 104/1000 | Loss: 0.00004937
Iteration 105/1000 | Loss: 0.00004937
Iteration 106/1000 | Loss: 0.00004937
Iteration 107/1000 | Loss: 0.00004937
Iteration 108/1000 | Loss: 0.00004937
Iteration 109/1000 | Loss: 0.00004937
Iteration 110/1000 | Loss: 0.00004937
Iteration 111/1000 | Loss: 0.00004937
Iteration 112/1000 | Loss: 0.00004937
Iteration 113/1000 | Loss: 0.00004937
Iteration 114/1000 | Loss: 0.00004937
Iteration 115/1000 | Loss: 0.00004937
Iteration 116/1000 | Loss: 0.00004936
Iteration 117/1000 | Loss: 0.00004936
Iteration 118/1000 | Loss: 0.00004936
Iteration 119/1000 | Loss: 0.00004936
Iteration 120/1000 | Loss: 0.00004936
Iteration 121/1000 | Loss: 0.00004936
Iteration 122/1000 | Loss: 0.00004936
Iteration 123/1000 | Loss: 0.00004936
Iteration 124/1000 | Loss: 0.00004936
Iteration 125/1000 | Loss: 0.00004936
Iteration 126/1000 | Loss: 0.00004936
Iteration 127/1000 | Loss: 0.00004936
Iteration 128/1000 | Loss: 0.00004936
Iteration 129/1000 | Loss: 0.00004936
Iteration 130/1000 | Loss: 0.00004936
Iteration 131/1000 | Loss: 0.00004936
Iteration 132/1000 | Loss: 0.00004936
Iteration 133/1000 | Loss: 0.00004936
Iteration 134/1000 | Loss: 0.00004936
Iteration 135/1000 | Loss: 0.00004936
Iteration 136/1000 | Loss: 0.00004936
Iteration 137/1000 | Loss: 0.00004936
Iteration 138/1000 | Loss: 0.00004936
Iteration 139/1000 | Loss: 0.00004936
Iteration 140/1000 | Loss: 0.00004936
Iteration 141/1000 | Loss: 0.00004936
Iteration 142/1000 | Loss: 0.00004936
Iteration 143/1000 | Loss: 0.00004936
Iteration 144/1000 | Loss: 0.00004936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [4.936412733513862e-05, 4.936412733513862e-05, 4.936412733513862e-05, 4.936412733513862e-05, 4.936412733513862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.936412733513862e-05

Optimization complete. Final v2v error: 6.278823375701904 mm

Highest mean error: 6.654221534729004 mm for frame 49

Lowest mean error: 5.940688133239746 mm for frame 175

Saving results

Total time: 42.107399225234985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998181
Iteration 2/25 | Loss: 0.00240004
Iteration 3/25 | Loss: 0.00234185
Iteration 4/25 | Loss: 0.00232707
Iteration 5/25 | Loss: 0.00232273
Iteration 6/25 | Loss: 0.00232258
Iteration 7/25 | Loss: 0.00232258
Iteration 8/25 | Loss: 0.00232258
Iteration 9/25 | Loss: 0.00232258
Iteration 10/25 | Loss: 0.00232258
Iteration 11/25 | Loss: 0.00232258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002322582993656397, 0.002322582993656397, 0.002322582993656397, 0.002322582993656397, 0.002322582993656397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002322582993656397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39710808
Iteration 2/25 | Loss: 0.00277359
Iteration 3/25 | Loss: 0.00277358
Iteration 4/25 | Loss: 0.00277358
Iteration 5/25 | Loss: 0.00277358
Iteration 6/25 | Loss: 0.00277358
Iteration 7/25 | Loss: 0.00277358
Iteration 8/25 | Loss: 0.00277358
Iteration 9/25 | Loss: 0.00277358
Iteration 10/25 | Loss: 0.00277358
Iteration 11/25 | Loss: 0.00277358
Iteration 12/25 | Loss: 0.00277358
Iteration 13/25 | Loss: 0.00277358
Iteration 14/25 | Loss: 0.00277358
Iteration 15/25 | Loss: 0.00277358
Iteration 16/25 | Loss: 0.00277358
Iteration 17/25 | Loss: 0.00277358
Iteration 18/25 | Loss: 0.00277358
Iteration 19/25 | Loss: 0.00277358
Iteration 20/25 | Loss: 0.00277358
Iteration 21/25 | Loss: 0.00277358
Iteration 22/25 | Loss: 0.00277358
Iteration 23/25 | Loss: 0.00277358
Iteration 24/25 | Loss: 0.00277358
Iteration 25/25 | Loss: 0.00277358
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.002773578278720379, 0.002773578278720379, 0.002773578278720379, 0.002773578278720379, 0.002773578278720379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002773578278720379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277358
Iteration 2/1000 | Loss: 0.00013192
Iteration 3/1000 | Loss: 0.00009626
Iteration 4/1000 | Loss: 0.00007416
Iteration 5/1000 | Loss: 0.00006918
Iteration 6/1000 | Loss: 0.00006632
Iteration 7/1000 | Loss: 0.00006419
Iteration 8/1000 | Loss: 0.00006304
Iteration 9/1000 | Loss: 0.00006196
Iteration 10/1000 | Loss: 0.00006128
Iteration 11/1000 | Loss: 0.00006090
Iteration 12/1000 | Loss: 0.00006049
Iteration 13/1000 | Loss: 0.00006020
Iteration 14/1000 | Loss: 0.00006020
Iteration 15/1000 | Loss: 0.00005999
Iteration 16/1000 | Loss: 0.00005983
Iteration 17/1000 | Loss: 0.00005975
Iteration 18/1000 | Loss: 0.00005974
Iteration 19/1000 | Loss: 0.00005973
Iteration 20/1000 | Loss: 0.00005972
Iteration 21/1000 | Loss: 0.00005963
Iteration 22/1000 | Loss: 0.00005962
Iteration 23/1000 | Loss: 0.00005959
Iteration 24/1000 | Loss: 0.00005959
Iteration 25/1000 | Loss: 0.00005959
Iteration 26/1000 | Loss: 0.00005959
Iteration 27/1000 | Loss: 0.00005959
Iteration 28/1000 | Loss: 0.00005959
Iteration 29/1000 | Loss: 0.00005959
Iteration 30/1000 | Loss: 0.00005959
Iteration 31/1000 | Loss: 0.00005959
Iteration 32/1000 | Loss: 0.00005959
Iteration 33/1000 | Loss: 0.00005959
Iteration 34/1000 | Loss: 0.00005958
Iteration 35/1000 | Loss: 0.00005958
Iteration 36/1000 | Loss: 0.00005958
Iteration 37/1000 | Loss: 0.00005958
Iteration 38/1000 | Loss: 0.00005957
Iteration 39/1000 | Loss: 0.00005957
Iteration 40/1000 | Loss: 0.00005957
Iteration 41/1000 | Loss: 0.00005957
Iteration 42/1000 | Loss: 0.00005957
Iteration 43/1000 | Loss: 0.00005957
Iteration 44/1000 | Loss: 0.00005957
Iteration 45/1000 | Loss: 0.00005957
Iteration 46/1000 | Loss: 0.00005957
Iteration 47/1000 | Loss: 0.00005957
Iteration 48/1000 | Loss: 0.00005957
Iteration 49/1000 | Loss: 0.00005956
Iteration 50/1000 | Loss: 0.00005956
Iteration 51/1000 | Loss: 0.00005956
Iteration 52/1000 | Loss: 0.00005956
Iteration 53/1000 | Loss: 0.00005956
Iteration 54/1000 | Loss: 0.00005956
Iteration 55/1000 | Loss: 0.00005956
Iteration 56/1000 | Loss: 0.00005956
Iteration 57/1000 | Loss: 0.00005956
Iteration 58/1000 | Loss: 0.00005956
Iteration 59/1000 | Loss: 0.00005955
Iteration 60/1000 | Loss: 0.00005955
Iteration 61/1000 | Loss: 0.00005955
Iteration 62/1000 | Loss: 0.00005955
Iteration 63/1000 | Loss: 0.00005955
Iteration 64/1000 | Loss: 0.00005955
Iteration 65/1000 | Loss: 0.00005954
Iteration 66/1000 | Loss: 0.00005954
Iteration 67/1000 | Loss: 0.00005954
Iteration 68/1000 | Loss: 0.00005954
Iteration 69/1000 | Loss: 0.00005954
Iteration 70/1000 | Loss: 0.00005954
Iteration 71/1000 | Loss: 0.00005954
Iteration 72/1000 | Loss: 0.00005954
Iteration 73/1000 | Loss: 0.00005953
Iteration 74/1000 | Loss: 0.00005953
Iteration 75/1000 | Loss: 0.00005953
Iteration 76/1000 | Loss: 0.00005953
Iteration 77/1000 | Loss: 0.00005953
Iteration 78/1000 | Loss: 0.00005953
Iteration 79/1000 | Loss: 0.00005953
Iteration 80/1000 | Loss: 0.00005953
Iteration 81/1000 | Loss: 0.00005953
Iteration 82/1000 | Loss: 0.00005953
Iteration 83/1000 | Loss: 0.00005953
Iteration 84/1000 | Loss: 0.00005953
Iteration 85/1000 | Loss: 0.00005953
Iteration 86/1000 | Loss: 0.00005953
Iteration 87/1000 | Loss: 0.00005953
Iteration 88/1000 | Loss: 0.00005953
Iteration 89/1000 | Loss: 0.00005952
Iteration 90/1000 | Loss: 0.00005952
Iteration 91/1000 | Loss: 0.00005952
Iteration 92/1000 | Loss: 0.00005952
Iteration 93/1000 | Loss: 0.00005952
Iteration 94/1000 | Loss: 0.00005952
Iteration 95/1000 | Loss: 0.00005952
Iteration 96/1000 | Loss: 0.00005952
Iteration 97/1000 | Loss: 0.00005952
Iteration 98/1000 | Loss: 0.00005952
Iteration 99/1000 | Loss: 0.00005952
Iteration 100/1000 | Loss: 0.00005952
Iteration 101/1000 | Loss: 0.00005952
Iteration 102/1000 | Loss: 0.00005952
Iteration 103/1000 | Loss: 0.00005952
Iteration 104/1000 | Loss: 0.00005952
Iteration 105/1000 | Loss: 0.00005951
Iteration 106/1000 | Loss: 0.00005951
Iteration 107/1000 | Loss: 0.00005951
Iteration 108/1000 | Loss: 0.00005951
Iteration 109/1000 | Loss: 0.00005951
Iteration 110/1000 | Loss: 0.00005951
Iteration 111/1000 | Loss: 0.00005951
Iteration 112/1000 | Loss: 0.00005951
Iteration 113/1000 | Loss: 0.00005951
Iteration 114/1000 | Loss: 0.00005951
Iteration 115/1000 | Loss: 0.00005951
Iteration 116/1000 | Loss: 0.00005951
Iteration 117/1000 | Loss: 0.00005951
Iteration 118/1000 | Loss: 0.00005951
Iteration 119/1000 | Loss: 0.00005951
Iteration 120/1000 | Loss: 0.00005951
Iteration 121/1000 | Loss: 0.00005951
Iteration 122/1000 | Loss: 0.00005951
Iteration 123/1000 | Loss: 0.00005951
Iteration 124/1000 | Loss: 0.00005951
Iteration 125/1000 | Loss: 0.00005951
Iteration 126/1000 | Loss: 0.00005951
Iteration 127/1000 | Loss: 0.00005951
Iteration 128/1000 | Loss: 0.00005951
Iteration 129/1000 | Loss: 0.00005951
Iteration 130/1000 | Loss: 0.00005951
Iteration 131/1000 | Loss: 0.00005951
Iteration 132/1000 | Loss: 0.00005951
Iteration 133/1000 | Loss: 0.00005951
Iteration 134/1000 | Loss: 0.00005951
Iteration 135/1000 | Loss: 0.00005951
Iteration 136/1000 | Loss: 0.00005951
Iteration 137/1000 | Loss: 0.00005951
Iteration 138/1000 | Loss: 0.00005951
Iteration 139/1000 | Loss: 0.00005951
Iteration 140/1000 | Loss: 0.00005951
Iteration 141/1000 | Loss: 0.00005951
Iteration 142/1000 | Loss: 0.00005951
Iteration 143/1000 | Loss: 0.00005951
Iteration 144/1000 | Loss: 0.00005951
Iteration 145/1000 | Loss: 0.00005951
Iteration 146/1000 | Loss: 0.00005951
Iteration 147/1000 | Loss: 0.00005951
Iteration 148/1000 | Loss: 0.00005951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [5.9510515711735934e-05, 5.9510515711735934e-05, 5.9510515711735934e-05, 5.9510515711735934e-05, 5.9510515711735934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9510515711735934e-05

Optimization complete. Final v2v error: 6.902500629425049 mm

Highest mean error: 7.07573938369751 mm for frame 26

Lowest mean error: 6.701099872589111 mm for frame 144

Saving results

Total time: 36.02030396461487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_58_us_2277/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_58_us_2277/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01162237
Iteration 2/25 | Loss: 0.00579971
Iteration 3/25 | Loss: 0.00415719
Iteration 4/25 | Loss: 0.00382437
Iteration 5/25 | Loss: 0.00335319
Iteration 6/25 | Loss: 0.00325346
Iteration 7/25 | Loss: 0.00314566
Iteration 8/25 | Loss: 0.00309246
Iteration 9/25 | Loss: 0.00307762
Iteration 10/25 | Loss: 0.00307011
Iteration 11/25 | Loss: 0.00306768
Iteration 12/25 | Loss: 0.00305978
Iteration 13/25 | Loss: 0.00303772
Iteration 14/25 | Loss: 0.00303043
Iteration 15/25 | Loss: 0.00303242
Iteration 16/25 | Loss: 0.00303252
Iteration 17/25 | Loss: 0.00304471
Iteration 18/25 | Loss: 0.00302676
Iteration 19/25 | Loss: 0.00302505
Iteration 20/25 | Loss: 0.00301908
Iteration 21/25 | Loss: 0.00300796
Iteration 22/25 | Loss: 0.00300472
Iteration 23/25 | Loss: 0.00300430
Iteration 24/25 | Loss: 0.00300420
Iteration 25/25 | Loss: 0.00300419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38531888
Iteration 2/25 | Loss: 0.01262833
Iteration 3/25 | Loss: 0.01262833
Iteration 4/25 | Loss: 0.01262833
Iteration 5/25 | Loss: 0.01262833
Iteration 6/25 | Loss: 0.01262833
Iteration 7/25 | Loss: 0.01262833
Iteration 8/25 | Loss: 0.01262833
Iteration 9/25 | Loss: 0.01262833
Iteration 10/25 | Loss: 0.01262833
Iteration 11/25 | Loss: 0.01262833
Iteration 12/25 | Loss: 0.01262833
Iteration 13/25 | Loss: 0.01262833
Iteration 14/25 | Loss: 0.01262833
Iteration 15/25 | Loss: 0.01262833
Iteration 16/25 | Loss: 0.01262833
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.012628328055143356, 0.012628328055143356, 0.012628328055143356, 0.012628328055143356, 0.012628328055143356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.012628328055143356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01262833
Iteration 2/1000 | Loss: 0.00206715
Iteration 3/1000 | Loss: 0.00143608
Iteration 4/1000 | Loss: 0.00107229
Iteration 5/1000 | Loss: 0.00087718
Iteration 6/1000 | Loss: 0.00078805
Iteration 7/1000 | Loss: 0.00071149
Iteration 8/1000 | Loss: 0.00065706
Iteration 9/1000 | Loss: 0.00105271
Iteration 10/1000 | Loss: 0.00061250
Iteration 11/1000 | Loss: 0.00079642
Iteration 12/1000 | Loss: 0.00247261
Iteration 13/1000 | Loss: 0.00185649
Iteration 14/1000 | Loss: 0.00416511
Iteration 15/1000 | Loss: 0.00812988
Iteration 16/1000 | Loss: 0.00541863
Iteration 17/1000 | Loss: 0.00169095
Iteration 18/1000 | Loss: 0.00616439
Iteration 19/1000 | Loss: 0.00544085
Iteration 20/1000 | Loss: 0.00380727
Iteration 21/1000 | Loss: 0.00353961
Iteration 22/1000 | Loss: 0.00295304
Iteration 23/1000 | Loss: 0.00637331
Iteration 24/1000 | Loss: 0.00428099
Iteration 25/1000 | Loss: 0.00493852
Iteration 26/1000 | Loss: 0.00409149
Iteration 27/1000 | Loss: 0.00361924
Iteration 28/1000 | Loss: 0.00217531
Iteration 29/1000 | Loss: 0.00241346
Iteration 30/1000 | Loss: 0.00182062
Iteration 31/1000 | Loss: 0.00237277
Iteration 32/1000 | Loss: 0.00190568
Iteration 33/1000 | Loss: 0.00232795
Iteration 34/1000 | Loss: 0.00230420
Iteration 35/1000 | Loss: 0.00332364
Iteration 36/1000 | Loss: 0.00217208
Iteration 37/1000 | Loss: 0.00260542
Iteration 38/1000 | Loss: 0.00245836
Iteration 39/1000 | Loss: 0.00225292
Iteration 40/1000 | Loss: 0.00254881
Iteration 41/1000 | Loss: 0.00211682
Iteration 42/1000 | Loss: 0.00246787
Iteration 43/1000 | Loss: 0.00264472
Iteration 44/1000 | Loss: 0.00269985
Iteration 45/1000 | Loss: 0.00244105
Iteration 46/1000 | Loss: 0.00301083
Iteration 47/1000 | Loss: 0.00348529
Iteration 48/1000 | Loss: 0.00302554
Iteration 49/1000 | Loss: 0.00373910
Iteration 50/1000 | Loss: 0.00285602
Iteration 51/1000 | Loss: 0.00300626
Iteration 52/1000 | Loss: 0.00299652
Iteration 53/1000 | Loss: 0.00281953
Iteration 54/1000 | Loss: 0.00302506
Iteration 55/1000 | Loss: 0.00291885
Iteration 56/1000 | Loss: 0.00275259
Iteration 57/1000 | Loss: 0.00287533
Iteration 58/1000 | Loss: 0.00271155
Iteration 59/1000 | Loss: 0.00203059
Iteration 60/1000 | Loss: 0.00498758
Iteration 61/1000 | Loss: 0.00375704
Iteration 62/1000 | Loss: 0.00266102
Iteration 63/1000 | Loss: 0.00329035
Iteration 64/1000 | Loss: 0.00362442
Iteration 65/1000 | Loss: 0.00313281
Iteration 66/1000 | Loss: 0.00377626
Iteration 67/1000 | Loss: 0.00277169
Iteration 68/1000 | Loss: 0.00308318
Iteration 69/1000 | Loss: 0.00367439
Iteration 70/1000 | Loss: 0.00291975
Iteration 71/1000 | Loss: 0.00366269
Iteration 72/1000 | Loss: 0.00326901
Iteration 73/1000 | Loss: 0.00328247
Iteration 74/1000 | Loss: 0.00293201
Iteration 75/1000 | Loss: 0.00517328
Iteration 76/1000 | Loss: 0.00332831
Iteration 77/1000 | Loss: 0.00407253
Iteration 78/1000 | Loss: 0.00318197
Iteration 79/1000 | Loss: 0.00298235
Iteration 80/1000 | Loss: 0.00213067
Iteration 81/1000 | Loss: 0.00467838
Iteration 82/1000 | Loss: 0.00314437
Iteration 83/1000 | Loss: 0.00405871
Iteration 84/1000 | Loss: 0.00542127
Iteration 85/1000 | Loss: 0.00540869
Iteration 86/1000 | Loss: 0.00562097
Iteration 87/1000 | Loss: 0.00404021
Iteration 88/1000 | Loss: 0.00482179
Iteration 89/1000 | Loss: 0.00458961
Iteration 90/1000 | Loss: 0.00444689
Iteration 91/1000 | Loss: 0.00422139
Iteration 92/1000 | Loss: 0.00359504
Iteration 93/1000 | Loss: 0.00384527
Iteration 94/1000 | Loss: 0.00459560
Iteration 95/1000 | Loss: 0.00309748
Iteration 96/1000 | Loss: 0.00369491
Iteration 97/1000 | Loss: 0.00269252
Iteration 98/1000 | Loss: 0.00212702
Iteration 99/1000 | Loss: 0.00223446
Iteration 100/1000 | Loss: 0.00274735
Iteration 101/1000 | Loss: 0.00200503
Iteration 102/1000 | Loss: 0.00169742
Iteration 103/1000 | Loss: 0.00190435
Iteration 104/1000 | Loss: 0.00293669
Iteration 105/1000 | Loss: 0.00192514
Iteration 106/1000 | Loss: 0.00184701
Iteration 107/1000 | Loss: 0.00196588
Iteration 108/1000 | Loss: 0.00184941
Iteration 109/1000 | Loss: 0.00205684
Iteration 110/1000 | Loss: 0.00194405
Iteration 111/1000 | Loss: 0.00215555
Iteration 112/1000 | Loss: 0.00210411
Iteration 113/1000 | Loss: 0.00222436
Iteration 114/1000 | Loss: 0.00156060
Iteration 115/1000 | Loss: 0.00121675
Iteration 116/1000 | Loss: 0.00111089
Iteration 117/1000 | Loss: 0.00181444
Iteration 118/1000 | Loss: 0.00138588
Iteration 119/1000 | Loss: 0.00118995
Iteration 120/1000 | Loss: 0.00135852
Iteration 121/1000 | Loss: 0.00109930
Iteration 122/1000 | Loss: 0.00173290
Iteration 123/1000 | Loss: 0.00110978
Iteration 124/1000 | Loss: 0.00089944
Iteration 125/1000 | Loss: 0.00101675
Iteration 126/1000 | Loss: 0.00096915
Iteration 127/1000 | Loss: 0.00068273
Iteration 128/1000 | Loss: 0.00091398
Iteration 129/1000 | Loss: 0.00092114
Iteration 130/1000 | Loss: 0.00122522
Iteration 131/1000 | Loss: 0.00099671
Iteration 132/1000 | Loss: 0.00083501
Iteration 133/1000 | Loss: 0.00127281
Iteration 134/1000 | Loss: 0.00100264
Iteration 135/1000 | Loss: 0.00079121
Iteration 136/1000 | Loss: 0.00106778
Iteration 137/1000 | Loss: 0.00087731
Iteration 138/1000 | Loss: 0.00069361
Iteration 139/1000 | Loss: 0.00116943
Iteration 140/1000 | Loss: 0.00080894
Iteration 141/1000 | Loss: 0.00119059
Iteration 142/1000 | Loss: 0.00163247
Iteration 143/1000 | Loss: 0.00116541
Iteration 144/1000 | Loss: 0.00195689
Iteration 145/1000 | Loss: 0.00088691
Iteration 146/1000 | Loss: 0.00079398
Iteration 147/1000 | Loss: 0.00082748
Iteration 148/1000 | Loss: 0.00132313
Iteration 149/1000 | Loss: 0.00065252
Iteration 150/1000 | Loss: 0.00133035
Iteration 151/1000 | Loss: 0.00075802
Iteration 152/1000 | Loss: 0.00091975
Iteration 153/1000 | Loss: 0.00093773
Iteration 154/1000 | Loss: 0.00071420
Iteration 155/1000 | Loss: 0.00075147
Iteration 156/1000 | Loss: 0.00071154
Iteration 157/1000 | Loss: 0.00087101
Iteration 158/1000 | Loss: 0.00075354
Iteration 159/1000 | Loss: 0.00121256
Iteration 160/1000 | Loss: 0.00058241
Iteration 161/1000 | Loss: 0.00069560
Iteration 162/1000 | Loss: 0.00077137
Iteration 163/1000 | Loss: 0.00077985
Iteration 164/1000 | Loss: 0.00079733
Iteration 165/1000 | Loss: 0.00079647
Iteration 166/1000 | Loss: 0.00097899
Iteration 167/1000 | Loss: 0.00096693
Iteration 168/1000 | Loss: 0.00066879
Iteration 169/1000 | Loss: 0.00062685
Iteration 170/1000 | Loss: 0.00052418
Iteration 171/1000 | Loss: 0.00066529
Iteration 172/1000 | Loss: 0.00067044
Iteration 173/1000 | Loss: 0.00060617
Iteration 174/1000 | Loss: 0.00078529
Iteration 175/1000 | Loss: 0.00095945
Iteration 176/1000 | Loss: 0.00094914
Iteration 177/1000 | Loss: 0.00056902
Iteration 178/1000 | Loss: 0.00074170
Iteration 179/1000 | Loss: 0.00111039
Iteration 180/1000 | Loss: 0.00095478
Iteration 181/1000 | Loss: 0.00061497
Iteration 182/1000 | Loss: 0.00098707
Iteration 183/1000 | Loss: 0.00099013
Iteration 184/1000 | Loss: 0.00093981
Iteration 185/1000 | Loss: 0.00077037
Iteration 186/1000 | Loss: 0.00068117
Iteration 187/1000 | Loss: 0.00085266
Iteration 188/1000 | Loss: 0.00143074
Iteration 189/1000 | Loss: 0.00099961
Iteration 190/1000 | Loss: 0.00069853
Iteration 191/1000 | Loss: 0.00081822
Iteration 192/1000 | Loss: 0.00083063
Iteration 193/1000 | Loss: 0.00079661
Iteration 194/1000 | Loss: 0.00073676
Iteration 195/1000 | Loss: 0.00068142
Iteration 196/1000 | Loss: 0.00076884
Iteration 197/1000 | Loss: 0.00081792
Iteration 198/1000 | Loss: 0.00062256
Iteration 199/1000 | Loss: 0.00066432
Iteration 200/1000 | Loss: 0.00069245
Iteration 201/1000 | Loss: 0.00077726
Iteration 202/1000 | Loss: 0.00056811
Iteration 203/1000 | Loss: 0.00058914
Iteration 204/1000 | Loss: 0.00063122
Iteration 205/1000 | Loss: 0.00060481
Iteration 206/1000 | Loss: 0.00054621
Iteration 207/1000 | Loss: 0.00055518
Iteration 208/1000 | Loss: 0.00059057
Iteration 209/1000 | Loss: 0.00066803
Iteration 210/1000 | Loss: 0.00053723
Iteration 211/1000 | Loss: 0.00065285
Iteration 212/1000 | Loss: 0.00068076
Iteration 213/1000 | Loss: 0.00075086
Iteration 214/1000 | Loss: 0.00064817
Iteration 215/1000 | Loss: 0.00069194
Iteration 216/1000 | Loss: 0.00069515
Iteration 217/1000 | Loss: 0.00060316
Iteration 218/1000 | Loss: 0.00054624
Iteration 219/1000 | Loss: 0.00069676
Iteration 220/1000 | Loss: 0.00065098
Iteration 221/1000 | Loss: 0.00062575
Iteration 222/1000 | Loss: 0.00047383
Iteration 223/1000 | Loss: 0.00051294
Iteration 224/1000 | Loss: 0.00050382
Iteration 225/1000 | Loss: 0.00049141
Iteration 226/1000 | Loss: 0.00049934
Iteration 227/1000 | Loss: 0.00051566
Iteration 228/1000 | Loss: 0.00051847
Iteration 229/1000 | Loss: 0.00052124
Iteration 230/1000 | Loss: 0.00053179
Iteration 231/1000 | Loss: 0.00087796
Iteration 232/1000 | Loss: 0.00053989
Iteration 233/1000 | Loss: 0.00055115
Iteration 234/1000 | Loss: 0.00052938
Iteration 235/1000 | Loss: 0.00051244
Iteration 236/1000 | Loss: 0.00053378
Iteration 237/1000 | Loss: 0.00050883
Iteration 238/1000 | Loss: 0.00067318
Iteration 239/1000 | Loss: 0.00047473
Iteration 240/1000 | Loss: 0.00043384
Iteration 241/1000 | Loss: 0.00046765
Iteration 242/1000 | Loss: 0.00057899
Iteration 243/1000 | Loss: 0.00047471
Iteration 244/1000 | Loss: 0.00048314
Iteration 245/1000 | Loss: 0.00047309
Iteration 246/1000 | Loss: 0.00044375
Iteration 247/1000 | Loss: 0.00041943
Iteration 248/1000 | Loss: 0.00038537
Iteration 249/1000 | Loss: 0.00079282
Iteration 250/1000 | Loss: 0.00039071
Iteration 251/1000 | Loss: 0.00045353
Iteration 252/1000 | Loss: 0.00047334
Iteration 253/1000 | Loss: 0.00040800
Iteration 254/1000 | Loss: 0.00041271
Iteration 255/1000 | Loss: 0.00052083
Iteration 256/1000 | Loss: 0.00041715
Iteration 257/1000 | Loss: 0.00042788
Iteration 258/1000 | Loss: 0.00045938
Iteration 259/1000 | Loss: 0.00042566
Iteration 260/1000 | Loss: 0.00047294
Iteration 261/1000 | Loss: 0.00040530
Iteration 262/1000 | Loss: 0.00047241
Iteration 263/1000 | Loss: 0.00043954
Iteration 264/1000 | Loss: 0.00048680
Iteration 265/1000 | Loss: 0.00043692
Iteration 266/1000 | Loss: 0.00047982
Iteration 267/1000 | Loss: 0.00039227
Iteration 268/1000 | Loss: 0.00040714
Iteration 269/1000 | Loss: 0.00054724
Iteration 270/1000 | Loss: 0.00050214
Iteration 271/1000 | Loss: 0.00055846
Iteration 272/1000 | Loss: 0.00037748
Iteration 273/1000 | Loss: 0.00037527
Iteration 274/1000 | Loss: 0.00048144
Iteration 275/1000 | Loss: 0.00040521
Iteration 276/1000 | Loss: 0.00042132
Iteration 277/1000 | Loss: 0.00042468
Iteration 278/1000 | Loss: 0.00039325
Iteration 279/1000 | Loss: 0.00044122
Iteration 280/1000 | Loss: 0.00044124
Iteration 281/1000 | Loss: 0.00040154
Iteration 282/1000 | Loss: 0.00046989
Iteration 283/1000 | Loss: 0.00042964
Iteration 284/1000 | Loss: 0.00039025
Iteration 285/1000 | Loss: 0.00044378
Iteration 286/1000 | Loss: 0.00052704
Iteration 287/1000 | Loss: 0.00044760
Iteration 288/1000 | Loss: 0.00051509
Iteration 289/1000 | Loss: 0.00038715
Iteration 290/1000 | Loss: 0.00040316
Iteration 291/1000 | Loss: 0.00046075
Iteration 292/1000 | Loss: 0.00048102
Iteration 293/1000 | Loss: 0.00042745
Iteration 294/1000 | Loss: 0.00043374
Iteration 295/1000 | Loss: 0.00046443
Iteration 296/1000 | Loss: 0.00043474
Iteration 297/1000 | Loss: 0.00044105
Iteration 298/1000 | Loss: 0.00042509
Iteration 299/1000 | Loss: 0.00044835
Iteration 300/1000 | Loss: 0.00038918
Iteration 301/1000 | Loss: 0.00037825
Iteration 302/1000 | Loss: 0.00042069
Iteration 303/1000 | Loss: 0.00041916
Iteration 304/1000 | Loss: 0.00042665
Iteration 305/1000 | Loss: 0.00048060
Iteration 306/1000 | Loss: 0.00037927
Iteration 307/1000 | Loss: 0.00037589
Iteration 308/1000 | Loss: 0.00037434
Iteration 309/1000 | Loss: 0.00037335
Iteration 310/1000 | Loss: 0.00037276
Iteration 311/1000 | Loss: 0.00056841
Iteration 312/1000 | Loss: 0.00049669
Iteration 313/1000 | Loss: 0.00047371
Iteration 314/1000 | Loss: 0.00037883
Iteration 315/1000 | Loss: 0.00046774
Iteration 316/1000 | Loss: 0.00049040
Iteration 317/1000 | Loss: 0.00040726
Iteration 318/1000 | Loss: 0.00039715
Iteration 319/1000 | Loss: 0.00037310
Iteration 320/1000 | Loss: 0.00037068
Iteration 321/1000 | Loss: 0.00036969
Iteration 322/1000 | Loss: 0.00036888
Iteration 323/1000 | Loss: 0.00036851
Iteration 324/1000 | Loss: 0.00036826
Iteration 325/1000 | Loss: 0.00036809
Iteration 326/1000 | Loss: 0.00036805
Iteration 327/1000 | Loss: 0.00036805
Iteration 328/1000 | Loss: 0.00036798
Iteration 329/1000 | Loss: 0.00036797
Iteration 330/1000 | Loss: 0.00036794
Iteration 331/1000 | Loss: 0.00036792
Iteration 332/1000 | Loss: 0.00036789
Iteration 333/1000 | Loss: 0.00036789
Iteration 334/1000 | Loss: 0.00036789
Iteration 335/1000 | Loss: 0.00036788
Iteration 336/1000 | Loss: 0.00036788
Iteration 337/1000 | Loss: 0.00036778
Iteration 338/1000 | Loss: 0.00036776
Iteration 339/1000 | Loss: 0.00036775
Iteration 340/1000 | Loss: 0.00036773
Iteration 341/1000 | Loss: 0.00036773
Iteration 342/1000 | Loss: 0.00036772
Iteration 343/1000 | Loss: 0.00036772
Iteration 344/1000 | Loss: 0.00036772
Iteration 345/1000 | Loss: 0.00036772
Iteration 346/1000 | Loss: 0.00036772
Iteration 347/1000 | Loss: 0.00036772
Iteration 348/1000 | Loss: 0.00036772
Iteration 349/1000 | Loss: 0.00036772
Iteration 350/1000 | Loss: 0.00036772
Iteration 351/1000 | Loss: 0.00036772
Iteration 352/1000 | Loss: 0.00036772
Iteration 353/1000 | Loss: 0.00036772
Iteration 354/1000 | Loss: 0.00036771
Iteration 355/1000 | Loss: 0.00036768
Iteration 356/1000 | Loss: 0.00036767
Iteration 357/1000 | Loss: 0.00036767
Iteration 358/1000 | Loss: 0.00036766
Iteration 359/1000 | Loss: 0.00036766
Iteration 360/1000 | Loss: 0.00036766
Iteration 361/1000 | Loss: 0.00036766
Iteration 362/1000 | Loss: 0.00036765
Iteration 363/1000 | Loss: 0.00036765
Iteration 364/1000 | Loss: 0.00036765
Iteration 365/1000 | Loss: 0.00036764
Iteration 366/1000 | Loss: 0.00036764
Iteration 367/1000 | Loss: 0.00036764
Iteration 368/1000 | Loss: 0.00036764
Iteration 369/1000 | Loss: 0.00036764
Iteration 370/1000 | Loss: 0.00036764
Iteration 371/1000 | Loss: 0.00036764
Iteration 372/1000 | Loss: 0.00036764
Iteration 373/1000 | Loss: 0.00036764
Iteration 374/1000 | Loss: 0.00036763
Iteration 375/1000 | Loss: 0.00036763
Iteration 376/1000 | Loss: 0.00036763
Iteration 377/1000 | Loss: 0.00036763
Iteration 378/1000 | Loss: 0.00036763
Iteration 379/1000 | Loss: 0.00036763
Iteration 380/1000 | Loss: 0.00036763
Iteration 381/1000 | Loss: 0.00036763
Iteration 382/1000 | Loss: 0.00036763
Iteration 383/1000 | Loss: 0.00036763
Iteration 384/1000 | Loss: 0.00036763
Iteration 385/1000 | Loss: 0.00036763
Iteration 386/1000 | Loss: 0.00036762
Iteration 387/1000 | Loss: 0.00036762
Iteration 388/1000 | Loss: 0.00036762
Iteration 389/1000 | Loss: 0.00036762
Iteration 390/1000 | Loss: 0.00036762
Iteration 391/1000 | Loss: 0.00036762
Iteration 392/1000 | Loss: 0.00036762
Iteration 393/1000 | Loss: 0.00036762
Iteration 394/1000 | Loss: 0.00036762
Iteration 395/1000 | Loss: 0.00036762
Iteration 396/1000 | Loss: 0.00036762
Iteration 397/1000 | Loss: 0.00036762
Iteration 398/1000 | Loss: 0.00036762
Iteration 399/1000 | Loss: 0.00036762
Iteration 400/1000 | Loss: 0.00036761
Iteration 401/1000 | Loss: 0.00036761
Iteration 402/1000 | Loss: 0.00036761
Iteration 403/1000 | Loss: 0.00036761
Iteration 404/1000 | Loss: 0.00036761
Iteration 405/1000 | Loss: 0.00036761
Iteration 406/1000 | Loss: 0.00036761
Iteration 407/1000 | Loss: 0.00036761
Iteration 408/1000 | Loss: 0.00036761
Iteration 409/1000 | Loss: 0.00036761
Iteration 410/1000 | Loss: 0.00036761
Iteration 411/1000 | Loss: 0.00036761
Iteration 412/1000 | Loss: 0.00036761
Iteration 413/1000 | Loss: 0.00036761
Iteration 414/1000 | Loss: 0.00036760
Iteration 415/1000 | Loss: 0.00036760
Iteration 416/1000 | Loss: 0.00036760
Iteration 417/1000 | Loss: 0.00036760
Iteration 418/1000 | Loss: 0.00036760
Iteration 419/1000 | Loss: 0.00036760
Iteration 420/1000 | Loss: 0.00036760
Iteration 421/1000 | Loss: 0.00036760
Iteration 422/1000 | Loss: 0.00036760
Iteration 423/1000 | Loss: 0.00036760
Iteration 424/1000 | Loss: 0.00036760
Iteration 425/1000 | Loss: 0.00036760
Iteration 426/1000 | Loss: 0.00036759
Iteration 427/1000 | Loss: 0.00036759
Iteration 428/1000 | Loss: 0.00036759
Iteration 429/1000 | Loss: 0.00036759
Iteration 430/1000 | Loss: 0.00036759
Iteration 431/1000 | Loss: 0.00036759
Iteration 432/1000 | Loss: 0.00036759
Iteration 433/1000 | Loss: 0.00036759
Iteration 434/1000 | Loss: 0.00036759
Iteration 435/1000 | Loss: 0.00036759
Iteration 436/1000 | Loss: 0.00036759
Iteration 437/1000 | Loss: 0.00036759
Iteration 438/1000 | Loss: 0.00036759
Iteration 439/1000 | Loss: 0.00036758
Iteration 440/1000 | Loss: 0.00036758
Iteration 441/1000 | Loss: 0.00036758
Iteration 442/1000 | Loss: 0.00036758
Iteration 443/1000 | Loss: 0.00036758
Iteration 444/1000 | Loss: 0.00036758
Iteration 445/1000 | Loss: 0.00036758
Iteration 446/1000 | Loss: 0.00036758
Iteration 447/1000 | Loss: 0.00036758
Iteration 448/1000 | Loss: 0.00036758
Iteration 449/1000 | Loss: 0.00036758
Iteration 450/1000 | Loss: 0.00036758
Iteration 451/1000 | Loss: 0.00036758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 451. Stopping optimization.
Last 5 losses: [0.0003675828338600695, 0.0003675828338600695, 0.0003675828338600695, 0.0003675828338600695, 0.0003675828338600695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003675828338600695

Optimization complete. Final v2v error: 11.3238525390625 mm

Highest mean error: 16.875200271606445 mm for frame 25

Lowest mean error: 7.3895792961120605 mm for frame 31

Saving results

Total time: 568.6875243186951
