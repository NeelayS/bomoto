Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=175, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9800-9855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00584194
Iteration 2/25 | Loss: 0.00129026
Iteration 3/25 | Loss: 0.00108260
Iteration 4/25 | Loss: 0.00105862
Iteration 5/25 | Loss: 0.00105379
Iteration 6/25 | Loss: 0.00105165
Iteration 7/25 | Loss: 0.00104982
Iteration 8/25 | Loss: 0.00104886
Iteration 9/25 | Loss: 0.00104975
Iteration 10/25 | Loss: 0.00105182
Iteration 11/25 | Loss: 0.00105139
Iteration 12/25 | Loss: 0.00105172
Iteration 13/25 | Loss: 0.00105118
Iteration 14/25 | Loss: 0.00105162
Iteration 15/25 | Loss: 0.00105161
Iteration 16/25 | Loss: 0.00105177
Iteration 17/25 | Loss: 0.00105127
Iteration 18/25 | Loss: 0.00105094
Iteration 19/25 | Loss: 0.00105063
Iteration 20/25 | Loss: 0.00105021
Iteration 21/25 | Loss: 0.00105161
Iteration 22/25 | Loss: 0.00104928
Iteration 23/25 | Loss: 0.00104843
Iteration 24/25 | Loss: 0.00104686
Iteration 25/25 | Loss: 0.00104645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06403637
Iteration 2/25 | Loss: 0.00068218
Iteration 3/25 | Loss: 0.00068215
Iteration 4/25 | Loss: 0.00068215
Iteration 5/25 | Loss: 0.00068215
Iteration 6/25 | Loss: 0.00068215
Iteration 7/25 | Loss: 0.00068214
Iteration 8/25 | Loss: 0.00068214
Iteration 9/25 | Loss: 0.00068214
Iteration 10/25 | Loss: 0.00068214
Iteration 11/25 | Loss: 0.00068214
Iteration 12/25 | Loss: 0.00068214
Iteration 13/25 | Loss: 0.00068214
Iteration 14/25 | Loss: 0.00068214
Iteration 15/25 | Loss: 0.00068214
Iteration 16/25 | Loss: 0.00068214
Iteration 17/25 | Loss: 0.00068214
Iteration 18/25 | Loss: 0.00068214
Iteration 19/25 | Loss: 0.00068214
Iteration 20/25 | Loss: 0.00068214
Iteration 21/25 | Loss: 0.00068214
Iteration 22/25 | Loss: 0.00068214
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006821434362791479, 0.0006821434362791479, 0.0006821434362791479, 0.0006821434362791479, 0.0006821434362791479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006821434362791479

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068214
Iteration 2/1000 | Loss: 0.00003753
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001215
Iteration 10/1000 | Loss: 0.00001198
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001170
Iteration 19/1000 | Loss: 0.00001169
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001168
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001167
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001162
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001154
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001146
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001145
Iteration 100/1000 | Loss: 0.00001145
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001145
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001145
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Iteration 119/1000 | Loss: 0.00001144
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001143
Iteration 124/1000 | Loss: 0.00001143
Iteration 125/1000 | Loss: 0.00001143
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001142
Iteration 134/1000 | Loss: 0.00001142
Iteration 135/1000 | Loss: 0.00001142
Iteration 136/1000 | Loss: 0.00001142
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001142
Iteration 153/1000 | Loss: 0.00001142
Iteration 154/1000 | Loss: 0.00001142
Iteration 155/1000 | Loss: 0.00001142
Iteration 156/1000 | Loss: 0.00001142
Iteration 157/1000 | Loss: 0.00001142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.1417583664297126e-05, 1.1417583664297126e-05, 1.1417583664297126e-05, 1.1417583664297126e-05, 1.1417583664297126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1417583664297126e-05

Optimization complete. Final v2v error: 2.8403873443603516 mm

Highest mean error: 3.8938214778900146 mm for frame 88

Lowest mean error: 2.4814705848693848 mm for frame 0

Saving results

Total time: 74.19204378128052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055941
Iteration 2/25 | Loss: 0.01055941
Iteration 3/25 | Loss: 0.01055941
Iteration 4/25 | Loss: 0.00316331
Iteration 5/25 | Loss: 0.00192575
Iteration 6/25 | Loss: 0.00177662
Iteration 7/25 | Loss: 0.00167441
Iteration 8/25 | Loss: 0.00159521
Iteration 9/25 | Loss: 0.00155544
Iteration 10/25 | Loss: 0.00152315
Iteration 11/25 | Loss: 0.00145310
Iteration 12/25 | Loss: 0.00144966
Iteration 13/25 | Loss: 0.00141268
Iteration 14/25 | Loss: 0.00140621
Iteration 15/25 | Loss: 0.00137447
Iteration 16/25 | Loss: 0.00137210
Iteration 17/25 | Loss: 0.00135314
Iteration 18/25 | Loss: 0.00134733
Iteration 19/25 | Loss: 0.00134915
Iteration 20/25 | Loss: 0.00134636
Iteration 21/25 | Loss: 0.00135264
Iteration 22/25 | Loss: 0.00134474
Iteration 23/25 | Loss: 0.00134324
Iteration 24/25 | Loss: 0.00135144
Iteration 25/25 | Loss: 0.00132448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34778345
Iteration 2/25 | Loss: 0.00387973
Iteration 3/25 | Loss: 0.00316912
Iteration 4/25 | Loss: 0.00316912
Iteration 5/25 | Loss: 0.00316912
Iteration 6/25 | Loss: 0.00316912
Iteration 7/25 | Loss: 0.00316912
Iteration 8/25 | Loss: 0.00316912
Iteration 9/25 | Loss: 0.00316912
Iteration 10/25 | Loss: 0.00316912
Iteration 11/25 | Loss: 0.00316912
Iteration 12/25 | Loss: 0.00316911
Iteration 13/25 | Loss: 0.00316911
Iteration 14/25 | Loss: 0.00316911
Iteration 15/25 | Loss: 0.00316911
Iteration 16/25 | Loss: 0.00316911
Iteration 17/25 | Loss: 0.00316911
Iteration 18/25 | Loss: 0.00316911
Iteration 19/25 | Loss: 0.00316911
Iteration 20/25 | Loss: 0.00316911
Iteration 21/25 | Loss: 0.00316911
Iteration 22/25 | Loss: 0.00316911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0031691144686192274, 0.0031691144686192274, 0.0031691144686192274, 0.0031691144686192274, 0.0031691144686192274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031691144686192274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316911
Iteration 2/1000 | Loss: 0.00244528
Iteration 3/1000 | Loss: 0.00161366
Iteration 4/1000 | Loss: 0.00162484
Iteration 5/1000 | Loss: 0.00174313
Iteration 6/1000 | Loss: 0.00142673
Iteration 7/1000 | Loss: 0.00111445
Iteration 8/1000 | Loss: 0.00463448
Iteration 9/1000 | Loss: 0.01425624
Iteration 10/1000 | Loss: 0.00568307
Iteration 11/1000 | Loss: 0.00493609
Iteration 12/1000 | Loss: 0.00286977
Iteration 13/1000 | Loss: 0.00186991
Iteration 14/1000 | Loss: 0.00052697
Iteration 15/1000 | Loss: 0.00103746
Iteration 16/1000 | Loss: 0.00074491
Iteration 17/1000 | Loss: 0.00172915
Iteration 18/1000 | Loss: 0.00377695
Iteration 19/1000 | Loss: 0.00241096
Iteration 20/1000 | Loss: 0.00057563
Iteration 21/1000 | Loss: 0.00030373
Iteration 22/1000 | Loss: 0.00034303
Iteration 23/1000 | Loss: 0.00082484
Iteration 24/1000 | Loss: 0.00128049
Iteration 25/1000 | Loss: 0.00151702
Iteration 26/1000 | Loss: 0.00105260
Iteration 27/1000 | Loss: 0.00060654
Iteration 28/1000 | Loss: 0.00021068
Iteration 29/1000 | Loss: 0.00027265
Iteration 30/1000 | Loss: 0.00022576
Iteration 31/1000 | Loss: 0.00021004
Iteration 32/1000 | Loss: 0.00274379
Iteration 33/1000 | Loss: 0.00283781
Iteration 34/1000 | Loss: 0.00223406
Iteration 35/1000 | Loss: 0.01079709
Iteration 36/1000 | Loss: 0.01142765
Iteration 37/1000 | Loss: 0.00699961
Iteration 38/1000 | Loss: 0.00594416
Iteration 39/1000 | Loss: 0.01187212
Iteration 40/1000 | Loss: 0.00605700
Iteration 41/1000 | Loss: 0.00422083
Iteration 42/1000 | Loss: 0.00526647
Iteration 43/1000 | Loss: 0.00417314
Iteration 44/1000 | Loss: 0.00255208
Iteration 45/1000 | Loss: 0.00698982
Iteration 46/1000 | Loss: 0.00526672
Iteration 47/1000 | Loss: 0.00553156
Iteration 48/1000 | Loss: 0.00705540
Iteration 49/1000 | Loss: 0.00766261
Iteration 50/1000 | Loss: 0.00719942
Iteration 51/1000 | Loss: 0.00355481
Iteration 52/1000 | Loss: 0.00328950
Iteration 53/1000 | Loss: 0.00178568
Iteration 54/1000 | Loss: 0.00543192
Iteration 55/1000 | Loss: 0.00448018
Iteration 56/1000 | Loss: 0.00088460
Iteration 57/1000 | Loss: 0.00050309
Iteration 58/1000 | Loss: 0.00048593
Iteration 59/1000 | Loss: 0.00152185
Iteration 60/1000 | Loss: 0.00214907
Iteration 61/1000 | Loss: 0.00203211
Iteration 62/1000 | Loss: 0.00126610
Iteration 63/1000 | Loss: 0.00147609
Iteration 64/1000 | Loss: 0.00255927
Iteration 65/1000 | Loss: 0.00162409
Iteration 66/1000 | Loss: 0.00132631
Iteration 67/1000 | Loss: 0.00095775
Iteration 68/1000 | Loss: 0.00073026
Iteration 69/1000 | Loss: 0.00290177
Iteration 70/1000 | Loss: 0.00082811
Iteration 71/1000 | Loss: 0.00138035
Iteration 72/1000 | Loss: 0.00113835
Iteration 73/1000 | Loss: 0.00173109
Iteration 74/1000 | Loss: 0.00148138
Iteration 75/1000 | Loss: 0.00187137
Iteration 76/1000 | Loss: 0.00113962
Iteration 77/1000 | Loss: 0.00164824
Iteration 78/1000 | Loss: 0.00191966
Iteration 79/1000 | Loss: 0.00121890
Iteration 80/1000 | Loss: 0.00444563
Iteration 81/1000 | Loss: 0.00097139
Iteration 82/1000 | Loss: 0.00141309
Iteration 83/1000 | Loss: 0.00089079
Iteration 84/1000 | Loss: 0.00076520
Iteration 85/1000 | Loss: 0.00067292
Iteration 86/1000 | Loss: 0.00072658
Iteration 87/1000 | Loss: 0.00186267
Iteration 88/1000 | Loss: 0.00079843
Iteration 89/1000 | Loss: 0.00131535
Iteration 90/1000 | Loss: 0.00187695
Iteration 91/1000 | Loss: 0.00192417
Iteration 92/1000 | Loss: 0.00126592
Iteration 93/1000 | Loss: 0.00077563
Iteration 94/1000 | Loss: 0.00227528
Iteration 95/1000 | Loss: 0.00116449
Iteration 96/1000 | Loss: 0.00182994
Iteration 97/1000 | Loss: 0.00088057
Iteration 98/1000 | Loss: 0.00088467
Iteration 99/1000 | Loss: 0.00110360
Iteration 100/1000 | Loss: 0.00190264
Iteration 101/1000 | Loss: 0.00099999
Iteration 102/1000 | Loss: 0.00110454
Iteration 103/1000 | Loss: 0.00082222
Iteration 104/1000 | Loss: 0.00213227
Iteration 105/1000 | Loss: 0.00073721
Iteration 106/1000 | Loss: 0.00076753
Iteration 107/1000 | Loss: 0.00092329
Iteration 108/1000 | Loss: 0.00089915
Iteration 109/1000 | Loss: 0.00063024
Iteration 110/1000 | Loss: 0.00052145
Iteration 111/1000 | Loss: 0.00157803
Iteration 112/1000 | Loss: 0.00065573
Iteration 113/1000 | Loss: 0.00077959
Iteration 114/1000 | Loss: 0.00058202
Iteration 115/1000 | Loss: 0.00059776
Iteration 116/1000 | Loss: 0.00075451
Iteration 117/1000 | Loss: 0.00032247
Iteration 118/1000 | Loss: 0.00016151
Iteration 119/1000 | Loss: 0.00068211
Iteration 120/1000 | Loss: 0.00024019
Iteration 121/1000 | Loss: 0.00053587
Iteration 122/1000 | Loss: 0.00022309
Iteration 123/1000 | Loss: 0.00030857
Iteration 124/1000 | Loss: 0.00021787
Iteration 125/1000 | Loss: 0.00005046
Iteration 126/1000 | Loss: 0.00003796
Iteration 127/1000 | Loss: 0.00073323
Iteration 128/1000 | Loss: 0.00004334
Iteration 129/1000 | Loss: 0.00003334
Iteration 130/1000 | Loss: 0.00020094
Iteration 131/1000 | Loss: 0.00013410
Iteration 132/1000 | Loss: 0.00017093
Iteration 133/1000 | Loss: 0.00037097
Iteration 134/1000 | Loss: 0.00004636
Iteration 135/1000 | Loss: 0.00003891
Iteration 136/1000 | Loss: 0.00003312
Iteration 137/1000 | Loss: 0.00031897
Iteration 138/1000 | Loss: 0.00006264
Iteration 139/1000 | Loss: 0.00022898
Iteration 140/1000 | Loss: 0.00008080
Iteration 141/1000 | Loss: 0.00003298
Iteration 142/1000 | Loss: 0.00022701
Iteration 143/1000 | Loss: 0.00040033
Iteration 144/1000 | Loss: 0.00061566
Iteration 145/1000 | Loss: 0.00207185
Iteration 146/1000 | Loss: 0.00066992
Iteration 147/1000 | Loss: 0.00092954
Iteration 148/1000 | Loss: 0.00234513
Iteration 149/1000 | Loss: 0.00012006
Iteration 150/1000 | Loss: 0.00029693
Iteration 151/1000 | Loss: 0.00012887
Iteration 152/1000 | Loss: 0.00048951
Iteration 153/1000 | Loss: 0.00012736
Iteration 154/1000 | Loss: 0.00016377
Iteration 155/1000 | Loss: 0.00011585
Iteration 156/1000 | Loss: 0.00053134
Iteration 157/1000 | Loss: 0.00050733
Iteration 158/1000 | Loss: 0.00002877
Iteration 159/1000 | Loss: 0.00004146
Iteration 160/1000 | Loss: 0.00006328
Iteration 161/1000 | Loss: 0.00006568
Iteration 162/1000 | Loss: 0.00007151
Iteration 163/1000 | Loss: 0.00003824
Iteration 164/1000 | Loss: 0.00006858
Iteration 165/1000 | Loss: 0.00005934
Iteration 166/1000 | Loss: 0.00057709
Iteration 167/1000 | Loss: 0.00045631
Iteration 168/1000 | Loss: 0.00031223
Iteration 169/1000 | Loss: 0.00043661
Iteration 170/1000 | Loss: 0.00023234
Iteration 171/1000 | Loss: 0.00013601
Iteration 172/1000 | Loss: 0.00023079
Iteration 173/1000 | Loss: 0.00020018
Iteration 174/1000 | Loss: 0.00015335
Iteration 175/1000 | Loss: 0.00039029
Iteration 176/1000 | Loss: 0.00197413
Iteration 177/1000 | Loss: 0.00044132
Iteration 178/1000 | Loss: 0.00148659
Iteration 179/1000 | Loss: 0.00097049
Iteration 180/1000 | Loss: 0.00057271
Iteration 181/1000 | Loss: 0.00095305
Iteration 182/1000 | Loss: 0.00062109
Iteration 183/1000 | Loss: 0.00005690
Iteration 184/1000 | Loss: 0.00030765
Iteration 185/1000 | Loss: 0.00094889
Iteration 186/1000 | Loss: 0.00013641
Iteration 187/1000 | Loss: 0.00007666
Iteration 188/1000 | Loss: 0.00003842
Iteration 189/1000 | Loss: 0.00009091
Iteration 190/1000 | Loss: 0.00018400
Iteration 191/1000 | Loss: 0.00034559
Iteration 192/1000 | Loss: 0.00017544
Iteration 193/1000 | Loss: 0.00044397
Iteration 194/1000 | Loss: 0.00009876
Iteration 195/1000 | Loss: 0.00080217
Iteration 196/1000 | Loss: 0.00007620
Iteration 197/1000 | Loss: 0.00019449
Iteration 198/1000 | Loss: 0.00042974
Iteration 199/1000 | Loss: 0.00003792
Iteration 200/1000 | Loss: 0.00004720
Iteration 201/1000 | Loss: 0.00004317
Iteration 202/1000 | Loss: 0.00004298
Iteration 203/1000 | Loss: 0.00004265
Iteration 204/1000 | Loss: 0.00042566
Iteration 205/1000 | Loss: 0.00042086
Iteration 206/1000 | Loss: 0.00004657
Iteration 207/1000 | Loss: 0.00012933
Iteration 208/1000 | Loss: 0.00004894
Iteration 209/1000 | Loss: 0.00003950
Iteration 210/1000 | Loss: 0.00003523
Iteration 211/1000 | Loss: 0.00028528
Iteration 212/1000 | Loss: 0.00024058
Iteration 213/1000 | Loss: 0.00031975
Iteration 214/1000 | Loss: 0.00019883
Iteration 215/1000 | Loss: 0.00022750
Iteration 216/1000 | Loss: 0.00017654
Iteration 217/1000 | Loss: 0.00033305
Iteration 218/1000 | Loss: 0.00027726
Iteration 219/1000 | Loss: 0.00014660
Iteration 220/1000 | Loss: 0.00034649
Iteration 221/1000 | Loss: 0.00026808
Iteration 222/1000 | Loss: 0.00008213
Iteration 223/1000 | Loss: 0.00028069
Iteration 224/1000 | Loss: 0.00030943
Iteration 225/1000 | Loss: 0.00145226
Iteration 226/1000 | Loss: 0.00016419
Iteration 227/1000 | Loss: 0.00028243
Iteration 228/1000 | Loss: 0.00009327
Iteration 229/1000 | Loss: 0.00042870
Iteration 230/1000 | Loss: 0.00026205
Iteration 231/1000 | Loss: 0.00003510
Iteration 232/1000 | Loss: 0.00039721
Iteration 233/1000 | Loss: 0.00006387
Iteration 234/1000 | Loss: 0.00003676
Iteration 235/1000 | Loss: 0.00002909
Iteration 236/1000 | Loss: 0.00004422
Iteration 237/1000 | Loss: 0.00037720
Iteration 238/1000 | Loss: 0.00007247
Iteration 239/1000 | Loss: 0.00017798
Iteration 240/1000 | Loss: 0.00036644
Iteration 241/1000 | Loss: 0.00025052
Iteration 242/1000 | Loss: 0.00016963
Iteration 243/1000 | Loss: 0.00007320
Iteration 244/1000 | Loss: 0.00014562
Iteration 245/1000 | Loss: 0.00017038
Iteration 246/1000 | Loss: 0.00010918
Iteration 247/1000 | Loss: 0.00048259
Iteration 248/1000 | Loss: 0.00021184
Iteration 249/1000 | Loss: 0.00003991
Iteration 250/1000 | Loss: 0.00003277
Iteration 251/1000 | Loss: 0.00003246
Iteration 252/1000 | Loss: 0.00034146
Iteration 253/1000 | Loss: 0.00005797
Iteration 254/1000 | Loss: 0.00004628
Iteration 255/1000 | Loss: 0.00004338
Iteration 256/1000 | Loss: 0.00003972
Iteration 257/1000 | Loss: 0.00003758
Iteration 258/1000 | Loss: 0.00027248
Iteration 259/1000 | Loss: 0.00002965
Iteration 260/1000 | Loss: 0.00003768
Iteration 261/1000 | Loss: 0.00025915
Iteration 262/1000 | Loss: 0.00121812
Iteration 263/1000 | Loss: 0.00037906
Iteration 264/1000 | Loss: 0.00027252
Iteration 265/1000 | Loss: 0.00018407
Iteration 266/1000 | Loss: 0.00047487
Iteration 267/1000 | Loss: 0.00054641
Iteration 268/1000 | Loss: 0.00037351
Iteration 269/1000 | Loss: 0.00026899
Iteration 270/1000 | Loss: 0.00041341
Iteration 271/1000 | Loss: 0.00023480
Iteration 272/1000 | Loss: 0.00018917
Iteration 273/1000 | Loss: 0.00003151
Iteration 274/1000 | Loss: 0.00052269
Iteration 275/1000 | Loss: 0.00061069
Iteration 276/1000 | Loss: 0.00045486
Iteration 277/1000 | Loss: 0.00039907
Iteration 278/1000 | Loss: 0.00030530
Iteration 279/1000 | Loss: 0.00068655
Iteration 280/1000 | Loss: 0.00028198
Iteration 281/1000 | Loss: 0.00016654
Iteration 282/1000 | Loss: 0.00003157
Iteration 283/1000 | Loss: 0.00002464
Iteration 284/1000 | Loss: 0.00018276
Iteration 285/1000 | Loss: 0.00021478
Iteration 286/1000 | Loss: 0.00002315
Iteration 287/1000 | Loss: 0.00015772
Iteration 288/1000 | Loss: 0.00012890
Iteration 289/1000 | Loss: 0.00071403
Iteration 290/1000 | Loss: 0.00035322
Iteration 291/1000 | Loss: 0.00004721
Iteration 292/1000 | Loss: 0.00050480
Iteration 293/1000 | Loss: 0.00018952
Iteration 294/1000 | Loss: 0.00025499
Iteration 295/1000 | Loss: 0.00299505
Iteration 296/1000 | Loss: 0.00143652
Iteration 297/1000 | Loss: 0.00034761
Iteration 298/1000 | Loss: 0.00021525
Iteration 299/1000 | Loss: 0.00005713
Iteration 300/1000 | Loss: 0.00015436
Iteration 301/1000 | Loss: 0.00023694
Iteration 302/1000 | Loss: 0.00008243
Iteration 303/1000 | Loss: 0.00009125
Iteration 304/1000 | Loss: 0.00002998
Iteration 305/1000 | Loss: 0.00063415
Iteration 306/1000 | Loss: 0.00054271
Iteration 307/1000 | Loss: 0.00075584
Iteration 308/1000 | Loss: 0.00059044
Iteration 309/1000 | Loss: 0.00041407
Iteration 310/1000 | Loss: 0.00006443
Iteration 311/1000 | Loss: 0.00027125
Iteration 312/1000 | Loss: 0.00020317
Iteration 313/1000 | Loss: 0.00002480
Iteration 314/1000 | Loss: 0.00002418
Iteration 315/1000 | Loss: 0.00040317
Iteration 316/1000 | Loss: 0.00006371
Iteration 317/1000 | Loss: 0.00003615
Iteration 318/1000 | Loss: 0.00018824
Iteration 319/1000 | Loss: 0.00029867
Iteration 320/1000 | Loss: 0.00037304
Iteration 321/1000 | Loss: 0.00023338
Iteration 322/1000 | Loss: 0.00014449
Iteration 323/1000 | Loss: 0.00065114
Iteration 324/1000 | Loss: 0.00030607
Iteration 325/1000 | Loss: 0.00030056
Iteration 326/1000 | Loss: 0.00024958
Iteration 327/1000 | Loss: 0.00027951
Iteration 328/1000 | Loss: 0.00054706
Iteration 329/1000 | Loss: 0.00022060
Iteration 330/1000 | Loss: 0.00017953
Iteration 331/1000 | Loss: 0.00043123
Iteration 332/1000 | Loss: 0.00016076
Iteration 333/1000 | Loss: 0.00053748
Iteration 334/1000 | Loss: 0.00013881
Iteration 335/1000 | Loss: 0.00030627
Iteration 336/1000 | Loss: 0.00029620
Iteration 337/1000 | Loss: 0.00033341
Iteration 338/1000 | Loss: 0.00063990
Iteration 339/1000 | Loss: 0.00037111
Iteration 340/1000 | Loss: 0.00025290
Iteration 341/1000 | Loss: 0.00025591
Iteration 342/1000 | Loss: 0.00009441
Iteration 343/1000 | Loss: 0.00037560
Iteration 344/1000 | Loss: 0.00010490
Iteration 345/1000 | Loss: 0.00010425
Iteration 346/1000 | Loss: 0.00043979
Iteration 347/1000 | Loss: 0.00030727
Iteration 348/1000 | Loss: 0.00047811
Iteration 349/1000 | Loss: 0.00048901
Iteration 350/1000 | Loss: 0.00052191
Iteration 351/1000 | Loss: 0.00014944
Iteration 352/1000 | Loss: 0.00034187
Iteration 353/1000 | Loss: 0.00059062
Iteration 354/1000 | Loss: 0.00071041
Iteration 355/1000 | Loss: 0.00026044
Iteration 356/1000 | Loss: 0.00024841
Iteration 357/1000 | Loss: 0.00034531
Iteration 358/1000 | Loss: 0.00031601
Iteration 359/1000 | Loss: 0.00038876
Iteration 360/1000 | Loss: 0.00133198
Iteration 361/1000 | Loss: 0.00242379
Iteration 362/1000 | Loss: 0.00029856
Iteration 363/1000 | Loss: 0.00066583
Iteration 364/1000 | Loss: 0.00055568
Iteration 365/1000 | Loss: 0.00064122
Iteration 366/1000 | Loss: 0.00038591
Iteration 367/1000 | Loss: 0.00020264
Iteration 368/1000 | Loss: 0.00062059
Iteration 369/1000 | Loss: 0.00007145
Iteration 370/1000 | Loss: 0.00002304
Iteration 371/1000 | Loss: 0.00002194
Iteration 372/1000 | Loss: 0.00011165
Iteration 373/1000 | Loss: 0.00008421
Iteration 374/1000 | Loss: 0.00089368
Iteration 375/1000 | Loss: 0.00021677
Iteration 376/1000 | Loss: 0.00038551
Iteration 377/1000 | Loss: 0.00037858
Iteration 378/1000 | Loss: 0.00002965
Iteration 379/1000 | Loss: 0.00002121
Iteration 380/1000 | Loss: 0.00001930
Iteration 381/1000 | Loss: 0.00001815
Iteration 382/1000 | Loss: 0.00013631
Iteration 383/1000 | Loss: 0.00023174
Iteration 384/1000 | Loss: 0.00010038
Iteration 385/1000 | Loss: 0.00009191
Iteration 386/1000 | Loss: 0.00008453
Iteration 387/1000 | Loss: 0.00012738
Iteration 388/1000 | Loss: 0.00039031
Iteration 389/1000 | Loss: 0.00033333
Iteration 390/1000 | Loss: 0.00035700
Iteration 391/1000 | Loss: 0.00031203
Iteration 392/1000 | Loss: 0.00083481
Iteration 393/1000 | Loss: 0.00043373
Iteration 394/1000 | Loss: 0.00024859
Iteration 395/1000 | Loss: 0.00002394
Iteration 396/1000 | Loss: 0.00002021
Iteration 397/1000 | Loss: 0.00001885
Iteration 398/1000 | Loss: 0.00001759
Iteration 399/1000 | Loss: 0.00001853
Iteration 400/1000 | Loss: 0.00043431
Iteration 401/1000 | Loss: 0.00001680
Iteration 402/1000 | Loss: 0.00001770
Iteration 403/1000 | Loss: 0.00001507
Iteration 404/1000 | Loss: 0.00002052
Iteration 405/1000 | Loss: 0.00001462
Iteration 406/1000 | Loss: 0.00001440
Iteration 407/1000 | Loss: 0.00001434
Iteration 408/1000 | Loss: 0.00001997
Iteration 409/1000 | Loss: 0.00001431
Iteration 410/1000 | Loss: 0.00001449
Iteration 411/1000 | Loss: 0.00001394
Iteration 412/1000 | Loss: 0.00001394
Iteration 413/1000 | Loss: 0.00001394
Iteration 414/1000 | Loss: 0.00001394
Iteration 415/1000 | Loss: 0.00001393
Iteration 416/1000 | Loss: 0.00001459
Iteration 417/1000 | Loss: 0.00001384
Iteration 418/1000 | Loss: 0.00001384
Iteration 419/1000 | Loss: 0.00001384
Iteration 420/1000 | Loss: 0.00001384
Iteration 421/1000 | Loss: 0.00001384
Iteration 422/1000 | Loss: 0.00001383
Iteration 423/1000 | Loss: 0.00001383
Iteration 424/1000 | Loss: 0.00001383
Iteration 425/1000 | Loss: 0.00001383
Iteration 426/1000 | Loss: 0.00001383
Iteration 427/1000 | Loss: 0.00001383
Iteration 428/1000 | Loss: 0.00001383
Iteration 429/1000 | Loss: 0.00001383
Iteration 430/1000 | Loss: 0.00001382
Iteration 431/1000 | Loss: 0.00001379
Iteration 432/1000 | Loss: 0.00001379
Iteration 433/1000 | Loss: 0.00001379
Iteration 434/1000 | Loss: 0.00001379
Iteration 435/1000 | Loss: 0.00001379
Iteration 436/1000 | Loss: 0.00001379
Iteration 437/1000 | Loss: 0.00001379
Iteration 438/1000 | Loss: 0.00001379
Iteration 439/1000 | Loss: 0.00001379
Iteration 440/1000 | Loss: 0.00001379
Iteration 441/1000 | Loss: 0.00001379
Iteration 442/1000 | Loss: 0.00001379
Iteration 443/1000 | Loss: 0.00001378
Iteration 444/1000 | Loss: 0.00001378
Iteration 445/1000 | Loss: 0.00001378
Iteration 446/1000 | Loss: 0.00001378
Iteration 447/1000 | Loss: 0.00001377
Iteration 448/1000 | Loss: 0.00001377
Iteration 449/1000 | Loss: 0.00001377
Iteration 450/1000 | Loss: 0.00001376
Iteration 451/1000 | Loss: 0.00001376
Iteration 452/1000 | Loss: 0.00001375
Iteration 453/1000 | Loss: 0.00001453
Iteration 454/1000 | Loss: 0.00001372
Iteration 455/1000 | Loss: 0.00001371
Iteration 456/1000 | Loss: 0.00001371
Iteration 457/1000 | Loss: 0.00001371
Iteration 458/1000 | Loss: 0.00001371
Iteration 459/1000 | Loss: 0.00001371
Iteration 460/1000 | Loss: 0.00001371
Iteration 461/1000 | Loss: 0.00001371
Iteration 462/1000 | Loss: 0.00001371
Iteration 463/1000 | Loss: 0.00001371
Iteration 464/1000 | Loss: 0.00001371
Iteration 465/1000 | Loss: 0.00001371
Iteration 466/1000 | Loss: 0.00001371
Iteration 467/1000 | Loss: 0.00001371
Iteration 468/1000 | Loss: 0.00001371
Iteration 469/1000 | Loss: 0.00001371
Iteration 470/1000 | Loss: 0.00001371
Iteration 471/1000 | Loss: 0.00001371
Iteration 472/1000 | Loss: 0.00001371
Iteration 473/1000 | Loss: 0.00001371
Iteration 474/1000 | Loss: 0.00001371
Iteration 475/1000 | Loss: 0.00001371
Iteration 476/1000 | Loss: 0.00001371
Iteration 477/1000 | Loss: 0.00001371
Iteration 478/1000 | Loss: 0.00001371
Iteration 479/1000 | Loss: 0.00001371
Iteration 480/1000 | Loss: 0.00001371
Iteration 481/1000 | Loss: 0.00001371
Iteration 482/1000 | Loss: 0.00001371
Iteration 483/1000 | Loss: 0.00001371
Iteration 484/1000 | Loss: 0.00001371
Iteration 485/1000 | Loss: 0.00001371
Iteration 486/1000 | Loss: 0.00001371
Iteration 487/1000 | Loss: 0.00001371
Iteration 488/1000 | Loss: 0.00001371
Iteration 489/1000 | Loss: 0.00001371
Iteration 490/1000 | Loss: 0.00001371
Iteration 491/1000 | Loss: 0.00001371
Iteration 492/1000 | Loss: 0.00001371
Iteration 493/1000 | Loss: 0.00001371
Iteration 494/1000 | Loss: 0.00001371
Iteration 495/1000 | Loss: 0.00001371
Iteration 496/1000 | Loss: 0.00001371
Iteration 497/1000 | Loss: 0.00001371
Iteration 498/1000 | Loss: 0.00001371
Iteration 499/1000 | Loss: 0.00001371
Iteration 500/1000 | Loss: 0.00001371
Iteration 501/1000 | Loss: 0.00001371
Iteration 502/1000 | Loss: 0.00001371
Iteration 503/1000 | Loss: 0.00001371
Iteration 504/1000 | Loss: 0.00001371
Iteration 505/1000 | Loss: 0.00001371
Iteration 506/1000 | Loss: 0.00001371
Iteration 507/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 507. Stopping optimization.
Last 5 losses: [1.3705618584936019e-05, 1.3705618584936019e-05, 1.3705618584936019e-05, 1.3705618584936019e-05, 1.3705618584936019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3705618584936019e-05

Optimization complete. Final v2v error: 2.924424409866333 mm

Highest mean error: 5.581411361694336 mm for frame 48

Lowest mean error: 2.331228494644165 mm for frame 169

Saving results

Total time: 717.0773508548737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574084
Iteration 2/25 | Loss: 0.00121227
Iteration 3/25 | Loss: 0.00110120
Iteration 4/25 | Loss: 0.00108090
Iteration 5/25 | Loss: 0.00107376
Iteration 6/25 | Loss: 0.00107239
Iteration 7/25 | Loss: 0.00107239
Iteration 8/25 | Loss: 0.00107239
Iteration 9/25 | Loss: 0.00107239
Iteration 10/25 | Loss: 0.00107239
Iteration 11/25 | Loss: 0.00107239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010723945451900363, 0.0010723945451900363, 0.0010723945451900363, 0.0010723945451900363, 0.0010723945451900363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010723945451900363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34864509
Iteration 2/25 | Loss: 0.00090580
Iteration 3/25 | Loss: 0.00090574
Iteration 4/25 | Loss: 0.00090574
Iteration 5/25 | Loss: 0.00090574
Iteration 6/25 | Loss: 0.00090574
Iteration 7/25 | Loss: 0.00090574
Iteration 8/25 | Loss: 0.00090574
Iteration 9/25 | Loss: 0.00090574
Iteration 10/25 | Loss: 0.00090574
Iteration 11/25 | Loss: 0.00090574
Iteration 12/25 | Loss: 0.00090574
Iteration 13/25 | Loss: 0.00090574
Iteration 14/25 | Loss: 0.00090574
Iteration 15/25 | Loss: 0.00090574
Iteration 16/25 | Loss: 0.00090574
Iteration 17/25 | Loss: 0.00090574
Iteration 18/25 | Loss: 0.00090574
Iteration 19/25 | Loss: 0.00090574
Iteration 20/25 | Loss: 0.00090574
Iteration 21/25 | Loss: 0.00090574
Iteration 22/25 | Loss: 0.00090574
Iteration 23/25 | Loss: 0.00090574
Iteration 24/25 | Loss: 0.00090574
Iteration 25/25 | Loss: 0.00090574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090574
Iteration 2/1000 | Loss: 0.00004235
Iteration 3/1000 | Loss: 0.00002638
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002134
Iteration 6/1000 | Loss: 0.00002031
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001930
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001844
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001809
Iteration 14/1000 | Loss: 0.00001799
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001793
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001792
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001781
Iteration 22/1000 | Loss: 0.00001779
Iteration 23/1000 | Loss: 0.00001779
Iteration 24/1000 | Loss: 0.00001778
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001774
Iteration 30/1000 | Loss: 0.00001774
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001773
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001772
Iteration 35/1000 | Loss: 0.00001771
Iteration 36/1000 | Loss: 0.00001771
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001770
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001768
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00001768
Iteration 58/1000 | Loss: 0.00001768
Iteration 59/1000 | Loss: 0.00001768
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001767
Iteration 62/1000 | Loss: 0.00001767
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001766
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001766
Iteration 67/1000 | Loss: 0.00001766
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001765
Iteration 70/1000 | Loss: 0.00001765
Iteration 71/1000 | Loss: 0.00001765
Iteration 72/1000 | Loss: 0.00001765
Iteration 73/1000 | Loss: 0.00001765
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001765
Iteration 77/1000 | Loss: 0.00001765
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00001764
Iteration 84/1000 | Loss: 0.00001764
Iteration 85/1000 | Loss: 0.00001764
Iteration 86/1000 | Loss: 0.00001764
Iteration 87/1000 | Loss: 0.00001764
Iteration 88/1000 | Loss: 0.00001764
Iteration 89/1000 | Loss: 0.00001764
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001762
Iteration 106/1000 | Loss: 0.00001762
Iteration 107/1000 | Loss: 0.00001762
Iteration 108/1000 | Loss: 0.00001762
Iteration 109/1000 | Loss: 0.00001762
Iteration 110/1000 | Loss: 0.00001761
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001761
Iteration 116/1000 | Loss: 0.00001761
Iteration 117/1000 | Loss: 0.00001761
Iteration 118/1000 | Loss: 0.00001761
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001761
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001760
Iteration 125/1000 | Loss: 0.00001760
Iteration 126/1000 | Loss: 0.00001760
Iteration 127/1000 | Loss: 0.00001760
Iteration 128/1000 | Loss: 0.00001760
Iteration 129/1000 | Loss: 0.00001760
Iteration 130/1000 | Loss: 0.00001760
Iteration 131/1000 | Loss: 0.00001760
Iteration 132/1000 | Loss: 0.00001760
Iteration 133/1000 | Loss: 0.00001760
Iteration 134/1000 | Loss: 0.00001760
Iteration 135/1000 | Loss: 0.00001760
Iteration 136/1000 | Loss: 0.00001760
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00001760
Iteration 139/1000 | Loss: 0.00001760
Iteration 140/1000 | Loss: 0.00001760
Iteration 141/1000 | Loss: 0.00001760
Iteration 142/1000 | Loss: 0.00001759
Iteration 143/1000 | Loss: 0.00001759
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001759
Iteration 146/1000 | Loss: 0.00001759
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001759
Iteration 153/1000 | Loss: 0.00001759
Iteration 154/1000 | Loss: 0.00001759
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001759
Iteration 157/1000 | Loss: 0.00001758
Iteration 158/1000 | Loss: 0.00001758
Iteration 159/1000 | Loss: 0.00001758
Iteration 160/1000 | Loss: 0.00001758
Iteration 161/1000 | Loss: 0.00001758
Iteration 162/1000 | Loss: 0.00001758
Iteration 163/1000 | Loss: 0.00001758
Iteration 164/1000 | Loss: 0.00001758
Iteration 165/1000 | Loss: 0.00001758
Iteration 166/1000 | Loss: 0.00001758
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001758
Iteration 169/1000 | Loss: 0.00001758
Iteration 170/1000 | Loss: 0.00001758
Iteration 171/1000 | Loss: 0.00001757
Iteration 172/1000 | Loss: 0.00001757
Iteration 173/1000 | Loss: 0.00001757
Iteration 174/1000 | Loss: 0.00001757
Iteration 175/1000 | Loss: 0.00001757
Iteration 176/1000 | Loss: 0.00001757
Iteration 177/1000 | Loss: 0.00001757
Iteration 178/1000 | Loss: 0.00001757
Iteration 179/1000 | Loss: 0.00001757
Iteration 180/1000 | Loss: 0.00001757
Iteration 181/1000 | Loss: 0.00001757
Iteration 182/1000 | Loss: 0.00001757
Iteration 183/1000 | Loss: 0.00001757
Iteration 184/1000 | Loss: 0.00001757
Iteration 185/1000 | Loss: 0.00001757
Iteration 186/1000 | Loss: 0.00001757
Iteration 187/1000 | Loss: 0.00001757
Iteration 188/1000 | Loss: 0.00001756
Iteration 189/1000 | Loss: 0.00001756
Iteration 190/1000 | Loss: 0.00001756
Iteration 191/1000 | Loss: 0.00001756
Iteration 192/1000 | Loss: 0.00001756
Iteration 193/1000 | Loss: 0.00001756
Iteration 194/1000 | Loss: 0.00001756
Iteration 195/1000 | Loss: 0.00001756
Iteration 196/1000 | Loss: 0.00001756
Iteration 197/1000 | Loss: 0.00001756
Iteration 198/1000 | Loss: 0.00001756
Iteration 199/1000 | Loss: 0.00001756
Iteration 200/1000 | Loss: 0.00001756
Iteration 201/1000 | Loss: 0.00001756
Iteration 202/1000 | Loss: 0.00001756
Iteration 203/1000 | Loss: 0.00001756
Iteration 204/1000 | Loss: 0.00001755
Iteration 205/1000 | Loss: 0.00001755
Iteration 206/1000 | Loss: 0.00001755
Iteration 207/1000 | Loss: 0.00001755
Iteration 208/1000 | Loss: 0.00001755
Iteration 209/1000 | Loss: 0.00001755
Iteration 210/1000 | Loss: 0.00001755
Iteration 211/1000 | Loss: 0.00001755
Iteration 212/1000 | Loss: 0.00001755
Iteration 213/1000 | Loss: 0.00001755
Iteration 214/1000 | Loss: 0.00001755
Iteration 215/1000 | Loss: 0.00001755
Iteration 216/1000 | Loss: 0.00001755
Iteration 217/1000 | Loss: 0.00001755
Iteration 218/1000 | Loss: 0.00001755
Iteration 219/1000 | Loss: 0.00001755
Iteration 220/1000 | Loss: 0.00001755
Iteration 221/1000 | Loss: 0.00001755
Iteration 222/1000 | Loss: 0.00001755
Iteration 223/1000 | Loss: 0.00001755
Iteration 224/1000 | Loss: 0.00001755
Iteration 225/1000 | Loss: 0.00001755
Iteration 226/1000 | Loss: 0.00001755
Iteration 227/1000 | Loss: 0.00001755
Iteration 228/1000 | Loss: 0.00001755
Iteration 229/1000 | Loss: 0.00001755
Iteration 230/1000 | Loss: 0.00001755
Iteration 231/1000 | Loss: 0.00001755
Iteration 232/1000 | Loss: 0.00001755
Iteration 233/1000 | Loss: 0.00001755
Iteration 234/1000 | Loss: 0.00001755
Iteration 235/1000 | Loss: 0.00001755
Iteration 236/1000 | Loss: 0.00001755
Iteration 237/1000 | Loss: 0.00001755
Iteration 238/1000 | Loss: 0.00001755
Iteration 239/1000 | Loss: 0.00001755
Iteration 240/1000 | Loss: 0.00001755
Iteration 241/1000 | Loss: 0.00001755
Iteration 242/1000 | Loss: 0.00001755
Iteration 243/1000 | Loss: 0.00001755
Iteration 244/1000 | Loss: 0.00001755
Iteration 245/1000 | Loss: 0.00001755
Iteration 246/1000 | Loss: 0.00001755
Iteration 247/1000 | Loss: 0.00001755
Iteration 248/1000 | Loss: 0.00001755
Iteration 249/1000 | Loss: 0.00001755
Iteration 250/1000 | Loss: 0.00001755
Iteration 251/1000 | Loss: 0.00001755
Iteration 252/1000 | Loss: 0.00001755
Iteration 253/1000 | Loss: 0.00001755
Iteration 254/1000 | Loss: 0.00001755
Iteration 255/1000 | Loss: 0.00001755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.75526238308521e-05, 1.75526238308521e-05, 1.75526238308521e-05, 1.75526238308521e-05, 1.75526238308521e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.75526238308521e-05

Optimization complete. Final v2v error: 3.503115177154541 mm

Highest mean error: 4.0187554359436035 mm for frame 12

Lowest mean error: 3.164153575897217 mm for frame 114

Saving results

Total time: 51.24069094657898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00914995
Iteration 2/25 | Loss: 0.00132923
Iteration 3/25 | Loss: 0.00109609
Iteration 4/25 | Loss: 0.00106774
Iteration 5/25 | Loss: 0.00105921
Iteration 6/25 | Loss: 0.00105672
Iteration 7/25 | Loss: 0.00105650
Iteration 8/25 | Loss: 0.00105650
Iteration 9/25 | Loss: 0.00105650
Iteration 10/25 | Loss: 0.00105650
Iteration 11/25 | Loss: 0.00105650
Iteration 12/25 | Loss: 0.00105650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010564987314864993, 0.0010564987314864993, 0.0010564987314864993, 0.0010564987314864993, 0.0010564987314864993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010564987314864993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34222162
Iteration 2/25 | Loss: 0.00055276
Iteration 3/25 | Loss: 0.00055274
Iteration 4/25 | Loss: 0.00055274
Iteration 5/25 | Loss: 0.00055274
Iteration 6/25 | Loss: 0.00055274
Iteration 7/25 | Loss: 0.00055274
Iteration 8/25 | Loss: 0.00055274
Iteration 9/25 | Loss: 0.00055274
Iteration 10/25 | Loss: 0.00055274
Iteration 11/25 | Loss: 0.00055274
Iteration 12/25 | Loss: 0.00055274
Iteration 13/25 | Loss: 0.00055274
Iteration 14/25 | Loss: 0.00055274
Iteration 15/25 | Loss: 0.00055274
Iteration 16/25 | Loss: 0.00055274
Iteration 17/25 | Loss: 0.00055274
Iteration 18/25 | Loss: 0.00055274
Iteration 19/25 | Loss: 0.00055274
Iteration 20/25 | Loss: 0.00055274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005527358152903616, 0.0005527358152903616, 0.0005527358152903616, 0.0005527358152903616, 0.0005527358152903616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005527358152903616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055274
Iteration 2/1000 | Loss: 0.00003791
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002163
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001860
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001729
Iteration 13/1000 | Loss: 0.00001728
Iteration 14/1000 | Loss: 0.00001727
Iteration 15/1000 | Loss: 0.00001721
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001701
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001699
Iteration 22/1000 | Loss: 0.00001698
Iteration 23/1000 | Loss: 0.00001698
Iteration 24/1000 | Loss: 0.00001697
Iteration 25/1000 | Loss: 0.00001697
Iteration 26/1000 | Loss: 0.00001696
Iteration 27/1000 | Loss: 0.00001695
Iteration 28/1000 | Loss: 0.00001695
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001692
Iteration 33/1000 | Loss: 0.00001692
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001689
Iteration 36/1000 | Loss: 0.00001689
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001688
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001688
Iteration 43/1000 | Loss: 0.00001688
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001688
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001687
Iteration 51/1000 | Loss: 0.00001687
Iteration 52/1000 | Loss: 0.00001687
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001687
Iteration 58/1000 | Loss: 0.00001687
Iteration 59/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.6873109416337684e-05, 1.6873109416337684e-05, 1.6873109416337684e-05, 1.6873109416337684e-05, 1.6873109416337684e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6873109416337684e-05

Optimization complete. Final v2v error: 3.4426867961883545 mm

Highest mean error: 4.589722156524658 mm for frame 67

Lowest mean error: 2.8454082012176514 mm for frame 46

Saving results

Total time: 31.574331521987915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459673
Iteration 2/25 | Loss: 0.00105670
Iteration 3/25 | Loss: 0.00098037
Iteration 4/25 | Loss: 0.00096696
Iteration 5/25 | Loss: 0.00096335
Iteration 6/25 | Loss: 0.00096298
Iteration 7/25 | Loss: 0.00096298
Iteration 8/25 | Loss: 0.00096298
Iteration 9/25 | Loss: 0.00096298
Iteration 10/25 | Loss: 0.00096298
Iteration 11/25 | Loss: 0.00096298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009629841661080718, 0.0009629841661080718, 0.0009629841661080718, 0.0009629841661080718, 0.0009629841661080718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009629841661080718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11345184
Iteration 2/25 | Loss: 0.00042246
Iteration 3/25 | Loss: 0.00042246
Iteration 4/25 | Loss: 0.00042246
Iteration 5/25 | Loss: 0.00042246
Iteration 6/25 | Loss: 0.00042246
Iteration 7/25 | Loss: 0.00042246
Iteration 8/25 | Loss: 0.00042246
Iteration 9/25 | Loss: 0.00042246
Iteration 10/25 | Loss: 0.00042246
Iteration 11/25 | Loss: 0.00042246
Iteration 12/25 | Loss: 0.00042246
Iteration 13/25 | Loss: 0.00042246
Iteration 14/25 | Loss: 0.00042246
Iteration 15/25 | Loss: 0.00042246
Iteration 16/25 | Loss: 0.00042246
Iteration 17/25 | Loss: 0.00042246
Iteration 18/25 | Loss: 0.00042246
Iteration 19/25 | Loss: 0.00042246
Iteration 20/25 | Loss: 0.00042246
Iteration 21/25 | Loss: 0.00042246
Iteration 22/25 | Loss: 0.00042246
Iteration 23/25 | Loss: 0.00042246
Iteration 24/25 | Loss: 0.00042246
Iteration 25/25 | Loss: 0.00042246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042246
Iteration 2/1000 | Loss: 0.00004089
Iteration 3/1000 | Loss: 0.00002569
Iteration 4/1000 | Loss: 0.00002158
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001924
Iteration 7/1000 | Loss: 0.00001846
Iteration 8/1000 | Loss: 0.00001796
Iteration 9/1000 | Loss: 0.00001762
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001704
Iteration 12/1000 | Loss: 0.00001681
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001635
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001630
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001630
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001626
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00001618
Iteration 107/1000 | Loss: 0.00001618
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001617
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001617
Iteration 133/1000 | Loss: 0.00001617
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001616
Iteration 139/1000 | Loss: 0.00001616
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001615
Iteration 151/1000 | Loss: 0.00001615
Iteration 152/1000 | Loss: 0.00001615
Iteration 153/1000 | Loss: 0.00001615
Iteration 154/1000 | Loss: 0.00001615
Iteration 155/1000 | Loss: 0.00001615
Iteration 156/1000 | Loss: 0.00001615
Iteration 157/1000 | Loss: 0.00001615
Iteration 158/1000 | Loss: 0.00001615
Iteration 159/1000 | Loss: 0.00001615
Iteration 160/1000 | Loss: 0.00001615
Iteration 161/1000 | Loss: 0.00001615
Iteration 162/1000 | Loss: 0.00001615
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00001615
Iteration 165/1000 | Loss: 0.00001615
Iteration 166/1000 | Loss: 0.00001615
Iteration 167/1000 | Loss: 0.00001615
Iteration 168/1000 | Loss: 0.00001615
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00001615
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Iteration 184/1000 | Loss: 0.00001615
Iteration 185/1000 | Loss: 0.00001615
Iteration 186/1000 | Loss: 0.00001615
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001615
Iteration 192/1000 | Loss: 0.00001615
Iteration 193/1000 | Loss: 0.00001615
Iteration 194/1000 | Loss: 0.00001615
Iteration 195/1000 | Loss: 0.00001615
Iteration 196/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.615330620552413e-05, 1.615330620552413e-05, 1.615330620552413e-05, 1.615330620552413e-05, 1.615330620552413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.615330620552413e-05

Optimization complete. Final v2v error: 3.342052936553955 mm

Highest mean error: 3.365504026412964 mm for frame 25

Lowest mean error: 3.317348003387451 mm for frame 89

Saving results

Total time: 37.08613467216492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364367
Iteration 2/25 | Loss: 0.00115533
Iteration 3/25 | Loss: 0.00101128
Iteration 4/25 | Loss: 0.00097707
Iteration 5/25 | Loss: 0.00096969
Iteration 6/25 | Loss: 0.00096660
Iteration 7/25 | Loss: 0.00096638
Iteration 8/25 | Loss: 0.00096638
Iteration 9/25 | Loss: 0.00096638
Iteration 10/25 | Loss: 0.00096638
Iteration 11/25 | Loss: 0.00096638
Iteration 12/25 | Loss: 0.00096638
Iteration 13/25 | Loss: 0.00096638
Iteration 14/25 | Loss: 0.00096638
Iteration 15/25 | Loss: 0.00096638
Iteration 16/25 | Loss: 0.00096638
Iteration 17/25 | Loss: 0.00096638
Iteration 18/25 | Loss: 0.00096638
Iteration 19/25 | Loss: 0.00096638
Iteration 20/25 | Loss: 0.00096638
Iteration 21/25 | Loss: 0.00096638
Iteration 22/25 | Loss: 0.00096638
Iteration 23/25 | Loss: 0.00096638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009663759847171605, 0.0009663759847171605, 0.0009663759847171605, 0.0009663759847171605, 0.0009663759847171605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009663759847171605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35656202
Iteration 2/25 | Loss: 0.00056160
Iteration 3/25 | Loss: 0.00056159
Iteration 4/25 | Loss: 0.00056159
Iteration 5/25 | Loss: 0.00056159
Iteration 6/25 | Loss: 0.00056159
Iteration 7/25 | Loss: 0.00056159
Iteration 8/25 | Loss: 0.00056159
Iteration 9/25 | Loss: 0.00056159
Iteration 10/25 | Loss: 0.00056159
Iteration 11/25 | Loss: 0.00056159
Iteration 12/25 | Loss: 0.00056159
Iteration 13/25 | Loss: 0.00056159
Iteration 14/25 | Loss: 0.00056159
Iteration 15/25 | Loss: 0.00056159
Iteration 16/25 | Loss: 0.00056159
Iteration 17/25 | Loss: 0.00056159
Iteration 18/25 | Loss: 0.00056159
Iteration 19/25 | Loss: 0.00056159
Iteration 20/25 | Loss: 0.00056159
Iteration 21/25 | Loss: 0.00056159
Iteration 22/25 | Loss: 0.00056159
Iteration 23/25 | Loss: 0.00056159
Iteration 24/25 | Loss: 0.00056159
Iteration 25/25 | Loss: 0.00056159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056159
Iteration 2/1000 | Loss: 0.00002436
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001485
Iteration 5/1000 | Loss: 0.00001371
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001178
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001163
Iteration 26/1000 | Loss: 0.00001163
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001157
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001153
Iteration 45/1000 | Loss: 0.00001152
Iteration 46/1000 | Loss: 0.00001152
Iteration 47/1000 | Loss: 0.00001152
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001151
Iteration 50/1000 | Loss: 0.00001151
Iteration 51/1000 | Loss: 0.00001151
Iteration 52/1000 | Loss: 0.00001151
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001150
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001148
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001145
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001142
Iteration 74/1000 | Loss: 0.00001142
Iteration 75/1000 | Loss: 0.00001142
Iteration 76/1000 | Loss: 0.00001141
Iteration 77/1000 | Loss: 0.00001141
Iteration 78/1000 | Loss: 0.00001141
Iteration 79/1000 | Loss: 0.00001141
Iteration 80/1000 | Loss: 0.00001141
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001140
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001138
Iteration 87/1000 | Loss: 0.00001138
Iteration 88/1000 | Loss: 0.00001138
Iteration 89/1000 | Loss: 0.00001138
Iteration 90/1000 | Loss: 0.00001138
Iteration 91/1000 | Loss: 0.00001138
Iteration 92/1000 | Loss: 0.00001137
Iteration 93/1000 | Loss: 0.00001137
Iteration 94/1000 | Loss: 0.00001137
Iteration 95/1000 | Loss: 0.00001137
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001137
Iteration 98/1000 | Loss: 0.00001137
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001136
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001136
Iteration 104/1000 | Loss: 0.00001136
Iteration 105/1000 | Loss: 0.00001136
Iteration 106/1000 | Loss: 0.00001136
Iteration 107/1000 | Loss: 0.00001136
Iteration 108/1000 | Loss: 0.00001136
Iteration 109/1000 | Loss: 0.00001136
Iteration 110/1000 | Loss: 0.00001135
Iteration 111/1000 | Loss: 0.00001135
Iteration 112/1000 | Loss: 0.00001135
Iteration 113/1000 | Loss: 0.00001135
Iteration 114/1000 | Loss: 0.00001135
Iteration 115/1000 | Loss: 0.00001135
Iteration 116/1000 | Loss: 0.00001135
Iteration 117/1000 | Loss: 0.00001134
Iteration 118/1000 | Loss: 0.00001134
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001133
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.1281070328550413e-05, 1.1281070328550413e-05, 1.1281070328550413e-05, 1.1281070328550413e-05, 1.1281070328550413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1281070328550413e-05

Optimization complete. Final v2v error: 2.8002207279205322 mm

Highest mean error: 3.0216381549835205 mm for frame 144

Lowest mean error: 2.3420181274414062 mm for frame 2

Saving results

Total time: 42.59819769859314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418464
Iteration 2/25 | Loss: 0.00112484
Iteration 3/25 | Loss: 0.00099803
Iteration 4/25 | Loss: 0.00099461
Iteration 5/25 | Loss: 0.00099362
Iteration 6/25 | Loss: 0.00099362
Iteration 7/25 | Loss: 0.00099362
Iteration 8/25 | Loss: 0.00099362
Iteration 9/25 | Loss: 0.00099362
Iteration 10/25 | Loss: 0.00099362
Iteration 11/25 | Loss: 0.00099362
Iteration 12/25 | Loss: 0.00099362
Iteration 13/25 | Loss: 0.00099362
Iteration 14/25 | Loss: 0.00099362
Iteration 15/25 | Loss: 0.00099362
Iteration 16/25 | Loss: 0.00099362
Iteration 17/25 | Loss: 0.00099362
Iteration 18/25 | Loss: 0.00099362
Iteration 19/25 | Loss: 0.00099362
Iteration 20/25 | Loss: 0.00099362
Iteration 21/25 | Loss: 0.00099362
Iteration 22/25 | Loss: 0.00099362
Iteration 23/25 | Loss: 0.00099362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009936188580468297, 0.0009936188580468297, 0.0009936188580468297, 0.0009936188580468297, 0.0009936188580468297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009936188580468297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18637097
Iteration 2/25 | Loss: 0.00035617
Iteration 3/25 | Loss: 0.00035617
Iteration 4/25 | Loss: 0.00035617
Iteration 5/25 | Loss: 0.00035617
Iteration 6/25 | Loss: 0.00035617
Iteration 7/25 | Loss: 0.00035617
Iteration 8/25 | Loss: 0.00035616
Iteration 9/25 | Loss: 0.00035616
Iteration 10/25 | Loss: 0.00035616
Iteration 11/25 | Loss: 0.00035616
Iteration 12/25 | Loss: 0.00035616
Iteration 13/25 | Loss: 0.00035616
Iteration 14/25 | Loss: 0.00035616
Iteration 15/25 | Loss: 0.00035616
Iteration 16/25 | Loss: 0.00035616
Iteration 17/25 | Loss: 0.00035616
Iteration 18/25 | Loss: 0.00035616
Iteration 19/25 | Loss: 0.00035616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003561644989531487, 0.0003561644989531487, 0.0003561644989531487, 0.0003561644989531487, 0.0003561644989531487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003561644989531487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035616
Iteration 2/1000 | Loss: 0.00002439
Iteration 3/1000 | Loss: 0.00001531
Iteration 4/1000 | Loss: 0.00001244
Iteration 5/1000 | Loss: 0.00001148
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001027
Iteration 8/1000 | Loss: 0.00000994
Iteration 9/1000 | Loss: 0.00000958
Iteration 10/1000 | Loss: 0.00000954
Iteration 11/1000 | Loss: 0.00000953
Iteration 12/1000 | Loss: 0.00000953
Iteration 13/1000 | Loss: 0.00000952
Iteration 14/1000 | Loss: 0.00000947
Iteration 15/1000 | Loss: 0.00000930
Iteration 16/1000 | Loss: 0.00000926
Iteration 17/1000 | Loss: 0.00000922
Iteration 18/1000 | Loss: 0.00000916
Iteration 19/1000 | Loss: 0.00000916
Iteration 20/1000 | Loss: 0.00000916
Iteration 21/1000 | Loss: 0.00000916
Iteration 22/1000 | Loss: 0.00000913
Iteration 23/1000 | Loss: 0.00000903
Iteration 24/1000 | Loss: 0.00000903
Iteration 25/1000 | Loss: 0.00000903
Iteration 26/1000 | Loss: 0.00000903
Iteration 27/1000 | Loss: 0.00000903
Iteration 28/1000 | Loss: 0.00000903
Iteration 29/1000 | Loss: 0.00000903
Iteration 30/1000 | Loss: 0.00000903
Iteration 31/1000 | Loss: 0.00000903
Iteration 32/1000 | Loss: 0.00000903
Iteration 33/1000 | Loss: 0.00000902
Iteration 34/1000 | Loss: 0.00000902
Iteration 35/1000 | Loss: 0.00000902
Iteration 36/1000 | Loss: 0.00000902
Iteration 37/1000 | Loss: 0.00000902
Iteration 38/1000 | Loss: 0.00000902
Iteration 39/1000 | Loss: 0.00000902
Iteration 40/1000 | Loss: 0.00000902
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000901
Iteration 43/1000 | Loss: 0.00000901
Iteration 44/1000 | Loss: 0.00000900
Iteration 45/1000 | Loss: 0.00000900
Iteration 46/1000 | Loss: 0.00000900
Iteration 47/1000 | Loss: 0.00000900
Iteration 48/1000 | Loss: 0.00000900
Iteration 49/1000 | Loss: 0.00000900
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000899
Iteration 52/1000 | Loss: 0.00000899
Iteration 53/1000 | Loss: 0.00000899
Iteration 54/1000 | Loss: 0.00000899
Iteration 55/1000 | Loss: 0.00000898
Iteration 56/1000 | Loss: 0.00000898
Iteration 57/1000 | Loss: 0.00000898
Iteration 58/1000 | Loss: 0.00000898
Iteration 59/1000 | Loss: 0.00000898
Iteration 60/1000 | Loss: 0.00000898
Iteration 61/1000 | Loss: 0.00000898
Iteration 62/1000 | Loss: 0.00000897
Iteration 63/1000 | Loss: 0.00000897
Iteration 64/1000 | Loss: 0.00000897
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000896
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000896
Iteration 70/1000 | Loss: 0.00000896
Iteration 71/1000 | Loss: 0.00000896
Iteration 72/1000 | Loss: 0.00000896
Iteration 73/1000 | Loss: 0.00000896
Iteration 74/1000 | Loss: 0.00000896
Iteration 75/1000 | Loss: 0.00000896
Iteration 76/1000 | Loss: 0.00000896
Iteration 77/1000 | Loss: 0.00000896
Iteration 78/1000 | Loss: 0.00000896
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000896
Iteration 81/1000 | Loss: 0.00000896
Iteration 82/1000 | Loss: 0.00000895
Iteration 83/1000 | Loss: 0.00000895
Iteration 84/1000 | Loss: 0.00000895
Iteration 85/1000 | Loss: 0.00000895
Iteration 86/1000 | Loss: 0.00000895
Iteration 87/1000 | Loss: 0.00000895
Iteration 88/1000 | Loss: 0.00000895
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000895
Iteration 91/1000 | Loss: 0.00000895
Iteration 92/1000 | Loss: 0.00000895
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000894
Iteration 101/1000 | Loss: 0.00000894
Iteration 102/1000 | Loss: 0.00000894
Iteration 103/1000 | Loss: 0.00000894
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000893
Iteration 108/1000 | Loss: 0.00000893
Iteration 109/1000 | Loss: 0.00000893
Iteration 110/1000 | Loss: 0.00000893
Iteration 111/1000 | Loss: 0.00000893
Iteration 112/1000 | Loss: 0.00000893
Iteration 113/1000 | Loss: 0.00000893
Iteration 114/1000 | Loss: 0.00000893
Iteration 115/1000 | Loss: 0.00000893
Iteration 116/1000 | Loss: 0.00000893
Iteration 117/1000 | Loss: 0.00000893
Iteration 118/1000 | Loss: 0.00000893
Iteration 119/1000 | Loss: 0.00000893
Iteration 120/1000 | Loss: 0.00000893
Iteration 121/1000 | Loss: 0.00000893
Iteration 122/1000 | Loss: 0.00000893
Iteration 123/1000 | Loss: 0.00000893
Iteration 124/1000 | Loss: 0.00000893
Iteration 125/1000 | Loss: 0.00000893
Iteration 126/1000 | Loss: 0.00000893
Iteration 127/1000 | Loss: 0.00000893
Iteration 128/1000 | Loss: 0.00000893
Iteration 129/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [8.926460395741742e-06, 8.926460395741742e-06, 8.926460395741742e-06, 8.926460395741742e-06, 8.926460395741742e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.926460395741742e-06

Optimization complete. Final v2v error: 2.577669620513916 mm

Highest mean error: 2.6349728107452393 mm for frame 7

Lowest mean error: 2.5492236614227295 mm for frame 29

Saving results

Total time: 30.352120876312256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981630
Iteration 2/25 | Loss: 0.00158009
Iteration 3/25 | Loss: 0.00119427
Iteration 4/25 | Loss: 0.00113252
Iteration 5/25 | Loss: 0.00113550
Iteration 6/25 | Loss: 0.00110716
Iteration 7/25 | Loss: 0.00105498
Iteration 8/25 | Loss: 0.00105105
Iteration 9/25 | Loss: 0.00104270
Iteration 10/25 | Loss: 0.00104264
Iteration 11/25 | Loss: 0.00103557
Iteration 12/25 | Loss: 0.00102776
Iteration 13/25 | Loss: 0.00102416
Iteration 14/25 | Loss: 0.00102169
Iteration 15/25 | Loss: 0.00101393
Iteration 16/25 | Loss: 0.00100774
Iteration 17/25 | Loss: 0.00100442
Iteration 18/25 | Loss: 0.00100485
Iteration 19/25 | Loss: 0.00100143
Iteration 20/25 | Loss: 0.00100098
Iteration 21/25 | Loss: 0.00100083
Iteration 22/25 | Loss: 0.00100072
Iteration 23/25 | Loss: 0.00100071
Iteration 24/25 | Loss: 0.00100071
Iteration 25/25 | Loss: 0.00100071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48513508
Iteration 2/25 | Loss: 0.00090456
Iteration 3/25 | Loss: 0.00090456
Iteration 4/25 | Loss: 0.00090456
Iteration 5/25 | Loss: 0.00090456
Iteration 6/25 | Loss: 0.00090456
Iteration 7/25 | Loss: 0.00090456
Iteration 8/25 | Loss: 0.00090456
Iteration 9/25 | Loss: 0.00090456
Iteration 10/25 | Loss: 0.00090456
Iteration 11/25 | Loss: 0.00090456
Iteration 12/25 | Loss: 0.00090456
Iteration 13/25 | Loss: 0.00090456
Iteration 14/25 | Loss: 0.00090456
Iteration 15/25 | Loss: 0.00090456
Iteration 16/25 | Loss: 0.00090456
Iteration 17/25 | Loss: 0.00090456
Iteration 18/25 | Loss: 0.00090456
Iteration 19/25 | Loss: 0.00090456
Iteration 20/25 | Loss: 0.00090456
Iteration 21/25 | Loss: 0.00090456
Iteration 22/25 | Loss: 0.00090456
Iteration 23/25 | Loss: 0.00090456
Iteration 24/25 | Loss: 0.00090456
Iteration 25/25 | Loss: 0.00090456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090456
Iteration 2/1000 | Loss: 0.00005196
Iteration 3/1000 | Loss: 0.00006568
Iteration 4/1000 | Loss: 0.00006657
Iteration 5/1000 | Loss: 0.00003128
Iteration 6/1000 | Loss: 0.00002470
Iteration 7/1000 | Loss: 0.00002048
Iteration 8/1000 | Loss: 0.00007809
Iteration 9/1000 | Loss: 0.00004878
Iteration 10/1000 | Loss: 0.00007051
Iteration 11/1000 | Loss: 0.00015654
Iteration 12/1000 | Loss: 0.00004471
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00016543
Iteration 15/1000 | Loss: 0.00002683
Iteration 16/1000 | Loss: 0.00003208
Iteration 17/1000 | Loss: 0.00001836
Iteration 18/1000 | Loss: 0.00001818
Iteration 19/1000 | Loss: 0.00006876
Iteration 20/1000 | Loss: 0.00002810
Iteration 21/1000 | Loss: 0.00001809
Iteration 22/1000 | Loss: 0.00003642
Iteration 23/1000 | Loss: 0.00001791
Iteration 24/1000 | Loss: 0.00003796
Iteration 25/1000 | Loss: 0.00001909
Iteration 26/1000 | Loss: 0.00003065
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00010233
Iteration 29/1000 | Loss: 0.00005832
Iteration 30/1000 | Loss: 0.00003757
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00010054
Iteration 34/1000 | Loss: 0.00014235
Iteration 35/1000 | Loss: 0.00006772
Iteration 36/1000 | Loss: 0.00008219
Iteration 37/1000 | Loss: 0.00004252
Iteration 38/1000 | Loss: 0.00006433
Iteration 39/1000 | Loss: 0.00001438
Iteration 40/1000 | Loss: 0.00001319
Iteration 41/1000 | Loss: 0.00001260
Iteration 42/1000 | Loss: 0.00003950
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00006874
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00004876
Iteration 51/1000 | Loss: 0.00001137
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001070
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001070
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001066
Iteration 80/1000 | Loss: 0.00001066
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001063
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001061
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00004274
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001624
Iteration 98/1000 | Loss: 0.00001058
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001054
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001053
Iteration 122/1000 | Loss: 0.00001053
Iteration 123/1000 | Loss: 0.00001053
Iteration 124/1000 | Loss: 0.00001053
Iteration 125/1000 | Loss: 0.00003430
Iteration 126/1000 | Loss: 0.00001082
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001048
Iteration 130/1000 | Loss: 0.00001048
Iteration 131/1000 | Loss: 0.00001048
Iteration 132/1000 | Loss: 0.00001047
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001047
Iteration 136/1000 | Loss: 0.00001047
Iteration 137/1000 | Loss: 0.00001047
Iteration 138/1000 | Loss: 0.00001047
Iteration 139/1000 | Loss: 0.00001047
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001047
Iteration 143/1000 | Loss: 0.00001047
Iteration 144/1000 | Loss: 0.00001047
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001046
Iteration 147/1000 | Loss: 0.00001046
Iteration 148/1000 | Loss: 0.00001046
Iteration 149/1000 | Loss: 0.00001046
Iteration 150/1000 | Loss: 0.00001046
Iteration 151/1000 | Loss: 0.00001046
Iteration 152/1000 | Loss: 0.00001046
Iteration 153/1000 | Loss: 0.00001046
Iteration 154/1000 | Loss: 0.00001046
Iteration 155/1000 | Loss: 0.00001046
Iteration 156/1000 | Loss: 0.00001046
Iteration 157/1000 | Loss: 0.00001046
Iteration 158/1000 | Loss: 0.00001046
Iteration 159/1000 | Loss: 0.00001046
Iteration 160/1000 | Loss: 0.00001046
Iteration 161/1000 | Loss: 0.00001046
Iteration 162/1000 | Loss: 0.00001046
Iteration 163/1000 | Loss: 0.00001046
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001046
Iteration 167/1000 | Loss: 0.00001046
Iteration 168/1000 | Loss: 0.00001046
Iteration 169/1000 | Loss: 0.00001046
Iteration 170/1000 | Loss: 0.00001046
Iteration 171/1000 | Loss: 0.00001046
Iteration 172/1000 | Loss: 0.00001046
Iteration 173/1000 | Loss: 0.00001046
Iteration 174/1000 | Loss: 0.00001046
Iteration 175/1000 | Loss: 0.00001046
Iteration 176/1000 | Loss: 0.00001046
Iteration 177/1000 | Loss: 0.00001046
Iteration 178/1000 | Loss: 0.00001046
Iteration 179/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.0455652954988182e-05, 1.0455652954988182e-05, 1.0455652954988182e-05, 1.0455652954988182e-05, 1.0455652954988182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0455652954988182e-05

Optimization complete. Final v2v error: 2.720581531524658 mm

Highest mean error: 4.490250587463379 mm for frame 58

Lowest mean error: 2.121020793914795 mm for frame 97

Saving results

Total time: 124.74213576316833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001958
Iteration 2/25 | Loss: 0.00198032
Iteration 3/25 | Loss: 0.00147881
Iteration 4/25 | Loss: 0.00135179
Iteration 5/25 | Loss: 0.00132434
Iteration 6/25 | Loss: 0.00133709
Iteration 7/25 | Loss: 0.00132348
Iteration 8/25 | Loss: 0.00125999
Iteration 9/25 | Loss: 0.00123239
Iteration 10/25 | Loss: 0.00122731
Iteration 11/25 | Loss: 0.00121427
Iteration 12/25 | Loss: 0.00121867
Iteration 13/25 | Loss: 0.00121116
Iteration 14/25 | Loss: 0.00121302
Iteration 15/25 | Loss: 0.00121160
Iteration 16/25 | Loss: 0.00121052
Iteration 17/25 | Loss: 0.00120893
Iteration 18/25 | Loss: 0.00121044
Iteration 19/25 | Loss: 0.00121178
Iteration 20/25 | Loss: 0.00121157
Iteration 21/25 | Loss: 0.00121001
Iteration 22/25 | Loss: 0.00121080
Iteration 23/25 | Loss: 0.00121119
Iteration 24/25 | Loss: 0.00120934
Iteration 25/25 | Loss: 0.00121472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37081718
Iteration 2/25 | Loss: 0.00065339
Iteration 3/25 | Loss: 0.00065339
Iteration 4/25 | Loss: 0.00065339
Iteration 5/25 | Loss: 0.00065339
Iteration 6/25 | Loss: 0.00065339
Iteration 7/25 | Loss: 0.00065339
Iteration 8/25 | Loss: 0.00065339
Iteration 9/25 | Loss: 0.00065339
Iteration 10/25 | Loss: 0.00065339
Iteration 11/25 | Loss: 0.00065339
Iteration 12/25 | Loss: 0.00065339
Iteration 13/25 | Loss: 0.00065339
Iteration 14/25 | Loss: 0.00065339
Iteration 15/25 | Loss: 0.00065339
Iteration 16/25 | Loss: 0.00065339
Iteration 17/25 | Loss: 0.00065339
Iteration 18/25 | Loss: 0.00065339
Iteration 19/25 | Loss: 0.00065339
Iteration 20/25 | Loss: 0.00065339
Iteration 21/25 | Loss: 0.00065339
Iteration 22/25 | Loss: 0.00065339
Iteration 23/25 | Loss: 0.00065339
Iteration 24/25 | Loss: 0.00065339
Iteration 25/25 | Loss: 0.00065339

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065339
Iteration 2/1000 | Loss: 0.00010325
Iteration 3/1000 | Loss: 0.00007901
Iteration 4/1000 | Loss: 0.00009136
Iteration 5/1000 | Loss: 0.00074786
Iteration 6/1000 | Loss: 0.00010466
Iteration 7/1000 | Loss: 0.00008207
Iteration 8/1000 | Loss: 0.00030085
Iteration 9/1000 | Loss: 0.00004316
Iteration 10/1000 | Loss: 0.00005450
Iteration 11/1000 | Loss: 0.00005480
Iteration 12/1000 | Loss: 0.00005012
Iteration 13/1000 | Loss: 0.00017032
Iteration 14/1000 | Loss: 0.00014897
Iteration 15/1000 | Loss: 0.00011892
Iteration 16/1000 | Loss: 0.00003621
Iteration 17/1000 | Loss: 0.00008847
Iteration 18/1000 | Loss: 0.00037280
Iteration 19/1000 | Loss: 0.00009917
Iteration 20/1000 | Loss: 0.00005243
Iteration 21/1000 | Loss: 0.00003126
Iteration 22/1000 | Loss: 0.00002998
Iteration 23/1000 | Loss: 0.00002914
Iteration 24/1000 | Loss: 0.00002857
Iteration 25/1000 | Loss: 0.00002821
Iteration 26/1000 | Loss: 0.00002795
Iteration 27/1000 | Loss: 0.00002790
Iteration 28/1000 | Loss: 0.00002777
Iteration 29/1000 | Loss: 0.00002775
Iteration 30/1000 | Loss: 0.00002752
Iteration 31/1000 | Loss: 0.00002733
Iteration 32/1000 | Loss: 0.00002732
Iteration 33/1000 | Loss: 0.00002710
Iteration 34/1000 | Loss: 0.00002707
Iteration 35/1000 | Loss: 0.00002705
Iteration 36/1000 | Loss: 0.00002705
Iteration 37/1000 | Loss: 0.00002704
Iteration 38/1000 | Loss: 0.00002704
Iteration 39/1000 | Loss: 0.00002695
Iteration 40/1000 | Loss: 0.00002695
Iteration 41/1000 | Loss: 0.00002694
Iteration 42/1000 | Loss: 0.00002693
Iteration 43/1000 | Loss: 0.00002692
Iteration 44/1000 | Loss: 0.00002692
Iteration 45/1000 | Loss: 0.00002691
Iteration 46/1000 | Loss: 0.00002690
Iteration 47/1000 | Loss: 0.00002690
Iteration 48/1000 | Loss: 0.00002689
Iteration 49/1000 | Loss: 0.00002689
Iteration 50/1000 | Loss: 0.00002689
Iteration 51/1000 | Loss: 0.00002689
Iteration 52/1000 | Loss: 0.00002688
Iteration 53/1000 | Loss: 0.00002688
Iteration 54/1000 | Loss: 0.00002687
Iteration 55/1000 | Loss: 0.00002687
Iteration 56/1000 | Loss: 0.00002684
Iteration 57/1000 | Loss: 0.00002684
Iteration 58/1000 | Loss: 0.00002684
Iteration 59/1000 | Loss: 0.00002682
Iteration 60/1000 | Loss: 0.00002682
Iteration 61/1000 | Loss: 0.00002682
Iteration 62/1000 | Loss: 0.00002681
Iteration 63/1000 | Loss: 0.00002681
Iteration 64/1000 | Loss: 0.00002681
Iteration 65/1000 | Loss: 0.00002681
Iteration 66/1000 | Loss: 0.00002680
Iteration 67/1000 | Loss: 0.00002680
Iteration 68/1000 | Loss: 0.00002680
Iteration 69/1000 | Loss: 0.00002679
Iteration 70/1000 | Loss: 0.00002677
Iteration 71/1000 | Loss: 0.00002677
Iteration 72/1000 | Loss: 0.00002676
Iteration 73/1000 | Loss: 0.00002676
Iteration 74/1000 | Loss: 0.00002676
Iteration 75/1000 | Loss: 0.00002676
Iteration 76/1000 | Loss: 0.00002676
Iteration 77/1000 | Loss: 0.00002675
Iteration 78/1000 | Loss: 0.00002675
Iteration 79/1000 | Loss: 0.00002673
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002671
Iteration 82/1000 | Loss: 0.00002671
Iteration 83/1000 | Loss: 0.00002670
Iteration 84/1000 | Loss: 0.00019222
Iteration 85/1000 | Loss: 0.00002958
Iteration 86/1000 | Loss: 0.00002793
Iteration 87/1000 | Loss: 0.00002708
Iteration 88/1000 | Loss: 0.00002625
Iteration 89/1000 | Loss: 0.00002572
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002541
Iteration 92/1000 | Loss: 0.00002540
Iteration 93/1000 | Loss: 0.00002535
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002514
Iteration 96/1000 | Loss: 0.00002514
Iteration 97/1000 | Loss: 0.00002514
Iteration 98/1000 | Loss: 0.00002514
Iteration 99/1000 | Loss: 0.00002514
Iteration 100/1000 | Loss: 0.00002513
Iteration 101/1000 | Loss: 0.00002513
Iteration 102/1000 | Loss: 0.00002512
Iteration 103/1000 | Loss: 0.00002511
Iteration 104/1000 | Loss: 0.00002510
Iteration 105/1000 | Loss: 0.00002509
Iteration 106/1000 | Loss: 0.00002509
Iteration 107/1000 | Loss: 0.00002508
Iteration 108/1000 | Loss: 0.00002508
Iteration 109/1000 | Loss: 0.00002508
Iteration 110/1000 | Loss: 0.00002508
Iteration 111/1000 | Loss: 0.00002508
Iteration 112/1000 | Loss: 0.00002508
Iteration 113/1000 | Loss: 0.00002508
Iteration 114/1000 | Loss: 0.00002508
Iteration 115/1000 | Loss: 0.00002508
Iteration 116/1000 | Loss: 0.00002508
Iteration 117/1000 | Loss: 0.00002508
Iteration 118/1000 | Loss: 0.00002507
Iteration 119/1000 | Loss: 0.00002507
Iteration 120/1000 | Loss: 0.00002506
Iteration 121/1000 | Loss: 0.00002506
Iteration 122/1000 | Loss: 0.00002506
Iteration 123/1000 | Loss: 0.00002506
Iteration 124/1000 | Loss: 0.00002505
Iteration 125/1000 | Loss: 0.00002505
Iteration 126/1000 | Loss: 0.00002505
Iteration 127/1000 | Loss: 0.00002505
Iteration 128/1000 | Loss: 0.00002505
Iteration 129/1000 | Loss: 0.00002505
Iteration 130/1000 | Loss: 0.00002505
Iteration 131/1000 | Loss: 0.00002505
Iteration 132/1000 | Loss: 0.00002504
Iteration 133/1000 | Loss: 0.00002504
Iteration 134/1000 | Loss: 0.00002504
Iteration 135/1000 | Loss: 0.00002504
Iteration 136/1000 | Loss: 0.00002503
Iteration 137/1000 | Loss: 0.00002503
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002503
Iteration 140/1000 | Loss: 0.00002502
Iteration 141/1000 | Loss: 0.00002502
Iteration 142/1000 | Loss: 0.00002502
Iteration 143/1000 | Loss: 0.00002502
Iteration 144/1000 | Loss: 0.00002501
Iteration 145/1000 | Loss: 0.00002501
Iteration 146/1000 | Loss: 0.00002501
Iteration 147/1000 | Loss: 0.00002501
Iteration 148/1000 | Loss: 0.00002501
Iteration 149/1000 | Loss: 0.00002501
Iteration 150/1000 | Loss: 0.00002501
Iteration 151/1000 | Loss: 0.00002501
Iteration 152/1000 | Loss: 0.00002501
Iteration 153/1000 | Loss: 0.00002501
Iteration 154/1000 | Loss: 0.00002501
Iteration 155/1000 | Loss: 0.00002501
Iteration 156/1000 | Loss: 0.00002501
Iteration 157/1000 | Loss: 0.00002500
Iteration 158/1000 | Loss: 0.00002500
Iteration 159/1000 | Loss: 0.00002500
Iteration 160/1000 | Loss: 0.00002500
Iteration 161/1000 | Loss: 0.00002499
Iteration 162/1000 | Loss: 0.00002499
Iteration 163/1000 | Loss: 0.00002499
Iteration 164/1000 | Loss: 0.00002499
Iteration 165/1000 | Loss: 0.00002499
Iteration 166/1000 | Loss: 0.00002499
Iteration 167/1000 | Loss: 0.00002499
Iteration 168/1000 | Loss: 0.00002499
Iteration 169/1000 | Loss: 0.00002499
Iteration 170/1000 | Loss: 0.00002499
Iteration 171/1000 | Loss: 0.00002499
Iteration 172/1000 | Loss: 0.00002498
Iteration 173/1000 | Loss: 0.00002498
Iteration 174/1000 | Loss: 0.00002498
Iteration 175/1000 | Loss: 0.00002498
Iteration 176/1000 | Loss: 0.00002498
Iteration 177/1000 | Loss: 0.00002498
Iteration 178/1000 | Loss: 0.00002498
Iteration 179/1000 | Loss: 0.00002498
Iteration 180/1000 | Loss: 0.00002497
Iteration 181/1000 | Loss: 0.00002497
Iteration 182/1000 | Loss: 0.00002497
Iteration 183/1000 | Loss: 0.00002497
Iteration 184/1000 | Loss: 0.00002497
Iteration 185/1000 | Loss: 0.00002497
Iteration 186/1000 | Loss: 0.00002497
Iteration 187/1000 | Loss: 0.00002496
Iteration 188/1000 | Loss: 0.00002496
Iteration 189/1000 | Loss: 0.00002496
Iteration 190/1000 | Loss: 0.00002496
Iteration 191/1000 | Loss: 0.00002496
Iteration 192/1000 | Loss: 0.00002496
Iteration 193/1000 | Loss: 0.00002496
Iteration 194/1000 | Loss: 0.00002496
Iteration 195/1000 | Loss: 0.00002496
Iteration 196/1000 | Loss: 0.00002496
Iteration 197/1000 | Loss: 0.00002496
Iteration 198/1000 | Loss: 0.00002496
Iteration 199/1000 | Loss: 0.00002496
Iteration 200/1000 | Loss: 0.00002496
Iteration 201/1000 | Loss: 0.00002496
Iteration 202/1000 | Loss: 0.00002496
Iteration 203/1000 | Loss: 0.00002496
Iteration 204/1000 | Loss: 0.00002496
Iteration 205/1000 | Loss: 0.00002496
Iteration 206/1000 | Loss: 0.00002496
Iteration 207/1000 | Loss: 0.00002496
Iteration 208/1000 | Loss: 0.00002496
Iteration 209/1000 | Loss: 0.00002496
Iteration 210/1000 | Loss: 0.00002496
Iteration 211/1000 | Loss: 0.00002496
Iteration 212/1000 | Loss: 0.00002496
Iteration 213/1000 | Loss: 0.00002496
Iteration 214/1000 | Loss: 0.00002496
Iteration 215/1000 | Loss: 0.00002496
Iteration 216/1000 | Loss: 0.00002496
Iteration 217/1000 | Loss: 0.00002496
Iteration 218/1000 | Loss: 0.00002496
Iteration 219/1000 | Loss: 0.00002496
Iteration 220/1000 | Loss: 0.00002496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.4958013455034234e-05, 2.4958013455034234e-05, 2.4958013455034234e-05, 2.4958013455034234e-05, 2.4958013455034234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4958013455034234e-05

Optimization complete. Final v2v error: 4.179308891296387 mm

Highest mean error: 5.045354843139648 mm for frame 220

Lowest mean error: 3.6796398162841797 mm for frame 204

Saving results

Total time: 131.76203751564026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520960
Iteration 2/25 | Loss: 0.00130203
Iteration 3/25 | Loss: 0.00107270
Iteration 4/25 | Loss: 0.00105130
Iteration 5/25 | Loss: 0.00104712
Iteration 6/25 | Loss: 0.00104602
Iteration 7/25 | Loss: 0.00104596
Iteration 8/25 | Loss: 0.00104596
Iteration 9/25 | Loss: 0.00104596
Iteration 10/25 | Loss: 0.00104596
Iteration 11/25 | Loss: 0.00104596
Iteration 12/25 | Loss: 0.00104596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010459567420184612, 0.0010459567420184612, 0.0010459567420184612, 0.0010459567420184612, 0.0010459567420184612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010459567420184612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36663806
Iteration 2/25 | Loss: 0.00052386
Iteration 3/25 | Loss: 0.00052384
Iteration 4/25 | Loss: 0.00052384
Iteration 5/25 | Loss: 0.00052384
Iteration 6/25 | Loss: 0.00052384
Iteration 7/25 | Loss: 0.00052384
Iteration 8/25 | Loss: 0.00052383
Iteration 9/25 | Loss: 0.00052383
Iteration 10/25 | Loss: 0.00052383
Iteration 11/25 | Loss: 0.00052383
Iteration 12/25 | Loss: 0.00052383
Iteration 13/25 | Loss: 0.00052383
Iteration 14/25 | Loss: 0.00052383
Iteration 15/25 | Loss: 0.00052383
Iteration 16/25 | Loss: 0.00052383
Iteration 17/25 | Loss: 0.00052383
Iteration 18/25 | Loss: 0.00052383
Iteration 19/25 | Loss: 0.00052383
Iteration 20/25 | Loss: 0.00052383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005238342564553022, 0.0005238342564553022, 0.0005238342564553022, 0.0005238342564553022, 0.0005238342564553022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005238342564553022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052383
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00002091
Iteration 4/1000 | Loss: 0.00001676
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001458
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001421
Iteration 10/1000 | Loss: 0.00001403
Iteration 11/1000 | Loss: 0.00001397
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001386
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001367
Iteration 18/1000 | Loss: 0.00001366
Iteration 19/1000 | Loss: 0.00001366
Iteration 20/1000 | Loss: 0.00001364
Iteration 21/1000 | Loss: 0.00001364
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001360
Iteration 26/1000 | Loss: 0.00001360
Iteration 27/1000 | Loss: 0.00001360
Iteration 28/1000 | Loss: 0.00001360
Iteration 29/1000 | Loss: 0.00001360
Iteration 30/1000 | Loss: 0.00001360
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001357
Iteration 38/1000 | Loss: 0.00001356
Iteration 39/1000 | Loss: 0.00001356
Iteration 40/1000 | Loss: 0.00001356
Iteration 41/1000 | Loss: 0.00001355
Iteration 42/1000 | Loss: 0.00001355
Iteration 43/1000 | Loss: 0.00001355
Iteration 44/1000 | Loss: 0.00001355
Iteration 45/1000 | Loss: 0.00001355
Iteration 46/1000 | Loss: 0.00001354
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001353
Iteration 50/1000 | Loss: 0.00001353
Iteration 51/1000 | Loss: 0.00001353
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001351
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001348
Iteration 84/1000 | Loss: 0.00001348
Iteration 85/1000 | Loss: 0.00001348
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001347
Iteration 94/1000 | Loss: 0.00001347
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001345
Iteration 114/1000 | Loss: 0.00001344
Iteration 115/1000 | Loss: 0.00001344
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001342
Iteration 138/1000 | Loss: 0.00001342
Iteration 139/1000 | Loss: 0.00001342
Iteration 140/1000 | Loss: 0.00001342
Iteration 141/1000 | Loss: 0.00001342
Iteration 142/1000 | Loss: 0.00001342
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001341
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001339
Iteration 155/1000 | Loss: 0.00001339
Iteration 156/1000 | Loss: 0.00001339
Iteration 157/1000 | Loss: 0.00001339
Iteration 158/1000 | Loss: 0.00001339
Iteration 159/1000 | Loss: 0.00001339
Iteration 160/1000 | Loss: 0.00001339
Iteration 161/1000 | Loss: 0.00001339
Iteration 162/1000 | Loss: 0.00001339
Iteration 163/1000 | Loss: 0.00001339
Iteration 164/1000 | Loss: 0.00001339
Iteration 165/1000 | Loss: 0.00001339
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001338
Iteration 173/1000 | Loss: 0.00001337
Iteration 174/1000 | Loss: 0.00001337
Iteration 175/1000 | Loss: 0.00001337
Iteration 176/1000 | Loss: 0.00001337
Iteration 177/1000 | Loss: 0.00001337
Iteration 178/1000 | Loss: 0.00001337
Iteration 179/1000 | Loss: 0.00001337
Iteration 180/1000 | Loss: 0.00001337
Iteration 181/1000 | Loss: 0.00001337
Iteration 182/1000 | Loss: 0.00001337
Iteration 183/1000 | Loss: 0.00001337
Iteration 184/1000 | Loss: 0.00001337
Iteration 185/1000 | Loss: 0.00001337
Iteration 186/1000 | Loss: 0.00001336
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001336
Iteration 190/1000 | Loss: 0.00001336
Iteration 191/1000 | Loss: 0.00001336
Iteration 192/1000 | Loss: 0.00001336
Iteration 193/1000 | Loss: 0.00001336
Iteration 194/1000 | Loss: 0.00001336
Iteration 195/1000 | Loss: 0.00001336
Iteration 196/1000 | Loss: 0.00001336
Iteration 197/1000 | Loss: 0.00001336
Iteration 198/1000 | Loss: 0.00001336
Iteration 199/1000 | Loss: 0.00001336
Iteration 200/1000 | Loss: 0.00001336
Iteration 201/1000 | Loss: 0.00001336
Iteration 202/1000 | Loss: 0.00001336
Iteration 203/1000 | Loss: 0.00001336
Iteration 204/1000 | Loss: 0.00001335
Iteration 205/1000 | Loss: 0.00001335
Iteration 206/1000 | Loss: 0.00001335
Iteration 207/1000 | Loss: 0.00001335
Iteration 208/1000 | Loss: 0.00001335
Iteration 209/1000 | Loss: 0.00001335
Iteration 210/1000 | Loss: 0.00001335
Iteration 211/1000 | Loss: 0.00001335
Iteration 212/1000 | Loss: 0.00001335
Iteration 213/1000 | Loss: 0.00001335
Iteration 214/1000 | Loss: 0.00001335
Iteration 215/1000 | Loss: 0.00001335
Iteration 216/1000 | Loss: 0.00001335
Iteration 217/1000 | Loss: 0.00001335
Iteration 218/1000 | Loss: 0.00001335
Iteration 219/1000 | Loss: 0.00001335
Iteration 220/1000 | Loss: 0.00001335
Iteration 221/1000 | Loss: 0.00001335
Iteration 222/1000 | Loss: 0.00001335
Iteration 223/1000 | Loss: 0.00001335
Iteration 224/1000 | Loss: 0.00001334
Iteration 225/1000 | Loss: 0.00001334
Iteration 226/1000 | Loss: 0.00001334
Iteration 227/1000 | Loss: 0.00001334
Iteration 228/1000 | Loss: 0.00001334
Iteration 229/1000 | Loss: 0.00001334
Iteration 230/1000 | Loss: 0.00001334
Iteration 231/1000 | Loss: 0.00001334
Iteration 232/1000 | Loss: 0.00001334
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001334
Iteration 236/1000 | Loss: 0.00001334
Iteration 237/1000 | Loss: 0.00001334
Iteration 238/1000 | Loss: 0.00001334
Iteration 239/1000 | Loss: 0.00001334
Iteration 240/1000 | Loss: 0.00001334
Iteration 241/1000 | Loss: 0.00001333
Iteration 242/1000 | Loss: 0.00001333
Iteration 243/1000 | Loss: 0.00001333
Iteration 244/1000 | Loss: 0.00001333
Iteration 245/1000 | Loss: 0.00001333
Iteration 246/1000 | Loss: 0.00001333
Iteration 247/1000 | Loss: 0.00001333
Iteration 248/1000 | Loss: 0.00001333
Iteration 249/1000 | Loss: 0.00001333
Iteration 250/1000 | Loss: 0.00001333
Iteration 251/1000 | Loss: 0.00001333
Iteration 252/1000 | Loss: 0.00001333
Iteration 253/1000 | Loss: 0.00001333
Iteration 254/1000 | Loss: 0.00001333
Iteration 255/1000 | Loss: 0.00001333
Iteration 256/1000 | Loss: 0.00001333
Iteration 257/1000 | Loss: 0.00001333
Iteration 258/1000 | Loss: 0.00001333
Iteration 259/1000 | Loss: 0.00001333
Iteration 260/1000 | Loss: 0.00001333
Iteration 261/1000 | Loss: 0.00001333
Iteration 262/1000 | Loss: 0.00001333
Iteration 263/1000 | Loss: 0.00001332
Iteration 264/1000 | Loss: 0.00001332
Iteration 265/1000 | Loss: 0.00001332
Iteration 266/1000 | Loss: 0.00001332
Iteration 267/1000 | Loss: 0.00001332
Iteration 268/1000 | Loss: 0.00001332
Iteration 269/1000 | Loss: 0.00001332
Iteration 270/1000 | Loss: 0.00001332
Iteration 271/1000 | Loss: 0.00001332
Iteration 272/1000 | Loss: 0.00001332
Iteration 273/1000 | Loss: 0.00001332
Iteration 274/1000 | Loss: 0.00001332
Iteration 275/1000 | Loss: 0.00001332
Iteration 276/1000 | Loss: 0.00001332
Iteration 277/1000 | Loss: 0.00001332
Iteration 278/1000 | Loss: 0.00001332
Iteration 279/1000 | Loss: 0.00001332
Iteration 280/1000 | Loss: 0.00001332
Iteration 281/1000 | Loss: 0.00001332
Iteration 282/1000 | Loss: 0.00001332
Iteration 283/1000 | Loss: 0.00001332
Iteration 284/1000 | Loss: 0.00001332
Iteration 285/1000 | Loss: 0.00001332
Iteration 286/1000 | Loss: 0.00001332
Iteration 287/1000 | Loss: 0.00001332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [1.3315333490027115e-05, 1.3315333490027115e-05, 1.3315333490027115e-05, 1.3315333490027115e-05, 1.3315333490027115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3315333490027115e-05

Optimization complete. Final v2v error: 3.0240323543548584 mm

Highest mean error: 3.534430980682373 mm for frame 47

Lowest mean error: 2.5443248748779297 mm for frame 109

Saving results

Total time: 43.73437023162842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995051
Iteration 2/25 | Loss: 0.00207674
Iteration 3/25 | Loss: 0.00156630
Iteration 4/25 | Loss: 0.00143704
Iteration 5/25 | Loss: 0.00135845
Iteration 6/25 | Loss: 0.00137799
Iteration 7/25 | Loss: 0.00140353
Iteration 8/25 | Loss: 0.00143297
Iteration 9/25 | Loss: 0.00130525
Iteration 10/25 | Loss: 0.00119505
Iteration 11/25 | Loss: 0.00115106
Iteration 12/25 | Loss: 0.00113920
Iteration 13/25 | Loss: 0.00111524
Iteration 14/25 | Loss: 0.00109127
Iteration 15/25 | Loss: 0.00108576
Iteration 16/25 | Loss: 0.00107775
Iteration 17/25 | Loss: 0.00107478
Iteration 18/25 | Loss: 0.00107274
Iteration 19/25 | Loss: 0.00106982
Iteration 20/25 | Loss: 0.00106973
Iteration 21/25 | Loss: 0.00106848
Iteration 22/25 | Loss: 0.00106653
Iteration 23/25 | Loss: 0.00106514
Iteration 24/25 | Loss: 0.00106334
Iteration 25/25 | Loss: 0.00106107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40468216
Iteration 2/25 | Loss: 0.00073258
Iteration 3/25 | Loss: 0.00066370
Iteration 4/25 | Loss: 0.00066369
Iteration 5/25 | Loss: 0.00066369
Iteration 6/25 | Loss: 0.00066369
Iteration 7/25 | Loss: 0.00066369
Iteration 8/25 | Loss: 0.00066369
Iteration 9/25 | Loss: 0.00066369
Iteration 10/25 | Loss: 0.00066369
Iteration 11/25 | Loss: 0.00066369
Iteration 12/25 | Loss: 0.00066369
Iteration 13/25 | Loss: 0.00066369
Iteration 14/25 | Loss: 0.00066369
Iteration 15/25 | Loss: 0.00066369
Iteration 16/25 | Loss: 0.00066369
Iteration 17/25 | Loss: 0.00066369
Iteration 18/25 | Loss: 0.00066369
Iteration 19/25 | Loss: 0.00066369
Iteration 20/25 | Loss: 0.00066369
Iteration 21/25 | Loss: 0.00066369
Iteration 22/25 | Loss: 0.00066369
Iteration 23/25 | Loss: 0.00066369
Iteration 24/25 | Loss: 0.00066369
Iteration 25/25 | Loss: 0.00066369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066369
Iteration 2/1000 | Loss: 0.00009356
Iteration 3/1000 | Loss: 0.00004655
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002026
Iteration 7/1000 | Loss: 0.00011365
Iteration 8/1000 | Loss: 0.00001937
Iteration 9/1000 | Loss: 0.00003095
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00003838
Iteration 13/1000 | Loss: 0.00001788
Iteration 14/1000 | Loss: 0.00003125
Iteration 15/1000 | Loss: 0.00012369
Iteration 16/1000 | Loss: 0.00001899
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001705
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001610
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00009318
Iteration 23/1000 | Loss: 0.00022097
Iteration 24/1000 | Loss: 0.00042960
Iteration 25/1000 | Loss: 0.00014002
Iteration 26/1000 | Loss: 0.00055929
Iteration 27/1000 | Loss: 0.00013963
Iteration 28/1000 | Loss: 0.00010054
Iteration 29/1000 | Loss: 0.00016204
Iteration 30/1000 | Loss: 0.00003400
Iteration 31/1000 | Loss: 0.00009266
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00003123
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001419
Iteration 39/1000 | Loss: 0.00010485
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001391
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001390
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001389
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001386
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001383
Iteration 94/1000 | Loss: 0.00001383
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001383
Iteration 102/1000 | Loss: 0.00001383
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001382
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001381
Iteration 111/1000 | Loss: 0.00001381
Iteration 112/1000 | Loss: 0.00001381
Iteration 113/1000 | Loss: 0.00001381
Iteration 114/1000 | Loss: 0.00001381
Iteration 115/1000 | Loss: 0.00001381
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001379
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001377
Iteration 171/1000 | Loss: 0.00001377
Iteration 172/1000 | Loss: 0.00001377
Iteration 173/1000 | Loss: 0.00001377
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001377
Iteration 177/1000 | Loss: 0.00001377
Iteration 178/1000 | Loss: 0.00001377
Iteration 179/1000 | Loss: 0.00001377
Iteration 180/1000 | Loss: 0.00001377
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001376
Iteration 183/1000 | Loss: 0.00001376
Iteration 184/1000 | Loss: 0.00001376
Iteration 185/1000 | Loss: 0.00001376
Iteration 186/1000 | Loss: 0.00001376
Iteration 187/1000 | Loss: 0.00001376
Iteration 188/1000 | Loss: 0.00001376
Iteration 189/1000 | Loss: 0.00001376
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00001375
Iteration 194/1000 | Loss: 0.00001375
Iteration 195/1000 | Loss: 0.00001375
Iteration 196/1000 | Loss: 0.00001375
Iteration 197/1000 | Loss: 0.00001375
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001375
Iteration 201/1000 | Loss: 0.00001375
Iteration 202/1000 | Loss: 0.00001375
Iteration 203/1000 | Loss: 0.00001375
Iteration 204/1000 | Loss: 0.00001375
Iteration 205/1000 | Loss: 0.00001375
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.3751056030741893e-05, 1.3751056030741893e-05, 1.3751056030741893e-05, 1.3751056030741893e-05, 1.3751056030741893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3751056030741893e-05

Optimization complete. Final v2v error: 3.124199867248535 mm

Highest mean error: 5.545641899108887 mm for frame 201

Lowest mean error: 2.6853504180908203 mm for frame 3

Saving results

Total time: 128.2479956150055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646545
Iteration 2/25 | Loss: 0.00125030
Iteration 3/25 | Loss: 0.00108430
Iteration 4/25 | Loss: 0.00105626
Iteration 5/25 | Loss: 0.00105047
Iteration 6/25 | Loss: 0.00104566
Iteration 7/25 | Loss: 0.00103792
Iteration 8/25 | Loss: 0.00103525
Iteration 9/25 | Loss: 0.00103196
Iteration 10/25 | Loss: 0.00102984
Iteration 11/25 | Loss: 0.00102818
Iteration 12/25 | Loss: 0.00102767
Iteration 13/25 | Loss: 0.00102752
Iteration 14/25 | Loss: 0.00102696
Iteration 15/25 | Loss: 0.00102685
Iteration 16/25 | Loss: 0.00102770
Iteration 17/25 | Loss: 0.00102573
Iteration 18/25 | Loss: 0.00102534
Iteration 19/25 | Loss: 0.00102520
Iteration 20/25 | Loss: 0.00102518
Iteration 21/25 | Loss: 0.00102518
Iteration 22/25 | Loss: 0.00102518
Iteration 23/25 | Loss: 0.00102518
Iteration 24/25 | Loss: 0.00102518
Iteration 25/25 | Loss: 0.00102518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41718614
Iteration 2/25 | Loss: 0.00069590
Iteration 3/25 | Loss: 0.00069589
Iteration 4/25 | Loss: 0.00069589
Iteration 5/25 | Loss: 0.00069588
Iteration 6/25 | Loss: 0.00069588
Iteration 7/25 | Loss: 0.00069588
Iteration 8/25 | Loss: 0.00069588
Iteration 9/25 | Loss: 0.00069588
Iteration 10/25 | Loss: 0.00069588
Iteration 11/25 | Loss: 0.00069588
Iteration 12/25 | Loss: 0.00069588
Iteration 13/25 | Loss: 0.00069588
Iteration 14/25 | Loss: 0.00069588
Iteration 15/25 | Loss: 0.00069588
Iteration 16/25 | Loss: 0.00069588
Iteration 17/25 | Loss: 0.00069588
Iteration 18/25 | Loss: 0.00069588
Iteration 19/25 | Loss: 0.00069588
Iteration 20/25 | Loss: 0.00069588
Iteration 21/25 | Loss: 0.00069588
Iteration 22/25 | Loss: 0.00069588
Iteration 23/25 | Loss: 0.00069588
Iteration 24/25 | Loss: 0.00069588
Iteration 25/25 | Loss: 0.00069588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069588
Iteration 2/1000 | Loss: 0.00003487
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002256
Iteration 5/1000 | Loss: 0.00002133
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001968
Iteration 8/1000 | Loss: 0.00001907
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001782
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001736
Iteration 20/1000 | Loss: 0.00001736
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001727
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001720
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001718
Iteration 27/1000 | Loss: 0.00001718
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001716
Iteration 32/1000 | Loss: 0.00001715
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001714
Iteration 36/1000 | Loss: 0.00001714
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001710
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001708
Iteration 50/1000 | Loss: 0.00001708
Iteration 51/1000 | Loss: 0.00001708
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001704
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001697
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001695
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001693
Iteration 115/1000 | Loss: 0.00001693
Iteration 116/1000 | Loss: 0.00001693
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001692
Iteration 122/1000 | Loss: 0.00001692
Iteration 123/1000 | Loss: 0.00001692
Iteration 124/1000 | Loss: 0.00001692
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001691
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001691
Iteration 129/1000 | Loss: 0.00001691
Iteration 130/1000 | Loss: 0.00001690
Iteration 131/1000 | Loss: 0.00001690
Iteration 132/1000 | Loss: 0.00001690
Iteration 133/1000 | Loss: 0.00001690
Iteration 134/1000 | Loss: 0.00001690
Iteration 135/1000 | Loss: 0.00001690
Iteration 136/1000 | Loss: 0.00001690
Iteration 137/1000 | Loss: 0.00001690
Iteration 138/1000 | Loss: 0.00001690
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001689
Iteration 142/1000 | Loss: 0.00001689
Iteration 143/1000 | Loss: 0.00001689
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001687
Iteration 157/1000 | Loss: 0.00001687
Iteration 158/1000 | Loss: 0.00001687
Iteration 159/1000 | Loss: 0.00001687
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001687
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Iteration 164/1000 | Loss: 0.00001687
Iteration 165/1000 | Loss: 0.00001687
Iteration 166/1000 | Loss: 0.00001687
Iteration 167/1000 | Loss: 0.00001687
Iteration 168/1000 | Loss: 0.00001686
Iteration 169/1000 | Loss: 0.00001686
Iteration 170/1000 | Loss: 0.00001686
Iteration 171/1000 | Loss: 0.00001686
Iteration 172/1000 | Loss: 0.00001686
Iteration 173/1000 | Loss: 0.00001686
Iteration 174/1000 | Loss: 0.00001686
Iteration 175/1000 | Loss: 0.00001686
Iteration 176/1000 | Loss: 0.00001686
Iteration 177/1000 | Loss: 0.00001686
Iteration 178/1000 | Loss: 0.00001686
Iteration 179/1000 | Loss: 0.00001686
Iteration 180/1000 | Loss: 0.00001686
Iteration 181/1000 | Loss: 0.00001686
Iteration 182/1000 | Loss: 0.00001686
Iteration 183/1000 | Loss: 0.00001686
Iteration 184/1000 | Loss: 0.00001686
Iteration 185/1000 | Loss: 0.00001686
Iteration 186/1000 | Loss: 0.00001686
Iteration 187/1000 | Loss: 0.00001686
Iteration 188/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.6862153643160127e-05, 1.6862153643160127e-05, 1.6862153643160127e-05, 1.6862153643160127e-05, 1.6862153643160127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6862153643160127e-05

Optimization complete. Final v2v error: 3.4443588256835938 mm

Highest mean error: 4.340943336486816 mm for frame 158

Lowest mean error: 2.857851982116699 mm for frame 134

Saving results

Total time: 78.03596711158752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778475
Iteration 2/25 | Loss: 0.00149858
Iteration 3/25 | Loss: 0.00114085
Iteration 4/25 | Loss: 0.00112664
Iteration 5/25 | Loss: 0.00110557
Iteration 6/25 | Loss: 0.00112432
Iteration 7/25 | Loss: 0.00108750
Iteration 8/25 | Loss: 0.00108227
Iteration 9/25 | Loss: 0.00108551
Iteration 10/25 | Loss: 0.00108095
Iteration 11/25 | Loss: 0.00108009
Iteration 12/25 | Loss: 0.00108002
Iteration 13/25 | Loss: 0.00108002
Iteration 14/25 | Loss: 0.00108002
Iteration 15/25 | Loss: 0.00108002
Iteration 16/25 | Loss: 0.00108002
Iteration 17/25 | Loss: 0.00108002
Iteration 18/25 | Loss: 0.00108001
Iteration 19/25 | Loss: 0.00108001
Iteration 20/25 | Loss: 0.00108001
Iteration 21/25 | Loss: 0.00108001
Iteration 22/25 | Loss: 0.00108001
Iteration 23/25 | Loss: 0.00108001
Iteration 24/25 | Loss: 0.00108001
Iteration 25/25 | Loss: 0.00108001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.51778698
Iteration 2/25 | Loss: 0.00048681
Iteration 3/25 | Loss: 0.00048680
Iteration 4/25 | Loss: 0.00048680
Iteration 5/25 | Loss: 0.00048680
Iteration 6/25 | Loss: 0.00048680
Iteration 7/25 | Loss: 0.00048680
Iteration 8/25 | Loss: 0.00048680
Iteration 9/25 | Loss: 0.00048680
Iteration 10/25 | Loss: 0.00048680
Iteration 11/25 | Loss: 0.00048680
Iteration 12/25 | Loss: 0.00048680
Iteration 13/25 | Loss: 0.00048680
Iteration 14/25 | Loss: 0.00048680
Iteration 15/25 | Loss: 0.00048680
Iteration 16/25 | Loss: 0.00048680
Iteration 17/25 | Loss: 0.00048680
Iteration 18/25 | Loss: 0.00048680
Iteration 19/25 | Loss: 0.00048680
Iteration 20/25 | Loss: 0.00048680
Iteration 21/25 | Loss: 0.00048680
Iteration 22/25 | Loss: 0.00048680
Iteration 23/25 | Loss: 0.00048680
Iteration 24/25 | Loss: 0.00048680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00048679529572837055, 0.00048679529572837055, 0.00048679529572837055, 0.00048679529572837055, 0.00048679529572837055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00048679529572837055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048680
Iteration 2/1000 | Loss: 0.00002192
Iteration 3/1000 | Loss: 0.00014374
Iteration 4/1000 | Loss: 0.00003022
Iteration 5/1000 | Loss: 0.00002003
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00012477
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001378
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001366
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001358
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001355
Iteration 23/1000 | Loss: 0.00001354
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001351
Iteration 26/1000 | Loss: 0.00001351
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001351
Iteration 29/1000 | Loss: 0.00001351
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001350
Iteration 36/1000 | Loss: 0.00001349
Iteration 37/1000 | Loss: 0.00001349
Iteration 38/1000 | Loss: 0.00001348
Iteration 39/1000 | Loss: 0.00001348
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001347
Iteration 42/1000 | Loss: 0.00001347
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001343
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001337
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.337404319201596e-05, 1.337404319201596e-05, 1.337404319201596e-05, 1.337404319201596e-05, 1.337404319201596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.337404319201596e-05

Optimization complete. Final v2v error: 3.0844058990478516 mm

Highest mean error: 3.388901472091675 mm for frame 88

Lowest mean error: 2.818480968475342 mm for frame 228

Saving results

Total time: 51.037511110305786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890474
Iteration 2/25 | Loss: 0.00148189
Iteration 3/25 | Loss: 0.00128559
Iteration 4/25 | Loss: 0.00107243
Iteration 5/25 | Loss: 0.00102876
Iteration 6/25 | Loss: 0.00101959
Iteration 7/25 | Loss: 0.00101783
Iteration 8/25 | Loss: 0.00101672
Iteration 9/25 | Loss: 0.00101634
Iteration 10/25 | Loss: 0.00101552
Iteration 11/25 | Loss: 0.00101524
Iteration 12/25 | Loss: 0.00101512
Iteration 13/25 | Loss: 0.00101509
Iteration 14/25 | Loss: 0.00101509
Iteration 15/25 | Loss: 0.00101508
Iteration 16/25 | Loss: 0.00101508
Iteration 17/25 | Loss: 0.00101508
Iteration 18/25 | Loss: 0.00101508
Iteration 19/25 | Loss: 0.00101508
Iteration 20/25 | Loss: 0.00101508
Iteration 21/25 | Loss: 0.00101508
Iteration 22/25 | Loss: 0.00101508
Iteration 23/25 | Loss: 0.00101508
Iteration 24/25 | Loss: 0.00101508
Iteration 25/25 | Loss: 0.00101508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48335052
Iteration 2/25 | Loss: 0.00071109
Iteration 3/25 | Loss: 0.00071109
Iteration 4/25 | Loss: 0.00071109
Iteration 5/25 | Loss: 0.00071109
Iteration 6/25 | Loss: 0.00071109
Iteration 7/25 | Loss: 0.00071109
Iteration 8/25 | Loss: 0.00071109
Iteration 9/25 | Loss: 0.00071109
Iteration 10/25 | Loss: 0.00071109
Iteration 11/25 | Loss: 0.00071109
Iteration 12/25 | Loss: 0.00071109
Iteration 13/25 | Loss: 0.00071109
Iteration 14/25 | Loss: 0.00071109
Iteration 15/25 | Loss: 0.00071109
Iteration 16/25 | Loss: 0.00071109
Iteration 17/25 | Loss: 0.00071109
Iteration 18/25 | Loss: 0.00071109
Iteration 19/25 | Loss: 0.00071109
Iteration 20/25 | Loss: 0.00071109
Iteration 21/25 | Loss: 0.00071109
Iteration 22/25 | Loss: 0.00071109
Iteration 23/25 | Loss: 0.00071109
Iteration 24/25 | Loss: 0.00071109
Iteration 25/25 | Loss: 0.00071109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071109
Iteration 2/1000 | Loss: 0.00002519
Iteration 3/1000 | Loss: 0.00001696
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001436
Iteration 6/1000 | Loss: 0.00001396
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001367
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001338
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001316
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001305
Iteration 24/1000 | Loss: 0.00001304
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001301
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001285
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001283
Iteration 92/1000 | Loss: 0.00001283
Iteration 93/1000 | Loss: 0.00001283
Iteration 94/1000 | Loss: 0.00001283
Iteration 95/1000 | Loss: 0.00001281
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001279
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001276
Iteration 110/1000 | Loss: 0.00001276
Iteration 111/1000 | Loss: 0.00001275
Iteration 112/1000 | Loss: 0.00001275
Iteration 113/1000 | Loss: 0.00001275
Iteration 114/1000 | Loss: 0.00001275
Iteration 115/1000 | Loss: 0.00001275
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001274
Iteration 119/1000 | Loss: 0.00001274
Iteration 120/1000 | Loss: 0.00001274
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001273
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001272
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.272364806936821e-05, 1.272364806936821e-05, 1.272364806936821e-05, 1.272364806936821e-05, 1.272364806936821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.272364806936821e-05

Optimization complete. Final v2v error: 2.928997039794922 mm

Highest mean error: 3.4766576290130615 mm for frame 200

Lowest mean error: 2.2945315837860107 mm for frame 0

Saving results

Total time: 53.464680433273315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412966
Iteration 2/25 | Loss: 0.00117530
Iteration 3/25 | Loss: 0.00103978
Iteration 4/25 | Loss: 0.00102228
Iteration 5/25 | Loss: 0.00101726
Iteration 6/25 | Loss: 0.00101581
Iteration 7/25 | Loss: 0.00101531
Iteration 8/25 | Loss: 0.00101528
Iteration 9/25 | Loss: 0.00101528
Iteration 10/25 | Loss: 0.00101528
Iteration 11/25 | Loss: 0.00101528
Iteration 12/25 | Loss: 0.00101528
Iteration 13/25 | Loss: 0.00101528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010152844479307532, 0.0010152844479307532, 0.0010152844479307532, 0.0010152844479307532, 0.0010152844479307532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010152844479307532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50918317
Iteration 2/25 | Loss: 0.00069182
Iteration 3/25 | Loss: 0.00069182
Iteration 4/25 | Loss: 0.00069182
Iteration 5/25 | Loss: 0.00069182
Iteration 6/25 | Loss: 0.00069182
Iteration 7/25 | Loss: 0.00069182
Iteration 8/25 | Loss: 0.00069182
Iteration 9/25 | Loss: 0.00069182
Iteration 10/25 | Loss: 0.00069182
Iteration 11/25 | Loss: 0.00069182
Iteration 12/25 | Loss: 0.00069182
Iteration 13/25 | Loss: 0.00069182
Iteration 14/25 | Loss: 0.00069182
Iteration 15/25 | Loss: 0.00069182
Iteration 16/25 | Loss: 0.00069182
Iteration 17/25 | Loss: 0.00069182
Iteration 18/25 | Loss: 0.00069182
Iteration 19/25 | Loss: 0.00069182
Iteration 20/25 | Loss: 0.00069182
Iteration 21/25 | Loss: 0.00069182
Iteration 22/25 | Loss: 0.00069182
Iteration 23/25 | Loss: 0.00069182
Iteration 24/25 | Loss: 0.00069182
Iteration 25/25 | Loss: 0.00069182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069182
Iteration 2/1000 | Loss: 0.00002867
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001402
Iteration 5/1000 | Loss: 0.00001298
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001146
Iteration 10/1000 | Loss: 0.00001131
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001126
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001120
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001117
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001113
Iteration 21/1000 | Loss: 0.00001112
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001107
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001106
Iteration 31/1000 | Loss: 0.00001106
Iteration 32/1000 | Loss: 0.00001105
Iteration 33/1000 | Loss: 0.00001105
Iteration 34/1000 | Loss: 0.00001105
Iteration 35/1000 | Loss: 0.00001104
Iteration 36/1000 | Loss: 0.00001104
Iteration 37/1000 | Loss: 0.00001104
Iteration 38/1000 | Loss: 0.00001103
Iteration 39/1000 | Loss: 0.00001103
Iteration 40/1000 | Loss: 0.00001103
Iteration 41/1000 | Loss: 0.00001103
Iteration 42/1000 | Loss: 0.00001103
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001102
Iteration 46/1000 | Loss: 0.00001102
Iteration 47/1000 | Loss: 0.00001101
Iteration 48/1000 | Loss: 0.00001101
Iteration 49/1000 | Loss: 0.00001101
Iteration 50/1000 | Loss: 0.00001101
Iteration 51/1000 | Loss: 0.00001100
Iteration 52/1000 | Loss: 0.00001100
Iteration 53/1000 | Loss: 0.00001100
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001099
Iteration 56/1000 | Loss: 0.00001099
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001098
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001097
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001096
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001095
Iteration 77/1000 | Loss: 0.00001094
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001094
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001089
Iteration 101/1000 | Loss: 0.00001089
Iteration 102/1000 | Loss: 0.00001089
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001088
Iteration 105/1000 | Loss: 0.00001088
Iteration 106/1000 | Loss: 0.00001088
Iteration 107/1000 | Loss: 0.00001088
Iteration 108/1000 | Loss: 0.00001088
Iteration 109/1000 | Loss: 0.00001088
Iteration 110/1000 | Loss: 0.00001088
Iteration 111/1000 | Loss: 0.00001088
Iteration 112/1000 | Loss: 0.00001088
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001088
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001087
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001087
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001086
Iteration 142/1000 | Loss: 0.00001086
Iteration 143/1000 | Loss: 0.00001086
Iteration 144/1000 | Loss: 0.00001086
Iteration 145/1000 | Loss: 0.00001086
Iteration 146/1000 | Loss: 0.00001086
Iteration 147/1000 | Loss: 0.00001086
Iteration 148/1000 | Loss: 0.00001086
Iteration 149/1000 | Loss: 0.00001085
Iteration 150/1000 | Loss: 0.00001085
Iteration 151/1000 | Loss: 0.00001085
Iteration 152/1000 | Loss: 0.00001085
Iteration 153/1000 | Loss: 0.00001085
Iteration 154/1000 | Loss: 0.00001085
Iteration 155/1000 | Loss: 0.00001085
Iteration 156/1000 | Loss: 0.00001085
Iteration 157/1000 | Loss: 0.00001085
Iteration 158/1000 | Loss: 0.00001085
Iteration 159/1000 | Loss: 0.00001085
Iteration 160/1000 | Loss: 0.00001085
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001085
Iteration 163/1000 | Loss: 0.00001085
Iteration 164/1000 | Loss: 0.00001085
Iteration 165/1000 | Loss: 0.00001085
Iteration 166/1000 | Loss: 0.00001085
Iteration 167/1000 | Loss: 0.00001085
Iteration 168/1000 | Loss: 0.00001085
Iteration 169/1000 | Loss: 0.00001085
Iteration 170/1000 | Loss: 0.00001084
Iteration 171/1000 | Loss: 0.00001084
Iteration 172/1000 | Loss: 0.00001084
Iteration 173/1000 | Loss: 0.00001084
Iteration 174/1000 | Loss: 0.00001084
Iteration 175/1000 | Loss: 0.00001084
Iteration 176/1000 | Loss: 0.00001084
Iteration 177/1000 | Loss: 0.00001084
Iteration 178/1000 | Loss: 0.00001084
Iteration 179/1000 | Loss: 0.00001084
Iteration 180/1000 | Loss: 0.00001084
Iteration 181/1000 | Loss: 0.00001084
Iteration 182/1000 | Loss: 0.00001084
Iteration 183/1000 | Loss: 0.00001084
Iteration 184/1000 | Loss: 0.00001084
Iteration 185/1000 | Loss: 0.00001084
Iteration 186/1000 | Loss: 0.00001084
Iteration 187/1000 | Loss: 0.00001084
Iteration 188/1000 | Loss: 0.00001084
Iteration 189/1000 | Loss: 0.00001084
Iteration 190/1000 | Loss: 0.00001084
Iteration 191/1000 | Loss: 0.00001083
Iteration 192/1000 | Loss: 0.00001083
Iteration 193/1000 | Loss: 0.00001083
Iteration 194/1000 | Loss: 0.00001083
Iteration 195/1000 | Loss: 0.00001083
Iteration 196/1000 | Loss: 0.00001083
Iteration 197/1000 | Loss: 0.00001083
Iteration 198/1000 | Loss: 0.00001083
Iteration 199/1000 | Loss: 0.00001083
Iteration 200/1000 | Loss: 0.00001083
Iteration 201/1000 | Loss: 0.00001083
Iteration 202/1000 | Loss: 0.00001083
Iteration 203/1000 | Loss: 0.00001083
Iteration 204/1000 | Loss: 0.00001083
Iteration 205/1000 | Loss: 0.00001083
Iteration 206/1000 | Loss: 0.00001083
Iteration 207/1000 | Loss: 0.00001083
Iteration 208/1000 | Loss: 0.00001083
Iteration 209/1000 | Loss: 0.00001083
Iteration 210/1000 | Loss: 0.00001082
Iteration 211/1000 | Loss: 0.00001082
Iteration 212/1000 | Loss: 0.00001082
Iteration 213/1000 | Loss: 0.00001082
Iteration 214/1000 | Loss: 0.00001082
Iteration 215/1000 | Loss: 0.00001082
Iteration 216/1000 | Loss: 0.00001082
Iteration 217/1000 | Loss: 0.00001082
Iteration 218/1000 | Loss: 0.00001082
Iteration 219/1000 | Loss: 0.00001082
Iteration 220/1000 | Loss: 0.00001082
Iteration 221/1000 | Loss: 0.00001082
Iteration 222/1000 | Loss: 0.00001082
Iteration 223/1000 | Loss: 0.00001081
Iteration 224/1000 | Loss: 0.00001081
Iteration 225/1000 | Loss: 0.00001081
Iteration 226/1000 | Loss: 0.00001081
Iteration 227/1000 | Loss: 0.00001081
Iteration 228/1000 | Loss: 0.00001081
Iteration 229/1000 | Loss: 0.00001081
Iteration 230/1000 | Loss: 0.00001081
Iteration 231/1000 | Loss: 0.00001081
Iteration 232/1000 | Loss: 0.00001081
Iteration 233/1000 | Loss: 0.00001081
Iteration 234/1000 | Loss: 0.00001081
Iteration 235/1000 | Loss: 0.00001081
Iteration 236/1000 | Loss: 0.00001081
Iteration 237/1000 | Loss: 0.00001081
Iteration 238/1000 | Loss: 0.00001081
Iteration 239/1000 | Loss: 0.00001081
Iteration 240/1000 | Loss: 0.00001080
Iteration 241/1000 | Loss: 0.00001080
Iteration 242/1000 | Loss: 0.00001080
Iteration 243/1000 | Loss: 0.00001080
Iteration 244/1000 | Loss: 0.00001080
Iteration 245/1000 | Loss: 0.00001080
Iteration 246/1000 | Loss: 0.00001080
Iteration 247/1000 | Loss: 0.00001080
Iteration 248/1000 | Loss: 0.00001080
Iteration 249/1000 | Loss: 0.00001080
Iteration 250/1000 | Loss: 0.00001080
Iteration 251/1000 | Loss: 0.00001080
Iteration 252/1000 | Loss: 0.00001080
Iteration 253/1000 | Loss: 0.00001080
Iteration 254/1000 | Loss: 0.00001080
Iteration 255/1000 | Loss: 0.00001080
Iteration 256/1000 | Loss: 0.00001080
Iteration 257/1000 | Loss: 0.00001080
Iteration 258/1000 | Loss: 0.00001080
Iteration 259/1000 | Loss: 0.00001080
Iteration 260/1000 | Loss: 0.00001080
Iteration 261/1000 | Loss: 0.00001080
Iteration 262/1000 | Loss: 0.00001080
Iteration 263/1000 | Loss: 0.00001080
Iteration 264/1000 | Loss: 0.00001080
Iteration 265/1000 | Loss: 0.00001080
Iteration 266/1000 | Loss: 0.00001080
Iteration 267/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.0798219591379166e-05, 1.0798219591379166e-05, 1.0798219591379166e-05, 1.0798219591379166e-05, 1.0798219591379166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0798219591379166e-05

Optimization complete. Final v2v error: 2.7569680213928223 mm

Highest mean error: 3.9060451984405518 mm for frame 30

Lowest mean error: 2.389216899871826 mm for frame 17

Saving results

Total time: 41.59205627441406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809568
Iteration 2/25 | Loss: 0.00126181
Iteration 3/25 | Loss: 0.00100501
Iteration 4/25 | Loss: 0.00098699
Iteration 5/25 | Loss: 0.00098110
Iteration 6/25 | Loss: 0.00097903
Iteration 7/25 | Loss: 0.00097885
Iteration 8/25 | Loss: 0.00097885
Iteration 9/25 | Loss: 0.00097885
Iteration 10/25 | Loss: 0.00097885
Iteration 11/25 | Loss: 0.00097885
Iteration 12/25 | Loss: 0.00097885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009788490133360028, 0.0009788490133360028, 0.0009788490133360028, 0.0009788490133360028, 0.0009788490133360028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009788490133360028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19398522
Iteration 2/25 | Loss: 0.00075690
Iteration 3/25 | Loss: 0.00075690
Iteration 4/25 | Loss: 0.00075690
Iteration 5/25 | Loss: 0.00075690
Iteration 6/25 | Loss: 0.00075690
Iteration 7/25 | Loss: 0.00075690
Iteration 8/25 | Loss: 0.00075690
Iteration 9/25 | Loss: 0.00075690
Iteration 10/25 | Loss: 0.00075690
Iteration 11/25 | Loss: 0.00075689
Iteration 12/25 | Loss: 0.00075689
Iteration 13/25 | Loss: 0.00075689
Iteration 14/25 | Loss: 0.00075689
Iteration 15/25 | Loss: 0.00075689
Iteration 16/25 | Loss: 0.00075689
Iteration 17/25 | Loss: 0.00075689
Iteration 18/25 | Loss: 0.00075689
Iteration 19/25 | Loss: 0.00075689
Iteration 20/25 | Loss: 0.00075689
Iteration 21/25 | Loss: 0.00075689
Iteration 22/25 | Loss: 0.00075689
Iteration 23/25 | Loss: 0.00075689
Iteration 24/25 | Loss: 0.00075689
Iteration 25/25 | Loss: 0.00075689

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075689
Iteration 2/1000 | Loss: 0.00004412
Iteration 3/1000 | Loss: 0.00002290
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001446
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001241
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001190
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001186
Iteration 19/1000 | Loss: 0.00001183
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001180
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001179
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001175
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001168
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001152
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001146
Iteration 91/1000 | Loss: 0.00001146
Iteration 92/1000 | Loss: 0.00001146
Iteration 93/1000 | Loss: 0.00001146
Iteration 94/1000 | Loss: 0.00001146
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001143
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001140
Iteration 117/1000 | Loss: 0.00001140
Iteration 118/1000 | Loss: 0.00001140
Iteration 119/1000 | Loss: 0.00001140
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001138
Iteration 137/1000 | Loss: 0.00001138
Iteration 138/1000 | Loss: 0.00001138
Iteration 139/1000 | Loss: 0.00001138
Iteration 140/1000 | Loss: 0.00001138
Iteration 141/1000 | Loss: 0.00001138
Iteration 142/1000 | Loss: 0.00001138
Iteration 143/1000 | Loss: 0.00001138
Iteration 144/1000 | Loss: 0.00001138
Iteration 145/1000 | Loss: 0.00001138
Iteration 146/1000 | Loss: 0.00001138
Iteration 147/1000 | Loss: 0.00001138
Iteration 148/1000 | Loss: 0.00001138
Iteration 149/1000 | Loss: 0.00001137
Iteration 150/1000 | Loss: 0.00001137
Iteration 151/1000 | Loss: 0.00001137
Iteration 152/1000 | Loss: 0.00001137
Iteration 153/1000 | Loss: 0.00001137
Iteration 154/1000 | Loss: 0.00001137
Iteration 155/1000 | Loss: 0.00001137
Iteration 156/1000 | Loss: 0.00001137
Iteration 157/1000 | Loss: 0.00001137
Iteration 158/1000 | Loss: 0.00001137
Iteration 159/1000 | Loss: 0.00001136
Iteration 160/1000 | Loss: 0.00001136
Iteration 161/1000 | Loss: 0.00001136
Iteration 162/1000 | Loss: 0.00001136
Iteration 163/1000 | Loss: 0.00001136
Iteration 164/1000 | Loss: 0.00001136
Iteration 165/1000 | Loss: 0.00001136
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001135
Iteration 168/1000 | Loss: 0.00001135
Iteration 169/1000 | Loss: 0.00001135
Iteration 170/1000 | Loss: 0.00001135
Iteration 171/1000 | Loss: 0.00001135
Iteration 172/1000 | Loss: 0.00001135
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001134
Iteration 175/1000 | Loss: 0.00001134
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001134
Iteration 180/1000 | Loss: 0.00001134
Iteration 181/1000 | Loss: 0.00001134
Iteration 182/1000 | Loss: 0.00001134
Iteration 183/1000 | Loss: 0.00001134
Iteration 184/1000 | Loss: 0.00001134
Iteration 185/1000 | Loss: 0.00001134
Iteration 186/1000 | Loss: 0.00001134
Iteration 187/1000 | Loss: 0.00001134
Iteration 188/1000 | Loss: 0.00001134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.13415453597554e-05, 1.13415453597554e-05, 1.13415453597554e-05, 1.13415453597554e-05, 1.13415453597554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.13415453597554e-05

Optimization complete. Final v2v error: 2.691952705383301 mm

Highest mean error: 4.1357340812683105 mm for frame 63

Lowest mean error: 2.12139630317688 mm for frame 91

Saving results

Total time: 41.768691062927246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785727
Iteration 2/25 | Loss: 0.00143317
Iteration 3/25 | Loss: 0.00121680
Iteration 4/25 | Loss: 0.00119610
Iteration 5/25 | Loss: 0.00119066
Iteration 6/25 | Loss: 0.00118931
Iteration 7/25 | Loss: 0.00118926
Iteration 8/25 | Loss: 0.00118926
Iteration 9/25 | Loss: 0.00118926
Iteration 10/25 | Loss: 0.00118926
Iteration 11/25 | Loss: 0.00118926
Iteration 12/25 | Loss: 0.00118926
Iteration 13/25 | Loss: 0.00118926
Iteration 14/25 | Loss: 0.00118926
Iteration 15/25 | Loss: 0.00118926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011892563197761774, 0.0011892563197761774, 0.0011892563197761774, 0.0011892563197761774, 0.0011892563197761774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011892563197761774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38571012
Iteration 2/25 | Loss: 0.00071647
Iteration 3/25 | Loss: 0.00071644
Iteration 4/25 | Loss: 0.00071644
Iteration 5/25 | Loss: 0.00071644
Iteration 6/25 | Loss: 0.00071643
Iteration 7/25 | Loss: 0.00071643
Iteration 8/25 | Loss: 0.00071643
Iteration 9/25 | Loss: 0.00071643
Iteration 10/25 | Loss: 0.00071643
Iteration 11/25 | Loss: 0.00071643
Iteration 12/25 | Loss: 0.00071643
Iteration 13/25 | Loss: 0.00071643
Iteration 14/25 | Loss: 0.00071643
Iteration 15/25 | Loss: 0.00071643
Iteration 16/25 | Loss: 0.00071643
Iteration 17/25 | Loss: 0.00071643
Iteration 18/25 | Loss: 0.00071643
Iteration 19/25 | Loss: 0.00071643
Iteration 20/25 | Loss: 0.00071643
Iteration 21/25 | Loss: 0.00071643
Iteration 22/25 | Loss: 0.00071643
Iteration 23/25 | Loss: 0.00071643
Iteration 24/25 | Loss: 0.00071643
Iteration 25/25 | Loss: 0.00071643

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071643
Iteration 2/1000 | Loss: 0.00005031
Iteration 3/1000 | Loss: 0.00003227
Iteration 4/1000 | Loss: 0.00002854
Iteration 5/1000 | Loss: 0.00002736
Iteration 6/1000 | Loss: 0.00002689
Iteration 7/1000 | Loss: 0.00002662
Iteration 8/1000 | Loss: 0.00002635
Iteration 9/1000 | Loss: 0.00002618
Iteration 10/1000 | Loss: 0.00002616
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002611
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002609
Iteration 16/1000 | Loss: 0.00002604
Iteration 17/1000 | Loss: 0.00002591
Iteration 18/1000 | Loss: 0.00002589
Iteration 19/1000 | Loss: 0.00002583
Iteration 20/1000 | Loss: 0.00002578
Iteration 21/1000 | Loss: 0.00002577
Iteration 22/1000 | Loss: 0.00002577
Iteration 23/1000 | Loss: 0.00002576
Iteration 24/1000 | Loss: 0.00002576
Iteration 25/1000 | Loss: 0.00002576
Iteration 26/1000 | Loss: 0.00002576
Iteration 27/1000 | Loss: 0.00002576
Iteration 28/1000 | Loss: 0.00002576
Iteration 29/1000 | Loss: 0.00002576
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002576
Iteration 32/1000 | Loss: 0.00002576
Iteration 33/1000 | Loss: 0.00002576
Iteration 34/1000 | Loss: 0.00002575
Iteration 35/1000 | Loss: 0.00002575
Iteration 36/1000 | Loss: 0.00002575
Iteration 37/1000 | Loss: 0.00002575
Iteration 38/1000 | Loss: 0.00002575
Iteration 39/1000 | Loss: 0.00002575
Iteration 40/1000 | Loss: 0.00002575
Iteration 41/1000 | Loss: 0.00002575
Iteration 42/1000 | Loss: 0.00002574
Iteration 43/1000 | Loss: 0.00002574
Iteration 44/1000 | Loss: 0.00002574
Iteration 45/1000 | Loss: 0.00002573
Iteration 46/1000 | Loss: 0.00002573
Iteration 47/1000 | Loss: 0.00002573
Iteration 48/1000 | Loss: 0.00002572
Iteration 49/1000 | Loss: 0.00002572
Iteration 50/1000 | Loss: 0.00002571
Iteration 51/1000 | Loss: 0.00002571
Iteration 52/1000 | Loss: 0.00002571
Iteration 53/1000 | Loss: 0.00002571
Iteration 54/1000 | Loss: 0.00002570
Iteration 55/1000 | Loss: 0.00002570
Iteration 56/1000 | Loss: 0.00002570
Iteration 57/1000 | Loss: 0.00002570
Iteration 58/1000 | Loss: 0.00002570
Iteration 59/1000 | Loss: 0.00002570
Iteration 60/1000 | Loss: 0.00002568
Iteration 61/1000 | Loss: 0.00002568
Iteration 62/1000 | Loss: 0.00002567
Iteration 63/1000 | Loss: 0.00002567
Iteration 64/1000 | Loss: 0.00002567
Iteration 65/1000 | Loss: 0.00002567
Iteration 66/1000 | Loss: 0.00002567
Iteration 67/1000 | Loss: 0.00002567
Iteration 68/1000 | Loss: 0.00002567
Iteration 69/1000 | Loss: 0.00002566
Iteration 70/1000 | Loss: 0.00002566
Iteration 71/1000 | Loss: 0.00002565
Iteration 72/1000 | Loss: 0.00002564
Iteration 73/1000 | Loss: 0.00002563
Iteration 74/1000 | Loss: 0.00002562
Iteration 75/1000 | Loss: 0.00002562
Iteration 76/1000 | Loss: 0.00002562
Iteration 77/1000 | Loss: 0.00002562
Iteration 78/1000 | Loss: 0.00002562
Iteration 79/1000 | Loss: 0.00002562
Iteration 80/1000 | Loss: 0.00002562
Iteration 81/1000 | Loss: 0.00002562
Iteration 82/1000 | Loss: 0.00002562
Iteration 83/1000 | Loss: 0.00002562
Iteration 84/1000 | Loss: 0.00002562
Iteration 85/1000 | Loss: 0.00002561
Iteration 86/1000 | Loss: 0.00002559
Iteration 87/1000 | Loss: 0.00002559
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002559
Iteration 90/1000 | Loss: 0.00002559
Iteration 91/1000 | Loss: 0.00002559
Iteration 92/1000 | Loss: 0.00002559
Iteration 93/1000 | Loss: 0.00002559
Iteration 94/1000 | Loss: 0.00002559
Iteration 95/1000 | Loss: 0.00002558
Iteration 96/1000 | Loss: 0.00002558
Iteration 97/1000 | Loss: 0.00002558
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002557
Iteration 100/1000 | Loss: 0.00002557
Iteration 101/1000 | Loss: 0.00002555
Iteration 102/1000 | Loss: 0.00002554
Iteration 103/1000 | Loss: 0.00002554
Iteration 104/1000 | Loss: 0.00002553
Iteration 105/1000 | Loss: 0.00002552
Iteration 106/1000 | Loss: 0.00002552
Iteration 107/1000 | Loss: 0.00002550
Iteration 108/1000 | Loss: 0.00002550
Iteration 109/1000 | Loss: 0.00002550
Iteration 110/1000 | Loss: 0.00002546
Iteration 111/1000 | Loss: 0.00002545
Iteration 112/1000 | Loss: 0.00002545
Iteration 113/1000 | Loss: 0.00002544
Iteration 114/1000 | Loss: 0.00002543
Iteration 115/1000 | Loss: 0.00002542
Iteration 116/1000 | Loss: 0.00002542
Iteration 117/1000 | Loss: 0.00002542
Iteration 118/1000 | Loss: 0.00002541
Iteration 119/1000 | Loss: 0.00002541
Iteration 120/1000 | Loss: 0.00002541
Iteration 121/1000 | Loss: 0.00002541
Iteration 122/1000 | Loss: 0.00002541
Iteration 123/1000 | Loss: 0.00002541
Iteration 124/1000 | Loss: 0.00002541
Iteration 125/1000 | Loss: 0.00002541
Iteration 126/1000 | Loss: 0.00002541
Iteration 127/1000 | Loss: 0.00002541
Iteration 128/1000 | Loss: 0.00002541
Iteration 129/1000 | Loss: 0.00002541
Iteration 130/1000 | Loss: 0.00002541
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002540
Iteration 133/1000 | Loss: 0.00002538
Iteration 134/1000 | Loss: 0.00002538
Iteration 135/1000 | Loss: 0.00002538
Iteration 136/1000 | Loss: 0.00002538
Iteration 137/1000 | Loss: 0.00002538
Iteration 138/1000 | Loss: 0.00002537
Iteration 139/1000 | Loss: 0.00002537
Iteration 140/1000 | Loss: 0.00002537
Iteration 141/1000 | Loss: 0.00002537
Iteration 142/1000 | Loss: 0.00002537
Iteration 143/1000 | Loss: 0.00002537
Iteration 144/1000 | Loss: 0.00002537
Iteration 145/1000 | Loss: 0.00002537
Iteration 146/1000 | Loss: 0.00002537
Iteration 147/1000 | Loss: 0.00002536
Iteration 148/1000 | Loss: 0.00002536
Iteration 149/1000 | Loss: 0.00002534
Iteration 150/1000 | Loss: 0.00002533
Iteration 151/1000 | Loss: 0.00002533
Iteration 152/1000 | Loss: 0.00002533
Iteration 153/1000 | Loss: 0.00002532
Iteration 154/1000 | Loss: 0.00002532
Iteration 155/1000 | Loss: 0.00002532
Iteration 156/1000 | Loss: 0.00002532
Iteration 157/1000 | Loss: 0.00002531
Iteration 158/1000 | Loss: 0.00002529
Iteration 159/1000 | Loss: 0.00002529
Iteration 160/1000 | Loss: 0.00002529
Iteration 161/1000 | Loss: 0.00002529
Iteration 162/1000 | Loss: 0.00002528
Iteration 163/1000 | Loss: 0.00002528
Iteration 164/1000 | Loss: 0.00002528
Iteration 165/1000 | Loss: 0.00002528
Iteration 166/1000 | Loss: 0.00002528
Iteration 167/1000 | Loss: 0.00002528
Iteration 168/1000 | Loss: 0.00002528
Iteration 169/1000 | Loss: 0.00002528
Iteration 170/1000 | Loss: 0.00002528
Iteration 171/1000 | Loss: 0.00002528
Iteration 172/1000 | Loss: 0.00002528
Iteration 173/1000 | Loss: 0.00002528
Iteration 174/1000 | Loss: 0.00002528
Iteration 175/1000 | Loss: 0.00002528
Iteration 176/1000 | Loss: 0.00002526
Iteration 177/1000 | Loss: 0.00002526
Iteration 178/1000 | Loss: 0.00002526
Iteration 179/1000 | Loss: 0.00002526
Iteration 180/1000 | Loss: 0.00002526
Iteration 181/1000 | Loss: 0.00002526
Iteration 182/1000 | Loss: 0.00002526
Iteration 183/1000 | Loss: 0.00002526
Iteration 184/1000 | Loss: 0.00002526
Iteration 185/1000 | Loss: 0.00002526
Iteration 186/1000 | Loss: 0.00002526
Iteration 187/1000 | Loss: 0.00002526
Iteration 188/1000 | Loss: 0.00002526
Iteration 189/1000 | Loss: 0.00002526
Iteration 190/1000 | Loss: 0.00002525
Iteration 191/1000 | Loss: 0.00002525
Iteration 192/1000 | Loss: 0.00002525
Iteration 193/1000 | Loss: 0.00002525
Iteration 194/1000 | Loss: 0.00002525
Iteration 195/1000 | Loss: 0.00002525
Iteration 196/1000 | Loss: 0.00002525
Iteration 197/1000 | Loss: 0.00002525
Iteration 198/1000 | Loss: 0.00002525
Iteration 199/1000 | Loss: 0.00002525
Iteration 200/1000 | Loss: 0.00002524
Iteration 201/1000 | Loss: 0.00002524
Iteration 202/1000 | Loss: 0.00002524
Iteration 203/1000 | Loss: 0.00002524
Iteration 204/1000 | Loss: 0.00002524
Iteration 205/1000 | Loss: 0.00002524
Iteration 206/1000 | Loss: 0.00002524
Iteration 207/1000 | Loss: 0.00002524
Iteration 208/1000 | Loss: 0.00002524
Iteration 209/1000 | Loss: 0.00002524
Iteration 210/1000 | Loss: 0.00002524
Iteration 211/1000 | Loss: 0.00002524
Iteration 212/1000 | Loss: 0.00002524
Iteration 213/1000 | Loss: 0.00002524
Iteration 214/1000 | Loss: 0.00002524
Iteration 215/1000 | Loss: 0.00002524
Iteration 216/1000 | Loss: 0.00002524
Iteration 217/1000 | Loss: 0.00002524
Iteration 218/1000 | Loss: 0.00002524
Iteration 219/1000 | Loss: 0.00002524
Iteration 220/1000 | Loss: 0.00002524
Iteration 221/1000 | Loss: 0.00002524
Iteration 222/1000 | Loss: 0.00002524
Iteration 223/1000 | Loss: 0.00002524
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.524119190638885e-05, 2.524119190638885e-05, 2.524119190638885e-05, 2.524119190638885e-05, 2.524119190638885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.524119190638885e-05

Optimization complete. Final v2v error: 3.9717745780944824 mm

Highest mean error: 4.346693515777588 mm for frame 75

Lowest mean error: 2.974942684173584 mm for frame 0

Saving results

Total time: 42.36126351356506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079102
Iteration 2/25 | Loss: 0.00319462
Iteration 3/25 | Loss: 0.00234470
Iteration 4/25 | Loss: 0.00224067
Iteration 5/25 | Loss: 0.00179836
Iteration 6/25 | Loss: 0.00161620
Iteration 7/25 | Loss: 0.00157644
Iteration 8/25 | Loss: 0.00154170
Iteration 9/25 | Loss: 0.00145874
Iteration 10/25 | Loss: 0.00142154
Iteration 11/25 | Loss: 0.00138796
Iteration 12/25 | Loss: 0.00137535
Iteration 13/25 | Loss: 0.00137539
Iteration 14/25 | Loss: 0.00133532
Iteration 15/25 | Loss: 0.00132857
Iteration 16/25 | Loss: 0.00131696
Iteration 17/25 | Loss: 0.00130255
Iteration 18/25 | Loss: 0.00129464
Iteration 19/25 | Loss: 0.00129217
Iteration 20/25 | Loss: 0.00129526
Iteration 21/25 | Loss: 0.00128705
Iteration 22/25 | Loss: 0.00128555
Iteration 23/25 | Loss: 0.00128888
Iteration 24/25 | Loss: 0.00128162
Iteration 25/25 | Loss: 0.00128010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60916424
Iteration 2/25 | Loss: 0.00132685
Iteration 3/25 | Loss: 0.00132685
Iteration 4/25 | Loss: 0.00132685
Iteration 5/25 | Loss: 0.00132685
Iteration 6/25 | Loss: 0.00132685
Iteration 7/25 | Loss: 0.00132685
Iteration 8/25 | Loss: 0.00132685
Iteration 9/25 | Loss: 0.00132685
Iteration 10/25 | Loss: 0.00132685
Iteration 11/25 | Loss: 0.00132685
Iteration 12/25 | Loss: 0.00132685
Iteration 13/25 | Loss: 0.00132685
Iteration 14/25 | Loss: 0.00132685
Iteration 15/25 | Loss: 0.00132685
Iteration 16/25 | Loss: 0.00132685
Iteration 17/25 | Loss: 0.00132685
Iteration 18/25 | Loss: 0.00132685
Iteration 19/25 | Loss: 0.00132685
Iteration 20/25 | Loss: 0.00132685
Iteration 21/25 | Loss: 0.00132685
Iteration 22/25 | Loss: 0.00132685
Iteration 23/25 | Loss: 0.00132685
Iteration 24/25 | Loss: 0.00132685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013268479378893971, 0.0013268479378893971, 0.0013268479378893971, 0.0013268479378893971, 0.0013268479378893971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013268479378893971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132685
Iteration 2/1000 | Loss: 0.00014283
Iteration 3/1000 | Loss: 0.00009329
Iteration 4/1000 | Loss: 0.00253276
Iteration 5/1000 | Loss: 0.00792499
Iteration 6/1000 | Loss: 0.00171918
Iteration 7/1000 | Loss: 0.00018816
Iteration 8/1000 | Loss: 0.00015533
Iteration 9/1000 | Loss: 0.00088212
Iteration 10/1000 | Loss: 0.00009805
Iteration 11/1000 | Loss: 0.00009039
Iteration 12/1000 | Loss: 0.00008368
Iteration 13/1000 | Loss: 0.00007598
Iteration 14/1000 | Loss: 0.00007122
Iteration 15/1000 | Loss: 0.00006804
Iteration 16/1000 | Loss: 0.00047362
Iteration 17/1000 | Loss: 0.00055159
Iteration 18/1000 | Loss: 0.00015526
Iteration 19/1000 | Loss: 0.00007133
Iteration 20/1000 | Loss: 0.00050336
Iteration 21/1000 | Loss: 0.00008931
Iteration 22/1000 | Loss: 0.00007837
Iteration 23/1000 | Loss: 0.00006638
Iteration 24/1000 | Loss: 0.00006355
Iteration 25/1000 | Loss: 0.00006252
Iteration 26/1000 | Loss: 0.00006183
Iteration 27/1000 | Loss: 0.00006136
Iteration 28/1000 | Loss: 0.00006062
Iteration 29/1000 | Loss: 0.00005986
Iteration 30/1000 | Loss: 0.00005924
Iteration 31/1000 | Loss: 0.00005891
Iteration 32/1000 | Loss: 0.00005869
Iteration 33/1000 | Loss: 0.00005868
Iteration 34/1000 | Loss: 0.00005862
Iteration 35/1000 | Loss: 0.00005851
Iteration 36/1000 | Loss: 0.00005829
Iteration 37/1000 | Loss: 0.00005800
Iteration 38/1000 | Loss: 0.00025553
Iteration 39/1000 | Loss: 0.00019660
Iteration 40/1000 | Loss: 0.00006383
Iteration 41/1000 | Loss: 0.00005976
Iteration 42/1000 | Loss: 0.00005704
Iteration 43/1000 | Loss: 0.00005400
Iteration 44/1000 | Loss: 0.00005275
Iteration 45/1000 | Loss: 0.00005228
Iteration 46/1000 | Loss: 0.00005200
Iteration 47/1000 | Loss: 0.00005150
Iteration 48/1000 | Loss: 0.00005123
Iteration 49/1000 | Loss: 0.00005105
Iteration 50/1000 | Loss: 0.00005082
Iteration 51/1000 | Loss: 0.00005080
Iteration 52/1000 | Loss: 0.00005064
Iteration 53/1000 | Loss: 0.00005055
Iteration 54/1000 | Loss: 0.00005050
Iteration 55/1000 | Loss: 0.00005049
Iteration 56/1000 | Loss: 0.00005043
Iteration 57/1000 | Loss: 0.00005042
Iteration 58/1000 | Loss: 0.00005033
Iteration 59/1000 | Loss: 0.00005030
Iteration 60/1000 | Loss: 0.00022462
Iteration 61/1000 | Loss: 0.00135194
Iteration 62/1000 | Loss: 0.00046140
Iteration 63/1000 | Loss: 0.00042183
Iteration 64/1000 | Loss: 0.00083695
Iteration 65/1000 | Loss: 0.00055560
Iteration 66/1000 | Loss: 0.00082409
Iteration 67/1000 | Loss: 0.00052991
Iteration 68/1000 | Loss: 0.00008778
Iteration 69/1000 | Loss: 0.00007268
Iteration 70/1000 | Loss: 0.00020873
Iteration 71/1000 | Loss: 0.00005613
Iteration 72/1000 | Loss: 0.00005108
Iteration 73/1000 | Loss: 0.00004973
Iteration 74/1000 | Loss: 0.00004864
Iteration 75/1000 | Loss: 0.00004810
Iteration 76/1000 | Loss: 0.00004764
Iteration 77/1000 | Loss: 0.00004738
Iteration 78/1000 | Loss: 0.00004717
Iteration 79/1000 | Loss: 0.00004707
Iteration 80/1000 | Loss: 0.00004703
Iteration 81/1000 | Loss: 0.00004706
Iteration 82/1000 | Loss: 0.00004697
Iteration 83/1000 | Loss: 0.00004697
Iteration 84/1000 | Loss: 0.00004697
Iteration 85/1000 | Loss: 0.00004697
Iteration 86/1000 | Loss: 0.00004697
Iteration 87/1000 | Loss: 0.00004696
Iteration 88/1000 | Loss: 0.00004696
Iteration 89/1000 | Loss: 0.00004696
Iteration 90/1000 | Loss: 0.00004696
Iteration 91/1000 | Loss: 0.00004696
Iteration 92/1000 | Loss: 0.00004696
Iteration 93/1000 | Loss: 0.00004696
Iteration 94/1000 | Loss: 0.00004695
Iteration 95/1000 | Loss: 0.00004695
Iteration 96/1000 | Loss: 0.00004695
Iteration 97/1000 | Loss: 0.00004695
Iteration 98/1000 | Loss: 0.00004695
Iteration 99/1000 | Loss: 0.00004695
Iteration 100/1000 | Loss: 0.00004694
Iteration 101/1000 | Loss: 0.00004694
Iteration 102/1000 | Loss: 0.00004694
Iteration 103/1000 | Loss: 0.00004693
Iteration 104/1000 | Loss: 0.00004693
Iteration 105/1000 | Loss: 0.00004693
Iteration 106/1000 | Loss: 0.00004693
Iteration 107/1000 | Loss: 0.00004693
Iteration 108/1000 | Loss: 0.00004693
Iteration 109/1000 | Loss: 0.00004693
Iteration 110/1000 | Loss: 0.00004693
Iteration 111/1000 | Loss: 0.00004692
Iteration 112/1000 | Loss: 0.00004692
Iteration 113/1000 | Loss: 0.00004692
Iteration 114/1000 | Loss: 0.00004691
Iteration 115/1000 | Loss: 0.00004691
Iteration 116/1000 | Loss: 0.00004691
Iteration 117/1000 | Loss: 0.00004691
Iteration 118/1000 | Loss: 0.00004691
Iteration 119/1000 | Loss: 0.00004691
Iteration 120/1000 | Loss: 0.00004691
Iteration 121/1000 | Loss: 0.00004691
Iteration 122/1000 | Loss: 0.00004691
Iteration 123/1000 | Loss: 0.00004691
Iteration 124/1000 | Loss: 0.00004691
Iteration 125/1000 | Loss: 0.00004690
Iteration 126/1000 | Loss: 0.00004690
Iteration 127/1000 | Loss: 0.00004690
Iteration 128/1000 | Loss: 0.00004690
Iteration 129/1000 | Loss: 0.00004690
Iteration 130/1000 | Loss: 0.00004690
Iteration 131/1000 | Loss: 0.00004690
Iteration 132/1000 | Loss: 0.00004690
Iteration 133/1000 | Loss: 0.00004690
Iteration 134/1000 | Loss: 0.00004689
Iteration 135/1000 | Loss: 0.00004689
Iteration 136/1000 | Loss: 0.00004689
Iteration 137/1000 | Loss: 0.00004688
Iteration 138/1000 | Loss: 0.00004688
Iteration 139/1000 | Loss: 0.00004688
Iteration 140/1000 | Loss: 0.00004687
Iteration 141/1000 | Loss: 0.00004687
Iteration 142/1000 | Loss: 0.00004687
Iteration 143/1000 | Loss: 0.00004687
Iteration 144/1000 | Loss: 0.00004687
Iteration 145/1000 | Loss: 0.00004687
Iteration 146/1000 | Loss: 0.00004687
Iteration 147/1000 | Loss: 0.00004687
Iteration 148/1000 | Loss: 0.00004686
Iteration 149/1000 | Loss: 0.00004686
Iteration 150/1000 | Loss: 0.00004686
Iteration 151/1000 | Loss: 0.00004686
Iteration 152/1000 | Loss: 0.00004686
Iteration 153/1000 | Loss: 0.00004686
Iteration 154/1000 | Loss: 0.00004686
Iteration 155/1000 | Loss: 0.00004686
Iteration 156/1000 | Loss: 0.00004686
Iteration 157/1000 | Loss: 0.00004683
Iteration 158/1000 | Loss: 0.00004683
Iteration 159/1000 | Loss: 0.00004683
Iteration 160/1000 | Loss: 0.00004683
Iteration 161/1000 | Loss: 0.00004682
Iteration 162/1000 | Loss: 0.00004682
Iteration 163/1000 | Loss: 0.00004682
Iteration 164/1000 | Loss: 0.00004682
Iteration 165/1000 | Loss: 0.00004682
Iteration 166/1000 | Loss: 0.00004682
Iteration 167/1000 | Loss: 0.00004682
Iteration 168/1000 | Loss: 0.00004681
Iteration 169/1000 | Loss: 0.00004681
Iteration 170/1000 | Loss: 0.00004681
Iteration 171/1000 | Loss: 0.00004681
Iteration 172/1000 | Loss: 0.00004681
Iteration 173/1000 | Loss: 0.00004681
Iteration 174/1000 | Loss: 0.00004681
Iteration 175/1000 | Loss: 0.00004681
Iteration 176/1000 | Loss: 0.00004681
Iteration 177/1000 | Loss: 0.00004681
Iteration 178/1000 | Loss: 0.00004681
Iteration 179/1000 | Loss: 0.00004681
Iteration 180/1000 | Loss: 0.00004681
Iteration 181/1000 | Loss: 0.00004681
Iteration 182/1000 | Loss: 0.00004681
Iteration 183/1000 | Loss: 0.00004681
Iteration 184/1000 | Loss: 0.00004681
Iteration 185/1000 | Loss: 0.00004681
Iteration 186/1000 | Loss: 0.00004681
Iteration 187/1000 | Loss: 0.00004681
Iteration 188/1000 | Loss: 0.00004681
Iteration 189/1000 | Loss: 0.00004681
Iteration 190/1000 | Loss: 0.00004681
Iteration 191/1000 | Loss: 0.00004681
Iteration 192/1000 | Loss: 0.00004681
Iteration 193/1000 | Loss: 0.00004681
Iteration 194/1000 | Loss: 0.00004681
Iteration 195/1000 | Loss: 0.00004681
Iteration 196/1000 | Loss: 0.00004681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [4.680685015046038e-05, 4.680685015046038e-05, 4.680685015046038e-05, 4.680685015046038e-05, 4.680685015046038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.680685015046038e-05

Optimization complete. Final v2v error: 4.762270927429199 mm

Highest mean error: 20.71711540222168 mm for frame 121

Lowest mean error: 4.176888465881348 mm for frame 111

Saving results

Total time: 156.52179193496704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567893
Iteration 2/25 | Loss: 0.00132301
Iteration 3/25 | Loss: 0.00110041
Iteration 4/25 | Loss: 0.00099749
Iteration 5/25 | Loss: 0.00098864
Iteration 6/25 | Loss: 0.00098873
Iteration 7/25 | Loss: 0.00098527
Iteration 8/25 | Loss: 0.00098375
Iteration 9/25 | Loss: 0.00098307
Iteration 10/25 | Loss: 0.00098276
Iteration 11/25 | Loss: 0.00098261
Iteration 12/25 | Loss: 0.00098252
Iteration 13/25 | Loss: 0.00098252
Iteration 14/25 | Loss: 0.00098252
Iteration 15/25 | Loss: 0.00098252
Iteration 16/25 | Loss: 0.00098252
Iteration 17/25 | Loss: 0.00098252
Iteration 18/25 | Loss: 0.00098252
Iteration 19/25 | Loss: 0.00098252
Iteration 20/25 | Loss: 0.00098252
Iteration 21/25 | Loss: 0.00098252
Iteration 22/25 | Loss: 0.00098251
Iteration 23/25 | Loss: 0.00098251
Iteration 24/25 | Loss: 0.00098251
Iteration 25/25 | Loss: 0.00098251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.20098448
Iteration 2/25 | Loss: 0.00062093
Iteration 3/25 | Loss: 0.00062092
Iteration 4/25 | Loss: 0.00062092
Iteration 5/25 | Loss: 0.00062092
Iteration 6/25 | Loss: 0.00062092
Iteration 7/25 | Loss: 0.00062092
Iteration 8/25 | Loss: 0.00062092
Iteration 9/25 | Loss: 0.00062092
Iteration 10/25 | Loss: 0.00062091
Iteration 11/25 | Loss: 0.00062091
Iteration 12/25 | Loss: 0.00062091
Iteration 13/25 | Loss: 0.00062091
Iteration 14/25 | Loss: 0.00062091
Iteration 15/25 | Loss: 0.00062091
Iteration 16/25 | Loss: 0.00062091
Iteration 17/25 | Loss: 0.00062091
Iteration 18/25 | Loss: 0.00062091
Iteration 19/25 | Loss: 0.00062091
Iteration 20/25 | Loss: 0.00062091
Iteration 21/25 | Loss: 0.00062091
Iteration 22/25 | Loss: 0.00062091
Iteration 23/25 | Loss: 0.00062091
Iteration 24/25 | Loss: 0.00062091
Iteration 25/25 | Loss: 0.00062091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062091
Iteration 2/1000 | Loss: 0.00001722
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001174
Iteration 5/1000 | Loss: 0.00001105
Iteration 6/1000 | Loss: 0.00004521
Iteration 7/1000 | Loss: 0.00001541
Iteration 8/1000 | Loss: 0.00001067
Iteration 9/1000 | Loss: 0.00001151
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001021
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001017
Iteration 17/1000 | Loss: 0.00007131
Iteration 18/1000 | Loss: 0.00001037
Iteration 19/1000 | Loss: 0.00000993
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000989
Iteration 22/1000 | Loss: 0.00000988
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000983
Iteration 25/1000 | Loss: 0.00003308
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00000984
Iteration 29/1000 | Loss: 0.00000984
Iteration 30/1000 | Loss: 0.00000984
Iteration 31/1000 | Loss: 0.00000984
Iteration 32/1000 | Loss: 0.00000984
Iteration 33/1000 | Loss: 0.00000981
Iteration 34/1000 | Loss: 0.00000981
Iteration 35/1000 | Loss: 0.00000981
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000981
Iteration 38/1000 | Loss: 0.00000981
Iteration 39/1000 | Loss: 0.00000981
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000980
Iteration 45/1000 | Loss: 0.00000980
Iteration 46/1000 | Loss: 0.00000980
Iteration 47/1000 | Loss: 0.00000979
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000975
Iteration 53/1000 | Loss: 0.00000975
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000975
Iteration 60/1000 | Loss: 0.00000975
Iteration 61/1000 | Loss: 0.00000974
Iteration 62/1000 | Loss: 0.00000974
Iteration 63/1000 | Loss: 0.00000974
Iteration 64/1000 | Loss: 0.00000974
Iteration 65/1000 | Loss: 0.00000974
Iteration 66/1000 | Loss: 0.00000974
Iteration 67/1000 | Loss: 0.00000974
Iteration 68/1000 | Loss: 0.00000974
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000974
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000974
Iteration 75/1000 | Loss: 0.00000974
Iteration 76/1000 | Loss: 0.00000974
Iteration 77/1000 | Loss: 0.00000974
Iteration 78/1000 | Loss: 0.00000974
Iteration 79/1000 | Loss: 0.00000974
Iteration 80/1000 | Loss: 0.00000974
Iteration 81/1000 | Loss: 0.00000974
Iteration 82/1000 | Loss: 0.00000974
Iteration 83/1000 | Loss: 0.00000974
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000974
Iteration 89/1000 | Loss: 0.00000974
Iteration 90/1000 | Loss: 0.00000974
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000974
Iteration 93/1000 | Loss: 0.00000974
Iteration 94/1000 | Loss: 0.00000974
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [9.744040653458796e-06, 9.744040653458796e-06, 9.744040653458796e-06, 9.744040653458796e-06, 9.744040653458796e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.744040653458796e-06

Optimization complete. Final v2v error: 2.6440069675445557 mm

Highest mean error: 3.4241225719451904 mm for frame 213

Lowest mean error: 2.4093079566955566 mm for frame 32

Saving results

Total time: 53.404112577438354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988113
Iteration 2/25 | Loss: 0.00247507
Iteration 3/25 | Loss: 0.00188938
Iteration 4/25 | Loss: 0.00180365
Iteration 5/25 | Loss: 0.00173817
Iteration 6/25 | Loss: 0.00141360
Iteration 7/25 | Loss: 0.00125842
Iteration 8/25 | Loss: 0.00124573
Iteration 9/25 | Loss: 0.00121265
Iteration 10/25 | Loss: 0.00118560
Iteration 11/25 | Loss: 0.00116846
Iteration 12/25 | Loss: 0.00114575
Iteration 13/25 | Loss: 0.00113461
Iteration 14/25 | Loss: 0.00113232
Iteration 15/25 | Loss: 0.00113199
Iteration 16/25 | Loss: 0.00112053
Iteration 17/25 | Loss: 0.00111194
Iteration 18/25 | Loss: 0.00111080
Iteration 19/25 | Loss: 0.00110949
Iteration 20/25 | Loss: 0.00110828
Iteration 21/25 | Loss: 0.00110854
Iteration 22/25 | Loss: 0.00111584
Iteration 23/25 | Loss: 0.00111266
Iteration 24/25 | Loss: 0.00110740
Iteration 25/25 | Loss: 0.00110413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34758937
Iteration 2/25 | Loss: 0.00096275
Iteration 3/25 | Loss: 0.00096273
Iteration 4/25 | Loss: 0.00096273
Iteration 5/25 | Loss: 0.00096273
Iteration 6/25 | Loss: 0.00096273
Iteration 7/25 | Loss: 0.00096273
Iteration 8/25 | Loss: 0.00096273
Iteration 9/25 | Loss: 0.00096273
Iteration 10/25 | Loss: 0.00096273
Iteration 11/25 | Loss: 0.00096273
Iteration 12/25 | Loss: 0.00096273
Iteration 13/25 | Loss: 0.00096273
Iteration 14/25 | Loss: 0.00096273
Iteration 15/25 | Loss: 0.00096273
Iteration 16/25 | Loss: 0.00096273
Iteration 17/25 | Loss: 0.00096273
Iteration 18/25 | Loss: 0.00096273
Iteration 19/25 | Loss: 0.00096273
Iteration 20/25 | Loss: 0.00096273
Iteration 21/25 | Loss: 0.00096273
Iteration 22/25 | Loss: 0.00096273
Iteration 23/25 | Loss: 0.00096273
Iteration 24/25 | Loss: 0.00096273
Iteration 25/25 | Loss: 0.00096273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009627294493839145, 0.0009627294493839145, 0.0009627294493839145, 0.0009627294493839145, 0.0009627294493839145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009627294493839145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096273
Iteration 2/1000 | Loss: 0.00129397
Iteration 3/1000 | Loss: 0.00041041
Iteration 4/1000 | Loss: 0.00015758
Iteration 5/1000 | Loss: 0.00011077
Iteration 6/1000 | Loss: 0.00008730
Iteration 7/1000 | Loss: 0.00037366
Iteration 8/1000 | Loss: 0.00094332
Iteration 9/1000 | Loss: 0.00014839
Iteration 10/1000 | Loss: 0.00010655
Iteration 11/1000 | Loss: 0.00008563
Iteration 12/1000 | Loss: 0.00007497
Iteration 13/1000 | Loss: 0.00007237
Iteration 14/1000 | Loss: 0.00006105
Iteration 15/1000 | Loss: 0.00006234
Iteration 16/1000 | Loss: 0.00025626
Iteration 17/1000 | Loss: 0.00019600
Iteration 18/1000 | Loss: 0.00005915
Iteration 19/1000 | Loss: 0.00005759
Iteration 20/1000 | Loss: 0.00005249
Iteration 21/1000 | Loss: 0.00004361
Iteration 22/1000 | Loss: 0.00005799
Iteration 23/1000 | Loss: 0.00005008
Iteration 24/1000 | Loss: 0.00004751
Iteration 25/1000 | Loss: 0.00005312
Iteration 26/1000 | Loss: 0.00004409
Iteration 27/1000 | Loss: 0.00004066
Iteration 28/1000 | Loss: 0.00003977
Iteration 29/1000 | Loss: 0.00005044
Iteration 30/1000 | Loss: 0.00004835
Iteration 31/1000 | Loss: 0.00005116
Iteration 32/1000 | Loss: 0.00003481
Iteration 33/1000 | Loss: 0.00003273
Iteration 34/1000 | Loss: 0.00005599
Iteration 35/1000 | Loss: 0.00005185
Iteration 36/1000 | Loss: 0.00004869
Iteration 37/1000 | Loss: 0.00003102
Iteration 38/1000 | Loss: 0.00003185
Iteration 39/1000 | Loss: 0.00003735
Iteration 40/1000 | Loss: 0.00002908
Iteration 41/1000 | Loss: 0.00003054
Iteration 42/1000 | Loss: 0.00002250
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002823
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00002104
Iteration 48/1000 | Loss: 0.00002751
Iteration 49/1000 | Loss: 0.00002165
Iteration 50/1000 | Loss: 0.00003331
Iteration 51/1000 | Loss: 0.00002790
Iteration 52/1000 | Loss: 0.00002752
Iteration 53/1000 | Loss: 0.00002756
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00002598
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002268
Iteration 58/1000 | Loss: 0.00002540
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002613
Iteration 61/1000 | Loss: 0.00002550
Iteration 62/1000 | Loss: 0.00002851
Iteration 63/1000 | Loss: 0.00002496
Iteration 64/1000 | Loss: 0.00002374
Iteration 65/1000 | Loss: 0.00002387
Iteration 66/1000 | Loss: 0.00002401
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002840
Iteration 69/1000 | Loss: 0.00002331
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00003348
Iteration 72/1000 | Loss: 0.00002525
Iteration 73/1000 | Loss: 0.00003541
Iteration 74/1000 | Loss: 0.00002621
Iteration 75/1000 | Loss: 0.00002389
Iteration 76/1000 | Loss: 0.00002664
Iteration 77/1000 | Loss: 0.00001716
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001462
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001422
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001410
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001400
Iteration 101/1000 | Loss: 0.00001400
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001399
Iteration 106/1000 | Loss: 0.00001399
Iteration 107/1000 | Loss: 0.00001399
Iteration 108/1000 | Loss: 0.00001399
Iteration 109/1000 | Loss: 0.00001399
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001396
Iteration 120/1000 | Loss: 0.00001396
Iteration 121/1000 | Loss: 0.00001396
Iteration 122/1000 | Loss: 0.00001396
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001395
Iteration 130/1000 | Loss: 0.00001395
Iteration 131/1000 | Loss: 0.00001395
Iteration 132/1000 | Loss: 0.00001395
Iteration 133/1000 | Loss: 0.00001395
Iteration 134/1000 | Loss: 0.00001395
Iteration 135/1000 | Loss: 0.00001394
Iteration 136/1000 | Loss: 0.00001394
Iteration 137/1000 | Loss: 0.00001394
Iteration 138/1000 | Loss: 0.00001394
Iteration 139/1000 | Loss: 0.00001394
Iteration 140/1000 | Loss: 0.00001394
Iteration 141/1000 | Loss: 0.00001394
Iteration 142/1000 | Loss: 0.00001394
Iteration 143/1000 | Loss: 0.00001394
Iteration 144/1000 | Loss: 0.00001394
Iteration 145/1000 | Loss: 0.00001394
Iteration 146/1000 | Loss: 0.00001394
Iteration 147/1000 | Loss: 0.00001394
Iteration 148/1000 | Loss: 0.00001393
Iteration 149/1000 | Loss: 0.00001393
Iteration 150/1000 | Loss: 0.00001393
Iteration 151/1000 | Loss: 0.00001393
Iteration 152/1000 | Loss: 0.00001393
Iteration 153/1000 | Loss: 0.00001393
Iteration 154/1000 | Loss: 0.00001393
Iteration 155/1000 | Loss: 0.00001393
Iteration 156/1000 | Loss: 0.00001393
Iteration 157/1000 | Loss: 0.00001393
Iteration 158/1000 | Loss: 0.00001393
Iteration 159/1000 | Loss: 0.00001393
Iteration 160/1000 | Loss: 0.00001393
Iteration 161/1000 | Loss: 0.00001393
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001392
Iteration 169/1000 | Loss: 0.00001392
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001392
Iteration 173/1000 | Loss: 0.00001392
Iteration 174/1000 | Loss: 0.00001392
Iteration 175/1000 | Loss: 0.00001392
Iteration 176/1000 | Loss: 0.00001392
Iteration 177/1000 | Loss: 0.00001392
Iteration 178/1000 | Loss: 0.00001392
Iteration 179/1000 | Loss: 0.00001392
Iteration 180/1000 | Loss: 0.00001392
Iteration 181/1000 | Loss: 0.00001392
Iteration 182/1000 | Loss: 0.00001392
Iteration 183/1000 | Loss: 0.00001392
Iteration 184/1000 | Loss: 0.00001392
Iteration 185/1000 | Loss: 0.00001391
Iteration 186/1000 | Loss: 0.00001391
Iteration 187/1000 | Loss: 0.00001391
Iteration 188/1000 | Loss: 0.00001391
Iteration 189/1000 | Loss: 0.00001391
Iteration 190/1000 | Loss: 0.00001391
Iteration 191/1000 | Loss: 0.00001391
Iteration 192/1000 | Loss: 0.00001391
Iteration 193/1000 | Loss: 0.00001391
Iteration 194/1000 | Loss: 0.00001391
Iteration 195/1000 | Loss: 0.00001391
Iteration 196/1000 | Loss: 0.00001391
Iteration 197/1000 | Loss: 0.00001391
Iteration 198/1000 | Loss: 0.00001391
Iteration 199/1000 | Loss: 0.00001391
Iteration 200/1000 | Loss: 0.00001390
Iteration 201/1000 | Loss: 0.00001390
Iteration 202/1000 | Loss: 0.00001390
Iteration 203/1000 | Loss: 0.00001390
Iteration 204/1000 | Loss: 0.00001390
Iteration 205/1000 | Loss: 0.00001390
Iteration 206/1000 | Loss: 0.00001390
Iteration 207/1000 | Loss: 0.00001390
Iteration 208/1000 | Loss: 0.00001390
Iteration 209/1000 | Loss: 0.00001390
Iteration 210/1000 | Loss: 0.00001390
Iteration 211/1000 | Loss: 0.00001390
Iteration 212/1000 | Loss: 0.00001390
Iteration 213/1000 | Loss: 0.00001390
Iteration 214/1000 | Loss: 0.00001390
Iteration 215/1000 | Loss: 0.00001390
Iteration 216/1000 | Loss: 0.00001390
Iteration 217/1000 | Loss: 0.00001390
Iteration 218/1000 | Loss: 0.00001390
Iteration 219/1000 | Loss: 0.00001390
Iteration 220/1000 | Loss: 0.00001390
Iteration 221/1000 | Loss: 0.00001390
Iteration 222/1000 | Loss: 0.00001390
Iteration 223/1000 | Loss: 0.00001390
Iteration 224/1000 | Loss: 0.00001390
Iteration 225/1000 | Loss: 0.00001390
Iteration 226/1000 | Loss: 0.00001390
Iteration 227/1000 | Loss: 0.00001390
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.3899556506657973e-05, 1.3899556506657973e-05, 1.3899556506657973e-05, 1.3899556506657973e-05, 1.3899556506657973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3899556506657973e-05

Optimization complete. Final v2v error: 3.116163969039917 mm

Highest mean error: 3.8868637084960938 mm for frame 173

Lowest mean error: 2.676691770553589 mm for frame 166

Saving results

Total time: 194.172669172287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975800
Iteration 2/25 | Loss: 0.00165681
Iteration 3/25 | Loss: 0.00128711
Iteration 4/25 | Loss: 0.00126413
Iteration 5/25 | Loss: 0.00125741
Iteration 6/25 | Loss: 0.00125607
Iteration 7/25 | Loss: 0.00125607
Iteration 8/25 | Loss: 0.00125607
Iteration 9/25 | Loss: 0.00125607
Iteration 10/25 | Loss: 0.00125607
Iteration 11/25 | Loss: 0.00125607
Iteration 12/25 | Loss: 0.00125607
Iteration 13/25 | Loss: 0.00125607
Iteration 14/25 | Loss: 0.00125607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012560654431581497, 0.0012560654431581497, 0.0012560654431581497, 0.0012560654431581497, 0.0012560654431581497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560654431581497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.53839195
Iteration 2/25 | Loss: 0.00080027
Iteration 3/25 | Loss: 0.00080027
Iteration 4/25 | Loss: 0.00080027
Iteration 5/25 | Loss: 0.00080027
Iteration 6/25 | Loss: 0.00080027
Iteration 7/25 | Loss: 0.00080027
Iteration 8/25 | Loss: 0.00080027
Iteration 9/25 | Loss: 0.00080027
Iteration 10/25 | Loss: 0.00080027
Iteration 11/25 | Loss: 0.00080027
Iteration 12/25 | Loss: 0.00080027
Iteration 13/25 | Loss: 0.00080027
Iteration 14/25 | Loss: 0.00080027
Iteration 15/25 | Loss: 0.00080027
Iteration 16/25 | Loss: 0.00080027
Iteration 17/25 | Loss: 0.00080027
Iteration 18/25 | Loss: 0.00080027
Iteration 19/25 | Loss: 0.00080027
Iteration 20/25 | Loss: 0.00080027
Iteration 21/25 | Loss: 0.00080027
Iteration 22/25 | Loss: 0.00080027
Iteration 23/25 | Loss: 0.00080027
Iteration 24/25 | Loss: 0.00080027
Iteration 25/25 | Loss: 0.00080027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080026
Iteration 2/1000 | Loss: 0.00007609
Iteration 3/1000 | Loss: 0.00004856
Iteration 4/1000 | Loss: 0.00003958
Iteration 5/1000 | Loss: 0.00003708
Iteration 6/1000 | Loss: 0.00003585
Iteration 7/1000 | Loss: 0.00003506
Iteration 8/1000 | Loss: 0.00003437
Iteration 9/1000 | Loss: 0.00003390
Iteration 10/1000 | Loss: 0.00003357
Iteration 11/1000 | Loss: 0.00003329
Iteration 12/1000 | Loss: 0.00003305
Iteration 13/1000 | Loss: 0.00003277
Iteration 14/1000 | Loss: 0.00003254
Iteration 15/1000 | Loss: 0.00003234
Iteration 16/1000 | Loss: 0.00003210
Iteration 17/1000 | Loss: 0.00003189
Iteration 18/1000 | Loss: 0.00003180
Iteration 19/1000 | Loss: 0.00003172
Iteration 20/1000 | Loss: 0.00003166
Iteration 21/1000 | Loss: 0.00003164
Iteration 22/1000 | Loss: 0.00003153
Iteration 23/1000 | Loss: 0.00003149
Iteration 24/1000 | Loss: 0.00003149
Iteration 25/1000 | Loss: 0.00003149
Iteration 26/1000 | Loss: 0.00003147
Iteration 27/1000 | Loss: 0.00003145
Iteration 28/1000 | Loss: 0.00003145
Iteration 29/1000 | Loss: 0.00003145
Iteration 30/1000 | Loss: 0.00003145
Iteration 31/1000 | Loss: 0.00003145
Iteration 32/1000 | Loss: 0.00003145
Iteration 33/1000 | Loss: 0.00003145
Iteration 34/1000 | Loss: 0.00003144
Iteration 35/1000 | Loss: 0.00003144
Iteration 36/1000 | Loss: 0.00003144
Iteration 37/1000 | Loss: 0.00003144
Iteration 38/1000 | Loss: 0.00003143
Iteration 39/1000 | Loss: 0.00003143
Iteration 40/1000 | Loss: 0.00003143
Iteration 41/1000 | Loss: 0.00003143
Iteration 42/1000 | Loss: 0.00003143
Iteration 43/1000 | Loss: 0.00003142
Iteration 44/1000 | Loss: 0.00003142
Iteration 45/1000 | Loss: 0.00003142
Iteration 46/1000 | Loss: 0.00003142
Iteration 47/1000 | Loss: 0.00003142
Iteration 48/1000 | Loss: 0.00003141
Iteration 49/1000 | Loss: 0.00003141
Iteration 50/1000 | Loss: 0.00003141
Iteration 51/1000 | Loss: 0.00003141
Iteration 52/1000 | Loss: 0.00003140
Iteration 53/1000 | Loss: 0.00003140
Iteration 54/1000 | Loss: 0.00003140
Iteration 55/1000 | Loss: 0.00003140
Iteration 56/1000 | Loss: 0.00003140
Iteration 57/1000 | Loss: 0.00003140
Iteration 58/1000 | Loss: 0.00003140
Iteration 59/1000 | Loss: 0.00003140
Iteration 60/1000 | Loss: 0.00003139
Iteration 61/1000 | Loss: 0.00003139
Iteration 62/1000 | Loss: 0.00003139
Iteration 63/1000 | Loss: 0.00003139
Iteration 64/1000 | Loss: 0.00003139
Iteration 65/1000 | Loss: 0.00003138
Iteration 66/1000 | Loss: 0.00003138
Iteration 67/1000 | Loss: 0.00003138
Iteration 68/1000 | Loss: 0.00003138
Iteration 69/1000 | Loss: 0.00003138
Iteration 70/1000 | Loss: 0.00003137
Iteration 71/1000 | Loss: 0.00003137
Iteration 72/1000 | Loss: 0.00003137
Iteration 73/1000 | Loss: 0.00003137
Iteration 74/1000 | Loss: 0.00003137
Iteration 75/1000 | Loss: 0.00003136
Iteration 76/1000 | Loss: 0.00003136
Iteration 77/1000 | Loss: 0.00003136
Iteration 78/1000 | Loss: 0.00003136
Iteration 79/1000 | Loss: 0.00003136
Iteration 80/1000 | Loss: 0.00003136
Iteration 81/1000 | Loss: 0.00003136
Iteration 82/1000 | Loss: 0.00003135
Iteration 83/1000 | Loss: 0.00003135
Iteration 84/1000 | Loss: 0.00003135
Iteration 85/1000 | Loss: 0.00003135
Iteration 86/1000 | Loss: 0.00003135
Iteration 87/1000 | Loss: 0.00003135
Iteration 88/1000 | Loss: 0.00003135
Iteration 89/1000 | Loss: 0.00003135
Iteration 90/1000 | Loss: 0.00003135
Iteration 91/1000 | Loss: 0.00003134
Iteration 92/1000 | Loss: 0.00003134
Iteration 93/1000 | Loss: 0.00003134
Iteration 94/1000 | Loss: 0.00003134
Iteration 95/1000 | Loss: 0.00003134
Iteration 96/1000 | Loss: 0.00003134
Iteration 97/1000 | Loss: 0.00003134
Iteration 98/1000 | Loss: 0.00003134
Iteration 99/1000 | Loss: 0.00003134
Iteration 100/1000 | Loss: 0.00003134
Iteration 101/1000 | Loss: 0.00003134
Iteration 102/1000 | Loss: 0.00003134
Iteration 103/1000 | Loss: 0.00003134
Iteration 104/1000 | Loss: 0.00003134
Iteration 105/1000 | Loss: 0.00003134
Iteration 106/1000 | Loss: 0.00003134
Iteration 107/1000 | Loss: 0.00003133
Iteration 108/1000 | Loss: 0.00003133
Iteration 109/1000 | Loss: 0.00003133
Iteration 110/1000 | Loss: 0.00003133
Iteration 111/1000 | Loss: 0.00003133
Iteration 112/1000 | Loss: 0.00003133
Iteration 113/1000 | Loss: 0.00003133
Iteration 114/1000 | Loss: 0.00003133
Iteration 115/1000 | Loss: 0.00003132
Iteration 116/1000 | Loss: 0.00003132
Iteration 117/1000 | Loss: 0.00003132
Iteration 118/1000 | Loss: 0.00003132
Iteration 119/1000 | Loss: 0.00003131
Iteration 120/1000 | Loss: 0.00003131
Iteration 121/1000 | Loss: 0.00003131
Iteration 122/1000 | Loss: 0.00003131
Iteration 123/1000 | Loss: 0.00003131
Iteration 124/1000 | Loss: 0.00003131
Iteration 125/1000 | Loss: 0.00003131
Iteration 126/1000 | Loss: 0.00003131
Iteration 127/1000 | Loss: 0.00003131
Iteration 128/1000 | Loss: 0.00003131
Iteration 129/1000 | Loss: 0.00003131
Iteration 130/1000 | Loss: 0.00003130
Iteration 131/1000 | Loss: 0.00003130
Iteration 132/1000 | Loss: 0.00003130
Iteration 133/1000 | Loss: 0.00003130
Iteration 134/1000 | Loss: 0.00003130
Iteration 135/1000 | Loss: 0.00003130
Iteration 136/1000 | Loss: 0.00003130
Iteration 137/1000 | Loss: 0.00003130
Iteration 138/1000 | Loss: 0.00003130
Iteration 139/1000 | Loss: 0.00003130
Iteration 140/1000 | Loss: 0.00003130
Iteration 141/1000 | Loss: 0.00003130
Iteration 142/1000 | Loss: 0.00003130
Iteration 143/1000 | Loss: 0.00003130
Iteration 144/1000 | Loss: 0.00003130
Iteration 145/1000 | Loss: 0.00003130
Iteration 146/1000 | Loss: 0.00003130
Iteration 147/1000 | Loss: 0.00003130
Iteration 148/1000 | Loss: 0.00003130
Iteration 149/1000 | Loss: 0.00003130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.130088225589134e-05, 3.130088225589134e-05, 3.130088225589134e-05, 3.130088225589134e-05, 3.130088225589134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.130088225589134e-05

Optimization complete. Final v2v error: 4.5847954750061035 mm

Highest mean error: 5.521707057952881 mm for frame 8

Lowest mean error: 4.20327091217041 mm for frame 60

Saving results

Total time: 46.390061378479004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422794
Iteration 2/25 | Loss: 0.00105996
Iteration 3/25 | Loss: 0.00098621
Iteration 4/25 | Loss: 0.00097578
Iteration 5/25 | Loss: 0.00097392
Iteration 6/25 | Loss: 0.00097361
Iteration 7/25 | Loss: 0.00097361
Iteration 8/25 | Loss: 0.00097361
Iteration 9/25 | Loss: 0.00097361
Iteration 10/25 | Loss: 0.00097361
Iteration 11/25 | Loss: 0.00097361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009736131178215146, 0.0009736131178215146, 0.0009736131178215146, 0.0009736131178215146, 0.0009736131178215146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009736131178215146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11872625
Iteration 2/25 | Loss: 0.00043314
Iteration 3/25 | Loss: 0.00043313
Iteration 4/25 | Loss: 0.00043313
Iteration 5/25 | Loss: 0.00043313
Iteration 6/25 | Loss: 0.00043313
Iteration 7/25 | Loss: 0.00043313
Iteration 8/25 | Loss: 0.00043313
Iteration 9/25 | Loss: 0.00043313
Iteration 10/25 | Loss: 0.00043313
Iteration 11/25 | Loss: 0.00043313
Iteration 12/25 | Loss: 0.00043313
Iteration 13/25 | Loss: 0.00043313
Iteration 14/25 | Loss: 0.00043313
Iteration 15/25 | Loss: 0.00043313
Iteration 16/25 | Loss: 0.00043313
Iteration 17/25 | Loss: 0.00043313
Iteration 18/25 | Loss: 0.00043313
Iteration 19/25 | Loss: 0.00043313
Iteration 20/25 | Loss: 0.00043313
Iteration 21/25 | Loss: 0.00043313
Iteration 22/25 | Loss: 0.00043313
Iteration 23/25 | Loss: 0.00043313
Iteration 24/25 | Loss: 0.00043313
Iteration 25/25 | Loss: 0.00043313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043313
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001190
Iteration 10/1000 | Loss: 0.00001173
Iteration 11/1000 | Loss: 0.00001169
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001139
Iteration 16/1000 | Loss: 0.00001138
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001127
Iteration 23/1000 | Loss: 0.00001127
Iteration 24/1000 | Loss: 0.00001125
Iteration 25/1000 | Loss: 0.00001111
Iteration 26/1000 | Loss: 0.00001110
Iteration 27/1000 | Loss: 0.00001110
Iteration 28/1000 | Loss: 0.00001109
Iteration 29/1000 | Loss: 0.00001109
Iteration 30/1000 | Loss: 0.00001109
Iteration 31/1000 | Loss: 0.00001109
Iteration 32/1000 | Loss: 0.00001109
Iteration 33/1000 | Loss: 0.00001109
Iteration 34/1000 | Loss: 0.00001109
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001109
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001109
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001108
Iteration 43/1000 | Loss: 0.00001108
Iteration 44/1000 | Loss: 0.00001108
Iteration 45/1000 | Loss: 0.00001108
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001108
Iteration 48/1000 | Loss: 0.00001108
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001108
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001106
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001105
Iteration 66/1000 | Loss: 0.00001105
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001105
Iteration 70/1000 | Loss: 0.00001105
Iteration 71/1000 | Loss: 0.00001105
Iteration 72/1000 | Loss: 0.00001104
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001104
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001101
Iteration 110/1000 | Loss: 0.00001101
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001101
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001100
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001100
Iteration 121/1000 | Loss: 0.00001100
Iteration 122/1000 | Loss: 0.00001100
Iteration 123/1000 | Loss: 0.00001100
Iteration 124/1000 | Loss: 0.00001100
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001099
Iteration 128/1000 | Loss: 0.00001099
Iteration 129/1000 | Loss: 0.00001099
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001099
Iteration 133/1000 | Loss: 0.00001099
Iteration 134/1000 | Loss: 0.00001099
Iteration 135/1000 | Loss: 0.00001099
Iteration 136/1000 | Loss: 0.00001099
Iteration 137/1000 | Loss: 0.00001099
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001098
Iteration 140/1000 | Loss: 0.00001098
Iteration 141/1000 | Loss: 0.00001098
Iteration 142/1000 | Loss: 0.00001098
Iteration 143/1000 | Loss: 0.00001098
Iteration 144/1000 | Loss: 0.00001098
Iteration 145/1000 | Loss: 0.00001098
Iteration 146/1000 | Loss: 0.00001098
Iteration 147/1000 | Loss: 0.00001098
Iteration 148/1000 | Loss: 0.00001098
Iteration 149/1000 | Loss: 0.00001098
Iteration 150/1000 | Loss: 0.00001098
Iteration 151/1000 | Loss: 0.00001098
Iteration 152/1000 | Loss: 0.00001098
Iteration 153/1000 | Loss: 0.00001098
Iteration 154/1000 | Loss: 0.00001098
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Iteration 161/1000 | Loss: 0.00001097
Iteration 162/1000 | Loss: 0.00001097
Iteration 163/1000 | Loss: 0.00001097
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001097
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001097
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Iteration 172/1000 | Loss: 0.00001097
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001097
Iteration 179/1000 | Loss: 0.00001097
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001097
Iteration 182/1000 | Loss: 0.00001097
Iteration 183/1000 | Loss: 0.00001097
Iteration 184/1000 | Loss: 0.00001097
Iteration 185/1000 | Loss: 0.00001097
Iteration 186/1000 | Loss: 0.00001097
Iteration 187/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.0967502021230757e-05, 1.0967502021230757e-05, 1.0967502021230757e-05, 1.0967502021230757e-05, 1.0967502021230757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0967502021230757e-05

Optimization complete. Final v2v error: 2.8284530639648438 mm

Highest mean error: 2.8832509517669678 mm for frame 82

Lowest mean error: 2.8001577854156494 mm for frame 33

Saving results

Total time: 35.21493744850159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416215
Iteration 2/25 | Loss: 0.00107371
Iteration 3/25 | Loss: 0.00099816
Iteration 4/25 | Loss: 0.00098543
Iteration 5/25 | Loss: 0.00098133
Iteration 6/25 | Loss: 0.00098015
Iteration 7/25 | Loss: 0.00098014
Iteration 8/25 | Loss: 0.00098014
Iteration 9/25 | Loss: 0.00098014
Iteration 10/25 | Loss: 0.00098014
Iteration 11/25 | Loss: 0.00098014
Iteration 12/25 | Loss: 0.00098014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009801361011341214, 0.0009801361011341214, 0.0009801361011341214, 0.0009801361011341214, 0.0009801361011341214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009801361011341214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15709841
Iteration 2/25 | Loss: 0.00045320
Iteration 3/25 | Loss: 0.00045320
Iteration 4/25 | Loss: 0.00045320
Iteration 5/25 | Loss: 0.00045320
Iteration 6/25 | Loss: 0.00045320
Iteration 7/25 | Loss: 0.00045320
Iteration 8/25 | Loss: 0.00045320
Iteration 9/25 | Loss: 0.00045320
Iteration 10/25 | Loss: 0.00045320
Iteration 11/25 | Loss: 0.00045320
Iteration 12/25 | Loss: 0.00045320
Iteration 13/25 | Loss: 0.00045320
Iteration 14/25 | Loss: 0.00045320
Iteration 15/25 | Loss: 0.00045320
Iteration 16/25 | Loss: 0.00045320
Iteration 17/25 | Loss: 0.00045320
Iteration 18/25 | Loss: 0.00045320
Iteration 19/25 | Loss: 0.00045320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004531967279035598, 0.0004531967279035598, 0.0004531967279035598, 0.0004531967279035598, 0.0004531967279035598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004531967279035598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045320
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001383
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001200
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001125
Iteration 9/1000 | Loss: 0.00001092
Iteration 10/1000 | Loss: 0.00001063
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001027
Iteration 16/1000 | Loss: 0.00001021
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001021
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001014
Iteration 21/1000 | Loss: 0.00001013
Iteration 22/1000 | Loss: 0.00000999
Iteration 23/1000 | Loss: 0.00000999
Iteration 24/1000 | Loss: 0.00000998
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000997
Iteration 27/1000 | Loss: 0.00000997
Iteration 28/1000 | Loss: 0.00000997
Iteration 29/1000 | Loss: 0.00000997
Iteration 30/1000 | Loss: 0.00000996
Iteration 31/1000 | Loss: 0.00000996
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000996
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000996
Iteration 39/1000 | Loss: 0.00000996
Iteration 40/1000 | Loss: 0.00000996
Iteration 41/1000 | Loss: 0.00000996
Iteration 42/1000 | Loss: 0.00000996
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000996
Iteration 49/1000 | Loss: 0.00000996
Iteration 50/1000 | Loss: 0.00000996
Iteration 51/1000 | Loss: 0.00000996
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000996
Iteration 58/1000 | Loss: 0.00000996
Iteration 59/1000 | Loss: 0.00000996
Iteration 60/1000 | Loss: 0.00000996
Iteration 61/1000 | Loss: 0.00000996
Iteration 62/1000 | Loss: 0.00000996
Iteration 63/1000 | Loss: 0.00000996
Iteration 64/1000 | Loss: 0.00000996
Iteration 65/1000 | Loss: 0.00000996
Iteration 66/1000 | Loss: 0.00000996
Iteration 67/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [9.964891432900913e-06, 9.964891432900913e-06, 9.964891432900913e-06, 9.964891432900913e-06, 9.964891432900913e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.964891432900913e-06

Optimization complete. Final v2v error: 2.7045764923095703 mm

Highest mean error: 2.7339603900909424 mm for frame 45

Lowest mean error: 2.6938154697418213 mm for frame 6

Saving results

Total time: 28.918723583221436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792647
Iteration 2/25 | Loss: 0.00126445
Iteration 3/25 | Loss: 0.00111288
Iteration 4/25 | Loss: 0.00107066
Iteration 5/25 | Loss: 0.00106741
Iteration 6/25 | Loss: 0.00106602
Iteration 7/25 | Loss: 0.00106447
Iteration 8/25 | Loss: 0.00108527
Iteration 9/25 | Loss: 0.00108300
Iteration 10/25 | Loss: 0.00108628
Iteration 11/25 | Loss: 0.00107376
Iteration 12/25 | Loss: 0.00107990
Iteration 13/25 | Loss: 0.00105866
Iteration 14/25 | Loss: 0.00105613
Iteration 15/25 | Loss: 0.00105520
Iteration 16/25 | Loss: 0.00105513
Iteration 17/25 | Loss: 0.00105512
Iteration 18/25 | Loss: 0.00105512
Iteration 19/25 | Loss: 0.00105512
Iteration 20/25 | Loss: 0.00105512
Iteration 21/25 | Loss: 0.00105512
Iteration 22/25 | Loss: 0.00105512
Iteration 23/25 | Loss: 0.00105512
Iteration 24/25 | Loss: 0.00105512
Iteration 25/25 | Loss: 0.00105512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.92054701
Iteration 2/25 | Loss: 0.00057796
Iteration 3/25 | Loss: 0.00057789
Iteration 4/25 | Loss: 0.00057789
Iteration 5/25 | Loss: 0.00057788
Iteration 6/25 | Loss: 0.00057788
Iteration 7/25 | Loss: 0.00057788
Iteration 8/25 | Loss: 0.00057788
Iteration 9/25 | Loss: 0.00057788
Iteration 10/25 | Loss: 0.00057788
Iteration 11/25 | Loss: 0.00057788
Iteration 12/25 | Loss: 0.00057788
Iteration 13/25 | Loss: 0.00057788
Iteration 14/25 | Loss: 0.00057788
Iteration 15/25 | Loss: 0.00057788
Iteration 16/25 | Loss: 0.00057788
Iteration 17/25 | Loss: 0.00057788
Iteration 18/25 | Loss: 0.00057788
Iteration 19/25 | Loss: 0.00057788
Iteration 20/25 | Loss: 0.00057788
Iteration 21/25 | Loss: 0.00057788
Iteration 22/25 | Loss: 0.00057788
Iteration 23/25 | Loss: 0.00057788
Iteration 24/25 | Loss: 0.00057788
Iteration 25/25 | Loss: 0.00057788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057788
Iteration 2/1000 | Loss: 0.00002500
Iteration 3/1000 | Loss: 0.00001739
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001491
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001416
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00020852
Iteration 11/1000 | Loss: 0.00002137
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001377
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001347
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001341
Iteration 43/1000 | Loss: 0.00014966
Iteration 44/1000 | Loss: 0.00004594
Iteration 45/1000 | Loss: 0.00001344
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00014168
Iteration 63/1000 | Loss: 0.00006641
Iteration 64/1000 | Loss: 0.00001366
Iteration 65/1000 | Loss: 0.00001344
Iteration 66/1000 | Loss: 0.00017298
Iteration 67/1000 | Loss: 0.00009946
Iteration 68/1000 | Loss: 0.00014595
Iteration 69/1000 | Loss: 0.00008608
Iteration 70/1000 | Loss: 0.00001369
Iteration 71/1000 | Loss: 0.00014382
Iteration 72/1000 | Loss: 0.00008605
Iteration 73/1000 | Loss: 0.00024210
Iteration 74/1000 | Loss: 0.00021234
Iteration 75/1000 | Loss: 0.00002035
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001256
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001251
Iteration 86/1000 | Loss: 0.00001251
Iteration 87/1000 | Loss: 0.00001251
Iteration 88/1000 | Loss: 0.00001250
Iteration 89/1000 | Loss: 0.00001250
Iteration 90/1000 | Loss: 0.00001250
Iteration 91/1000 | Loss: 0.00001249
Iteration 92/1000 | Loss: 0.00001249
Iteration 93/1000 | Loss: 0.00001249
Iteration 94/1000 | Loss: 0.00001248
Iteration 95/1000 | Loss: 0.00001248
Iteration 96/1000 | Loss: 0.00001248
Iteration 97/1000 | Loss: 0.00001248
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001247
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001246
Iteration 107/1000 | Loss: 0.00001246
Iteration 108/1000 | Loss: 0.00001246
Iteration 109/1000 | Loss: 0.00001246
Iteration 110/1000 | Loss: 0.00001246
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001243
Iteration 128/1000 | Loss: 0.00001243
Iteration 129/1000 | Loss: 0.00001243
Iteration 130/1000 | Loss: 0.00001243
Iteration 131/1000 | Loss: 0.00001243
Iteration 132/1000 | Loss: 0.00001243
Iteration 133/1000 | Loss: 0.00001243
Iteration 134/1000 | Loss: 0.00001243
Iteration 135/1000 | Loss: 0.00001243
Iteration 136/1000 | Loss: 0.00001243
Iteration 137/1000 | Loss: 0.00001243
Iteration 138/1000 | Loss: 0.00001243
Iteration 139/1000 | Loss: 0.00001243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.243233145942213e-05, 1.243233145942213e-05, 1.243233145942213e-05, 1.243233145942213e-05, 1.243233145942213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.243233145942213e-05

Optimization complete. Final v2v error: 2.9259321689605713 mm

Highest mean error: 4.607434272766113 mm for frame 51

Lowest mean error: 2.3877944946289062 mm for frame 0

Saving results

Total time: 97.09068036079407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816490
Iteration 2/25 | Loss: 0.00132070
Iteration 3/25 | Loss: 0.00101633
Iteration 4/25 | Loss: 0.00098800
Iteration 5/25 | Loss: 0.00098404
Iteration 6/25 | Loss: 0.00098303
Iteration 7/25 | Loss: 0.00098275
Iteration 8/25 | Loss: 0.00098275
Iteration 9/25 | Loss: 0.00098275
Iteration 10/25 | Loss: 0.00098275
Iteration 11/25 | Loss: 0.00098275
Iteration 12/25 | Loss: 0.00098275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009827506728470325, 0.0009827506728470325, 0.0009827506728470325, 0.0009827506728470325, 0.0009827506728470325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009827506728470325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38456452
Iteration 2/25 | Loss: 0.00067209
Iteration 3/25 | Loss: 0.00067209
Iteration 4/25 | Loss: 0.00067209
Iteration 5/25 | Loss: 0.00067209
Iteration 6/25 | Loss: 0.00067209
Iteration 7/25 | Loss: 0.00067209
Iteration 8/25 | Loss: 0.00067209
Iteration 9/25 | Loss: 0.00067209
Iteration 10/25 | Loss: 0.00067209
Iteration 11/25 | Loss: 0.00067209
Iteration 12/25 | Loss: 0.00067209
Iteration 13/25 | Loss: 0.00067209
Iteration 14/25 | Loss: 0.00067209
Iteration 15/25 | Loss: 0.00067209
Iteration 16/25 | Loss: 0.00067209
Iteration 17/25 | Loss: 0.00067209
Iteration 18/25 | Loss: 0.00067209
Iteration 19/25 | Loss: 0.00067209
Iteration 20/25 | Loss: 0.00067209
Iteration 21/25 | Loss: 0.00067209
Iteration 22/25 | Loss: 0.00067209
Iteration 23/25 | Loss: 0.00067209
Iteration 24/25 | Loss: 0.00067209
Iteration 25/25 | Loss: 0.00067209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067209
Iteration 2/1000 | Loss: 0.00001795
Iteration 3/1000 | Loss: 0.00001130
Iteration 4/1000 | Loss: 0.00001008
Iteration 5/1000 | Loss: 0.00000952
Iteration 6/1000 | Loss: 0.00000910
Iteration 7/1000 | Loss: 0.00000876
Iteration 8/1000 | Loss: 0.00000858
Iteration 9/1000 | Loss: 0.00000851
Iteration 10/1000 | Loss: 0.00000839
Iteration 11/1000 | Loss: 0.00000827
Iteration 12/1000 | Loss: 0.00000826
Iteration 13/1000 | Loss: 0.00000824
Iteration 14/1000 | Loss: 0.00000823
Iteration 15/1000 | Loss: 0.00000823
Iteration 16/1000 | Loss: 0.00000821
Iteration 17/1000 | Loss: 0.00000821
Iteration 18/1000 | Loss: 0.00000821
Iteration 19/1000 | Loss: 0.00000820
Iteration 20/1000 | Loss: 0.00000820
Iteration 21/1000 | Loss: 0.00000817
Iteration 22/1000 | Loss: 0.00000815
Iteration 23/1000 | Loss: 0.00000815
Iteration 24/1000 | Loss: 0.00000811
Iteration 25/1000 | Loss: 0.00000810
Iteration 26/1000 | Loss: 0.00000810
Iteration 27/1000 | Loss: 0.00000810
Iteration 28/1000 | Loss: 0.00000810
Iteration 29/1000 | Loss: 0.00000810
Iteration 30/1000 | Loss: 0.00000810
Iteration 31/1000 | Loss: 0.00000809
Iteration 32/1000 | Loss: 0.00000809
Iteration 33/1000 | Loss: 0.00000809
Iteration 34/1000 | Loss: 0.00000809
Iteration 35/1000 | Loss: 0.00000809
Iteration 36/1000 | Loss: 0.00000809
Iteration 37/1000 | Loss: 0.00000809
Iteration 38/1000 | Loss: 0.00000808
Iteration 39/1000 | Loss: 0.00000808
Iteration 40/1000 | Loss: 0.00000808
Iteration 41/1000 | Loss: 0.00000808
Iteration 42/1000 | Loss: 0.00000808
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000807
Iteration 45/1000 | Loss: 0.00000807
Iteration 46/1000 | Loss: 0.00000807
Iteration 47/1000 | Loss: 0.00000807
Iteration 48/1000 | Loss: 0.00000806
Iteration 49/1000 | Loss: 0.00000806
Iteration 50/1000 | Loss: 0.00000806
Iteration 51/1000 | Loss: 0.00000806
Iteration 52/1000 | Loss: 0.00000806
Iteration 53/1000 | Loss: 0.00000806
Iteration 54/1000 | Loss: 0.00000806
Iteration 55/1000 | Loss: 0.00000805
Iteration 56/1000 | Loss: 0.00000805
Iteration 57/1000 | Loss: 0.00000805
Iteration 58/1000 | Loss: 0.00000805
Iteration 59/1000 | Loss: 0.00000805
Iteration 60/1000 | Loss: 0.00000805
Iteration 61/1000 | Loss: 0.00000805
Iteration 62/1000 | Loss: 0.00000805
Iteration 63/1000 | Loss: 0.00000805
Iteration 64/1000 | Loss: 0.00000805
Iteration 65/1000 | Loss: 0.00000805
Iteration 66/1000 | Loss: 0.00000804
Iteration 67/1000 | Loss: 0.00000804
Iteration 68/1000 | Loss: 0.00000804
Iteration 69/1000 | Loss: 0.00000804
Iteration 70/1000 | Loss: 0.00000804
Iteration 71/1000 | Loss: 0.00000804
Iteration 72/1000 | Loss: 0.00000804
Iteration 73/1000 | Loss: 0.00000804
Iteration 74/1000 | Loss: 0.00000803
Iteration 75/1000 | Loss: 0.00000803
Iteration 76/1000 | Loss: 0.00000803
Iteration 77/1000 | Loss: 0.00000803
Iteration 78/1000 | Loss: 0.00000803
Iteration 79/1000 | Loss: 0.00000802
Iteration 80/1000 | Loss: 0.00000802
Iteration 81/1000 | Loss: 0.00000802
Iteration 82/1000 | Loss: 0.00000802
Iteration 83/1000 | Loss: 0.00000802
Iteration 84/1000 | Loss: 0.00000802
Iteration 85/1000 | Loss: 0.00000802
Iteration 86/1000 | Loss: 0.00000802
Iteration 87/1000 | Loss: 0.00000802
Iteration 88/1000 | Loss: 0.00000802
Iteration 89/1000 | Loss: 0.00000802
Iteration 90/1000 | Loss: 0.00000802
Iteration 91/1000 | Loss: 0.00000801
Iteration 92/1000 | Loss: 0.00000801
Iteration 93/1000 | Loss: 0.00000801
Iteration 94/1000 | Loss: 0.00000801
Iteration 95/1000 | Loss: 0.00000801
Iteration 96/1000 | Loss: 0.00000801
Iteration 97/1000 | Loss: 0.00000801
Iteration 98/1000 | Loss: 0.00000801
Iteration 99/1000 | Loss: 0.00000801
Iteration 100/1000 | Loss: 0.00000801
Iteration 101/1000 | Loss: 0.00000801
Iteration 102/1000 | Loss: 0.00000800
Iteration 103/1000 | Loss: 0.00000800
Iteration 104/1000 | Loss: 0.00000800
Iteration 105/1000 | Loss: 0.00000800
Iteration 106/1000 | Loss: 0.00000800
Iteration 107/1000 | Loss: 0.00000800
Iteration 108/1000 | Loss: 0.00000799
Iteration 109/1000 | Loss: 0.00000799
Iteration 110/1000 | Loss: 0.00000799
Iteration 111/1000 | Loss: 0.00000799
Iteration 112/1000 | Loss: 0.00000799
Iteration 113/1000 | Loss: 0.00000799
Iteration 114/1000 | Loss: 0.00000799
Iteration 115/1000 | Loss: 0.00000799
Iteration 116/1000 | Loss: 0.00000799
Iteration 117/1000 | Loss: 0.00000799
Iteration 118/1000 | Loss: 0.00000799
Iteration 119/1000 | Loss: 0.00000799
Iteration 120/1000 | Loss: 0.00000798
Iteration 121/1000 | Loss: 0.00000798
Iteration 122/1000 | Loss: 0.00000798
Iteration 123/1000 | Loss: 0.00000798
Iteration 124/1000 | Loss: 0.00000798
Iteration 125/1000 | Loss: 0.00000798
Iteration 126/1000 | Loss: 0.00000798
Iteration 127/1000 | Loss: 0.00000798
Iteration 128/1000 | Loss: 0.00000798
Iteration 129/1000 | Loss: 0.00000798
Iteration 130/1000 | Loss: 0.00000798
Iteration 131/1000 | Loss: 0.00000798
Iteration 132/1000 | Loss: 0.00000798
Iteration 133/1000 | Loss: 0.00000798
Iteration 134/1000 | Loss: 0.00000798
Iteration 135/1000 | Loss: 0.00000798
Iteration 136/1000 | Loss: 0.00000798
Iteration 137/1000 | Loss: 0.00000797
Iteration 138/1000 | Loss: 0.00000797
Iteration 139/1000 | Loss: 0.00000797
Iteration 140/1000 | Loss: 0.00000797
Iteration 141/1000 | Loss: 0.00000797
Iteration 142/1000 | Loss: 0.00000797
Iteration 143/1000 | Loss: 0.00000797
Iteration 144/1000 | Loss: 0.00000797
Iteration 145/1000 | Loss: 0.00000797
Iteration 146/1000 | Loss: 0.00000797
Iteration 147/1000 | Loss: 0.00000797
Iteration 148/1000 | Loss: 0.00000797
Iteration 149/1000 | Loss: 0.00000797
Iteration 150/1000 | Loss: 0.00000797
Iteration 151/1000 | Loss: 0.00000797
Iteration 152/1000 | Loss: 0.00000797
Iteration 153/1000 | Loss: 0.00000797
Iteration 154/1000 | Loss: 0.00000797
Iteration 155/1000 | Loss: 0.00000797
Iteration 156/1000 | Loss: 0.00000797
Iteration 157/1000 | Loss: 0.00000796
Iteration 158/1000 | Loss: 0.00000796
Iteration 159/1000 | Loss: 0.00000796
Iteration 160/1000 | Loss: 0.00000796
Iteration 161/1000 | Loss: 0.00000796
Iteration 162/1000 | Loss: 0.00000796
Iteration 163/1000 | Loss: 0.00000796
Iteration 164/1000 | Loss: 0.00000796
Iteration 165/1000 | Loss: 0.00000796
Iteration 166/1000 | Loss: 0.00000796
Iteration 167/1000 | Loss: 0.00000796
Iteration 168/1000 | Loss: 0.00000796
Iteration 169/1000 | Loss: 0.00000795
Iteration 170/1000 | Loss: 0.00000795
Iteration 171/1000 | Loss: 0.00000795
Iteration 172/1000 | Loss: 0.00000795
Iteration 173/1000 | Loss: 0.00000795
Iteration 174/1000 | Loss: 0.00000795
Iteration 175/1000 | Loss: 0.00000795
Iteration 176/1000 | Loss: 0.00000795
Iteration 177/1000 | Loss: 0.00000795
Iteration 178/1000 | Loss: 0.00000795
Iteration 179/1000 | Loss: 0.00000795
Iteration 180/1000 | Loss: 0.00000795
Iteration 181/1000 | Loss: 0.00000795
Iteration 182/1000 | Loss: 0.00000795
Iteration 183/1000 | Loss: 0.00000795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [7.952359737828374e-06, 7.952359737828374e-06, 7.952359737828374e-06, 7.952359737828374e-06, 7.952359737828374e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.952359737828374e-06

Optimization complete. Final v2v error: 2.4226784706115723 mm

Highest mean error: 2.8044869899749756 mm for frame 103

Lowest mean error: 2.318873167037964 mm for frame 63

Saving results

Total time: 38.2887167930603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457510
Iteration 2/25 | Loss: 0.00107927
Iteration 3/25 | Loss: 0.00099608
Iteration 4/25 | Loss: 0.00099092
Iteration 5/25 | Loss: 0.00098963
Iteration 6/25 | Loss: 0.00098963
Iteration 7/25 | Loss: 0.00098963
Iteration 8/25 | Loss: 0.00098963
Iteration 9/25 | Loss: 0.00098963
Iteration 10/25 | Loss: 0.00098963
Iteration 11/25 | Loss: 0.00098963
Iteration 12/25 | Loss: 0.00098963
Iteration 13/25 | Loss: 0.00098963
Iteration 14/25 | Loss: 0.00098963
Iteration 15/25 | Loss: 0.00098963
Iteration 16/25 | Loss: 0.00098963
Iteration 17/25 | Loss: 0.00098963
Iteration 18/25 | Loss: 0.00098963
Iteration 19/25 | Loss: 0.00098963
Iteration 20/25 | Loss: 0.00098963
Iteration 21/25 | Loss: 0.00098963
Iteration 22/25 | Loss: 0.00098963
Iteration 23/25 | Loss: 0.00098963
Iteration 24/25 | Loss: 0.00098963
Iteration 25/25 | Loss: 0.00098963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38107038
Iteration 2/25 | Loss: 0.00072596
Iteration 3/25 | Loss: 0.00072594
Iteration 4/25 | Loss: 0.00072594
Iteration 5/25 | Loss: 0.00072594
Iteration 6/25 | Loss: 0.00072594
Iteration 7/25 | Loss: 0.00072594
Iteration 8/25 | Loss: 0.00072594
Iteration 9/25 | Loss: 0.00072594
Iteration 10/25 | Loss: 0.00072594
Iteration 11/25 | Loss: 0.00072594
Iteration 12/25 | Loss: 0.00072594
Iteration 13/25 | Loss: 0.00072594
Iteration 14/25 | Loss: 0.00072594
Iteration 15/25 | Loss: 0.00072594
Iteration 16/25 | Loss: 0.00072594
Iteration 17/25 | Loss: 0.00072594
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007259405101649463, 0.0007259405101649463, 0.0007259405101649463, 0.0007259405101649463, 0.0007259405101649463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007259405101649463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072594
Iteration 2/1000 | Loss: 0.00002064
Iteration 3/1000 | Loss: 0.00001267
Iteration 4/1000 | Loss: 0.00001075
Iteration 5/1000 | Loss: 0.00001003
Iteration 6/1000 | Loss: 0.00000959
Iteration 7/1000 | Loss: 0.00000931
Iteration 8/1000 | Loss: 0.00000909
Iteration 9/1000 | Loss: 0.00000903
Iteration 10/1000 | Loss: 0.00000902
Iteration 11/1000 | Loss: 0.00000901
Iteration 12/1000 | Loss: 0.00000901
Iteration 13/1000 | Loss: 0.00000900
Iteration 14/1000 | Loss: 0.00000900
Iteration 15/1000 | Loss: 0.00000899
Iteration 16/1000 | Loss: 0.00000899
Iteration 17/1000 | Loss: 0.00000899
Iteration 18/1000 | Loss: 0.00000899
Iteration 19/1000 | Loss: 0.00000899
Iteration 20/1000 | Loss: 0.00000899
Iteration 21/1000 | Loss: 0.00000898
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000897
Iteration 25/1000 | Loss: 0.00000897
Iteration 26/1000 | Loss: 0.00000896
Iteration 27/1000 | Loss: 0.00000896
Iteration 28/1000 | Loss: 0.00000894
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000893
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000891
Iteration 33/1000 | Loss: 0.00000891
Iteration 34/1000 | Loss: 0.00000891
Iteration 35/1000 | Loss: 0.00000891
Iteration 36/1000 | Loss: 0.00000891
Iteration 37/1000 | Loss: 0.00000891
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000889
Iteration 41/1000 | Loss: 0.00000888
Iteration 42/1000 | Loss: 0.00000888
Iteration 43/1000 | Loss: 0.00000887
Iteration 44/1000 | Loss: 0.00000887
Iteration 45/1000 | Loss: 0.00000886
Iteration 46/1000 | Loss: 0.00000885
Iteration 47/1000 | Loss: 0.00000885
Iteration 48/1000 | Loss: 0.00000884
Iteration 49/1000 | Loss: 0.00000884
Iteration 50/1000 | Loss: 0.00000883
Iteration 51/1000 | Loss: 0.00000883
Iteration 52/1000 | Loss: 0.00000883
Iteration 53/1000 | Loss: 0.00000883
Iteration 54/1000 | Loss: 0.00000882
Iteration 55/1000 | Loss: 0.00000882
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Iteration 59/1000 | Loss: 0.00000880
Iteration 60/1000 | Loss: 0.00000880
Iteration 61/1000 | Loss: 0.00000879
Iteration 62/1000 | Loss: 0.00000879
Iteration 63/1000 | Loss: 0.00000879
Iteration 64/1000 | Loss: 0.00000879
Iteration 65/1000 | Loss: 0.00000879
Iteration 66/1000 | Loss: 0.00000878
Iteration 67/1000 | Loss: 0.00000877
Iteration 68/1000 | Loss: 0.00000877
Iteration 69/1000 | Loss: 0.00000877
Iteration 70/1000 | Loss: 0.00000876
Iteration 71/1000 | Loss: 0.00000876
Iteration 72/1000 | Loss: 0.00000876
Iteration 73/1000 | Loss: 0.00000876
Iteration 74/1000 | Loss: 0.00000876
Iteration 75/1000 | Loss: 0.00000876
Iteration 76/1000 | Loss: 0.00000876
Iteration 77/1000 | Loss: 0.00000876
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000875
Iteration 80/1000 | Loss: 0.00000875
Iteration 81/1000 | Loss: 0.00000875
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000874
Iteration 84/1000 | Loss: 0.00000873
Iteration 85/1000 | Loss: 0.00000873
Iteration 86/1000 | Loss: 0.00000873
Iteration 87/1000 | Loss: 0.00000873
Iteration 88/1000 | Loss: 0.00000873
Iteration 89/1000 | Loss: 0.00000873
Iteration 90/1000 | Loss: 0.00000873
Iteration 91/1000 | Loss: 0.00000873
Iteration 92/1000 | Loss: 0.00000872
Iteration 93/1000 | Loss: 0.00000872
Iteration 94/1000 | Loss: 0.00000872
Iteration 95/1000 | Loss: 0.00000872
Iteration 96/1000 | Loss: 0.00000872
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000871
Iteration 99/1000 | Loss: 0.00000871
Iteration 100/1000 | Loss: 0.00000871
Iteration 101/1000 | Loss: 0.00000871
Iteration 102/1000 | Loss: 0.00000871
Iteration 103/1000 | Loss: 0.00000870
Iteration 104/1000 | Loss: 0.00000870
Iteration 105/1000 | Loss: 0.00000870
Iteration 106/1000 | Loss: 0.00000870
Iteration 107/1000 | Loss: 0.00000870
Iteration 108/1000 | Loss: 0.00000870
Iteration 109/1000 | Loss: 0.00000870
Iteration 110/1000 | Loss: 0.00000869
Iteration 111/1000 | Loss: 0.00000869
Iteration 112/1000 | Loss: 0.00000869
Iteration 113/1000 | Loss: 0.00000869
Iteration 114/1000 | Loss: 0.00000869
Iteration 115/1000 | Loss: 0.00000869
Iteration 116/1000 | Loss: 0.00000869
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000868
Iteration 120/1000 | Loss: 0.00000868
Iteration 121/1000 | Loss: 0.00000868
Iteration 122/1000 | Loss: 0.00000868
Iteration 123/1000 | Loss: 0.00000868
Iteration 124/1000 | Loss: 0.00000868
Iteration 125/1000 | Loss: 0.00000868
Iteration 126/1000 | Loss: 0.00000867
Iteration 127/1000 | Loss: 0.00000867
Iteration 128/1000 | Loss: 0.00000867
Iteration 129/1000 | Loss: 0.00000867
Iteration 130/1000 | Loss: 0.00000867
Iteration 131/1000 | Loss: 0.00000867
Iteration 132/1000 | Loss: 0.00000867
Iteration 133/1000 | Loss: 0.00000867
Iteration 134/1000 | Loss: 0.00000867
Iteration 135/1000 | Loss: 0.00000866
Iteration 136/1000 | Loss: 0.00000866
Iteration 137/1000 | Loss: 0.00000866
Iteration 138/1000 | Loss: 0.00000866
Iteration 139/1000 | Loss: 0.00000866
Iteration 140/1000 | Loss: 0.00000866
Iteration 141/1000 | Loss: 0.00000866
Iteration 142/1000 | Loss: 0.00000866
Iteration 143/1000 | Loss: 0.00000866
Iteration 144/1000 | Loss: 0.00000866
Iteration 145/1000 | Loss: 0.00000866
Iteration 146/1000 | Loss: 0.00000865
Iteration 147/1000 | Loss: 0.00000865
Iteration 148/1000 | Loss: 0.00000865
Iteration 149/1000 | Loss: 0.00000865
Iteration 150/1000 | Loss: 0.00000865
Iteration 151/1000 | Loss: 0.00000865
Iteration 152/1000 | Loss: 0.00000865
Iteration 153/1000 | Loss: 0.00000865
Iteration 154/1000 | Loss: 0.00000864
Iteration 155/1000 | Loss: 0.00000864
Iteration 156/1000 | Loss: 0.00000864
Iteration 157/1000 | Loss: 0.00000864
Iteration 158/1000 | Loss: 0.00000864
Iteration 159/1000 | Loss: 0.00000864
Iteration 160/1000 | Loss: 0.00000864
Iteration 161/1000 | Loss: 0.00000864
Iteration 162/1000 | Loss: 0.00000864
Iteration 163/1000 | Loss: 0.00000864
Iteration 164/1000 | Loss: 0.00000864
Iteration 165/1000 | Loss: 0.00000864
Iteration 166/1000 | Loss: 0.00000864
Iteration 167/1000 | Loss: 0.00000863
Iteration 168/1000 | Loss: 0.00000863
Iteration 169/1000 | Loss: 0.00000863
Iteration 170/1000 | Loss: 0.00000863
Iteration 171/1000 | Loss: 0.00000863
Iteration 172/1000 | Loss: 0.00000863
Iteration 173/1000 | Loss: 0.00000863
Iteration 174/1000 | Loss: 0.00000863
Iteration 175/1000 | Loss: 0.00000863
Iteration 176/1000 | Loss: 0.00000863
Iteration 177/1000 | Loss: 0.00000863
Iteration 178/1000 | Loss: 0.00000863
Iteration 179/1000 | Loss: 0.00000863
Iteration 180/1000 | Loss: 0.00000863
Iteration 181/1000 | Loss: 0.00000863
Iteration 182/1000 | Loss: 0.00000862
Iteration 183/1000 | Loss: 0.00000862
Iteration 184/1000 | Loss: 0.00000862
Iteration 185/1000 | Loss: 0.00000862
Iteration 186/1000 | Loss: 0.00000862
Iteration 187/1000 | Loss: 0.00000862
Iteration 188/1000 | Loss: 0.00000862
Iteration 189/1000 | Loss: 0.00000862
Iteration 190/1000 | Loss: 0.00000862
Iteration 191/1000 | Loss: 0.00000862
Iteration 192/1000 | Loss: 0.00000862
Iteration 193/1000 | Loss: 0.00000862
Iteration 194/1000 | Loss: 0.00000862
Iteration 195/1000 | Loss: 0.00000862
Iteration 196/1000 | Loss: 0.00000862
Iteration 197/1000 | Loss: 0.00000862
Iteration 198/1000 | Loss: 0.00000862
Iteration 199/1000 | Loss: 0.00000861
Iteration 200/1000 | Loss: 0.00000861
Iteration 201/1000 | Loss: 0.00000861
Iteration 202/1000 | Loss: 0.00000861
Iteration 203/1000 | Loss: 0.00000861
Iteration 204/1000 | Loss: 0.00000861
Iteration 205/1000 | Loss: 0.00000861
Iteration 206/1000 | Loss: 0.00000861
Iteration 207/1000 | Loss: 0.00000861
Iteration 208/1000 | Loss: 0.00000861
Iteration 209/1000 | Loss: 0.00000861
Iteration 210/1000 | Loss: 0.00000861
Iteration 211/1000 | Loss: 0.00000861
Iteration 212/1000 | Loss: 0.00000861
Iteration 213/1000 | Loss: 0.00000861
Iteration 214/1000 | Loss: 0.00000861
Iteration 215/1000 | Loss: 0.00000861
Iteration 216/1000 | Loss: 0.00000861
Iteration 217/1000 | Loss: 0.00000861
Iteration 218/1000 | Loss: 0.00000860
Iteration 219/1000 | Loss: 0.00000860
Iteration 220/1000 | Loss: 0.00000860
Iteration 221/1000 | Loss: 0.00000860
Iteration 222/1000 | Loss: 0.00000860
Iteration 223/1000 | Loss: 0.00000860
Iteration 224/1000 | Loss: 0.00000860
Iteration 225/1000 | Loss: 0.00000860
Iteration 226/1000 | Loss: 0.00000859
Iteration 227/1000 | Loss: 0.00000859
Iteration 228/1000 | Loss: 0.00000859
Iteration 229/1000 | Loss: 0.00000859
Iteration 230/1000 | Loss: 0.00000859
Iteration 231/1000 | Loss: 0.00000859
Iteration 232/1000 | Loss: 0.00000859
Iteration 233/1000 | Loss: 0.00000859
Iteration 234/1000 | Loss: 0.00000859
Iteration 235/1000 | Loss: 0.00000859
Iteration 236/1000 | Loss: 0.00000859
Iteration 237/1000 | Loss: 0.00000859
Iteration 238/1000 | Loss: 0.00000859
Iteration 239/1000 | Loss: 0.00000859
Iteration 240/1000 | Loss: 0.00000859
Iteration 241/1000 | Loss: 0.00000859
Iteration 242/1000 | Loss: 0.00000859
Iteration 243/1000 | Loss: 0.00000859
Iteration 244/1000 | Loss: 0.00000859
Iteration 245/1000 | Loss: 0.00000859
Iteration 246/1000 | Loss: 0.00000859
Iteration 247/1000 | Loss: 0.00000859
Iteration 248/1000 | Loss: 0.00000859
Iteration 249/1000 | Loss: 0.00000859
Iteration 250/1000 | Loss: 0.00000859
Iteration 251/1000 | Loss: 0.00000859
Iteration 252/1000 | Loss: 0.00000859
Iteration 253/1000 | Loss: 0.00000859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [8.587033335061278e-06, 8.587033335061278e-06, 8.587033335061278e-06, 8.587033335061278e-06, 8.587033335061278e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.587033335061278e-06

Optimization complete. Final v2v error: 2.379595994949341 mm

Highest mean error: 3.0355138778686523 mm for frame 71

Lowest mean error: 2.1143338680267334 mm for frame 116

Saving results

Total time: 35.89165019989014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394587
Iteration 2/25 | Loss: 0.00119704
Iteration 3/25 | Loss: 0.00100652
Iteration 4/25 | Loss: 0.00098978
Iteration 5/25 | Loss: 0.00098746
Iteration 6/25 | Loss: 0.00098691
Iteration 7/25 | Loss: 0.00098691
Iteration 8/25 | Loss: 0.00098691
Iteration 9/25 | Loss: 0.00098691
Iteration 10/25 | Loss: 0.00098691
Iteration 11/25 | Loss: 0.00098691
Iteration 12/25 | Loss: 0.00098691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009869069326668978, 0.0009869069326668978, 0.0009869069326668978, 0.0009869069326668978, 0.0009869069326668978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009869069326668978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37194431
Iteration 2/25 | Loss: 0.00047562
Iteration 3/25 | Loss: 0.00047562
Iteration 4/25 | Loss: 0.00047562
Iteration 5/25 | Loss: 0.00047561
Iteration 6/25 | Loss: 0.00047561
Iteration 7/25 | Loss: 0.00047561
Iteration 8/25 | Loss: 0.00047561
Iteration 9/25 | Loss: 0.00047561
Iteration 10/25 | Loss: 0.00047561
Iteration 11/25 | Loss: 0.00047561
Iteration 12/25 | Loss: 0.00047561
Iteration 13/25 | Loss: 0.00047561
Iteration 14/25 | Loss: 0.00047561
Iteration 15/25 | Loss: 0.00047561
Iteration 16/25 | Loss: 0.00047561
Iteration 17/25 | Loss: 0.00047561
Iteration 18/25 | Loss: 0.00047561
Iteration 19/25 | Loss: 0.00047561
Iteration 20/25 | Loss: 0.00047561
Iteration 21/25 | Loss: 0.00047561
Iteration 22/25 | Loss: 0.00047561
Iteration 23/25 | Loss: 0.00047561
Iteration 24/25 | Loss: 0.00047561
Iteration 25/25 | Loss: 0.00047561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047561
Iteration 2/1000 | Loss: 0.00002123
Iteration 3/1000 | Loss: 0.00001410
Iteration 4/1000 | Loss: 0.00001260
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001119
Iteration 7/1000 | Loss: 0.00001080
Iteration 8/1000 | Loss: 0.00001078
Iteration 9/1000 | Loss: 0.00001058
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001040
Iteration 12/1000 | Loss: 0.00001035
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001027
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001026
Iteration 24/1000 | Loss: 0.00001025
Iteration 25/1000 | Loss: 0.00001023
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001018
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001018
Iteration 33/1000 | Loss: 0.00001018
Iteration 34/1000 | Loss: 0.00001018
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001013
Iteration 39/1000 | Loss: 0.00001013
Iteration 40/1000 | Loss: 0.00001013
Iteration 41/1000 | Loss: 0.00001013
Iteration 42/1000 | Loss: 0.00001013
Iteration 43/1000 | Loss: 0.00001013
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001013
Iteration 47/1000 | Loss: 0.00001013
Iteration 48/1000 | Loss: 0.00001013
Iteration 49/1000 | Loss: 0.00001012
Iteration 50/1000 | Loss: 0.00001012
Iteration 51/1000 | Loss: 0.00001012
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001010
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001010
Iteration 56/1000 | Loss: 0.00001010
Iteration 57/1000 | Loss: 0.00001010
Iteration 58/1000 | Loss: 0.00001009
Iteration 59/1000 | Loss: 0.00001009
Iteration 60/1000 | Loss: 0.00001009
Iteration 61/1000 | Loss: 0.00001009
Iteration 62/1000 | Loss: 0.00001009
Iteration 63/1000 | Loss: 0.00001008
Iteration 64/1000 | Loss: 0.00001008
Iteration 65/1000 | Loss: 0.00001008
Iteration 66/1000 | Loss: 0.00001008
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001007
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001006
Iteration 76/1000 | Loss: 0.00001006
Iteration 77/1000 | Loss: 0.00001005
Iteration 78/1000 | Loss: 0.00001004
Iteration 79/1000 | Loss: 0.00001004
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001003
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00001003
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001002
Iteration 90/1000 | Loss: 0.00001002
Iteration 91/1000 | Loss: 0.00001002
Iteration 92/1000 | Loss: 0.00001002
Iteration 93/1000 | Loss: 0.00001001
Iteration 94/1000 | Loss: 0.00001001
Iteration 95/1000 | Loss: 0.00001001
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001000
Iteration 99/1000 | Loss: 0.00001000
Iteration 100/1000 | Loss: 0.00001000
Iteration 101/1000 | Loss: 0.00001000
Iteration 102/1000 | Loss: 0.00001000
Iteration 103/1000 | Loss: 0.00000999
Iteration 104/1000 | Loss: 0.00000999
Iteration 105/1000 | Loss: 0.00000999
Iteration 106/1000 | Loss: 0.00000998
Iteration 107/1000 | Loss: 0.00000998
Iteration 108/1000 | Loss: 0.00000998
Iteration 109/1000 | Loss: 0.00000998
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000994
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000992
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000992
Iteration 134/1000 | Loss: 0.00000992
Iteration 135/1000 | Loss: 0.00000992
Iteration 136/1000 | Loss: 0.00000991
Iteration 137/1000 | Loss: 0.00000991
Iteration 138/1000 | Loss: 0.00000991
Iteration 139/1000 | Loss: 0.00000990
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000989
Iteration 142/1000 | Loss: 0.00000989
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000988
Iteration 145/1000 | Loss: 0.00000988
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000984
Iteration 162/1000 | Loss: 0.00000984
Iteration 163/1000 | Loss: 0.00000984
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000983
Iteration 168/1000 | Loss: 0.00000983
Iteration 169/1000 | Loss: 0.00000983
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000982
Iteration 172/1000 | Loss: 0.00000982
Iteration 173/1000 | Loss: 0.00000982
Iteration 174/1000 | Loss: 0.00000982
Iteration 175/1000 | Loss: 0.00000982
Iteration 176/1000 | Loss: 0.00000982
Iteration 177/1000 | Loss: 0.00000982
Iteration 178/1000 | Loss: 0.00000982
Iteration 179/1000 | Loss: 0.00000981
Iteration 180/1000 | Loss: 0.00000981
Iteration 181/1000 | Loss: 0.00000981
Iteration 182/1000 | Loss: 0.00000981
Iteration 183/1000 | Loss: 0.00000981
Iteration 184/1000 | Loss: 0.00000981
Iteration 185/1000 | Loss: 0.00000981
Iteration 186/1000 | Loss: 0.00000981
Iteration 187/1000 | Loss: 0.00000980
Iteration 188/1000 | Loss: 0.00000980
Iteration 189/1000 | Loss: 0.00000980
Iteration 190/1000 | Loss: 0.00000980
Iteration 191/1000 | Loss: 0.00000980
Iteration 192/1000 | Loss: 0.00000979
Iteration 193/1000 | Loss: 0.00000979
Iteration 194/1000 | Loss: 0.00000979
Iteration 195/1000 | Loss: 0.00000979
Iteration 196/1000 | Loss: 0.00000979
Iteration 197/1000 | Loss: 0.00000979
Iteration 198/1000 | Loss: 0.00000979
Iteration 199/1000 | Loss: 0.00000979
Iteration 200/1000 | Loss: 0.00000978
Iteration 201/1000 | Loss: 0.00000978
Iteration 202/1000 | Loss: 0.00000978
Iteration 203/1000 | Loss: 0.00000978
Iteration 204/1000 | Loss: 0.00000978
Iteration 205/1000 | Loss: 0.00000978
Iteration 206/1000 | Loss: 0.00000978
Iteration 207/1000 | Loss: 0.00000978
Iteration 208/1000 | Loss: 0.00000978
Iteration 209/1000 | Loss: 0.00000978
Iteration 210/1000 | Loss: 0.00000978
Iteration 211/1000 | Loss: 0.00000978
Iteration 212/1000 | Loss: 0.00000978
Iteration 213/1000 | Loss: 0.00000977
Iteration 214/1000 | Loss: 0.00000977
Iteration 215/1000 | Loss: 0.00000977
Iteration 216/1000 | Loss: 0.00000977
Iteration 217/1000 | Loss: 0.00000977
Iteration 218/1000 | Loss: 0.00000977
Iteration 219/1000 | Loss: 0.00000977
Iteration 220/1000 | Loss: 0.00000977
Iteration 221/1000 | Loss: 0.00000977
Iteration 222/1000 | Loss: 0.00000977
Iteration 223/1000 | Loss: 0.00000977
Iteration 224/1000 | Loss: 0.00000977
Iteration 225/1000 | Loss: 0.00000977
Iteration 226/1000 | Loss: 0.00000977
Iteration 227/1000 | Loss: 0.00000977
Iteration 228/1000 | Loss: 0.00000977
Iteration 229/1000 | Loss: 0.00000977
Iteration 230/1000 | Loss: 0.00000977
Iteration 231/1000 | Loss: 0.00000977
Iteration 232/1000 | Loss: 0.00000977
Iteration 233/1000 | Loss: 0.00000977
Iteration 234/1000 | Loss: 0.00000977
Iteration 235/1000 | Loss: 0.00000977
Iteration 236/1000 | Loss: 0.00000976
Iteration 237/1000 | Loss: 0.00000976
Iteration 238/1000 | Loss: 0.00000976
Iteration 239/1000 | Loss: 0.00000976
Iteration 240/1000 | Loss: 0.00000976
Iteration 241/1000 | Loss: 0.00000976
Iteration 242/1000 | Loss: 0.00000976
Iteration 243/1000 | Loss: 0.00000976
Iteration 244/1000 | Loss: 0.00000976
Iteration 245/1000 | Loss: 0.00000975
Iteration 246/1000 | Loss: 0.00000975
Iteration 247/1000 | Loss: 0.00000975
Iteration 248/1000 | Loss: 0.00000975
Iteration 249/1000 | Loss: 0.00000975
Iteration 250/1000 | Loss: 0.00000975
Iteration 251/1000 | Loss: 0.00000975
Iteration 252/1000 | Loss: 0.00000975
Iteration 253/1000 | Loss: 0.00000975
Iteration 254/1000 | Loss: 0.00000975
Iteration 255/1000 | Loss: 0.00000975
Iteration 256/1000 | Loss: 0.00000975
Iteration 257/1000 | Loss: 0.00000975
Iteration 258/1000 | Loss: 0.00000975
Iteration 259/1000 | Loss: 0.00000975
Iteration 260/1000 | Loss: 0.00000975
Iteration 261/1000 | Loss: 0.00000975
Iteration 262/1000 | Loss: 0.00000975
Iteration 263/1000 | Loss: 0.00000975
Iteration 264/1000 | Loss: 0.00000975
Iteration 265/1000 | Loss: 0.00000975
Iteration 266/1000 | Loss: 0.00000975
Iteration 267/1000 | Loss: 0.00000975
Iteration 268/1000 | Loss: 0.00000975
Iteration 269/1000 | Loss: 0.00000975
Iteration 270/1000 | Loss: 0.00000975
Iteration 271/1000 | Loss: 0.00000975
Iteration 272/1000 | Loss: 0.00000975
Iteration 273/1000 | Loss: 0.00000975
Iteration 274/1000 | Loss: 0.00000975
Iteration 275/1000 | Loss: 0.00000975
Iteration 276/1000 | Loss: 0.00000975
Iteration 277/1000 | Loss: 0.00000975
Iteration 278/1000 | Loss: 0.00000975
Iteration 279/1000 | Loss: 0.00000975
Iteration 280/1000 | Loss: 0.00000975
Iteration 281/1000 | Loss: 0.00000975
Iteration 282/1000 | Loss: 0.00000975
Iteration 283/1000 | Loss: 0.00000975
Iteration 284/1000 | Loss: 0.00000975
Iteration 285/1000 | Loss: 0.00000975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [9.752324331202544e-06, 9.752324331202544e-06, 9.752324331202544e-06, 9.752324331202544e-06, 9.752324331202544e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.752324331202544e-06

Optimization complete. Final v2v error: 2.678327798843384 mm

Highest mean error: 2.807457447052002 mm for frame 28

Lowest mean error: 2.5569169521331787 mm for frame 1

Saving results

Total time: 41.23691010475159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829240
Iteration 2/25 | Loss: 0.00126674
Iteration 3/25 | Loss: 0.00104257
Iteration 4/25 | Loss: 0.00100795
Iteration 5/25 | Loss: 0.00100314
Iteration 6/25 | Loss: 0.00100319
Iteration 7/25 | Loss: 0.00100140
Iteration 8/25 | Loss: 0.00100068
Iteration 9/25 | Loss: 0.00100007
Iteration 10/25 | Loss: 0.00099966
Iteration 11/25 | Loss: 0.00099950
Iteration 12/25 | Loss: 0.00099941
Iteration 13/25 | Loss: 0.00099938
Iteration 14/25 | Loss: 0.00099937
Iteration 15/25 | Loss: 0.00099937
Iteration 16/25 | Loss: 0.00099937
Iteration 17/25 | Loss: 0.00099936
Iteration 18/25 | Loss: 0.00099936
Iteration 19/25 | Loss: 0.00099936
Iteration 20/25 | Loss: 0.00099936
Iteration 21/25 | Loss: 0.00099936
Iteration 22/25 | Loss: 0.00099936
Iteration 23/25 | Loss: 0.00099936
Iteration 24/25 | Loss: 0.00099935
Iteration 25/25 | Loss: 0.00099935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58398771
Iteration 2/25 | Loss: 0.00064308
Iteration 3/25 | Loss: 0.00064308
Iteration 4/25 | Loss: 0.00064308
Iteration 5/25 | Loss: 0.00064308
Iteration 6/25 | Loss: 0.00064308
Iteration 7/25 | Loss: 0.00064308
Iteration 8/25 | Loss: 0.00064308
Iteration 9/25 | Loss: 0.00064308
Iteration 10/25 | Loss: 0.00064308
Iteration 11/25 | Loss: 0.00064308
Iteration 12/25 | Loss: 0.00064308
Iteration 13/25 | Loss: 0.00064308
Iteration 14/25 | Loss: 0.00064308
Iteration 15/25 | Loss: 0.00064308
Iteration 16/25 | Loss: 0.00064308
Iteration 17/25 | Loss: 0.00064308
Iteration 18/25 | Loss: 0.00064308
Iteration 19/25 | Loss: 0.00064308
Iteration 20/25 | Loss: 0.00064308
Iteration 21/25 | Loss: 0.00064308
Iteration 22/25 | Loss: 0.00064308
Iteration 23/25 | Loss: 0.00064308
Iteration 24/25 | Loss: 0.00064308
Iteration 25/25 | Loss: 0.00064308
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006430805078707635, 0.0006430805078707635, 0.0006430805078707635, 0.0006430805078707635, 0.0006430805078707635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006430805078707635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064308
Iteration 2/1000 | Loss: 0.00001665
Iteration 3/1000 | Loss: 0.00001273
Iteration 4/1000 | Loss: 0.00001166
Iteration 5/1000 | Loss: 0.00001108
Iteration 6/1000 | Loss: 0.00001077
Iteration 7/1000 | Loss: 0.00001053
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001012
Iteration 10/1000 | Loss: 0.00001008
Iteration 11/1000 | Loss: 0.00001005
Iteration 12/1000 | Loss: 0.00000998
Iteration 13/1000 | Loss: 0.00000994
Iteration 14/1000 | Loss: 0.00000990
Iteration 15/1000 | Loss: 0.00000989
Iteration 16/1000 | Loss: 0.00000988
Iteration 17/1000 | Loss: 0.00000986
Iteration 18/1000 | Loss: 0.00000986
Iteration 19/1000 | Loss: 0.00000986
Iteration 20/1000 | Loss: 0.00000986
Iteration 21/1000 | Loss: 0.00000985
Iteration 22/1000 | Loss: 0.00000985
Iteration 23/1000 | Loss: 0.00000984
Iteration 24/1000 | Loss: 0.00000984
Iteration 25/1000 | Loss: 0.00000983
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000982
Iteration 28/1000 | Loss: 0.00000982
Iteration 29/1000 | Loss: 0.00000982
Iteration 30/1000 | Loss: 0.00000981
Iteration 31/1000 | Loss: 0.00000981
Iteration 32/1000 | Loss: 0.00000981
Iteration 33/1000 | Loss: 0.00000981
Iteration 34/1000 | Loss: 0.00000980
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000980
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000979
Iteration 39/1000 | Loss: 0.00000979
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000978
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000978
Iteration 45/1000 | Loss: 0.00000978
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000978
Iteration 49/1000 | Loss: 0.00000978
Iteration 50/1000 | Loss: 0.00000978
Iteration 51/1000 | Loss: 0.00000978
Iteration 52/1000 | Loss: 0.00000978
Iteration 53/1000 | Loss: 0.00000978
Iteration 54/1000 | Loss: 0.00000977
Iteration 55/1000 | Loss: 0.00000977
Iteration 56/1000 | Loss: 0.00000977
Iteration 57/1000 | Loss: 0.00000977
Iteration 58/1000 | Loss: 0.00000976
Iteration 59/1000 | Loss: 0.00000976
Iteration 60/1000 | Loss: 0.00000976
Iteration 61/1000 | Loss: 0.00000976
Iteration 62/1000 | Loss: 0.00000976
Iteration 63/1000 | Loss: 0.00000975
Iteration 64/1000 | Loss: 0.00000975
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000975
Iteration 68/1000 | Loss: 0.00000975
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000974
Iteration 73/1000 | Loss: 0.00000974
Iteration 74/1000 | Loss: 0.00000973
Iteration 75/1000 | Loss: 0.00000973
Iteration 76/1000 | Loss: 0.00000973
Iteration 77/1000 | Loss: 0.00000973
Iteration 78/1000 | Loss: 0.00000972
Iteration 79/1000 | Loss: 0.00000972
Iteration 80/1000 | Loss: 0.00000972
Iteration 81/1000 | Loss: 0.00000972
Iteration 82/1000 | Loss: 0.00000972
Iteration 83/1000 | Loss: 0.00000971
Iteration 84/1000 | Loss: 0.00000971
Iteration 85/1000 | Loss: 0.00000970
Iteration 86/1000 | Loss: 0.00000970
Iteration 87/1000 | Loss: 0.00000969
Iteration 88/1000 | Loss: 0.00000969
Iteration 89/1000 | Loss: 0.00000969
Iteration 90/1000 | Loss: 0.00000969
Iteration 91/1000 | Loss: 0.00000969
Iteration 92/1000 | Loss: 0.00000969
Iteration 93/1000 | Loss: 0.00000969
Iteration 94/1000 | Loss: 0.00000968
Iteration 95/1000 | Loss: 0.00000967
Iteration 96/1000 | Loss: 0.00000967
Iteration 97/1000 | Loss: 0.00000967
Iteration 98/1000 | Loss: 0.00000966
Iteration 99/1000 | Loss: 0.00000966
Iteration 100/1000 | Loss: 0.00000966
Iteration 101/1000 | Loss: 0.00000966
Iteration 102/1000 | Loss: 0.00000966
Iteration 103/1000 | Loss: 0.00000966
Iteration 104/1000 | Loss: 0.00000966
Iteration 105/1000 | Loss: 0.00000965
Iteration 106/1000 | Loss: 0.00000965
Iteration 107/1000 | Loss: 0.00000965
Iteration 108/1000 | Loss: 0.00000965
Iteration 109/1000 | Loss: 0.00000965
Iteration 110/1000 | Loss: 0.00000965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [9.65425351751037e-06, 9.65425351751037e-06, 9.65425351751037e-06, 9.65425351751037e-06, 9.65425351751037e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.65425351751037e-06

Optimization complete. Final v2v error: 2.6543939113616943 mm

Highest mean error: 2.895796298980713 mm for frame 232

Lowest mean error: 2.390868902206421 mm for frame 42

Saving results

Total time: 48.9088077545166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080007
Iteration 2/25 | Loss: 0.00286246
Iteration 3/25 | Loss: 0.00198024
Iteration 4/25 | Loss: 0.00207999
Iteration 5/25 | Loss: 0.00206481
Iteration 6/25 | Loss: 0.00203487
Iteration 7/25 | Loss: 0.00187843
Iteration 8/25 | Loss: 0.00174809
Iteration 9/25 | Loss: 0.00166082
Iteration 10/25 | Loss: 0.00160422
Iteration 11/25 | Loss: 0.00157089
Iteration 12/25 | Loss: 0.00156642
Iteration 13/25 | Loss: 0.00154286
Iteration 14/25 | Loss: 0.00152637
Iteration 15/25 | Loss: 0.00153033
Iteration 16/25 | Loss: 0.00152010
Iteration 17/25 | Loss: 0.00153783
Iteration 18/25 | Loss: 0.00152151
Iteration 19/25 | Loss: 0.00150622
Iteration 20/25 | Loss: 0.00149798
Iteration 21/25 | Loss: 0.00150083
Iteration 22/25 | Loss: 0.00148814
Iteration 23/25 | Loss: 0.00146989
Iteration 24/25 | Loss: 0.00146172
Iteration 25/25 | Loss: 0.00146548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67235750
Iteration 2/25 | Loss: 0.00159508
Iteration 3/25 | Loss: 0.00159507
Iteration 4/25 | Loss: 0.00159507
Iteration 5/25 | Loss: 0.00159507
Iteration 6/25 | Loss: 0.00159507
Iteration 7/25 | Loss: 0.00159507
Iteration 8/25 | Loss: 0.00159507
Iteration 9/25 | Loss: 0.00159507
Iteration 10/25 | Loss: 0.00159507
Iteration 11/25 | Loss: 0.00159507
Iteration 12/25 | Loss: 0.00159507
Iteration 13/25 | Loss: 0.00159507
Iteration 14/25 | Loss: 0.00159507
Iteration 15/25 | Loss: 0.00159507
Iteration 16/25 | Loss: 0.00159507
Iteration 17/25 | Loss: 0.00159507
Iteration 18/25 | Loss: 0.00159507
Iteration 19/25 | Loss: 0.00159507
Iteration 20/25 | Loss: 0.00159507
Iteration 21/25 | Loss: 0.00159507
Iteration 22/25 | Loss: 0.00159507
Iteration 23/25 | Loss: 0.00159507
Iteration 24/25 | Loss: 0.00159507
Iteration 25/25 | Loss: 0.00159507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159507
Iteration 2/1000 | Loss: 0.00070853
Iteration 3/1000 | Loss: 0.00074195
Iteration 4/1000 | Loss: 0.00043279
Iteration 5/1000 | Loss: 0.00045502
Iteration 6/1000 | Loss: 0.00079936
Iteration 7/1000 | Loss: 0.00097822
Iteration 8/1000 | Loss: 0.00153705
Iteration 9/1000 | Loss: 0.00093821
Iteration 10/1000 | Loss: 0.00125569
Iteration 11/1000 | Loss: 0.00089305
Iteration 12/1000 | Loss: 0.00116550
Iteration 13/1000 | Loss: 0.00125539
Iteration 14/1000 | Loss: 0.00043685
Iteration 15/1000 | Loss: 0.00065249
Iteration 16/1000 | Loss: 0.00050369
Iteration 17/1000 | Loss: 0.00049798
Iteration 18/1000 | Loss: 0.00098766
Iteration 19/1000 | Loss: 0.00095774
Iteration 20/1000 | Loss: 0.00056117
Iteration 21/1000 | Loss: 0.00054109
Iteration 22/1000 | Loss: 0.00059034
Iteration 23/1000 | Loss: 0.00060226
Iteration 24/1000 | Loss: 0.00066869
Iteration 25/1000 | Loss: 0.00100260
Iteration 26/1000 | Loss: 0.00078512
Iteration 27/1000 | Loss: 0.00085345
Iteration 28/1000 | Loss: 0.00105824
Iteration 29/1000 | Loss: 0.00105860
Iteration 30/1000 | Loss: 0.00076037
Iteration 31/1000 | Loss: 0.00084327
Iteration 32/1000 | Loss: 0.00107479
Iteration 33/1000 | Loss: 0.00123404
Iteration 34/1000 | Loss: 0.00133414
Iteration 35/1000 | Loss: 0.00084018
Iteration 36/1000 | Loss: 0.00042374
Iteration 37/1000 | Loss: 0.00060468
Iteration 38/1000 | Loss: 0.00047920
Iteration 39/1000 | Loss: 0.00032070
Iteration 40/1000 | Loss: 0.00081796
Iteration 41/1000 | Loss: 0.00052873
Iteration 42/1000 | Loss: 0.00145313
Iteration 43/1000 | Loss: 0.00161998
Iteration 44/1000 | Loss: 0.00158642
Iteration 45/1000 | Loss: 0.00070034
Iteration 46/1000 | Loss: 0.00046858
Iteration 47/1000 | Loss: 0.00086434
Iteration 48/1000 | Loss: 0.00061955
Iteration 49/1000 | Loss: 0.00024050
Iteration 50/1000 | Loss: 0.00048816
Iteration 51/1000 | Loss: 0.00056989
Iteration 52/1000 | Loss: 0.00082131
Iteration 53/1000 | Loss: 0.00011909
Iteration 54/1000 | Loss: 0.00020224
Iteration 55/1000 | Loss: 0.00016203
Iteration 56/1000 | Loss: 0.00010979
Iteration 57/1000 | Loss: 0.00009753
Iteration 58/1000 | Loss: 0.00055632
Iteration 59/1000 | Loss: 0.00055807
Iteration 60/1000 | Loss: 0.00045814
Iteration 61/1000 | Loss: 0.00053666
Iteration 62/1000 | Loss: 0.00042564
Iteration 63/1000 | Loss: 0.00051776
Iteration 64/1000 | Loss: 0.00036183
Iteration 65/1000 | Loss: 0.00055860
Iteration 66/1000 | Loss: 0.00050221
Iteration 67/1000 | Loss: 0.00042125
Iteration 68/1000 | Loss: 0.00059821
Iteration 69/1000 | Loss: 0.00046603
Iteration 70/1000 | Loss: 0.00035928
Iteration 71/1000 | Loss: 0.00056022
Iteration 72/1000 | Loss: 0.00037675
Iteration 73/1000 | Loss: 0.00030216
Iteration 74/1000 | Loss: 0.00041100
Iteration 75/1000 | Loss: 0.00052426
Iteration 76/1000 | Loss: 0.00028909
Iteration 77/1000 | Loss: 0.00020332
Iteration 78/1000 | Loss: 0.00017489
Iteration 79/1000 | Loss: 0.00026093
Iteration 80/1000 | Loss: 0.00022729
Iteration 81/1000 | Loss: 0.00034442
Iteration 82/1000 | Loss: 0.00015725
Iteration 83/1000 | Loss: 0.00021251
Iteration 84/1000 | Loss: 0.00015708
Iteration 85/1000 | Loss: 0.00031022
Iteration 86/1000 | Loss: 0.00085146
Iteration 87/1000 | Loss: 0.00041537
Iteration 88/1000 | Loss: 0.00067322
Iteration 89/1000 | Loss: 0.00010681
Iteration 90/1000 | Loss: 0.00012096
Iteration 91/1000 | Loss: 0.00020185
Iteration 92/1000 | Loss: 0.00017250
Iteration 93/1000 | Loss: 0.00017184
Iteration 94/1000 | Loss: 0.00009652
Iteration 95/1000 | Loss: 0.00008728
Iteration 96/1000 | Loss: 0.00009872
Iteration 97/1000 | Loss: 0.00009853
Iteration 98/1000 | Loss: 0.00010661
Iteration 99/1000 | Loss: 0.00015468
Iteration 100/1000 | Loss: 0.00019917
Iteration 101/1000 | Loss: 0.00020460
Iteration 102/1000 | Loss: 0.00016733
Iteration 103/1000 | Loss: 0.00030092
Iteration 104/1000 | Loss: 0.00024250
Iteration 105/1000 | Loss: 0.00015706
Iteration 106/1000 | Loss: 0.00015127
Iteration 107/1000 | Loss: 0.00019781
Iteration 108/1000 | Loss: 0.00008433
Iteration 109/1000 | Loss: 0.00008763
Iteration 110/1000 | Loss: 0.00009572
Iteration 111/1000 | Loss: 0.00024146
Iteration 112/1000 | Loss: 0.00032564
Iteration 113/1000 | Loss: 0.00040207
Iteration 114/1000 | Loss: 0.00029471
Iteration 115/1000 | Loss: 0.00036054
Iteration 116/1000 | Loss: 0.00034594
Iteration 117/1000 | Loss: 0.00033749
Iteration 118/1000 | Loss: 0.00033314
Iteration 119/1000 | Loss: 0.00009628
Iteration 120/1000 | Loss: 0.00010915
Iteration 121/1000 | Loss: 0.00010447
Iteration 122/1000 | Loss: 0.00029938
Iteration 123/1000 | Loss: 0.00035236
Iteration 124/1000 | Loss: 0.00031355
Iteration 125/1000 | Loss: 0.00009613
Iteration 126/1000 | Loss: 0.00008396
Iteration 127/1000 | Loss: 0.00058958
Iteration 128/1000 | Loss: 0.00040414
Iteration 129/1000 | Loss: 0.00065049
Iteration 130/1000 | Loss: 0.00043187
Iteration 131/1000 | Loss: 0.00025543
Iteration 132/1000 | Loss: 0.00064913
Iteration 133/1000 | Loss: 0.00009721
Iteration 134/1000 | Loss: 0.00008051
Iteration 135/1000 | Loss: 0.00026209
Iteration 136/1000 | Loss: 0.00009896
Iteration 137/1000 | Loss: 0.00009065
Iteration 138/1000 | Loss: 0.00009111
Iteration 139/1000 | Loss: 0.00008345
Iteration 140/1000 | Loss: 0.00008461
Iteration 141/1000 | Loss: 0.00009857
Iteration 142/1000 | Loss: 0.00008398
Iteration 143/1000 | Loss: 0.00008245
Iteration 144/1000 | Loss: 0.00008638
Iteration 145/1000 | Loss: 0.00008244
Iteration 146/1000 | Loss: 0.00008385
Iteration 147/1000 | Loss: 0.00009489
Iteration 148/1000 | Loss: 0.00009389
Iteration 149/1000 | Loss: 0.00009521
Iteration 150/1000 | Loss: 0.00009826
Iteration 151/1000 | Loss: 0.00011233
Iteration 152/1000 | Loss: 0.00009888
Iteration 153/1000 | Loss: 0.00010236
Iteration 154/1000 | Loss: 0.00008469
Iteration 155/1000 | Loss: 0.00008714
Iteration 156/1000 | Loss: 0.00009357
Iteration 157/1000 | Loss: 0.00010313
Iteration 158/1000 | Loss: 0.00009943
Iteration 159/1000 | Loss: 0.00009335
Iteration 160/1000 | Loss: 0.00007222
Iteration 161/1000 | Loss: 0.00025741
Iteration 162/1000 | Loss: 0.00019155
Iteration 163/1000 | Loss: 0.00017634
Iteration 164/1000 | Loss: 0.00009853
Iteration 165/1000 | Loss: 0.00008929
Iteration 166/1000 | Loss: 0.00008140
Iteration 167/1000 | Loss: 0.00007535
Iteration 168/1000 | Loss: 0.00006391
Iteration 169/1000 | Loss: 0.00007871
Iteration 170/1000 | Loss: 0.00007358
Iteration 171/1000 | Loss: 0.00007565
Iteration 172/1000 | Loss: 0.00007257
Iteration 173/1000 | Loss: 0.00007393
Iteration 174/1000 | Loss: 0.00007357
Iteration 175/1000 | Loss: 0.00008117
Iteration 176/1000 | Loss: 0.00007587
Iteration 177/1000 | Loss: 0.00006678
Iteration 178/1000 | Loss: 0.00006886
Iteration 179/1000 | Loss: 0.00007747
Iteration 180/1000 | Loss: 0.00008809
Iteration 181/1000 | Loss: 0.00007755
Iteration 182/1000 | Loss: 0.00007470
Iteration 183/1000 | Loss: 0.00007598
Iteration 184/1000 | Loss: 0.00007910
Iteration 185/1000 | Loss: 0.00007332
Iteration 186/1000 | Loss: 0.00006547
Iteration 187/1000 | Loss: 0.00007684
Iteration 188/1000 | Loss: 0.00007605
Iteration 189/1000 | Loss: 0.00007248
Iteration 190/1000 | Loss: 0.00007266
Iteration 191/1000 | Loss: 0.00006542
Iteration 192/1000 | Loss: 0.00009115
Iteration 193/1000 | Loss: 0.00006598
Iteration 194/1000 | Loss: 0.00006284
Iteration 195/1000 | Loss: 0.00006144
Iteration 196/1000 | Loss: 0.00006078
Iteration 197/1000 | Loss: 0.00006040
Iteration 198/1000 | Loss: 0.00006029
Iteration 199/1000 | Loss: 0.00006017
Iteration 200/1000 | Loss: 0.00006006
Iteration 201/1000 | Loss: 0.00006001
Iteration 202/1000 | Loss: 0.00006001
Iteration 203/1000 | Loss: 0.00006001
Iteration 204/1000 | Loss: 0.00006001
Iteration 205/1000 | Loss: 0.00006001
Iteration 206/1000 | Loss: 0.00006000
Iteration 207/1000 | Loss: 0.00006000
Iteration 208/1000 | Loss: 0.00005999
Iteration 209/1000 | Loss: 0.00005998
Iteration 210/1000 | Loss: 0.00005998
Iteration 211/1000 | Loss: 0.00005998
Iteration 212/1000 | Loss: 0.00005997
Iteration 213/1000 | Loss: 0.00005997
Iteration 214/1000 | Loss: 0.00005986
Iteration 215/1000 | Loss: 0.00005986
Iteration 216/1000 | Loss: 0.00005982
Iteration 217/1000 | Loss: 0.00005981
Iteration 218/1000 | Loss: 0.00005981
Iteration 219/1000 | Loss: 0.00005979
Iteration 220/1000 | Loss: 0.00005978
Iteration 221/1000 | Loss: 0.00005978
Iteration 222/1000 | Loss: 0.00005978
Iteration 223/1000 | Loss: 0.00005977
Iteration 224/1000 | Loss: 0.00005977
Iteration 225/1000 | Loss: 0.00005977
Iteration 226/1000 | Loss: 0.00005977
Iteration 227/1000 | Loss: 0.00005977
Iteration 228/1000 | Loss: 0.00005977
Iteration 229/1000 | Loss: 0.00005977
Iteration 230/1000 | Loss: 0.00005977
Iteration 231/1000 | Loss: 0.00005977
Iteration 232/1000 | Loss: 0.00005977
Iteration 233/1000 | Loss: 0.00005977
Iteration 234/1000 | Loss: 0.00005976
Iteration 235/1000 | Loss: 0.00005973
Iteration 236/1000 | Loss: 0.00005973
Iteration 237/1000 | Loss: 0.00005972
Iteration 238/1000 | Loss: 0.00005972
Iteration 239/1000 | Loss: 0.00005971
Iteration 240/1000 | Loss: 0.00005971
Iteration 241/1000 | Loss: 0.00005971
Iteration 242/1000 | Loss: 0.00005971
Iteration 243/1000 | Loss: 0.00005971
Iteration 244/1000 | Loss: 0.00005971
Iteration 245/1000 | Loss: 0.00005971
Iteration 246/1000 | Loss: 0.00005971
Iteration 247/1000 | Loss: 0.00005971
Iteration 248/1000 | Loss: 0.00005970
Iteration 249/1000 | Loss: 0.00005970
Iteration 250/1000 | Loss: 0.00005970
Iteration 251/1000 | Loss: 0.00005969
Iteration 252/1000 | Loss: 0.00005969
Iteration 253/1000 | Loss: 0.00005969
Iteration 254/1000 | Loss: 0.00005969
Iteration 255/1000 | Loss: 0.00005969
Iteration 256/1000 | Loss: 0.00005969
Iteration 257/1000 | Loss: 0.00005969
Iteration 258/1000 | Loss: 0.00005969
Iteration 259/1000 | Loss: 0.00005969
Iteration 260/1000 | Loss: 0.00005969
Iteration 261/1000 | Loss: 0.00005969
Iteration 262/1000 | Loss: 0.00005969
Iteration 263/1000 | Loss: 0.00005969
Iteration 264/1000 | Loss: 0.00005969
Iteration 265/1000 | Loss: 0.00005969
Iteration 266/1000 | Loss: 0.00005969
Iteration 267/1000 | Loss: 0.00005969
Iteration 268/1000 | Loss: 0.00005969
Iteration 269/1000 | Loss: 0.00005969
Iteration 270/1000 | Loss: 0.00005969
Iteration 271/1000 | Loss: 0.00005969
Iteration 272/1000 | Loss: 0.00005969
Iteration 273/1000 | Loss: 0.00005969
Iteration 274/1000 | Loss: 0.00005969
Iteration 275/1000 | Loss: 0.00005969
Iteration 276/1000 | Loss: 0.00005969
Iteration 277/1000 | Loss: 0.00005969
Iteration 278/1000 | Loss: 0.00005969
Iteration 279/1000 | Loss: 0.00005969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [5.968922778265551e-05, 5.968922778265551e-05, 5.968922778265551e-05, 5.968922778265551e-05, 5.968922778265551e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.968922778265551e-05

Optimization complete. Final v2v error: 4.687317371368408 mm

Highest mean error: 19.292255401611328 mm for frame 28

Lowest mean error: 3.645829677581787 mm for frame 30

Saving results

Total time: 342.51769399642944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608415
Iteration 2/25 | Loss: 0.00142179
Iteration 3/25 | Loss: 0.00125082
Iteration 4/25 | Loss: 0.00123251
Iteration 5/25 | Loss: 0.00122762
Iteration 6/25 | Loss: 0.00122751
Iteration 7/25 | Loss: 0.00122751
Iteration 8/25 | Loss: 0.00122751
Iteration 9/25 | Loss: 0.00122751
Iteration 10/25 | Loss: 0.00122751
Iteration 11/25 | Loss: 0.00122751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012275107437744737, 0.0012275107437744737, 0.0012275107437744737, 0.0012275107437744737, 0.0012275107437744737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012275107437744737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64053231
Iteration 2/25 | Loss: 0.00057478
Iteration 3/25 | Loss: 0.00057478
Iteration 4/25 | Loss: 0.00057477
Iteration 5/25 | Loss: 0.00057477
Iteration 6/25 | Loss: 0.00057477
Iteration 7/25 | Loss: 0.00057477
Iteration 8/25 | Loss: 0.00057477
Iteration 9/25 | Loss: 0.00057477
Iteration 10/25 | Loss: 0.00057477
Iteration 11/25 | Loss: 0.00057477
Iteration 12/25 | Loss: 0.00057477
Iteration 13/25 | Loss: 0.00057477
Iteration 14/25 | Loss: 0.00057477
Iteration 15/25 | Loss: 0.00057477
Iteration 16/25 | Loss: 0.00057477
Iteration 17/25 | Loss: 0.00057477
Iteration 18/25 | Loss: 0.00057477
Iteration 19/25 | Loss: 0.00057477
Iteration 20/25 | Loss: 0.00057477
Iteration 21/25 | Loss: 0.00057477
Iteration 22/25 | Loss: 0.00057477
Iteration 23/25 | Loss: 0.00057477
Iteration 24/25 | Loss: 0.00057477
Iteration 25/25 | Loss: 0.00057477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057477
Iteration 2/1000 | Loss: 0.00004312
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00003034
Iteration 5/1000 | Loss: 0.00002973
Iteration 6/1000 | Loss: 0.00002920
Iteration 7/1000 | Loss: 0.00002887
Iteration 8/1000 | Loss: 0.00002870
Iteration 9/1000 | Loss: 0.00002850
Iteration 10/1000 | Loss: 0.00002842
Iteration 11/1000 | Loss: 0.00002841
Iteration 12/1000 | Loss: 0.00002840
Iteration 13/1000 | Loss: 0.00002836
Iteration 14/1000 | Loss: 0.00002826
Iteration 15/1000 | Loss: 0.00002821
Iteration 16/1000 | Loss: 0.00002821
Iteration 17/1000 | Loss: 0.00002821
Iteration 18/1000 | Loss: 0.00002820
Iteration 19/1000 | Loss: 0.00002818
Iteration 20/1000 | Loss: 0.00002818
Iteration 21/1000 | Loss: 0.00002818
Iteration 22/1000 | Loss: 0.00002818
Iteration 23/1000 | Loss: 0.00002818
Iteration 24/1000 | Loss: 0.00002815
Iteration 25/1000 | Loss: 0.00002815
Iteration 26/1000 | Loss: 0.00002815
Iteration 27/1000 | Loss: 0.00002815
Iteration 28/1000 | Loss: 0.00002815
Iteration 29/1000 | Loss: 0.00002815
Iteration 30/1000 | Loss: 0.00002815
Iteration 31/1000 | Loss: 0.00002814
Iteration 32/1000 | Loss: 0.00002814
Iteration 33/1000 | Loss: 0.00002813
Iteration 34/1000 | Loss: 0.00002811
Iteration 35/1000 | Loss: 0.00002811
Iteration 36/1000 | Loss: 0.00002811
Iteration 37/1000 | Loss: 0.00002811
Iteration 38/1000 | Loss: 0.00002811
Iteration 39/1000 | Loss: 0.00002811
Iteration 40/1000 | Loss: 0.00002811
Iteration 41/1000 | Loss: 0.00002811
Iteration 42/1000 | Loss: 0.00002811
Iteration 43/1000 | Loss: 0.00002809
Iteration 44/1000 | Loss: 0.00002809
Iteration 45/1000 | Loss: 0.00002808
Iteration 46/1000 | Loss: 0.00002808
Iteration 47/1000 | Loss: 0.00002807
Iteration 48/1000 | Loss: 0.00002807
Iteration 49/1000 | Loss: 0.00002806
Iteration 50/1000 | Loss: 0.00002806
Iteration 51/1000 | Loss: 0.00002806
Iteration 52/1000 | Loss: 0.00002806
Iteration 53/1000 | Loss: 0.00002806
Iteration 54/1000 | Loss: 0.00002806
Iteration 55/1000 | Loss: 0.00002806
Iteration 56/1000 | Loss: 0.00002806
Iteration 57/1000 | Loss: 0.00002805
Iteration 58/1000 | Loss: 0.00002804
Iteration 59/1000 | Loss: 0.00002804
Iteration 60/1000 | Loss: 0.00002804
Iteration 61/1000 | Loss: 0.00002803
Iteration 62/1000 | Loss: 0.00002803
Iteration 63/1000 | Loss: 0.00002803
Iteration 64/1000 | Loss: 0.00002803
Iteration 65/1000 | Loss: 0.00002803
Iteration 66/1000 | Loss: 0.00002803
Iteration 67/1000 | Loss: 0.00002802
Iteration 68/1000 | Loss: 0.00002802
Iteration 69/1000 | Loss: 0.00002802
Iteration 70/1000 | Loss: 0.00002802
Iteration 71/1000 | Loss: 0.00002800
Iteration 72/1000 | Loss: 0.00002800
Iteration 73/1000 | Loss: 0.00002799
Iteration 74/1000 | Loss: 0.00002799
Iteration 75/1000 | Loss: 0.00002799
Iteration 76/1000 | Loss: 0.00002799
Iteration 77/1000 | Loss: 0.00002798
Iteration 78/1000 | Loss: 0.00002798
Iteration 79/1000 | Loss: 0.00002798
Iteration 80/1000 | Loss: 0.00002797
Iteration 81/1000 | Loss: 0.00002797
Iteration 82/1000 | Loss: 0.00002797
Iteration 83/1000 | Loss: 0.00002797
Iteration 84/1000 | Loss: 0.00002797
Iteration 85/1000 | Loss: 0.00002797
Iteration 86/1000 | Loss: 0.00002797
Iteration 87/1000 | Loss: 0.00002797
Iteration 88/1000 | Loss: 0.00002797
Iteration 89/1000 | Loss: 0.00002797
Iteration 90/1000 | Loss: 0.00002797
Iteration 91/1000 | Loss: 0.00002797
Iteration 92/1000 | Loss: 0.00002796
Iteration 93/1000 | Loss: 0.00002796
Iteration 94/1000 | Loss: 0.00002796
Iteration 95/1000 | Loss: 0.00002796
Iteration 96/1000 | Loss: 0.00002796
Iteration 97/1000 | Loss: 0.00002796
Iteration 98/1000 | Loss: 0.00002796
Iteration 99/1000 | Loss: 0.00002796
Iteration 100/1000 | Loss: 0.00002795
Iteration 101/1000 | Loss: 0.00002795
Iteration 102/1000 | Loss: 0.00002795
Iteration 103/1000 | Loss: 0.00002795
Iteration 104/1000 | Loss: 0.00002795
Iteration 105/1000 | Loss: 0.00002795
Iteration 106/1000 | Loss: 0.00002795
Iteration 107/1000 | Loss: 0.00002795
Iteration 108/1000 | Loss: 0.00002794
Iteration 109/1000 | Loss: 0.00002794
Iteration 110/1000 | Loss: 0.00002794
Iteration 111/1000 | Loss: 0.00002794
Iteration 112/1000 | Loss: 0.00002794
Iteration 113/1000 | Loss: 0.00002794
Iteration 114/1000 | Loss: 0.00002794
Iteration 115/1000 | Loss: 0.00002794
Iteration 116/1000 | Loss: 0.00002794
Iteration 117/1000 | Loss: 0.00002794
Iteration 118/1000 | Loss: 0.00002794
Iteration 119/1000 | Loss: 0.00002794
Iteration 120/1000 | Loss: 0.00002794
Iteration 121/1000 | Loss: 0.00002794
Iteration 122/1000 | Loss: 0.00002794
Iteration 123/1000 | Loss: 0.00002794
Iteration 124/1000 | Loss: 0.00002793
Iteration 125/1000 | Loss: 0.00002792
Iteration 126/1000 | Loss: 0.00002792
Iteration 127/1000 | Loss: 0.00002792
Iteration 128/1000 | Loss: 0.00002792
Iteration 129/1000 | Loss: 0.00002792
Iteration 130/1000 | Loss: 0.00002792
Iteration 131/1000 | Loss: 0.00002792
Iteration 132/1000 | Loss: 0.00002792
Iteration 133/1000 | Loss: 0.00002792
Iteration 134/1000 | Loss: 0.00002792
Iteration 135/1000 | Loss: 0.00002792
Iteration 136/1000 | Loss: 0.00002792
Iteration 137/1000 | Loss: 0.00002792
Iteration 138/1000 | Loss: 0.00002792
Iteration 139/1000 | Loss: 0.00002792
Iteration 140/1000 | Loss: 0.00002792
Iteration 141/1000 | Loss: 0.00002792
Iteration 142/1000 | Loss: 0.00002792
Iteration 143/1000 | Loss: 0.00002792
Iteration 144/1000 | Loss: 0.00002792
Iteration 145/1000 | Loss: 0.00002792
Iteration 146/1000 | Loss: 0.00002792
Iteration 147/1000 | Loss: 0.00002792
Iteration 148/1000 | Loss: 0.00002792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.792332998069469e-05, 2.792332998069469e-05, 2.792332998069469e-05, 2.792332998069469e-05, 2.792332998069469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.792332998069469e-05

Optimization complete. Final v2v error: 4.386595249176025 mm

Highest mean error: 4.488113880157471 mm for frame 1

Lowest mean error: 4.211711406707764 mm for frame 129

Saving results

Total time: 36.734893560409546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00364709
Iteration 2/25 | Loss: 0.00103974
Iteration 3/25 | Loss: 0.00096708
Iteration 4/25 | Loss: 0.00095909
Iteration 5/25 | Loss: 0.00095649
Iteration 6/25 | Loss: 0.00095573
Iteration 7/25 | Loss: 0.00095570
Iteration 8/25 | Loss: 0.00095570
Iteration 9/25 | Loss: 0.00095570
Iteration 10/25 | Loss: 0.00095570
Iteration 11/25 | Loss: 0.00095570
Iteration 12/25 | Loss: 0.00095570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009557019802741706, 0.0009557019802741706, 0.0009557019802741706, 0.0009557019802741706, 0.0009557019802741706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009557019802741706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77014947
Iteration 2/25 | Loss: 0.00070303
Iteration 3/25 | Loss: 0.00070303
Iteration 4/25 | Loss: 0.00070303
Iteration 5/25 | Loss: 0.00070303
Iteration 6/25 | Loss: 0.00070302
Iteration 7/25 | Loss: 0.00070302
Iteration 8/25 | Loss: 0.00070302
Iteration 9/25 | Loss: 0.00070302
Iteration 10/25 | Loss: 0.00070302
Iteration 11/25 | Loss: 0.00070302
Iteration 12/25 | Loss: 0.00070302
Iteration 13/25 | Loss: 0.00070302
Iteration 14/25 | Loss: 0.00070302
Iteration 15/25 | Loss: 0.00070302
Iteration 16/25 | Loss: 0.00070302
Iteration 17/25 | Loss: 0.00070302
Iteration 18/25 | Loss: 0.00070302
Iteration 19/25 | Loss: 0.00070302
Iteration 20/25 | Loss: 0.00070302
Iteration 21/25 | Loss: 0.00070302
Iteration 22/25 | Loss: 0.00070302
Iteration 23/25 | Loss: 0.00070302
Iteration 24/25 | Loss: 0.00070302
Iteration 25/25 | Loss: 0.00070302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070302
Iteration 2/1000 | Loss: 0.00001758
Iteration 3/1000 | Loss: 0.00001058
Iteration 4/1000 | Loss: 0.00000936
Iteration 5/1000 | Loss: 0.00000837
Iteration 6/1000 | Loss: 0.00000794
Iteration 7/1000 | Loss: 0.00000763
Iteration 8/1000 | Loss: 0.00000751
Iteration 9/1000 | Loss: 0.00000750
Iteration 10/1000 | Loss: 0.00000749
Iteration 11/1000 | Loss: 0.00000747
Iteration 12/1000 | Loss: 0.00000743
Iteration 13/1000 | Loss: 0.00000743
Iteration 14/1000 | Loss: 0.00000743
Iteration 15/1000 | Loss: 0.00000742
Iteration 16/1000 | Loss: 0.00000742
Iteration 17/1000 | Loss: 0.00000739
Iteration 18/1000 | Loss: 0.00000738
Iteration 19/1000 | Loss: 0.00000736
Iteration 20/1000 | Loss: 0.00000736
Iteration 21/1000 | Loss: 0.00000735
Iteration 22/1000 | Loss: 0.00000735
Iteration 23/1000 | Loss: 0.00000735
Iteration 24/1000 | Loss: 0.00000730
Iteration 25/1000 | Loss: 0.00000729
Iteration 26/1000 | Loss: 0.00000729
Iteration 27/1000 | Loss: 0.00000728
Iteration 28/1000 | Loss: 0.00000728
Iteration 29/1000 | Loss: 0.00000727
Iteration 30/1000 | Loss: 0.00000727
Iteration 31/1000 | Loss: 0.00000727
Iteration 32/1000 | Loss: 0.00000726
Iteration 33/1000 | Loss: 0.00000725
Iteration 34/1000 | Loss: 0.00000723
Iteration 35/1000 | Loss: 0.00000722
Iteration 36/1000 | Loss: 0.00000722
Iteration 37/1000 | Loss: 0.00000722
Iteration 38/1000 | Loss: 0.00000722
Iteration 39/1000 | Loss: 0.00000721
Iteration 40/1000 | Loss: 0.00000721
Iteration 41/1000 | Loss: 0.00000721
Iteration 42/1000 | Loss: 0.00000721
Iteration 43/1000 | Loss: 0.00000720
Iteration 44/1000 | Loss: 0.00000720
Iteration 45/1000 | Loss: 0.00000719
Iteration 46/1000 | Loss: 0.00000719
Iteration 47/1000 | Loss: 0.00000719
Iteration 48/1000 | Loss: 0.00000718
Iteration 49/1000 | Loss: 0.00000718
Iteration 50/1000 | Loss: 0.00000718
Iteration 51/1000 | Loss: 0.00000718
Iteration 52/1000 | Loss: 0.00000717
Iteration 53/1000 | Loss: 0.00000717
Iteration 54/1000 | Loss: 0.00000717
Iteration 55/1000 | Loss: 0.00000717
Iteration 56/1000 | Loss: 0.00000717
Iteration 57/1000 | Loss: 0.00000716
Iteration 58/1000 | Loss: 0.00000716
Iteration 59/1000 | Loss: 0.00000716
Iteration 60/1000 | Loss: 0.00000716
Iteration 61/1000 | Loss: 0.00000715
Iteration 62/1000 | Loss: 0.00000715
Iteration 63/1000 | Loss: 0.00000714
Iteration 64/1000 | Loss: 0.00000714
Iteration 65/1000 | Loss: 0.00000713
Iteration 66/1000 | Loss: 0.00000712
Iteration 67/1000 | Loss: 0.00000712
Iteration 68/1000 | Loss: 0.00000712
Iteration 69/1000 | Loss: 0.00000711
Iteration 70/1000 | Loss: 0.00000711
Iteration 71/1000 | Loss: 0.00000711
Iteration 72/1000 | Loss: 0.00000711
Iteration 73/1000 | Loss: 0.00000710
Iteration 74/1000 | Loss: 0.00000710
Iteration 75/1000 | Loss: 0.00000710
Iteration 76/1000 | Loss: 0.00000709
Iteration 77/1000 | Loss: 0.00000709
Iteration 78/1000 | Loss: 0.00000709
Iteration 79/1000 | Loss: 0.00000709
Iteration 80/1000 | Loss: 0.00000709
Iteration 81/1000 | Loss: 0.00000708
Iteration 82/1000 | Loss: 0.00000708
Iteration 83/1000 | Loss: 0.00000707
Iteration 84/1000 | Loss: 0.00000707
Iteration 85/1000 | Loss: 0.00000707
Iteration 86/1000 | Loss: 0.00000706
Iteration 87/1000 | Loss: 0.00000706
Iteration 88/1000 | Loss: 0.00000705
Iteration 89/1000 | Loss: 0.00000705
Iteration 90/1000 | Loss: 0.00000705
Iteration 91/1000 | Loss: 0.00000705
Iteration 92/1000 | Loss: 0.00000705
Iteration 93/1000 | Loss: 0.00000705
Iteration 94/1000 | Loss: 0.00000705
Iteration 95/1000 | Loss: 0.00000705
Iteration 96/1000 | Loss: 0.00000705
Iteration 97/1000 | Loss: 0.00000705
Iteration 98/1000 | Loss: 0.00000705
Iteration 99/1000 | Loss: 0.00000705
Iteration 100/1000 | Loss: 0.00000704
Iteration 101/1000 | Loss: 0.00000704
Iteration 102/1000 | Loss: 0.00000704
Iteration 103/1000 | Loss: 0.00000704
Iteration 104/1000 | Loss: 0.00000704
Iteration 105/1000 | Loss: 0.00000704
Iteration 106/1000 | Loss: 0.00000704
Iteration 107/1000 | Loss: 0.00000703
Iteration 108/1000 | Loss: 0.00000703
Iteration 109/1000 | Loss: 0.00000703
Iteration 110/1000 | Loss: 0.00000703
Iteration 111/1000 | Loss: 0.00000703
Iteration 112/1000 | Loss: 0.00000703
Iteration 113/1000 | Loss: 0.00000703
Iteration 114/1000 | Loss: 0.00000703
Iteration 115/1000 | Loss: 0.00000703
Iteration 116/1000 | Loss: 0.00000703
Iteration 117/1000 | Loss: 0.00000702
Iteration 118/1000 | Loss: 0.00000702
Iteration 119/1000 | Loss: 0.00000702
Iteration 120/1000 | Loss: 0.00000702
Iteration 121/1000 | Loss: 0.00000702
Iteration 122/1000 | Loss: 0.00000702
Iteration 123/1000 | Loss: 0.00000702
Iteration 124/1000 | Loss: 0.00000702
Iteration 125/1000 | Loss: 0.00000702
Iteration 126/1000 | Loss: 0.00000702
Iteration 127/1000 | Loss: 0.00000701
Iteration 128/1000 | Loss: 0.00000701
Iteration 129/1000 | Loss: 0.00000701
Iteration 130/1000 | Loss: 0.00000701
Iteration 131/1000 | Loss: 0.00000701
Iteration 132/1000 | Loss: 0.00000701
Iteration 133/1000 | Loss: 0.00000700
Iteration 134/1000 | Loss: 0.00000700
Iteration 135/1000 | Loss: 0.00000700
Iteration 136/1000 | Loss: 0.00000699
Iteration 137/1000 | Loss: 0.00000699
Iteration 138/1000 | Loss: 0.00000699
Iteration 139/1000 | Loss: 0.00000699
Iteration 140/1000 | Loss: 0.00000699
Iteration 141/1000 | Loss: 0.00000699
Iteration 142/1000 | Loss: 0.00000699
Iteration 143/1000 | Loss: 0.00000699
Iteration 144/1000 | Loss: 0.00000699
Iteration 145/1000 | Loss: 0.00000699
Iteration 146/1000 | Loss: 0.00000699
Iteration 147/1000 | Loss: 0.00000699
Iteration 148/1000 | Loss: 0.00000699
Iteration 149/1000 | Loss: 0.00000699
Iteration 150/1000 | Loss: 0.00000698
Iteration 151/1000 | Loss: 0.00000698
Iteration 152/1000 | Loss: 0.00000698
Iteration 153/1000 | Loss: 0.00000698
Iteration 154/1000 | Loss: 0.00000698
Iteration 155/1000 | Loss: 0.00000698
Iteration 156/1000 | Loss: 0.00000698
Iteration 157/1000 | Loss: 0.00000698
Iteration 158/1000 | Loss: 0.00000698
Iteration 159/1000 | Loss: 0.00000698
Iteration 160/1000 | Loss: 0.00000698
Iteration 161/1000 | Loss: 0.00000698
Iteration 162/1000 | Loss: 0.00000697
Iteration 163/1000 | Loss: 0.00000697
Iteration 164/1000 | Loss: 0.00000697
Iteration 165/1000 | Loss: 0.00000697
Iteration 166/1000 | Loss: 0.00000697
Iteration 167/1000 | Loss: 0.00000697
Iteration 168/1000 | Loss: 0.00000697
Iteration 169/1000 | Loss: 0.00000697
Iteration 170/1000 | Loss: 0.00000697
Iteration 171/1000 | Loss: 0.00000697
Iteration 172/1000 | Loss: 0.00000696
Iteration 173/1000 | Loss: 0.00000696
Iteration 174/1000 | Loss: 0.00000696
Iteration 175/1000 | Loss: 0.00000696
Iteration 176/1000 | Loss: 0.00000696
Iteration 177/1000 | Loss: 0.00000696
Iteration 178/1000 | Loss: 0.00000696
Iteration 179/1000 | Loss: 0.00000696
Iteration 180/1000 | Loss: 0.00000696
Iteration 181/1000 | Loss: 0.00000696
Iteration 182/1000 | Loss: 0.00000696
Iteration 183/1000 | Loss: 0.00000696
Iteration 184/1000 | Loss: 0.00000696
Iteration 185/1000 | Loss: 0.00000695
Iteration 186/1000 | Loss: 0.00000695
Iteration 187/1000 | Loss: 0.00000695
Iteration 188/1000 | Loss: 0.00000695
Iteration 189/1000 | Loss: 0.00000695
Iteration 190/1000 | Loss: 0.00000695
Iteration 191/1000 | Loss: 0.00000695
Iteration 192/1000 | Loss: 0.00000695
Iteration 193/1000 | Loss: 0.00000695
Iteration 194/1000 | Loss: 0.00000694
Iteration 195/1000 | Loss: 0.00000694
Iteration 196/1000 | Loss: 0.00000694
Iteration 197/1000 | Loss: 0.00000694
Iteration 198/1000 | Loss: 0.00000694
Iteration 199/1000 | Loss: 0.00000694
Iteration 200/1000 | Loss: 0.00000694
Iteration 201/1000 | Loss: 0.00000694
Iteration 202/1000 | Loss: 0.00000694
Iteration 203/1000 | Loss: 0.00000694
Iteration 204/1000 | Loss: 0.00000694
Iteration 205/1000 | Loss: 0.00000694
Iteration 206/1000 | Loss: 0.00000694
Iteration 207/1000 | Loss: 0.00000694
Iteration 208/1000 | Loss: 0.00000694
Iteration 209/1000 | Loss: 0.00000694
Iteration 210/1000 | Loss: 0.00000694
Iteration 211/1000 | Loss: 0.00000694
Iteration 212/1000 | Loss: 0.00000694
Iteration 213/1000 | Loss: 0.00000694
Iteration 214/1000 | Loss: 0.00000694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [6.937870693946024e-06, 6.937870693946024e-06, 6.937870693946024e-06, 6.937870693946024e-06, 6.937870693946024e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.937870693946024e-06

Optimization complete. Final v2v error: 2.252514362335205 mm

Highest mean error: 2.816540002822876 mm for frame 78

Lowest mean error: 2.147714138031006 mm for frame 112

Saving results

Total time: 36.79515862464905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453725
Iteration 2/25 | Loss: 0.00107010
Iteration 3/25 | Loss: 0.00100637
Iteration 4/25 | Loss: 0.00099647
Iteration 5/25 | Loss: 0.00099350
Iteration 6/25 | Loss: 0.00099291
Iteration 7/25 | Loss: 0.00099291
Iteration 8/25 | Loss: 0.00099291
Iteration 9/25 | Loss: 0.00099291
Iteration 10/25 | Loss: 0.00099291
Iteration 11/25 | Loss: 0.00099291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000992911751382053, 0.000992911751382053, 0.000992911751382053, 0.000992911751382053, 0.000992911751382053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000992911751382053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34463024
Iteration 2/25 | Loss: 0.00061516
Iteration 3/25 | Loss: 0.00061516
Iteration 4/25 | Loss: 0.00061516
Iteration 5/25 | Loss: 0.00061516
Iteration 6/25 | Loss: 0.00061516
Iteration 7/25 | Loss: 0.00061516
Iteration 8/25 | Loss: 0.00061516
Iteration 9/25 | Loss: 0.00061516
Iteration 10/25 | Loss: 0.00061516
Iteration 11/25 | Loss: 0.00061516
Iteration 12/25 | Loss: 0.00061516
Iteration 13/25 | Loss: 0.00061516
Iteration 14/25 | Loss: 0.00061516
Iteration 15/25 | Loss: 0.00061516
Iteration 16/25 | Loss: 0.00061516
Iteration 17/25 | Loss: 0.00061516
Iteration 18/25 | Loss: 0.00061516
Iteration 19/25 | Loss: 0.00061516
Iteration 20/25 | Loss: 0.00061516
Iteration 21/25 | Loss: 0.00061516
Iteration 22/25 | Loss: 0.00061516
Iteration 23/25 | Loss: 0.00061516
Iteration 24/25 | Loss: 0.00061516
Iteration 25/25 | Loss: 0.00061516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061516
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001255
Iteration 5/1000 | Loss: 0.00001181
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001115
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001099
Iteration 10/1000 | Loss: 0.00001099
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001097
Iteration 13/1000 | Loss: 0.00001080
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001070
Iteration 16/1000 | Loss: 0.00001068
Iteration 17/1000 | Loss: 0.00001068
Iteration 18/1000 | Loss: 0.00001067
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001063
Iteration 21/1000 | Loss: 0.00001062
Iteration 22/1000 | Loss: 0.00001061
Iteration 23/1000 | Loss: 0.00001059
Iteration 24/1000 | Loss: 0.00001058
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001057
Iteration 27/1000 | Loss: 0.00001056
Iteration 28/1000 | Loss: 0.00001056
Iteration 29/1000 | Loss: 0.00001055
Iteration 30/1000 | Loss: 0.00001055
Iteration 31/1000 | Loss: 0.00001054
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001050
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001046
Iteration 37/1000 | Loss: 0.00001046
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001046
Iteration 41/1000 | Loss: 0.00001046
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001045
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001045
Iteration 46/1000 | Loss: 0.00001044
Iteration 47/1000 | Loss: 0.00001044
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001042
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001040
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001040
Iteration 59/1000 | Loss: 0.00001040
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001039
Iteration 63/1000 | Loss: 0.00001039
Iteration 64/1000 | Loss: 0.00001038
Iteration 65/1000 | Loss: 0.00001038
Iteration 66/1000 | Loss: 0.00001038
Iteration 67/1000 | Loss: 0.00001037
Iteration 68/1000 | Loss: 0.00001037
Iteration 69/1000 | Loss: 0.00001037
Iteration 70/1000 | Loss: 0.00001036
Iteration 71/1000 | Loss: 0.00001036
Iteration 72/1000 | Loss: 0.00001036
Iteration 73/1000 | Loss: 0.00001035
Iteration 74/1000 | Loss: 0.00001035
Iteration 75/1000 | Loss: 0.00001035
Iteration 76/1000 | Loss: 0.00001035
Iteration 77/1000 | Loss: 0.00001035
Iteration 78/1000 | Loss: 0.00001035
Iteration 79/1000 | Loss: 0.00001035
Iteration 80/1000 | Loss: 0.00001035
Iteration 81/1000 | Loss: 0.00001034
Iteration 82/1000 | Loss: 0.00001034
Iteration 83/1000 | Loss: 0.00001034
Iteration 84/1000 | Loss: 0.00001034
Iteration 85/1000 | Loss: 0.00001033
Iteration 86/1000 | Loss: 0.00001033
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001033
Iteration 90/1000 | Loss: 0.00001033
Iteration 91/1000 | Loss: 0.00001033
Iteration 92/1000 | Loss: 0.00001033
Iteration 93/1000 | Loss: 0.00001033
Iteration 94/1000 | Loss: 0.00001032
Iteration 95/1000 | Loss: 0.00001032
Iteration 96/1000 | Loss: 0.00001031
Iteration 97/1000 | Loss: 0.00001031
Iteration 98/1000 | Loss: 0.00001030
Iteration 99/1000 | Loss: 0.00001030
Iteration 100/1000 | Loss: 0.00001030
Iteration 101/1000 | Loss: 0.00001029
Iteration 102/1000 | Loss: 0.00001029
Iteration 103/1000 | Loss: 0.00001029
Iteration 104/1000 | Loss: 0.00001028
Iteration 105/1000 | Loss: 0.00001028
Iteration 106/1000 | Loss: 0.00001028
Iteration 107/1000 | Loss: 0.00001027
Iteration 108/1000 | Loss: 0.00001027
Iteration 109/1000 | Loss: 0.00001026
Iteration 110/1000 | Loss: 0.00001026
Iteration 111/1000 | Loss: 0.00001026
Iteration 112/1000 | Loss: 0.00001025
Iteration 113/1000 | Loss: 0.00001025
Iteration 114/1000 | Loss: 0.00001024
Iteration 115/1000 | Loss: 0.00001024
Iteration 116/1000 | Loss: 0.00001024
Iteration 117/1000 | Loss: 0.00001023
Iteration 118/1000 | Loss: 0.00001023
Iteration 119/1000 | Loss: 0.00001023
Iteration 120/1000 | Loss: 0.00001023
Iteration 121/1000 | Loss: 0.00001023
Iteration 122/1000 | Loss: 0.00001022
Iteration 123/1000 | Loss: 0.00001022
Iteration 124/1000 | Loss: 0.00001021
Iteration 125/1000 | Loss: 0.00001021
Iteration 126/1000 | Loss: 0.00001021
Iteration 127/1000 | Loss: 0.00001020
Iteration 128/1000 | Loss: 0.00001020
Iteration 129/1000 | Loss: 0.00001020
Iteration 130/1000 | Loss: 0.00001020
Iteration 131/1000 | Loss: 0.00001020
Iteration 132/1000 | Loss: 0.00001020
Iteration 133/1000 | Loss: 0.00001020
Iteration 134/1000 | Loss: 0.00001020
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001019
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001015
Iteration 151/1000 | Loss: 0.00001015
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001011
Iteration 163/1000 | Loss: 0.00001011
Iteration 164/1000 | Loss: 0.00001011
Iteration 165/1000 | Loss: 0.00001011
Iteration 166/1000 | Loss: 0.00001011
Iteration 167/1000 | Loss: 0.00001011
Iteration 168/1000 | Loss: 0.00001010
Iteration 169/1000 | Loss: 0.00001010
Iteration 170/1000 | Loss: 0.00001010
Iteration 171/1000 | Loss: 0.00001010
Iteration 172/1000 | Loss: 0.00001010
Iteration 173/1000 | Loss: 0.00001010
Iteration 174/1000 | Loss: 0.00001010
Iteration 175/1000 | Loss: 0.00001010
Iteration 176/1000 | Loss: 0.00001009
Iteration 177/1000 | Loss: 0.00001009
Iteration 178/1000 | Loss: 0.00001009
Iteration 179/1000 | Loss: 0.00001009
Iteration 180/1000 | Loss: 0.00001009
Iteration 181/1000 | Loss: 0.00001009
Iteration 182/1000 | Loss: 0.00001009
Iteration 183/1000 | Loss: 0.00001009
Iteration 184/1000 | Loss: 0.00001009
Iteration 185/1000 | Loss: 0.00001009
Iteration 186/1000 | Loss: 0.00001009
Iteration 187/1000 | Loss: 0.00001009
Iteration 188/1000 | Loss: 0.00001009
Iteration 189/1000 | Loss: 0.00001008
Iteration 190/1000 | Loss: 0.00001008
Iteration 191/1000 | Loss: 0.00001008
Iteration 192/1000 | Loss: 0.00001008
Iteration 193/1000 | Loss: 0.00001008
Iteration 194/1000 | Loss: 0.00001008
Iteration 195/1000 | Loss: 0.00001008
Iteration 196/1000 | Loss: 0.00001008
Iteration 197/1000 | Loss: 0.00001008
Iteration 198/1000 | Loss: 0.00001008
Iteration 199/1000 | Loss: 0.00001008
Iteration 200/1000 | Loss: 0.00001008
Iteration 201/1000 | Loss: 0.00001008
Iteration 202/1000 | Loss: 0.00001008
Iteration 203/1000 | Loss: 0.00001008
Iteration 204/1000 | Loss: 0.00001008
Iteration 205/1000 | Loss: 0.00001008
Iteration 206/1000 | Loss: 0.00001008
Iteration 207/1000 | Loss: 0.00001007
Iteration 208/1000 | Loss: 0.00001007
Iteration 209/1000 | Loss: 0.00001007
Iteration 210/1000 | Loss: 0.00001006
Iteration 211/1000 | Loss: 0.00001006
Iteration 212/1000 | Loss: 0.00001006
Iteration 213/1000 | Loss: 0.00001006
Iteration 214/1000 | Loss: 0.00001006
Iteration 215/1000 | Loss: 0.00001005
Iteration 216/1000 | Loss: 0.00001005
Iteration 217/1000 | Loss: 0.00001005
Iteration 218/1000 | Loss: 0.00001005
Iteration 219/1000 | Loss: 0.00001005
Iteration 220/1000 | Loss: 0.00001005
Iteration 221/1000 | Loss: 0.00001005
Iteration 222/1000 | Loss: 0.00001005
Iteration 223/1000 | Loss: 0.00001005
Iteration 224/1000 | Loss: 0.00001005
Iteration 225/1000 | Loss: 0.00001005
Iteration 226/1000 | Loss: 0.00001005
Iteration 227/1000 | Loss: 0.00001005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.0049360753328074e-05, 1.0049360753328074e-05, 1.0049360753328074e-05, 1.0049360753328074e-05, 1.0049360753328074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0049360753328074e-05

Optimization complete. Final v2v error: 2.7047388553619385 mm

Highest mean error: 2.825026750564575 mm for frame 73

Lowest mean error: 2.5742387771606445 mm for frame 46

Saving results

Total time: 39.62159466743469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997206
Iteration 2/25 | Loss: 0.00997206
Iteration 3/25 | Loss: 0.00997206
Iteration 4/25 | Loss: 0.00997206
Iteration 5/25 | Loss: 0.00997205
Iteration 6/25 | Loss: 0.00997205
Iteration 7/25 | Loss: 0.00997205
Iteration 8/25 | Loss: 0.00997205
Iteration 9/25 | Loss: 0.00997205
Iteration 10/25 | Loss: 0.00997204
Iteration 11/25 | Loss: 0.00997204
Iteration 12/25 | Loss: 0.00415487
Iteration 13/25 | Loss: 0.00332562
Iteration 14/25 | Loss: 0.00260508
Iteration 15/25 | Loss: 0.00224275
Iteration 16/25 | Loss: 0.00208844
Iteration 17/25 | Loss: 0.00203700
Iteration 18/25 | Loss: 0.00204630
Iteration 19/25 | Loss: 0.00197191
Iteration 20/25 | Loss: 0.00174375
Iteration 21/25 | Loss: 0.00163927
Iteration 22/25 | Loss: 0.00160718
Iteration 23/25 | Loss: 0.00160101
Iteration 24/25 | Loss: 0.00156580
Iteration 25/25 | Loss: 0.00156307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34425771
Iteration 2/25 | Loss: 0.00446117
Iteration 3/25 | Loss: 0.00396875
Iteration 4/25 | Loss: 0.00396871
Iteration 5/25 | Loss: 0.00396871
Iteration 6/25 | Loss: 0.00396871
Iteration 7/25 | Loss: 0.00396871
Iteration 8/25 | Loss: 0.00396871
Iteration 9/25 | Loss: 0.00396871
Iteration 10/25 | Loss: 0.00396871
Iteration 11/25 | Loss: 0.00396871
Iteration 12/25 | Loss: 0.00396871
Iteration 13/25 | Loss: 0.00396871
Iteration 14/25 | Loss: 0.00396871
Iteration 15/25 | Loss: 0.00396871
Iteration 16/25 | Loss: 0.00396871
Iteration 17/25 | Loss: 0.00396871
Iteration 18/25 | Loss: 0.00396871
Iteration 19/25 | Loss: 0.00396871
Iteration 20/25 | Loss: 0.00396871
Iteration 21/25 | Loss: 0.00396871
Iteration 22/25 | Loss: 0.00396871
Iteration 23/25 | Loss: 0.00396871
Iteration 24/25 | Loss: 0.00396871
Iteration 25/25 | Loss: 0.00396871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00396871
Iteration 2/1000 | Loss: 0.00407648
Iteration 3/1000 | Loss: 0.01105078
Iteration 4/1000 | Loss: 0.00097823
Iteration 5/1000 | Loss: 0.00054649
Iteration 6/1000 | Loss: 0.00091819
Iteration 7/1000 | Loss: 0.00078805
Iteration 8/1000 | Loss: 0.00073532
Iteration 9/1000 | Loss: 0.00183032
Iteration 10/1000 | Loss: 0.00061714
Iteration 11/1000 | Loss: 0.00149973
Iteration 12/1000 | Loss: 0.00107118
Iteration 13/1000 | Loss: 0.00046291
Iteration 14/1000 | Loss: 0.00050465
Iteration 15/1000 | Loss: 0.00030387
Iteration 16/1000 | Loss: 0.00014708
Iteration 17/1000 | Loss: 0.00035898
Iteration 18/1000 | Loss: 0.00111336
Iteration 19/1000 | Loss: 0.00036633
Iteration 20/1000 | Loss: 0.00040221
Iteration 21/1000 | Loss: 0.00021890
Iteration 22/1000 | Loss: 0.00013226
Iteration 23/1000 | Loss: 0.00016742
Iteration 24/1000 | Loss: 0.00024309
Iteration 25/1000 | Loss: 0.00049791
Iteration 26/1000 | Loss: 0.00015265
Iteration 27/1000 | Loss: 0.00025974
Iteration 28/1000 | Loss: 0.00070438
Iteration 29/1000 | Loss: 0.00043153
Iteration 30/1000 | Loss: 0.00064128
Iteration 31/1000 | Loss: 0.00067834
Iteration 32/1000 | Loss: 0.00096286
Iteration 33/1000 | Loss: 0.00047681
Iteration 34/1000 | Loss: 0.00032027
Iteration 35/1000 | Loss: 0.00011421
Iteration 36/1000 | Loss: 0.00010853
Iteration 37/1000 | Loss: 0.00037757
Iteration 38/1000 | Loss: 0.00028479
Iteration 39/1000 | Loss: 0.00011483
Iteration 40/1000 | Loss: 0.00017391
Iteration 41/1000 | Loss: 0.00010424
Iteration 42/1000 | Loss: 0.00017212
Iteration 43/1000 | Loss: 0.00011817
Iteration 44/1000 | Loss: 0.00010038
Iteration 45/1000 | Loss: 0.00022208
Iteration 46/1000 | Loss: 0.00009892
Iteration 47/1000 | Loss: 0.00028721
Iteration 48/1000 | Loss: 0.00027622
Iteration 49/1000 | Loss: 0.00010372
Iteration 50/1000 | Loss: 0.00075103
Iteration 51/1000 | Loss: 0.00142049
Iteration 52/1000 | Loss: 0.00103726
Iteration 53/1000 | Loss: 0.00124225
Iteration 54/1000 | Loss: 0.00035712
Iteration 55/1000 | Loss: 0.00058031
Iteration 56/1000 | Loss: 0.00026245
Iteration 57/1000 | Loss: 0.00025377
Iteration 58/1000 | Loss: 0.00019243
Iteration 59/1000 | Loss: 0.00019156
Iteration 60/1000 | Loss: 0.00064764
Iteration 61/1000 | Loss: 0.00010542
Iteration 62/1000 | Loss: 0.00009935
Iteration 63/1000 | Loss: 0.00015955
Iteration 64/1000 | Loss: 0.00032353
Iteration 65/1000 | Loss: 0.00027337
Iteration 66/1000 | Loss: 0.00013773
Iteration 67/1000 | Loss: 0.00021763
Iteration 68/1000 | Loss: 0.00008205
Iteration 69/1000 | Loss: 0.00026772
Iteration 70/1000 | Loss: 0.00051484
Iteration 71/1000 | Loss: 0.00009866
Iteration 72/1000 | Loss: 0.00007780
Iteration 73/1000 | Loss: 0.00008599
Iteration 74/1000 | Loss: 0.00027358
Iteration 75/1000 | Loss: 0.00027841
Iteration 76/1000 | Loss: 0.00007787
Iteration 77/1000 | Loss: 0.00007335
Iteration 78/1000 | Loss: 0.00026202
Iteration 79/1000 | Loss: 0.00026979
Iteration 80/1000 | Loss: 0.00007126
Iteration 81/1000 | Loss: 0.00006976
Iteration 82/1000 | Loss: 0.00018982
Iteration 83/1000 | Loss: 0.00019980
Iteration 84/1000 | Loss: 0.00035137
Iteration 85/1000 | Loss: 0.00032408
Iteration 86/1000 | Loss: 0.00038189
Iteration 87/1000 | Loss: 0.00007023
Iteration 88/1000 | Loss: 0.00012768
Iteration 89/1000 | Loss: 0.00006736
Iteration 90/1000 | Loss: 0.00022944
Iteration 91/1000 | Loss: 0.00006673
Iteration 92/1000 | Loss: 0.00006640
Iteration 93/1000 | Loss: 0.00023686
Iteration 94/1000 | Loss: 0.00111231
Iteration 95/1000 | Loss: 0.00032802
Iteration 96/1000 | Loss: 0.00047562
Iteration 97/1000 | Loss: 0.00008499
Iteration 98/1000 | Loss: 0.00013962
Iteration 99/1000 | Loss: 0.00007249
Iteration 100/1000 | Loss: 0.00006844
Iteration 101/1000 | Loss: 0.00006750
Iteration 102/1000 | Loss: 0.00018560
Iteration 103/1000 | Loss: 0.00024494
Iteration 104/1000 | Loss: 0.00007157
Iteration 105/1000 | Loss: 0.00008300
Iteration 106/1000 | Loss: 0.00012201
Iteration 107/1000 | Loss: 0.00012746
Iteration 108/1000 | Loss: 0.00042144
Iteration 109/1000 | Loss: 0.00015336
Iteration 110/1000 | Loss: 0.00011516
Iteration 111/1000 | Loss: 0.00012782
Iteration 112/1000 | Loss: 0.00024188
Iteration 113/1000 | Loss: 0.00007368
Iteration 114/1000 | Loss: 0.00006769
Iteration 115/1000 | Loss: 0.00006919
Iteration 116/1000 | Loss: 0.00006706
Iteration 117/1000 | Loss: 0.00006683
Iteration 118/1000 | Loss: 0.00021288
Iteration 119/1000 | Loss: 0.00025285
Iteration 120/1000 | Loss: 0.00021552
Iteration 121/1000 | Loss: 0.00015648
Iteration 122/1000 | Loss: 0.00006659
Iteration 123/1000 | Loss: 0.00021409
Iteration 124/1000 | Loss: 0.00006933
Iteration 125/1000 | Loss: 0.00006892
Iteration 126/1000 | Loss: 0.00016068
Iteration 127/1000 | Loss: 0.00006784
Iteration 128/1000 | Loss: 0.00029405
Iteration 129/1000 | Loss: 0.00023719
Iteration 130/1000 | Loss: 0.00015942
Iteration 131/1000 | Loss: 0.00022453
Iteration 132/1000 | Loss: 0.00021466
Iteration 133/1000 | Loss: 0.00059561
Iteration 134/1000 | Loss: 0.00006769
Iteration 135/1000 | Loss: 0.00013848
Iteration 136/1000 | Loss: 0.00006433
Iteration 137/1000 | Loss: 0.00029440
Iteration 138/1000 | Loss: 0.00011176
Iteration 139/1000 | Loss: 0.00007835
Iteration 140/1000 | Loss: 0.00006307
Iteration 141/1000 | Loss: 0.00006392
Iteration 142/1000 | Loss: 0.00006139
Iteration 143/1000 | Loss: 0.00006128
Iteration 144/1000 | Loss: 0.00006498
Iteration 145/1000 | Loss: 0.00006455
Iteration 146/1000 | Loss: 0.00012189
Iteration 147/1000 | Loss: 0.00006711
Iteration 148/1000 | Loss: 0.00006070
Iteration 149/1000 | Loss: 0.00029915
Iteration 150/1000 | Loss: 0.00021808
Iteration 151/1000 | Loss: 0.00023489
Iteration 152/1000 | Loss: 0.00015668
Iteration 153/1000 | Loss: 0.00015454
Iteration 154/1000 | Loss: 0.00008096
Iteration 155/1000 | Loss: 0.00006511
Iteration 156/1000 | Loss: 0.00006178
Iteration 157/1000 | Loss: 0.00006055
Iteration 158/1000 | Loss: 0.00021585
Iteration 159/1000 | Loss: 0.00008950
Iteration 160/1000 | Loss: 0.00012819
Iteration 161/1000 | Loss: 0.00007428
Iteration 162/1000 | Loss: 0.00005980
Iteration 163/1000 | Loss: 0.00005955
Iteration 164/1000 | Loss: 0.00005953
Iteration 165/1000 | Loss: 0.00005951
Iteration 166/1000 | Loss: 0.00005951
Iteration 167/1000 | Loss: 0.00005946
Iteration 168/1000 | Loss: 0.00005945
Iteration 169/1000 | Loss: 0.00005944
Iteration 170/1000 | Loss: 0.00005944
Iteration 171/1000 | Loss: 0.00005941
Iteration 172/1000 | Loss: 0.00005941
Iteration 173/1000 | Loss: 0.00005940
Iteration 174/1000 | Loss: 0.00005939
Iteration 175/1000 | Loss: 0.00005938
Iteration 176/1000 | Loss: 0.00005933
Iteration 177/1000 | Loss: 0.00005930
Iteration 178/1000 | Loss: 0.00005930
Iteration 179/1000 | Loss: 0.00005929
Iteration 180/1000 | Loss: 0.00013544
Iteration 181/1000 | Loss: 0.00006086
Iteration 182/1000 | Loss: 0.00007297
Iteration 183/1000 | Loss: 0.00005933
Iteration 184/1000 | Loss: 0.00005931
Iteration 185/1000 | Loss: 0.00005924
Iteration 186/1000 | Loss: 0.00005923
Iteration 187/1000 | Loss: 0.00005923
Iteration 188/1000 | Loss: 0.00005923
Iteration 189/1000 | Loss: 0.00005923
Iteration 190/1000 | Loss: 0.00005923
Iteration 191/1000 | Loss: 0.00005923
Iteration 192/1000 | Loss: 0.00005923
Iteration 193/1000 | Loss: 0.00005922
Iteration 194/1000 | Loss: 0.00005922
Iteration 195/1000 | Loss: 0.00005922
Iteration 196/1000 | Loss: 0.00005922
Iteration 197/1000 | Loss: 0.00005922
Iteration 198/1000 | Loss: 0.00005922
Iteration 199/1000 | Loss: 0.00005921
Iteration 200/1000 | Loss: 0.00005921
Iteration 201/1000 | Loss: 0.00005921
Iteration 202/1000 | Loss: 0.00005921
Iteration 203/1000 | Loss: 0.00005920
Iteration 204/1000 | Loss: 0.00005920
Iteration 205/1000 | Loss: 0.00005920
Iteration 206/1000 | Loss: 0.00005920
Iteration 207/1000 | Loss: 0.00005920
Iteration 208/1000 | Loss: 0.00005920
Iteration 209/1000 | Loss: 0.00005920
Iteration 210/1000 | Loss: 0.00005920
Iteration 211/1000 | Loss: 0.00005920
Iteration 212/1000 | Loss: 0.00005920
Iteration 213/1000 | Loss: 0.00005920
Iteration 214/1000 | Loss: 0.00005919
Iteration 215/1000 | Loss: 0.00005919
Iteration 216/1000 | Loss: 0.00005919
Iteration 217/1000 | Loss: 0.00005919
Iteration 218/1000 | Loss: 0.00005918
Iteration 219/1000 | Loss: 0.00010289
Iteration 220/1000 | Loss: 0.00005925
Iteration 221/1000 | Loss: 0.00005919
Iteration 222/1000 | Loss: 0.00005917
Iteration 223/1000 | Loss: 0.00005917
Iteration 224/1000 | Loss: 0.00005917
Iteration 225/1000 | Loss: 0.00005917
Iteration 226/1000 | Loss: 0.00005917
Iteration 227/1000 | Loss: 0.00005917
Iteration 228/1000 | Loss: 0.00005917
Iteration 229/1000 | Loss: 0.00005916
Iteration 230/1000 | Loss: 0.00005916
Iteration 231/1000 | Loss: 0.00005915
Iteration 232/1000 | Loss: 0.00005915
Iteration 233/1000 | Loss: 0.00005915
Iteration 234/1000 | Loss: 0.00005915
Iteration 235/1000 | Loss: 0.00005915
Iteration 236/1000 | Loss: 0.00005915
Iteration 237/1000 | Loss: 0.00005914
Iteration 238/1000 | Loss: 0.00005914
Iteration 239/1000 | Loss: 0.00005914
Iteration 240/1000 | Loss: 0.00005914
Iteration 241/1000 | Loss: 0.00005914
Iteration 242/1000 | Loss: 0.00005914
Iteration 243/1000 | Loss: 0.00005914
Iteration 244/1000 | Loss: 0.00005914
Iteration 245/1000 | Loss: 0.00005914
Iteration 246/1000 | Loss: 0.00005914
Iteration 247/1000 | Loss: 0.00005914
Iteration 248/1000 | Loss: 0.00005914
Iteration 249/1000 | Loss: 0.00005914
Iteration 250/1000 | Loss: 0.00005914
Iteration 251/1000 | Loss: 0.00005914
Iteration 252/1000 | Loss: 0.00005914
Iteration 253/1000 | Loss: 0.00005913
Iteration 254/1000 | Loss: 0.00005913
Iteration 255/1000 | Loss: 0.00005913
Iteration 256/1000 | Loss: 0.00005913
Iteration 257/1000 | Loss: 0.00005913
Iteration 258/1000 | Loss: 0.00005913
Iteration 259/1000 | Loss: 0.00005913
Iteration 260/1000 | Loss: 0.00005913
Iteration 261/1000 | Loss: 0.00005913
Iteration 262/1000 | Loss: 0.00005913
Iteration 263/1000 | Loss: 0.00005913
Iteration 264/1000 | Loss: 0.00005913
Iteration 265/1000 | Loss: 0.00005913
Iteration 266/1000 | Loss: 0.00005913
Iteration 267/1000 | Loss: 0.00005913
Iteration 268/1000 | Loss: 0.00005913
Iteration 269/1000 | Loss: 0.00005913
Iteration 270/1000 | Loss: 0.00005913
Iteration 271/1000 | Loss: 0.00005913
Iteration 272/1000 | Loss: 0.00005913
Iteration 273/1000 | Loss: 0.00005913
Iteration 274/1000 | Loss: 0.00005913
Iteration 275/1000 | Loss: 0.00005913
Iteration 276/1000 | Loss: 0.00005913
Iteration 277/1000 | Loss: 0.00005913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [5.913333370699547e-05, 5.913333370699547e-05, 5.913333370699547e-05, 5.913333370699547e-05, 5.913333370699547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.913333370699547e-05

Optimization complete. Final v2v error: 4.322585105895996 mm

Highest mean error: 11.487086296081543 mm for frame 68

Lowest mean error: 3.146327257156372 mm for frame 14

Saving results

Total time: 314.0808832645416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391413
Iteration 2/25 | Loss: 0.00110869
Iteration 3/25 | Loss: 0.00101741
Iteration 4/25 | Loss: 0.00101041
Iteration 5/25 | Loss: 0.00100848
Iteration 6/25 | Loss: 0.00100790
Iteration 7/25 | Loss: 0.00100790
Iteration 8/25 | Loss: 0.00100790
Iteration 9/25 | Loss: 0.00100790
Iteration 10/25 | Loss: 0.00100790
Iteration 11/25 | Loss: 0.00100790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010079017374664545, 0.0010079017374664545, 0.0010079017374664545, 0.0010079017374664545, 0.0010079017374664545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010079017374664545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38601375
Iteration 2/25 | Loss: 0.00067062
Iteration 3/25 | Loss: 0.00067061
Iteration 4/25 | Loss: 0.00067061
Iteration 5/25 | Loss: 0.00067061
Iteration 6/25 | Loss: 0.00067061
Iteration 7/25 | Loss: 0.00067061
Iteration 8/25 | Loss: 0.00067061
Iteration 9/25 | Loss: 0.00067061
Iteration 10/25 | Loss: 0.00067061
Iteration 11/25 | Loss: 0.00067061
Iteration 12/25 | Loss: 0.00067061
Iteration 13/25 | Loss: 0.00067061
Iteration 14/25 | Loss: 0.00067061
Iteration 15/25 | Loss: 0.00067061
Iteration 16/25 | Loss: 0.00067061
Iteration 17/25 | Loss: 0.00067061
Iteration 18/25 | Loss: 0.00067061
Iteration 19/25 | Loss: 0.00067061
Iteration 20/25 | Loss: 0.00067061
Iteration 21/25 | Loss: 0.00067061
Iteration 22/25 | Loss: 0.00067061
Iteration 23/25 | Loss: 0.00067061
Iteration 24/25 | Loss: 0.00067061
Iteration 25/25 | Loss: 0.00067061

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067061
Iteration 2/1000 | Loss: 0.00002994
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001233
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001225
Iteration 13/1000 | Loss: 0.00001224
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001195
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001178
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001177
Iteration 78/1000 | Loss: 0.00001177
Iteration 79/1000 | Loss: 0.00001177
Iteration 80/1000 | Loss: 0.00001177
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001175
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001175
Iteration 98/1000 | Loss: 0.00001175
Iteration 99/1000 | Loss: 0.00001175
Iteration 100/1000 | Loss: 0.00001175
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001174
Iteration 106/1000 | Loss: 0.00001174
Iteration 107/1000 | Loss: 0.00001174
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001173
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001170
Iteration 148/1000 | Loss: 0.00001170
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Iteration 157/1000 | Loss: 0.00001169
Iteration 158/1000 | Loss: 0.00001169
Iteration 159/1000 | Loss: 0.00001169
Iteration 160/1000 | Loss: 0.00001169
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001168
Iteration 170/1000 | Loss: 0.00001168
Iteration 171/1000 | Loss: 0.00001168
Iteration 172/1000 | Loss: 0.00001168
Iteration 173/1000 | Loss: 0.00001168
Iteration 174/1000 | Loss: 0.00001168
Iteration 175/1000 | Loss: 0.00001168
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001167
Iteration 180/1000 | Loss: 0.00001167
Iteration 181/1000 | Loss: 0.00001167
Iteration 182/1000 | Loss: 0.00001167
Iteration 183/1000 | Loss: 0.00001167
Iteration 184/1000 | Loss: 0.00001167
Iteration 185/1000 | Loss: 0.00001167
Iteration 186/1000 | Loss: 0.00001167
Iteration 187/1000 | Loss: 0.00001167
Iteration 188/1000 | Loss: 0.00001167
Iteration 189/1000 | Loss: 0.00001167
Iteration 190/1000 | Loss: 0.00001167
Iteration 191/1000 | Loss: 0.00001167
Iteration 192/1000 | Loss: 0.00001167
Iteration 193/1000 | Loss: 0.00001167
Iteration 194/1000 | Loss: 0.00001167
Iteration 195/1000 | Loss: 0.00001167
Iteration 196/1000 | Loss: 0.00001167
Iteration 197/1000 | Loss: 0.00001167
Iteration 198/1000 | Loss: 0.00001167
Iteration 199/1000 | Loss: 0.00001167
Iteration 200/1000 | Loss: 0.00001167
Iteration 201/1000 | Loss: 0.00001167
Iteration 202/1000 | Loss: 0.00001167
Iteration 203/1000 | Loss: 0.00001167
Iteration 204/1000 | Loss: 0.00001167
Iteration 205/1000 | Loss: 0.00001167
Iteration 206/1000 | Loss: 0.00001167
Iteration 207/1000 | Loss: 0.00001167
Iteration 208/1000 | Loss: 0.00001167
Iteration 209/1000 | Loss: 0.00001167
Iteration 210/1000 | Loss: 0.00001167
Iteration 211/1000 | Loss: 0.00001167
Iteration 212/1000 | Loss: 0.00001167
Iteration 213/1000 | Loss: 0.00001167
Iteration 214/1000 | Loss: 0.00001167
Iteration 215/1000 | Loss: 0.00001167
Iteration 216/1000 | Loss: 0.00001167
Iteration 217/1000 | Loss: 0.00001167
Iteration 218/1000 | Loss: 0.00001167
Iteration 219/1000 | Loss: 0.00001167
Iteration 220/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.1670187632262241e-05, 1.1670187632262241e-05, 1.1670187632262241e-05, 1.1670187632262241e-05, 1.1670187632262241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1670187632262241e-05

Optimization complete. Final v2v error: 2.799736738204956 mm

Highest mean error: 3.1015121936798096 mm for frame 21

Lowest mean error: 2.4337446689605713 mm for frame 65

Saving results

Total time: 39.511579513549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878331
Iteration 2/25 | Loss: 0.00161671
Iteration 3/25 | Loss: 0.00124642
Iteration 4/25 | Loss: 0.00118755
Iteration 5/25 | Loss: 0.00118632
Iteration 6/25 | Loss: 0.00116638
Iteration 7/25 | Loss: 0.00111968
Iteration 8/25 | Loss: 0.00110484
Iteration 9/25 | Loss: 0.00110020
Iteration 10/25 | Loss: 0.00109688
Iteration 11/25 | Loss: 0.00109626
Iteration 12/25 | Loss: 0.00109613
Iteration 13/25 | Loss: 0.00109604
Iteration 14/25 | Loss: 0.00109598
Iteration 15/25 | Loss: 0.00109597
Iteration 16/25 | Loss: 0.00109597
Iteration 17/25 | Loss: 0.00109597
Iteration 18/25 | Loss: 0.00109597
Iteration 19/25 | Loss: 0.00109597
Iteration 20/25 | Loss: 0.00109596
Iteration 21/25 | Loss: 0.00109596
Iteration 22/25 | Loss: 0.00109596
Iteration 23/25 | Loss: 0.00109596
Iteration 24/25 | Loss: 0.00109596
Iteration 25/25 | Loss: 0.00109596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55026221
Iteration 2/25 | Loss: 0.00057179
Iteration 3/25 | Loss: 0.00057174
Iteration 4/25 | Loss: 0.00057174
Iteration 5/25 | Loss: 0.00057174
Iteration 6/25 | Loss: 0.00057174
Iteration 7/25 | Loss: 0.00057174
Iteration 8/25 | Loss: 0.00057174
Iteration 9/25 | Loss: 0.00057174
Iteration 10/25 | Loss: 0.00057174
Iteration 11/25 | Loss: 0.00057174
Iteration 12/25 | Loss: 0.00057174
Iteration 13/25 | Loss: 0.00057174
Iteration 14/25 | Loss: 0.00057174
Iteration 15/25 | Loss: 0.00057174
Iteration 16/25 | Loss: 0.00057174
Iteration 17/25 | Loss: 0.00057174
Iteration 18/25 | Loss: 0.00057174
Iteration 19/25 | Loss: 0.00057174
Iteration 20/25 | Loss: 0.00057174
Iteration 21/25 | Loss: 0.00057174
Iteration 22/25 | Loss: 0.00057174
Iteration 23/25 | Loss: 0.00057174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000571735086850822, 0.000571735086850822, 0.000571735086850822, 0.000571735086850822, 0.000571735086850822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000571735086850822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057174
Iteration 2/1000 | Loss: 0.00005053
Iteration 3/1000 | Loss: 0.00003358
Iteration 4/1000 | Loss: 0.00002897
Iteration 5/1000 | Loss: 0.00002684
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002451
Iteration 8/1000 | Loss: 0.00002394
Iteration 9/1000 | Loss: 0.00002348
Iteration 10/1000 | Loss: 0.00020336
Iteration 11/1000 | Loss: 0.00004964
Iteration 12/1000 | Loss: 0.00002578
Iteration 13/1000 | Loss: 0.00011338
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002217
Iteration 16/1000 | Loss: 0.00002171
Iteration 17/1000 | Loss: 0.00002135
Iteration 18/1000 | Loss: 0.00002114
Iteration 19/1000 | Loss: 0.00004928
Iteration 20/1000 | Loss: 0.00003794
Iteration 21/1000 | Loss: 0.00004985
Iteration 22/1000 | Loss: 0.00002096
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002093
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002089
Iteration 29/1000 | Loss: 0.00002088
Iteration 30/1000 | Loss: 0.00002087
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002086
Iteration 35/1000 | Loss: 0.00002086
Iteration 36/1000 | Loss: 0.00002085
Iteration 37/1000 | Loss: 0.00003519
Iteration 38/1000 | Loss: 0.00008525
Iteration 39/1000 | Loss: 0.00002084
Iteration 40/1000 | Loss: 0.00002083
Iteration 41/1000 | Loss: 0.00002081
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002079
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002079
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00002079
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002078
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002078
Iteration 58/1000 | Loss: 0.00003298
Iteration 59/1000 | Loss: 0.00007886
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00003180
Iteration 65/1000 | Loss: 0.00006434
Iteration 66/1000 | Loss: 0.00002579
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002115
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002069
Iteration 89/1000 | Loss: 0.00002069
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002068
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002067
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002066
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002066
Iteration 121/1000 | Loss: 0.00002066
Iteration 122/1000 | Loss: 0.00002066
Iteration 123/1000 | Loss: 0.00002066
Iteration 124/1000 | Loss: 0.00002066
Iteration 125/1000 | Loss: 0.00002066
Iteration 126/1000 | Loss: 0.00002066
Iteration 127/1000 | Loss: 0.00002066
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002066
Iteration 130/1000 | Loss: 0.00002066
Iteration 131/1000 | Loss: 0.00002066
Iteration 132/1000 | Loss: 0.00002066
Iteration 133/1000 | Loss: 0.00002065
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002065
Iteration 136/1000 | Loss: 0.00002065
Iteration 137/1000 | Loss: 0.00002064
Iteration 138/1000 | Loss: 0.00002064
Iteration 139/1000 | Loss: 0.00002064
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00004264
Iteration 142/1000 | Loss: 0.00002147
Iteration 143/1000 | Loss: 0.00002112
Iteration 144/1000 | Loss: 0.00002065
Iteration 145/1000 | Loss: 0.00002062
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002062
Iteration 148/1000 | Loss: 0.00002062
Iteration 149/1000 | Loss: 0.00002062
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002062
Iteration 152/1000 | Loss: 0.00002062
Iteration 153/1000 | Loss: 0.00002062
Iteration 154/1000 | Loss: 0.00002062
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002062
Iteration 158/1000 | Loss: 0.00002062
Iteration 159/1000 | Loss: 0.00002061
Iteration 160/1000 | Loss: 0.00002061
Iteration 161/1000 | Loss: 0.00002061
Iteration 162/1000 | Loss: 0.00002061
Iteration 163/1000 | Loss: 0.00002061
Iteration 164/1000 | Loss: 0.00002061
Iteration 165/1000 | Loss: 0.00002061
Iteration 166/1000 | Loss: 0.00002061
Iteration 167/1000 | Loss: 0.00002061
Iteration 168/1000 | Loss: 0.00002061
Iteration 169/1000 | Loss: 0.00002061
Iteration 170/1000 | Loss: 0.00002061
Iteration 171/1000 | Loss: 0.00002061
Iteration 172/1000 | Loss: 0.00002061
Iteration 173/1000 | Loss: 0.00002061
Iteration 174/1000 | Loss: 0.00002061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.0609095372492447e-05, 2.0609095372492447e-05, 2.0609095372492447e-05, 2.0609095372492447e-05, 2.0609095372492447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0609095372492447e-05

Optimization complete. Final v2v error: 3.7584474086761475 mm

Highest mean error: 4.425521373748779 mm for frame 53

Lowest mean error: 3.020662307739258 mm for frame 150

Saving results

Total time: 92.76625967025757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806946
Iteration 2/25 | Loss: 0.00158189
Iteration 3/25 | Loss: 0.00128137
Iteration 4/25 | Loss: 0.00125355
Iteration 5/25 | Loss: 0.00124844
Iteration 6/25 | Loss: 0.00124745
Iteration 7/25 | Loss: 0.00124745
Iteration 8/25 | Loss: 0.00124745
Iteration 9/25 | Loss: 0.00124745
Iteration 10/25 | Loss: 0.00124745
Iteration 11/25 | Loss: 0.00124745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012474473332986236, 0.0012474473332986236, 0.0012474473332986236, 0.0012474473332986236, 0.0012474473332986236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012474473332986236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26140317
Iteration 2/25 | Loss: 0.00109558
Iteration 3/25 | Loss: 0.00109558
Iteration 4/25 | Loss: 0.00109558
Iteration 5/25 | Loss: 0.00109558
Iteration 6/25 | Loss: 0.00109558
Iteration 7/25 | Loss: 0.00109558
Iteration 8/25 | Loss: 0.00109558
Iteration 9/25 | Loss: 0.00109558
Iteration 10/25 | Loss: 0.00109558
Iteration 11/25 | Loss: 0.00109558
Iteration 12/25 | Loss: 0.00109558
Iteration 13/25 | Loss: 0.00109558
Iteration 14/25 | Loss: 0.00109558
Iteration 15/25 | Loss: 0.00109558
Iteration 16/25 | Loss: 0.00109558
Iteration 17/25 | Loss: 0.00109558
Iteration 18/25 | Loss: 0.00109558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001095578190870583, 0.001095578190870583, 0.001095578190870583, 0.001095578190870583, 0.001095578190870583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095578190870583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109558
Iteration 2/1000 | Loss: 0.00006450
Iteration 3/1000 | Loss: 0.00004181
Iteration 4/1000 | Loss: 0.00003221
Iteration 5/1000 | Loss: 0.00002923
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002630
Iteration 8/1000 | Loss: 0.00002544
Iteration 9/1000 | Loss: 0.00002503
Iteration 10/1000 | Loss: 0.00002483
Iteration 11/1000 | Loss: 0.00002465
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002437
Iteration 14/1000 | Loss: 0.00002434
Iteration 15/1000 | Loss: 0.00002432
Iteration 16/1000 | Loss: 0.00002427
Iteration 17/1000 | Loss: 0.00002424
Iteration 18/1000 | Loss: 0.00002419
Iteration 19/1000 | Loss: 0.00002416
Iteration 20/1000 | Loss: 0.00002414
Iteration 21/1000 | Loss: 0.00002413
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002413
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002411
Iteration 26/1000 | Loss: 0.00002411
Iteration 27/1000 | Loss: 0.00002411
Iteration 28/1000 | Loss: 0.00002410
Iteration 29/1000 | Loss: 0.00002409
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002409
Iteration 33/1000 | Loss: 0.00002409
Iteration 34/1000 | Loss: 0.00002409
Iteration 35/1000 | Loss: 0.00002409
Iteration 36/1000 | Loss: 0.00002409
Iteration 37/1000 | Loss: 0.00002408
Iteration 38/1000 | Loss: 0.00002407
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002406
Iteration 41/1000 | Loss: 0.00002406
Iteration 42/1000 | Loss: 0.00002406
Iteration 43/1000 | Loss: 0.00002406
Iteration 44/1000 | Loss: 0.00002406
Iteration 45/1000 | Loss: 0.00002406
Iteration 46/1000 | Loss: 0.00002406
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00002406
Iteration 49/1000 | Loss: 0.00002406
Iteration 50/1000 | Loss: 0.00002405
Iteration 51/1000 | Loss: 0.00002405
Iteration 52/1000 | Loss: 0.00002405
Iteration 53/1000 | Loss: 0.00002405
Iteration 54/1000 | Loss: 0.00002405
Iteration 55/1000 | Loss: 0.00002404
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002403
Iteration 61/1000 | Loss: 0.00002403
Iteration 62/1000 | Loss: 0.00002403
Iteration 63/1000 | Loss: 0.00002403
Iteration 64/1000 | Loss: 0.00002403
Iteration 65/1000 | Loss: 0.00002403
Iteration 66/1000 | Loss: 0.00002403
Iteration 67/1000 | Loss: 0.00002403
Iteration 68/1000 | Loss: 0.00002403
Iteration 69/1000 | Loss: 0.00002403
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002402
Iteration 72/1000 | Loss: 0.00002402
Iteration 73/1000 | Loss: 0.00002402
Iteration 74/1000 | Loss: 0.00002402
Iteration 75/1000 | Loss: 0.00002402
Iteration 76/1000 | Loss: 0.00002402
Iteration 77/1000 | Loss: 0.00002401
Iteration 78/1000 | Loss: 0.00002401
Iteration 79/1000 | Loss: 0.00002401
Iteration 80/1000 | Loss: 0.00002401
Iteration 81/1000 | Loss: 0.00002401
Iteration 82/1000 | Loss: 0.00002401
Iteration 83/1000 | Loss: 0.00002401
Iteration 84/1000 | Loss: 0.00002401
Iteration 85/1000 | Loss: 0.00002401
Iteration 86/1000 | Loss: 0.00002401
Iteration 87/1000 | Loss: 0.00002400
Iteration 88/1000 | Loss: 0.00002400
Iteration 89/1000 | Loss: 0.00002400
Iteration 90/1000 | Loss: 0.00002400
Iteration 91/1000 | Loss: 0.00002400
Iteration 92/1000 | Loss: 0.00002400
Iteration 93/1000 | Loss: 0.00002400
Iteration 94/1000 | Loss: 0.00002400
Iteration 95/1000 | Loss: 0.00002400
Iteration 96/1000 | Loss: 0.00002400
Iteration 97/1000 | Loss: 0.00002399
Iteration 98/1000 | Loss: 0.00002399
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002399
Iteration 101/1000 | Loss: 0.00002399
Iteration 102/1000 | Loss: 0.00002399
Iteration 103/1000 | Loss: 0.00002399
Iteration 104/1000 | Loss: 0.00002399
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002398
Iteration 108/1000 | Loss: 0.00002398
Iteration 109/1000 | Loss: 0.00002398
Iteration 110/1000 | Loss: 0.00002398
Iteration 111/1000 | Loss: 0.00002398
Iteration 112/1000 | Loss: 0.00002398
Iteration 113/1000 | Loss: 0.00002398
Iteration 114/1000 | Loss: 0.00002398
Iteration 115/1000 | Loss: 0.00002398
Iteration 116/1000 | Loss: 0.00002398
Iteration 117/1000 | Loss: 0.00002398
Iteration 118/1000 | Loss: 0.00002398
Iteration 119/1000 | Loss: 0.00002398
Iteration 120/1000 | Loss: 0.00002398
Iteration 121/1000 | Loss: 0.00002398
Iteration 122/1000 | Loss: 0.00002398
Iteration 123/1000 | Loss: 0.00002398
Iteration 124/1000 | Loss: 0.00002398
Iteration 125/1000 | Loss: 0.00002398
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002398
Iteration 130/1000 | Loss: 0.00002398
Iteration 131/1000 | Loss: 0.00002398
Iteration 132/1000 | Loss: 0.00002398
Iteration 133/1000 | Loss: 0.00002398
Iteration 134/1000 | Loss: 0.00002398
Iteration 135/1000 | Loss: 0.00002398
Iteration 136/1000 | Loss: 0.00002398
Iteration 137/1000 | Loss: 0.00002398
Iteration 138/1000 | Loss: 0.00002398
Iteration 139/1000 | Loss: 0.00002398
Iteration 140/1000 | Loss: 0.00002398
Iteration 141/1000 | Loss: 0.00002398
Iteration 142/1000 | Loss: 0.00002398
Iteration 143/1000 | Loss: 0.00002398
Iteration 144/1000 | Loss: 0.00002398
Iteration 145/1000 | Loss: 0.00002398
Iteration 146/1000 | Loss: 0.00002398
Iteration 147/1000 | Loss: 0.00002398
Iteration 148/1000 | Loss: 0.00002398
Iteration 149/1000 | Loss: 0.00002398
Iteration 150/1000 | Loss: 0.00002398
Iteration 151/1000 | Loss: 0.00002398
Iteration 152/1000 | Loss: 0.00002398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.3977858290891163e-05, 2.3977858290891163e-05, 2.3977858290891163e-05, 2.3977858290891163e-05, 2.3977858290891163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3977858290891163e-05

Optimization complete. Final v2v error: 3.9135403633117676 mm

Highest mean error: 4.733376502990723 mm for frame 9

Lowest mean error: 3.219531536102295 mm for frame 154

Saving results

Total time: 37.18210434913635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806543
Iteration 2/25 | Loss: 0.00112740
Iteration 3/25 | Loss: 0.00103274
Iteration 4/25 | Loss: 0.00101865
Iteration 5/25 | Loss: 0.00101499
Iteration 6/25 | Loss: 0.00101429
Iteration 7/25 | Loss: 0.00101429
Iteration 8/25 | Loss: 0.00101429
Iteration 9/25 | Loss: 0.00101429
Iteration 10/25 | Loss: 0.00101429
Iteration 11/25 | Loss: 0.00101429
Iteration 12/25 | Loss: 0.00101429
Iteration 13/25 | Loss: 0.00101429
Iteration 14/25 | Loss: 0.00101429
Iteration 15/25 | Loss: 0.00101429
Iteration 16/25 | Loss: 0.00101429
Iteration 17/25 | Loss: 0.00101429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010142857208848, 0.0010142857208848, 0.0010142857208848, 0.0010142857208848, 0.0010142857208848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010142857208848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42311251
Iteration 2/25 | Loss: 0.00062042
Iteration 3/25 | Loss: 0.00062042
Iteration 4/25 | Loss: 0.00062042
Iteration 5/25 | Loss: 0.00062042
Iteration 6/25 | Loss: 0.00062042
Iteration 7/25 | Loss: 0.00062042
Iteration 8/25 | Loss: 0.00062042
Iteration 9/25 | Loss: 0.00062042
Iteration 10/25 | Loss: 0.00062042
Iteration 11/25 | Loss: 0.00062042
Iteration 12/25 | Loss: 0.00062042
Iteration 13/25 | Loss: 0.00062042
Iteration 14/25 | Loss: 0.00062042
Iteration 15/25 | Loss: 0.00062042
Iteration 16/25 | Loss: 0.00062042
Iteration 17/25 | Loss: 0.00062042
Iteration 18/25 | Loss: 0.00062042
Iteration 19/25 | Loss: 0.00062042
Iteration 20/25 | Loss: 0.00062042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006204195669852197, 0.0006204195669852197, 0.0006204195669852197, 0.0006204195669852197, 0.0006204195669852197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006204195669852197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062042
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00001691
Iteration 4/1000 | Loss: 0.00001398
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001237
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001185
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001180
Iteration 15/1000 | Loss: 0.00001179
Iteration 16/1000 | Loss: 0.00001171
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001147
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001146
Iteration 40/1000 | Loss: 0.00001146
Iteration 41/1000 | Loss: 0.00001146
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001143
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001143
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001141
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001141
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001140
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001137
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001134
Iteration 98/1000 | Loss: 0.00001134
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001128
Iteration 171/1000 | Loss: 0.00001128
Iteration 172/1000 | Loss: 0.00001128
Iteration 173/1000 | Loss: 0.00001128
Iteration 174/1000 | Loss: 0.00001128
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001127
Iteration 180/1000 | Loss: 0.00001126
Iteration 181/1000 | Loss: 0.00001126
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001125
Iteration 186/1000 | Loss: 0.00001125
Iteration 187/1000 | Loss: 0.00001125
Iteration 188/1000 | Loss: 0.00001125
Iteration 189/1000 | Loss: 0.00001125
Iteration 190/1000 | Loss: 0.00001125
Iteration 191/1000 | Loss: 0.00001125
Iteration 192/1000 | Loss: 0.00001125
Iteration 193/1000 | Loss: 0.00001125
Iteration 194/1000 | Loss: 0.00001125
Iteration 195/1000 | Loss: 0.00001125
Iteration 196/1000 | Loss: 0.00001125
Iteration 197/1000 | Loss: 0.00001125
Iteration 198/1000 | Loss: 0.00001125
Iteration 199/1000 | Loss: 0.00001125
Iteration 200/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1245037967455573e-05, 1.1245037967455573e-05, 1.1245037967455573e-05, 1.1245037967455573e-05, 1.1245037967455573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1245037967455573e-05

Optimization complete. Final v2v error: 2.8685131072998047 mm

Highest mean error: 3.6002888679504395 mm for frame 86

Lowest mean error: 2.6801044940948486 mm for frame 135

Saving results

Total time: 38.19421458244324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806780
Iteration 2/25 | Loss: 0.00119913
Iteration 3/25 | Loss: 0.00098653
Iteration 4/25 | Loss: 0.00096076
Iteration 5/25 | Loss: 0.00095596
Iteration 6/25 | Loss: 0.00095534
Iteration 7/25 | Loss: 0.00095534
Iteration 8/25 | Loss: 0.00095534
Iteration 9/25 | Loss: 0.00095534
Iteration 10/25 | Loss: 0.00095534
Iteration 11/25 | Loss: 0.00095534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009553361451253295, 0.0009553361451253295, 0.0009553361451253295, 0.0009553361451253295, 0.0009553361451253295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009553361451253295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37368703
Iteration 2/25 | Loss: 0.00065922
Iteration 3/25 | Loss: 0.00065922
Iteration 4/25 | Loss: 0.00065922
Iteration 5/25 | Loss: 0.00065922
Iteration 6/25 | Loss: 0.00065922
Iteration 7/25 | Loss: 0.00065922
Iteration 8/25 | Loss: 0.00065922
Iteration 9/25 | Loss: 0.00065921
Iteration 10/25 | Loss: 0.00065921
Iteration 11/25 | Loss: 0.00065921
Iteration 12/25 | Loss: 0.00065921
Iteration 13/25 | Loss: 0.00065921
Iteration 14/25 | Loss: 0.00065921
Iteration 15/25 | Loss: 0.00065921
Iteration 16/25 | Loss: 0.00065921
Iteration 17/25 | Loss: 0.00065921
Iteration 18/25 | Loss: 0.00065921
Iteration 19/25 | Loss: 0.00065921
Iteration 20/25 | Loss: 0.00065921
Iteration 21/25 | Loss: 0.00065921
Iteration 22/25 | Loss: 0.00065921
Iteration 23/25 | Loss: 0.00065921
Iteration 24/25 | Loss: 0.00065921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006592139252461493, 0.0006592139252461493, 0.0006592139252461493, 0.0006592139252461493, 0.0006592139252461493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006592139252461493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065921
Iteration 2/1000 | Loss: 0.00002121
Iteration 3/1000 | Loss: 0.00001205
Iteration 4/1000 | Loss: 0.00001037
Iteration 5/1000 | Loss: 0.00000946
Iteration 6/1000 | Loss: 0.00000883
Iteration 7/1000 | Loss: 0.00000848
Iteration 8/1000 | Loss: 0.00000823
Iteration 9/1000 | Loss: 0.00000799
Iteration 10/1000 | Loss: 0.00000776
Iteration 11/1000 | Loss: 0.00000771
Iteration 12/1000 | Loss: 0.00000770
Iteration 13/1000 | Loss: 0.00000768
Iteration 14/1000 | Loss: 0.00000767
Iteration 15/1000 | Loss: 0.00000767
Iteration 16/1000 | Loss: 0.00000765
Iteration 17/1000 | Loss: 0.00000765
Iteration 18/1000 | Loss: 0.00000764
Iteration 19/1000 | Loss: 0.00000763
Iteration 20/1000 | Loss: 0.00000763
Iteration 21/1000 | Loss: 0.00000761
Iteration 22/1000 | Loss: 0.00000760
Iteration 23/1000 | Loss: 0.00000755
Iteration 24/1000 | Loss: 0.00000753
Iteration 25/1000 | Loss: 0.00000752
Iteration 26/1000 | Loss: 0.00000752
Iteration 27/1000 | Loss: 0.00000752
Iteration 28/1000 | Loss: 0.00000751
Iteration 29/1000 | Loss: 0.00000751
Iteration 30/1000 | Loss: 0.00000751
Iteration 31/1000 | Loss: 0.00000750
Iteration 32/1000 | Loss: 0.00000750
Iteration 33/1000 | Loss: 0.00000750
Iteration 34/1000 | Loss: 0.00000749
Iteration 35/1000 | Loss: 0.00000748
Iteration 36/1000 | Loss: 0.00000748
Iteration 37/1000 | Loss: 0.00000747
Iteration 38/1000 | Loss: 0.00000747
Iteration 39/1000 | Loss: 0.00000746
Iteration 40/1000 | Loss: 0.00000746
Iteration 41/1000 | Loss: 0.00000746
Iteration 42/1000 | Loss: 0.00000745
Iteration 43/1000 | Loss: 0.00000745
Iteration 44/1000 | Loss: 0.00000745
Iteration 45/1000 | Loss: 0.00000745
Iteration 46/1000 | Loss: 0.00000744
Iteration 47/1000 | Loss: 0.00000744
Iteration 48/1000 | Loss: 0.00000743
Iteration 49/1000 | Loss: 0.00000743
Iteration 50/1000 | Loss: 0.00000743
Iteration 51/1000 | Loss: 0.00000743
Iteration 52/1000 | Loss: 0.00000743
Iteration 53/1000 | Loss: 0.00000743
Iteration 54/1000 | Loss: 0.00000743
Iteration 55/1000 | Loss: 0.00000743
Iteration 56/1000 | Loss: 0.00000743
Iteration 57/1000 | Loss: 0.00000743
Iteration 58/1000 | Loss: 0.00000742
Iteration 59/1000 | Loss: 0.00000742
Iteration 60/1000 | Loss: 0.00000742
Iteration 61/1000 | Loss: 0.00000742
Iteration 62/1000 | Loss: 0.00000742
Iteration 63/1000 | Loss: 0.00000742
Iteration 64/1000 | Loss: 0.00000741
Iteration 65/1000 | Loss: 0.00000741
Iteration 66/1000 | Loss: 0.00000741
Iteration 67/1000 | Loss: 0.00000741
Iteration 68/1000 | Loss: 0.00000741
Iteration 69/1000 | Loss: 0.00000741
Iteration 70/1000 | Loss: 0.00000741
Iteration 71/1000 | Loss: 0.00000741
Iteration 72/1000 | Loss: 0.00000741
Iteration 73/1000 | Loss: 0.00000741
Iteration 74/1000 | Loss: 0.00000741
Iteration 75/1000 | Loss: 0.00000741
Iteration 76/1000 | Loss: 0.00000741
Iteration 77/1000 | Loss: 0.00000741
Iteration 78/1000 | Loss: 0.00000741
Iteration 79/1000 | Loss: 0.00000741
Iteration 80/1000 | Loss: 0.00000741
Iteration 81/1000 | Loss: 0.00000741
Iteration 82/1000 | Loss: 0.00000741
Iteration 83/1000 | Loss: 0.00000741
Iteration 84/1000 | Loss: 0.00000741
Iteration 85/1000 | Loss: 0.00000741
Iteration 86/1000 | Loss: 0.00000741
Iteration 87/1000 | Loss: 0.00000741
Iteration 88/1000 | Loss: 0.00000741
Iteration 89/1000 | Loss: 0.00000741
Iteration 90/1000 | Loss: 0.00000741
Iteration 91/1000 | Loss: 0.00000741
Iteration 92/1000 | Loss: 0.00000741
Iteration 93/1000 | Loss: 0.00000741
Iteration 94/1000 | Loss: 0.00000741
Iteration 95/1000 | Loss: 0.00000741
Iteration 96/1000 | Loss: 0.00000741
Iteration 97/1000 | Loss: 0.00000741
Iteration 98/1000 | Loss: 0.00000741
Iteration 99/1000 | Loss: 0.00000741
Iteration 100/1000 | Loss: 0.00000741
Iteration 101/1000 | Loss: 0.00000741
Iteration 102/1000 | Loss: 0.00000741
Iteration 103/1000 | Loss: 0.00000741
Iteration 104/1000 | Loss: 0.00000741
Iteration 105/1000 | Loss: 0.00000741
Iteration 106/1000 | Loss: 0.00000741
Iteration 107/1000 | Loss: 0.00000741
Iteration 108/1000 | Loss: 0.00000741
Iteration 109/1000 | Loss: 0.00000741
Iteration 110/1000 | Loss: 0.00000741
Iteration 111/1000 | Loss: 0.00000741
Iteration 112/1000 | Loss: 0.00000741
Iteration 113/1000 | Loss: 0.00000741
Iteration 114/1000 | Loss: 0.00000741
Iteration 115/1000 | Loss: 0.00000741
Iteration 116/1000 | Loss: 0.00000741
Iteration 117/1000 | Loss: 0.00000741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [7.406212716887239e-06, 7.406212716887239e-06, 7.406212716887239e-06, 7.406212716887239e-06, 7.406212716887239e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.406212716887239e-06

Optimization complete. Final v2v error: 2.3469483852386475 mm

Highest mean error: 2.517394542694092 mm for frame 138

Lowest mean error: 2.2360618114471436 mm for frame 80

Saving results

Total time: 34.09550213813782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392455
Iteration 2/25 | Loss: 0.00104348
Iteration 3/25 | Loss: 0.00096043
Iteration 4/25 | Loss: 0.00094689
Iteration 5/25 | Loss: 0.00094260
Iteration 6/25 | Loss: 0.00094176
Iteration 7/25 | Loss: 0.00094176
Iteration 8/25 | Loss: 0.00094176
Iteration 9/25 | Loss: 0.00094176
Iteration 10/25 | Loss: 0.00094176
Iteration 11/25 | Loss: 0.00094176
Iteration 12/25 | Loss: 0.00094176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009417576948180795, 0.0009417576948180795, 0.0009417576948180795, 0.0009417576948180795, 0.0009417576948180795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009417576948180795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96257877
Iteration 2/25 | Loss: 0.00058898
Iteration 3/25 | Loss: 0.00058897
Iteration 4/25 | Loss: 0.00058897
Iteration 5/25 | Loss: 0.00058897
Iteration 6/25 | Loss: 0.00058897
Iteration 7/25 | Loss: 0.00058897
Iteration 8/25 | Loss: 0.00058897
Iteration 9/25 | Loss: 0.00058897
Iteration 10/25 | Loss: 0.00058897
Iteration 11/25 | Loss: 0.00058897
Iteration 12/25 | Loss: 0.00058897
Iteration 13/25 | Loss: 0.00058897
Iteration 14/25 | Loss: 0.00058897
Iteration 15/25 | Loss: 0.00058897
Iteration 16/25 | Loss: 0.00058897
Iteration 17/25 | Loss: 0.00058897
Iteration 18/25 | Loss: 0.00058897
Iteration 19/25 | Loss: 0.00058897
Iteration 20/25 | Loss: 0.00058897
Iteration 21/25 | Loss: 0.00058897
Iteration 22/25 | Loss: 0.00058897
Iteration 23/25 | Loss: 0.00058897
Iteration 24/25 | Loss: 0.00058897
Iteration 25/25 | Loss: 0.00058897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058897
Iteration 2/1000 | Loss: 0.00001666
Iteration 3/1000 | Loss: 0.00001198
Iteration 4/1000 | Loss: 0.00001032
Iteration 5/1000 | Loss: 0.00000953
Iteration 6/1000 | Loss: 0.00000917
Iteration 7/1000 | Loss: 0.00000887
Iteration 8/1000 | Loss: 0.00000870
Iteration 9/1000 | Loss: 0.00000867
Iteration 10/1000 | Loss: 0.00000859
Iteration 11/1000 | Loss: 0.00000853
Iteration 12/1000 | Loss: 0.00000852
Iteration 13/1000 | Loss: 0.00000846
Iteration 14/1000 | Loss: 0.00000845
Iteration 15/1000 | Loss: 0.00000844
Iteration 16/1000 | Loss: 0.00000843
Iteration 17/1000 | Loss: 0.00000842
Iteration 18/1000 | Loss: 0.00000840
Iteration 19/1000 | Loss: 0.00000840
Iteration 20/1000 | Loss: 0.00000839
Iteration 21/1000 | Loss: 0.00000839
Iteration 22/1000 | Loss: 0.00000838
Iteration 23/1000 | Loss: 0.00000838
Iteration 24/1000 | Loss: 0.00000837
Iteration 25/1000 | Loss: 0.00000837
Iteration 26/1000 | Loss: 0.00000837
Iteration 27/1000 | Loss: 0.00000836
Iteration 28/1000 | Loss: 0.00000836
Iteration 29/1000 | Loss: 0.00000836
Iteration 30/1000 | Loss: 0.00000835
Iteration 31/1000 | Loss: 0.00000834
Iteration 32/1000 | Loss: 0.00000833
Iteration 33/1000 | Loss: 0.00000833
Iteration 34/1000 | Loss: 0.00000832
Iteration 35/1000 | Loss: 0.00000832
Iteration 36/1000 | Loss: 0.00000831
Iteration 37/1000 | Loss: 0.00000830
Iteration 38/1000 | Loss: 0.00000829
Iteration 39/1000 | Loss: 0.00000829
Iteration 40/1000 | Loss: 0.00000828
Iteration 41/1000 | Loss: 0.00000828
Iteration 42/1000 | Loss: 0.00000828
Iteration 43/1000 | Loss: 0.00000827
Iteration 44/1000 | Loss: 0.00000826
Iteration 45/1000 | Loss: 0.00000826
Iteration 46/1000 | Loss: 0.00000825
Iteration 47/1000 | Loss: 0.00000824
Iteration 48/1000 | Loss: 0.00000823
Iteration 49/1000 | Loss: 0.00000823
Iteration 50/1000 | Loss: 0.00000822
Iteration 51/1000 | Loss: 0.00000822
Iteration 52/1000 | Loss: 0.00000822
Iteration 53/1000 | Loss: 0.00000822
Iteration 54/1000 | Loss: 0.00000821
Iteration 55/1000 | Loss: 0.00000820
Iteration 56/1000 | Loss: 0.00000820
Iteration 57/1000 | Loss: 0.00000820
Iteration 58/1000 | Loss: 0.00000820
Iteration 59/1000 | Loss: 0.00000819
Iteration 60/1000 | Loss: 0.00000819
Iteration 61/1000 | Loss: 0.00000818
Iteration 62/1000 | Loss: 0.00000818
Iteration 63/1000 | Loss: 0.00000817
Iteration 64/1000 | Loss: 0.00000817
Iteration 65/1000 | Loss: 0.00000817
Iteration 66/1000 | Loss: 0.00000816
Iteration 67/1000 | Loss: 0.00000816
Iteration 68/1000 | Loss: 0.00000816
Iteration 69/1000 | Loss: 0.00000816
Iteration 70/1000 | Loss: 0.00000816
Iteration 71/1000 | Loss: 0.00000815
Iteration 72/1000 | Loss: 0.00000815
Iteration 73/1000 | Loss: 0.00000815
Iteration 74/1000 | Loss: 0.00000815
Iteration 75/1000 | Loss: 0.00000815
Iteration 76/1000 | Loss: 0.00000815
Iteration 77/1000 | Loss: 0.00000814
Iteration 78/1000 | Loss: 0.00000814
Iteration 79/1000 | Loss: 0.00000814
Iteration 80/1000 | Loss: 0.00000814
Iteration 81/1000 | Loss: 0.00000814
Iteration 82/1000 | Loss: 0.00000814
Iteration 83/1000 | Loss: 0.00000814
Iteration 84/1000 | Loss: 0.00000814
Iteration 85/1000 | Loss: 0.00000814
Iteration 86/1000 | Loss: 0.00000814
Iteration 87/1000 | Loss: 0.00000813
Iteration 88/1000 | Loss: 0.00000813
Iteration 89/1000 | Loss: 0.00000813
Iteration 90/1000 | Loss: 0.00000812
Iteration 91/1000 | Loss: 0.00000812
Iteration 92/1000 | Loss: 0.00000812
Iteration 93/1000 | Loss: 0.00000812
Iteration 94/1000 | Loss: 0.00000812
Iteration 95/1000 | Loss: 0.00000811
Iteration 96/1000 | Loss: 0.00000810
Iteration 97/1000 | Loss: 0.00000810
Iteration 98/1000 | Loss: 0.00000810
Iteration 99/1000 | Loss: 0.00000810
Iteration 100/1000 | Loss: 0.00000810
Iteration 101/1000 | Loss: 0.00000810
Iteration 102/1000 | Loss: 0.00000810
Iteration 103/1000 | Loss: 0.00000809
Iteration 104/1000 | Loss: 0.00000809
Iteration 105/1000 | Loss: 0.00000809
Iteration 106/1000 | Loss: 0.00000809
Iteration 107/1000 | Loss: 0.00000809
Iteration 108/1000 | Loss: 0.00000809
Iteration 109/1000 | Loss: 0.00000809
Iteration 110/1000 | Loss: 0.00000809
Iteration 111/1000 | Loss: 0.00000808
Iteration 112/1000 | Loss: 0.00000807
Iteration 113/1000 | Loss: 0.00000806
Iteration 114/1000 | Loss: 0.00000806
Iteration 115/1000 | Loss: 0.00000806
Iteration 116/1000 | Loss: 0.00000806
Iteration 117/1000 | Loss: 0.00000806
Iteration 118/1000 | Loss: 0.00000805
Iteration 119/1000 | Loss: 0.00000805
Iteration 120/1000 | Loss: 0.00000805
Iteration 121/1000 | Loss: 0.00000805
Iteration 122/1000 | Loss: 0.00000805
Iteration 123/1000 | Loss: 0.00000805
Iteration 124/1000 | Loss: 0.00000804
Iteration 125/1000 | Loss: 0.00000804
Iteration 126/1000 | Loss: 0.00000804
Iteration 127/1000 | Loss: 0.00000804
Iteration 128/1000 | Loss: 0.00000803
Iteration 129/1000 | Loss: 0.00000803
Iteration 130/1000 | Loss: 0.00000803
Iteration 131/1000 | Loss: 0.00000803
Iteration 132/1000 | Loss: 0.00000803
Iteration 133/1000 | Loss: 0.00000803
Iteration 134/1000 | Loss: 0.00000802
Iteration 135/1000 | Loss: 0.00000802
Iteration 136/1000 | Loss: 0.00000802
Iteration 137/1000 | Loss: 0.00000802
Iteration 138/1000 | Loss: 0.00000802
Iteration 139/1000 | Loss: 0.00000802
Iteration 140/1000 | Loss: 0.00000801
Iteration 141/1000 | Loss: 0.00000801
Iteration 142/1000 | Loss: 0.00000801
Iteration 143/1000 | Loss: 0.00000801
Iteration 144/1000 | Loss: 0.00000801
Iteration 145/1000 | Loss: 0.00000801
Iteration 146/1000 | Loss: 0.00000800
Iteration 147/1000 | Loss: 0.00000800
Iteration 148/1000 | Loss: 0.00000799
Iteration 149/1000 | Loss: 0.00000799
Iteration 150/1000 | Loss: 0.00000798
Iteration 151/1000 | Loss: 0.00000798
Iteration 152/1000 | Loss: 0.00000797
Iteration 153/1000 | Loss: 0.00000797
Iteration 154/1000 | Loss: 0.00000797
Iteration 155/1000 | Loss: 0.00000797
Iteration 156/1000 | Loss: 0.00000797
Iteration 157/1000 | Loss: 0.00000796
Iteration 158/1000 | Loss: 0.00000796
Iteration 159/1000 | Loss: 0.00000796
Iteration 160/1000 | Loss: 0.00000796
Iteration 161/1000 | Loss: 0.00000796
Iteration 162/1000 | Loss: 0.00000795
Iteration 163/1000 | Loss: 0.00000795
Iteration 164/1000 | Loss: 0.00000795
Iteration 165/1000 | Loss: 0.00000795
Iteration 166/1000 | Loss: 0.00000795
Iteration 167/1000 | Loss: 0.00000795
Iteration 168/1000 | Loss: 0.00000795
Iteration 169/1000 | Loss: 0.00000794
Iteration 170/1000 | Loss: 0.00000794
Iteration 171/1000 | Loss: 0.00000794
Iteration 172/1000 | Loss: 0.00000794
Iteration 173/1000 | Loss: 0.00000794
Iteration 174/1000 | Loss: 0.00000794
Iteration 175/1000 | Loss: 0.00000794
Iteration 176/1000 | Loss: 0.00000794
Iteration 177/1000 | Loss: 0.00000793
Iteration 178/1000 | Loss: 0.00000793
Iteration 179/1000 | Loss: 0.00000793
Iteration 180/1000 | Loss: 0.00000793
Iteration 181/1000 | Loss: 0.00000793
Iteration 182/1000 | Loss: 0.00000793
Iteration 183/1000 | Loss: 0.00000793
Iteration 184/1000 | Loss: 0.00000793
Iteration 185/1000 | Loss: 0.00000793
Iteration 186/1000 | Loss: 0.00000793
Iteration 187/1000 | Loss: 0.00000793
Iteration 188/1000 | Loss: 0.00000793
Iteration 189/1000 | Loss: 0.00000793
Iteration 190/1000 | Loss: 0.00000792
Iteration 191/1000 | Loss: 0.00000792
Iteration 192/1000 | Loss: 0.00000792
Iteration 193/1000 | Loss: 0.00000792
Iteration 194/1000 | Loss: 0.00000792
Iteration 195/1000 | Loss: 0.00000792
Iteration 196/1000 | Loss: 0.00000792
Iteration 197/1000 | Loss: 0.00000792
Iteration 198/1000 | Loss: 0.00000792
Iteration 199/1000 | Loss: 0.00000792
Iteration 200/1000 | Loss: 0.00000792
Iteration 201/1000 | Loss: 0.00000791
Iteration 202/1000 | Loss: 0.00000791
Iteration 203/1000 | Loss: 0.00000791
Iteration 204/1000 | Loss: 0.00000791
Iteration 205/1000 | Loss: 0.00000791
Iteration 206/1000 | Loss: 0.00000791
Iteration 207/1000 | Loss: 0.00000791
Iteration 208/1000 | Loss: 0.00000791
Iteration 209/1000 | Loss: 0.00000791
Iteration 210/1000 | Loss: 0.00000791
Iteration 211/1000 | Loss: 0.00000791
Iteration 212/1000 | Loss: 0.00000791
Iteration 213/1000 | Loss: 0.00000790
Iteration 214/1000 | Loss: 0.00000790
Iteration 215/1000 | Loss: 0.00000790
Iteration 216/1000 | Loss: 0.00000790
Iteration 217/1000 | Loss: 0.00000790
Iteration 218/1000 | Loss: 0.00000790
Iteration 219/1000 | Loss: 0.00000790
Iteration 220/1000 | Loss: 0.00000790
Iteration 221/1000 | Loss: 0.00000790
Iteration 222/1000 | Loss: 0.00000790
Iteration 223/1000 | Loss: 0.00000790
Iteration 224/1000 | Loss: 0.00000789
Iteration 225/1000 | Loss: 0.00000789
Iteration 226/1000 | Loss: 0.00000789
Iteration 227/1000 | Loss: 0.00000789
Iteration 228/1000 | Loss: 0.00000789
Iteration 229/1000 | Loss: 0.00000789
Iteration 230/1000 | Loss: 0.00000789
Iteration 231/1000 | Loss: 0.00000789
Iteration 232/1000 | Loss: 0.00000789
Iteration 233/1000 | Loss: 0.00000788
Iteration 234/1000 | Loss: 0.00000788
Iteration 235/1000 | Loss: 0.00000788
Iteration 236/1000 | Loss: 0.00000788
Iteration 237/1000 | Loss: 0.00000788
Iteration 238/1000 | Loss: 0.00000788
Iteration 239/1000 | Loss: 0.00000788
Iteration 240/1000 | Loss: 0.00000788
Iteration 241/1000 | Loss: 0.00000788
Iteration 242/1000 | Loss: 0.00000788
Iteration 243/1000 | Loss: 0.00000788
Iteration 244/1000 | Loss: 0.00000787
Iteration 245/1000 | Loss: 0.00000787
Iteration 246/1000 | Loss: 0.00000787
Iteration 247/1000 | Loss: 0.00000787
Iteration 248/1000 | Loss: 0.00000787
Iteration 249/1000 | Loss: 0.00000787
Iteration 250/1000 | Loss: 0.00000787
Iteration 251/1000 | Loss: 0.00000787
Iteration 252/1000 | Loss: 0.00000787
Iteration 253/1000 | Loss: 0.00000786
Iteration 254/1000 | Loss: 0.00000786
Iteration 255/1000 | Loss: 0.00000786
Iteration 256/1000 | Loss: 0.00000786
Iteration 257/1000 | Loss: 0.00000786
Iteration 258/1000 | Loss: 0.00000786
Iteration 259/1000 | Loss: 0.00000786
Iteration 260/1000 | Loss: 0.00000786
Iteration 261/1000 | Loss: 0.00000786
Iteration 262/1000 | Loss: 0.00000786
Iteration 263/1000 | Loss: 0.00000786
Iteration 264/1000 | Loss: 0.00000786
Iteration 265/1000 | Loss: 0.00000786
Iteration 266/1000 | Loss: 0.00000786
Iteration 267/1000 | Loss: 0.00000786
Iteration 268/1000 | Loss: 0.00000785
Iteration 269/1000 | Loss: 0.00000785
Iteration 270/1000 | Loss: 0.00000785
Iteration 271/1000 | Loss: 0.00000785
Iteration 272/1000 | Loss: 0.00000785
Iteration 273/1000 | Loss: 0.00000785
Iteration 274/1000 | Loss: 0.00000785
Iteration 275/1000 | Loss: 0.00000785
Iteration 276/1000 | Loss: 0.00000785
Iteration 277/1000 | Loss: 0.00000785
Iteration 278/1000 | Loss: 0.00000785
Iteration 279/1000 | Loss: 0.00000785
Iteration 280/1000 | Loss: 0.00000785
Iteration 281/1000 | Loss: 0.00000785
Iteration 282/1000 | Loss: 0.00000785
Iteration 283/1000 | Loss: 0.00000785
Iteration 284/1000 | Loss: 0.00000785
Iteration 285/1000 | Loss: 0.00000785
Iteration 286/1000 | Loss: 0.00000785
Iteration 287/1000 | Loss: 0.00000785
Iteration 288/1000 | Loss: 0.00000785
Iteration 289/1000 | Loss: 0.00000785
Iteration 290/1000 | Loss: 0.00000785
Iteration 291/1000 | Loss: 0.00000785
Iteration 292/1000 | Loss: 0.00000784
Iteration 293/1000 | Loss: 0.00000784
Iteration 294/1000 | Loss: 0.00000784
Iteration 295/1000 | Loss: 0.00000784
Iteration 296/1000 | Loss: 0.00000784
Iteration 297/1000 | Loss: 0.00000784
Iteration 298/1000 | Loss: 0.00000784
Iteration 299/1000 | Loss: 0.00000784
Iteration 300/1000 | Loss: 0.00000784
Iteration 301/1000 | Loss: 0.00000784
Iteration 302/1000 | Loss: 0.00000784
Iteration 303/1000 | Loss: 0.00000784
Iteration 304/1000 | Loss: 0.00000784
Iteration 305/1000 | Loss: 0.00000784
Iteration 306/1000 | Loss: 0.00000784
Iteration 307/1000 | Loss: 0.00000784
Iteration 308/1000 | Loss: 0.00000784
Iteration 309/1000 | Loss: 0.00000784
Iteration 310/1000 | Loss: 0.00000784
Iteration 311/1000 | Loss: 0.00000784
Iteration 312/1000 | Loss: 0.00000784
Iteration 313/1000 | Loss: 0.00000784
Iteration 314/1000 | Loss: 0.00000784
Iteration 315/1000 | Loss: 0.00000784
Iteration 316/1000 | Loss: 0.00000784
Iteration 317/1000 | Loss: 0.00000784
Iteration 318/1000 | Loss: 0.00000784
Iteration 319/1000 | Loss: 0.00000784
Iteration 320/1000 | Loss: 0.00000784
Iteration 321/1000 | Loss: 0.00000784
Iteration 322/1000 | Loss: 0.00000784
Iteration 323/1000 | Loss: 0.00000784
Iteration 324/1000 | Loss: 0.00000784
Iteration 325/1000 | Loss: 0.00000784
Iteration 326/1000 | Loss: 0.00000784
Iteration 327/1000 | Loss: 0.00000784
Iteration 328/1000 | Loss: 0.00000784
Iteration 329/1000 | Loss: 0.00000784
Iteration 330/1000 | Loss: 0.00000784
Iteration 331/1000 | Loss: 0.00000784
Iteration 332/1000 | Loss: 0.00000784
Iteration 333/1000 | Loss: 0.00000784
Iteration 334/1000 | Loss: 0.00000784
Iteration 335/1000 | Loss: 0.00000784
Iteration 336/1000 | Loss: 0.00000784
Iteration 337/1000 | Loss: 0.00000784
Iteration 338/1000 | Loss: 0.00000784
Iteration 339/1000 | Loss: 0.00000784
Iteration 340/1000 | Loss: 0.00000784
Iteration 341/1000 | Loss: 0.00000784
Iteration 342/1000 | Loss: 0.00000784
Iteration 343/1000 | Loss: 0.00000784
Iteration 344/1000 | Loss: 0.00000784
Iteration 345/1000 | Loss: 0.00000784
Iteration 346/1000 | Loss: 0.00000784
Iteration 347/1000 | Loss: 0.00000784
Iteration 348/1000 | Loss: 0.00000784
Iteration 349/1000 | Loss: 0.00000784
Iteration 350/1000 | Loss: 0.00000784
Iteration 351/1000 | Loss: 0.00000784
Iteration 352/1000 | Loss: 0.00000784
Iteration 353/1000 | Loss: 0.00000784
Iteration 354/1000 | Loss: 0.00000784
Iteration 355/1000 | Loss: 0.00000784
Iteration 356/1000 | Loss: 0.00000784
Iteration 357/1000 | Loss: 0.00000784
Iteration 358/1000 | Loss: 0.00000784
Iteration 359/1000 | Loss: 0.00000784
Iteration 360/1000 | Loss: 0.00000784
Iteration 361/1000 | Loss: 0.00000784
Iteration 362/1000 | Loss: 0.00000784
Iteration 363/1000 | Loss: 0.00000784
Iteration 364/1000 | Loss: 0.00000784
Iteration 365/1000 | Loss: 0.00000784
Iteration 366/1000 | Loss: 0.00000784
Iteration 367/1000 | Loss: 0.00000784
Iteration 368/1000 | Loss: 0.00000784
Iteration 369/1000 | Loss: 0.00000784
Iteration 370/1000 | Loss: 0.00000784
Iteration 371/1000 | Loss: 0.00000784
Iteration 372/1000 | Loss: 0.00000784
Iteration 373/1000 | Loss: 0.00000784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [7.843344974389765e-06, 7.843344974389765e-06, 7.843344974389765e-06, 7.843344974389765e-06, 7.843344974389765e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.843344974389765e-06

Optimization complete. Final v2v error: 2.4152305126190186 mm

Highest mean error: 2.8148772716522217 mm for frame 90

Lowest mean error: 2.311984062194824 mm for frame 135

Saving results

Total time: 45.323142290115356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067324
Iteration 2/25 | Loss: 0.00123990
Iteration 3/25 | Loss: 0.00102823
Iteration 4/25 | Loss: 0.00099793
Iteration 5/25 | Loss: 0.00099345
Iteration 6/25 | Loss: 0.00099306
Iteration 7/25 | Loss: 0.00099306
Iteration 8/25 | Loss: 0.00099306
Iteration 9/25 | Loss: 0.00099306
Iteration 10/25 | Loss: 0.00099306
Iteration 11/25 | Loss: 0.00099306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009930606465786695, 0.0009930606465786695, 0.0009930606465786695, 0.0009930606465786695, 0.0009930606465786695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009930606465786695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13999891
Iteration 2/25 | Loss: 0.00063575
Iteration 3/25 | Loss: 0.00063575
Iteration 4/25 | Loss: 0.00063575
Iteration 5/25 | Loss: 0.00063575
Iteration 6/25 | Loss: 0.00063575
Iteration 7/25 | Loss: 0.00063575
Iteration 8/25 | Loss: 0.00063575
Iteration 9/25 | Loss: 0.00063575
Iteration 10/25 | Loss: 0.00063575
Iteration 11/25 | Loss: 0.00063575
Iteration 12/25 | Loss: 0.00063575
Iteration 13/25 | Loss: 0.00063575
Iteration 14/25 | Loss: 0.00063575
Iteration 15/25 | Loss: 0.00063575
Iteration 16/25 | Loss: 0.00063575
Iteration 17/25 | Loss: 0.00063575
Iteration 18/25 | Loss: 0.00063575
Iteration 19/25 | Loss: 0.00063575
Iteration 20/25 | Loss: 0.00063575
Iteration 21/25 | Loss: 0.00063575
Iteration 22/25 | Loss: 0.00063575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006357457023113966, 0.0006357457023113966, 0.0006357457023113966, 0.0006357457023113966, 0.0006357457023113966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006357457023113966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063575
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001293
Iteration 5/1000 | Loss: 0.00001182
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001055
Iteration 8/1000 | Loss: 0.00001020
Iteration 9/1000 | Loss: 0.00000999
Iteration 10/1000 | Loss: 0.00000975
Iteration 11/1000 | Loss: 0.00000971
Iteration 12/1000 | Loss: 0.00000971
Iteration 13/1000 | Loss: 0.00000963
Iteration 14/1000 | Loss: 0.00000959
Iteration 15/1000 | Loss: 0.00000950
Iteration 16/1000 | Loss: 0.00000949
Iteration 17/1000 | Loss: 0.00000941
Iteration 18/1000 | Loss: 0.00000939
Iteration 19/1000 | Loss: 0.00000939
Iteration 20/1000 | Loss: 0.00000938
Iteration 21/1000 | Loss: 0.00000938
Iteration 22/1000 | Loss: 0.00000937
Iteration 23/1000 | Loss: 0.00000936
Iteration 24/1000 | Loss: 0.00000936
Iteration 25/1000 | Loss: 0.00000935
Iteration 26/1000 | Loss: 0.00000935
Iteration 27/1000 | Loss: 0.00000935
Iteration 28/1000 | Loss: 0.00000934
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000934
Iteration 31/1000 | Loss: 0.00000934
Iteration 32/1000 | Loss: 0.00000933
Iteration 33/1000 | Loss: 0.00000933
Iteration 34/1000 | Loss: 0.00000933
Iteration 35/1000 | Loss: 0.00000933
Iteration 36/1000 | Loss: 0.00000932
Iteration 37/1000 | Loss: 0.00000932
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000932
Iteration 41/1000 | Loss: 0.00000932
Iteration 42/1000 | Loss: 0.00000932
Iteration 43/1000 | Loss: 0.00000932
Iteration 44/1000 | Loss: 0.00000931
Iteration 45/1000 | Loss: 0.00000931
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000931
Iteration 48/1000 | Loss: 0.00000931
Iteration 49/1000 | Loss: 0.00000931
Iteration 50/1000 | Loss: 0.00000931
Iteration 51/1000 | Loss: 0.00000931
Iteration 52/1000 | Loss: 0.00000931
Iteration 53/1000 | Loss: 0.00000930
Iteration 54/1000 | Loss: 0.00000930
Iteration 55/1000 | Loss: 0.00000930
Iteration 56/1000 | Loss: 0.00000930
Iteration 57/1000 | Loss: 0.00000930
Iteration 58/1000 | Loss: 0.00000930
Iteration 59/1000 | Loss: 0.00000930
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000930
Iteration 62/1000 | Loss: 0.00000929
Iteration 63/1000 | Loss: 0.00000929
Iteration 64/1000 | Loss: 0.00000929
Iteration 65/1000 | Loss: 0.00000929
Iteration 66/1000 | Loss: 0.00000929
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000928
Iteration 70/1000 | Loss: 0.00000928
Iteration 71/1000 | Loss: 0.00000928
Iteration 72/1000 | Loss: 0.00000928
Iteration 73/1000 | Loss: 0.00000928
Iteration 74/1000 | Loss: 0.00000928
Iteration 75/1000 | Loss: 0.00000928
Iteration 76/1000 | Loss: 0.00000928
Iteration 77/1000 | Loss: 0.00000928
Iteration 78/1000 | Loss: 0.00000928
Iteration 79/1000 | Loss: 0.00000927
Iteration 80/1000 | Loss: 0.00000927
Iteration 81/1000 | Loss: 0.00000927
Iteration 82/1000 | Loss: 0.00000927
Iteration 83/1000 | Loss: 0.00000927
Iteration 84/1000 | Loss: 0.00000927
Iteration 85/1000 | Loss: 0.00000927
Iteration 86/1000 | Loss: 0.00000927
Iteration 87/1000 | Loss: 0.00000927
Iteration 88/1000 | Loss: 0.00000927
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000926
Iteration 92/1000 | Loss: 0.00000926
Iteration 93/1000 | Loss: 0.00000926
Iteration 94/1000 | Loss: 0.00000926
Iteration 95/1000 | Loss: 0.00000926
Iteration 96/1000 | Loss: 0.00000926
Iteration 97/1000 | Loss: 0.00000926
Iteration 98/1000 | Loss: 0.00000926
Iteration 99/1000 | Loss: 0.00000926
Iteration 100/1000 | Loss: 0.00000926
Iteration 101/1000 | Loss: 0.00000926
Iteration 102/1000 | Loss: 0.00000926
Iteration 103/1000 | Loss: 0.00000925
Iteration 104/1000 | Loss: 0.00000925
Iteration 105/1000 | Loss: 0.00000925
Iteration 106/1000 | Loss: 0.00000925
Iteration 107/1000 | Loss: 0.00000925
Iteration 108/1000 | Loss: 0.00000925
Iteration 109/1000 | Loss: 0.00000925
Iteration 110/1000 | Loss: 0.00000925
Iteration 111/1000 | Loss: 0.00000925
Iteration 112/1000 | Loss: 0.00000925
Iteration 113/1000 | Loss: 0.00000925
Iteration 114/1000 | Loss: 0.00000925
Iteration 115/1000 | Loss: 0.00000925
Iteration 116/1000 | Loss: 0.00000925
Iteration 117/1000 | Loss: 0.00000925
Iteration 118/1000 | Loss: 0.00000925
Iteration 119/1000 | Loss: 0.00000925
Iteration 120/1000 | Loss: 0.00000925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [9.248020433005877e-06, 9.248020433005877e-06, 9.248020433005877e-06, 9.248020433005877e-06, 9.248020433005877e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.248020433005877e-06

Optimization complete. Final v2v error: 2.5708162784576416 mm

Highest mean error: 2.8814547061920166 mm for frame 205

Lowest mean error: 2.340855836868286 mm for frame 255

Saving results

Total time: 38.14052653312683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00278808
Iteration 2/25 | Loss: 0.00128868
Iteration 3/25 | Loss: 0.00104797
Iteration 4/25 | Loss: 0.00099847
Iteration 5/25 | Loss: 0.00098527
Iteration 6/25 | Loss: 0.00097997
Iteration 7/25 | Loss: 0.00097782
Iteration 8/25 | Loss: 0.00097703
Iteration 9/25 | Loss: 0.00097675
Iteration 10/25 | Loss: 0.00097662
Iteration 11/25 | Loss: 0.00097662
Iteration 12/25 | Loss: 0.00097662
Iteration 13/25 | Loss: 0.00097662
Iteration 14/25 | Loss: 0.00097662
Iteration 15/25 | Loss: 0.00097662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000976617680862546, 0.000976617680862546, 0.000976617680862546, 0.000976617680862546, 0.000976617680862546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000976617680862546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37228239
Iteration 2/25 | Loss: 0.00147744
Iteration 3/25 | Loss: 0.00147744
Iteration 4/25 | Loss: 0.00147744
Iteration 5/25 | Loss: 0.00147743
Iteration 6/25 | Loss: 0.00147743
Iteration 7/25 | Loss: 0.00147743
Iteration 8/25 | Loss: 0.00147743
Iteration 9/25 | Loss: 0.00147743
Iteration 10/25 | Loss: 0.00147743
Iteration 11/25 | Loss: 0.00147743
Iteration 12/25 | Loss: 0.00147743
Iteration 13/25 | Loss: 0.00147743
Iteration 14/25 | Loss: 0.00147743
Iteration 15/25 | Loss: 0.00147743
Iteration 16/25 | Loss: 0.00147743
Iteration 17/25 | Loss: 0.00147743
Iteration 18/25 | Loss: 0.00147743
Iteration 19/25 | Loss: 0.00147743
Iteration 20/25 | Loss: 0.00147743
Iteration 21/25 | Loss: 0.00147743
Iteration 22/25 | Loss: 0.00147743
Iteration 23/25 | Loss: 0.00147743
Iteration 24/25 | Loss: 0.00147743
Iteration 25/25 | Loss: 0.00147743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147743
Iteration 2/1000 | Loss: 0.00023062
Iteration 3/1000 | Loss: 0.00016113
Iteration 4/1000 | Loss: 0.00008674
Iteration 5/1000 | Loss: 0.00011250
Iteration 6/1000 | Loss: 0.00006245
Iteration 7/1000 | Loss: 0.00007927
Iteration 8/1000 | Loss: 0.00008891
Iteration 9/1000 | Loss: 0.00007211
Iteration 10/1000 | Loss: 0.00008517
Iteration 11/1000 | Loss: 0.00007857
Iteration 12/1000 | Loss: 0.00011639
Iteration 13/1000 | Loss: 0.00006982
Iteration 14/1000 | Loss: 0.00009614
Iteration 15/1000 | Loss: 0.00008525
Iteration 16/1000 | Loss: 0.00010019
Iteration 17/1000 | Loss: 0.00008493
Iteration 18/1000 | Loss: 0.00009465
Iteration 19/1000 | Loss: 0.00009681
Iteration 20/1000 | Loss: 0.00008798
Iteration 21/1000 | Loss: 0.00012152
Iteration 22/1000 | Loss: 0.00007511
Iteration 23/1000 | Loss: 0.00013020
Iteration 24/1000 | Loss: 0.00009615
Iteration 25/1000 | Loss: 0.00013568
Iteration 26/1000 | Loss: 0.00009515
Iteration 27/1000 | Loss: 0.00010377
Iteration 28/1000 | Loss: 0.00008149
Iteration 29/1000 | Loss: 0.00009066
Iteration 30/1000 | Loss: 0.00010283
Iteration 31/1000 | Loss: 0.00005682
Iteration 32/1000 | Loss: 0.00004338
Iteration 33/1000 | Loss: 0.00004155
Iteration 34/1000 | Loss: 0.00003723
Iteration 35/1000 | Loss: 0.00003141
Iteration 36/1000 | Loss: 0.00002621
Iteration 37/1000 | Loss: 0.00002248
Iteration 38/1000 | Loss: 0.00003195
Iteration 39/1000 | Loss: 0.00003375
Iteration 40/1000 | Loss: 0.00003356
Iteration 41/1000 | Loss: 0.00002094
Iteration 42/1000 | Loss: 0.00002768
Iteration 43/1000 | Loss: 0.00003119
Iteration 44/1000 | Loss: 0.00004995
Iteration 45/1000 | Loss: 0.00003679
Iteration 46/1000 | Loss: 0.00004994
Iteration 47/1000 | Loss: 0.00003417
Iteration 48/1000 | Loss: 0.00002275
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00003274
Iteration 51/1000 | Loss: 0.00002660
Iteration 52/1000 | Loss: 0.00002851
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00003178
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001925
Iteration 57/1000 | Loss: 0.00003133
Iteration 58/1000 | Loss: 0.00001441
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001068
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001051
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001044
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001044
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001042
Iteration 76/1000 | Loss: 0.00001042
Iteration 77/1000 | Loss: 0.00001042
Iteration 78/1000 | Loss: 0.00001041
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001039
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001036
Iteration 84/1000 | Loss: 0.00001036
Iteration 85/1000 | Loss: 0.00001035
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001034
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001033
Iteration 90/1000 | Loss: 0.00001032
Iteration 91/1000 | Loss: 0.00001031
Iteration 92/1000 | Loss: 0.00001031
Iteration 93/1000 | Loss: 0.00001030
Iteration 94/1000 | Loss: 0.00001030
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001026
Iteration 99/1000 | Loss: 0.00001025
Iteration 100/1000 | Loss: 0.00001025
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001024
Iteration 103/1000 | Loss: 0.00001024
Iteration 104/1000 | Loss: 0.00001023
Iteration 105/1000 | Loss: 0.00001023
Iteration 106/1000 | Loss: 0.00001023
Iteration 107/1000 | Loss: 0.00001022
Iteration 108/1000 | Loss: 0.00001022
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001021
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001020
Iteration 126/1000 | Loss: 0.00001020
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001018
Iteration 131/1000 | Loss: 0.00001018
Iteration 132/1000 | Loss: 0.00001018
Iteration 133/1000 | Loss: 0.00001018
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001017
Iteration 138/1000 | Loss: 0.00001017
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001016
Iteration 141/1000 | Loss: 0.00001016
Iteration 142/1000 | Loss: 0.00001016
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001015
Iteration 150/1000 | Loss: 0.00001015
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001013
Iteration 160/1000 | Loss: 0.00001013
Iteration 161/1000 | Loss: 0.00001013
Iteration 162/1000 | Loss: 0.00001013
Iteration 163/1000 | Loss: 0.00001013
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001012
Iteration 167/1000 | Loss: 0.00001012
Iteration 168/1000 | Loss: 0.00001012
Iteration 169/1000 | Loss: 0.00001012
Iteration 170/1000 | Loss: 0.00001012
Iteration 171/1000 | Loss: 0.00001012
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001011
Iteration 177/1000 | Loss: 0.00001011
Iteration 178/1000 | Loss: 0.00001011
Iteration 179/1000 | Loss: 0.00001011
Iteration 180/1000 | Loss: 0.00001011
Iteration 181/1000 | Loss: 0.00001011
Iteration 182/1000 | Loss: 0.00001011
Iteration 183/1000 | Loss: 0.00001011
Iteration 184/1000 | Loss: 0.00001011
Iteration 185/1000 | Loss: 0.00001011
Iteration 186/1000 | Loss: 0.00001011
Iteration 187/1000 | Loss: 0.00001011
Iteration 188/1000 | Loss: 0.00001010
Iteration 189/1000 | Loss: 0.00001010
Iteration 190/1000 | Loss: 0.00001010
Iteration 191/1000 | Loss: 0.00001010
Iteration 192/1000 | Loss: 0.00001010
Iteration 193/1000 | Loss: 0.00001010
Iteration 194/1000 | Loss: 0.00001010
Iteration 195/1000 | Loss: 0.00001009
Iteration 196/1000 | Loss: 0.00001009
Iteration 197/1000 | Loss: 0.00001009
Iteration 198/1000 | Loss: 0.00001009
Iteration 199/1000 | Loss: 0.00001009
Iteration 200/1000 | Loss: 0.00001009
Iteration 201/1000 | Loss: 0.00001009
Iteration 202/1000 | Loss: 0.00001009
Iteration 203/1000 | Loss: 0.00001008
Iteration 204/1000 | Loss: 0.00001008
Iteration 205/1000 | Loss: 0.00001008
Iteration 206/1000 | Loss: 0.00001008
Iteration 207/1000 | Loss: 0.00001008
Iteration 208/1000 | Loss: 0.00001008
Iteration 209/1000 | Loss: 0.00001008
Iteration 210/1000 | Loss: 0.00001007
Iteration 211/1000 | Loss: 0.00001007
Iteration 212/1000 | Loss: 0.00001007
Iteration 213/1000 | Loss: 0.00001007
Iteration 214/1000 | Loss: 0.00001007
Iteration 215/1000 | Loss: 0.00001007
Iteration 216/1000 | Loss: 0.00001006
Iteration 217/1000 | Loss: 0.00001006
Iteration 218/1000 | Loss: 0.00001006
Iteration 219/1000 | Loss: 0.00001006
Iteration 220/1000 | Loss: 0.00001006
Iteration 221/1000 | Loss: 0.00001006
Iteration 222/1000 | Loss: 0.00001006
Iteration 223/1000 | Loss: 0.00001005
Iteration 224/1000 | Loss: 0.00001005
Iteration 225/1000 | Loss: 0.00001005
Iteration 226/1000 | Loss: 0.00001004
Iteration 227/1000 | Loss: 0.00001004
Iteration 228/1000 | Loss: 0.00001004
Iteration 229/1000 | Loss: 0.00001004
Iteration 230/1000 | Loss: 0.00001003
Iteration 231/1000 | Loss: 0.00001003
Iteration 232/1000 | Loss: 0.00001003
Iteration 233/1000 | Loss: 0.00001003
Iteration 234/1000 | Loss: 0.00001003
Iteration 235/1000 | Loss: 0.00001003
Iteration 236/1000 | Loss: 0.00001003
Iteration 237/1000 | Loss: 0.00001003
Iteration 238/1000 | Loss: 0.00001003
Iteration 239/1000 | Loss: 0.00001003
Iteration 240/1000 | Loss: 0.00001003
Iteration 241/1000 | Loss: 0.00001003
Iteration 242/1000 | Loss: 0.00001003
Iteration 243/1000 | Loss: 0.00001003
Iteration 244/1000 | Loss: 0.00001003
Iteration 245/1000 | Loss: 0.00001003
Iteration 246/1000 | Loss: 0.00001003
Iteration 247/1000 | Loss: 0.00001003
Iteration 248/1000 | Loss: 0.00001003
Iteration 249/1000 | Loss: 0.00001003
Iteration 250/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.0029931218014099e-05, 1.0029931218014099e-05, 1.0029931218014099e-05, 1.0029931218014099e-05, 1.0029931218014099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0029931218014099e-05

Optimization complete. Final v2v error: 2.7081193923950195 mm

Highest mean error: 3.5068752765655518 mm for frame 49

Lowest mean error: 2.438565492630005 mm for frame 187

Saving results

Total time: 124.70948433876038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695126
Iteration 2/25 | Loss: 0.00152061
Iteration 3/25 | Loss: 0.00121688
Iteration 4/25 | Loss: 0.00120046
Iteration 5/25 | Loss: 0.00119772
Iteration 6/25 | Loss: 0.00119772
Iteration 7/25 | Loss: 0.00119772
Iteration 8/25 | Loss: 0.00119772
Iteration 9/25 | Loss: 0.00119772
Iteration 10/25 | Loss: 0.00119772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011977247195318341, 0.0011977247195318341, 0.0011977247195318341, 0.0011977247195318341, 0.0011977247195318341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011977247195318341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89732862
Iteration 2/25 | Loss: 0.00074301
Iteration 3/25 | Loss: 0.00074299
Iteration 4/25 | Loss: 0.00074298
Iteration 5/25 | Loss: 0.00074298
Iteration 6/25 | Loss: 0.00074298
Iteration 7/25 | Loss: 0.00074298
Iteration 8/25 | Loss: 0.00074298
Iteration 9/25 | Loss: 0.00074298
Iteration 10/25 | Loss: 0.00074298
Iteration 11/25 | Loss: 0.00074298
Iteration 12/25 | Loss: 0.00074298
Iteration 13/25 | Loss: 0.00074298
Iteration 14/25 | Loss: 0.00074298
Iteration 15/25 | Loss: 0.00074298
Iteration 16/25 | Loss: 0.00074298
Iteration 17/25 | Loss: 0.00074298
Iteration 18/25 | Loss: 0.00074298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007429826655425131, 0.0007429826655425131, 0.0007429826655425131, 0.0007429826655425131, 0.0007429826655425131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007429826655425131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074298
Iteration 2/1000 | Loss: 0.00005385
Iteration 3/1000 | Loss: 0.00002975
Iteration 4/1000 | Loss: 0.00002549
Iteration 5/1000 | Loss: 0.00002372
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002184
Iteration 8/1000 | Loss: 0.00002150
Iteration 9/1000 | Loss: 0.00002119
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002073
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00002056
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002048
Iteration 20/1000 | Loss: 0.00002047
Iteration 21/1000 | Loss: 0.00002045
Iteration 22/1000 | Loss: 0.00002038
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002036
Iteration 25/1000 | Loss: 0.00002035
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002034
Iteration 28/1000 | Loss: 0.00002033
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00002033
Iteration 31/1000 | Loss: 0.00002033
Iteration 32/1000 | Loss: 0.00002033
Iteration 33/1000 | Loss: 0.00002033
Iteration 34/1000 | Loss: 0.00002032
Iteration 35/1000 | Loss: 0.00002032
Iteration 36/1000 | Loss: 0.00002032
Iteration 37/1000 | Loss: 0.00002032
Iteration 38/1000 | Loss: 0.00002032
Iteration 39/1000 | Loss: 0.00002032
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00002031
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002031
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002030
Iteration 49/1000 | Loss: 0.00002030
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002030
Iteration 55/1000 | Loss: 0.00002030
Iteration 56/1000 | Loss: 0.00002030
Iteration 57/1000 | Loss: 0.00002030
Iteration 58/1000 | Loss: 0.00002029
Iteration 59/1000 | Loss: 0.00002029
Iteration 60/1000 | Loss: 0.00002029
Iteration 61/1000 | Loss: 0.00002029
Iteration 62/1000 | Loss: 0.00002029
Iteration 63/1000 | Loss: 0.00002029
Iteration 64/1000 | Loss: 0.00002029
Iteration 65/1000 | Loss: 0.00002028
Iteration 66/1000 | Loss: 0.00002028
Iteration 67/1000 | Loss: 0.00002028
Iteration 68/1000 | Loss: 0.00002028
Iteration 69/1000 | Loss: 0.00002028
Iteration 70/1000 | Loss: 0.00002028
Iteration 71/1000 | Loss: 0.00002028
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002027
Iteration 81/1000 | Loss: 0.00002027
Iteration 82/1000 | Loss: 0.00002027
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002024
Iteration 94/1000 | Loss: 0.00002024
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002024
Iteration 101/1000 | Loss: 0.00002024
Iteration 102/1000 | Loss: 0.00002024
Iteration 103/1000 | Loss: 0.00002024
Iteration 104/1000 | Loss: 0.00002024
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002023
Iteration 108/1000 | Loss: 0.00002023
Iteration 109/1000 | Loss: 0.00002023
Iteration 110/1000 | Loss: 0.00002023
Iteration 111/1000 | Loss: 0.00002023
Iteration 112/1000 | Loss: 0.00002023
Iteration 113/1000 | Loss: 0.00002023
Iteration 114/1000 | Loss: 0.00002023
Iteration 115/1000 | Loss: 0.00002023
Iteration 116/1000 | Loss: 0.00002023
Iteration 117/1000 | Loss: 0.00002023
Iteration 118/1000 | Loss: 0.00002023
Iteration 119/1000 | Loss: 0.00002023
Iteration 120/1000 | Loss: 0.00002023
Iteration 121/1000 | Loss: 0.00002023
Iteration 122/1000 | Loss: 0.00002023
Iteration 123/1000 | Loss: 0.00002023
Iteration 124/1000 | Loss: 0.00002023
Iteration 125/1000 | Loss: 0.00002023
Iteration 126/1000 | Loss: 0.00002023
Iteration 127/1000 | Loss: 0.00002023
Iteration 128/1000 | Loss: 0.00002023
Iteration 129/1000 | Loss: 0.00002023
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002023
Iteration 132/1000 | Loss: 0.00002023
Iteration 133/1000 | Loss: 0.00002023
Iteration 134/1000 | Loss: 0.00002023
Iteration 135/1000 | Loss: 0.00002023
Iteration 136/1000 | Loss: 0.00002023
Iteration 137/1000 | Loss: 0.00002023
Iteration 138/1000 | Loss: 0.00002023
Iteration 139/1000 | Loss: 0.00002023
Iteration 140/1000 | Loss: 0.00002023
Iteration 141/1000 | Loss: 0.00002023
Iteration 142/1000 | Loss: 0.00002023
Iteration 143/1000 | Loss: 0.00002023
Iteration 144/1000 | Loss: 0.00002023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.0234514522599056e-05, 2.0234514522599056e-05, 2.0234514522599056e-05, 2.0234514522599056e-05, 2.0234514522599056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0234514522599056e-05

Optimization complete. Final v2v error: 3.802922487258911 mm

Highest mean error: 4.118492126464844 mm for frame 216

Lowest mean error: 3.5152034759521484 mm for frame 132

Saving results

Total time: 38.94004726409912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780859
Iteration 2/25 | Loss: 0.00164667
Iteration 3/25 | Loss: 0.00123618
Iteration 4/25 | Loss: 0.00114751
Iteration 5/25 | Loss: 0.00111950
Iteration 6/25 | Loss: 0.00111068
Iteration 7/25 | Loss: 0.00110915
Iteration 8/25 | Loss: 0.00110889
Iteration 9/25 | Loss: 0.00110881
Iteration 10/25 | Loss: 0.00110881
Iteration 11/25 | Loss: 0.00110881
Iteration 12/25 | Loss: 0.00110881
Iteration 13/25 | Loss: 0.00110880
Iteration 14/25 | Loss: 0.00110880
Iteration 15/25 | Loss: 0.00110880
Iteration 16/25 | Loss: 0.00110880
Iteration 17/25 | Loss: 0.00110880
Iteration 18/25 | Loss: 0.00110880
Iteration 19/25 | Loss: 0.00110880
Iteration 20/25 | Loss: 0.00110880
Iteration 21/25 | Loss: 0.00110880
Iteration 22/25 | Loss: 0.00110880
Iteration 23/25 | Loss: 0.00110880
Iteration 24/25 | Loss: 0.00110880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011088023893535137, 0.0011088023893535137, 0.0011088023893535137, 0.0011088023893535137, 0.0011088023893535137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011088023893535137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34225595
Iteration 2/25 | Loss: 0.00054583
Iteration 3/25 | Loss: 0.00054583
Iteration 4/25 | Loss: 0.00054583
Iteration 5/25 | Loss: 0.00054583
Iteration 6/25 | Loss: 0.00054583
Iteration 7/25 | Loss: 0.00054583
Iteration 8/25 | Loss: 0.00054583
Iteration 9/25 | Loss: 0.00054583
Iteration 10/25 | Loss: 0.00054583
Iteration 11/25 | Loss: 0.00054583
Iteration 12/25 | Loss: 0.00054583
Iteration 13/25 | Loss: 0.00054583
Iteration 14/25 | Loss: 0.00054583
Iteration 15/25 | Loss: 0.00054583
Iteration 16/25 | Loss: 0.00054583
Iteration 17/25 | Loss: 0.00054583
Iteration 18/25 | Loss: 0.00054583
Iteration 19/25 | Loss: 0.00054583
Iteration 20/25 | Loss: 0.00054583
Iteration 21/25 | Loss: 0.00054583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005458290688693523, 0.0005458290688693523, 0.0005458290688693523, 0.0005458290688693523, 0.0005458290688693523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005458290688693523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054583
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00001811
Iteration 4/1000 | Loss: 0.00001641
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001457
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001446
Iteration 14/1000 | Loss: 0.00001446
Iteration 15/1000 | Loss: 0.00001445
Iteration 16/1000 | Loss: 0.00001444
Iteration 17/1000 | Loss: 0.00001442
Iteration 18/1000 | Loss: 0.00001442
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001441
Iteration 26/1000 | Loss: 0.00001440
Iteration 27/1000 | Loss: 0.00001439
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001436
Iteration 41/1000 | Loss: 0.00001436
Iteration 42/1000 | Loss: 0.00001436
Iteration 43/1000 | Loss: 0.00001436
Iteration 44/1000 | Loss: 0.00001436
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001436
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001436
Iteration 55/1000 | Loss: 0.00001436
Iteration 56/1000 | Loss: 0.00001436
Iteration 57/1000 | Loss: 0.00001436
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.4358553926285822e-05, 1.4358553926285822e-05, 1.4358553926285822e-05, 1.4358553926285822e-05, 1.4358553926285822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4358553926285822e-05

Optimization complete. Final v2v error: 3.1884195804595947 mm

Highest mean error: 3.769343614578247 mm for frame 182

Lowest mean error: 2.983983278274536 mm for frame 128

Saving results

Total time: 36.28371858596802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660663
Iteration 2/25 | Loss: 0.00130198
Iteration 3/25 | Loss: 0.00112502
Iteration 4/25 | Loss: 0.00111175
Iteration 5/25 | Loss: 0.00110934
Iteration 6/25 | Loss: 0.00110896
Iteration 7/25 | Loss: 0.00110896
Iteration 8/25 | Loss: 0.00110896
Iteration 9/25 | Loss: 0.00110896
Iteration 10/25 | Loss: 0.00110896
Iteration 11/25 | Loss: 0.00110896
Iteration 12/25 | Loss: 0.00110896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011089558247476816, 0.0011089558247476816, 0.0011089558247476816, 0.0011089558247476816, 0.0011089558247476816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011089558247476816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.97616291
Iteration 2/25 | Loss: 0.00062772
Iteration 3/25 | Loss: 0.00062752
Iteration 4/25 | Loss: 0.00062752
Iteration 5/25 | Loss: 0.00062752
Iteration 6/25 | Loss: 0.00062752
Iteration 7/25 | Loss: 0.00062752
Iteration 8/25 | Loss: 0.00062752
Iteration 9/25 | Loss: 0.00062752
Iteration 10/25 | Loss: 0.00062752
Iteration 11/25 | Loss: 0.00062752
Iteration 12/25 | Loss: 0.00062752
Iteration 13/25 | Loss: 0.00062752
Iteration 14/25 | Loss: 0.00062752
Iteration 15/25 | Loss: 0.00062752
Iteration 16/25 | Loss: 0.00062752
Iteration 17/25 | Loss: 0.00062752
Iteration 18/25 | Loss: 0.00062752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006275178748182952, 0.0006275178748182952, 0.0006275178748182952, 0.0006275178748182952, 0.0006275178748182952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006275178748182952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062752
Iteration 2/1000 | Loss: 0.00003958
Iteration 3/1000 | Loss: 0.00002637
Iteration 4/1000 | Loss: 0.00002220
Iteration 5/1000 | Loss: 0.00002040
Iteration 6/1000 | Loss: 0.00001946
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001821
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001784
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001747
Iteration 17/1000 | Loss: 0.00001746
Iteration 18/1000 | Loss: 0.00001743
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001743
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00001740
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001735
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001734
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001730
Iteration 53/1000 | Loss: 0.00001730
Iteration 54/1000 | Loss: 0.00001730
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001730
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001729
Iteration 60/1000 | Loss: 0.00001729
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001724
Iteration 67/1000 | Loss: 0.00001724
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001721
Iteration 74/1000 | Loss: 0.00001721
Iteration 75/1000 | Loss: 0.00001721
Iteration 76/1000 | Loss: 0.00001721
Iteration 77/1000 | Loss: 0.00001720
Iteration 78/1000 | Loss: 0.00001720
Iteration 79/1000 | Loss: 0.00001720
Iteration 80/1000 | Loss: 0.00001720
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001719
Iteration 84/1000 | Loss: 0.00001719
Iteration 85/1000 | Loss: 0.00001718
Iteration 86/1000 | Loss: 0.00001718
Iteration 87/1000 | Loss: 0.00001718
Iteration 88/1000 | Loss: 0.00001718
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001716
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001715
Iteration 95/1000 | Loss: 0.00001715
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001714
Iteration 99/1000 | Loss: 0.00001714
Iteration 100/1000 | Loss: 0.00001714
Iteration 101/1000 | Loss: 0.00001714
Iteration 102/1000 | Loss: 0.00001714
Iteration 103/1000 | Loss: 0.00001714
Iteration 104/1000 | Loss: 0.00001714
Iteration 105/1000 | Loss: 0.00001714
Iteration 106/1000 | Loss: 0.00001714
Iteration 107/1000 | Loss: 0.00001714
Iteration 108/1000 | Loss: 0.00001713
Iteration 109/1000 | Loss: 0.00001713
Iteration 110/1000 | Loss: 0.00001713
Iteration 111/1000 | Loss: 0.00001713
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001711
Iteration 117/1000 | Loss: 0.00001711
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001709
Iteration 126/1000 | Loss: 0.00001709
Iteration 127/1000 | Loss: 0.00001709
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001708
Iteration 133/1000 | Loss: 0.00001708
Iteration 134/1000 | Loss: 0.00001708
Iteration 135/1000 | Loss: 0.00001708
Iteration 136/1000 | Loss: 0.00001708
Iteration 137/1000 | Loss: 0.00001708
Iteration 138/1000 | Loss: 0.00001708
Iteration 139/1000 | Loss: 0.00001707
Iteration 140/1000 | Loss: 0.00001707
Iteration 141/1000 | Loss: 0.00001707
Iteration 142/1000 | Loss: 0.00001707
Iteration 143/1000 | Loss: 0.00001707
Iteration 144/1000 | Loss: 0.00001707
Iteration 145/1000 | Loss: 0.00001706
Iteration 146/1000 | Loss: 0.00001706
Iteration 147/1000 | Loss: 0.00001706
Iteration 148/1000 | Loss: 0.00001706
Iteration 149/1000 | Loss: 0.00001706
Iteration 150/1000 | Loss: 0.00001706
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001706
Iteration 153/1000 | Loss: 0.00001706
Iteration 154/1000 | Loss: 0.00001705
Iteration 155/1000 | Loss: 0.00001705
Iteration 156/1000 | Loss: 0.00001705
Iteration 157/1000 | Loss: 0.00001705
Iteration 158/1000 | Loss: 0.00001705
Iteration 159/1000 | Loss: 0.00001705
Iteration 160/1000 | Loss: 0.00001705
Iteration 161/1000 | Loss: 0.00001704
Iteration 162/1000 | Loss: 0.00001704
Iteration 163/1000 | Loss: 0.00001704
Iteration 164/1000 | Loss: 0.00001704
Iteration 165/1000 | Loss: 0.00001704
Iteration 166/1000 | Loss: 0.00001704
Iteration 167/1000 | Loss: 0.00001704
Iteration 168/1000 | Loss: 0.00001704
Iteration 169/1000 | Loss: 0.00001704
Iteration 170/1000 | Loss: 0.00001704
Iteration 171/1000 | Loss: 0.00001704
Iteration 172/1000 | Loss: 0.00001704
Iteration 173/1000 | Loss: 0.00001703
Iteration 174/1000 | Loss: 0.00001703
Iteration 175/1000 | Loss: 0.00001703
Iteration 176/1000 | Loss: 0.00001703
Iteration 177/1000 | Loss: 0.00001703
Iteration 178/1000 | Loss: 0.00001703
Iteration 179/1000 | Loss: 0.00001703
Iteration 180/1000 | Loss: 0.00001703
Iteration 181/1000 | Loss: 0.00001703
Iteration 182/1000 | Loss: 0.00001702
Iteration 183/1000 | Loss: 0.00001702
Iteration 184/1000 | Loss: 0.00001702
Iteration 185/1000 | Loss: 0.00001702
Iteration 186/1000 | Loss: 0.00001702
Iteration 187/1000 | Loss: 0.00001702
Iteration 188/1000 | Loss: 0.00001701
Iteration 189/1000 | Loss: 0.00001701
Iteration 190/1000 | Loss: 0.00001701
Iteration 191/1000 | Loss: 0.00001701
Iteration 192/1000 | Loss: 0.00001701
Iteration 193/1000 | Loss: 0.00001701
Iteration 194/1000 | Loss: 0.00001701
Iteration 195/1000 | Loss: 0.00001701
Iteration 196/1000 | Loss: 0.00001701
Iteration 197/1000 | Loss: 0.00001701
Iteration 198/1000 | Loss: 0.00001701
Iteration 199/1000 | Loss: 0.00001701
Iteration 200/1000 | Loss: 0.00001701
Iteration 201/1000 | Loss: 0.00001701
Iteration 202/1000 | Loss: 0.00001701
Iteration 203/1000 | Loss: 0.00001701
Iteration 204/1000 | Loss: 0.00001701
Iteration 205/1000 | Loss: 0.00001701
Iteration 206/1000 | Loss: 0.00001701
Iteration 207/1000 | Loss: 0.00001701
Iteration 208/1000 | Loss: 0.00001701
Iteration 209/1000 | Loss: 0.00001701
Iteration 210/1000 | Loss: 0.00001701
Iteration 211/1000 | Loss: 0.00001701
Iteration 212/1000 | Loss: 0.00001701
Iteration 213/1000 | Loss: 0.00001701
Iteration 214/1000 | Loss: 0.00001701
Iteration 215/1000 | Loss: 0.00001701
Iteration 216/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.7008549548336305e-05, 1.7008549548336305e-05, 1.7008549548336305e-05, 1.7008549548336305e-05, 1.7008549548336305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7008549548336305e-05

Optimization complete. Final v2v error: 3.3952338695526123 mm

Highest mean error: 4.133537769317627 mm for frame 124

Lowest mean error: 2.563211441040039 mm for frame 21

Saving results

Total time: 41.334489822387695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827491
Iteration 2/25 | Loss: 0.00130861
Iteration 3/25 | Loss: 0.00108345
Iteration 4/25 | Loss: 0.00106272
Iteration 5/25 | Loss: 0.00106109
Iteration 6/25 | Loss: 0.00106106
Iteration 7/25 | Loss: 0.00106106
Iteration 8/25 | Loss: 0.00106106
Iteration 9/25 | Loss: 0.00106106
Iteration 10/25 | Loss: 0.00106106
Iteration 11/25 | Loss: 0.00106106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010610638419166207, 0.0010610638419166207, 0.0010610638419166207, 0.0010610638419166207, 0.0010610638419166207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010610638419166207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99228615
Iteration 2/25 | Loss: 0.00032859
Iteration 3/25 | Loss: 0.00032858
Iteration 4/25 | Loss: 0.00032858
Iteration 5/25 | Loss: 0.00032858
Iteration 6/25 | Loss: 0.00032858
Iteration 7/25 | Loss: 0.00032858
Iteration 8/25 | Loss: 0.00032858
Iteration 9/25 | Loss: 0.00032858
Iteration 10/25 | Loss: 0.00032858
Iteration 11/25 | Loss: 0.00032858
Iteration 12/25 | Loss: 0.00032858
Iteration 13/25 | Loss: 0.00032858
Iteration 14/25 | Loss: 0.00032858
Iteration 15/25 | Loss: 0.00032858
Iteration 16/25 | Loss: 0.00032858
Iteration 17/25 | Loss: 0.00032858
Iteration 18/25 | Loss: 0.00032858
Iteration 19/25 | Loss: 0.00032858
Iteration 20/25 | Loss: 0.00032858
Iteration 21/25 | Loss: 0.00032858
Iteration 22/25 | Loss: 0.00032858
Iteration 23/25 | Loss: 0.00032858
Iteration 24/25 | Loss: 0.00032858
Iteration 25/25 | Loss: 0.00032858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032858
Iteration 2/1000 | Loss: 0.00002718
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00002028
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001945
Iteration 7/1000 | Loss: 0.00001900
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001819
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001801
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001799
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001796
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001796
Iteration 25/1000 | Loss: 0.00001795
Iteration 26/1000 | Loss: 0.00001792
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001790
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001788
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001788
Iteration 34/1000 | Loss: 0.00001788
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001776
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001775
Iteration 42/1000 | Loss: 0.00001775
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001774
Iteration 53/1000 | Loss: 0.00001774
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001773
Iteration 56/1000 | Loss: 0.00001773
Iteration 57/1000 | Loss: 0.00001773
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001769
Iteration 81/1000 | Loss: 0.00001769
Iteration 82/1000 | Loss: 0.00001769
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001768
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001767
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001767
Iteration 99/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.766909917932935e-05, 1.766909917932935e-05, 1.766909917932935e-05, 1.766909917932935e-05, 1.766909917932935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766909917932935e-05

Optimization complete. Final v2v error: 3.483717679977417 mm

Highest mean error: 3.625717878341675 mm for frame 107

Lowest mean error: 3.3535220623016357 mm for frame 72

Saving results

Total time: 29.11217188835144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495542
Iteration 2/25 | Loss: 0.00114787
Iteration 3/25 | Loss: 0.00104557
Iteration 4/25 | Loss: 0.00102917
Iteration 5/25 | Loss: 0.00102230
Iteration 6/25 | Loss: 0.00102122
Iteration 7/25 | Loss: 0.00102122
Iteration 8/25 | Loss: 0.00102122
Iteration 9/25 | Loss: 0.00102122
Iteration 10/25 | Loss: 0.00102122
Iteration 11/25 | Loss: 0.00102122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010212165070697665, 0.0010212165070697665, 0.0010212165070697665, 0.0010212165070697665, 0.0010212165070697665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010212165070697665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80788922
Iteration 2/25 | Loss: 0.00066238
Iteration 3/25 | Loss: 0.00066238
Iteration 4/25 | Loss: 0.00066238
Iteration 5/25 | Loss: 0.00066238
Iteration 6/25 | Loss: 0.00066238
Iteration 7/25 | Loss: 0.00066238
Iteration 8/25 | Loss: 0.00066238
Iteration 9/25 | Loss: 0.00066238
Iteration 10/25 | Loss: 0.00066238
Iteration 11/25 | Loss: 0.00066238
Iteration 12/25 | Loss: 0.00066238
Iteration 13/25 | Loss: 0.00066238
Iteration 14/25 | Loss: 0.00066238
Iteration 15/25 | Loss: 0.00066238
Iteration 16/25 | Loss: 0.00066238
Iteration 17/25 | Loss: 0.00066238
Iteration 18/25 | Loss: 0.00066238
Iteration 19/25 | Loss: 0.00066238
Iteration 20/25 | Loss: 0.00066238
Iteration 21/25 | Loss: 0.00066238
Iteration 22/25 | Loss: 0.00066238
Iteration 23/25 | Loss: 0.00066238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006623772787861526, 0.0006623772787861526, 0.0006623772787861526, 0.0006623772787861526, 0.0006623772787861526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006623772787861526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066238
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00002355
Iteration 4/1000 | Loss: 0.00002150
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00002006
Iteration 7/1000 | Loss: 0.00001952
Iteration 8/1000 | Loss: 0.00001908
Iteration 9/1000 | Loss: 0.00001875
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001821
Iteration 12/1000 | Loss: 0.00001798
Iteration 13/1000 | Loss: 0.00001791
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001775
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001753
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001722
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001716
Iteration 31/1000 | Loss: 0.00001712
Iteration 32/1000 | Loss: 0.00001712
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001711
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001711
Iteration 45/1000 | Loss: 0.00001711
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001710
Iteration 48/1000 | Loss: 0.00001710
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001697
Iteration 79/1000 | Loss: 0.00001697
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001695
Iteration 85/1000 | Loss: 0.00001694
Iteration 86/1000 | Loss: 0.00001694
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Iteration 91/1000 | Loss: 0.00001689
Iteration 92/1000 | Loss: 0.00001689
Iteration 93/1000 | Loss: 0.00001689
Iteration 94/1000 | Loss: 0.00001688
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001688
Iteration 98/1000 | Loss: 0.00001688
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001686
Iteration 103/1000 | Loss: 0.00001686
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001685
Iteration 109/1000 | Loss: 0.00001685
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001684
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001684
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001683
Iteration 117/1000 | Loss: 0.00001683
Iteration 118/1000 | Loss: 0.00001682
Iteration 119/1000 | Loss: 0.00001682
Iteration 120/1000 | Loss: 0.00001682
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001681
Iteration 135/1000 | Loss: 0.00001681
Iteration 136/1000 | Loss: 0.00001681
Iteration 137/1000 | Loss: 0.00001681
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001681
Iteration 145/1000 | Loss: 0.00001681
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001678
Iteration 151/1000 | Loss: 0.00001678
Iteration 152/1000 | Loss: 0.00001678
Iteration 153/1000 | Loss: 0.00001678
Iteration 154/1000 | Loss: 0.00001678
Iteration 155/1000 | Loss: 0.00001678
Iteration 156/1000 | Loss: 0.00001677
Iteration 157/1000 | Loss: 0.00001677
Iteration 158/1000 | Loss: 0.00001677
Iteration 159/1000 | Loss: 0.00001677
Iteration 160/1000 | Loss: 0.00001677
Iteration 161/1000 | Loss: 0.00001677
Iteration 162/1000 | Loss: 0.00001677
Iteration 163/1000 | Loss: 0.00001677
Iteration 164/1000 | Loss: 0.00001676
Iteration 165/1000 | Loss: 0.00001676
Iteration 166/1000 | Loss: 0.00001676
Iteration 167/1000 | Loss: 0.00001676
Iteration 168/1000 | Loss: 0.00001676
Iteration 169/1000 | Loss: 0.00001676
Iteration 170/1000 | Loss: 0.00001676
Iteration 171/1000 | Loss: 0.00001676
Iteration 172/1000 | Loss: 0.00001675
Iteration 173/1000 | Loss: 0.00001675
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001675
Iteration 176/1000 | Loss: 0.00001675
Iteration 177/1000 | Loss: 0.00001675
Iteration 178/1000 | Loss: 0.00001675
Iteration 179/1000 | Loss: 0.00001675
Iteration 180/1000 | Loss: 0.00001675
Iteration 181/1000 | Loss: 0.00001675
Iteration 182/1000 | Loss: 0.00001675
Iteration 183/1000 | Loss: 0.00001674
Iteration 184/1000 | Loss: 0.00001674
Iteration 185/1000 | Loss: 0.00001674
Iteration 186/1000 | Loss: 0.00001674
Iteration 187/1000 | Loss: 0.00001674
Iteration 188/1000 | Loss: 0.00001674
Iteration 189/1000 | Loss: 0.00001674
Iteration 190/1000 | Loss: 0.00001674
Iteration 191/1000 | Loss: 0.00001674
Iteration 192/1000 | Loss: 0.00001674
Iteration 193/1000 | Loss: 0.00001674
Iteration 194/1000 | Loss: 0.00001673
Iteration 195/1000 | Loss: 0.00001673
Iteration 196/1000 | Loss: 0.00001673
Iteration 197/1000 | Loss: 0.00001673
Iteration 198/1000 | Loss: 0.00001673
Iteration 199/1000 | Loss: 0.00001673
Iteration 200/1000 | Loss: 0.00001673
Iteration 201/1000 | Loss: 0.00001673
Iteration 202/1000 | Loss: 0.00001673
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001673
Iteration 205/1000 | Loss: 0.00001673
Iteration 206/1000 | Loss: 0.00001672
Iteration 207/1000 | Loss: 0.00001672
Iteration 208/1000 | Loss: 0.00001672
Iteration 209/1000 | Loss: 0.00001672
Iteration 210/1000 | Loss: 0.00001672
Iteration 211/1000 | Loss: 0.00001672
Iteration 212/1000 | Loss: 0.00001672
Iteration 213/1000 | Loss: 0.00001672
Iteration 214/1000 | Loss: 0.00001672
Iteration 215/1000 | Loss: 0.00001672
Iteration 216/1000 | Loss: 0.00001672
Iteration 217/1000 | Loss: 0.00001672
Iteration 218/1000 | Loss: 0.00001672
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001671
Iteration 221/1000 | Loss: 0.00001671
Iteration 222/1000 | Loss: 0.00001671
Iteration 223/1000 | Loss: 0.00001670
Iteration 224/1000 | Loss: 0.00001670
Iteration 225/1000 | Loss: 0.00001670
Iteration 226/1000 | Loss: 0.00001670
Iteration 227/1000 | Loss: 0.00001670
Iteration 228/1000 | Loss: 0.00001670
Iteration 229/1000 | Loss: 0.00001670
Iteration 230/1000 | Loss: 0.00001670
Iteration 231/1000 | Loss: 0.00001670
Iteration 232/1000 | Loss: 0.00001670
Iteration 233/1000 | Loss: 0.00001670
Iteration 234/1000 | Loss: 0.00001670
Iteration 235/1000 | Loss: 0.00001670
Iteration 236/1000 | Loss: 0.00001670
Iteration 237/1000 | Loss: 0.00001670
Iteration 238/1000 | Loss: 0.00001669
Iteration 239/1000 | Loss: 0.00001669
Iteration 240/1000 | Loss: 0.00001669
Iteration 241/1000 | Loss: 0.00001669
Iteration 242/1000 | Loss: 0.00001669
Iteration 243/1000 | Loss: 0.00001669
Iteration 244/1000 | Loss: 0.00001669
Iteration 245/1000 | Loss: 0.00001669
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001668
Iteration 248/1000 | Loss: 0.00001668
Iteration 249/1000 | Loss: 0.00001668
Iteration 250/1000 | Loss: 0.00001668
Iteration 251/1000 | Loss: 0.00001668
Iteration 252/1000 | Loss: 0.00001668
Iteration 253/1000 | Loss: 0.00001668
Iteration 254/1000 | Loss: 0.00001668
Iteration 255/1000 | Loss: 0.00001668
Iteration 256/1000 | Loss: 0.00001668
Iteration 257/1000 | Loss: 0.00001668
Iteration 258/1000 | Loss: 0.00001668
Iteration 259/1000 | Loss: 0.00001668
Iteration 260/1000 | Loss: 0.00001667
Iteration 261/1000 | Loss: 0.00001667
Iteration 262/1000 | Loss: 0.00001667
Iteration 263/1000 | Loss: 0.00001667
Iteration 264/1000 | Loss: 0.00001667
Iteration 265/1000 | Loss: 0.00001667
Iteration 266/1000 | Loss: 0.00001666
Iteration 267/1000 | Loss: 0.00001666
Iteration 268/1000 | Loss: 0.00001666
Iteration 269/1000 | Loss: 0.00001666
Iteration 270/1000 | Loss: 0.00001666
Iteration 271/1000 | Loss: 0.00001666
Iteration 272/1000 | Loss: 0.00001666
Iteration 273/1000 | Loss: 0.00001665
Iteration 274/1000 | Loss: 0.00001665
Iteration 275/1000 | Loss: 0.00001665
Iteration 276/1000 | Loss: 0.00001665
Iteration 277/1000 | Loss: 0.00001665
Iteration 278/1000 | Loss: 0.00001665
Iteration 279/1000 | Loss: 0.00001665
Iteration 280/1000 | Loss: 0.00001665
Iteration 281/1000 | Loss: 0.00001665
Iteration 282/1000 | Loss: 0.00001665
Iteration 283/1000 | Loss: 0.00001664
Iteration 284/1000 | Loss: 0.00001664
Iteration 285/1000 | Loss: 0.00001664
Iteration 286/1000 | Loss: 0.00001664
Iteration 287/1000 | Loss: 0.00001664
Iteration 288/1000 | Loss: 0.00001664
Iteration 289/1000 | Loss: 0.00001664
Iteration 290/1000 | Loss: 0.00001664
Iteration 291/1000 | Loss: 0.00001664
Iteration 292/1000 | Loss: 0.00001664
Iteration 293/1000 | Loss: 0.00001664
Iteration 294/1000 | Loss: 0.00001664
Iteration 295/1000 | Loss: 0.00001664
Iteration 296/1000 | Loss: 0.00001664
Iteration 297/1000 | Loss: 0.00001664
Iteration 298/1000 | Loss: 0.00001664
Iteration 299/1000 | Loss: 0.00001664
Iteration 300/1000 | Loss: 0.00001664
Iteration 301/1000 | Loss: 0.00001664
Iteration 302/1000 | Loss: 0.00001664
Iteration 303/1000 | Loss: 0.00001664
Iteration 304/1000 | Loss: 0.00001664
Iteration 305/1000 | Loss: 0.00001664
Iteration 306/1000 | Loss: 0.00001664
Iteration 307/1000 | Loss: 0.00001664
Iteration 308/1000 | Loss: 0.00001664
Iteration 309/1000 | Loss: 0.00001664
Iteration 310/1000 | Loss: 0.00001664
Iteration 311/1000 | Loss: 0.00001664
Iteration 312/1000 | Loss: 0.00001664
Iteration 313/1000 | Loss: 0.00001664
Iteration 314/1000 | Loss: 0.00001664
Iteration 315/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.663880357227754e-05, 1.663880357227754e-05, 1.663880357227754e-05, 1.663880357227754e-05, 1.663880357227754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.663880357227754e-05

Optimization complete. Final v2v error: 3.418978452682495 mm

Highest mean error: 3.8977010250091553 mm for frame 10

Lowest mean error: 3.2594616413116455 mm for frame 35

Saving results

Total time: 64.58530926704407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00778244
Iteration 2/25 | Loss: 0.00148129
Iteration 3/25 | Loss: 0.00116925
Iteration 4/25 | Loss: 0.00113954
Iteration 5/25 | Loss: 0.00113632
Iteration 6/25 | Loss: 0.00113632
Iteration 7/25 | Loss: 0.00113632
Iteration 8/25 | Loss: 0.00113632
Iteration 9/25 | Loss: 0.00113632
Iteration 10/25 | Loss: 0.00113632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011363219236955047, 0.0011363219236955047, 0.0011363219236955047, 0.0011363219236955047, 0.0011363219236955047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011363219236955047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45923996
Iteration 2/25 | Loss: 0.00061042
Iteration 3/25 | Loss: 0.00061040
Iteration 4/25 | Loss: 0.00061040
Iteration 5/25 | Loss: 0.00061040
Iteration 6/25 | Loss: 0.00061040
Iteration 7/25 | Loss: 0.00061040
Iteration 8/25 | Loss: 0.00061040
Iteration 9/25 | Loss: 0.00061040
Iteration 10/25 | Loss: 0.00061040
Iteration 11/25 | Loss: 0.00061040
Iteration 12/25 | Loss: 0.00061040
Iteration 13/25 | Loss: 0.00061040
Iteration 14/25 | Loss: 0.00061040
Iteration 15/25 | Loss: 0.00061040
Iteration 16/25 | Loss: 0.00061040
Iteration 17/25 | Loss: 0.00061040
Iteration 18/25 | Loss: 0.00061040
Iteration 19/25 | Loss: 0.00061040
Iteration 20/25 | Loss: 0.00061040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000610398012213409, 0.000610398012213409, 0.000610398012213409, 0.000610398012213409, 0.000610398012213409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000610398012213409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061040
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001844
Iteration 7/1000 | Loss: 0.00001812
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001748
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001731
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001730
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001729
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001711
Iteration 24/1000 | Loss: 0.00001711
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001711
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001710
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001706
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001705
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001704
Iteration 45/1000 | Loss: 0.00001704
Iteration 46/1000 | Loss: 0.00001704
Iteration 47/1000 | Loss: 0.00001704
Iteration 48/1000 | Loss: 0.00001703
Iteration 49/1000 | Loss: 0.00001703
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001703
Iteration 53/1000 | Loss: 0.00001703
Iteration 54/1000 | Loss: 0.00001702
Iteration 55/1000 | Loss: 0.00001702
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001698
Iteration 73/1000 | Loss: 0.00001698
Iteration 74/1000 | Loss: 0.00001698
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001697
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001691
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001689
Iteration 101/1000 | Loss: 0.00001689
Iteration 102/1000 | Loss: 0.00001688
Iteration 103/1000 | Loss: 0.00001688
Iteration 104/1000 | Loss: 0.00001688
Iteration 105/1000 | Loss: 0.00001688
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001687
Iteration 108/1000 | Loss: 0.00001687
Iteration 109/1000 | Loss: 0.00001687
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00001687
Iteration 112/1000 | Loss: 0.00001687
Iteration 113/1000 | Loss: 0.00001687
Iteration 114/1000 | Loss: 0.00001687
Iteration 115/1000 | Loss: 0.00001687
Iteration 116/1000 | Loss: 0.00001687
Iteration 117/1000 | Loss: 0.00001687
Iteration 118/1000 | Loss: 0.00001687
Iteration 119/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.6871494153747335e-05, 1.6871494153747335e-05, 1.6871494153747335e-05, 1.6871494153747335e-05, 1.6871494153747335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6871494153747335e-05

Optimization complete. Final v2v error: 3.4497268199920654 mm

Highest mean error: 3.5951449871063232 mm for frame 2

Lowest mean error: 3.258371591567993 mm for frame 212

Saving results

Total time: 37.27804946899414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821842
Iteration 2/25 | Loss: 0.00140984
Iteration 3/25 | Loss: 0.00112261
Iteration 4/25 | Loss: 0.00106952
Iteration 5/25 | Loss: 0.00105249
Iteration 6/25 | Loss: 0.00104786
Iteration 7/25 | Loss: 0.00104361
Iteration 8/25 | Loss: 0.00104848
Iteration 9/25 | Loss: 0.00105362
Iteration 10/25 | Loss: 0.00105165
Iteration 11/25 | Loss: 0.00106159
Iteration 12/25 | Loss: 0.00105511
Iteration 13/25 | Loss: 0.00105815
Iteration 14/25 | Loss: 0.00105689
Iteration 15/25 | Loss: 0.00105786
Iteration 16/25 | Loss: 0.00105723
Iteration 17/25 | Loss: 0.00105639
Iteration 18/25 | Loss: 0.00105260
Iteration 19/25 | Loss: 0.00105264
Iteration 20/25 | Loss: 0.00105070
Iteration 21/25 | Loss: 0.00105320
Iteration 22/25 | Loss: 0.00105003
Iteration 23/25 | Loss: 0.00104583
Iteration 24/25 | Loss: 0.00104939
Iteration 25/25 | Loss: 0.00104946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37882364
Iteration 2/25 | Loss: 0.00078128
Iteration 3/25 | Loss: 0.00078128
Iteration 4/25 | Loss: 0.00078128
Iteration 5/25 | Loss: 0.00078128
Iteration 6/25 | Loss: 0.00078128
Iteration 7/25 | Loss: 0.00078128
Iteration 8/25 | Loss: 0.00078128
Iteration 9/25 | Loss: 0.00078128
Iteration 10/25 | Loss: 0.00078128
Iteration 11/25 | Loss: 0.00078128
Iteration 12/25 | Loss: 0.00078128
Iteration 13/25 | Loss: 0.00078128
Iteration 14/25 | Loss: 0.00078128
Iteration 15/25 | Loss: 0.00078128
Iteration 16/25 | Loss: 0.00078128
Iteration 17/25 | Loss: 0.00078128
Iteration 18/25 | Loss: 0.00078128
Iteration 19/25 | Loss: 0.00078128
Iteration 20/25 | Loss: 0.00078128
Iteration 21/25 | Loss: 0.00078128
Iteration 22/25 | Loss: 0.00078128
Iteration 23/25 | Loss: 0.00078128
Iteration 24/25 | Loss: 0.00078128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007812779513187706, 0.0007812779513187706, 0.0007812779513187706, 0.0007812779513187706, 0.0007812779513187706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007812779513187706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078128
Iteration 2/1000 | Loss: 0.00010068
Iteration 3/1000 | Loss: 0.00013694
Iteration 4/1000 | Loss: 0.00013334
Iteration 5/1000 | Loss: 0.00019261
Iteration 6/1000 | Loss: 0.00021348
Iteration 7/1000 | Loss: 0.00022694
Iteration 8/1000 | Loss: 0.00024268
Iteration 9/1000 | Loss: 0.00024993
Iteration 10/1000 | Loss: 0.00024421
Iteration 11/1000 | Loss: 0.00025745
Iteration 12/1000 | Loss: 0.00024718
Iteration 13/1000 | Loss: 0.00025542
Iteration 14/1000 | Loss: 0.00024428
Iteration 15/1000 | Loss: 0.00029167
Iteration 16/1000 | Loss: 0.00027757
Iteration 17/1000 | Loss: 0.00023741
Iteration 18/1000 | Loss: 0.00024858
Iteration 19/1000 | Loss: 0.00027645
Iteration 20/1000 | Loss: 0.00026421
Iteration 21/1000 | Loss: 0.00029365
Iteration 22/1000 | Loss: 0.00028082
Iteration 23/1000 | Loss: 0.00029116
Iteration 24/1000 | Loss: 0.00020340
Iteration 25/1000 | Loss: 0.00013777
Iteration 26/1000 | Loss: 0.00026329
Iteration 27/1000 | Loss: 0.00019652
Iteration 28/1000 | Loss: 0.00018824
Iteration 29/1000 | Loss: 0.00025891
Iteration 30/1000 | Loss: 0.00024080
Iteration 31/1000 | Loss: 0.00028832
Iteration 32/1000 | Loss: 0.00027152
Iteration 33/1000 | Loss: 0.00029065
Iteration 34/1000 | Loss: 0.00032405
Iteration 35/1000 | Loss: 0.00024805
Iteration 36/1000 | Loss: 0.00022111
Iteration 37/1000 | Loss: 0.00020812
Iteration 38/1000 | Loss: 0.00015802
Iteration 39/1000 | Loss: 0.00022015
Iteration 40/1000 | Loss: 0.00025228
Iteration 41/1000 | Loss: 0.00024413
Iteration 42/1000 | Loss: 0.00018469
Iteration 43/1000 | Loss: 0.00014968
Iteration 44/1000 | Loss: 0.00011964
Iteration 45/1000 | Loss: 0.00012469
Iteration 46/1000 | Loss: 0.00013352
Iteration 47/1000 | Loss: 0.00015871
Iteration 48/1000 | Loss: 0.00023176
Iteration 49/1000 | Loss: 0.00019425
Iteration 50/1000 | Loss: 0.00022358
Iteration 51/1000 | Loss: 0.00019425
Iteration 52/1000 | Loss: 0.00016361
Iteration 53/1000 | Loss: 0.00015594
Iteration 54/1000 | Loss: 0.00018423
Iteration 55/1000 | Loss: 0.00021784
Iteration 56/1000 | Loss: 0.00021275
Iteration 57/1000 | Loss: 0.00023330
Iteration 58/1000 | Loss: 0.00013242
Iteration 59/1000 | Loss: 0.00012710
Iteration 60/1000 | Loss: 0.00009435
Iteration 61/1000 | Loss: 0.00010909
Iteration 62/1000 | Loss: 0.00015585
Iteration 63/1000 | Loss: 0.00015141
Iteration 64/1000 | Loss: 0.00016267
Iteration 65/1000 | Loss: 0.00015335
Iteration 66/1000 | Loss: 0.00013377
Iteration 67/1000 | Loss: 0.00012418
Iteration 68/1000 | Loss: 0.00016453
Iteration 69/1000 | Loss: 0.00018926
Iteration 70/1000 | Loss: 0.00012874
Iteration 71/1000 | Loss: 0.00018198
Iteration 72/1000 | Loss: 0.00015438
Iteration 73/1000 | Loss: 0.00017368
Iteration 74/1000 | Loss: 0.00018923
Iteration 75/1000 | Loss: 0.00017800
Iteration 76/1000 | Loss: 0.00019495
Iteration 77/1000 | Loss: 0.00017351
Iteration 78/1000 | Loss: 0.00017933
Iteration 79/1000 | Loss: 0.00017950
Iteration 80/1000 | Loss: 0.00018682
Iteration 81/1000 | Loss: 0.00017940
Iteration 82/1000 | Loss: 0.00018735
Iteration 83/1000 | Loss: 0.00019509
Iteration 84/1000 | Loss: 0.00019713
Iteration 85/1000 | Loss: 0.00017286
Iteration 86/1000 | Loss: 0.00017187
Iteration 87/1000 | Loss: 0.00020878
Iteration 88/1000 | Loss: 0.00019357
Iteration 89/1000 | Loss: 0.00018652
Iteration 90/1000 | Loss: 0.00015281
Iteration 91/1000 | Loss: 0.00007768
Iteration 92/1000 | Loss: 0.00011122
Iteration 93/1000 | Loss: 0.00009466
Iteration 94/1000 | Loss: 0.00009169
Iteration 95/1000 | Loss: 0.00007823
Iteration 96/1000 | Loss: 0.00011507
Iteration 97/1000 | Loss: 0.00013940
Iteration 98/1000 | Loss: 0.00010103
Iteration 99/1000 | Loss: 0.00005402
Iteration 100/1000 | Loss: 0.00005309
Iteration 101/1000 | Loss: 0.00004252
Iteration 102/1000 | Loss: 0.00005367
Iteration 103/1000 | Loss: 0.00005238
Iteration 104/1000 | Loss: 0.00004162
Iteration 105/1000 | Loss: 0.00003909
Iteration 106/1000 | Loss: 0.00007188
Iteration 107/1000 | Loss: 0.00006652
Iteration 108/1000 | Loss: 0.00005295
Iteration 109/1000 | Loss: 0.00006300
Iteration 110/1000 | Loss: 0.00006354
Iteration 111/1000 | Loss: 0.00006681
Iteration 112/1000 | Loss: 0.00007003
Iteration 113/1000 | Loss: 0.00007251
Iteration 114/1000 | Loss: 0.00006574
Iteration 115/1000 | Loss: 0.00004571
Iteration 116/1000 | Loss: 0.00007082
Iteration 117/1000 | Loss: 0.00003829
Iteration 118/1000 | Loss: 0.00004288
Iteration 119/1000 | Loss: 0.00003529
Iteration 120/1000 | Loss: 0.00004426
Iteration 121/1000 | Loss: 0.00006164
Iteration 122/1000 | Loss: 0.00004888
Iteration 123/1000 | Loss: 0.00005879
Iteration 124/1000 | Loss: 0.00004889
Iteration 125/1000 | Loss: 0.00004983
Iteration 126/1000 | Loss: 0.00004141
Iteration 127/1000 | Loss: 0.00006902
Iteration 128/1000 | Loss: 0.00004995
Iteration 129/1000 | Loss: 0.00004001
Iteration 130/1000 | Loss: 0.00004131
Iteration 131/1000 | Loss: 0.00003643
Iteration 132/1000 | Loss: 0.00003994
Iteration 133/1000 | Loss: 0.00005469
Iteration 134/1000 | Loss: 0.00006456
Iteration 135/1000 | Loss: 0.00005441
Iteration 136/1000 | Loss: 0.00005959
Iteration 137/1000 | Loss: 0.00005851
Iteration 138/1000 | Loss: 0.00005166
Iteration 139/1000 | Loss: 0.00004493
Iteration 140/1000 | Loss: 0.00002184
Iteration 141/1000 | Loss: 0.00002860
Iteration 142/1000 | Loss: 0.00002157
Iteration 143/1000 | Loss: 0.00002334
Iteration 144/1000 | Loss: 0.00002104
Iteration 145/1000 | Loss: 0.00002445
Iteration 146/1000 | Loss: 0.00002490
Iteration 147/1000 | Loss: 0.00002273
Iteration 148/1000 | Loss: 0.00002252
Iteration 149/1000 | Loss: 0.00001539
Iteration 150/1000 | Loss: 0.00001572
Iteration 151/1000 | Loss: 0.00002505
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00003525
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00001646
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001718
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00003286
Iteration 162/1000 | Loss: 0.00002632
Iteration 163/1000 | Loss: 0.00001708
Iteration 164/1000 | Loss: 0.00002295
Iteration 165/1000 | Loss: 0.00001714
Iteration 166/1000 | Loss: 0.00002116
Iteration 167/1000 | Loss: 0.00001627
Iteration 168/1000 | Loss: 0.00002190
Iteration 169/1000 | Loss: 0.00001611
Iteration 170/1000 | Loss: 0.00002438
Iteration 171/1000 | Loss: 0.00002587
Iteration 172/1000 | Loss: 0.00001602
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00003300
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001218
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001066
Iteration 179/1000 | Loss: 0.00001035
Iteration 180/1000 | Loss: 0.00001016
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000989
Iteration 183/1000 | Loss: 0.00000977
Iteration 184/1000 | Loss: 0.00000974
Iteration 185/1000 | Loss: 0.00000950
Iteration 186/1000 | Loss: 0.00000934
Iteration 187/1000 | Loss: 0.00000932
Iteration 188/1000 | Loss: 0.00000915
Iteration 189/1000 | Loss: 0.00000913
Iteration 190/1000 | Loss: 0.00000910
Iteration 191/1000 | Loss: 0.00000909
Iteration 192/1000 | Loss: 0.00000909
Iteration 193/1000 | Loss: 0.00000909
Iteration 194/1000 | Loss: 0.00000908
Iteration 195/1000 | Loss: 0.00000908
Iteration 196/1000 | Loss: 0.00000907
Iteration 197/1000 | Loss: 0.00000905
Iteration 198/1000 | Loss: 0.00000904
Iteration 199/1000 | Loss: 0.00000903
Iteration 200/1000 | Loss: 0.00000901
Iteration 201/1000 | Loss: 0.00000901
Iteration 202/1000 | Loss: 0.00000901
Iteration 203/1000 | Loss: 0.00000901
Iteration 204/1000 | Loss: 0.00000900
Iteration 205/1000 | Loss: 0.00000900
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000899
Iteration 211/1000 | Loss: 0.00000897
Iteration 212/1000 | Loss: 0.00000897
Iteration 213/1000 | Loss: 0.00000897
Iteration 214/1000 | Loss: 0.00000897
Iteration 215/1000 | Loss: 0.00000897
Iteration 216/1000 | Loss: 0.00000897
Iteration 217/1000 | Loss: 0.00000897
Iteration 218/1000 | Loss: 0.00000897
Iteration 219/1000 | Loss: 0.00000897
Iteration 220/1000 | Loss: 0.00000897
Iteration 221/1000 | Loss: 0.00000897
Iteration 222/1000 | Loss: 0.00000896
Iteration 223/1000 | Loss: 0.00000896
Iteration 224/1000 | Loss: 0.00000896
Iteration 225/1000 | Loss: 0.00000895
Iteration 226/1000 | Loss: 0.00000895
Iteration 227/1000 | Loss: 0.00000894
Iteration 228/1000 | Loss: 0.00000894
Iteration 229/1000 | Loss: 0.00000893
Iteration 230/1000 | Loss: 0.00000893
Iteration 231/1000 | Loss: 0.00000893
Iteration 232/1000 | Loss: 0.00000893
Iteration 233/1000 | Loss: 0.00000893
Iteration 234/1000 | Loss: 0.00000893
Iteration 235/1000 | Loss: 0.00000892
Iteration 236/1000 | Loss: 0.00000892
Iteration 237/1000 | Loss: 0.00000892
Iteration 238/1000 | Loss: 0.00000892
Iteration 239/1000 | Loss: 0.00000891
Iteration 240/1000 | Loss: 0.00000891
Iteration 241/1000 | Loss: 0.00000891
Iteration 242/1000 | Loss: 0.00000890
Iteration 243/1000 | Loss: 0.00000890
Iteration 244/1000 | Loss: 0.00000888
Iteration 245/1000 | Loss: 0.00000887
Iteration 246/1000 | Loss: 0.00000886
Iteration 247/1000 | Loss: 0.00000886
Iteration 248/1000 | Loss: 0.00000886
Iteration 249/1000 | Loss: 0.00000886
Iteration 250/1000 | Loss: 0.00000886
Iteration 251/1000 | Loss: 0.00000886
Iteration 252/1000 | Loss: 0.00000886
Iteration 253/1000 | Loss: 0.00000886
Iteration 254/1000 | Loss: 0.00000886
Iteration 255/1000 | Loss: 0.00000886
Iteration 256/1000 | Loss: 0.00000886
Iteration 257/1000 | Loss: 0.00000886
Iteration 258/1000 | Loss: 0.00000885
Iteration 259/1000 | Loss: 0.00000885
Iteration 260/1000 | Loss: 0.00000885
Iteration 261/1000 | Loss: 0.00000885
Iteration 262/1000 | Loss: 0.00000885
Iteration 263/1000 | Loss: 0.00000885
Iteration 264/1000 | Loss: 0.00000885
Iteration 265/1000 | Loss: 0.00000885
Iteration 266/1000 | Loss: 0.00000884
Iteration 267/1000 | Loss: 0.00000884
Iteration 268/1000 | Loss: 0.00000884
Iteration 269/1000 | Loss: 0.00000884
Iteration 270/1000 | Loss: 0.00000884
Iteration 271/1000 | Loss: 0.00000884
Iteration 272/1000 | Loss: 0.00000883
Iteration 273/1000 | Loss: 0.00000883
Iteration 274/1000 | Loss: 0.00000883
Iteration 275/1000 | Loss: 0.00000883
Iteration 276/1000 | Loss: 0.00000882
Iteration 277/1000 | Loss: 0.00000882
Iteration 278/1000 | Loss: 0.00000882
Iteration 279/1000 | Loss: 0.00000881
Iteration 280/1000 | Loss: 0.00000881
Iteration 281/1000 | Loss: 0.00000881
Iteration 282/1000 | Loss: 0.00000881
Iteration 283/1000 | Loss: 0.00000881
Iteration 284/1000 | Loss: 0.00000881
Iteration 285/1000 | Loss: 0.00000881
Iteration 286/1000 | Loss: 0.00000881
Iteration 287/1000 | Loss: 0.00000881
Iteration 288/1000 | Loss: 0.00000881
Iteration 289/1000 | Loss: 0.00000881
Iteration 290/1000 | Loss: 0.00000881
Iteration 291/1000 | Loss: 0.00000881
Iteration 292/1000 | Loss: 0.00000880
Iteration 293/1000 | Loss: 0.00000880
Iteration 294/1000 | Loss: 0.00000880
Iteration 295/1000 | Loss: 0.00000880
Iteration 296/1000 | Loss: 0.00000879
Iteration 297/1000 | Loss: 0.00000879
Iteration 298/1000 | Loss: 0.00000879
Iteration 299/1000 | Loss: 0.00000879
Iteration 300/1000 | Loss: 0.00000879
Iteration 301/1000 | Loss: 0.00000879
Iteration 302/1000 | Loss: 0.00000879
Iteration 303/1000 | Loss: 0.00000879
Iteration 304/1000 | Loss: 0.00000879
Iteration 305/1000 | Loss: 0.00000879
Iteration 306/1000 | Loss: 0.00000879
Iteration 307/1000 | Loss: 0.00000879
Iteration 308/1000 | Loss: 0.00000879
Iteration 309/1000 | Loss: 0.00000879
Iteration 310/1000 | Loss: 0.00000878
Iteration 311/1000 | Loss: 0.00000878
Iteration 312/1000 | Loss: 0.00000878
Iteration 313/1000 | Loss: 0.00000878
Iteration 314/1000 | Loss: 0.00000878
Iteration 315/1000 | Loss: 0.00000878
Iteration 316/1000 | Loss: 0.00000878
Iteration 317/1000 | Loss: 0.00000878
Iteration 318/1000 | Loss: 0.00000878
Iteration 319/1000 | Loss: 0.00000878
Iteration 320/1000 | Loss: 0.00000878
Iteration 321/1000 | Loss: 0.00000878
Iteration 322/1000 | Loss: 0.00000878
Iteration 323/1000 | Loss: 0.00000878
Iteration 324/1000 | Loss: 0.00000878
Iteration 325/1000 | Loss: 0.00000878
Iteration 326/1000 | Loss: 0.00000878
Iteration 327/1000 | Loss: 0.00000878
Iteration 328/1000 | Loss: 0.00000878
Iteration 329/1000 | Loss: 0.00000878
Iteration 330/1000 | Loss: 0.00000878
Iteration 331/1000 | Loss: 0.00000878
Iteration 332/1000 | Loss: 0.00000878
Iteration 333/1000 | Loss: 0.00000877
Iteration 334/1000 | Loss: 0.00000877
Iteration 335/1000 | Loss: 0.00000877
Iteration 336/1000 | Loss: 0.00000877
Iteration 337/1000 | Loss: 0.00000877
Iteration 338/1000 | Loss: 0.00000877
Iteration 339/1000 | Loss: 0.00000877
Iteration 340/1000 | Loss: 0.00000877
Iteration 341/1000 | Loss: 0.00000877
Iteration 342/1000 | Loss: 0.00000877
Iteration 343/1000 | Loss: 0.00000877
Iteration 344/1000 | Loss: 0.00000877
Iteration 345/1000 | Loss: 0.00000877
Iteration 346/1000 | Loss: 0.00000877
Iteration 347/1000 | Loss: 0.00000877
Iteration 348/1000 | Loss: 0.00000877
Iteration 349/1000 | Loss: 0.00000877
Iteration 350/1000 | Loss: 0.00000877
Iteration 351/1000 | Loss: 0.00000877
Iteration 352/1000 | Loss: 0.00000877
Iteration 353/1000 | Loss: 0.00000877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [8.769669875619002e-06, 8.769669875619002e-06, 8.769669875619002e-06, 8.769669875619002e-06, 8.769669875619002e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.769669875619002e-06

Optimization complete. Final v2v error: 2.544051170349121 mm

Highest mean error: 3.519146203994751 mm for frame 102

Lowest mean error: 2.359225273132324 mm for frame 35

Saving results

Total time: 358.372620344162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969167
Iteration 2/25 | Loss: 0.00245546
Iteration 3/25 | Loss: 0.00165707
Iteration 4/25 | Loss: 0.00151412
Iteration 5/25 | Loss: 0.00147757
Iteration 6/25 | Loss: 0.00146722
Iteration 7/25 | Loss: 0.00140914
Iteration 8/25 | Loss: 0.00130395
Iteration 9/25 | Loss: 0.00131933
Iteration 10/25 | Loss: 0.00125396
Iteration 11/25 | Loss: 0.00123284
Iteration 12/25 | Loss: 0.00121921
Iteration 13/25 | Loss: 0.00120965
Iteration 14/25 | Loss: 0.00119697
Iteration 15/25 | Loss: 0.00120835
Iteration 16/25 | Loss: 0.00119939
Iteration 17/25 | Loss: 0.00118735
Iteration 18/25 | Loss: 0.00118446
Iteration 19/25 | Loss: 0.00117178
Iteration 20/25 | Loss: 0.00116567
Iteration 21/25 | Loss: 0.00115657
Iteration 22/25 | Loss: 0.00116433
Iteration 23/25 | Loss: 0.00116408
Iteration 24/25 | Loss: 0.00115540
Iteration 25/25 | Loss: 0.00114560

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40024507
Iteration 2/25 | Loss: 0.00229522
Iteration 3/25 | Loss: 0.00162302
Iteration 4/25 | Loss: 0.00162302
Iteration 5/25 | Loss: 0.00162302
Iteration 6/25 | Loss: 0.00162302
Iteration 7/25 | Loss: 0.00162302
Iteration 8/25 | Loss: 0.00162302
Iteration 9/25 | Loss: 0.00162302
Iteration 10/25 | Loss: 0.00162302
Iteration 11/25 | Loss: 0.00162302
Iteration 12/25 | Loss: 0.00162302
Iteration 13/25 | Loss: 0.00162302
Iteration 14/25 | Loss: 0.00162302
Iteration 15/25 | Loss: 0.00162302
Iteration 16/25 | Loss: 0.00162302
Iteration 17/25 | Loss: 0.00162302
Iteration 18/25 | Loss: 0.00162302
Iteration 19/25 | Loss: 0.00162302
Iteration 20/25 | Loss: 0.00162302
Iteration 21/25 | Loss: 0.00162302
Iteration 22/25 | Loss: 0.00162302
Iteration 23/25 | Loss: 0.00162302
Iteration 24/25 | Loss: 0.00162302
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016230186447501183, 0.0016230186447501183, 0.0016230186447501183, 0.0016230186447501183, 0.0016230186447501183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016230186447501183

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162302
Iteration 2/1000 | Loss: 0.00072711
Iteration 3/1000 | Loss: 0.00332488
Iteration 4/1000 | Loss: 0.00079437
Iteration 5/1000 | Loss: 0.00025042
Iteration 6/1000 | Loss: 0.00033816
Iteration 7/1000 | Loss: 0.00019486
Iteration 8/1000 | Loss: 0.00020741
Iteration 9/1000 | Loss: 0.00027274
Iteration 10/1000 | Loss: 0.00033855
Iteration 11/1000 | Loss: 0.00027175
Iteration 12/1000 | Loss: 0.00030173
Iteration 13/1000 | Loss: 0.00029637
Iteration 14/1000 | Loss: 0.00008432
Iteration 15/1000 | Loss: 0.00007928
Iteration 16/1000 | Loss: 0.00007500
Iteration 17/1000 | Loss: 0.00018206
Iteration 18/1000 | Loss: 0.00062450
Iteration 19/1000 | Loss: 0.00018228
Iteration 20/1000 | Loss: 0.00041962
Iteration 21/1000 | Loss: 0.00007047
Iteration 22/1000 | Loss: 0.00029651
Iteration 23/1000 | Loss: 0.00037502
Iteration 24/1000 | Loss: 0.00028610
Iteration 25/1000 | Loss: 0.00020906
Iteration 26/1000 | Loss: 0.00008738
Iteration 27/1000 | Loss: 0.00007577
Iteration 28/1000 | Loss: 0.00031120
Iteration 29/1000 | Loss: 0.00023045
Iteration 30/1000 | Loss: 0.00016924
Iteration 31/1000 | Loss: 0.00016595
Iteration 32/1000 | Loss: 0.00018179
Iteration 33/1000 | Loss: 0.00019977
Iteration 34/1000 | Loss: 0.00007633
Iteration 35/1000 | Loss: 0.00007214
Iteration 36/1000 | Loss: 0.00006997
Iteration 37/1000 | Loss: 0.00006861
Iteration 38/1000 | Loss: 0.00006694
Iteration 39/1000 | Loss: 0.00006550
Iteration 40/1000 | Loss: 0.00006460
Iteration 41/1000 | Loss: 0.00006383
Iteration 42/1000 | Loss: 0.00006340
Iteration 43/1000 | Loss: 0.00006296
Iteration 44/1000 | Loss: 0.00044544
Iteration 45/1000 | Loss: 0.00075526
Iteration 46/1000 | Loss: 0.00019145
Iteration 47/1000 | Loss: 0.00008625
Iteration 48/1000 | Loss: 0.00008628
Iteration 49/1000 | Loss: 0.00073208
Iteration 50/1000 | Loss: 0.00009881
Iteration 51/1000 | Loss: 0.00006160
Iteration 52/1000 | Loss: 0.00005631
Iteration 53/1000 | Loss: 0.00005197
Iteration 54/1000 | Loss: 0.00019120
Iteration 55/1000 | Loss: 0.00026630
Iteration 56/1000 | Loss: 0.00018038
Iteration 57/1000 | Loss: 0.00005888
Iteration 58/1000 | Loss: 0.00004739
Iteration 59/1000 | Loss: 0.00004623
Iteration 60/1000 | Loss: 0.00037558
Iteration 61/1000 | Loss: 0.00004708
Iteration 62/1000 | Loss: 0.00004457
Iteration 63/1000 | Loss: 0.00042214
Iteration 64/1000 | Loss: 0.00041229
Iteration 65/1000 | Loss: 0.00004582
Iteration 66/1000 | Loss: 0.00004186
Iteration 67/1000 | Loss: 0.00003981
Iteration 68/1000 | Loss: 0.00003733
Iteration 69/1000 | Loss: 0.00003627
Iteration 70/1000 | Loss: 0.00073120
Iteration 71/1000 | Loss: 0.00009527
Iteration 72/1000 | Loss: 0.00003663
Iteration 73/1000 | Loss: 0.00007719
Iteration 74/1000 | Loss: 0.00003286
Iteration 75/1000 | Loss: 0.00003100
Iteration 76/1000 | Loss: 0.00003002
Iteration 77/1000 | Loss: 0.00002954
Iteration 78/1000 | Loss: 0.00002918
Iteration 79/1000 | Loss: 0.00002915
Iteration 80/1000 | Loss: 0.00037323
Iteration 81/1000 | Loss: 0.00003053
Iteration 82/1000 | Loss: 0.00002850
Iteration 83/1000 | Loss: 0.00002760
Iteration 84/1000 | Loss: 0.00002665
Iteration 85/1000 | Loss: 0.00002609
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002567
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002548
Iteration 90/1000 | Loss: 0.00002541
Iteration 91/1000 | Loss: 0.00002529
Iteration 92/1000 | Loss: 0.00002528
Iteration 93/1000 | Loss: 0.00002523
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002520
Iteration 96/1000 | Loss: 0.00002520
Iteration 97/1000 | Loss: 0.00002520
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002520
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002520
Iteration 103/1000 | Loss: 0.00002520
Iteration 104/1000 | Loss: 0.00002519
Iteration 105/1000 | Loss: 0.00002519
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002517
Iteration 108/1000 | Loss: 0.00002517
Iteration 109/1000 | Loss: 0.00002516
Iteration 110/1000 | Loss: 0.00002516
Iteration 111/1000 | Loss: 0.00002515
Iteration 112/1000 | Loss: 0.00002515
Iteration 113/1000 | Loss: 0.00002515
Iteration 114/1000 | Loss: 0.00002515
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002515
Iteration 117/1000 | Loss: 0.00002514
Iteration 118/1000 | Loss: 0.00002514
Iteration 119/1000 | Loss: 0.00002514
Iteration 120/1000 | Loss: 0.00002513
Iteration 121/1000 | Loss: 0.00002513
Iteration 122/1000 | Loss: 0.00002513
Iteration 123/1000 | Loss: 0.00002513
Iteration 124/1000 | Loss: 0.00002512
Iteration 125/1000 | Loss: 0.00002512
Iteration 126/1000 | Loss: 0.00002512
Iteration 127/1000 | Loss: 0.00002511
Iteration 128/1000 | Loss: 0.00002511
Iteration 129/1000 | Loss: 0.00002511
Iteration 130/1000 | Loss: 0.00002511
Iteration 131/1000 | Loss: 0.00002511
Iteration 132/1000 | Loss: 0.00002511
Iteration 133/1000 | Loss: 0.00002510
Iteration 134/1000 | Loss: 0.00002510
Iteration 135/1000 | Loss: 0.00002510
Iteration 136/1000 | Loss: 0.00002510
Iteration 137/1000 | Loss: 0.00002510
Iteration 138/1000 | Loss: 0.00002510
Iteration 139/1000 | Loss: 0.00002509
Iteration 140/1000 | Loss: 0.00002509
Iteration 141/1000 | Loss: 0.00002509
Iteration 142/1000 | Loss: 0.00002509
Iteration 143/1000 | Loss: 0.00002509
Iteration 144/1000 | Loss: 0.00002509
Iteration 145/1000 | Loss: 0.00002509
Iteration 146/1000 | Loss: 0.00002509
Iteration 147/1000 | Loss: 0.00002508
Iteration 148/1000 | Loss: 0.00002508
Iteration 149/1000 | Loss: 0.00002508
Iteration 150/1000 | Loss: 0.00002508
Iteration 151/1000 | Loss: 0.00002508
Iteration 152/1000 | Loss: 0.00002508
Iteration 153/1000 | Loss: 0.00002508
Iteration 154/1000 | Loss: 0.00002508
Iteration 155/1000 | Loss: 0.00002508
Iteration 156/1000 | Loss: 0.00002508
Iteration 157/1000 | Loss: 0.00002507
Iteration 158/1000 | Loss: 0.00002507
Iteration 159/1000 | Loss: 0.00002507
Iteration 160/1000 | Loss: 0.00002507
Iteration 161/1000 | Loss: 0.00002507
Iteration 162/1000 | Loss: 0.00002507
Iteration 163/1000 | Loss: 0.00002507
Iteration 164/1000 | Loss: 0.00002507
Iteration 165/1000 | Loss: 0.00002507
Iteration 166/1000 | Loss: 0.00002507
Iteration 167/1000 | Loss: 0.00002507
Iteration 168/1000 | Loss: 0.00002507
Iteration 169/1000 | Loss: 0.00002507
Iteration 170/1000 | Loss: 0.00002506
Iteration 171/1000 | Loss: 0.00002506
Iteration 172/1000 | Loss: 0.00002506
Iteration 173/1000 | Loss: 0.00002506
Iteration 174/1000 | Loss: 0.00002506
Iteration 175/1000 | Loss: 0.00002506
Iteration 176/1000 | Loss: 0.00002506
Iteration 177/1000 | Loss: 0.00002506
Iteration 178/1000 | Loss: 0.00002505
Iteration 179/1000 | Loss: 0.00002505
Iteration 180/1000 | Loss: 0.00002505
Iteration 181/1000 | Loss: 0.00002504
Iteration 182/1000 | Loss: 0.00002504
Iteration 183/1000 | Loss: 0.00002504
Iteration 184/1000 | Loss: 0.00002504
Iteration 185/1000 | Loss: 0.00002504
Iteration 186/1000 | Loss: 0.00002504
Iteration 187/1000 | Loss: 0.00002504
Iteration 188/1000 | Loss: 0.00002503
Iteration 189/1000 | Loss: 0.00002503
Iteration 190/1000 | Loss: 0.00002503
Iteration 191/1000 | Loss: 0.00002503
Iteration 192/1000 | Loss: 0.00002503
Iteration 193/1000 | Loss: 0.00002503
Iteration 194/1000 | Loss: 0.00002503
Iteration 195/1000 | Loss: 0.00002503
Iteration 196/1000 | Loss: 0.00002503
Iteration 197/1000 | Loss: 0.00002503
Iteration 198/1000 | Loss: 0.00002503
Iteration 199/1000 | Loss: 0.00002503
Iteration 200/1000 | Loss: 0.00002503
Iteration 201/1000 | Loss: 0.00002503
Iteration 202/1000 | Loss: 0.00002503
Iteration 203/1000 | Loss: 0.00002502
Iteration 204/1000 | Loss: 0.00002502
Iteration 205/1000 | Loss: 0.00002502
Iteration 206/1000 | Loss: 0.00002502
Iteration 207/1000 | Loss: 0.00002502
Iteration 208/1000 | Loss: 0.00002502
Iteration 209/1000 | Loss: 0.00002502
Iteration 210/1000 | Loss: 0.00002502
Iteration 211/1000 | Loss: 0.00002502
Iteration 212/1000 | Loss: 0.00002502
Iteration 213/1000 | Loss: 0.00002502
Iteration 214/1000 | Loss: 0.00002501
Iteration 215/1000 | Loss: 0.00002501
Iteration 216/1000 | Loss: 0.00002501
Iteration 217/1000 | Loss: 0.00002501
Iteration 218/1000 | Loss: 0.00002501
Iteration 219/1000 | Loss: 0.00002501
Iteration 220/1000 | Loss: 0.00002501
Iteration 221/1000 | Loss: 0.00002500
Iteration 222/1000 | Loss: 0.00002500
Iteration 223/1000 | Loss: 0.00002500
Iteration 224/1000 | Loss: 0.00002500
Iteration 225/1000 | Loss: 0.00002500
Iteration 226/1000 | Loss: 0.00002500
Iteration 227/1000 | Loss: 0.00002500
Iteration 228/1000 | Loss: 0.00002500
Iteration 229/1000 | Loss: 0.00002500
Iteration 230/1000 | Loss: 0.00002499
Iteration 231/1000 | Loss: 0.00002499
Iteration 232/1000 | Loss: 0.00002499
Iteration 233/1000 | Loss: 0.00002499
Iteration 234/1000 | Loss: 0.00002499
Iteration 235/1000 | Loss: 0.00002499
Iteration 236/1000 | Loss: 0.00002499
Iteration 237/1000 | Loss: 0.00002498
Iteration 238/1000 | Loss: 0.00002498
Iteration 239/1000 | Loss: 0.00002498
Iteration 240/1000 | Loss: 0.00002498
Iteration 241/1000 | Loss: 0.00002498
Iteration 242/1000 | Loss: 0.00002498
Iteration 243/1000 | Loss: 0.00002498
Iteration 244/1000 | Loss: 0.00002498
Iteration 245/1000 | Loss: 0.00002498
Iteration 246/1000 | Loss: 0.00002498
Iteration 247/1000 | Loss: 0.00002498
Iteration 248/1000 | Loss: 0.00002498
Iteration 249/1000 | Loss: 0.00002498
Iteration 250/1000 | Loss: 0.00002498
Iteration 251/1000 | Loss: 0.00002498
Iteration 252/1000 | Loss: 0.00002498
Iteration 253/1000 | Loss: 0.00002498
Iteration 254/1000 | Loss: 0.00002498
Iteration 255/1000 | Loss: 0.00002498
Iteration 256/1000 | Loss: 0.00002498
Iteration 257/1000 | Loss: 0.00002498
Iteration 258/1000 | Loss: 0.00002498
Iteration 259/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [2.4978417059173808e-05, 2.4978417059173808e-05, 2.4978417059173808e-05, 2.4978417059173808e-05, 2.4978417059173808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4978417059173808e-05

Optimization complete. Final v2v error: 3.914534091949463 mm

Highest mean error: 10.831538200378418 mm for frame 0

Lowest mean error: 2.8623342514038086 mm for frame 1

Saving results

Total time: 186.19204115867615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865130
Iteration 2/25 | Loss: 0.00280991
Iteration 3/25 | Loss: 0.00169261
Iteration 4/25 | Loss: 0.00136438
Iteration 5/25 | Loss: 0.00132868
Iteration 6/25 | Loss: 0.00132484
Iteration 7/25 | Loss: 0.00131309
Iteration 8/25 | Loss: 0.00129356
Iteration 9/25 | Loss: 0.00127888
Iteration 10/25 | Loss: 0.00127648
Iteration 11/25 | Loss: 0.00127052
Iteration 12/25 | Loss: 0.00126954
Iteration 13/25 | Loss: 0.00127245
Iteration 14/25 | Loss: 0.00126991
Iteration 15/25 | Loss: 0.00126865
Iteration 16/25 | Loss: 0.00126813
Iteration 17/25 | Loss: 0.00126759
Iteration 18/25 | Loss: 0.00126989
Iteration 19/25 | Loss: 0.00126715
Iteration 20/25 | Loss: 0.00126706
Iteration 21/25 | Loss: 0.00126698
Iteration 22/25 | Loss: 0.00127036
Iteration 23/25 | Loss: 0.00126689
Iteration 24/25 | Loss: 0.00126688
Iteration 25/25 | Loss: 0.00126688

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04853582
Iteration 2/25 | Loss: 0.00485433
Iteration 3/25 | Loss: 0.00251510
Iteration 4/25 | Loss: 0.00251510
Iteration 5/25 | Loss: 0.00251510
Iteration 6/25 | Loss: 0.00251510
Iteration 7/25 | Loss: 0.00251510
Iteration 8/25 | Loss: 0.00251510
Iteration 9/25 | Loss: 0.00251510
Iteration 10/25 | Loss: 0.00251510
Iteration 11/25 | Loss: 0.00251510
Iteration 12/25 | Loss: 0.00251510
Iteration 13/25 | Loss: 0.00251510
Iteration 14/25 | Loss: 0.00251510
Iteration 15/25 | Loss: 0.00251510
Iteration 16/25 | Loss: 0.00251510
Iteration 17/25 | Loss: 0.00251510
Iteration 18/25 | Loss: 0.00251510
Iteration 19/25 | Loss: 0.00251510
Iteration 20/25 | Loss: 0.00251510
Iteration 21/25 | Loss: 0.00251510
Iteration 22/25 | Loss: 0.00251510
Iteration 23/25 | Loss: 0.00251510
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002515096217393875, 0.002515096217393875, 0.002515096217393875, 0.002515096217393875, 0.002515096217393875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002515096217393875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251510
Iteration 2/1000 | Loss: 0.00391767
Iteration 3/1000 | Loss: 0.00413261
Iteration 4/1000 | Loss: 0.00046859
Iteration 5/1000 | Loss: 0.00067867
Iteration 6/1000 | Loss: 0.00132486
Iteration 7/1000 | Loss: 0.00213500
Iteration 8/1000 | Loss: 0.00673498
Iteration 9/1000 | Loss: 0.00082519
Iteration 10/1000 | Loss: 0.00081633
Iteration 11/1000 | Loss: 0.00188541
Iteration 12/1000 | Loss: 0.00169651
Iteration 13/1000 | Loss: 0.00100224
Iteration 14/1000 | Loss: 0.00014079
Iteration 15/1000 | Loss: 0.00014585
Iteration 16/1000 | Loss: 0.00010765
Iteration 17/1000 | Loss: 0.00014073
Iteration 18/1000 | Loss: 0.00009449
Iteration 19/1000 | Loss: 0.00078985
Iteration 20/1000 | Loss: 0.00237011
Iteration 21/1000 | Loss: 0.00037440
Iteration 22/1000 | Loss: 0.00038269
Iteration 23/1000 | Loss: 0.00008956
Iteration 24/1000 | Loss: 0.00112243
Iteration 25/1000 | Loss: 0.00176597
Iteration 26/1000 | Loss: 0.00060558
Iteration 27/1000 | Loss: 0.00225178
Iteration 28/1000 | Loss: 0.00052771
Iteration 29/1000 | Loss: 0.00163244
Iteration 30/1000 | Loss: 0.00134933
Iteration 31/1000 | Loss: 0.00199661
Iteration 32/1000 | Loss: 0.00126200
Iteration 33/1000 | Loss: 0.00064063
Iteration 34/1000 | Loss: 0.00058627
Iteration 35/1000 | Loss: 0.00132300
Iteration 36/1000 | Loss: 0.00052352
Iteration 37/1000 | Loss: 0.00317443
Iteration 38/1000 | Loss: 0.00184229
Iteration 39/1000 | Loss: 0.00120549
Iteration 40/1000 | Loss: 0.00314259
Iteration 41/1000 | Loss: 0.00377625
Iteration 42/1000 | Loss: 0.00676294
Iteration 43/1000 | Loss: 0.00295790
Iteration 44/1000 | Loss: 0.00389804
Iteration 45/1000 | Loss: 0.00337578
Iteration 46/1000 | Loss: 0.00478124
Iteration 47/1000 | Loss: 0.00196216
Iteration 48/1000 | Loss: 0.00315067
Iteration 49/1000 | Loss: 0.00111428
Iteration 50/1000 | Loss: 0.00175845
Iteration 51/1000 | Loss: 0.00066807
Iteration 52/1000 | Loss: 0.00046756
Iteration 53/1000 | Loss: 0.00102626
Iteration 54/1000 | Loss: 0.00137812
Iteration 55/1000 | Loss: 0.00149717
Iteration 56/1000 | Loss: 0.00127126
Iteration 57/1000 | Loss: 0.00099792
Iteration 58/1000 | Loss: 0.00075135
Iteration 59/1000 | Loss: 0.00188003
Iteration 60/1000 | Loss: 0.00137878
Iteration 61/1000 | Loss: 0.00061439
Iteration 62/1000 | Loss: 0.00090013
Iteration 63/1000 | Loss: 0.00173845
Iteration 64/1000 | Loss: 0.00147749
Iteration 65/1000 | Loss: 0.00035666
Iteration 66/1000 | Loss: 0.00013488
Iteration 67/1000 | Loss: 0.00040382
Iteration 68/1000 | Loss: 0.00042420
Iteration 69/1000 | Loss: 0.00174189
Iteration 70/1000 | Loss: 0.00021019
Iteration 71/1000 | Loss: 0.00091806
Iteration 72/1000 | Loss: 0.00047173
Iteration 73/1000 | Loss: 0.00011209
Iteration 74/1000 | Loss: 0.00078354
Iteration 75/1000 | Loss: 0.00038300
Iteration 76/1000 | Loss: 0.00161363
Iteration 77/1000 | Loss: 0.00075159
Iteration 78/1000 | Loss: 0.00042375
Iteration 79/1000 | Loss: 0.00032028
Iteration 80/1000 | Loss: 0.00070696
Iteration 81/1000 | Loss: 0.00127592
Iteration 82/1000 | Loss: 0.00027968
Iteration 83/1000 | Loss: 0.00080424
Iteration 84/1000 | Loss: 0.00199632
Iteration 85/1000 | Loss: 0.00073492
Iteration 86/1000 | Loss: 0.00075717
Iteration 87/1000 | Loss: 0.00075581
Iteration 88/1000 | Loss: 0.00092110
Iteration 89/1000 | Loss: 0.00059845
Iteration 90/1000 | Loss: 0.00104868
Iteration 91/1000 | Loss: 0.00065718
Iteration 92/1000 | Loss: 0.00034875
Iteration 93/1000 | Loss: 0.00027925
Iteration 94/1000 | Loss: 0.00026159
Iteration 95/1000 | Loss: 0.00015281
Iteration 96/1000 | Loss: 0.00004285
Iteration 97/1000 | Loss: 0.00044773
Iteration 98/1000 | Loss: 0.00123315
Iteration 99/1000 | Loss: 0.00052107
Iteration 100/1000 | Loss: 0.00043768
Iteration 101/1000 | Loss: 0.00015979
Iteration 102/1000 | Loss: 0.00019783
Iteration 103/1000 | Loss: 0.00029651
Iteration 104/1000 | Loss: 0.00033747
Iteration 105/1000 | Loss: 0.00019669
Iteration 106/1000 | Loss: 0.00031916
Iteration 107/1000 | Loss: 0.00016740
Iteration 108/1000 | Loss: 0.00014808
Iteration 109/1000 | Loss: 0.00035107
Iteration 110/1000 | Loss: 0.00005214
Iteration 111/1000 | Loss: 0.00004271
Iteration 112/1000 | Loss: 0.00003766
Iteration 113/1000 | Loss: 0.00066675
Iteration 114/1000 | Loss: 0.00155424
Iteration 115/1000 | Loss: 0.00253071
Iteration 116/1000 | Loss: 0.00148094
Iteration 117/1000 | Loss: 0.00184225
Iteration 118/1000 | Loss: 0.00036073
Iteration 119/1000 | Loss: 0.00026405
Iteration 120/1000 | Loss: 0.00109844
Iteration 121/1000 | Loss: 0.00147960
Iteration 122/1000 | Loss: 0.00048529
Iteration 123/1000 | Loss: 0.00011548
Iteration 124/1000 | Loss: 0.00049143
Iteration 125/1000 | Loss: 0.00068726
Iteration 126/1000 | Loss: 0.00011958
Iteration 127/1000 | Loss: 0.00003902
Iteration 128/1000 | Loss: 0.00019536
Iteration 129/1000 | Loss: 0.00109957
Iteration 130/1000 | Loss: 0.00014793
Iteration 131/1000 | Loss: 0.00040843
Iteration 132/1000 | Loss: 0.00024141
Iteration 133/1000 | Loss: 0.00011593
Iteration 134/1000 | Loss: 0.00101280
Iteration 135/1000 | Loss: 0.00019374
Iteration 136/1000 | Loss: 0.00007984
Iteration 137/1000 | Loss: 0.00004229
Iteration 138/1000 | Loss: 0.00002323
Iteration 139/1000 | Loss: 0.00048479
Iteration 140/1000 | Loss: 0.00029310
Iteration 141/1000 | Loss: 0.00009316
Iteration 142/1000 | Loss: 0.00036941
Iteration 143/1000 | Loss: 0.00005898
Iteration 144/1000 | Loss: 0.00017373
Iteration 145/1000 | Loss: 0.00002281
Iteration 146/1000 | Loss: 0.00061097
Iteration 147/1000 | Loss: 0.00036312
Iteration 148/1000 | Loss: 0.00052454
Iteration 149/1000 | Loss: 0.00062630
Iteration 150/1000 | Loss: 0.00017892
Iteration 151/1000 | Loss: 0.00042294
Iteration 152/1000 | Loss: 0.00006727
Iteration 153/1000 | Loss: 0.00002341
Iteration 154/1000 | Loss: 0.00001941
Iteration 155/1000 | Loss: 0.00007294
Iteration 156/1000 | Loss: 0.00009166
Iteration 157/1000 | Loss: 0.00001843
Iteration 158/1000 | Loss: 0.00006246
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00003928
Iteration 161/1000 | Loss: 0.00001678
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001606
Iteration 164/1000 | Loss: 0.00001603
Iteration 165/1000 | Loss: 0.00001578
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001538
Iteration 169/1000 | Loss: 0.00004281
Iteration 170/1000 | Loss: 0.00002156
Iteration 171/1000 | Loss: 0.00003763
Iteration 172/1000 | Loss: 0.00001521
Iteration 173/1000 | Loss: 0.00001520
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00053027
Iteration 177/1000 | Loss: 0.00096714
Iteration 178/1000 | Loss: 0.00020067
Iteration 179/1000 | Loss: 0.00007671
Iteration 180/1000 | Loss: 0.00007017
Iteration 181/1000 | Loss: 0.00045026
Iteration 182/1000 | Loss: 0.00021391
Iteration 183/1000 | Loss: 0.00002883
Iteration 184/1000 | Loss: 0.00001609
Iteration 185/1000 | Loss: 0.00007713
Iteration 186/1000 | Loss: 0.00001566
Iteration 187/1000 | Loss: 0.00001535
Iteration 188/1000 | Loss: 0.00009486
Iteration 189/1000 | Loss: 0.00001957
Iteration 190/1000 | Loss: 0.00008763
Iteration 191/1000 | Loss: 0.00003944
Iteration 192/1000 | Loss: 0.00005648
Iteration 193/1000 | Loss: 0.00001548
Iteration 194/1000 | Loss: 0.00001524
Iteration 195/1000 | Loss: 0.00001516
Iteration 196/1000 | Loss: 0.00001516
Iteration 197/1000 | Loss: 0.00001513
Iteration 198/1000 | Loss: 0.00001513
Iteration 199/1000 | Loss: 0.00001512
Iteration 200/1000 | Loss: 0.00001512
Iteration 201/1000 | Loss: 0.00001512
Iteration 202/1000 | Loss: 0.00001512
Iteration 203/1000 | Loss: 0.00001512
Iteration 204/1000 | Loss: 0.00001512
Iteration 205/1000 | Loss: 0.00001512
Iteration 206/1000 | Loss: 0.00001511
Iteration 207/1000 | Loss: 0.00001511
Iteration 208/1000 | Loss: 0.00001511
Iteration 209/1000 | Loss: 0.00001511
Iteration 210/1000 | Loss: 0.00001511
Iteration 211/1000 | Loss: 0.00001510
Iteration 212/1000 | Loss: 0.00001510
Iteration 213/1000 | Loss: 0.00001510
Iteration 214/1000 | Loss: 0.00001509
Iteration 215/1000 | Loss: 0.00001509
Iteration 216/1000 | Loss: 0.00001509
Iteration 217/1000 | Loss: 0.00001509
Iteration 218/1000 | Loss: 0.00001509
Iteration 219/1000 | Loss: 0.00001509
Iteration 220/1000 | Loss: 0.00001509
Iteration 221/1000 | Loss: 0.00001509
Iteration 222/1000 | Loss: 0.00001509
Iteration 223/1000 | Loss: 0.00001509
Iteration 224/1000 | Loss: 0.00001509
Iteration 225/1000 | Loss: 0.00001509
Iteration 226/1000 | Loss: 0.00001509
Iteration 227/1000 | Loss: 0.00001509
Iteration 228/1000 | Loss: 0.00001509
Iteration 229/1000 | Loss: 0.00001508
Iteration 230/1000 | Loss: 0.00001508
Iteration 231/1000 | Loss: 0.00001508
Iteration 232/1000 | Loss: 0.00001508
Iteration 233/1000 | Loss: 0.00001508
Iteration 234/1000 | Loss: 0.00001508
Iteration 235/1000 | Loss: 0.00001508
Iteration 236/1000 | Loss: 0.00001508
Iteration 237/1000 | Loss: 0.00001508
Iteration 238/1000 | Loss: 0.00001507
Iteration 239/1000 | Loss: 0.00001507
Iteration 240/1000 | Loss: 0.00001507
Iteration 241/1000 | Loss: 0.00001506
Iteration 242/1000 | Loss: 0.00001505
Iteration 243/1000 | Loss: 0.00001503
Iteration 244/1000 | Loss: 0.00001503
Iteration 245/1000 | Loss: 0.00001503
Iteration 246/1000 | Loss: 0.00001503
Iteration 247/1000 | Loss: 0.00001503
Iteration 248/1000 | Loss: 0.00001502
Iteration 249/1000 | Loss: 0.00001502
Iteration 250/1000 | Loss: 0.00001502
Iteration 251/1000 | Loss: 0.00001502
Iteration 252/1000 | Loss: 0.00001502
Iteration 253/1000 | Loss: 0.00001501
Iteration 254/1000 | Loss: 0.00001500
Iteration 255/1000 | Loss: 0.00001500
Iteration 256/1000 | Loss: 0.00045691
Iteration 257/1000 | Loss: 0.00013918
Iteration 258/1000 | Loss: 0.00017358
Iteration 259/1000 | Loss: 0.00022728
Iteration 260/1000 | Loss: 0.00072079
Iteration 261/1000 | Loss: 0.00006228
Iteration 262/1000 | Loss: 0.00027266
Iteration 263/1000 | Loss: 0.00072544
Iteration 264/1000 | Loss: 0.00041349
Iteration 265/1000 | Loss: 0.00039082
Iteration 266/1000 | Loss: 0.00041494
Iteration 267/1000 | Loss: 0.00014483
Iteration 268/1000 | Loss: 0.00001910
Iteration 269/1000 | Loss: 0.00003021
Iteration 270/1000 | Loss: 0.00001586
Iteration 271/1000 | Loss: 0.00001550
Iteration 272/1000 | Loss: 0.00001544
Iteration 273/1000 | Loss: 0.00001540
Iteration 274/1000 | Loss: 0.00007959
Iteration 275/1000 | Loss: 0.00004263
Iteration 276/1000 | Loss: 0.00002870
Iteration 277/1000 | Loss: 0.00001678
Iteration 278/1000 | Loss: 0.00002971
Iteration 279/1000 | Loss: 0.00001591
Iteration 280/1000 | Loss: 0.00001539
Iteration 281/1000 | Loss: 0.00007028
Iteration 282/1000 | Loss: 0.00001889
Iteration 283/1000 | Loss: 0.00009318
Iteration 284/1000 | Loss: 0.00001668
Iteration 285/1000 | Loss: 0.00001583
Iteration 286/1000 | Loss: 0.00001530
Iteration 287/1000 | Loss: 0.00001510
Iteration 288/1000 | Loss: 0.00001508
Iteration 289/1000 | Loss: 0.00001507
Iteration 290/1000 | Loss: 0.00001507
Iteration 291/1000 | Loss: 0.00001506
Iteration 292/1000 | Loss: 0.00001506
Iteration 293/1000 | Loss: 0.00001506
Iteration 294/1000 | Loss: 0.00001506
Iteration 295/1000 | Loss: 0.00001506
Iteration 296/1000 | Loss: 0.00001505
Iteration 297/1000 | Loss: 0.00001504
Iteration 298/1000 | Loss: 0.00004307
Iteration 299/1000 | Loss: 0.00001532
Iteration 300/1000 | Loss: 0.00001494
Iteration 301/1000 | Loss: 0.00001492
Iteration 302/1000 | Loss: 0.00001491
Iteration 303/1000 | Loss: 0.00001489
Iteration 304/1000 | Loss: 0.00001488
Iteration 305/1000 | Loss: 0.00001488
Iteration 306/1000 | Loss: 0.00001488
Iteration 307/1000 | Loss: 0.00001487
Iteration 308/1000 | Loss: 0.00001487
Iteration 309/1000 | Loss: 0.00001487
Iteration 310/1000 | Loss: 0.00001486
Iteration 311/1000 | Loss: 0.00001486
Iteration 312/1000 | Loss: 0.00001485
Iteration 313/1000 | Loss: 0.00001481
Iteration 314/1000 | Loss: 0.00001469
Iteration 315/1000 | Loss: 0.00001469
Iteration 316/1000 | Loss: 0.00001468
Iteration 317/1000 | Loss: 0.00001467
Iteration 318/1000 | Loss: 0.00001467
Iteration 319/1000 | Loss: 0.00001463
Iteration 320/1000 | Loss: 0.00001462
Iteration 321/1000 | Loss: 0.00001462
Iteration 322/1000 | Loss: 0.00001461
Iteration 323/1000 | Loss: 0.00001461
Iteration 324/1000 | Loss: 0.00001460
Iteration 325/1000 | Loss: 0.00001460
Iteration 326/1000 | Loss: 0.00001459
Iteration 327/1000 | Loss: 0.00001459
Iteration 328/1000 | Loss: 0.00001459
Iteration 329/1000 | Loss: 0.00001459
Iteration 330/1000 | Loss: 0.00001459
Iteration 331/1000 | Loss: 0.00001459
Iteration 332/1000 | Loss: 0.00001458
Iteration 333/1000 | Loss: 0.00001458
Iteration 334/1000 | Loss: 0.00001458
Iteration 335/1000 | Loss: 0.00001458
Iteration 336/1000 | Loss: 0.00001458
Iteration 337/1000 | Loss: 0.00001457
Iteration 338/1000 | Loss: 0.00001457
Iteration 339/1000 | Loss: 0.00001457
Iteration 340/1000 | Loss: 0.00001457
Iteration 341/1000 | Loss: 0.00001457
Iteration 342/1000 | Loss: 0.00001457
Iteration 343/1000 | Loss: 0.00001457
Iteration 344/1000 | Loss: 0.00001457
Iteration 345/1000 | Loss: 0.00001457
Iteration 346/1000 | Loss: 0.00001456
Iteration 347/1000 | Loss: 0.00001456
Iteration 348/1000 | Loss: 0.00001456
Iteration 349/1000 | Loss: 0.00001456
Iteration 350/1000 | Loss: 0.00001456
Iteration 351/1000 | Loss: 0.00001456
Iteration 352/1000 | Loss: 0.00001456
Iteration 353/1000 | Loss: 0.00001456
Iteration 354/1000 | Loss: 0.00001456
Iteration 355/1000 | Loss: 0.00001456
Iteration 356/1000 | Loss: 0.00001456
Iteration 357/1000 | Loss: 0.00001455
Iteration 358/1000 | Loss: 0.00001455
Iteration 359/1000 | Loss: 0.00001455
Iteration 360/1000 | Loss: 0.00001455
Iteration 361/1000 | Loss: 0.00001455
Iteration 362/1000 | Loss: 0.00001455
Iteration 363/1000 | Loss: 0.00001455
Iteration 364/1000 | Loss: 0.00001455
Iteration 365/1000 | Loss: 0.00001455
Iteration 366/1000 | Loss: 0.00001455
Iteration 367/1000 | Loss: 0.00001455
Iteration 368/1000 | Loss: 0.00001455
Iteration 369/1000 | Loss: 0.00001455
Iteration 370/1000 | Loss: 0.00001455
Iteration 371/1000 | Loss: 0.00001455
Iteration 372/1000 | Loss: 0.00001455
Iteration 373/1000 | Loss: 0.00001455
Iteration 374/1000 | Loss: 0.00001455
Iteration 375/1000 | Loss: 0.00001455
Iteration 376/1000 | Loss: 0.00001455
Iteration 377/1000 | Loss: 0.00001455
Iteration 378/1000 | Loss: 0.00001455
Iteration 379/1000 | Loss: 0.00001455
Iteration 380/1000 | Loss: 0.00001455
Iteration 381/1000 | Loss: 0.00001455
Iteration 382/1000 | Loss: 0.00001455
Iteration 383/1000 | Loss: 0.00001455
Iteration 384/1000 | Loss: 0.00001455
Iteration 385/1000 | Loss: 0.00001455
Iteration 386/1000 | Loss: 0.00001455
Iteration 387/1000 | Loss: 0.00001455
Iteration 388/1000 | Loss: 0.00001455
Iteration 389/1000 | Loss: 0.00001455
Iteration 390/1000 | Loss: 0.00001455
Iteration 391/1000 | Loss: 0.00001455
Iteration 392/1000 | Loss: 0.00001455
Iteration 393/1000 | Loss: 0.00001455
Iteration 394/1000 | Loss: 0.00001455
Iteration 395/1000 | Loss: 0.00001455
Iteration 396/1000 | Loss: 0.00001455
Iteration 397/1000 | Loss: 0.00001455
Iteration 398/1000 | Loss: 0.00001455
Iteration 399/1000 | Loss: 0.00001455
Iteration 400/1000 | Loss: 0.00001455
Iteration 401/1000 | Loss: 0.00001455
Iteration 402/1000 | Loss: 0.00001455
Iteration 403/1000 | Loss: 0.00001455
Iteration 404/1000 | Loss: 0.00001455
Iteration 405/1000 | Loss: 0.00001455
Iteration 406/1000 | Loss: 0.00001455
Iteration 407/1000 | Loss: 0.00001455
Iteration 408/1000 | Loss: 0.00001455
Iteration 409/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [1.4552174434356857e-05, 1.4552174434356857e-05, 1.4552174434356857e-05, 1.4552174434356857e-05, 1.4552174434356857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4552174434356857e-05

Optimization complete. Final v2v error: 2.9164679050445557 mm

Highest mean error: 12.14216136932373 mm for frame 100

Lowest mean error: 2.304147243499756 mm for frame 141

Saving results

Total time: 409.9030821323395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803958
Iteration 2/25 | Loss: 0.00117494
Iteration 3/25 | Loss: 0.00103984
Iteration 4/25 | Loss: 0.00101394
Iteration 5/25 | Loss: 0.00100475
Iteration 6/25 | Loss: 0.00100254
Iteration 7/25 | Loss: 0.00100254
Iteration 8/25 | Loss: 0.00100254
Iteration 9/25 | Loss: 0.00100254
Iteration 10/25 | Loss: 0.00100254
Iteration 11/25 | Loss: 0.00100254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001002538832835853, 0.001002538832835853, 0.001002538832835853, 0.001002538832835853, 0.001002538832835853]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001002538832835853

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54078627
Iteration 2/25 | Loss: 0.00076382
Iteration 3/25 | Loss: 0.00076372
Iteration 4/25 | Loss: 0.00076372
Iteration 5/25 | Loss: 0.00076372
Iteration 6/25 | Loss: 0.00076372
Iteration 7/25 | Loss: 0.00076372
Iteration 8/25 | Loss: 0.00076372
Iteration 9/25 | Loss: 0.00076372
Iteration 10/25 | Loss: 0.00076372
Iteration 11/25 | Loss: 0.00076372
Iteration 12/25 | Loss: 0.00076372
Iteration 13/25 | Loss: 0.00076372
Iteration 14/25 | Loss: 0.00076372
Iteration 15/25 | Loss: 0.00076372
Iteration 16/25 | Loss: 0.00076372
Iteration 17/25 | Loss: 0.00076372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007637200178578496, 0.0007637200178578496, 0.0007637200178578496, 0.0007637200178578496, 0.0007637200178578496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007637200178578496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076372
Iteration 2/1000 | Loss: 0.00004462
Iteration 3/1000 | Loss: 0.00002926
Iteration 4/1000 | Loss: 0.00002240
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001776
Iteration 8/1000 | Loss: 0.00001691
Iteration 9/1000 | Loss: 0.00001636
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001586
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001574
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001565
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001546
Iteration 23/1000 | Loss: 0.00001545
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001542
Iteration 26/1000 | Loss: 0.00001541
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00001539
Iteration 30/1000 | Loss: 0.00001534
Iteration 31/1000 | Loss: 0.00001526
Iteration 32/1000 | Loss: 0.00001526
Iteration 33/1000 | Loss: 0.00001526
Iteration 34/1000 | Loss: 0.00001526
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001526
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001525
Iteration 47/1000 | Loss: 0.00001524
Iteration 48/1000 | Loss: 0.00001524
Iteration 49/1000 | Loss: 0.00001523
Iteration 50/1000 | Loss: 0.00001523
Iteration 51/1000 | Loss: 0.00001523
Iteration 52/1000 | Loss: 0.00001522
Iteration 53/1000 | Loss: 0.00001522
Iteration 54/1000 | Loss: 0.00001522
Iteration 55/1000 | Loss: 0.00001521
Iteration 56/1000 | Loss: 0.00001521
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001518
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001517
Iteration 68/1000 | Loss: 0.00001517
Iteration 69/1000 | Loss: 0.00001517
Iteration 70/1000 | Loss: 0.00001516
Iteration 71/1000 | Loss: 0.00001516
Iteration 72/1000 | Loss: 0.00001516
Iteration 73/1000 | Loss: 0.00001516
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001515
Iteration 76/1000 | Loss: 0.00001515
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001512
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001511
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001511
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001511
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001510
Iteration 114/1000 | Loss: 0.00001510
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001510
Iteration 120/1000 | Loss: 0.00001510
Iteration 121/1000 | Loss: 0.00001510
Iteration 122/1000 | Loss: 0.00001510
Iteration 123/1000 | Loss: 0.00001510
Iteration 124/1000 | Loss: 0.00001510
Iteration 125/1000 | Loss: 0.00001510
Iteration 126/1000 | Loss: 0.00001509
Iteration 127/1000 | Loss: 0.00001509
Iteration 128/1000 | Loss: 0.00001509
Iteration 129/1000 | Loss: 0.00001509
Iteration 130/1000 | Loss: 0.00001509
Iteration 131/1000 | Loss: 0.00001509
Iteration 132/1000 | Loss: 0.00001509
Iteration 133/1000 | Loss: 0.00001509
Iteration 134/1000 | Loss: 0.00001509
Iteration 135/1000 | Loss: 0.00001509
Iteration 136/1000 | Loss: 0.00001509
Iteration 137/1000 | Loss: 0.00001509
Iteration 138/1000 | Loss: 0.00001509
Iteration 139/1000 | Loss: 0.00001509
Iteration 140/1000 | Loss: 0.00001509
Iteration 141/1000 | Loss: 0.00001509
Iteration 142/1000 | Loss: 0.00001509
Iteration 143/1000 | Loss: 0.00001509
Iteration 144/1000 | Loss: 0.00001508
Iteration 145/1000 | Loss: 0.00001508
Iteration 146/1000 | Loss: 0.00001508
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001508
Iteration 149/1000 | Loss: 0.00001508
Iteration 150/1000 | Loss: 0.00001508
Iteration 151/1000 | Loss: 0.00001508
Iteration 152/1000 | Loss: 0.00001508
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5079184777277987e-05, 1.5079184777277987e-05, 1.5079184777277987e-05, 1.5079184777277987e-05, 1.5079184777277987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5079184777277987e-05

Optimization complete. Final v2v error: 3.3148579597473145 mm

Highest mean error: 3.6054985523223877 mm for frame 132

Lowest mean error: 2.918557643890381 mm for frame 182

Saving results

Total time: 41.86513113975525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838728
Iteration 2/25 | Loss: 0.00144097
Iteration 3/25 | Loss: 0.00116693
Iteration 4/25 | Loss: 0.00113866
Iteration 5/25 | Loss: 0.00113361
Iteration 6/25 | Loss: 0.00113255
Iteration 7/25 | Loss: 0.00113255
Iteration 8/25 | Loss: 0.00113255
Iteration 9/25 | Loss: 0.00113255
Iteration 10/25 | Loss: 0.00113255
Iteration 11/25 | Loss: 0.00113255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001132552744820714, 0.001132552744820714, 0.001132552744820714, 0.001132552744820714, 0.001132552744820714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132552744820714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39314973
Iteration 2/25 | Loss: 0.00031193
Iteration 3/25 | Loss: 0.00031193
Iteration 4/25 | Loss: 0.00031193
Iteration 5/25 | Loss: 0.00031193
Iteration 6/25 | Loss: 0.00031193
Iteration 7/25 | Loss: 0.00031193
Iteration 8/25 | Loss: 0.00031193
Iteration 9/25 | Loss: 0.00031193
Iteration 10/25 | Loss: 0.00031193
Iteration 11/25 | Loss: 0.00031193
Iteration 12/25 | Loss: 0.00031193
Iteration 13/25 | Loss: 0.00031193
Iteration 14/25 | Loss: 0.00031193
Iteration 15/25 | Loss: 0.00031193
Iteration 16/25 | Loss: 0.00031193
Iteration 17/25 | Loss: 0.00031193
Iteration 18/25 | Loss: 0.00031193
Iteration 19/25 | Loss: 0.00031193
Iteration 20/25 | Loss: 0.00031193
Iteration 21/25 | Loss: 0.00031193
Iteration 22/25 | Loss: 0.00031193
Iteration 23/25 | Loss: 0.00031193
Iteration 24/25 | Loss: 0.00031193
Iteration 25/25 | Loss: 0.00031193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031193
Iteration 2/1000 | Loss: 0.00005028
Iteration 3/1000 | Loss: 0.00004256
Iteration 4/1000 | Loss: 0.00004090
Iteration 5/1000 | Loss: 0.00003956
Iteration 6/1000 | Loss: 0.00003837
Iteration 7/1000 | Loss: 0.00003756
Iteration 8/1000 | Loss: 0.00003698
Iteration 9/1000 | Loss: 0.00003663
Iteration 10/1000 | Loss: 0.00003639
Iteration 11/1000 | Loss: 0.00003624
Iteration 12/1000 | Loss: 0.00003622
Iteration 13/1000 | Loss: 0.00003621
Iteration 14/1000 | Loss: 0.00003620
Iteration 15/1000 | Loss: 0.00003619
Iteration 16/1000 | Loss: 0.00003619
Iteration 17/1000 | Loss: 0.00003618
Iteration 18/1000 | Loss: 0.00003618
Iteration 19/1000 | Loss: 0.00003611
Iteration 20/1000 | Loss: 0.00003609
Iteration 21/1000 | Loss: 0.00003609
Iteration 22/1000 | Loss: 0.00003609
Iteration 23/1000 | Loss: 0.00003609
Iteration 24/1000 | Loss: 0.00003609
Iteration 25/1000 | Loss: 0.00003609
Iteration 26/1000 | Loss: 0.00003608
Iteration 27/1000 | Loss: 0.00003608
Iteration 28/1000 | Loss: 0.00003608
Iteration 29/1000 | Loss: 0.00003608
Iteration 30/1000 | Loss: 0.00003608
Iteration 31/1000 | Loss: 0.00003608
Iteration 32/1000 | Loss: 0.00003608
Iteration 33/1000 | Loss: 0.00003608
Iteration 34/1000 | Loss: 0.00003608
Iteration 35/1000 | Loss: 0.00003608
Iteration 36/1000 | Loss: 0.00003608
Iteration 37/1000 | Loss: 0.00003608
Iteration 38/1000 | Loss: 0.00003608
Iteration 39/1000 | Loss: 0.00003608
Iteration 40/1000 | Loss: 0.00003608
Iteration 41/1000 | Loss: 0.00003608
Iteration 42/1000 | Loss: 0.00003608
Iteration 43/1000 | Loss: 0.00003608
Iteration 44/1000 | Loss: 0.00003608
Iteration 45/1000 | Loss: 0.00003608
Iteration 46/1000 | Loss: 0.00003606
Iteration 47/1000 | Loss: 0.00003605
Iteration 48/1000 | Loss: 0.00003605
Iteration 49/1000 | Loss: 0.00003605
Iteration 50/1000 | Loss: 0.00003605
Iteration 51/1000 | Loss: 0.00003605
Iteration 52/1000 | Loss: 0.00003605
Iteration 53/1000 | Loss: 0.00003603
Iteration 54/1000 | Loss: 0.00003603
Iteration 55/1000 | Loss: 0.00003602
Iteration 56/1000 | Loss: 0.00003602
Iteration 57/1000 | Loss: 0.00003602
Iteration 58/1000 | Loss: 0.00003602
Iteration 59/1000 | Loss: 0.00003602
Iteration 60/1000 | Loss: 0.00003601
Iteration 61/1000 | Loss: 0.00003601
Iteration 62/1000 | Loss: 0.00003601
Iteration 63/1000 | Loss: 0.00003601
Iteration 64/1000 | Loss: 0.00003601
Iteration 65/1000 | Loss: 0.00003600
Iteration 66/1000 | Loss: 0.00003600
Iteration 67/1000 | Loss: 0.00003600
Iteration 68/1000 | Loss: 0.00003600
Iteration 69/1000 | Loss: 0.00003600
Iteration 70/1000 | Loss: 0.00003599
Iteration 71/1000 | Loss: 0.00003599
Iteration 72/1000 | Loss: 0.00003599
Iteration 73/1000 | Loss: 0.00003599
Iteration 74/1000 | Loss: 0.00003599
Iteration 75/1000 | Loss: 0.00003599
Iteration 76/1000 | Loss: 0.00003599
Iteration 77/1000 | Loss: 0.00003599
Iteration 78/1000 | Loss: 0.00003599
Iteration 79/1000 | Loss: 0.00003598
Iteration 80/1000 | Loss: 0.00003598
Iteration 81/1000 | Loss: 0.00003598
Iteration 82/1000 | Loss: 0.00003598
Iteration 83/1000 | Loss: 0.00003598
Iteration 84/1000 | Loss: 0.00003598
Iteration 85/1000 | Loss: 0.00003597
Iteration 86/1000 | Loss: 0.00003597
Iteration 87/1000 | Loss: 0.00003597
Iteration 88/1000 | Loss: 0.00003597
Iteration 89/1000 | Loss: 0.00003597
Iteration 90/1000 | Loss: 0.00003597
Iteration 91/1000 | Loss: 0.00003597
Iteration 92/1000 | Loss: 0.00003597
Iteration 93/1000 | Loss: 0.00003597
Iteration 94/1000 | Loss: 0.00003597
Iteration 95/1000 | Loss: 0.00003597
Iteration 96/1000 | Loss: 0.00003597
Iteration 97/1000 | Loss: 0.00003597
Iteration 98/1000 | Loss: 0.00003596
Iteration 99/1000 | Loss: 0.00003596
Iteration 100/1000 | Loss: 0.00003596
Iteration 101/1000 | Loss: 0.00003596
Iteration 102/1000 | Loss: 0.00003596
Iteration 103/1000 | Loss: 0.00003596
Iteration 104/1000 | Loss: 0.00003596
Iteration 105/1000 | Loss: 0.00003596
Iteration 106/1000 | Loss: 0.00003596
Iteration 107/1000 | Loss: 0.00003596
Iteration 108/1000 | Loss: 0.00003596
Iteration 109/1000 | Loss: 0.00003596
Iteration 110/1000 | Loss: 0.00003596
Iteration 111/1000 | Loss: 0.00003595
Iteration 112/1000 | Loss: 0.00003595
Iteration 113/1000 | Loss: 0.00003595
Iteration 114/1000 | Loss: 0.00003595
Iteration 115/1000 | Loss: 0.00003595
Iteration 116/1000 | Loss: 0.00003595
Iteration 117/1000 | Loss: 0.00003595
Iteration 118/1000 | Loss: 0.00003595
Iteration 119/1000 | Loss: 0.00003595
Iteration 120/1000 | Loss: 0.00003595
Iteration 121/1000 | Loss: 0.00003595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.59546575054992e-05, 3.59546575054992e-05, 3.59546575054992e-05, 3.59546575054992e-05, 3.59546575054992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.59546575054992e-05

Optimization complete. Final v2v error: 4.9380574226379395 mm

Highest mean error: 5.4194254875183105 mm for frame 18

Lowest mean error: 4.408869743347168 mm for frame 30

Saving results

Total time: 31.900017499923706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888830
Iteration 2/25 | Loss: 0.00126375
Iteration 3/25 | Loss: 0.00112628
Iteration 4/25 | Loss: 0.00110942
Iteration 5/25 | Loss: 0.00110650
Iteration 6/25 | Loss: 0.00110650
Iteration 7/25 | Loss: 0.00110650
Iteration 8/25 | Loss: 0.00110650
Iteration 9/25 | Loss: 0.00110650
Iteration 10/25 | Loss: 0.00110650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011065020225942135, 0.0011065020225942135, 0.0011065020225942135, 0.0011065020225942135, 0.0011065020225942135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011065020225942135

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31713295
Iteration 2/25 | Loss: 0.00070690
Iteration 3/25 | Loss: 0.00070683
Iteration 4/25 | Loss: 0.00070683
Iteration 5/25 | Loss: 0.00070683
Iteration 6/25 | Loss: 0.00070683
Iteration 7/25 | Loss: 0.00070683
Iteration 8/25 | Loss: 0.00070683
Iteration 9/25 | Loss: 0.00070683
Iteration 10/25 | Loss: 0.00070683
Iteration 11/25 | Loss: 0.00070683
Iteration 12/25 | Loss: 0.00070683
Iteration 13/25 | Loss: 0.00070683
Iteration 14/25 | Loss: 0.00070683
Iteration 15/25 | Loss: 0.00070683
Iteration 16/25 | Loss: 0.00070683
Iteration 17/25 | Loss: 0.00070683
Iteration 18/25 | Loss: 0.00070683
Iteration 19/25 | Loss: 0.00070683
Iteration 20/25 | Loss: 0.00070683
Iteration 21/25 | Loss: 0.00070683
Iteration 22/25 | Loss: 0.00070683
Iteration 23/25 | Loss: 0.00070683
Iteration 24/25 | Loss: 0.00070683
Iteration 25/25 | Loss: 0.00070683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070683
Iteration 2/1000 | Loss: 0.00004582
Iteration 3/1000 | Loss: 0.00002940
Iteration 4/1000 | Loss: 0.00002693
Iteration 5/1000 | Loss: 0.00002583
Iteration 6/1000 | Loss: 0.00002482
Iteration 7/1000 | Loss: 0.00002420
Iteration 8/1000 | Loss: 0.00002386
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002333
Iteration 11/1000 | Loss: 0.00002316
Iteration 12/1000 | Loss: 0.00002313
Iteration 13/1000 | Loss: 0.00002290
Iteration 14/1000 | Loss: 0.00002281
Iteration 15/1000 | Loss: 0.00002280
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002279
Iteration 18/1000 | Loss: 0.00002279
Iteration 19/1000 | Loss: 0.00002267
Iteration 20/1000 | Loss: 0.00002262
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002260
Iteration 23/1000 | Loss: 0.00002260
Iteration 24/1000 | Loss: 0.00002260
Iteration 25/1000 | Loss: 0.00002260
Iteration 26/1000 | Loss: 0.00002260
Iteration 27/1000 | Loss: 0.00002259
Iteration 28/1000 | Loss: 0.00002259
Iteration 29/1000 | Loss: 0.00002258
Iteration 30/1000 | Loss: 0.00002258
Iteration 31/1000 | Loss: 0.00002258
Iteration 32/1000 | Loss: 0.00002257
Iteration 33/1000 | Loss: 0.00002254
Iteration 34/1000 | Loss: 0.00002254
Iteration 35/1000 | Loss: 0.00002254
Iteration 36/1000 | Loss: 0.00002253
Iteration 37/1000 | Loss: 0.00002252
Iteration 38/1000 | Loss: 0.00002251
Iteration 39/1000 | Loss: 0.00002250
Iteration 40/1000 | Loss: 0.00002249
Iteration 41/1000 | Loss: 0.00002249
Iteration 42/1000 | Loss: 0.00002249
Iteration 43/1000 | Loss: 0.00002249
Iteration 44/1000 | Loss: 0.00002249
Iteration 45/1000 | Loss: 0.00002249
Iteration 46/1000 | Loss: 0.00002249
Iteration 47/1000 | Loss: 0.00002248
Iteration 48/1000 | Loss: 0.00002247
Iteration 49/1000 | Loss: 0.00002246
Iteration 50/1000 | Loss: 0.00002246
Iteration 51/1000 | Loss: 0.00002245
Iteration 52/1000 | Loss: 0.00002245
Iteration 53/1000 | Loss: 0.00002245
Iteration 54/1000 | Loss: 0.00002245
Iteration 55/1000 | Loss: 0.00002245
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002244
Iteration 58/1000 | Loss: 0.00002244
Iteration 59/1000 | Loss: 0.00002244
Iteration 60/1000 | Loss: 0.00002244
Iteration 61/1000 | Loss: 0.00002244
Iteration 62/1000 | Loss: 0.00002244
Iteration 63/1000 | Loss: 0.00002244
Iteration 64/1000 | Loss: 0.00002244
Iteration 65/1000 | Loss: 0.00002244
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002243
Iteration 69/1000 | Loss: 0.00002243
Iteration 70/1000 | Loss: 0.00002243
Iteration 71/1000 | Loss: 0.00002242
Iteration 72/1000 | Loss: 0.00002242
Iteration 73/1000 | Loss: 0.00002242
Iteration 74/1000 | Loss: 0.00002242
Iteration 75/1000 | Loss: 0.00002241
Iteration 76/1000 | Loss: 0.00002241
Iteration 77/1000 | Loss: 0.00002241
Iteration 78/1000 | Loss: 0.00002241
Iteration 79/1000 | Loss: 0.00002241
Iteration 80/1000 | Loss: 0.00002241
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00002240
Iteration 84/1000 | Loss: 0.00002240
Iteration 85/1000 | Loss: 0.00002240
Iteration 86/1000 | Loss: 0.00002240
Iteration 87/1000 | Loss: 0.00002239
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002238
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002238
Iteration 94/1000 | Loss: 0.00002238
Iteration 95/1000 | Loss: 0.00002238
Iteration 96/1000 | Loss: 0.00002238
Iteration 97/1000 | Loss: 0.00002238
Iteration 98/1000 | Loss: 0.00002238
Iteration 99/1000 | Loss: 0.00002238
Iteration 100/1000 | Loss: 0.00002237
Iteration 101/1000 | Loss: 0.00002237
Iteration 102/1000 | Loss: 0.00002237
Iteration 103/1000 | Loss: 0.00002237
Iteration 104/1000 | Loss: 0.00002237
Iteration 105/1000 | Loss: 0.00002236
Iteration 106/1000 | Loss: 0.00002236
Iteration 107/1000 | Loss: 0.00002236
Iteration 108/1000 | Loss: 0.00002236
Iteration 109/1000 | Loss: 0.00002236
Iteration 110/1000 | Loss: 0.00002235
Iteration 111/1000 | Loss: 0.00002235
Iteration 112/1000 | Loss: 0.00002235
Iteration 113/1000 | Loss: 0.00002235
Iteration 114/1000 | Loss: 0.00002235
Iteration 115/1000 | Loss: 0.00002235
Iteration 116/1000 | Loss: 0.00002235
Iteration 117/1000 | Loss: 0.00002235
Iteration 118/1000 | Loss: 0.00002234
Iteration 119/1000 | Loss: 0.00002234
Iteration 120/1000 | Loss: 0.00002234
Iteration 121/1000 | Loss: 0.00002234
Iteration 122/1000 | Loss: 0.00002234
Iteration 123/1000 | Loss: 0.00002233
Iteration 124/1000 | Loss: 0.00002233
Iteration 125/1000 | Loss: 0.00002233
Iteration 126/1000 | Loss: 0.00002233
Iteration 127/1000 | Loss: 0.00002233
Iteration 128/1000 | Loss: 0.00002233
Iteration 129/1000 | Loss: 0.00002233
Iteration 130/1000 | Loss: 0.00002233
Iteration 131/1000 | Loss: 0.00002233
Iteration 132/1000 | Loss: 0.00002233
Iteration 133/1000 | Loss: 0.00002233
Iteration 134/1000 | Loss: 0.00002233
Iteration 135/1000 | Loss: 0.00002232
Iteration 136/1000 | Loss: 0.00002232
Iteration 137/1000 | Loss: 0.00002232
Iteration 138/1000 | Loss: 0.00002232
Iteration 139/1000 | Loss: 0.00002231
Iteration 140/1000 | Loss: 0.00002231
Iteration 141/1000 | Loss: 0.00002231
Iteration 142/1000 | Loss: 0.00002231
Iteration 143/1000 | Loss: 0.00002231
Iteration 144/1000 | Loss: 0.00002231
Iteration 145/1000 | Loss: 0.00002231
Iteration 146/1000 | Loss: 0.00002231
Iteration 147/1000 | Loss: 0.00002231
Iteration 148/1000 | Loss: 0.00002231
Iteration 149/1000 | Loss: 0.00002231
Iteration 150/1000 | Loss: 0.00002231
Iteration 151/1000 | Loss: 0.00002231
Iteration 152/1000 | Loss: 0.00002231
Iteration 153/1000 | Loss: 0.00002231
Iteration 154/1000 | Loss: 0.00002231
Iteration 155/1000 | Loss: 0.00002231
Iteration 156/1000 | Loss: 0.00002231
Iteration 157/1000 | Loss: 0.00002231
Iteration 158/1000 | Loss: 0.00002231
Iteration 159/1000 | Loss: 0.00002231
Iteration 160/1000 | Loss: 0.00002231
Iteration 161/1000 | Loss: 0.00002231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.230541213066317e-05, 2.230541213066317e-05, 2.230541213066317e-05, 2.230541213066317e-05, 2.230541213066317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.230541213066317e-05

Optimization complete. Final v2v error: 3.8828377723693848 mm

Highest mean error: 4.207446575164795 mm for frame 149

Lowest mean error: 3.3380041122436523 mm for frame 30

Saving results

Total time: 44.198347330093384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096013
Iteration 2/25 | Loss: 0.01096013
Iteration 3/25 | Loss: 0.00345747
Iteration 4/25 | Loss: 0.00279834
Iteration 5/25 | Loss: 0.00239831
Iteration 6/25 | Loss: 0.00231837
Iteration 7/25 | Loss: 0.00212732
Iteration 8/25 | Loss: 0.00201534
Iteration 9/25 | Loss: 0.00196225
Iteration 10/25 | Loss: 0.00182813
Iteration 11/25 | Loss: 0.00175389
Iteration 12/25 | Loss: 0.00166441
Iteration 13/25 | Loss: 0.00160223
Iteration 14/25 | Loss: 0.00153460
Iteration 15/25 | Loss: 0.00148335
Iteration 16/25 | Loss: 0.00146560
Iteration 17/25 | Loss: 0.00144945
Iteration 18/25 | Loss: 0.00142981
Iteration 19/25 | Loss: 0.00140214
Iteration 20/25 | Loss: 0.00137819
Iteration 21/25 | Loss: 0.00137381
Iteration 22/25 | Loss: 0.00136162
Iteration 23/25 | Loss: 0.00136051
Iteration 24/25 | Loss: 0.00135993
Iteration 25/25 | Loss: 0.00136635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18314135
Iteration 2/25 | Loss: 0.00349945
Iteration 3/25 | Loss: 0.00345461
Iteration 4/25 | Loss: 0.00345461
Iteration 5/25 | Loss: 0.00345461
Iteration 6/25 | Loss: 0.00345461
Iteration 7/25 | Loss: 0.00345460
Iteration 8/25 | Loss: 0.00345460
Iteration 9/25 | Loss: 0.00345460
Iteration 10/25 | Loss: 0.00345460
Iteration 11/25 | Loss: 0.00345460
Iteration 12/25 | Loss: 0.00345460
Iteration 13/25 | Loss: 0.00345460
Iteration 14/25 | Loss: 0.00345460
Iteration 15/25 | Loss: 0.00345460
Iteration 16/25 | Loss: 0.00345460
Iteration 17/25 | Loss: 0.00345460
Iteration 18/25 | Loss: 0.00345460
Iteration 19/25 | Loss: 0.00345460
Iteration 20/25 | Loss: 0.00345460
Iteration 21/25 | Loss: 0.00345460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.003454602090641856, 0.003454602090641856, 0.003454602090641856, 0.003454602090641856, 0.003454602090641856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003454602090641856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345460
Iteration 2/1000 | Loss: 0.00074174
Iteration 3/1000 | Loss: 0.00067880
Iteration 4/1000 | Loss: 0.00038120
Iteration 5/1000 | Loss: 0.00058004
Iteration 6/1000 | Loss: 0.00047086
Iteration 7/1000 | Loss: 0.00045209
Iteration 8/1000 | Loss: 0.00032833
Iteration 9/1000 | Loss: 0.00108204
Iteration 10/1000 | Loss: 0.00058601
Iteration 11/1000 | Loss: 0.00038769
Iteration 12/1000 | Loss: 0.00049817
Iteration 13/1000 | Loss: 0.00034817
Iteration 14/1000 | Loss: 0.00047581
Iteration 15/1000 | Loss: 0.00048467
Iteration 16/1000 | Loss: 0.00055564
Iteration 17/1000 | Loss: 0.00047130
Iteration 18/1000 | Loss: 0.00050856
Iteration 19/1000 | Loss: 0.00037042
Iteration 20/1000 | Loss: 0.00044733
Iteration 21/1000 | Loss: 0.00047390
Iteration 22/1000 | Loss: 0.00056233
Iteration 23/1000 | Loss: 0.00050167
Iteration 24/1000 | Loss: 0.00054437
Iteration 25/1000 | Loss: 0.00015966
Iteration 26/1000 | Loss: 0.00021912
Iteration 27/1000 | Loss: 0.00019634
Iteration 28/1000 | Loss: 0.00029636
Iteration 29/1000 | Loss: 0.00042143
Iteration 30/1000 | Loss: 0.00031008
Iteration 31/1000 | Loss: 0.00022669
Iteration 32/1000 | Loss: 0.00038553
Iteration 33/1000 | Loss: 0.00066161
Iteration 34/1000 | Loss: 0.00045115
Iteration 35/1000 | Loss: 0.00057899
Iteration 36/1000 | Loss: 0.00040359
Iteration 37/1000 | Loss: 0.00024921
Iteration 38/1000 | Loss: 0.00033811
Iteration 39/1000 | Loss: 0.00038179
Iteration 40/1000 | Loss: 0.00039629
Iteration 41/1000 | Loss: 0.00030842
Iteration 42/1000 | Loss: 0.00050589
Iteration 43/1000 | Loss: 0.00068804
Iteration 44/1000 | Loss: 0.00028910
Iteration 45/1000 | Loss: 0.00034458
Iteration 46/1000 | Loss: 0.00033610
Iteration 47/1000 | Loss: 0.00065285
Iteration 48/1000 | Loss: 0.00090581
Iteration 49/1000 | Loss: 0.00039534
Iteration 50/1000 | Loss: 0.00055338
Iteration 51/1000 | Loss: 0.00054154
Iteration 52/1000 | Loss: 0.00058664
Iteration 53/1000 | Loss: 0.00026641
Iteration 54/1000 | Loss: 0.00026495
Iteration 55/1000 | Loss: 0.00042095
Iteration 56/1000 | Loss: 0.00109992
Iteration 57/1000 | Loss: 0.00054159
Iteration 58/1000 | Loss: 0.00055452
Iteration 59/1000 | Loss: 0.00046144
Iteration 60/1000 | Loss: 0.00016282
Iteration 61/1000 | Loss: 0.00067830
Iteration 62/1000 | Loss: 0.00055527
Iteration 63/1000 | Loss: 0.00042010
Iteration 64/1000 | Loss: 0.00043098
Iteration 65/1000 | Loss: 0.00031532
Iteration 66/1000 | Loss: 0.00032132
Iteration 67/1000 | Loss: 0.00055555
Iteration 68/1000 | Loss: 0.00019761
Iteration 69/1000 | Loss: 0.00071620
Iteration 70/1000 | Loss: 0.00029094
Iteration 71/1000 | Loss: 0.00045157
Iteration 72/1000 | Loss: 0.00026389
Iteration 73/1000 | Loss: 0.00049539
Iteration 74/1000 | Loss: 0.00034859
Iteration 75/1000 | Loss: 0.00088683
Iteration 76/1000 | Loss: 0.00215869
Iteration 77/1000 | Loss: 0.00205441
Iteration 78/1000 | Loss: 0.00067015
Iteration 79/1000 | Loss: 0.00035666
Iteration 80/1000 | Loss: 0.00013030
Iteration 81/1000 | Loss: 0.00021345
Iteration 82/1000 | Loss: 0.00061923
Iteration 83/1000 | Loss: 0.00046940
Iteration 84/1000 | Loss: 0.00044128
Iteration 85/1000 | Loss: 0.00056157
Iteration 86/1000 | Loss: 0.00037206
Iteration 87/1000 | Loss: 0.00040343
Iteration 88/1000 | Loss: 0.00020566
Iteration 89/1000 | Loss: 0.00027501
Iteration 90/1000 | Loss: 0.00013171
Iteration 91/1000 | Loss: 0.00049751
Iteration 92/1000 | Loss: 0.00032797
Iteration 93/1000 | Loss: 0.00009001
Iteration 94/1000 | Loss: 0.00023554
Iteration 95/1000 | Loss: 0.00008395
Iteration 96/1000 | Loss: 0.00036935
Iteration 97/1000 | Loss: 0.00079152
Iteration 98/1000 | Loss: 0.00050593
Iteration 99/1000 | Loss: 0.00027949
Iteration 100/1000 | Loss: 0.00026160
Iteration 101/1000 | Loss: 0.00014993
Iteration 102/1000 | Loss: 0.00010751
Iteration 103/1000 | Loss: 0.00007238
Iteration 104/1000 | Loss: 0.00010157
Iteration 105/1000 | Loss: 0.00006862
Iteration 106/1000 | Loss: 0.00034094
Iteration 107/1000 | Loss: 0.00040912
Iteration 108/1000 | Loss: 0.00027252
Iteration 109/1000 | Loss: 0.00024631
Iteration 110/1000 | Loss: 0.00008265
Iteration 111/1000 | Loss: 0.00007483
Iteration 112/1000 | Loss: 0.00036934
Iteration 113/1000 | Loss: 0.00078240
Iteration 114/1000 | Loss: 0.00031568
Iteration 115/1000 | Loss: 0.00047801
Iteration 116/1000 | Loss: 0.00010792
Iteration 117/1000 | Loss: 0.00023148
Iteration 118/1000 | Loss: 0.00024492
Iteration 119/1000 | Loss: 0.00007498
Iteration 120/1000 | Loss: 0.00052904
Iteration 121/1000 | Loss: 0.00029587
Iteration 122/1000 | Loss: 0.00061789
Iteration 123/1000 | Loss: 0.00032575
Iteration 124/1000 | Loss: 0.00032210
Iteration 125/1000 | Loss: 0.00037755
Iteration 126/1000 | Loss: 0.00040509
Iteration 127/1000 | Loss: 0.00035603
Iteration 128/1000 | Loss: 0.00018147
Iteration 129/1000 | Loss: 0.00017564
Iteration 130/1000 | Loss: 0.00008740
Iteration 131/1000 | Loss: 0.00020112
Iteration 132/1000 | Loss: 0.00017965
Iteration 133/1000 | Loss: 0.00007129
Iteration 134/1000 | Loss: 0.00007023
Iteration 135/1000 | Loss: 0.00008016
Iteration 136/1000 | Loss: 0.00005364
Iteration 137/1000 | Loss: 0.00006300
Iteration 138/1000 | Loss: 0.00005179
Iteration 139/1000 | Loss: 0.00019790
Iteration 140/1000 | Loss: 0.00008843
Iteration 141/1000 | Loss: 0.00006146
Iteration 142/1000 | Loss: 0.00009664
Iteration 143/1000 | Loss: 0.00006460
Iteration 144/1000 | Loss: 0.00007755
Iteration 145/1000 | Loss: 0.00018580
Iteration 146/1000 | Loss: 0.00019348
Iteration 147/1000 | Loss: 0.00018293
Iteration 148/1000 | Loss: 0.00017450
Iteration 149/1000 | Loss: 0.00018537
Iteration 150/1000 | Loss: 0.00004921
Iteration 151/1000 | Loss: 0.00006028
Iteration 152/1000 | Loss: 0.00004452
Iteration 153/1000 | Loss: 0.00005556
Iteration 154/1000 | Loss: 0.00004975
Iteration 155/1000 | Loss: 0.00005838
Iteration 156/1000 | Loss: 0.00005205
Iteration 157/1000 | Loss: 0.00005327
Iteration 158/1000 | Loss: 0.00004986
Iteration 159/1000 | Loss: 0.00005254
Iteration 160/1000 | Loss: 0.00073800
Iteration 161/1000 | Loss: 0.00059402
Iteration 162/1000 | Loss: 0.00052319
Iteration 163/1000 | Loss: 0.00006640
Iteration 164/1000 | Loss: 0.00006184
Iteration 165/1000 | Loss: 0.00004488
Iteration 166/1000 | Loss: 0.00016565
Iteration 167/1000 | Loss: 0.00007100
Iteration 168/1000 | Loss: 0.00004138
Iteration 169/1000 | Loss: 0.00004042
Iteration 170/1000 | Loss: 0.00020321
Iteration 171/1000 | Loss: 0.00016718
Iteration 172/1000 | Loss: 0.00014809
Iteration 173/1000 | Loss: 0.00015170
Iteration 174/1000 | Loss: 0.00016880
Iteration 175/1000 | Loss: 0.00022302
Iteration 176/1000 | Loss: 0.00011629
Iteration 177/1000 | Loss: 0.00004656
Iteration 178/1000 | Loss: 0.00005095
Iteration 179/1000 | Loss: 0.00004261
Iteration 180/1000 | Loss: 0.00005766
Iteration 181/1000 | Loss: 0.00004448
Iteration 182/1000 | Loss: 0.00005775
Iteration 183/1000 | Loss: 0.00067440
Iteration 184/1000 | Loss: 0.00076951
Iteration 185/1000 | Loss: 0.00068485
Iteration 186/1000 | Loss: 0.00029696
Iteration 187/1000 | Loss: 0.00005041
Iteration 188/1000 | Loss: 0.00010648
Iteration 189/1000 | Loss: 0.00012754
Iteration 190/1000 | Loss: 0.00010116
Iteration 191/1000 | Loss: 0.00004736
Iteration 192/1000 | Loss: 0.00004308
Iteration 193/1000 | Loss: 0.00004823
Iteration 194/1000 | Loss: 0.00021622
Iteration 195/1000 | Loss: 0.00010454
Iteration 196/1000 | Loss: 0.00008160
Iteration 197/1000 | Loss: 0.00015941
Iteration 198/1000 | Loss: 0.00004444
Iteration 199/1000 | Loss: 0.00005663
Iteration 200/1000 | Loss: 0.00026074
Iteration 201/1000 | Loss: 0.00010551
Iteration 202/1000 | Loss: 0.00011820
Iteration 203/1000 | Loss: 0.00006422
Iteration 204/1000 | Loss: 0.00017990
Iteration 205/1000 | Loss: 0.00005368
Iteration 206/1000 | Loss: 0.00010622
Iteration 207/1000 | Loss: 0.00017664
Iteration 208/1000 | Loss: 0.00011002
Iteration 209/1000 | Loss: 0.00004516
Iteration 210/1000 | Loss: 0.00004218
Iteration 211/1000 | Loss: 0.00003952
Iteration 212/1000 | Loss: 0.00003833
Iteration 213/1000 | Loss: 0.00003725
Iteration 214/1000 | Loss: 0.00019382
Iteration 215/1000 | Loss: 0.00005570
Iteration 216/1000 | Loss: 0.00005488
Iteration 217/1000 | Loss: 0.00004681
Iteration 218/1000 | Loss: 0.00003989
Iteration 219/1000 | Loss: 0.00004295
Iteration 220/1000 | Loss: 0.00003632
Iteration 221/1000 | Loss: 0.00003641
Iteration 222/1000 | Loss: 0.00004353
Iteration 223/1000 | Loss: 0.00003564
Iteration 224/1000 | Loss: 0.00004223
Iteration 225/1000 | Loss: 0.00004935
Iteration 226/1000 | Loss: 0.00003488
Iteration 227/1000 | Loss: 0.00003415
Iteration 228/1000 | Loss: 0.00003816
Iteration 229/1000 | Loss: 0.00003432
Iteration 230/1000 | Loss: 0.00003350
Iteration 231/1000 | Loss: 0.00003954
Iteration 232/1000 | Loss: 0.00003432
Iteration 233/1000 | Loss: 0.00003371
Iteration 234/1000 | Loss: 0.00004514
Iteration 235/1000 | Loss: 0.00004455
Iteration 236/1000 | Loss: 0.00004396
Iteration 237/1000 | Loss: 0.00004613
Iteration 238/1000 | Loss: 0.00004323
Iteration 239/1000 | Loss: 0.00004486
Iteration 240/1000 | Loss: 0.00004184
Iteration 241/1000 | Loss: 0.00004161
Iteration 242/1000 | Loss: 0.00009149
Iteration 243/1000 | Loss: 0.00005158
Iteration 244/1000 | Loss: 0.00004094
Iteration 245/1000 | Loss: 0.00004066
Iteration 246/1000 | Loss: 0.00003959
Iteration 247/1000 | Loss: 0.00004327
Iteration 248/1000 | Loss: 0.00004238
Iteration 249/1000 | Loss: 0.00004552
Iteration 250/1000 | Loss: 0.00004232
Iteration 251/1000 | Loss: 0.00003631
Iteration 252/1000 | Loss: 0.00005575
Iteration 253/1000 | Loss: 0.00004343
Iteration 254/1000 | Loss: 0.00003477
Iteration 255/1000 | Loss: 0.00003895
Iteration 256/1000 | Loss: 0.00004541
Iteration 257/1000 | Loss: 0.00004507
Iteration 258/1000 | Loss: 0.00004345
Iteration 259/1000 | Loss: 0.00004240
Iteration 260/1000 | Loss: 0.00004967
Iteration 261/1000 | Loss: 0.00005868
Iteration 262/1000 | Loss: 0.00004282
Iteration 263/1000 | Loss: 0.00004041
Iteration 264/1000 | Loss: 0.00004624
Iteration 265/1000 | Loss: 0.00004408
Iteration 266/1000 | Loss: 0.00003688
Iteration 267/1000 | Loss: 0.00004242
Iteration 268/1000 | Loss: 0.00004443
Iteration 269/1000 | Loss: 0.00004857
Iteration 270/1000 | Loss: 0.00003947
Iteration 271/1000 | Loss: 0.00004537
Iteration 272/1000 | Loss: 0.00004243
Iteration 273/1000 | Loss: 0.00005418
Iteration 274/1000 | Loss: 0.00004354
Iteration 275/1000 | Loss: 0.00004056
Iteration 276/1000 | Loss: 0.00005054
Iteration 277/1000 | Loss: 0.00004674
Iteration 278/1000 | Loss: 0.00004078
Iteration 279/1000 | Loss: 0.00003845
Iteration 280/1000 | Loss: 0.00004262
Iteration 281/1000 | Loss: 0.00004809
Iteration 282/1000 | Loss: 0.00004352
Iteration 283/1000 | Loss: 0.00003819
Iteration 284/1000 | Loss: 0.00004102
Iteration 285/1000 | Loss: 0.00003502
Iteration 286/1000 | Loss: 0.00003952
Iteration 287/1000 | Loss: 0.00003261
Iteration 288/1000 | Loss: 0.00003195
Iteration 289/1000 | Loss: 0.00003246
Iteration 290/1000 | Loss: 0.00003191
Iteration 291/1000 | Loss: 0.00003157
Iteration 292/1000 | Loss: 0.00003157
Iteration 293/1000 | Loss: 0.00003157
Iteration 294/1000 | Loss: 0.00003156
Iteration 295/1000 | Loss: 0.00003156
Iteration 296/1000 | Loss: 0.00003156
Iteration 297/1000 | Loss: 0.00003156
Iteration 298/1000 | Loss: 0.00003156
Iteration 299/1000 | Loss: 0.00003156
Iteration 300/1000 | Loss: 0.00003156
Iteration 301/1000 | Loss: 0.00003156
Iteration 302/1000 | Loss: 0.00003156
Iteration 303/1000 | Loss: 0.00003156
Iteration 304/1000 | Loss: 0.00003155
Iteration 305/1000 | Loss: 0.00003155
Iteration 306/1000 | Loss: 0.00003154
Iteration 307/1000 | Loss: 0.00003154
Iteration 308/1000 | Loss: 0.00003154
Iteration 309/1000 | Loss: 0.00003154
Iteration 310/1000 | Loss: 0.00003153
Iteration 311/1000 | Loss: 0.00003153
Iteration 312/1000 | Loss: 0.00003153
Iteration 313/1000 | Loss: 0.00003153
Iteration 314/1000 | Loss: 0.00003153
Iteration 315/1000 | Loss: 0.00003153
Iteration 316/1000 | Loss: 0.00003153
Iteration 317/1000 | Loss: 0.00003153
Iteration 318/1000 | Loss: 0.00003153
Iteration 319/1000 | Loss: 0.00003153
Iteration 320/1000 | Loss: 0.00003153
Iteration 321/1000 | Loss: 0.00003153
Iteration 322/1000 | Loss: 0.00003153
Iteration 323/1000 | Loss: 0.00003152
Iteration 324/1000 | Loss: 0.00003152
Iteration 325/1000 | Loss: 0.00003152
Iteration 326/1000 | Loss: 0.00003152
Iteration 327/1000 | Loss: 0.00003152
Iteration 328/1000 | Loss: 0.00003152
Iteration 329/1000 | Loss: 0.00003152
Iteration 330/1000 | Loss: 0.00003151
Iteration 331/1000 | Loss: 0.00003151
Iteration 332/1000 | Loss: 0.00003151
Iteration 333/1000 | Loss: 0.00003151
Iteration 334/1000 | Loss: 0.00003150
Iteration 335/1000 | Loss: 0.00003150
Iteration 336/1000 | Loss: 0.00003150
Iteration 337/1000 | Loss: 0.00003150
Iteration 338/1000 | Loss: 0.00003150
Iteration 339/1000 | Loss: 0.00003150
Iteration 340/1000 | Loss: 0.00003150
Iteration 341/1000 | Loss: 0.00003150
Iteration 342/1000 | Loss: 0.00003149
Iteration 343/1000 | Loss: 0.00003149
Iteration 344/1000 | Loss: 0.00003149
Iteration 345/1000 | Loss: 0.00003149
Iteration 346/1000 | Loss: 0.00003149
Iteration 347/1000 | Loss: 0.00003149
Iteration 348/1000 | Loss: 0.00003148
Iteration 349/1000 | Loss: 0.00003148
Iteration 350/1000 | Loss: 0.00003148
Iteration 351/1000 | Loss: 0.00003147
Iteration 352/1000 | Loss: 0.00003147
Iteration 353/1000 | Loss: 0.00003147
Iteration 354/1000 | Loss: 0.00003146
Iteration 355/1000 | Loss: 0.00003146
Iteration 356/1000 | Loss: 0.00003146
Iteration 357/1000 | Loss: 0.00003145
Iteration 358/1000 | Loss: 0.00003145
Iteration 359/1000 | Loss: 0.00003145
Iteration 360/1000 | Loss: 0.00003145
Iteration 361/1000 | Loss: 0.00003144
Iteration 362/1000 | Loss: 0.00003144
Iteration 363/1000 | Loss: 0.00003144
Iteration 364/1000 | Loss: 0.00003144
Iteration 365/1000 | Loss: 0.00003144
Iteration 366/1000 | Loss: 0.00003143
Iteration 367/1000 | Loss: 0.00003143
Iteration 368/1000 | Loss: 0.00003143
Iteration 369/1000 | Loss: 0.00003142
Iteration 370/1000 | Loss: 0.00005117
Iteration 371/1000 | Loss: 0.00003159
Iteration 372/1000 | Loss: 0.00003141
Iteration 373/1000 | Loss: 0.00003141
Iteration 374/1000 | Loss: 0.00003136
Iteration 375/1000 | Loss: 0.00003135
Iteration 376/1000 | Loss: 0.00003135
Iteration 377/1000 | Loss: 0.00003135
Iteration 378/1000 | Loss: 0.00003135
Iteration 379/1000 | Loss: 0.00003135
Iteration 380/1000 | Loss: 0.00003135
Iteration 381/1000 | Loss: 0.00003135
Iteration 382/1000 | Loss: 0.00003135
Iteration 383/1000 | Loss: 0.00003135
Iteration 384/1000 | Loss: 0.00003135
Iteration 385/1000 | Loss: 0.00003135
Iteration 386/1000 | Loss: 0.00003134
Iteration 387/1000 | Loss: 0.00003134
Iteration 388/1000 | Loss: 0.00003134
Iteration 389/1000 | Loss: 0.00003134
Iteration 390/1000 | Loss: 0.00003134
Iteration 391/1000 | Loss: 0.00003133
Iteration 392/1000 | Loss: 0.00003133
Iteration 393/1000 | Loss: 0.00003133
Iteration 394/1000 | Loss: 0.00003133
Iteration 395/1000 | Loss: 0.00003133
Iteration 396/1000 | Loss: 0.00003133
Iteration 397/1000 | Loss: 0.00003133
Iteration 398/1000 | Loss: 0.00003132
Iteration 399/1000 | Loss: 0.00003132
Iteration 400/1000 | Loss: 0.00003132
Iteration 401/1000 | Loss: 0.00003132
Iteration 402/1000 | Loss: 0.00003132
Iteration 403/1000 | Loss: 0.00003132
Iteration 404/1000 | Loss: 0.00003132
Iteration 405/1000 | Loss: 0.00003132
Iteration 406/1000 | Loss: 0.00003132
Iteration 407/1000 | Loss: 0.00003132
Iteration 408/1000 | Loss: 0.00003132
Iteration 409/1000 | Loss: 0.00003132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [3.1320148991653696e-05, 3.1320148991653696e-05, 3.1320148991653696e-05, 3.1320148991653696e-05, 3.1320148991653696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1320148991653696e-05

Optimization complete. Final v2v error: 3.978766441345215 mm

Highest mean error: 10.830885887145996 mm for frame 119

Lowest mean error: 3.2659237384796143 mm for frame 19

Saving results

Total time: 530.5029671192169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767373
Iteration 2/25 | Loss: 0.00122773
Iteration 3/25 | Loss: 0.00106183
Iteration 4/25 | Loss: 0.00103364
Iteration 5/25 | Loss: 0.00102672
Iteration 6/25 | Loss: 0.00102545
Iteration 7/25 | Loss: 0.00102545
Iteration 8/25 | Loss: 0.00102545
Iteration 9/25 | Loss: 0.00102545
Iteration 10/25 | Loss: 0.00102545
Iteration 11/25 | Loss: 0.00102545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010254534427076578, 0.0010254534427076578, 0.0010254534427076578, 0.0010254534427076578, 0.0010254534427076578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010254534427076578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42950571
Iteration 2/25 | Loss: 0.00083132
Iteration 3/25 | Loss: 0.00083132
Iteration 4/25 | Loss: 0.00083132
Iteration 5/25 | Loss: 0.00083132
Iteration 6/25 | Loss: 0.00083132
Iteration 7/25 | Loss: 0.00083132
Iteration 8/25 | Loss: 0.00083132
Iteration 9/25 | Loss: 0.00083132
Iteration 10/25 | Loss: 0.00083132
Iteration 11/25 | Loss: 0.00083132
Iteration 12/25 | Loss: 0.00083132
Iteration 13/25 | Loss: 0.00083132
Iteration 14/25 | Loss: 0.00083132
Iteration 15/25 | Loss: 0.00083132
Iteration 16/25 | Loss: 0.00083132
Iteration 17/25 | Loss: 0.00083132
Iteration 18/25 | Loss: 0.00083132
Iteration 19/25 | Loss: 0.00083132
Iteration 20/25 | Loss: 0.00083132
Iteration 21/25 | Loss: 0.00083132
Iteration 22/25 | Loss: 0.00083132
Iteration 23/25 | Loss: 0.00083132
Iteration 24/25 | Loss: 0.00083132
Iteration 25/25 | Loss: 0.00083132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083132
Iteration 2/1000 | Loss: 0.00003007
Iteration 3/1000 | Loss: 0.00002024
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001498
Iteration 6/1000 | Loss: 0.00001423
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001342
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001297
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001259
Iteration 14/1000 | Loss: 0.00001257
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001248
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001244
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001228
Iteration 36/1000 | Loss: 0.00001228
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001221
Iteration 48/1000 | Loss: 0.00001220
Iteration 49/1000 | Loss: 0.00001220
Iteration 50/1000 | Loss: 0.00001219
Iteration 51/1000 | Loss: 0.00001218
Iteration 52/1000 | Loss: 0.00001218
Iteration 53/1000 | Loss: 0.00001218
Iteration 54/1000 | Loss: 0.00001218
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001216
Iteration 58/1000 | Loss: 0.00001216
Iteration 59/1000 | Loss: 0.00001216
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001215
Iteration 64/1000 | Loss: 0.00001215
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001213
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001212
Iteration 75/1000 | Loss: 0.00001212
Iteration 76/1000 | Loss: 0.00001212
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001211
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001208
Iteration 100/1000 | Loss: 0.00001208
Iteration 101/1000 | Loss: 0.00001208
Iteration 102/1000 | Loss: 0.00001208
Iteration 103/1000 | Loss: 0.00001208
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001205
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001202
Iteration 139/1000 | Loss: 0.00001202
Iteration 140/1000 | Loss: 0.00001202
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001202
Iteration 148/1000 | Loss: 0.00001202
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001201
Iteration 152/1000 | Loss: 0.00001201
Iteration 153/1000 | Loss: 0.00001201
Iteration 154/1000 | Loss: 0.00001201
Iteration 155/1000 | Loss: 0.00001201
Iteration 156/1000 | Loss: 0.00001201
Iteration 157/1000 | Loss: 0.00001201
Iteration 158/1000 | Loss: 0.00001201
Iteration 159/1000 | Loss: 0.00001201
Iteration 160/1000 | Loss: 0.00001201
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001200
Iteration 165/1000 | Loss: 0.00001200
Iteration 166/1000 | Loss: 0.00001200
Iteration 167/1000 | Loss: 0.00001200
Iteration 168/1000 | Loss: 0.00001200
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001200
Iteration 171/1000 | Loss: 0.00001200
Iteration 172/1000 | Loss: 0.00001200
Iteration 173/1000 | Loss: 0.00001200
Iteration 174/1000 | Loss: 0.00001200
Iteration 175/1000 | Loss: 0.00001200
Iteration 176/1000 | Loss: 0.00001200
Iteration 177/1000 | Loss: 0.00001200
Iteration 178/1000 | Loss: 0.00001199
Iteration 179/1000 | Loss: 0.00001199
Iteration 180/1000 | Loss: 0.00001199
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001199
Iteration 185/1000 | Loss: 0.00001199
Iteration 186/1000 | Loss: 0.00001199
Iteration 187/1000 | Loss: 0.00001199
Iteration 188/1000 | Loss: 0.00001199
Iteration 189/1000 | Loss: 0.00001198
Iteration 190/1000 | Loss: 0.00001198
Iteration 191/1000 | Loss: 0.00001198
Iteration 192/1000 | Loss: 0.00001198
Iteration 193/1000 | Loss: 0.00001198
Iteration 194/1000 | Loss: 0.00001198
Iteration 195/1000 | Loss: 0.00001198
Iteration 196/1000 | Loss: 0.00001198
Iteration 197/1000 | Loss: 0.00001197
Iteration 198/1000 | Loss: 0.00001197
Iteration 199/1000 | Loss: 0.00001197
Iteration 200/1000 | Loss: 0.00001197
Iteration 201/1000 | Loss: 0.00001197
Iteration 202/1000 | Loss: 0.00001197
Iteration 203/1000 | Loss: 0.00001197
Iteration 204/1000 | Loss: 0.00001197
Iteration 205/1000 | Loss: 0.00001197
Iteration 206/1000 | Loss: 0.00001197
Iteration 207/1000 | Loss: 0.00001197
Iteration 208/1000 | Loss: 0.00001197
Iteration 209/1000 | Loss: 0.00001197
Iteration 210/1000 | Loss: 0.00001197
Iteration 211/1000 | Loss: 0.00001197
Iteration 212/1000 | Loss: 0.00001197
Iteration 213/1000 | Loss: 0.00001197
Iteration 214/1000 | Loss: 0.00001197
Iteration 215/1000 | Loss: 0.00001197
Iteration 216/1000 | Loss: 0.00001197
Iteration 217/1000 | Loss: 0.00001197
Iteration 218/1000 | Loss: 0.00001197
Iteration 219/1000 | Loss: 0.00001197
Iteration 220/1000 | Loss: 0.00001197
Iteration 221/1000 | Loss: 0.00001197
Iteration 222/1000 | Loss: 0.00001197
Iteration 223/1000 | Loss: 0.00001197
Iteration 224/1000 | Loss: 0.00001197
Iteration 225/1000 | Loss: 0.00001197
Iteration 226/1000 | Loss: 0.00001197
Iteration 227/1000 | Loss: 0.00001197
Iteration 228/1000 | Loss: 0.00001197
Iteration 229/1000 | Loss: 0.00001197
Iteration 230/1000 | Loss: 0.00001197
Iteration 231/1000 | Loss: 0.00001197
Iteration 232/1000 | Loss: 0.00001197
Iteration 233/1000 | Loss: 0.00001197
Iteration 234/1000 | Loss: 0.00001197
Iteration 235/1000 | Loss: 0.00001197
Iteration 236/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.1970047125942074e-05, 1.1970047125942074e-05, 1.1970047125942074e-05, 1.1970047125942074e-05, 1.1970047125942074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1970047125942074e-05

Optimization complete. Final v2v error: 2.9381980895996094 mm

Highest mean error: 3.262369394302368 mm for frame 25

Lowest mean error: 2.2971439361572266 mm for frame 168

Saving results

Total time: 46.684383392333984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520399
Iteration 2/25 | Loss: 0.00111644
Iteration 3/25 | Loss: 0.00102431
Iteration 4/25 | Loss: 0.00101129
Iteration 5/25 | Loss: 0.00100998
Iteration 6/25 | Loss: 0.00100998
Iteration 7/25 | Loss: 0.00100998
Iteration 8/25 | Loss: 0.00100998
Iteration 9/25 | Loss: 0.00100998
Iteration 10/25 | Loss: 0.00100998
Iteration 11/25 | Loss: 0.00100998
Iteration 12/25 | Loss: 0.00100998
Iteration 13/25 | Loss: 0.00100998
Iteration 14/25 | Loss: 0.00100998
Iteration 15/25 | Loss: 0.00100998
Iteration 16/25 | Loss: 0.00100998
Iteration 17/25 | Loss: 0.00100998
Iteration 18/25 | Loss: 0.00100998
Iteration 19/25 | Loss: 0.00100998
Iteration 20/25 | Loss: 0.00100998
Iteration 21/25 | Loss: 0.00100998
Iteration 22/25 | Loss: 0.00100998
Iteration 23/25 | Loss: 0.00100998
Iteration 24/25 | Loss: 0.00100998
Iteration 25/25 | Loss: 0.00100998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79653192
Iteration 2/25 | Loss: 0.00062970
Iteration 3/25 | Loss: 0.00062968
Iteration 4/25 | Loss: 0.00062968
Iteration 5/25 | Loss: 0.00062968
Iteration 6/25 | Loss: 0.00062968
Iteration 7/25 | Loss: 0.00062968
Iteration 8/25 | Loss: 0.00062968
Iteration 9/25 | Loss: 0.00062968
Iteration 10/25 | Loss: 0.00062968
Iteration 11/25 | Loss: 0.00062968
Iteration 12/25 | Loss: 0.00062968
Iteration 13/25 | Loss: 0.00062968
Iteration 14/25 | Loss: 0.00062968
Iteration 15/25 | Loss: 0.00062968
Iteration 16/25 | Loss: 0.00062968
Iteration 17/25 | Loss: 0.00062968
Iteration 18/25 | Loss: 0.00062968
Iteration 19/25 | Loss: 0.00062968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000629681337159127, 0.000629681337159127, 0.000629681337159127, 0.000629681337159127, 0.000629681337159127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000629681337159127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062968
Iteration 2/1000 | Loss: 0.00002270
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001450
Iteration 5/1000 | Loss: 0.00001353
Iteration 6/1000 | Loss: 0.00001305
Iteration 7/1000 | Loss: 0.00001271
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001184
Iteration 12/1000 | Loss: 0.00001181
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001176
Iteration 16/1000 | Loss: 0.00001176
Iteration 17/1000 | Loss: 0.00001174
Iteration 18/1000 | Loss: 0.00001174
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001174
Iteration 25/1000 | Loss: 0.00001172
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001169
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001167
Iteration 34/1000 | Loss: 0.00001167
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001166
Iteration 37/1000 | Loss: 0.00001166
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001163
Iteration 48/1000 | Loss: 0.00001163
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Iteration 57/1000 | Loss: 0.00001160
Iteration 58/1000 | Loss: 0.00001160
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001159
Iteration 63/1000 | Loss: 0.00001158
Iteration 64/1000 | Loss: 0.00001158
Iteration 65/1000 | Loss: 0.00001158
Iteration 66/1000 | Loss: 0.00001157
Iteration 67/1000 | Loss: 0.00001157
Iteration 68/1000 | Loss: 0.00001157
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001152
Iteration 108/1000 | Loss: 0.00001152
Iteration 109/1000 | Loss: 0.00001152
Iteration 110/1000 | Loss: 0.00001152
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001151
Iteration 118/1000 | Loss: 0.00001151
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001150
Iteration 139/1000 | Loss: 0.00001150
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001149
Iteration 145/1000 | Loss: 0.00001149
Iteration 146/1000 | Loss: 0.00001149
Iteration 147/1000 | Loss: 0.00001149
Iteration 148/1000 | Loss: 0.00001149
Iteration 149/1000 | Loss: 0.00001149
Iteration 150/1000 | Loss: 0.00001149
Iteration 151/1000 | Loss: 0.00001149
Iteration 152/1000 | Loss: 0.00001149
Iteration 153/1000 | Loss: 0.00001149
Iteration 154/1000 | Loss: 0.00001149
Iteration 155/1000 | Loss: 0.00001148
Iteration 156/1000 | Loss: 0.00001148
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001147
Iteration 163/1000 | Loss: 0.00001147
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001147
Iteration 169/1000 | Loss: 0.00001147
Iteration 170/1000 | Loss: 0.00001147
Iteration 171/1000 | Loss: 0.00001147
Iteration 172/1000 | Loss: 0.00001147
Iteration 173/1000 | Loss: 0.00001147
Iteration 174/1000 | Loss: 0.00001147
Iteration 175/1000 | Loss: 0.00001147
Iteration 176/1000 | Loss: 0.00001147
Iteration 177/1000 | Loss: 0.00001147
Iteration 178/1000 | Loss: 0.00001147
Iteration 179/1000 | Loss: 0.00001147
Iteration 180/1000 | Loss: 0.00001147
Iteration 181/1000 | Loss: 0.00001147
Iteration 182/1000 | Loss: 0.00001147
Iteration 183/1000 | Loss: 0.00001147
Iteration 184/1000 | Loss: 0.00001147
Iteration 185/1000 | Loss: 0.00001147
Iteration 186/1000 | Loss: 0.00001147
Iteration 187/1000 | Loss: 0.00001147
Iteration 188/1000 | Loss: 0.00001147
Iteration 189/1000 | Loss: 0.00001147
Iteration 190/1000 | Loss: 0.00001147
Iteration 191/1000 | Loss: 0.00001147
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001147
Iteration 194/1000 | Loss: 0.00001147
Iteration 195/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1469537639641203e-05, 1.1469537639641203e-05, 1.1469537639641203e-05, 1.1469537639641203e-05, 1.1469537639641203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1469537639641203e-05

Optimization complete. Final v2v error: 2.852071523666382 mm

Highest mean error: 3.1078293323516846 mm for frame 126

Lowest mean error: 2.608471155166626 mm for frame 255

Saving results

Total time: 40.55389857292175
