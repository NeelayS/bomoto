Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=183, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10248-10303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091291
Iteration 2/25 | Loss: 0.00274598
Iteration 3/25 | Loss: 0.00163997
Iteration 4/25 | Loss: 0.00143803
Iteration 5/25 | Loss: 0.00145213
Iteration 6/25 | Loss: 0.00123594
Iteration 7/25 | Loss: 0.00118958
Iteration 8/25 | Loss: 0.00115993
Iteration 9/25 | Loss: 0.00115068
Iteration 10/25 | Loss: 0.00113825
Iteration 11/25 | Loss: 0.00112289
Iteration 12/25 | Loss: 0.00110877
Iteration 13/25 | Loss: 0.00109218
Iteration 14/25 | Loss: 0.00108723
Iteration 15/25 | Loss: 0.00108554
Iteration 16/25 | Loss: 0.00108543
Iteration 17/25 | Loss: 0.00108539
Iteration 18/25 | Loss: 0.00108335
Iteration 19/25 | Loss: 0.00108426
Iteration 20/25 | Loss: 0.00108283
Iteration 21/25 | Loss: 0.00108006
Iteration 22/25 | Loss: 0.00108316
Iteration 23/25 | Loss: 0.00108652
Iteration 24/25 | Loss: 0.00107917
Iteration 25/25 | Loss: 0.00107463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46118748
Iteration 2/25 | Loss: 0.00564017
Iteration 3/25 | Loss: 0.00371417
Iteration 4/25 | Loss: 0.00371417
Iteration 5/25 | Loss: 0.00371417
Iteration 6/25 | Loss: 0.00371417
Iteration 7/25 | Loss: 0.00371417
Iteration 8/25 | Loss: 0.00371417
Iteration 9/25 | Loss: 0.00371417
Iteration 10/25 | Loss: 0.00371417
Iteration 11/25 | Loss: 0.00371417
Iteration 12/25 | Loss: 0.00371417
Iteration 13/25 | Loss: 0.00371417
Iteration 14/25 | Loss: 0.00371417
Iteration 15/25 | Loss: 0.00371417
Iteration 16/25 | Loss: 0.00371417
Iteration 17/25 | Loss: 0.00371417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003714166581630707, 0.003714166581630707, 0.003714166581630707, 0.003714166581630707, 0.003714166581630707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003714166581630707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371417
Iteration 2/1000 | Loss: 0.00811038
Iteration 3/1000 | Loss: 0.00497845
Iteration 4/1000 | Loss: 0.00119489
Iteration 5/1000 | Loss: 0.00272262
Iteration 6/1000 | Loss: 0.00245922
Iteration 7/1000 | Loss: 0.00237633
Iteration 8/1000 | Loss: 0.00228758
Iteration 9/1000 | Loss: 0.00174214
Iteration 10/1000 | Loss: 0.00177922
Iteration 11/1000 | Loss: 0.00056761
Iteration 12/1000 | Loss: 0.00197286
Iteration 13/1000 | Loss: 0.00232316
Iteration 14/1000 | Loss: 0.00022522
Iteration 15/1000 | Loss: 0.00085102
Iteration 16/1000 | Loss: 0.00039039
Iteration 17/1000 | Loss: 0.00064029
Iteration 18/1000 | Loss: 0.00387923
Iteration 19/1000 | Loss: 0.00786304
Iteration 20/1000 | Loss: 0.00666942
Iteration 21/1000 | Loss: 0.00321398
Iteration 22/1000 | Loss: 0.00390961
Iteration 23/1000 | Loss: 0.00452640
Iteration 24/1000 | Loss: 0.00528931
Iteration 25/1000 | Loss: 0.00350332
Iteration 26/1000 | Loss: 0.00157456
Iteration 27/1000 | Loss: 0.00157092
Iteration 28/1000 | Loss: 0.00374798
Iteration 29/1000 | Loss: 0.00056001
Iteration 30/1000 | Loss: 0.00146087
Iteration 31/1000 | Loss: 0.00137063
Iteration 32/1000 | Loss: 0.00186222
Iteration 33/1000 | Loss: 0.00025904
Iteration 34/1000 | Loss: 0.00147594
Iteration 35/1000 | Loss: 0.00082122
Iteration 36/1000 | Loss: 0.00108211
Iteration 37/1000 | Loss: 0.00282953
Iteration 38/1000 | Loss: 0.00122578
Iteration 39/1000 | Loss: 0.00054257
Iteration 40/1000 | Loss: 0.00043471
Iteration 41/1000 | Loss: 0.00070031
Iteration 42/1000 | Loss: 0.00043638
Iteration 43/1000 | Loss: 0.00087661
Iteration 44/1000 | Loss: 0.00217459
Iteration 45/1000 | Loss: 0.00083896
Iteration 46/1000 | Loss: 0.00072946
Iteration 47/1000 | Loss: 0.00083407
Iteration 48/1000 | Loss: 0.00087389
Iteration 49/1000 | Loss: 0.00056346
Iteration 50/1000 | Loss: 0.00038895
Iteration 51/1000 | Loss: 0.00030988
Iteration 52/1000 | Loss: 0.00072613
Iteration 53/1000 | Loss: 0.00017008
Iteration 54/1000 | Loss: 0.00142935
Iteration 55/1000 | Loss: 0.00044021
Iteration 56/1000 | Loss: 0.00061826
Iteration 57/1000 | Loss: 0.00014761
Iteration 58/1000 | Loss: 0.00047343
Iteration 59/1000 | Loss: 0.00021643
Iteration 60/1000 | Loss: 0.00049323
Iteration 61/1000 | Loss: 0.00131139
Iteration 62/1000 | Loss: 0.00044282
Iteration 63/1000 | Loss: 0.00043968
Iteration 64/1000 | Loss: 0.00037816
Iteration 65/1000 | Loss: 0.00074461
Iteration 66/1000 | Loss: 0.00041871
Iteration 67/1000 | Loss: 0.00089192
Iteration 68/1000 | Loss: 0.00028885
Iteration 69/1000 | Loss: 0.00027066
Iteration 70/1000 | Loss: 0.00054325
Iteration 71/1000 | Loss: 0.00022850
Iteration 72/1000 | Loss: 0.00029911
Iteration 73/1000 | Loss: 0.00020245
Iteration 74/1000 | Loss: 0.00050386
Iteration 75/1000 | Loss: 0.00098574
Iteration 76/1000 | Loss: 0.00052652
Iteration 77/1000 | Loss: 0.00049933
Iteration 78/1000 | Loss: 0.00045187
Iteration 79/1000 | Loss: 0.00073785
Iteration 80/1000 | Loss: 0.00042024
Iteration 81/1000 | Loss: 0.00015560
Iteration 82/1000 | Loss: 0.00006023
Iteration 83/1000 | Loss: 0.00050439
Iteration 84/1000 | Loss: 0.00009470
Iteration 85/1000 | Loss: 0.00071654
Iteration 86/1000 | Loss: 0.00049738
Iteration 87/1000 | Loss: 0.00055425
Iteration 88/1000 | Loss: 0.00005298
Iteration 89/1000 | Loss: 0.00061492
Iteration 90/1000 | Loss: 0.00025278
Iteration 91/1000 | Loss: 0.00005871
Iteration 92/1000 | Loss: 0.00004738
Iteration 93/1000 | Loss: 0.00022484
Iteration 94/1000 | Loss: 0.00007048
Iteration 95/1000 | Loss: 0.00005269
Iteration 96/1000 | Loss: 0.00032658
Iteration 97/1000 | Loss: 0.00072927
Iteration 98/1000 | Loss: 0.00062796
Iteration 99/1000 | Loss: 0.00016377
Iteration 100/1000 | Loss: 0.00015902
Iteration 101/1000 | Loss: 0.00012044
Iteration 102/1000 | Loss: 0.00012657
Iteration 103/1000 | Loss: 0.00046973
Iteration 104/1000 | Loss: 0.00025285
Iteration 105/1000 | Loss: 0.00010357
Iteration 106/1000 | Loss: 0.00004823
Iteration 107/1000 | Loss: 0.00004316
Iteration 108/1000 | Loss: 0.00004066
Iteration 109/1000 | Loss: 0.00022105
Iteration 110/1000 | Loss: 0.00049306
Iteration 111/1000 | Loss: 0.00009127
Iteration 112/1000 | Loss: 0.00008545
Iteration 113/1000 | Loss: 0.00003948
Iteration 114/1000 | Loss: 0.00003609
Iteration 115/1000 | Loss: 0.00003431
Iteration 116/1000 | Loss: 0.00005022
Iteration 117/1000 | Loss: 0.00025950
Iteration 118/1000 | Loss: 0.00014346
Iteration 119/1000 | Loss: 0.00022109
Iteration 120/1000 | Loss: 0.00010443
Iteration 121/1000 | Loss: 0.00021075
Iteration 122/1000 | Loss: 0.00018346
Iteration 123/1000 | Loss: 0.00021826
Iteration 124/1000 | Loss: 0.00008973
Iteration 125/1000 | Loss: 0.00003702
Iteration 126/1000 | Loss: 0.00016346
Iteration 127/1000 | Loss: 0.00004231
Iteration 128/1000 | Loss: 0.00003051
Iteration 129/1000 | Loss: 0.00003402
Iteration 130/1000 | Loss: 0.00002937
Iteration 131/1000 | Loss: 0.00004855
Iteration 132/1000 | Loss: 0.00003544
Iteration 133/1000 | Loss: 0.00002847
Iteration 134/1000 | Loss: 0.00036137
Iteration 135/1000 | Loss: 0.00013513
Iteration 136/1000 | Loss: 0.00003227
Iteration 137/1000 | Loss: 0.00002817
Iteration 138/1000 | Loss: 0.00003507
Iteration 139/1000 | Loss: 0.00002541
Iteration 140/1000 | Loss: 0.00002450
Iteration 141/1000 | Loss: 0.00002712
Iteration 142/1000 | Loss: 0.00002378
Iteration 143/1000 | Loss: 0.00002651
Iteration 144/1000 | Loss: 0.00002443
Iteration 145/1000 | Loss: 0.00002333
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002326
Iteration 148/1000 | Loss: 0.00002462
Iteration 149/1000 | Loss: 0.00002462
Iteration 150/1000 | Loss: 0.00004862
Iteration 151/1000 | Loss: 0.00010680
Iteration 152/1000 | Loss: 0.00002780
Iteration 153/1000 | Loss: 0.00002807
Iteration 154/1000 | Loss: 0.00002334
Iteration 155/1000 | Loss: 0.00002601
Iteration 156/1000 | Loss: 0.00002314
Iteration 157/1000 | Loss: 0.00002314
Iteration 158/1000 | Loss: 0.00002314
Iteration 159/1000 | Loss: 0.00002314
Iteration 160/1000 | Loss: 0.00002314
Iteration 161/1000 | Loss: 0.00002313
Iteration 162/1000 | Loss: 0.00002313
Iteration 163/1000 | Loss: 0.00002312
Iteration 164/1000 | Loss: 0.00002312
Iteration 165/1000 | Loss: 0.00002311
Iteration 166/1000 | Loss: 0.00003622
Iteration 167/1000 | Loss: 0.00002311
Iteration 168/1000 | Loss: 0.00002311
Iteration 169/1000 | Loss: 0.00002310
Iteration 170/1000 | Loss: 0.00002310
Iteration 171/1000 | Loss: 0.00002310
Iteration 172/1000 | Loss: 0.00002310
Iteration 173/1000 | Loss: 0.00002310
Iteration 174/1000 | Loss: 0.00002310
Iteration 175/1000 | Loss: 0.00002310
Iteration 176/1000 | Loss: 0.00002310
Iteration 177/1000 | Loss: 0.00002310
Iteration 178/1000 | Loss: 0.00002310
Iteration 179/1000 | Loss: 0.00002310
Iteration 180/1000 | Loss: 0.00002310
Iteration 181/1000 | Loss: 0.00002309
Iteration 182/1000 | Loss: 0.00002309
Iteration 183/1000 | Loss: 0.00002309
Iteration 184/1000 | Loss: 0.00002309
Iteration 185/1000 | Loss: 0.00002309
Iteration 186/1000 | Loss: 0.00002309
Iteration 187/1000 | Loss: 0.00002309
Iteration 188/1000 | Loss: 0.00002309
Iteration 189/1000 | Loss: 0.00002309
Iteration 190/1000 | Loss: 0.00002308
Iteration 191/1000 | Loss: 0.00002308
Iteration 192/1000 | Loss: 0.00002308
Iteration 193/1000 | Loss: 0.00002308
Iteration 194/1000 | Loss: 0.00002308
Iteration 195/1000 | Loss: 0.00002308
Iteration 196/1000 | Loss: 0.00002308
Iteration 197/1000 | Loss: 0.00002308
Iteration 198/1000 | Loss: 0.00002308
Iteration 199/1000 | Loss: 0.00002308
Iteration 200/1000 | Loss: 0.00002308
Iteration 201/1000 | Loss: 0.00002308
Iteration 202/1000 | Loss: 0.00002308
Iteration 203/1000 | Loss: 0.00002308
Iteration 204/1000 | Loss: 0.00002308
Iteration 205/1000 | Loss: 0.00002308
Iteration 206/1000 | Loss: 0.00002308
Iteration 207/1000 | Loss: 0.00002308
Iteration 208/1000 | Loss: 0.00002308
Iteration 209/1000 | Loss: 0.00002308
Iteration 210/1000 | Loss: 0.00002308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.3079715901985765e-05, 2.3079715901985765e-05, 2.3079715901985765e-05, 2.3079715901985765e-05, 2.3079715901985765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3079715901985765e-05

Optimization complete. Final v2v error: 3.941061496734619 mm

Highest mean error: 6.274824619293213 mm for frame 35

Lowest mean error: 3.2536418437957764 mm for frame 52

Saving results

Total time: 308.9119906425476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406531
Iteration 2/25 | Loss: 0.00096813
Iteration 3/25 | Loss: 0.00083668
Iteration 4/25 | Loss: 0.00080752
Iteration 5/25 | Loss: 0.00079854
Iteration 6/25 | Loss: 0.00079676
Iteration 7/25 | Loss: 0.00079618
Iteration 8/25 | Loss: 0.00079618
Iteration 9/25 | Loss: 0.00079618
Iteration 10/25 | Loss: 0.00079618
Iteration 11/25 | Loss: 0.00079618
Iteration 12/25 | Loss: 0.00079618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007961781811900437, 0.0007961781811900437, 0.0007961781811900437, 0.0007961781811900437, 0.0007961781811900437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007961781811900437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14121628
Iteration 2/25 | Loss: 0.00086949
Iteration 3/25 | Loss: 0.00086949
Iteration 4/25 | Loss: 0.00086949
Iteration 5/25 | Loss: 0.00086949
Iteration 6/25 | Loss: 0.00086949
Iteration 7/25 | Loss: 0.00086949
Iteration 8/25 | Loss: 0.00086949
Iteration 9/25 | Loss: 0.00086949
Iteration 10/25 | Loss: 0.00086949
Iteration 11/25 | Loss: 0.00086949
Iteration 12/25 | Loss: 0.00086949
Iteration 13/25 | Loss: 0.00086949
Iteration 14/25 | Loss: 0.00086949
Iteration 15/25 | Loss: 0.00086949
Iteration 16/25 | Loss: 0.00086949
Iteration 17/25 | Loss: 0.00086949
Iteration 18/25 | Loss: 0.00086949
Iteration 19/25 | Loss: 0.00086949
Iteration 20/25 | Loss: 0.00086949
Iteration 21/25 | Loss: 0.00086949
Iteration 22/25 | Loss: 0.00086949
Iteration 23/25 | Loss: 0.00086949
Iteration 24/25 | Loss: 0.00086949
Iteration 25/25 | Loss: 0.00086949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008694874704815447, 0.0008694874704815447, 0.0008694874704815447, 0.0008694874704815447, 0.0008694874704815447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008694874704815447

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086949
Iteration 2/1000 | Loss: 0.00004928
Iteration 3/1000 | Loss: 0.00003400
Iteration 4/1000 | Loss: 0.00003067
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002843
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00002725
Iteration 9/1000 | Loss: 0.00002688
Iteration 10/1000 | Loss: 0.00002660
Iteration 11/1000 | Loss: 0.00002659
Iteration 12/1000 | Loss: 0.00002642
Iteration 13/1000 | Loss: 0.00002632
Iteration 14/1000 | Loss: 0.00002628
Iteration 15/1000 | Loss: 0.00002627
Iteration 16/1000 | Loss: 0.00002627
Iteration 17/1000 | Loss: 0.00002626
Iteration 18/1000 | Loss: 0.00002626
Iteration 19/1000 | Loss: 0.00002625
Iteration 20/1000 | Loss: 0.00002625
Iteration 21/1000 | Loss: 0.00002623
Iteration 22/1000 | Loss: 0.00002623
Iteration 23/1000 | Loss: 0.00002623
Iteration 24/1000 | Loss: 0.00002622
Iteration 25/1000 | Loss: 0.00002622
Iteration 26/1000 | Loss: 0.00002622
Iteration 27/1000 | Loss: 0.00002622
Iteration 28/1000 | Loss: 0.00002622
Iteration 29/1000 | Loss: 0.00002621
Iteration 30/1000 | Loss: 0.00002621
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00002619
Iteration 33/1000 | Loss: 0.00002619
Iteration 34/1000 | Loss: 0.00002619
Iteration 35/1000 | Loss: 0.00002619
Iteration 36/1000 | Loss: 0.00002619
Iteration 37/1000 | Loss: 0.00002619
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002619
Iteration 40/1000 | Loss: 0.00002619
Iteration 41/1000 | Loss: 0.00002619
Iteration 42/1000 | Loss: 0.00002618
Iteration 43/1000 | Loss: 0.00002618
Iteration 44/1000 | Loss: 0.00002618
Iteration 45/1000 | Loss: 0.00002618
Iteration 46/1000 | Loss: 0.00002618
Iteration 47/1000 | Loss: 0.00002617
Iteration 48/1000 | Loss: 0.00002617
Iteration 49/1000 | Loss: 0.00002615
Iteration 50/1000 | Loss: 0.00002614
Iteration 51/1000 | Loss: 0.00002614
Iteration 52/1000 | Loss: 0.00002613
Iteration 53/1000 | Loss: 0.00002610
Iteration 54/1000 | Loss: 0.00002609
Iteration 55/1000 | Loss: 0.00002608
Iteration 56/1000 | Loss: 0.00002604
Iteration 57/1000 | Loss: 0.00002604
Iteration 58/1000 | Loss: 0.00002603
Iteration 59/1000 | Loss: 0.00002600
Iteration 60/1000 | Loss: 0.00002600
Iteration 61/1000 | Loss: 0.00002600
Iteration 62/1000 | Loss: 0.00002599
Iteration 63/1000 | Loss: 0.00002599
Iteration 64/1000 | Loss: 0.00002598
Iteration 65/1000 | Loss: 0.00002598
Iteration 66/1000 | Loss: 0.00002597
Iteration 67/1000 | Loss: 0.00002597
Iteration 68/1000 | Loss: 0.00002596
Iteration 69/1000 | Loss: 0.00002596
Iteration 70/1000 | Loss: 0.00002596
Iteration 71/1000 | Loss: 0.00002596
Iteration 72/1000 | Loss: 0.00002596
Iteration 73/1000 | Loss: 0.00002596
Iteration 74/1000 | Loss: 0.00002596
Iteration 75/1000 | Loss: 0.00002596
Iteration 76/1000 | Loss: 0.00002596
Iteration 77/1000 | Loss: 0.00002595
Iteration 78/1000 | Loss: 0.00002595
Iteration 79/1000 | Loss: 0.00002595
Iteration 80/1000 | Loss: 0.00002595
Iteration 81/1000 | Loss: 0.00002595
Iteration 82/1000 | Loss: 0.00002594
Iteration 83/1000 | Loss: 0.00002594
Iteration 84/1000 | Loss: 0.00002594
Iteration 85/1000 | Loss: 0.00002594
Iteration 86/1000 | Loss: 0.00002594
Iteration 87/1000 | Loss: 0.00002594
Iteration 88/1000 | Loss: 0.00002594
Iteration 89/1000 | Loss: 0.00002594
Iteration 90/1000 | Loss: 0.00002593
Iteration 91/1000 | Loss: 0.00002593
Iteration 92/1000 | Loss: 0.00002593
Iteration 93/1000 | Loss: 0.00002593
Iteration 94/1000 | Loss: 0.00002593
Iteration 95/1000 | Loss: 0.00002593
Iteration 96/1000 | Loss: 0.00002593
Iteration 97/1000 | Loss: 0.00002593
Iteration 98/1000 | Loss: 0.00002593
Iteration 99/1000 | Loss: 0.00002592
Iteration 100/1000 | Loss: 0.00002592
Iteration 101/1000 | Loss: 0.00002592
Iteration 102/1000 | Loss: 0.00002591
Iteration 103/1000 | Loss: 0.00002591
Iteration 104/1000 | Loss: 0.00002591
Iteration 105/1000 | Loss: 0.00002591
Iteration 106/1000 | Loss: 0.00002591
Iteration 107/1000 | Loss: 0.00002591
Iteration 108/1000 | Loss: 0.00002591
Iteration 109/1000 | Loss: 0.00002590
Iteration 110/1000 | Loss: 0.00002590
Iteration 111/1000 | Loss: 0.00002590
Iteration 112/1000 | Loss: 0.00002590
Iteration 113/1000 | Loss: 0.00002590
Iteration 114/1000 | Loss: 0.00002590
Iteration 115/1000 | Loss: 0.00002590
Iteration 116/1000 | Loss: 0.00002590
Iteration 117/1000 | Loss: 0.00002590
Iteration 118/1000 | Loss: 0.00002590
Iteration 119/1000 | Loss: 0.00002589
Iteration 120/1000 | Loss: 0.00002589
Iteration 121/1000 | Loss: 0.00002589
Iteration 122/1000 | Loss: 0.00002589
Iteration 123/1000 | Loss: 0.00002589
Iteration 124/1000 | Loss: 0.00002588
Iteration 125/1000 | Loss: 0.00002588
Iteration 126/1000 | Loss: 0.00002588
Iteration 127/1000 | Loss: 0.00002588
Iteration 128/1000 | Loss: 0.00002588
Iteration 129/1000 | Loss: 0.00002588
Iteration 130/1000 | Loss: 0.00002588
Iteration 131/1000 | Loss: 0.00002588
Iteration 132/1000 | Loss: 0.00002588
Iteration 133/1000 | Loss: 0.00002588
Iteration 134/1000 | Loss: 0.00002587
Iteration 135/1000 | Loss: 0.00002587
Iteration 136/1000 | Loss: 0.00002587
Iteration 137/1000 | Loss: 0.00002587
Iteration 138/1000 | Loss: 0.00002587
Iteration 139/1000 | Loss: 0.00002587
Iteration 140/1000 | Loss: 0.00002587
Iteration 141/1000 | Loss: 0.00002587
Iteration 142/1000 | Loss: 0.00002587
Iteration 143/1000 | Loss: 0.00002587
Iteration 144/1000 | Loss: 0.00002587
Iteration 145/1000 | Loss: 0.00002587
Iteration 146/1000 | Loss: 0.00002586
Iteration 147/1000 | Loss: 0.00002586
Iteration 148/1000 | Loss: 0.00002586
Iteration 149/1000 | Loss: 0.00002586
Iteration 150/1000 | Loss: 0.00002586
Iteration 151/1000 | Loss: 0.00002586
Iteration 152/1000 | Loss: 0.00002586
Iteration 153/1000 | Loss: 0.00002586
Iteration 154/1000 | Loss: 0.00002586
Iteration 155/1000 | Loss: 0.00002585
Iteration 156/1000 | Loss: 0.00002585
Iteration 157/1000 | Loss: 0.00002585
Iteration 158/1000 | Loss: 0.00002585
Iteration 159/1000 | Loss: 0.00002585
Iteration 160/1000 | Loss: 0.00002585
Iteration 161/1000 | Loss: 0.00002585
Iteration 162/1000 | Loss: 0.00002585
Iteration 163/1000 | Loss: 0.00002585
Iteration 164/1000 | Loss: 0.00002585
Iteration 165/1000 | Loss: 0.00002585
Iteration 166/1000 | Loss: 0.00002585
Iteration 167/1000 | Loss: 0.00002585
Iteration 168/1000 | Loss: 0.00002585
Iteration 169/1000 | Loss: 0.00002585
Iteration 170/1000 | Loss: 0.00002585
Iteration 171/1000 | Loss: 0.00002585
Iteration 172/1000 | Loss: 0.00002585
Iteration 173/1000 | Loss: 0.00002585
Iteration 174/1000 | Loss: 0.00002584
Iteration 175/1000 | Loss: 0.00002584
Iteration 176/1000 | Loss: 0.00002584
Iteration 177/1000 | Loss: 0.00002584
Iteration 178/1000 | Loss: 0.00002584
Iteration 179/1000 | Loss: 0.00002584
Iteration 180/1000 | Loss: 0.00002584
Iteration 181/1000 | Loss: 0.00002584
Iteration 182/1000 | Loss: 0.00002584
Iteration 183/1000 | Loss: 0.00002584
Iteration 184/1000 | Loss: 0.00002584
Iteration 185/1000 | Loss: 0.00002584
Iteration 186/1000 | Loss: 0.00002584
Iteration 187/1000 | Loss: 0.00002584
Iteration 188/1000 | Loss: 0.00002584
Iteration 189/1000 | Loss: 0.00002584
Iteration 190/1000 | Loss: 0.00002584
Iteration 191/1000 | Loss: 0.00002584
Iteration 192/1000 | Loss: 0.00002584
Iteration 193/1000 | Loss: 0.00002584
Iteration 194/1000 | Loss: 0.00002584
Iteration 195/1000 | Loss: 0.00002584
Iteration 196/1000 | Loss: 0.00002584
Iteration 197/1000 | Loss: 0.00002584
Iteration 198/1000 | Loss: 0.00002584
Iteration 199/1000 | Loss: 0.00002584
Iteration 200/1000 | Loss: 0.00002584
Iteration 201/1000 | Loss: 0.00002584
Iteration 202/1000 | Loss: 0.00002584
Iteration 203/1000 | Loss: 0.00002584
Iteration 204/1000 | Loss: 0.00002584
Iteration 205/1000 | Loss: 0.00002584
Iteration 206/1000 | Loss: 0.00002584
Iteration 207/1000 | Loss: 0.00002584
Iteration 208/1000 | Loss: 0.00002584
Iteration 209/1000 | Loss: 0.00002584
Iteration 210/1000 | Loss: 0.00002584
Iteration 211/1000 | Loss: 0.00002584
Iteration 212/1000 | Loss: 0.00002584
Iteration 213/1000 | Loss: 0.00002584
Iteration 214/1000 | Loss: 0.00002584
Iteration 215/1000 | Loss: 0.00002584
Iteration 216/1000 | Loss: 0.00002584
Iteration 217/1000 | Loss: 0.00002584
Iteration 218/1000 | Loss: 0.00002584
Iteration 219/1000 | Loss: 0.00002584
Iteration 220/1000 | Loss: 0.00002584
Iteration 221/1000 | Loss: 0.00002584
Iteration 222/1000 | Loss: 0.00002584
Iteration 223/1000 | Loss: 0.00002584
Iteration 224/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.5836954591795802e-05, 2.5836954591795802e-05, 2.5836954591795802e-05, 2.5836954591795802e-05, 2.5836954591795802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5836954591795802e-05

Optimization complete. Final v2v error: 4.216993808746338 mm

Highest mean error: 4.7138352394104 mm for frame 30

Lowest mean error: 3.8424785137176514 mm for frame 60

Saving results

Total time: 41.91248846054077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946953
Iteration 2/25 | Loss: 0.00111293
Iteration 3/25 | Loss: 0.00090238
Iteration 4/25 | Loss: 0.00086227
Iteration 5/25 | Loss: 0.00084312
Iteration 6/25 | Loss: 0.00083899
Iteration 7/25 | Loss: 0.00083801
Iteration 8/25 | Loss: 0.00083797
Iteration 9/25 | Loss: 0.00083797
Iteration 10/25 | Loss: 0.00083797
Iteration 11/25 | Loss: 0.00083797
Iteration 12/25 | Loss: 0.00083797
Iteration 13/25 | Loss: 0.00083797
Iteration 14/25 | Loss: 0.00083797
Iteration 15/25 | Loss: 0.00083797
Iteration 16/25 | Loss: 0.00083797
Iteration 17/25 | Loss: 0.00083797
Iteration 18/25 | Loss: 0.00083797
Iteration 19/25 | Loss: 0.00083797
Iteration 20/25 | Loss: 0.00083797
Iteration 21/25 | Loss: 0.00083797
Iteration 22/25 | Loss: 0.00083797
Iteration 23/25 | Loss: 0.00083797
Iteration 24/25 | Loss: 0.00083797
Iteration 25/25 | Loss: 0.00083797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52926898
Iteration 2/25 | Loss: 0.00075924
Iteration 3/25 | Loss: 0.00075922
Iteration 4/25 | Loss: 0.00075922
Iteration 5/25 | Loss: 0.00075922
Iteration 6/25 | Loss: 0.00075922
Iteration 7/25 | Loss: 0.00075922
Iteration 8/25 | Loss: 0.00075922
Iteration 9/25 | Loss: 0.00075922
Iteration 10/25 | Loss: 0.00075922
Iteration 11/25 | Loss: 0.00075922
Iteration 12/25 | Loss: 0.00075922
Iteration 13/25 | Loss: 0.00075922
Iteration 14/25 | Loss: 0.00075922
Iteration 15/25 | Loss: 0.00075922
Iteration 16/25 | Loss: 0.00075922
Iteration 17/25 | Loss: 0.00075922
Iteration 18/25 | Loss: 0.00075922
Iteration 19/25 | Loss: 0.00075922
Iteration 20/25 | Loss: 0.00075922
Iteration 21/25 | Loss: 0.00075922
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007592184701934457, 0.0007592184701934457, 0.0007592184701934457, 0.0007592184701934457, 0.0007592184701934457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007592184701934457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075922
Iteration 2/1000 | Loss: 0.00006881
Iteration 3/1000 | Loss: 0.00004259
Iteration 4/1000 | Loss: 0.00003595
Iteration 5/1000 | Loss: 0.00003457
Iteration 6/1000 | Loss: 0.00003327
Iteration 7/1000 | Loss: 0.00003224
Iteration 8/1000 | Loss: 0.00003137
Iteration 9/1000 | Loss: 0.00003069
Iteration 10/1000 | Loss: 0.00003028
Iteration 11/1000 | Loss: 0.00003003
Iteration 12/1000 | Loss: 0.00002984
Iteration 13/1000 | Loss: 0.00002966
Iteration 14/1000 | Loss: 0.00002966
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00002950
Iteration 17/1000 | Loss: 0.00002950
Iteration 18/1000 | Loss: 0.00002950
Iteration 19/1000 | Loss: 0.00002950
Iteration 20/1000 | Loss: 0.00002950
Iteration 21/1000 | Loss: 0.00002950
Iteration 22/1000 | Loss: 0.00002950
Iteration 23/1000 | Loss: 0.00002950
Iteration 24/1000 | Loss: 0.00002949
Iteration 25/1000 | Loss: 0.00002949
Iteration 26/1000 | Loss: 0.00002948
Iteration 27/1000 | Loss: 0.00002946
Iteration 28/1000 | Loss: 0.00002946
Iteration 29/1000 | Loss: 0.00002945
Iteration 30/1000 | Loss: 0.00002944
Iteration 31/1000 | Loss: 0.00002940
Iteration 32/1000 | Loss: 0.00002940
Iteration 33/1000 | Loss: 0.00002940
Iteration 34/1000 | Loss: 0.00002939
Iteration 35/1000 | Loss: 0.00002938
Iteration 36/1000 | Loss: 0.00002938
Iteration 37/1000 | Loss: 0.00002937
Iteration 38/1000 | Loss: 0.00002936
Iteration 39/1000 | Loss: 0.00002936
Iteration 40/1000 | Loss: 0.00002935
Iteration 41/1000 | Loss: 0.00002935
Iteration 42/1000 | Loss: 0.00002935
Iteration 43/1000 | Loss: 0.00002935
Iteration 44/1000 | Loss: 0.00002934
Iteration 45/1000 | Loss: 0.00002934
Iteration 46/1000 | Loss: 0.00002934
Iteration 47/1000 | Loss: 0.00002933
Iteration 48/1000 | Loss: 0.00002933
Iteration 49/1000 | Loss: 0.00002932
Iteration 50/1000 | Loss: 0.00002932
Iteration 51/1000 | Loss: 0.00002932
Iteration 52/1000 | Loss: 0.00002932
Iteration 53/1000 | Loss: 0.00002931
Iteration 54/1000 | Loss: 0.00002931
Iteration 55/1000 | Loss: 0.00002931
Iteration 56/1000 | Loss: 0.00002930
Iteration 57/1000 | Loss: 0.00002930
Iteration 58/1000 | Loss: 0.00002930
Iteration 59/1000 | Loss: 0.00002929
Iteration 60/1000 | Loss: 0.00002929
Iteration 61/1000 | Loss: 0.00002929
Iteration 62/1000 | Loss: 0.00002928
Iteration 63/1000 | Loss: 0.00002928
Iteration 64/1000 | Loss: 0.00002927
Iteration 65/1000 | Loss: 0.00002927
Iteration 66/1000 | Loss: 0.00002926
Iteration 67/1000 | Loss: 0.00002926
Iteration 68/1000 | Loss: 0.00002925
Iteration 69/1000 | Loss: 0.00002925
Iteration 70/1000 | Loss: 0.00002925
Iteration 71/1000 | Loss: 0.00002924
Iteration 72/1000 | Loss: 0.00002924
Iteration 73/1000 | Loss: 0.00002923
Iteration 74/1000 | Loss: 0.00002922
Iteration 75/1000 | Loss: 0.00002922
Iteration 76/1000 | Loss: 0.00002922
Iteration 77/1000 | Loss: 0.00002921
Iteration 78/1000 | Loss: 0.00002921
Iteration 79/1000 | Loss: 0.00002921
Iteration 80/1000 | Loss: 0.00002920
Iteration 81/1000 | Loss: 0.00002920
Iteration 82/1000 | Loss: 0.00002920
Iteration 83/1000 | Loss: 0.00002920
Iteration 84/1000 | Loss: 0.00002919
Iteration 85/1000 | Loss: 0.00002919
Iteration 86/1000 | Loss: 0.00002919
Iteration 87/1000 | Loss: 0.00002919
Iteration 88/1000 | Loss: 0.00002919
Iteration 89/1000 | Loss: 0.00002919
Iteration 90/1000 | Loss: 0.00002919
Iteration 91/1000 | Loss: 0.00002919
Iteration 92/1000 | Loss: 0.00002919
Iteration 93/1000 | Loss: 0.00002919
Iteration 94/1000 | Loss: 0.00002919
Iteration 95/1000 | Loss: 0.00002919
Iteration 96/1000 | Loss: 0.00002918
Iteration 97/1000 | Loss: 0.00002918
Iteration 98/1000 | Loss: 0.00002918
Iteration 99/1000 | Loss: 0.00002918
Iteration 100/1000 | Loss: 0.00002918
Iteration 101/1000 | Loss: 0.00002917
Iteration 102/1000 | Loss: 0.00002917
Iteration 103/1000 | Loss: 0.00002917
Iteration 104/1000 | Loss: 0.00002917
Iteration 105/1000 | Loss: 0.00002917
Iteration 106/1000 | Loss: 0.00002916
Iteration 107/1000 | Loss: 0.00002916
Iteration 108/1000 | Loss: 0.00002916
Iteration 109/1000 | Loss: 0.00002916
Iteration 110/1000 | Loss: 0.00002916
Iteration 111/1000 | Loss: 0.00002916
Iteration 112/1000 | Loss: 0.00002916
Iteration 113/1000 | Loss: 0.00002916
Iteration 114/1000 | Loss: 0.00002915
Iteration 115/1000 | Loss: 0.00002915
Iteration 116/1000 | Loss: 0.00002915
Iteration 117/1000 | Loss: 0.00002915
Iteration 118/1000 | Loss: 0.00002915
Iteration 119/1000 | Loss: 0.00002914
Iteration 120/1000 | Loss: 0.00002914
Iteration 121/1000 | Loss: 0.00002914
Iteration 122/1000 | Loss: 0.00002914
Iteration 123/1000 | Loss: 0.00002914
Iteration 124/1000 | Loss: 0.00002914
Iteration 125/1000 | Loss: 0.00002914
Iteration 126/1000 | Loss: 0.00002914
Iteration 127/1000 | Loss: 0.00002914
Iteration 128/1000 | Loss: 0.00002913
Iteration 129/1000 | Loss: 0.00002913
Iteration 130/1000 | Loss: 0.00002913
Iteration 131/1000 | Loss: 0.00002913
Iteration 132/1000 | Loss: 0.00002913
Iteration 133/1000 | Loss: 0.00002913
Iteration 134/1000 | Loss: 0.00002913
Iteration 135/1000 | Loss: 0.00002913
Iteration 136/1000 | Loss: 0.00002913
Iteration 137/1000 | Loss: 0.00002912
Iteration 138/1000 | Loss: 0.00002912
Iteration 139/1000 | Loss: 0.00002912
Iteration 140/1000 | Loss: 0.00002912
Iteration 141/1000 | Loss: 0.00002912
Iteration 142/1000 | Loss: 0.00002912
Iteration 143/1000 | Loss: 0.00002912
Iteration 144/1000 | Loss: 0.00002912
Iteration 145/1000 | Loss: 0.00002912
Iteration 146/1000 | Loss: 0.00002912
Iteration 147/1000 | Loss: 0.00002912
Iteration 148/1000 | Loss: 0.00002912
Iteration 149/1000 | Loss: 0.00002912
Iteration 150/1000 | Loss: 0.00002912
Iteration 151/1000 | Loss: 0.00002912
Iteration 152/1000 | Loss: 0.00002912
Iteration 153/1000 | Loss: 0.00002912
Iteration 154/1000 | Loss: 0.00002912
Iteration 155/1000 | Loss: 0.00002912
Iteration 156/1000 | Loss: 0.00002912
Iteration 157/1000 | Loss: 0.00002912
Iteration 158/1000 | Loss: 0.00002912
Iteration 159/1000 | Loss: 0.00002912
Iteration 160/1000 | Loss: 0.00002912
Iteration 161/1000 | Loss: 0.00002912
Iteration 162/1000 | Loss: 0.00002912
Iteration 163/1000 | Loss: 0.00002912
Iteration 164/1000 | Loss: 0.00002912
Iteration 165/1000 | Loss: 0.00002912
Iteration 166/1000 | Loss: 0.00002912
Iteration 167/1000 | Loss: 0.00002912
Iteration 168/1000 | Loss: 0.00002912
Iteration 169/1000 | Loss: 0.00002912
Iteration 170/1000 | Loss: 0.00002912
Iteration 171/1000 | Loss: 0.00002912
Iteration 172/1000 | Loss: 0.00002912
Iteration 173/1000 | Loss: 0.00002912
Iteration 174/1000 | Loss: 0.00002912
Iteration 175/1000 | Loss: 0.00002912
Iteration 176/1000 | Loss: 0.00002912
Iteration 177/1000 | Loss: 0.00002912
Iteration 178/1000 | Loss: 0.00002912
Iteration 179/1000 | Loss: 0.00002912
Iteration 180/1000 | Loss: 0.00002912
Iteration 181/1000 | Loss: 0.00002912
Iteration 182/1000 | Loss: 0.00002912
Iteration 183/1000 | Loss: 0.00002912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.9115415600244887e-05, 2.9115415600244887e-05, 2.9115415600244887e-05, 2.9115415600244887e-05, 2.9115415600244887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9115415600244887e-05

Optimization complete. Final v2v error: 4.517318248748779 mm

Highest mean error: 5.980531215667725 mm for frame 68

Lowest mean error: 3.8736441135406494 mm for frame 44

Saving results

Total time: 41.884395122528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00652892
Iteration 2/25 | Loss: 0.00129592
Iteration 3/25 | Loss: 0.00094463
Iteration 4/25 | Loss: 0.00087204
Iteration 5/25 | Loss: 0.00084886
Iteration 6/25 | Loss: 0.00082902
Iteration 7/25 | Loss: 0.00083950
Iteration 8/25 | Loss: 0.00082648
Iteration 9/25 | Loss: 0.00082611
Iteration 10/25 | Loss: 0.00083782
Iteration 11/25 | Loss: 0.00082600
Iteration 12/25 | Loss: 0.00082586
Iteration 13/25 | Loss: 0.00082586
Iteration 14/25 | Loss: 0.00082586
Iteration 15/25 | Loss: 0.00082586
Iteration 16/25 | Loss: 0.00082586
Iteration 17/25 | Loss: 0.00082586
Iteration 18/25 | Loss: 0.00082586
Iteration 19/25 | Loss: 0.00082586
Iteration 20/25 | Loss: 0.00082585
Iteration 21/25 | Loss: 0.00082585
Iteration 22/25 | Loss: 0.00082585
Iteration 23/25 | Loss: 0.00082585
Iteration 24/25 | Loss: 0.00082585
Iteration 25/25 | Loss: 0.00082585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82536662
Iteration 2/25 | Loss: 0.00121622
Iteration 3/25 | Loss: 0.00121621
Iteration 4/25 | Loss: 0.00121621
Iteration 5/25 | Loss: 0.00121621
Iteration 6/25 | Loss: 0.00121621
Iteration 7/25 | Loss: 0.00121621
Iteration 8/25 | Loss: 0.00121621
Iteration 9/25 | Loss: 0.00121621
Iteration 10/25 | Loss: 0.00121621
Iteration 11/25 | Loss: 0.00121621
Iteration 12/25 | Loss: 0.00121621
Iteration 13/25 | Loss: 0.00121621
Iteration 14/25 | Loss: 0.00121621
Iteration 15/25 | Loss: 0.00121621
Iteration 16/25 | Loss: 0.00121621
Iteration 17/25 | Loss: 0.00121621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012162101920694113, 0.0012162101920694113, 0.0012162101920694113, 0.0012162101920694113, 0.0012162101920694113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012162101920694113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121621
Iteration 2/1000 | Loss: 0.00009607
Iteration 3/1000 | Loss: 0.00010850
Iteration 4/1000 | Loss: 0.00005321
Iteration 5/1000 | Loss: 0.00004495
Iteration 6/1000 | Loss: 0.00003906
Iteration 7/1000 | Loss: 0.00006346
Iteration 8/1000 | Loss: 0.00005170
Iteration 9/1000 | Loss: 0.00005819
Iteration 10/1000 | Loss: 0.00073338
Iteration 11/1000 | Loss: 0.00033693
Iteration 12/1000 | Loss: 0.00003901
Iteration 13/1000 | Loss: 0.00019412
Iteration 14/1000 | Loss: 0.00003489
Iteration 15/1000 | Loss: 0.00021827
Iteration 16/1000 | Loss: 0.00003215
Iteration 17/1000 | Loss: 0.00003087
Iteration 18/1000 | Loss: 0.00002926
Iteration 19/1000 | Loss: 0.00002826
Iteration 20/1000 | Loss: 0.00002760
Iteration 21/1000 | Loss: 0.00002708
Iteration 22/1000 | Loss: 0.00002668
Iteration 23/1000 | Loss: 0.00002633
Iteration 24/1000 | Loss: 0.00002605
Iteration 25/1000 | Loss: 0.00002580
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00002552
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002535
Iteration 30/1000 | Loss: 0.00002535
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002532
Iteration 33/1000 | Loss: 0.00002531
Iteration 34/1000 | Loss: 0.00002526
Iteration 35/1000 | Loss: 0.00002521
Iteration 36/1000 | Loss: 0.00002520
Iteration 37/1000 | Loss: 0.00002520
Iteration 38/1000 | Loss: 0.00002520
Iteration 39/1000 | Loss: 0.00002519
Iteration 40/1000 | Loss: 0.00002519
Iteration 41/1000 | Loss: 0.00002518
Iteration 42/1000 | Loss: 0.00002518
Iteration 43/1000 | Loss: 0.00002518
Iteration 44/1000 | Loss: 0.00002517
Iteration 45/1000 | Loss: 0.00002517
Iteration 46/1000 | Loss: 0.00002516
Iteration 47/1000 | Loss: 0.00002515
Iteration 48/1000 | Loss: 0.00002515
Iteration 49/1000 | Loss: 0.00002515
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002513
Iteration 56/1000 | Loss: 0.00002513
Iteration 57/1000 | Loss: 0.00002512
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002512
Iteration 60/1000 | Loss: 0.00002512
Iteration 61/1000 | Loss: 0.00002511
Iteration 62/1000 | Loss: 0.00002511
Iteration 63/1000 | Loss: 0.00002511
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002509
Iteration 67/1000 | Loss: 0.00002509
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002509
Iteration 72/1000 | Loss: 0.00002509
Iteration 73/1000 | Loss: 0.00002509
Iteration 74/1000 | Loss: 0.00002509
Iteration 75/1000 | Loss: 0.00002509
Iteration 76/1000 | Loss: 0.00002509
Iteration 77/1000 | Loss: 0.00002509
Iteration 78/1000 | Loss: 0.00002508
Iteration 79/1000 | Loss: 0.00002508
Iteration 80/1000 | Loss: 0.00002508
Iteration 81/1000 | Loss: 0.00002507
Iteration 82/1000 | Loss: 0.00002507
Iteration 83/1000 | Loss: 0.00002507
Iteration 84/1000 | Loss: 0.00002506
Iteration 85/1000 | Loss: 0.00002506
Iteration 86/1000 | Loss: 0.00002506
Iteration 87/1000 | Loss: 0.00002505
Iteration 88/1000 | Loss: 0.00002505
Iteration 89/1000 | Loss: 0.00002505
Iteration 90/1000 | Loss: 0.00002504
Iteration 91/1000 | Loss: 0.00002504
Iteration 92/1000 | Loss: 0.00002504
Iteration 93/1000 | Loss: 0.00002503
Iteration 94/1000 | Loss: 0.00002503
Iteration 95/1000 | Loss: 0.00002503
Iteration 96/1000 | Loss: 0.00002503
Iteration 97/1000 | Loss: 0.00002502
Iteration 98/1000 | Loss: 0.00002502
Iteration 99/1000 | Loss: 0.00002502
Iteration 100/1000 | Loss: 0.00002501
Iteration 101/1000 | Loss: 0.00002501
Iteration 102/1000 | Loss: 0.00002501
Iteration 103/1000 | Loss: 0.00002501
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002500
Iteration 106/1000 | Loss: 0.00002500
Iteration 107/1000 | Loss: 0.00002499
Iteration 108/1000 | Loss: 0.00002499
Iteration 109/1000 | Loss: 0.00002499
Iteration 110/1000 | Loss: 0.00002499
Iteration 111/1000 | Loss: 0.00002498
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Iteration 116/1000 | Loss: 0.00002498
Iteration 117/1000 | Loss: 0.00002498
Iteration 118/1000 | Loss: 0.00002498
Iteration 119/1000 | Loss: 0.00002498
Iteration 120/1000 | Loss: 0.00002498
Iteration 121/1000 | Loss: 0.00002497
Iteration 122/1000 | Loss: 0.00002497
Iteration 123/1000 | Loss: 0.00002497
Iteration 124/1000 | Loss: 0.00002497
Iteration 125/1000 | Loss: 0.00002497
Iteration 126/1000 | Loss: 0.00002497
Iteration 127/1000 | Loss: 0.00002497
Iteration 128/1000 | Loss: 0.00002497
Iteration 129/1000 | Loss: 0.00002496
Iteration 130/1000 | Loss: 0.00002496
Iteration 131/1000 | Loss: 0.00002496
Iteration 132/1000 | Loss: 0.00002496
Iteration 133/1000 | Loss: 0.00002496
Iteration 134/1000 | Loss: 0.00002496
Iteration 135/1000 | Loss: 0.00002496
Iteration 136/1000 | Loss: 0.00002496
Iteration 137/1000 | Loss: 0.00002496
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002496
Iteration 140/1000 | Loss: 0.00002496
Iteration 141/1000 | Loss: 0.00002496
Iteration 142/1000 | Loss: 0.00002496
Iteration 143/1000 | Loss: 0.00002496
Iteration 144/1000 | Loss: 0.00002496
Iteration 145/1000 | Loss: 0.00002496
Iteration 146/1000 | Loss: 0.00002496
Iteration 147/1000 | Loss: 0.00002496
Iteration 148/1000 | Loss: 0.00002496
Iteration 149/1000 | Loss: 0.00002496
Iteration 150/1000 | Loss: 0.00002496
Iteration 151/1000 | Loss: 0.00002496
Iteration 152/1000 | Loss: 0.00002496
Iteration 153/1000 | Loss: 0.00002496
Iteration 154/1000 | Loss: 0.00002496
Iteration 155/1000 | Loss: 0.00002496
Iteration 156/1000 | Loss: 0.00002496
Iteration 157/1000 | Loss: 0.00002496
Iteration 158/1000 | Loss: 0.00002496
Iteration 159/1000 | Loss: 0.00002496
Iteration 160/1000 | Loss: 0.00002496
Iteration 161/1000 | Loss: 0.00002496
Iteration 162/1000 | Loss: 0.00002496
Iteration 163/1000 | Loss: 0.00002496
Iteration 164/1000 | Loss: 0.00002496
Iteration 165/1000 | Loss: 0.00002496
Iteration 166/1000 | Loss: 0.00002496
Iteration 167/1000 | Loss: 0.00002496
Iteration 168/1000 | Loss: 0.00002496
Iteration 169/1000 | Loss: 0.00002496
Iteration 170/1000 | Loss: 0.00002496
Iteration 171/1000 | Loss: 0.00002496
Iteration 172/1000 | Loss: 0.00002496
Iteration 173/1000 | Loss: 0.00002496
Iteration 174/1000 | Loss: 0.00002496
Iteration 175/1000 | Loss: 0.00002496
Iteration 176/1000 | Loss: 0.00002496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.495519584044814e-05, 2.495519584044814e-05, 2.495519584044814e-05, 2.495519584044814e-05, 2.495519584044814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.495519584044814e-05

Optimization complete. Final v2v error: 4.204716205596924 mm

Highest mean error: 5.2940239906311035 mm for frame 59

Lowest mean error: 3.5015790462493896 mm for frame 73

Saving results

Total time: 70.35977602005005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828782
Iteration 2/25 | Loss: 0.00089659
Iteration 3/25 | Loss: 0.00074295
Iteration 4/25 | Loss: 0.00072424
Iteration 5/25 | Loss: 0.00071978
Iteration 6/25 | Loss: 0.00071777
Iteration 7/25 | Loss: 0.00071703
Iteration 8/25 | Loss: 0.00071703
Iteration 9/25 | Loss: 0.00071703
Iteration 10/25 | Loss: 0.00071703
Iteration 11/25 | Loss: 0.00071703
Iteration 12/25 | Loss: 0.00071703
Iteration 13/25 | Loss: 0.00071703
Iteration 14/25 | Loss: 0.00071703
Iteration 15/25 | Loss: 0.00071703
Iteration 16/25 | Loss: 0.00071703
Iteration 17/25 | Loss: 0.00071703
Iteration 18/25 | Loss: 0.00071703
Iteration 19/25 | Loss: 0.00071703
Iteration 20/25 | Loss: 0.00071703
Iteration 21/25 | Loss: 0.00071703
Iteration 22/25 | Loss: 0.00071703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007170260651037097, 0.0007170260651037097, 0.0007170260651037097, 0.0007170260651037097, 0.0007170260651037097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007170260651037097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54313028
Iteration 2/25 | Loss: 0.00091981
Iteration 3/25 | Loss: 0.00091981
Iteration 4/25 | Loss: 0.00091981
Iteration 5/25 | Loss: 0.00091981
Iteration 6/25 | Loss: 0.00091981
Iteration 7/25 | Loss: 0.00091981
Iteration 8/25 | Loss: 0.00091981
Iteration 9/25 | Loss: 0.00091981
Iteration 10/25 | Loss: 0.00091981
Iteration 11/25 | Loss: 0.00091981
Iteration 12/25 | Loss: 0.00091981
Iteration 13/25 | Loss: 0.00091981
Iteration 14/25 | Loss: 0.00091981
Iteration 15/25 | Loss: 0.00091981
Iteration 16/25 | Loss: 0.00091981
Iteration 17/25 | Loss: 0.00091981
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009198084007948637, 0.0009198084007948637, 0.0009198084007948637, 0.0009198084007948637, 0.0009198084007948637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009198084007948637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091981
Iteration 2/1000 | Loss: 0.00002476
Iteration 3/1000 | Loss: 0.00001584
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001307
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001254
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001248
Iteration 12/1000 | Loss: 0.00001248
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001217
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001206
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001204
Iteration 33/1000 | Loss: 0.00001204
Iteration 34/1000 | Loss: 0.00001203
Iteration 35/1000 | Loss: 0.00001203
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001190
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001190
Iteration 75/1000 | Loss: 0.00001190
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001188
Iteration 88/1000 | Loss: 0.00001188
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001188
Iteration 92/1000 | Loss: 0.00001188
Iteration 93/1000 | Loss: 0.00001188
Iteration 94/1000 | Loss: 0.00001188
Iteration 95/1000 | Loss: 0.00001188
Iteration 96/1000 | Loss: 0.00001188
Iteration 97/1000 | Loss: 0.00001188
Iteration 98/1000 | Loss: 0.00001188
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1883748811669648e-05, 1.1883748811669648e-05, 1.1883748811669648e-05, 1.1883748811669648e-05, 1.1883748811669648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1883748811669648e-05

Optimization complete. Final v2v error: 2.8967394828796387 mm

Highest mean error: 3.319261312484741 mm for frame 91

Lowest mean error: 2.74757981300354 mm for frame 34

Saving results

Total time: 33.16075897216797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067522
Iteration 2/25 | Loss: 0.00220772
Iteration 3/25 | Loss: 0.00128437
Iteration 4/25 | Loss: 0.00106312
Iteration 5/25 | Loss: 0.00104617
Iteration 6/25 | Loss: 0.00114609
Iteration 7/25 | Loss: 0.00116591
Iteration 8/25 | Loss: 0.00105427
Iteration 9/25 | Loss: 0.00097698
Iteration 10/25 | Loss: 0.00091483
Iteration 11/25 | Loss: 0.00086751
Iteration 12/25 | Loss: 0.00084913
Iteration 13/25 | Loss: 0.00083042
Iteration 14/25 | Loss: 0.00082208
Iteration 15/25 | Loss: 0.00081874
Iteration 16/25 | Loss: 0.00080460
Iteration 17/25 | Loss: 0.00079175
Iteration 18/25 | Loss: 0.00077539
Iteration 19/25 | Loss: 0.00076725
Iteration 20/25 | Loss: 0.00077077
Iteration 21/25 | Loss: 0.00076671
Iteration 22/25 | Loss: 0.00075414
Iteration 23/25 | Loss: 0.00074173
Iteration 24/25 | Loss: 0.00073375
Iteration 25/25 | Loss: 0.00072962

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.41429234
Iteration 2/25 | Loss: 0.00124365
Iteration 3/25 | Loss: 0.00107177
Iteration 4/25 | Loss: 0.00107177
Iteration 5/25 | Loss: 0.00107177
Iteration 6/25 | Loss: 0.00107177
Iteration 7/25 | Loss: 0.00107177
Iteration 8/25 | Loss: 0.00107177
Iteration 9/25 | Loss: 0.00107177
Iteration 10/25 | Loss: 0.00107177
Iteration 11/25 | Loss: 0.00107177
Iteration 12/25 | Loss: 0.00107177
Iteration 13/25 | Loss: 0.00107177
Iteration 14/25 | Loss: 0.00107177
Iteration 15/25 | Loss: 0.00107177
Iteration 16/25 | Loss: 0.00107177
Iteration 17/25 | Loss: 0.00107177
Iteration 18/25 | Loss: 0.00107177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010717706754803658, 0.0010717706754803658, 0.0010717706754803658, 0.0010717706754803658, 0.0010717706754803658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010717706754803658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107177
Iteration 2/1000 | Loss: 0.00014443
Iteration 3/1000 | Loss: 0.00009703
Iteration 4/1000 | Loss: 0.00037306
Iteration 5/1000 | Loss: 0.00015325
Iteration 6/1000 | Loss: 0.00007546
Iteration 7/1000 | Loss: 0.00008438
Iteration 8/1000 | Loss: 0.00022131
Iteration 9/1000 | Loss: 0.00009013
Iteration 10/1000 | Loss: 0.00005977
Iteration 11/1000 | Loss: 0.00006566
Iteration 12/1000 | Loss: 0.00005605
Iteration 13/1000 | Loss: 0.00003562
Iteration 14/1000 | Loss: 0.00002764
Iteration 15/1000 | Loss: 0.00013572
Iteration 16/1000 | Loss: 0.00008410
Iteration 17/1000 | Loss: 0.00019441
Iteration 18/1000 | Loss: 0.00012929
Iteration 19/1000 | Loss: 0.00016765
Iteration 20/1000 | Loss: 0.00016437
Iteration 21/1000 | Loss: 0.00017079
Iteration 22/1000 | Loss: 0.00015114
Iteration 23/1000 | Loss: 0.00010733
Iteration 24/1000 | Loss: 0.00030164
Iteration 25/1000 | Loss: 0.00008398
Iteration 26/1000 | Loss: 0.00021815
Iteration 27/1000 | Loss: 0.00012882
Iteration 28/1000 | Loss: 0.00011067
Iteration 29/1000 | Loss: 0.00017278
Iteration 30/1000 | Loss: 0.00005241
Iteration 31/1000 | Loss: 0.00003581
Iteration 32/1000 | Loss: 0.00025853
Iteration 33/1000 | Loss: 0.00005395
Iteration 34/1000 | Loss: 0.00002417
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001687
Iteration 37/1000 | Loss: 0.00003654
Iteration 38/1000 | Loss: 0.00003759
Iteration 39/1000 | Loss: 0.00003314
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001295
Iteration 43/1000 | Loss: 0.00001220
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001124
Iteration 47/1000 | Loss: 0.00001120
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001099
Iteration 51/1000 | Loss: 0.00001096
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001082
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001072
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001070
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001068
Iteration 66/1000 | Loss: 0.00001068
Iteration 67/1000 | Loss: 0.00001066
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001062
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001061
Iteration 74/1000 | Loss: 0.00001061
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001060
Iteration 78/1000 | Loss: 0.00001059
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001058
Iteration 90/1000 | Loss: 0.00001058
Iteration 91/1000 | Loss: 0.00001058
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001056
Iteration 98/1000 | Loss: 0.00001056
Iteration 99/1000 | Loss: 0.00001055
Iteration 100/1000 | Loss: 0.00001055
Iteration 101/1000 | Loss: 0.00001055
Iteration 102/1000 | Loss: 0.00001055
Iteration 103/1000 | Loss: 0.00001055
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001054
Iteration 106/1000 | Loss: 0.00001054
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001053
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001053
Iteration 115/1000 | Loss: 0.00001053
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001053
Iteration 119/1000 | Loss: 0.00001053
Iteration 120/1000 | Loss: 0.00001053
Iteration 121/1000 | Loss: 0.00001053
Iteration 122/1000 | Loss: 0.00001053
Iteration 123/1000 | Loss: 0.00001053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.052946117852116e-05, 1.052946117852116e-05, 1.052946117852116e-05, 1.052946117852116e-05, 1.052946117852116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.052946117852116e-05

Optimization complete. Final v2v error: 2.799497365951538 mm

Highest mean error: 3.9408929347991943 mm for frame 7

Lowest mean error: 2.5505926609039307 mm for frame 89

Saving results

Total time: 126.32074761390686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856410
Iteration 2/25 | Loss: 0.00120821
Iteration 3/25 | Loss: 0.00085817
Iteration 4/25 | Loss: 0.00080515
Iteration 5/25 | Loss: 0.00079461
Iteration 6/25 | Loss: 0.00079338
Iteration 7/25 | Loss: 0.00079338
Iteration 8/25 | Loss: 0.00079338
Iteration 9/25 | Loss: 0.00079338
Iteration 10/25 | Loss: 0.00079338
Iteration 11/25 | Loss: 0.00079338
Iteration 12/25 | Loss: 0.00079338
Iteration 13/25 | Loss: 0.00079338
Iteration 14/25 | Loss: 0.00079338
Iteration 15/25 | Loss: 0.00079338
Iteration 16/25 | Loss: 0.00079338
Iteration 17/25 | Loss: 0.00079338
Iteration 18/25 | Loss: 0.00079338
Iteration 19/25 | Loss: 0.00079338
Iteration 20/25 | Loss: 0.00079338
Iteration 21/25 | Loss: 0.00079338
Iteration 22/25 | Loss: 0.00079338
Iteration 23/25 | Loss: 0.00079338
Iteration 24/25 | Loss: 0.00079338
Iteration 25/25 | Loss: 0.00079338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09809196
Iteration 2/25 | Loss: 0.00070774
Iteration 3/25 | Loss: 0.00070773
Iteration 4/25 | Loss: 0.00070773
Iteration 5/25 | Loss: 0.00070773
Iteration 6/25 | Loss: 0.00070772
Iteration 7/25 | Loss: 0.00070772
Iteration 8/25 | Loss: 0.00070772
Iteration 9/25 | Loss: 0.00070772
Iteration 10/25 | Loss: 0.00070772
Iteration 11/25 | Loss: 0.00070772
Iteration 12/25 | Loss: 0.00070772
Iteration 13/25 | Loss: 0.00070772
Iteration 14/25 | Loss: 0.00070772
Iteration 15/25 | Loss: 0.00070772
Iteration 16/25 | Loss: 0.00070772
Iteration 17/25 | Loss: 0.00070772
Iteration 18/25 | Loss: 0.00070772
Iteration 19/25 | Loss: 0.00070772
Iteration 20/25 | Loss: 0.00070772
Iteration 21/25 | Loss: 0.00070772
Iteration 22/25 | Loss: 0.00070772
Iteration 23/25 | Loss: 0.00070772
Iteration 24/25 | Loss: 0.00070772
Iteration 25/25 | Loss: 0.00070772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070772
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00002214
Iteration 4/1000 | Loss: 0.00002076
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001846
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001845
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001828
Iteration 15/1000 | Loss: 0.00001827
Iteration 16/1000 | Loss: 0.00001826
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001820
Iteration 19/1000 | Loss: 0.00001820
Iteration 20/1000 | Loss: 0.00001819
Iteration 21/1000 | Loss: 0.00001819
Iteration 22/1000 | Loss: 0.00001818
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001818
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001818
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001817
Iteration 30/1000 | Loss: 0.00001817
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001814
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001812
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001811
Iteration 47/1000 | Loss: 0.00001811
Iteration 48/1000 | Loss: 0.00001811
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001810
Iteration 57/1000 | Loss: 0.00001810
Iteration 58/1000 | Loss: 0.00001810
Iteration 59/1000 | Loss: 0.00001810
Iteration 60/1000 | Loss: 0.00001810
Iteration 61/1000 | Loss: 0.00001810
Iteration 62/1000 | Loss: 0.00001810
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.8096638086717576e-05, 1.8096638086717576e-05, 1.8096638086717576e-05, 1.8096638086717576e-05, 1.8096638086717576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8096638086717576e-05

Optimization complete. Final v2v error: 3.6335365772247314 mm

Highest mean error: 4.239398956298828 mm for frame 1

Lowest mean error: 3.2848665714263916 mm for frame 51

Saving results

Total time: 28.623695373535156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037068
Iteration 2/25 | Loss: 0.00113669
Iteration 3/25 | Loss: 0.00084994
Iteration 4/25 | Loss: 0.00078813
Iteration 5/25 | Loss: 0.00077127
Iteration 6/25 | Loss: 0.00078058
Iteration 7/25 | Loss: 0.00077701
Iteration 8/25 | Loss: 0.00076733
Iteration 9/25 | Loss: 0.00075780
Iteration 10/25 | Loss: 0.00075297
Iteration 11/25 | Loss: 0.00075053
Iteration 12/25 | Loss: 0.00074978
Iteration 13/25 | Loss: 0.00074858
Iteration 14/25 | Loss: 0.00074764
Iteration 15/25 | Loss: 0.00074723
Iteration 16/25 | Loss: 0.00074712
Iteration 17/25 | Loss: 0.00074704
Iteration 18/25 | Loss: 0.00074700
Iteration 19/25 | Loss: 0.00074699
Iteration 20/25 | Loss: 0.00074699
Iteration 21/25 | Loss: 0.00074699
Iteration 22/25 | Loss: 0.00074699
Iteration 23/25 | Loss: 0.00074699
Iteration 24/25 | Loss: 0.00074699
Iteration 25/25 | Loss: 0.00074699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06523037
Iteration 2/25 | Loss: 0.00089570
Iteration 3/25 | Loss: 0.00089570
Iteration 4/25 | Loss: 0.00089570
Iteration 5/25 | Loss: 0.00089570
Iteration 6/25 | Loss: 0.00089570
Iteration 7/25 | Loss: 0.00089570
Iteration 8/25 | Loss: 0.00089570
Iteration 9/25 | Loss: 0.00089570
Iteration 10/25 | Loss: 0.00089570
Iteration 11/25 | Loss: 0.00089570
Iteration 12/25 | Loss: 0.00089570
Iteration 13/25 | Loss: 0.00089570
Iteration 14/25 | Loss: 0.00089570
Iteration 15/25 | Loss: 0.00089570
Iteration 16/25 | Loss: 0.00089570
Iteration 17/25 | Loss: 0.00089570
Iteration 18/25 | Loss: 0.00089570
Iteration 19/25 | Loss: 0.00089570
Iteration 20/25 | Loss: 0.00089570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008956980309449136, 0.0008956980309449136, 0.0008956980309449136, 0.0008956980309449136, 0.0008956980309449136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008956980309449136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089570
Iteration 2/1000 | Loss: 0.00002950
Iteration 3/1000 | Loss: 0.00001946
Iteration 4/1000 | Loss: 0.00001805
Iteration 5/1000 | Loss: 0.00001723
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001648
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001615
Iteration 10/1000 | Loss: 0.00001597
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001569
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001557
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001545
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001540
Iteration 20/1000 | Loss: 0.00001537
Iteration 21/1000 | Loss: 0.00001535
Iteration 22/1000 | Loss: 0.00001535
Iteration 23/1000 | Loss: 0.00001533
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001531
Iteration 29/1000 | Loss: 0.00001528
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001527
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001526
Iteration 39/1000 | Loss: 0.00001526
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001526
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001525
Iteration 45/1000 | Loss: 0.00001525
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001520
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001517
Iteration 60/1000 | Loss: 0.00001517
Iteration 61/1000 | Loss: 0.00001517
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001516
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001515
Iteration 74/1000 | Loss: 0.00001515
Iteration 75/1000 | Loss: 0.00001515
Iteration 76/1000 | Loss: 0.00001515
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Iteration 105/1000 | Loss: 0.00001512
Iteration 106/1000 | Loss: 0.00001512
Iteration 107/1000 | Loss: 0.00001512
Iteration 108/1000 | Loss: 0.00001512
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001512
Iteration 114/1000 | Loss: 0.00001512
Iteration 115/1000 | Loss: 0.00001512
Iteration 116/1000 | Loss: 0.00001512
Iteration 117/1000 | Loss: 0.00001512
Iteration 118/1000 | Loss: 0.00001512
Iteration 119/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.5116678696358576e-05, 1.5116678696358576e-05, 1.5116678696358576e-05, 1.5116678696358576e-05, 1.5116678696358576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5116678696358576e-05

Optimization complete. Final v2v error: 3.2938950061798096 mm

Highest mean error: 3.6908068656921387 mm for frame 73

Lowest mean error: 3.0364952087402344 mm for frame 111

Saving results

Total time: 55.154515743255615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846749
Iteration 2/25 | Loss: 0.00118383
Iteration 3/25 | Loss: 0.00080651
Iteration 4/25 | Loss: 0.00073898
Iteration 5/25 | Loss: 0.00072539
Iteration 6/25 | Loss: 0.00072162
Iteration 7/25 | Loss: 0.00072006
Iteration 8/25 | Loss: 0.00071955
Iteration 9/25 | Loss: 0.00071955
Iteration 10/25 | Loss: 0.00071955
Iteration 11/25 | Loss: 0.00071955
Iteration 12/25 | Loss: 0.00071955
Iteration 13/25 | Loss: 0.00071955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007195451762527227, 0.0007195451762527227, 0.0007195451762527227, 0.0007195451762527227, 0.0007195451762527227]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007195451762527227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53966248
Iteration 2/25 | Loss: 0.00087910
Iteration 3/25 | Loss: 0.00087909
Iteration 4/25 | Loss: 0.00087909
Iteration 5/25 | Loss: 0.00087909
Iteration 6/25 | Loss: 0.00087909
Iteration 7/25 | Loss: 0.00087909
Iteration 8/25 | Loss: 0.00087909
Iteration 9/25 | Loss: 0.00087909
Iteration 10/25 | Loss: 0.00087909
Iteration 11/25 | Loss: 0.00087909
Iteration 12/25 | Loss: 0.00087909
Iteration 13/25 | Loss: 0.00087909
Iteration 14/25 | Loss: 0.00087909
Iteration 15/25 | Loss: 0.00087909
Iteration 16/25 | Loss: 0.00087909
Iteration 17/25 | Loss: 0.00087909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008790921419858932, 0.0008790921419858932, 0.0008790921419858932, 0.0008790921419858932, 0.0008790921419858932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008790921419858932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087909
Iteration 2/1000 | Loss: 0.00002377
Iteration 3/1000 | Loss: 0.00001568
Iteration 4/1000 | Loss: 0.00001413
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001246
Iteration 9/1000 | Loss: 0.00001244
Iteration 10/1000 | Loss: 0.00001242
Iteration 11/1000 | Loss: 0.00001239
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001215
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001213
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001210
Iteration 35/1000 | Loss: 0.00001209
Iteration 36/1000 | Loss: 0.00001208
Iteration 37/1000 | Loss: 0.00001207
Iteration 38/1000 | Loss: 0.00001207
Iteration 39/1000 | Loss: 0.00001207
Iteration 40/1000 | Loss: 0.00001206
Iteration 41/1000 | Loss: 0.00001206
Iteration 42/1000 | Loss: 0.00001206
Iteration 43/1000 | Loss: 0.00001206
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001205
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001203
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001200
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001191
Iteration 117/1000 | Loss: 0.00001191
Iteration 118/1000 | Loss: 0.00001191
Iteration 119/1000 | Loss: 0.00001191
Iteration 120/1000 | Loss: 0.00001191
Iteration 121/1000 | Loss: 0.00001191
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001191
Iteration 130/1000 | Loss: 0.00001191
Iteration 131/1000 | Loss: 0.00001191
Iteration 132/1000 | Loss: 0.00001191
Iteration 133/1000 | Loss: 0.00001191
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.190871171274921e-05, 1.190871171274921e-05, 1.190871171274921e-05, 1.190871171274921e-05, 1.190871171274921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.190871171274921e-05

Optimization complete. Final v2v error: 2.906822681427002 mm

Highest mean error: 3.161750555038452 mm for frame 100

Lowest mean error: 2.684258460998535 mm for frame 185

Saving results

Total time: 36.11359643936157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843596
Iteration 2/25 | Loss: 0.00101463
Iteration 3/25 | Loss: 0.00076540
Iteration 4/25 | Loss: 0.00073459
Iteration 5/25 | Loss: 0.00072881
Iteration 6/25 | Loss: 0.00072664
Iteration 7/25 | Loss: 0.00072610
Iteration 8/25 | Loss: 0.00072610
Iteration 9/25 | Loss: 0.00072610
Iteration 10/25 | Loss: 0.00072610
Iteration 11/25 | Loss: 0.00072610
Iteration 12/25 | Loss: 0.00072610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007261017453856766, 0.0007261017453856766, 0.0007261017453856766, 0.0007261017453856766, 0.0007261017453856766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007261017453856766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54481828
Iteration 2/25 | Loss: 0.00094040
Iteration 3/25 | Loss: 0.00094040
Iteration 4/25 | Loss: 0.00094040
Iteration 5/25 | Loss: 0.00094040
Iteration 6/25 | Loss: 0.00094040
Iteration 7/25 | Loss: 0.00094040
Iteration 8/25 | Loss: 0.00094040
Iteration 9/25 | Loss: 0.00094040
Iteration 10/25 | Loss: 0.00094040
Iteration 11/25 | Loss: 0.00094040
Iteration 12/25 | Loss: 0.00094040
Iteration 13/25 | Loss: 0.00094040
Iteration 14/25 | Loss: 0.00094040
Iteration 15/25 | Loss: 0.00094040
Iteration 16/25 | Loss: 0.00094040
Iteration 17/25 | Loss: 0.00094040
Iteration 18/25 | Loss: 0.00094040
Iteration 19/25 | Loss: 0.00094040
Iteration 20/25 | Loss: 0.00094040
Iteration 21/25 | Loss: 0.00094040
Iteration 22/25 | Loss: 0.00094040
Iteration 23/25 | Loss: 0.00094040
Iteration 24/25 | Loss: 0.00094040
Iteration 25/25 | Loss: 0.00094040

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094040
Iteration 2/1000 | Loss: 0.00002676
Iteration 3/1000 | Loss: 0.00001535
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001345
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001208
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001193
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001184
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001182
Iteration 29/1000 | Loss: 0.00001180
Iteration 30/1000 | Loss: 0.00001177
Iteration 31/1000 | Loss: 0.00001175
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001173
Iteration 37/1000 | Loss: 0.00001173
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001171
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001169
Iteration 50/1000 | Loss: 0.00001169
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001168
Iteration 53/1000 | Loss: 0.00001168
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001167
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001164
Iteration 65/1000 | Loss: 0.00001164
Iteration 66/1000 | Loss: 0.00001164
Iteration 67/1000 | Loss: 0.00001164
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001163
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001162
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001161
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001160
Iteration 132/1000 | Loss: 0.00001160
Iteration 133/1000 | Loss: 0.00001160
Iteration 134/1000 | Loss: 0.00001160
Iteration 135/1000 | Loss: 0.00001160
Iteration 136/1000 | Loss: 0.00001160
Iteration 137/1000 | Loss: 0.00001160
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001160
Iteration 140/1000 | Loss: 0.00001160
Iteration 141/1000 | Loss: 0.00001160
Iteration 142/1000 | Loss: 0.00001160
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00001160
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001160
Iteration 148/1000 | Loss: 0.00001160
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.1598176570259966e-05, 1.1598176570259966e-05, 1.1598176570259966e-05, 1.1598176570259966e-05, 1.1598176570259966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1598176570259966e-05

Optimization complete. Final v2v error: 2.8620948791503906 mm

Highest mean error: 3.190188407897949 mm for frame 114

Lowest mean error: 2.755396842956543 mm for frame 64

Saving results

Total time: 40.06484794616699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759775
Iteration 2/25 | Loss: 0.00157422
Iteration 3/25 | Loss: 0.00099703
Iteration 4/25 | Loss: 0.00090131
Iteration 5/25 | Loss: 0.00088710
Iteration 6/25 | Loss: 0.00092153
Iteration 7/25 | Loss: 0.00089275
Iteration 8/25 | Loss: 0.00088691
Iteration 9/25 | Loss: 0.00091556
Iteration 10/25 | Loss: 0.00088594
Iteration 11/25 | Loss: 0.00088141
Iteration 12/25 | Loss: 0.00087732
Iteration 13/25 | Loss: 0.00088054
Iteration 14/25 | Loss: 0.00087239
Iteration 15/25 | Loss: 0.00087343
Iteration 16/25 | Loss: 0.00087350
Iteration 17/25 | Loss: 0.00087325
Iteration 18/25 | Loss: 0.00086836
Iteration 19/25 | Loss: 0.00086743
Iteration 20/25 | Loss: 0.00086682
Iteration 21/25 | Loss: 0.00086372
Iteration 22/25 | Loss: 0.00086193
Iteration 23/25 | Loss: 0.00086162
Iteration 24/25 | Loss: 0.00086154
Iteration 25/25 | Loss: 0.00086154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80886626
Iteration 2/25 | Loss: 0.00086693
Iteration 3/25 | Loss: 0.00086689
Iteration 4/25 | Loss: 0.00086689
Iteration 5/25 | Loss: 0.00086689
Iteration 6/25 | Loss: 0.00086689
Iteration 7/25 | Loss: 0.00086689
Iteration 8/25 | Loss: 0.00086689
Iteration 9/25 | Loss: 0.00086689
Iteration 10/25 | Loss: 0.00086689
Iteration 11/25 | Loss: 0.00086689
Iteration 12/25 | Loss: 0.00086689
Iteration 13/25 | Loss: 0.00086689
Iteration 14/25 | Loss: 0.00086689
Iteration 15/25 | Loss: 0.00086689
Iteration 16/25 | Loss: 0.00086689
Iteration 17/25 | Loss: 0.00086689
Iteration 18/25 | Loss: 0.00086689
Iteration 19/25 | Loss: 0.00086689
Iteration 20/25 | Loss: 0.00086689
Iteration 21/25 | Loss: 0.00086689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008668898954056203, 0.0008668898954056203, 0.0008668898954056203, 0.0008668898954056203, 0.0008668898954056203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008668898954056203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086689
Iteration 2/1000 | Loss: 0.00005169
Iteration 3/1000 | Loss: 0.00003415
Iteration 4/1000 | Loss: 0.00002888
Iteration 5/1000 | Loss: 0.00002700
Iteration 6/1000 | Loss: 0.00002573
Iteration 7/1000 | Loss: 0.00002502
Iteration 8/1000 | Loss: 0.00002454
Iteration 9/1000 | Loss: 0.00002414
Iteration 10/1000 | Loss: 0.00002387
Iteration 11/1000 | Loss: 0.00002368
Iteration 12/1000 | Loss: 0.00002358
Iteration 13/1000 | Loss: 0.00002355
Iteration 14/1000 | Loss: 0.00002342
Iteration 15/1000 | Loss: 0.00002331
Iteration 16/1000 | Loss: 0.00002331
Iteration 17/1000 | Loss: 0.00002329
Iteration 18/1000 | Loss: 0.00002328
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002326
Iteration 21/1000 | Loss: 0.00002326
Iteration 22/1000 | Loss: 0.00002326
Iteration 23/1000 | Loss: 0.00002324
Iteration 24/1000 | Loss: 0.00002324
Iteration 25/1000 | Loss: 0.00002323
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002322
Iteration 28/1000 | Loss: 0.00002322
Iteration 29/1000 | Loss: 0.00002322
Iteration 30/1000 | Loss: 0.00002322
Iteration 31/1000 | Loss: 0.00002322
Iteration 32/1000 | Loss: 0.00002322
Iteration 33/1000 | Loss: 0.00002322
Iteration 34/1000 | Loss: 0.00002321
Iteration 35/1000 | Loss: 0.00002321
Iteration 36/1000 | Loss: 0.00002320
Iteration 37/1000 | Loss: 0.00002320
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002319
Iteration 40/1000 | Loss: 0.00002319
Iteration 41/1000 | Loss: 0.00002319
Iteration 42/1000 | Loss: 0.00002318
Iteration 43/1000 | Loss: 0.00002318
Iteration 44/1000 | Loss: 0.00002318
Iteration 45/1000 | Loss: 0.00002317
Iteration 46/1000 | Loss: 0.00002317
Iteration 47/1000 | Loss: 0.00002317
Iteration 48/1000 | Loss: 0.00002317
Iteration 49/1000 | Loss: 0.00002316
Iteration 50/1000 | Loss: 0.00002316
Iteration 51/1000 | Loss: 0.00002316
Iteration 52/1000 | Loss: 0.00002315
Iteration 53/1000 | Loss: 0.00002315
Iteration 54/1000 | Loss: 0.00002315
Iteration 55/1000 | Loss: 0.00002315
Iteration 56/1000 | Loss: 0.00002315
Iteration 57/1000 | Loss: 0.00002315
Iteration 58/1000 | Loss: 0.00002315
Iteration 59/1000 | Loss: 0.00002315
Iteration 60/1000 | Loss: 0.00002315
Iteration 61/1000 | Loss: 0.00002314
Iteration 62/1000 | Loss: 0.00002314
Iteration 63/1000 | Loss: 0.00002314
Iteration 64/1000 | Loss: 0.00002314
Iteration 65/1000 | Loss: 0.00002314
Iteration 66/1000 | Loss: 0.00002314
Iteration 67/1000 | Loss: 0.00002314
Iteration 68/1000 | Loss: 0.00002314
Iteration 69/1000 | Loss: 0.00002314
Iteration 70/1000 | Loss: 0.00002313
Iteration 71/1000 | Loss: 0.00002313
Iteration 72/1000 | Loss: 0.00002313
Iteration 73/1000 | Loss: 0.00002313
Iteration 74/1000 | Loss: 0.00002313
Iteration 75/1000 | Loss: 0.00002313
Iteration 76/1000 | Loss: 0.00002313
Iteration 77/1000 | Loss: 0.00002312
Iteration 78/1000 | Loss: 0.00002312
Iteration 79/1000 | Loss: 0.00002312
Iteration 80/1000 | Loss: 0.00002312
Iteration 81/1000 | Loss: 0.00002312
Iteration 82/1000 | Loss: 0.00002311
Iteration 83/1000 | Loss: 0.00002311
Iteration 84/1000 | Loss: 0.00002311
Iteration 85/1000 | Loss: 0.00002311
Iteration 86/1000 | Loss: 0.00002311
Iteration 87/1000 | Loss: 0.00002310
Iteration 88/1000 | Loss: 0.00002310
Iteration 89/1000 | Loss: 0.00002310
Iteration 90/1000 | Loss: 0.00002310
Iteration 91/1000 | Loss: 0.00002309
Iteration 92/1000 | Loss: 0.00002309
Iteration 93/1000 | Loss: 0.00002309
Iteration 94/1000 | Loss: 0.00002309
Iteration 95/1000 | Loss: 0.00002309
Iteration 96/1000 | Loss: 0.00002308
Iteration 97/1000 | Loss: 0.00002308
Iteration 98/1000 | Loss: 0.00002308
Iteration 99/1000 | Loss: 0.00002308
Iteration 100/1000 | Loss: 0.00002308
Iteration 101/1000 | Loss: 0.00002308
Iteration 102/1000 | Loss: 0.00002308
Iteration 103/1000 | Loss: 0.00002307
Iteration 104/1000 | Loss: 0.00002307
Iteration 105/1000 | Loss: 0.00002307
Iteration 106/1000 | Loss: 0.00002307
Iteration 107/1000 | Loss: 0.00002307
Iteration 108/1000 | Loss: 0.00002306
Iteration 109/1000 | Loss: 0.00002306
Iteration 110/1000 | Loss: 0.00002306
Iteration 111/1000 | Loss: 0.00002305
Iteration 112/1000 | Loss: 0.00002305
Iteration 113/1000 | Loss: 0.00002305
Iteration 114/1000 | Loss: 0.00002305
Iteration 115/1000 | Loss: 0.00002305
Iteration 116/1000 | Loss: 0.00002305
Iteration 117/1000 | Loss: 0.00002305
Iteration 118/1000 | Loss: 0.00002305
Iteration 119/1000 | Loss: 0.00002305
Iteration 120/1000 | Loss: 0.00002305
Iteration 121/1000 | Loss: 0.00002304
Iteration 122/1000 | Loss: 0.00002304
Iteration 123/1000 | Loss: 0.00002304
Iteration 124/1000 | Loss: 0.00002304
Iteration 125/1000 | Loss: 0.00002303
Iteration 126/1000 | Loss: 0.00002303
Iteration 127/1000 | Loss: 0.00002303
Iteration 128/1000 | Loss: 0.00002303
Iteration 129/1000 | Loss: 0.00002302
Iteration 130/1000 | Loss: 0.00002302
Iteration 131/1000 | Loss: 0.00002302
Iteration 132/1000 | Loss: 0.00002302
Iteration 133/1000 | Loss: 0.00002302
Iteration 134/1000 | Loss: 0.00002301
Iteration 135/1000 | Loss: 0.00002301
Iteration 136/1000 | Loss: 0.00002301
Iteration 137/1000 | Loss: 0.00002301
Iteration 138/1000 | Loss: 0.00002301
Iteration 139/1000 | Loss: 0.00002300
Iteration 140/1000 | Loss: 0.00002300
Iteration 141/1000 | Loss: 0.00002300
Iteration 142/1000 | Loss: 0.00002300
Iteration 143/1000 | Loss: 0.00002300
Iteration 144/1000 | Loss: 0.00002300
Iteration 145/1000 | Loss: 0.00002300
Iteration 146/1000 | Loss: 0.00002300
Iteration 147/1000 | Loss: 0.00002300
Iteration 148/1000 | Loss: 0.00002300
Iteration 149/1000 | Loss: 0.00002300
Iteration 150/1000 | Loss: 0.00002299
Iteration 151/1000 | Loss: 0.00002299
Iteration 152/1000 | Loss: 0.00002299
Iteration 153/1000 | Loss: 0.00002299
Iteration 154/1000 | Loss: 0.00002299
Iteration 155/1000 | Loss: 0.00002299
Iteration 156/1000 | Loss: 0.00002298
Iteration 157/1000 | Loss: 0.00002298
Iteration 158/1000 | Loss: 0.00002298
Iteration 159/1000 | Loss: 0.00002298
Iteration 160/1000 | Loss: 0.00002298
Iteration 161/1000 | Loss: 0.00002298
Iteration 162/1000 | Loss: 0.00002297
Iteration 163/1000 | Loss: 0.00002297
Iteration 164/1000 | Loss: 0.00002297
Iteration 165/1000 | Loss: 0.00002297
Iteration 166/1000 | Loss: 0.00002297
Iteration 167/1000 | Loss: 0.00002297
Iteration 168/1000 | Loss: 0.00002297
Iteration 169/1000 | Loss: 0.00002297
Iteration 170/1000 | Loss: 0.00002297
Iteration 171/1000 | Loss: 0.00002297
Iteration 172/1000 | Loss: 0.00002297
Iteration 173/1000 | Loss: 0.00002297
Iteration 174/1000 | Loss: 0.00002297
Iteration 175/1000 | Loss: 0.00002297
Iteration 176/1000 | Loss: 0.00002297
Iteration 177/1000 | Loss: 0.00002297
Iteration 178/1000 | Loss: 0.00002297
Iteration 179/1000 | Loss: 0.00002297
Iteration 180/1000 | Loss: 0.00002297
Iteration 181/1000 | Loss: 0.00002297
Iteration 182/1000 | Loss: 0.00002297
Iteration 183/1000 | Loss: 0.00002297
Iteration 184/1000 | Loss: 0.00002297
Iteration 185/1000 | Loss: 0.00002297
Iteration 186/1000 | Loss: 0.00002297
Iteration 187/1000 | Loss: 0.00002297
Iteration 188/1000 | Loss: 0.00002297
Iteration 189/1000 | Loss: 0.00002297
Iteration 190/1000 | Loss: 0.00002297
Iteration 191/1000 | Loss: 0.00002297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.2973015802563168e-05, 2.2973015802563168e-05, 2.2973015802563168e-05, 2.2973015802563168e-05, 2.2973015802563168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2973015802563168e-05

Optimization complete. Final v2v error: 3.795929431915283 mm

Highest mean error: 4.968189239501953 mm for frame 77

Lowest mean error: 2.8713643550872803 mm for frame 189

Saving results

Total time: 83.99317979812622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052027
Iteration 2/25 | Loss: 0.00338595
Iteration 3/25 | Loss: 0.00219665
Iteration 4/25 | Loss: 0.00189600
Iteration 5/25 | Loss: 0.00176076
Iteration 6/25 | Loss: 0.00169967
Iteration 7/25 | Loss: 0.00166754
Iteration 8/25 | Loss: 0.00163222
Iteration 9/25 | Loss: 0.00160101
Iteration 10/25 | Loss: 0.00159265
Iteration 11/25 | Loss: 0.00158085
Iteration 12/25 | Loss: 0.00157546
Iteration 13/25 | Loss: 0.00157304
Iteration 14/25 | Loss: 0.00157234
Iteration 15/25 | Loss: 0.00157212
Iteration 16/25 | Loss: 0.00157203
Iteration 17/25 | Loss: 0.00157201
Iteration 18/25 | Loss: 0.00157200
Iteration 19/25 | Loss: 0.00157200
Iteration 20/25 | Loss: 0.00157200
Iteration 21/25 | Loss: 0.00157200
Iteration 22/25 | Loss: 0.00157200
Iteration 23/25 | Loss: 0.00157199
Iteration 24/25 | Loss: 0.00157199
Iteration 25/25 | Loss: 0.00157199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51790881
Iteration 2/25 | Loss: 0.00451150
Iteration 3/25 | Loss: 0.00451150
Iteration 4/25 | Loss: 0.00451150
Iteration 5/25 | Loss: 0.00451149
Iteration 6/25 | Loss: 0.00451149
Iteration 7/25 | Loss: 0.00451149
Iteration 8/25 | Loss: 0.00451149
Iteration 9/25 | Loss: 0.00451149
Iteration 10/25 | Loss: 0.00451149
Iteration 11/25 | Loss: 0.00451149
Iteration 12/25 | Loss: 0.00451149
Iteration 13/25 | Loss: 0.00451149
Iteration 14/25 | Loss: 0.00451149
Iteration 15/25 | Loss: 0.00451149
Iteration 16/25 | Loss: 0.00451149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0045114923268556595, 0.0045114923268556595, 0.0045114923268556595, 0.0045114923268556595, 0.0045114923268556595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045114923268556595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00451149
Iteration 2/1000 | Loss: 0.00076036
Iteration 3/1000 | Loss: 0.00056074
Iteration 4/1000 | Loss: 0.00048872
Iteration 5/1000 | Loss: 0.00043529
Iteration 6/1000 | Loss: 0.00038825
Iteration 7/1000 | Loss: 0.03278621
Iteration 8/1000 | Loss: 0.00273616
Iteration 9/1000 | Loss: 0.00041100
Iteration 10/1000 | Loss: 0.00026001
Iteration 11/1000 | Loss: 0.00016448
Iteration 12/1000 | Loss: 0.00010406
Iteration 13/1000 | Loss: 0.00006916
Iteration 14/1000 | Loss: 0.00005093
Iteration 15/1000 | Loss: 0.00004013
Iteration 16/1000 | Loss: 0.00003119
Iteration 17/1000 | Loss: 0.00002696
Iteration 18/1000 | Loss: 0.00002431
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002080
Iteration 21/1000 | Loss: 0.00001942
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001769
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001708
Iteration 26/1000 | Loss: 0.00001692
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001681
Iteration 29/1000 | Loss: 0.00001679
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00001676
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001673
Iteration 34/1000 | Loss: 0.00001673
Iteration 35/1000 | Loss: 0.00001672
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001672
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001669
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001664
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001660
Iteration 46/1000 | Loss: 0.00001659
Iteration 47/1000 | Loss: 0.00001658
Iteration 48/1000 | Loss: 0.00001658
Iteration 49/1000 | Loss: 0.00001658
Iteration 50/1000 | Loss: 0.00001658
Iteration 51/1000 | Loss: 0.00001658
Iteration 52/1000 | Loss: 0.00001658
Iteration 53/1000 | Loss: 0.00001658
Iteration 54/1000 | Loss: 0.00001657
Iteration 55/1000 | Loss: 0.00001657
Iteration 56/1000 | Loss: 0.00001656
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001656
Iteration 60/1000 | Loss: 0.00001655
Iteration 61/1000 | Loss: 0.00001655
Iteration 62/1000 | Loss: 0.00001655
Iteration 63/1000 | Loss: 0.00001655
Iteration 64/1000 | Loss: 0.00001655
Iteration 65/1000 | Loss: 0.00001654
Iteration 66/1000 | Loss: 0.00001654
Iteration 67/1000 | Loss: 0.00001654
Iteration 68/1000 | Loss: 0.00001654
Iteration 69/1000 | Loss: 0.00001654
Iteration 70/1000 | Loss: 0.00001654
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001654
Iteration 76/1000 | Loss: 0.00001654
Iteration 77/1000 | Loss: 0.00001654
Iteration 78/1000 | Loss: 0.00001654
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001654
Iteration 82/1000 | Loss: 0.00001654
Iteration 83/1000 | Loss: 0.00001654
Iteration 84/1000 | Loss: 0.00001654
Iteration 85/1000 | Loss: 0.00001654
Iteration 86/1000 | Loss: 0.00001654
Iteration 87/1000 | Loss: 0.00001654
Iteration 88/1000 | Loss: 0.00001654
Iteration 89/1000 | Loss: 0.00001654
Iteration 90/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.6541627701371908e-05, 1.6541627701371908e-05, 1.6541627701371908e-05, 1.6541627701371908e-05, 1.6541627701371908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6541627701371908e-05

Optimization complete. Final v2v error: 3.4210712909698486 mm

Highest mean error: 3.475863456726074 mm for frame 73

Lowest mean error: 3.25079083442688 mm for frame 13

Saving results

Total time: 70.45882511138916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464198
Iteration 2/25 | Loss: 0.00083961
Iteration 3/25 | Loss: 0.00072651
Iteration 4/25 | Loss: 0.00070967
Iteration 5/25 | Loss: 0.00070517
Iteration 6/25 | Loss: 0.00070337
Iteration 7/25 | Loss: 0.00070274
Iteration 8/25 | Loss: 0.00070274
Iteration 9/25 | Loss: 0.00070274
Iteration 10/25 | Loss: 0.00070274
Iteration 11/25 | Loss: 0.00070274
Iteration 12/25 | Loss: 0.00070274
Iteration 13/25 | Loss: 0.00070274
Iteration 14/25 | Loss: 0.00070274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007027374813333154, 0.0007027374813333154, 0.0007027374813333154, 0.0007027374813333154, 0.0007027374813333154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007027374813333154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52010798
Iteration 2/25 | Loss: 0.00066881
Iteration 3/25 | Loss: 0.00066879
Iteration 4/25 | Loss: 0.00066879
Iteration 5/25 | Loss: 0.00066879
Iteration 6/25 | Loss: 0.00066879
Iteration 7/25 | Loss: 0.00066879
Iteration 8/25 | Loss: 0.00066879
Iteration 9/25 | Loss: 0.00066879
Iteration 10/25 | Loss: 0.00066879
Iteration 11/25 | Loss: 0.00066879
Iteration 12/25 | Loss: 0.00066879
Iteration 13/25 | Loss: 0.00066879
Iteration 14/25 | Loss: 0.00066879
Iteration 15/25 | Loss: 0.00066879
Iteration 16/25 | Loss: 0.00066879
Iteration 17/25 | Loss: 0.00066879
Iteration 18/25 | Loss: 0.00066879
Iteration 19/25 | Loss: 0.00066879
Iteration 20/25 | Loss: 0.00066879
Iteration 21/25 | Loss: 0.00066879
Iteration 22/25 | Loss: 0.00066879
Iteration 23/25 | Loss: 0.00066879
Iteration 24/25 | Loss: 0.00066879
Iteration 25/25 | Loss: 0.00066879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006687851273454726, 0.0006687851273454726, 0.0006687851273454726, 0.0006687851273454726, 0.0006687851273454726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006687851273454726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066879
Iteration 2/1000 | Loss: 0.00003211
Iteration 3/1000 | Loss: 0.00001921
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001291
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001252
Iteration 9/1000 | Loss: 0.00001240
Iteration 10/1000 | Loss: 0.00001232
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001224
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001220
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001210
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001205
Iteration 44/1000 | Loss: 0.00001205
Iteration 45/1000 | Loss: 0.00001205
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001199
Iteration 68/1000 | Loss: 0.00001199
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001197
Iteration 73/1000 | Loss: 0.00001197
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001191
Iteration 102/1000 | Loss: 0.00001191
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001190
Iteration 118/1000 | Loss: 0.00001190
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001189
Iteration 126/1000 | Loss: 0.00001189
Iteration 127/1000 | Loss: 0.00001189
Iteration 128/1000 | Loss: 0.00001189
Iteration 129/1000 | Loss: 0.00001189
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001187
Iteration 158/1000 | Loss: 0.00001187
Iteration 159/1000 | Loss: 0.00001187
Iteration 160/1000 | Loss: 0.00001187
Iteration 161/1000 | Loss: 0.00001187
Iteration 162/1000 | Loss: 0.00001187
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001186
Iteration 169/1000 | Loss: 0.00001186
Iteration 170/1000 | Loss: 0.00001186
Iteration 171/1000 | Loss: 0.00001186
Iteration 172/1000 | Loss: 0.00001186
Iteration 173/1000 | Loss: 0.00001186
Iteration 174/1000 | Loss: 0.00001186
Iteration 175/1000 | Loss: 0.00001186
Iteration 176/1000 | Loss: 0.00001186
Iteration 177/1000 | Loss: 0.00001186
Iteration 178/1000 | Loss: 0.00001186
Iteration 179/1000 | Loss: 0.00001186
Iteration 180/1000 | Loss: 0.00001186
Iteration 181/1000 | Loss: 0.00001186
Iteration 182/1000 | Loss: 0.00001186
Iteration 183/1000 | Loss: 0.00001186
Iteration 184/1000 | Loss: 0.00001186
Iteration 185/1000 | Loss: 0.00001186
Iteration 186/1000 | Loss: 0.00001186
Iteration 187/1000 | Loss: 0.00001186
Iteration 188/1000 | Loss: 0.00001186
Iteration 189/1000 | Loss: 0.00001186
Iteration 190/1000 | Loss: 0.00001186
Iteration 191/1000 | Loss: 0.00001186
Iteration 192/1000 | Loss: 0.00001186
Iteration 193/1000 | Loss: 0.00001186
Iteration 194/1000 | Loss: 0.00001186
Iteration 195/1000 | Loss: 0.00001186
Iteration 196/1000 | Loss: 0.00001186
Iteration 197/1000 | Loss: 0.00001186
Iteration 198/1000 | Loss: 0.00001186
Iteration 199/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1856099263241049e-05, 1.1856099263241049e-05, 1.1856099263241049e-05, 1.1856099263241049e-05, 1.1856099263241049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1856099263241049e-05

Optimization complete. Final v2v error: 2.7904207706451416 mm

Highest mean error: 3.5209743976593018 mm for frame 70

Lowest mean error: 2.5217196941375732 mm for frame 116

Saving results

Total time: 36.0742347240448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026246
Iteration 2/25 | Loss: 0.01026246
Iteration 3/25 | Loss: 0.00270560
Iteration 4/25 | Loss: 0.00169197
Iteration 5/25 | Loss: 0.00133195
Iteration 6/25 | Loss: 0.00171595
Iteration 7/25 | Loss: 0.00160988
Iteration 8/25 | Loss: 0.00138571
Iteration 9/25 | Loss: 0.00104718
Iteration 10/25 | Loss: 0.00100373
Iteration 11/25 | Loss: 0.00099105
Iteration 12/25 | Loss: 0.00099062
Iteration 13/25 | Loss: 0.00096640
Iteration 14/25 | Loss: 0.00094040
Iteration 15/25 | Loss: 0.00097338
Iteration 16/25 | Loss: 0.00093810
Iteration 17/25 | Loss: 0.00093203
Iteration 18/25 | Loss: 0.00093046
Iteration 19/25 | Loss: 0.00094947
Iteration 20/25 | Loss: 0.00093762
Iteration 21/25 | Loss: 0.00095081
Iteration 22/25 | Loss: 0.00095084
Iteration 23/25 | Loss: 0.00092410
Iteration 24/25 | Loss: 0.00093289
Iteration 25/25 | Loss: 0.00091442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49560082
Iteration 2/25 | Loss: 0.00111770
Iteration 3/25 | Loss: 0.00111770
Iteration 4/25 | Loss: 0.00111770
Iteration 5/25 | Loss: 0.00111770
Iteration 6/25 | Loss: 0.00111770
Iteration 7/25 | Loss: 0.00111770
Iteration 8/25 | Loss: 0.00111770
Iteration 9/25 | Loss: 0.00111770
Iteration 10/25 | Loss: 0.00111770
Iteration 11/25 | Loss: 0.00111770
Iteration 12/25 | Loss: 0.00111770
Iteration 13/25 | Loss: 0.00111770
Iteration 14/25 | Loss: 0.00111770
Iteration 15/25 | Loss: 0.00111770
Iteration 16/25 | Loss: 0.00111770
Iteration 17/25 | Loss: 0.00111770
Iteration 18/25 | Loss: 0.00111770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011176973348483443, 0.0011176973348483443, 0.0011176973348483443, 0.0011176973348483443, 0.0011176973348483443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011176973348483443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111770
Iteration 2/1000 | Loss: 0.00106036
Iteration 3/1000 | Loss: 0.00134956
Iteration 4/1000 | Loss: 0.00083268
Iteration 5/1000 | Loss: 0.00243386
Iteration 6/1000 | Loss: 0.00069877
Iteration 7/1000 | Loss: 0.00086187
Iteration 8/1000 | Loss: 0.00008906
Iteration 9/1000 | Loss: 0.00009953
Iteration 10/1000 | Loss: 0.00006671
Iteration 11/1000 | Loss: 0.00006144
Iteration 12/1000 | Loss: 0.00038860
Iteration 13/1000 | Loss: 0.00041019
Iteration 14/1000 | Loss: 0.00006091
Iteration 15/1000 | Loss: 0.00045896
Iteration 16/1000 | Loss: 0.00005886
Iteration 17/1000 | Loss: 0.00005526
Iteration 18/1000 | Loss: 0.00077650
Iteration 19/1000 | Loss: 0.00092851
Iteration 20/1000 | Loss: 0.00107159
Iteration 21/1000 | Loss: 0.00123246
Iteration 22/1000 | Loss: 0.00039351
Iteration 23/1000 | Loss: 0.00021537
Iteration 24/1000 | Loss: 0.00013196
Iteration 25/1000 | Loss: 0.00011938
Iteration 26/1000 | Loss: 0.00010464
Iteration 27/1000 | Loss: 0.00014827
Iteration 28/1000 | Loss: 0.00008464
Iteration 29/1000 | Loss: 0.00010605
Iteration 30/1000 | Loss: 0.00014916
Iteration 31/1000 | Loss: 0.00010159
Iteration 32/1000 | Loss: 0.00013697
Iteration 33/1000 | Loss: 0.00009821
Iteration 34/1000 | Loss: 0.00011011
Iteration 35/1000 | Loss: 0.00005280
Iteration 36/1000 | Loss: 0.00004728
Iteration 37/1000 | Loss: 0.00004406
Iteration 38/1000 | Loss: 0.00004180
Iteration 39/1000 | Loss: 0.00004042
Iteration 40/1000 | Loss: 0.00003952
Iteration 41/1000 | Loss: 0.00003880
Iteration 42/1000 | Loss: 0.00031119
Iteration 43/1000 | Loss: 0.00013083
Iteration 44/1000 | Loss: 0.00004020
Iteration 45/1000 | Loss: 0.00003837
Iteration 46/1000 | Loss: 0.00031245
Iteration 47/1000 | Loss: 0.00026033
Iteration 48/1000 | Loss: 0.00027020
Iteration 49/1000 | Loss: 0.00014097
Iteration 50/1000 | Loss: 0.00004915
Iteration 51/1000 | Loss: 0.00004471
Iteration 52/1000 | Loss: 0.00004248
Iteration 53/1000 | Loss: 0.00004032
Iteration 54/1000 | Loss: 0.00003859
Iteration 55/1000 | Loss: 0.00003736
Iteration 56/1000 | Loss: 0.00003676
Iteration 57/1000 | Loss: 0.00003613
Iteration 58/1000 | Loss: 0.00003578
Iteration 59/1000 | Loss: 0.00003555
Iteration 60/1000 | Loss: 0.00003538
Iteration 61/1000 | Loss: 0.00003535
Iteration 62/1000 | Loss: 0.00003532
Iteration 63/1000 | Loss: 0.00003530
Iteration 64/1000 | Loss: 0.00003529
Iteration 65/1000 | Loss: 0.00003529
Iteration 66/1000 | Loss: 0.00003528
Iteration 67/1000 | Loss: 0.00003528
Iteration 68/1000 | Loss: 0.00003528
Iteration 69/1000 | Loss: 0.00003526
Iteration 70/1000 | Loss: 0.00003526
Iteration 71/1000 | Loss: 0.00003526
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00003525
Iteration 74/1000 | Loss: 0.00003525
Iteration 75/1000 | Loss: 0.00003525
Iteration 76/1000 | Loss: 0.00003525
Iteration 77/1000 | Loss: 0.00003525
Iteration 78/1000 | Loss: 0.00003525
Iteration 79/1000 | Loss: 0.00003525
Iteration 80/1000 | Loss: 0.00003524
Iteration 81/1000 | Loss: 0.00003524
Iteration 82/1000 | Loss: 0.00003524
Iteration 83/1000 | Loss: 0.00003524
Iteration 84/1000 | Loss: 0.00003523
Iteration 85/1000 | Loss: 0.00003523
Iteration 86/1000 | Loss: 0.00003523
Iteration 87/1000 | Loss: 0.00003523
Iteration 88/1000 | Loss: 0.00003523
Iteration 89/1000 | Loss: 0.00003523
Iteration 90/1000 | Loss: 0.00003522
Iteration 91/1000 | Loss: 0.00003522
Iteration 92/1000 | Loss: 0.00003522
Iteration 93/1000 | Loss: 0.00003522
Iteration 94/1000 | Loss: 0.00003521
Iteration 95/1000 | Loss: 0.00003521
Iteration 96/1000 | Loss: 0.00003521
Iteration 97/1000 | Loss: 0.00003521
Iteration 98/1000 | Loss: 0.00003521
Iteration 99/1000 | Loss: 0.00003521
Iteration 100/1000 | Loss: 0.00003521
Iteration 101/1000 | Loss: 0.00003520
Iteration 102/1000 | Loss: 0.00003520
Iteration 103/1000 | Loss: 0.00003520
Iteration 104/1000 | Loss: 0.00003520
Iteration 105/1000 | Loss: 0.00003520
Iteration 106/1000 | Loss: 0.00003520
Iteration 107/1000 | Loss: 0.00003520
Iteration 108/1000 | Loss: 0.00003520
Iteration 109/1000 | Loss: 0.00003520
Iteration 110/1000 | Loss: 0.00003520
Iteration 111/1000 | Loss: 0.00003519
Iteration 112/1000 | Loss: 0.00003519
Iteration 113/1000 | Loss: 0.00003519
Iteration 114/1000 | Loss: 0.00003519
Iteration 115/1000 | Loss: 0.00003519
Iteration 116/1000 | Loss: 0.00003519
Iteration 117/1000 | Loss: 0.00003519
Iteration 118/1000 | Loss: 0.00003519
Iteration 119/1000 | Loss: 0.00003519
Iteration 120/1000 | Loss: 0.00003519
Iteration 121/1000 | Loss: 0.00003519
Iteration 122/1000 | Loss: 0.00003519
Iteration 123/1000 | Loss: 0.00003519
Iteration 124/1000 | Loss: 0.00003519
Iteration 125/1000 | Loss: 0.00003519
Iteration 126/1000 | Loss: 0.00003519
Iteration 127/1000 | Loss: 0.00003518
Iteration 128/1000 | Loss: 0.00003518
Iteration 129/1000 | Loss: 0.00003518
Iteration 130/1000 | Loss: 0.00003518
Iteration 131/1000 | Loss: 0.00003518
Iteration 132/1000 | Loss: 0.00003518
Iteration 133/1000 | Loss: 0.00003518
Iteration 134/1000 | Loss: 0.00003518
Iteration 135/1000 | Loss: 0.00003518
Iteration 136/1000 | Loss: 0.00003518
Iteration 137/1000 | Loss: 0.00003518
Iteration 138/1000 | Loss: 0.00003518
Iteration 139/1000 | Loss: 0.00003518
Iteration 140/1000 | Loss: 0.00003518
Iteration 141/1000 | Loss: 0.00003518
Iteration 142/1000 | Loss: 0.00003517
Iteration 143/1000 | Loss: 0.00003517
Iteration 144/1000 | Loss: 0.00003517
Iteration 145/1000 | Loss: 0.00003517
Iteration 146/1000 | Loss: 0.00003517
Iteration 147/1000 | Loss: 0.00003517
Iteration 148/1000 | Loss: 0.00003517
Iteration 149/1000 | Loss: 0.00003517
Iteration 150/1000 | Loss: 0.00003517
Iteration 151/1000 | Loss: 0.00003517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [3.517323420965113e-05, 3.517323420965113e-05, 3.517323420965113e-05, 3.517323420965113e-05, 3.517323420965113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.517323420965113e-05

Optimization complete. Final v2v error: 3.956819534301758 mm

Highest mean error: 10.06486988067627 mm for frame 0

Lowest mean error: 3.1466667652130127 mm for frame 29

Saving results

Total time: 155.56072688102722
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093957
Iteration 2/25 | Loss: 0.01093957
Iteration 3/25 | Loss: 0.00147532
Iteration 4/25 | Loss: 0.00105920
Iteration 5/25 | Loss: 0.00095209
Iteration 6/25 | Loss: 0.00091426
Iteration 7/25 | Loss: 0.00095485
Iteration 8/25 | Loss: 0.00091228
Iteration 9/25 | Loss: 0.00083004
Iteration 10/25 | Loss: 0.00080042
Iteration 11/25 | Loss: 0.00079232
Iteration 12/25 | Loss: 0.00079055
Iteration 13/25 | Loss: 0.00079022
Iteration 14/25 | Loss: 0.00079019
Iteration 15/25 | Loss: 0.00079019
Iteration 16/25 | Loss: 0.00079019
Iteration 17/25 | Loss: 0.00079019
Iteration 18/25 | Loss: 0.00079019
Iteration 19/25 | Loss: 0.00079019
Iteration 20/25 | Loss: 0.00079019
Iteration 21/25 | Loss: 0.00079019
Iteration 22/25 | Loss: 0.00079019
Iteration 23/25 | Loss: 0.00079019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007901876815594733, 0.0007901876815594733, 0.0007901876815594733, 0.0007901876815594733, 0.0007901876815594733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007901876815594733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52996087
Iteration 2/25 | Loss: 0.00093643
Iteration 3/25 | Loss: 0.00093643
Iteration 4/25 | Loss: 0.00093642
Iteration 5/25 | Loss: 0.00093642
Iteration 6/25 | Loss: 0.00093642
Iteration 7/25 | Loss: 0.00093642
Iteration 8/25 | Loss: 0.00093642
Iteration 9/25 | Loss: 0.00093642
Iteration 10/25 | Loss: 0.00093642
Iteration 11/25 | Loss: 0.00093642
Iteration 12/25 | Loss: 0.00093642
Iteration 13/25 | Loss: 0.00093642
Iteration 14/25 | Loss: 0.00093642
Iteration 15/25 | Loss: 0.00093642
Iteration 16/25 | Loss: 0.00093642
Iteration 17/25 | Loss: 0.00093642
Iteration 18/25 | Loss: 0.00093642
Iteration 19/25 | Loss: 0.00093642
Iteration 20/25 | Loss: 0.00093642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009364229626953602, 0.0009364229626953602, 0.0009364229626953602, 0.0009364229626953602, 0.0009364229626953602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009364229626953602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093642
Iteration 2/1000 | Loss: 0.00003566
Iteration 3/1000 | Loss: 0.00002471
Iteration 4/1000 | Loss: 0.00002340
Iteration 5/1000 | Loss: 0.00002202
Iteration 6/1000 | Loss: 0.00002132
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002040
Iteration 10/1000 | Loss: 0.00002027
Iteration 11/1000 | Loss: 0.00002027
Iteration 12/1000 | Loss: 0.00002027
Iteration 13/1000 | Loss: 0.00002021
Iteration 14/1000 | Loss: 0.00002021
Iteration 15/1000 | Loss: 0.00002008
Iteration 16/1000 | Loss: 0.00002007
Iteration 17/1000 | Loss: 0.00002007
Iteration 18/1000 | Loss: 0.00002004
Iteration 19/1000 | Loss: 0.00002004
Iteration 20/1000 | Loss: 0.00002004
Iteration 21/1000 | Loss: 0.00002003
Iteration 22/1000 | Loss: 0.00002003
Iteration 23/1000 | Loss: 0.00002003
Iteration 24/1000 | Loss: 0.00002002
Iteration 25/1000 | Loss: 0.00002001
Iteration 26/1000 | Loss: 0.00002001
Iteration 27/1000 | Loss: 0.00002000
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00002000
Iteration 30/1000 | Loss: 0.00002000
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001998
Iteration 33/1000 | Loss: 0.00001997
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001996
Iteration 41/1000 | Loss: 0.00001996
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001993
Iteration 45/1000 | Loss: 0.00001993
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001992
Iteration 53/1000 | Loss: 0.00001992
Iteration 54/1000 | Loss: 0.00001992
Iteration 55/1000 | Loss: 0.00001991
Iteration 56/1000 | Loss: 0.00001991
Iteration 57/1000 | Loss: 0.00001991
Iteration 58/1000 | Loss: 0.00001990
Iteration 59/1000 | Loss: 0.00001990
Iteration 60/1000 | Loss: 0.00001990
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001985
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001984
Iteration 79/1000 | Loss: 0.00001984
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001983
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00001983
Iteration 84/1000 | Loss: 0.00001982
Iteration 85/1000 | Loss: 0.00001982
Iteration 86/1000 | Loss: 0.00001982
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001982
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001982
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001981
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001980
Iteration 104/1000 | Loss: 0.00001980
Iteration 105/1000 | Loss: 0.00001980
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001979
Iteration 110/1000 | Loss: 0.00001979
Iteration 111/1000 | Loss: 0.00001979
Iteration 112/1000 | Loss: 0.00001979
Iteration 113/1000 | Loss: 0.00001979
Iteration 114/1000 | Loss: 0.00001979
Iteration 115/1000 | Loss: 0.00001979
Iteration 116/1000 | Loss: 0.00001979
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001978
Iteration 120/1000 | Loss: 0.00001978
Iteration 121/1000 | Loss: 0.00001978
Iteration 122/1000 | Loss: 0.00001978
Iteration 123/1000 | Loss: 0.00001978
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001977
Iteration 126/1000 | Loss: 0.00001977
Iteration 127/1000 | Loss: 0.00001977
Iteration 128/1000 | Loss: 0.00001977
Iteration 129/1000 | Loss: 0.00001977
Iteration 130/1000 | Loss: 0.00001977
Iteration 131/1000 | Loss: 0.00001977
Iteration 132/1000 | Loss: 0.00001977
Iteration 133/1000 | Loss: 0.00001977
Iteration 134/1000 | Loss: 0.00001976
Iteration 135/1000 | Loss: 0.00001976
Iteration 136/1000 | Loss: 0.00001975
Iteration 137/1000 | Loss: 0.00001975
Iteration 138/1000 | Loss: 0.00001975
Iteration 139/1000 | Loss: 0.00001975
Iteration 140/1000 | Loss: 0.00001975
Iteration 141/1000 | Loss: 0.00001975
Iteration 142/1000 | Loss: 0.00001975
Iteration 143/1000 | Loss: 0.00001975
Iteration 144/1000 | Loss: 0.00001975
Iteration 145/1000 | Loss: 0.00001974
Iteration 146/1000 | Loss: 0.00001974
Iteration 147/1000 | Loss: 0.00001974
Iteration 148/1000 | Loss: 0.00001974
Iteration 149/1000 | Loss: 0.00001974
Iteration 150/1000 | Loss: 0.00001973
Iteration 151/1000 | Loss: 0.00001973
Iteration 152/1000 | Loss: 0.00001972
Iteration 153/1000 | Loss: 0.00001972
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001972
Iteration 157/1000 | Loss: 0.00001972
Iteration 158/1000 | Loss: 0.00001972
Iteration 159/1000 | Loss: 0.00001972
Iteration 160/1000 | Loss: 0.00001972
Iteration 161/1000 | Loss: 0.00001972
Iteration 162/1000 | Loss: 0.00001972
Iteration 163/1000 | Loss: 0.00001972
Iteration 164/1000 | Loss: 0.00001972
Iteration 165/1000 | Loss: 0.00001972
Iteration 166/1000 | Loss: 0.00001972
Iteration 167/1000 | Loss: 0.00001972
Iteration 168/1000 | Loss: 0.00001972
Iteration 169/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.972143945749849e-05, 1.972143945749849e-05, 1.972143945749849e-05, 1.972143945749849e-05, 1.972143945749849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.972143945749849e-05

Optimization complete. Final v2v error: 3.728551149368286 mm

Highest mean error: 4.399717807769775 mm for frame 15

Lowest mean error: 3.3108227252960205 mm for frame 99

Saving results

Total time: 53.94237661361694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798506
Iteration 2/25 | Loss: 0.00130745
Iteration 3/25 | Loss: 0.00087652
Iteration 4/25 | Loss: 0.00081533
Iteration 5/25 | Loss: 0.00080437
Iteration 6/25 | Loss: 0.00080156
Iteration 7/25 | Loss: 0.00080124
Iteration 8/25 | Loss: 0.00080124
Iteration 9/25 | Loss: 0.00080124
Iteration 10/25 | Loss: 0.00080124
Iteration 11/25 | Loss: 0.00080124
Iteration 12/25 | Loss: 0.00080124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008012375910766423, 0.0008012375910766423, 0.0008012375910766423, 0.0008012375910766423, 0.0008012375910766423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008012375910766423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65706265
Iteration 2/25 | Loss: 0.00087249
Iteration 3/25 | Loss: 0.00087247
Iteration 4/25 | Loss: 0.00087247
Iteration 5/25 | Loss: 0.00087247
Iteration 6/25 | Loss: 0.00087247
Iteration 7/25 | Loss: 0.00087247
Iteration 8/25 | Loss: 0.00087247
Iteration 9/25 | Loss: 0.00087247
Iteration 10/25 | Loss: 0.00087247
Iteration 11/25 | Loss: 0.00087247
Iteration 12/25 | Loss: 0.00087247
Iteration 13/25 | Loss: 0.00087247
Iteration 14/25 | Loss: 0.00087247
Iteration 15/25 | Loss: 0.00087247
Iteration 16/25 | Loss: 0.00087247
Iteration 17/25 | Loss: 0.00087247
Iteration 18/25 | Loss: 0.00087247
Iteration 19/25 | Loss: 0.00087247
Iteration 20/25 | Loss: 0.00087247
Iteration 21/25 | Loss: 0.00087247
Iteration 22/25 | Loss: 0.00087247
Iteration 23/25 | Loss: 0.00087247
Iteration 24/25 | Loss: 0.00087247
Iteration 25/25 | Loss: 0.00087247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087247
Iteration 2/1000 | Loss: 0.00003174
Iteration 3/1000 | Loss: 0.00002394
Iteration 4/1000 | Loss: 0.00002228
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00002089
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00002021
Iteration 9/1000 | Loss: 0.00002001
Iteration 10/1000 | Loss: 0.00001978
Iteration 11/1000 | Loss: 0.00001973
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001959
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001952
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001946
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001942
Iteration 36/1000 | Loss: 0.00001942
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001940
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001939
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001938
Iteration 56/1000 | Loss: 0.00001938
Iteration 57/1000 | Loss: 0.00001938
Iteration 58/1000 | Loss: 0.00001938
Iteration 59/1000 | Loss: 0.00001938
Iteration 60/1000 | Loss: 0.00001938
Iteration 61/1000 | Loss: 0.00001938
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001938
Iteration 64/1000 | Loss: 0.00001938
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001937
Iteration 72/1000 | Loss: 0.00001937
Iteration 73/1000 | Loss: 0.00001937
Iteration 74/1000 | Loss: 0.00001937
Iteration 75/1000 | Loss: 0.00001937
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001936
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001935
Iteration 83/1000 | Loss: 0.00001935
Iteration 84/1000 | Loss: 0.00001935
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001934
Iteration 90/1000 | Loss: 0.00001934
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001933
Iteration 93/1000 | Loss: 0.00001933
Iteration 94/1000 | Loss: 0.00001933
Iteration 95/1000 | Loss: 0.00001933
Iteration 96/1000 | Loss: 0.00001933
Iteration 97/1000 | Loss: 0.00001933
Iteration 98/1000 | Loss: 0.00001933
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001932
Iteration 105/1000 | Loss: 0.00001932
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001932
Iteration 119/1000 | Loss: 0.00001932
Iteration 120/1000 | Loss: 0.00001932
Iteration 121/1000 | Loss: 0.00001932
Iteration 122/1000 | Loss: 0.00001932
Iteration 123/1000 | Loss: 0.00001932
Iteration 124/1000 | Loss: 0.00001932
Iteration 125/1000 | Loss: 0.00001932
Iteration 126/1000 | Loss: 0.00001932
Iteration 127/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.9322582375025377e-05, 1.9322582375025377e-05, 1.9322582375025377e-05, 1.9322582375025377e-05, 1.9322582375025377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9322582375025377e-05

Optimization complete. Final v2v error: 3.724919080734253 mm

Highest mean error: 3.9406065940856934 mm for frame 222

Lowest mean error: 3.5197181701660156 mm for frame 181

Saving results

Total time: 38.60680294036865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024282
Iteration 2/25 | Loss: 0.00399175
Iteration 3/25 | Loss: 0.00305326
Iteration 4/25 | Loss: 0.00231259
Iteration 5/25 | Loss: 0.00226153
Iteration 6/25 | Loss: 0.00219102
Iteration 7/25 | Loss: 0.00244384
Iteration 8/25 | Loss: 0.00217028
Iteration 9/25 | Loss: 0.00204278
Iteration 10/25 | Loss: 0.00193048
Iteration 11/25 | Loss: 0.00192686
Iteration 12/25 | Loss: 0.00184207
Iteration 13/25 | Loss: 0.00176355
Iteration 14/25 | Loss: 0.00174273
Iteration 15/25 | Loss: 0.00165427
Iteration 16/25 | Loss: 0.00167745
Iteration 17/25 | Loss: 0.00160220
Iteration 18/25 | Loss: 0.00160481
Iteration 19/25 | Loss: 0.00164394
Iteration 20/25 | Loss: 0.00151774
Iteration 21/25 | Loss: 0.00158771
Iteration 22/25 | Loss: 0.00156909
Iteration 23/25 | Loss: 0.00152547
Iteration 24/25 | Loss: 0.00153679
Iteration 25/25 | Loss: 0.00154189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30660462
Iteration 2/25 | Loss: 0.02538434
Iteration 3/25 | Loss: 0.01140477
Iteration 4/25 | Loss: 0.01137120
Iteration 5/25 | Loss: 0.01137120
Iteration 6/25 | Loss: 0.01137120
Iteration 7/25 | Loss: 0.01137120
Iteration 8/25 | Loss: 0.01137120
Iteration 9/25 | Loss: 0.01137120
Iteration 10/25 | Loss: 0.01137120
Iteration 11/25 | Loss: 0.01137119
Iteration 12/25 | Loss: 0.01137120
Iteration 13/25 | Loss: 0.01137120
Iteration 14/25 | Loss: 0.01137119
Iteration 15/25 | Loss: 0.01137119
Iteration 16/25 | Loss: 0.01137119
Iteration 17/25 | Loss: 0.01137119
Iteration 18/25 | Loss: 0.01137119
Iteration 19/25 | Loss: 0.01137119
Iteration 20/25 | Loss: 0.01137119
Iteration 21/25 | Loss: 0.01137119
Iteration 22/25 | Loss: 0.01137119
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.011371193453669548, 0.011371193453669548, 0.011371193453669548, 0.011371193453669548, 0.011371193453669548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.011371193453669548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01137119
Iteration 2/1000 | Loss: 0.00857871
Iteration 3/1000 | Loss: 0.01976973
Iteration 4/1000 | Loss: 0.01113089
Iteration 5/1000 | Loss: 0.02544208
Iteration 6/1000 | Loss: 0.01038934
Iteration 7/1000 | Loss: 0.02394097
Iteration 8/1000 | Loss: 0.01440958
Iteration 9/1000 | Loss: 0.01646844
Iteration 10/1000 | Loss: 0.01764135
Iteration 11/1000 | Loss: 0.01463665
Iteration 12/1000 | Loss: 0.00985628
Iteration 13/1000 | Loss: 0.02268781
Iteration 14/1000 | Loss: 0.01584383
Iteration 15/1000 | Loss: 0.01360017
Iteration 16/1000 | Loss: 0.02104985
Iteration 17/1000 | Loss: 0.01348820
Iteration 18/1000 | Loss: 0.01235361
Iteration 19/1000 | Loss: 0.01212125
Iteration 20/1000 | Loss: 0.01695467
Iteration 21/1000 | Loss: 0.01291322
Iteration 22/1000 | Loss: 0.01794016
Iteration 23/1000 | Loss: 0.01175393
Iteration 24/1000 | Loss: 0.01658432
Iteration 25/1000 | Loss: 0.01549148
Iteration 26/1000 | Loss: 0.01387367
Iteration 27/1000 | Loss: 0.01029722
Iteration 28/1000 | Loss: 0.01070418
Iteration 29/1000 | Loss: 0.01045651
Iteration 30/1000 | Loss: 0.01341851
Iteration 31/1000 | Loss: 0.01089695
Iteration 32/1000 | Loss: 0.01677754
Iteration 33/1000 | Loss: 0.01037046
Iteration 34/1000 | Loss: 0.01097913
Iteration 35/1000 | Loss: 0.00973025
Iteration 36/1000 | Loss: 0.01067531
Iteration 37/1000 | Loss: 0.01235668
Iteration 38/1000 | Loss: 0.01356042
Iteration 39/1000 | Loss: 0.01206006
Iteration 40/1000 | Loss: 0.00985624
Iteration 41/1000 | Loss: 0.01312651
Iteration 42/1000 | Loss: 0.01075482
Iteration 43/1000 | Loss: 0.00907612
Iteration 44/1000 | Loss: 0.00805977
Iteration 45/1000 | Loss: 0.01061446
Iteration 46/1000 | Loss: 0.01670715
Iteration 47/1000 | Loss: 0.01047674
Iteration 48/1000 | Loss: 0.01581354
Iteration 49/1000 | Loss: 0.01193668
Iteration 50/1000 | Loss: 0.01565516
Iteration 51/1000 | Loss: 0.01105302
Iteration 52/1000 | Loss: 0.00879859
Iteration 53/1000 | Loss: 0.01068326
Iteration 54/1000 | Loss: 0.00850343
Iteration 55/1000 | Loss: 0.00930695
Iteration 56/1000 | Loss: 0.01174685
Iteration 57/1000 | Loss: 0.00917343
Iteration 58/1000 | Loss: 0.00945837
Iteration 59/1000 | Loss: 0.00730198
Iteration 60/1000 | Loss: 0.00599530
Iteration 61/1000 | Loss: 0.01522898
Iteration 62/1000 | Loss: 0.00751264
Iteration 63/1000 | Loss: 0.01291366
Iteration 64/1000 | Loss: 0.00739564
Iteration 65/1000 | Loss: 0.01138634
Iteration 66/1000 | Loss: 0.00645225
Iteration 67/1000 | Loss: 0.00667445
Iteration 68/1000 | Loss: 0.00542145
Iteration 69/1000 | Loss: 0.00846624
Iteration 70/1000 | Loss: 0.00584373
Iteration 71/1000 | Loss: 0.00639856
Iteration 72/1000 | Loss: 0.00523161
Iteration 73/1000 | Loss: 0.02391707
Iteration 74/1000 | Loss: 0.01748800
Iteration 75/1000 | Loss: 0.00993840
Iteration 76/1000 | Loss: 0.00982598
Iteration 77/1000 | Loss: 0.00904612
Iteration 78/1000 | Loss: 0.01176130
Iteration 79/1000 | Loss: 0.00536295
Iteration 80/1000 | Loss: 0.00540071
Iteration 81/1000 | Loss: 0.00595871
Iteration 82/1000 | Loss: 0.00614469
Iteration 83/1000 | Loss: 0.00685565
Iteration 84/1000 | Loss: 0.00549644
Iteration 85/1000 | Loss: 0.00487225
Iteration 86/1000 | Loss: 0.00839442
Iteration 87/1000 | Loss: 0.00593019
Iteration 88/1000 | Loss: 0.00432172
Iteration 89/1000 | Loss: 0.01060406
Iteration 90/1000 | Loss: 0.00865137
Iteration 91/1000 | Loss: 0.00664377
Iteration 92/1000 | Loss: 0.00551724
Iteration 93/1000 | Loss: 0.00769025
Iteration 94/1000 | Loss: 0.00556985
Iteration 95/1000 | Loss: 0.00962569
Iteration 96/1000 | Loss: 0.00473560
Iteration 97/1000 | Loss: 0.00666658
Iteration 98/1000 | Loss: 0.00566282
Iteration 99/1000 | Loss: 0.00514262
Iteration 100/1000 | Loss: 0.00546318
Iteration 101/1000 | Loss: 0.00946178
Iteration 102/1000 | Loss: 0.00706383
Iteration 103/1000 | Loss: 0.00546727
Iteration 104/1000 | Loss: 0.00964207
Iteration 105/1000 | Loss: 0.00613640
Iteration 106/1000 | Loss: 0.00599326
Iteration 107/1000 | Loss: 0.00438713
Iteration 108/1000 | Loss: 0.00474978
Iteration 109/1000 | Loss: 0.01402812
Iteration 110/1000 | Loss: 0.00629616
Iteration 111/1000 | Loss: 0.00946283
Iteration 112/1000 | Loss: 0.00832519
Iteration 113/1000 | Loss: 0.00779149
Iteration 114/1000 | Loss: 0.00725749
Iteration 115/1000 | Loss: 0.00738094
Iteration 116/1000 | Loss: 0.00615132
Iteration 117/1000 | Loss: 0.00507421
Iteration 118/1000 | Loss: 0.00742988
Iteration 119/1000 | Loss: 0.00766811
Iteration 120/1000 | Loss: 0.00546342
Iteration 121/1000 | Loss: 0.00679309
Iteration 122/1000 | Loss: 0.00790170
Iteration 123/1000 | Loss: 0.00555102
Iteration 124/1000 | Loss: 0.01339863
Iteration 125/1000 | Loss: 0.00575116
Iteration 126/1000 | Loss: 0.00759320
Iteration 127/1000 | Loss: 0.00491280
Iteration 128/1000 | Loss: 0.01152736
Iteration 129/1000 | Loss: 0.00515989
Iteration 130/1000 | Loss: 0.00735426
Iteration 131/1000 | Loss: 0.00765430
Iteration 132/1000 | Loss: 0.00797644
Iteration 133/1000 | Loss: 0.00468927
Iteration 134/1000 | Loss: 0.00818599
Iteration 135/1000 | Loss: 0.00542020
Iteration 136/1000 | Loss: 0.00479726
Iteration 137/1000 | Loss: 0.00539178
Iteration 138/1000 | Loss: 0.00595844
Iteration 139/1000 | Loss: 0.00463441
Iteration 140/1000 | Loss: 0.00655980
Iteration 141/1000 | Loss: 0.00874625
Iteration 142/1000 | Loss: 0.00574076
Iteration 143/1000 | Loss: 0.01203176
Iteration 144/1000 | Loss: 0.00432300
Iteration 145/1000 | Loss: 0.00490053
Iteration 146/1000 | Loss: 0.00887439
Iteration 147/1000 | Loss: 0.00716767
Iteration 148/1000 | Loss: 0.00379751
Iteration 149/1000 | Loss: 0.00736387
Iteration 150/1000 | Loss: 0.00493704
Iteration 151/1000 | Loss: 0.00395598
Iteration 152/1000 | Loss: 0.01020773
Iteration 153/1000 | Loss: 0.00340646
Iteration 154/1000 | Loss: 0.00794483
Iteration 155/1000 | Loss: 0.00530386
Iteration 156/1000 | Loss: 0.00801695
Iteration 157/1000 | Loss: 0.00354366
Iteration 158/1000 | Loss: 0.00871529
Iteration 159/1000 | Loss: 0.00463762
Iteration 160/1000 | Loss: 0.00317713
Iteration 161/1000 | Loss: 0.00666264
Iteration 162/1000 | Loss: 0.00358756
Iteration 163/1000 | Loss: 0.00711652
Iteration 164/1000 | Loss: 0.00386943
Iteration 165/1000 | Loss: 0.00285163
Iteration 166/1000 | Loss: 0.00439677
Iteration 167/1000 | Loss: 0.00404232
Iteration 168/1000 | Loss: 0.00326394
Iteration 169/1000 | Loss: 0.00822689
Iteration 170/1000 | Loss: 0.00286783
Iteration 171/1000 | Loss: 0.00410965
Iteration 172/1000 | Loss: 0.00320684
Iteration 173/1000 | Loss: 0.00388037
Iteration 174/1000 | Loss: 0.00290002
Iteration 175/1000 | Loss: 0.00329823
Iteration 176/1000 | Loss: 0.00341071
Iteration 177/1000 | Loss: 0.00278500
Iteration 178/1000 | Loss: 0.00248148
Iteration 179/1000 | Loss: 0.00358362
Iteration 180/1000 | Loss: 0.00786496
Iteration 181/1000 | Loss: 0.00637401
Iteration 182/1000 | Loss: 0.00408250
Iteration 183/1000 | Loss: 0.00305676
Iteration 184/1000 | Loss: 0.00454657
Iteration 185/1000 | Loss: 0.00230063
Iteration 186/1000 | Loss: 0.00279570
Iteration 187/1000 | Loss: 0.00417677
Iteration 188/1000 | Loss: 0.00260806
Iteration 189/1000 | Loss: 0.00744572
Iteration 190/1000 | Loss: 0.00236535
Iteration 191/1000 | Loss: 0.00407628
Iteration 192/1000 | Loss: 0.00233706
Iteration 193/1000 | Loss: 0.00043867
Iteration 194/1000 | Loss: 0.00138159
Iteration 195/1000 | Loss: 0.00071459
Iteration 196/1000 | Loss: 0.00145794
Iteration 197/1000 | Loss: 0.00828109
Iteration 198/1000 | Loss: 0.00630682
Iteration 199/1000 | Loss: 0.00325117
Iteration 200/1000 | Loss: 0.00236043
Iteration 201/1000 | Loss: 0.00168845
Iteration 202/1000 | Loss: 0.00134854
Iteration 203/1000 | Loss: 0.00236715
Iteration 204/1000 | Loss: 0.00127271
Iteration 205/1000 | Loss: 0.00363399
Iteration 206/1000 | Loss: 0.00125947
Iteration 207/1000 | Loss: 0.00465847
Iteration 208/1000 | Loss: 0.00405265
Iteration 209/1000 | Loss: 0.00283619
Iteration 210/1000 | Loss: 0.00138849
Iteration 211/1000 | Loss: 0.00099449
Iteration 212/1000 | Loss: 0.00170797
Iteration 213/1000 | Loss: 0.00268964
Iteration 214/1000 | Loss: 0.00056626
Iteration 215/1000 | Loss: 0.00175038
Iteration 216/1000 | Loss: 0.00397249
Iteration 217/1000 | Loss: 0.00078043
Iteration 218/1000 | Loss: 0.00152618
Iteration 219/1000 | Loss: 0.00209655
Iteration 220/1000 | Loss: 0.00204999
Iteration 221/1000 | Loss: 0.00219396
Iteration 222/1000 | Loss: 0.00319942
Iteration 223/1000 | Loss: 0.00464497
Iteration 224/1000 | Loss: 0.00022701
Iteration 225/1000 | Loss: 0.00299781
Iteration 226/1000 | Loss: 0.00445984
Iteration 227/1000 | Loss: 0.00390418
Iteration 228/1000 | Loss: 0.00260448
Iteration 229/1000 | Loss: 0.00176496
Iteration 230/1000 | Loss: 0.00052669
Iteration 231/1000 | Loss: 0.00072023
Iteration 232/1000 | Loss: 0.00205169
Iteration 233/1000 | Loss: 0.00018716
Iteration 234/1000 | Loss: 0.00068405
Iteration 235/1000 | Loss: 0.00015134
Iteration 236/1000 | Loss: 0.00134502
Iteration 237/1000 | Loss: 0.00013922
Iteration 238/1000 | Loss: 0.00006543
Iteration 239/1000 | Loss: 0.00005652
Iteration 240/1000 | Loss: 0.00026685
Iteration 241/1000 | Loss: 0.00024503
Iteration 242/1000 | Loss: 0.00175184
Iteration 243/1000 | Loss: 0.00025659
Iteration 244/1000 | Loss: 0.00021734
Iteration 245/1000 | Loss: 0.00111245
Iteration 246/1000 | Loss: 0.00023110
Iteration 247/1000 | Loss: 0.00048714
Iteration 248/1000 | Loss: 0.00047646
Iteration 249/1000 | Loss: 0.00007134
Iteration 250/1000 | Loss: 0.00022603
Iteration 251/1000 | Loss: 0.00258056
Iteration 252/1000 | Loss: 0.00074951
Iteration 253/1000 | Loss: 0.00008934
Iteration 254/1000 | Loss: 0.00015329
Iteration 255/1000 | Loss: 0.00016187
Iteration 256/1000 | Loss: 0.00024348
Iteration 257/1000 | Loss: 0.00058735
Iteration 258/1000 | Loss: 0.00018346
Iteration 259/1000 | Loss: 0.00006024
Iteration 260/1000 | Loss: 0.00003898
Iteration 261/1000 | Loss: 0.00003591
Iteration 262/1000 | Loss: 0.00003415
Iteration 263/1000 | Loss: 0.00066666
Iteration 264/1000 | Loss: 0.00012513
Iteration 265/1000 | Loss: 0.00089562
Iteration 266/1000 | Loss: 0.00059196
Iteration 267/1000 | Loss: 0.00044915
Iteration 268/1000 | Loss: 0.00056970
Iteration 269/1000 | Loss: 0.00041476
Iteration 270/1000 | Loss: 0.00072375
Iteration 271/1000 | Loss: 0.00034783
Iteration 272/1000 | Loss: 0.00105124
Iteration 273/1000 | Loss: 0.00055538
Iteration 274/1000 | Loss: 0.00007231
Iteration 275/1000 | Loss: 0.00044544
Iteration 276/1000 | Loss: 0.00035818
Iteration 277/1000 | Loss: 0.00059911
Iteration 278/1000 | Loss: 0.00034692
Iteration 279/1000 | Loss: 0.00030588
Iteration 280/1000 | Loss: 0.00112917
Iteration 281/1000 | Loss: 0.00029592
Iteration 282/1000 | Loss: 0.00028129
Iteration 283/1000 | Loss: 0.00033097
Iteration 284/1000 | Loss: 0.00099862
Iteration 285/1000 | Loss: 0.00008945
Iteration 286/1000 | Loss: 0.00028874
Iteration 287/1000 | Loss: 0.00008928
Iteration 288/1000 | Loss: 0.00014818
Iteration 289/1000 | Loss: 0.00012252
Iteration 290/1000 | Loss: 0.00052074
Iteration 291/1000 | Loss: 0.00066543
Iteration 292/1000 | Loss: 0.00065532
Iteration 293/1000 | Loss: 0.00063848
Iteration 294/1000 | Loss: 0.00167581
Iteration 295/1000 | Loss: 0.00026658
Iteration 296/1000 | Loss: 0.00005024
Iteration 297/1000 | Loss: 0.00048938
Iteration 298/1000 | Loss: 0.00061787
Iteration 299/1000 | Loss: 0.00004144
Iteration 300/1000 | Loss: 0.00003640
Iteration 301/1000 | Loss: 0.00013338
Iteration 302/1000 | Loss: 0.00005262
Iteration 303/1000 | Loss: 0.00007112
Iteration 304/1000 | Loss: 0.00006046
Iteration 305/1000 | Loss: 0.00003097
Iteration 306/1000 | Loss: 0.00006333
Iteration 307/1000 | Loss: 0.00002717
Iteration 308/1000 | Loss: 0.00002474
Iteration 309/1000 | Loss: 0.00002308
Iteration 310/1000 | Loss: 0.00002201
Iteration 311/1000 | Loss: 0.00002117
Iteration 312/1000 | Loss: 0.00002079
Iteration 313/1000 | Loss: 0.00002041
Iteration 314/1000 | Loss: 0.00002012
Iteration 315/1000 | Loss: 0.00001990
Iteration 316/1000 | Loss: 0.00001975
Iteration 317/1000 | Loss: 0.00001969
Iteration 318/1000 | Loss: 0.00001960
Iteration 319/1000 | Loss: 0.00001958
Iteration 320/1000 | Loss: 0.00001954
Iteration 321/1000 | Loss: 0.00001950
Iteration 322/1000 | Loss: 0.00023076
Iteration 323/1000 | Loss: 0.00003236
Iteration 324/1000 | Loss: 0.00002475
Iteration 325/1000 | Loss: 0.00002015
Iteration 326/1000 | Loss: 0.00001942
Iteration 327/1000 | Loss: 0.00001931
Iteration 328/1000 | Loss: 0.00001929
Iteration 329/1000 | Loss: 0.00001929
Iteration 330/1000 | Loss: 0.00001929
Iteration 331/1000 | Loss: 0.00001929
Iteration 332/1000 | Loss: 0.00001929
Iteration 333/1000 | Loss: 0.00001929
Iteration 334/1000 | Loss: 0.00001929
Iteration 335/1000 | Loss: 0.00001929
Iteration 336/1000 | Loss: 0.00001929
Iteration 337/1000 | Loss: 0.00001929
Iteration 338/1000 | Loss: 0.00001928
Iteration 339/1000 | Loss: 0.00001928
Iteration 340/1000 | Loss: 0.00001927
Iteration 341/1000 | Loss: 0.00001927
Iteration 342/1000 | Loss: 0.00001927
Iteration 343/1000 | Loss: 0.00001926
Iteration 344/1000 | Loss: 0.00001926
Iteration 345/1000 | Loss: 0.00001926
Iteration 346/1000 | Loss: 0.00001926
Iteration 347/1000 | Loss: 0.00001925
Iteration 348/1000 | Loss: 0.00001925
Iteration 349/1000 | Loss: 0.00001925
Iteration 350/1000 | Loss: 0.00001924
Iteration 351/1000 | Loss: 0.00001924
Iteration 352/1000 | Loss: 0.00001924
Iteration 353/1000 | Loss: 0.00001922
Iteration 354/1000 | Loss: 0.00001922
Iteration 355/1000 | Loss: 0.00001922
Iteration 356/1000 | Loss: 0.00001922
Iteration 357/1000 | Loss: 0.00001922
Iteration 358/1000 | Loss: 0.00001922
Iteration 359/1000 | Loss: 0.00001922
Iteration 360/1000 | Loss: 0.00001922
Iteration 361/1000 | Loss: 0.00001922
Iteration 362/1000 | Loss: 0.00001922
Iteration 363/1000 | Loss: 0.00001921
Iteration 364/1000 | Loss: 0.00001921
Iteration 365/1000 | Loss: 0.00001921
Iteration 366/1000 | Loss: 0.00001921
Iteration 367/1000 | Loss: 0.00001920
Iteration 368/1000 | Loss: 0.00001920
Iteration 369/1000 | Loss: 0.00001919
Iteration 370/1000 | Loss: 0.00001919
Iteration 371/1000 | Loss: 0.00001919
Iteration 372/1000 | Loss: 0.00001919
Iteration 373/1000 | Loss: 0.00001919
Iteration 374/1000 | Loss: 0.00001919
Iteration 375/1000 | Loss: 0.00001919
Iteration 376/1000 | Loss: 0.00001919
Iteration 377/1000 | Loss: 0.00001918
Iteration 378/1000 | Loss: 0.00001918
Iteration 379/1000 | Loss: 0.00001918
Iteration 380/1000 | Loss: 0.00001918
Iteration 381/1000 | Loss: 0.00001917
Iteration 382/1000 | Loss: 0.00001917
Iteration 383/1000 | Loss: 0.00001917
Iteration 384/1000 | Loss: 0.00001917
Iteration 385/1000 | Loss: 0.00001916
Iteration 386/1000 | Loss: 0.00001916
Iteration 387/1000 | Loss: 0.00001916
Iteration 388/1000 | Loss: 0.00001916
Iteration 389/1000 | Loss: 0.00001916
Iteration 390/1000 | Loss: 0.00001916
Iteration 391/1000 | Loss: 0.00001916
Iteration 392/1000 | Loss: 0.00001915
Iteration 393/1000 | Loss: 0.00001915
Iteration 394/1000 | Loss: 0.00001915
Iteration 395/1000 | Loss: 0.00001915
Iteration 396/1000 | Loss: 0.00001915
Iteration 397/1000 | Loss: 0.00001915
Iteration 398/1000 | Loss: 0.00001915
Iteration 399/1000 | Loss: 0.00001914
Iteration 400/1000 | Loss: 0.00001914
Iteration 401/1000 | Loss: 0.00001914
Iteration 402/1000 | Loss: 0.00001914
Iteration 403/1000 | Loss: 0.00001914
Iteration 404/1000 | Loss: 0.00001914
Iteration 405/1000 | Loss: 0.00001914
Iteration 406/1000 | Loss: 0.00001914
Iteration 407/1000 | Loss: 0.00001914
Iteration 408/1000 | Loss: 0.00001914
Iteration 409/1000 | Loss: 0.00001913
Iteration 410/1000 | Loss: 0.00001913
Iteration 411/1000 | Loss: 0.00001913
Iteration 412/1000 | Loss: 0.00001912
Iteration 413/1000 | Loss: 0.00001912
Iteration 414/1000 | Loss: 0.00001912
Iteration 415/1000 | Loss: 0.00001912
Iteration 416/1000 | Loss: 0.00001912
Iteration 417/1000 | Loss: 0.00001911
Iteration 418/1000 | Loss: 0.00001911
Iteration 419/1000 | Loss: 0.00001911
Iteration 420/1000 | Loss: 0.00001911
Iteration 421/1000 | Loss: 0.00001911
Iteration 422/1000 | Loss: 0.00001911
Iteration 423/1000 | Loss: 0.00001911
Iteration 424/1000 | Loss: 0.00001911
Iteration 425/1000 | Loss: 0.00001911
Iteration 426/1000 | Loss: 0.00001911
Iteration 427/1000 | Loss: 0.00001911
Iteration 428/1000 | Loss: 0.00001910
Iteration 429/1000 | Loss: 0.00001910
Iteration 430/1000 | Loss: 0.00001910
Iteration 431/1000 | Loss: 0.00001910
Iteration 432/1000 | Loss: 0.00001910
Iteration 433/1000 | Loss: 0.00001910
Iteration 434/1000 | Loss: 0.00001910
Iteration 435/1000 | Loss: 0.00001910
Iteration 436/1000 | Loss: 0.00001910
Iteration 437/1000 | Loss: 0.00001909
Iteration 438/1000 | Loss: 0.00001909
Iteration 439/1000 | Loss: 0.00001909
Iteration 440/1000 | Loss: 0.00001909
Iteration 441/1000 | Loss: 0.00001909
Iteration 442/1000 | Loss: 0.00001909
Iteration 443/1000 | Loss: 0.00001908
Iteration 444/1000 | Loss: 0.00001908
Iteration 445/1000 | Loss: 0.00001908
Iteration 446/1000 | Loss: 0.00001908
Iteration 447/1000 | Loss: 0.00001908
Iteration 448/1000 | Loss: 0.00001908
Iteration 449/1000 | Loss: 0.00001908
Iteration 450/1000 | Loss: 0.00001908
Iteration 451/1000 | Loss: 0.00001908
Iteration 452/1000 | Loss: 0.00001908
Iteration 453/1000 | Loss: 0.00001908
Iteration 454/1000 | Loss: 0.00001908
Iteration 455/1000 | Loss: 0.00001908
Iteration 456/1000 | Loss: 0.00001908
Iteration 457/1000 | Loss: 0.00001908
Iteration 458/1000 | Loss: 0.00001908
Iteration 459/1000 | Loss: 0.00001908
Iteration 460/1000 | Loss: 0.00001908
Iteration 461/1000 | Loss: 0.00001908
Iteration 462/1000 | Loss: 0.00001908
Iteration 463/1000 | Loss: 0.00001908
Iteration 464/1000 | Loss: 0.00001908
Iteration 465/1000 | Loss: 0.00001908
Iteration 466/1000 | Loss: 0.00001908
Iteration 467/1000 | Loss: 0.00001908
Iteration 468/1000 | Loss: 0.00001908
Iteration 469/1000 | Loss: 0.00001908
Iteration 470/1000 | Loss: 0.00001908
Iteration 471/1000 | Loss: 0.00001908
Iteration 472/1000 | Loss: 0.00001908
Iteration 473/1000 | Loss: 0.00001908
Iteration 474/1000 | Loss: 0.00001908
Iteration 475/1000 | Loss: 0.00001908
Iteration 476/1000 | Loss: 0.00001908
Iteration 477/1000 | Loss: 0.00001908
Iteration 478/1000 | Loss: 0.00001908
Iteration 479/1000 | Loss: 0.00001908
Iteration 480/1000 | Loss: 0.00001908
Iteration 481/1000 | Loss: 0.00001908
Iteration 482/1000 | Loss: 0.00001908
Iteration 483/1000 | Loss: 0.00001908
Iteration 484/1000 | Loss: 0.00001908
Iteration 485/1000 | Loss: 0.00001908
Iteration 486/1000 | Loss: 0.00001908
Iteration 487/1000 | Loss: 0.00001908
Iteration 488/1000 | Loss: 0.00001908
Iteration 489/1000 | Loss: 0.00001908
Iteration 490/1000 | Loss: 0.00001908
Iteration 491/1000 | Loss: 0.00001908
Iteration 492/1000 | Loss: 0.00001908
Iteration 493/1000 | Loss: 0.00001908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 493. Stopping optimization.
Last 5 losses: [1.9075421732850373e-05, 1.9075421732850373e-05, 1.9075421732850373e-05, 1.9075421732850373e-05, 1.9075421732850373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9075421732850373e-05

Optimization complete. Final v2v error: 3.617880344390869 mm

Highest mean error: 5.557421684265137 mm for frame 45

Lowest mean error: 2.7895870208740234 mm for frame 99

Saving results

Total time: 512.5207605361938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413423
Iteration 2/25 | Loss: 0.00082294
Iteration 3/25 | Loss: 0.00071920
Iteration 4/25 | Loss: 0.00069966
Iteration 5/25 | Loss: 0.00069341
Iteration 6/25 | Loss: 0.00069171
Iteration 7/25 | Loss: 0.00069139
Iteration 8/25 | Loss: 0.00069139
Iteration 9/25 | Loss: 0.00069139
Iteration 10/25 | Loss: 0.00069139
Iteration 11/25 | Loss: 0.00069139
Iteration 12/25 | Loss: 0.00069139
Iteration 13/25 | Loss: 0.00069139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006913944962434471, 0.0006913944962434471, 0.0006913944962434471, 0.0006913944962434471, 0.0006913944962434471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006913944962434471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93745971
Iteration 2/25 | Loss: 0.00082303
Iteration 3/25 | Loss: 0.00082303
Iteration 4/25 | Loss: 0.00082303
Iteration 5/25 | Loss: 0.00082303
Iteration 6/25 | Loss: 0.00082303
Iteration 7/25 | Loss: 0.00082303
Iteration 8/25 | Loss: 0.00082303
Iteration 9/25 | Loss: 0.00082303
Iteration 10/25 | Loss: 0.00082303
Iteration 11/25 | Loss: 0.00082303
Iteration 12/25 | Loss: 0.00082303
Iteration 13/25 | Loss: 0.00082303
Iteration 14/25 | Loss: 0.00082303
Iteration 15/25 | Loss: 0.00082303
Iteration 16/25 | Loss: 0.00082303
Iteration 17/25 | Loss: 0.00082303
Iteration 18/25 | Loss: 0.00082303
Iteration 19/25 | Loss: 0.00082303
Iteration 20/25 | Loss: 0.00082303
Iteration 21/25 | Loss: 0.00082303
Iteration 22/25 | Loss: 0.00082303
Iteration 23/25 | Loss: 0.00082303
Iteration 24/25 | Loss: 0.00082303
Iteration 25/25 | Loss: 0.00082303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082303
Iteration 2/1000 | Loss: 0.00002333
Iteration 3/1000 | Loss: 0.00001586
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001335
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001318
Iteration 16/1000 | Loss: 0.00001316
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001298
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001294
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001292
Iteration 29/1000 | Loss: 0.00001292
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001290
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001286
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001285
Iteration 56/1000 | Loss: 0.00001281
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001273
Iteration 88/1000 | Loss: 0.00001273
Iteration 89/1000 | Loss: 0.00001273
Iteration 90/1000 | Loss: 0.00001273
Iteration 91/1000 | Loss: 0.00001273
Iteration 92/1000 | Loss: 0.00001272
Iteration 93/1000 | Loss: 0.00001272
Iteration 94/1000 | Loss: 0.00001272
Iteration 95/1000 | Loss: 0.00001272
Iteration 96/1000 | Loss: 0.00001272
Iteration 97/1000 | Loss: 0.00001272
Iteration 98/1000 | Loss: 0.00001272
Iteration 99/1000 | Loss: 0.00001272
Iteration 100/1000 | Loss: 0.00001272
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001271
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001270
Iteration 108/1000 | Loss: 0.00001270
Iteration 109/1000 | Loss: 0.00001270
Iteration 110/1000 | Loss: 0.00001270
Iteration 111/1000 | Loss: 0.00001269
Iteration 112/1000 | Loss: 0.00001269
Iteration 113/1000 | Loss: 0.00001269
Iteration 114/1000 | Loss: 0.00001269
Iteration 115/1000 | Loss: 0.00001269
Iteration 116/1000 | Loss: 0.00001269
Iteration 117/1000 | Loss: 0.00001269
Iteration 118/1000 | Loss: 0.00001269
Iteration 119/1000 | Loss: 0.00001269
Iteration 120/1000 | Loss: 0.00001268
Iteration 121/1000 | Loss: 0.00001268
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001268
Iteration 127/1000 | Loss: 0.00001268
Iteration 128/1000 | Loss: 0.00001268
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001267
Iteration 136/1000 | Loss: 0.00001267
Iteration 137/1000 | Loss: 0.00001267
Iteration 138/1000 | Loss: 0.00001267
Iteration 139/1000 | Loss: 0.00001267
Iteration 140/1000 | Loss: 0.00001267
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001267
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.266548497369513e-05, 1.266548497369513e-05, 1.266548497369513e-05, 1.266548497369513e-05, 1.266548497369513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.266548497369513e-05

Optimization complete. Final v2v error: 3.0551700592041016 mm

Highest mean error: 3.294339179992676 mm for frame 121

Lowest mean error: 2.8771674633026123 mm for frame 154

Saving results

Total time: 37.33119297027588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434165
Iteration 2/25 | Loss: 0.00088808
Iteration 3/25 | Loss: 0.00073875
Iteration 4/25 | Loss: 0.00070278
Iteration 5/25 | Loss: 0.00069737
Iteration 6/25 | Loss: 0.00069601
Iteration 7/25 | Loss: 0.00069582
Iteration 8/25 | Loss: 0.00069582
Iteration 9/25 | Loss: 0.00069582
Iteration 10/25 | Loss: 0.00069582
Iteration 11/25 | Loss: 0.00069582
Iteration 12/25 | Loss: 0.00069582
Iteration 13/25 | Loss: 0.00069582
Iteration 14/25 | Loss: 0.00069582
Iteration 15/25 | Loss: 0.00069582
Iteration 16/25 | Loss: 0.00069582
Iteration 17/25 | Loss: 0.00069582
Iteration 18/25 | Loss: 0.00069582
Iteration 19/25 | Loss: 0.00069582
Iteration 20/25 | Loss: 0.00069582
Iteration 21/25 | Loss: 0.00069582
Iteration 22/25 | Loss: 0.00069582
Iteration 23/25 | Loss: 0.00069582
Iteration 24/25 | Loss: 0.00069582
Iteration 25/25 | Loss: 0.00069582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52801514
Iteration 2/25 | Loss: 0.00077543
Iteration 3/25 | Loss: 0.00077540
Iteration 4/25 | Loss: 0.00077540
Iteration 5/25 | Loss: 0.00077540
Iteration 6/25 | Loss: 0.00077540
Iteration 7/25 | Loss: 0.00077540
Iteration 8/25 | Loss: 0.00077540
Iteration 9/25 | Loss: 0.00077539
Iteration 10/25 | Loss: 0.00077539
Iteration 11/25 | Loss: 0.00077539
Iteration 12/25 | Loss: 0.00077539
Iteration 13/25 | Loss: 0.00077539
Iteration 14/25 | Loss: 0.00077539
Iteration 15/25 | Loss: 0.00077539
Iteration 16/25 | Loss: 0.00077539
Iteration 17/25 | Loss: 0.00077539
Iteration 18/25 | Loss: 0.00077539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007753942627459764, 0.0007753942627459764, 0.0007753942627459764, 0.0007753942627459764, 0.0007753942627459764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007753942627459764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077539
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001555
Iteration 5/1000 | Loss: 0.00001435
Iteration 6/1000 | Loss: 0.00001368
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001305
Iteration 9/1000 | Loss: 0.00001303
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001284
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001275
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001270
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001265
Iteration 27/1000 | Loss: 0.00001265
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001260
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001255
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001236
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001235
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00001235
Iteration 108/1000 | Loss: 0.00001235
Iteration 109/1000 | Loss: 0.00001235
Iteration 110/1000 | Loss: 0.00001235
Iteration 111/1000 | Loss: 0.00001235
Iteration 112/1000 | Loss: 0.00001235
Iteration 113/1000 | Loss: 0.00001235
Iteration 114/1000 | Loss: 0.00001235
Iteration 115/1000 | Loss: 0.00001235
Iteration 116/1000 | Loss: 0.00001235
Iteration 117/1000 | Loss: 0.00001234
Iteration 118/1000 | Loss: 0.00001234
Iteration 119/1000 | Loss: 0.00001234
Iteration 120/1000 | Loss: 0.00001234
Iteration 121/1000 | Loss: 0.00001234
Iteration 122/1000 | Loss: 0.00001233
Iteration 123/1000 | Loss: 0.00001233
Iteration 124/1000 | Loss: 0.00001233
Iteration 125/1000 | Loss: 0.00001233
Iteration 126/1000 | Loss: 0.00001233
Iteration 127/1000 | Loss: 0.00001232
Iteration 128/1000 | Loss: 0.00001232
Iteration 129/1000 | Loss: 0.00001232
Iteration 130/1000 | Loss: 0.00001232
Iteration 131/1000 | Loss: 0.00001232
Iteration 132/1000 | Loss: 0.00001232
Iteration 133/1000 | Loss: 0.00001232
Iteration 134/1000 | Loss: 0.00001232
Iteration 135/1000 | Loss: 0.00001232
Iteration 136/1000 | Loss: 0.00001232
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001231
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001231
Iteration 141/1000 | Loss: 0.00001231
Iteration 142/1000 | Loss: 0.00001231
Iteration 143/1000 | Loss: 0.00001231
Iteration 144/1000 | Loss: 0.00001231
Iteration 145/1000 | Loss: 0.00001231
Iteration 146/1000 | Loss: 0.00001231
Iteration 147/1000 | Loss: 0.00001231
Iteration 148/1000 | Loss: 0.00001231
Iteration 149/1000 | Loss: 0.00001231
Iteration 150/1000 | Loss: 0.00001231
Iteration 151/1000 | Loss: 0.00001231
Iteration 152/1000 | Loss: 0.00001231
Iteration 153/1000 | Loss: 0.00001231
Iteration 154/1000 | Loss: 0.00001231
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001231
Iteration 157/1000 | Loss: 0.00001231
Iteration 158/1000 | Loss: 0.00001231
Iteration 159/1000 | Loss: 0.00001231
Iteration 160/1000 | Loss: 0.00001231
Iteration 161/1000 | Loss: 0.00001231
Iteration 162/1000 | Loss: 0.00001231
Iteration 163/1000 | Loss: 0.00001231
Iteration 164/1000 | Loss: 0.00001231
Iteration 165/1000 | Loss: 0.00001231
Iteration 166/1000 | Loss: 0.00001231
Iteration 167/1000 | Loss: 0.00001231
Iteration 168/1000 | Loss: 0.00001231
Iteration 169/1000 | Loss: 0.00001231
Iteration 170/1000 | Loss: 0.00001231
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.2309460544202011e-05, 1.2309460544202011e-05, 1.2309460544202011e-05, 1.2309460544202011e-05, 1.2309460544202011e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2309460544202011e-05

Optimization complete. Final v2v error: 2.975159168243408 mm

Highest mean error: 3.562711238861084 mm for frame 25

Lowest mean error: 2.6098878383636475 mm for frame 29

Saving results

Total time: 36.16099214553833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964222
Iteration 2/25 | Loss: 0.00211753
Iteration 3/25 | Loss: 0.00116567
Iteration 4/25 | Loss: 0.00092490
Iteration 5/25 | Loss: 0.00086370
Iteration 6/25 | Loss: 0.00085111
Iteration 7/25 | Loss: 0.00084430
Iteration 8/25 | Loss: 0.00083140
Iteration 9/25 | Loss: 0.00082896
Iteration 10/25 | Loss: 0.00083210
Iteration 11/25 | Loss: 0.00081874
Iteration 12/25 | Loss: 0.00081635
Iteration 13/25 | Loss: 0.00080860
Iteration 14/25 | Loss: 0.00080848
Iteration 15/25 | Loss: 0.00081360
Iteration 16/25 | Loss: 0.00081171
Iteration 17/25 | Loss: 0.00081451
Iteration 18/25 | Loss: 0.00080999
Iteration 19/25 | Loss: 0.00080611
Iteration 20/25 | Loss: 0.00080904
Iteration 21/25 | Loss: 0.00081320
Iteration 22/25 | Loss: 0.00081326
Iteration 23/25 | Loss: 0.00082396
Iteration 24/25 | Loss: 0.00082498
Iteration 25/25 | Loss: 0.00082111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45983362
Iteration 2/25 | Loss: 0.00115691
Iteration 3/25 | Loss: 0.00115691
Iteration 4/25 | Loss: 0.00115691
Iteration 5/25 | Loss: 0.00115691
Iteration 6/25 | Loss: 0.00115691
Iteration 7/25 | Loss: 0.00115691
Iteration 8/25 | Loss: 0.00115691
Iteration 9/25 | Loss: 0.00115691
Iteration 10/25 | Loss: 0.00115691
Iteration 11/25 | Loss: 0.00115691
Iteration 12/25 | Loss: 0.00115691
Iteration 13/25 | Loss: 0.00115691
Iteration 14/25 | Loss: 0.00115691
Iteration 15/25 | Loss: 0.00115691
Iteration 16/25 | Loss: 0.00115691
Iteration 17/25 | Loss: 0.00115691
Iteration 18/25 | Loss: 0.00115691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011569058988243341, 0.0011569058988243341, 0.0011569058988243341, 0.0011569058988243341, 0.0011569058988243341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011569058988243341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115691
Iteration 2/1000 | Loss: 0.00044674
Iteration 3/1000 | Loss: 0.00016391
Iteration 4/1000 | Loss: 0.00025533
Iteration 5/1000 | Loss: 0.00018129
Iteration 6/1000 | Loss: 0.00023427
Iteration 7/1000 | Loss: 0.00013375
Iteration 8/1000 | Loss: 0.00007816
Iteration 9/1000 | Loss: 0.00025147
Iteration 10/1000 | Loss: 0.00012061
Iteration 11/1000 | Loss: 0.00013167
Iteration 12/1000 | Loss: 0.00015890
Iteration 13/1000 | Loss: 0.00011116
Iteration 14/1000 | Loss: 0.00046883
Iteration 15/1000 | Loss: 0.00016510
Iteration 16/1000 | Loss: 0.00022796
Iteration 17/1000 | Loss: 0.00012938
Iteration 18/1000 | Loss: 0.00016154
Iteration 19/1000 | Loss: 0.00013210
Iteration 20/1000 | Loss: 0.00020785
Iteration 21/1000 | Loss: 0.00018852
Iteration 22/1000 | Loss: 0.00006029
Iteration 23/1000 | Loss: 0.00005442
Iteration 24/1000 | Loss: 0.00007454
Iteration 25/1000 | Loss: 0.00010179
Iteration 26/1000 | Loss: 0.00020228
Iteration 27/1000 | Loss: 0.00052587
Iteration 28/1000 | Loss: 0.00027169
Iteration 29/1000 | Loss: 0.00021918
Iteration 30/1000 | Loss: 0.00021494
Iteration 31/1000 | Loss: 0.00015613
Iteration 32/1000 | Loss: 0.00017562
Iteration 33/1000 | Loss: 0.00021033
Iteration 34/1000 | Loss: 0.00005291
Iteration 35/1000 | Loss: 0.00019109
Iteration 36/1000 | Loss: 0.00019681
Iteration 37/1000 | Loss: 0.00020876
Iteration 38/1000 | Loss: 0.00016243
Iteration 39/1000 | Loss: 0.00049601
Iteration 40/1000 | Loss: 0.00019295
Iteration 41/1000 | Loss: 0.00022025
Iteration 42/1000 | Loss: 0.00018356
Iteration 43/1000 | Loss: 0.00021348
Iteration 44/1000 | Loss: 0.00014687
Iteration 45/1000 | Loss: 0.00009489
Iteration 46/1000 | Loss: 0.00015327
Iteration 47/1000 | Loss: 0.00011488
Iteration 48/1000 | Loss: 0.00003444
Iteration 49/1000 | Loss: 0.00004274
Iteration 50/1000 | Loss: 0.00002744
Iteration 51/1000 | Loss: 0.00019752
Iteration 52/1000 | Loss: 0.00043114
Iteration 53/1000 | Loss: 0.00024602
Iteration 54/1000 | Loss: 0.00016340
Iteration 55/1000 | Loss: 0.00040287
Iteration 56/1000 | Loss: 0.00008651
Iteration 57/1000 | Loss: 0.00021952
Iteration 58/1000 | Loss: 0.00012907
Iteration 59/1000 | Loss: 0.00019504
Iteration 60/1000 | Loss: 0.00021508
Iteration 61/1000 | Loss: 0.00032294
Iteration 62/1000 | Loss: 0.00028994
Iteration 63/1000 | Loss: 0.00011364
Iteration 64/1000 | Loss: 0.00008758
Iteration 65/1000 | Loss: 0.00026063
Iteration 66/1000 | Loss: 0.00024230
Iteration 67/1000 | Loss: 0.00031147
Iteration 68/1000 | Loss: 0.00003231
Iteration 69/1000 | Loss: 0.00004280
Iteration 70/1000 | Loss: 0.00014478
Iteration 71/1000 | Loss: 0.00012632
Iteration 72/1000 | Loss: 0.00003011
Iteration 73/1000 | Loss: 0.00031126
Iteration 74/1000 | Loss: 0.00008732
Iteration 75/1000 | Loss: 0.00005950
Iteration 76/1000 | Loss: 0.00002590
Iteration 77/1000 | Loss: 0.00002399
Iteration 78/1000 | Loss: 0.00002244
Iteration 79/1000 | Loss: 0.00002195
Iteration 80/1000 | Loss: 0.00002156
Iteration 81/1000 | Loss: 0.00002121
Iteration 82/1000 | Loss: 0.00004237
Iteration 83/1000 | Loss: 0.00020606
Iteration 84/1000 | Loss: 0.00006412
Iteration 85/1000 | Loss: 0.00002075
Iteration 86/1000 | Loss: 0.00016902
Iteration 87/1000 | Loss: 0.00044479
Iteration 88/1000 | Loss: 0.00005250
Iteration 89/1000 | Loss: 0.00002342
Iteration 90/1000 | Loss: 0.00022980
Iteration 91/1000 | Loss: 0.00016118
Iteration 92/1000 | Loss: 0.00005588
Iteration 93/1000 | Loss: 0.00012322
Iteration 94/1000 | Loss: 0.00010686
Iteration 95/1000 | Loss: 0.00009620
Iteration 96/1000 | Loss: 0.00022195
Iteration 97/1000 | Loss: 0.00011925
Iteration 98/1000 | Loss: 0.00005857
Iteration 99/1000 | Loss: 0.00017503
Iteration 100/1000 | Loss: 0.00042904
Iteration 101/1000 | Loss: 0.00017003
Iteration 102/1000 | Loss: 0.00018269
Iteration 103/1000 | Loss: 0.00022436
Iteration 104/1000 | Loss: 0.00015885
Iteration 105/1000 | Loss: 0.00034008
Iteration 106/1000 | Loss: 0.00024210
Iteration 107/1000 | Loss: 0.00002992
Iteration 108/1000 | Loss: 0.00002459
Iteration 109/1000 | Loss: 0.00002203
Iteration 110/1000 | Loss: 0.00018473
Iteration 111/1000 | Loss: 0.00002461
Iteration 112/1000 | Loss: 0.00027739
Iteration 113/1000 | Loss: 0.00031509
Iteration 114/1000 | Loss: 0.00034412
Iteration 115/1000 | Loss: 0.00017872
Iteration 116/1000 | Loss: 0.00072641
Iteration 117/1000 | Loss: 0.00082757
Iteration 118/1000 | Loss: 0.00051878
Iteration 119/1000 | Loss: 0.00007625
Iteration 120/1000 | Loss: 0.00021627
Iteration 121/1000 | Loss: 0.00025380
Iteration 122/1000 | Loss: 0.00024403
Iteration 123/1000 | Loss: 0.00002894
Iteration 124/1000 | Loss: 0.00018345
Iteration 125/1000 | Loss: 0.00003376
Iteration 126/1000 | Loss: 0.00017637
Iteration 127/1000 | Loss: 0.00019625
Iteration 128/1000 | Loss: 0.00005270
Iteration 129/1000 | Loss: 0.00004738
Iteration 130/1000 | Loss: 0.00015284
Iteration 131/1000 | Loss: 0.00019727
Iteration 132/1000 | Loss: 0.00018414
Iteration 133/1000 | Loss: 0.00003378
Iteration 134/1000 | Loss: 0.00002747
Iteration 135/1000 | Loss: 0.00002335
Iteration 136/1000 | Loss: 0.00002184
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002005
Iteration 139/1000 | Loss: 0.00001962
Iteration 140/1000 | Loss: 0.00001917
Iteration 141/1000 | Loss: 0.00001861
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00003754
Iteration 144/1000 | Loss: 0.00002401
Iteration 145/1000 | Loss: 0.00003291
Iteration 146/1000 | Loss: 0.00002154
Iteration 147/1000 | Loss: 0.00002618
Iteration 148/1000 | Loss: 0.00001995
Iteration 149/1000 | Loss: 0.00002758
Iteration 150/1000 | Loss: 0.00001999
Iteration 151/1000 | Loss: 0.00002103
Iteration 152/1000 | Loss: 0.00003935
Iteration 153/1000 | Loss: 0.00001901
Iteration 154/1000 | Loss: 0.00003639
Iteration 155/1000 | Loss: 0.00001760
Iteration 156/1000 | Loss: 0.00001724
Iteration 157/1000 | Loss: 0.00001705
Iteration 158/1000 | Loss: 0.00001704
Iteration 159/1000 | Loss: 0.00001698
Iteration 160/1000 | Loss: 0.00001692
Iteration 161/1000 | Loss: 0.00001689
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001679
Iteration 164/1000 | Loss: 0.00001673
Iteration 165/1000 | Loss: 0.00001673
Iteration 166/1000 | Loss: 0.00001671
Iteration 167/1000 | Loss: 0.00001670
Iteration 168/1000 | Loss: 0.00001668
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001660
Iteration 172/1000 | Loss: 0.00001660
Iteration 173/1000 | Loss: 0.00001656
Iteration 174/1000 | Loss: 0.00001656
Iteration 175/1000 | Loss: 0.00001656
Iteration 176/1000 | Loss: 0.00001656
Iteration 177/1000 | Loss: 0.00001656
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001655
Iteration 180/1000 | Loss: 0.00001655
Iteration 181/1000 | Loss: 0.00001655
Iteration 182/1000 | Loss: 0.00001655
Iteration 183/1000 | Loss: 0.00001655
Iteration 184/1000 | Loss: 0.00001655
Iteration 185/1000 | Loss: 0.00001655
Iteration 186/1000 | Loss: 0.00001655
Iteration 187/1000 | Loss: 0.00001655
Iteration 188/1000 | Loss: 0.00001655
Iteration 189/1000 | Loss: 0.00001655
Iteration 190/1000 | Loss: 0.00001655
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001654
Iteration 193/1000 | Loss: 0.00001654
Iteration 194/1000 | Loss: 0.00001654
Iteration 195/1000 | Loss: 0.00001654
Iteration 196/1000 | Loss: 0.00001653
Iteration 197/1000 | Loss: 0.00001653
Iteration 198/1000 | Loss: 0.00001653
Iteration 199/1000 | Loss: 0.00001653
Iteration 200/1000 | Loss: 0.00001653
Iteration 201/1000 | Loss: 0.00001653
Iteration 202/1000 | Loss: 0.00001652
Iteration 203/1000 | Loss: 0.00001652
Iteration 204/1000 | Loss: 0.00001652
Iteration 205/1000 | Loss: 0.00001652
Iteration 206/1000 | Loss: 0.00001652
Iteration 207/1000 | Loss: 0.00001652
Iteration 208/1000 | Loss: 0.00001652
Iteration 209/1000 | Loss: 0.00001651
Iteration 210/1000 | Loss: 0.00001651
Iteration 211/1000 | Loss: 0.00001651
Iteration 212/1000 | Loss: 0.00001651
Iteration 213/1000 | Loss: 0.00001651
Iteration 214/1000 | Loss: 0.00001651
Iteration 215/1000 | Loss: 0.00001651
Iteration 216/1000 | Loss: 0.00001651
Iteration 217/1000 | Loss: 0.00001651
Iteration 218/1000 | Loss: 0.00001651
Iteration 219/1000 | Loss: 0.00001651
Iteration 220/1000 | Loss: 0.00001650
Iteration 221/1000 | Loss: 0.00001650
Iteration 222/1000 | Loss: 0.00001650
Iteration 223/1000 | Loss: 0.00001650
Iteration 224/1000 | Loss: 0.00001650
Iteration 225/1000 | Loss: 0.00001650
Iteration 226/1000 | Loss: 0.00001650
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001650
Iteration 229/1000 | Loss: 0.00001650
Iteration 230/1000 | Loss: 0.00001650
Iteration 231/1000 | Loss: 0.00001650
Iteration 232/1000 | Loss: 0.00001650
Iteration 233/1000 | Loss: 0.00001650
Iteration 234/1000 | Loss: 0.00001650
Iteration 235/1000 | Loss: 0.00001650
Iteration 236/1000 | Loss: 0.00001650
Iteration 237/1000 | Loss: 0.00001650
Iteration 238/1000 | Loss: 0.00001650
Iteration 239/1000 | Loss: 0.00001650
Iteration 240/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [1.6502668586326763e-05, 1.6502668586326763e-05, 1.6502668586326763e-05, 1.6502668586326763e-05, 1.6502668586326763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6502668586326763e-05

Optimization complete. Final v2v error: 3.436445951461792 mm

Highest mean error: 4.798922061920166 mm for frame 94

Lowest mean error: 2.935485601425171 mm for frame 44

Saving results

Total time: 295.76649284362793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017456
Iteration 2/25 | Loss: 0.00473337
Iteration 3/25 | Loss: 0.00249652
Iteration 4/25 | Loss: 0.00213463
Iteration 5/25 | Loss: 0.00202021
Iteration 6/25 | Loss: 0.00191016
Iteration 7/25 | Loss: 0.00182507
Iteration 8/25 | Loss: 0.00182530
Iteration 9/25 | Loss: 0.00169676
Iteration 10/25 | Loss: 0.00165663
Iteration 11/25 | Loss: 0.00162110
Iteration 12/25 | Loss: 0.00157661
Iteration 13/25 | Loss: 0.00153713
Iteration 14/25 | Loss: 0.00152288
Iteration 15/25 | Loss: 0.00150627
Iteration 16/25 | Loss: 0.00148331
Iteration 17/25 | Loss: 0.00148272
Iteration 18/25 | Loss: 0.00148502
Iteration 19/25 | Loss: 0.00148301
Iteration 20/25 | Loss: 0.00148275
Iteration 21/25 | Loss: 0.00147677
Iteration 22/25 | Loss: 0.00147236
Iteration 23/25 | Loss: 0.00147367
Iteration 24/25 | Loss: 0.00147224
Iteration 25/25 | Loss: 0.00147241

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51495981
Iteration 2/25 | Loss: 0.00438016
Iteration 3/25 | Loss: 0.00438015
Iteration 4/25 | Loss: 0.00438015
Iteration 5/25 | Loss: 0.00438015
Iteration 6/25 | Loss: 0.00438015
Iteration 7/25 | Loss: 0.00438015
Iteration 8/25 | Loss: 0.00438015
Iteration 9/25 | Loss: 0.00438015
Iteration 10/25 | Loss: 0.00438015
Iteration 11/25 | Loss: 0.00438015
Iteration 12/25 | Loss: 0.00438015
Iteration 13/25 | Loss: 0.00438015
Iteration 14/25 | Loss: 0.00438015
Iteration 15/25 | Loss: 0.00438015
Iteration 16/25 | Loss: 0.00438015
Iteration 17/25 | Loss: 0.00438015
Iteration 18/25 | Loss: 0.00438015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004380148369818926, 0.004380148369818926, 0.004380148369818926, 0.004380148369818926, 0.004380148369818926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004380148369818926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438015
Iteration 2/1000 | Loss: 0.00148109
Iteration 3/1000 | Loss: 0.00091452
Iteration 4/1000 | Loss: 0.00097610
Iteration 5/1000 | Loss: 0.00074446
Iteration 6/1000 | Loss: 0.00065406
Iteration 7/1000 | Loss: 0.00049026
Iteration 8/1000 | Loss: 0.00059438
Iteration 9/1000 | Loss: 0.00125026
Iteration 10/1000 | Loss: 0.00131492
Iteration 11/1000 | Loss: 0.00573250
Iteration 12/1000 | Loss: 0.01178602
Iteration 13/1000 | Loss: 0.00567007
Iteration 14/1000 | Loss: 0.00206564
Iteration 15/1000 | Loss: 0.00074129
Iteration 16/1000 | Loss: 0.00041394
Iteration 17/1000 | Loss: 0.00029697
Iteration 18/1000 | Loss: 0.00024024
Iteration 19/1000 | Loss: 0.00024104
Iteration 20/1000 | Loss: 0.00043954
Iteration 21/1000 | Loss: 0.00031736
Iteration 22/1000 | Loss: 0.00040210
Iteration 23/1000 | Loss: 0.00025909
Iteration 24/1000 | Loss: 0.00008454
Iteration 25/1000 | Loss: 0.00007059
Iteration 26/1000 | Loss: 0.00029482
Iteration 27/1000 | Loss: 0.00011511
Iteration 28/1000 | Loss: 0.00026523
Iteration 29/1000 | Loss: 0.00011491
Iteration 30/1000 | Loss: 0.00030175
Iteration 31/1000 | Loss: 0.00042588
Iteration 32/1000 | Loss: 0.00036081
Iteration 33/1000 | Loss: 0.00019477
Iteration 34/1000 | Loss: 0.00057913
Iteration 35/1000 | Loss: 0.00027558
Iteration 36/1000 | Loss: 0.00041000
Iteration 37/1000 | Loss: 0.00028827
Iteration 38/1000 | Loss: 0.00032038
Iteration 39/1000 | Loss: 0.00027884
Iteration 40/1000 | Loss: 0.00027220
Iteration 41/1000 | Loss: 0.00028398
Iteration 42/1000 | Loss: 0.00025682
Iteration 43/1000 | Loss: 0.00023307
Iteration 44/1000 | Loss: 0.00024889
Iteration 45/1000 | Loss: 0.00014776
Iteration 46/1000 | Loss: 0.00003357
Iteration 47/1000 | Loss: 0.00003091
Iteration 48/1000 | Loss: 0.00002714
Iteration 49/1000 | Loss: 0.00002471
Iteration 50/1000 | Loss: 0.00002249
Iteration 51/1000 | Loss: 0.00047044
Iteration 52/1000 | Loss: 0.00018032
Iteration 53/1000 | Loss: 0.00025333
Iteration 54/1000 | Loss: 0.00002841
Iteration 55/1000 | Loss: 0.00003331
Iteration 56/1000 | Loss: 0.00002313
Iteration 57/1000 | Loss: 0.00002153
Iteration 58/1000 | Loss: 0.00023241
Iteration 59/1000 | Loss: 0.00005106
Iteration 60/1000 | Loss: 0.00002161
Iteration 61/1000 | Loss: 0.00025558
Iteration 62/1000 | Loss: 0.00004410
Iteration 63/1000 | Loss: 0.00002814
Iteration 64/1000 | Loss: 0.00002406
Iteration 65/1000 | Loss: 0.00002161
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00025910
Iteration 68/1000 | Loss: 0.00009206
Iteration 69/1000 | Loss: 0.00007455
Iteration 70/1000 | Loss: 0.00002057
Iteration 71/1000 | Loss: 0.00027952
Iteration 72/1000 | Loss: 0.00002750
Iteration 73/1000 | Loss: 0.00002264
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002014
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001888
Iteration 78/1000 | Loss: 0.00001827
Iteration 79/1000 | Loss: 0.00001761
Iteration 80/1000 | Loss: 0.00001734
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001717
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001714
Iteration 90/1000 | Loss: 0.00001714
Iteration 91/1000 | Loss: 0.00001714
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001713
Iteration 95/1000 | Loss: 0.00001713
Iteration 96/1000 | Loss: 0.00001713
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001710
Iteration 104/1000 | Loss: 0.00001710
Iteration 105/1000 | Loss: 0.00001710
Iteration 106/1000 | Loss: 0.00001710
Iteration 107/1000 | Loss: 0.00001709
Iteration 108/1000 | Loss: 0.00001709
Iteration 109/1000 | Loss: 0.00001709
Iteration 110/1000 | Loss: 0.00001709
Iteration 111/1000 | Loss: 0.00001709
Iteration 112/1000 | Loss: 0.00001709
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001708
Iteration 117/1000 | Loss: 0.00001708
Iteration 118/1000 | Loss: 0.00001708
Iteration 119/1000 | Loss: 0.00001708
Iteration 120/1000 | Loss: 0.00001708
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001707
Iteration 124/1000 | Loss: 0.00001707
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001706
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001704
Iteration 135/1000 | Loss: 0.00001704
Iteration 136/1000 | Loss: 0.00001703
Iteration 137/1000 | Loss: 0.00001703
Iteration 138/1000 | Loss: 0.00001703
Iteration 139/1000 | Loss: 0.00001703
Iteration 140/1000 | Loss: 0.00001703
Iteration 141/1000 | Loss: 0.00001703
Iteration 142/1000 | Loss: 0.00001703
Iteration 143/1000 | Loss: 0.00001703
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001703
Iteration 149/1000 | Loss: 0.00001703
Iteration 150/1000 | Loss: 0.00001703
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001702
Iteration 156/1000 | Loss: 0.00001702
Iteration 157/1000 | Loss: 0.00001702
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001701
Iteration 162/1000 | Loss: 0.00001701
Iteration 163/1000 | Loss: 0.00001701
Iteration 164/1000 | Loss: 0.00001701
Iteration 165/1000 | Loss: 0.00001701
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Iteration 168/1000 | Loss: 0.00001700
Iteration 169/1000 | Loss: 0.00001700
Iteration 170/1000 | Loss: 0.00001700
Iteration 171/1000 | Loss: 0.00001700
Iteration 172/1000 | Loss: 0.00001700
Iteration 173/1000 | Loss: 0.00001700
Iteration 174/1000 | Loss: 0.00001700
Iteration 175/1000 | Loss: 0.00001700
Iteration 176/1000 | Loss: 0.00001700
Iteration 177/1000 | Loss: 0.00001699
Iteration 178/1000 | Loss: 0.00001699
Iteration 179/1000 | Loss: 0.00001699
Iteration 180/1000 | Loss: 0.00001699
Iteration 181/1000 | Loss: 0.00001699
Iteration 182/1000 | Loss: 0.00001699
Iteration 183/1000 | Loss: 0.00001699
Iteration 184/1000 | Loss: 0.00001699
Iteration 185/1000 | Loss: 0.00001698
Iteration 186/1000 | Loss: 0.00001698
Iteration 187/1000 | Loss: 0.00001698
Iteration 188/1000 | Loss: 0.00001698
Iteration 189/1000 | Loss: 0.00001698
Iteration 190/1000 | Loss: 0.00001698
Iteration 191/1000 | Loss: 0.00001698
Iteration 192/1000 | Loss: 0.00001698
Iteration 193/1000 | Loss: 0.00001698
Iteration 194/1000 | Loss: 0.00001698
Iteration 195/1000 | Loss: 0.00001698
Iteration 196/1000 | Loss: 0.00001697
Iteration 197/1000 | Loss: 0.00001697
Iteration 198/1000 | Loss: 0.00001697
Iteration 199/1000 | Loss: 0.00001697
Iteration 200/1000 | Loss: 0.00001697
Iteration 201/1000 | Loss: 0.00001697
Iteration 202/1000 | Loss: 0.00001696
Iteration 203/1000 | Loss: 0.00001696
Iteration 204/1000 | Loss: 0.00001696
Iteration 205/1000 | Loss: 0.00001696
Iteration 206/1000 | Loss: 0.00001695
Iteration 207/1000 | Loss: 0.00001695
Iteration 208/1000 | Loss: 0.00001695
Iteration 209/1000 | Loss: 0.00001695
Iteration 210/1000 | Loss: 0.00001695
Iteration 211/1000 | Loss: 0.00001695
Iteration 212/1000 | Loss: 0.00001695
Iteration 213/1000 | Loss: 0.00001695
Iteration 214/1000 | Loss: 0.00001695
Iteration 215/1000 | Loss: 0.00001695
Iteration 216/1000 | Loss: 0.00001695
Iteration 217/1000 | Loss: 0.00001695
Iteration 218/1000 | Loss: 0.00001695
Iteration 219/1000 | Loss: 0.00001694
Iteration 220/1000 | Loss: 0.00001694
Iteration 221/1000 | Loss: 0.00001694
Iteration 222/1000 | Loss: 0.00001694
Iteration 223/1000 | Loss: 0.00001694
Iteration 224/1000 | Loss: 0.00001694
Iteration 225/1000 | Loss: 0.00001694
Iteration 226/1000 | Loss: 0.00001694
Iteration 227/1000 | Loss: 0.00001694
Iteration 228/1000 | Loss: 0.00001693
Iteration 229/1000 | Loss: 0.00001693
Iteration 230/1000 | Loss: 0.00001693
Iteration 231/1000 | Loss: 0.00001693
Iteration 232/1000 | Loss: 0.00001693
Iteration 233/1000 | Loss: 0.00001693
Iteration 234/1000 | Loss: 0.00001693
Iteration 235/1000 | Loss: 0.00001692
Iteration 236/1000 | Loss: 0.00001692
Iteration 237/1000 | Loss: 0.00001692
Iteration 238/1000 | Loss: 0.00001692
Iteration 239/1000 | Loss: 0.00001692
Iteration 240/1000 | Loss: 0.00001692
Iteration 241/1000 | Loss: 0.00001692
Iteration 242/1000 | Loss: 0.00001692
Iteration 243/1000 | Loss: 0.00001692
Iteration 244/1000 | Loss: 0.00001692
Iteration 245/1000 | Loss: 0.00001691
Iteration 246/1000 | Loss: 0.00001691
Iteration 247/1000 | Loss: 0.00001691
Iteration 248/1000 | Loss: 0.00001691
Iteration 249/1000 | Loss: 0.00001691
Iteration 250/1000 | Loss: 0.00001691
Iteration 251/1000 | Loss: 0.00001691
Iteration 252/1000 | Loss: 0.00001691
Iteration 253/1000 | Loss: 0.00001691
Iteration 254/1000 | Loss: 0.00001691
Iteration 255/1000 | Loss: 0.00001691
Iteration 256/1000 | Loss: 0.00001690
Iteration 257/1000 | Loss: 0.00001690
Iteration 258/1000 | Loss: 0.00001690
Iteration 259/1000 | Loss: 0.00001690
Iteration 260/1000 | Loss: 0.00001690
Iteration 261/1000 | Loss: 0.00001690
Iteration 262/1000 | Loss: 0.00001690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.6904448784771375e-05, 1.6904448784771375e-05, 1.6904448784771375e-05, 1.6904448784771375e-05, 1.6904448784771375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6904448784771375e-05

Optimization complete. Final v2v error: 3.410381317138672 mm

Highest mean error: 6.651429176330566 mm for frame 103

Lowest mean error: 3.183415174484253 mm for frame 4

Saving results

Total time: 196.92907428741455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785146
Iteration 2/25 | Loss: 0.00158427
Iteration 3/25 | Loss: 0.00109346
Iteration 4/25 | Loss: 0.00096621
Iteration 5/25 | Loss: 0.00095439
Iteration 6/25 | Loss: 0.00096312
Iteration 7/25 | Loss: 0.00091746
Iteration 8/25 | Loss: 0.00088453
Iteration 9/25 | Loss: 0.00087572
Iteration 10/25 | Loss: 0.00086476
Iteration 11/25 | Loss: 0.00085913
Iteration 12/25 | Loss: 0.00085403
Iteration 13/25 | Loss: 0.00084903
Iteration 14/25 | Loss: 0.00084545
Iteration 15/25 | Loss: 0.00084341
Iteration 16/25 | Loss: 0.00084279
Iteration 17/25 | Loss: 0.00084363
Iteration 18/25 | Loss: 0.00084213
Iteration 19/25 | Loss: 0.00084300
Iteration 20/25 | Loss: 0.00084280
Iteration 21/25 | Loss: 0.00084190
Iteration 22/25 | Loss: 0.00083996
Iteration 23/25 | Loss: 0.00083975
Iteration 24/25 | Loss: 0.00083969
Iteration 25/25 | Loss: 0.00083969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56239450
Iteration 2/25 | Loss: 0.00134595
Iteration 3/25 | Loss: 0.00134593
Iteration 4/25 | Loss: 0.00134593
Iteration 5/25 | Loss: 0.00134593
Iteration 6/25 | Loss: 0.00134593
Iteration 7/25 | Loss: 0.00134593
Iteration 8/25 | Loss: 0.00134593
Iteration 9/25 | Loss: 0.00134593
Iteration 10/25 | Loss: 0.00134593
Iteration 11/25 | Loss: 0.00134593
Iteration 12/25 | Loss: 0.00134593
Iteration 13/25 | Loss: 0.00134593
Iteration 14/25 | Loss: 0.00134593
Iteration 15/25 | Loss: 0.00134593
Iteration 16/25 | Loss: 0.00134593
Iteration 17/25 | Loss: 0.00134593
Iteration 18/25 | Loss: 0.00134593
Iteration 19/25 | Loss: 0.00134593
Iteration 20/25 | Loss: 0.00134593
Iteration 21/25 | Loss: 0.00134593
Iteration 22/25 | Loss: 0.00134593
Iteration 23/25 | Loss: 0.00134593
Iteration 24/25 | Loss: 0.00134593
Iteration 25/25 | Loss: 0.00134593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134593
Iteration 2/1000 | Loss: 0.00007949
Iteration 3/1000 | Loss: 0.00005602
Iteration 4/1000 | Loss: 0.00004788
Iteration 5/1000 | Loss: 0.00004448
Iteration 6/1000 | Loss: 0.00004254
Iteration 7/1000 | Loss: 0.00004073
Iteration 8/1000 | Loss: 0.00003967
Iteration 9/1000 | Loss: 0.00003899
Iteration 10/1000 | Loss: 0.00003837
Iteration 11/1000 | Loss: 0.00003802
Iteration 12/1000 | Loss: 0.00003774
Iteration 13/1000 | Loss: 0.00003755
Iteration 14/1000 | Loss: 0.00047998
Iteration 15/1000 | Loss: 0.00044936
Iteration 16/1000 | Loss: 0.00094200
Iteration 17/1000 | Loss: 0.00108043
Iteration 18/1000 | Loss: 0.00006945
Iteration 19/1000 | Loss: 0.00004529
Iteration 20/1000 | Loss: 0.00003908
Iteration 21/1000 | Loss: 0.00003565
Iteration 22/1000 | Loss: 0.00003386
Iteration 23/1000 | Loss: 0.00003209
Iteration 24/1000 | Loss: 0.00003096
Iteration 25/1000 | Loss: 0.00003029
Iteration 26/1000 | Loss: 0.00002989
Iteration 27/1000 | Loss: 0.00002948
Iteration 28/1000 | Loss: 0.00002921
Iteration 29/1000 | Loss: 0.00002901
Iteration 30/1000 | Loss: 0.00002900
Iteration 31/1000 | Loss: 0.00002882
Iteration 32/1000 | Loss: 0.00002872
Iteration 33/1000 | Loss: 0.00002869
Iteration 34/1000 | Loss: 0.00002864
Iteration 35/1000 | Loss: 0.00002864
Iteration 36/1000 | Loss: 0.00002861
Iteration 37/1000 | Loss: 0.00002861
Iteration 38/1000 | Loss: 0.00002861
Iteration 39/1000 | Loss: 0.00002860
Iteration 40/1000 | Loss: 0.00002860
Iteration 41/1000 | Loss: 0.00002859
Iteration 42/1000 | Loss: 0.00002859
Iteration 43/1000 | Loss: 0.00002859
Iteration 44/1000 | Loss: 0.00002859
Iteration 45/1000 | Loss: 0.00002859
Iteration 46/1000 | Loss: 0.00002859
Iteration 47/1000 | Loss: 0.00002858
Iteration 48/1000 | Loss: 0.00002858
Iteration 49/1000 | Loss: 0.00002856
Iteration 50/1000 | Loss: 0.00002856
Iteration 51/1000 | Loss: 0.00002856
Iteration 52/1000 | Loss: 0.00002855
Iteration 53/1000 | Loss: 0.00002855
Iteration 54/1000 | Loss: 0.00002854
Iteration 55/1000 | Loss: 0.00002854
Iteration 56/1000 | Loss: 0.00002854
Iteration 57/1000 | Loss: 0.00002853
Iteration 58/1000 | Loss: 0.00002853
Iteration 59/1000 | Loss: 0.00002853
Iteration 60/1000 | Loss: 0.00002853
Iteration 61/1000 | Loss: 0.00002852
Iteration 62/1000 | Loss: 0.00002852
Iteration 63/1000 | Loss: 0.00002852
Iteration 64/1000 | Loss: 0.00002852
Iteration 65/1000 | Loss: 0.00002851
Iteration 66/1000 | Loss: 0.00002851
Iteration 67/1000 | Loss: 0.00002851
Iteration 68/1000 | Loss: 0.00002851
Iteration 69/1000 | Loss: 0.00002851
Iteration 70/1000 | Loss: 0.00002851
Iteration 71/1000 | Loss: 0.00002850
Iteration 72/1000 | Loss: 0.00002850
Iteration 73/1000 | Loss: 0.00002850
Iteration 74/1000 | Loss: 0.00002849
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002849
Iteration 77/1000 | Loss: 0.00002849
Iteration 78/1000 | Loss: 0.00002848
Iteration 79/1000 | Loss: 0.00002848
Iteration 80/1000 | Loss: 0.00002848
Iteration 81/1000 | Loss: 0.00002848
Iteration 82/1000 | Loss: 0.00002848
Iteration 83/1000 | Loss: 0.00002848
Iteration 84/1000 | Loss: 0.00002848
Iteration 85/1000 | Loss: 0.00002848
Iteration 86/1000 | Loss: 0.00002847
Iteration 87/1000 | Loss: 0.00002847
Iteration 88/1000 | Loss: 0.00002847
Iteration 89/1000 | Loss: 0.00002847
Iteration 90/1000 | Loss: 0.00002847
Iteration 91/1000 | Loss: 0.00002847
Iteration 92/1000 | Loss: 0.00002847
Iteration 93/1000 | Loss: 0.00002847
Iteration 94/1000 | Loss: 0.00002847
Iteration 95/1000 | Loss: 0.00002847
Iteration 96/1000 | Loss: 0.00002847
Iteration 97/1000 | Loss: 0.00002847
Iteration 98/1000 | Loss: 0.00002847
Iteration 99/1000 | Loss: 0.00002847
Iteration 100/1000 | Loss: 0.00002847
Iteration 101/1000 | Loss: 0.00002847
Iteration 102/1000 | Loss: 0.00002847
Iteration 103/1000 | Loss: 0.00002847
Iteration 104/1000 | Loss: 0.00002847
Iteration 105/1000 | Loss: 0.00002847
Iteration 106/1000 | Loss: 0.00002847
Iteration 107/1000 | Loss: 0.00002847
Iteration 108/1000 | Loss: 0.00002847
Iteration 109/1000 | Loss: 0.00002847
Iteration 110/1000 | Loss: 0.00002847
Iteration 111/1000 | Loss: 0.00002847
Iteration 112/1000 | Loss: 0.00002847
Iteration 113/1000 | Loss: 0.00002847
Iteration 114/1000 | Loss: 0.00002847
Iteration 115/1000 | Loss: 0.00002847
Iteration 116/1000 | Loss: 0.00002847
Iteration 117/1000 | Loss: 0.00002847
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00002847
Iteration 120/1000 | Loss: 0.00002847
Iteration 121/1000 | Loss: 0.00002847
Iteration 122/1000 | Loss: 0.00002847
Iteration 123/1000 | Loss: 0.00002847
Iteration 124/1000 | Loss: 0.00002847
Iteration 125/1000 | Loss: 0.00002847
Iteration 126/1000 | Loss: 0.00002847
Iteration 127/1000 | Loss: 0.00002847
Iteration 128/1000 | Loss: 0.00002847
Iteration 129/1000 | Loss: 0.00002847
Iteration 130/1000 | Loss: 0.00002847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.8471986297518015e-05, 2.8471986297518015e-05, 2.8471986297518015e-05, 2.8471986297518015e-05, 2.8471986297518015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8471986297518015e-05

Optimization complete. Final v2v error: 4.207733154296875 mm

Highest mean error: 6.627859592437744 mm for frame 26

Lowest mean error: 3.4300384521484375 mm for frame 103

Saving results

Total time: 106.3889548778534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966325
Iteration 2/25 | Loss: 0.00154252
Iteration 3/25 | Loss: 0.00093410
Iteration 4/25 | Loss: 0.00088002
Iteration 5/25 | Loss: 0.00086092
Iteration 6/25 | Loss: 0.00085723
Iteration 7/25 | Loss: 0.00085641
Iteration 8/25 | Loss: 0.00085636
Iteration 9/25 | Loss: 0.00085636
Iteration 10/25 | Loss: 0.00085636
Iteration 11/25 | Loss: 0.00085636
Iteration 12/25 | Loss: 0.00085636
Iteration 13/25 | Loss: 0.00085636
Iteration 14/25 | Loss: 0.00085636
Iteration 15/25 | Loss: 0.00085636
Iteration 16/25 | Loss: 0.00085636
Iteration 17/25 | Loss: 0.00085636
Iteration 18/25 | Loss: 0.00085636
Iteration 19/25 | Loss: 0.00085636
Iteration 20/25 | Loss: 0.00085636
Iteration 21/25 | Loss: 0.00085636
Iteration 22/25 | Loss: 0.00085636
Iteration 23/25 | Loss: 0.00085636
Iteration 24/25 | Loss: 0.00085636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008563635055907071, 0.0008563635055907071, 0.0008563635055907071, 0.0008563635055907071, 0.0008563635055907071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008563635055907071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96265352
Iteration 2/25 | Loss: 0.00062814
Iteration 3/25 | Loss: 0.00062813
Iteration 4/25 | Loss: 0.00062812
Iteration 5/25 | Loss: 0.00062812
Iteration 6/25 | Loss: 0.00062812
Iteration 7/25 | Loss: 0.00062812
Iteration 8/25 | Loss: 0.00062812
Iteration 9/25 | Loss: 0.00062812
Iteration 10/25 | Loss: 0.00062812
Iteration 11/25 | Loss: 0.00062812
Iteration 12/25 | Loss: 0.00062812
Iteration 13/25 | Loss: 0.00062812
Iteration 14/25 | Loss: 0.00062812
Iteration 15/25 | Loss: 0.00062812
Iteration 16/25 | Loss: 0.00062812
Iteration 17/25 | Loss: 0.00062812
Iteration 18/25 | Loss: 0.00062812
Iteration 19/25 | Loss: 0.00062812
Iteration 20/25 | Loss: 0.00062812
Iteration 21/25 | Loss: 0.00062812
Iteration 22/25 | Loss: 0.00062812
Iteration 23/25 | Loss: 0.00062812
Iteration 24/25 | Loss: 0.00062812
Iteration 25/25 | Loss: 0.00062812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062812
Iteration 2/1000 | Loss: 0.00005630
Iteration 3/1000 | Loss: 0.00004130
Iteration 4/1000 | Loss: 0.00003615
Iteration 5/1000 | Loss: 0.00003414
Iteration 6/1000 | Loss: 0.00003254
Iteration 7/1000 | Loss: 0.00003176
Iteration 8/1000 | Loss: 0.00003127
Iteration 9/1000 | Loss: 0.00003083
Iteration 10/1000 | Loss: 0.00003053
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00003015
Iteration 13/1000 | Loss: 0.00003008
Iteration 14/1000 | Loss: 0.00003002
Iteration 15/1000 | Loss: 0.00002999
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002978
Iteration 18/1000 | Loss: 0.00002976
Iteration 19/1000 | Loss: 0.00002970
Iteration 20/1000 | Loss: 0.00002970
Iteration 21/1000 | Loss: 0.00002968
Iteration 22/1000 | Loss: 0.00002967
Iteration 23/1000 | Loss: 0.00002966
Iteration 24/1000 | Loss: 0.00002965
Iteration 25/1000 | Loss: 0.00002962
Iteration 26/1000 | Loss: 0.00002961
Iteration 27/1000 | Loss: 0.00002961
Iteration 28/1000 | Loss: 0.00002960
Iteration 29/1000 | Loss: 0.00002959
Iteration 30/1000 | Loss: 0.00002958
Iteration 31/1000 | Loss: 0.00002956
Iteration 32/1000 | Loss: 0.00002955
Iteration 33/1000 | Loss: 0.00002954
Iteration 34/1000 | Loss: 0.00002954
Iteration 35/1000 | Loss: 0.00002951
Iteration 36/1000 | Loss: 0.00002951
Iteration 37/1000 | Loss: 0.00002951
Iteration 38/1000 | Loss: 0.00002950
Iteration 39/1000 | Loss: 0.00002950
Iteration 40/1000 | Loss: 0.00002949
Iteration 41/1000 | Loss: 0.00002949
Iteration 42/1000 | Loss: 0.00002948
Iteration 43/1000 | Loss: 0.00002948
Iteration 44/1000 | Loss: 0.00002948
Iteration 45/1000 | Loss: 0.00002947
Iteration 46/1000 | Loss: 0.00002947
Iteration 47/1000 | Loss: 0.00002947
Iteration 48/1000 | Loss: 0.00002946
Iteration 49/1000 | Loss: 0.00002946
Iteration 50/1000 | Loss: 0.00002945
Iteration 51/1000 | Loss: 0.00002945
Iteration 52/1000 | Loss: 0.00002945
Iteration 53/1000 | Loss: 0.00002945
Iteration 54/1000 | Loss: 0.00002945
Iteration 55/1000 | Loss: 0.00002945
Iteration 56/1000 | Loss: 0.00002945
Iteration 57/1000 | Loss: 0.00002945
Iteration 58/1000 | Loss: 0.00002945
Iteration 59/1000 | Loss: 0.00002945
Iteration 60/1000 | Loss: 0.00002945
Iteration 61/1000 | Loss: 0.00002945
Iteration 62/1000 | Loss: 0.00002944
Iteration 63/1000 | Loss: 0.00002944
Iteration 64/1000 | Loss: 0.00002944
Iteration 65/1000 | Loss: 0.00002944
Iteration 66/1000 | Loss: 0.00002944
Iteration 67/1000 | Loss: 0.00002943
Iteration 68/1000 | Loss: 0.00002943
Iteration 69/1000 | Loss: 0.00002943
Iteration 70/1000 | Loss: 0.00002943
Iteration 71/1000 | Loss: 0.00002943
Iteration 72/1000 | Loss: 0.00002943
Iteration 73/1000 | Loss: 0.00002943
Iteration 74/1000 | Loss: 0.00002943
Iteration 75/1000 | Loss: 0.00002943
Iteration 76/1000 | Loss: 0.00002943
Iteration 77/1000 | Loss: 0.00002942
Iteration 78/1000 | Loss: 0.00002942
Iteration 79/1000 | Loss: 0.00002942
Iteration 80/1000 | Loss: 0.00002941
Iteration 81/1000 | Loss: 0.00002941
Iteration 82/1000 | Loss: 0.00002941
Iteration 83/1000 | Loss: 0.00002941
Iteration 84/1000 | Loss: 0.00002941
Iteration 85/1000 | Loss: 0.00002941
Iteration 86/1000 | Loss: 0.00002941
Iteration 87/1000 | Loss: 0.00002940
Iteration 88/1000 | Loss: 0.00002940
Iteration 89/1000 | Loss: 0.00002940
Iteration 90/1000 | Loss: 0.00002940
Iteration 91/1000 | Loss: 0.00002940
Iteration 92/1000 | Loss: 0.00002939
Iteration 93/1000 | Loss: 0.00002939
Iteration 94/1000 | Loss: 0.00002939
Iteration 95/1000 | Loss: 0.00002939
Iteration 96/1000 | Loss: 0.00002939
Iteration 97/1000 | Loss: 0.00002939
Iteration 98/1000 | Loss: 0.00002938
Iteration 99/1000 | Loss: 0.00002938
Iteration 100/1000 | Loss: 0.00002938
Iteration 101/1000 | Loss: 0.00002938
Iteration 102/1000 | Loss: 0.00002938
Iteration 103/1000 | Loss: 0.00002938
Iteration 104/1000 | Loss: 0.00002938
Iteration 105/1000 | Loss: 0.00002938
Iteration 106/1000 | Loss: 0.00002938
Iteration 107/1000 | Loss: 0.00002937
Iteration 108/1000 | Loss: 0.00002937
Iteration 109/1000 | Loss: 0.00002937
Iteration 110/1000 | Loss: 0.00002937
Iteration 111/1000 | Loss: 0.00002937
Iteration 112/1000 | Loss: 0.00002937
Iteration 113/1000 | Loss: 0.00002937
Iteration 114/1000 | Loss: 0.00002937
Iteration 115/1000 | Loss: 0.00002936
Iteration 116/1000 | Loss: 0.00002936
Iteration 117/1000 | Loss: 0.00002936
Iteration 118/1000 | Loss: 0.00002936
Iteration 119/1000 | Loss: 0.00002936
Iteration 120/1000 | Loss: 0.00002936
Iteration 121/1000 | Loss: 0.00002936
Iteration 122/1000 | Loss: 0.00002935
Iteration 123/1000 | Loss: 0.00002935
Iteration 124/1000 | Loss: 0.00002935
Iteration 125/1000 | Loss: 0.00002935
Iteration 126/1000 | Loss: 0.00002934
Iteration 127/1000 | Loss: 0.00002934
Iteration 128/1000 | Loss: 0.00002934
Iteration 129/1000 | Loss: 0.00002934
Iteration 130/1000 | Loss: 0.00002934
Iteration 131/1000 | Loss: 0.00002934
Iteration 132/1000 | Loss: 0.00002933
Iteration 133/1000 | Loss: 0.00002933
Iteration 134/1000 | Loss: 0.00002933
Iteration 135/1000 | Loss: 0.00002933
Iteration 136/1000 | Loss: 0.00002933
Iteration 137/1000 | Loss: 0.00002933
Iteration 138/1000 | Loss: 0.00002933
Iteration 139/1000 | Loss: 0.00002933
Iteration 140/1000 | Loss: 0.00002933
Iteration 141/1000 | Loss: 0.00002933
Iteration 142/1000 | Loss: 0.00002933
Iteration 143/1000 | Loss: 0.00002933
Iteration 144/1000 | Loss: 0.00002932
Iteration 145/1000 | Loss: 0.00002932
Iteration 146/1000 | Loss: 0.00002932
Iteration 147/1000 | Loss: 0.00002932
Iteration 148/1000 | Loss: 0.00002932
Iteration 149/1000 | Loss: 0.00002932
Iteration 150/1000 | Loss: 0.00002932
Iteration 151/1000 | Loss: 0.00002932
Iteration 152/1000 | Loss: 0.00002932
Iteration 153/1000 | Loss: 0.00002932
Iteration 154/1000 | Loss: 0.00002932
Iteration 155/1000 | Loss: 0.00002932
Iteration 156/1000 | Loss: 0.00002931
Iteration 157/1000 | Loss: 0.00002931
Iteration 158/1000 | Loss: 0.00002931
Iteration 159/1000 | Loss: 0.00002931
Iteration 160/1000 | Loss: 0.00002931
Iteration 161/1000 | Loss: 0.00002931
Iteration 162/1000 | Loss: 0.00002931
Iteration 163/1000 | Loss: 0.00002931
Iteration 164/1000 | Loss: 0.00002931
Iteration 165/1000 | Loss: 0.00002931
Iteration 166/1000 | Loss: 0.00002930
Iteration 167/1000 | Loss: 0.00002930
Iteration 168/1000 | Loss: 0.00002930
Iteration 169/1000 | Loss: 0.00002930
Iteration 170/1000 | Loss: 0.00002930
Iteration 171/1000 | Loss: 0.00002930
Iteration 172/1000 | Loss: 0.00002930
Iteration 173/1000 | Loss: 0.00002930
Iteration 174/1000 | Loss: 0.00002930
Iteration 175/1000 | Loss: 0.00002930
Iteration 176/1000 | Loss: 0.00002930
Iteration 177/1000 | Loss: 0.00002930
Iteration 178/1000 | Loss: 0.00002930
Iteration 179/1000 | Loss: 0.00002930
Iteration 180/1000 | Loss: 0.00002930
Iteration 181/1000 | Loss: 0.00002930
Iteration 182/1000 | Loss: 0.00002930
Iteration 183/1000 | Loss: 0.00002930
Iteration 184/1000 | Loss: 0.00002930
Iteration 185/1000 | Loss: 0.00002930
Iteration 186/1000 | Loss: 0.00002930
Iteration 187/1000 | Loss: 0.00002930
Iteration 188/1000 | Loss: 0.00002930
Iteration 189/1000 | Loss: 0.00002930
Iteration 190/1000 | Loss: 0.00002930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.9297871151356958e-05, 2.9297871151356958e-05, 2.9297871151356958e-05, 2.9297871151356958e-05, 2.9297871151356958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9297871151356958e-05

Optimization complete. Final v2v error: 4.489785671234131 mm

Highest mean error: 5.424065589904785 mm for frame 133

Lowest mean error: 3.702242374420166 mm for frame 37

Saving results

Total time: 46.33035612106323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787958
Iteration 2/25 | Loss: 0.00137632
Iteration 3/25 | Loss: 0.00085209
Iteration 4/25 | Loss: 0.00077426
Iteration 5/25 | Loss: 0.00076453
Iteration 6/25 | Loss: 0.00076262
Iteration 7/25 | Loss: 0.00076259
Iteration 8/25 | Loss: 0.00076259
Iteration 9/25 | Loss: 0.00076259
Iteration 10/25 | Loss: 0.00076259
Iteration 11/25 | Loss: 0.00076259
Iteration 12/25 | Loss: 0.00076259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007625893340446055, 0.0007625893340446055, 0.0007625893340446055, 0.0007625893340446055, 0.0007625893340446055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007625893340446055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52695358
Iteration 2/25 | Loss: 0.00089710
Iteration 3/25 | Loss: 0.00089708
Iteration 4/25 | Loss: 0.00089708
Iteration 5/25 | Loss: 0.00089708
Iteration 6/25 | Loss: 0.00089708
Iteration 7/25 | Loss: 0.00089708
Iteration 8/25 | Loss: 0.00089708
Iteration 9/25 | Loss: 0.00089708
Iteration 10/25 | Loss: 0.00089708
Iteration 11/25 | Loss: 0.00089708
Iteration 12/25 | Loss: 0.00089708
Iteration 13/25 | Loss: 0.00089708
Iteration 14/25 | Loss: 0.00089708
Iteration 15/25 | Loss: 0.00089708
Iteration 16/25 | Loss: 0.00089708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008970775525085628, 0.0008970775525085628, 0.0008970775525085628, 0.0008970775525085628, 0.0008970775525085628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008970775525085628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089708
Iteration 2/1000 | Loss: 0.00002497
Iteration 3/1000 | Loss: 0.00001826
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001493
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001446
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001426
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001424
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001418
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001414
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001414
Iteration 29/1000 | Loss: 0.00001414
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001405
Iteration 36/1000 | Loss: 0.00001405
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001399
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001397
Iteration 50/1000 | Loss: 0.00001397
Iteration 51/1000 | Loss: 0.00001397
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.396971128997393e-05, 1.396971128997393e-05, 1.396971128997393e-05, 1.396971128997393e-05, 1.396971128997393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.396971128997393e-05

Optimization complete. Final v2v error: 3.2020955085754395 mm

Highest mean error: 3.4446358680725098 mm for frame 0

Lowest mean error: 3.07470703125 mm for frame 179

Saving results

Total time: 30.599120378494263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099635
Iteration 2/25 | Loss: 0.01099635
Iteration 3/25 | Loss: 0.01099635
Iteration 4/25 | Loss: 0.01099635
Iteration 5/25 | Loss: 0.01099634
Iteration 6/25 | Loss: 0.01099634
Iteration 7/25 | Loss: 0.01099634
Iteration 8/25 | Loss: 0.01099634
Iteration 9/25 | Loss: 0.01099634
Iteration 10/25 | Loss: 0.01099633
Iteration 11/25 | Loss: 0.01099633
Iteration 12/25 | Loss: 0.01099633
Iteration 13/25 | Loss: 0.01099633
Iteration 14/25 | Loss: 0.01099633
Iteration 15/25 | Loss: 0.01099633
Iteration 16/25 | Loss: 0.01099633
Iteration 17/25 | Loss: 0.01099633
Iteration 18/25 | Loss: 0.01099632
Iteration 19/25 | Loss: 0.01099632
Iteration 20/25 | Loss: 0.01099632
Iteration 21/25 | Loss: 0.01099631
Iteration 22/25 | Loss: 0.01099631
Iteration 23/25 | Loss: 0.01099631
Iteration 24/25 | Loss: 0.01099631
Iteration 25/25 | Loss: 0.01099631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06096840
Iteration 2/25 | Loss: 0.09305892
Iteration 3/25 | Loss: 0.08916158
Iteration 4/25 | Loss: 0.09104937
Iteration 5/25 | Loss: 0.09162413
Iteration 6/25 | Loss: 0.08814871
Iteration 7/25 | Loss: 0.08814871
Iteration 8/25 | Loss: 0.08814870
Iteration 9/25 | Loss: 0.08814870
Iteration 10/25 | Loss: 0.08814869
Iteration 11/25 | Loss: 0.08814869
Iteration 12/25 | Loss: 0.08814869
Iteration 13/25 | Loss: 0.08814869
Iteration 14/25 | Loss: 0.08814869
Iteration 15/25 | Loss: 0.08814869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08814869076013565, 0.08814869076013565, 0.08814869076013565, 0.08814869076013565, 0.08814869076013565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08814869076013565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08814870
Iteration 2/1000 | Loss: 0.00374088
Iteration 3/1000 | Loss: 0.00232230
Iteration 4/1000 | Loss: 0.00080993
Iteration 5/1000 | Loss: 0.00306151
Iteration 6/1000 | Loss: 0.00396341
Iteration 7/1000 | Loss: 0.00083643
Iteration 8/1000 | Loss: 0.00389401
Iteration 9/1000 | Loss: 0.00157424
Iteration 10/1000 | Loss: 0.00022044
Iteration 11/1000 | Loss: 0.00011135
Iteration 12/1000 | Loss: 0.00007294
Iteration 13/1000 | Loss: 0.00012656
Iteration 14/1000 | Loss: 0.00083345
Iteration 15/1000 | Loss: 0.00005466
Iteration 16/1000 | Loss: 0.00031283
Iteration 17/1000 | Loss: 0.00005554
Iteration 18/1000 | Loss: 0.00009149
Iteration 19/1000 | Loss: 0.00034043
Iteration 20/1000 | Loss: 0.00003163
Iteration 21/1000 | Loss: 0.00005191
Iteration 22/1000 | Loss: 0.00022215
Iteration 23/1000 | Loss: 0.00019484
Iteration 24/1000 | Loss: 0.00023362
Iteration 25/1000 | Loss: 0.00013310
Iteration 26/1000 | Loss: 0.00019393
Iteration 27/1000 | Loss: 0.00019776
Iteration 28/1000 | Loss: 0.00011581
Iteration 29/1000 | Loss: 0.00002839
Iteration 30/1000 | Loss: 0.00002576
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002354
Iteration 33/1000 | Loss: 0.00029938
Iteration 34/1000 | Loss: 0.00003950
Iteration 35/1000 | Loss: 0.00002375
Iteration 36/1000 | Loss: 0.00004135
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00001952
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001832
Iteration 41/1000 | Loss: 0.00005140
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00004006
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00004226
Iteration 47/1000 | Loss: 0.00001703
Iteration 48/1000 | Loss: 0.00003222
Iteration 49/1000 | Loss: 0.00001595
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001542
Iteration 61/1000 | Loss: 0.00001542
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001539
Iteration 67/1000 | Loss: 0.00001537
Iteration 68/1000 | Loss: 0.00001536
Iteration 69/1000 | Loss: 0.00001536
Iteration 70/1000 | Loss: 0.00001534
Iteration 71/1000 | Loss: 0.00001533
Iteration 72/1000 | Loss: 0.00001533
Iteration 73/1000 | Loss: 0.00001533
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001531
Iteration 77/1000 | Loss: 0.00001531
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00002620
Iteration 81/1000 | Loss: 0.00009135
Iteration 82/1000 | Loss: 0.00002494
Iteration 83/1000 | Loss: 0.00001569
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00008393
Iteration 86/1000 | Loss: 0.00002540
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001514
Iteration 93/1000 | Loss: 0.00001514
Iteration 94/1000 | Loss: 0.00001514
Iteration 95/1000 | Loss: 0.00001514
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001514
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001508
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001507
Iteration 119/1000 | Loss: 0.00001507
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001506
Iteration 123/1000 | Loss: 0.00001506
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001504
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001503
Iteration 136/1000 | Loss: 0.00001503
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001501
Iteration 148/1000 | Loss: 0.00001501
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001501
Iteration 151/1000 | Loss: 0.00001500
Iteration 152/1000 | Loss: 0.00001500
Iteration 153/1000 | Loss: 0.00001500
Iteration 154/1000 | Loss: 0.00001500
Iteration 155/1000 | Loss: 0.00001500
Iteration 156/1000 | Loss: 0.00001500
Iteration 157/1000 | Loss: 0.00001500
Iteration 158/1000 | Loss: 0.00001500
Iteration 159/1000 | Loss: 0.00001500
Iteration 160/1000 | Loss: 0.00001500
Iteration 161/1000 | Loss: 0.00001500
Iteration 162/1000 | Loss: 0.00001499
Iteration 163/1000 | Loss: 0.00001499
Iteration 164/1000 | Loss: 0.00001499
Iteration 165/1000 | Loss: 0.00001499
Iteration 166/1000 | Loss: 0.00001499
Iteration 167/1000 | Loss: 0.00001499
Iteration 168/1000 | Loss: 0.00001499
Iteration 169/1000 | Loss: 0.00001499
Iteration 170/1000 | Loss: 0.00001498
Iteration 171/1000 | Loss: 0.00001498
Iteration 172/1000 | Loss: 0.00001498
Iteration 173/1000 | Loss: 0.00001498
Iteration 174/1000 | Loss: 0.00001498
Iteration 175/1000 | Loss: 0.00001498
Iteration 176/1000 | Loss: 0.00001498
Iteration 177/1000 | Loss: 0.00001498
Iteration 178/1000 | Loss: 0.00001498
Iteration 179/1000 | Loss: 0.00001498
Iteration 180/1000 | Loss: 0.00001497
Iteration 181/1000 | Loss: 0.00001497
Iteration 182/1000 | Loss: 0.00001497
Iteration 183/1000 | Loss: 0.00001497
Iteration 184/1000 | Loss: 0.00001497
Iteration 185/1000 | Loss: 0.00001497
Iteration 186/1000 | Loss: 0.00001497
Iteration 187/1000 | Loss: 0.00001497
Iteration 188/1000 | Loss: 0.00001497
Iteration 189/1000 | Loss: 0.00001497
Iteration 190/1000 | Loss: 0.00001497
Iteration 191/1000 | Loss: 0.00001497
Iteration 192/1000 | Loss: 0.00001497
Iteration 193/1000 | Loss: 0.00001497
Iteration 194/1000 | Loss: 0.00001497
Iteration 195/1000 | Loss: 0.00001497
Iteration 196/1000 | Loss: 0.00001497
Iteration 197/1000 | Loss: 0.00001497
Iteration 198/1000 | Loss: 0.00001497
Iteration 199/1000 | Loss: 0.00001497
Iteration 200/1000 | Loss: 0.00001497
Iteration 201/1000 | Loss: 0.00001497
Iteration 202/1000 | Loss: 0.00001497
Iteration 203/1000 | Loss: 0.00001497
Iteration 204/1000 | Loss: 0.00001497
Iteration 205/1000 | Loss: 0.00001497
Iteration 206/1000 | Loss: 0.00001497
Iteration 207/1000 | Loss: 0.00001497
Iteration 208/1000 | Loss: 0.00001497
Iteration 209/1000 | Loss: 0.00001497
Iteration 210/1000 | Loss: 0.00001497
Iteration 211/1000 | Loss: 0.00001497
Iteration 212/1000 | Loss: 0.00001497
Iteration 213/1000 | Loss: 0.00001497
Iteration 214/1000 | Loss: 0.00001497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.4966008166084066e-05, 1.4966008166084066e-05, 1.4966008166084066e-05, 1.4966008166084066e-05, 1.4966008166084066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4966008166084066e-05

Optimization complete. Final v2v error: 3.279546022415161 mm

Highest mean error: 10.051847457885742 mm for frame 212

Lowest mean error: 2.7366397380828857 mm for frame 237

Saving results

Total time: 123.97218203544617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005200
Iteration 2/25 | Loss: 0.01005199
Iteration 3/25 | Loss: 0.01005199
Iteration 4/25 | Loss: 0.01005199
Iteration 5/25 | Loss: 0.01005199
Iteration 6/25 | Loss: 0.01005199
Iteration 7/25 | Loss: 0.01005198
Iteration 8/25 | Loss: 0.01005198
Iteration 9/25 | Loss: 0.01005198
Iteration 10/25 | Loss: 0.01005198
Iteration 11/25 | Loss: 0.01005198
Iteration 12/25 | Loss: 0.01005198
Iteration 13/25 | Loss: 0.01005198
Iteration 14/25 | Loss: 0.01005197
Iteration 15/25 | Loss: 0.01005197
Iteration 16/25 | Loss: 0.01005197
Iteration 17/25 | Loss: 0.01005197
Iteration 18/25 | Loss: 0.01005197
Iteration 19/25 | Loss: 0.01005196
Iteration 20/25 | Loss: 0.01005196
Iteration 21/25 | Loss: 0.01005196
Iteration 22/25 | Loss: 0.01005196
Iteration 23/25 | Loss: 0.01005196
Iteration 24/25 | Loss: 0.01005196
Iteration 25/25 | Loss: 0.01005196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03272080
Iteration 2/25 | Loss: 0.17676218
Iteration 3/25 | Loss: 0.17518868
Iteration 4/25 | Loss: 0.17500238
Iteration 5/25 | Loss: 0.17500234
Iteration 6/25 | Loss: 0.17500232
Iteration 7/25 | Loss: 0.17500231
Iteration 8/25 | Loss: 0.17500231
Iteration 9/25 | Loss: 0.17500231
Iteration 10/25 | Loss: 0.17500232
Iteration 11/25 | Loss: 0.17500231
Iteration 12/25 | Loss: 0.17500231
Iteration 13/25 | Loss: 0.17500231
Iteration 14/25 | Loss: 0.17500231
Iteration 15/25 | Loss: 0.17500231
Iteration 16/25 | Loss: 0.17500231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.1750023066997528, 0.1750023066997528, 0.1750023066997528, 0.1750023066997528, 0.1750023066997528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1750023066997528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17500231
Iteration 2/1000 | Loss: 0.00962315
Iteration 3/1000 | Loss: 0.00598199
Iteration 4/1000 | Loss: 0.00140198
Iteration 5/1000 | Loss: 0.00168884
Iteration 6/1000 | Loss: 0.00079547
Iteration 7/1000 | Loss: 0.00028025
Iteration 8/1000 | Loss: 0.00015953
Iteration 9/1000 | Loss: 0.00014151
Iteration 10/1000 | Loss: 0.00011631
Iteration 11/1000 | Loss: 0.00009917
Iteration 12/1000 | Loss: 0.00008516
Iteration 13/1000 | Loss: 0.00007322
Iteration 14/1000 | Loss: 0.00006402
Iteration 15/1000 | Loss: 0.00005387
Iteration 16/1000 | Loss: 0.00004712
Iteration 17/1000 | Loss: 0.00003949
Iteration 18/1000 | Loss: 0.00003505
Iteration 19/1000 | Loss: 0.00003272
Iteration 20/1000 | Loss: 0.00003092
Iteration 21/1000 | Loss: 0.00002943
Iteration 22/1000 | Loss: 0.00002846
Iteration 23/1000 | Loss: 0.00002771
Iteration 24/1000 | Loss: 0.00002698
Iteration 25/1000 | Loss: 0.00002637
Iteration 26/1000 | Loss: 0.00002595
Iteration 27/1000 | Loss: 0.00002560
Iteration 28/1000 | Loss: 0.00002526
Iteration 29/1000 | Loss: 0.00002499
Iteration 30/1000 | Loss: 0.00002483
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002480
Iteration 33/1000 | Loss: 0.00002467
Iteration 34/1000 | Loss: 0.00002462
Iteration 35/1000 | Loss: 0.00002460
Iteration 36/1000 | Loss: 0.00002459
Iteration 37/1000 | Loss: 0.00002459
Iteration 38/1000 | Loss: 0.00002459
Iteration 39/1000 | Loss: 0.00002458
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002455
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002451
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002450
Iteration 48/1000 | Loss: 0.00002450
Iteration 49/1000 | Loss: 0.00002449
Iteration 50/1000 | Loss: 0.00002448
Iteration 51/1000 | Loss: 0.00002448
Iteration 52/1000 | Loss: 0.00002448
Iteration 53/1000 | Loss: 0.00002448
Iteration 54/1000 | Loss: 0.00002447
Iteration 55/1000 | Loss: 0.00002446
Iteration 56/1000 | Loss: 0.00002446
Iteration 57/1000 | Loss: 0.00002445
Iteration 58/1000 | Loss: 0.00002445
Iteration 59/1000 | Loss: 0.00002445
Iteration 60/1000 | Loss: 0.00002444
Iteration 61/1000 | Loss: 0.00002444
Iteration 62/1000 | Loss: 0.00002444
Iteration 63/1000 | Loss: 0.00002444
Iteration 64/1000 | Loss: 0.00002444
Iteration 65/1000 | Loss: 0.00002444
Iteration 66/1000 | Loss: 0.00002444
Iteration 67/1000 | Loss: 0.00002444
Iteration 68/1000 | Loss: 0.00002444
Iteration 69/1000 | Loss: 0.00002444
Iteration 70/1000 | Loss: 0.00002444
Iteration 71/1000 | Loss: 0.00002444
Iteration 72/1000 | Loss: 0.00002444
Iteration 73/1000 | Loss: 0.00002444
Iteration 74/1000 | Loss: 0.00002444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [2.444088204356376e-05, 2.444088204356376e-05, 2.444088204356376e-05, 2.444088204356376e-05, 2.444088204356376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.444088204356376e-05

Optimization complete. Final v2v error: 4.206299304962158 mm

Highest mean error: 4.556499481201172 mm for frame 139

Lowest mean error: 3.8000733852386475 mm for frame 35

Saving results

Total time: 62.6473708152771
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852761
Iteration 2/25 | Loss: 0.00169494
Iteration 3/25 | Loss: 0.00115216
Iteration 4/25 | Loss: 0.00102631
Iteration 5/25 | Loss: 0.00103839
Iteration 6/25 | Loss: 0.00103291
Iteration 7/25 | Loss: 0.00098037
Iteration 8/25 | Loss: 0.00094500
Iteration 9/25 | Loss: 0.00093151
Iteration 10/25 | Loss: 0.00092486
Iteration 11/25 | Loss: 0.00092105
Iteration 12/25 | Loss: 0.00091940
Iteration 13/25 | Loss: 0.00091869
Iteration 14/25 | Loss: 0.00091845
Iteration 15/25 | Loss: 0.00091829
Iteration 16/25 | Loss: 0.00091826
Iteration 17/25 | Loss: 0.00091826
Iteration 18/25 | Loss: 0.00091826
Iteration 19/25 | Loss: 0.00091826
Iteration 20/25 | Loss: 0.00091826
Iteration 21/25 | Loss: 0.00091825
Iteration 22/25 | Loss: 0.00091825
Iteration 23/25 | Loss: 0.00091825
Iteration 24/25 | Loss: 0.00091825
Iteration 25/25 | Loss: 0.00091825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55577946
Iteration 2/25 | Loss: 0.00107027
Iteration 3/25 | Loss: 0.00107027
Iteration 4/25 | Loss: 0.00107027
Iteration 5/25 | Loss: 0.00107027
Iteration 6/25 | Loss: 0.00107027
Iteration 7/25 | Loss: 0.00107027
Iteration 8/25 | Loss: 0.00107027
Iteration 9/25 | Loss: 0.00107027
Iteration 10/25 | Loss: 0.00107027
Iteration 11/25 | Loss: 0.00107027
Iteration 12/25 | Loss: 0.00107027
Iteration 13/25 | Loss: 0.00107027
Iteration 14/25 | Loss: 0.00107027
Iteration 15/25 | Loss: 0.00107027
Iteration 16/25 | Loss: 0.00107027
Iteration 17/25 | Loss: 0.00107027
Iteration 18/25 | Loss: 0.00107027
Iteration 19/25 | Loss: 0.00107027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00107026647310704, 0.00107026647310704, 0.00107026647310704, 0.00107026647310704, 0.00107026647310704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00107026647310704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107027
Iteration 2/1000 | Loss: 0.00264925
Iteration 3/1000 | Loss: 0.00287304
Iteration 4/1000 | Loss: 0.00222076
Iteration 5/1000 | Loss: 0.00325693
Iteration 6/1000 | Loss: 0.00107714
Iteration 7/1000 | Loss: 0.00072479
Iteration 8/1000 | Loss: 0.00019467
Iteration 9/1000 | Loss: 0.00012631
Iteration 10/1000 | Loss: 0.00009517
Iteration 11/1000 | Loss: 0.00063982
Iteration 12/1000 | Loss: 0.00194171
Iteration 13/1000 | Loss: 0.00187727
Iteration 14/1000 | Loss: 0.00188675
Iteration 15/1000 | Loss: 0.00174063
Iteration 16/1000 | Loss: 0.00072546
Iteration 17/1000 | Loss: 0.00082107
Iteration 18/1000 | Loss: 0.00026206
Iteration 19/1000 | Loss: 0.00030430
Iteration 20/1000 | Loss: 0.00015387
Iteration 21/1000 | Loss: 0.00013590
Iteration 22/1000 | Loss: 0.00026770
Iteration 23/1000 | Loss: 0.00036313
Iteration 24/1000 | Loss: 0.00013707
Iteration 25/1000 | Loss: 0.00033605
Iteration 26/1000 | Loss: 0.00041803
Iteration 27/1000 | Loss: 0.00034028
Iteration 28/1000 | Loss: 0.00055896
Iteration 29/1000 | Loss: 0.00085453
Iteration 30/1000 | Loss: 0.00080295
Iteration 31/1000 | Loss: 0.00076086
Iteration 32/1000 | Loss: 0.00028121
Iteration 33/1000 | Loss: 0.00019074
Iteration 34/1000 | Loss: 0.00017665
Iteration 35/1000 | Loss: 0.00043586
Iteration 36/1000 | Loss: 0.00040545
Iteration 37/1000 | Loss: 0.00030649
Iteration 38/1000 | Loss: 0.00020297
Iteration 39/1000 | Loss: 0.00031856
Iteration 40/1000 | Loss: 0.00029529
Iteration 41/1000 | Loss: 0.00031480
Iteration 42/1000 | Loss: 0.00030060
Iteration 43/1000 | Loss: 0.00020361
Iteration 44/1000 | Loss: 0.00025818
Iteration 45/1000 | Loss: 0.00024162
Iteration 46/1000 | Loss: 0.00027094
Iteration 47/1000 | Loss: 0.00020778
Iteration 48/1000 | Loss: 0.00012285
Iteration 49/1000 | Loss: 0.00011142
Iteration 50/1000 | Loss: 0.00014979
Iteration 51/1000 | Loss: 0.00014493
Iteration 52/1000 | Loss: 0.00010064
Iteration 53/1000 | Loss: 0.00032332
Iteration 54/1000 | Loss: 0.00032836
Iteration 55/1000 | Loss: 0.00039159
Iteration 56/1000 | Loss: 0.00028063
Iteration 57/1000 | Loss: 0.00042447
Iteration 58/1000 | Loss: 0.00026318
Iteration 59/1000 | Loss: 0.00036585
Iteration 60/1000 | Loss: 0.00022858
Iteration 61/1000 | Loss: 0.00033416
Iteration 62/1000 | Loss: 0.00007631
Iteration 63/1000 | Loss: 0.00006141
Iteration 64/1000 | Loss: 0.00012912
Iteration 65/1000 | Loss: 0.00005905
Iteration 66/1000 | Loss: 0.00007827
Iteration 67/1000 | Loss: 0.00007712
Iteration 68/1000 | Loss: 0.00005107
Iteration 69/1000 | Loss: 0.00009834
Iteration 70/1000 | Loss: 0.00007075
Iteration 71/1000 | Loss: 0.00007100
Iteration 72/1000 | Loss: 0.00008002
Iteration 73/1000 | Loss: 0.00009048
Iteration 74/1000 | Loss: 0.00007548
Iteration 75/1000 | Loss: 0.00054298
Iteration 76/1000 | Loss: 0.00043797
Iteration 77/1000 | Loss: 0.00011517
Iteration 78/1000 | Loss: 0.00007758
Iteration 79/1000 | Loss: 0.00005898
Iteration 80/1000 | Loss: 0.00016916
Iteration 81/1000 | Loss: 0.00015909
Iteration 82/1000 | Loss: 0.00014410
Iteration 83/1000 | Loss: 0.00013384
Iteration 84/1000 | Loss: 0.00009761
Iteration 85/1000 | Loss: 0.00029168
Iteration 86/1000 | Loss: 0.00018541
Iteration 87/1000 | Loss: 0.00016004
Iteration 88/1000 | Loss: 0.00015584
Iteration 89/1000 | Loss: 0.00038774
Iteration 90/1000 | Loss: 0.00017310
Iteration 91/1000 | Loss: 0.00014118
Iteration 92/1000 | Loss: 0.00026887
Iteration 93/1000 | Loss: 0.00013891
Iteration 94/1000 | Loss: 0.00013255
Iteration 95/1000 | Loss: 0.00011384
Iteration 96/1000 | Loss: 0.00015855
Iteration 97/1000 | Loss: 0.00004628
Iteration 98/1000 | Loss: 0.00009343
Iteration 99/1000 | Loss: 0.00010773
Iteration 100/1000 | Loss: 0.00008382
Iteration 101/1000 | Loss: 0.00010967
Iteration 102/1000 | Loss: 0.00007905
Iteration 103/1000 | Loss: 0.00032455
Iteration 104/1000 | Loss: 0.00026455
Iteration 105/1000 | Loss: 0.00007241
Iteration 106/1000 | Loss: 0.00005365
Iteration 107/1000 | Loss: 0.00004799
Iteration 108/1000 | Loss: 0.00040203
Iteration 109/1000 | Loss: 0.00041356
Iteration 110/1000 | Loss: 0.00017864
Iteration 111/1000 | Loss: 0.00004516
Iteration 112/1000 | Loss: 0.00004103
Iteration 113/1000 | Loss: 0.00003890
Iteration 114/1000 | Loss: 0.00003785
Iteration 115/1000 | Loss: 0.00022771
Iteration 116/1000 | Loss: 0.00039330
Iteration 117/1000 | Loss: 0.00026661
Iteration 118/1000 | Loss: 0.00003623
Iteration 119/1000 | Loss: 0.00003343
Iteration 120/1000 | Loss: 0.00003266
Iteration 121/1000 | Loss: 0.00030158
Iteration 122/1000 | Loss: 0.00004332
Iteration 123/1000 | Loss: 0.00003524
Iteration 124/1000 | Loss: 0.00003232
Iteration 125/1000 | Loss: 0.00003045
Iteration 126/1000 | Loss: 0.00002987
Iteration 127/1000 | Loss: 0.00002948
Iteration 128/1000 | Loss: 0.00002902
Iteration 129/1000 | Loss: 0.00002861
Iteration 130/1000 | Loss: 0.00002831
Iteration 131/1000 | Loss: 0.00047979
Iteration 132/1000 | Loss: 0.00004021
Iteration 133/1000 | Loss: 0.00003469
Iteration 134/1000 | Loss: 0.00003217
Iteration 135/1000 | Loss: 0.00003111
Iteration 136/1000 | Loss: 0.00003059
Iteration 137/1000 | Loss: 0.00041257
Iteration 138/1000 | Loss: 0.00004515
Iteration 139/1000 | Loss: 0.00003781
Iteration 140/1000 | Loss: 0.00003586
Iteration 141/1000 | Loss: 0.00003498
Iteration 142/1000 | Loss: 0.00003416
Iteration 143/1000 | Loss: 0.00025294
Iteration 144/1000 | Loss: 0.00004501
Iteration 145/1000 | Loss: 0.00003370
Iteration 146/1000 | Loss: 0.00003104
Iteration 147/1000 | Loss: 0.00002966
Iteration 148/1000 | Loss: 0.00002889
Iteration 149/1000 | Loss: 0.00002809
Iteration 150/1000 | Loss: 0.00002767
Iteration 151/1000 | Loss: 0.00002737
Iteration 152/1000 | Loss: 0.00002733
Iteration 153/1000 | Loss: 0.00002718
Iteration 154/1000 | Loss: 0.00002716
Iteration 155/1000 | Loss: 0.00002716
Iteration 156/1000 | Loss: 0.00002713
Iteration 157/1000 | Loss: 0.00002712
Iteration 158/1000 | Loss: 0.00002711
Iteration 159/1000 | Loss: 0.00002703
Iteration 160/1000 | Loss: 0.00002701
Iteration 161/1000 | Loss: 0.00002700
Iteration 162/1000 | Loss: 0.00002700
Iteration 163/1000 | Loss: 0.00002700
Iteration 164/1000 | Loss: 0.00002700
Iteration 165/1000 | Loss: 0.00002700
Iteration 166/1000 | Loss: 0.00002700
Iteration 167/1000 | Loss: 0.00002699
Iteration 168/1000 | Loss: 0.00002699
Iteration 169/1000 | Loss: 0.00002699
Iteration 170/1000 | Loss: 0.00002699
Iteration 171/1000 | Loss: 0.00002699
Iteration 172/1000 | Loss: 0.00002699
Iteration 173/1000 | Loss: 0.00002699
Iteration 174/1000 | Loss: 0.00002699
Iteration 175/1000 | Loss: 0.00002699
Iteration 176/1000 | Loss: 0.00002699
Iteration 177/1000 | Loss: 0.00002699
Iteration 178/1000 | Loss: 0.00002698
Iteration 179/1000 | Loss: 0.00002698
Iteration 180/1000 | Loss: 0.00002698
Iteration 181/1000 | Loss: 0.00002698
Iteration 182/1000 | Loss: 0.00002697
Iteration 183/1000 | Loss: 0.00002697
Iteration 184/1000 | Loss: 0.00002697
Iteration 185/1000 | Loss: 0.00002697
Iteration 186/1000 | Loss: 0.00002697
Iteration 187/1000 | Loss: 0.00002697
Iteration 188/1000 | Loss: 0.00002697
Iteration 189/1000 | Loss: 0.00002697
Iteration 190/1000 | Loss: 0.00002697
Iteration 191/1000 | Loss: 0.00002697
Iteration 192/1000 | Loss: 0.00002697
Iteration 193/1000 | Loss: 0.00002697
Iteration 194/1000 | Loss: 0.00002697
Iteration 195/1000 | Loss: 0.00002697
Iteration 196/1000 | Loss: 0.00002697
Iteration 197/1000 | Loss: 0.00002697
Iteration 198/1000 | Loss: 0.00002697
Iteration 199/1000 | Loss: 0.00002697
Iteration 200/1000 | Loss: 0.00002697
Iteration 201/1000 | Loss: 0.00002697
Iteration 202/1000 | Loss: 0.00002697
Iteration 203/1000 | Loss: 0.00002697
Iteration 204/1000 | Loss: 0.00002697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.6966457880917005e-05, 2.6966457880917005e-05, 2.6966457880917005e-05, 2.6966457880917005e-05, 2.6966457880917005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6966457880917005e-05

Optimization complete. Final v2v error: 4.100024700164795 mm

Highest mean error: 6.04718542098999 mm for frame 149

Lowest mean error: 3.5265164375305176 mm for frame 151

Saving results

Total time: 279.7733471393585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805733
Iteration 2/25 | Loss: 0.00133522
Iteration 3/25 | Loss: 0.00105189
Iteration 4/25 | Loss: 0.00096502
Iteration 5/25 | Loss: 0.00092702
Iteration 6/25 | Loss: 0.00090678
Iteration 7/25 | Loss: 0.00089559
Iteration 8/25 | Loss: 0.00089693
Iteration 9/25 | Loss: 0.00088862
Iteration 10/25 | Loss: 0.00087926
Iteration 11/25 | Loss: 0.00087697
Iteration 12/25 | Loss: 0.00087653
Iteration 13/25 | Loss: 0.00087589
Iteration 14/25 | Loss: 0.00087541
Iteration 15/25 | Loss: 0.00087508
Iteration 16/25 | Loss: 0.00087400
Iteration 17/25 | Loss: 0.00087706
Iteration 18/25 | Loss: 0.00087751
Iteration 19/25 | Loss: 0.00087640
Iteration 20/25 | Loss: 0.00087474
Iteration 21/25 | Loss: 0.00087685
Iteration 22/25 | Loss: 0.00087553
Iteration 23/25 | Loss: 0.00087413
Iteration 24/25 | Loss: 0.00087464
Iteration 25/25 | Loss: 0.00087609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47173142
Iteration 2/25 | Loss: 0.00152255
Iteration 3/25 | Loss: 0.00152248
Iteration 4/25 | Loss: 0.00152248
Iteration 5/25 | Loss: 0.00152247
Iteration 6/25 | Loss: 0.00152247
Iteration 7/25 | Loss: 0.00152247
Iteration 8/25 | Loss: 0.00152247
Iteration 9/25 | Loss: 0.00152247
Iteration 10/25 | Loss: 0.00152247
Iteration 11/25 | Loss: 0.00152247
Iteration 12/25 | Loss: 0.00152247
Iteration 13/25 | Loss: 0.00152247
Iteration 14/25 | Loss: 0.00152247
Iteration 15/25 | Loss: 0.00152247
Iteration 16/25 | Loss: 0.00152247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015224734088405967, 0.0015224734088405967, 0.0015224734088405967, 0.0015224734088405967, 0.0015224734088405967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015224734088405967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152247
Iteration 2/1000 | Loss: 0.00061367
Iteration 3/1000 | Loss: 0.00014679
Iteration 4/1000 | Loss: 0.00067138
Iteration 5/1000 | Loss: 0.00114879
Iteration 6/1000 | Loss: 0.00034155
Iteration 7/1000 | Loss: 0.00051570
Iteration 8/1000 | Loss: 0.00064697
Iteration 9/1000 | Loss: 0.00085750
Iteration 10/1000 | Loss: 0.00057225
Iteration 11/1000 | Loss: 0.00027444
Iteration 12/1000 | Loss: 0.00050472
Iteration 13/1000 | Loss: 0.00006831
Iteration 14/1000 | Loss: 0.00047998
Iteration 15/1000 | Loss: 0.00040077
Iteration 16/1000 | Loss: 0.00025218
Iteration 17/1000 | Loss: 0.00006102
Iteration 18/1000 | Loss: 0.00017140
Iteration 19/1000 | Loss: 0.00005407
Iteration 20/1000 | Loss: 0.00047921
Iteration 21/1000 | Loss: 0.00080649
Iteration 22/1000 | Loss: 0.00044119
Iteration 23/1000 | Loss: 0.00125883
Iteration 24/1000 | Loss: 0.00052879
Iteration 25/1000 | Loss: 0.00068371
Iteration 26/1000 | Loss: 0.00043496
Iteration 27/1000 | Loss: 0.00040690
Iteration 28/1000 | Loss: 0.00084780
Iteration 29/1000 | Loss: 0.00024128
Iteration 30/1000 | Loss: 0.00029413
Iteration 31/1000 | Loss: 0.00022719
Iteration 32/1000 | Loss: 0.00042902
Iteration 33/1000 | Loss: 0.00044548
Iteration 34/1000 | Loss: 0.00037817
Iteration 35/1000 | Loss: 0.00059594
Iteration 36/1000 | Loss: 0.00031471
Iteration 37/1000 | Loss: 0.00058331
Iteration 38/1000 | Loss: 0.00006490
Iteration 39/1000 | Loss: 0.00053209
Iteration 40/1000 | Loss: 0.00009112
Iteration 41/1000 | Loss: 0.00005880
Iteration 42/1000 | Loss: 0.00023873
Iteration 43/1000 | Loss: 0.00040891
Iteration 44/1000 | Loss: 0.00026545
Iteration 45/1000 | Loss: 0.00034931
Iteration 46/1000 | Loss: 0.00038481
Iteration 47/1000 | Loss: 0.00043670
Iteration 48/1000 | Loss: 0.00039843
Iteration 49/1000 | Loss: 0.00036232
Iteration 50/1000 | Loss: 0.00053670
Iteration 51/1000 | Loss: 0.00035415
Iteration 52/1000 | Loss: 0.00034980
Iteration 53/1000 | Loss: 0.00037352
Iteration 54/1000 | Loss: 0.00032093
Iteration 55/1000 | Loss: 0.00057447
Iteration 56/1000 | Loss: 0.00035545
Iteration 57/1000 | Loss: 0.00008153
Iteration 58/1000 | Loss: 0.00005594
Iteration 59/1000 | Loss: 0.00004992
Iteration 60/1000 | Loss: 0.00004407
Iteration 61/1000 | Loss: 0.00039314
Iteration 62/1000 | Loss: 0.00004415
Iteration 63/1000 | Loss: 0.00003756
Iteration 64/1000 | Loss: 0.00003559
Iteration 65/1000 | Loss: 0.00003463
Iteration 66/1000 | Loss: 0.00003386
Iteration 67/1000 | Loss: 0.00039755
Iteration 68/1000 | Loss: 0.00004548
Iteration 69/1000 | Loss: 0.00003687
Iteration 70/1000 | Loss: 0.00003395
Iteration 71/1000 | Loss: 0.00003265
Iteration 72/1000 | Loss: 0.00003196
Iteration 73/1000 | Loss: 0.00003125
Iteration 74/1000 | Loss: 0.00003065
Iteration 75/1000 | Loss: 0.00003024
Iteration 76/1000 | Loss: 0.00002982
Iteration 77/1000 | Loss: 0.00002952
Iteration 78/1000 | Loss: 0.00002934
Iteration 79/1000 | Loss: 0.00002934
Iteration 80/1000 | Loss: 0.00002928
Iteration 81/1000 | Loss: 0.00002926
Iteration 82/1000 | Loss: 0.00002926
Iteration 83/1000 | Loss: 0.00002926
Iteration 84/1000 | Loss: 0.00002925
Iteration 85/1000 | Loss: 0.00002925
Iteration 86/1000 | Loss: 0.00002925
Iteration 87/1000 | Loss: 0.00002925
Iteration 88/1000 | Loss: 0.00002925
Iteration 89/1000 | Loss: 0.00002925
Iteration 90/1000 | Loss: 0.00002925
Iteration 91/1000 | Loss: 0.00002925
Iteration 92/1000 | Loss: 0.00002925
Iteration 93/1000 | Loss: 0.00002925
Iteration 94/1000 | Loss: 0.00002924
Iteration 95/1000 | Loss: 0.00002924
Iteration 96/1000 | Loss: 0.00002922
Iteration 97/1000 | Loss: 0.00002920
Iteration 98/1000 | Loss: 0.00040122
Iteration 99/1000 | Loss: 0.00004683
Iteration 100/1000 | Loss: 0.00003286
Iteration 101/1000 | Loss: 0.00003007
Iteration 102/1000 | Loss: 0.00002891
Iteration 103/1000 | Loss: 0.00002800
Iteration 104/1000 | Loss: 0.00002746
Iteration 105/1000 | Loss: 0.00002717
Iteration 106/1000 | Loss: 0.00002696
Iteration 107/1000 | Loss: 0.00002680
Iteration 108/1000 | Loss: 0.00002671
Iteration 109/1000 | Loss: 0.00002668
Iteration 110/1000 | Loss: 0.00002667
Iteration 111/1000 | Loss: 0.00002666
Iteration 112/1000 | Loss: 0.00002665
Iteration 113/1000 | Loss: 0.00002665
Iteration 114/1000 | Loss: 0.00002664
Iteration 115/1000 | Loss: 0.00002663
Iteration 116/1000 | Loss: 0.00002663
Iteration 117/1000 | Loss: 0.00002662
Iteration 118/1000 | Loss: 0.00002662
Iteration 119/1000 | Loss: 0.00002661
Iteration 120/1000 | Loss: 0.00002658
Iteration 121/1000 | Loss: 0.00002658
Iteration 122/1000 | Loss: 0.00002658
Iteration 123/1000 | Loss: 0.00002657
Iteration 124/1000 | Loss: 0.00002657
Iteration 125/1000 | Loss: 0.00002656
Iteration 126/1000 | Loss: 0.00002654
Iteration 127/1000 | Loss: 0.00002654
Iteration 128/1000 | Loss: 0.00002654
Iteration 129/1000 | Loss: 0.00002653
Iteration 130/1000 | Loss: 0.00002653
Iteration 131/1000 | Loss: 0.00002652
Iteration 132/1000 | Loss: 0.00002652
Iteration 133/1000 | Loss: 0.00002651
Iteration 134/1000 | Loss: 0.00002650
Iteration 135/1000 | Loss: 0.00002649
Iteration 136/1000 | Loss: 0.00002649
Iteration 137/1000 | Loss: 0.00002649
Iteration 138/1000 | Loss: 0.00002649
Iteration 139/1000 | Loss: 0.00002649
Iteration 140/1000 | Loss: 0.00002649
Iteration 141/1000 | Loss: 0.00002648
Iteration 142/1000 | Loss: 0.00002648
Iteration 143/1000 | Loss: 0.00002648
Iteration 144/1000 | Loss: 0.00002648
Iteration 145/1000 | Loss: 0.00002648
Iteration 146/1000 | Loss: 0.00002648
Iteration 147/1000 | Loss: 0.00002648
Iteration 148/1000 | Loss: 0.00002648
Iteration 149/1000 | Loss: 0.00002647
Iteration 150/1000 | Loss: 0.00002647
Iteration 151/1000 | Loss: 0.00002647
Iteration 152/1000 | Loss: 0.00002647
Iteration 153/1000 | Loss: 0.00002646
Iteration 154/1000 | Loss: 0.00002646
Iteration 155/1000 | Loss: 0.00002646
Iteration 156/1000 | Loss: 0.00002645
Iteration 157/1000 | Loss: 0.00002645
Iteration 158/1000 | Loss: 0.00002645
Iteration 159/1000 | Loss: 0.00002644
Iteration 160/1000 | Loss: 0.00002644
Iteration 161/1000 | Loss: 0.00002644
Iteration 162/1000 | Loss: 0.00002644
Iteration 163/1000 | Loss: 0.00002644
Iteration 164/1000 | Loss: 0.00002644
Iteration 165/1000 | Loss: 0.00002644
Iteration 166/1000 | Loss: 0.00002644
Iteration 167/1000 | Loss: 0.00002643
Iteration 168/1000 | Loss: 0.00002643
Iteration 169/1000 | Loss: 0.00002643
Iteration 170/1000 | Loss: 0.00002643
Iteration 171/1000 | Loss: 0.00002643
Iteration 172/1000 | Loss: 0.00002643
Iteration 173/1000 | Loss: 0.00002643
Iteration 174/1000 | Loss: 0.00002643
Iteration 175/1000 | Loss: 0.00002642
Iteration 176/1000 | Loss: 0.00002642
Iteration 177/1000 | Loss: 0.00002642
Iteration 178/1000 | Loss: 0.00002642
Iteration 179/1000 | Loss: 0.00002641
Iteration 180/1000 | Loss: 0.00002641
Iteration 181/1000 | Loss: 0.00002641
Iteration 182/1000 | Loss: 0.00002640
Iteration 183/1000 | Loss: 0.00002640
Iteration 184/1000 | Loss: 0.00002640
Iteration 185/1000 | Loss: 0.00002640
Iteration 186/1000 | Loss: 0.00002640
Iteration 187/1000 | Loss: 0.00002640
Iteration 188/1000 | Loss: 0.00002640
Iteration 189/1000 | Loss: 0.00002640
Iteration 190/1000 | Loss: 0.00002640
Iteration 191/1000 | Loss: 0.00002640
Iteration 192/1000 | Loss: 0.00002640
Iteration 193/1000 | Loss: 0.00002640
Iteration 194/1000 | Loss: 0.00002640
Iteration 195/1000 | Loss: 0.00002640
Iteration 196/1000 | Loss: 0.00002640
Iteration 197/1000 | Loss: 0.00002640
Iteration 198/1000 | Loss: 0.00002640
Iteration 199/1000 | Loss: 0.00002640
Iteration 200/1000 | Loss: 0.00002640
Iteration 201/1000 | Loss: 0.00002640
Iteration 202/1000 | Loss: 0.00002640
Iteration 203/1000 | Loss: 0.00002640
Iteration 204/1000 | Loss: 0.00002640
Iteration 205/1000 | Loss: 0.00002640
Iteration 206/1000 | Loss: 0.00002640
Iteration 207/1000 | Loss: 0.00002640
Iteration 208/1000 | Loss: 0.00002640
Iteration 209/1000 | Loss: 0.00002640
Iteration 210/1000 | Loss: 0.00002640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [2.6398192858323455e-05, 2.6398192858323455e-05, 2.6398192858323455e-05, 2.6398192858323455e-05, 2.6398192858323455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6398192858323455e-05

Optimization complete. Final v2v error: 4.265903472900391 mm

Highest mean error: 5.564478397369385 mm for frame 183

Lowest mean error: 3.9634029865264893 mm for frame 19

Saving results

Total time: 205.99877381324768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378472
Iteration 2/25 | Loss: 0.00102758
Iteration 3/25 | Loss: 0.00080578
Iteration 4/25 | Loss: 0.00075055
Iteration 5/25 | Loss: 0.00073994
Iteration 6/25 | Loss: 0.00073833
Iteration 7/25 | Loss: 0.00073824
Iteration 8/25 | Loss: 0.00073824
Iteration 9/25 | Loss: 0.00073824
Iteration 10/25 | Loss: 0.00073824
Iteration 11/25 | Loss: 0.00073824
Iteration 12/25 | Loss: 0.00073824
Iteration 13/25 | Loss: 0.00073824
Iteration 14/25 | Loss: 0.00073824
Iteration 15/25 | Loss: 0.00073824
Iteration 16/25 | Loss: 0.00073824
Iteration 17/25 | Loss: 0.00073824
Iteration 18/25 | Loss: 0.00073824
Iteration 19/25 | Loss: 0.00073824
Iteration 20/25 | Loss: 0.00073824
Iteration 21/25 | Loss: 0.00073824
Iteration 22/25 | Loss: 0.00073824
Iteration 23/25 | Loss: 0.00073824
Iteration 24/25 | Loss: 0.00073824
Iteration 25/25 | Loss: 0.00073824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53663373
Iteration 2/25 | Loss: 0.00090854
Iteration 3/25 | Loss: 0.00090853
Iteration 4/25 | Loss: 0.00090853
Iteration 5/25 | Loss: 0.00090853
Iteration 6/25 | Loss: 0.00090853
Iteration 7/25 | Loss: 0.00090853
Iteration 8/25 | Loss: 0.00090853
Iteration 9/25 | Loss: 0.00090853
Iteration 10/25 | Loss: 0.00090853
Iteration 11/25 | Loss: 0.00090853
Iteration 12/25 | Loss: 0.00090853
Iteration 13/25 | Loss: 0.00090853
Iteration 14/25 | Loss: 0.00090853
Iteration 15/25 | Loss: 0.00090853
Iteration 16/25 | Loss: 0.00090853
Iteration 17/25 | Loss: 0.00090853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009085287456400692, 0.0009085287456400692, 0.0009085287456400692, 0.0009085287456400692, 0.0009085287456400692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009085287456400692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090853
Iteration 2/1000 | Loss: 0.00003262
Iteration 3/1000 | Loss: 0.00001973
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001558
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001398
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001385
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001378
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001356
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001352
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001345
Iteration 46/1000 | Loss: 0.00001344
Iteration 47/1000 | Loss: 0.00001344
Iteration 48/1000 | Loss: 0.00001344
Iteration 49/1000 | Loss: 0.00001344
Iteration 50/1000 | Loss: 0.00001344
Iteration 51/1000 | Loss: 0.00001344
Iteration 52/1000 | Loss: 0.00001344
Iteration 53/1000 | Loss: 0.00001344
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001343
Iteration 62/1000 | Loss: 0.00001343
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001342
Iteration 68/1000 | Loss: 0.00001342
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001341
Iteration 78/1000 | Loss: 0.00001341
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001340
Iteration 82/1000 | Loss: 0.00001340
Iteration 83/1000 | Loss: 0.00001340
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001340
Iteration 86/1000 | Loss: 0.00001340
Iteration 87/1000 | Loss: 0.00001340
Iteration 88/1000 | Loss: 0.00001340
Iteration 89/1000 | Loss: 0.00001340
Iteration 90/1000 | Loss: 0.00001340
Iteration 91/1000 | Loss: 0.00001340
Iteration 92/1000 | Loss: 0.00001340
Iteration 93/1000 | Loss: 0.00001340
Iteration 94/1000 | Loss: 0.00001340
Iteration 95/1000 | Loss: 0.00001339
Iteration 96/1000 | Loss: 0.00001339
Iteration 97/1000 | Loss: 0.00001339
Iteration 98/1000 | Loss: 0.00001339
Iteration 99/1000 | Loss: 0.00001339
Iteration 100/1000 | Loss: 0.00001339
Iteration 101/1000 | Loss: 0.00001339
Iteration 102/1000 | Loss: 0.00001339
Iteration 103/1000 | Loss: 0.00001339
Iteration 104/1000 | Loss: 0.00001339
Iteration 105/1000 | Loss: 0.00001339
Iteration 106/1000 | Loss: 0.00001339
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001339
Iteration 112/1000 | Loss: 0.00001339
Iteration 113/1000 | Loss: 0.00001339
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001338
Iteration 116/1000 | Loss: 0.00001338
Iteration 117/1000 | Loss: 0.00001338
Iteration 118/1000 | Loss: 0.00001338
Iteration 119/1000 | Loss: 0.00001338
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001338
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001338
Iteration 130/1000 | Loss: 0.00001338
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.3382124961935915e-05, 1.3382124961935915e-05, 1.3382124961935915e-05, 1.3382124961935915e-05, 1.3382124961935915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3382124961935915e-05

Optimization complete. Final v2v error: 3.04996395111084 mm

Highest mean error: 3.336237668991089 mm for frame 0

Lowest mean error: 2.929117441177368 mm for frame 153

Saving results

Total time: 35.22219800949097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023045
Iteration 2/25 | Loss: 0.00320586
Iteration 3/25 | Loss: 0.00190166
Iteration 4/25 | Loss: 0.00192800
Iteration 5/25 | Loss: 0.00166565
Iteration 6/25 | Loss: 0.00152836
Iteration 7/25 | Loss: 0.00149509
Iteration 8/25 | Loss: 0.00152018
Iteration 9/25 | Loss: 0.00149480
Iteration 10/25 | Loss: 0.00149193
Iteration 11/25 | Loss: 0.00143787
Iteration 12/25 | Loss: 0.00135598
Iteration 13/25 | Loss: 0.00129958
Iteration 14/25 | Loss: 0.00129217
Iteration 15/25 | Loss: 0.00125687
Iteration 16/25 | Loss: 0.00125813
Iteration 17/25 | Loss: 0.00123567
Iteration 18/25 | Loss: 0.00124300
Iteration 19/25 | Loss: 0.00119917
Iteration 20/25 | Loss: 0.00119962
Iteration 21/25 | Loss: 0.00119638
Iteration 22/25 | Loss: 0.00118969
Iteration 23/25 | Loss: 0.00119658
Iteration 24/25 | Loss: 0.00116677
Iteration 25/25 | Loss: 0.00118074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56081104
Iteration 2/25 | Loss: 0.00734350
Iteration 3/25 | Loss: 0.00440952
Iteration 4/25 | Loss: 0.00440951
Iteration 5/25 | Loss: 0.00440951
Iteration 6/25 | Loss: 0.00440951
Iteration 7/25 | Loss: 0.00440951
Iteration 8/25 | Loss: 0.00440951
Iteration 9/25 | Loss: 0.00440951
Iteration 10/25 | Loss: 0.00440951
Iteration 11/25 | Loss: 0.00440951
Iteration 12/25 | Loss: 0.00440951
Iteration 13/25 | Loss: 0.00440951
Iteration 14/25 | Loss: 0.00440951
Iteration 15/25 | Loss: 0.00440951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004409506916999817, 0.004409506916999817, 0.004409506916999817, 0.004409506916999817, 0.004409506916999817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004409506916999817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00440951
Iteration 2/1000 | Loss: 0.00531357
Iteration 3/1000 | Loss: 0.00395455
Iteration 4/1000 | Loss: 0.00319444
Iteration 5/1000 | Loss: 0.00282923
Iteration 6/1000 | Loss: 0.00223419
Iteration 7/1000 | Loss: 0.00204096
Iteration 8/1000 | Loss: 0.00211484
Iteration 9/1000 | Loss: 0.00136428
Iteration 10/1000 | Loss: 0.00257881
Iteration 11/1000 | Loss: 0.00129545
Iteration 12/1000 | Loss: 0.00136045
Iteration 13/1000 | Loss: 0.00172893
Iteration 14/1000 | Loss: 0.00201144
Iteration 15/1000 | Loss: 0.00059448
Iteration 16/1000 | Loss: 0.00194020
Iteration 17/1000 | Loss: 0.00084441
Iteration 18/1000 | Loss: 0.00161152
Iteration 19/1000 | Loss: 0.00061382
Iteration 20/1000 | Loss: 0.00076645
Iteration 21/1000 | Loss: 0.00058013
Iteration 22/1000 | Loss: 0.00064257
Iteration 23/1000 | Loss: 0.00088241
Iteration 24/1000 | Loss: 0.00055605
Iteration 25/1000 | Loss: 0.00165146
Iteration 26/1000 | Loss: 0.00021741
Iteration 27/1000 | Loss: 0.00016468
Iteration 28/1000 | Loss: 0.00092421
Iteration 29/1000 | Loss: 0.00217978
Iteration 30/1000 | Loss: 0.00045171
Iteration 31/1000 | Loss: 0.00017026
Iteration 32/1000 | Loss: 0.00059812
Iteration 33/1000 | Loss: 0.00071767
Iteration 34/1000 | Loss: 0.00235215
Iteration 35/1000 | Loss: 0.00078589
Iteration 36/1000 | Loss: 0.00148839
Iteration 37/1000 | Loss: 0.00432045
Iteration 38/1000 | Loss: 0.00416817
Iteration 39/1000 | Loss: 0.00428134
Iteration 40/1000 | Loss: 0.00279850
Iteration 41/1000 | Loss: 0.00238651
Iteration 42/1000 | Loss: 0.00116410
Iteration 43/1000 | Loss: 0.00142552
Iteration 44/1000 | Loss: 0.00074445
Iteration 45/1000 | Loss: 0.00041519
Iteration 46/1000 | Loss: 0.00123413
Iteration 47/1000 | Loss: 0.00131151
Iteration 48/1000 | Loss: 0.00028516
Iteration 49/1000 | Loss: 0.00021750
Iteration 50/1000 | Loss: 0.00020676
Iteration 51/1000 | Loss: 0.00021827
Iteration 52/1000 | Loss: 0.00027676
Iteration 53/1000 | Loss: 0.00013112
Iteration 54/1000 | Loss: 0.00085442
Iteration 55/1000 | Loss: 0.00073474
Iteration 56/1000 | Loss: 0.00213033
Iteration 57/1000 | Loss: 0.00017363
Iteration 58/1000 | Loss: 0.00031900
Iteration 59/1000 | Loss: 0.00011414
Iteration 60/1000 | Loss: 0.00023571
Iteration 61/1000 | Loss: 0.00028007
Iteration 62/1000 | Loss: 0.00008846
Iteration 63/1000 | Loss: 0.00031444
Iteration 64/1000 | Loss: 0.00007785
Iteration 65/1000 | Loss: 0.00007829
Iteration 66/1000 | Loss: 0.00007530
Iteration 67/1000 | Loss: 0.00092357
Iteration 68/1000 | Loss: 0.00010996
Iteration 69/1000 | Loss: 0.00005000
Iteration 70/1000 | Loss: 0.00009344
Iteration 71/1000 | Loss: 0.00016009
Iteration 72/1000 | Loss: 0.00049012
Iteration 73/1000 | Loss: 0.00027016
Iteration 74/1000 | Loss: 0.00012091
Iteration 75/1000 | Loss: 0.00026219
Iteration 76/1000 | Loss: 0.00008563
Iteration 77/1000 | Loss: 0.00007286
Iteration 78/1000 | Loss: 0.00014384
Iteration 79/1000 | Loss: 0.00053788
Iteration 80/1000 | Loss: 0.00018474
Iteration 81/1000 | Loss: 0.00021816
Iteration 82/1000 | Loss: 0.00016600
Iteration 83/1000 | Loss: 0.00009786
Iteration 84/1000 | Loss: 0.00011902
Iteration 85/1000 | Loss: 0.00008466
Iteration 86/1000 | Loss: 0.00007876
Iteration 87/1000 | Loss: 0.00024960
Iteration 88/1000 | Loss: 0.00022066
Iteration 89/1000 | Loss: 0.00020405
Iteration 90/1000 | Loss: 0.00042698
Iteration 91/1000 | Loss: 0.00027893
Iteration 92/1000 | Loss: 0.00027587
Iteration 93/1000 | Loss: 0.00018405
Iteration 94/1000 | Loss: 0.00021974
Iteration 95/1000 | Loss: 0.00008236
Iteration 96/1000 | Loss: 0.00010906
Iteration 97/1000 | Loss: 0.00007220
Iteration 98/1000 | Loss: 0.00007516
Iteration 99/1000 | Loss: 0.00007399
Iteration 100/1000 | Loss: 0.00009993
Iteration 101/1000 | Loss: 0.00015501
Iteration 102/1000 | Loss: 0.00007658
Iteration 103/1000 | Loss: 0.00009922
Iteration 104/1000 | Loss: 0.00012710
Iteration 105/1000 | Loss: 0.00008564
Iteration 106/1000 | Loss: 0.00007207
Iteration 107/1000 | Loss: 0.00013877
Iteration 108/1000 | Loss: 0.00006922
Iteration 109/1000 | Loss: 0.00007585
Iteration 110/1000 | Loss: 0.00007428
Iteration 111/1000 | Loss: 0.00014259
Iteration 112/1000 | Loss: 0.00008505
Iteration 113/1000 | Loss: 0.00008525
Iteration 114/1000 | Loss: 0.00008078
Iteration 115/1000 | Loss: 0.00007691
Iteration 116/1000 | Loss: 0.00013141
Iteration 117/1000 | Loss: 0.00009144
Iteration 118/1000 | Loss: 0.00007430
Iteration 119/1000 | Loss: 0.00008656
Iteration 120/1000 | Loss: 0.00006822
Iteration 121/1000 | Loss: 0.00015579
Iteration 122/1000 | Loss: 0.00006637
Iteration 123/1000 | Loss: 0.00010320
Iteration 124/1000 | Loss: 0.00021038
Iteration 125/1000 | Loss: 0.00146166
Iteration 126/1000 | Loss: 0.00002959
Iteration 127/1000 | Loss: 0.00002111
Iteration 128/1000 | Loss: 0.00022628
Iteration 129/1000 | Loss: 0.00030786
Iteration 130/1000 | Loss: 0.00003264
Iteration 131/1000 | Loss: 0.00005232
Iteration 132/1000 | Loss: 0.00003747
Iteration 133/1000 | Loss: 0.00013981
Iteration 134/1000 | Loss: 0.00050747
Iteration 135/1000 | Loss: 0.00004005
Iteration 136/1000 | Loss: 0.00013965
Iteration 137/1000 | Loss: 0.00032894
Iteration 138/1000 | Loss: 0.00022242
Iteration 139/1000 | Loss: 0.00004030
Iteration 140/1000 | Loss: 0.00012324
Iteration 141/1000 | Loss: 0.00002631
Iteration 142/1000 | Loss: 0.00001635
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00004872
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00014536
Iteration 147/1000 | Loss: 0.00003369
Iteration 148/1000 | Loss: 0.00001487
Iteration 149/1000 | Loss: 0.00019673
Iteration 150/1000 | Loss: 0.00018572
Iteration 151/1000 | Loss: 0.00020407
Iteration 152/1000 | Loss: 0.00003297
Iteration 153/1000 | Loss: 0.00004557
Iteration 154/1000 | Loss: 0.00001485
Iteration 155/1000 | Loss: 0.00001429
Iteration 156/1000 | Loss: 0.00001413
Iteration 157/1000 | Loss: 0.00001412
Iteration 158/1000 | Loss: 0.00001402
Iteration 159/1000 | Loss: 0.00001398
Iteration 160/1000 | Loss: 0.00006410
Iteration 161/1000 | Loss: 0.00002496
Iteration 162/1000 | Loss: 0.00005114
Iteration 163/1000 | Loss: 0.00007576
Iteration 164/1000 | Loss: 0.00005000
Iteration 165/1000 | Loss: 0.00019876
Iteration 166/1000 | Loss: 0.00003065
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001379
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001375
Iteration 179/1000 | Loss: 0.00001375
Iteration 180/1000 | Loss: 0.00001375
Iteration 181/1000 | Loss: 0.00001375
Iteration 182/1000 | Loss: 0.00001375
Iteration 183/1000 | Loss: 0.00001375
Iteration 184/1000 | Loss: 0.00001374
Iteration 185/1000 | Loss: 0.00001374
Iteration 186/1000 | Loss: 0.00001374
Iteration 187/1000 | Loss: 0.00001374
Iteration 188/1000 | Loss: 0.00001373
Iteration 189/1000 | Loss: 0.00001372
Iteration 190/1000 | Loss: 0.00005355
Iteration 191/1000 | Loss: 0.00005355
Iteration 192/1000 | Loss: 0.00008809
Iteration 193/1000 | Loss: 0.00002095
Iteration 194/1000 | Loss: 0.00001376
Iteration 195/1000 | Loss: 0.00001369
Iteration 196/1000 | Loss: 0.00001368
Iteration 197/1000 | Loss: 0.00001368
Iteration 198/1000 | Loss: 0.00001368
Iteration 199/1000 | Loss: 0.00001368
Iteration 200/1000 | Loss: 0.00001368
Iteration 201/1000 | Loss: 0.00001368
Iteration 202/1000 | Loss: 0.00001368
Iteration 203/1000 | Loss: 0.00001368
Iteration 204/1000 | Loss: 0.00001367
Iteration 205/1000 | Loss: 0.00001367
Iteration 206/1000 | Loss: 0.00001367
Iteration 207/1000 | Loss: 0.00001367
Iteration 208/1000 | Loss: 0.00001366
Iteration 209/1000 | Loss: 0.00001366
Iteration 210/1000 | Loss: 0.00001366
Iteration 211/1000 | Loss: 0.00001366
Iteration 212/1000 | Loss: 0.00001366
Iteration 213/1000 | Loss: 0.00001366
Iteration 214/1000 | Loss: 0.00001366
Iteration 215/1000 | Loss: 0.00001366
Iteration 216/1000 | Loss: 0.00001366
Iteration 217/1000 | Loss: 0.00001366
Iteration 218/1000 | Loss: 0.00001365
Iteration 219/1000 | Loss: 0.00001365
Iteration 220/1000 | Loss: 0.00001365
Iteration 221/1000 | Loss: 0.00001365
Iteration 222/1000 | Loss: 0.00001365
Iteration 223/1000 | Loss: 0.00001365
Iteration 224/1000 | Loss: 0.00001365
Iteration 225/1000 | Loss: 0.00001365
Iteration 226/1000 | Loss: 0.00001365
Iteration 227/1000 | Loss: 0.00001365
Iteration 228/1000 | Loss: 0.00001364
Iteration 229/1000 | Loss: 0.00001364
Iteration 230/1000 | Loss: 0.00001364
Iteration 231/1000 | Loss: 0.00001364
Iteration 232/1000 | Loss: 0.00001364
Iteration 233/1000 | Loss: 0.00001364
Iteration 234/1000 | Loss: 0.00001364
Iteration 235/1000 | Loss: 0.00001364
Iteration 236/1000 | Loss: 0.00001364
Iteration 237/1000 | Loss: 0.00001364
Iteration 238/1000 | Loss: 0.00001363
Iteration 239/1000 | Loss: 0.00005493
Iteration 240/1000 | Loss: 0.00005545
Iteration 241/1000 | Loss: 0.00001395
Iteration 242/1000 | Loss: 0.00003495
Iteration 243/1000 | Loss: 0.00011606
Iteration 244/1000 | Loss: 0.00012489
Iteration 245/1000 | Loss: 0.00037222
Iteration 246/1000 | Loss: 0.00056369
Iteration 247/1000 | Loss: 0.00028170
Iteration 248/1000 | Loss: 0.00033017
Iteration 249/1000 | Loss: 0.00025983
Iteration 250/1000 | Loss: 0.00005302
Iteration 251/1000 | Loss: 0.00002862
Iteration 252/1000 | Loss: 0.00002378
Iteration 253/1000 | Loss: 0.00001411
Iteration 254/1000 | Loss: 0.00001398
Iteration 255/1000 | Loss: 0.00003220
Iteration 256/1000 | Loss: 0.00001491
Iteration 257/1000 | Loss: 0.00001380
Iteration 258/1000 | Loss: 0.00001379
Iteration 259/1000 | Loss: 0.00001378
Iteration 260/1000 | Loss: 0.00001378
Iteration 261/1000 | Loss: 0.00001694
Iteration 262/1000 | Loss: 0.00001374
Iteration 263/1000 | Loss: 0.00001374
Iteration 264/1000 | Loss: 0.00001374
Iteration 265/1000 | Loss: 0.00001374
Iteration 266/1000 | Loss: 0.00001374
Iteration 267/1000 | Loss: 0.00001374
Iteration 268/1000 | Loss: 0.00001374
Iteration 269/1000 | Loss: 0.00001374
Iteration 270/1000 | Loss: 0.00001374
Iteration 271/1000 | Loss: 0.00001373
Iteration 272/1000 | Loss: 0.00001373
Iteration 273/1000 | Loss: 0.00001373
Iteration 274/1000 | Loss: 0.00001373
Iteration 275/1000 | Loss: 0.00001373
Iteration 276/1000 | Loss: 0.00001373
Iteration 277/1000 | Loss: 0.00001373
Iteration 278/1000 | Loss: 0.00001373
Iteration 279/1000 | Loss: 0.00001373
Iteration 280/1000 | Loss: 0.00001373
Iteration 281/1000 | Loss: 0.00001373
Iteration 282/1000 | Loss: 0.00001372
Iteration 283/1000 | Loss: 0.00001372
Iteration 284/1000 | Loss: 0.00001372
Iteration 285/1000 | Loss: 0.00001372
Iteration 286/1000 | Loss: 0.00001372
Iteration 287/1000 | Loss: 0.00001372
Iteration 288/1000 | Loss: 0.00001372
Iteration 289/1000 | Loss: 0.00001372
Iteration 290/1000 | Loss: 0.00001372
Iteration 291/1000 | Loss: 0.00001372
Iteration 292/1000 | Loss: 0.00001372
Iteration 293/1000 | Loss: 0.00001372
Iteration 294/1000 | Loss: 0.00001371
Iteration 295/1000 | Loss: 0.00001371
Iteration 296/1000 | Loss: 0.00001371
Iteration 297/1000 | Loss: 0.00005888
Iteration 298/1000 | Loss: 0.00001381
Iteration 299/1000 | Loss: 0.00001374
Iteration 300/1000 | Loss: 0.00001373
Iteration 301/1000 | Loss: 0.00002697
Iteration 302/1000 | Loss: 0.00001371
Iteration 303/1000 | Loss: 0.00001370
Iteration 304/1000 | Loss: 0.00001370
Iteration 305/1000 | Loss: 0.00001369
Iteration 306/1000 | Loss: 0.00001369
Iteration 307/1000 | Loss: 0.00001369
Iteration 308/1000 | Loss: 0.00001369
Iteration 309/1000 | Loss: 0.00001369
Iteration 310/1000 | Loss: 0.00001369
Iteration 311/1000 | Loss: 0.00001369
Iteration 312/1000 | Loss: 0.00001369
Iteration 313/1000 | Loss: 0.00001369
Iteration 314/1000 | Loss: 0.00001369
Iteration 315/1000 | Loss: 0.00001368
Iteration 316/1000 | Loss: 0.00001368
Iteration 317/1000 | Loss: 0.00001368
Iteration 318/1000 | Loss: 0.00001367
Iteration 319/1000 | Loss: 0.00001367
Iteration 320/1000 | Loss: 0.00001367
Iteration 321/1000 | Loss: 0.00001367
Iteration 322/1000 | Loss: 0.00001367
Iteration 323/1000 | Loss: 0.00001367
Iteration 324/1000 | Loss: 0.00001367
Iteration 325/1000 | Loss: 0.00001367
Iteration 326/1000 | Loss: 0.00001367
Iteration 327/1000 | Loss: 0.00001366
Iteration 328/1000 | Loss: 0.00001366
Iteration 329/1000 | Loss: 0.00001366
Iteration 330/1000 | Loss: 0.00001366
Iteration 331/1000 | Loss: 0.00001366
Iteration 332/1000 | Loss: 0.00001365
Iteration 333/1000 | Loss: 0.00001365
Iteration 334/1000 | Loss: 0.00001365
Iteration 335/1000 | Loss: 0.00001365
Iteration 336/1000 | Loss: 0.00001365
Iteration 337/1000 | Loss: 0.00001365
Iteration 338/1000 | Loss: 0.00001365
Iteration 339/1000 | Loss: 0.00001365
Iteration 340/1000 | Loss: 0.00001365
Iteration 341/1000 | Loss: 0.00001364
Iteration 342/1000 | Loss: 0.00001364
Iteration 343/1000 | Loss: 0.00001364
Iteration 344/1000 | Loss: 0.00001364
Iteration 345/1000 | Loss: 0.00001364
Iteration 346/1000 | Loss: 0.00001364
Iteration 347/1000 | Loss: 0.00001364
Iteration 348/1000 | Loss: 0.00001364
Iteration 349/1000 | Loss: 0.00001364
Iteration 350/1000 | Loss: 0.00001364
Iteration 351/1000 | Loss: 0.00001363
Iteration 352/1000 | Loss: 0.00001363
Iteration 353/1000 | Loss: 0.00001363
Iteration 354/1000 | Loss: 0.00001363
Iteration 355/1000 | Loss: 0.00001363
Iteration 356/1000 | Loss: 0.00001363
Iteration 357/1000 | Loss: 0.00001363
Iteration 358/1000 | Loss: 0.00001363
Iteration 359/1000 | Loss: 0.00001363
Iteration 360/1000 | Loss: 0.00001363
Iteration 361/1000 | Loss: 0.00001363
Iteration 362/1000 | Loss: 0.00001363
Iteration 363/1000 | Loss: 0.00001363
Iteration 364/1000 | Loss: 0.00001363
Iteration 365/1000 | Loss: 0.00001362
Iteration 366/1000 | Loss: 0.00001362
Iteration 367/1000 | Loss: 0.00001362
Iteration 368/1000 | Loss: 0.00001362
Iteration 369/1000 | Loss: 0.00001362
Iteration 370/1000 | Loss: 0.00001362
Iteration 371/1000 | Loss: 0.00001362
Iteration 372/1000 | Loss: 0.00001362
Iteration 373/1000 | Loss: 0.00001362
Iteration 374/1000 | Loss: 0.00001362
Iteration 375/1000 | Loss: 0.00001361
Iteration 376/1000 | Loss: 0.00001361
Iteration 377/1000 | Loss: 0.00001361
Iteration 378/1000 | Loss: 0.00001361
Iteration 379/1000 | Loss: 0.00001361
Iteration 380/1000 | Loss: 0.00001361
Iteration 381/1000 | Loss: 0.00001361
Iteration 382/1000 | Loss: 0.00001361
Iteration 383/1000 | Loss: 0.00001361
Iteration 384/1000 | Loss: 0.00001361
Iteration 385/1000 | Loss: 0.00001361
Iteration 386/1000 | Loss: 0.00001361
Iteration 387/1000 | Loss: 0.00001361
Iteration 388/1000 | Loss: 0.00001361
Iteration 389/1000 | Loss: 0.00001361
Iteration 390/1000 | Loss: 0.00001361
Iteration 391/1000 | Loss: 0.00001361
Iteration 392/1000 | Loss: 0.00001361
Iteration 393/1000 | Loss: 0.00001361
Iteration 394/1000 | Loss: 0.00001361
Iteration 395/1000 | Loss: 0.00001361
Iteration 396/1000 | Loss: 0.00001361
Iteration 397/1000 | Loss: 0.00001361
Iteration 398/1000 | Loss: 0.00001361
Iteration 399/1000 | Loss: 0.00001361
Iteration 400/1000 | Loss: 0.00001361
Iteration 401/1000 | Loss: 0.00001361
Iteration 402/1000 | Loss: 0.00001361
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 402. Stopping optimization.
Last 5 losses: [1.3609504094347358e-05, 1.3609504094347358e-05, 1.3609504094347358e-05, 1.3609504094347358e-05, 1.3609504094347358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3609504094347358e-05

Optimization complete. Final v2v error: 3.1028339862823486 mm

Highest mean error: 4.187410831451416 mm for frame 153

Lowest mean error: 2.6956682205200195 mm for frame 207

Saving results

Total time: 369.62431502342224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970318
Iteration 2/25 | Loss: 0.00141168
Iteration 3/25 | Loss: 0.00101991
Iteration 4/25 | Loss: 0.00087665
Iteration 5/25 | Loss: 0.00085122
Iteration 6/25 | Loss: 0.00081703
Iteration 7/25 | Loss: 0.00081520
Iteration 8/25 | Loss: 0.00080351
Iteration 9/25 | Loss: 0.00080228
Iteration 10/25 | Loss: 0.00080869
Iteration 11/25 | Loss: 0.00079938
Iteration 12/25 | Loss: 0.00079304
Iteration 13/25 | Loss: 0.00079099
Iteration 14/25 | Loss: 0.00079021
Iteration 15/25 | Loss: 0.00078968
Iteration 16/25 | Loss: 0.00078944
Iteration 17/25 | Loss: 0.00078941
Iteration 18/25 | Loss: 0.00078941
Iteration 19/25 | Loss: 0.00078941
Iteration 20/25 | Loss: 0.00078941
Iteration 21/25 | Loss: 0.00078941
Iteration 22/25 | Loss: 0.00078941
Iteration 23/25 | Loss: 0.00078941
Iteration 24/25 | Loss: 0.00078940
Iteration 25/25 | Loss: 0.00078940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63491678
Iteration 2/25 | Loss: 0.00089992
Iteration 3/25 | Loss: 0.00089990
Iteration 4/25 | Loss: 0.00089990
Iteration 5/25 | Loss: 0.00089990
Iteration 6/25 | Loss: 0.00089990
Iteration 7/25 | Loss: 0.00089990
Iteration 8/25 | Loss: 0.00089990
Iteration 9/25 | Loss: 0.00089990
Iteration 10/25 | Loss: 0.00089990
Iteration 11/25 | Loss: 0.00089990
Iteration 12/25 | Loss: 0.00089990
Iteration 13/25 | Loss: 0.00089990
Iteration 14/25 | Loss: 0.00089990
Iteration 15/25 | Loss: 0.00089990
Iteration 16/25 | Loss: 0.00089990
Iteration 17/25 | Loss: 0.00089990
Iteration 18/25 | Loss: 0.00089990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008998980047181249, 0.0008998980047181249, 0.0008998980047181249, 0.0008998980047181249, 0.0008998980047181249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008998980047181249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089990
Iteration 2/1000 | Loss: 0.00006161
Iteration 3/1000 | Loss: 0.00006098
Iteration 4/1000 | Loss: 0.00003000
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002199
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002054
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00001999
Iteration 14/1000 | Loss: 0.00001989
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001982
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001975
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001974
Iteration 25/1000 | Loss: 0.00001974
Iteration 26/1000 | Loss: 0.00001974
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001973
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001972
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001972
Iteration 35/1000 | Loss: 0.00001972
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001971
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001971
Iteration 42/1000 | Loss: 0.00001971
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001970
Iteration 45/1000 | Loss: 0.00001970
Iteration 46/1000 | Loss: 0.00001970
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001969
Iteration 49/1000 | Loss: 0.00001969
Iteration 50/1000 | Loss: 0.00001969
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001964
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001963
Iteration 75/1000 | Loss: 0.00001963
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001962
Iteration 80/1000 | Loss: 0.00001962
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001960
Iteration 96/1000 | Loss: 0.00001960
Iteration 97/1000 | Loss: 0.00001960
Iteration 98/1000 | Loss: 0.00001960
Iteration 99/1000 | Loss: 0.00001960
Iteration 100/1000 | Loss: 0.00001960
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.9600036466727033e-05, 1.9600036466727033e-05, 1.9600036466727033e-05, 1.9600036466727033e-05, 1.9600036466727033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9600036466727033e-05

Optimization complete. Final v2v error: 3.7479326725006104 mm

Highest mean error: 4.820616722106934 mm for frame 39

Lowest mean error: 3.184321641921997 mm for frame 200

Saving results

Total time: 58.36599111557007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375501
Iteration 2/25 | Loss: 0.00081086
Iteration 3/25 | Loss: 0.00071807
Iteration 4/25 | Loss: 0.00070990
Iteration 5/25 | Loss: 0.00070682
Iteration 6/25 | Loss: 0.00070588
Iteration 7/25 | Loss: 0.00070585
Iteration 8/25 | Loss: 0.00070585
Iteration 9/25 | Loss: 0.00070585
Iteration 10/25 | Loss: 0.00070585
Iteration 11/25 | Loss: 0.00070585
Iteration 12/25 | Loss: 0.00070585
Iteration 13/25 | Loss: 0.00070585
Iteration 14/25 | Loss: 0.00070585
Iteration 15/25 | Loss: 0.00070585
Iteration 16/25 | Loss: 0.00070585
Iteration 17/25 | Loss: 0.00070585
Iteration 18/25 | Loss: 0.00070585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007058487390168011, 0.0007058487390168011, 0.0007058487390168011, 0.0007058487390168011, 0.0007058487390168011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007058487390168011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57692885
Iteration 2/25 | Loss: 0.00093637
Iteration 3/25 | Loss: 0.00093637
Iteration 4/25 | Loss: 0.00093637
Iteration 5/25 | Loss: 0.00093637
Iteration 6/25 | Loss: 0.00093637
Iteration 7/25 | Loss: 0.00093637
Iteration 8/25 | Loss: 0.00093637
Iteration 9/25 | Loss: 0.00093637
Iteration 10/25 | Loss: 0.00093637
Iteration 11/25 | Loss: 0.00093637
Iteration 12/25 | Loss: 0.00093637
Iteration 13/25 | Loss: 0.00093637
Iteration 14/25 | Loss: 0.00093637
Iteration 15/25 | Loss: 0.00093637
Iteration 16/25 | Loss: 0.00093637
Iteration 17/25 | Loss: 0.00093637
Iteration 18/25 | Loss: 0.00093637
Iteration 19/25 | Loss: 0.00093637
Iteration 20/25 | Loss: 0.00093637
Iteration 21/25 | Loss: 0.00093637
Iteration 22/25 | Loss: 0.00093637
Iteration 23/25 | Loss: 0.00093637
Iteration 24/25 | Loss: 0.00093637
Iteration 25/25 | Loss: 0.00093637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093637
Iteration 2/1000 | Loss: 0.00002499
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001364
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001249
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001223
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001185
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00001180
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001180
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001179
Iteration 22/1000 | Loss: 0.00001176
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001174
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001172
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001168
Iteration 32/1000 | Loss: 0.00001168
Iteration 33/1000 | Loss: 0.00001168
Iteration 34/1000 | Loss: 0.00001168
Iteration 35/1000 | Loss: 0.00001167
Iteration 36/1000 | Loss: 0.00001167
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001167
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001165
Iteration 42/1000 | Loss: 0.00001165
Iteration 43/1000 | Loss: 0.00001165
Iteration 44/1000 | Loss: 0.00001165
Iteration 45/1000 | Loss: 0.00001165
Iteration 46/1000 | Loss: 0.00001164
Iteration 47/1000 | Loss: 0.00001164
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001164
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001163
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001161
Iteration 56/1000 | Loss: 0.00001161
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001155
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001154
Iteration 103/1000 | Loss: 0.00001154
Iteration 104/1000 | Loss: 0.00001154
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001153
Iteration 117/1000 | Loss: 0.00001153
Iteration 118/1000 | Loss: 0.00001153
Iteration 119/1000 | Loss: 0.00001153
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1526923117344268e-05, 1.1526923117344268e-05, 1.1526923117344268e-05, 1.1526923117344268e-05, 1.1526923117344268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1526923117344268e-05

Optimization complete. Final v2v error: 2.857015371322632 mm

Highest mean error: 3.0444793701171875 mm for frame 12

Lowest mean error: 2.665820837020874 mm for frame 133

Saving results

Total time: 32.42938446998596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487169
Iteration 2/25 | Loss: 0.00089496
Iteration 3/25 | Loss: 0.00075155
Iteration 4/25 | Loss: 0.00072513
Iteration 5/25 | Loss: 0.00071843
Iteration 6/25 | Loss: 0.00071715
Iteration 7/25 | Loss: 0.00071701
Iteration 8/25 | Loss: 0.00071701
Iteration 9/25 | Loss: 0.00071701
Iteration 10/25 | Loss: 0.00071701
Iteration 11/25 | Loss: 0.00071701
Iteration 12/25 | Loss: 0.00071701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007170098833739758, 0.0007170098833739758, 0.0007170098833739758, 0.0007170098833739758, 0.0007170098833739758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007170098833739758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.84164667
Iteration 2/25 | Loss: 0.00075082
Iteration 3/25 | Loss: 0.00075081
Iteration 4/25 | Loss: 0.00075081
Iteration 5/25 | Loss: 0.00075081
Iteration 6/25 | Loss: 0.00075081
Iteration 7/25 | Loss: 0.00075081
Iteration 8/25 | Loss: 0.00075081
Iteration 9/25 | Loss: 0.00075081
Iteration 10/25 | Loss: 0.00075081
Iteration 11/25 | Loss: 0.00075081
Iteration 12/25 | Loss: 0.00075081
Iteration 13/25 | Loss: 0.00075081
Iteration 14/25 | Loss: 0.00075081
Iteration 15/25 | Loss: 0.00075081
Iteration 16/25 | Loss: 0.00075081
Iteration 17/25 | Loss: 0.00075081
Iteration 18/25 | Loss: 0.00075081
Iteration 19/25 | Loss: 0.00075081
Iteration 20/25 | Loss: 0.00075081
Iteration 21/25 | Loss: 0.00075081
Iteration 22/25 | Loss: 0.00075081
Iteration 23/25 | Loss: 0.00075081
Iteration 24/25 | Loss: 0.00075081
Iteration 25/25 | Loss: 0.00075081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075081
Iteration 2/1000 | Loss: 0.00002730
Iteration 3/1000 | Loss: 0.00001731
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001462
Iteration 6/1000 | Loss: 0.00001415
Iteration 7/1000 | Loss: 0.00001385
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001340
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001327
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001319
Iteration 19/1000 | Loss: 0.00001317
Iteration 20/1000 | Loss: 0.00001317
Iteration 21/1000 | Loss: 0.00001316
Iteration 22/1000 | Loss: 0.00001316
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001314
Iteration 26/1000 | Loss: 0.00001313
Iteration 27/1000 | Loss: 0.00001313
Iteration 28/1000 | Loss: 0.00001312
Iteration 29/1000 | Loss: 0.00001312
Iteration 30/1000 | Loss: 0.00001312
Iteration 31/1000 | Loss: 0.00001312
Iteration 32/1000 | Loss: 0.00001311
Iteration 33/1000 | Loss: 0.00001311
Iteration 34/1000 | Loss: 0.00001311
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001310
Iteration 38/1000 | Loss: 0.00001310
Iteration 39/1000 | Loss: 0.00001310
Iteration 40/1000 | Loss: 0.00001310
Iteration 41/1000 | Loss: 0.00001309
Iteration 42/1000 | Loss: 0.00001309
Iteration 43/1000 | Loss: 0.00001309
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001306
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001304
Iteration 67/1000 | Loss: 0.00001304
Iteration 68/1000 | Loss: 0.00001304
Iteration 69/1000 | Loss: 0.00001304
Iteration 70/1000 | Loss: 0.00001304
Iteration 71/1000 | Loss: 0.00001303
Iteration 72/1000 | Loss: 0.00001303
Iteration 73/1000 | Loss: 0.00001303
Iteration 74/1000 | Loss: 0.00001303
Iteration 75/1000 | Loss: 0.00001303
Iteration 76/1000 | Loss: 0.00001303
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001302
Iteration 79/1000 | Loss: 0.00001302
Iteration 80/1000 | Loss: 0.00001302
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001302
Iteration 84/1000 | Loss: 0.00001302
Iteration 85/1000 | Loss: 0.00001302
Iteration 86/1000 | Loss: 0.00001302
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001300
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001299
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001297
Iteration 106/1000 | Loss: 0.00001297
Iteration 107/1000 | Loss: 0.00001297
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2944115042046178e-05, 1.2944115042046178e-05, 1.2944115042046178e-05, 1.2944115042046178e-05, 1.2944115042046178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2944115042046178e-05

Optimization complete. Final v2v error: 3.0839433670043945 mm

Highest mean error: 3.395287275314331 mm for frame 220

Lowest mean error: 2.7776196002960205 mm for frame 116

Saving results

Total time: 37.57759714126587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401943
Iteration 2/25 | Loss: 0.00097203
Iteration 3/25 | Loss: 0.00073690
Iteration 4/25 | Loss: 0.00070746
Iteration 5/25 | Loss: 0.00069995
Iteration 6/25 | Loss: 0.00069790
Iteration 7/25 | Loss: 0.00069727
Iteration 8/25 | Loss: 0.00069724
Iteration 9/25 | Loss: 0.00069724
Iteration 10/25 | Loss: 0.00069724
Iteration 11/25 | Loss: 0.00069724
Iteration 12/25 | Loss: 0.00069724
Iteration 13/25 | Loss: 0.00069724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006972415139898658, 0.0006972415139898658, 0.0006972415139898658, 0.0006972415139898658, 0.0006972415139898658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006972415139898658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52854598
Iteration 2/25 | Loss: 0.00081397
Iteration 3/25 | Loss: 0.00081396
Iteration 4/25 | Loss: 0.00081396
Iteration 5/25 | Loss: 0.00081396
Iteration 6/25 | Loss: 0.00081396
Iteration 7/25 | Loss: 0.00081396
Iteration 8/25 | Loss: 0.00081396
Iteration 9/25 | Loss: 0.00081396
Iteration 10/25 | Loss: 0.00081396
Iteration 11/25 | Loss: 0.00081396
Iteration 12/25 | Loss: 0.00081396
Iteration 13/25 | Loss: 0.00081396
Iteration 14/25 | Loss: 0.00081396
Iteration 15/25 | Loss: 0.00081396
Iteration 16/25 | Loss: 0.00081396
Iteration 17/25 | Loss: 0.00081396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008139586425386369, 0.0008139586425386369, 0.0008139586425386369, 0.0008139586425386369, 0.0008139586425386369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139586425386369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081396
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001210
Iteration 11/1000 | Loss: 0.00001203
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001189
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001188
Iteration 27/1000 | Loss: 0.00001187
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001184
Iteration 35/1000 | Loss: 0.00001184
Iteration 36/1000 | Loss: 0.00001183
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00001182
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001180
Iteration 52/1000 | Loss: 0.00001180
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001179
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001179
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001174
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.1735360203601886e-05, 1.1735360203601886e-05, 1.1735360203601886e-05, 1.1735360203601886e-05, 1.1735360203601886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1735360203601886e-05

Optimization complete. Final v2v error: 2.9405391216278076 mm

Highest mean error: 3.0115673542022705 mm for frame 19

Lowest mean error: 2.838289976119995 mm for frame 97

Saving results

Total time: 31.526841640472412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01108900
Iteration 2/25 | Loss: 0.00231182
Iteration 3/25 | Loss: 0.00158574
Iteration 4/25 | Loss: 0.00192830
Iteration 5/25 | Loss: 0.00151543
Iteration 6/25 | Loss: 0.00100353
Iteration 7/25 | Loss: 0.00092015
Iteration 8/25 | Loss: 0.00091408
Iteration 9/25 | Loss: 0.00091142
Iteration 10/25 | Loss: 0.00090452
Iteration 11/25 | Loss: 0.00090159
Iteration 12/25 | Loss: 0.00089970
Iteration 13/25 | Loss: 0.00089890
Iteration 14/25 | Loss: 0.00089860
Iteration 15/25 | Loss: 0.00089853
Iteration 16/25 | Loss: 0.00089853
Iteration 17/25 | Loss: 0.00089853
Iteration 18/25 | Loss: 0.00089853
Iteration 19/25 | Loss: 0.00089853
Iteration 20/25 | Loss: 0.00089852
Iteration 21/25 | Loss: 0.00089852
Iteration 22/25 | Loss: 0.00089852
Iteration 23/25 | Loss: 0.00089852
Iteration 24/25 | Loss: 0.00089852
Iteration 25/25 | Loss: 0.00089852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33404279
Iteration 2/25 | Loss: 0.00055653
Iteration 3/25 | Loss: 0.00055652
Iteration 4/25 | Loss: 0.00055652
Iteration 5/25 | Loss: 0.00055651
Iteration 6/25 | Loss: 0.00055651
Iteration 7/25 | Loss: 0.00055651
Iteration 8/25 | Loss: 0.00055651
Iteration 9/25 | Loss: 0.00055651
Iteration 10/25 | Loss: 0.00055651
Iteration 11/25 | Loss: 0.00055651
Iteration 12/25 | Loss: 0.00055651
Iteration 13/25 | Loss: 0.00055651
Iteration 14/25 | Loss: 0.00055651
Iteration 15/25 | Loss: 0.00055651
Iteration 16/25 | Loss: 0.00055651
Iteration 17/25 | Loss: 0.00055651
Iteration 18/25 | Loss: 0.00055651
Iteration 19/25 | Loss: 0.00055651
Iteration 20/25 | Loss: 0.00055651
Iteration 21/25 | Loss: 0.00055651
Iteration 22/25 | Loss: 0.00055651
Iteration 23/25 | Loss: 0.00055651
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005565129104070365, 0.0005565129104070365, 0.0005565129104070365, 0.0005565129104070365, 0.0005565129104070365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005565129104070365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055651
Iteration 2/1000 | Loss: 0.00004227
Iteration 3/1000 | Loss: 0.00002946
Iteration 4/1000 | Loss: 0.00002635
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002453
Iteration 7/1000 | Loss: 0.00002407
Iteration 8/1000 | Loss: 0.00002370
Iteration 9/1000 | Loss: 0.00002351
Iteration 10/1000 | Loss: 0.00002339
Iteration 11/1000 | Loss: 0.00002337
Iteration 12/1000 | Loss: 0.00002336
Iteration 13/1000 | Loss: 0.00002336
Iteration 14/1000 | Loss: 0.00002334
Iteration 15/1000 | Loss: 0.00002332
Iteration 16/1000 | Loss: 0.00002331
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002330
Iteration 19/1000 | Loss: 0.00002328
Iteration 20/1000 | Loss: 0.00002328
Iteration 21/1000 | Loss: 0.00002326
Iteration 22/1000 | Loss: 0.00002325
Iteration 23/1000 | Loss: 0.00002324
Iteration 24/1000 | Loss: 0.00002323
Iteration 25/1000 | Loss: 0.00002323
Iteration 26/1000 | Loss: 0.00002322
Iteration 27/1000 | Loss: 0.00002322
Iteration 28/1000 | Loss: 0.00002320
Iteration 29/1000 | Loss: 0.00002320
Iteration 30/1000 | Loss: 0.00002319
Iteration 31/1000 | Loss: 0.00002317
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002313
Iteration 34/1000 | Loss: 0.00002312
Iteration 35/1000 | Loss: 0.00002312
Iteration 36/1000 | Loss: 0.00002312
Iteration 37/1000 | Loss: 0.00002312
Iteration 38/1000 | Loss: 0.00002311
Iteration 39/1000 | Loss: 0.00002311
Iteration 40/1000 | Loss: 0.00002311
Iteration 41/1000 | Loss: 0.00002311
Iteration 42/1000 | Loss: 0.00002311
Iteration 43/1000 | Loss: 0.00002311
Iteration 44/1000 | Loss: 0.00002311
Iteration 45/1000 | Loss: 0.00002311
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002310
Iteration 48/1000 | Loss: 0.00002309
Iteration 49/1000 | Loss: 0.00002309
Iteration 50/1000 | Loss: 0.00002309
Iteration 51/1000 | Loss: 0.00002308
Iteration 52/1000 | Loss: 0.00002308
Iteration 53/1000 | Loss: 0.00002308
Iteration 54/1000 | Loss: 0.00002308
Iteration 55/1000 | Loss: 0.00002308
Iteration 56/1000 | Loss: 0.00002308
Iteration 57/1000 | Loss: 0.00002308
Iteration 58/1000 | Loss: 0.00002308
Iteration 59/1000 | Loss: 0.00002307
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002307
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002307
Iteration 65/1000 | Loss: 0.00002307
Iteration 66/1000 | Loss: 0.00002307
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002306
Iteration 69/1000 | Loss: 0.00002306
Iteration 70/1000 | Loss: 0.00002306
Iteration 71/1000 | Loss: 0.00002306
Iteration 72/1000 | Loss: 0.00002306
Iteration 73/1000 | Loss: 0.00002306
Iteration 74/1000 | Loss: 0.00002306
Iteration 75/1000 | Loss: 0.00002306
Iteration 76/1000 | Loss: 0.00002306
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002306
Iteration 79/1000 | Loss: 0.00002306
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002305
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002305
Iteration 85/1000 | Loss: 0.00002305
Iteration 86/1000 | Loss: 0.00002304
Iteration 87/1000 | Loss: 0.00002304
Iteration 88/1000 | Loss: 0.00002304
Iteration 89/1000 | Loss: 0.00002304
Iteration 90/1000 | Loss: 0.00002304
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002304
Iteration 95/1000 | Loss: 0.00002304
Iteration 96/1000 | Loss: 0.00002304
Iteration 97/1000 | Loss: 0.00002304
Iteration 98/1000 | Loss: 0.00002304
Iteration 99/1000 | Loss: 0.00002304
Iteration 100/1000 | Loss: 0.00002304
Iteration 101/1000 | Loss: 0.00002304
Iteration 102/1000 | Loss: 0.00002304
Iteration 103/1000 | Loss: 0.00002304
Iteration 104/1000 | Loss: 0.00002304
Iteration 105/1000 | Loss: 0.00002304
Iteration 106/1000 | Loss: 0.00002304
Iteration 107/1000 | Loss: 0.00002304
Iteration 108/1000 | Loss: 0.00002304
Iteration 109/1000 | Loss: 0.00002304
Iteration 110/1000 | Loss: 0.00002304
Iteration 111/1000 | Loss: 0.00002304
Iteration 112/1000 | Loss: 0.00002304
Iteration 113/1000 | Loss: 0.00002304
Iteration 114/1000 | Loss: 0.00002304
Iteration 115/1000 | Loss: 0.00002304
Iteration 116/1000 | Loss: 0.00002304
Iteration 117/1000 | Loss: 0.00002304
Iteration 118/1000 | Loss: 0.00002304
Iteration 119/1000 | Loss: 0.00002304
Iteration 120/1000 | Loss: 0.00002304
Iteration 121/1000 | Loss: 0.00002304
Iteration 122/1000 | Loss: 0.00002304
Iteration 123/1000 | Loss: 0.00002304
Iteration 124/1000 | Loss: 0.00002304
Iteration 125/1000 | Loss: 0.00002304
Iteration 126/1000 | Loss: 0.00002304
Iteration 127/1000 | Loss: 0.00002304
Iteration 128/1000 | Loss: 0.00002304
Iteration 129/1000 | Loss: 0.00002304
Iteration 130/1000 | Loss: 0.00002304
Iteration 131/1000 | Loss: 0.00002304
Iteration 132/1000 | Loss: 0.00002304
Iteration 133/1000 | Loss: 0.00002304
Iteration 134/1000 | Loss: 0.00002304
Iteration 135/1000 | Loss: 0.00002304
Iteration 136/1000 | Loss: 0.00002304
Iteration 137/1000 | Loss: 0.00002304
Iteration 138/1000 | Loss: 0.00002304
Iteration 139/1000 | Loss: 0.00002304
Iteration 140/1000 | Loss: 0.00002304
Iteration 141/1000 | Loss: 0.00002304
Iteration 142/1000 | Loss: 0.00002304
Iteration 143/1000 | Loss: 0.00002304
Iteration 144/1000 | Loss: 0.00002304
Iteration 145/1000 | Loss: 0.00002304
Iteration 146/1000 | Loss: 0.00002304
Iteration 147/1000 | Loss: 0.00002304
Iteration 148/1000 | Loss: 0.00002304
Iteration 149/1000 | Loss: 0.00002304
Iteration 150/1000 | Loss: 0.00002304
Iteration 151/1000 | Loss: 0.00002304
Iteration 152/1000 | Loss: 0.00002304
Iteration 153/1000 | Loss: 0.00002304
Iteration 154/1000 | Loss: 0.00002304
Iteration 155/1000 | Loss: 0.00002304
Iteration 156/1000 | Loss: 0.00002304
Iteration 157/1000 | Loss: 0.00002304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.3038117433316074e-05, 2.3038117433316074e-05, 2.3038117433316074e-05, 2.3038117433316074e-05, 2.3038117433316074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3038117433316074e-05

Optimization complete. Final v2v error: 3.8049020767211914 mm

Highest mean error: 4.305257320404053 mm for frame 239

Lowest mean error: 3.495440721511841 mm for frame 17

Saving results

Total time: 57.165401220321655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482784
Iteration 2/25 | Loss: 0.00102929
Iteration 3/25 | Loss: 0.00081180
Iteration 4/25 | Loss: 0.00075972
Iteration 5/25 | Loss: 0.00074686
Iteration 6/25 | Loss: 0.00074381
Iteration 7/25 | Loss: 0.00074347
Iteration 8/25 | Loss: 0.00074347
Iteration 9/25 | Loss: 0.00074347
Iteration 10/25 | Loss: 0.00074347
Iteration 11/25 | Loss: 0.00074347
Iteration 12/25 | Loss: 0.00074347
Iteration 13/25 | Loss: 0.00074347
Iteration 14/25 | Loss: 0.00074347
Iteration 15/25 | Loss: 0.00074347
Iteration 16/25 | Loss: 0.00074347
Iteration 17/25 | Loss: 0.00074347
Iteration 18/25 | Loss: 0.00074347
Iteration 19/25 | Loss: 0.00074347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007434657891280949, 0.0007434657891280949, 0.0007434657891280949, 0.0007434657891280949, 0.0007434657891280949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007434657891280949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50638247
Iteration 2/25 | Loss: 0.00073245
Iteration 3/25 | Loss: 0.00073245
Iteration 4/25 | Loss: 0.00073245
Iteration 5/25 | Loss: 0.00073245
Iteration 6/25 | Loss: 0.00073245
Iteration 7/25 | Loss: 0.00073245
Iteration 8/25 | Loss: 0.00073245
Iteration 9/25 | Loss: 0.00073245
Iteration 10/25 | Loss: 0.00073245
Iteration 11/25 | Loss: 0.00073245
Iteration 12/25 | Loss: 0.00073245
Iteration 13/25 | Loss: 0.00073245
Iteration 14/25 | Loss: 0.00073245
Iteration 15/25 | Loss: 0.00073245
Iteration 16/25 | Loss: 0.00073245
Iteration 17/25 | Loss: 0.00073245
Iteration 18/25 | Loss: 0.00073245
Iteration 19/25 | Loss: 0.00073245
Iteration 20/25 | Loss: 0.00073245
Iteration 21/25 | Loss: 0.00073245
Iteration 22/25 | Loss: 0.00073245
Iteration 23/25 | Loss: 0.00073245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007324488251470029, 0.0007324488251470029, 0.0007324488251470029, 0.0007324488251470029, 0.0007324488251470029]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007324488251470029

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073245
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00001736
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001361
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001283
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001251
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001244
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001240
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001233
Iteration 23/1000 | Loss: 0.00001232
Iteration 24/1000 | Loss: 0.00001231
Iteration 25/1000 | Loss: 0.00001231
Iteration 26/1000 | Loss: 0.00001231
Iteration 27/1000 | Loss: 0.00001230
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001229
Iteration 32/1000 | Loss: 0.00001229
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001228
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001227
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001226
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001225
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001224
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001224
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [1.224498191731982e-05, 1.224498191731982e-05, 1.224498191731982e-05, 1.224498191731982e-05, 1.224498191731982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.224498191731982e-05

Optimization complete. Final v2v error: 2.9588067531585693 mm

Highest mean error: 3.0935251712799072 mm for frame 222

Lowest mean error: 2.855356216430664 mm for frame 36

Saving results

Total time: 32.69886016845703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358838
Iteration 2/25 | Loss: 0.00078194
Iteration 3/25 | Loss: 0.00069061
Iteration 4/25 | Loss: 0.00066834
Iteration 5/25 | Loss: 0.00066423
Iteration 6/25 | Loss: 0.00066334
Iteration 7/25 | Loss: 0.00066334
Iteration 8/25 | Loss: 0.00066334
Iteration 9/25 | Loss: 0.00066334
Iteration 10/25 | Loss: 0.00066334
Iteration 11/25 | Loss: 0.00066334
Iteration 12/25 | Loss: 0.00066334
Iteration 13/25 | Loss: 0.00066334
Iteration 14/25 | Loss: 0.00066334
Iteration 15/25 | Loss: 0.00066334
Iteration 16/25 | Loss: 0.00066334
Iteration 17/25 | Loss: 0.00066334
Iteration 18/25 | Loss: 0.00066334
Iteration 19/25 | Loss: 0.00066334
Iteration 20/25 | Loss: 0.00066334
Iteration 21/25 | Loss: 0.00066334
Iteration 22/25 | Loss: 0.00066334
Iteration 23/25 | Loss: 0.00066334
Iteration 24/25 | Loss: 0.00066334
Iteration 25/25 | Loss: 0.00066334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54256511
Iteration 2/25 | Loss: 0.00083874
Iteration 3/25 | Loss: 0.00083874
Iteration 4/25 | Loss: 0.00083874
Iteration 5/25 | Loss: 0.00083874
Iteration 6/25 | Loss: 0.00083874
Iteration 7/25 | Loss: 0.00083874
Iteration 8/25 | Loss: 0.00083874
Iteration 9/25 | Loss: 0.00083874
Iteration 10/25 | Loss: 0.00083874
Iteration 11/25 | Loss: 0.00083874
Iteration 12/25 | Loss: 0.00083874
Iteration 13/25 | Loss: 0.00083874
Iteration 14/25 | Loss: 0.00083874
Iteration 15/25 | Loss: 0.00083874
Iteration 16/25 | Loss: 0.00083874
Iteration 17/25 | Loss: 0.00083874
Iteration 18/25 | Loss: 0.00083874
Iteration 19/25 | Loss: 0.00083874
Iteration 20/25 | Loss: 0.00083874
Iteration 21/25 | Loss: 0.00083874
Iteration 22/25 | Loss: 0.00083874
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008387364796362817, 0.0008387364796362817, 0.0008387364796362817, 0.0008387364796362817, 0.0008387364796362817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008387364796362817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083874
Iteration 2/1000 | Loss: 0.00002026
Iteration 3/1000 | Loss: 0.00001296
Iteration 4/1000 | Loss: 0.00001210
Iteration 5/1000 | Loss: 0.00001155
Iteration 6/1000 | Loss: 0.00001116
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001089
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001081
Iteration 12/1000 | Loss: 0.00001077
Iteration 13/1000 | Loss: 0.00001072
Iteration 14/1000 | Loss: 0.00001072
Iteration 15/1000 | Loss: 0.00001072
Iteration 16/1000 | Loss: 0.00001071
Iteration 17/1000 | Loss: 0.00001071
Iteration 18/1000 | Loss: 0.00001071
Iteration 19/1000 | Loss: 0.00001070
Iteration 20/1000 | Loss: 0.00001070
Iteration 21/1000 | Loss: 0.00001068
Iteration 22/1000 | Loss: 0.00001068
Iteration 23/1000 | Loss: 0.00001067
Iteration 24/1000 | Loss: 0.00001067
Iteration 25/1000 | Loss: 0.00001066
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001063
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001062
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001061
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001058
Iteration 38/1000 | Loss: 0.00001058
Iteration 39/1000 | Loss: 0.00001058
Iteration 40/1000 | Loss: 0.00001057
Iteration 41/1000 | Loss: 0.00001057
Iteration 42/1000 | Loss: 0.00001057
Iteration 43/1000 | Loss: 0.00001055
Iteration 44/1000 | Loss: 0.00001054
Iteration 45/1000 | Loss: 0.00001054
Iteration 46/1000 | Loss: 0.00001054
Iteration 47/1000 | Loss: 0.00001053
Iteration 48/1000 | Loss: 0.00001053
Iteration 49/1000 | Loss: 0.00001053
Iteration 50/1000 | Loss: 0.00001052
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00001051
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001050
Iteration 56/1000 | Loss: 0.00001050
Iteration 57/1000 | Loss: 0.00001050
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001049
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001049
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001048
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001047
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001046
Iteration 87/1000 | Loss: 0.00001046
Iteration 88/1000 | Loss: 0.00001045
Iteration 89/1000 | Loss: 0.00001045
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001044
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001042
Iteration 103/1000 | Loss: 0.00001042
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001041
Iteration 107/1000 | Loss: 0.00001041
Iteration 108/1000 | Loss: 0.00001040
Iteration 109/1000 | Loss: 0.00001040
Iteration 110/1000 | Loss: 0.00001040
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001039
Iteration 117/1000 | Loss: 0.00001039
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001039
Iteration 121/1000 | Loss: 0.00001039
Iteration 122/1000 | Loss: 0.00001039
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001038
Iteration 125/1000 | Loss: 0.00001038
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001038
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001037
Iteration 136/1000 | Loss: 0.00001037
Iteration 137/1000 | Loss: 0.00001037
Iteration 138/1000 | Loss: 0.00001037
Iteration 139/1000 | Loss: 0.00001037
Iteration 140/1000 | Loss: 0.00001037
Iteration 141/1000 | Loss: 0.00001037
Iteration 142/1000 | Loss: 0.00001037
Iteration 143/1000 | Loss: 0.00001036
Iteration 144/1000 | Loss: 0.00001036
Iteration 145/1000 | Loss: 0.00001036
Iteration 146/1000 | Loss: 0.00001036
Iteration 147/1000 | Loss: 0.00001036
Iteration 148/1000 | Loss: 0.00001036
Iteration 149/1000 | Loss: 0.00001035
Iteration 150/1000 | Loss: 0.00001035
Iteration 151/1000 | Loss: 0.00001035
Iteration 152/1000 | Loss: 0.00001035
Iteration 153/1000 | Loss: 0.00001035
Iteration 154/1000 | Loss: 0.00001035
Iteration 155/1000 | Loss: 0.00001035
Iteration 156/1000 | Loss: 0.00001035
Iteration 157/1000 | Loss: 0.00001034
Iteration 158/1000 | Loss: 0.00001034
Iteration 159/1000 | Loss: 0.00001034
Iteration 160/1000 | Loss: 0.00001034
Iteration 161/1000 | Loss: 0.00001034
Iteration 162/1000 | Loss: 0.00001034
Iteration 163/1000 | Loss: 0.00001034
Iteration 164/1000 | Loss: 0.00001034
Iteration 165/1000 | Loss: 0.00001034
Iteration 166/1000 | Loss: 0.00001034
Iteration 167/1000 | Loss: 0.00001034
Iteration 168/1000 | Loss: 0.00001034
Iteration 169/1000 | Loss: 0.00001034
Iteration 170/1000 | Loss: 0.00001034
Iteration 171/1000 | Loss: 0.00001034
Iteration 172/1000 | Loss: 0.00001034
Iteration 173/1000 | Loss: 0.00001034
Iteration 174/1000 | Loss: 0.00001034
Iteration 175/1000 | Loss: 0.00001034
Iteration 176/1000 | Loss: 0.00001034
Iteration 177/1000 | Loss: 0.00001034
Iteration 178/1000 | Loss: 0.00001034
Iteration 179/1000 | Loss: 0.00001034
Iteration 180/1000 | Loss: 0.00001034
Iteration 181/1000 | Loss: 0.00001034
Iteration 182/1000 | Loss: 0.00001034
Iteration 183/1000 | Loss: 0.00001034
Iteration 184/1000 | Loss: 0.00001034
Iteration 185/1000 | Loss: 0.00001034
Iteration 186/1000 | Loss: 0.00001034
Iteration 187/1000 | Loss: 0.00001034
Iteration 188/1000 | Loss: 0.00001034
Iteration 189/1000 | Loss: 0.00001034
Iteration 190/1000 | Loss: 0.00001034
Iteration 191/1000 | Loss: 0.00001034
Iteration 192/1000 | Loss: 0.00001034
Iteration 193/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.0335486877011135e-05, 1.0335486877011135e-05, 1.0335486877011135e-05, 1.0335486877011135e-05, 1.0335486877011135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0335486877011135e-05

Optimization complete. Final v2v error: 2.753262758255005 mm

Highest mean error: 2.9254982471466064 mm for frame 145

Lowest mean error: 2.7157137393951416 mm for frame 154

Saving results

Total time: 34.99820566177368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074524
Iteration 2/25 | Loss: 0.00400696
Iteration 3/25 | Loss: 0.00256832
Iteration 4/25 | Loss: 0.00160097
Iteration 5/25 | Loss: 0.00128743
Iteration 6/25 | Loss: 0.00117580
Iteration 7/25 | Loss: 0.00112062
Iteration 8/25 | Loss: 0.00104046
Iteration 9/25 | Loss: 0.00095906
Iteration 10/25 | Loss: 0.00092485
Iteration 11/25 | Loss: 0.00091884
Iteration 12/25 | Loss: 0.00091073
Iteration 13/25 | Loss: 0.00090656
Iteration 14/25 | Loss: 0.00090431
Iteration 15/25 | Loss: 0.00091290
Iteration 16/25 | Loss: 0.00091216
Iteration 17/25 | Loss: 0.00090228
Iteration 18/25 | Loss: 0.00089714
Iteration 19/25 | Loss: 0.00089877
Iteration 20/25 | Loss: 0.00090297
Iteration 21/25 | Loss: 0.00089644
Iteration 22/25 | Loss: 0.00088963
Iteration 23/25 | Loss: 0.00089151
Iteration 24/25 | Loss: 0.00088803
Iteration 25/25 | Loss: 0.00088914

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56412542
Iteration 2/25 | Loss: 0.00208369
Iteration 3/25 | Loss: 0.00208369
Iteration 4/25 | Loss: 0.00208369
Iteration 5/25 | Loss: 0.00208369
Iteration 6/25 | Loss: 0.00208369
Iteration 7/25 | Loss: 0.00208369
Iteration 8/25 | Loss: 0.00208369
Iteration 9/25 | Loss: 0.00208368
Iteration 10/25 | Loss: 0.00208368
Iteration 11/25 | Loss: 0.00208369
Iteration 12/25 | Loss: 0.00208369
Iteration 13/25 | Loss: 0.00208369
Iteration 14/25 | Loss: 0.00208369
Iteration 15/25 | Loss: 0.00208368
Iteration 16/25 | Loss: 0.00208369
Iteration 17/25 | Loss: 0.00208369
Iteration 18/25 | Loss: 0.00208369
Iteration 19/25 | Loss: 0.00208369
Iteration 20/25 | Loss: 0.00208369
Iteration 21/25 | Loss: 0.00208369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002083685016259551, 0.002083685016259551, 0.002083685016259551, 0.002083685016259551, 0.002083685016259551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002083685016259551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208369
Iteration 2/1000 | Loss: 0.00048077
Iteration 3/1000 | Loss: 0.00063389
Iteration 4/1000 | Loss: 0.00012527
Iteration 5/1000 | Loss: 0.00045555
Iteration 6/1000 | Loss: 0.00046653
Iteration 7/1000 | Loss: 0.00087255
Iteration 8/1000 | Loss: 0.00024165
Iteration 9/1000 | Loss: 0.00046388
Iteration 10/1000 | Loss: 0.00070488
Iteration 11/1000 | Loss: 0.00057913
Iteration 12/1000 | Loss: 0.00030620
Iteration 13/1000 | Loss: 0.00062389
Iteration 14/1000 | Loss: 0.00034203
Iteration 15/1000 | Loss: 0.00029163
Iteration 16/1000 | Loss: 0.00029056
Iteration 17/1000 | Loss: 0.00026967
Iteration 18/1000 | Loss: 0.00017063
Iteration 19/1000 | Loss: 0.00034284
Iteration 20/1000 | Loss: 0.00027404
Iteration 21/1000 | Loss: 0.00060851
Iteration 22/1000 | Loss: 0.00037323
Iteration 23/1000 | Loss: 0.00019091
Iteration 24/1000 | Loss: 0.00018560
Iteration 25/1000 | Loss: 0.00007339
Iteration 26/1000 | Loss: 0.00022957
Iteration 27/1000 | Loss: 0.00034386
Iteration 28/1000 | Loss: 0.00013921
Iteration 29/1000 | Loss: 0.00050160
Iteration 30/1000 | Loss: 0.00016326
Iteration 31/1000 | Loss: 0.00016838
Iteration 32/1000 | Loss: 0.00023797
Iteration 33/1000 | Loss: 0.00014322
Iteration 34/1000 | Loss: 0.00017806
Iteration 35/1000 | Loss: 0.00012128
Iteration 36/1000 | Loss: 0.00070201
Iteration 37/1000 | Loss: 0.00061186
Iteration 38/1000 | Loss: 0.00007890
Iteration 39/1000 | Loss: 0.00045831
Iteration 40/1000 | Loss: 0.00017776
Iteration 41/1000 | Loss: 0.00019775
Iteration 42/1000 | Loss: 0.00005876
Iteration 43/1000 | Loss: 0.00005498
Iteration 44/1000 | Loss: 0.00005248
Iteration 45/1000 | Loss: 0.00005110
Iteration 46/1000 | Loss: 0.00005029
Iteration 47/1000 | Loss: 0.00004952
Iteration 48/1000 | Loss: 0.00004910
Iteration 49/1000 | Loss: 0.00004860
Iteration 50/1000 | Loss: 0.00004813
Iteration 51/1000 | Loss: 0.00004785
Iteration 52/1000 | Loss: 0.00004761
Iteration 53/1000 | Loss: 0.00004744
Iteration 54/1000 | Loss: 0.00004720
Iteration 55/1000 | Loss: 0.00004713
Iteration 56/1000 | Loss: 0.00063787
Iteration 57/1000 | Loss: 0.00005873
Iteration 58/1000 | Loss: 0.00004977
Iteration 59/1000 | Loss: 0.00004624
Iteration 60/1000 | Loss: 0.00004523
Iteration 61/1000 | Loss: 0.00004435
Iteration 62/1000 | Loss: 0.00004393
Iteration 63/1000 | Loss: 0.00004387
Iteration 64/1000 | Loss: 0.00004378
Iteration 65/1000 | Loss: 0.00004372
Iteration 66/1000 | Loss: 0.00004368
Iteration 67/1000 | Loss: 0.00004368
Iteration 68/1000 | Loss: 0.00004365
Iteration 69/1000 | Loss: 0.00004365
Iteration 70/1000 | Loss: 0.00004365
Iteration 71/1000 | Loss: 0.00004365
Iteration 72/1000 | Loss: 0.00004365
Iteration 73/1000 | Loss: 0.00004364
Iteration 74/1000 | Loss: 0.00004363
Iteration 75/1000 | Loss: 0.00004361
Iteration 76/1000 | Loss: 0.00004360
Iteration 77/1000 | Loss: 0.00004360
Iteration 78/1000 | Loss: 0.00004359
Iteration 79/1000 | Loss: 0.00004359
Iteration 80/1000 | Loss: 0.00004356
Iteration 81/1000 | Loss: 0.00004353
Iteration 82/1000 | Loss: 0.00004350
Iteration 83/1000 | Loss: 0.00004346
Iteration 84/1000 | Loss: 0.00004346
Iteration 85/1000 | Loss: 0.00004343
Iteration 86/1000 | Loss: 0.00004343
Iteration 87/1000 | Loss: 0.00004342
Iteration 88/1000 | Loss: 0.00004342
Iteration 89/1000 | Loss: 0.00004341
Iteration 90/1000 | Loss: 0.00004341
Iteration 91/1000 | Loss: 0.00004340
Iteration 92/1000 | Loss: 0.00004338
Iteration 93/1000 | Loss: 0.00004337
Iteration 94/1000 | Loss: 0.00004337
Iteration 95/1000 | Loss: 0.00004333
Iteration 96/1000 | Loss: 0.00004318
Iteration 97/1000 | Loss: 0.00004317
Iteration 98/1000 | Loss: 0.00004297
Iteration 99/1000 | Loss: 0.00004282
Iteration 100/1000 | Loss: 0.00004278
Iteration 101/1000 | Loss: 0.00004275
Iteration 102/1000 | Loss: 0.00004272
Iteration 103/1000 | Loss: 0.00004272
Iteration 104/1000 | Loss: 0.00004271
Iteration 105/1000 | Loss: 0.00004266
Iteration 106/1000 | Loss: 0.00004265
Iteration 107/1000 | Loss: 0.00004265
Iteration 108/1000 | Loss: 0.00004265
Iteration 109/1000 | Loss: 0.00004264
Iteration 110/1000 | Loss: 0.00004264
Iteration 111/1000 | Loss: 0.00080549
Iteration 112/1000 | Loss: 0.00005742
Iteration 113/1000 | Loss: 0.00004775
Iteration 114/1000 | Loss: 0.00004261
Iteration 115/1000 | Loss: 0.00004153
Iteration 116/1000 | Loss: 0.00004064
Iteration 117/1000 | Loss: 0.00004011
Iteration 118/1000 | Loss: 0.00003991
Iteration 119/1000 | Loss: 0.00003985
Iteration 120/1000 | Loss: 0.00003985
Iteration 121/1000 | Loss: 0.00003985
Iteration 122/1000 | Loss: 0.00003984
Iteration 123/1000 | Loss: 0.00003983
Iteration 124/1000 | Loss: 0.00003982
Iteration 125/1000 | Loss: 0.00003980
Iteration 126/1000 | Loss: 0.00003980
Iteration 127/1000 | Loss: 0.00003980
Iteration 128/1000 | Loss: 0.00003980
Iteration 129/1000 | Loss: 0.00003980
Iteration 130/1000 | Loss: 0.00003980
Iteration 131/1000 | Loss: 0.00003980
Iteration 132/1000 | Loss: 0.00003979
Iteration 133/1000 | Loss: 0.00003979
Iteration 134/1000 | Loss: 0.00003979
Iteration 135/1000 | Loss: 0.00003979
Iteration 136/1000 | Loss: 0.00003979
Iteration 137/1000 | Loss: 0.00003979
Iteration 138/1000 | Loss: 0.00003975
Iteration 139/1000 | Loss: 0.00003975
Iteration 140/1000 | Loss: 0.00003974
Iteration 141/1000 | Loss: 0.00003973
Iteration 142/1000 | Loss: 0.00003973
Iteration 143/1000 | Loss: 0.00003973
Iteration 144/1000 | Loss: 0.00003972
Iteration 145/1000 | Loss: 0.00003972
Iteration 146/1000 | Loss: 0.00003972
Iteration 147/1000 | Loss: 0.00003971
Iteration 148/1000 | Loss: 0.00003971
Iteration 149/1000 | Loss: 0.00003970
Iteration 150/1000 | Loss: 0.00003970
Iteration 151/1000 | Loss: 0.00003970
Iteration 152/1000 | Loss: 0.00003969
Iteration 153/1000 | Loss: 0.00003968
Iteration 154/1000 | Loss: 0.00003968
Iteration 155/1000 | Loss: 0.00003968
Iteration 156/1000 | Loss: 0.00003968
Iteration 157/1000 | Loss: 0.00003968
Iteration 158/1000 | Loss: 0.00003967
Iteration 159/1000 | Loss: 0.00003967
Iteration 160/1000 | Loss: 0.00003967
Iteration 161/1000 | Loss: 0.00003967
Iteration 162/1000 | Loss: 0.00003967
Iteration 163/1000 | Loss: 0.00003967
Iteration 164/1000 | Loss: 0.00003967
Iteration 165/1000 | Loss: 0.00003967
Iteration 166/1000 | Loss: 0.00003967
Iteration 167/1000 | Loss: 0.00003967
Iteration 168/1000 | Loss: 0.00003966
Iteration 169/1000 | Loss: 0.00003966
Iteration 170/1000 | Loss: 0.00003966
Iteration 171/1000 | Loss: 0.00003965
Iteration 172/1000 | Loss: 0.00003965
Iteration 173/1000 | Loss: 0.00003965
Iteration 174/1000 | Loss: 0.00003964
Iteration 175/1000 | Loss: 0.00003964
Iteration 176/1000 | Loss: 0.00003964
Iteration 177/1000 | Loss: 0.00003964
Iteration 178/1000 | Loss: 0.00003963
Iteration 179/1000 | Loss: 0.00003963
Iteration 180/1000 | Loss: 0.00003963
Iteration 181/1000 | Loss: 0.00003963
Iteration 182/1000 | Loss: 0.00003963
Iteration 183/1000 | Loss: 0.00003963
Iteration 184/1000 | Loss: 0.00003963
Iteration 185/1000 | Loss: 0.00003963
Iteration 186/1000 | Loss: 0.00003963
Iteration 187/1000 | Loss: 0.00003963
Iteration 188/1000 | Loss: 0.00003962
Iteration 189/1000 | Loss: 0.00003962
Iteration 190/1000 | Loss: 0.00003962
Iteration 191/1000 | Loss: 0.00003962
Iteration 192/1000 | Loss: 0.00003962
Iteration 193/1000 | Loss: 0.00003962
Iteration 194/1000 | Loss: 0.00003962
Iteration 195/1000 | Loss: 0.00003961
Iteration 196/1000 | Loss: 0.00003961
Iteration 197/1000 | Loss: 0.00003961
Iteration 198/1000 | Loss: 0.00003961
Iteration 199/1000 | Loss: 0.00003961
Iteration 200/1000 | Loss: 0.00003961
Iteration 201/1000 | Loss: 0.00003960
Iteration 202/1000 | Loss: 0.00003960
Iteration 203/1000 | Loss: 0.00003960
Iteration 204/1000 | Loss: 0.00003959
Iteration 205/1000 | Loss: 0.00003959
Iteration 206/1000 | Loss: 0.00003959
Iteration 207/1000 | Loss: 0.00003959
Iteration 208/1000 | Loss: 0.00003958
Iteration 209/1000 | Loss: 0.00003958
Iteration 210/1000 | Loss: 0.00003958
Iteration 211/1000 | Loss: 0.00003958
Iteration 212/1000 | Loss: 0.00003958
Iteration 213/1000 | Loss: 0.00003957
Iteration 214/1000 | Loss: 0.00003957
Iteration 215/1000 | Loss: 0.00003957
Iteration 216/1000 | Loss: 0.00003957
Iteration 217/1000 | Loss: 0.00003956
Iteration 218/1000 | Loss: 0.00003956
Iteration 219/1000 | Loss: 0.00003956
Iteration 220/1000 | Loss: 0.00003955
Iteration 221/1000 | Loss: 0.00003955
Iteration 222/1000 | Loss: 0.00003955
Iteration 223/1000 | Loss: 0.00003955
Iteration 224/1000 | Loss: 0.00003955
Iteration 225/1000 | Loss: 0.00003954
Iteration 226/1000 | Loss: 0.00003954
Iteration 227/1000 | Loss: 0.00003954
Iteration 228/1000 | Loss: 0.00003953
Iteration 229/1000 | Loss: 0.00003953
Iteration 230/1000 | Loss: 0.00003953
Iteration 231/1000 | Loss: 0.00003953
Iteration 232/1000 | Loss: 0.00003952
Iteration 233/1000 | Loss: 0.00003952
Iteration 234/1000 | Loss: 0.00003952
Iteration 235/1000 | Loss: 0.00003952
Iteration 236/1000 | Loss: 0.00003952
Iteration 237/1000 | Loss: 0.00003952
Iteration 238/1000 | Loss: 0.00003952
Iteration 239/1000 | Loss: 0.00003952
Iteration 240/1000 | Loss: 0.00003952
Iteration 241/1000 | Loss: 0.00003951
Iteration 242/1000 | Loss: 0.00003951
Iteration 243/1000 | Loss: 0.00003950
Iteration 244/1000 | Loss: 0.00003950
Iteration 245/1000 | Loss: 0.00003950
Iteration 246/1000 | Loss: 0.00003949
Iteration 247/1000 | Loss: 0.00003949
Iteration 248/1000 | Loss: 0.00003948
Iteration 249/1000 | Loss: 0.00003948
Iteration 250/1000 | Loss: 0.00003947
Iteration 251/1000 | Loss: 0.00003947
Iteration 252/1000 | Loss: 0.00003946
Iteration 253/1000 | Loss: 0.00003945
Iteration 254/1000 | Loss: 0.00003941
Iteration 255/1000 | Loss: 0.00003938
Iteration 256/1000 | Loss: 0.00003938
Iteration 257/1000 | Loss: 0.00003937
Iteration 258/1000 | Loss: 0.00003937
Iteration 259/1000 | Loss: 0.00003937
Iteration 260/1000 | Loss: 0.00003936
Iteration 261/1000 | Loss: 0.00003936
Iteration 262/1000 | Loss: 0.00003935
Iteration 263/1000 | Loss: 0.00003935
Iteration 264/1000 | Loss: 0.00003934
Iteration 265/1000 | Loss: 0.00003934
Iteration 266/1000 | Loss: 0.00003930
Iteration 267/1000 | Loss: 0.00003929
Iteration 268/1000 | Loss: 0.00003929
Iteration 269/1000 | Loss: 0.00003928
Iteration 270/1000 | Loss: 0.00003927
Iteration 271/1000 | Loss: 0.00003927
Iteration 272/1000 | Loss: 0.00003927
Iteration 273/1000 | Loss: 0.00003927
Iteration 274/1000 | Loss: 0.00003927
Iteration 275/1000 | Loss: 0.00003926
Iteration 276/1000 | Loss: 0.00003926
Iteration 277/1000 | Loss: 0.00003926
Iteration 278/1000 | Loss: 0.00003926
Iteration 279/1000 | Loss: 0.00003926
Iteration 280/1000 | Loss: 0.00003926
Iteration 281/1000 | Loss: 0.00003926
Iteration 282/1000 | Loss: 0.00003926
Iteration 283/1000 | Loss: 0.00003926
Iteration 284/1000 | Loss: 0.00003926
Iteration 285/1000 | Loss: 0.00003926
Iteration 286/1000 | Loss: 0.00003926
Iteration 287/1000 | Loss: 0.00003925
Iteration 288/1000 | Loss: 0.00003925
Iteration 289/1000 | Loss: 0.00003925
Iteration 290/1000 | Loss: 0.00003925
Iteration 291/1000 | Loss: 0.00003925
Iteration 292/1000 | Loss: 0.00003924
Iteration 293/1000 | Loss: 0.00003924
Iteration 294/1000 | Loss: 0.00003924
Iteration 295/1000 | Loss: 0.00003924
Iteration 296/1000 | Loss: 0.00003923
Iteration 297/1000 | Loss: 0.00003923
Iteration 298/1000 | Loss: 0.00003923
Iteration 299/1000 | Loss: 0.00003923
Iteration 300/1000 | Loss: 0.00003923
Iteration 301/1000 | Loss: 0.00003923
Iteration 302/1000 | Loss: 0.00003923
Iteration 303/1000 | Loss: 0.00003923
Iteration 304/1000 | Loss: 0.00003923
Iteration 305/1000 | Loss: 0.00003923
Iteration 306/1000 | Loss: 0.00003923
Iteration 307/1000 | Loss: 0.00003923
Iteration 308/1000 | Loss: 0.00003923
Iteration 309/1000 | Loss: 0.00003922
Iteration 310/1000 | Loss: 0.00003922
Iteration 311/1000 | Loss: 0.00003922
Iteration 312/1000 | Loss: 0.00003922
Iteration 313/1000 | Loss: 0.00003922
Iteration 314/1000 | Loss: 0.00003922
Iteration 315/1000 | Loss: 0.00003922
Iteration 316/1000 | Loss: 0.00003922
Iteration 317/1000 | Loss: 0.00003922
Iteration 318/1000 | Loss: 0.00003921
Iteration 319/1000 | Loss: 0.00003921
Iteration 320/1000 | Loss: 0.00003921
Iteration 321/1000 | Loss: 0.00003921
Iteration 322/1000 | Loss: 0.00003921
Iteration 323/1000 | Loss: 0.00003921
Iteration 324/1000 | Loss: 0.00003920
Iteration 325/1000 | Loss: 0.00003920
Iteration 326/1000 | Loss: 0.00003920
Iteration 327/1000 | Loss: 0.00003919
Iteration 328/1000 | Loss: 0.00003919
Iteration 329/1000 | Loss: 0.00003919
Iteration 330/1000 | Loss: 0.00003918
Iteration 331/1000 | Loss: 0.00003918
Iteration 332/1000 | Loss: 0.00003918
Iteration 333/1000 | Loss: 0.00003918
Iteration 334/1000 | Loss: 0.00003918
Iteration 335/1000 | Loss: 0.00003918
Iteration 336/1000 | Loss: 0.00003918
Iteration 337/1000 | Loss: 0.00003918
Iteration 338/1000 | Loss: 0.00003918
Iteration 339/1000 | Loss: 0.00003918
Iteration 340/1000 | Loss: 0.00003918
Iteration 341/1000 | Loss: 0.00003918
Iteration 342/1000 | Loss: 0.00003917
Iteration 343/1000 | Loss: 0.00003917
Iteration 344/1000 | Loss: 0.00003917
Iteration 345/1000 | Loss: 0.00003917
Iteration 346/1000 | Loss: 0.00003917
Iteration 347/1000 | Loss: 0.00003917
Iteration 348/1000 | Loss: 0.00003917
Iteration 349/1000 | Loss: 0.00003917
Iteration 350/1000 | Loss: 0.00003917
Iteration 351/1000 | Loss: 0.00003916
Iteration 352/1000 | Loss: 0.00003916
Iteration 353/1000 | Loss: 0.00003916
Iteration 354/1000 | Loss: 0.00003916
Iteration 355/1000 | Loss: 0.00003916
Iteration 356/1000 | Loss: 0.00003916
Iteration 357/1000 | Loss: 0.00003915
Iteration 358/1000 | Loss: 0.00003915
Iteration 359/1000 | Loss: 0.00003915
Iteration 360/1000 | Loss: 0.00003915
Iteration 361/1000 | Loss: 0.00003915
Iteration 362/1000 | Loss: 0.00003915
Iteration 363/1000 | Loss: 0.00003915
Iteration 364/1000 | Loss: 0.00003915
Iteration 365/1000 | Loss: 0.00003915
Iteration 366/1000 | Loss: 0.00003915
Iteration 367/1000 | Loss: 0.00003915
Iteration 368/1000 | Loss: 0.00003915
Iteration 369/1000 | Loss: 0.00003915
Iteration 370/1000 | Loss: 0.00003915
Iteration 371/1000 | Loss: 0.00003914
Iteration 372/1000 | Loss: 0.00003914
Iteration 373/1000 | Loss: 0.00003914
Iteration 374/1000 | Loss: 0.00003914
Iteration 375/1000 | Loss: 0.00003914
Iteration 376/1000 | Loss: 0.00003914
Iteration 377/1000 | Loss: 0.00003914
Iteration 378/1000 | Loss: 0.00003914
Iteration 379/1000 | Loss: 0.00003914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 379. Stopping optimization.
Last 5 losses: [3.9143909816630185e-05, 3.9143909816630185e-05, 3.9143909816630185e-05, 3.9143909816630185e-05, 3.9143909816630185e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9143909816630185e-05

Optimization complete. Final v2v error: 4.3990159034729 mm

Highest mean error: 12.059366226196289 mm for frame 149

Lowest mean error: 3.231718063354492 mm for frame 152

Saving results

Total time: 180.19149351119995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810980
Iteration 2/25 | Loss: 0.00135490
Iteration 3/25 | Loss: 0.00097631
Iteration 4/25 | Loss: 0.00092530
Iteration 5/25 | Loss: 0.00091784
Iteration 6/25 | Loss: 0.00091643
Iteration 7/25 | Loss: 0.00091626
Iteration 8/25 | Loss: 0.00091626
Iteration 9/25 | Loss: 0.00091626
Iteration 10/25 | Loss: 0.00091626
Iteration 11/25 | Loss: 0.00091626
Iteration 12/25 | Loss: 0.00091626
Iteration 13/25 | Loss: 0.00091626
Iteration 14/25 | Loss: 0.00091626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000916259188670665, 0.000916259188670665, 0.000916259188670665, 0.000916259188670665, 0.000916259188670665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000916259188670665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50528145
Iteration 2/25 | Loss: 0.00051024
Iteration 3/25 | Loss: 0.00051021
Iteration 4/25 | Loss: 0.00051020
Iteration 5/25 | Loss: 0.00051020
Iteration 6/25 | Loss: 0.00051020
Iteration 7/25 | Loss: 0.00051020
Iteration 8/25 | Loss: 0.00051020
Iteration 9/25 | Loss: 0.00051020
Iteration 10/25 | Loss: 0.00051020
Iteration 11/25 | Loss: 0.00051020
Iteration 12/25 | Loss: 0.00051020
Iteration 13/25 | Loss: 0.00051020
Iteration 14/25 | Loss: 0.00051020
Iteration 15/25 | Loss: 0.00051020
Iteration 16/25 | Loss: 0.00051020
Iteration 17/25 | Loss: 0.00051020
Iteration 18/25 | Loss: 0.00051020
Iteration 19/25 | Loss: 0.00051020
Iteration 20/25 | Loss: 0.00051020
Iteration 21/25 | Loss: 0.00051020
Iteration 22/25 | Loss: 0.00051020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005102019058540463, 0.0005102019058540463, 0.0005102019058540463, 0.0005102019058540463, 0.0005102019058540463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005102019058540463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051020
Iteration 2/1000 | Loss: 0.00004968
Iteration 3/1000 | Loss: 0.00003348
Iteration 4/1000 | Loss: 0.00003079
Iteration 5/1000 | Loss: 0.00002975
Iteration 6/1000 | Loss: 0.00002912
Iteration 7/1000 | Loss: 0.00002863
Iteration 8/1000 | Loss: 0.00002824
Iteration 9/1000 | Loss: 0.00002801
Iteration 10/1000 | Loss: 0.00002786
Iteration 11/1000 | Loss: 0.00002782
Iteration 12/1000 | Loss: 0.00002782
Iteration 13/1000 | Loss: 0.00002782
Iteration 14/1000 | Loss: 0.00002781
Iteration 15/1000 | Loss: 0.00002778
Iteration 16/1000 | Loss: 0.00002778
Iteration 17/1000 | Loss: 0.00002777
Iteration 18/1000 | Loss: 0.00002777
Iteration 19/1000 | Loss: 0.00002773
Iteration 20/1000 | Loss: 0.00002771
Iteration 21/1000 | Loss: 0.00002769
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00002768
Iteration 24/1000 | Loss: 0.00002768
Iteration 25/1000 | Loss: 0.00002767
Iteration 26/1000 | Loss: 0.00002767
Iteration 27/1000 | Loss: 0.00002766
Iteration 28/1000 | Loss: 0.00002766
Iteration 29/1000 | Loss: 0.00002765
Iteration 30/1000 | Loss: 0.00002765
Iteration 31/1000 | Loss: 0.00002765
Iteration 32/1000 | Loss: 0.00002765
Iteration 33/1000 | Loss: 0.00002765
Iteration 34/1000 | Loss: 0.00002765
Iteration 35/1000 | Loss: 0.00002765
Iteration 36/1000 | Loss: 0.00002765
Iteration 37/1000 | Loss: 0.00002764
Iteration 38/1000 | Loss: 0.00002764
Iteration 39/1000 | Loss: 0.00002764
Iteration 40/1000 | Loss: 0.00002764
Iteration 41/1000 | Loss: 0.00002764
Iteration 42/1000 | Loss: 0.00002764
Iteration 43/1000 | Loss: 0.00002764
Iteration 44/1000 | Loss: 0.00002764
Iteration 45/1000 | Loss: 0.00002764
Iteration 46/1000 | Loss: 0.00002764
Iteration 47/1000 | Loss: 0.00002764
Iteration 48/1000 | Loss: 0.00002763
Iteration 49/1000 | Loss: 0.00002763
Iteration 50/1000 | Loss: 0.00002763
Iteration 51/1000 | Loss: 0.00002763
Iteration 52/1000 | Loss: 0.00002763
Iteration 53/1000 | Loss: 0.00002763
Iteration 54/1000 | Loss: 0.00002763
Iteration 55/1000 | Loss: 0.00002763
Iteration 56/1000 | Loss: 0.00002763
Iteration 57/1000 | Loss: 0.00002763
Iteration 58/1000 | Loss: 0.00002763
Iteration 59/1000 | Loss: 0.00002763
Iteration 60/1000 | Loss: 0.00002763
Iteration 61/1000 | Loss: 0.00002762
Iteration 62/1000 | Loss: 0.00002762
Iteration 63/1000 | Loss: 0.00002762
Iteration 64/1000 | Loss: 0.00002762
Iteration 65/1000 | Loss: 0.00002762
Iteration 66/1000 | Loss: 0.00002762
Iteration 67/1000 | Loss: 0.00002762
Iteration 68/1000 | Loss: 0.00002762
Iteration 69/1000 | Loss: 0.00002762
Iteration 70/1000 | Loss: 0.00002762
Iteration 71/1000 | Loss: 0.00002762
Iteration 72/1000 | Loss: 0.00002762
Iteration 73/1000 | Loss: 0.00002762
Iteration 74/1000 | Loss: 0.00002762
Iteration 75/1000 | Loss: 0.00002762
Iteration 76/1000 | Loss: 0.00002762
Iteration 77/1000 | Loss: 0.00002762
Iteration 78/1000 | Loss: 0.00002762
Iteration 79/1000 | Loss: 0.00002761
Iteration 80/1000 | Loss: 0.00002761
Iteration 81/1000 | Loss: 0.00002761
Iteration 82/1000 | Loss: 0.00002761
Iteration 83/1000 | Loss: 0.00002761
Iteration 84/1000 | Loss: 0.00002761
Iteration 85/1000 | Loss: 0.00002761
Iteration 86/1000 | Loss: 0.00002761
Iteration 87/1000 | Loss: 0.00002761
Iteration 88/1000 | Loss: 0.00002761
Iteration 89/1000 | Loss: 0.00002761
Iteration 90/1000 | Loss: 0.00002761
Iteration 91/1000 | Loss: 0.00002761
Iteration 92/1000 | Loss: 0.00002761
Iteration 93/1000 | Loss: 0.00002760
Iteration 94/1000 | Loss: 0.00002760
Iteration 95/1000 | Loss: 0.00002760
Iteration 96/1000 | Loss: 0.00002760
Iteration 97/1000 | Loss: 0.00002760
Iteration 98/1000 | Loss: 0.00002760
Iteration 99/1000 | Loss: 0.00002760
Iteration 100/1000 | Loss: 0.00002760
Iteration 101/1000 | Loss: 0.00002760
Iteration 102/1000 | Loss: 0.00002759
Iteration 103/1000 | Loss: 0.00002759
Iteration 104/1000 | Loss: 0.00002759
Iteration 105/1000 | Loss: 0.00002759
Iteration 106/1000 | Loss: 0.00002759
Iteration 107/1000 | Loss: 0.00002759
Iteration 108/1000 | Loss: 0.00002758
Iteration 109/1000 | Loss: 0.00002758
Iteration 110/1000 | Loss: 0.00002758
Iteration 111/1000 | Loss: 0.00002758
Iteration 112/1000 | Loss: 0.00002758
Iteration 113/1000 | Loss: 0.00002758
Iteration 114/1000 | Loss: 0.00002758
Iteration 115/1000 | Loss: 0.00002758
Iteration 116/1000 | Loss: 0.00002758
Iteration 117/1000 | Loss: 0.00002758
Iteration 118/1000 | Loss: 0.00002757
Iteration 119/1000 | Loss: 0.00002757
Iteration 120/1000 | Loss: 0.00002757
Iteration 121/1000 | Loss: 0.00002757
Iteration 122/1000 | Loss: 0.00002757
Iteration 123/1000 | Loss: 0.00002757
Iteration 124/1000 | Loss: 0.00002757
Iteration 125/1000 | Loss: 0.00002757
Iteration 126/1000 | Loss: 0.00002757
Iteration 127/1000 | Loss: 0.00002757
Iteration 128/1000 | Loss: 0.00002756
Iteration 129/1000 | Loss: 0.00002756
Iteration 130/1000 | Loss: 0.00002756
Iteration 131/1000 | Loss: 0.00002756
Iteration 132/1000 | Loss: 0.00002756
Iteration 133/1000 | Loss: 0.00002755
Iteration 134/1000 | Loss: 0.00002755
Iteration 135/1000 | Loss: 0.00002755
Iteration 136/1000 | Loss: 0.00002755
Iteration 137/1000 | Loss: 0.00002755
Iteration 138/1000 | Loss: 0.00002754
Iteration 139/1000 | Loss: 0.00002754
Iteration 140/1000 | Loss: 0.00002754
Iteration 141/1000 | Loss: 0.00002754
Iteration 142/1000 | Loss: 0.00002754
Iteration 143/1000 | Loss: 0.00002753
Iteration 144/1000 | Loss: 0.00002753
Iteration 145/1000 | Loss: 0.00002753
Iteration 146/1000 | Loss: 0.00002753
Iteration 147/1000 | Loss: 0.00002753
Iteration 148/1000 | Loss: 0.00002752
Iteration 149/1000 | Loss: 0.00002752
Iteration 150/1000 | Loss: 0.00002752
Iteration 151/1000 | Loss: 0.00002752
Iteration 152/1000 | Loss: 0.00002752
Iteration 153/1000 | Loss: 0.00002752
Iteration 154/1000 | Loss: 0.00002752
Iteration 155/1000 | Loss: 0.00002751
Iteration 156/1000 | Loss: 0.00002751
Iteration 157/1000 | Loss: 0.00002751
Iteration 158/1000 | Loss: 0.00002751
Iteration 159/1000 | Loss: 0.00002750
Iteration 160/1000 | Loss: 0.00002750
Iteration 161/1000 | Loss: 0.00002750
Iteration 162/1000 | Loss: 0.00002750
Iteration 163/1000 | Loss: 0.00002749
Iteration 164/1000 | Loss: 0.00002749
Iteration 165/1000 | Loss: 0.00002749
Iteration 166/1000 | Loss: 0.00002749
Iteration 167/1000 | Loss: 0.00002749
Iteration 168/1000 | Loss: 0.00002749
Iteration 169/1000 | Loss: 0.00002749
Iteration 170/1000 | Loss: 0.00002749
Iteration 171/1000 | Loss: 0.00002749
Iteration 172/1000 | Loss: 0.00002748
Iteration 173/1000 | Loss: 0.00002748
Iteration 174/1000 | Loss: 0.00002748
Iteration 175/1000 | Loss: 0.00002748
Iteration 176/1000 | Loss: 0.00002748
Iteration 177/1000 | Loss: 0.00002748
Iteration 178/1000 | Loss: 0.00002748
Iteration 179/1000 | Loss: 0.00002748
Iteration 180/1000 | Loss: 0.00002748
Iteration 181/1000 | Loss: 0.00002747
Iteration 182/1000 | Loss: 0.00002747
Iteration 183/1000 | Loss: 0.00002747
Iteration 184/1000 | Loss: 0.00002747
Iteration 185/1000 | Loss: 0.00002747
Iteration 186/1000 | Loss: 0.00002747
Iteration 187/1000 | Loss: 0.00002747
Iteration 188/1000 | Loss: 0.00002747
Iteration 189/1000 | Loss: 0.00002746
Iteration 190/1000 | Loss: 0.00002746
Iteration 191/1000 | Loss: 0.00002746
Iteration 192/1000 | Loss: 0.00002746
Iteration 193/1000 | Loss: 0.00002746
Iteration 194/1000 | Loss: 0.00002746
Iteration 195/1000 | Loss: 0.00002746
Iteration 196/1000 | Loss: 0.00002746
Iteration 197/1000 | Loss: 0.00002746
Iteration 198/1000 | Loss: 0.00002746
Iteration 199/1000 | Loss: 0.00002745
Iteration 200/1000 | Loss: 0.00002745
Iteration 201/1000 | Loss: 0.00002745
Iteration 202/1000 | Loss: 0.00002745
Iteration 203/1000 | Loss: 0.00002744
Iteration 204/1000 | Loss: 0.00002744
Iteration 205/1000 | Loss: 0.00002744
Iteration 206/1000 | Loss: 0.00002744
Iteration 207/1000 | Loss: 0.00002744
Iteration 208/1000 | Loss: 0.00002744
Iteration 209/1000 | Loss: 0.00002744
Iteration 210/1000 | Loss: 0.00002744
Iteration 211/1000 | Loss: 0.00002744
Iteration 212/1000 | Loss: 0.00002744
Iteration 213/1000 | Loss: 0.00002744
Iteration 214/1000 | Loss: 0.00002744
Iteration 215/1000 | Loss: 0.00002744
Iteration 216/1000 | Loss: 0.00002744
Iteration 217/1000 | Loss: 0.00002744
Iteration 218/1000 | Loss: 0.00002744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.7437727112555876e-05, 2.7437727112555876e-05, 2.7437727112555876e-05, 2.7437727112555876e-05, 2.7437727112555876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7437727112555876e-05

Optimization complete. Final v2v error: 4.199263095855713 mm

Highest mean error: 4.565486431121826 mm for frame 80

Lowest mean error: 3.2610318660736084 mm for frame 4

Saving results

Total time: 38.23123288154602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021506
Iteration 2/25 | Loss: 0.00160783
Iteration 3/25 | Loss: 0.00101288
Iteration 4/25 | Loss: 0.00096585
Iteration 5/25 | Loss: 0.00095133
Iteration 6/25 | Loss: 0.00094732
Iteration 7/25 | Loss: 0.00094649
Iteration 8/25 | Loss: 0.00094638
Iteration 9/25 | Loss: 0.00094638
Iteration 10/25 | Loss: 0.00094638
Iteration 11/25 | Loss: 0.00094638
Iteration 12/25 | Loss: 0.00094638
Iteration 13/25 | Loss: 0.00094638
Iteration 14/25 | Loss: 0.00094638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009463765891268849, 0.0009463765891268849, 0.0009463765891268849, 0.0009463765891268849, 0.0009463765891268849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009463765891268849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89251232
Iteration 2/25 | Loss: 0.00054368
Iteration 3/25 | Loss: 0.00054366
Iteration 4/25 | Loss: 0.00054366
Iteration 5/25 | Loss: 0.00054366
Iteration 6/25 | Loss: 0.00054366
Iteration 7/25 | Loss: 0.00054366
Iteration 8/25 | Loss: 0.00054366
Iteration 9/25 | Loss: 0.00054366
Iteration 10/25 | Loss: 0.00054366
Iteration 11/25 | Loss: 0.00054366
Iteration 12/25 | Loss: 0.00054366
Iteration 13/25 | Loss: 0.00054366
Iteration 14/25 | Loss: 0.00054366
Iteration 15/25 | Loss: 0.00054366
Iteration 16/25 | Loss: 0.00054366
Iteration 17/25 | Loss: 0.00054366
Iteration 18/25 | Loss: 0.00054366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005436610081233084, 0.0005436610081233084, 0.0005436610081233084, 0.0005436610081233084, 0.0005436610081233084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005436610081233084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054366
Iteration 2/1000 | Loss: 0.00007653
Iteration 3/1000 | Loss: 0.00004463
Iteration 4/1000 | Loss: 0.00004011
Iteration 5/1000 | Loss: 0.00003823
Iteration 6/1000 | Loss: 0.00003724
Iteration 7/1000 | Loss: 0.00003667
Iteration 8/1000 | Loss: 0.00003615
Iteration 9/1000 | Loss: 0.00003563
Iteration 10/1000 | Loss: 0.00003526
Iteration 11/1000 | Loss: 0.00003496
Iteration 12/1000 | Loss: 0.00003474
Iteration 13/1000 | Loss: 0.00003450
Iteration 14/1000 | Loss: 0.00003431
Iteration 15/1000 | Loss: 0.00003417
Iteration 16/1000 | Loss: 0.00003405
Iteration 17/1000 | Loss: 0.00003399
Iteration 18/1000 | Loss: 0.00003393
Iteration 19/1000 | Loss: 0.00003391
Iteration 20/1000 | Loss: 0.00003385
Iteration 21/1000 | Loss: 0.00003378
Iteration 22/1000 | Loss: 0.00003377
Iteration 23/1000 | Loss: 0.00003375
Iteration 24/1000 | Loss: 0.00003373
Iteration 25/1000 | Loss: 0.00003373
Iteration 26/1000 | Loss: 0.00003370
Iteration 27/1000 | Loss: 0.00003369
Iteration 28/1000 | Loss: 0.00003369
Iteration 29/1000 | Loss: 0.00003368
Iteration 30/1000 | Loss: 0.00003368
Iteration 31/1000 | Loss: 0.00003368
Iteration 32/1000 | Loss: 0.00003368
Iteration 33/1000 | Loss: 0.00003367
Iteration 34/1000 | Loss: 0.00003367
Iteration 35/1000 | Loss: 0.00003367
Iteration 36/1000 | Loss: 0.00003367
Iteration 37/1000 | Loss: 0.00003367
Iteration 38/1000 | Loss: 0.00003366
Iteration 39/1000 | Loss: 0.00003366
Iteration 40/1000 | Loss: 0.00003366
Iteration 41/1000 | Loss: 0.00003365
Iteration 42/1000 | Loss: 0.00003365
Iteration 43/1000 | Loss: 0.00003363
Iteration 44/1000 | Loss: 0.00003363
Iteration 45/1000 | Loss: 0.00003363
Iteration 46/1000 | Loss: 0.00003363
Iteration 47/1000 | Loss: 0.00003363
Iteration 48/1000 | Loss: 0.00003362
Iteration 49/1000 | Loss: 0.00003360
Iteration 50/1000 | Loss: 0.00003360
Iteration 51/1000 | Loss: 0.00003358
Iteration 52/1000 | Loss: 0.00003358
Iteration 53/1000 | Loss: 0.00003357
Iteration 54/1000 | Loss: 0.00003355
Iteration 55/1000 | Loss: 0.00003355
Iteration 56/1000 | Loss: 0.00003355
Iteration 57/1000 | Loss: 0.00003355
Iteration 58/1000 | Loss: 0.00003355
Iteration 59/1000 | Loss: 0.00003355
Iteration 60/1000 | Loss: 0.00003355
Iteration 61/1000 | Loss: 0.00003354
Iteration 62/1000 | Loss: 0.00003354
Iteration 63/1000 | Loss: 0.00003354
Iteration 64/1000 | Loss: 0.00003354
Iteration 65/1000 | Loss: 0.00003354
Iteration 66/1000 | Loss: 0.00003354
Iteration 67/1000 | Loss: 0.00003354
Iteration 68/1000 | Loss: 0.00003354
Iteration 69/1000 | Loss: 0.00003353
Iteration 70/1000 | Loss: 0.00003352
Iteration 71/1000 | Loss: 0.00003352
Iteration 72/1000 | Loss: 0.00003351
Iteration 73/1000 | Loss: 0.00003351
Iteration 74/1000 | Loss: 0.00003351
Iteration 75/1000 | Loss: 0.00003351
Iteration 76/1000 | Loss: 0.00003351
Iteration 77/1000 | Loss: 0.00003351
Iteration 78/1000 | Loss: 0.00003350
Iteration 79/1000 | Loss: 0.00003350
Iteration 80/1000 | Loss: 0.00003350
Iteration 81/1000 | Loss: 0.00003350
Iteration 82/1000 | Loss: 0.00003349
Iteration 83/1000 | Loss: 0.00003349
Iteration 84/1000 | Loss: 0.00003349
Iteration 85/1000 | Loss: 0.00003349
Iteration 86/1000 | Loss: 0.00003348
Iteration 87/1000 | Loss: 0.00003348
Iteration 88/1000 | Loss: 0.00003348
Iteration 89/1000 | Loss: 0.00003348
Iteration 90/1000 | Loss: 0.00003348
Iteration 91/1000 | Loss: 0.00003348
Iteration 92/1000 | Loss: 0.00003348
Iteration 93/1000 | Loss: 0.00003348
Iteration 94/1000 | Loss: 0.00003348
Iteration 95/1000 | Loss: 0.00003348
Iteration 96/1000 | Loss: 0.00003348
Iteration 97/1000 | Loss: 0.00003348
Iteration 98/1000 | Loss: 0.00003348
Iteration 99/1000 | Loss: 0.00003347
Iteration 100/1000 | Loss: 0.00003347
Iteration 101/1000 | Loss: 0.00003347
Iteration 102/1000 | Loss: 0.00003346
Iteration 103/1000 | Loss: 0.00003346
Iteration 104/1000 | Loss: 0.00003346
Iteration 105/1000 | Loss: 0.00003345
Iteration 106/1000 | Loss: 0.00003345
Iteration 107/1000 | Loss: 0.00003345
Iteration 108/1000 | Loss: 0.00003345
Iteration 109/1000 | Loss: 0.00003345
Iteration 110/1000 | Loss: 0.00003344
Iteration 111/1000 | Loss: 0.00003344
Iteration 112/1000 | Loss: 0.00003344
Iteration 113/1000 | Loss: 0.00003344
Iteration 114/1000 | Loss: 0.00003344
Iteration 115/1000 | Loss: 0.00003344
Iteration 116/1000 | Loss: 0.00003344
Iteration 117/1000 | Loss: 0.00003344
Iteration 118/1000 | Loss: 0.00003344
Iteration 119/1000 | Loss: 0.00003343
Iteration 120/1000 | Loss: 0.00003343
Iteration 121/1000 | Loss: 0.00003343
Iteration 122/1000 | Loss: 0.00003343
Iteration 123/1000 | Loss: 0.00003343
Iteration 124/1000 | Loss: 0.00003342
Iteration 125/1000 | Loss: 0.00003342
Iteration 126/1000 | Loss: 0.00003342
Iteration 127/1000 | Loss: 0.00003342
Iteration 128/1000 | Loss: 0.00003342
Iteration 129/1000 | Loss: 0.00003342
Iteration 130/1000 | Loss: 0.00003342
Iteration 131/1000 | Loss: 0.00003342
Iteration 132/1000 | Loss: 0.00003342
Iteration 133/1000 | Loss: 0.00003342
Iteration 134/1000 | Loss: 0.00003341
Iteration 135/1000 | Loss: 0.00003341
Iteration 136/1000 | Loss: 0.00003341
Iteration 137/1000 | Loss: 0.00003341
Iteration 138/1000 | Loss: 0.00003340
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003340
Iteration 141/1000 | Loss: 0.00003340
Iteration 142/1000 | Loss: 0.00003340
Iteration 143/1000 | Loss: 0.00003340
Iteration 144/1000 | Loss: 0.00003340
Iteration 145/1000 | Loss: 0.00003340
Iteration 146/1000 | Loss: 0.00003340
Iteration 147/1000 | Loss: 0.00003339
Iteration 148/1000 | Loss: 0.00003339
Iteration 149/1000 | Loss: 0.00003339
Iteration 150/1000 | Loss: 0.00003339
Iteration 151/1000 | Loss: 0.00003339
Iteration 152/1000 | Loss: 0.00003339
Iteration 153/1000 | Loss: 0.00003339
Iteration 154/1000 | Loss: 0.00003339
Iteration 155/1000 | Loss: 0.00003339
Iteration 156/1000 | Loss: 0.00003339
Iteration 157/1000 | Loss: 0.00003339
Iteration 158/1000 | Loss: 0.00003339
Iteration 159/1000 | Loss: 0.00003339
Iteration 160/1000 | Loss: 0.00003339
Iteration 161/1000 | Loss: 0.00003338
Iteration 162/1000 | Loss: 0.00003338
Iteration 163/1000 | Loss: 0.00003338
Iteration 164/1000 | Loss: 0.00003338
Iteration 165/1000 | Loss: 0.00003338
Iteration 166/1000 | Loss: 0.00003338
Iteration 167/1000 | Loss: 0.00003338
Iteration 168/1000 | Loss: 0.00003338
Iteration 169/1000 | Loss: 0.00003338
Iteration 170/1000 | Loss: 0.00003338
Iteration 171/1000 | Loss: 0.00003338
Iteration 172/1000 | Loss: 0.00003337
Iteration 173/1000 | Loss: 0.00003337
Iteration 174/1000 | Loss: 0.00003337
Iteration 175/1000 | Loss: 0.00003337
Iteration 176/1000 | Loss: 0.00003337
Iteration 177/1000 | Loss: 0.00003337
Iteration 178/1000 | Loss: 0.00003337
Iteration 179/1000 | Loss: 0.00003337
Iteration 180/1000 | Loss: 0.00003337
Iteration 181/1000 | Loss: 0.00003337
Iteration 182/1000 | Loss: 0.00003337
Iteration 183/1000 | Loss: 0.00003336
Iteration 184/1000 | Loss: 0.00003336
Iteration 185/1000 | Loss: 0.00003336
Iteration 186/1000 | Loss: 0.00003336
Iteration 187/1000 | Loss: 0.00003336
Iteration 188/1000 | Loss: 0.00003336
Iteration 189/1000 | Loss: 0.00003336
Iteration 190/1000 | Loss: 0.00003336
Iteration 191/1000 | Loss: 0.00003336
Iteration 192/1000 | Loss: 0.00003336
Iteration 193/1000 | Loss: 0.00003336
Iteration 194/1000 | Loss: 0.00003336
Iteration 195/1000 | Loss: 0.00003336
Iteration 196/1000 | Loss: 0.00003336
Iteration 197/1000 | Loss: 0.00003336
Iteration 198/1000 | Loss: 0.00003336
Iteration 199/1000 | Loss: 0.00003336
Iteration 200/1000 | Loss: 0.00003336
Iteration 201/1000 | Loss: 0.00003336
Iteration 202/1000 | Loss: 0.00003336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.3360505767632276e-05, 3.3360505767632276e-05, 3.3360505767632276e-05, 3.3360505767632276e-05, 3.3360505767632276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3360505767632276e-05

Optimization complete. Final v2v error: 4.696007251739502 mm

Highest mean error: 5.506875991821289 mm for frame 45

Lowest mean error: 3.79384446144104 mm for frame 30

Saving results

Total time: 52.74606251716614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113865
Iteration 2/25 | Loss: 0.00145531
Iteration 3/25 | Loss: 0.00096658
Iteration 4/25 | Loss: 0.00089232
Iteration 5/25 | Loss: 0.00085767
Iteration 6/25 | Loss: 0.00085288
Iteration 7/25 | Loss: 0.00084852
Iteration 8/25 | Loss: 0.00084706
Iteration 9/25 | Loss: 0.00084166
Iteration 10/25 | Loss: 0.00084163
Iteration 11/25 | Loss: 0.00084163
Iteration 12/25 | Loss: 0.00084162
Iteration 13/25 | Loss: 0.00084162
Iteration 14/25 | Loss: 0.00084162
Iteration 15/25 | Loss: 0.00084162
Iteration 16/25 | Loss: 0.00084162
Iteration 17/25 | Loss: 0.00084162
Iteration 18/25 | Loss: 0.00084162
Iteration 19/25 | Loss: 0.00084162
Iteration 20/25 | Loss: 0.00084162
Iteration 21/25 | Loss: 0.00084162
Iteration 22/25 | Loss: 0.00084162
Iteration 23/25 | Loss: 0.00084162
Iteration 24/25 | Loss: 0.00084162
Iteration 25/25 | Loss: 0.00084162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60864198
Iteration 2/25 | Loss: 0.00056681
Iteration 3/25 | Loss: 0.00056678
Iteration 4/25 | Loss: 0.00056678
Iteration 5/25 | Loss: 0.00056678
Iteration 6/25 | Loss: 0.00056678
Iteration 7/25 | Loss: 0.00056678
Iteration 8/25 | Loss: 0.00056678
Iteration 9/25 | Loss: 0.00056678
Iteration 10/25 | Loss: 0.00056678
Iteration 11/25 | Loss: 0.00056678
Iteration 12/25 | Loss: 0.00056678
Iteration 13/25 | Loss: 0.00056678
Iteration 14/25 | Loss: 0.00056678
Iteration 15/25 | Loss: 0.00056678
Iteration 16/25 | Loss: 0.00056678
Iteration 17/25 | Loss: 0.00056678
Iteration 18/25 | Loss: 0.00056678
Iteration 19/25 | Loss: 0.00056678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000566782196983695, 0.000566782196983695, 0.000566782196983695, 0.000566782196983695, 0.000566782196983695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000566782196983695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056678
Iteration 2/1000 | Loss: 0.00004306
Iteration 3/1000 | Loss: 0.00003223
Iteration 4/1000 | Loss: 0.00002864
Iteration 5/1000 | Loss: 0.00002736
Iteration 6/1000 | Loss: 0.00002672
Iteration 7/1000 | Loss: 0.00002596
Iteration 8/1000 | Loss: 0.00002542
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00002484
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002461
Iteration 13/1000 | Loss: 0.00002455
Iteration 14/1000 | Loss: 0.00002444
Iteration 15/1000 | Loss: 0.00002444
Iteration 16/1000 | Loss: 0.00002444
Iteration 17/1000 | Loss: 0.00002444
Iteration 18/1000 | Loss: 0.00002444
Iteration 19/1000 | Loss: 0.00002443
Iteration 20/1000 | Loss: 0.00002443
Iteration 21/1000 | Loss: 0.00002443
Iteration 22/1000 | Loss: 0.00002443
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002442
Iteration 25/1000 | Loss: 0.00002442
Iteration 26/1000 | Loss: 0.00002442
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002442
Iteration 29/1000 | Loss: 0.00002442
Iteration 30/1000 | Loss: 0.00002442
Iteration 31/1000 | Loss: 0.00002442
Iteration 32/1000 | Loss: 0.00002441
Iteration 33/1000 | Loss: 0.00002441
Iteration 34/1000 | Loss: 0.00002441
Iteration 35/1000 | Loss: 0.00002441
Iteration 36/1000 | Loss: 0.00002441
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002440
Iteration 39/1000 | Loss: 0.00002440
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002439
Iteration 42/1000 | Loss: 0.00002438
Iteration 43/1000 | Loss: 0.00002438
Iteration 44/1000 | Loss: 0.00002438
Iteration 45/1000 | Loss: 0.00002438
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002438
Iteration 50/1000 | Loss: 0.00002438
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002437
Iteration 53/1000 | Loss: 0.00002437
Iteration 54/1000 | Loss: 0.00002437
Iteration 55/1000 | Loss: 0.00002437
Iteration 56/1000 | Loss: 0.00002437
Iteration 57/1000 | Loss: 0.00002437
Iteration 58/1000 | Loss: 0.00002436
Iteration 59/1000 | Loss: 0.00002436
Iteration 60/1000 | Loss: 0.00002436
Iteration 61/1000 | Loss: 0.00002436
Iteration 62/1000 | Loss: 0.00002436
Iteration 63/1000 | Loss: 0.00002436
Iteration 64/1000 | Loss: 0.00002436
Iteration 65/1000 | Loss: 0.00002436
Iteration 66/1000 | Loss: 0.00002436
Iteration 67/1000 | Loss: 0.00002436
Iteration 68/1000 | Loss: 0.00002435
Iteration 69/1000 | Loss: 0.00002435
Iteration 70/1000 | Loss: 0.00002435
Iteration 71/1000 | Loss: 0.00002435
Iteration 72/1000 | Loss: 0.00002435
Iteration 73/1000 | Loss: 0.00002435
Iteration 74/1000 | Loss: 0.00002435
Iteration 75/1000 | Loss: 0.00002435
Iteration 76/1000 | Loss: 0.00002435
Iteration 77/1000 | Loss: 0.00002435
Iteration 78/1000 | Loss: 0.00002435
Iteration 79/1000 | Loss: 0.00002435
Iteration 80/1000 | Loss: 0.00002435
Iteration 81/1000 | Loss: 0.00002435
Iteration 82/1000 | Loss: 0.00002434
Iteration 83/1000 | Loss: 0.00002434
Iteration 84/1000 | Loss: 0.00002434
Iteration 85/1000 | Loss: 0.00002434
Iteration 86/1000 | Loss: 0.00002434
Iteration 87/1000 | Loss: 0.00002434
Iteration 88/1000 | Loss: 0.00002434
Iteration 89/1000 | Loss: 0.00002434
Iteration 90/1000 | Loss: 0.00002434
Iteration 91/1000 | Loss: 0.00002434
Iteration 92/1000 | Loss: 0.00002434
Iteration 93/1000 | Loss: 0.00002434
Iteration 94/1000 | Loss: 0.00002434
Iteration 95/1000 | Loss: 0.00002434
Iteration 96/1000 | Loss: 0.00002434
Iteration 97/1000 | Loss: 0.00002434
Iteration 98/1000 | Loss: 0.00002434
Iteration 99/1000 | Loss: 0.00002434
Iteration 100/1000 | Loss: 0.00002434
Iteration 101/1000 | Loss: 0.00002434
Iteration 102/1000 | Loss: 0.00002434
Iteration 103/1000 | Loss: 0.00002434
Iteration 104/1000 | Loss: 0.00002434
Iteration 105/1000 | Loss: 0.00002434
Iteration 106/1000 | Loss: 0.00002434
Iteration 107/1000 | Loss: 0.00002434
Iteration 108/1000 | Loss: 0.00002434
Iteration 109/1000 | Loss: 0.00002434
Iteration 110/1000 | Loss: 0.00002434
Iteration 111/1000 | Loss: 0.00002434
Iteration 112/1000 | Loss: 0.00002434
Iteration 113/1000 | Loss: 0.00002434
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002434
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002434
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.4335813577636145e-05, 2.4335813577636145e-05, 2.4335813577636145e-05, 2.4335813577636145e-05, 2.4335813577636145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4335813577636145e-05

Optimization complete. Final v2v error: 4.172873020172119 mm

Highest mean error: 4.605026721954346 mm for frame 12

Lowest mean error: 3.7334401607513428 mm for frame 109

Saving results

Total time: 38.567387104034424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052609
Iteration 2/25 | Loss: 0.00181199
Iteration 3/25 | Loss: 0.00110109
Iteration 4/25 | Loss: 0.00101597
Iteration 5/25 | Loss: 0.00097441
Iteration 6/25 | Loss: 0.00092551
Iteration 7/25 | Loss: 0.00089402
Iteration 8/25 | Loss: 0.00087220
Iteration 9/25 | Loss: 0.00085330
Iteration 10/25 | Loss: 0.00084339
Iteration 11/25 | Loss: 0.00083781
Iteration 12/25 | Loss: 0.00083495
Iteration 13/25 | Loss: 0.00083269
Iteration 14/25 | Loss: 0.00082364
Iteration 15/25 | Loss: 0.00082088
Iteration 16/25 | Loss: 0.00082534
Iteration 17/25 | Loss: 0.00082085
Iteration 18/25 | Loss: 0.00082025
Iteration 19/25 | Loss: 0.00082001
Iteration 20/25 | Loss: 0.00081863
Iteration 21/25 | Loss: 0.00081796
Iteration 22/25 | Loss: 0.00081592
Iteration 23/25 | Loss: 0.00081487
Iteration 24/25 | Loss: 0.00081916
Iteration 25/25 | Loss: 0.00081530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52443969
Iteration 2/25 | Loss: 0.00255029
Iteration 3/25 | Loss: 0.00199734
Iteration 4/25 | Loss: 0.00199734
Iteration 5/25 | Loss: 0.00199733
Iteration 6/25 | Loss: 0.00199733
Iteration 7/25 | Loss: 0.00199733
Iteration 8/25 | Loss: 0.00199733
Iteration 9/25 | Loss: 0.00199733
Iteration 10/25 | Loss: 0.00199733
Iteration 11/25 | Loss: 0.00199733
Iteration 12/25 | Loss: 0.00199733
Iteration 13/25 | Loss: 0.00199733
Iteration 14/25 | Loss: 0.00199733
Iteration 15/25 | Loss: 0.00199733
Iteration 16/25 | Loss: 0.00199733
Iteration 17/25 | Loss: 0.00199733
Iteration 18/25 | Loss: 0.00199733
Iteration 19/25 | Loss: 0.00199733
Iteration 20/25 | Loss: 0.00199733
Iteration 21/25 | Loss: 0.00199733
Iteration 22/25 | Loss: 0.00199733
Iteration 23/25 | Loss: 0.00199733
Iteration 24/25 | Loss: 0.00199733
Iteration 25/25 | Loss: 0.00199733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199733
Iteration 2/1000 | Loss: 0.00201301
Iteration 3/1000 | Loss: 0.00230443
Iteration 4/1000 | Loss: 0.00028899
Iteration 5/1000 | Loss: 0.00031545
Iteration 6/1000 | Loss: 0.00039358
Iteration 7/1000 | Loss: 0.00028853
Iteration 8/1000 | Loss: 0.00055717
Iteration 9/1000 | Loss: 0.00237648
Iteration 10/1000 | Loss: 0.00056329
Iteration 11/1000 | Loss: 0.00047479
Iteration 12/1000 | Loss: 0.00049631
Iteration 13/1000 | Loss: 0.00041165
Iteration 14/1000 | Loss: 0.00064655
Iteration 15/1000 | Loss: 0.00029756
Iteration 16/1000 | Loss: 0.00056342
Iteration 17/1000 | Loss: 0.00211118
Iteration 18/1000 | Loss: 0.00038793
Iteration 19/1000 | Loss: 0.00016056
Iteration 20/1000 | Loss: 0.00012206
Iteration 21/1000 | Loss: 0.00012138
Iteration 22/1000 | Loss: 0.00114842
Iteration 23/1000 | Loss: 0.00028746
Iteration 24/1000 | Loss: 0.00089584
Iteration 25/1000 | Loss: 0.00074586
Iteration 26/1000 | Loss: 0.00052189
Iteration 27/1000 | Loss: 0.00076736
Iteration 28/1000 | Loss: 0.00060536
Iteration 29/1000 | Loss: 0.00031353
Iteration 30/1000 | Loss: 0.00014514
Iteration 31/1000 | Loss: 0.00024135
Iteration 32/1000 | Loss: 0.00133707
Iteration 33/1000 | Loss: 0.00016179
Iteration 34/1000 | Loss: 0.00010257
Iteration 35/1000 | Loss: 0.00006373
Iteration 36/1000 | Loss: 0.00076217
Iteration 37/1000 | Loss: 0.00075977
Iteration 38/1000 | Loss: 0.00013144
Iteration 39/1000 | Loss: 0.00121028
Iteration 40/1000 | Loss: 0.00013880
Iteration 41/1000 | Loss: 0.00004999
Iteration 42/1000 | Loss: 0.00086113
Iteration 43/1000 | Loss: 0.00025860
Iteration 44/1000 | Loss: 0.00007169
Iteration 45/1000 | Loss: 0.00008195
Iteration 46/1000 | Loss: 0.00003046
Iteration 47/1000 | Loss: 0.00025466
Iteration 48/1000 | Loss: 0.00013443
Iteration 49/1000 | Loss: 0.00017687
Iteration 50/1000 | Loss: 0.00022698
Iteration 51/1000 | Loss: 0.00018367
Iteration 52/1000 | Loss: 0.00003635
Iteration 53/1000 | Loss: 0.00010232
Iteration 54/1000 | Loss: 0.00003668
Iteration 55/1000 | Loss: 0.00005138
Iteration 56/1000 | Loss: 0.00003769
Iteration 57/1000 | Loss: 0.00004870
Iteration 58/1000 | Loss: 0.00002794
Iteration 59/1000 | Loss: 0.00003841
Iteration 60/1000 | Loss: 0.00002676
Iteration 61/1000 | Loss: 0.00008281
Iteration 62/1000 | Loss: 0.00025469
Iteration 63/1000 | Loss: 0.00110208
Iteration 64/1000 | Loss: 0.00002706
Iteration 65/1000 | Loss: 0.00003919
Iteration 66/1000 | Loss: 0.00002554
Iteration 67/1000 | Loss: 0.00016458
Iteration 68/1000 | Loss: 0.00012523
Iteration 69/1000 | Loss: 0.00003315
Iteration 70/1000 | Loss: 0.00007553
Iteration 71/1000 | Loss: 0.00002662
Iteration 72/1000 | Loss: 0.00004807
Iteration 73/1000 | Loss: 0.00004319
Iteration 74/1000 | Loss: 0.00006545
Iteration 75/1000 | Loss: 0.00005333
Iteration 76/1000 | Loss: 0.00003743
Iteration 77/1000 | Loss: 0.00006976
Iteration 78/1000 | Loss: 0.00007377
Iteration 79/1000 | Loss: 0.00005674
Iteration 80/1000 | Loss: 0.00004735
Iteration 81/1000 | Loss: 0.00005809
Iteration 82/1000 | Loss: 0.00006063
Iteration 83/1000 | Loss: 0.00005194
Iteration 84/1000 | Loss: 0.00004067
Iteration 85/1000 | Loss: 0.00004401
Iteration 86/1000 | Loss: 0.00002900
Iteration 87/1000 | Loss: 0.00002672
Iteration 88/1000 | Loss: 0.00003730
Iteration 89/1000 | Loss: 0.00002736
Iteration 90/1000 | Loss: 0.00002499
Iteration 91/1000 | Loss: 0.00003253
Iteration 92/1000 | Loss: 0.00002826
Iteration 93/1000 | Loss: 0.00030061
Iteration 94/1000 | Loss: 0.00056363
Iteration 95/1000 | Loss: 0.00049536
Iteration 96/1000 | Loss: 0.00006396
Iteration 97/1000 | Loss: 0.00015516
Iteration 98/1000 | Loss: 0.00006762
Iteration 99/1000 | Loss: 0.00004143
Iteration 100/1000 | Loss: 0.00003645
Iteration 101/1000 | Loss: 0.00002400
Iteration 102/1000 | Loss: 0.00042332
Iteration 103/1000 | Loss: 0.00077658
Iteration 104/1000 | Loss: 0.00009653
Iteration 105/1000 | Loss: 0.00009684
Iteration 106/1000 | Loss: 0.00006231
Iteration 107/1000 | Loss: 0.00005224
Iteration 108/1000 | Loss: 0.00012695
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00007018
Iteration 111/1000 | Loss: 0.00029011
Iteration 112/1000 | Loss: 0.00031671
Iteration 113/1000 | Loss: 0.00053893
Iteration 114/1000 | Loss: 0.00020255
Iteration 115/1000 | Loss: 0.00003204
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00007630
Iteration 118/1000 | Loss: 0.00003679
Iteration 119/1000 | Loss: 0.00006308
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00007947
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001425
Iteration 125/1000 | Loss: 0.00001423
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001420
Iteration 128/1000 | Loss: 0.00001420
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001418
Iteration 132/1000 | Loss: 0.00001416
Iteration 133/1000 | Loss: 0.00002331
Iteration 134/1000 | Loss: 0.00003629
Iteration 135/1000 | Loss: 0.00012541
Iteration 136/1000 | Loss: 0.00002306
Iteration 137/1000 | Loss: 0.00004613
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001398
Iteration 140/1000 | Loss: 0.00001395
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001393
Iteration 153/1000 | Loss: 0.00001393
Iteration 154/1000 | Loss: 0.00004703
Iteration 155/1000 | Loss: 0.00001532
Iteration 156/1000 | Loss: 0.00001396
Iteration 157/1000 | Loss: 0.00001395
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001392
Iteration 165/1000 | Loss: 0.00001392
Iteration 166/1000 | Loss: 0.00001390
Iteration 167/1000 | Loss: 0.00001390
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001388
Iteration 172/1000 | Loss: 0.00001388
Iteration 173/1000 | Loss: 0.00001387
Iteration 174/1000 | Loss: 0.00001862
Iteration 175/1000 | Loss: 0.00001385
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001384
Iteration 180/1000 | Loss: 0.00001384
Iteration 181/1000 | Loss: 0.00001384
Iteration 182/1000 | Loss: 0.00001384
Iteration 183/1000 | Loss: 0.00001384
Iteration 184/1000 | Loss: 0.00001384
Iteration 185/1000 | Loss: 0.00001384
Iteration 186/1000 | Loss: 0.00001384
Iteration 187/1000 | Loss: 0.00001384
Iteration 188/1000 | Loss: 0.00001384
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001383
Iteration 194/1000 | Loss: 0.00001383
Iteration 195/1000 | Loss: 0.00001383
Iteration 196/1000 | Loss: 0.00001382
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00003655
Iteration 200/1000 | Loss: 0.00007506
Iteration 201/1000 | Loss: 0.00001815
Iteration 202/1000 | Loss: 0.00003890
Iteration 203/1000 | Loss: 0.00001514
Iteration 204/1000 | Loss: 0.00001398
Iteration 205/1000 | Loss: 0.00001960
Iteration 206/1000 | Loss: 0.00001381
Iteration 207/1000 | Loss: 0.00001378
Iteration 208/1000 | Loss: 0.00001378
Iteration 209/1000 | Loss: 0.00001378
Iteration 210/1000 | Loss: 0.00001378
Iteration 211/1000 | Loss: 0.00001378
Iteration 212/1000 | Loss: 0.00001378
Iteration 213/1000 | Loss: 0.00001378
Iteration 214/1000 | Loss: 0.00001378
Iteration 215/1000 | Loss: 0.00001378
Iteration 216/1000 | Loss: 0.00001378
Iteration 217/1000 | Loss: 0.00001378
Iteration 218/1000 | Loss: 0.00001378
Iteration 219/1000 | Loss: 0.00001378
Iteration 220/1000 | Loss: 0.00001378
Iteration 221/1000 | Loss: 0.00001378
Iteration 222/1000 | Loss: 0.00001378
Iteration 223/1000 | Loss: 0.00001378
Iteration 224/1000 | Loss: 0.00001378
Iteration 225/1000 | Loss: 0.00001378
Iteration 226/1000 | Loss: 0.00001378
Iteration 227/1000 | Loss: 0.00001378
Iteration 228/1000 | Loss: 0.00001377
Iteration 229/1000 | Loss: 0.00001377
Iteration 230/1000 | Loss: 0.00001377
Iteration 231/1000 | Loss: 0.00001377
Iteration 232/1000 | Loss: 0.00001377
Iteration 233/1000 | Loss: 0.00001377
Iteration 234/1000 | Loss: 0.00001377
Iteration 235/1000 | Loss: 0.00001377
Iteration 236/1000 | Loss: 0.00001377
Iteration 237/1000 | Loss: 0.00001377
Iteration 238/1000 | Loss: 0.00001377
Iteration 239/1000 | Loss: 0.00001377
Iteration 240/1000 | Loss: 0.00001377
Iteration 241/1000 | Loss: 0.00001377
Iteration 242/1000 | Loss: 0.00001377
Iteration 243/1000 | Loss: 0.00001377
Iteration 244/1000 | Loss: 0.00001377
Iteration 245/1000 | Loss: 0.00001377
Iteration 246/1000 | Loss: 0.00001377
Iteration 247/1000 | Loss: 0.00001377
Iteration 248/1000 | Loss: 0.00001377
Iteration 249/1000 | Loss: 0.00001377
Iteration 250/1000 | Loss: 0.00001377
Iteration 251/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.3773536920780316e-05, 1.3773536920780316e-05, 1.3773536920780316e-05, 1.3773536920780316e-05, 1.3773536920780316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3773536920780316e-05

Optimization complete. Final v2v error: 2.949768543243408 mm

Highest mean error: 11.973555564880371 mm for frame 7

Lowest mean error: 2.5257418155670166 mm for frame 103

Saving results

Total time: 282.7783613204956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434410
Iteration 2/25 | Loss: 0.00084749
Iteration 3/25 | Loss: 0.00076570
Iteration 4/25 | Loss: 0.00075008
Iteration 5/25 | Loss: 0.00074336
Iteration 6/25 | Loss: 0.00074164
Iteration 7/25 | Loss: 0.00074164
Iteration 8/25 | Loss: 0.00074164
Iteration 9/25 | Loss: 0.00074164
Iteration 10/25 | Loss: 0.00074164
Iteration 11/25 | Loss: 0.00074164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007416357402689755, 0.0007416357402689755, 0.0007416357402689755, 0.0007416357402689755, 0.0007416357402689755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007416357402689755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55373573
Iteration 2/25 | Loss: 0.00091776
Iteration 3/25 | Loss: 0.00091776
Iteration 4/25 | Loss: 0.00091776
Iteration 5/25 | Loss: 0.00091776
Iteration 6/25 | Loss: 0.00091776
Iteration 7/25 | Loss: 0.00091776
Iteration 8/25 | Loss: 0.00091776
Iteration 9/25 | Loss: 0.00091776
Iteration 10/25 | Loss: 0.00091776
Iteration 11/25 | Loss: 0.00091776
Iteration 12/25 | Loss: 0.00091776
Iteration 13/25 | Loss: 0.00091776
Iteration 14/25 | Loss: 0.00091776
Iteration 15/25 | Loss: 0.00091776
Iteration 16/25 | Loss: 0.00091776
Iteration 17/25 | Loss: 0.00091776
Iteration 18/25 | Loss: 0.00091776
Iteration 19/25 | Loss: 0.00091776
Iteration 20/25 | Loss: 0.00091776
Iteration 21/25 | Loss: 0.00091776
Iteration 22/25 | Loss: 0.00091776
Iteration 23/25 | Loss: 0.00091776
Iteration 24/25 | Loss: 0.00091776
Iteration 25/25 | Loss: 0.00091776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009177580359391868, 0.0009177580359391868, 0.0009177580359391868, 0.0009177580359391868, 0.0009177580359391868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009177580359391868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091776
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001510
Iteration 6/1000 | Loss: 0.00001474
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001459
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001407
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001407
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001406
Iteration 21/1000 | Loss: 0.00001406
Iteration 22/1000 | Loss: 0.00001405
Iteration 23/1000 | Loss: 0.00001405
Iteration 24/1000 | Loss: 0.00001405
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001399
Iteration 28/1000 | Loss: 0.00001399
Iteration 29/1000 | Loss: 0.00001399
Iteration 30/1000 | Loss: 0.00001398
Iteration 31/1000 | Loss: 0.00001398
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001398
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001397
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001396
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001395
Iteration 47/1000 | Loss: 0.00001395
Iteration 48/1000 | Loss: 0.00001395
Iteration 49/1000 | Loss: 0.00001395
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001395
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001394
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001394
Iteration 57/1000 | Loss: 0.00001393
Iteration 58/1000 | Loss: 0.00001393
Iteration 59/1000 | Loss: 0.00001393
Iteration 60/1000 | Loss: 0.00001393
Iteration 61/1000 | Loss: 0.00001393
Iteration 62/1000 | Loss: 0.00001392
Iteration 63/1000 | Loss: 0.00001392
Iteration 64/1000 | Loss: 0.00001392
Iteration 65/1000 | Loss: 0.00001392
Iteration 66/1000 | Loss: 0.00001392
Iteration 67/1000 | Loss: 0.00001392
Iteration 68/1000 | Loss: 0.00001392
Iteration 69/1000 | Loss: 0.00001392
Iteration 70/1000 | Loss: 0.00001392
Iteration 71/1000 | Loss: 0.00001392
Iteration 72/1000 | Loss: 0.00001392
Iteration 73/1000 | Loss: 0.00001392
Iteration 74/1000 | Loss: 0.00001392
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001391
Iteration 78/1000 | Loss: 0.00001391
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001390
Iteration 86/1000 | Loss: 0.00001390
Iteration 87/1000 | Loss: 0.00001390
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001389
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001388
Iteration 111/1000 | Loss: 0.00001388
Iteration 112/1000 | Loss: 0.00001388
Iteration 113/1000 | Loss: 0.00001388
Iteration 114/1000 | Loss: 0.00001388
Iteration 115/1000 | Loss: 0.00001388
Iteration 116/1000 | Loss: 0.00001388
Iteration 117/1000 | Loss: 0.00001388
Iteration 118/1000 | Loss: 0.00001388
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001387
Iteration 124/1000 | Loss: 0.00001387
Iteration 125/1000 | Loss: 0.00001387
Iteration 126/1000 | Loss: 0.00001387
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001386
Iteration 129/1000 | Loss: 0.00001386
Iteration 130/1000 | Loss: 0.00001386
Iteration 131/1000 | Loss: 0.00001386
Iteration 132/1000 | Loss: 0.00001386
Iteration 133/1000 | Loss: 0.00001386
Iteration 134/1000 | Loss: 0.00001386
Iteration 135/1000 | Loss: 0.00001386
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001383
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00001382
Iteration 174/1000 | Loss: 0.00001382
Iteration 175/1000 | Loss: 0.00001382
Iteration 176/1000 | Loss: 0.00001382
Iteration 177/1000 | Loss: 0.00001382
Iteration 178/1000 | Loss: 0.00001382
Iteration 179/1000 | Loss: 0.00001382
Iteration 180/1000 | Loss: 0.00001382
Iteration 181/1000 | Loss: 0.00001382
Iteration 182/1000 | Loss: 0.00001382
Iteration 183/1000 | Loss: 0.00001382
Iteration 184/1000 | Loss: 0.00001382
Iteration 185/1000 | Loss: 0.00001382
Iteration 186/1000 | Loss: 0.00001382
Iteration 187/1000 | Loss: 0.00001382
Iteration 188/1000 | Loss: 0.00001382
Iteration 189/1000 | Loss: 0.00001382
Iteration 190/1000 | Loss: 0.00001382
Iteration 191/1000 | Loss: 0.00001382
Iteration 192/1000 | Loss: 0.00001382
Iteration 193/1000 | Loss: 0.00001382
Iteration 194/1000 | Loss: 0.00001382
Iteration 195/1000 | Loss: 0.00001382
Iteration 196/1000 | Loss: 0.00001382
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001382
Iteration 202/1000 | Loss: 0.00001382
Iteration 203/1000 | Loss: 0.00001382
Iteration 204/1000 | Loss: 0.00001382
Iteration 205/1000 | Loss: 0.00001382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.3823933841194957e-05, 1.3823933841194957e-05, 1.3823933841194957e-05, 1.3823933841194957e-05, 1.3823933841194957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3823933841194957e-05

Optimization complete. Final v2v error: 3.1436431407928467 mm

Highest mean error: 3.233767509460449 mm for frame 171

Lowest mean error: 3.0146870613098145 mm for frame 3

Saving results

Total time: 39.23925447463989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875129
Iteration 2/25 | Loss: 0.00138540
Iteration 3/25 | Loss: 0.00094288
Iteration 4/25 | Loss: 0.00088362
Iteration 5/25 | Loss: 0.00087404
Iteration 6/25 | Loss: 0.00087159
Iteration 7/25 | Loss: 0.00087003
Iteration 8/25 | Loss: 0.00086850
Iteration 9/25 | Loss: 0.00086819
Iteration 10/25 | Loss: 0.00086807
Iteration 11/25 | Loss: 0.00086805
Iteration 12/25 | Loss: 0.00086805
Iteration 13/25 | Loss: 0.00086805
Iteration 14/25 | Loss: 0.00086805
Iteration 15/25 | Loss: 0.00086805
Iteration 16/25 | Loss: 0.00086805
Iteration 17/25 | Loss: 0.00086805
Iteration 18/25 | Loss: 0.00086805
Iteration 19/25 | Loss: 0.00086805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008680454338900745, 0.0008680454338900745, 0.0008680454338900745, 0.0008680454338900745, 0.0008680454338900745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008680454338900745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45608187
Iteration 2/25 | Loss: 0.00096915
Iteration 3/25 | Loss: 0.00096914
Iteration 4/25 | Loss: 0.00096914
Iteration 5/25 | Loss: 0.00096914
Iteration 6/25 | Loss: 0.00096914
Iteration 7/25 | Loss: 0.00096914
Iteration 8/25 | Loss: 0.00096914
Iteration 9/25 | Loss: 0.00096914
Iteration 10/25 | Loss: 0.00096914
Iteration 11/25 | Loss: 0.00096914
Iteration 12/25 | Loss: 0.00096914
Iteration 13/25 | Loss: 0.00096914
Iteration 14/25 | Loss: 0.00096914
Iteration 15/25 | Loss: 0.00096914
Iteration 16/25 | Loss: 0.00096914
Iteration 17/25 | Loss: 0.00096914
Iteration 18/25 | Loss: 0.00096914
Iteration 19/25 | Loss: 0.00096914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009691426530480385, 0.0009691426530480385, 0.0009691426530480385, 0.0009691426530480385, 0.0009691426530480385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009691426530480385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096914
Iteration 2/1000 | Loss: 0.00003972
Iteration 3/1000 | Loss: 0.00003180
Iteration 4/1000 | Loss: 0.00002990
Iteration 5/1000 | Loss: 0.00002861
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00002690
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002603
Iteration 10/1000 | Loss: 0.00002587
Iteration 11/1000 | Loss: 0.00002580
Iteration 12/1000 | Loss: 0.00002573
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002572
Iteration 15/1000 | Loss: 0.00002570
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002569
Iteration 18/1000 | Loss: 0.00002569
Iteration 19/1000 | Loss: 0.00002568
Iteration 20/1000 | Loss: 0.00002567
Iteration 21/1000 | Loss: 0.00002566
Iteration 22/1000 | Loss: 0.00002566
Iteration 23/1000 | Loss: 0.00002566
Iteration 24/1000 | Loss: 0.00002566
Iteration 25/1000 | Loss: 0.00002566
Iteration 26/1000 | Loss: 0.00002566
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002566
Iteration 29/1000 | Loss: 0.00002566
Iteration 30/1000 | Loss: 0.00002566
Iteration 31/1000 | Loss: 0.00002565
Iteration 32/1000 | Loss: 0.00002565
Iteration 33/1000 | Loss: 0.00002564
Iteration 34/1000 | Loss: 0.00002564
Iteration 35/1000 | Loss: 0.00002564
Iteration 36/1000 | Loss: 0.00002564
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00002563
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00002563
Iteration 41/1000 | Loss: 0.00002563
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002563
Iteration 44/1000 | Loss: 0.00002563
Iteration 45/1000 | Loss: 0.00002563
Iteration 46/1000 | Loss: 0.00002563
Iteration 47/1000 | Loss: 0.00002563
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002562
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00002562
Iteration 54/1000 | Loss: 0.00002561
Iteration 55/1000 | Loss: 0.00002561
Iteration 56/1000 | Loss: 0.00002561
Iteration 57/1000 | Loss: 0.00002561
Iteration 58/1000 | Loss: 0.00002561
Iteration 59/1000 | Loss: 0.00002561
Iteration 60/1000 | Loss: 0.00002561
Iteration 61/1000 | Loss: 0.00002561
Iteration 62/1000 | Loss: 0.00002560
Iteration 63/1000 | Loss: 0.00002560
Iteration 64/1000 | Loss: 0.00002560
Iteration 65/1000 | Loss: 0.00002560
Iteration 66/1000 | Loss: 0.00002560
Iteration 67/1000 | Loss: 0.00002560
Iteration 68/1000 | Loss: 0.00002560
Iteration 69/1000 | Loss: 0.00002559
Iteration 70/1000 | Loss: 0.00002559
Iteration 71/1000 | Loss: 0.00002559
Iteration 72/1000 | Loss: 0.00002559
Iteration 73/1000 | Loss: 0.00002559
Iteration 74/1000 | Loss: 0.00002558
Iteration 75/1000 | Loss: 0.00002558
Iteration 76/1000 | Loss: 0.00002558
Iteration 77/1000 | Loss: 0.00002558
Iteration 78/1000 | Loss: 0.00002557
Iteration 79/1000 | Loss: 0.00002557
Iteration 80/1000 | Loss: 0.00002557
Iteration 81/1000 | Loss: 0.00002557
Iteration 82/1000 | Loss: 0.00002557
Iteration 83/1000 | Loss: 0.00002557
Iteration 84/1000 | Loss: 0.00002556
Iteration 85/1000 | Loss: 0.00002556
Iteration 86/1000 | Loss: 0.00002556
Iteration 87/1000 | Loss: 0.00002556
Iteration 88/1000 | Loss: 0.00002555
Iteration 89/1000 | Loss: 0.00002555
Iteration 90/1000 | Loss: 0.00002555
Iteration 91/1000 | Loss: 0.00002555
Iteration 92/1000 | Loss: 0.00002555
Iteration 93/1000 | Loss: 0.00002555
Iteration 94/1000 | Loss: 0.00002555
Iteration 95/1000 | Loss: 0.00002555
Iteration 96/1000 | Loss: 0.00002554
Iteration 97/1000 | Loss: 0.00002554
Iteration 98/1000 | Loss: 0.00002554
Iteration 99/1000 | Loss: 0.00002554
Iteration 100/1000 | Loss: 0.00002554
Iteration 101/1000 | Loss: 0.00002554
Iteration 102/1000 | Loss: 0.00002554
Iteration 103/1000 | Loss: 0.00002554
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002554
Iteration 106/1000 | Loss: 0.00002554
Iteration 107/1000 | Loss: 0.00002554
Iteration 108/1000 | Loss: 0.00002554
Iteration 109/1000 | Loss: 0.00002554
Iteration 110/1000 | Loss: 0.00002554
Iteration 111/1000 | Loss: 0.00002554
Iteration 112/1000 | Loss: 0.00002554
Iteration 113/1000 | Loss: 0.00002554
Iteration 114/1000 | Loss: 0.00002554
Iteration 115/1000 | Loss: 0.00002554
Iteration 116/1000 | Loss: 0.00002554
Iteration 117/1000 | Loss: 0.00002554
Iteration 118/1000 | Loss: 0.00002554
Iteration 119/1000 | Loss: 0.00002554
Iteration 120/1000 | Loss: 0.00002554
Iteration 121/1000 | Loss: 0.00002554
Iteration 122/1000 | Loss: 0.00002554
Iteration 123/1000 | Loss: 0.00002554
Iteration 124/1000 | Loss: 0.00002554
Iteration 125/1000 | Loss: 0.00002554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.5536519387969747e-05, 2.5536519387969747e-05, 2.5536519387969747e-05, 2.5536519387969747e-05, 2.5536519387969747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5536519387969747e-05

Optimization complete. Final v2v error: 4.207004547119141 mm

Highest mean error: 4.890287399291992 mm for frame 134

Lowest mean error: 3.614222764968872 mm for frame 82

Saving results

Total time: 45.40542507171631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429256
Iteration 2/25 | Loss: 0.00090348
Iteration 3/25 | Loss: 0.00076232
Iteration 4/25 | Loss: 0.00074714
Iteration 5/25 | Loss: 0.00074216
Iteration 6/25 | Loss: 0.00074096
Iteration 7/25 | Loss: 0.00074071
Iteration 8/25 | Loss: 0.00074071
Iteration 9/25 | Loss: 0.00074071
Iteration 10/25 | Loss: 0.00074071
Iteration 11/25 | Loss: 0.00074071
Iteration 12/25 | Loss: 0.00074071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007407054072245955, 0.0007407054072245955, 0.0007407054072245955, 0.0007407054072245955, 0.0007407054072245955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007407054072245955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.71975183
Iteration 2/25 | Loss: 0.00083240
Iteration 3/25 | Loss: 0.00083240
Iteration 4/25 | Loss: 0.00083240
Iteration 5/25 | Loss: 0.00083240
Iteration 6/25 | Loss: 0.00083240
Iteration 7/25 | Loss: 0.00083240
Iteration 8/25 | Loss: 0.00083240
Iteration 9/25 | Loss: 0.00083239
Iteration 10/25 | Loss: 0.00083239
Iteration 11/25 | Loss: 0.00083239
Iteration 12/25 | Loss: 0.00083239
Iteration 13/25 | Loss: 0.00083239
Iteration 14/25 | Loss: 0.00083239
Iteration 15/25 | Loss: 0.00083239
Iteration 16/25 | Loss: 0.00083239
Iteration 17/25 | Loss: 0.00083239
Iteration 18/25 | Loss: 0.00083239
Iteration 19/25 | Loss: 0.00083239
Iteration 20/25 | Loss: 0.00083239
Iteration 21/25 | Loss: 0.00083239
Iteration 22/25 | Loss: 0.00083239
Iteration 23/25 | Loss: 0.00083239
Iteration 24/25 | Loss: 0.00083239
Iteration 25/25 | Loss: 0.00083239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083239
Iteration 2/1000 | Loss: 0.00002520
Iteration 3/1000 | Loss: 0.00001782
Iteration 4/1000 | Loss: 0.00001673
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001504
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001482
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001480
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001467
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001464
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001457
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001452
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001451
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001450
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001450
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001447
Iteration 149/1000 | Loss: 0.00001447
Iteration 150/1000 | Loss: 0.00001447
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001446
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001446
Iteration 159/1000 | Loss: 0.00001446
Iteration 160/1000 | Loss: 0.00001446
Iteration 161/1000 | Loss: 0.00001446
Iteration 162/1000 | Loss: 0.00001446
Iteration 163/1000 | Loss: 0.00001446
Iteration 164/1000 | Loss: 0.00001446
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001445
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001445
Iteration 170/1000 | Loss: 0.00001445
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001445
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001444
Iteration 180/1000 | Loss: 0.00001444
Iteration 181/1000 | Loss: 0.00001444
Iteration 182/1000 | Loss: 0.00001444
Iteration 183/1000 | Loss: 0.00001444
Iteration 184/1000 | Loss: 0.00001444
Iteration 185/1000 | Loss: 0.00001444
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001444
Iteration 194/1000 | Loss: 0.00001444
Iteration 195/1000 | Loss: 0.00001444
Iteration 196/1000 | Loss: 0.00001444
Iteration 197/1000 | Loss: 0.00001444
Iteration 198/1000 | Loss: 0.00001444
Iteration 199/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4437205209105741e-05, 1.4437205209105741e-05, 1.4437205209105741e-05, 1.4437205209105741e-05, 1.4437205209105741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4437205209105741e-05

Optimization complete. Final v2v error: 3.2188565731048584 mm

Highest mean error: 3.5369505882263184 mm for frame 82

Lowest mean error: 3.063364028930664 mm for frame 27

Saving results

Total time: 37.7212929725647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495774
Iteration 2/25 | Loss: 0.00101679
Iteration 3/25 | Loss: 0.00086538
Iteration 4/25 | Loss: 0.00081507
Iteration 5/25 | Loss: 0.00079849
Iteration 6/25 | Loss: 0.00079428
Iteration 7/25 | Loss: 0.00079267
Iteration 8/25 | Loss: 0.00079246
Iteration 9/25 | Loss: 0.00079246
Iteration 10/25 | Loss: 0.00079246
Iteration 11/25 | Loss: 0.00079246
Iteration 12/25 | Loss: 0.00079246
Iteration 13/25 | Loss: 0.00079246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007924630190245807, 0.0007924630190245807, 0.0007924630190245807, 0.0007924630190245807, 0.0007924630190245807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007924630190245807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73930204
Iteration 2/25 | Loss: 0.00104682
Iteration 3/25 | Loss: 0.00104681
Iteration 4/25 | Loss: 0.00104681
Iteration 5/25 | Loss: 0.00104681
Iteration 6/25 | Loss: 0.00104681
Iteration 7/25 | Loss: 0.00104681
Iteration 8/25 | Loss: 0.00104681
Iteration 9/25 | Loss: 0.00104681
Iteration 10/25 | Loss: 0.00104681
Iteration 11/25 | Loss: 0.00104681
Iteration 12/25 | Loss: 0.00104681
Iteration 13/25 | Loss: 0.00104681
Iteration 14/25 | Loss: 0.00104681
Iteration 15/25 | Loss: 0.00104681
Iteration 16/25 | Loss: 0.00104681
Iteration 17/25 | Loss: 0.00104681
Iteration 18/25 | Loss: 0.00104681
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010468123946338892, 0.0010468123946338892, 0.0010468123946338892, 0.0010468123946338892, 0.0010468123946338892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010468123946338892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104681
Iteration 2/1000 | Loss: 0.00004803
Iteration 3/1000 | Loss: 0.00003360
Iteration 4/1000 | Loss: 0.00002927
Iteration 5/1000 | Loss: 0.00002737
Iteration 6/1000 | Loss: 0.00002626
Iteration 7/1000 | Loss: 0.00002535
Iteration 8/1000 | Loss: 0.00002471
Iteration 9/1000 | Loss: 0.00002416
Iteration 10/1000 | Loss: 0.00002383
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002333
Iteration 13/1000 | Loss: 0.00002308
Iteration 14/1000 | Loss: 0.00002289
Iteration 15/1000 | Loss: 0.00002271
Iteration 16/1000 | Loss: 0.00002265
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002256
Iteration 19/1000 | Loss: 0.00002253
Iteration 20/1000 | Loss: 0.00002251
Iteration 21/1000 | Loss: 0.00002249
Iteration 22/1000 | Loss: 0.00002247
Iteration 23/1000 | Loss: 0.00002245
Iteration 24/1000 | Loss: 0.00002241
Iteration 25/1000 | Loss: 0.00002241
Iteration 26/1000 | Loss: 0.00002239
Iteration 27/1000 | Loss: 0.00002238
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00002238
Iteration 30/1000 | Loss: 0.00002238
Iteration 31/1000 | Loss: 0.00002238
Iteration 32/1000 | Loss: 0.00002237
Iteration 33/1000 | Loss: 0.00002237
Iteration 34/1000 | Loss: 0.00002236
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002235
Iteration 37/1000 | Loss: 0.00002233
Iteration 38/1000 | Loss: 0.00002233
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00002231
Iteration 41/1000 | Loss: 0.00002230
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002228
Iteration 46/1000 | Loss: 0.00002228
Iteration 47/1000 | Loss: 0.00002228
Iteration 48/1000 | Loss: 0.00002228
Iteration 49/1000 | Loss: 0.00002227
Iteration 50/1000 | Loss: 0.00002227
Iteration 51/1000 | Loss: 0.00002227
Iteration 52/1000 | Loss: 0.00002226
Iteration 53/1000 | Loss: 0.00002226
Iteration 54/1000 | Loss: 0.00002225
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002225
Iteration 57/1000 | Loss: 0.00002225
Iteration 58/1000 | Loss: 0.00002224
Iteration 59/1000 | Loss: 0.00002224
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002223
Iteration 62/1000 | Loss: 0.00002223
Iteration 63/1000 | Loss: 0.00002223
Iteration 64/1000 | Loss: 0.00002223
Iteration 65/1000 | Loss: 0.00002223
Iteration 66/1000 | Loss: 0.00002223
Iteration 67/1000 | Loss: 0.00002223
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00002223
Iteration 70/1000 | Loss: 0.00002222
Iteration 71/1000 | Loss: 0.00002222
Iteration 72/1000 | Loss: 0.00002222
Iteration 73/1000 | Loss: 0.00002222
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002221
Iteration 79/1000 | Loss: 0.00002221
Iteration 80/1000 | Loss: 0.00002221
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002221
Iteration 83/1000 | Loss: 0.00002221
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00002220
Iteration 88/1000 | Loss: 0.00002220
Iteration 89/1000 | Loss: 0.00002219
Iteration 90/1000 | Loss: 0.00002219
Iteration 91/1000 | Loss: 0.00002219
Iteration 92/1000 | Loss: 0.00002219
Iteration 93/1000 | Loss: 0.00002219
Iteration 94/1000 | Loss: 0.00002219
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002218
Iteration 97/1000 | Loss: 0.00002218
Iteration 98/1000 | Loss: 0.00002218
Iteration 99/1000 | Loss: 0.00002218
Iteration 100/1000 | Loss: 0.00002218
Iteration 101/1000 | Loss: 0.00002218
Iteration 102/1000 | Loss: 0.00002218
Iteration 103/1000 | Loss: 0.00002218
Iteration 104/1000 | Loss: 0.00002218
Iteration 105/1000 | Loss: 0.00002217
Iteration 106/1000 | Loss: 0.00002217
Iteration 107/1000 | Loss: 0.00002217
Iteration 108/1000 | Loss: 0.00002217
Iteration 109/1000 | Loss: 0.00002217
Iteration 110/1000 | Loss: 0.00002217
Iteration 111/1000 | Loss: 0.00002217
Iteration 112/1000 | Loss: 0.00002217
Iteration 113/1000 | Loss: 0.00002217
Iteration 114/1000 | Loss: 0.00002217
Iteration 115/1000 | Loss: 0.00002217
Iteration 116/1000 | Loss: 0.00002217
Iteration 117/1000 | Loss: 0.00002216
Iteration 118/1000 | Loss: 0.00002216
Iteration 119/1000 | Loss: 0.00002216
Iteration 120/1000 | Loss: 0.00002216
Iteration 121/1000 | Loss: 0.00002216
Iteration 122/1000 | Loss: 0.00002216
Iteration 123/1000 | Loss: 0.00002216
Iteration 124/1000 | Loss: 0.00002216
Iteration 125/1000 | Loss: 0.00002216
Iteration 126/1000 | Loss: 0.00002216
Iteration 127/1000 | Loss: 0.00002216
Iteration 128/1000 | Loss: 0.00002216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2164904294186272e-05, 2.2164904294186272e-05, 2.2164904294186272e-05, 2.2164904294186272e-05, 2.2164904294186272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2164904294186272e-05

Optimization complete. Final v2v error: 3.908064603805542 mm

Highest mean error: 5.0810136795043945 mm for frame 54

Lowest mean error: 3.040182113647461 mm for frame 239

Saving results

Total time: 49.90295696258545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404197
Iteration 2/25 | Loss: 0.00094939
Iteration 3/25 | Loss: 0.00077215
Iteration 4/25 | Loss: 0.00073959
Iteration 5/25 | Loss: 0.00072890
Iteration 6/25 | Loss: 0.00072659
Iteration 7/25 | Loss: 0.00072595
Iteration 8/25 | Loss: 0.00072589
Iteration 9/25 | Loss: 0.00072589
Iteration 10/25 | Loss: 0.00072589
Iteration 11/25 | Loss: 0.00072589
Iteration 12/25 | Loss: 0.00072589
Iteration 13/25 | Loss: 0.00072589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007258896948769689, 0.0007258896948769689, 0.0007258896948769689, 0.0007258896948769689, 0.0007258896948769689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007258896948769689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59505141
Iteration 2/25 | Loss: 0.00094580
Iteration 3/25 | Loss: 0.00094580
Iteration 4/25 | Loss: 0.00094580
Iteration 5/25 | Loss: 0.00094580
Iteration 6/25 | Loss: 0.00094580
Iteration 7/25 | Loss: 0.00094580
Iteration 8/25 | Loss: 0.00094580
Iteration 9/25 | Loss: 0.00094580
Iteration 10/25 | Loss: 0.00094580
Iteration 11/25 | Loss: 0.00094580
Iteration 12/25 | Loss: 0.00094580
Iteration 13/25 | Loss: 0.00094580
Iteration 14/25 | Loss: 0.00094580
Iteration 15/25 | Loss: 0.00094580
Iteration 16/25 | Loss: 0.00094580
Iteration 17/25 | Loss: 0.00094580
Iteration 18/25 | Loss: 0.00094580
Iteration 19/25 | Loss: 0.00094580
Iteration 20/25 | Loss: 0.00094580
Iteration 21/25 | Loss: 0.00094580
Iteration 22/25 | Loss: 0.00094580
Iteration 23/25 | Loss: 0.00094580
Iteration 24/25 | Loss: 0.00094580
Iteration 25/25 | Loss: 0.00094580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094580
Iteration 2/1000 | Loss: 0.00004054
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001427
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001364
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001352
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001351
Iteration 17/1000 | Loss: 0.00001348
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001336
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001334
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001329
Iteration 33/1000 | Loss: 0.00001329
Iteration 34/1000 | Loss: 0.00001329
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001327
Iteration 40/1000 | Loss: 0.00001326
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001326
Iteration 43/1000 | Loss: 0.00001326
Iteration 44/1000 | Loss: 0.00001326
Iteration 45/1000 | Loss: 0.00001325
Iteration 46/1000 | Loss: 0.00001325
Iteration 47/1000 | Loss: 0.00001325
Iteration 48/1000 | Loss: 0.00001325
Iteration 49/1000 | Loss: 0.00001325
Iteration 50/1000 | Loss: 0.00001325
Iteration 51/1000 | Loss: 0.00001324
Iteration 52/1000 | Loss: 0.00001324
Iteration 53/1000 | Loss: 0.00001324
Iteration 54/1000 | Loss: 0.00001324
Iteration 55/1000 | Loss: 0.00001324
Iteration 56/1000 | Loss: 0.00001323
Iteration 57/1000 | Loss: 0.00001323
Iteration 58/1000 | Loss: 0.00001323
Iteration 59/1000 | Loss: 0.00001322
Iteration 60/1000 | Loss: 0.00001322
Iteration 61/1000 | Loss: 0.00001321
Iteration 62/1000 | Loss: 0.00001321
Iteration 63/1000 | Loss: 0.00001321
Iteration 64/1000 | Loss: 0.00001321
Iteration 65/1000 | Loss: 0.00001321
Iteration 66/1000 | Loss: 0.00001320
Iteration 67/1000 | Loss: 0.00001320
Iteration 68/1000 | Loss: 0.00001320
Iteration 69/1000 | Loss: 0.00001319
Iteration 70/1000 | Loss: 0.00001319
Iteration 71/1000 | Loss: 0.00001319
Iteration 72/1000 | Loss: 0.00001319
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001317
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001316
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001315
Iteration 86/1000 | Loss: 0.00001315
Iteration 87/1000 | Loss: 0.00001315
Iteration 88/1000 | Loss: 0.00001315
Iteration 89/1000 | Loss: 0.00001315
Iteration 90/1000 | Loss: 0.00001315
Iteration 91/1000 | Loss: 0.00001315
Iteration 92/1000 | Loss: 0.00001315
Iteration 93/1000 | Loss: 0.00001314
Iteration 94/1000 | Loss: 0.00001314
Iteration 95/1000 | Loss: 0.00001314
Iteration 96/1000 | Loss: 0.00001314
Iteration 97/1000 | Loss: 0.00001314
Iteration 98/1000 | Loss: 0.00001314
Iteration 99/1000 | Loss: 0.00001314
Iteration 100/1000 | Loss: 0.00001314
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.3138820577296428e-05, 1.3138820577296428e-05, 1.3138820577296428e-05, 1.3138820577296428e-05, 1.3138820577296428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3138820577296428e-05

Optimization complete. Final v2v error: 3.0816123485565186 mm

Highest mean error: 3.6006996631622314 mm for frame 105

Lowest mean error: 2.80334734916687 mm for frame 151

Saving results

Total time: 37.10090470314026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407893
Iteration 2/25 | Loss: 0.00086838
Iteration 3/25 | Loss: 0.00077713
Iteration 4/25 | Loss: 0.00075026
Iteration 5/25 | Loss: 0.00074266
Iteration 6/25 | Loss: 0.00074091
Iteration 7/25 | Loss: 0.00074063
Iteration 8/25 | Loss: 0.00074063
Iteration 9/25 | Loss: 0.00074063
Iteration 10/25 | Loss: 0.00074063
Iteration 11/25 | Loss: 0.00074063
Iteration 12/25 | Loss: 0.00074063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007406280492432415, 0.0007406280492432415, 0.0007406280492432415, 0.0007406280492432415, 0.0007406280492432415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007406280492432415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52010810
Iteration 2/25 | Loss: 0.00082130
Iteration 3/25 | Loss: 0.00082130
Iteration 4/25 | Loss: 0.00082130
Iteration 5/25 | Loss: 0.00082130
Iteration 6/25 | Loss: 0.00082130
Iteration 7/25 | Loss: 0.00082130
Iteration 8/25 | Loss: 0.00082130
Iteration 9/25 | Loss: 0.00082130
Iteration 10/25 | Loss: 0.00082130
Iteration 11/25 | Loss: 0.00082130
Iteration 12/25 | Loss: 0.00082130
Iteration 13/25 | Loss: 0.00082130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008212957764044404, 0.0008212957764044404, 0.0008212957764044404, 0.0008212957764044404, 0.0008212957764044404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008212957764044404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082130
Iteration 2/1000 | Loss: 0.00003268
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001893
Iteration 6/1000 | Loss: 0.00001852
Iteration 7/1000 | Loss: 0.00001807
Iteration 8/1000 | Loss: 0.00001794
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001772
Iteration 11/1000 | Loss: 0.00001770
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001764
Iteration 14/1000 | Loss: 0.00001763
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001762
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001762
Iteration 22/1000 | Loss: 0.00001762
Iteration 23/1000 | Loss: 0.00001762
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001762
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001755
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001754
Iteration 35/1000 | Loss: 0.00001754
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001754
Iteration 41/1000 | Loss: 0.00001754
Iteration 42/1000 | Loss: 0.00001754
Iteration 43/1000 | Loss: 0.00001754
Iteration 44/1000 | Loss: 0.00001754
Iteration 45/1000 | Loss: 0.00001753
Iteration 46/1000 | Loss: 0.00001753
Iteration 47/1000 | Loss: 0.00001752
Iteration 48/1000 | Loss: 0.00001752
Iteration 49/1000 | Loss: 0.00001751
Iteration 50/1000 | Loss: 0.00001751
Iteration 51/1000 | Loss: 0.00001751
Iteration 52/1000 | Loss: 0.00001750
Iteration 53/1000 | Loss: 0.00001750
Iteration 54/1000 | Loss: 0.00001750
Iteration 55/1000 | Loss: 0.00001749
Iteration 56/1000 | Loss: 0.00001749
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001746
Iteration 71/1000 | Loss: 0.00001746
Iteration 72/1000 | Loss: 0.00001746
Iteration 73/1000 | Loss: 0.00001746
Iteration 74/1000 | Loss: 0.00001746
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001744
Iteration 81/1000 | Loss: 0.00001744
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001744
Iteration 86/1000 | Loss: 0.00001744
Iteration 87/1000 | Loss: 0.00001744
Iteration 88/1000 | Loss: 0.00001743
Iteration 89/1000 | Loss: 0.00001743
Iteration 90/1000 | Loss: 0.00001743
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001742
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001742
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001742
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001741
Iteration 105/1000 | Loss: 0.00001741
Iteration 106/1000 | Loss: 0.00001741
Iteration 107/1000 | Loss: 0.00001741
Iteration 108/1000 | Loss: 0.00001741
Iteration 109/1000 | Loss: 0.00001741
Iteration 110/1000 | Loss: 0.00001741
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001736
Iteration 124/1000 | Loss: 0.00001736
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001735
Iteration 128/1000 | Loss: 0.00001735
Iteration 129/1000 | Loss: 0.00001735
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001729
Iteration 137/1000 | Loss: 0.00001729
Iteration 138/1000 | Loss: 0.00001729
Iteration 139/1000 | Loss: 0.00001729
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001729
Iteration 144/1000 | Loss: 0.00001729
Iteration 145/1000 | Loss: 0.00001729
Iteration 146/1000 | Loss: 0.00001729
Iteration 147/1000 | Loss: 0.00001728
Iteration 148/1000 | Loss: 0.00001728
Iteration 149/1000 | Loss: 0.00001728
Iteration 150/1000 | Loss: 0.00001728
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Iteration 155/1000 | Loss: 0.00001727
Iteration 156/1000 | Loss: 0.00001727
Iteration 157/1000 | Loss: 0.00001727
Iteration 158/1000 | Loss: 0.00001727
Iteration 159/1000 | Loss: 0.00001726
Iteration 160/1000 | Loss: 0.00001726
Iteration 161/1000 | Loss: 0.00001726
Iteration 162/1000 | Loss: 0.00001726
Iteration 163/1000 | Loss: 0.00001726
Iteration 164/1000 | Loss: 0.00001726
Iteration 165/1000 | Loss: 0.00001726
Iteration 166/1000 | Loss: 0.00001726
Iteration 167/1000 | Loss: 0.00001726
Iteration 168/1000 | Loss: 0.00001726
Iteration 169/1000 | Loss: 0.00001726
Iteration 170/1000 | Loss: 0.00001726
Iteration 171/1000 | Loss: 0.00001725
Iteration 172/1000 | Loss: 0.00001725
Iteration 173/1000 | Loss: 0.00001725
Iteration 174/1000 | Loss: 0.00001725
Iteration 175/1000 | Loss: 0.00001725
Iteration 176/1000 | Loss: 0.00001724
Iteration 177/1000 | Loss: 0.00001724
Iteration 178/1000 | Loss: 0.00001724
Iteration 179/1000 | Loss: 0.00001724
Iteration 180/1000 | Loss: 0.00001724
Iteration 181/1000 | Loss: 0.00001724
Iteration 182/1000 | Loss: 0.00001724
Iteration 183/1000 | Loss: 0.00001724
Iteration 184/1000 | Loss: 0.00001724
Iteration 185/1000 | Loss: 0.00001724
Iteration 186/1000 | Loss: 0.00001724
Iteration 187/1000 | Loss: 0.00001724
Iteration 188/1000 | Loss: 0.00001724
Iteration 189/1000 | Loss: 0.00001724
Iteration 190/1000 | Loss: 0.00001724
Iteration 191/1000 | Loss: 0.00001724
Iteration 192/1000 | Loss: 0.00001724
Iteration 193/1000 | Loss: 0.00001724
Iteration 194/1000 | Loss: 0.00001724
Iteration 195/1000 | Loss: 0.00001724
Iteration 196/1000 | Loss: 0.00001724
Iteration 197/1000 | Loss: 0.00001724
Iteration 198/1000 | Loss: 0.00001724
Iteration 199/1000 | Loss: 0.00001724
Iteration 200/1000 | Loss: 0.00001724
Iteration 201/1000 | Loss: 0.00001724
Iteration 202/1000 | Loss: 0.00001724
Iteration 203/1000 | Loss: 0.00001724
Iteration 204/1000 | Loss: 0.00001724
Iteration 205/1000 | Loss: 0.00001724
Iteration 206/1000 | Loss: 0.00001724
Iteration 207/1000 | Loss: 0.00001724
Iteration 208/1000 | Loss: 0.00001724
Iteration 209/1000 | Loss: 0.00001724
Iteration 210/1000 | Loss: 0.00001724
Iteration 211/1000 | Loss: 0.00001724
Iteration 212/1000 | Loss: 0.00001724
Iteration 213/1000 | Loss: 0.00001724
Iteration 214/1000 | Loss: 0.00001724
Iteration 215/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.723939749354031e-05, 1.723939749354031e-05, 1.723939749354031e-05, 1.723939749354031e-05, 1.723939749354031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.723939749354031e-05

Optimization complete. Final v2v error: 3.499779462814331 mm

Highest mean error: 3.5394229888916016 mm for frame 92

Lowest mean error: 3.4003169536590576 mm for frame 115

Saving results

Total time: 35.14189124107361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126671
Iteration 2/25 | Loss: 0.00217082
Iteration 3/25 | Loss: 0.00266889
Iteration 4/25 | Loss: 0.00143998
Iteration 5/25 | Loss: 0.00098175
Iteration 6/25 | Loss: 0.00090710
Iteration 7/25 | Loss: 0.00089830
Iteration 8/25 | Loss: 0.00089364
Iteration 9/25 | Loss: 0.00089118
Iteration 10/25 | Loss: 0.00089003
Iteration 11/25 | Loss: 0.00088927
Iteration 12/25 | Loss: 0.00088871
Iteration 13/25 | Loss: 0.00088830
Iteration 14/25 | Loss: 0.00088812
Iteration 15/25 | Loss: 0.00088808
Iteration 16/25 | Loss: 0.00088808
Iteration 17/25 | Loss: 0.00088808
Iteration 18/25 | Loss: 0.00088808
Iteration 19/25 | Loss: 0.00088808
Iteration 20/25 | Loss: 0.00088808
Iteration 21/25 | Loss: 0.00088808
Iteration 22/25 | Loss: 0.00088807
Iteration 23/25 | Loss: 0.00088807
Iteration 24/25 | Loss: 0.00088807
Iteration 25/25 | Loss: 0.00088807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99557078
Iteration 2/25 | Loss: 0.00060536
Iteration 3/25 | Loss: 0.00060535
Iteration 4/25 | Loss: 0.00060535
Iteration 5/25 | Loss: 0.00060535
Iteration 6/25 | Loss: 0.00060535
Iteration 7/25 | Loss: 0.00060535
Iteration 8/25 | Loss: 0.00060535
Iteration 9/25 | Loss: 0.00060535
Iteration 10/25 | Loss: 0.00060535
Iteration 11/25 | Loss: 0.00060535
Iteration 12/25 | Loss: 0.00060535
Iteration 13/25 | Loss: 0.00060535
Iteration 14/25 | Loss: 0.00060535
Iteration 15/25 | Loss: 0.00060535
Iteration 16/25 | Loss: 0.00060535
Iteration 17/25 | Loss: 0.00060535
Iteration 18/25 | Loss: 0.00060535
Iteration 19/25 | Loss: 0.00060535
Iteration 20/25 | Loss: 0.00060535
Iteration 21/25 | Loss: 0.00060535
Iteration 22/25 | Loss: 0.00060535
Iteration 23/25 | Loss: 0.00060535
Iteration 24/25 | Loss: 0.00060535
Iteration 25/25 | Loss: 0.00060535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060535
Iteration 2/1000 | Loss: 0.00006498
Iteration 3/1000 | Loss: 0.00004234
Iteration 4/1000 | Loss: 0.00003751
Iteration 5/1000 | Loss: 0.00003556
Iteration 6/1000 | Loss: 0.00003449
Iteration 7/1000 | Loss: 0.00003371
Iteration 8/1000 | Loss: 0.00003310
Iteration 9/1000 | Loss: 0.00048515
Iteration 10/1000 | Loss: 0.00003340
Iteration 11/1000 | Loss: 0.00003140
Iteration 12/1000 | Loss: 0.00003042
Iteration 13/1000 | Loss: 0.00002985
Iteration 14/1000 | Loss: 0.00002955
Iteration 15/1000 | Loss: 0.00002941
Iteration 16/1000 | Loss: 0.00002941
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002913
Iteration 19/1000 | Loss: 0.00002912
Iteration 20/1000 | Loss: 0.00002899
Iteration 21/1000 | Loss: 0.00002898
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002890
Iteration 24/1000 | Loss: 0.00002889
Iteration 25/1000 | Loss: 0.00002888
Iteration 26/1000 | Loss: 0.00002888
Iteration 27/1000 | Loss: 0.00002887
Iteration 28/1000 | Loss: 0.00002887
Iteration 29/1000 | Loss: 0.00002887
Iteration 30/1000 | Loss: 0.00002887
Iteration 31/1000 | Loss: 0.00002886
Iteration 32/1000 | Loss: 0.00002886
Iteration 33/1000 | Loss: 0.00002885
Iteration 34/1000 | Loss: 0.00002885
Iteration 35/1000 | Loss: 0.00002883
Iteration 36/1000 | Loss: 0.00002882
Iteration 37/1000 | Loss: 0.00002881
Iteration 38/1000 | Loss: 0.00002880
Iteration 39/1000 | Loss: 0.00002880
Iteration 40/1000 | Loss: 0.00002879
Iteration 41/1000 | Loss: 0.00002879
Iteration 42/1000 | Loss: 0.00002878
Iteration 43/1000 | Loss: 0.00002878
Iteration 44/1000 | Loss: 0.00002877
Iteration 45/1000 | Loss: 0.00002877
Iteration 46/1000 | Loss: 0.00002876
Iteration 47/1000 | Loss: 0.00002876
Iteration 48/1000 | Loss: 0.00002876
Iteration 49/1000 | Loss: 0.00002875
Iteration 50/1000 | Loss: 0.00002875
Iteration 51/1000 | Loss: 0.00002875
Iteration 52/1000 | Loss: 0.00002874
Iteration 53/1000 | Loss: 0.00002874
Iteration 54/1000 | Loss: 0.00002873
Iteration 55/1000 | Loss: 0.00002872
Iteration 56/1000 | Loss: 0.00002872
Iteration 57/1000 | Loss: 0.00002871
Iteration 58/1000 | Loss: 0.00002871
Iteration 59/1000 | Loss: 0.00002871
Iteration 60/1000 | Loss: 0.00002871
Iteration 61/1000 | Loss: 0.00002871
Iteration 62/1000 | Loss: 0.00002871
Iteration 63/1000 | Loss: 0.00002870
Iteration 64/1000 | Loss: 0.00002870
Iteration 65/1000 | Loss: 0.00002870
Iteration 66/1000 | Loss: 0.00002869
Iteration 67/1000 | Loss: 0.00002869
Iteration 68/1000 | Loss: 0.00002869
Iteration 69/1000 | Loss: 0.00002869
Iteration 70/1000 | Loss: 0.00002869
Iteration 71/1000 | Loss: 0.00002868
Iteration 72/1000 | Loss: 0.00002868
Iteration 73/1000 | Loss: 0.00002868
Iteration 74/1000 | Loss: 0.00002868
Iteration 75/1000 | Loss: 0.00002868
Iteration 76/1000 | Loss: 0.00002868
Iteration 77/1000 | Loss: 0.00002868
Iteration 78/1000 | Loss: 0.00002868
Iteration 79/1000 | Loss: 0.00002868
Iteration 80/1000 | Loss: 0.00002868
Iteration 81/1000 | Loss: 0.00002868
Iteration 82/1000 | Loss: 0.00002868
Iteration 83/1000 | Loss: 0.00002868
Iteration 84/1000 | Loss: 0.00002868
Iteration 85/1000 | Loss: 0.00002868
Iteration 86/1000 | Loss: 0.00002868
Iteration 87/1000 | Loss: 0.00002868
Iteration 88/1000 | Loss: 0.00002868
Iteration 89/1000 | Loss: 0.00002868
Iteration 90/1000 | Loss: 0.00002868
Iteration 91/1000 | Loss: 0.00002868
Iteration 92/1000 | Loss: 0.00002868
Iteration 93/1000 | Loss: 0.00002868
Iteration 94/1000 | Loss: 0.00002868
Iteration 95/1000 | Loss: 0.00002868
Iteration 96/1000 | Loss: 0.00002868
Iteration 97/1000 | Loss: 0.00002868
Iteration 98/1000 | Loss: 0.00002868
Iteration 99/1000 | Loss: 0.00002868
Iteration 100/1000 | Loss: 0.00002868
Iteration 101/1000 | Loss: 0.00002868
Iteration 102/1000 | Loss: 0.00002868
Iteration 103/1000 | Loss: 0.00002868
Iteration 104/1000 | Loss: 0.00002868
Iteration 105/1000 | Loss: 0.00002868
Iteration 106/1000 | Loss: 0.00002868
Iteration 107/1000 | Loss: 0.00002868
Iteration 108/1000 | Loss: 0.00002868
Iteration 109/1000 | Loss: 0.00002868
Iteration 110/1000 | Loss: 0.00002868
Iteration 111/1000 | Loss: 0.00002868
Iteration 112/1000 | Loss: 0.00002868
Iteration 113/1000 | Loss: 0.00002868
Iteration 114/1000 | Loss: 0.00002868
Iteration 115/1000 | Loss: 0.00002868
Iteration 116/1000 | Loss: 0.00002868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.8678850867436267e-05, 2.8678850867436267e-05, 2.8678850867436267e-05, 2.8678850867436267e-05, 2.8678850867436267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8678850867436267e-05

Optimization complete. Final v2v error: 4.377732753753662 mm

Highest mean error: 5.295454978942871 mm for frame 121

Lowest mean error: 3.298553228378296 mm for frame 42

Saving results

Total time: 57.1723792552948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061352
Iteration 2/25 | Loss: 0.00234920
Iteration 3/25 | Loss: 0.00157008
Iteration 4/25 | Loss: 0.00126337
Iteration 5/25 | Loss: 0.00117888
Iteration 6/25 | Loss: 0.00114539
Iteration 7/25 | Loss: 0.00116674
Iteration 8/25 | Loss: 0.00103776
Iteration 9/25 | Loss: 0.00099367
Iteration 10/25 | Loss: 0.00100362
Iteration 11/25 | Loss: 0.00098850
Iteration 12/25 | Loss: 0.00093296
Iteration 13/25 | Loss: 0.00088767
Iteration 14/25 | Loss: 0.00087617
Iteration 15/25 | Loss: 0.00086704
Iteration 16/25 | Loss: 0.00086175
Iteration 17/25 | Loss: 0.00085640
Iteration 18/25 | Loss: 0.00085873
Iteration 19/25 | Loss: 0.00085636
Iteration 20/25 | Loss: 0.00085623
Iteration 21/25 | Loss: 0.00085364
Iteration 22/25 | Loss: 0.00085382
Iteration 23/25 | Loss: 0.00085443
Iteration 24/25 | Loss: 0.00085100
Iteration 25/25 | Loss: 0.00085058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55791450
Iteration 2/25 | Loss: 0.00126110
Iteration 3/25 | Loss: 0.00123896
Iteration 4/25 | Loss: 0.00123896
Iteration 5/25 | Loss: 0.00123896
Iteration 6/25 | Loss: 0.00123896
Iteration 7/25 | Loss: 0.00123896
Iteration 8/25 | Loss: 0.00123896
Iteration 9/25 | Loss: 0.00123896
Iteration 10/25 | Loss: 0.00123896
Iteration 11/25 | Loss: 0.00123896
Iteration 12/25 | Loss: 0.00123896
Iteration 13/25 | Loss: 0.00123896
Iteration 14/25 | Loss: 0.00123896
Iteration 15/25 | Loss: 0.00123896
Iteration 16/25 | Loss: 0.00123896
Iteration 17/25 | Loss: 0.00123896
Iteration 18/25 | Loss: 0.00123896
Iteration 19/25 | Loss: 0.00123896
Iteration 20/25 | Loss: 0.00123896
Iteration 21/25 | Loss: 0.00123896
Iteration 22/25 | Loss: 0.00123896
Iteration 23/25 | Loss: 0.00123896
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001238955301232636, 0.001238955301232636, 0.001238955301232636, 0.001238955301232636, 0.001238955301232636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001238955301232636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123896
Iteration 2/1000 | Loss: 0.00061369
Iteration 3/1000 | Loss: 0.00041448
Iteration 4/1000 | Loss: 0.00012948
Iteration 5/1000 | Loss: 0.00025944
Iteration 6/1000 | Loss: 0.00032352
Iteration 7/1000 | Loss: 0.00027947
Iteration 8/1000 | Loss: 0.00020837
Iteration 9/1000 | Loss: 0.00009171
Iteration 10/1000 | Loss: 0.00006287
Iteration 11/1000 | Loss: 0.00048438
Iteration 12/1000 | Loss: 0.00011516
Iteration 13/1000 | Loss: 0.00028082
Iteration 14/1000 | Loss: 0.00030901
Iteration 15/1000 | Loss: 0.00026346
Iteration 16/1000 | Loss: 0.00059094
Iteration 17/1000 | Loss: 0.00050576
Iteration 18/1000 | Loss: 0.00006711
Iteration 19/1000 | Loss: 0.00005019
Iteration 20/1000 | Loss: 0.00026047
Iteration 21/1000 | Loss: 0.00004712
Iteration 22/1000 | Loss: 0.00035936
Iteration 23/1000 | Loss: 0.00005142
Iteration 24/1000 | Loss: 0.00017573
Iteration 25/1000 | Loss: 0.00008498
Iteration 26/1000 | Loss: 0.00014317
Iteration 27/1000 | Loss: 0.00010389
Iteration 28/1000 | Loss: 0.00004079
Iteration 29/1000 | Loss: 0.00003731
Iteration 30/1000 | Loss: 0.00003520
Iteration 31/1000 | Loss: 0.00010217
Iteration 32/1000 | Loss: 0.00011137
Iteration 33/1000 | Loss: 0.00008497
Iteration 34/1000 | Loss: 0.00013562
Iteration 35/1000 | Loss: 0.00003987
Iteration 36/1000 | Loss: 0.00014633
Iteration 37/1000 | Loss: 0.00007692
Iteration 38/1000 | Loss: 0.00019634
Iteration 39/1000 | Loss: 0.00025743
Iteration 40/1000 | Loss: 0.00016392
Iteration 41/1000 | Loss: 0.00003299
Iteration 42/1000 | Loss: 0.00012594
Iteration 43/1000 | Loss: 0.00005994
Iteration 44/1000 | Loss: 0.00004240
Iteration 45/1000 | Loss: 0.00007969
Iteration 46/1000 | Loss: 0.00007912
Iteration 47/1000 | Loss: 0.00009733
Iteration 48/1000 | Loss: 0.00009728
Iteration 49/1000 | Loss: 0.00016650
Iteration 50/1000 | Loss: 0.00006914
Iteration 51/1000 | Loss: 0.00003599
Iteration 52/1000 | Loss: 0.00010384
Iteration 53/1000 | Loss: 0.00022942
Iteration 54/1000 | Loss: 0.00022037
Iteration 55/1000 | Loss: 0.00016680
Iteration 56/1000 | Loss: 0.00017173
Iteration 57/1000 | Loss: 0.00025373
Iteration 58/1000 | Loss: 0.00006118
Iteration 59/1000 | Loss: 0.00012290
Iteration 60/1000 | Loss: 0.00009584
Iteration 61/1000 | Loss: 0.00006192
Iteration 62/1000 | Loss: 0.00013179
Iteration 63/1000 | Loss: 0.00009363
Iteration 64/1000 | Loss: 0.00014676
Iteration 65/1000 | Loss: 0.00013208
Iteration 66/1000 | Loss: 0.00009808
Iteration 67/1000 | Loss: 0.00011741
Iteration 68/1000 | Loss: 0.00008224
Iteration 69/1000 | Loss: 0.00010896
Iteration 70/1000 | Loss: 0.00005635
Iteration 71/1000 | Loss: 0.00008515
Iteration 72/1000 | Loss: 0.00006152
Iteration 73/1000 | Loss: 0.00023051
Iteration 74/1000 | Loss: 0.00007436
Iteration 75/1000 | Loss: 0.00003367
Iteration 76/1000 | Loss: 0.00010653
Iteration 77/1000 | Loss: 0.00017491
Iteration 78/1000 | Loss: 0.00009759
Iteration 79/1000 | Loss: 0.00013568
Iteration 80/1000 | Loss: 0.00009283
Iteration 81/1000 | Loss: 0.00020679
Iteration 82/1000 | Loss: 0.00011185
Iteration 83/1000 | Loss: 0.00012493
Iteration 84/1000 | Loss: 0.00013506
Iteration 85/1000 | Loss: 0.00019937
Iteration 86/1000 | Loss: 0.00012124
Iteration 87/1000 | Loss: 0.00027270
Iteration 88/1000 | Loss: 0.00055960
Iteration 89/1000 | Loss: 0.00037395
Iteration 90/1000 | Loss: 0.00023441
Iteration 91/1000 | Loss: 0.00023431
Iteration 92/1000 | Loss: 0.00025459
Iteration 93/1000 | Loss: 0.00068824
Iteration 94/1000 | Loss: 0.00021837
Iteration 95/1000 | Loss: 0.00006902
Iteration 96/1000 | Loss: 0.00008943
Iteration 97/1000 | Loss: 0.00016602
Iteration 98/1000 | Loss: 0.00017862
Iteration 99/1000 | Loss: 0.00015755
Iteration 100/1000 | Loss: 0.00005195
Iteration 101/1000 | Loss: 0.00003840
Iteration 102/1000 | Loss: 0.00021747
Iteration 103/1000 | Loss: 0.00015137
Iteration 104/1000 | Loss: 0.00003195
Iteration 105/1000 | Loss: 0.00016107
Iteration 106/1000 | Loss: 0.00003104
Iteration 107/1000 | Loss: 0.00010522
Iteration 108/1000 | Loss: 0.00006603
Iteration 109/1000 | Loss: 0.00010071
Iteration 110/1000 | Loss: 0.00015188
Iteration 111/1000 | Loss: 0.00028901
Iteration 112/1000 | Loss: 0.00022736
Iteration 113/1000 | Loss: 0.00005752
Iteration 114/1000 | Loss: 0.00004691
Iteration 115/1000 | Loss: 0.00003126
Iteration 116/1000 | Loss: 0.00002893
Iteration 117/1000 | Loss: 0.00003689
Iteration 118/1000 | Loss: 0.00003096
Iteration 119/1000 | Loss: 0.00003047
Iteration 120/1000 | Loss: 0.00002531
Iteration 121/1000 | Loss: 0.00002719
Iteration 122/1000 | Loss: 0.00002627
Iteration 123/1000 | Loss: 0.00002855
Iteration 124/1000 | Loss: 0.00003230
Iteration 125/1000 | Loss: 0.00003182
Iteration 126/1000 | Loss: 0.00003473
Iteration 127/1000 | Loss: 0.00002730
Iteration 128/1000 | Loss: 0.00003817
Iteration 129/1000 | Loss: 0.00003344
Iteration 130/1000 | Loss: 0.00003556
Iteration 131/1000 | Loss: 0.00004028
Iteration 132/1000 | Loss: 0.00003613
Iteration 133/1000 | Loss: 0.00003431
Iteration 134/1000 | Loss: 0.00003554
Iteration 135/1000 | Loss: 0.00003969
Iteration 136/1000 | Loss: 0.00003702
Iteration 137/1000 | Loss: 0.00003656
Iteration 138/1000 | Loss: 0.00003498
Iteration 139/1000 | Loss: 0.00004062
Iteration 140/1000 | Loss: 0.00003891
Iteration 141/1000 | Loss: 0.00002576
Iteration 142/1000 | Loss: 0.00003197
Iteration 143/1000 | Loss: 0.00003497
Iteration 144/1000 | Loss: 0.00003597
Iteration 145/1000 | Loss: 0.00003476
Iteration 146/1000 | Loss: 0.00004407
Iteration 147/1000 | Loss: 0.00003382
Iteration 148/1000 | Loss: 0.00003707
Iteration 149/1000 | Loss: 0.00003431
Iteration 150/1000 | Loss: 0.00003341
Iteration 151/1000 | Loss: 0.00003723
Iteration 152/1000 | Loss: 0.00004378
Iteration 153/1000 | Loss: 0.00003410
Iteration 154/1000 | Loss: 0.00003217
Iteration 155/1000 | Loss: 0.00003167
Iteration 156/1000 | Loss: 0.00003548
Iteration 157/1000 | Loss: 0.00003381
Iteration 158/1000 | Loss: 0.00003184
Iteration 159/1000 | Loss: 0.00004515
Iteration 160/1000 | Loss: 0.00003344
Iteration 161/1000 | Loss: 0.00003265
Iteration 162/1000 | Loss: 0.00004048
Iteration 163/1000 | Loss: 0.00004053
Iteration 164/1000 | Loss: 0.00025685
Iteration 165/1000 | Loss: 0.00017822
Iteration 166/1000 | Loss: 0.00005639
Iteration 167/1000 | Loss: 0.00005471
Iteration 168/1000 | Loss: 0.00003374
Iteration 169/1000 | Loss: 0.00003011
Iteration 170/1000 | Loss: 0.00004279
Iteration 171/1000 | Loss: 0.00004672
Iteration 172/1000 | Loss: 0.00034652
Iteration 173/1000 | Loss: 0.00017217
Iteration 174/1000 | Loss: 0.00002713
Iteration 175/1000 | Loss: 0.00028427
Iteration 176/1000 | Loss: 0.00016229
Iteration 177/1000 | Loss: 0.00017514
Iteration 178/1000 | Loss: 0.00002895
Iteration 179/1000 | Loss: 0.00002490
Iteration 180/1000 | Loss: 0.00002308
Iteration 181/1000 | Loss: 0.00002208
Iteration 182/1000 | Loss: 0.00002140
Iteration 183/1000 | Loss: 0.00024596
Iteration 184/1000 | Loss: 0.00002932
Iteration 185/1000 | Loss: 0.00002134
Iteration 186/1000 | Loss: 0.00001933
Iteration 187/1000 | Loss: 0.00001892
Iteration 188/1000 | Loss: 0.00001863
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001842
Iteration 191/1000 | Loss: 0.00001830
Iteration 192/1000 | Loss: 0.00001826
Iteration 193/1000 | Loss: 0.00001822
Iteration 194/1000 | Loss: 0.00001822
Iteration 195/1000 | Loss: 0.00001819
Iteration 196/1000 | Loss: 0.00013915
Iteration 197/1000 | Loss: 0.00002122
Iteration 198/1000 | Loss: 0.00001995
Iteration 199/1000 | Loss: 0.00015136
Iteration 200/1000 | Loss: 0.00013662
Iteration 201/1000 | Loss: 0.00002663
Iteration 202/1000 | Loss: 0.00002211
Iteration 203/1000 | Loss: 0.00002073
Iteration 204/1000 | Loss: 0.00001960
Iteration 205/1000 | Loss: 0.00001899
Iteration 206/1000 | Loss: 0.00001870
Iteration 207/1000 | Loss: 0.00001843
Iteration 208/1000 | Loss: 0.00001840
Iteration 209/1000 | Loss: 0.00017059
Iteration 210/1000 | Loss: 0.00002403
Iteration 211/1000 | Loss: 0.00002067
Iteration 212/1000 | Loss: 0.00001896
Iteration 213/1000 | Loss: 0.00001840
Iteration 214/1000 | Loss: 0.00001818
Iteration 215/1000 | Loss: 0.00001814
Iteration 216/1000 | Loss: 0.00001813
Iteration 217/1000 | Loss: 0.00001813
Iteration 218/1000 | Loss: 0.00001812
Iteration 219/1000 | Loss: 0.00001812
Iteration 220/1000 | Loss: 0.00001812
Iteration 221/1000 | Loss: 0.00001812
Iteration 222/1000 | Loss: 0.00001812
Iteration 223/1000 | Loss: 0.00001812
Iteration 224/1000 | Loss: 0.00001811
Iteration 225/1000 | Loss: 0.00001811
Iteration 226/1000 | Loss: 0.00001811
Iteration 227/1000 | Loss: 0.00001811
Iteration 228/1000 | Loss: 0.00001811
Iteration 229/1000 | Loss: 0.00001811
Iteration 230/1000 | Loss: 0.00001811
Iteration 231/1000 | Loss: 0.00001811
Iteration 232/1000 | Loss: 0.00001811
Iteration 233/1000 | Loss: 0.00001811
Iteration 234/1000 | Loss: 0.00001811
Iteration 235/1000 | Loss: 0.00001811
Iteration 236/1000 | Loss: 0.00001810
Iteration 237/1000 | Loss: 0.00001810
Iteration 238/1000 | Loss: 0.00001810
Iteration 239/1000 | Loss: 0.00001810
Iteration 240/1000 | Loss: 0.00001810
Iteration 241/1000 | Loss: 0.00001810
Iteration 242/1000 | Loss: 0.00001810
Iteration 243/1000 | Loss: 0.00001810
Iteration 244/1000 | Loss: 0.00001809
Iteration 245/1000 | Loss: 0.00001809
Iteration 246/1000 | Loss: 0.00001809
Iteration 247/1000 | Loss: 0.00001809
Iteration 248/1000 | Loss: 0.00001809
Iteration 249/1000 | Loss: 0.00001807
Iteration 250/1000 | Loss: 0.00001807
Iteration 251/1000 | Loss: 0.00001807
Iteration 252/1000 | Loss: 0.00001807
Iteration 253/1000 | Loss: 0.00001807
Iteration 254/1000 | Loss: 0.00001807
Iteration 255/1000 | Loss: 0.00001807
Iteration 256/1000 | Loss: 0.00001807
Iteration 257/1000 | Loss: 0.00001807
Iteration 258/1000 | Loss: 0.00001806
Iteration 259/1000 | Loss: 0.00001806
Iteration 260/1000 | Loss: 0.00001806
Iteration 261/1000 | Loss: 0.00001806
Iteration 262/1000 | Loss: 0.00001804
Iteration 263/1000 | Loss: 0.00001804
Iteration 264/1000 | Loss: 0.00001804
Iteration 265/1000 | Loss: 0.00001803
Iteration 266/1000 | Loss: 0.00001803
Iteration 267/1000 | Loss: 0.00001803
Iteration 268/1000 | Loss: 0.00001803
Iteration 269/1000 | Loss: 0.00001803
Iteration 270/1000 | Loss: 0.00001803
Iteration 271/1000 | Loss: 0.00001803
Iteration 272/1000 | Loss: 0.00001803
Iteration 273/1000 | Loss: 0.00001803
Iteration 274/1000 | Loss: 0.00001803
Iteration 275/1000 | Loss: 0.00001802
Iteration 276/1000 | Loss: 0.00001802
Iteration 277/1000 | Loss: 0.00001802
Iteration 278/1000 | Loss: 0.00001802
Iteration 279/1000 | Loss: 0.00001802
Iteration 280/1000 | Loss: 0.00001802
Iteration 281/1000 | Loss: 0.00001802
Iteration 282/1000 | Loss: 0.00001801
Iteration 283/1000 | Loss: 0.00001801
Iteration 284/1000 | Loss: 0.00001801
Iteration 285/1000 | Loss: 0.00001801
Iteration 286/1000 | Loss: 0.00001801
Iteration 287/1000 | Loss: 0.00001801
Iteration 288/1000 | Loss: 0.00001801
Iteration 289/1000 | Loss: 0.00001801
Iteration 290/1000 | Loss: 0.00001801
Iteration 291/1000 | Loss: 0.00001801
Iteration 292/1000 | Loss: 0.00001801
Iteration 293/1000 | Loss: 0.00001801
Iteration 294/1000 | Loss: 0.00001800
Iteration 295/1000 | Loss: 0.00001800
Iteration 296/1000 | Loss: 0.00001800
Iteration 297/1000 | Loss: 0.00001800
Iteration 298/1000 | Loss: 0.00001800
Iteration 299/1000 | Loss: 0.00001800
Iteration 300/1000 | Loss: 0.00001800
Iteration 301/1000 | Loss: 0.00001800
Iteration 302/1000 | Loss: 0.00001800
Iteration 303/1000 | Loss: 0.00001800
Iteration 304/1000 | Loss: 0.00001800
Iteration 305/1000 | Loss: 0.00001800
Iteration 306/1000 | Loss: 0.00001800
Iteration 307/1000 | Loss: 0.00001800
Iteration 308/1000 | Loss: 0.00001800
Iteration 309/1000 | Loss: 0.00001799
Iteration 310/1000 | Loss: 0.00001799
Iteration 311/1000 | Loss: 0.00001799
Iteration 312/1000 | Loss: 0.00001799
Iteration 313/1000 | Loss: 0.00001799
Iteration 314/1000 | Loss: 0.00001799
Iteration 315/1000 | Loss: 0.00001799
Iteration 316/1000 | Loss: 0.00001799
Iteration 317/1000 | Loss: 0.00001799
Iteration 318/1000 | Loss: 0.00001799
Iteration 319/1000 | Loss: 0.00001798
Iteration 320/1000 | Loss: 0.00001798
Iteration 321/1000 | Loss: 0.00001798
Iteration 322/1000 | Loss: 0.00001798
Iteration 323/1000 | Loss: 0.00001798
Iteration 324/1000 | Loss: 0.00001798
Iteration 325/1000 | Loss: 0.00001798
Iteration 326/1000 | Loss: 0.00001798
Iteration 327/1000 | Loss: 0.00001798
Iteration 328/1000 | Loss: 0.00001798
Iteration 329/1000 | Loss: 0.00001798
Iteration 330/1000 | Loss: 0.00001798
Iteration 331/1000 | Loss: 0.00001798
Iteration 332/1000 | Loss: 0.00001798
Iteration 333/1000 | Loss: 0.00001798
Iteration 334/1000 | Loss: 0.00001798
Iteration 335/1000 | Loss: 0.00001798
Iteration 336/1000 | Loss: 0.00001798
Iteration 337/1000 | Loss: 0.00001798
Iteration 338/1000 | Loss: 0.00001798
Iteration 339/1000 | Loss: 0.00001798
Iteration 340/1000 | Loss: 0.00001798
Iteration 341/1000 | Loss: 0.00001798
Iteration 342/1000 | Loss: 0.00001798
Iteration 343/1000 | Loss: 0.00001798
Iteration 344/1000 | Loss: 0.00001798
Iteration 345/1000 | Loss: 0.00001798
Iteration 346/1000 | Loss: 0.00001798
Iteration 347/1000 | Loss: 0.00001798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [1.797937511582859e-05, 1.797937511582859e-05, 1.797937511582859e-05, 1.797937511582859e-05, 1.797937511582859e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.797937511582859e-05

Optimization complete. Final v2v error: 3.5031075477600098 mm

Highest mean error: 7.0790791511535645 mm for frame 146

Lowest mean error: 3.1274518966674805 mm for frame 114

Saving results

Total time: 398.9482612609863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00477669
Iteration 2/25 | Loss: 0.00083343
Iteration 3/25 | Loss: 0.00072239
Iteration 4/25 | Loss: 0.00070208
Iteration 5/25 | Loss: 0.00069607
Iteration 6/25 | Loss: 0.00069452
Iteration 7/25 | Loss: 0.00069436
Iteration 8/25 | Loss: 0.00069436
Iteration 9/25 | Loss: 0.00069436
Iteration 10/25 | Loss: 0.00069436
Iteration 11/25 | Loss: 0.00069436
Iteration 12/25 | Loss: 0.00069436
Iteration 13/25 | Loss: 0.00069435
Iteration 14/25 | Loss: 0.00069435
Iteration 15/25 | Loss: 0.00069435
Iteration 16/25 | Loss: 0.00069435
Iteration 17/25 | Loss: 0.00069435
Iteration 18/25 | Loss: 0.00069435
Iteration 19/25 | Loss: 0.00069435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006943476619198918, 0.0006943476619198918, 0.0006943476619198918, 0.0006943476619198918, 0.0006943476619198918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006943476619198918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90637064
Iteration 2/25 | Loss: 0.00082077
Iteration 3/25 | Loss: 0.00082077
Iteration 4/25 | Loss: 0.00082076
Iteration 5/25 | Loss: 0.00082076
Iteration 6/25 | Loss: 0.00082076
Iteration 7/25 | Loss: 0.00082076
Iteration 8/25 | Loss: 0.00082076
Iteration 9/25 | Loss: 0.00082076
Iteration 10/25 | Loss: 0.00082076
Iteration 11/25 | Loss: 0.00082076
Iteration 12/25 | Loss: 0.00082076
Iteration 13/25 | Loss: 0.00082076
Iteration 14/25 | Loss: 0.00082076
Iteration 15/25 | Loss: 0.00082076
Iteration 16/25 | Loss: 0.00082076
Iteration 17/25 | Loss: 0.00082076
Iteration 18/25 | Loss: 0.00082076
Iteration 19/25 | Loss: 0.00082076
Iteration 20/25 | Loss: 0.00082076
Iteration 21/25 | Loss: 0.00082076
Iteration 22/25 | Loss: 0.00082076
Iteration 23/25 | Loss: 0.00082076
Iteration 24/25 | Loss: 0.00082076
Iteration 25/25 | Loss: 0.00082076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082076
Iteration 2/1000 | Loss: 0.00002577
Iteration 3/1000 | Loss: 0.00001620
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001394
Iteration 7/1000 | Loss: 0.00001371
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001312
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001310
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001309
Iteration 21/1000 | Loss: 0.00001306
Iteration 22/1000 | Loss: 0.00001304
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001303
Iteration 26/1000 | Loss: 0.00001303
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001299
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001295
Iteration 43/1000 | Loss: 0.00001295
Iteration 44/1000 | Loss: 0.00001295
Iteration 45/1000 | Loss: 0.00001295
Iteration 46/1000 | Loss: 0.00001295
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001293
Iteration 62/1000 | Loss: 0.00001293
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001292
Iteration 69/1000 | Loss: 0.00001292
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001287
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001283
Iteration 104/1000 | Loss: 0.00001283
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001282
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001281
Iteration 110/1000 | Loss: 0.00001281
Iteration 111/1000 | Loss: 0.00001281
Iteration 112/1000 | Loss: 0.00001281
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001280
Iteration 119/1000 | Loss: 0.00001280
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001280
Iteration 124/1000 | Loss: 0.00001280
Iteration 125/1000 | Loss: 0.00001280
Iteration 126/1000 | Loss: 0.00001280
Iteration 127/1000 | Loss: 0.00001280
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Iteration 135/1000 | Loss: 0.00001279
Iteration 136/1000 | Loss: 0.00001279
Iteration 137/1000 | Loss: 0.00001279
Iteration 138/1000 | Loss: 0.00001279
Iteration 139/1000 | Loss: 0.00001278
Iteration 140/1000 | Loss: 0.00001278
Iteration 141/1000 | Loss: 0.00001278
Iteration 142/1000 | Loss: 0.00001278
Iteration 143/1000 | Loss: 0.00001278
Iteration 144/1000 | Loss: 0.00001278
Iteration 145/1000 | Loss: 0.00001278
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.2777196388924494e-05, 1.2777196388924494e-05, 1.2777196388924494e-05, 1.2777196388924494e-05, 1.2777196388924494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2777196388924494e-05

Optimization complete. Final v2v error: 3.0740268230438232 mm

Highest mean error: 3.697572708129883 mm for frame 187

Lowest mean error: 2.7899768352508545 mm for frame 149

Saving results

Total time: 38.8172869682312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849548
Iteration 2/25 | Loss: 0.00085231
Iteration 3/25 | Loss: 0.00071767
Iteration 4/25 | Loss: 0.00069922
Iteration 5/25 | Loss: 0.00069432
Iteration 6/25 | Loss: 0.00069265
Iteration 7/25 | Loss: 0.00069249
Iteration 8/25 | Loss: 0.00069249
Iteration 9/25 | Loss: 0.00069249
Iteration 10/25 | Loss: 0.00069249
Iteration 11/25 | Loss: 0.00069249
Iteration 12/25 | Loss: 0.00069249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006924876943230629, 0.0006924876943230629, 0.0006924876943230629, 0.0006924876943230629, 0.0006924876943230629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006924876943230629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53522170
Iteration 2/25 | Loss: 0.00080445
Iteration 3/25 | Loss: 0.00080445
Iteration 4/25 | Loss: 0.00080445
Iteration 5/25 | Loss: 0.00080445
Iteration 6/25 | Loss: 0.00080445
Iteration 7/25 | Loss: 0.00080445
Iteration 8/25 | Loss: 0.00080445
Iteration 9/25 | Loss: 0.00080445
Iteration 10/25 | Loss: 0.00080445
Iteration 11/25 | Loss: 0.00080445
Iteration 12/25 | Loss: 0.00080445
Iteration 13/25 | Loss: 0.00080445
Iteration 14/25 | Loss: 0.00080445
Iteration 15/25 | Loss: 0.00080445
Iteration 16/25 | Loss: 0.00080445
Iteration 17/25 | Loss: 0.00080445
Iteration 18/25 | Loss: 0.00080445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008044458809308708, 0.0008044458809308708, 0.0008044458809308708, 0.0008044458809308708, 0.0008044458809308708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008044458809308708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080445
Iteration 2/1000 | Loss: 0.00002386
Iteration 3/1000 | Loss: 0.00001473
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001247
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001215
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001196
Iteration 12/1000 | Loss: 0.00001188
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001159
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001158
Iteration 24/1000 | Loss: 0.00001157
Iteration 25/1000 | Loss: 0.00001156
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001145
Iteration 32/1000 | Loss: 0.00001144
Iteration 33/1000 | Loss: 0.00001144
Iteration 34/1000 | Loss: 0.00001144
Iteration 35/1000 | Loss: 0.00001144
Iteration 36/1000 | Loss: 0.00001144
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001143
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001141
Iteration 44/1000 | Loss: 0.00001141
Iteration 45/1000 | Loss: 0.00001141
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001141
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001140
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001140
Iteration 58/1000 | Loss: 0.00001140
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001135
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001133
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001133
Iteration 86/1000 | Loss: 0.00001133
Iteration 87/1000 | Loss: 0.00001133
Iteration 88/1000 | Loss: 0.00001133
Iteration 89/1000 | Loss: 0.00001132
Iteration 90/1000 | Loss: 0.00001132
Iteration 91/1000 | Loss: 0.00001132
Iteration 92/1000 | Loss: 0.00001132
Iteration 93/1000 | Loss: 0.00001132
Iteration 94/1000 | Loss: 0.00001131
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001131
Iteration 97/1000 | Loss: 0.00001131
Iteration 98/1000 | Loss: 0.00001130
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001130
Iteration 101/1000 | Loss: 0.00001129
Iteration 102/1000 | Loss: 0.00001129
Iteration 103/1000 | Loss: 0.00001129
Iteration 104/1000 | Loss: 0.00001129
Iteration 105/1000 | Loss: 0.00001129
Iteration 106/1000 | Loss: 0.00001129
Iteration 107/1000 | Loss: 0.00001129
Iteration 108/1000 | Loss: 0.00001128
Iteration 109/1000 | Loss: 0.00001128
Iteration 110/1000 | Loss: 0.00001127
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Iteration 113/1000 | Loss: 0.00001127
Iteration 114/1000 | Loss: 0.00001127
Iteration 115/1000 | Loss: 0.00001127
Iteration 116/1000 | Loss: 0.00001127
Iteration 117/1000 | Loss: 0.00001127
Iteration 118/1000 | Loss: 0.00001127
Iteration 119/1000 | Loss: 0.00001127
Iteration 120/1000 | Loss: 0.00001127
Iteration 121/1000 | Loss: 0.00001127
Iteration 122/1000 | Loss: 0.00001127
Iteration 123/1000 | Loss: 0.00001126
Iteration 124/1000 | Loss: 0.00001126
Iteration 125/1000 | Loss: 0.00001126
Iteration 126/1000 | Loss: 0.00001126
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001125
Iteration 132/1000 | Loss: 0.00001125
Iteration 133/1000 | Loss: 0.00001125
Iteration 134/1000 | Loss: 0.00001125
Iteration 135/1000 | Loss: 0.00001125
Iteration 136/1000 | Loss: 0.00001125
Iteration 137/1000 | Loss: 0.00001125
Iteration 138/1000 | Loss: 0.00001125
Iteration 139/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1245624591538217e-05, 1.1245624591538217e-05, 1.1245624591538217e-05, 1.1245624591538217e-05, 1.1245624591538217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1245624591538217e-05

Optimization complete. Final v2v error: 2.8161957263946533 mm

Highest mean error: 3.057560682296753 mm for frame 92

Lowest mean error: 2.6552770137786865 mm for frame 184

Saving results

Total time: 38.739269971847534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869729
Iteration 2/25 | Loss: 0.00097615
Iteration 3/25 | Loss: 0.00080748
Iteration 4/25 | Loss: 0.00078328
Iteration 5/25 | Loss: 0.00077659
Iteration 6/25 | Loss: 0.00077472
Iteration 7/25 | Loss: 0.00077430
Iteration 8/25 | Loss: 0.00077430
Iteration 9/25 | Loss: 0.00077430
Iteration 10/25 | Loss: 0.00077430
Iteration 11/25 | Loss: 0.00077430
Iteration 12/25 | Loss: 0.00077430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007743003661744297, 0.0007743003661744297, 0.0007743003661744297, 0.0007743003661744297, 0.0007743003661744297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007743003661744297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51809478
Iteration 2/25 | Loss: 0.00091028
Iteration 3/25 | Loss: 0.00091028
Iteration 4/25 | Loss: 0.00091028
Iteration 5/25 | Loss: 0.00091028
Iteration 6/25 | Loss: 0.00091028
Iteration 7/25 | Loss: 0.00091028
Iteration 8/25 | Loss: 0.00091028
Iteration 9/25 | Loss: 0.00091028
Iteration 10/25 | Loss: 0.00091028
Iteration 11/25 | Loss: 0.00091028
Iteration 12/25 | Loss: 0.00091028
Iteration 13/25 | Loss: 0.00091028
Iteration 14/25 | Loss: 0.00091028
Iteration 15/25 | Loss: 0.00091028
Iteration 16/25 | Loss: 0.00091028
Iteration 17/25 | Loss: 0.00091028
Iteration 18/25 | Loss: 0.00091028
Iteration 19/25 | Loss: 0.00091028
Iteration 20/25 | Loss: 0.00091028
Iteration 21/25 | Loss: 0.00091028
Iteration 22/25 | Loss: 0.00091028
Iteration 23/25 | Loss: 0.00091028
Iteration 24/25 | Loss: 0.00091028
Iteration 25/25 | Loss: 0.00091028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091028
Iteration 2/1000 | Loss: 0.00003395
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00002173
Iteration 5/1000 | Loss: 0.00002060
Iteration 6/1000 | Loss: 0.00001991
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001916
Iteration 9/1000 | Loss: 0.00001911
Iteration 10/1000 | Loss: 0.00001893
Iteration 11/1000 | Loss: 0.00001888
Iteration 12/1000 | Loss: 0.00001878
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001873
Iteration 15/1000 | Loss: 0.00001870
Iteration 16/1000 | Loss: 0.00001869
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001865
Iteration 22/1000 | Loss: 0.00001865
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001865
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001864
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001862
Iteration 35/1000 | Loss: 0.00001862
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001862
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001862
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001862
Iteration 51/1000 | Loss: 0.00001862
Iteration 52/1000 | Loss: 0.00001862
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001861
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001861
Iteration 58/1000 | Loss: 0.00001861
Iteration 59/1000 | Loss: 0.00001861
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001861
Iteration 63/1000 | Loss: 0.00001861
Iteration 64/1000 | Loss: 0.00001861
Iteration 65/1000 | Loss: 0.00001861
Iteration 66/1000 | Loss: 0.00001861
Iteration 67/1000 | Loss: 0.00001861
Iteration 68/1000 | Loss: 0.00001861
Iteration 69/1000 | Loss: 0.00001861
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.8611301129567437e-05, 1.8611301129567437e-05, 1.8611301129567437e-05, 1.8611301129567437e-05, 1.8611301129567437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8611301129567437e-05

Optimization complete. Final v2v error: 3.641145706176758 mm

Highest mean error: 4.1567301750183105 mm for frame 158

Lowest mean error: 3.2575876712799072 mm for frame 20

Saving results

Total time: 28.983457565307617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113056
Iteration 2/25 | Loss: 0.00248516
Iteration 3/25 | Loss: 0.00123844
Iteration 4/25 | Loss: 0.00104698
Iteration 5/25 | Loss: 0.00100164
Iteration 6/25 | Loss: 0.00094769
Iteration 7/25 | Loss: 0.00090549
Iteration 8/25 | Loss: 0.00086322
Iteration 9/25 | Loss: 0.00082781
Iteration 10/25 | Loss: 0.00081968
Iteration 11/25 | Loss: 0.00082345
Iteration 12/25 | Loss: 0.00082126
Iteration 13/25 | Loss: 0.00081856
Iteration 14/25 | Loss: 0.00080938
Iteration 15/25 | Loss: 0.00081045
Iteration 16/25 | Loss: 0.00080263
Iteration 17/25 | Loss: 0.00080322
Iteration 18/25 | Loss: 0.00080184
Iteration 19/25 | Loss: 0.00080200
Iteration 20/25 | Loss: 0.00079764
Iteration 21/25 | Loss: 0.00079472
Iteration 22/25 | Loss: 0.00078715
Iteration 23/25 | Loss: 0.00078710
Iteration 24/25 | Loss: 0.00078155
Iteration 25/25 | Loss: 0.00077781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50298536
Iteration 2/25 | Loss: 0.00082725
Iteration 3/25 | Loss: 0.00082725
Iteration 4/25 | Loss: 0.00082725
Iteration 5/25 | Loss: 0.00082725
Iteration 6/25 | Loss: 0.00082725
Iteration 7/25 | Loss: 0.00082725
Iteration 8/25 | Loss: 0.00082725
Iteration 9/25 | Loss: 0.00082725
Iteration 10/25 | Loss: 0.00082725
Iteration 11/25 | Loss: 0.00082725
Iteration 12/25 | Loss: 0.00082725
Iteration 13/25 | Loss: 0.00082725
Iteration 14/25 | Loss: 0.00082725
Iteration 15/25 | Loss: 0.00082725
Iteration 16/25 | Loss: 0.00082725
Iteration 17/25 | Loss: 0.00082725
Iteration 18/25 | Loss: 0.00082725
Iteration 19/25 | Loss: 0.00082725
Iteration 20/25 | Loss: 0.00082725
Iteration 21/25 | Loss: 0.00082725
Iteration 22/25 | Loss: 0.00082725
Iteration 23/25 | Loss: 0.00082725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000827251875307411, 0.000827251875307411, 0.000827251875307411, 0.000827251875307411, 0.000827251875307411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000827251875307411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082725
Iteration 2/1000 | Loss: 0.00005199
Iteration 3/1000 | Loss: 0.00005135
Iteration 4/1000 | Loss: 0.00004933
Iteration 5/1000 | Loss: 0.00004887
Iteration 6/1000 | Loss: 0.00003677
Iteration 7/1000 | Loss: 0.00003640
Iteration 8/1000 | Loss: 0.00006029
Iteration 9/1000 | Loss: 0.00005601
Iteration 10/1000 | Loss: 0.00005824
Iteration 11/1000 | Loss: 0.00005791
Iteration 12/1000 | Loss: 0.00005216
Iteration 13/1000 | Loss: 0.00005814
Iteration 14/1000 | Loss: 0.00005839
Iteration 15/1000 | Loss: 0.00006395
Iteration 16/1000 | Loss: 0.00005529
Iteration 17/1000 | Loss: 0.00004769
Iteration 18/1000 | Loss: 0.00005855
Iteration 19/1000 | Loss: 0.00005724
Iteration 20/1000 | Loss: 0.00005849
Iteration 21/1000 | Loss: 0.00005706
Iteration 22/1000 | Loss: 0.00006441
Iteration 23/1000 | Loss: 0.00005735
Iteration 24/1000 | Loss: 0.00005794
Iteration 25/1000 | Loss: 0.00005746
Iteration 26/1000 | Loss: 0.00005827
Iteration 27/1000 | Loss: 0.00005749
Iteration 28/1000 | Loss: 0.00005991
Iteration 29/1000 | Loss: 0.00006400
Iteration 30/1000 | Loss: 0.00005857
Iteration 31/1000 | Loss: 0.00006031
Iteration 32/1000 | Loss: 0.00006012
Iteration 33/1000 | Loss: 0.00005956
Iteration 34/1000 | Loss: 0.00004810
Iteration 35/1000 | Loss: 0.00005281
Iteration 36/1000 | Loss: 0.00005683
Iteration 37/1000 | Loss: 0.00005132
Iteration 38/1000 | Loss: 0.00005554
Iteration 39/1000 | Loss: 0.00005042
Iteration 40/1000 | Loss: 0.00004324
Iteration 41/1000 | Loss: 0.00005436
Iteration 42/1000 | Loss: 0.00005535
Iteration 43/1000 | Loss: 0.00004778
Iteration 44/1000 | Loss: 0.00005683
Iteration 45/1000 | Loss: 0.00004939
Iteration 46/1000 | Loss: 0.00005003
Iteration 47/1000 | Loss: 0.00004422
Iteration 48/1000 | Loss: 0.00006540
Iteration 49/1000 | Loss: 0.00005783
Iteration 50/1000 | Loss: 0.00005547
Iteration 51/1000 | Loss: 0.00006032
Iteration 52/1000 | Loss: 0.00006319
Iteration 53/1000 | Loss: 0.00006004
Iteration 54/1000 | Loss: 0.00006078
Iteration 55/1000 | Loss: 0.00005964
Iteration 56/1000 | Loss: 0.00004025
Iteration 57/1000 | Loss: 0.00005718
Iteration 58/1000 | Loss: 0.00005503
Iteration 59/1000 | Loss: 0.00005441
Iteration 60/1000 | Loss: 0.00006029
Iteration 61/1000 | Loss: 0.00005625
Iteration 62/1000 | Loss: 0.00005607
Iteration 63/1000 | Loss: 0.00005839
Iteration 64/1000 | Loss: 0.00005531
Iteration 65/1000 | Loss: 0.00005706
Iteration 66/1000 | Loss: 0.00006523
Iteration 67/1000 | Loss: 0.00005664
Iteration 68/1000 | Loss: 0.00005055
Iteration 69/1000 | Loss: 0.00006769
Iteration 70/1000 | Loss: 0.00005753
Iteration 71/1000 | Loss: 0.00005558
Iteration 72/1000 | Loss: 0.00005690
Iteration 73/1000 | Loss: 0.00006342
Iteration 74/1000 | Loss: 0.00005895
Iteration 75/1000 | Loss: 0.00005814
Iteration 76/1000 | Loss: 0.00005675
Iteration 77/1000 | Loss: 0.00006950
Iteration 78/1000 | Loss: 0.00004224
Iteration 79/1000 | Loss: 0.00005246
Iteration 80/1000 | Loss: 0.00006841
Iteration 81/1000 | Loss: 0.00005759
Iteration 82/1000 | Loss: 0.00006590
Iteration 83/1000 | Loss: 0.00006718
Iteration 84/1000 | Loss: 0.00006294
Iteration 85/1000 | Loss: 0.00005855
Iteration 86/1000 | Loss: 0.00005985
Iteration 87/1000 | Loss: 0.00007033
Iteration 88/1000 | Loss: 0.00005871
Iteration 89/1000 | Loss: 0.00005925
Iteration 90/1000 | Loss: 0.00005737
Iteration 91/1000 | Loss: 0.00005773
Iteration 92/1000 | Loss: 0.00005498
Iteration 93/1000 | Loss: 0.00005566
Iteration 94/1000 | Loss: 0.00005824
Iteration 95/1000 | Loss: 0.00005608
Iteration 96/1000 | Loss: 0.00006071
Iteration 97/1000 | Loss: 0.00005641
Iteration 98/1000 | Loss: 0.00005832
Iteration 99/1000 | Loss: 0.00006303
Iteration 100/1000 | Loss: 0.00006133
Iteration 101/1000 | Loss: 0.00005670
Iteration 102/1000 | Loss: 0.00005288
Iteration 103/1000 | Loss: 0.00004934
Iteration 104/1000 | Loss: 0.00005160
Iteration 105/1000 | Loss: 0.00004919
Iteration 106/1000 | Loss: 0.00006149
Iteration 107/1000 | Loss: 0.00006538
Iteration 108/1000 | Loss: 0.00006747
Iteration 109/1000 | Loss: 0.00006516
Iteration 110/1000 | Loss: 0.00006545
Iteration 111/1000 | Loss: 0.00006515
Iteration 112/1000 | Loss: 0.00006201
Iteration 113/1000 | Loss: 0.00005682
Iteration 114/1000 | Loss: 0.00005845
Iteration 115/1000 | Loss: 0.00005659
Iteration 116/1000 | Loss: 0.00006249
Iteration 117/1000 | Loss: 0.00005647
Iteration 118/1000 | Loss: 0.00006309
Iteration 119/1000 | Loss: 0.00005888
Iteration 120/1000 | Loss: 0.00006150
Iteration 121/1000 | Loss: 0.00005844
Iteration 122/1000 | Loss: 0.00005843
Iteration 123/1000 | Loss: 0.00005492
Iteration 124/1000 | Loss: 0.00005558
Iteration 125/1000 | Loss: 0.00005471
Iteration 126/1000 | Loss: 0.00005685
Iteration 127/1000 | Loss: 0.00005500
Iteration 128/1000 | Loss: 0.00005538
Iteration 129/1000 | Loss: 0.00005562
Iteration 130/1000 | Loss: 0.00005584
Iteration 131/1000 | Loss: 0.00005606
Iteration 132/1000 | Loss: 0.00005732
Iteration 133/1000 | Loss: 0.00005595
Iteration 134/1000 | Loss: 0.00008503
Iteration 135/1000 | Loss: 0.00005761
Iteration 136/1000 | Loss: 0.00005734
Iteration 137/1000 | Loss: 0.00005651
Iteration 138/1000 | Loss: 0.00005715
Iteration 139/1000 | Loss: 0.00005495
Iteration 140/1000 | Loss: 0.00005337
Iteration 141/1000 | Loss: 0.00005505
Iteration 142/1000 | Loss: 0.00005553
Iteration 143/1000 | Loss: 0.00005438
Iteration 144/1000 | Loss: 0.00005175
Iteration 145/1000 | Loss: 0.00005572
Iteration 146/1000 | Loss: 0.00005555
Iteration 147/1000 | Loss: 0.00005928
Iteration 148/1000 | Loss: 0.00005516
Iteration 149/1000 | Loss: 0.00005554
Iteration 150/1000 | Loss: 0.00005845
Iteration 151/1000 | Loss: 0.00005487
Iteration 152/1000 | Loss: 0.00005821
Iteration 153/1000 | Loss: 0.00005437
Iteration 154/1000 | Loss: 0.00005105
Iteration 155/1000 | Loss: 0.00005951
Iteration 156/1000 | Loss: 0.00005729
Iteration 157/1000 | Loss: 0.00006990
Iteration 158/1000 | Loss: 0.00005651
Iteration 159/1000 | Loss: 0.00006050
Iteration 160/1000 | Loss: 0.00005449
Iteration 161/1000 | Loss: 0.00005520
Iteration 162/1000 | Loss: 0.00005433
Iteration 163/1000 | Loss: 0.00005446
Iteration 164/1000 | Loss: 0.00006459
Iteration 165/1000 | Loss: 0.00005908
Iteration 166/1000 | Loss: 0.00003446
Iteration 167/1000 | Loss: 0.00002809
Iteration 168/1000 | Loss: 0.00002385
Iteration 169/1000 | Loss: 0.00002259
Iteration 170/1000 | Loss: 0.00002190
Iteration 171/1000 | Loss: 0.00002152
Iteration 172/1000 | Loss: 0.00002133
Iteration 173/1000 | Loss: 0.00002114
Iteration 174/1000 | Loss: 0.00002106
Iteration 175/1000 | Loss: 0.00002105
Iteration 176/1000 | Loss: 0.00002104
Iteration 177/1000 | Loss: 0.00002104
Iteration 178/1000 | Loss: 0.00002104
Iteration 179/1000 | Loss: 0.00002100
Iteration 180/1000 | Loss: 0.00002097
Iteration 181/1000 | Loss: 0.00002097
Iteration 182/1000 | Loss: 0.00002096
Iteration 183/1000 | Loss: 0.00002092
Iteration 184/1000 | Loss: 0.00002092
Iteration 185/1000 | Loss: 0.00002092
Iteration 186/1000 | Loss: 0.00002090
Iteration 187/1000 | Loss: 0.00002090
Iteration 188/1000 | Loss: 0.00002090
Iteration 189/1000 | Loss: 0.00002089
Iteration 190/1000 | Loss: 0.00002088
Iteration 191/1000 | Loss: 0.00002088
Iteration 192/1000 | Loss: 0.00002088
Iteration 193/1000 | Loss: 0.00002087
Iteration 194/1000 | Loss: 0.00002087
Iteration 195/1000 | Loss: 0.00002087
Iteration 196/1000 | Loss: 0.00002086
Iteration 197/1000 | Loss: 0.00002086
Iteration 198/1000 | Loss: 0.00002086
Iteration 199/1000 | Loss: 0.00002085
Iteration 200/1000 | Loss: 0.00002084
Iteration 201/1000 | Loss: 0.00002084
Iteration 202/1000 | Loss: 0.00002081
Iteration 203/1000 | Loss: 0.00002080
Iteration 204/1000 | Loss: 0.00002080
Iteration 205/1000 | Loss: 0.00002079
Iteration 206/1000 | Loss: 0.00002077
Iteration 207/1000 | Loss: 0.00002077
Iteration 208/1000 | Loss: 0.00002076
Iteration 209/1000 | Loss: 0.00002076
Iteration 210/1000 | Loss: 0.00002076
Iteration 211/1000 | Loss: 0.00002076
Iteration 212/1000 | Loss: 0.00002075
Iteration 213/1000 | Loss: 0.00002075
Iteration 214/1000 | Loss: 0.00002075
Iteration 215/1000 | Loss: 0.00002075
Iteration 216/1000 | Loss: 0.00002075
Iteration 217/1000 | Loss: 0.00002075
Iteration 218/1000 | Loss: 0.00002075
Iteration 219/1000 | Loss: 0.00002074
Iteration 220/1000 | Loss: 0.00002074
Iteration 221/1000 | Loss: 0.00002074
Iteration 222/1000 | Loss: 0.00002074
Iteration 223/1000 | Loss: 0.00002073
Iteration 224/1000 | Loss: 0.00002073
Iteration 225/1000 | Loss: 0.00002073
Iteration 226/1000 | Loss: 0.00002073
Iteration 227/1000 | Loss: 0.00002073
Iteration 228/1000 | Loss: 0.00002073
Iteration 229/1000 | Loss: 0.00002073
Iteration 230/1000 | Loss: 0.00002072
Iteration 231/1000 | Loss: 0.00002072
Iteration 232/1000 | Loss: 0.00002072
Iteration 233/1000 | Loss: 0.00002072
Iteration 234/1000 | Loss: 0.00002072
Iteration 235/1000 | Loss: 0.00002072
Iteration 236/1000 | Loss: 0.00002072
Iteration 237/1000 | Loss: 0.00002071
Iteration 238/1000 | Loss: 0.00002071
Iteration 239/1000 | Loss: 0.00002070
Iteration 240/1000 | Loss: 0.00002070
Iteration 241/1000 | Loss: 0.00002070
Iteration 242/1000 | Loss: 0.00002070
Iteration 243/1000 | Loss: 0.00002070
Iteration 244/1000 | Loss: 0.00002070
Iteration 245/1000 | Loss: 0.00002070
Iteration 246/1000 | Loss: 0.00002070
Iteration 247/1000 | Loss: 0.00002070
Iteration 248/1000 | Loss: 0.00002070
Iteration 249/1000 | Loss: 0.00002070
Iteration 250/1000 | Loss: 0.00002070
Iteration 251/1000 | Loss: 0.00002069
Iteration 252/1000 | Loss: 0.00002069
Iteration 253/1000 | Loss: 0.00002069
Iteration 254/1000 | Loss: 0.00002069
Iteration 255/1000 | Loss: 0.00002069
Iteration 256/1000 | Loss: 0.00002069
Iteration 257/1000 | Loss: 0.00002068
Iteration 258/1000 | Loss: 0.00002068
Iteration 259/1000 | Loss: 0.00002068
Iteration 260/1000 | Loss: 0.00002067
Iteration 261/1000 | Loss: 0.00002067
Iteration 262/1000 | Loss: 0.00002067
Iteration 263/1000 | Loss: 0.00002067
Iteration 264/1000 | Loss: 0.00002067
Iteration 265/1000 | Loss: 0.00002067
Iteration 266/1000 | Loss: 0.00002067
Iteration 267/1000 | Loss: 0.00002067
Iteration 268/1000 | Loss: 0.00002067
Iteration 269/1000 | Loss: 0.00002067
Iteration 270/1000 | Loss: 0.00002066
Iteration 271/1000 | Loss: 0.00002066
Iteration 272/1000 | Loss: 0.00002066
Iteration 273/1000 | Loss: 0.00002066
Iteration 274/1000 | Loss: 0.00002066
Iteration 275/1000 | Loss: 0.00002066
Iteration 276/1000 | Loss: 0.00002066
Iteration 277/1000 | Loss: 0.00002066
Iteration 278/1000 | Loss: 0.00002065
Iteration 279/1000 | Loss: 0.00002065
Iteration 280/1000 | Loss: 0.00002065
Iteration 281/1000 | Loss: 0.00002065
Iteration 282/1000 | Loss: 0.00002065
Iteration 283/1000 | Loss: 0.00002065
Iteration 284/1000 | Loss: 0.00002065
Iteration 285/1000 | Loss: 0.00002065
Iteration 286/1000 | Loss: 0.00002065
Iteration 287/1000 | Loss: 0.00002065
Iteration 288/1000 | Loss: 0.00002065
Iteration 289/1000 | Loss: 0.00002065
Iteration 290/1000 | Loss: 0.00002065
Iteration 291/1000 | Loss: 0.00002065
Iteration 292/1000 | Loss: 0.00002065
Iteration 293/1000 | Loss: 0.00002065
Iteration 294/1000 | Loss: 0.00002065
Iteration 295/1000 | Loss: 0.00002065
Iteration 296/1000 | Loss: 0.00002065
Iteration 297/1000 | Loss: 0.00002064
Iteration 298/1000 | Loss: 0.00002064
Iteration 299/1000 | Loss: 0.00002064
Iteration 300/1000 | Loss: 0.00002064
Iteration 301/1000 | Loss: 0.00002064
Iteration 302/1000 | Loss: 0.00002064
Iteration 303/1000 | Loss: 0.00002064
Iteration 304/1000 | Loss: 0.00002064
Iteration 305/1000 | Loss: 0.00002064
Iteration 306/1000 | Loss: 0.00002064
Iteration 307/1000 | Loss: 0.00002064
Iteration 308/1000 | Loss: 0.00002064
Iteration 309/1000 | Loss: 0.00002064
Iteration 310/1000 | Loss: 0.00002064
Iteration 311/1000 | Loss: 0.00002064
Iteration 312/1000 | Loss: 0.00002064
Iteration 313/1000 | Loss: 0.00002064
Iteration 314/1000 | Loss: 0.00002064
Iteration 315/1000 | Loss: 0.00002064
Iteration 316/1000 | Loss: 0.00002063
Iteration 317/1000 | Loss: 0.00002063
Iteration 318/1000 | Loss: 0.00002063
Iteration 319/1000 | Loss: 0.00002063
Iteration 320/1000 | Loss: 0.00002063
Iteration 321/1000 | Loss: 0.00002063
Iteration 322/1000 | Loss: 0.00002063
Iteration 323/1000 | Loss: 0.00002063
Iteration 324/1000 | Loss: 0.00002063
Iteration 325/1000 | Loss: 0.00002063
Iteration 326/1000 | Loss: 0.00002063
Iteration 327/1000 | Loss: 0.00002063
Iteration 328/1000 | Loss: 0.00002063
Iteration 329/1000 | Loss: 0.00002063
Iteration 330/1000 | Loss: 0.00002063
Iteration 331/1000 | Loss: 0.00002063
Iteration 332/1000 | Loss: 0.00002062
Iteration 333/1000 | Loss: 0.00002062
Iteration 334/1000 | Loss: 0.00002062
Iteration 335/1000 | Loss: 0.00002062
Iteration 336/1000 | Loss: 0.00002062
Iteration 337/1000 | Loss: 0.00002062
Iteration 338/1000 | Loss: 0.00002062
Iteration 339/1000 | Loss: 0.00002062
Iteration 340/1000 | Loss: 0.00002061
Iteration 341/1000 | Loss: 0.00002061
Iteration 342/1000 | Loss: 0.00002061
Iteration 343/1000 | Loss: 0.00002061
Iteration 344/1000 | Loss: 0.00002061
Iteration 345/1000 | Loss: 0.00002061
Iteration 346/1000 | Loss: 0.00002061
Iteration 347/1000 | Loss: 0.00002060
Iteration 348/1000 | Loss: 0.00002060
Iteration 349/1000 | Loss: 0.00002060
Iteration 350/1000 | Loss: 0.00002060
Iteration 351/1000 | Loss: 0.00002060
Iteration 352/1000 | Loss: 0.00002060
Iteration 353/1000 | Loss: 0.00002060
Iteration 354/1000 | Loss: 0.00002060
Iteration 355/1000 | Loss: 0.00002060
Iteration 356/1000 | Loss: 0.00002060
Iteration 357/1000 | Loss: 0.00002060
Iteration 358/1000 | Loss: 0.00002060
Iteration 359/1000 | Loss: 0.00002060
Iteration 360/1000 | Loss: 0.00002060
Iteration 361/1000 | Loss: 0.00002060
Iteration 362/1000 | Loss: 0.00002059
Iteration 363/1000 | Loss: 0.00002059
Iteration 364/1000 | Loss: 0.00002059
Iteration 365/1000 | Loss: 0.00002059
Iteration 366/1000 | Loss: 0.00002059
Iteration 367/1000 | Loss: 0.00002059
Iteration 368/1000 | Loss: 0.00002059
Iteration 369/1000 | Loss: 0.00002059
Iteration 370/1000 | Loss: 0.00002059
Iteration 371/1000 | Loss: 0.00002058
Iteration 372/1000 | Loss: 0.00002058
Iteration 373/1000 | Loss: 0.00002058
Iteration 374/1000 | Loss: 0.00002058
Iteration 375/1000 | Loss: 0.00002058
Iteration 376/1000 | Loss: 0.00002057
Iteration 377/1000 | Loss: 0.00002057
Iteration 378/1000 | Loss: 0.00002057
Iteration 379/1000 | Loss: 0.00002057
Iteration 380/1000 | Loss: 0.00002057
Iteration 381/1000 | Loss: 0.00002057
Iteration 382/1000 | Loss: 0.00002057
Iteration 383/1000 | Loss: 0.00002057
Iteration 384/1000 | Loss: 0.00002057
Iteration 385/1000 | Loss: 0.00002057
Iteration 386/1000 | Loss: 0.00002057
Iteration 387/1000 | Loss: 0.00002057
Iteration 388/1000 | Loss: 0.00002057
Iteration 389/1000 | Loss: 0.00002057
Iteration 390/1000 | Loss: 0.00002057
Iteration 391/1000 | Loss: 0.00002057
Iteration 392/1000 | Loss: 0.00002057
Iteration 393/1000 | Loss: 0.00002057
Iteration 394/1000 | Loss: 0.00002057
Iteration 395/1000 | Loss: 0.00002057
Iteration 396/1000 | Loss: 0.00002056
Iteration 397/1000 | Loss: 0.00002056
Iteration 398/1000 | Loss: 0.00002056
Iteration 399/1000 | Loss: 0.00002056
Iteration 400/1000 | Loss: 0.00002056
Iteration 401/1000 | Loss: 0.00002056
Iteration 402/1000 | Loss: 0.00002056
Iteration 403/1000 | Loss: 0.00002056
Iteration 404/1000 | Loss: 0.00002056
Iteration 405/1000 | Loss: 0.00002056
Iteration 406/1000 | Loss: 0.00002056
Iteration 407/1000 | Loss: 0.00002056
Iteration 408/1000 | Loss: 0.00002056
Iteration 409/1000 | Loss: 0.00002056
Iteration 410/1000 | Loss: 0.00002056
Iteration 411/1000 | Loss: 0.00002056
Iteration 412/1000 | Loss: 0.00002056
Iteration 413/1000 | Loss: 0.00002056
Iteration 414/1000 | Loss: 0.00002056
Iteration 415/1000 | Loss: 0.00002056
Iteration 416/1000 | Loss: 0.00002056
Iteration 417/1000 | Loss: 0.00002056
Iteration 418/1000 | Loss: 0.00002056
Iteration 419/1000 | Loss: 0.00002056
Iteration 420/1000 | Loss: 0.00002056
Iteration 421/1000 | Loss: 0.00002056
Iteration 422/1000 | Loss: 0.00002056
Iteration 423/1000 | Loss: 0.00002056
Iteration 424/1000 | Loss: 0.00002056
Iteration 425/1000 | Loss: 0.00002056
Iteration 426/1000 | Loss: 0.00002056
Iteration 427/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 427. Stopping optimization.
Last 5 losses: [2.0558190954034217e-05, 2.0558190954034217e-05, 2.0558190954034217e-05, 2.0558190954034217e-05, 2.0558190954034217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0558190954034217e-05

Optimization complete. Final v2v error: 3.778369665145874 mm

Highest mean error: 6.712636947631836 mm for frame 83

Lowest mean error: 3.5301008224487305 mm for frame 50

Saving results

Total time: 323.85277795791626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077155
Iteration 2/25 | Loss: 0.00252818
Iteration 3/25 | Loss: 0.00151176
Iteration 4/25 | Loss: 0.00124284
Iteration 5/25 | Loss: 0.00113827
Iteration 6/25 | Loss: 0.00113546
Iteration 7/25 | Loss: 0.00112578
Iteration 8/25 | Loss: 0.00105552
Iteration 9/25 | Loss: 0.00087470
Iteration 10/25 | Loss: 0.00081943
Iteration 11/25 | Loss: 0.00081090
Iteration 12/25 | Loss: 0.00076746
Iteration 13/25 | Loss: 0.00076079
Iteration 14/25 | Loss: 0.00075377
Iteration 15/25 | Loss: 0.00074131
Iteration 16/25 | Loss: 0.00073235
Iteration 17/25 | Loss: 0.00073527
Iteration 18/25 | Loss: 0.00073538
Iteration 19/25 | Loss: 0.00072866
Iteration 20/25 | Loss: 0.00073313
Iteration 21/25 | Loss: 0.00072855
Iteration 22/25 | Loss: 0.00072983
Iteration 23/25 | Loss: 0.00072659
Iteration 24/25 | Loss: 0.00072657
Iteration 25/25 | Loss: 0.00072657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.10413623
Iteration 2/25 | Loss: 0.00111003
Iteration 3/25 | Loss: 0.00086238
Iteration 4/25 | Loss: 0.00086238
Iteration 5/25 | Loss: 0.00086238
Iteration 6/25 | Loss: 0.00086238
Iteration 7/25 | Loss: 0.00086238
Iteration 8/25 | Loss: 0.00086238
Iteration 9/25 | Loss: 0.00086238
Iteration 10/25 | Loss: 0.00086238
Iteration 11/25 | Loss: 0.00086238
Iteration 12/25 | Loss: 0.00086238
Iteration 13/25 | Loss: 0.00086238
Iteration 14/25 | Loss: 0.00086238
Iteration 15/25 | Loss: 0.00086238
Iteration 16/25 | Loss: 0.00086238
Iteration 17/25 | Loss: 0.00086238
Iteration 18/25 | Loss: 0.00086238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000862379209138453, 0.000862379209138453, 0.000862379209138453, 0.000862379209138453, 0.000862379209138453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000862379209138453

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086238
Iteration 2/1000 | Loss: 0.00012847
Iteration 3/1000 | Loss: 0.00039978
Iteration 4/1000 | Loss: 0.00065411
Iteration 5/1000 | Loss: 0.00261465
Iteration 6/1000 | Loss: 0.00002354
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00034987
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00016903
Iteration 12/1000 | Loss: 0.00011469
Iteration 13/1000 | Loss: 0.00003678
Iteration 14/1000 | Loss: 0.00004646
Iteration 15/1000 | Loss: 0.00003340
Iteration 16/1000 | Loss: 0.00013013
Iteration 17/1000 | Loss: 0.00002524
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00004135
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001590
Iteration 23/1000 | Loss: 0.00009306
Iteration 24/1000 | Loss: 0.00016152
Iteration 25/1000 | Loss: 0.00011666
Iteration 26/1000 | Loss: 0.00003424
Iteration 27/1000 | Loss: 0.00005944
Iteration 28/1000 | Loss: 0.00001960
Iteration 29/1000 | Loss: 0.00004049
Iteration 30/1000 | Loss: 0.00015943
Iteration 31/1000 | Loss: 0.00032155
Iteration 32/1000 | Loss: 0.00348865
Iteration 33/1000 | Loss: 0.00011712
Iteration 34/1000 | Loss: 0.00010110
Iteration 35/1000 | Loss: 0.00005164
Iteration 36/1000 | Loss: 0.00003515
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00003214
Iteration 39/1000 | Loss: 0.00010748
Iteration 40/1000 | Loss: 0.00003697
Iteration 41/1000 | Loss: 0.00009174
Iteration 42/1000 | Loss: 0.00016777
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00002339
Iteration 45/1000 | Loss: 0.00009509
Iteration 46/1000 | Loss: 0.00005583
Iteration 47/1000 | Loss: 0.00005058
Iteration 48/1000 | Loss: 0.00008749
Iteration 49/1000 | Loss: 0.00001932
Iteration 50/1000 | Loss: 0.00003756
Iteration 51/1000 | Loss: 0.00004075
Iteration 52/1000 | Loss: 0.00012854
Iteration 53/1000 | Loss: 0.00004061
Iteration 54/1000 | Loss: 0.00002104
Iteration 55/1000 | Loss: 0.00002234
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001554
Iteration 58/1000 | Loss: 0.00001554
Iteration 59/1000 | Loss: 0.00001554
Iteration 60/1000 | Loss: 0.00001554
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001553
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001552
Iteration 71/1000 | Loss: 0.00001552
Iteration 72/1000 | Loss: 0.00001552
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001552
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001551
Iteration 78/1000 | Loss: 0.00001551
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001551
Iteration 84/1000 | Loss: 0.00001551
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.551362220197916e-05, 1.551362220197916e-05, 1.551362220197916e-05, 1.551362220197916e-05, 1.551362220197916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.551362220197916e-05

Optimization complete. Final v2v error: 3.2970004081726074 mm

Highest mean error: 4.095294952392578 mm for frame 176

Lowest mean error: 3.0033133029937744 mm for frame 61

Saving results

Total time: 125.53715944290161
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_daniel_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_daniel_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763578
Iteration 2/25 | Loss: 0.00123951
Iteration 3/25 | Loss: 0.00092035
Iteration 4/25 | Loss: 0.00084167
Iteration 5/25 | Loss: 0.00081991
Iteration 6/25 | Loss: 0.00081508
Iteration 7/25 | Loss: 0.00081362
Iteration 8/25 | Loss: 0.00081322
Iteration 9/25 | Loss: 0.00081309
Iteration 10/25 | Loss: 0.00081308
Iteration 11/25 | Loss: 0.00081308
Iteration 12/25 | Loss: 0.00081308
Iteration 13/25 | Loss: 0.00081308
Iteration 14/25 | Loss: 0.00081308
Iteration 15/25 | Loss: 0.00081308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008130759815685451, 0.0008130759815685451, 0.0008130759815685451, 0.0008130759815685451, 0.0008130759815685451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008130759815685451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45685649
Iteration 2/25 | Loss: 0.00092249
Iteration 3/25 | Loss: 0.00092249
Iteration 4/25 | Loss: 0.00092249
Iteration 5/25 | Loss: 0.00092249
Iteration 6/25 | Loss: 0.00092249
Iteration 7/25 | Loss: 0.00092249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0009224903187714517, 0.0009224903187714517, 0.0009224903187714517, 0.0009224903187714517, 0.0009224903187714517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009224903187714517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092249
Iteration 2/1000 | Loss: 0.00003582
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002443
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002282
Iteration 7/1000 | Loss: 0.00002241
Iteration 8/1000 | Loss: 0.00002203
Iteration 9/1000 | Loss: 0.00002179
Iteration 10/1000 | Loss: 0.00002172
Iteration 11/1000 | Loss: 0.00002172
Iteration 12/1000 | Loss: 0.00002171
Iteration 13/1000 | Loss: 0.00002170
Iteration 14/1000 | Loss: 0.00002164
Iteration 15/1000 | Loss: 0.00002164
Iteration 16/1000 | Loss: 0.00002149
Iteration 17/1000 | Loss: 0.00002149
Iteration 18/1000 | Loss: 0.00002148
Iteration 19/1000 | Loss: 0.00002142
Iteration 20/1000 | Loss: 0.00002142
Iteration 21/1000 | Loss: 0.00002141
Iteration 22/1000 | Loss: 0.00002141
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002138
Iteration 26/1000 | Loss: 0.00002138
Iteration 27/1000 | Loss: 0.00002138
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002137
Iteration 31/1000 | Loss: 0.00002137
Iteration 32/1000 | Loss: 0.00002137
Iteration 33/1000 | Loss: 0.00002137
Iteration 34/1000 | Loss: 0.00002137
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002137
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002137
Iteration 39/1000 | Loss: 0.00002136
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002132
Iteration 48/1000 | Loss: 0.00002132
Iteration 49/1000 | Loss: 0.00002132
Iteration 50/1000 | Loss: 0.00002131
Iteration 51/1000 | Loss: 0.00002130
Iteration 52/1000 | Loss: 0.00002130
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002129
Iteration 55/1000 | Loss: 0.00002129
Iteration 56/1000 | Loss: 0.00002129
Iteration 57/1000 | Loss: 0.00002128
Iteration 58/1000 | Loss: 0.00002128
Iteration 59/1000 | Loss: 0.00002128
Iteration 60/1000 | Loss: 0.00002127
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002127
Iteration 63/1000 | Loss: 0.00002126
Iteration 64/1000 | Loss: 0.00002126
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002126
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.126305116689764e-05, 2.126305116689764e-05, 2.126305116689764e-05, 2.126305116689764e-05, 2.126305116689764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.126305116689764e-05

Optimization complete. Final v2v error: 3.898512363433838 mm

Highest mean error: 4.295856952667236 mm for frame 233

Lowest mean error: 3.4454057216644287 mm for frame 144

Saving results

Total time: 39.69279408454895
