Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=276, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15456-15511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380879
Iteration 2/25 | Loss: 0.00092989
Iteration 3/25 | Loss: 0.00083527
Iteration 4/25 | Loss: 0.00082635
Iteration 5/25 | Loss: 0.00082303
Iteration 6/25 | Loss: 0.00082214
Iteration 7/25 | Loss: 0.00082214
Iteration 8/25 | Loss: 0.00082214
Iteration 9/25 | Loss: 0.00082214
Iteration 10/25 | Loss: 0.00082214
Iteration 11/25 | Loss: 0.00082214
Iteration 12/25 | Loss: 0.00082214
Iteration 13/25 | Loss: 0.00082214
Iteration 14/25 | Loss: 0.00082214
Iteration 15/25 | Loss: 0.00082214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008221374591812491, 0.0008221374591812491, 0.0008221374591812491, 0.0008221374591812491, 0.0008221374591812491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008221374591812491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55318177
Iteration 2/25 | Loss: 0.00063620
Iteration 3/25 | Loss: 0.00063619
Iteration 4/25 | Loss: 0.00063619
Iteration 5/25 | Loss: 0.00063619
Iteration 6/25 | Loss: 0.00063619
Iteration 7/25 | Loss: 0.00063619
Iteration 8/25 | Loss: 0.00063619
Iteration 9/25 | Loss: 0.00063619
Iteration 10/25 | Loss: 0.00063619
Iteration 11/25 | Loss: 0.00063619
Iteration 12/25 | Loss: 0.00063619
Iteration 13/25 | Loss: 0.00063619
Iteration 14/25 | Loss: 0.00063619
Iteration 15/25 | Loss: 0.00063619
Iteration 16/25 | Loss: 0.00063619
Iteration 17/25 | Loss: 0.00063619
Iteration 18/25 | Loss: 0.00063619
Iteration 19/25 | Loss: 0.00063619
Iteration 20/25 | Loss: 0.00063619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006361923296935856, 0.0006361923296935856, 0.0006361923296935856, 0.0006361923296935856, 0.0006361923296935856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006361923296935856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063619
Iteration 2/1000 | Loss: 0.00002082
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001345
Iteration 5/1000 | Loss: 0.00001269
Iteration 6/1000 | Loss: 0.00001265
Iteration 7/1000 | Loss: 0.00001236
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001217
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001200
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001196
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001180
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001177
Iteration 44/1000 | Loss: 0.00001177
Iteration 45/1000 | Loss: 0.00001176
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001174
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001171
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001169
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001167
Iteration 72/1000 | Loss: 0.00001167
Iteration 73/1000 | Loss: 0.00001166
Iteration 74/1000 | Loss: 0.00001166
Iteration 75/1000 | Loss: 0.00001166
Iteration 76/1000 | Loss: 0.00001166
Iteration 77/1000 | Loss: 0.00001166
Iteration 78/1000 | Loss: 0.00001166
Iteration 79/1000 | Loss: 0.00001166
Iteration 80/1000 | Loss: 0.00001165
Iteration 81/1000 | Loss: 0.00001165
Iteration 82/1000 | Loss: 0.00001165
Iteration 83/1000 | Loss: 0.00001165
Iteration 84/1000 | Loss: 0.00001165
Iteration 85/1000 | Loss: 0.00001164
Iteration 86/1000 | Loss: 0.00001164
Iteration 87/1000 | Loss: 0.00001164
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001160
Iteration 114/1000 | Loss: 0.00001160
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001159
Iteration 122/1000 | Loss: 0.00001159
Iteration 123/1000 | Loss: 0.00001159
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001158
Iteration 130/1000 | Loss: 0.00001158
Iteration 131/1000 | Loss: 0.00001158
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001155
Iteration 145/1000 | Loss: 0.00001155
Iteration 146/1000 | Loss: 0.00001155
Iteration 147/1000 | Loss: 0.00001155
Iteration 148/1000 | Loss: 0.00001155
Iteration 149/1000 | Loss: 0.00001155
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001154
Iteration 156/1000 | Loss: 0.00001154
Iteration 157/1000 | Loss: 0.00001154
Iteration 158/1000 | Loss: 0.00001154
Iteration 159/1000 | Loss: 0.00001154
Iteration 160/1000 | Loss: 0.00001154
Iteration 161/1000 | Loss: 0.00001154
Iteration 162/1000 | Loss: 0.00001154
Iteration 163/1000 | Loss: 0.00001154
Iteration 164/1000 | Loss: 0.00001154
Iteration 165/1000 | Loss: 0.00001154
Iteration 166/1000 | Loss: 0.00001154
Iteration 167/1000 | Loss: 0.00001154
Iteration 168/1000 | Loss: 0.00001154
Iteration 169/1000 | Loss: 0.00001154
Iteration 170/1000 | Loss: 0.00001154
Iteration 171/1000 | Loss: 0.00001154
Iteration 172/1000 | Loss: 0.00001154
Iteration 173/1000 | Loss: 0.00001154
Iteration 174/1000 | Loss: 0.00001154
Iteration 175/1000 | Loss: 0.00001153
Iteration 176/1000 | Loss: 0.00001153
Iteration 177/1000 | Loss: 0.00001153
Iteration 178/1000 | Loss: 0.00001153
Iteration 179/1000 | Loss: 0.00001153
Iteration 180/1000 | Loss: 0.00001153
Iteration 181/1000 | Loss: 0.00001153
Iteration 182/1000 | Loss: 0.00001153
Iteration 183/1000 | Loss: 0.00001153
Iteration 184/1000 | Loss: 0.00001153
Iteration 185/1000 | Loss: 0.00001153
Iteration 186/1000 | Loss: 0.00001153
Iteration 187/1000 | Loss: 0.00001153
Iteration 188/1000 | Loss: 0.00001153
Iteration 189/1000 | Loss: 0.00001153
Iteration 190/1000 | Loss: 0.00001153
Iteration 191/1000 | Loss: 0.00001153
Iteration 192/1000 | Loss: 0.00001153
Iteration 193/1000 | Loss: 0.00001153
Iteration 194/1000 | Loss: 0.00001153
Iteration 195/1000 | Loss: 0.00001153
Iteration 196/1000 | Loss: 0.00001153
Iteration 197/1000 | Loss: 0.00001152
Iteration 198/1000 | Loss: 0.00001152
Iteration 199/1000 | Loss: 0.00001152
Iteration 200/1000 | Loss: 0.00001152
Iteration 201/1000 | Loss: 0.00001152
Iteration 202/1000 | Loss: 0.00001152
Iteration 203/1000 | Loss: 0.00001152
Iteration 204/1000 | Loss: 0.00001152
Iteration 205/1000 | Loss: 0.00001152
Iteration 206/1000 | Loss: 0.00001152
Iteration 207/1000 | Loss: 0.00001152
Iteration 208/1000 | Loss: 0.00001152
Iteration 209/1000 | Loss: 0.00001152
Iteration 210/1000 | Loss: 0.00001152
Iteration 211/1000 | Loss: 0.00001152
Iteration 212/1000 | Loss: 0.00001152
Iteration 213/1000 | Loss: 0.00001152
Iteration 214/1000 | Loss: 0.00001152
Iteration 215/1000 | Loss: 0.00001152
Iteration 216/1000 | Loss: 0.00001151
Iteration 217/1000 | Loss: 0.00001151
Iteration 218/1000 | Loss: 0.00001151
Iteration 219/1000 | Loss: 0.00001151
Iteration 220/1000 | Loss: 0.00001151
Iteration 221/1000 | Loss: 0.00001151
Iteration 222/1000 | Loss: 0.00001151
Iteration 223/1000 | Loss: 0.00001151
Iteration 224/1000 | Loss: 0.00001151
Iteration 225/1000 | Loss: 0.00001151
Iteration 226/1000 | Loss: 0.00001151
Iteration 227/1000 | Loss: 0.00001151
Iteration 228/1000 | Loss: 0.00001151
Iteration 229/1000 | Loss: 0.00001151
Iteration 230/1000 | Loss: 0.00001151
Iteration 231/1000 | Loss: 0.00001151
Iteration 232/1000 | Loss: 0.00001151
Iteration 233/1000 | Loss: 0.00001151
Iteration 234/1000 | Loss: 0.00001151
Iteration 235/1000 | Loss: 0.00001151
Iteration 236/1000 | Loss: 0.00001150
Iteration 237/1000 | Loss: 0.00001150
Iteration 238/1000 | Loss: 0.00001150
Iteration 239/1000 | Loss: 0.00001150
Iteration 240/1000 | Loss: 0.00001150
Iteration 241/1000 | Loss: 0.00001150
Iteration 242/1000 | Loss: 0.00001150
Iteration 243/1000 | Loss: 0.00001150
Iteration 244/1000 | Loss: 0.00001150
Iteration 245/1000 | Loss: 0.00001150
Iteration 246/1000 | Loss: 0.00001150
Iteration 247/1000 | Loss: 0.00001150
Iteration 248/1000 | Loss: 0.00001150
Iteration 249/1000 | Loss: 0.00001150
Iteration 250/1000 | Loss: 0.00001150
Iteration 251/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.1501353583298624e-05, 1.1501353583298624e-05, 1.1501353583298624e-05, 1.1501353583298624e-05, 1.1501353583298624e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1501353583298624e-05

Optimization complete. Final v2v error: 2.8567752838134766 mm

Highest mean error: 3.0532429218292236 mm for frame 12

Lowest mean error: 2.662548065185547 mm for frame 133

Saving results

Total time: 38.59903383255005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386258
Iteration 2/25 | Loss: 0.00106962
Iteration 3/25 | Loss: 0.00089725
Iteration 4/25 | Loss: 0.00086425
Iteration 5/25 | Loss: 0.00085790
Iteration 6/25 | Loss: 0.00085651
Iteration 7/25 | Loss: 0.00085607
Iteration 8/25 | Loss: 0.00085607
Iteration 9/25 | Loss: 0.00085607
Iteration 10/25 | Loss: 0.00085607
Iteration 11/25 | Loss: 0.00085607
Iteration 12/25 | Loss: 0.00085607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008560697315260768, 0.0008560697315260768, 0.0008560697315260768, 0.0008560697315260768, 0.0008560697315260768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008560697315260768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49049783
Iteration 2/25 | Loss: 0.00061303
Iteration 3/25 | Loss: 0.00061303
Iteration 4/25 | Loss: 0.00061303
Iteration 5/25 | Loss: 0.00061303
Iteration 6/25 | Loss: 0.00061303
Iteration 7/25 | Loss: 0.00061303
Iteration 8/25 | Loss: 0.00061303
Iteration 9/25 | Loss: 0.00061303
Iteration 10/25 | Loss: 0.00061303
Iteration 11/25 | Loss: 0.00061303
Iteration 12/25 | Loss: 0.00061303
Iteration 13/25 | Loss: 0.00061303
Iteration 14/25 | Loss: 0.00061303
Iteration 15/25 | Loss: 0.00061303
Iteration 16/25 | Loss: 0.00061303
Iteration 17/25 | Loss: 0.00061303
Iteration 18/25 | Loss: 0.00061303
Iteration 19/25 | Loss: 0.00061303
Iteration 20/25 | Loss: 0.00061303
Iteration 21/25 | Loss: 0.00061303
Iteration 22/25 | Loss: 0.00061303
Iteration 23/25 | Loss: 0.00061303
Iteration 24/25 | Loss: 0.00061303
Iteration 25/25 | Loss: 0.00061303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006130279507488012, 0.0006130279507488012, 0.0006130279507488012, 0.0006130279507488012, 0.0006130279507488012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006130279507488012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061303
Iteration 2/1000 | Loss: 0.00005302
Iteration 3/1000 | Loss: 0.00003855
Iteration 4/1000 | Loss: 0.00003121
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002721
Iteration 7/1000 | Loss: 0.00002608
Iteration 8/1000 | Loss: 0.00002539
Iteration 9/1000 | Loss: 0.00002469
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002357
Iteration 13/1000 | Loss: 0.00002332
Iteration 14/1000 | Loss: 0.00002314
Iteration 15/1000 | Loss: 0.00002299
Iteration 16/1000 | Loss: 0.00002285
Iteration 17/1000 | Loss: 0.00002283
Iteration 18/1000 | Loss: 0.00002282
Iteration 19/1000 | Loss: 0.00002281
Iteration 20/1000 | Loss: 0.00002276
Iteration 21/1000 | Loss: 0.00002276
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002274
Iteration 24/1000 | Loss: 0.00002274
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002269
Iteration 27/1000 | Loss: 0.00002267
Iteration 28/1000 | Loss: 0.00002266
Iteration 29/1000 | Loss: 0.00002262
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002260
Iteration 32/1000 | Loss: 0.00002260
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002259
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002258
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002256
Iteration 49/1000 | Loss: 0.00002256
Iteration 50/1000 | Loss: 0.00002255
Iteration 51/1000 | Loss: 0.00002255
Iteration 52/1000 | Loss: 0.00002255
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002255
Iteration 55/1000 | Loss: 0.00002255
Iteration 56/1000 | Loss: 0.00002254
Iteration 57/1000 | Loss: 0.00002254
Iteration 58/1000 | Loss: 0.00002253
Iteration 59/1000 | Loss: 0.00002253
Iteration 60/1000 | Loss: 0.00002253
Iteration 61/1000 | Loss: 0.00002252
Iteration 62/1000 | Loss: 0.00002252
Iteration 63/1000 | Loss: 0.00002252
Iteration 64/1000 | Loss: 0.00002252
Iteration 65/1000 | Loss: 0.00002252
Iteration 66/1000 | Loss: 0.00002251
Iteration 67/1000 | Loss: 0.00002251
Iteration 68/1000 | Loss: 0.00002251
Iteration 69/1000 | Loss: 0.00002251
Iteration 70/1000 | Loss: 0.00002251
Iteration 71/1000 | Loss: 0.00002251
Iteration 72/1000 | Loss: 0.00002250
Iteration 73/1000 | Loss: 0.00002250
Iteration 74/1000 | Loss: 0.00002250
Iteration 75/1000 | Loss: 0.00002250
Iteration 76/1000 | Loss: 0.00002250
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002249
Iteration 80/1000 | Loss: 0.00002249
Iteration 81/1000 | Loss: 0.00002249
Iteration 82/1000 | Loss: 0.00002249
Iteration 83/1000 | Loss: 0.00002248
Iteration 84/1000 | Loss: 0.00002248
Iteration 85/1000 | Loss: 0.00002248
Iteration 86/1000 | Loss: 0.00002248
Iteration 87/1000 | Loss: 0.00002248
Iteration 88/1000 | Loss: 0.00002248
Iteration 89/1000 | Loss: 0.00002248
Iteration 90/1000 | Loss: 0.00002248
Iteration 91/1000 | Loss: 0.00002248
Iteration 92/1000 | Loss: 0.00002248
Iteration 93/1000 | Loss: 0.00002247
Iteration 94/1000 | Loss: 0.00002247
Iteration 95/1000 | Loss: 0.00002247
Iteration 96/1000 | Loss: 0.00002247
Iteration 97/1000 | Loss: 0.00002247
Iteration 98/1000 | Loss: 0.00002247
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002246
Iteration 101/1000 | Loss: 0.00002246
Iteration 102/1000 | Loss: 0.00002246
Iteration 103/1000 | Loss: 0.00002246
Iteration 104/1000 | Loss: 0.00002246
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002246
Iteration 112/1000 | Loss: 0.00002246
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002245
Iteration 117/1000 | Loss: 0.00002245
Iteration 118/1000 | Loss: 0.00002245
Iteration 119/1000 | Loss: 0.00002245
Iteration 120/1000 | Loss: 0.00002245
Iteration 121/1000 | Loss: 0.00002245
Iteration 122/1000 | Loss: 0.00002245
Iteration 123/1000 | Loss: 0.00002245
Iteration 124/1000 | Loss: 0.00002245
Iteration 125/1000 | Loss: 0.00002245
Iteration 126/1000 | Loss: 0.00002245
Iteration 127/1000 | Loss: 0.00002245
Iteration 128/1000 | Loss: 0.00002245
Iteration 129/1000 | Loss: 0.00002245
Iteration 130/1000 | Loss: 0.00002245
Iteration 131/1000 | Loss: 0.00002245
Iteration 132/1000 | Loss: 0.00002245
Iteration 133/1000 | Loss: 0.00002244
Iteration 134/1000 | Loss: 0.00002244
Iteration 135/1000 | Loss: 0.00002244
Iteration 136/1000 | Loss: 0.00002244
Iteration 137/1000 | Loss: 0.00002244
Iteration 138/1000 | Loss: 0.00002244
Iteration 139/1000 | Loss: 0.00002244
Iteration 140/1000 | Loss: 0.00002244
Iteration 141/1000 | Loss: 0.00002244
Iteration 142/1000 | Loss: 0.00002244
Iteration 143/1000 | Loss: 0.00002244
Iteration 144/1000 | Loss: 0.00002244
Iteration 145/1000 | Loss: 0.00002244
Iteration 146/1000 | Loss: 0.00002244
Iteration 147/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.243899325549137e-05, 2.243899325549137e-05, 2.243899325549137e-05, 2.243899325549137e-05, 2.243899325549137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.243899325549137e-05

Optimization complete. Final v2v error: 3.923950672149658 mm

Highest mean error: 4.150532245635986 mm for frame 52

Lowest mean error: 3.29990816116333 mm for frame 11

Saving results

Total time: 46.34196639060974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832758
Iteration 2/25 | Loss: 0.00112589
Iteration 3/25 | Loss: 0.00095358
Iteration 4/25 | Loss: 0.00092362
Iteration 5/25 | Loss: 0.00091554
Iteration 6/25 | Loss: 0.00091287
Iteration 7/25 | Loss: 0.00091207
Iteration 8/25 | Loss: 0.00091153
Iteration 9/25 | Loss: 0.00091504
Iteration 10/25 | Loss: 0.00091442
Iteration 11/25 | Loss: 0.00091290
Iteration 12/25 | Loss: 0.00091000
Iteration 13/25 | Loss: 0.00091241
Iteration 14/25 | Loss: 0.00091400
Iteration 15/25 | Loss: 0.00091449
Iteration 16/25 | Loss: 0.00091319
Iteration 17/25 | Loss: 0.00091284
Iteration 18/25 | Loss: 0.00091317
Iteration 19/25 | Loss: 0.00091194
Iteration 20/25 | Loss: 0.00091275
Iteration 21/25 | Loss: 0.00091399
Iteration 22/25 | Loss: 0.00091381
Iteration 23/25 | Loss: 0.00091307
Iteration 24/25 | Loss: 0.00091313
Iteration 25/25 | Loss: 0.00091315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51587319
Iteration 2/25 | Loss: 0.00175140
Iteration 3/25 | Loss: 0.00175139
Iteration 4/25 | Loss: 0.00175139
Iteration 5/25 | Loss: 0.00175139
Iteration 6/25 | Loss: 0.00175139
Iteration 7/25 | Loss: 0.00175139
Iteration 8/25 | Loss: 0.00175139
Iteration 9/25 | Loss: 0.00175139
Iteration 10/25 | Loss: 0.00175139
Iteration 11/25 | Loss: 0.00175139
Iteration 12/25 | Loss: 0.00175139
Iteration 13/25 | Loss: 0.00175139
Iteration 14/25 | Loss: 0.00175139
Iteration 15/25 | Loss: 0.00175139
Iteration 16/25 | Loss: 0.00175139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017513885395601392, 0.0017513885395601392, 0.0017513885395601392, 0.0017513885395601392, 0.0017513885395601392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017513885395601392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175139
Iteration 2/1000 | Loss: 0.00018730
Iteration 3/1000 | Loss: 0.00010891
Iteration 4/1000 | Loss: 0.00010636
Iteration 5/1000 | Loss: 0.00018533
Iteration 6/1000 | Loss: 0.00016517
Iteration 7/1000 | Loss: 0.00029184
Iteration 8/1000 | Loss: 0.00013156
Iteration 9/1000 | Loss: 0.00017599
Iteration 10/1000 | Loss: 0.00007796
Iteration 11/1000 | Loss: 0.00029143
Iteration 12/1000 | Loss: 0.00007042
Iteration 13/1000 | Loss: 0.00006767
Iteration 14/1000 | Loss: 0.00006524
Iteration 15/1000 | Loss: 0.00006342
Iteration 16/1000 | Loss: 0.00006223
Iteration 17/1000 | Loss: 0.00006149
Iteration 18/1000 | Loss: 0.00006068
Iteration 19/1000 | Loss: 0.00006020
Iteration 20/1000 | Loss: 0.00005962
Iteration 21/1000 | Loss: 0.00005924
Iteration 22/1000 | Loss: 0.00005889
Iteration 23/1000 | Loss: 0.00005856
Iteration 24/1000 | Loss: 0.00005825
Iteration 25/1000 | Loss: 0.00005804
Iteration 26/1000 | Loss: 0.00005801
Iteration 27/1000 | Loss: 0.00005797
Iteration 28/1000 | Loss: 0.00005796
Iteration 29/1000 | Loss: 0.00005790
Iteration 30/1000 | Loss: 0.00005790
Iteration 31/1000 | Loss: 0.00005775
Iteration 32/1000 | Loss: 0.00005766
Iteration 33/1000 | Loss: 0.00005765
Iteration 34/1000 | Loss: 0.00005764
Iteration 35/1000 | Loss: 0.00005764
Iteration 36/1000 | Loss: 0.00005763
Iteration 37/1000 | Loss: 0.00005763
Iteration 38/1000 | Loss: 0.00005762
Iteration 39/1000 | Loss: 0.00005762
Iteration 40/1000 | Loss: 0.00005762
Iteration 41/1000 | Loss: 0.00005761
Iteration 42/1000 | Loss: 0.00005761
Iteration 43/1000 | Loss: 0.00005760
Iteration 44/1000 | Loss: 0.00005760
Iteration 45/1000 | Loss: 0.00005759
Iteration 46/1000 | Loss: 0.00005755
Iteration 47/1000 | Loss: 0.00005749
Iteration 48/1000 | Loss: 0.00005747
Iteration 49/1000 | Loss: 0.00005746
Iteration 50/1000 | Loss: 0.00005745
Iteration 51/1000 | Loss: 0.00005745
Iteration 52/1000 | Loss: 0.00005744
Iteration 53/1000 | Loss: 0.00005744
Iteration 54/1000 | Loss: 0.00005743
Iteration 55/1000 | Loss: 0.00005743
Iteration 56/1000 | Loss: 0.00005743
Iteration 57/1000 | Loss: 0.00005742
Iteration 58/1000 | Loss: 0.00005742
Iteration 59/1000 | Loss: 0.00005741
Iteration 60/1000 | Loss: 0.00005740
Iteration 61/1000 | Loss: 0.00005739
Iteration 62/1000 | Loss: 0.00005738
Iteration 63/1000 | Loss: 0.00005738
Iteration 64/1000 | Loss: 0.00005737
Iteration 65/1000 | Loss: 0.00005737
Iteration 66/1000 | Loss: 0.00005736
Iteration 67/1000 | Loss: 0.00005735
Iteration 68/1000 | Loss: 0.00005733
Iteration 69/1000 | Loss: 0.00005732
Iteration 70/1000 | Loss: 0.00005732
Iteration 71/1000 | Loss: 0.00005731
Iteration 72/1000 | Loss: 0.00005731
Iteration 73/1000 | Loss: 0.00005730
Iteration 74/1000 | Loss: 0.00005727
Iteration 75/1000 | Loss: 0.00005723
Iteration 76/1000 | Loss: 0.00005723
Iteration 77/1000 | Loss: 0.00005723
Iteration 78/1000 | Loss: 0.00005722
Iteration 79/1000 | Loss: 0.00005719
Iteration 80/1000 | Loss: 0.00005714
Iteration 81/1000 | Loss: 0.00005714
Iteration 82/1000 | Loss: 0.00005713
Iteration 83/1000 | Loss: 0.00005713
Iteration 84/1000 | Loss: 0.00005712
Iteration 85/1000 | Loss: 0.00005711
Iteration 86/1000 | Loss: 0.00005710
Iteration 87/1000 | Loss: 0.00005710
Iteration 88/1000 | Loss: 0.00005709
Iteration 89/1000 | Loss: 0.00005708
Iteration 90/1000 | Loss: 0.00005708
Iteration 91/1000 | Loss: 0.00005706
Iteration 92/1000 | Loss: 0.00005706
Iteration 93/1000 | Loss: 0.00005703
Iteration 94/1000 | Loss: 0.00005700
Iteration 95/1000 | Loss: 0.00005700
Iteration 96/1000 | Loss: 0.00005700
Iteration 97/1000 | Loss: 0.00005697
Iteration 98/1000 | Loss: 0.00005697
Iteration 99/1000 | Loss: 0.00005697
Iteration 100/1000 | Loss: 0.00005697
Iteration 101/1000 | Loss: 0.00005697
Iteration 102/1000 | Loss: 0.00005696
Iteration 103/1000 | Loss: 0.00005696
Iteration 104/1000 | Loss: 0.00005696
Iteration 105/1000 | Loss: 0.00005696
Iteration 106/1000 | Loss: 0.00005696
Iteration 107/1000 | Loss: 0.00005695
Iteration 108/1000 | Loss: 0.00005694
Iteration 109/1000 | Loss: 0.00005694
Iteration 110/1000 | Loss: 0.00005694
Iteration 111/1000 | Loss: 0.00005693
Iteration 112/1000 | Loss: 0.00005690
Iteration 113/1000 | Loss: 0.00005690
Iteration 114/1000 | Loss: 0.00005690
Iteration 115/1000 | Loss: 0.00005689
Iteration 116/1000 | Loss: 0.00005689
Iteration 117/1000 | Loss: 0.00005688
Iteration 118/1000 | Loss: 0.00005687
Iteration 119/1000 | Loss: 0.00005687
Iteration 120/1000 | Loss: 0.00005686
Iteration 121/1000 | Loss: 0.00005686
Iteration 122/1000 | Loss: 0.00005686
Iteration 123/1000 | Loss: 0.00005686
Iteration 124/1000 | Loss: 0.00005685
Iteration 125/1000 | Loss: 0.00005685
Iteration 126/1000 | Loss: 0.00005685
Iteration 127/1000 | Loss: 0.00005684
Iteration 128/1000 | Loss: 0.00005684
Iteration 129/1000 | Loss: 0.00005684
Iteration 130/1000 | Loss: 0.00005682
Iteration 131/1000 | Loss: 0.00005682
Iteration 132/1000 | Loss: 0.00005681
Iteration 133/1000 | Loss: 0.00005681
Iteration 134/1000 | Loss: 0.00005680
Iteration 135/1000 | Loss: 0.00005680
Iteration 136/1000 | Loss: 0.00005679
Iteration 137/1000 | Loss: 0.00005679
Iteration 138/1000 | Loss: 0.00005678
Iteration 139/1000 | Loss: 0.00005678
Iteration 140/1000 | Loss: 0.00005678
Iteration 141/1000 | Loss: 0.00005677
Iteration 142/1000 | Loss: 0.00005677
Iteration 143/1000 | Loss: 0.00005676
Iteration 144/1000 | Loss: 0.00005676
Iteration 145/1000 | Loss: 0.00005676
Iteration 146/1000 | Loss: 0.00005675
Iteration 147/1000 | Loss: 0.00005675
Iteration 148/1000 | Loss: 0.00005675
Iteration 149/1000 | Loss: 0.00005675
Iteration 150/1000 | Loss: 0.00005674
Iteration 151/1000 | Loss: 0.00005674
Iteration 152/1000 | Loss: 0.00005673
Iteration 153/1000 | Loss: 0.00005673
Iteration 154/1000 | Loss: 0.00005673
Iteration 155/1000 | Loss: 0.00005672
Iteration 156/1000 | Loss: 0.00005672
Iteration 157/1000 | Loss: 0.00005672
Iteration 158/1000 | Loss: 0.00005671
Iteration 159/1000 | Loss: 0.00005671
Iteration 160/1000 | Loss: 0.00005671
Iteration 161/1000 | Loss: 0.00005670
Iteration 162/1000 | Loss: 0.00005670
Iteration 163/1000 | Loss: 0.00005670
Iteration 164/1000 | Loss: 0.00005670
Iteration 165/1000 | Loss: 0.00005670
Iteration 166/1000 | Loss: 0.00005669
Iteration 167/1000 | Loss: 0.00065599
Iteration 168/1000 | Loss: 0.00030488
Iteration 169/1000 | Loss: 0.00005988
Iteration 170/1000 | Loss: 0.00005754
Iteration 171/1000 | Loss: 0.00005732
Iteration 172/1000 | Loss: 0.00005731
Iteration 173/1000 | Loss: 0.00005723
Iteration 174/1000 | Loss: 0.00005723
Iteration 175/1000 | Loss: 0.00005723
Iteration 176/1000 | Loss: 0.00005723
Iteration 177/1000 | Loss: 0.00005723
Iteration 178/1000 | Loss: 0.00005723
Iteration 179/1000 | Loss: 0.00005723
Iteration 180/1000 | Loss: 0.00005722
Iteration 181/1000 | Loss: 0.00005722
Iteration 182/1000 | Loss: 0.00005722
Iteration 183/1000 | Loss: 0.00005721
Iteration 184/1000 | Loss: 0.00005720
Iteration 185/1000 | Loss: 0.00005720
Iteration 186/1000 | Loss: 0.00005719
Iteration 187/1000 | Loss: 0.00005719
Iteration 188/1000 | Loss: 0.00005719
Iteration 189/1000 | Loss: 0.00005719
Iteration 190/1000 | Loss: 0.00005719
Iteration 191/1000 | Loss: 0.00005718
Iteration 192/1000 | Loss: 0.00005718
Iteration 193/1000 | Loss: 0.00005718
Iteration 194/1000 | Loss: 0.00005718
Iteration 195/1000 | Loss: 0.00005717
Iteration 196/1000 | Loss: 0.00005717
Iteration 197/1000 | Loss: 0.00005717
Iteration 198/1000 | Loss: 0.00005717
Iteration 199/1000 | Loss: 0.00005716
Iteration 200/1000 | Loss: 0.00005716
Iteration 201/1000 | Loss: 0.00005716
Iteration 202/1000 | Loss: 0.00005716
Iteration 203/1000 | Loss: 0.00005716
Iteration 204/1000 | Loss: 0.00005716
Iteration 205/1000 | Loss: 0.00005715
Iteration 206/1000 | Loss: 0.00005715
Iteration 207/1000 | Loss: 0.00005715
Iteration 208/1000 | Loss: 0.00005715
Iteration 209/1000 | Loss: 0.00005715
Iteration 210/1000 | Loss: 0.00005715
Iteration 211/1000 | Loss: 0.00005715
Iteration 212/1000 | Loss: 0.00005715
Iteration 213/1000 | Loss: 0.00005715
Iteration 214/1000 | Loss: 0.00005714
Iteration 215/1000 | Loss: 0.00005714
Iteration 216/1000 | Loss: 0.00005714
Iteration 217/1000 | Loss: 0.00005713
Iteration 218/1000 | Loss: 0.00005713
Iteration 219/1000 | Loss: 0.00005713
Iteration 220/1000 | Loss: 0.00005713
Iteration 221/1000 | Loss: 0.00005713
Iteration 222/1000 | Loss: 0.00005713
Iteration 223/1000 | Loss: 0.00005712
Iteration 224/1000 | Loss: 0.00005712
Iteration 225/1000 | Loss: 0.00005712
Iteration 226/1000 | Loss: 0.00005711
Iteration 227/1000 | Loss: 0.00005711
Iteration 228/1000 | Loss: 0.00005709
Iteration 229/1000 | Loss: 0.00005708
Iteration 230/1000 | Loss: 0.00005708
Iteration 231/1000 | Loss: 0.00005704
Iteration 232/1000 | Loss: 0.00005703
Iteration 233/1000 | Loss: 0.00005703
Iteration 234/1000 | Loss: 0.00005700
Iteration 235/1000 | Loss: 0.00005700
Iteration 236/1000 | Loss: 0.00005700
Iteration 237/1000 | Loss: 0.00005700
Iteration 238/1000 | Loss: 0.00005700
Iteration 239/1000 | Loss: 0.00005700
Iteration 240/1000 | Loss: 0.00005700
Iteration 241/1000 | Loss: 0.00005700
Iteration 242/1000 | Loss: 0.00005700
Iteration 243/1000 | Loss: 0.00005699
Iteration 244/1000 | Loss: 0.00005699
Iteration 245/1000 | Loss: 0.00005699
Iteration 246/1000 | Loss: 0.00005698
Iteration 247/1000 | Loss: 0.00005698
Iteration 248/1000 | Loss: 0.00005698
Iteration 249/1000 | Loss: 0.00005698
Iteration 250/1000 | Loss: 0.00005697
Iteration 251/1000 | Loss: 0.00005697
Iteration 252/1000 | Loss: 0.00005697
Iteration 253/1000 | Loss: 0.00005696
Iteration 254/1000 | Loss: 0.00005696
Iteration 255/1000 | Loss: 0.00005695
Iteration 256/1000 | Loss: 0.00005695
Iteration 257/1000 | Loss: 0.00005695
Iteration 258/1000 | Loss: 0.00005695
Iteration 259/1000 | Loss: 0.00005695
Iteration 260/1000 | Loss: 0.00005695
Iteration 261/1000 | Loss: 0.00005695
Iteration 262/1000 | Loss: 0.00005693
Iteration 263/1000 | Loss: 0.00005693
Iteration 264/1000 | Loss: 0.00005690
Iteration 265/1000 | Loss: 0.00005685
Iteration 266/1000 | Loss: 0.00005684
Iteration 267/1000 | Loss: 0.00005683
Iteration 268/1000 | Loss: 0.00005682
Iteration 269/1000 | Loss: 0.00005681
Iteration 270/1000 | Loss: 0.00005681
Iteration 271/1000 | Loss: 0.00005681
Iteration 272/1000 | Loss: 0.00005680
Iteration 273/1000 | Loss: 0.00005680
Iteration 274/1000 | Loss: 0.00005680
Iteration 275/1000 | Loss: 0.00005680
Iteration 276/1000 | Loss: 0.00005679
Iteration 277/1000 | Loss: 0.00005679
Iteration 278/1000 | Loss: 0.00005679
Iteration 279/1000 | Loss: 0.00005678
Iteration 280/1000 | Loss: 0.00005677
Iteration 281/1000 | Loss: 0.00005677
Iteration 282/1000 | Loss: 0.00005677
Iteration 283/1000 | Loss: 0.00005676
Iteration 284/1000 | Loss: 0.00005676
Iteration 285/1000 | Loss: 0.00005676
Iteration 286/1000 | Loss: 0.00005675
Iteration 287/1000 | Loss: 0.00005675
Iteration 288/1000 | Loss: 0.00005675
Iteration 289/1000 | Loss: 0.00005674
Iteration 290/1000 | Loss: 0.00005674
Iteration 291/1000 | Loss: 0.00005674
Iteration 292/1000 | Loss: 0.00005674
Iteration 293/1000 | Loss: 0.00005674
Iteration 294/1000 | Loss: 0.00005674
Iteration 295/1000 | Loss: 0.00005673
Iteration 296/1000 | Loss: 0.00005673
Iteration 297/1000 | Loss: 0.00005673
Iteration 298/1000 | Loss: 0.00005673
Iteration 299/1000 | Loss: 0.00005672
Iteration 300/1000 | Loss: 0.00005672
Iteration 301/1000 | Loss: 0.00005672
Iteration 302/1000 | Loss: 0.00005672
Iteration 303/1000 | Loss: 0.00005671
Iteration 304/1000 | Loss: 0.00005671
Iteration 305/1000 | Loss: 0.00005670
Iteration 306/1000 | Loss: 0.00005670
Iteration 307/1000 | Loss: 0.00005670
Iteration 308/1000 | Loss: 0.00005670
Iteration 309/1000 | Loss: 0.00005670
Iteration 310/1000 | Loss: 0.00005669
Iteration 311/1000 | Loss: 0.00005669
Iteration 312/1000 | Loss: 0.00005669
Iteration 313/1000 | Loss: 0.00005669
Iteration 314/1000 | Loss: 0.00005669
Iteration 315/1000 | Loss: 0.00005669
Iteration 316/1000 | Loss: 0.00005668
Iteration 317/1000 | Loss: 0.00005668
Iteration 318/1000 | Loss: 0.00005668
Iteration 319/1000 | Loss: 0.00005668
Iteration 320/1000 | Loss: 0.00005667
Iteration 321/1000 | Loss: 0.00005667
Iteration 322/1000 | Loss: 0.00062388
Iteration 323/1000 | Loss: 0.00006375
Iteration 324/1000 | Loss: 0.00005693
Iteration 325/1000 | Loss: 0.00005600
Iteration 326/1000 | Loss: 0.00005544
Iteration 327/1000 | Loss: 0.00005513
Iteration 328/1000 | Loss: 0.00005507
Iteration 329/1000 | Loss: 0.00005490
Iteration 330/1000 | Loss: 0.00005489
Iteration 331/1000 | Loss: 0.00005488
Iteration 332/1000 | Loss: 0.00005485
Iteration 333/1000 | Loss: 0.00005485
Iteration 334/1000 | Loss: 0.00005484
Iteration 335/1000 | Loss: 0.00005484
Iteration 336/1000 | Loss: 0.00005483
Iteration 337/1000 | Loss: 0.00005482
Iteration 338/1000 | Loss: 0.00005482
Iteration 339/1000 | Loss: 0.00005481
Iteration 340/1000 | Loss: 0.00005480
Iteration 341/1000 | Loss: 0.00005474
Iteration 342/1000 | Loss: 0.00005471
Iteration 343/1000 | Loss: 0.00005471
Iteration 344/1000 | Loss: 0.00005471
Iteration 345/1000 | Loss: 0.00005470
Iteration 346/1000 | Loss: 0.00005469
Iteration 347/1000 | Loss: 0.00005469
Iteration 348/1000 | Loss: 0.00005468
Iteration 349/1000 | Loss: 0.00005468
Iteration 350/1000 | Loss: 0.00005468
Iteration 351/1000 | Loss: 0.00005468
Iteration 352/1000 | Loss: 0.00005468
Iteration 353/1000 | Loss: 0.00005468
Iteration 354/1000 | Loss: 0.00005468
Iteration 355/1000 | Loss: 0.00005467
Iteration 356/1000 | Loss: 0.00005467
Iteration 357/1000 | Loss: 0.00005467
Iteration 358/1000 | Loss: 0.00005467
Iteration 359/1000 | Loss: 0.00005466
Iteration 360/1000 | Loss: 0.00005466
Iteration 361/1000 | Loss: 0.00005465
Iteration 362/1000 | Loss: 0.00005465
Iteration 363/1000 | Loss: 0.00005465
Iteration 364/1000 | Loss: 0.00005465
Iteration 365/1000 | Loss: 0.00005465
Iteration 366/1000 | Loss: 0.00005465
Iteration 367/1000 | Loss: 0.00005465
Iteration 368/1000 | Loss: 0.00005465
Iteration 369/1000 | Loss: 0.00005465
Iteration 370/1000 | Loss: 0.00005465
Iteration 371/1000 | Loss: 0.00005465
Iteration 372/1000 | Loss: 0.00005465
Iteration 373/1000 | Loss: 0.00005465
Iteration 374/1000 | Loss: 0.00005464
Iteration 375/1000 | Loss: 0.00005464
Iteration 376/1000 | Loss: 0.00005464
Iteration 377/1000 | Loss: 0.00005464
Iteration 378/1000 | Loss: 0.00005464
Iteration 379/1000 | Loss: 0.00005464
Iteration 380/1000 | Loss: 0.00005464
Iteration 381/1000 | Loss: 0.00005464
Iteration 382/1000 | Loss: 0.00005464
Iteration 383/1000 | Loss: 0.00005464
Iteration 384/1000 | Loss: 0.00005464
Iteration 385/1000 | Loss: 0.00005464
Iteration 386/1000 | Loss: 0.00005464
Iteration 387/1000 | Loss: 0.00005463
Iteration 388/1000 | Loss: 0.00005463
Iteration 389/1000 | Loss: 0.00005463
Iteration 390/1000 | Loss: 0.00005463
Iteration 391/1000 | Loss: 0.00005463
Iteration 392/1000 | Loss: 0.00005463
Iteration 393/1000 | Loss: 0.00005463
Iteration 394/1000 | Loss: 0.00005463
Iteration 395/1000 | Loss: 0.00005463
Iteration 396/1000 | Loss: 0.00005463
Iteration 397/1000 | Loss: 0.00005463
Iteration 398/1000 | Loss: 0.00005463
Iteration 399/1000 | Loss: 0.00005463
Iteration 400/1000 | Loss: 0.00005463
Iteration 401/1000 | Loss: 0.00005463
Iteration 402/1000 | Loss: 0.00005463
Iteration 403/1000 | Loss: 0.00005462
Iteration 404/1000 | Loss: 0.00005462
Iteration 405/1000 | Loss: 0.00005462
Iteration 406/1000 | Loss: 0.00005462
Iteration 407/1000 | Loss: 0.00005462
Iteration 408/1000 | Loss: 0.00005462
Iteration 409/1000 | Loss: 0.00005462
Iteration 410/1000 | Loss: 0.00005462
Iteration 411/1000 | Loss: 0.00005462
Iteration 412/1000 | Loss: 0.00005462
Iteration 413/1000 | Loss: 0.00005462
Iteration 414/1000 | Loss: 0.00005462
Iteration 415/1000 | Loss: 0.00005462
Iteration 416/1000 | Loss: 0.00005462
Iteration 417/1000 | Loss: 0.00005462
Iteration 418/1000 | Loss: 0.00005462
Iteration 419/1000 | Loss: 0.00005462
Iteration 420/1000 | Loss: 0.00005462
Iteration 421/1000 | Loss: 0.00005462
Iteration 422/1000 | Loss: 0.00005462
Iteration 423/1000 | Loss: 0.00005462
Iteration 424/1000 | Loss: 0.00005462
Iteration 425/1000 | Loss: 0.00005462
Iteration 426/1000 | Loss: 0.00005462
Iteration 427/1000 | Loss: 0.00005462
Iteration 428/1000 | Loss: 0.00005462
Iteration 429/1000 | Loss: 0.00005462
Iteration 430/1000 | Loss: 0.00005462
Iteration 431/1000 | Loss: 0.00005462
Iteration 432/1000 | Loss: 0.00005462
Iteration 433/1000 | Loss: 0.00005462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 433. Stopping optimization.
Last 5 losses: [5.462087210617028e-05, 5.462087210617028e-05, 5.462087210617028e-05, 5.462087210617028e-05, 5.462087210617028e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.462087210617028e-05

Optimization complete. Final v2v error: 3.5604307651519775 mm

Highest mean error: 11.166939735412598 mm for frame 104

Lowest mean error: 2.501080274581909 mm for frame 60

Saving results

Total time: 146.61283946037292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821954
Iteration 2/25 | Loss: 0.00132061
Iteration 3/25 | Loss: 0.00096495
Iteration 4/25 | Loss: 0.00088933
Iteration 5/25 | Loss: 0.00087495
Iteration 6/25 | Loss: 0.00086778
Iteration 7/25 | Loss: 0.00086732
Iteration 8/25 | Loss: 0.00086599
Iteration 9/25 | Loss: 0.00086716
Iteration 10/25 | Loss: 0.00086582
Iteration 11/25 | Loss: 0.00086580
Iteration 12/25 | Loss: 0.00086580
Iteration 13/25 | Loss: 0.00086580
Iteration 14/25 | Loss: 0.00086580
Iteration 15/25 | Loss: 0.00086579
Iteration 16/25 | Loss: 0.00086579
Iteration 17/25 | Loss: 0.00086579
Iteration 18/25 | Loss: 0.00086579
Iteration 19/25 | Loss: 0.00086579
Iteration 20/25 | Loss: 0.00086579
Iteration 21/25 | Loss: 0.00086579
Iteration 22/25 | Loss: 0.00086579
Iteration 23/25 | Loss: 0.00086579
Iteration 24/25 | Loss: 0.00086579
Iteration 25/25 | Loss: 0.00086579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.29213142
Iteration 2/25 | Loss: 0.00057282
Iteration 3/25 | Loss: 0.00057279
Iteration 4/25 | Loss: 0.00057279
Iteration 5/25 | Loss: 0.00057279
Iteration 6/25 | Loss: 0.00057278
Iteration 7/25 | Loss: 0.00057278
Iteration 8/25 | Loss: 0.00057278
Iteration 9/25 | Loss: 0.00057278
Iteration 10/25 | Loss: 0.00057278
Iteration 11/25 | Loss: 0.00057278
Iteration 12/25 | Loss: 0.00057278
Iteration 13/25 | Loss: 0.00057278
Iteration 14/25 | Loss: 0.00057278
Iteration 15/25 | Loss: 0.00057278
Iteration 16/25 | Loss: 0.00057278
Iteration 17/25 | Loss: 0.00057278
Iteration 18/25 | Loss: 0.00057278
Iteration 19/25 | Loss: 0.00057278
Iteration 20/25 | Loss: 0.00057278
Iteration 21/25 | Loss: 0.00057278
Iteration 22/25 | Loss: 0.00057278
Iteration 23/25 | Loss: 0.00057278
Iteration 24/25 | Loss: 0.00057278
Iteration 25/25 | Loss: 0.00057278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057278
Iteration 2/1000 | Loss: 0.00002847
Iteration 3/1000 | Loss: 0.00002149
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001811
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001733
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001696
Iteration 15/1000 | Loss: 0.00001695
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001688
Iteration 21/1000 | Loss: 0.00001687
Iteration 22/1000 | Loss: 0.00001687
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001685
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001684
Iteration 28/1000 | Loss: 0.00001682
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001681
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001679
Iteration 34/1000 | Loss: 0.00001679
Iteration 35/1000 | Loss: 0.00001678
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001678
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001677
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001676
Iteration 42/1000 | Loss: 0.00001676
Iteration 43/1000 | Loss: 0.00001675
Iteration 44/1000 | Loss: 0.00001675
Iteration 45/1000 | Loss: 0.00001674
Iteration 46/1000 | Loss: 0.00001674
Iteration 47/1000 | Loss: 0.00001673
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00001673
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001670
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001669
Iteration 59/1000 | Loss: 0.00001669
Iteration 60/1000 | Loss: 0.00001669
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001668
Iteration 63/1000 | Loss: 0.00001667
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001666
Iteration 70/1000 | Loss: 0.00001666
Iteration 71/1000 | Loss: 0.00001666
Iteration 72/1000 | Loss: 0.00001666
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001666
Iteration 77/1000 | Loss: 0.00001666
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001664
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001664
Iteration 100/1000 | Loss: 0.00001664
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001663
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001663
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.6634288840577938e-05, 1.6634288840577938e-05, 1.6634288840577938e-05, 1.6634288840577938e-05, 1.6634288840577938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6634288840577938e-05

Optimization complete. Final v2v error: 3.4358973503112793 mm

Highest mean error: 3.8597934246063232 mm for frame 212

Lowest mean error: 3.1052122116088867 mm for frame 194

Saving results

Total time: 48.93535280227661
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705583
Iteration 2/25 | Loss: 0.00144477
Iteration 3/25 | Loss: 0.00101634
Iteration 4/25 | Loss: 0.00094952
Iteration 5/25 | Loss: 0.00093972
Iteration 6/25 | Loss: 0.00093799
Iteration 7/25 | Loss: 0.00093799
Iteration 8/25 | Loss: 0.00093799
Iteration 9/25 | Loss: 0.00093799
Iteration 10/25 | Loss: 0.00093799
Iteration 11/25 | Loss: 0.00093799
Iteration 12/25 | Loss: 0.00093799
Iteration 13/25 | Loss: 0.00093799
Iteration 14/25 | Loss: 0.00093799
Iteration 15/25 | Loss: 0.00093799
Iteration 16/25 | Loss: 0.00093799
Iteration 17/25 | Loss: 0.00093799
Iteration 18/25 | Loss: 0.00093799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009379893308505416, 0.0009379893308505416, 0.0009379893308505416, 0.0009379893308505416, 0.0009379893308505416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009379893308505416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03391933
Iteration 2/25 | Loss: 0.00069777
Iteration 3/25 | Loss: 0.00069774
Iteration 4/25 | Loss: 0.00069774
Iteration 5/25 | Loss: 0.00069774
Iteration 6/25 | Loss: 0.00069774
Iteration 7/25 | Loss: 0.00069774
Iteration 8/25 | Loss: 0.00069774
Iteration 9/25 | Loss: 0.00069774
Iteration 10/25 | Loss: 0.00069774
Iteration 11/25 | Loss: 0.00069774
Iteration 12/25 | Loss: 0.00069774
Iteration 13/25 | Loss: 0.00069774
Iteration 14/25 | Loss: 0.00069774
Iteration 15/25 | Loss: 0.00069774
Iteration 16/25 | Loss: 0.00069774
Iteration 17/25 | Loss: 0.00069774
Iteration 18/25 | Loss: 0.00069774
Iteration 19/25 | Loss: 0.00069774
Iteration 20/25 | Loss: 0.00069774
Iteration 21/25 | Loss: 0.00069774
Iteration 22/25 | Loss: 0.00069774
Iteration 23/25 | Loss: 0.00069774
Iteration 24/25 | Loss: 0.00069774
Iteration 25/25 | Loss: 0.00069774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069774
Iteration 2/1000 | Loss: 0.00003785
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002198
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00001973
Iteration 7/1000 | Loss: 0.00001919
Iteration 8/1000 | Loss: 0.00001897
Iteration 9/1000 | Loss: 0.00001873
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001829
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001802
Iteration 29/1000 | Loss: 0.00001802
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001801
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001798
Iteration 34/1000 | Loss: 0.00001797
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001792
Iteration 38/1000 | Loss: 0.00001792
Iteration 39/1000 | Loss: 0.00001791
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001791
Iteration 42/1000 | Loss: 0.00001790
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001789
Iteration 45/1000 | Loss: 0.00001789
Iteration 46/1000 | Loss: 0.00001789
Iteration 47/1000 | Loss: 0.00001789
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001788
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001788
Iteration 52/1000 | Loss: 0.00001788
Iteration 53/1000 | Loss: 0.00001788
Iteration 54/1000 | Loss: 0.00001788
Iteration 55/1000 | Loss: 0.00001788
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001787
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001787
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001787
Iteration 72/1000 | Loss: 0.00001787
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001787
Iteration 76/1000 | Loss: 0.00001787
Iteration 77/1000 | Loss: 0.00001787
Iteration 78/1000 | Loss: 0.00001787
Iteration 79/1000 | Loss: 0.00001787
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.7872938769869506e-05, 1.7872938769869506e-05, 1.7872938769869506e-05, 1.7872938769869506e-05, 1.7872938769869506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7872938769869506e-05

Optimization complete. Final v2v error: 3.602553129196167 mm

Highest mean error: 3.888110399246216 mm for frame 217

Lowest mean error: 3.319028615951538 mm for frame 94

Saving results

Total time: 35.97223711013794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401684
Iteration 2/25 | Loss: 0.00094530
Iteration 3/25 | Loss: 0.00083510
Iteration 4/25 | Loss: 0.00081579
Iteration 5/25 | Loss: 0.00080902
Iteration 6/25 | Loss: 0.00080758
Iteration 7/25 | Loss: 0.00080730
Iteration 8/25 | Loss: 0.00080730
Iteration 9/25 | Loss: 0.00080730
Iteration 10/25 | Loss: 0.00080730
Iteration 11/25 | Loss: 0.00080730
Iteration 12/25 | Loss: 0.00080730
Iteration 13/25 | Loss: 0.00080730
Iteration 14/25 | Loss: 0.00080730
Iteration 15/25 | Loss: 0.00080730
Iteration 16/25 | Loss: 0.00080730
Iteration 17/25 | Loss: 0.00080730
Iteration 18/25 | Loss: 0.00080730
Iteration 19/25 | Loss: 0.00080730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008073031785897911, 0.0008073031785897911, 0.0008073031785897911, 0.0008073031785897911, 0.0008073031785897911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008073031785897911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90430045
Iteration 2/25 | Loss: 0.00054596
Iteration 3/25 | Loss: 0.00054596
Iteration 4/25 | Loss: 0.00054596
Iteration 5/25 | Loss: 0.00054596
Iteration 6/25 | Loss: 0.00054596
Iteration 7/25 | Loss: 0.00054596
Iteration 8/25 | Loss: 0.00054596
Iteration 9/25 | Loss: 0.00054596
Iteration 10/25 | Loss: 0.00054596
Iteration 11/25 | Loss: 0.00054596
Iteration 12/25 | Loss: 0.00054596
Iteration 13/25 | Loss: 0.00054596
Iteration 14/25 | Loss: 0.00054596
Iteration 15/25 | Loss: 0.00054596
Iteration 16/25 | Loss: 0.00054596
Iteration 17/25 | Loss: 0.00054596
Iteration 18/25 | Loss: 0.00054596
Iteration 19/25 | Loss: 0.00054596
Iteration 20/25 | Loss: 0.00054596
Iteration 21/25 | Loss: 0.00054596
Iteration 22/25 | Loss: 0.00054596
Iteration 23/25 | Loss: 0.00054596
Iteration 24/25 | Loss: 0.00054596
Iteration 25/25 | Loss: 0.00054596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054596
Iteration 2/1000 | Loss: 0.00002361
Iteration 3/1000 | Loss: 0.00001970
Iteration 4/1000 | Loss: 0.00001860
Iteration 5/1000 | Loss: 0.00001744
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001608
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001587
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001583
Iteration 23/1000 | Loss: 0.00001582
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001580
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001579
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001574
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001558
Iteration 68/1000 | Loss: 0.00001558
Iteration 69/1000 | Loss: 0.00001558
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001555
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001555
Iteration 85/1000 | Loss: 0.00001555
Iteration 86/1000 | Loss: 0.00001555
Iteration 87/1000 | Loss: 0.00001555
Iteration 88/1000 | Loss: 0.00001555
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.5547881048405543e-05, 1.5547881048405543e-05, 1.5547881048405543e-05, 1.5547881048405543e-05, 1.5547881048405543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5547881048405543e-05

Optimization complete. Final v2v error: 3.366680383682251 mm

Highest mean error: 3.5632426738739014 mm for frame 147

Lowest mean error: 3.1633567810058594 mm for frame 115

Saving results

Total time: 34.0546236038208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856110
Iteration 2/25 | Loss: 0.00122123
Iteration 3/25 | Loss: 0.00086685
Iteration 4/25 | Loss: 0.00082983
Iteration 5/25 | Loss: 0.00081993
Iteration 6/25 | Loss: 0.00081727
Iteration 7/25 | Loss: 0.00081630
Iteration 8/25 | Loss: 0.00081630
Iteration 9/25 | Loss: 0.00081630
Iteration 10/25 | Loss: 0.00081630
Iteration 11/25 | Loss: 0.00081630
Iteration 12/25 | Loss: 0.00081630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008162989397533238, 0.0008162989397533238, 0.0008162989397533238, 0.0008162989397533238, 0.0008162989397533238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008162989397533238

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.98772073
Iteration 2/25 | Loss: 0.00061073
Iteration 3/25 | Loss: 0.00061073
Iteration 4/25 | Loss: 0.00061073
Iteration 5/25 | Loss: 0.00061073
Iteration 6/25 | Loss: 0.00061073
Iteration 7/25 | Loss: 0.00061073
Iteration 8/25 | Loss: 0.00061073
Iteration 9/25 | Loss: 0.00061073
Iteration 10/25 | Loss: 0.00061073
Iteration 11/25 | Loss: 0.00061073
Iteration 12/25 | Loss: 0.00061073
Iteration 13/25 | Loss: 0.00061073
Iteration 14/25 | Loss: 0.00061073
Iteration 15/25 | Loss: 0.00061073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006107254885137081, 0.0006107254885137081, 0.0006107254885137081, 0.0006107254885137081, 0.0006107254885137081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006107254885137081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061073
Iteration 2/1000 | Loss: 0.00002665
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001456
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001373
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001343
Iteration 12/1000 | Loss: 0.00001327
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001320
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001310
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001309
Iteration 24/1000 | Loss: 0.00001308
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001306
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001296
Iteration 35/1000 | Loss: 0.00001296
Iteration 36/1000 | Loss: 0.00001296
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001295
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001294
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001292
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001288
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001283
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001283
Iteration 138/1000 | Loss: 0.00001283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.2826611055061221e-05, 1.2826611055061221e-05, 1.2826611055061221e-05, 1.2826611055061221e-05, 1.2826611055061221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2826611055061221e-05

Optimization complete. Final v2v error: 3.0795767307281494 mm

Highest mean error: 3.4835920333862305 mm for frame 172

Lowest mean error: 2.8182120323181152 mm for frame 2

Saving results

Total time: 40.991318702697754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397118
Iteration 2/25 | Loss: 0.00095916
Iteration 3/25 | Loss: 0.00086623
Iteration 4/25 | Loss: 0.00085464
Iteration 5/25 | Loss: 0.00084861
Iteration 6/25 | Loss: 0.00084707
Iteration 7/25 | Loss: 0.00084707
Iteration 8/25 | Loss: 0.00084707
Iteration 9/25 | Loss: 0.00084707
Iteration 10/25 | Loss: 0.00084707
Iteration 11/25 | Loss: 0.00084707
Iteration 12/25 | Loss: 0.00084707
Iteration 13/25 | Loss: 0.00084707
Iteration 14/25 | Loss: 0.00084707
Iteration 15/25 | Loss: 0.00084707
Iteration 16/25 | Loss: 0.00084707
Iteration 17/25 | Loss: 0.00084707
Iteration 18/25 | Loss: 0.00084707
Iteration 19/25 | Loss: 0.00084707
Iteration 20/25 | Loss: 0.00084707
Iteration 21/25 | Loss: 0.00084707
Iteration 22/25 | Loss: 0.00084707
Iteration 23/25 | Loss: 0.00084707
Iteration 24/25 | Loss: 0.00084707
Iteration 25/25 | Loss: 0.00084707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76676679
Iteration 2/25 | Loss: 0.00068197
Iteration 3/25 | Loss: 0.00068197
Iteration 4/25 | Loss: 0.00068197
Iteration 5/25 | Loss: 0.00068197
Iteration 6/25 | Loss: 0.00068197
Iteration 7/25 | Loss: 0.00068197
Iteration 8/25 | Loss: 0.00068197
Iteration 9/25 | Loss: 0.00068197
Iteration 10/25 | Loss: 0.00068196
Iteration 11/25 | Loss: 0.00068196
Iteration 12/25 | Loss: 0.00068196
Iteration 13/25 | Loss: 0.00068196
Iteration 14/25 | Loss: 0.00068196
Iteration 15/25 | Loss: 0.00068196
Iteration 16/25 | Loss: 0.00068196
Iteration 17/25 | Loss: 0.00068196
Iteration 18/25 | Loss: 0.00068196
Iteration 19/25 | Loss: 0.00068196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000681964447721839, 0.000681964447721839, 0.000681964447721839, 0.000681964447721839, 0.000681964447721839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000681964447721839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068196
Iteration 2/1000 | Loss: 0.00003130
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001586
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001458
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001429
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001388
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001377
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001370
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001366
Iteration 30/1000 | Loss: 0.00001365
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001359
Iteration 62/1000 | Loss: 0.00001359
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001358
Iteration 67/1000 | Loss: 0.00001358
Iteration 68/1000 | Loss: 0.00001358
Iteration 69/1000 | Loss: 0.00001358
Iteration 70/1000 | Loss: 0.00001358
Iteration 71/1000 | Loss: 0.00001358
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001357
Iteration 74/1000 | Loss: 0.00001357
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001356
Iteration 78/1000 | Loss: 0.00001356
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001356
Iteration 83/1000 | Loss: 0.00001356
Iteration 84/1000 | Loss: 0.00001356
Iteration 85/1000 | Loss: 0.00001356
Iteration 86/1000 | Loss: 0.00001355
Iteration 87/1000 | Loss: 0.00001355
Iteration 88/1000 | Loss: 0.00001355
Iteration 89/1000 | Loss: 0.00001355
Iteration 90/1000 | Loss: 0.00001355
Iteration 91/1000 | Loss: 0.00001355
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.3545600268116686e-05, 1.3545600268116686e-05, 1.3545600268116686e-05, 1.3545600268116686e-05, 1.3545600268116686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3545600268116686e-05

Optimization complete. Final v2v error: 3.1342856884002686 mm

Highest mean error: 3.3830339908599854 mm for frame 63

Lowest mean error: 2.9170639514923096 mm for frame 264

Saving results

Total time: 38.39517569541931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052870
Iteration 2/25 | Loss: 0.00124218
Iteration 3/25 | Loss: 0.00088918
Iteration 4/25 | Loss: 0.00084471
Iteration 5/25 | Loss: 0.00083387
Iteration 6/25 | Loss: 0.00083233
Iteration 7/25 | Loss: 0.00083021
Iteration 8/25 | Loss: 0.00083013
Iteration 9/25 | Loss: 0.00083013
Iteration 10/25 | Loss: 0.00083013
Iteration 11/25 | Loss: 0.00083012
Iteration 12/25 | Loss: 0.00083011
Iteration 13/25 | Loss: 0.00083010
Iteration 14/25 | Loss: 0.00083010
Iteration 15/25 | Loss: 0.00083010
Iteration 16/25 | Loss: 0.00083010
Iteration 17/25 | Loss: 0.00083010
Iteration 18/25 | Loss: 0.00083010
Iteration 19/25 | Loss: 0.00083010
Iteration 20/25 | Loss: 0.00083010
Iteration 21/25 | Loss: 0.00083010
Iteration 22/25 | Loss: 0.00083010
Iteration 23/25 | Loss: 0.00083010
Iteration 24/25 | Loss: 0.00083010
Iteration 25/25 | Loss: 0.00083010

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50479221
Iteration 2/25 | Loss: 0.00055050
Iteration 3/25 | Loss: 0.00055050
Iteration 4/25 | Loss: 0.00055049
Iteration 5/25 | Loss: 0.00055049
Iteration 6/25 | Loss: 0.00055049
Iteration 7/25 | Loss: 0.00055049
Iteration 8/25 | Loss: 0.00055049
Iteration 9/25 | Loss: 0.00055049
Iteration 10/25 | Loss: 0.00055049
Iteration 11/25 | Loss: 0.00055049
Iteration 12/25 | Loss: 0.00055049
Iteration 13/25 | Loss: 0.00055049
Iteration 14/25 | Loss: 0.00055049
Iteration 15/25 | Loss: 0.00055049
Iteration 16/25 | Loss: 0.00055049
Iteration 17/25 | Loss: 0.00055049
Iteration 18/25 | Loss: 0.00055049
Iteration 19/25 | Loss: 0.00055049
Iteration 20/25 | Loss: 0.00055049
Iteration 21/25 | Loss: 0.00055049
Iteration 22/25 | Loss: 0.00055049
Iteration 23/25 | Loss: 0.00055049
Iteration 24/25 | Loss: 0.00055049
Iteration 25/25 | Loss: 0.00055049

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055049
Iteration 2/1000 | Loss: 0.00002925
Iteration 3/1000 | Loss: 0.00003132
Iteration 4/1000 | Loss: 0.00001824
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001622
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001582
Iteration 11/1000 | Loss: 0.00002810
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00002243
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001524
Iteration 18/1000 | Loss: 0.00001524
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001521
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001516
Iteration 33/1000 | Loss: 0.00001516
Iteration 34/1000 | Loss: 0.00001516
Iteration 35/1000 | Loss: 0.00001516
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001514
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001975
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001507
Iteration 50/1000 | Loss: 0.00001505
Iteration 51/1000 | Loss: 0.00001505
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001505
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001504
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001503
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001502
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001502
Iteration 69/1000 | Loss: 0.00001502
Iteration 70/1000 | Loss: 0.00001501
Iteration 71/1000 | Loss: 0.00001501
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001500
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00001499
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001497
Iteration 88/1000 | Loss: 0.00001497
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001495
Iteration 106/1000 | Loss: 0.00001495
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001495
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00002017
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001494
Iteration 119/1000 | Loss: 0.00001494
Iteration 120/1000 | Loss: 0.00001494
Iteration 121/1000 | Loss: 0.00001494
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001493
Iteration 144/1000 | Loss: 0.00001493
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001493
Iteration 149/1000 | Loss: 0.00001493
Iteration 150/1000 | Loss: 0.00001493
Iteration 151/1000 | Loss: 0.00001493
Iteration 152/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.4933044440113008e-05, 1.4933044440113008e-05, 1.4933044440113008e-05, 1.4933044440113008e-05, 1.4933044440113008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4933044440113008e-05

Optimization complete. Final v2v error: 3.3244147300720215 mm

Highest mean error: 3.4665377140045166 mm for frame 54

Lowest mean error: 2.9253900051116943 mm for frame 0

Saving results

Total time: 46.268369913101196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118197
Iteration 2/25 | Loss: 0.00182140
Iteration 3/25 | Loss: 0.00122840
Iteration 4/25 | Loss: 0.00114241
Iteration 5/25 | Loss: 0.00110941
Iteration 6/25 | Loss: 0.00128375
Iteration 7/25 | Loss: 0.00118830
Iteration 8/25 | Loss: 0.00119263
Iteration 9/25 | Loss: 0.00106820
Iteration 10/25 | Loss: 0.00104521
Iteration 11/25 | Loss: 0.00103207
Iteration 12/25 | Loss: 0.00102832
Iteration 13/25 | Loss: 0.00106417
Iteration 14/25 | Loss: 0.00103778
Iteration 15/25 | Loss: 0.00098038
Iteration 16/25 | Loss: 0.00096900
Iteration 17/25 | Loss: 0.00100735
Iteration 18/25 | Loss: 0.00097219
Iteration 19/25 | Loss: 0.00100095
Iteration 20/25 | Loss: 0.00100677
Iteration 21/25 | Loss: 0.00098642
Iteration 22/25 | Loss: 0.00097502
Iteration 23/25 | Loss: 0.00100496
Iteration 24/25 | Loss: 0.00097571
Iteration 25/25 | Loss: 0.00096524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15113389
Iteration 2/25 | Loss: 0.00068164
Iteration 3/25 | Loss: 0.00068163
Iteration 4/25 | Loss: 0.00068163
Iteration 5/25 | Loss: 0.00068163
Iteration 6/25 | Loss: 0.00068163
Iteration 7/25 | Loss: 0.00068163
Iteration 8/25 | Loss: 0.00068163
Iteration 9/25 | Loss: 0.00068163
Iteration 10/25 | Loss: 0.00068163
Iteration 11/25 | Loss: 0.00068163
Iteration 12/25 | Loss: 0.00068163
Iteration 13/25 | Loss: 0.00068163
Iteration 14/25 | Loss: 0.00068163
Iteration 15/25 | Loss: 0.00068163
Iteration 16/25 | Loss: 0.00068163
Iteration 17/25 | Loss: 0.00068163
Iteration 18/25 | Loss: 0.00068163
Iteration 19/25 | Loss: 0.00068163
Iteration 20/25 | Loss: 0.00068163
Iteration 21/25 | Loss: 0.00068163
Iteration 22/25 | Loss: 0.00068163
Iteration 23/25 | Loss: 0.00068163
Iteration 24/25 | Loss: 0.00068163
Iteration 25/25 | Loss: 0.00068163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068163
Iteration 2/1000 | Loss: 0.00006129
Iteration 3/1000 | Loss: 0.00004032
Iteration 4/1000 | Loss: 0.00003492
Iteration 5/1000 | Loss: 0.00003291
Iteration 6/1000 | Loss: 0.00003186
Iteration 7/1000 | Loss: 0.00003095
Iteration 8/1000 | Loss: 0.00002991
Iteration 9/1000 | Loss: 0.00008570
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002819
Iteration 12/1000 | Loss: 0.00002787
Iteration 13/1000 | Loss: 0.00002754
Iteration 14/1000 | Loss: 0.00002736
Iteration 15/1000 | Loss: 0.00002715
Iteration 16/1000 | Loss: 0.00002696
Iteration 17/1000 | Loss: 0.00002693
Iteration 18/1000 | Loss: 0.00002678
Iteration 19/1000 | Loss: 0.00002670
Iteration 20/1000 | Loss: 0.00002667
Iteration 21/1000 | Loss: 0.00002667
Iteration 22/1000 | Loss: 0.00002666
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002664
Iteration 25/1000 | Loss: 0.00002664
Iteration 26/1000 | Loss: 0.00002664
Iteration 27/1000 | Loss: 0.00002664
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002664
Iteration 30/1000 | Loss: 0.00002664
Iteration 31/1000 | Loss: 0.00002664
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002664
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002663
Iteration 36/1000 | Loss: 0.00002663
Iteration 37/1000 | Loss: 0.00002663
Iteration 38/1000 | Loss: 0.00002663
Iteration 39/1000 | Loss: 0.00002663
Iteration 40/1000 | Loss: 0.00002663
Iteration 41/1000 | Loss: 0.00002663
Iteration 42/1000 | Loss: 0.00002663
Iteration 43/1000 | Loss: 0.00002660
Iteration 44/1000 | Loss: 0.00002660
Iteration 45/1000 | Loss: 0.00002659
Iteration 46/1000 | Loss: 0.00002659
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002658
Iteration 49/1000 | Loss: 0.00002657
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002657
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002655
Iteration 56/1000 | Loss: 0.00002653
Iteration 57/1000 | Loss: 0.00002653
Iteration 58/1000 | Loss: 0.00002653
Iteration 59/1000 | Loss: 0.00002652
Iteration 60/1000 | Loss: 0.00002652
Iteration 61/1000 | Loss: 0.00002651
Iteration 62/1000 | Loss: 0.00002651
Iteration 63/1000 | Loss: 0.00002650
Iteration 64/1000 | Loss: 0.00002650
Iteration 65/1000 | Loss: 0.00002650
Iteration 66/1000 | Loss: 0.00002649
Iteration 67/1000 | Loss: 0.00002649
Iteration 68/1000 | Loss: 0.00002649
Iteration 69/1000 | Loss: 0.00002648
Iteration 70/1000 | Loss: 0.00002648
Iteration 71/1000 | Loss: 0.00002648
Iteration 72/1000 | Loss: 0.00002648
Iteration 73/1000 | Loss: 0.00002647
Iteration 74/1000 | Loss: 0.00002647
Iteration 75/1000 | Loss: 0.00002647
Iteration 76/1000 | Loss: 0.00002646
Iteration 77/1000 | Loss: 0.00002646
Iteration 78/1000 | Loss: 0.00002646
Iteration 79/1000 | Loss: 0.00002645
Iteration 80/1000 | Loss: 0.00002645
Iteration 81/1000 | Loss: 0.00002645
Iteration 82/1000 | Loss: 0.00002645
Iteration 83/1000 | Loss: 0.00002645
Iteration 84/1000 | Loss: 0.00002645
Iteration 85/1000 | Loss: 0.00002644
Iteration 86/1000 | Loss: 0.00002644
Iteration 87/1000 | Loss: 0.00002644
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002644
Iteration 90/1000 | Loss: 0.00002644
Iteration 91/1000 | Loss: 0.00002644
Iteration 92/1000 | Loss: 0.00002643
Iteration 93/1000 | Loss: 0.00002643
Iteration 94/1000 | Loss: 0.00002643
Iteration 95/1000 | Loss: 0.00002643
Iteration 96/1000 | Loss: 0.00002642
Iteration 97/1000 | Loss: 0.00002642
Iteration 98/1000 | Loss: 0.00002642
Iteration 99/1000 | Loss: 0.00002642
Iteration 100/1000 | Loss: 0.00002641
Iteration 101/1000 | Loss: 0.00002641
Iteration 102/1000 | Loss: 0.00002641
Iteration 103/1000 | Loss: 0.00002641
Iteration 104/1000 | Loss: 0.00002641
Iteration 105/1000 | Loss: 0.00002641
Iteration 106/1000 | Loss: 0.00002641
Iteration 107/1000 | Loss: 0.00002640
Iteration 108/1000 | Loss: 0.00002640
Iteration 109/1000 | Loss: 0.00002640
Iteration 110/1000 | Loss: 0.00002640
Iteration 111/1000 | Loss: 0.00002640
Iteration 112/1000 | Loss: 0.00002640
Iteration 113/1000 | Loss: 0.00002640
Iteration 114/1000 | Loss: 0.00002640
Iteration 115/1000 | Loss: 0.00002640
Iteration 116/1000 | Loss: 0.00002640
Iteration 117/1000 | Loss: 0.00002639
Iteration 118/1000 | Loss: 0.00002639
Iteration 119/1000 | Loss: 0.00002639
Iteration 120/1000 | Loss: 0.00002639
Iteration 121/1000 | Loss: 0.00002639
Iteration 122/1000 | Loss: 0.00002639
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002638
Iteration 125/1000 | Loss: 0.00002638
Iteration 126/1000 | Loss: 0.00002637
Iteration 127/1000 | Loss: 0.00002637
Iteration 128/1000 | Loss: 0.00002637
Iteration 129/1000 | Loss: 0.00002637
Iteration 130/1000 | Loss: 0.00002636
Iteration 131/1000 | Loss: 0.00002636
Iteration 132/1000 | Loss: 0.00002636
Iteration 133/1000 | Loss: 0.00002635
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002635
Iteration 136/1000 | Loss: 0.00002635
Iteration 137/1000 | Loss: 0.00002635
Iteration 138/1000 | Loss: 0.00002635
Iteration 139/1000 | Loss: 0.00002635
Iteration 140/1000 | Loss: 0.00002635
Iteration 141/1000 | Loss: 0.00002635
Iteration 142/1000 | Loss: 0.00002634
Iteration 143/1000 | Loss: 0.00002634
Iteration 144/1000 | Loss: 0.00002634
Iteration 145/1000 | Loss: 0.00002634
Iteration 146/1000 | Loss: 0.00002634
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002633
Iteration 149/1000 | Loss: 0.00002633
Iteration 150/1000 | Loss: 0.00002633
Iteration 151/1000 | Loss: 0.00002633
Iteration 152/1000 | Loss: 0.00002633
Iteration 153/1000 | Loss: 0.00002633
Iteration 154/1000 | Loss: 0.00002633
Iteration 155/1000 | Loss: 0.00002632
Iteration 156/1000 | Loss: 0.00002632
Iteration 157/1000 | Loss: 0.00002632
Iteration 158/1000 | Loss: 0.00002632
Iteration 159/1000 | Loss: 0.00002632
Iteration 160/1000 | Loss: 0.00002632
Iteration 161/1000 | Loss: 0.00002632
Iteration 162/1000 | Loss: 0.00002632
Iteration 163/1000 | Loss: 0.00002632
Iteration 164/1000 | Loss: 0.00002632
Iteration 165/1000 | Loss: 0.00002632
Iteration 166/1000 | Loss: 0.00002631
Iteration 167/1000 | Loss: 0.00002631
Iteration 168/1000 | Loss: 0.00002631
Iteration 169/1000 | Loss: 0.00002631
Iteration 170/1000 | Loss: 0.00002631
Iteration 171/1000 | Loss: 0.00002631
Iteration 172/1000 | Loss: 0.00002631
Iteration 173/1000 | Loss: 0.00002631
Iteration 174/1000 | Loss: 0.00002631
Iteration 175/1000 | Loss: 0.00002631
Iteration 176/1000 | Loss: 0.00002631
Iteration 177/1000 | Loss: 0.00002631
Iteration 178/1000 | Loss: 0.00002631
Iteration 179/1000 | Loss: 0.00002631
Iteration 180/1000 | Loss: 0.00002631
Iteration 181/1000 | Loss: 0.00002631
Iteration 182/1000 | Loss: 0.00002631
Iteration 183/1000 | Loss: 0.00002631
Iteration 184/1000 | Loss: 0.00002631
Iteration 185/1000 | Loss: 0.00002631
Iteration 186/1000 | Loss: 0.00002631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.6314684873796068e-05, 2.6314684873796068e-05, 2.6314684873796068e-05, 2.6314684873796068e-05, 2.6314684873796068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6314684873796068e-05

Optimization complete. Final v2v error: 4.086763381958008 mm

Highest mean error: 5.475542068481445 mm for frame 83

Lowest mean error: 3.2608604431152344 mm for frame 99

Saving results

Total time: 83.78726363182068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549714
Iteration 2/25 | Loss: 0.00129047
Iteration 3/25 | Loss: 0.00090727
Iteration 4/25 | Loss: 0.00089991
Iteration 5/25 | Loss: 0.00084544
Iteration 6/25 | Loss: 0.00084332
Iteration 7/25 | Loss: 0.00084257
Iteration 8/25 | Loss: 0.00084240
Iteration 9/25 | Loss: 0.00084240
Iteration 10/25 | Loss: 0.00084240
Iteration 11/25 | Loss: 0.00084240
Iteration 12/25 | Loss: 0.00084240
Iteration 13/25 | Loss: 0.00084240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008424048428423703, 0.0008424048428423703, 0.0008424048428423703, 0.0008424048428423703, 0.0008424048428423703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008424048428423703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.35851240
Iteration 2/25 | Loss: 0.00065958
Iteration 3/25 | Loss: 0.00065956
Iteration 4/25 | Loss: 0.00065956
Iteration 5/25 | Loss: 0.00065956
Iteration 6/25 | Loss: 0.00065956
Iteration 7/25 | Loss: 0.00065955
Iteration 8/25 | Loss: 0.00065955
Iteration 9/25 | Loss: 0.00065955
Iteration 10/25 | Loss: 0.00065955
Iteration 11/25 | Loss: 0.00065955
Iteration 12/25 | Loss: 0.00065955
Iteration 13/25 | Loss: 0.00065955
Iteration 14/25 | Loss: 0.00065955
Iteration 15/25 | Loss: 0.00065955
Iteration 16/25 | Loss: 0.00065955
Iteration 17/25 | Loss: 0.00065955
Iteration 18/25 | Loss: 0.00065955
Iteration 19/25 | Loss: 0.00065955
Iteration 20/25 | Loss: 0.00065955
Iteration 21/25 | Loss: 0.00065955
Iteration 22/25 | Loss: 0.00065955
Iteration 23/25 | Loss: 0.00065955
Iteration 24/25 | Loss: 0.00065955
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006595537415705621, 0.0006595537415705621, 0.0006595537415705621, 0.0006595537415705621, 0.0006595537415705621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006595537415705621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065955
Iteration 2/1000 | Loss: 0.00002665
Iteration 3/1000 | Loss: 0.00001679
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00002951
Iteration 6/1000 | Loss: 0.00003075
Iteration 7/1000 | Loss: 0.00001325
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001252
Iteration 13/1000 | Loss: 0.00001252
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001251
Iteration 16/1000 | Loss: 0.00001250
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00003018
Iteration 20/1000 | Loss: 0.00001718
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001222
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001222
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001221
Iteration 33/1000 | Loss: 0.00001221
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001918
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001212
Iteration 38/1000 | Loss: 0.00001211
Iteration 39/1000 | Loss: 0.00001211
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001759
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001202
Iteration 71/1000 | Loss: 0.00001202
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001201
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001198
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001197
Iteration 83/1000 | Loss: 0.00001197
Iteration 84/1000 | Loss: 0.00001197
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001196
Iteration 87/1000 | Loss: 0.00001196
Iteration 88/1000 | Loss: 0.00001196
Iteration 89/1000 | Loss: 0.00001196
Iteration 90/1000 | Loss: 0.00001195
Iteration 91/1000 | Loss: 0.00001195
Iteration 92/1000 | Loss: 0.00001195
Iteration 93/1000 | Loss: 0.00001195
Iteration 94/1000 | Loss: 0.00001195
Iteration 95/1000 | Loss: 0.00001195
Iteration 96/1000 | Loss: 0.00001195
Iteration 97/1000 | Loss: 0.00001195
Iteration 98/1000 | Loss: 0.00001195
Iteration 99/1000 | Loss: 0.00001195
Iteration 100/1000 | Loss: 0.00001195
Iteration 101/1000 | Loss: 0.00001195
Iteration 102/1000 | Loss: 0.00001194
Iteration 103/1000 | Loss: 0.00001194
Iteration 104/1000 | Loss: 0.00001194
Iteration 105/1000 | Loss: 0.00001194
Iteration 106/1000 | Loss: 0.00001194
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001191
Iteration 128/1000 | Loss: 0.00001191
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001190
Iteration 134/1000 | Loss: 0.00001190
Iteration 135/1000 | Loss: 0.00001190
Iteration 136/1000 | Loss: 0.00001190
Iteration 137/1000 | Loss: 0.00001190
Iteration 138/1000 | Loss: 0.00001190
Iteration 139/1000 | Loss: 0.00001190
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001190
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001190
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.1895564966835082e-05, 1.1895564966835082e-05, 1.1895564966835082e-05, 1.1895564966835082e-05, 1.1895564966835082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1895564966835082e-05

Optimization complete. Final v2v error: 2.895730972290039 mm

Highest mean error: 3.580172061920166 mm for frame 166

Lowest mean error: 2.547487735748291 mm for frame 143

Saving results

Total time: 47.652350425720215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977472
Iteration 2/25 | Loss: 0.00387637
Iteration 3/25 | Loss: 0.00254547
Iteration 4/25 | Loss: 0.00172420
Iteration 5/25 | Loss: 0.00150113
Iteration 6/25 | Loss: 0.00142279
Iteration 7/25 | Loss: 0.00142119
Iteration 8/25 | Loss: 0.00137243
Iteration 9/25 | Loss: 0.00135112
Iteration 10/25 | Loss: 0.00133348
Iteration 11/25 | Loss: 0.00133204
Iteration 12/25 | Loss: 0.00131092
Iteration 13/25 | Loss: 0.00131076
Iteration 14/25 | Loss: 0.00129888
Iteration 15/25 | Loss: 0.00128484
Iteration 16/25 | Loss: 0.00127418
Iteration 17/25 | Loss: 0.00127177
Iteration 18/25 | Loss: 0.00126287
Iteration 19/25 | Loss: 0.00126974
Iteration 20/25 | Loss: 0.00127653
Iteration 21/25 | Loss: 0.00125507
Iteration 22/25 | Loss: 0.00124230
Iteration 23/25 | Loss: 0.00123197
Iteration 24/25 | Loss: 0.00123225
Iteration 25/25 | Loss: 0.00122745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.48824978
Iteration 2/25 | Loss: 0.00417319
Iteration 3/25 | Loss: 0.00394035
Iteration 4/25 | Loss: 0.00394035
Iteration 5/25 | Loss: 0.00394035
Iteration 6/25 | Loss: 0.00394034
Iteration 7/25 | Loss: 0.00394034
Iteration 8/25 | Loss: 0.00394034
Iteration 9/25 | Loss: 0.00394034
Iteration 10/25 | Loss: 0.00394034
Iteration 11/25 | Loss: 0.00394034
Iteration 12/25 | Loss: 0.00394034
Iteration 13/25 | Loss: 0.00394034
Iteration 14/25 | Loss: 0.00394034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003940343391150236, 0.003940343391150236, 0.003940343391150236, 0.003940343391150236, 0.003940343391150236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003940343391150236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00394034
Iteration 2/1000 | Loss: 0.00251142
Iteration 3/1000 | Loss: 0.00081694
Iteration 4/1000 | Loss: 0.00094977
Iteration 5/1000 | Loss: 0.00054937
Iteration 6/1000 | Loss: 0.00035248
Iteration 7/1000 | Loss: 0.00071243
Iteration 8/1000 | Loss: 0.00112016
Iteration 9/1000 | Loss: 0.00074492
Iteration 10/1000 | Loss: 0.00269996
Iteration 11/1000 | Loss: 0.00186900
Iteration 12/1000 | Loss: 0.00150545
Iteration 13/1000 | Loss: 0.00505334
Iteration 14/1000 | Loss: 0.00181499
Iteration 15/1000 | Loss: 0.00142537
Iteration 16/1000 | Loss: 0.00102832
Iteration 17/1000 | Loss: 0.00084986
Iteration 18/1000 | Loss: 0.00084702
Iteration 19/1000 | Loss: 0.00155036
Iteration 20/1000 | Loss: 0.00090446
Iteration 21/1000 | Loss: 0.00055972
Iteration 22/1000 | Loss: 0.00030266
Iteration 23/1000 | Loss: 0.00142361
Iteration 24/1000 | Loss: 0.00040158
Iteration 25/1000 | Loss: 0.00079662
Iteration 26/1000 | Loss: 0.00020276
Iteration 27/1000 | Loss: 0.00020615
Iteration 28/1000 | Loss: 0.00016085
Iteration 29/1000 | Loss: 0.00143173
Iteration 30/1000 | Loss: 0.00054636
Iteration 31/1000 | Loss: 0.00084685
Iteration 32/1000 | Loss: 0.00086143
Iteration 33/1000 | Loss: 0.00058603
Iteration 34/1000 | Loss: 0.00073274
Iteration 35/1000 | Loss: 0.00101654
Iteration 36/1000 | Loss: 0.00079233
Iteration 37/1000 | Loss: 0.00079584
Iteration 38/1000 | Loss: 0.00195766
Iteration 39/1000 | Loss: 0.00074150
Iteration 40/1000 | Loss: 0.00055188
Iteration 41/1000 | Loss: 0.00049888
Iteration 42/1000 | Loss: 0.00038595
Iteration 43/1000 | Loss: 0.00034328
Iteration 44/1000 | Loss: 0.00013371
Iteration 45/1000 | Loss: 0.00029116
Iteration 46/1000 | Loss: 0.00012611
Iteration 47/1000 | Loss: 0.00011907
Iteration 48/1000 | Loss: 0.00119119
Iteration 49/1000 | Loss: 0.00012488
Iteration 50/1000 | Loss: 0.00060171
Iteration 51/1000 | Loss: 0.00082866
Iteration 52/1000 | Loss: 0.00144288
Iteration 53/1000 | Loss: 0.00127420
Iteration 54/1000 | Loss: 0.00117894
Iteration 55/1000 | Loss: 0.00155651
Iteration 56/1000 | Loss: 0.00047992
Iteration 57/1000 | Loss: 0.00082072
Iteration 58/1000 | Loss: 0.00050301
Iteration 59/1000 | Loss: 0.00010447
Iteration 60/1000 | Loss: 0.00009312
Iteration 61/1000 | Loss: 0.00079484
Iteration 62/1000 | Loss: 0.00347547
Iteration 63/1000 | Loss: 0.00141482
Iteration 64/1000 | Loss: 0.00046352
Iteration 65/1000 | Loss: 0.00035504
Iteration 66/1000 | Loss: 0.00269114
Iteration 67/1000 | Loss: 0.00034161
Iteration 68/1000 | Loss: 0.00009902
Iteration 69/1000 | Loss: 0.00050886
Iteration 70/1000 | Loss: 0.00101161
Iteration 71/1000 | Loss: 0.00027501
Iteration 72/1000 | Loss: 0.00009883
Iteration 73/1000 | Loss: 0.00008578
Iteration 74/1000 | Loss: 0.00007815
Iteration 75/1000 | Loss: 0.00167041
Iteration 76/1000 | Loss: 0.00189981
Iteration 77/1000 | Loss: 0.00291173
Iteration 78/1000 | Loss: 0.00013813
Iteration 79/1000 | Loss: 0.00070941
Iteration 80/1000 | Loss: 0.00008083
Iteration 81/1000 | Loss: 0.00064471
Iteration 82/1000 | Loss: 0.00006784
Iteration 83/1000 | Loss: 0.00006407
Iteration 84/1000 | Loss: 0.00022908
Iteration 85/1000 | Loss: 0.00005758
Iteration 86/1000 | Loss: 0.00005578
Iteration 87/1000 | Loss: 0.00005448
Iteration 88/1000 | Loss: 0.00019501
Iteration 89/1000 | Loss: 0.00005258
Iteration 90/1000 | Loss: 0.00044271
Iteration 91/1000 | Loss: 0.00005293
Iteration 92/1000 | Loss: 0.00005102
Iteration 93/1000 | Loss: 0.00061565
Iteration 94/1000 | Loss: 0.00093819
Iteration 95/1000 | Loss: 0.00112909
Iteration 96/1000 | Loss: 0.00005379
Iteration 97/1000 | Loss: 0.00004966
Iteration 98/1000 | Loss: 0.00004698
Iteration 99/1000 | Loss: 0.00086298
Iteration 100/1000 | Loss: 0.00024094
Iteration 101/1000 | Loss: 0.00135536
Iteration 102/1000 | Loss: 0.00077777
Iteration 103/1000 | Loss: 0.00006002
Iteration 104/1000 | Loss: 0.00004743
Iteration 105/1000 | Loss: 0.00004450
Iteration 106/1000 | Loss: 0.00004217
Iteration 107/1000 | Loss: 0.00003997
Iteration 108/1000 | Loss: 0.00041758
Iteration 109/1000 | Loss: 0.00004018
Iteration 110/1000 | Loss: 0.00003809
Iteration 111/1000 | Loss: 0.00003689
Iteration 112/1000 | Loss: 0.00003596
Iteration 113/1000 | Loss: 0.00074257
Iteration 114/1000 | Loss: 0.00029130
Iteration 115/1000 | Loss: 0.00025043
Iteration 116/1000 | Loss: 0.00023265
Iteration 117/1000 | Loss: 0.00024865
Iteration 118/1000 | Loss: 0.00016287
Iteration 119/1000 | Loss: 0.00003728
Iteration 120/1000 | Loss: 0.00003367
Iteration 121/1000 | Loss: 0.00003214
Iteration 122/1000 | Loss: 0.00003112
Iteration 123/1000 | Loss: 0.00003024
Iteration 124/1000 | Loss: 0.00002968
Iteration 125/1000 | Loss: 0.00002939
Iteration 126/1000 | Loss: 0.00002914
Iteration 127/1000 | Loss: 0.00041786
Iteration 128/1000 | Loss: 0.00002981
Iteration 129/1000 | Loss: 0.00002824
Iteration 130/1000 | Loss: 0.00002756
Iteration 131/1000 | Loss: 0.00002699
Iteration 132/1000 | Loss: 0.00002655
Iteration 133/1000 | Loss: 0.00002630
Iteration 134/1000 | Loss: 0.00002609
Iteration 135/1000 | Loss: 0.00002589
Iteration 136/1000 | Loss: 0.00002587
Iteration 137/1000 | Loss: 0.00002585
Iteration 138/1000 | Loss: 0.00002570
Iteration 139/1000 | Loss: 0.00002570
Iteration 140/1000 | Loss: 0.00002569
Iteration 141/1000 | Loss: 0.00002568
Iteration 142/1000 | Loss: 0.00002566
Iteration 143/1000 | Loss: 0.00002565
Iteration 144/1000 | Loss: 0.00002565
Iteration 145/1000 | Loss: 0.00002564
Iteration 146/1000 | Loss: 0.00002563
Iteration 147/1000 | Loss: 0.00002563
Iteration 148/1000 | Loss: 0.00002561
Iteration 149/1000 | Loss: 0.00002561
Iteration 150/1000 | Loss: 0.00002560
Iteration 151/1000 | Loss: 0.00002560
Iteration 152/1000 | Loss: 0.00002560
Iteration 153/1000 | Loss: 0.00002559
Iteration 154/1000 | Loss: 0.00002559
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002559
Iteration 157/1000 | Loss: 0.00002559
Iteration 158/1000 | Loss: 0.00002559
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002557
Iteration 165/1000 | Loss: 0.00002557
Iteration 166/1000 | Loss: 0.00002557
Iteration 167/1000 | Loss: 0.00002557
Iteration 168/1000 | Loss: 0.00002557
Iteration 169/1000 | Loss: 0.00002557
Iteration 170/1000 | Loss: 0.00002556
Iteration 171/1000 | Loss: 0.00002556
Iteration 172/1000 | Loss: 0.00036213
Iteration 173/1000 | Loss: 0.00002606
Iteration 174/1000 | Loss: 0.00002497
Iteration 175/1000 | Loss: 0.00002430
Iteration 176/1000 | Loss: 0.00002387
Iteration 177/1000 | Loss: 0.00002361
Iteration 178/1000 | Loss: 0.00002349
Iteration 179/1000 | Loss: 0.00002347
Iteration 180/1000 | Loss: 0.00002345
Iteration 181/1000 | Loss: 0.00002344
Iteration 182/1000 | Loss: 0.00002344
Iteration 183/1000 | Loss: 0.00002343
Iteration 184/1000 | Loss: 0.00002332
Iteration 185/1000 | Loss: 0.00002325
Iteration 186/1000 | Loss: 0.00002323
Iteration 187/1000 | Loss: 0.00002322
Iteration 188/1000 | Loss: 0.00002321
Iteration 189/1000 | Loss: 0.00002319
Iteration 190/1000 | Loss: 0.00002319
Iteration 191/1000 | Loss: 0.00002319
Iteration 192/1000 | Loss: 0.00002319
Iteration 193/1000 | Loss: 0.00002319
Iteration 194/1000 | Loss: 0.00002318
Iteration 195/1000 | Loss: 0.00002318
Iteration 196/1000 | Loss: 0.00002318
Iteration 197/1000 | Loss: 0.00002317
Iteration 198/1000 | Loss: 0.00002317
Iteration 199/1000 | Loss: 0.00002317
Iteration 200/1000 | Loss: 0.00002317
Iteration 201/1000 | Loss: 0.00002317
Iteration 202/1000 | Loss: 0.00002317
Iteration 203/1000 | Loss: 0.00002317
Iteration 204/1000 | Loss: 0.00002317
Iteration 205/1000 | Loss: 0.00002317
Iteration 206/1000 | Loss: 0.00002317
Iteration 207/1000 | Loss: 0.00002317
Iteration 208/1000 | Loss: 0.00002317
Iteration 209/1000 | Loss: 0.00002317
Iteration 210/1000 | Loss: 0.00002316
Iteration 211/1000 | Loss: 0.00002316
Iteration 212/1000 | Loss: 0.00002316
Iteration 213/1000 | Loss: 0.00002316
Iteration 214/1000 | Loss: 0.00002316
Iteration 215/1000 | Loss: 0.00002316
Iteration 216/1000 | Loss: 0.00002316
Iteration 217/1000 | Loss: 0.00002316
Iteration 218/1000 | Loss: 0.00002315
Iteration 219/1000 | Loss: 0.00002315
Iteration 220/1000 | Loss: 0.00002315
Iteration 221/1000 | Loss: 0.00002315
Iteration 222/1000 | Loss: 0.00002315
Iteration 223/1000 | Loss: 0.00002315
Iteration 224/1000 | Loss: 0.00002315
Iteration 225/1000 | Loss: 0.00002315
Iteration 226/1000 | Loss: 0.00002315
Iteration 227/1000 | Loss: 0.00002314
Iteration 228/1000 | Loss: 0.00002314
Iteration 229/1000 | Loss: 0.00002314
Iteration 230/1000 | Loss: 0.00002313
Iteration 231/1000 | Loss: 0.00002313
Iteration 232/1000 | Loss: 0.00002313
Iteration 233/1000 | Loss: 0.00002312
Iteration 234/1000 | Loss: 0.00002312
Iteration 235/1000 | Loss: 0.00002311
Iteration 236/1000 | Loss: 0.00002311
Iteration 237/1000 | Loss: 0.00002311
Iteration 238/1000 | Loss: 0.00002311
Iteration 239/1000 | Loss: 0.00002311
Iteration 240/1000 | Loss: 0.00002311
Iteration 241/1000 | Loss: 0.00002311
Iteration 242/1000 | Loss: 0.00002311
Iteration 243/1000 | Loss: 0.00002311
Iteration 244/1000 | Loss: 0.00002311
Iteration 245/1000 | Loss: 0.00002310
Iteration 246/1000 | Loss: 0.00002310
Iteration 247/1000 | Loss: 0.00002310
Iteration 248/1000 | Loss: 0.00002310
Iteration 249/1000 | Loss: 0.00002310
Iteration 250/1000 | Loss: 0.00002310
Iteration 251/1000 | Loss: 0.00002310
Iteration 252/1000 | Loss: 0.00002310
Iteration 253/1000 | Loss: 0.00002310
Iteration 254/1000 | Loss: 0.00002310
Iteration 255/1000 | Loss: 0.00002310
Iteration 256/1000 | Loss: 0.00002310
Iteration 257/1000 | Loss: 0.00002310
Iteration 258/1000 | Loss: 0.00002310
Iteration 259/1000 | Loss: 0.00002310
Iteration 260/1000 | Loss: 0.00002310
Iteration 261/1000 | Loss: 0.00002310
Iteration 262/1000 | Loss: 0.00002310
Iteration 263/1000 | Loss: 0.00002310
Iteration 264/1000 | Loss: 0.00002310
Iteration 265/1000 | Loss: 0.00002310
Iteration 266/1000 | Loss: 0.00002310
Iteration 267/1000 | Loss: 0.00002310
Iteration 268/1000 | Loss: 0.00002309
Iteration 269/1000 | Loss: 0.00002309
Iteration 270/1000 | Loss: 0.00002309
Iteration 271/1000 | Loss: 0.00002309
Iteration 272/1000 | Loss: 0.00002309
Iteration 273/1000 | Loss: 0.00002309
Iteration 274/1000 | Loss: 0.00002309
Iteration 275/1000 | Loss: 0.00002309
Iteration 276/1000 | Loss: 0.00002309
Iteration 277/1000 | Loss: 0.00002309
Iteration 278/1000 | Loss: 0.00002309
Iteration 279/1000 | Loss: 0.00002309
Iteration 280/1000 | Loss: 0.00002309
Iteration 281/1000 | Loss: 0.00002309
Iteration 282/1000 | Loss: 0.00002309
Iteration 283/1000 | Loss: 0.00002309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [2.309492447238881e-05, 2.309492447238881e-05, 2.309492447238881e-05, 2.309492447238881e-05, 2.309492447238881e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.309492447238881e-05

Optimization complete. Final v2v error: 3.834245204925537 mm

Highest mean error: 5.4244065284729 mm for frame 114

Lowest mean error: 2.8811473846435547 mm for frame 190

Saving results

Total time: 275.0144786834717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754193
Iteration 2/25 | Loss: 0.00195220
Iteration 3/25 | Loss: 0.00146765
Iteration 4/25 | Loss: 0.00137347
Iteration 5/25 | Loss: 0.00127579
Iteration 6/25 | Loss: 0.00124889
Iteration 7/25 | Loss: 0.00127586
Iteration 8/25 | Loss: 0.00162843
Iteration 9/25 | Loss: 0.00138877
Iteration 10/25 | Loss: 0.00101248
Iteration 11/25 | Loss: 0.00097996
Iteration 12/25 | Loss: 0.00092099
Iteration 13/25 | Loss: 0.00091817
Iteration 14/25 | Loss: 0.00091701
Iteration 15/25 | Loss: 0.00091685
Iteration 16/25 | Loss: 0.00091684
Iteration 17/25 | Loss: 0.00091683
Iteration 18/25 | Loss: 0.00091683
Iteration 19/25 | Loss: 0.00091683
Iteration 20/25 | Loss: 0.00091683
Iteration 21/25 | Loss: 0.00091683
Iteration 22/25 | Loss: 0.00091683
Iteration 23/25 | Loss: 0.00091683
Iteration 24/25 | Loss: 0.00091683
Iteration 25/25 | Loss: 0.00091683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56624484
Iteration 2/25 | Loss: 0.00137593
Iteration 3/25 | Loss: 0.00118006
Iteration 4/25 | Loss: 0.00118006
Iteration 5/25 | Loss: 0.00118006
Iteration 6/25 | Loss: 0.00118005
Iteration 7/25 | Loss: 0.00118005
Iteration 8/25 | Loss: 0.00118005
Iteration 9/25 | Loss: 0.00118005
Iteration 10/25 | Loss: 0.00118005
Iteration 11/25 | Loss: 0.00118005
Iteration 12/25 | Loss: 0.00118005
Iteration 13/25 | Loss: 0.00118005
Iteration 14/25 | Loss: 0.00118005
Iteration 15/25 | Loss: 0.00118005
Iteration 16/25 | Loss: 0.00118005
Iteration 17/25 | Loss: 0.00118005
Iteration 18/25 | Loss: 0.00118005
Iteration 19/25 | Loss: 0.00118005
Iteration 20/25 | Loss: 0.00118005
Iteration 21/25 | Loss: 0.00118005
Iteration 22/25 | Loss: 0.00118005
Iteration 23/25 | Loss: 0.00118005
Iteration 24/25 | Loss: 0.00118005
Iteration 25/25 | Loss: 0.00118005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118005
Iteration 2/1000 | Loss: 0.00127830
Iteration 3/1000 | Loss: 0.00006659
Iteration 4/1000 | Loss: 0.00004271
Iteration 5/1000 | Loss: 0.00003679
Iteration 6/1000 | Loss: 0.00007326
Iteration 7/1000 | Loss: 0.00003259
Iteration 8/1000 | Loss: 0.00003097
Iteration 9/1000 | Loss: 0.00002963
Iteration 10/1000 | Loss: 0.00002893
Iteration 11/1000 | Loss: 0.00002833
Iteration 12/1000 | Loss: 0.00002781
Iteration 13/1000 | Loss: 0.00002756
Iteration 14/1000 | Loss: 0.00002728
Iteration 15/1000 | Loss: 0.00002708
Iteration 16/1000 | Loss: 0.00002694
Iteration 17/1000 | Loss: 0.00002693
Iteration 18/1000 | Loss: 0.00002677
Iteration 19/1000 | Loss: 0.00002676
Iteration 20/1000 | Loss: 0.00002669
Iteration 21/1000 | Loss: 0.00002654
Iteration 22/1000 | Loss: 0.00002652
Iteration 23/1000 | Loss: 0.00002650
Iteration 24/1000 | Loss: 0.00002644
Iteration 25/1000 | Loss: 0.00002641
Iteration 26/1000 | Loss: 0.00002640
Iteration 27/1000 | Loss: 0.00002639
Iteration 28/1000 | Loss: 0.00002639
Iteration 29/1000 | Loss: 0.00002639
Iteration 30/1000 | Loss: 0.00002639
Iteration 31/1000 | Loss: 0.00002639
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002638
Iteration 34/1000 | Loss: 0.00002636
Iteration 35/1000 | Loss: 0.00002633
Iteration 36/1000 | Loss: 0.00002632
Iteration 37/1000 | Loss: 0.00002629
Iteration 38/1000 | Loss: 0.00002629
Iteration 39/1000 | Loss: 0.00002628
Iteration 40/1000 | Loss: 0.00002626
Iteration 41/1000 | Loss: 0.00002626
Iteration 42/1000 | Loss: 0.00002626
Iteration 43/1000 | Loss: 0.00002626
Iteration 44/1000 | Loss: 0.00002626
Iteration 45/1000 | Loss: 0.00002625
Iteration 46/1000 | Loss: 0.00002625
Iteration 47/1000 | Loss: 0.00002625
Iteration 48/1000 | Loss: 0.00002625
Iteration 49/1000 | Loss: 0.00002625
Iteration 50/1000 | Loss: 0.00002625
Iteration 51/1000 | Loss: 0.00002625
Iteration 52/1000 | Loss: 0.00002623
Iteration 53/1000 | Loss: 0.00002622
Iteration 54/1000 | Loss: 0.00002622
Iteration 55/1000 | Loss: 0.00002621
Iteration 56/1000 | Loss: 0.00002620
Iteration 57/1000 | Loss: 0.00002620
Iteration 58/1000 | Loss: 0.00002620
Iteration 59/1000 | Loss: 0.00002619
Iteration 60/1000 | Loss: 0.00002618
Iteration 61/1000 | Loss: 0.00002618
Iteration 62/1000 | Loss: 0.00002618
Iteration 63/1000 | Loss: 0.00002617
Iteration 64/1000 | Loss: 0.00002617
Iteration 65/1000 | Loss: 0.00002615
Iteration 66/1000 | Loss: 0.00002614
Iteration 67/1000 | Loss: 0.00002614
Iteration 68/1000 | Loss: 0.00002614
Iteration 69/1000 | Loss: 0.00002613
Iteration 70/1000 | Loss: 0.00002612
Iteration 71/1000 | Loss: 0.00002612
Iteration 72/1000 | Loss: 0.00002612
Iteration 73/1000 | Loss: 0.00002612
Iteration 74/1000 | Loss: 0.00002612
Iteration 75/1000 | Loss: 0.00002611
Iteration 76/1000 | Loss: 0.00002611
Iteration 77/1000 | Loss: 0.00002611
Iteration 78/1000 | Loss: 0.00002610
Iteration 79/1000 | Loss: 0.00002610
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002609
Iteration 82/1000 | Loss: 0.00002608
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002608
Iteration 85/1000 | Loss: 0.00002608
Iteration 86/1000 | Loss: 0.00002608
Iteration 87/1000 | Loss: 0.00002607
Iteration 88/1000 | Loss: 0.00002607
Iteration 89/1000 | Loss: 0.00002607
Iteration 90/1000 | Loss: 0.00002606
Iteration 91/1000 | Loss: 0.00002606
Iteration 92/1000 | Loss: 0.00002606
Iteration 93/1000 | Loss: 0.00002605
Iteration 94/1000 | Loss: 0.00002605
Iteration 95/1000 | Loss: 0.00002605
Iteration 96/1000 | Loss: 0.00002604
Iteration 97/1000 | Loss: 0.00002604
Iteration 98/1000 | Loss: 0.00002603
Iteration 99/1000 | Loss: 0.00002602
Iteration 100/1000 | Loss: 0.00002602
Iteration 101/1000 | Loss: 0.00002602
Iteration 102/1000 | Loss: 0.00002601
Iteration 103/1000 | Loss: 0.00002600
Iteration 104/1000 | Loss: 0.00002600
Iteration 105/1000 | Loss: 0.00002600
Iteration 106/1000 | Loss: 0.00002599
Iteration 107/1000 | Loss: 0.00002599
Iteration 108/1000 | Loss: 0.00002599
Iteration 109/1000 | Loss: 0.00002598
Iteration 110/1000 | Loss: 0.00002598
Iteration 111/1000 | Loss: 0.00002598
Iteration 112/1000 | Loss: 0.00002598
Iteration 113/1000 | Loss: 0.00002598
Iteration 114/1000 | Loss: 0.00002598
Iteration 115/1000 | Loss: 0.00002598
Iteration 116/1000 | Loss: 0.00002598
Iteration 117/1000 | Loss: 0.00002597
Iteration 118/1000 | Loss: 0.00002597
Iteration 119/1000 | Loss: 0.00002597
Iteration 120/1000 | Loss: 0.00002597
Iteration 121/1000 | Loss: 0.00002597
Iteration 122/1000 | Loss: 0.00002596
Iteration 123/1000 | Loss: 0.00002596
Iteration 124/1000 | Loss: 0.00002596
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002595
Iteration 127/1000 | Loss: 0.00002595
Iteration 128/1000 | Loss: 0.00002595
Iteration 129/1000 | Loss: 0.00002595
Iteration 130/1000 | Loss: 0.00002594
Iteration 131/1000 | Loss: 0.00002594
Iteration 132/1000 | Loss: 0.00002594
Iteration 133/1000 | Loss: 0.00002594
Iteration 134/1000 | Loss: 0.00002593
Iteration 135/1000 | Loss: 0.00002593
Iteration 136/1000 | Loss: 0.00002593
Iteration 137/1000 | Loss: 0.00002593
Iteration 138/1000 | Loss: 0.00002592
Iteration 139/1000 | Loss: 0.00002592
Iteration 140/1000 | Loss: 0.00002592
Iteration 141/1000 | Loss: 0.00002592
Iteration 142/1000 | Loss: 0.00002591
Iteration 143/1000 | Loss: 0.00002591
Iteration 144/1000 | Loss: 0.00002591
Iteration 145/1000 | Loss: 0.00002590
Iteration 146/1000 | Loss: 0.00002590
Iteration 147/1000 | Loss: 0.00002590
Iteration 148/1000 | Loss: 0.00002589
Iteration 149/1000 | Loss: 0.00002589
Iteration 150/1000 | Loss: 0.00002589
Iteration 151/1000 | Loss: 0.00002589
Iteration 152/1000 | Loss: 0.00002589
Iteration 153/1000 | Loss: 0.00002588
Iteration 154/1000 | Loss: 0.00002588
Iteration 155/1000 | Loss: 0.00002588
Iteration 156/1000 | Loss: 0.00002588
Iteration 157/1000 | Loss: 0.00002587
Iteration 158/1000 | Loss: 0.00002587
Iteration 159/1000 | Loss: 0.00002587
Iteration 160/1000 | Loss: 0.00002587
Iteration 161/1000 | Loss: 0.00002586
Iteration 162/1000 | Loss: 0.00002586
Iteration 163/1000 | Loss: 0.00002586
Iteration 164/1000 | Loss: 0.00002586
Iteration 165/1000 | Loss: 0.00002586
Iteration 166/1000 | Loss: 0.00002586
Iteration 167/1000 | Loss: 0.00002585
Iteration 168/1000 | Loss: 0.00002585
Iteration 169/1000 | Loss: 0.00002585
Iteration 170/1000 | Loss: 0.00002585
Iteration 171/1000 | Loss: 0.00002585
Iteration 172/1000 | Loss: 0.00002585
Iteration 173/1000 | Loss: 0.00002585
Iteration 174/1000 | Loss: 0.00002585
Iteration 175/1000 | Loss: 0.00002585
Iteration 176/1000 | Loss: 0.00002585
Iteration 177/1000 | Loss: 0.00002585
Iteration 178/1000 | Loss: 0.00002585
Iteration 179/1000 | Loss: 0.00002585
Iteration 180/1000 | Loss: 0.00002585
Iteration 181/1000 | Loss: 0.00002584
Iteration 182/1000 | Loss: 0.00002584
Iteration 183/1000 | Loss: 0.00002584
Iteration 184/1000 | Loss: 0.00002584
Iteration 185/1000 | Loss: 0.00002584
Iteration 186/1000 | Loss: 0.00002584
Iteration 187/1000 | Loss: 0.00002584
Iteration 188/1000 | Loss: 0.00002584
Iteration 189/1000 | Loss: 0.00002584
Iteration 190/1000 | Loss: 0.00002584
Iteration 191/1000 | Loss: 0.00002584
Iteration 192/1000 | Loss: 0.00002584
Iteration 193/1000 | Loss: 0.00002584
Iteration 194/1000 | Loss: 0.00002584
Iteration 195/1000 | Loss: 0.00002584
Iteration 196/1000 | Loss: 0.00002584
Iteration 197/1000 | Loss: 0.00002584
Iteration 198/1000 | Loss: 0.00002584
Iteration 199/1000 | Loss: 0.00002584
Iteration 200/1000 | Loss: 0.00002584
Iteration 201/1000 | Loss: 0.00002584
Iteration 202/1000 | Loss: 0.00002584
Iteration 203/1000 | Loss: 0.00002584
Iteration 204/1000 | Loss: 0.00002584
Iteration 205/1000 | Loss: 0.00002584
Iteration 206/1000 | Loss: 0.00002584
Iteration 207/1000 | Loss: 0.00002584
Iteration 208/1000 | Loss: 0.00002584
Iteration 209/1000 | Loss: 0.00002584
Iteration 210/1000 | Loss: 0.00002584
Iteration 211/1000 | Loss: 0.00002584
Iteration 212/1000 | Loss: 0.00002584
Iteration 213/1000 | Loss: 0.00002584
Iteration 214/1000 | Loss: 0.00002584
Iteration 215/1000 | Loss: 0.00002584
Iteration 216/1000 | Loss: 0.00002584
Iteration 217/1000 | Loss: 0.00002584
Iteration 218/1000 | Loss: 0.00002584
Iteration 219/1000 | Loss: 0.00002584
Iteration 220/1000 | Loss: 0.00002584
Iteration 221/1000 | Loss: 0.00002584
Iteration 222/1000 | Loss: 0.00002584
Iteration 223/1000 | Loss: 0.00002584
Iteration 224/1000 | Loss: 0.00002584
Iteration 225/1000 | Loss: 0.00002584
Iteration 226/1000 | Loss: 0.00002584
Iteration 227/1000 | Loss: 0.00002584
Iteration 228/1000 | Loss: 0.00002584
Iteration 229/1000 | Loss: 0.00002584
Iteration 230/1000 | Loss: 0.00002584
Iteration 231/1000 | Loss: 0.00002584
Iteration 232/1000 | Loss: 0.00002584
Iteration 233/1000 | Loss: 0.00002584
Iteration 234/1000 | Loss: 0.00002584
Iteration 235/1000 | Loss: 0.00002584
Iteration 236/1000 | Loss: 0.00002584
Iteration 237/1000 | Loss: 0.00002584
Iteration 238/1000 | Loss: 0.00002584
Iteration 239/1000 | Loss: 0.00002584
Iteration 240/1000 | Loss: 0.00002584
Iteration 241/1000 | Loss: 0.00002584
Iteration 242/1000 | Loss: 0.00002584
Iteration 243/1000 | Loss: 0.00002584
Iteration 244/1000 | Loss: 0.00002584
Iteration 245/1000 | Loss: 0.00002584
Iteration 246/1000 | Loss: 0.00002584
Iteration 247/1000 | Loss: 0.00002584
Iteration 248/1000 | Loss: 0.00002584
Iteration 249/1000 | Loss: 0.00002584
Iteration 250/1000 | Loss: 0.00002584
Iteration 251/1000 | Loss: 0.00002584
Iteration 252/1000 | Loss: 0.00002584
Iteration 253/1000 | Loss: 0.00002584
Iteration 254/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.5842638933681883e-05, 2.5842638933681883e-05, 2.5842638933681883e-05, 2.5842638933681883e-05, 2.5842638933681883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5842638933681883e-05

Optimization complete. Final v2v error: 4.146003723144531 mm

Highest mean error: 6.277546405792236 mm for frame 158

Lowest mean error: 3.2867112159729004 mm for frame 200

Saving results

Total time: 83.49805498123169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00529603
Iteration 2/25 | Loss: 0.00116154
Iteration 3/25 | Loss: 0.00088324
Iteration 4/25 | Loss: 0.00085312
Iteration 5/25 | Loss: 0.00084516
Iteration 6/25 | Loss: 0.00084228
Iteration 7/25 | Loss: 0.00084139
Iteration 8/25 | Loss: 0.00084139
Iteration 9/25 | Loss: 0.00084139
Iteration 10/25 | Loss: 0.00084139
Iteration 11/25 | Loss: 0.00084139
Iteration 12/25 | Loss: 0.00084139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008413912728428841, 0.0008413912728428841, 0.0008413912728428841, 0.0008413912728428841, 0.0008413912728428841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008413912728428841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86208898
Iteration 2/25 | Loss: 0.00045772
Iteration 3/25 | Loss: 0.00045771
Iteration 4/25 | Loss: 0.00045771
Iteration 5/25 | Loss: 0.00045771
Iteration 6/25 | Loss: 0.00045771
Iteration 7/25 | Loss: 0.00045771
Iteration 8/25 | Loss: 0.00045771
Iteration 9/25 | Loss: 0.00045771
Iteration 10/25 | Loss: 0.00045771
Iteration 11/25 | Loss: 0.00045771
Iteration 12/25 | Loss: 0.00045771
Iteration 13/25 | Loss: 0.00045771
Iteration 14/25 | Loss: 0.00045771
Iteration 15/25 | Loss: 0.00045771
Iteration 16/25 | Loss: 0.00045771
Iteration 17/25 | Loss: 0.00045771
Iteration 18/25 | Loss: 0.00045771
Iteration 19/25 | Loss: 0.00045771
Iteration 20/25 | Loss: 0.00045771
Iteration 21/25 | Loss: 0.00045771
Iteration 22/25 | Loss: 0.00045771
Iteration 23/25 | Loss: 0.00045771
Iteration 24/25 | Loss: 0.00045771
Iteration 25/25 | Loss: 0.00045771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045771
Iteration 2/1000 | Loss: 0.00003605
Iteration 3/1000 | Loss: 0.00001733
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001294
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001242
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001237
Iteration 50/1000 | Loss: 0.00001237
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001236
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001230
Iteration 80/1000 | Loss: 0.00001230
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001228
Iteration 86/1000 | Loss: 0.00001228
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001226
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001226
Iteration 106/1000 | Loss: 0.00001226
Iteration 107/1000 | Loss: 0.00001226
Iteration 108/1000 | Loss: 0.00001226
Iteration 109/1000 | Loss: 0.00001226
Iteration 110/1000 | Loss: 0.00001226
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2261974916327745e-05, 1.2261974916327745e-05, 1.2261974916327745e-05, 1.2261974916327745e-05, 1.2261974916327745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2261974916327745e-05

Optimization complete. Final v2v error: 2.972736358642578 mm

Highest mean error: 3.4564433097839355 mm for frame 239

Lowest mean error: 2.8829097747802734 mm for frame 23

Saving results

Total time: 43.24265813827515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840772
Iteration 2/25 | Loss: 0.00124224
Iteration 3/25 | Loss: 0.00094436
Iteration 4/25 | Loss: 0.00091006
Iteration 5/25 | Loss: 0.00090325
Iteration 6/25 | Loss: 0.00090272
Iteration 7/25 | Loss: 0.00090272
Iteration 8/25 | Loss: 0.00090272
Iteration 9/25 | Loss: 0.00090272
Iteration 10/25 | Loss: 0.00090272
Iteration 11/25 | Loss: 0.00090272
Iteration 12/25 | Loss: 0.00090272
Iteration 13/25 | Loss: 0.00090272
Iteration 14/25 | Loss: 0.00090272
Iteration 15/25 | Loss: 0.00090272
Iteration 16/25 | Loss: 0.00090272
Iteration 17/25 | Loss: 0.00090272
Iteration 18/25 | Loss: 0.00090272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009027203777804971, 0.0009027203777804971, 0.0009027203777804971, 0.0009027203777804971, 0.0009027203777804971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009027203777804971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01564407
Iteration 2/25 | Loss: 0.00054008
Iteration 3/25 | Loss: 0.00054008
Iteration 4/25 | Loss: 0.00054008
Iteration 5/25 | Loss: 0.00054008
Iteration 6/25 | Loss: 0.00054008
Iteration 7/25 | Loss: 0.00054008
Iteration 8/25 | Loss: 0.00054008
Iteration 9/25 | Loss: 0.00054008
Iteration 10/25 | Loss: 0.00054008
Iteration 11/25 | Loss: 0.00054008
Iteration 12/25 | Loss: 0.00054008
Iteration 13/25 | Loss: 0.00054008
Iteration 14/25 | Loss: 0.00054008
Iteration 15/25 | Loss: 0.00054008
Iteration 16/25 | Loss: 0.00054008
Iteration 17/25 | Loss: 0.00054008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005400810041464865, 0.0005400810041464865, 0.0005400810041464865, 0.0005400810041464865, 0.0005400810041464865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005400810041464865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054008
Iteration 2/1000 | Loss: 0.00002818
Iteration 3/1000 | Loss: 0.00002403
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002061
Iteration 8/1000 | Loss: 0.00002031
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00002010
Iteration 11/1000 | Loss: 0.00001996
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001991
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001985
Iteration 19/1000 | Loss: 0.00001985
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001983
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001982
Iteration 29/1000 | Loss: 0.00001982
Iteration 30/1000 | Loss: 0.00001982
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001982
Iteration 33/1000 | Loss: 0.00001982
Iteration 34/1000 | Loss: 0.00001982
Iteration 35/1000 | Loss: 0.00001982
Iteration 36/1000 | Loss: 0.00001982
Iteration 37/1000 | Loss: 0.00001981
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00001981
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001980
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001980
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001978
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001975
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001975
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001975
Iteration 57/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.9748757040360942e-05, 1.9748757040360942e-05, 1.9748757040360942e-05, 1.9748757040360942e-05, 1.9748757040360942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9748757040360942e-05

Optimization complete. Final v2v error: 3.798978567123413 mm

Highest mean error: 4.016616344451904 mm for frame 154

Lowest mean error: 3.624558210372925 mm for frame 168

Saving results

Total time: 29.369055032730103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009037
Iteration 2/25 | Loss: 0.00261376
Iteration 3/25 | Loss: 0.00176660
Iteration 4/25 | Loss: 0.00151611
Iteration 5/25 | Loss: 0.00156887
Iteration 6/25 | Loss: 0.00155250
Iteration 7/25 | Loss: 0.00155962
Iteration 8/25 | Loss: 0.00149075
Iteration 9/25 | Loss: 0.00140188
Iteration 10/25 | Loss: 0.00136711
Iteration 11/25 | Loss: 0.00134117
Iteration 12/25 | Loss: 0.00129499
Iteration 13/25 | Loss: 0.00124552
Iteration 14/25 | Loss: 0.00119514
Iteration 15/25 | Loss: 0.00115889
Iteration 16/25 | Loss: 0.00114352
Iteration 17/25 | Loss: 0.00111001
Iteration 18/25 | Loss: 0.00109192
Iteration 19/25 | Loss: 0.00106577
Iteration 20/25 | Loss: 0.00105312
Iteration 21/25 | Loss: 0.00105035
Iteration 22/25 | Loss: 0.00104116
Iteration 23/25 | Loss: 0.00104077
Iteration 24/25 | Loss: 0.00103054
Iteration 25/25 | Loss: 0.00103211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59378099
Iteration 2/25 | Loss: 0.00535842
Iteration 3/25 | Loss: 0.00290511
Iteration 4/25 | Loss: 0.00290510
Iteration 5/25 | Loss: 0.00290509
Iteration 6/25 | Loss: 0.00290509
Iteration 7/25 | Loss: 0.00290509
Iteration 8/25 | Loss: 0.00290509
Iteration 9/25 | Loss: 0.00290509
Iteration 10/25 | Loss: 0.00290509
Iteration 11/25 | Loss: 0.00290509
Iteration 12/25 | Loss: 0.00290509
Iteration 13/25 | Loss: 0.00290509
Iteration 14/25 | Loss: 0.00290509
Iteration 15/25 | Loss: 0.00290509
Iteration 16/25 | Loss: 0.00290509
Iteration 17/25 | Loss: 0.00290509
Iteration 18/25 | Loss: 0.00290509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002905091270804405, 0.002905091270804405, 0.002905091270804405, 0.002905091270804405, 0.002905091270804405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002905091270804405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290509
Iteration 2/1000 | Loss: 0.00131519
Iteration 3/1000 | Loss: 0.00365612
Iteration 4/1000 | Loss: 0.00056830
Iteration 5/1000 | Loss: 0.00090924
Iteration 6/1000 | Loss: 0.00083957
Iteration 7/1000 | Loss: 0.00204194
Iteration 8/1000 | Loss: 0.00080985
Iteration 9/1000 | Loss: 0.00115751
Iteration 10/1000 | Loss: 0.00048541
Iteration 11/1000 | Loss: 0.00148863
Iteration 12/1000 | Loss: 0.00061869
Iteration 13/1000 | Loss: 0.00092782
Iteration 14/1000 | Loss: 0.00084089
Iteration 15/1000 | Loss: 0.00607782
Iteration 16/1000 | Loss: 0.00818836
Iteration 17/1000 | Loss: 0.00705117
Iteration 18/1000 | Loss: 0.00538630
Iteration 19/1000 | Loss: 0.00148963
Iteration 20/1000 | Loss: 0.00114266
Iteration 21/1000 | Loss: 0.00081596
Iteration 22/1000 | Loss: 0.00064713
Iteration 23/1000 | Loss: 0.00192416
Iteration 24/1000 | Loss: 0.00121794
Iteration 25/1000 | Loss: 0.00073968
Iteration 26/1000 | Loss: 0.00035983
Iteration 27/1000 | Loss: 0.00224185
Iteration 28/1000 | Loss: 0.00270470
Iteration 29/1000 | Loss: 0.00163624
Iteration 30/1000 | Loss: 0.00192593
Iteration 31/1000 | Loss: 0.00164257
Iteration 32/1000 | Loss: 0.00462619
Iteration 33/1000 | Loss: 0.00330531
Iteration 34/1000 | Loss: 0.00266017
Iteration 35/1000 | Loss: 0.00180445
Iteration 36/1000 | Loss: 0.00072492
Iteration 37/1000 | Loss: 0.00416580
Iteration 38/1000 | Loss: 0.00333719
Iteration 39/1000 | Loss: 0.00268811
Iteration 40/1000 | Loss: 0.00139321
Iteration 41/1000 | Loss: 0.00247269
Iteration 42/1000 | Loss: 0.00137138
Iteration 43/1000 | Loss: 0.00155323
Iteration 44/1000 | Loss: 0.00278094
Iteration 45/1000 | Loss: 0.00112147
Iteration 46/1000 | Loss: 0.00064816
Iteration 47/1000 | Loss: 0.00177445
Iteration 48/1000 | Loss: 0.00155121
Iteration 49/1000 | Loss: 0.00056741
Iteration 50/1000 | Loss: 0.00061551
Iteration 51/1000 | Loss: 0.00353508
Iteration 52/1000 | Loss: 0.00350171
Iteration 53/1000 | Loss: 0.00135768
Iteration 54/1000 | Loss: 0.00199611
Iteration 55/1000 | Loss: 0.00057537
Iteration 56/1000 | Loss: 0.00012841
Iteration 57/1000 | Loss: 0.00179694
Iteration 58/1000 | Loss: 0.00088969
Iteration 59/1000 | Loss: 0.00083899
Iteration 60/1000 | Loss: 0.00157925
Iteration 61/1000 | Loss: 0.00106811
Iteration 62/1000 | Loss: 0.00065938
Iteration 63/1000 | Loss: 0.00076758
Iteration 64/1000 | Loss: 0.00081564
Iteration 65/1000 | Loss: 0.00033868
Iteration 66/1000 | Loss: 0.00056532
Iteration 67/1000 | Loss: 0.00024123
Iteration 68/1000 | Loss: 0.00008617
Iteration 69/1000 | Loss: 0.00280261
Iteration 70/1000 | Loss: 0.00589396
Iteration 71/1000 | Loss: 0.00312681
Iteration 72/1000 | Loss: 0.00254053
Iteration 73/1000 | Loss: 0.00104942
Iteration 74/1000 | Loss: 0.00026574
Iteration 75/1000 | Loss: 0.00041480
Iteration 76/1000 | Loss: 0.00304812
Iteration 77/1000 | Loss: 0.00200604
Iteration 78/1000 | Loss: 0.00080310
Iteration 79/1000 | Loss: 0.00063642
Iteration 80/1000 | Loss: 0.00079324
Iteration 81/1000 | Loss: 0.00027562
Iteration 82/1000 | Loss: 0.00007626
Iteration 83/1000 | Loss: 0.00044731
Iteration 84/1000 | Loss: 0.00119550
Iteration 85/1000 | Loss: 0.00091537
Iteration 86/1000 | Loss: 0.00100664
Iteration 87/1000 | Loss: 0.00121794
Iteration 88/1000 | Loss: 0.00044655
Iteration 89/1000 | Loss: 0.00052896
Iteration 90/1000 | Loss: 0.00076683
Iteration 91/1000 | Loss: 0.00118079
Iteration 92/1000 | Loss: 0.00063228
Iteration 93/1000 | Loss: 0.00042089
Iteration 94/1000 | Loss: 0.00052476
Iteration 95/1000 | Loss: 0.00050326
Iteration 96/1000 | Loss: 0.00213043
Iteration 97/1000 | Loss: 0.00080882
Iteration 98/1000 | Loss: 0.00189063
Iteration 99/1000 | Loss: 0.00066568
Iteration 100/1000 | Loss: 0.00176689
Iteration 101/1000 | Loss: 0.00135277
Iteration 102/1000 | Loss: 0.00084056
Iteration 103/1000 | Loss: 0.00048388
Iteration 104/1000 | Loss: 0.00022179
Iteration 105/1000 | Loss: 0.00029985
Iteration 106/1000 | Loss: 0.00016923
Iteration 107/1000 | Loss: 0.00161079
Iteration 108/1000 | Loss: 0.00109403
Iteration 109/1000 | Loss: 0.00095399
Iteration 110/1000 | Loss: 0.00091485
Iteration 111/1000 | Loss: 0.00090283
Iteration 112/1000 | Loss: 0.00062751
Iteration 113/1000 | Loss: 0.00088427
Iteration 114/1000 | Loss: 0.00031015
Iteration 115/1000 | Loss: 0.00035283
Iteration 116/1000 | Loss: 0.00028595
Iteration 117/1000 | Loss: 0.00073494
Iteration 118/1000 | Loss: 0.00240401
Iteration 119/1000 | Loss: 0.00102079
Iteration 120/1000 | Loss: 0.00115707
Iteration 121/1000 | Loss: 0.00050742
Iteration 122/1000 | Loss: 0.00079498
Iteration 123/1000 | Loss: 0.00048719
Iteration 124/1000 | Loss: 0.00081740
Iteration 125/1000 | Loss: 0.00115293
Iteration 126/1000 | Loss: 0.00112864
Iteration 127/1000 | Loss: 0.00281566
Iteration 128/1000 | Loss: 0.00238423
Iteration 129/1000 | Loss: 0.00175324
Iteration 130/1000 | Loss: 0.00084723
Iteration 131/1000 | Loss: 0.00043702
Iteration 132/1000 | Loss: 0.00039178
Iteration 133/1000 | Loss: 0.00021273
Iteration 134/1000 | Loss: 0.00038827
Iteration 135/1000 | Loss: 0.00060827
Iteration 136/1000 | Loss: 0.00031034
Iteration 137/1000 | Loss: 0.00163261
Iteration 138/1000 | Loss: 0.00063871
Iteration 139/1000 | Loss: 0.00065413
Iteration 140/1000 | Loss: 0.00062302
Iteration 141/1000 | Loss: 0.00057212
Iteration 142/1000 | Loss: 0.00106125
Iteration 143/1000 | Loss: 0.00038019
Iteration 144/1000 | Loss: 0.00076096
Iteration 145/1000 | Loss: 0.00084843
Iteration 146/1000 | Loss: 0.00216335
Iteration 147/1000 | Loss: 0.00244488
Iteration 148/1000 | Loss: 0.00188699
Iteration 149/1000 | Loss: 0.00054035
Iteration 150/1000 | Loss: 0.00217162
Iteration 151/1000 | Loss: 0.00269920
Iteration 152/1000 | Loss: 0.00129257
Iteration 153/1000 | Loss: 0.00119789
Iteration 154/1000 | Loss: 0.00097351
Iteration 155/1000 | Loss: 0.00058358
Iteration 156/1000 | Loss: 0.00049488
Iteration 157/1000 | Loss: 0.00069593
Iteration 158/1000 | Loss: 0.00018611
Iteration 159/1000 | Loss: 0.00013593
Iteration 160/1000 | Loss: 0.00004750
Iteration 161/1000 | Loss: 0.00012346
Iteration 162/1000 | Loss: 0.00009961
Iteration 163/1000 | Loss: 0.00022970
Iteration 164/1000 | Loss: 0.00031045
Iteration 165/1000 | Loss: 0.00034252
Iteration 166/1000 | Loss: 0.00008789
Iteration 167/1000 | Loss: 0.00010919
Iteration 168/1000 | Loss: 0.00026737
Iteration 169/1000 | Loss: 0.00016472
Iteration 170/1000 | Loss: 0.00051215
Iteration 171/1000 | Loss: 0.00020733
Iteration 172/1000 | Loss: 0.00024505
Iteration 173/1000 | Loss: 0.00021662
Iteration 174/1000 | Loss: 0.00013843
Iteration 175/1000 | Loss: 0.00010594
Iteration 176/1000 | Loss: 0.00007698
Iteration 177/1000 | Loss: 0.00025096
Iteration 178/1000 | Loss: 0.00052302
Iteration 179/1000 | Loss: 0.00021021
Iteration 180/1000 | Loss: 0.00018601
Iteration 181/1000 | Loss: 0.00064890
Iteration 182/1000 | Loss: 0.00017819
Iteration 183/1000 | Loss: 0.00063900
Iteration 184/1000 | Loss: 0.00035361
Iteration 185/1000 | Loss: 0.00024727
Iteration 186/1000 | Loss: 0.00019896
Iteration 187/1000 | Loss: 0.00022046
Iteration 188/1000 | Loss: 0.00015502
Iteration 189/1000 | Loss: 0.00010628
Iteration 190/1000 | Loss: 0.00022213
Iteration 191/1000 | Loss: 0.00038088
Iteration 192/1000 | Loss: 0.00033673
Iteration 193/1000 | Loss: 0.00036771
Iteration 194/1000 | Loss: 0.00040967
Iteration 195/1000 | Loss: 0.00041002
Iteration 196/1000 | Loss: 0.00073977
Iteration 197/1000 | Loss: 0.00010108
Iteration 198/1000 | Loss: 0.00011836
Iteration 199/1000 | Loss: 0.00003664
Iteration 200/1000 | Loss: 0.00019770
Iteration 201/1000 | Loss: 0.00003184
Iteration 202/1000 | Loss: 0.00002952
Iteration 203/1000 | Loss: 0.00010803
Iteration 204/1000 | Loss: 0.00002801
Iteration 205/1000 | Loss: 0.00026656
Iteration 206/1000 | Loss: 0.00008303
Iteration 207/1000 | Loss: 0.00013680
Iteration 208/1000 | Loss: 0.00006743
Iteration 209/1000 | Loss: 0.00069751
Iteration 210/1000 | Loss: 0.00021514
Iteration 211/1000 | Loss: 0.00003372
Iteration 212/1000 | Loss: 0.00006749
Iteration 213/1000 | Loss: 0.00004796
Iteration 214/1000 | Loss: 0.00002692
Iteration 215/1000 | Loss: 0.00025844
Iteration 216/1000 | Loss: 0.00003257
Iteration 217/1000 | Loss: 0.00002501
Iteration 218/1000 | Loss: 0.00102243
Iteration 219/1000 | Loss: 0.00012414
Iteration 220/1000 | Loss: 0.00022291
Iteration 221/1000 | Loss: 0.00002290
Iteration 222/1000 | Loss: 0.00010897
Iteration 223/1000 | Loss: 0.00002130
Iteration 224/1000 | Loss: 0.00106702
Iteration 225/1000 | Loss: 0.00016418
Iteration 226/1000 | Loss: 0.00002386
Iteration 227/1000 | Loss: 0.00011836
Iteration 228/1000 | Loss: 0.00017830
Iteration 229/1000 | Loss: 0.00091569
Iteration 230/1000 | Loss: 0.00006329
Iteration 231/1000 | Loss: 0.00002156
Iteration 232/1000 | Loss: 0.00011500
Iteration 233/1000 | Loss: 0.00001972
Iteration 234/1000 | Loss: 0.00088210
Iteration 235/1000 | Loss: 0.00010381
Iteration 236/1000 | Loss: 0.00002814
Iteration 237/1000 | Loss: 0.00005018
Iteration 238/1000 | Loss: 0.00001935
Iteration 239/1000 | Loss: 0.00105652
Iteration 240/1000 | Loss: 0.00014763
Iteration 241/1000 | Loss: 0.00002177
Iteration 242/1000 | Loss: 0.00005622
Iteration 243/1000 | Loss: 0.00001912
Iteration 244/1000 | Loss: 0.00096623
Iteration 245/1000 | Loss: 0.00088222
Iteration 246/1000 | Loss: 0.00092168
Iteration 247/1000 | Loss: 0.00063505
Iteration 248/1000 | Loss: 0.00073105
Iteration 249/1000 | Loss: 0.00060708
Iteration 250/1000 | Loss: 0.00064229
Iteration 251/1000 | Loss: 0.00059408
Iteration 252/1000 | Loss: 0.00059372
Iteration 253/1000 | Loss: 0.00002041
Iteration 254/1000 | Loss: 0.00001880
Iteration 255/1000 | Loss: 0.00010488
Iteration 256/1000 | Loss: 0.00001695
Iteration 257/1000 | Loss: 0.00001617
Iteration 258/1000 | Loss: 0.00001561
Iteration 259/1000 | Loss: 0.00019461
Iteration 260/1000 | Loss: 0.00001744
Iteration 261/1000 | Loss: 0.00001533
Iteration 262/1000 | Loss: 0.00001524
Iteration 263/1000 | Loss: 0.00001514
Iteration 264/1000 | Loss: 0.00001513
Iteration 265/1000 | Loss: 0.00001513
Iteration 266/1000 | Loss: 0.00001513
Iteration 267/1000 | Loss: 0.00001512
Iteration 268/1000 | Loss: 0.00001512
Iteration 269/1000 | Loss: 0.00001511
Iteration 270/1000 | Loss: 0.00001510
Iteration 271/1000 | Loss: 0.00001510
Iteration 272/1000 | Loss: 0.00001509
Iteration 273/1000 | Loss: 0.00001509
Iteration 274/1000 | Loss: 0.00001509
Iteration 275/1000 | Loss: 0.00001508
Iteration 276/1000 | Loss: 0.00001508
Iteration 277/1000 | Loss: 0.00001508
Iteration 278/1000 | Loss: 0.00001507
Iteration 279/1000 | Loss: 0.00001507
Iteration 280/1000 | Loss: 0.00001507
Iteration 281/1000 | Loss: 0.00001506
Iteration 282/1000 | Loss: 0.00001506
Iteration 283/1000 | Loss: 0.00001504
Iteration 284/1000 | Loss: 0.00001504
Iteration 285/1000 | Loss: 0.00001494
Iteration 286/1000 | Loss: 0.00001493
Iteration 287/1000 | Loss: 0.00001493
Iteration 288/1000 | Loss: 0.00001492
Iteration 289/1000 | Loss: 0.00001491
Iteration 290/1000 | Loss: 0.00001491
Iteration 291/1000 | Loss: 0.00001491
Iteration 292/1000 | Loss: 0.00001491
Iteration 293/1000 | Loss: 0.00001491
Iteration 294/1000 | Loss: 0.00001491
Iteration 295/1000 | Loss: 0.00001491
Iteration 296/1000 | Loss: 0.00001491
Iteration 297/1000 | Loss: 0.00001491
Iteration 298/1000 | Loss: 0.00001491
Iteration 299/1000 | Loss: 0.00001490
Iteration 300/1000 | Loss: 0.00001490
Iteration 301/1000 | Loss: 0.00001490
Iteration 302/1000 | Loss: 0.00001490
Iteration 303/1000 | Loss: 0.00001489
Iteration 304/1000 | Loss: 0.00001489
Iteration 305/1000 | Loss: 0.00001488
Iteration 306/1000 | Loss: 0.00001488
Iteration 307/1000 | Loss: 0.00001488
Iteration 308/1000 | Loss: 0.00001487
Iteration 309/1000 | Loss: 0.00001487
Iteration 310/1000 | Loss: 0.00001487
Iteration 311/1000 | Loss: 0.00001487
Iteration 312/1000 | Loss: 0.00001487
Iteration 313/1000 | Loss: 0.00001487
Iteration 314/1000 | Loss: 0.00001487
Iteration 315/1000 | Loss: 0.00001487
Iteration 316/1000 | Loss: 0.00001487
Iteration 317/1000 | Loss: 0.00001487
Iteration 318/1000 | Loss: 0.00001486
Iteration 319/1000 | Loss: 0.00001486
Iteration 320/1000 | Loss: 0.00001486
Iteration 321/1000 | Loss: 0.00001486
Iteration 322/1000 | Loss: 0.00001486
Iteration 323/1000 | Loss: 0.00001486
Iteration 324/1000 | Loss: 0.00001486
Iteration 325/1000 | Loss: 0.00001486
Iteration 326/1000 | Loss: 0.00001486
Iteration 327/1000 | Loss: 0.00001486
Iteration 328/1000 | Loss: 0.00001486
Iteration 329/1000 | Loss: 0.00001486
Iteration 330/1000 | Loss: 0.00001486
Iteration 331/1000 | Loss: 0.00001485
Iteration 332/1000 | Loss: 0.00001485
Iteration 333/1000 | Loss: 0.00001485
Iteration 334/1000 | Loss: 0.00001485
Iteration 335/1000 | Loss: 0.00001485
Iteration 336/1000 | Loss: 0.00001485
Iteration 337/1000 | Loss: 0.00001485
Iteration 338/1000 | Loss: 0.00001484
Iteration 339/1000 | Loss: 0.00001484
Iteration 340/1000 | Loss: 0.00001484
Iteration 341/1000 | Loss: 0.00001484
Iteration 342/1000 | Loss: 0.00001484
Iteration 343/1000 | Loss: 0.00001484
Iteration 344/1000 | Loss: 0.00001484
Iteration 345/1000 | Loss: 0.00001483
Iteration 346/1000 | Loss: 0.00001483
Iteration 347/1000 | Loss: 0.00001483
Iteration 348/1000 | Loss: 0.00001483
Iteration 349/1000 | Loss: 0.00001482
Iteration 350/1000 | Loss: 0.00001481
Iteration 351/1000 | Loss: 0.00001480
Iteration 352/1000 | Loss: 0.00001480
Iteration 353/1000 | Loss: 0.00001480
Iteration 354/1000 | Loss: 0.00001480
Iteration 355/1000 | Loss: 0.00001480
Iteration 356/1000 | Loss: 0.00001479
Iteration 357/1000 | Loss: 0.00001479
Iteration 358/1000 | Loss: 0.00001479
Iteration 359/1000 | Loss: 0.00001479
Iteration 360/1000 | Loss: 0.00001479
Iteration 361/1000 | Loss: 0.00001479
Iteration 362/1000 | Loss: 0.00001478
Iteration 363/1000 | Loss: 0.00001478
Iteration 364/1000 | Loss: 0.00001478
Iteration 365/1000 | Loss: 0.00001478
Iteration 366/1000 | Loss: 0.00001478
Iteration 367/1000 | Loss: 0.00001478
Iteration 368/1000 | Loss: 0.00001478
Iteration 369/1000 | Loss: 0.00001478
Iteration 370/1000 | Loss: 0.00001478
Iteration 371/1000 | Loss: 0.00001478
Iteration 372/1000 | Loss: 0.00001478
Iteration 373/1000 | Loss: 0.00001477
Iteration 374/1000 | Loss: 0.00001477
Iteration 375/1000 | Loss: 0.00001477
Iteration 376/1000 | Loss: 0.00001477
Iteration 377/1000 | Loss: 0.00001477
Iteration 378/1000 | Loss: 0.00001477
Iteration 379/1000 | Loss: 0.00001477
Iteration 380/1000 | Loss: 0.00001476
Iteration 381/1000 | Loss: 0.00001476
Iteration 382/1000 | Loss: 0.00001476
Iteration 383/1000 | Loss: 0.00001476
Iteration 384/1000 | Loss: 0.00001475
Iteration 385/1000 | Loss: 0.00001475
Iteration 386/1000 | Loss: 0.00001475
Iteration 387/1000 | Loss: 0.00001475
Iteration 388/1000 | Loss: 0.00001475
Iteration 389/1000 | Loss: 0.00001474
Iteration 390/1000 | Loss: 0.00001474
Iteration 391/1000 | Loss: 0.00001474
Iteration 392/1000 | Loss: 0.00001474
Iteration 393/1000 | Loss: 0.00001474
Iteration 394/1000 | Loss: 0.00001474
Iteration 395/1000 | Loss: 0.00001474
Iteration 396/1000 | Loss: 0.00001473
Iteration 397/1000 | Loss: 0.00001473
Iteration 398/1000 | Loss: 0.00001473
Iteration 399/1000 | Loss: 0.00001473
Iteration 400/1000 | Loss: 0.00001473
Iteration 401/1000 | Loss: 0.00001473
Iteration 402/1000 | Loss: 0.00001473
Iteration 403/1000 | Loss: 0.00001473
Iteration 404/1000 | Loss: 0.00001473
Iteration 405/1000 | Loss: 0.00001473
Iteration 406/1000 | Loss: 0.00001473
Iteration 407/1000 | Loss: 0.00001473
Iteration 408/1000 | Loss: 0.00001473
Iteration 409/1000 | Loss: 0.00001473
Iteration 410/1000 | Loss: 0.00001473
Iteration 411/1000 | Loss: 0.00001473
Iteration 412/1000 | Loss: 0.00001473
Iteration 413/1000 | Loss: 0.00001472
Iteration 414/1000 | Loss: 0.00001472
Iteration 415/1000 | Loss: 0.00001472
Iteration 416/1000 | Loss: 0.00001472
Iteration 417/1000 | Loss: 0.00001472
Iteration 418/1000 | Loss: 0.00001472
Iteration 419/1000 | Loss: 0.00001472
Iteration 420/1000 | Loss: 0.00001472
Iteration 421/1000 | Loss: 0.00001472
Iteration 422/1000 | Loss: 0.00001472
Iteration 423/1000 | Loss: 0.00001472
Iteration 424/1000 | Loss: 0.00001472
Iteration 425/1000 | Loss: 0.00001472
Iteration 426/1000 | Loss: 0.00001472
Iteration 427/1000 | Loss: 0.00001472
Iteration 428/1000 | Loss: 0.00001472
Iteration 429/1000 | Loss: 0.00001472
Iteration 430/1000 | Loss: 0.00001472
Iteration 431/1000 | Loss: 0.00001472
Iteration 432/1000 | Loss: 0.00001472
Iteration 433/1000 | Loss: 0.00001472
Iteration 434/1000 | Loss: 0.00001472
Iteration 435/1000 | Loss: 0.00001472
Iteration 436/1000 | Loss: 0.00001472
Iteration 437/1000 | Loss: 0.00001472
Iteration 438/1000 | Loss: 0.00001472
Iteration 439/1000 | Loss: 0.00001472
Iteration 440/1000 | Loss: 0.00001472
Iteration 441/1000 | Loss: 0.00001472
Iteration 442/1000 | Loss: 0.00001472
Iteration 443/1000 | Loss: 0.00001472
Iteration 444/1000 | Loss: 0.00001472
Iteration 445/1000 | Loss: 0.00001472
Iteration 446/1000 | Loss: 0.00001472
Iteration 447/1000 | Loss: 0.00001472
Iteration 448/1000 | Loss: 0.00001472
Iteration 449/1000 | Loss: 0.00001472
Iteration 450/1000 | Loss: 0.00001472
Iteration 451/1000 | Loss: 0.00001472
Iteration 452/1000 | Loss: 0.00001472
Iteration 453/1000 | Loss: 0.00001472
Iteration 454/1000 | Loss: 0.00001472
Iteration 455/1000 | Loss: 0.00001472
Iteration 456/1000 | Loss: 0.00001472
Iteration 457/1000 | Loss: 0.00001472
Iteration 458/1000 | Loss: 0.00001472
Iteration 459/1000 | Loss: 0.00001472
Iteration 460/1000 | Loss: 0.00001472
Iteration 461/1000 | Loss: 0.00001472
Iteration 462/1000 | Loss: 0.00001472
Iteration 463/1000 | Loss: 0.00001472
Iteration 464/1000 | Loss: 0.00001472
Iteration 465/1000 | Loss: 0.00001472
Iteration 466/1000 | Loss: 0.00001472
Iteration 467/1000 | Loss: 0.00001472
Iteration 468/1000 | Loss: 0.00001472
Iteration 469/1000 | Loss: 0.00001472
Iteration 470/1000 | Loss: 0.00001472
Iteration 471/1000 | Loss: 0.00001472
Iteration 472/1000 | Loss: 0.00001472
Iteration 473/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 473. Stopping optimization.
Last 5 losses: [1.471673840569565e-05, 1.471673840569565e-05, 1.471673840569565e-05, 1.471673840569565e-05, 1.471673840569565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.471673840569565e-05

Optimization complete. Final v2v error: 3.263728141784668 mm

Highest mean error: 4.59306526184082 mm for frame 51

Lowest mean error: 2.9215104579925537 mm for frame 12

Saving results

Total time: 420.53565740585327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853934
Iteration 2/25 | Loss: 0.00206197
Iteration 3/25 | Loss: 0.00122217
Iteration 4/25 | Loss: 0.00102547
Iteration 5/25 | Loss: 0.00099938
Iteration 6/25 | Loss: 0.00099511
Iteration 7/25 | Loss: 0.00099110
Iteration 8/25 | Loss: 0.00098856
Iteration 9/25 | Loss: 0.00098694
Iteration 10/25 | Loss: 0.00098554
Iteration 11/25 | Loss: 0.00098492
Iteration 12/25 | Loss: 0.00098453
Iteration 13/25 | Loss: 0.00098451
Iteration 14/25 | Loss: 0.00098451
Iteration 15/25 | Loss: 0.00098451
Iteration 16/25 | Loss: 0.00098451
Iteration 17/25 | Loss: 0.00098451
Iteration 18/25 | Loss: 0.00098451
Iteration 19/25 | Loss: 0.00098451
Iteration 20/25 | Loss: 0.00098451
Iteration 21/25 | Loss: 0.00098450
Iteration 22/25 | Loss: 0.00098450
Iteration 23/25 | Loss: 0.00098450
Iteration 24/25 | Loss: 0.00098450
Iteration 25/25 | Loss: 0.00098450

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34289205
Iteration 2/25 | Loss: 0.00068882
Iteration 3/25 | Loss: 0.00068882
Iteration 4/25 | Loss: 0.00068882
Iteration 5/25 | Loss: 0.00068882
Iteration 6/25 | Loss: 0.00068881
Iteration 7/25 | Loss: 0.00068881
Iteration 8/25 | Loss: 0.00068881
Iteration 9/25 | Loss: 0.00068881
Iteration 10/25 | Loss: 0.00068881
Iteration 11/25 | Loss: 0.00068881
Iteration 12/25 | Loss: 0.00068881
Iteration 13/25 | Loss: 0.00068881
Iteration 14/25 | Loss: 0.00068881
Iteration 15/25 | Loss: 0.00068881
Iteration 16/25 | Loss: 0.00068881
Iteration 17/25 | Loss: 0.00068881
Iteration 18/25 | Loss: 0.00068881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006888136267662048, 0.0006888136267662048, 0.0006888136267662048, 0.0006888136267662048, 0.0006888136267662048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006888136267662048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068881
Iteration 2/1000 | Loss: 0.00004003
Iteration 3/1000 | Loss: 0.00002661
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002396
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002258
Iteration 9/1000 | Loss: 0.00002233
Iteration 10/1000 | Loss: 0.00002225
Iteration 11/1000 | Loss: 0.00002225
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002205
Iteration 17/1000 | Loss: 0.00002204
Iteration 18/1000 | Loss: 0.00002203
Iteration 19/1000 | Loss: 0.00002203
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002193
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002192
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002191
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002188
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002186
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002186
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002184
Iteration 58/1000 | Loss: 0.00002184
Iteration 59/1000 | Loss: 0.00002184
Iteration 60/1000 | Loss: 0.00002184
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002184
Iteration 63/1000 | Loss: 0.00002184
Iteration 64/1000 | Loss: 0.00002184
Iteration 65/1000 | Loss: 0.00002183
Iteration 66/1000 | Loss: 0.00002183
Iteration 67/1000 | Loss: 0.00002183
Iteration 68/1000 | Loss: 0.00002183
Iteration 69/1000 | Loss: 0.00002183
Iteration 70/1000 | Loss: 0.00002183
Iteration 71/1000 | Loss: 0.00002183
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002182
Iteration 78/1000 | Loss: 0.00002182
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002182
Iteration 85/1000 | Loss: 0.00002182
Iteration 86/1000 | Loss: 0.00002182
Iteration 87/1000 | Loss: 0.00002182
Iteration 88/1000 | Loss: 0.00002182
Iteration 89/1000 | Loss: 0.00002182
Iteration 90/1000 | Loss: 0.00002182
Iteration 91/1000 | Loss: 0.00002182
Iteration 92/1000 | Loss: 0.00002182
Iteration 93/1000 | Loss: 0.00002182
Iteration 94/1000 | Loss: 0.00002182
Iteration 95/1000 | Loss: 0.00002182
Iteration 96/1000 | Loss: 0.00002182
Iteration 97/1000 | Loss: 0.00002182
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002182
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.1820351321366616e-05, 2.1820351321366616e-05, 2.1820351321366616e-05, 2.1820351321366616e-05, 2.1820351321366616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1820351321366616e-05

Optimization complete. Final v2v error: 3.81449031829834 mm

Highest mean error: 3.974802255630493 mm for frame 169

Lowest mean error: 3.676978349685669 mm for frame 68

Saving results

Total time: 49.986189126968384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00670725
Iteration 2/25 | Loss: 0.00099981
Iteration 3/25 | Loss: 0.00088200
Iteration 4/25 | Loss: 0.00085354
Iteration 5/25 | Loss: 0.00084546
Iteration 6/25 | Loss: 0.00084388
Iteration 7/25 | Loss: 0.00084351
Iteration 8/25 | Loss: 0.00084351
Iteration 9/25 | Loss: 0.00084351
Iteration 10/25 | Loss: 0.00084351
Iteration 11/25 | Loss: 0.00084351
Iteration 12/25 | Loss: 0.00084351
Iteration 13/25 | Loss: 0.00084351
Iteration 14/25 | Loss: 0.00084351
Iteration 15/25 | Loss: 0.00084351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008435091585852206, 0.0008435091585852206, 0.0008435091585852206, 0.0008435091585852206, 0.0008435091585852206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008435091585852206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.43988276
Iteration 2/25 | Loss: 0.00065265
Iteration 3/25 | Loss: 0.00065265
Iteration 4/25 | Loss: 0.00065265
Iteration 5/25 | Loss: 0.00065265
Iteration 6/25 | Loss: 0.00065265
Iteration 7/25 | Loss: 0.00065265
Iteration 8/25 | Loss: 0.00065265
Iteration 9/25 | Loss: 0.00065265
Iteration 10/25 | Loss: 0.00065265
Iteration 11/25 | Loss: 0.00065265
Iteration 12/25 | Loss: 0.00065265
Iteration 13/25 | Loss: 0.00065265
Iteration 14/25 | Loss: 0.00065265
Iteration 15/25 | Loss: 0.00065265
Iteration 16/25 | Loss: 0.00065265
Iteration 17/25 | Loss: 0.00065265
Iteration 18/25 | Loss: 0.00065265
Iteration 19/25 | Loss: 0.00065265
Iteration 20/25 | Loss: 0.00065265
Iteration 21/25 | Loss: 0.00065265
Iteration 22/25 | Loss: 0.00065265
Iteration 23/25 | Loss: 0.00065265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006526451325044036, 0.0006526451325044036, 0.0006526451325044036, 0.0006526451325044036, 0.0006526451325044036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006526451325044036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065265
Iteration 2/1000 | Loss: 0.00002586
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001632
Iteration 6/1000 | Loss: 0.00001591
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001540
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001480
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001472
Iteration 23/1000 | Loss: 0.00001472
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001468
Iteration 29/1000 | Loss: 0.00001468
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001464
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001461
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001456
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00001451
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001445
Iteration 101/1000 | Loss: 0.00001445
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001445
Iteration 105/1000 | Loss: 0.00001445
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001445
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001444
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001444
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00001444
Iteration 143/1000 | Loss: 0.00001444
Iteration 144/1000 | Loss: 0.00001444
Iteration 145/1000 | Loss: 0.00001444
Iteration 146/1000 | Loss: 0.00001444
Iteration 147/1000 | Loss: 0.00001444
Iteration 148/1000 | Loss: 0.00001444
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001444
Iteration 151/1000 | Loss: 0.00001444
Iteration 152/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.4437347090279218e-05, 1.4437347090279218e-05, 1.4437347090279218e-05, 1.4437347090279218e-05, 1.4437347090279218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4437347090279218e-05

Optimization complete. Final v2v error: 3.2120401859283447 mm

Highest mean error: 3.4299514293670654 mm for frame 21

Lowest mean error: 3.0557544231414795 mm for frame 142

Saving results

Total time: 35.24424719810486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852967
Iteration 2/25 | Loss: 0.00100595
Iteration 3/25 | Loss: 0.00086976
Iteration 4/25 | Loss: 0.00084179
Iteration 5/25 | Loss: 0.00083805
Iteration 6/25 | Loss: 0.00083751
Iteration 7/25 | Loss: 0.00083751
Iteration 8/25 | Loss: 0.00083751
Iteration 9/25 | Loss: 0.00083751
Iteration 10/25 | Loss: 0.00083751
Iteration 11/25 | Loss: 0.00083751
Iteration 12/25 | Loss: 0.00083751
Iteration 13/25 | Loss: 0.00083751
Iteration 14/25 | Loss: 0.00083744
Iteration 15/25 | Loss: 0.00083744
Iteration 16/25 | Loss: 0.00083744
Iteration 17/25 | Loss: 0.00083744
Iteration 18/25 | Loss: 0.00083744
Iteration 19/25 | Loss: 0.00083744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008374445606023073, 0.0008374445606023073, 0.0008374445606023073, 0.0008374445606023073, 0.0008374445606023073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008374445606023073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45361638
Iteration 2/25 | Loss: 0.00048004
Iteration 3/25 | Loss: 0.00047996
Iteration 4/25 | Loss: 0.00047996
Iteration 5/25 | Loss: 0.00047996
Iteration 6/25 | Loss: 0.00047996
Iteration 7/25 | Loss: 0.00047996
Iteration 8/25 | Loss: 0.00047996
Iteration 9/25 | Loss: 0.00047996
Iteration 10/25 | Loss: 0.00047996
Iteration 11/25 | Loss: 0.00047996
Iteration 12/25 | Loss: 0.00047996
Iteration 13/25 | Loss: 0.00047996
Iteration 14/25 | Loss: 0.00047996
Iteration 15/25 | Loss: 0.00047996
Iteration 16/25 | Loss: 0.00047996
Iteration 17/25 | Loss: 0.00047996
Iteration 18/25 | Loss: 0.00047996
Iteration 19/25 | Loss: 0.00047996
Iteration 20/25 | Loss: 0.00047996
Iteration 21/25 | Loss: 0.00047996
Iteration 22/25 | Loss: 0.00047996
Iteration 23/25 | Loss: 0.00047996
Iteration 24/25 | Loss: 0.00047996
Iteration 25/25 | Loss: 0.00047996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047996
Iteration 2/1000 | Loss: 0.00003107
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001750
Iteration 5/1000 | Loss: 0.00001630
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001416
Iteration 10/1000 | Loss: 0.00001415
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001399
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001394
Iteration 17/1000 | Loss: 0.00001387
Iteration 18/1000 | Loss: 0.00001383
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001368
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001354
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001340
Iteration 38/1000 | Loss: 0.00001340
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001339
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001338
Iteration 46/1000 | Loss: 0.00001338
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001337
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001336
Iteration 54/1000 | Loss: 0.00001336
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001334
Iteration 61/1000 | Loss: 0.00001334
Iteration 62/1000 | Loss: 0.00001334
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001334
Iteration 79/1000 | Loss: 0.00001334
Iteration 80/1000 | Loss: 0.00001334
Iteration 81/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.3338177268451545e-05, 1.3338177268451545e-05, 1.3338177268451545e-05, 1.3338177268451545e-05, 1.3338177268451545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3338177268451545e-05

Optimization complete. Final v2v error: 3.1752512454986572 mm

Highest mean error: 3.512530565261841 mm for frame 124

Lowest mean error: 2.986940860748291 mm for frame 99

Saving results

Total time: 33.495983600616455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483887
Iteration 2/25 | Loss: 0.00103461
Iteration 3/25 | Loss: 0.00091396
Iteration 4/25 | Loss: 0.00087444
Iteration 5/25 | Loss: 0.00086343
Iteration 6/25 | Loss: 0.00086125
Iteration 7/25 | Loss: 0.00086056
Iteration 8/25 | Loss: 0.00086056
Iteration 9/25 | Loss: 0.00086056
Iteration 10/25 | Loss: 0.00086056
Iteration 11/25 | Loss: 0.00086056
Iteration 12/25 | Loss: 0.00086056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008605620823800564, 0.0008605620823800564, 0.0008605620823800564, 0.0008605620823800564, 0.0008605620823800564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008605620823800564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.64848804
Iteration 2/25 | Loss: 0.00056977
Iteration 3/25 | Loss: 0.00056975
Iteration 4/25 | Loss: 0.00056975
Iteration 5/25 | Loss: 0.00056975
Iteration 6/25 | Loss: 0.00056975
Iteration 7/25 | Loss: 0.00056975
Iteration 8/25 | Loss: 0.00056975
Iteration 9/25 | Loss: 0.00056975
Iteration 10/25 | Loss: 0.00056975
Iteration 11/25 | Loss: 0.00056975
Iteration 12/25 | Loss: 0.00056975
Iteration 13/25 | Loss: 0.00056975
Iteration 14/25 | Loss: 0.00056975
Iteration 15/25 | Loss: 0.00056975
Iteration 16/25 | Loss: 0.00056975
Iteration 17/25 | Loss: 0.00056975
Iteration 18/25 | Loss: 0.00056975
Iteration 19/25 | Loss: 0.00056975
Iteration 20/25 | Loss: 0.00056975
Iteration 21/25 | Loss: 0.00056975
Iteration 22/25 | Loss: 0.00056975
Iteration 23/25 | Loss: 0.00056975
Iteration 24/25 | Loss: 0.00056975
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005697506130672991, 0.0005697506130672991, 0.0005697506130672991, 0.0005697506130672991, 0.0005697506130672991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005697506130672991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056975
Iteration 2/1000 | Loss: 0.00003504
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002452
Iteration 6/1000 | Loss: 0.00002379
Iteration 7/1000 | Loss: 0.00002323
Iteration 8/1000 | Loss: 0.00002283
Iteration 9/1000 | Loss: 0.00002257
Iteration 10/1000 | Loss: 0.00002233
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002206
Iteration 13/1000 | Loss: 0.00002199
Iteration 14/1000 | Loss: 0.00002195
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002194
Iteration 17/1000 | Loss: 0.00002194
Iteration 18/1000 | Loss: 0.00002194
Iteration 19/1000 | Loss: 0.00002193
Iteration 20/1000 | Loss: 0.00002193
Iteration 21/1000 | Loss: 0.00002192
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002191
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002185
Iteration 46/1000 | Loss: 0.00002185
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002185
Iteration 49/1000 | Loss: 0.00002185
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00002184
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002184
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002180
Iteration 62/1000 | Loss: 0.00002180
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002179
Iteration 65/1000 | Loss: 0.00002179
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002176
Iteration 69/1000 | Loss: 0.00002176
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002173
Iteration 75/1000 | Loss: 0.00002172
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002171
Iteration 78/1000 | Loss: 0.00002171
Iteration 79/1000 | Loss: 0.00002171
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002169
Iteration 83/1000 | Loss: 0.00002169
Iteration 84/1000 | Loss: 0.00002169
Iteration 85/1000 | Loss: 0.00002169
Iteration 86/1000 | Loss: 0.00002168
Iteration 87/1000 | Loss: 0.00002168
Iteration 88/1000 | Loss: 0.00002168
Iteration 89/1000 | Loss: 0.00002168
Iteration 90/1000 | Loss: 0.00002168
Iteration 91/1000 | Loss: 0.00002168
Iteration 92/1000 | Loss: 0.00002167
Iteration 93/1000 | Loss: 0.00002167
Iteration 94/1000 | Loss: 0.00002167
Iteration 95/1000 | Loss: 0.00002167
Iteration 96/1000 | Loss: 0.00002167
Iteration 97/1000 | Loss: 0.00002166
Iteration 98/1000 | Loss: 0.00002166
Iteration 99/1000 | Loss: 0.00002166
Iteration 100/1000 | Loss: 0.00002165
Iteration 101/1000 | Loss: 0.00002165
Iteration 102/1000 | Loss: 0.00002165
Iteration 103/1000 | Loss: 0.00002165
Iteration 104/1000 | Loss: 0.00002165
Iteration 105/1000 | Loss: 0.00002165
Iteration 106/1000 | Loss: 0.00002165
Iteration 107/1000 | Loss: 0.00002165
Iteration 108/1000 | Loss: 0.00002165
Iteration 109/1000 | Loss: 0.00002164
Iteration 110/1000 | Loss: 0.00002164
Iteration 111/1000 | Loss: 0.00002164
Iteration 112/1000 | Loss: 0.00002164
Iteration 113/1000 | Loss: 0.00002164
Iteration 114/1000 | Loss: 0.00002164
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00002164
Iteration 118/1000 | Loss: 0.00002163
Iteration 119/1000 | Loss: 0.00002163
Iteration 120/1000 | Loss: 0.00002163
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002162
Iteration 123/1000 | Loss: 0.00002162
Iteration 124/1000 | Loss: 0.00002162
Iteration 125/1000 | Loss: 0.00002162
Iteration 126/1000 | Loss: 0.00002162
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002161
Iteration 129/1000 | Loss: 0.00002161
Iteration 130/1000 | Loss: 0.00002161
Iteration 131/1000 | Loss: 0.00002161
Iteration 132/1000 | Loss: 0.00002161
Iteration 133/1000 | Loss: 0.00002161
Iteration 134/1000 | Loss: 0.00002161
Iteration 135/1000 | Loss: 0.00002161
Iteration 136/1000 | Loss: 0.00002161
Iteration 137/1000 | Loss: 0.00002161
Iteration 138/1000 | Loss: 0.00002161
Iteration 139/1000 | Loss: 0.00002161
Iteration 140/1000 | Loss: 0.00002161
Iteration 141/1000 | Loss: 0.00002160
Iteration 142/1000 | Loss: 0.00002160
Iteration 143/1000 | Loss: 0.00002160
Iteration 144/1000 | Loss: 0.00002160
Iteration 145/1000 | Loss: 0.00002160
Iteration 146/1000 | Loss: 0.00002160
Iteration 147/1000 | Loss: 0.00002160
Iteration 148/1000 | Loss: 0.00002160
Iteration 149/1000 | Loss: 0.00002160
Iteration 150/1000 | Loss: 0.00002160
Iteration 151/1000 | Loss: 0.00002160
Iteration 152/1000 | Loss: 0.00002160
Iteration 153/1000 | Loss: 0.00002160
Iteration 154/1000 | Loss: 0.00002160
Iteration 155/1000 | Loss: 0.00002160
Iteration 156/1000 | Loss: 0.00002159
Iteration 157/1000 | Loss: 0.00002159
Iteration 158/1000 | Loss: 0.00002159
Iteration 159/1000 | Loss: 0.00002159
Iteration 160/1000 | Loss: 0.00002159
Iteration 161/1000 | Loss: 0.00002159
Iteration 162/1000 | Loss: 0.00002159
Iteration 163/1000 | Loss: 0.00002159
Iteration 164/1000 | Loss: 0.00002159
Iteration 165/1000 | Loss: 0.00002159
Iteration 166/1000 | Loss: 0.00002159
Iteration 167/1000 | Loss: 0.00002159
Iteration 168/1000 | Loss: 0.00002159
Iteration 169/1000 | Loss: 0.00002159
Iteration 170/1000 | Loss: 0.00002159
Iteration 171/1000 | Loss: 0.00002159
Iteration 172/1000 | Loss: 0.00002159
Iteration 173/1000 | Loss: 0.00002159
Iteration 174/1000 | Loss: 0.00002159
Iteration 175/1000 | Loss: 0.00002159
Iteration 176/1000 | Loss: 0.00002159
Iteration 177/1000 | Loss: 0.00002159
Iteration 178/1000 | Loss: 0.00002159
Iteration 179/1000 | Loss: 0.00002159
Iteration 180/1000 | Loss: 0.00002159
Iteration 181/1000 | Loss: 0.00002159
Iteration 182/1000 | Loss: 0.00002159
Iteration 183/1000 | Loss: 0.00002159
Iteration 184/1000 | Loss: 0.00002159
Iteration 185/1000 | Loss: 0.00002159
Iteration 186/1000 | Loss: 0.00002159
Iteration 187/1000 | Loss: 0.00002159
Iteration 188/1000 | Loss: 0.00002159
Iteration 189/1000 | Loss: 0.00002159
Iteration 190/1000 | Loss: 0.00002159
Iteration 191/1000 | Loss: 0.00002159
Iteration 192/1000 | Loss: 0.00002159
Iteration 193/1000 | Loss: 0.00002159
Iteration 194/1000 | Loss: 0.00002159
Iteration 195/1000 | Loss: 0.00002159
Iteration 196/1000 | Loss: 0.00002159
Iteration 197/1000 | Loss: 0.00002159
Iteration 198/1000 | Loss: 0.00002159
Iteration 199/1000 | Loss: 0.00002159
Iteration 200/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.1590019969153218e-05, 2.1590019969153218e-05, 2.1590019969153218e-05, 2.1590019969153218e-05, 2.1590019969153218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1590019969153218e-05

Optimization complete. Final v2v error: 3.8806376457214355 mm

Highest mean error: 4.31536340713501 mm for frame 20

Lowest mean error: 3.3599133491516113 mm for frame 45

Saving results

Total time: 39.573370933532715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844625
Iteration 2/25 | Loss: 0.00127444
Iteration 3/25 | Loss: 0.00101366
Iteration 4/25 | Loss: 0.00097514
Iteration 5/25 | Loss: 0.00096895
Iteration 6/25 | Loss: 0.00096736
Iteration 7/25 | Loss: 0.00096705
Iteration 8/25 | Loss: 0.00096705
Iteration 9/25 | Loss: 0.00096705
Iteration 10/25 | Loss: 0.00096705
Iteration 11/25 | Loss: 0.00096705
Iteration 12/25 | Loss: 0.00096705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009670518920756876, 0.0009670518920756876, 0.0009670518920756876, 0.0009670518920756876, 0.0009670518920756876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009670518920756876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69550824
Iteration 2/25 | Loss: 0.00063608
Iteration 3/25 | Loss: 0.00063602
Iteration 4/25 | Loss: 0.00063602
Iteration 5/25 | Loss: 0.00063602
Iteration 6/25 | Loss: 0.00063602
Iteration 7/25 | Loss: 0.00063602
Iteration 8/25 | Loss: 0.00063602
Iteration 9/25 | Loss: 0.00063602
Iteration 10/25 | Loss: 0.00063602
Iteration 11/25 | Loss: 0.00063602
Iteration 12/25 | Loss: 0.00063602
Iteration 13/25 | Loss: 0.00063602
Iteration 14/25 | Loss: 0.00063602
Iteration 15/25 | Loss: 0.00063602
Iteration 16/25 | Loss: 0.00063602
Iteration 17/25 | Loss: 0.00063602
Iteration 18/25 | Loss: 0.00063602
Iteration 19/25 | Loss: 0.00063602
Iteration 20/25 | Loss: 0.00063602
Iteration 21/25 | Loss: 0.00063602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006360173574648798, 0.0006360173574648798, 0.0006360173574648798, 0.0006360173574648798, 0.0006360173574648798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006360173574648798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063602
Iteration 2/1000 | Loss: 0.00007594
Iteration 3/1000 | Loss: 0.00005208
Iteration 4/1000 | Loss: 0.00004109
Iteration 5/1000 | Loss: 0.00003738
Iteration 6/1000 | Loss: 0.00003514
Iteration 7/1000 | Loss: 0.00003390
Iteration 8/1000 | Loss: 0.00003297
Iteration 9/1000 | Loss: 0.00003222
Iteration 10/1000 | Loss: 0.00003152
Iteration 11/1000 | Loss: 0.00003105
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003029
Iteration 14/1000 | Loss: 0.00003002
Iteration 15/1000 | Loss: 0.00002975
Iteration 16/1000 | Loss: 0.00002954
Iteration 17/1000 | Loss: 0.00002935
Iteration 18/1000 | Loss: 0.00002924
Iteration 19/1000 | Loss: 0.00002922
Iteration 20/1000 | Loss: 0.00002918
Iteration 21/1000 | Loss: 0.00002902
Iteration 22/1000 | Loss: 0.00002888
Iteration 23/1000 | Loss: 0.00002886
Iteration 24/1000 | Loss: 0.00002885
Iteration 25/1000 | Loss: 0.00002885
Iteration 26/1000 | Loss: 0.00002884
Iteration 27/1000 | Loss: 0.00002883
Iteration 28/1000 | Loss: 0.00002883
Iteration 29/1000 | Loss: 0.00002882
Iteration 30/1000 | Loss: 0.00002881
Iteration 31/1000 | Loss: 0.00002880
Iteration 32/1000 | Loss: 0.00002879
Iteration 33/1000 | Loss: 0.00002879
Iteration 34/1000 | Loss: 0.00002879
Iteration 35/1000 | Loss: 0.00002879
Iteration 36/1000 | Loss: 0.00002878
Iteration 37/1000 | Loss: 0.00002878
Iteration 38/1000 | Loss: 0.00002878
Iteration 39/1000 | Loss: 0.00002877
Iteration 40/1000 | Loss: 0.00002877
Iteration 41/1000 | Loss: 0.00002876
Iteration 42/1000 | Loss: 0.00002876
Iteration 43/1000 | Loss: 0.00002875
Iteration 44/1000 | Loss: 0.00002875
Iteration 45/1000 | Loss: 0.00002875
Iteration 46/1000 | Loss: 0.00002874
Iteration 47/1000 | Loss: 0.00002874
Iteration 48/1000 | Loss: 0.00002874
Iteration 49/1000 | Loss: 0.00002873
Iteration 50/1000 | Loss: 0.00002873
Iteration 51/1000 | Loss: 0.00002872
Iteration 52/1000 | Loss: 0.00002872
Iteration 53/1000 | Loss: 0.00002872
Iteration 54/1000 | Loss: 0.00002871
Iteration 55/1000 | Loss: 0.00002871
Iteration 56/1000 | Loss: 0.00002871
Iteration 57/1000 | Loss: 0.00002871
Iteration 58/1000 | Loss: 0.00002870
Iteration 59/1000 | Loss: 0.00002870
Iteration 60/1000 | Loss: 0.00002870
Iteration 61/1000 | Loss: 0.00002870
Iteration 62/1000 | Loss: 0.00002869
Iteration 63/1000 | Loss: 0.00002869
Iteration 64/1000 | Loss: 0.00002869
Iteration 65/1000 | Loss: 0.00002869
Iteration 66/1000 | Loss: 0.00002868
Iteration 67/1000 | Loss: 0.00002868
Iteration 68/1000 | Loss: 0.00002868
Iteration 69/1000 | Loss: 0.00002868
Iteration 70/1000 | Loss: 0.00002868
Iteration 71/1000 | Loss: 0.00002868
Iteration 72/1000 | Loss: 0.00002868
Iteration 73/1000 | Loss: 0.00002868
Iteration 74/1000 | Loss: 0.00002867
Iteration 75/1000 | Loss: 0.00002867
Iteration 76/1000 | Loss: 0.00002867
Iteration 77/1000 | Loss: 0.00002867
Iteration 78/1000 | Loss: 0.00002866
Iteration 79/1000 | Loss: 0.00002866
Iteration 80/1000 | Loss: 0.00002866
Iteration 81/1000 | Loss: 0.00002866
Iteration 82/1000 | Loss: 0.00002866
Iteration 83/1000 | Loss: 0.00002866
Iteration 84/1000 | Loss: 0.00002866
Iteration 85/1000 | Loss: 0.00002866
Iteration 86/1000 | Loss: 0.00002866
Iteration 87/1000 | Loss: 0.00002865
Iteration 88/1000 | Loss: 0.00002865
Iteration 89/1000 | Loss: 0.00002865
Iteration 90/1000 | Loss: 0.00002865
Iteration 91/1000 | Loss: 0.00002864
Iteration 92/1000 | Loss: 0.00002864
Iteration 93/1000 | Loss: 0.00002864
Iteration 94/1000 | Loss: 0.00002864
Iteration 95/1000 | Loss: 0.00002864
Iteration 96/1000 | Loss: 0.00002864
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002864
Iteration 99/1000 | Loss: 0.00002864
Iteration 100/1000 | Loss: 0.00002864
Iteration 101/1000 | Loss: 0.00002864
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002863
Iteration 104/1000 | Loss: 0.00002863
Iteration 105/1000 | Loss: 0.00002863
Iteration 106/1000 | Loss: 0.00002863
Iteration 107/1000 | Loss: 0.00002863
Iteration 108/1000 | Loss: 0.00002863
Iteration 109/1000 | Loss: 0.00002863
Iteration 110/1000 | Loss: 0.00002863
Iteration 111/1000 | Loss: 0.00002863
Iteration 112/1000 | Loss: 0.00002862
Iteration 113/1000 | Loss: 0.00002862
Iteration 114/1000 | Loss: 0.00002862
Iteration 115/1000 | Loss: 0.00002862
Iteration 116/1000 | Loss: 0.00002862
Iteration 117/1000 | Loss: 0.00002862
Iteration 118/1000 | Loss: 0.00002862
Iteration 119/1000 | Loss: 0.00002862
Iteration 120/1000 | Loss: 0.00002862
Iteration 121/1000 | Loss: 0.00002862
Iteration 122/1000 | Loss: 0.00002862
Iteration 123/1000 | Loss: 0.00002862
Iteration 124/1000 | Loss: 0.00002862
Iteration 125/1000 | Loss: 0.00002862
Iteration 126/1000 | Loss: 0.00002862
Iteration 127/1000 | Loss: 0.00002862
Iteration 128/1000 | Loss: 0.00002862
Iteration 129/1000 | Loss: 0.00002862
Iteration 130/1000 | Loss: 0.00002861
Iteration 131/1000 | Loss: 0.00002861
Iteration 132/1000 | Loss: 0.00002861
Iteration 133/1000 | Loss: 0.00002861
Iteration 134/1000 | Loss: 0.00002861
Iteration 135/1000 | Loss: 0.00002861
Iteration 136/1000 | Loss: 0.00002861
Iteration 137/1000 | Loss: 0.00002861
Iteration 138/1000 | Loss: 0.00002861
Iteration 139/1000 | Loss: 0.00002861
Iteration 140/1000 | Loss: 0.00002861
Iteration 141/1000 | Loss: 0.00002861
Iteration 142/1000 | Loss: 0.00002861
Iteration 143/1000 | Loss: 0.00002861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.8607142667169683e-05, 2.8607142667169683e-05, 2.8607142667169683e-05, 2.8607142667169683e-05, 2.8607142667169683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8607142667169683e-05

Optimization complete. Final v2v error: 4.252026081085205 mm

Highest mean error: 6.039066791534424 mm for frame 163

Lowest mean error: 3.0382657051086426 mm for frame 55

Saving results

Total time: 47.60708975791931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047053
Iteration 2/25 | Loss: 0.00235961
Iteration 3/25 | Loss: 0.00163184
Iteration 4/25 | Loss: 0.00182050
Iteration 5/25 | Loss: 0.00143850
Iteration 6/25 | Loss: 0.00120179
Iteration 7/25 | Loss: 0.00103129
Iteration 8/25 | Loss: 0.00107624
Iteration 9/25 | Loss: 0.00101595
Iteration 10/25 | Loss: 0.00096242
Iteration 11/25 | Loss: 0.00092786
Iteration 12/25 | Loss: 0.00091401
Iteration 13/25 | Loss: 0.00090904
Iteration 14/25 | Loss: 0.00090778
Iteration 15/25 | Loss: 0.00090416
Iteration 16/25 | Loss: 0.00090560
Iteration 17/25 | Loss: 0.00090484
Iteration 18/25 | Loss: 0.00090062
Iteration 19/25 | Loss: 0.00090008
Iteration 20/25 | Loss: 0.00090448
Iteration 21/25 | Loss: 0.00090811
Iteration 22/25 | Loss: 0.00090065
Iteration 23/25 | Loss: 0.00089474
Iteration 24/25 | Loss: 0.00089213
Iteration 25/25 | Loss: 0.00089208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51914787
Iteration 2/25 | Loss: 0.00087898
Iteration 3/25 | Loss: 0.00087898
Iteration 4/25 | Loss: 0.00087898
Iteration 5/25 | Loss: 0.00087898
Iteration 6/25 | Loss: 0.00087898
Iteration 7/25 | Loss: 0.00087898
Iteration 8/25 | Loss: 0.00087898
Iteration 9/25 | Loss: 0.00087898
Iteration 10/25 | Loss: 0.00087897
Iteration 11/25 | Loss: 0.00087897
Iteration 12/25 | Loss: 0.00087897
Iteration 13/25 | Loss: 0.00087897
Iteration 14/25 | Loss: 0.00087897
Iteration 15/25 | Loss: 0.00087897
Iteration 16/25 | Loss: 0.00087897
Iteration 17/25 | Loss: 0.00087897
Iteration 18/25 | Loss: 0.00087897
Iteration 19/25 | Loss: 0.00087897
Iteration 20/25 | Loss: 0.00087897
Iteration 21/25 | Loss: 0.00087897
Iteration 22/25 | Loss: 0.00087897
Iteration 23/25 | Loss: 0.00087897
Iteration 24/25 | Loss: 0.00087897
Iteration 25/25 | Loss: 0.00087897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087897
Iteration 2/1000 | Loss: 0.00007642
Iteration 3/1000 | Loss: 0.00005483
Iteration 4/1000 | Loss: 0.00004639
Iteration 5/1000 | Loss: 0.00004158
Iteration 6/1000 | Loss: 0.00003750
Iteration 7/1000 | Loss: 0.00003532
Iteration 8/1000 | Loss: 0.00248031
Iteration 9/1000 | Loss: 0.00005437
Iteration 10/1000 | Loss: 0.00003693
Iteration 11/1000 | Loss: 0.00002672
Iteration 12/1000 | Loss: 0.00002131
Iteration 13/1000 | Loss: 0.00001903
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001612
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001468
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001461
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001457
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001447
Iteration 32/1000 | Loss: 0.00001446
Iteration 33/1000 | Loss: 0.00001442
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001437
Iteration 37/1000 | Loss: 0.00001437
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001434
Iteration 42/1000 | Loss: 0.00001434
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001432
Iteration 51/1000 | Loss: 0.00001432
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001431
Iteration 54/1000 | Loss: 0.00001431
Iteration 55/1000 | Loss: 0.00001431
Iteration 56/1000 | Loss: 0.00001431
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001429
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001429
Iteration 69/1000 | Loss: 0.00001429
Iteration 70/1000 | Loss: 0.00001429
Iteration 71/1000 | Loss: 0.00001428
Iteration 72/1000 | Loss: 0.00001428
Iteration 73/1000 | Loss: 0.00001428
Iteration 74/1000 | Loss: 0.00001428
Iteration 75/1000 | Loss: 0.00001428
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001426
Iteration 82/1000 | Loss: 0.00001426
Iteration 83/1000 | Loss: 0.00001426
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001426
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001425
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001424
Iteration 106/1000 | Loss: 0.00001424
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001422
Iteration 112/1000 | Loss: 0.00001422
Iteration 113/1000 | Loss: 0.00001422
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001422
Iteration 125/1000 | Loss: 0.00001422
Iteration 126/1000 | Loss: 0.00001422
Iteration 127/1000 | Loss: 0.00001422
Iteration 128/1000 | Loss: 0.00001422
Iteration 129/1000 | Loss: 0.00001421
Iteration 130/1000 | Loss: 0.00001421
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001421
Iteration 137/1000 | Loss: 0.00001421
Iteration 138/1000 | Loss: 0.00001421
Iteration 139/1000 | Loss: 0.00001421
Iteration 140/1000 | Loss: 0.00001421
Iteration 141/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4213608665158972e-05, 1.4213608665158972e-05, 1.4213608665158972e-05, 1.4213608665158972e-05, 1.4213608665158972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4213608665158972e-05

Optimization complete. Final v2v error: 3.174203634262085 mm

Highest mean error: 3.699342727661133 mm for frame 34

Lowest mean error: 2.8154194355010986 mm for frame 107

Saving results

Total time: 81.08245372772217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823047
Iteration 2/25 | Loss: 0.00148749
Iteration 3/25 | Loss: 0.00107842
Iteration 4/25 | Loss: 0.00095999
Iteration 5/25 | Loss: 0.00092946
Iteration 6/25 | Loss: 0.00091779
Iteration 7/25 | Loss: 0.00091502
Iteration 8/25 | Loss: 0.00091427
Iteration 9/25 | Loss: 0.00091408
Iteration 10/25 | Loss: 0.00091408
Iteration 11/25 | Loss: 0.00091408
Iteration 12/25 | Loss: 0.00091408
Iteration 13/25 | Loss: 0.00091408
Iteration 14/25 | Loss: 0.00091408
Iteration 15/25 | Loss: 0.00091408
Iteration 16/25 | Loss: 0.00091408
Iteration 17/25 | Loss: 0.00091408
Iteration 18/25 | Loss: 0.00091408
Iteration 19/25 | Loss: 0.00091408
Iteration 20/25 | Loss: 0.00091408
Iteration 21/25 | Loss: 0.00091408
Iteration 22/25 | Loss: 0.00091408
Iteration 23/25 | Loss: 0.00091408
Iteration 24/25 | Loss: 0.00091408
Iteration 25/25 | Loss: 0.00091408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27576780
Iteration 2/25 | Loss: 0.00063248
Iteration 3/25 | Loss: 0.00063247
Iteration 4/25 | Loss: 0.00063247
Iteration 5/25 | Loss: 0.00063246
Iteration 6/25 | Loss: 0.00063246
Iteration 7/25 | Loss: 0.00063246
Iteration 8/25 | Loss: 0.00063246
Iteration 9/25 | Loss: 0.00063246
Iteration 10/25 | Loss: 0.00063246
Iteration 11/25 | Loss: 0.00063246
Iteration 12/25 | Loss: 0.00063246
Iteration 13/25 | Loss: 0.00063246
Iteration 14/25 | Loss: 0.00063246
Iteration 15/25 | Loss: 0.00063246
Iteration 16/25 | Loss: 0.00063246
Iteration 17/25 | Loss: 0.00063246
Iteration 18/25 | Loss: 0.00063246
Iteration 19/25 | Loss: 0.00063246
Iteration 20/25 | Loss: 0.00063246
Iteration 21/25 | Loss: 0.00063246
Iteration 22/25 | Loss: 0.00063246
Iteration 23/25 | Loss: 0.00063246
Iteration 24/25 | Loss: 0.00063246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006324630812741816, 0.0006324630812741816, 0.0006324630812741816, 0.0006324630812741816, 0.0006324630812741816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006324630812741816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063246
Iteration 2/1000 | Loss: 0.00004110
Iteration 3/1000 | Loss: 0.00023028
Iteration 4/1000 | Loss: 0.00002573
Iteration 5/1000 | Loss: 0.00002426
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00002300
Iteration 8/1000 | Loss: 0.00002274
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002216
Iteration 11/1000 | Loss: 0.00002208
Iteration 12/1000 | Loss: 0.00002200
Iteration 13/1000 | Loss: 0.00002197
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002190
Iteration 16/1000 | Loss: 0.00002181
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002176
Iteration 21/1000 | Loss: 0.00002175
Iteration 22/1000 | Loss: 0.00002174
Iteration 23/1000 | Loss: 0.00002173
Iteration 24/1000 | Loss: 0.00002169
Iteration 25/1000 | Loss: 0.00002168
Iteration 26/1000 | Loss: 0.00002164
Iteration 27/1000 | Loss: 0.00002161
Iteration 28/1000 | Loss: 0.00002161
Iteration 29/1000 | Loss: 0.00002160
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002159
Iteration 32/1000 | Loss: 0.00002158
Iteration 33/1000 | Loss: 0.00002158
Iteration 34/1000 | Loss: 0.00002157
Iteration 35/1000 | Loss: 0.00002157
Iteration 36/1000 | Loss: 0.00002156
Iteration 37/1000 | Loss: 0.00002156
Iteration 38/1000 | Loss: 0.00002156
Iteration 39/1000 | Loss: 0.00002156
Iteration 40/1000 | Loss: 0.00002155
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002155
Iteration 43/1000 | Loss: 0.00002153
Iteration 44/1000 | Loss: 0.00002153
Iteration 45/1000 | Loss: 0.00002153
Iteration 46/1000 | Loss: 0.00002152
Iteration 47/1000 | Loss: 0.00002152
Iteration 48/1000 | Loss: 0.00002152
Iteration 49/1000 | Loss: 0.00002151
Iteration 50/1000 | Loss: 0.00002151
Iteration 51/1000 | Loss: 0.00002151
Iteration 52/1000 | Loss: 0.00002150
Iteration 53/1000 | Loss: 0.00002150
Iteration 54/1000 | Loss: 0.00002150
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002148
Iteration 60/1000 | Loss: 0.00002148
Iteration 61/1000 | Loss: 0.00002148
Iteration 62/1000 | Loss: 0.00002148
Iteration 63/1000 | Loss: 0.00002147
Iteration 64/1000 | Loss: 0.00002147
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00002147
Iteration 67/1000 | Loss: 0.00002146
Iteration 68/1000 | Loss: 0.00002146
Iteration 69/1000 | Loss: 0.00002146
Iteration 70/1000 | Loss: 0.00002146
Iteration 71/1000 | Loss: 0.00002146
Iteration 72/1000 | Loss: 0.00002145
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002144
Iteration 76/1000 | Loss: 0.00002144
Iteration 77/1000 | Loss: 0.00002144
Iteration 78/1000 | Loss: 0.00002144
Iteration 79/1000 | Loss: 0.00002144
Iteration 80/1000 | Loss: 0.00002143
Iteration 81/1000 | Loss: 0.00002143
Iteration 82/1000 | Loss: 0.00002143
Iteration 83/1000 | Loss: 0.00002142
Iteration 84/1000 | Loss: 0.00002142
Iteration 85/1000 | Loss: 0.00002142
Iteration 86/1000 | Loss: 0.00002142
Iteration 87/1000 | Loss: 0.00002142
Iteration 88/1000 | Loss: 0.00002142
Iteration 89/1000 | Loss: 0.00002142
Iteration 90/1000 | Loss: 0.00002142
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002142
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002142
Iteration 96/1000 | Loss: 0.00002142
Iteration 97/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.1418352844193578e-05, 2.1418352844193578e-05, 2.1418352844193578e-05, 2.1418352844193578e-05, 2.1418352844193578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1418352844193578e-05

Optimization complete. Final v2v error: 3.894500970840454 mm

Highest mean error: 4.327480316162109 mm for frame 220

Lowest mean error: 3.361642837524414 mm for frame 229

Saving results

Total time: 45.16119384765625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00983228
Iteration 2/25 | Loss: 0.00204657
Iteration 3/25 | Loss: 0.00117461
Iteration 4/25 | Loss: 0.00100965
Iteration 5/25 | Loss: 0.00099374
Iteration 6/25 | Loss: 0.00094928
Iteration 7/25 | Loss: 0.00092254
Iteration 8/25 | Loss: 0.00091907
Iteration 9/25 | Loss: 0.00091616
Iteration 10/25 | Loss: 0.00090701
Iteration 11/25 | Loss: 0.00090556
Iteration 12/25 | Loss: 0.00090375
Iteration 13/25 | Loss: 0.00090481
Iteration 14/25 | Loss: 0.00090138
Iteration 15/25 | Loss: 0.00090081
Iteration 16/25 | Loss: 0.00090023
Iteration 17/25 | Loss: 0.00090009
Iteration 18/25 | Loss: 0.00090008
Iteration 19/25 | Loss: 0.00090008
Iteration 20/25 | Loss: 0.00090008
Iteration 21/25 | Loss: 0.00090008
Iteration 22/25 | Loss: 0.00090008
Iteration 23/25 | Loss: 0.00090008
Iteration 24/25 | Loss: 0.00090008
Iteration 25/25 | Loss: 0.00090008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00512815
Iteration 2/25 | Loss: 0.00062808
Iteration 3/25 | Loss: 0.00056147
Iteration 4/25 | Loss: 0.00056147
Iteration 5/25 | Loss: 0.00056147
Iteration 6/25 | Loss: 0.00056147
Iteration 7/25 | Loss: 0.00056147
Iteration 8/25 | Loss: 0.00056147
Iteration 9/25 | Loss: 0.00056147
Iteration 10/25 | Loss: 0.00056147
Iteration 11/25 | Loss: 0.00056147
Iteration 12/25 | Loss: 0.00056147
Iteration 13/25 | Loss: 0.00056147
Iteration 14/25 | Loss: 0.00056147
Iteration 15/25 | Loss: 0.00056147
Iteration 16/25 | Loss: 0.00056147
Iteration 17/25 | Loss: 0.00056147
Iteration 18/25 | Loss: 0.00056147
Iteration 19/25 | Loss: 0.00056147
Iteration 20/25 | Loss: 0.00056147
Iteration 21/25 | Loss: 0.00056147
Iteration 22/25 | Loss: 0.00056147
Iteration 23/25 | Loss: 0.00056147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005614675465039909, 0.0005614675465039909, 0.0005614675465039909, 0.0005614675465039909, 0.0005614675465039909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005614675465039909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056147
Iteration 2/1000 | Loss: 0.00006185
Iteration 3/1000 | Loss: 0.00002615
Iteration 4/1000 | Loss: 0.00004685
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002280
Iteration 7/1000 | Loss: 0.00002207
Iteration 8/1000 | Loss: 0.00006684
Iteration 9/1000 | Loss: 0.00107189
Iteration 10/1000 | Loss: 0.00159691
Iteration 11/1000 | Loss: 0.00439819
Iteration 12/1000 | Loss: 0.00392770
Iteration 13/1000 | Loss: 0.00010076
Iteration 14/1000 | Loss: 0.00095992
Iteration 15/1000 | Loss: 0.00012779
Iteration 16/1000 | Loss: 0.00002750
Iteration 17/1000 | Loss: 0.00009850
Iteration 18/1000 | Loss: 0.00009104
Iteration 19/1000 | Loss: 0.00007478
Iteration 20/1000 | Loss: 0.00003384
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00003144
Iteration 23/1000 | Loss: 0.00006201
Iteration 24/1000 | Loss: 0.00012705
Iteration 25/1000 | Loss: 0.00002234
Iteration 26/1000 | Loss: 0.00006794
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00005908
Iteration 30/1000 | Loss: 0.00002167
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00007861
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002132
Iteration 35/1000 | Loss: 0.00006093
Iteration 36/1000 | Loss: 0.00004096
Iteration 37/1000 | Loss: 0.00002133
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002123
Iteration 41/1000 | Loss: 0.00002123
Iteration 42/1000 | Loss: 0.00002123
Iteration 43/1000 | Loss: 0.00002122
Iteration 44/1000 | Loss: 0.00002122
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00003691
Iteration 47/1000 | Loss: 0.00002729
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002723
Iteration 50/1000 | Loss: 0.00002116
Iteration 51/1000 | Loss: 0.00002116
Iteration 52/1000 | Loss: 0.00002116
Iteration 53/1000 | Loss: 0.00002116
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002115
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002115
Iteration 64/1000 | Loss: 0.00002115
Iteration 65/1000 | Loss: 0.00002115
Iteration 66/1000 | Loss: 0.00002115
Iteration 67/1000 | Loss: 0.00002115
Iteration 68/1000 | Loss: 0.00011480
Iteration 69/1000 | Loss: 0.00005595
Iteration 70/1000 | Loss: 0.00031004
Iteration 71/1000 | Loss: 0.00005859
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00009604
Iteration 74/1000 | Loss: 0.00043464
Iteration 75/1000 | Loss: 0.00022284
Iteration 76/1000 | Loss: 0.00002632
Iteration 77/1000 | Loss: 0.00001968
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00012412
Iteration 80/1000 | Loss: 0.00036616
Iteration 81/1000 | Loss: 0.00030954
Iteration 82/1000 | Loss: 0.00040545
Iteration 83/1000 | Loss: 0.00002135
Iteration 84/1000 | Loss: 0.00008219
Iteration 85/1000 | Loss: 0.00006648
Iteration 86/1000 | Loss: 0.00004050
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001928
Iteration 89/1000 | Loss: 0.00005634
Iteration 90/1000 | Loss: 0.00011536
Iteration 91/1000 | Loss: 0.00007225
Iteration 92/1000 | Loss: 0.00006720
Iteration 93/1000 | Loss: 0.00005260
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00005195
Iteration 100/1000 | Loss: 0.00005195
Iteration 101/1000 | Loss: 0.00002642
Iteration 102/1000 | Loss: 0.00001925
Iteration 103/1000 | Loss: 0.00001920
Iteration 104/1000 | Loss: 0.00003557
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001917
Iteration 107/1000 | Loss: 0.00002091
Iteration 108/1000 | Loss: 0.00002090
Iteration 109/1000 | Loss: 0.00005408
Iteration 110/1000 | Loss: 0.00003236
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00002016
Iteration 113/1000 | Loss: 0.00004213
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00001916
Iteration 116/1000 | Loss: 0.00001916
Iteration 117/1000 | Loss: 0.00001916
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001915
Iteration 121/1000 | Loss: 0.00001915
Iteration 122/1000 | Loss: 0.00001915
Iteration 123/1000 | Loss: 0.00001915
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001914
Iteration 126/1000 | Loss: 0.00001914
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001914
Iteration 130/1000 | Loss: 0.00001914
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00001912
Iteration 139/1000 | Loss: 0.00001912
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00001911
Iteration 143/1000 | Loss: 0.00001911
Iteration 144/1000 | Loss: 0.00001910
Iteration 145/1000 | Loss: 0.00001910
Iteration 146/1000 | Loss: 0.00001910
Iteration 147/1000 | Loss: 0.00001910
Iteration 148/1000 | Loss: 0.00001910
Iteration 149/1000 | Loss: 0.00001910
Iteration 150/1000 | Loss: 0.00001910
Iteration 151/1000 | Loss: 0.00001910
Iteration 152/1000 | Loss: 0.00001910
Iteration 153/1000 | Loss: 0.00001910
Iteration 154/1000 | Loss: 0.00001910
Iteration 155/1000 | Loss: 0.00001910
Iteration 156/1000 | Loss: 0.00001910
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001909
Iteration 162/1000 | Loss: 0.00001909
Iteration 163/1000 | Loss: 0.00001909
Iteration 164/1000 | Loss: 0.00001909
Iteration 165/1000 | Loss: 0.00001909
Iteration 166/1000 | Loss: 0.00001909
Iteration 167/1000 | Loss: 0.00001909
Iteration 168/1000 | Loss: 0.00001909
Iteration 169/1000 | Loss: 0.00001909
Iteration 170/1000 | Loss: 0.00001909
Iteration 171/1000 | Loss: 0.00001909
Iteration 172/1000 | Loss: 0.00001909
Iteration 173/1000 | Loss: 0.00001909
Iteration 174/1000 | Loss: 0.00001909
Iteration 175/1000 | Loss: 0.00001909
Iteration 176/1000 | Loss: 0.00001909
Iteration 177/1000 | Loss: 0.00001909
Iteration 178/1000 | Loss: 0.00001909
Iteration 179/1000 | Loss: 0.00001909
Iteration 180/1000 | Loss: 0.00001909
Iteration 181/1000 | Loss: 0.00001909
Iteration 182/1000 | Loss: 0.00001909
Iteration 183/1000 | Loss: 0.00001909
Iteration 184/1000 | Loss: 0.00001909
Iteration 185/1000 | Loss: 0.00001909
Iteration 186/1000 | Loss: 0.00001909
Iteration 187/1000 | Loss: 0.00001909
Iteration 188/1000 | Loss: 0.00001909
Iteration 189/1000 | Loss: 0.00001909
Iteration 190/1000 | Loss: 0.00001909
Iteration 191/1000 | Loss: 0.00001909
Iteration 192/1000 | Loss: 0.00001909
Iteration 193/1000 | Loss: 0.00001909
Iteration 194/1000 | Loss: 0.00001909
Iteration 195/1000 | Loss: 0.00001909
Iteration 196/1000 | Loss: 0.00001909
Iteration 197/1000 | Loss: 0.00001909
Iteration 198/1000 | Loss: 0.00001909
Iteration 199/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.9089264242211357e-05, 1.9089264242211357e-05, 1.9089264242211357e-05, 1.9089264242211357e-05, 1.9089264242211357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9089264242211357e-05

Optimization complete. Final v2v error: 3.7266671657562256 mm

Highest mean error: 9.021682739257812 mm for frame 155

Lowest mean error: 3.2250921726226807 mm for frame 106

Saving results

Total time: 157.63594222068787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045374
Iteration 2/25 | Loss: 0.00180311
Iteration 3/25 | Loss: 0.00121089
Iteration 4/25 | Loss: 0.00119077
Iteration 5/25 | Loss: 0.00108798
Iteration 6/25 | Loss: 0.00104390
Iteration 7/25 | Loss: 0.00102382
Iteration 8/25 | Loss: 0.00101889
Iteration 9/25 | Loss: 0.00101175
Iteration 10/25 | Loss: 0.00101213
Iteration 11/25 | Loss: 0.00101002
Iteration 12/25 | Loss: 0.00100913
Iteration 13/25 | Loss: 0.00100652
Iteration 14/25 | Loss: 0.00101124
Iteration 15/25 | Loss: 0.00100888
Iteration 16/25 | Loss: 0.00100894
Iteration 17/25 | Loss: 0.00100516
Iteration 18/25 | Loss: 0.00100444
Iteration 19/25 | Loss: 0.00099374
Iteration 20/25 | Loss: 0.00099758
Iteration 21/25 | Loss: 0.00099393
Iteration 22/25 | Loss: 0.00099522
Iteration 23/25 | Loss: 0.00099959
Iteration 24/25 | Loss: 0.00099450
Iteration 25/25 | Loss: 0.00099404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.94835663
Iteration 2/25 | Loss: 0.00128995
Iteration 3/25 | Loss: 0.00128832
Iteration 4/25 | Loss: 0.00128831
Iteration 5/25 | Loss: 0.00128831
Iteration 6/25 | Loss: 0.00128831
Iteration 7/25 | Loss: 0.00128831
Iteration 8/25 | Loss: 0.00128831
Iteration 9/25 | Loss: 0.00128831
Iteration 10/25 | Loss: 0.00128831
Iteration 11/25 | Loss: 0.00128831
Iteration 12/25 | Loss: 0.00128831
Iteration 13/25 | Loss: 0.00128831
Iteration 14/25 | Loss: 0.00128831
Iteration 15/25 | Loss: 0.00128831
Iteration 16/25 | Loss: 0.00128831
Iteration 17/25 | Loss: 0.00128831
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012883124873042107, 0.0012883124873042107, 0.0012883124873042107, 0.0012883124873042107, 0.0012883124873042107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012883124873042107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128831
Iteration 2/1000 | Loss: 0.00044996
Iteration 3/1000 | Loss: 0.00082287
Iteration 4/1000 | Loss: 0.00079379
Iteration 5/1000 | Loss: 0.00131418
Iteration 6/1000 | Loss: 0.00138193
Iteration 7/1000 | Loss: 0.00073770
Iteration 8/1000 | Loss: 0.00076906
Iteration 9/1000 | Loss: 0.00108787
Iteration 10/1000 | Loss: 0.00096540
Iteration 11/1000 | Loss: 0.00786806
Iteration 12/1000 | Loss: 0.00989430
Iteration 13/1000 | Loss: 0.00049631
Iteration 14/1000 | Loss: 0.00053895
Iteration 15/1000 | Loss: 0.00031716
Iteration 16/1000 | Loss: 0.00012561
Iteration 17/1000 | Loss: 0.00019701
Iteration 18/1000 | Loss: 0.00010017
Iteration 19/1000 | Loss: 0.00012932
Iteration 20/1000 | Loss: 0.00015243
Iteration 21/1000 | Loss: 0.00011701
Iteration 22/1000 | Loss: 0.00012231
Iteration 23/1000 | Loss: 0.00024712
Iteration 24/1000 | Loss: 0.00017950
Iteration 25/1000 | Loss: 0.00022001
Iteration 26/1000 | Loss: 0.00005530
Iteration 27/1000 | Loss: 0.00007373
Iteration 28/1000 | Loss: 0.00003743
Iteration 29/1000 | Loss: 0.00003453
Iteration 30/1000 | Loss: 0.00003326
Iteration 31/1000 | Loss: 0.00003228
Iteration 32/1000 | Loss: 0.00003152
Iteration 33/1000 | Loss: 0.00003097
Iteration 34/1000 | Loss: 0.00003062
Iteration 35/1000 | Loss: 0.00003040
Iteration 36/1000 | Loss: 0.00003019
Iteration 37/1000 | Loss: 0.00003017
Iteration 38/1000 | Loss: 0.00003000
Iteration 39/1000 | Loss: 0.00002995
Iteration 40/1000 | Loss: 0.00002994
Iteration 41/1000 | Loss: 0.00002980
Iteration 42/1000 | Loss: 0.00002978
Iteration 43/1000 | Loss: 0.00002978
Iteration 44/1000 | Loss: 0.00002977
Iteration 45/1000 | Loss: 0.00002977
Iteration 46/1000 | Loss: 0.00002976
Iteration 47/1000 | Loss: 0.00002976
Iteration 48/1000 | Loss: 0.00002975
Iteration 49/1000 | Loss: 0.00002975
Iteration 50/1000 | Loss: 0.00002974
Iteration 51/1000 | Loss: 0.00002974
Iteration 52/1000 | Loss: 0.00002973
Iteration 53/1000 | Loss: 0.00002973
Iteration 54/1000 | Loss: 0.00002972
Iteration 55/1000 | Loss: 0.00002972
Iteration 56/1000 | Loss: 0.00002972
Iteration 57/1000 | Loss: 0.00002971
Iteration 58/1000 | Loss: 0.00002971
Iteration 59/1000 | Loss: 0.00002971
Iteration 60/1000 | Loss: 0.00002971
Iteration 61/1000 | Loss: 0.00002970
Iteration 62/1000 | Loss: 0.00002968
Iteration 63/1000 | Loss: 0.00002965
Iteration 64/1000 | Loss: 0.00002964
Iteration 65/1000 | Loss: 0.00002964
Iteration 66/1000 | Loss: 0.00002961
Iteration 67/1000 | Loss: 0.00002961
Iteration 68/1000 | Loss: 0.00002958
Iteration 69/1000 | Loss: 0.00002956
Iteration 70/1000 | Loss: 0.00002951
Iteration 71/1000 | Loss: 0.00002950
Iteration 72/1000 | Loss: 0.00002950
Iteration 73/1000 | Loss: 0.00002949
Iteration 74/1000 | Loss: 0.00002948
Iteration 75/1000 | Loss: 0.00002948
Iteration 76/1000 | Loss: 0.00002947
Iteration 77/1000 | Loss: 0.00002947
Iteration 78/1000 | Loss: 0.00002947
Iteration 79/1000 | Loss: 0.00002946
Iteration 80/1000 | Loss: 0.00002945
Iteration 81/1000 | Loss: 0.00002944
Iteration 82/1000 | Loss: 0.00002943
Iteration 83/1000 | Loss: 0.00002943
Iteration 84/1000 | Loss: 0.00002942
Iteration 85/1000 | Loss: 0.00002942
Iteration 86/1000 | Loss: 0.00002942
Iteration 87/1000 | Loss: 0.00002942
Iteration 88/1000 | Loss: 0.00002941
Iteration 89/1000 | Loss: 0.00002941
Iteration 90/1000 | Loss: 0.00002941
Iteration 91/1000 | Loss: 0.00002940
Iteration 92/1000 | Loss: 0.00002940
Iteration 93/1000 | Loss: 0.00002940
Iteration 94/1000 | Loss: 0.00002940
Iteration 95/1000 | Loss: 0.00002940
Iteration 96/1000 | Loss: 0.00002940
Iteration 97/1000 | Loss: 0.00002939
Iteration 98/1000 | Loss: 0.00002939
Iteration 99/1000 | Loss: 0.00002939
Iteration 100/1000 | Loss: 0.00002939
Iteration 101/1000 | Loss: 0.00002939
Iteration 102/1000 | Loss: 0.00002939
Iteration 103/1000 | Loss: 0.00002939
Iteration 104/1000 | Loss: 0.00002939
Iteration 105/1000 | Loss: 0.00002939
Iteration 106/1000 | Loss: 0.00002938
Iteration 107/1000 | Loss: 0.00002938
Iteration 108/1000 | Loss: 0.00002938
Iteration 109/1000 | Loss: 0.00002937
Iteration 110/1000 | Loss: 0.00002937
Iteration 111/1000 | Loss: 0.00002937
Iteration 112/1000 | Loss: 0.00002937
Iteration 113/1000 | Loss: 0.00002937
Iteration 114/1000 | Loss: 0.00002937
Iteration 115/1000 | Loss: 0.00002937
Iteration 116/1000 | Loss: 0.00002937
Iteration 117/1000 | Loss: 0.00002936
Iteration 118/1000 | Loss: 0.00002936
Iteration 119/1000 | Loss: 0.00002936
Iteration 120/1000 | Loss: 0.00002936
Iteration 121/1000 | Loss: 0.00002935
Iteration 122/1000 | Loss: 0.00002935
Iteration 123/1000 | Loss: 0.00002935
Iteration 124/1000 | Loss: 0.00002935
Iteration 125/1000 | Loss: 0.00002934
Iteration 126/1000 | Loss: 0.00002934
Iteration 127/1000 | Loss: 0.00002934
Iteration 128/1000 | Loss: 0.00002934
Iteration 129/1000 | Loss: 0.00002934
Iteration 130/1000 | Loss: 0.00002934
Iteration 131/1000 | Loss: 0.00002934
Iteration 132/1000 | Loss: 0.00002933
Iteration 133/1000 | Loss: 0.00002933
Iteration 134/1000 | Loss: 0.00002933
Iteration 135/1000 | Loss: 0.00002933
Iteration 136/1000 | Loss: 0.00002933
Iteration 137/1000 | Loss: 0.00002932
Iteration 138/1000 | Loss: 0.00002932
Iteration 139/1000 | Loss: 0.00002932
Iteration 140/1000 | Loss: 0.00002932
Iteration 141/1000 | Loss: 0.00002931
Iteration 142/1000 | Loss: 0.00002931
Iteration 143/1000 | Loss: 0.00002931
Iteration 144/1000 | Loss: 0.00002931
Iteration 145/1000 | Loss: 0.00002931
Iteration 146/1000 | Loss: 0.00002930
Iteration 147/1000 | Loss: 0.00002930
Iteration 148/1000 | Loss: 0.00002930
Iteration 149/1000 | Loss: 0.00002930
Iteration 150/1000 | Loss: 0.00002930
Iteration 151/1000 | Loss: 0.00002929
Iteration 152/1000 | Loss: 0.00002929
Iteration 153/1000 | Loss: 0.00002929
Iteration 154/1000 | Loss: 0.00002929
Iteration 155/1000 | Loss: 0.00002929
Iteration 156/1000 | Loss: 0.00002929
Iteration 157/1000 | Loss: 0.00002929
Iteration 158/1000 | Loss: 0.00002929
Iteration 159/1000 | Loss: 0.00002928
Iteration 160/1000 | Loss: 0.00002928
Iteration 161/1000 | Loss: 0.00002928
Iteration 162/1000 | Loss: 0.00002927
Iteration 163/1000 | Loss: 0.00002927
Iteration 164/1000 | Loss: 0.00002927
Iteration 165/1000 | Loss: 0.00002927
Iteration 166/1000 | Loss: 0.00002926
Iteration 167/1000 | Loss: 0.00002926
Iteration 168/1000 | Loss: 0.00002926
Iteration 169/1000 | Loss: 0.00002926
Iteration 170/1000 | Loss: 0.00002926
Iteration 171/1000 | Loss: 0.00002925
Iteration 172/1000 | Loss: 0.00002925
Iteration 173/1000 | Loss: 0.00002925
Iteration 174/1000 | Loss: 0.00002925
Iteration 175/1000 | Loss: 0.00002925
Iteration 176/1000 | Loss: 0.00002924
Iteration 177/1000 | Loss: 0.00002924
Iteration 178/1000 | Loss: 0.00002924
Iteration 179/1000 | Loss: 0.00002924
Iteration 180/1000 | Loss: 0.00002924
Iteration 181/1000 | Loss: 0.00002924
Iteration 182/1000 | Loss: 0.00002924
Iteration 183/1000 | Loss: 0.00002923
Iteration 184/1000 | Loss: 0.00002923
Iteration 185/1000 | Loss: 0.00002923
Iteration 186/1000 | Loss: 0.00002923
Iteration 187/1000 | Loss: 0.00002923
Iteration 188/1000 | Loss: 0.00002923
Iteration 189/1000 | Loss: 0.00002922
Iteration 190/1000 | Loss: 0.00002922
Iteration 191/1000 | Loss: 0.00002922
Iteration 192/1000 | Loss: 0.00002922
Iteration 193/1000 | Loss: 0.00002922
Iteration 194/1000 | Loss: 0.00002922
Iteration 195/1000 | Loss: 0.00002922
Iteration 196/1000 | Loss: 0.00002922
Iteration 197/1000 | Loss: 0.00002922
Iteration 198/1000 | Loss: 0.00002921
Iteration 199/1000 | Loss: 0.00002921
Iteration 200/1000 | Loss: 0.00002921
Iteration 201/1000 | Loss: 0.00002921
Iteration 202/1000 | Loss: 0.00002921
Iteration 203/1000 | Loss: 0.00002921
Iteration 204/1000 | Loss: 0.00002921
Iteration 205/1000 | Loss: 0.00002920
Iteration 206/1000 | Loss: 0.00002920
Iteration 207/1000 | Loss: 0.00002920
Iteration 208/1000 | Loss: 0.00002920
Iteration 209/1000 | Loss: 0.00002920
Iteration 210/1000 | Loss: 0.00002920
Iteration 211/1000 | Loss: 0.00002920
Iteration 212/1000 | Loss: 0.00002920
Iteration 213/1000 | Loss: 0.00002919
Iteration 214/1000 | Loss: 0.00002919
Iteration 215/1000 | Loss: 0.00002919
Iteration 216/1000 | Loss: 0.00002919
Iteration 217/1000 | Loss: 0.00002919
Iteration 218/1000 | Loss: 0.00002919
Iteration 219/1000 | Loss: 0.00002919
Iteration 220/1000 | Loss: 0.00002919
Iteration 221/1000 | Loss: 0.00002919
Iteration 222/1000 | Loss: 0.00002919
Iteration 223/1000 | Loss: 0.00002919
Iteration 224/1000 | Loss: 0.00002919
Iteration 225/1000 | Loss: 0.00002919
Iteration 226/1000 | Loss: 0.00002919
Iteration 227/1000 | Loss: 0.00002919
Iteration 228/1000 | Loss: 0.00002919
Iteration 229/1000 | Loss: 0.00002919
Iteration 230/1000 | Loss: 0.00002919
Iteration 231/1000 | Loss: 0.00002919
Iteration 232/1000 | Loss: 0.00002919
Iteration 233/1000 | Loss: 0.00002919
Iteration 234/1000 | Loss: 0.00002919
Iteration 235/1000 | Loss: 0.00002919
Iteration 236/1000 | Loss: 0.00002919
Iteration 237/1000 | Loss: 0.00002919
Iteration 238/1000 | Loss: 0.00002919
Iteration 239/1000 | Loss: 0.00002919
Iteration 240/1000 | Loss: 0.00002919
Iteration 241/1000 | Loss: 0.00002919
Iteration 242/1000 | Loss: 0.00002919
Iteration 243/1000 | Loss: 0.00002919
Iteration 244/1000 | Loss: 0.00002919
Iteration 245/1000 | Loss: 0.00002919
Iteration 246/1000 | Loss: 0.00002919
Iteration 247/1000 | Loss: 0.00002919
Iteration 248/1000 | Loss: 0.00002919
Iteration 249/1000 | Loss: 0.00002919
Iteration 250/1000 | Loss: 0.00002919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.9185559469624422e-05, 2.9185559469624422e-05, 2.9185559469624422e-05, 2.9185559469624422e-05, 2.9185559469624422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9185559469624422e-05

Optimization complete. Final v2v error: 4.449765205383301 mm

Highest mean error: 6.241220951080322 mm for frame 129

Lowest mean error: 3.236609935760498 mm for frame 98

Saving results

Total time: 121.30578923225403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061028
Iteration 2/25 | Loss: 0.00127279
Iteration 3/25 | Loss: 0.00096543
Iteration 4/25 | Loss: 0.00087975
Iteration 5/25 | Loss: 0.00085542
Iteration 6/25 | Loss: 0.00084891
Iteration 7/25 | Loss: 0.00085460
Iteration 8/25 | Loss: 0.00084289
Iteration 9/25 | Loss: 0.00084529
Iteration 10/25 | Loss: 0.00084032
Iteration 11/25 | Loss: 0.00084020
Iteration 12/25 | Loss: 0.00084019
Iteration 13/25 | Loss: 0.00084019
Iteration 14/25 | Loss: 0.00084019
Iteration 15/25 | Loss: 0.00084019
Iteration 16/25 | Loss: 0.00084019
Iteration 17/25 | Loss: 0.00084019
Iteration 18/25 | Loss: 0.00084019
Iteration 19/25 | Loss: 0.00084019
Iteration 20/25 | Loss: 0.00084019
Iteration 21/25 | Loss: 0.00084019
Iteration 22/25 | Loss: 0.00084018
Iteration 23/25 | Loss: 0.00084018
Iteration 24/25 | Loss: 0.00084018
Iteration 25/25 | Loss: 0.00084018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72404909
Iteration 2/25 | Loss: 0.00054792
Iteration 3/25 | Loss: 0.00054792
Iteration 4/25 | Loss: 0.00054792
Iteration 5/25 | Loss: 0.00054791
Iteration 6/25 | Loss: 0.00054791
Iteration 7/25 | Loss: 0.00054791
Iteration 8/25 | Loss: 0.00054791
Iteration 9/25 | Loss: 0.00054791
Iteration 10/25 | Loss: 0.00054791
Iteration 11/25 | Loss: 0.00054791
Iteration 12/25 | Loss: 0.00054791
Iteration 13/25 | Loss: 0.00054791
Iteration 14/25 | Loss: 0.00054791
Iteration 15/25 | Loss: 0.00054791
Iteration 16/25 | Loss: 0.00054791
Iteration 17/25 | Loss: 0.00054791
Iteration 18/25 | Loss: 0.00054791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005479119135998189, 0.0005479119135998189, 0.0005479119135998189, 0.0005479119135998189, 0.0005479119135998189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005479119135998189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054791
Iteration 2/1000 | Loss: 0.00002302
Iteration 3/1000 | Loss: 0.00001860
Iteration 4/1000 | Loss: 0.00001706
Iteration 5/1000 | Loss: 0.00007727
Iteration 6/1000 | Loss: 0.00037141
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00001766
Iteration 9/1000 | Loss: 0.00001558
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001481
Iteration 12/1000 | Loss: 0.00001455
Iteration 13/1000 | Loss: 0.00001436
Iteration 14/1000 | Loss: 0.00001418
Iteration 15/1000 | Loss: 0.00001418
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001412
Iteration 20/1000 | Loss: 0.00001411
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001406
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001402
Iteration 32/1000 | Loss: 0.00001402
Iteration 33/1000 | Loss: 0.00001402
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001401
Iteration 37/1000 | Loss: 0.00001400
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001397
Iteration 43/1000 | Loss: 0.00001397
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00008092
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001384
Iteration 57/1000 | Loss: 0.00001384
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001382
Iteration 61/1000 | Loss: 0.00001382
Iteration 62/1000 | Loss: 0.00001382
Iteration 63/1000 | Loss: 0.00001381
Iteration 64/1000 | Loss: 0.00001381
Iteration 65/1000 | Loss: 0.00001381
Iteration 66/1000 | Loss: 0.00001381
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001380
Iteration 69/1000 | Loss: 0.00001380
Iteration 70/1000 | Loss: 0.00001380
Iteration 71/1000 | Loss: 0.00001379
Iteration 72/1000 | Loss: 0.00001379
Iteration 73/1000 | Loss: 0.00001379
Iteration 74/1000 | Loss: 0.00001379
Iteration 75/1000 | Loss: 0.00001379
Iteration 76/1000 | Loss: 0.00001379
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001379
Iteration 79/1000 | Loss: 0.00001379
Iteration 80/1000 | Loss: 0.00001378
Iteration 81/1000 | Loss: 0.00001378
Iteration 82/1000 | Loss: 0.00001378
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001377
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001376
Iteration 92/1000 | Loss: 0.00001376
Iteration 93/1000 | Loss: 0.00004987
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00003730
Iteration 97/1000 | Loss: 0.00001423
Iteration 98/1000 | Loss: 0.00001405
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001378
Iteration 106/1000 | Loss: 0.00001378
Iteration 107/1000 | Loss: 0.00001377
Iteration 108/1000 | Loss: 0.00001377
Iteration 109/1000 | Loss: 0.00001376
Iteration 110/1000 | Loss: 0.00001375
Iteration 111/1000 | Loss: 0.00001375
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001374
Iteration 114/1000 | Loss: 0.00001374
Iteration 115/1000 | Loss: 0.00001373
Iteration 116/1000 | Loss: 0.00001373
Iteration 117/1000 | Loss: 0.00001373
Iteration 118/1000 | Loss: 0.00001372
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001371
Iteration 122/1000 | Loss: 0.00001371
Iteration 123/1000 | Loss: 0.00001371
Iteration 124/1000 | Loss: 0.00001370
Iteration 125/1000 | Loss: 0.00001370
Iteration 126/1000 | Loss: 0.00001370
Iteration 127/1000 | Loss: 0.00001369
Iteration 128/1000 | Loss: 0.00001369
Iteration 129/1000 | Loss: 0.00001369
Iteration 130/1000 | Loss: 0.00001369
Iteration 131/1000 | Loss: 0.00001369
Iteration 132/1000 | Loss: 0.00001369
Iteration 133/1000 | Loss: 0.00001369
Iteration 134/1000 | Loss: 0.00001369
Iteration 135/1000 | Loss: 0.00001369
Iteration 136/1000 | Loss: 0.00001369
Iteration 137/1000 | Loss: 0.00001368
Iteration 138/1000 | Loss: 0.00001368
Iteration 139/1000 | Loss: 0.00001368
Iteration 140/1000 | Loss: 0.00006251
Iteration 141/1000 | Loss: 0.00001387
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001725
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001378
Iteration 156/1000 | Loss: 0.00001378
Iteration 157/1000 | Loss: 0.00001378
Iteration 158/1000 | Loss: 0.00001378
Iteration 159/1000 | Loss: 0.00001378
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001377
Iteration 166/1000 | Loss: 0.00001377
Iteration 167/1000 | Loss: 0.00001376
Iteration 168/1000 | Loss: 0.00001376
Iteration 169/1000 | Loss: 0.00001376
Iteration 170/1000 | Loss: 0.00001376
Iteration 171/1000 | Loss: 0.00001376
Iteration 172/1000 | Loss: 0.00001376
Iteration 173/1000 | Loss: 0.00001376
Iteration 174/1000 | Loss: 0.00001376
Iteration 175/1000 | Loss: 0.00001376
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001376
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001375
Iteration 181/1000 | Loss: 0.00001375
Iteration 182/1000 | Loss: 0.00001375
Iteration 183/1000 | Loss: 0.00001375
Iteration 184/1000 | Loss: 0.00001375
Iteration 185/1000 | Loss: 0.00001375
Iteration 186/1000 | Loss: 0.00001375
Iteration 187/1000 | Loss: 0.00001375
Iteration 188/1000 | Loss: 0.00001375
Iteration 189/1000 | Loss: 0.00001375
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00001375
Iteration 194/1000 | Loss: 0.00001374
Iteration 195/1000 | Loss: 0.00001374
Iteration 196/1000 | Loss: 0.00001374
Iteration 197/1000 | Loss: 0.00001374
Iteration 198/1000 | Loss: 0.00001374
Iteration 199/1000 | Loss: 0.00001373
Iteration 200/1000 | Loss: 0.00001373
Iteration 201/1000 | Loss: 0.00001373
Iteration 202/1000 | Loss: 0.00001373
Iteration 203/1000 | Loss: 0.00001372
Iteration 204/1000 | Loss: 0.00001372
Iteration 205/1000 | Loss: 0.00001371
Iteration 206/1000 | Loss: 0.00001624
Iteration 207/1000 | Loss: 0.00001426
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001430
Iteration 214/1000 | Loss: 0.00001389
Iteration 215/1000 | Loss: 0.00001683
Iteration 216/1000 | Loss: 0.00001682
Iteration 217/1000 | Loss: 0.00001410
Iteration 218/1000 | Loss: 0.00001389
Iteration 219/1000 | Loss: 0.00001368
Iteration 220/1000 | Loss: 0.00001368
Iteration 221/1000 | Loss: 0.00001368
Iteration 222/1000 | Loss: 0.00001368
Iteration 223/1000 | Loss: 0.00001368
Iteration 224/1000 | Loss: 0.00001368
Iteration 225/1000 | Loss: 0.00001368
Iteration 226/1000 | Loss: 0.00001368
Iteration 227/1000 | Loss: 0.00001368
Iteration 228/1000 | Loss: 0.00001368
Iteration 229/1000 | Loss: 0.00001368
Iteration 230/1000 | Loss: 0.00001367
Iteration 231/1000 | Loss: 0.00001367
Iteration 232/1000 | Loss: 0.00001367
Iteration 233/1000 | Loss: 0.00001367
Iteration 234/1000 | Loss: 0.00001366
Iteration 235/1000 | Loss: 0.00001366
Iteration 236/1000 | Loss: 0.00001366
Iteration 237/1000 | Loss: 0.00001365
Iteration 238/1000 | Loss: 0.00001365
Iteration 239/1000 | Loss: 0.00001365
Iteration 240/1000 | Loss: 0.00001365
Iteration 241/1000 | Loss: 0.00001365
Iteration 242/1000 | Loss: 0.00001365
Iteration 243/1000 | Loss: 0.00001365
Iteration 244/1000 | Loss: 0.00001365
Iteration 245/1000 | Loss: 0.00001365
Iteration 246/1000 | Loss: 0.00001365
Iteration 247/1000 | Loss: 0.00001365
Iteration 248/1000 | Loss: 0.00001365
Iteration 249/1000 | Loss: 0.00001365
Iteration 250/1000 | Loss: 0.00001365
Iteration 251/1000 | Loss: 0.00001365
Iteration 252/1000 | Loss: 0.00001365
Iteration 253/1000 | Loss: 0.00001365
Iteration 254/1000 | Loss: 0.00001365
Iteration 255/1000 | Loss: 0.00001365
Iteration 256/1000 | Loss: 0.00001365
Iteration 257/1000 | Loss: 0.00001365
Iteration 258/1000 | Loss: 0.00001365
Iteration 259/1000 | Loss: 0.00001365
Iteration 260/1000 | Loss: 0.00001365
Iteration 261/1000 | Loss: 0.00001365
Iteration 262/1000 | Loss: 0.00001365
Iteration 263/1000 | Loss: 0.00001365
Iteration 264/1000 | Loss: 0.00001365
Iteration 265/1000 | Loss: 0.00001365
Iteration 266/1000 | Loss: 0.00001365
Iteration 267/1000 | Loss: 0.00001365
Iteration 268/1000 | Loss: 0.00001365
Iteration 269/1000 | Loss: 0.00001365
Iteration 270/1000 | Loss: 0.00001365
Iteration 271/1000 | Loss: 0.00001365
Iteration 272/1000 | Loss: 0.00001365
Iteration 273/1000 | Loss: 0.00001365
Iteration 274/1000 | Loss: 0.00001365
Iteration 275/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.3648259482579306e-05, 1.3648259482579306e-05, 1.3648259482579306e-05, 1.3648259482579306e-05, 1.3648259482579306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3648259482579306e-05

Optimization complete. Final v2v error: 3.1294219493865967 mm

Highest mean error: 4.154675006866455 mm for frame 226

Lowest mean error: 2.877880573272705 mm for frame 6

Saving results

Total time: 92.72490215301514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970697
Iteration 2/25 | Loss: 0.00168696
Iteration 3/25 | Loss: 0.00109675
Iteration 4/25 | Loss: 0.00105081
Iteration 5/25 | Loss: 0.00103489
Iteration 6/25 | Loss: 0.00103056
Iteration 7/25 | Loss: 0.00103004
Iteration 8/25 | Loss: 0.00103004
Iteration 9/25 | Loss: 0.00103004
Iteration 10/25 | Loss: 0.00103004
Iteration 11/25 | Loss: 0.00103004
Iteration 12/25 | Loss: 0.00103004
Iteration 13/25 | Loss: 0.00103004
Iteration 14/25 | Loss: 0.00103004
Iteration 15/25 | Loss: 0.00103004
Iteration 16/25 | Loss: 0.00103004
Iteration 17/25 | Loss: 0.00103004
Iteration 18/25 | Loss: 0.00103004
Iteration 19/25 | Loss: 0.00103004
Iteration 20/25 | Loss: 0.00103004
Iteration 21/25 | Loss: 0.00103004
Iteration 22/25 | Loss: 0.00103004
Iteration 23/25 | Loss: 0.00103004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001030037528835237, 0.001030037528835237, 0.001030037528835237, 0.001030037528835237, 0.001030037528835237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001030037528835237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97541028
Iteration 2/25 | Loss: 0.00053520
Iteration 3/25 | Loss: 0.00053520
Iteration 4/25 | Loss: 0.00053520
Iteration 5/25 | Loss: 0.00053520
Iteration 6/25 | Loss: 0.00053520
Iteration 7/25 | Loss: 0.00053520
Iteration 8/25 | Loss: 0.00053520
Iteration 9/25 | Loss: 0.00053520
Iteration 10/25 | Loss: 0.00053520
Iteration 11/25 | Loss: 0.00053520
Iteration 12/25 | Loss: 0.00053520
Iteration 13/25 | Loss: 0.00053520
Iteration 14/25 | Loss: 0.00053520
Iteration 15/25 | Loss: 0.00053520
Iteration 16/25 | Loss: 0.00053520
Iteration 17/25 | Loss: 0.00053520
Iteration 18/25 | Loss: 0.00053520
Iteration 19/25 | Loss: 0.00053520
Iteration 20/25 | Loss: 0.00053520
Iteration 21/25 | Loss: 0.00053520
Iteration 22/25 | Loss: 0.00053520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005351989530026913, 0.0005351989530026913, 0.0005351989530026913, 0.0005351989530026913, 0.0005351989530026913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005351989530026913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053520
Iteration 2/1000 | Loss: 0.00006294
Iteration 3/1000 | Loss: 0.00004404
Iteration 4/1000 | Loss: 0.00004098
Iteration 5/1000 | Loss: 0.00003928
Iteration 6/1000 | Loss: 0.00003840
Iteration 7/1000 | Loss: 0.00003740
Iteration 8/1000 | Loss: 0.00003689
Iteration 9/1000 | Loss: 0.00003644
Iteration 10/1000 | Loss: 0.00003612
Iteration 11/1000 | Loss: 0.00003582
Iteration 12/1000 | Loss: 0.00003555
Iteration 13/1000 | Loss: 0.00003525
Iteration 14/1000 | Loss: 0.00003504
Iteration 15/1000 | Loss: 0.00003481
Iteration 16/1000 | Loss: 0.00003461
Iteration 17/1000 | Loss: 0.00003447
Iteration 18/1000 | Loss: 0.00003444
Iteration 19/1000 | Loss: 0.00003443
Iteration 20/1000 | Loss: 0.00003436
Iteration 21/1000 | Loss: 0.00003436
Iteration 22/1000 | Loss: 0.00003434
Iteration 23/1000 | Loss: 0.00003432
Iteration 24/1000 | Loss: 0.00003430
Iteration 25/1000 | Loss: 0.00003429
Iteration 26/1000 | Loss: 0.00003428
Iteration 27/1000 | Loss: 0.00003423
Iteration 28/1000 | Loss: 0.00003422
Iteration 29/1000 | Loss: 0.00003422
Iteration 30/1000 | Loss: 0.00003421
Iteration 31/1000 | Loss: 0.00003421
Iteration 32/1000 | Loss: 0.00003419
Iteration 33/1000 | Loss: 0.00003418
Iteration 34/1000 | Loss: 0.00003418
Iteration 35/1000 | Loss: 0.00003417
Iteration 36/1000 | Loss: 0.00003416
Iteration 37/1000 | Loss: 0.00003416
Iteration 38/1000 | Loss: 0.00003415
Iteration 39/1000 | Loss: 0.00003414
Iteration 40/1000 | Loss: 0.00003414
Iteration 41/1000 | Loss: 0.00003412
Iteration 42/1000 | Loss: 0.00003412
Iteration 43/1000 | Loss: 0.00003412
Iteration 44/1000 | Loss: 0.00003411
Iteration 45/1000 | Loss: 0.00003410
Iteration 46/1000 | Loss: 0.00003410
Iteration 47/1000 | Loss: 0.00003410
Iteration 48/1000 | Loss: 0.00003410
Iteration 49/1000 | Loss: 0.00003410
Iteration 50/1000 | Loss: 0.00003410
Iteration 51/1000 | Loss: 0.00003410
Iteration 52/1000 | Loss: 0.00003409
Iteration 53/1000 | Loss: 0.00003409
Iteration 54/1000 | Loss: 0.00003409
Iteration 55/1000 | Loss: 0.00003409
Iteration 56/1000 | Loss: 0.00003409
Iteration 57/1000 | Loss: 0.00003409
Iteration 58/1000 | Loss: 0.00003409
Iteration 59/1000 | Loss: 0.00003409
Iteration 60/1000 | Loss: 0.00003409
Iteration 61/1000 | Loss: 0.00003408
Iteration 62/1000 | Loss: 0.00003408
Iteration 63/1000 | Loss: 0.00003408
Iteration 64/1000 | Loss: 0.00003407
Iteration 65/1000 | Loss: 0.00003407
Iteration 66/1000 | Loss: 0.00003407
Iteration 67/1000 | Loss: 0.00003407
Iteration 68/1000 | Loss: 0.00003407
Iteration 69/1000 | Loss: 0.00003407
Iteration 70/1000 | Loss: 0.00003407
Iteration 71/1000 | Loss: 0.00003406
Iteration 72/1000 | Loss: 0.00003406
Iteration 73/1000 | Loss: 0.00003406
Iteration 74/1000 | Loss: 0.00003406
Iteration 75/1000 | Loss: 0.00003406
Iteration 76/1000 | Loss: 0.00003406
Iteration 77/1000 | Loss: 0.00003406
Iteration 78/1000 | Loss: 0.00003405
Iteration 79/1000 | Loss: 0.00003405
Iteration 80/1000 | Loss: 0.00003405
Iteration 81/1000 | Loss: 0.00003404
Iteration 82/1000 | Loss: 0.00003404
Iteration 83/1000 | Loss: 0.00003404
Iteration 84/1000 | Loss: 0.00003404
Iteration 85/1000 | Loss: 0.00003404
Iteration 86/1000 | Loss: 0.00003403
Iteration 87/1000 | Loss: 0.00003403
Iteration 88/1000 | Loss: 0.00003403
Iteration 89/1000 | Loss: 0.00003403
Iteration 90/1000 | Loss: 0.00003403
Iteration 91/1000 | Loss: 0.00003403
Iteration 92/1000 | Loss: 0.00003403
Iteration 93/1000 | Loss: 0.00003403
Iteration 94/1000 | Loss: 0.00003403
Iteration 95/1000 | Loss: 0.00003402
Iteration 96/1000 | Loss: 0.00003402
Iteration 97/1000 | Loss: 0.00003402
Iteration 98/1000 | Loss: 0.00003402
Iteration 99/1000 | Loss: 0.00003402
Iteration 100/1000 | Loss: 0.00003402
Iteration 101/1000 | Loss: 0.00003402
Iteration 102/1000 | Loss: 0.00003401
Iteration 103/1000 | Loss: 0.00003401
Iteration 104/1000 | Loss: 0.00003401
Iteration 105/1000 | Loss: 0.00003401
Iteration 106/1000 | Loss: 0.00003401
Iteration 107/1000 | Loss: 0.00003401
Iteration 108/1000 | Loss: 0.00003401
Iteration 109/1000 | Loss: 0.00003401
Iteration 110/1000 | Loss: 0.00003401
Iteration 111/1000 | Loss: 0.00003400
Iteration 112/1000 | Loss: 0.00003400
Iteration 113/1000 | Loss: 0.00003400
Iteration 114/1000 | Loss: 0.00003400
Iteration 115/1000 | Loss: 0.00003400
Iteration 116/1000 | Loss: 0.00003400
Iteration 117/1000 | Loss: 0.00003400
Iteration 118/1000 | Loss: 0.00003400
Iteration 119/1000 | Loss: 0.00003400
Iteration 120/1000 | Loss: 0.00003400
Iteration 121/1000 | Loss: 0.00003400
Iteration 122/1000 | Loss: 0.00003400
Iteration 123/1000 | Loss: 0.00003399
Iteration 124/1000 | Loss: 0.00003399
Iteration 125/1000 | Loss: 0.00003399
Iteration 126/1000 | Loss: 0.00003399
Iteration 127/1000 | Loss: 0.00003399
Iteration 128/1000 | Loss: 0.00003399
Iteration 129/1000 | Loss: 0.00003399
Iteration 130/1000 | Loss: 0.00003399
Iteration 131/1000 | Loss: 0.00003398
Iteration 132/1000 | Loss: 0.00003398
Iteration 133/1000 | Loss: 0.00003398
Iteration 134/1000 | Loss: 0.00003398
Iteration 135/1000 | Loss: 0.00003398
Iteration 136/1000 | Loss: 0.00003398
Iteration 137/1000 | Loss: 0.00003397
Iteration 138/1000 | Loss: 0.00003397
Iteration 139/1000 | Loss: 0.00003397
Iteration 140/1000 | Loss: 0.00003397
Iteration 141/1000 | Loss: 0.00003397
Iteration 142/1000 | Loss: 0.00003397
Iteration 143/1000 | Loss: 0.00003397
Iteration 144/1000 | Loss: 0.00003397
Iteration 145/1000 | Loss: 0.00003396
Iteration 146/1000 | Loss: 0.00003396
Iteration 147/1000 | Loss: 0.00003396
Iteration 148/1000 | Loss: 0.00003396
Iteration 149/1000 | Loss: 0.00003396
Iteration 150/1000 | Loss: 0.00003396
Iteration 151/1000 | Loss: 0.00003396
Iteration 152/1000 | Loss: 0.00003396
Iteration 153/1000 | Loss: 0.00003396
Iteration 154/1000 | Loss: 0.00003396
Iteration 155/1000 | Loss: 0.00003396
Iteration 156/1000 | Loss: 0.00003396
Iteration 157/1000 | Loss: 0.00003395
Iteration 158/1000 | Loss: 0.00003395
Iteration 159/1000 | Loss: 0.00003395
Iteration 160/1000 | Loss: 0.00003395
Iteration 161/1000 | Loss: 0.00003395
Iteration 162/1000 | Loss: 0.00003395
Iteration 163/1000 | Loss: 0.00003395
Iteration 164/1000 | Loss: 0.00003395
Iteration 165/1000 | Loss: 0.00003395
Iteration 166/1000 | Loss: 0.00003395
Iteration 167/1000 | Loss: 0.00003394
Iteration 168/1000 | Loss: 0.00003394
Iteration 169/1000 | Loss: 0.00003394
Iteration 170/1000 | Loss: 0.00003394
Iteration 171/1000 | Loss: 0.00003394
Iteration 172/1000 | Loss: 0.00003394
Iteration 173/1000 | Loss: 0.00003394
Iteration 174/1000 | Loss: 0.00003394
Iteration 175/1000 | Loss: 0.00003394
Iteration 176/1000 | Loss: 0.00003394
Iteration 177/1000 | Loss: 0.00003393
Iteration 178/1000 | Loss: 0.00003393
Iteration 179/1000 | Loss: 0.00003393
Iteration 180/1000 | Loss: 0.00003393
Iteration 181/1000 | Loss: 0.00003393
Iteration 182/1000 | Loss: 0.00003393
Iteration 183/1000 | Loss: 0.00003393
Iteration 184/1000 | Loss: 0.00003393
Iteration 185/1000 | Loss: 0.00003393
Iteration 186/1000 | Loss: 0.00003393
Iteration 187/1000 | Loss: 0.00003393
Iteration 188/1000 | Loss: 0.00003393
Iteration 189/1000 | Loss: 0.00003393
Iteration 190/1000 | Loss: 0.00003393
Iteration 191/1000 | Loss: 0.00003392
Iteration 192/1000 | Loss: 0.00003392
Iteration 193/1000 | Loss: 0.00003392
Iteration 194/1000 | Loss: 0.00003392
Iteration 195/1000 | Loss: 0.00003392
Iteration 196/1000 | Loss: 0.00003392
Iteration 197/1000 | Loss: 0.00003392
Iteration 198/1000 | Loss: 0.00003392
Iteration 199/1000 | Loss: 0.00003392
Iteration 200/1000 | Loss: 0.00003392
Iteration 201/1000 | Loss: 0.00003392
Iteration 202/1000 | Loss: 0.00003392
Iteration 203/1000 | Loss: 0.00003392
Iteration 204/1000 | Loss: 0.00003392
Iteration 205/1000 | Loss: 0.00003392
Iteration 206/1000 | Loss: 0.00003392
Iteration 207/1000 | Loss: 0.00003391
Iteration 208/1000 | Loss: 0.00003391
Iteration 209/1000 | Loss: 0.00003391
Iteration 210/1000 | Loss: 0.00003391
Iteration 211/1000 | Loss: 0.00003391
Iteration 212/1000 | Loss: 0.00003391
Iteration 213/1000 | Loss: 0.00003391
Iteration 214/1000 | Loss: 0.00003391
Iteration 215/1000 | Loss: 0.00003391
Iteration 216/1000 | Loss: 0.00003391
Iteration 217/1000 | Loss: 0.00003391
Iteration 218/1000 | Loss: 0.00003391
Iteration 219/1000 | Loss: 0.00003391
Iteration 220/1000 | Loss: 0.00003391
Iteration 221/1000 | Loss: 0.00003391
Iteration 222/1000 | Loss: 0.00003391
Iteration 223/1000 | Loss: 0.00003391
Iteration 224/1000 | Loss: 0.00003391
Iteration 225/1000 | Loss: 0.00003391
Iteration 226/1000 | Loss: 0.00003391
Iteration 227/1000 | Loss: 0.00003391
Iteration 228/1000 | Loss: 0.00003391
Iteration 229/1000 | Loss: 0.00003391
Iteration 230/1000 | Loss: 0.00003391
Iteration 231/1000 | Loss: 0.00003391
Iteration 232/1000 | Loss: 0.00003391
Iteration 233/1000 | Loss: 0.00003391
Iteration 234/1000 | Loss: 0.00003391
Iteration 235/1000 | Loss: 0.00003391
Iteration 236/1000 | Loss: 0.00003391
Iteration 237/1000 | Loss: 0.00003391
Iteration 238/1000 | Loss: 0.00003391
Iteration 239/1000 | Loss: 0.00003391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [3.391050995560363e-05, 3.391050995560363e-05, 3.391050995560363e-05, 3.391050995560363e-05, 3.391050995560363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.391050995560363e-05

Optimization complete. Final v2v error: 4.819293975830078 mm

Highest mean error: 6.137209892272949 mm for frame 119

Lowest mean error: 4.02271032333374 mm for frame 2

Saving results

Total time: 57.590734243392944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460624
Iteration 2/25 | Loss: 0.00101933
Iteration 3/25 | Loss: 0.00093032
Iteration 4/25 | Loss: 0.00090641
Iteration 5/25 | Loss: 0.00090103
Iteration 6/25 | Loss: 0.00089997
Iteration 7/25 | Loss: 0.00089986
Iteration 8/25 | Loss: 0.00089986
Iteration 9/25 | Loss: 0.00089986
Iteration 10/25 | Loss: 0.00089986
Iteration 11/25 | Loss: 0.00089986
Iteration 12/25 | Loss: 0.00089986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008998640114441514, 0.0008998640114441514, 0.0008998640114441514, 0.0008998640114441514, 0.0008998640114441514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008998640114441514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48234951
Iteration 2/25 | Loss: 0.00057846
Iteration 3/25 | Loss: 0.00057846
Iteration 4/25 | Loss: 0.00057845
Iteration 5/25 | Loss: 0.00057845
Iteration 6/25 | Loss: 0.00057845
Iteration 7/25 | Loss: 0.00057845
Iteration 8/25 | Loss: 0.00057845
Iteration 9/25 | Loss: 0.00057845
Iteration 10/25 | Loss: 0.00057845
Iteration 11/25 | Loss: 0.00057845
Iteration 12/25 | Loss: 0.00057845
Iteration 13/25 | Loss: 0.00057845
Iteration 14/25 | Loss: 0.00057845
Iteration 15/25 | Loss: 0.00057845
Iteration 16/25 | Loss: 0.00057845
Iteration 17/25 | Loss: 0.00057845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005784521927125752, 0.0005784521927125752, 0.0005784521927125752, 0.0005784521927125752, 0.0005784521927125752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005784521927125752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057845
Iteration 2/1000 | Loss: 0.00004620
Iteration 3/1000 | Loss: 0.00003440
Iteration 4/1000 | Loss: 0.00003224
Iteration 5/1000 | Loss: 0.00003041
Iteration 6/1000 | Loss: 0.00002946
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002805
Iteration 9/1000 | Loss: 0.00002767
Iteration 10/1000 | Loss: 0.00002738
Iteration 11/1000 | Loss: 0.00002721
Iteration 12/1000 | Loss: 0.00002702
Iteration 13/1000 | Loss: 0.00002694
Iteration 14/1000 | Loss: 0.00002693
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002692
Iteration 17/1000 | Loss: 0.00002691
Iteration 18/1000 | Loss: 0.00002677
Iteration 19/1000 | Loss: 0.00002661
Iteration 20/1000 | Loss: 0.00002658
Iteration 21/1000 | Loss: 0.00002656
Iteration 22/1000 | Loss: 0.00002655
Iteration 23/1000 | Loss: 0.00002655
Iteration 24/1000 | Loss: 0.00002654
Iteration 25/1000 | Loss: 0.00002654
Iteration 26/1000 | Loss: 0.00002653
Iteration 27/1000 | Loss: 0.00002653
Iteration 28/1000 | Loss: 0.00002652
Iteration 29/1000 | Loss: 0.00002652
Iteration 30/1000 | Loss: 0.00002652
Iteration 31/1000 | Loss: 0.00002652
Iteration 32/1000 | Loss: 0.00002651
Iteration 33/1000 | Loss: 0.00002651
Iteration 34/1000 | Loss: 0.00002651
Iteration 35/1000 | Loss: 0.00002651
Iteration 36/1000 | Loss: 0.00002650
Iteration 37/1000 | Loss: 0.00002650
Iteration 38/1000 | Loss: 0.00002649
Iteration 39/1000 | Loss: 0.00002649
Iteration 40/1000 | Loss: 0.00002649
Iteration 41/1000 | Loss: 0.00002648
Iteration 42/1000 | Loss: 0.00002648
Iteration 43/1000 | Loss: 0.00002648
Iteration 44/1000 | Loss: 0.00002648
Iteration 45/1000 | Loss: 0.00002647
Iteration 46/1000 | Loss: 0.00002647
Iteration 47/1000 | Loss: 0.00002647
Iteration 48/1000 | Loss: 0.00002647
Iteration 49/1000 | Loss: 0.00002647
Iteration 50/1000 | Loss: 0.00002646
Iteration 51/1000 | Loss: 0.00002646
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00002646
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002646
Iteration 56/1000 | Loss: 0.00002646
Iteration 57/1000 | Loss: 0.00002645
Iteration 58/1000 | Loss: 0.00002645
Iteration 59/1000 | Loss: 0.00002645
Iteration 60/1000 | Loss: 0.00002645
Iteration 61/1000 | Loss: 0.00002645
Iteration 62/1000 | Loss: 0.00002644
Iteration 63/1000 | Loss: 0.00002644
Iteration 64/1000 | Loss: 0.00002644
Iteration 65/1000 | Loss: 0.00002643
Iteration 66/1000 | Loss: 0.00002643
Iteration 67/1000 | Loss: 0.00002642
Iteration 68/1000 | Loss: 0.00002642
Iteration 69/1000 | Loss: 0.00002641
Iteration 70/1000 | Loss: 0.00002641
Iteration 71/1000 | Loss: 0.00002641
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002640
Iteration 74/1000 | Loss: 0.00002639
Iteration 75/1000 | Loss: 0.00002639
Iteration 76/1000 | Loss: 0.00002639
Iteration 77/1000 | Loss: 0.00002639
Iteration 78/1000 | Loss: 0.00002639
Iteration 79/1000 | Loss: 0.00002639
Iteration 80/1000 | Loss: 0.00002639
Iteration 81/1000 | Loss: 0.00002639
Iteration 82/1000 | Loss: 0.00002639
Iteration 83/1000 | Loss: 0.00002638
Iteration 84/1000 | Loss: 0.00002638
Iteration 85/1000 | Loss: 0.00002638
Iteration 86/1000 | Loss: 0.00002638
Iteration 87/1000 | Loss: 0.00002637
Iteration 88/1000 | Loss: 0.00002637
Iteration 89/1000 | Loss: 0.00002637
Iteration 90/1000 | Loss: 0.00002637
Iteration 91/1000 | Loss: 0.00002637
Iteration 92/1000 | Loss: 0.00002637
Iteration 93/1000 | Loss: 0.00002637
Iteration 94/1000 | Loss: 0.00002636
Iteration 95/1000 | Loss: 0.00002636
Iteration 96/1000 | Loss: 0.00002636
Iteration 97/1000 | Loss: 0.00002636
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002636
Iteration 101/1000 | Loss: 0.00002636
Iteration 102/1000 | Loss: 0.00002636
Iteration 103/1000 | Loss: 0.00002636
Iteration 104/1000 | Loss: 0.00002636
Iteration 105/1000 | Loss: 0.00002636
Iteration 106/1000 | Loss: 0.00002636
Iteration 107/1000 | Loss: 0.00002635
Iteration 108/1000 | Loss: 0.00002635
Iteration 109/1000 | Loss: 0.00002635
Iteration 110/1000 | Loss: 0.00002635
Iteration 111/1000 | Loss: 0.00002635
Iteration 112/1000 | Loss: 0.00002635
Iteration 113/1000 | Loss: 0.00002635
Iteration 114/1000 | Loss: 0.00002635
Iteration 115/1000 | Loss: 0.00002635
Iteration 116/1000 | Loss: 0.00002634
Iteration 117/1000 | Loss: 0.00002634
Iteration 118/1000 | Loss: 0.00002634
Iteration 119/1000 | Loss: 0.00002634
Iteration 120/1000 | Loss: 0.00002634
Iteration 121/1000 | Loss: 0.00002634
Iteration 122/1000 | Loss: 0.00002634
Iteration 123/1000 | Loss: 0.00002634
Iteration 124/1000 | Loss: 0.00002634
Iteration 125/1000 | Loss: 0.00002634
Iteration 126/1000 | Loss: 0.00002634
Iteration 127/1000 | Loss: 0.00002634
Iteration 128/1000 | Loss: 0.00002634
Iteration 129/1000 | Loss: 0.00002634
Iteration 130/1000 | Loss: 0.00002634
Iteration 131/1000 | Loss: 0.00002633
Iteration 132/1000 | Loss: 0.00002633
Iteration 133/1000 | Loss: 0.00002633
Iteration 134/1000 | Loss: 0.00002633
Iteration 135/1000 | Loss: 0.00002633
Iteration 136/1000 | Loss: 0.00002633
Iteration 137/1000 | Loss: 0.00002633
Iteration 138/1000 | Loss: 0.00002633
Iteration 139/1000 | Loss: 0.00002633
Iteration 140/1000 | Loss: 0.00002633
Iteration 141/1000 | Loss: 0.00002633
Iteration 142/1000 | Loss: 0.00002633
Iteration 143/1000 | Loss: 0.00002633
Iteration 144/1000 | Loss: 0.00002633
Iteration 145/1000 | Loss: 0.00002633
Iteration 146/1000 | Loss: 0.00002633
Iteration 147/1000 | Loss: 0.00002633
Iteration 148/1000 | Loss: 0.00002633
Iteration 149/1000 | Loss: 0.00002633
Iteration 150/1000 | Loss: 0.00002633
Iteration 151/1000 | Loss: 0.00002633
Iteration 152/1000 | Loss: 0.00002633
Iteration 153/1000 | Loss: 0.00002633
Iteration 154/1000 | Loss: 0.00002633
Iteration 155/1000 | Loss: 0.00002633
Iteration 156/1000 | Loss: 0.00002633
Iteration 157/1000 | Loss: 0.00002633
Iteration 158/1000 | Loss: 0.00002633
Iteration 159/1000 | Loss: 0.00002633
Iteration 160/1000 | Loss: 0.00002633
Iteration 161/1000 | Loss: 0.00002633
Iteration 162/1000 | Loss: 0.00002633
Iteration 163/1000 | Loss: 0.00002633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.632913674460724e-05, 2.632913674460724e-05, 2.632913674460724e-05, 2.632913674460724e-05, 2.632913674460724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.632913674460724e-05

Optimization complete. Final v2v error: 4.281787872314453 mm

Highest mean error: 4.490023136138916 mm for frame 123

Lowest mean error: 3.9919493198394775 mm for frame 45

Saving results

Total time: 39.22259211540222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00561954
Iteration 2/25 | Loss: 0.00115645
Iteration 3/25 | Loss: 0.00091100
Iteration 4/25 | Loss: 0.00085668
Iteration 5/25 | Loss: 0.00085808
Iteration 6/25 | Loss: 0.00084282
Iteration 7/25 | Loss: 0.00084592
Iteration 8/25 | Loss: 0.00084815
Iteration 9/25 | Loss: 0.00084532
Iteration 10/25 | Loss: 0.00084048
Iteration 11/25 | Loss: 0.00084043
Iteration 12/25 | Loss: 0.00084043
Iteration 13/25 | Loss: 0.00084043
Iteration 14/25 | Loss: 0.00084042
Iteration 15/25 | Loss: 0.00084042
Iteration 16/25 | Loss: 0.00084042
Iteration 17/25 | Loss: 0.00084042
Iteration 18/25 | Loss: 0.00084042
Iteration 19/25 | Loss: 0.00084042
Iteration 20/25 | Loss: 0.00084042
Iteration 21/25 | Loss: 0.00084042
Iteration 22/25 | Loss: 0.00084042
Iteration 23/25 | Loss: 0.00084042
Iteration 24/25 | Loss: 0.00084041
Iteration 25/25 | Loss: 0.00084041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.36394882
Iteration 2/25 | Loss: 0.00064420
Iteration 3/25 | Loss: 0.00061596
Iteration 4/25 | Loss: 0.00061596
Iteration 5/25 | Loss: 0.00061596
Iteration 6/25 | Loss: 0.00061596
Iteration 7/25 | Loss: 0.00061596
Iteration 8/25 | Loss: 0.00061596
Iteration 9/25 | Loss: 0.00061596
Iteration 10/25 | Loss: 0.00061596
Iteration 11/25 | Loss: 0.00061596
Iteration 12/25 | Loss: 0.00061596
Iteration 13/25 | Loss: 0.00061596
Iteration 14/25 | Loss: 0.00061596
Iteration 15/25 | Loss: 0.00061596
Iteration 16/25 | Loss: 0.00061596
Iteration 17/25 | Loss: 0.00061596
Iteration 18/25 | Loss: 0.00061596
Iteration 19/25 | Loss: 0.00061596
Iteration 20/25 | Loss: 0.00061596
Iteration 21/25 | Loss: 0.00061596
Iteration 22/25 | Loss: 0.00061596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000615960918366909, 0.000615960918366909, 0.000615960918366909, 0.000615960918366909, 0.000615960918366909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000615960918366909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061596
Iteration 2/1000 | Loss: 0.00005405
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00006006
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00004566
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00002313
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00004380
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001265
Iteration 15/1000 | Loss: 0.00001264
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001263
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001259
Iteration 25/1000 | Loss: 0.00004909
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001250
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00004909
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.2375011465337593e-05, 1.2375011465337593e-05, 1.2375011465337593e-05, 1.2375011465337593e-05, 1.2375011465337593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2375011465337593e-05

Optimization complete. Final v2v error: 2.9980664253234863 mm

Highest mean error: 3.4659342765808105 mm for frame 194

Lowest mean error: 2.7162187099456787 mm for frame 104

Saving results

Total time: 49.41554141044617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847160
Iteration 2/25 | Loss: 0.00127092
Iteration 3/25 | Loss: 0.00094661
Iteration 4/25 | Loss: 0.00090000
Iteration 5/25 | Loss: 0.00089317
Iteration 6/25 | Loss: 0.00089228
Iteration 7/25 | Loss: 0.00089228
Iteration 8/25 | Loss: 0.00089228
Iteration 9/25 | Loss: 0.00089228
Iteration 10/25 | Loss: 0.00089228
Iteration 11/25 | Loss: 0.00089228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000892279960680753, 0.000892279960680753, 0.000892279960680753, 0.000892279960680753, 0.000892279960680753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000892279960680753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08071840
Iteration 2/25 | Loss: 0.00051526
Iteration 3/25 | Loss: 0.00051525
Iteration 4/25 | Loss: 0.00051525
Iteration 5/25 | Loss: 0.00051524
Iteration 6/25 | Loss: 0.00051524
Iteration 7/25 | Loss: 0.00051524
Iteration 8/25 | Loss: 0.00051524
Iteration 9/25 | Loss: 0.00051524
Iteration 10/25 | Loss: 0.00051524
Iteration 11/25 | Loss: 0.00051524
Iteration 12/25 | Loss: 0.00051524
Iteration 13/25 | Loss: 0.00051524
Iteration 14/25 | Loss: 0.00051524
Iteration 15/25 | Loss: 0.00051524
Iteration 16/25 | Loss: 0.00051524
Iteration 17/25 | Loss: 0.00051524
Iteration 18/25 | Loss: 0.00051524
Iteration 19/25 | Loss: 0.00051524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005152432713657618, 0.0005152432713657618, 0.0005152432713657618, 0.0005152432713657618, 0.0005152432713657618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005152432713657618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051524
Iteration 2/1000 | Loss: 0.00002895
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001994
Iteration 7/1000 | Loss: 0.00001930
Iteration 8/1000 | Loss: 0.00001890
Iteration 9/1000 | Loss: 0.00001863
Iteration 10/1000 | Loss: 0.00001861
Iteration 11/1000 | Loss: 0.00001841
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001829
Iteration 14/1000 | Loss: 0.00001824
Iteration 15/1000 | Loss: 0.00001824
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001824
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001823
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001820
Iteration 23/1000 | Loss: 0.00001820
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001815
Iteration 26/1000 | Loss: 0.00001815
Iteration 27/1000 | Loss: 0.00001812
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001812
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001812
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001812
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001806
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001805
Iteration 64/1000 | Loss: 0.00001805
Iteration 65/1000 | Loss: 0.00001805
Iteration 66/1000 | Loss: 0.00001805
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001805
Iteration 75/1000 | Loss: 0.00001805
Iteration 76/1000 | Loss: 0.00001805
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001805
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.8050352082354948e-05, 1.8050352082354948e-05, 1.8050352082354948e-05, 1.8050352082354948e-05, 1.8050352082354948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8050352082354948e-05

Optimization complete. Final v2v error: 3.632162094116211 mm

Highest mean error: 4.218143939971924 mm for frame 1

Lowest mean error: 3.2953298091888428 mm for frame 51

Saving results

Total time: 30.89000368118286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839721
Iteration 2/25 | Loss: 0.00139788
Iteration 3/25 | Loss: 0.00102660
Iteration 4/25 | Loss: 0.00095067
Iteration 5/25 | Loss: 0.00092625
Iteration 6/25 | Loss: 0.00091974
Iteration 7/25 | Loss: 0.00091774
Iteration 8/25 | Loss: 0.00091721
Iteration 9/25 | Loss: 0.00091697
Iteration 10/25 | Loss: 0.00091683
Iteration 11/25 | Loss: 0.00091672
Iteration 12/25 | Loss: 0.00091664
Iteration 13/25 | Loss: 0.00091657
Iteration 14/25 | Loss: 0.00091971
Iteration 15/25 | Loss: 0.00091798
Iteration 16/25 | Loss: 0.00091686
Iteration 17/25 | Loss: 0.00091652
Iteration 18/25 | Loss: 0.00091603
Iteration 19/25 | Loss: 0.00091461
Iteration 20/25 | Loss: 0.00091361
Iteration 21/25 | Loss: 0.00091316
Iteration 22/25 | Loss: 0.00091299
Iteration 23/25 | Loss: 0.00091299
Iteration 24/25 | Loss: 0.00091298
Iteration 25/25 | Loss: 0.00091298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.80787277
Iteration 2/25 | Loss: 0.00062879
Iteration 3/25 | Loss: 0.00062873
Iteration 4/25 | Loss: 0.00062872
Iteration 5/25 | Loss: 0.00062872
Iteration 6/25 | Loss: 0.00062872
Iteration 7/25 | Loss: 0.00062872
Iteration 8/25 | Loss: 0.00062872
Iteration 9/25 | Loss: 0.00062872
Iteration 10/25 | Loss: 0.00062872
Iteration 11/25 | Loss: 0.00062872
Iteration 12/25 | Loss: 0.00062872
Iteration 13/25 | Loss: 0.00062872
Iteration 14/25 | Loss: 0.00062872
Iteration 15/25 | Loss: 0.00062872
Iteration 16/25 | Loss: 0.00062872
Iteration 17/25 | Loss: 0.00062872
Iteration 18/25 | Loss: 0.00062872
Iteration 19/25 | Loss: 0.00062872
Iteration 20/25 | Loss: 0.00062872
Iteration 21/25 | Loss: 0.00062872
Iteration 22/25 | Loss: 0.00062872
Iteration 23/25 | Loss: 0.00062872
Iteration 24/25 | Loss: 0.00062872
Iteration 25/25 | Loss: 0.00062872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062872
Iteration 2/1000 | Loss: 0.00005935
Iteration 3/1000 | Loss: 0.00004061
Iteration 4/1000 | Loss: 0.00003444
Iteration 5/1000 | Loss: 0.00003193
Iteration 6/1000 | Loss: 0.00003027
Iteration 7/1000 | Loss: 0.00002917
Iteration 8/1000 | Loss: 0.00002842
Iteration 9/1000 | Loss: 0.00002786
Iteration 10/1000 | Loss: 0.00002748
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00002702
Iteration 13/1000 | Loss: 0.00002685
Iteration 14/1000 | Loss: 0.00002677
Iteration 15/1000 | Loss: 0.00002663
Iteration 16/1000 | Loss: 0.00002650
Iteration 17/1000 | Loss: 0.00002646
Iteration 18/1000 | Loss: 0.00002646
Iteration 19/1000 | Loss: 0.00002644
Iteration 20/1000 | Loss: 0.00002643
Iteration 21/1000 | Loss: 0.00002643
Iteration 22/1000 | Loss: 0.00002643
Iteration 23/1000 | Loss: 0.00002642
Iteration 24/1000 | Loss: 0.00002642
Iteration 25/1000 | Loss: 0.00002641
Iteration 26/1000 | Loss: 0.00002641
Iteration 27/1000 | Loss: 0.00002640
Iteration 28/1000 | Loss: 0.00002640
Iteration 29/1000 | Loss: 0.00002640
Iteration 30/1000 | Loss: 0.00002640
Iteration 31/1000 | Loss: 0.00002639
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002639
Iteration 34/1000 | Loss: 0.00002638
Iteration 35/1000 | Loss: 0.00002638
Iteration 36/1000 | Loss: 0.00002638
Iteration 37/1000 | Loss: 0.00002638
Iteration 38/1000 | Loss: 0.00002637
Iteration 39/1000 | Loss: 0.00002637
Iteration 40/1000 | Loss: 0.00002636
Iteration 41/1000 | Loss: 0.00002635
Iteration 42/1000 | Loss: 0.00002634
Iteration 43/1000 | Loss: 0.00002634
Iteration 44/1000 | Loss: 0.00002634
Iteration 45/1000 | Loss: 0.00002633
Iteration 46/1000 | Loss: 0.00002633
Iteration 47/1000 | Loss: 0.00002633
Iteration 48/1000 | Loss: 0.00002633
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002633
Iteration 53/1000 | Loss: 0.00002633
Iteration 54/1000 | Loss: 0.00002633
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002632
Iteration 58/1000 | Loss: 0.00002631
Iteration 59/1000 | Loss: 0.00002631
Iteration 60/1000 | Loss: 0.00002631
Iteration 61/1000 | Loss: 0.00002631
Iteration 62/1000 | Loss: 0.00002631
Iteration 63/1000 | Loss: 0.00002631
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002630
Iteration 68/1000 | Loss: 0.00002630
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002629
Iteration 76/1000 | Loss: 0.00002628
Iteration 77/1000 | Loss: 0.00002628
Iteration 78/1000 | Loss: 0.00002628
Iteration 79/1000 | Loss: 0.00002628
Iteration 80/1000 | Loss: 0.00002628
Iteration 81/1000 | Loss: 0.00002628
Iteration 82/1000 | Loss: 0.00002627
Iteration 83/1000 | Loss: 0.00002627
Iteration 84/1000 | Loss: 0.00002627
Iteration 85/1000 | Loss: 0.00002627
Iteration 86/1000 | Loss: 0.00002626
Iteration 87/1000 | Loss: 0.00002626
Iteration 88/1000 | Loss: 0.00002626
Iteration 89/1000 | Loss: 0.00002626
Iteration 90/1000 | Loss: 0.00002626
Iteration 91/1000 | Loss: 0.00002626
Iteration 92/1000 | Loss: 0.00002625
Iteration 93/1000 | Loss: 0.00002625
Iteration 94/1000 | Loss: 0.00002625
Iteration 95/1000 | Loss: 0.00002624
Iteration 96/1000 | Loss: 0.00002624
Iteration 97/1000 | Loss: 0.00002624
Iteration 98/1000 | Loss: 0.00002624
Iteration 99/1000 | Loss: 0.00002624
Iteration 100/1000 | Loss: 0.00002624
Iteration 101/1000 | Loss: 0.00002624
Iteration 102/1000 | Loss: 0.00002623
Iteration 103/1000 | Loss: 0.00002623
Iteration 104/1000 | Loss: 0.00002623
Iteration 105/1000 | Loss: 0.00002623
Iteration 106/1000 | Loss: 0.00002623
Iteration 107/1000 | Loss: 0.00002623
Iteration 108/1000 | Loss: 0.00002623
Iteration 109/1000 | Loss: 0.00002622
Iteration 110/1000 | Loss: 0.00002622
Iteration 111/1000 | Loss: 0.00002622
Iteration 112/1000 | Loss: 0.00002622
Iteration 113/1000 | Loss: 0.00002621
Iteration 114/1000 | Loss: 0.00002621
Iteration 115/1000 | Loss: 0.00002621
Iteration 116/1000 | Loss: 0.00002621
Iteration 117/1000 | Loss: 0.00002621
Iteration 118/1000 | Loss: 0.00002621
Iteration 119/1000 | Loss: 0.00002621
Iteration 120/1000 | Loss: 0.00002621
Iteration 121/1000 | Loss: 0.00002620
Iteration 122/1000 | Loss: 0.00002620
Iteration 123/1000 | Loss: 0.00002620
Iteration 124/1000 | Loss: 0.00002620
Iteration 125/1000 | Loss: 0.00002620
Iteration 126/1000 | Loss: 0.00002620
Iteration 127/1000 | Loss: 0.00002620
Iteration 128/1000 | Loss: 0.00002620
Iteration 129/1000 | Loss: 0.00002619
Iteration 130/1000 | Loss: 0.00002619
Iteration 131/1000 | Loss: 0.00002619
Iteration 132/1000 | Loss: 0.00002619
Iteration 133/1000 | Loss: 0.00002619
Iteration 134/1000 | Loss: 0.00002619
Iteration 135/1000 | Loss: 0.00002619
Iteration 136/1000 | Loss: 0.00002619
Iteration 137/1000 | Loss: 0.00002619
Iteration 138/1000 | Loss: 0.00002618
Iteration 139/1000 | Loss: 0.00002618
Iteration 140/1000 | Loss: 0.00002618
Iteration 141/1000 | Loss: 0.00002618
Iteration 142/1000 | Loss: 0.00002618
Iteration 143/1000 | Loss: 0.00002618
Iteration 144/1000 | Loss: 0.00002618
Iteration 145/1000 | Loss: 0.00002618
Iteration 146/1000 | Loss: 0.00002618
Iteration 147/1000 | Loss: 0.00002617
Iteration 148/1000 | Loss: 0.00002617
Iteration 149/1000 | Loss: 0.00002617
Iteration 150/1000 | Loss: 0.00002617
Iteration 151/1000 | Loss: 0.00002617
Iteration 152/1000 | Loss: 0.00002617
Iteration 153/1000 | Loss: 0.00002617
Iteration 154/1000 | Loss: 0.00002617
Iteration 155/1000 | Loss: 0.00002617
Iteration 156/1000 | Loss: 0.00002616
Iteration 157/1000 | Loss: 0.00002616
Iteration 158/1000 | Loss: 0.00002616
Iteration 159/1000 | Loss: 0.00002616
Iteration 160/1000 | Loss: 0.00002616
Iteration 161/1000 | Loss: 0.00002616
Iteration 162/1000 | Loss: 0.00002616
Iteration 163/1000 | Loss: 0.00002616
Iteration 164/1000 | Loss: 0.00002616
Iteration 165/1000 | Loss: 0.00002616
Iteration 166/1000 | Loss: 0.00002616
Iteration 167/1000 | Loss: 0.00002616
Iteration 168/1000 | Loss: 0.00002616
Iteration 169/1000 | Loss: 0.00002616
Iteration 170/1000 | Loss: 0.00002616
Iteration 171/1000 | Loss: 0.00002615
Iteration 172/1000 | Loss: 0.00002615
Iteration 173/1000 | Loss: 0.00002615
Iteration 174/1000 | Loss: 0.00002615
Iteration 175/1000 | Loss: 0.00002615
Iteration 176/1000 | Loss: 0.00002615
Iteration 177/1000 | Loss: 0.00002615
Iteration 178/1000 | Loss: 0.00002615
Iteration 179/1000 | Loss: 0.00002615
Iteration 180/1000 | Loss: 0.00002615
Iteration 181/1000 | Loss: 0.00002615
Iteration 182/1000 | Loss: 0.00002615
Iteration 183/1000 | Loss: 0.00002615
Iteration 184/1000 | Loss: 0.00002614
Iteration 185/1000 | Loss: 0.00002614
Iteration 186/1000 | Loss: 0.00002614
Iteration 187/1000 | Loss: 0.00002614
Iteration 188/1000 | Loss: 0.00002614
Iteration 189/1000 | Loss: 0.00002614
Iteration 190/1000 | Loss: 0.00002614
Iteration 191/1000 | Loss: 0.00002614
Iteration 192/1000 | Loss: 0.00002614
Iteration 193/1000 | Loss: 0.00002614
Iteration 194/1000 | Loss: 0.00002614
Iteration 195/1000 | Loss: 0.00002614
Iteration 196/1000 | Loss: 0.00002614
Iteration 197/1000 | Loss: 0.00002614
Iteration 198/1000 | Loss: 0.00002614
Iteration 199/1000 | Loss: 0.00002614
Iteration 200/1000 | Loss: 0.00002614
Iteration 201/1000 | Loss: 0.00002614
Iteration 202/1000 | Loss: 0.00002614
Iteration 203/1000 | Loss: 0.00002614
Iteration 204/1000 | Loss: 0.00002614
Iteration 205/1000 | Loss: 0.00002614
Iteration 206/1000 | Loss: 0.00002614
Iteration 207/1000 | Loss: 0.00002614
Iteration 208/1000 | Loss: 0.00002614
Iteration 209/1000 | Loss: 0.00002614
Iteration 210/1000 | Loss: 0.00002614
Iteration 211/1000 | Loss: 0.00002614
Iteration 212/1000 | Loss: 0.00002614
Iteration 213/1000 | Loss: 0.00002614
Iteration 214/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.6137378881685436e-05, 2.6137378881685436e-05, 2.6137378881685436e-05, 2.6137378881685436e-05, 2.6137378881685436e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6137378881685436e-05

Optimization complete. Final v2v error: 4.286855697631836 mm

Highest mean error: 6.077346324920654 mm for frame 95

Lowest mean error: 3.098763942718506 mm for frame 28

Saving results

Total time: 70.49365830421448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619540
Iteration 2/25 | Loss: 0.00167241
Iteration 3/25 | Loss: 0.00104830
Iteration 4/25 | Loss: 0.00100986
Iteration 5/25 | Loss: 0.00100335
Iteration 6/25 | Loss: 0.00100013
Iteration 7/25 | Loss: 0.00099958
Iteration 8/25 | Loss: 0.00099958
Iteration 9/25 | Loss: 0.00099958
Iteration 10/25 | Loss: 0.00099958
Iteration 11/25 | Loss: 0.00099958
Iteration 12/25 | Loss: 0.00099958
Iteration 13/25 | Loss: 0.00099958
Iteration 14/25 | Loss: 0.00099958
Iteration 15/25 | Loss: 0.00099958
Iteration 16/25 | Loss: 0.00099958
Iteration 17/25 | Loss: 0.00099958
Iteration 18/25 | Loss: 0.00099958
Iteration 19/25 | Loss: 0.00099953
Iteration 20/25 | Loss: 0.00099953
Iteration 21/25 | Loss: 0.00099953
Iteration 22/25 | Loss: 0.00099953
Iteration 23/25 | Loss: 0.00099953
Iteration 24/25 | Loss: 0.00099953
Iteration 25/25 | Loss: 0.00099953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12342739
Iteration 2/25 | Loss: 0.00070699
Iteration 3/25 | Loss: 0.00070697
Iteration 4/25 | Loss: 0.00070697
Iteration 5/25 | Loss: 0.00070697
Iteration 6/25 | Loss: 0.00070697
Iteration 7/25 | Loss: 0.00070697
Iteration 8/25 | Loss: 0.00070697
Iteration 9/25 | Loss: 0.00070697
Iteration 10/25 | Loss: 0.00070697
Iteration 11/25 | Loss: 0.00070697
Iteration 12/25 | Loss: 0.00070697
Iteration 13/25 | Loss: 0.00070697
Iteration 14/25 | Loss: 0.00070697
Iteration 15/25 | Loss: 0.00070697
Iteration 16/25 | Loss: 0.00070697
Iteration 17/25 | Loss: 0.00070697
Iteration 18/25 | Loss: 0.00070697
Iteration 19/25 | Loss: 0.00070697
Iteration 20/25 | Loss: 0.00070697
Iteration 21/25 | Loss: 0.00070697
Iteration 22/25 | Loss: 0.00070697
Iteration 23/25 | Loss: 0.00070697
Iteration 24/25 | Loss: 0.00070697
Iteration 25/25 | Loss: 0.00070697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070697
Iteration 2/1000 | Loss: 0.00006460
Iteration 3/1000 | Loss: 0.00004609
Iteration 4/1000 | Loss: 0.00004141
Iteration 5/1000 | Loss: 0.00003976
Iteration 6/1000 | Loss: 0.00003863
Iteration 7/1000 | Loss: 0.00003786
Iteration 8/1000 | Loss: 0.00003701
Iteration 9/1000 | Loss: 0.00003640
Iteration 10/1000 | Loss: 0.00003603
Iteration 11/1000 | Loss: 0.00003565
Iteration 12/1000 | Loss: 0.00003521
Iteration 13/1000 | Loss: 0.00003485
Iteration 14/1000 | Loss: 0.00003460
Iteration 15/1000 | Loss: 0.00003436
Iteration 16/1000 | Loss: 0.00003413
Iteration 17/1000 | Loss: 0.00003388
Iteration 18/1000 | Loss: 0.00003363
Iteration 19/1000 | Loss: 0.00003349
Iteration 20/1000 | Loss: 0.00003346
Iteration 21/1000 | Loss: 0.00003334
Iteration 22/1000 | Loss: 0.00003327
Iteration 23/1000 | Loss: 0.00003312
Iteration 24/1000 | Loss: 0.00003304
Iteration 25/1000 | Loss: 0.00003304
Iteration 26/1000 | Loss: 0.00003303
Iteration 27/1000 | Loss: 0.00003303
Iteration 28/1000 | Loss: 0.00003301
Iteration 29/1000 | Loss: 0.00003301
Iteration 30/1000 | Loss: 0.00003300
Iteration 31/1000 | Loss: 0.00003298
Iteration 32/1000 | Loss: 0.00003298
Iteration 33/1000 | Loss: 0.00003297
Iteration 34/1000 | Loss: 0.00003297
Iteration 35/1000 | Loss: 0.00003297
Iteration 36/1000 | Loss: 0.00003296
Iteration 37/1000 | Loss: 0.00003296
Iteration 38/1000 | Loss: 0.00003296
Iteration 39/1000 | Loss: 0.00003296
Iteration 40/1000 | Loss: 0.00003296
Iteration 41/1000 | Loss: 0.00003296
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003296
Iteration 44/1000 | Loss: 0.00003296
Iteration 45/1000 | Loss: 0.00003296
Iteration 46/1000 | Loss: 0.00003296
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003294
Iteration 49/1000 | Loss: 0.00003294
Iteration 50/1000 | Loss: 0.00003294
Iteration 51/1000 | Loss: 0.00003294
Iteration 52/1000 | Loss: 0.00003293
Iteration 53/1000 | Loss: 0.00003293
Iteration 54/1000 | Loss: 0.00003293
Iteration 55/1000 | Loss: 0.00003293
Iteration 56/1000 | Loss: 0.00003293
Iteration 57/1000 | Loss: 0.00003293
Iteration 58/1000 | Loss: 0.00003293
Iteration 59/1000 | Loss: 0.00003293
Iteration 60/1000 | Loss: 0.00003293
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003293
Iteration 63/1000 | Loss: 0.00003293
Iteration 64/1000 | Loss: 0.00003293
Iteration 65/1000 | Loss: 0.00003293
Iteration 66/1000 | Loss: 0.00003293
Iteration 67/1000 | Loss: 0.00003293
Iteration 68/1000 | Loss: 0.00003293
Iteration 69/1000 | Loss: 0.00003293
Iteration 70/1000 | Loss: 0.00003293
Iteration 71/1000 | Loss: 0.00003293
Iteration 72/1000 | Loss: 0.00003293
Iteration 73/1000 | Loss: 0.00003293
Iteration 74/1000 | Loss: 0.00003293
Iteration 75/1000 | Loss: 0.00003293
Iteration 76/1000 | Loss: 0.00003293
Iteration 77/1000 | Loss: 0.00003293
Iteration 78/1000 | Loss: 0.00003293
Iteration 79/1000 | Loss: 0.00003293
Iteration 80/1000 | Loss: 0.00003293
Iteration 81/1000 | Loss: 0.00003293
Iteration 82/1000 | Loss: 0.00003293
Iteration 83/1000 | Loss: 0.00003293
Iteration 84/1000 | Loss: 0.00003293
Iteration 85/1000 | Loss: 0.00003293
Iteration 86/1000 | Loss: 0.00003293
Iteration 87/1000 | Loss: 0.00003293
Iteration 88/1000 | Loss: 0.00003293
Iteration 89/1000 | Loss: 0.00003293
Iteration 90/1000 | Loss: 0.00003293
Iteration 91/1000 | Loss: 0.00003293
Iteration 92/1000 | Loss: 0.00003293
Iteration 93/1000 | Loss: 0.00003293
Iteration 94/1000 | Loss: 0.00003293
Iteration 95/1000 | Loss: 0.00003293
Iteration 96/1000 | Loss: 0.00003293
Iteration 97/1000 | Loss: 0.00003293
Iteration 98/1000 | Loss: 0.00003293
Iteration 99/1000 | Loss: 0.00003293
Iteration 100/1000 | Loss: 0.00003293
Iteration 101/1000 | Loss: 0.00003293
Iteration 102/1000 | Loss: 0.00003293
Iteration 103/1000 | Loss: 0.00003293
Iteration 104/1000 | Loss: 0.00003293
Iteration 105/1000 | Loss: 0.00003293
Iteration 106/1000 | Loss: 0.00003292
Iteration 107/1000 | Loss: 0.00003292
Iteration 108/1000 | Loss: 0.00003292
Iteration 109/1000 | Loss: 0.00003292
Iteration 110/1000 | Loss: 0.00003292
Iteration 111/1000 | Loss: 0.00003292
Iteration 112/1000 | Loss: 0.00003292
Iteration 113/1000 | Loss: 0.00003292
Iteration 114/1000 | Loss: 0.00003292
Iteration 115/1000 | Loss: 0.00003292
Iteration 116/1000 | Loss: 0.00003292
Iteration 117/1000 | Loss: 0.00003292
Iteration 118/1000 | Loss: 0.00003292
Iteration 119/1000 | Loss: 0.00003292
Iteration 120/1000 | Loss: 0.00003292
Iteration 121/1000 | Loss: 0.00003292
Iteration 122/1000 | Loss: 0.00003292
Iteration 123/1000 | Loss: 0.00003292
Iteration 124/1000 | Loss: 0.00003292
Iteration 125/1000 | Loss: 0.00003292
Iteration 126/1000 | Loss: 0.00003292
Iteration 127/1000 | Loss: 0.00003292
Iteration 128/1000 | Loss: 0.00003292
Iteration 129/1000 | Loss: 0.00003292
Iteration 130/1000 | Loss: 0.00003292
Iteration 131/1000 | Loss: 0.00003292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.2923668186413124e-05, 3.2923668186413124e-05, 3.2923668186413124e-05, 3.2923668186413124e-05, 3.2923668186413124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2923668186413124e-05

Optimization complete. Final v2v error: 4.393590450286865 mm

Highest mean error: 5.529088497161865 mm for frame 147

Lowest mean error: 3.312901020050049 mm for frame 61

Saving results

Total time: 49.031538009643555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057387
Iteration 2/25 | Loss: 0.00148823
Iteration 3/25 | Loss: 0.00105952
Iteration 4/25 | Loss: 0.00100706
Iteration 5/25 | Loss: 0.00099584
Iteration 6/25 | Loss: 0.00099354
Iteration 7/25 | Loss: 0.00099332
Iteration 8/25 | Loss: 0.00099332
Iteration 9/25 | Loss: 0.00099332
Iteration 10/25 | Loss: 0.00099332
Iteration 11/25 | Loss: 0.00099332
Iteration 12/25 | Loss: 0.00099332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009933192050084472, 0.0009933192050084472, 0.0009933192050084472, 0.0009933192050084472, 0.0009933192050084472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009933192050084472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00819421
Iteration 2/25 | Loss: 0.00046426
Iteration 3/25 | Loss: 0.00046423
Iteration 4/25 | Loss: 0.00046423
Iteration 5/25 | Loss: 0.00046423
Iteration 6/25 | Loss: 0.00046423
Iteration 7/25 | Loss: 0.00046423
Iteration 8/25 | Loss: 0.00046423
Iteration 9/25 | Loss: 0.00046423
Iteration 10/25 | Loss: 0.00046423
Iteration 11/25 | Loss: 0.00046423
Iteration 12/25 | Loss: 0.00046423
Iteration 13/25 | Loss: 0.00046423
Iteration 14/25 | Loss: 0.00046423
Iteration 15/25 | Loss: 0.00046423
Iteration 16/25 | Loss: 0.00046423
Iteration 17/25 | Loss: 0.00046423
Iteration 18/25 | Loss: 0.00046423
Iteration 19/25 | Loss: 0.00046423
Iteration 20/25 | Loss: 0.00046423
Iteration 21/25 | Loss: 0.00046423
Iteration 22/25 | Loss: 0.00046423
Iteration 23/25 | Loss: 0.00046423
Iteration 24/25 | Loss: 0.00046423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004642281273845583, 0.0004642281273845583, 0.0004642281273845583, 0.0004642281273845583, 0.0004642281273845583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004642281273845583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046423
Iteration 2/1000 | Loss: 0.00005342
Iteration 3/1000 | Loss: 0.00003705
Iteration 4/1000 | Loss: 0.00003329
Iteration 5/1000 | Loss: 0.00003200
Iteration 6/1000 | Loss: 0.00003121
Iteration 7/1000 | Loss: 0.00003069
Iteration 8/1000 | Loss: 0.00003028
Iteration 9/1000 | Loss: 0.00002997
Iteration 10/1000 | Loss: 0.00002976
Iteration 11/1000 | Loss: 0.00002959
Iteration 12/1000 | Loss: 0.00002945
Iteration 13/1000 | Loss: 0.00002929
Iteration 14/1000 | Loss: 0.00002923
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002918
Iteration 18/1000 | Loss: 0.00002917
Iteration 19/1000 | Loss: 0.00002916
Iteration 20/1000 | Loss: 0.00002915
Iteration 21/1000 | Loss: 0.00002915
Iteration 22/1000 | Loss: 0.00002914
Iteration 23/1000 | Loss: 0.00002914
Iteration 24/1000 | Loss: 0.00002913
Iteration 25/1000 | Loss: 0.00002912
Iteration 26/1000 | Loss: 0.00002912
Iteration 27/1000 | Loss: 0.00002910
Iteration 28/1000 | Loss: 0.00002909
Iteration 29/1000 | Loss: 0.00002905
Iteration 30/1000 | Loss: 0.00002904
Iteration 31/1000 | Loss: 0.00002904
Iteration 32/1000 | Loss: 0.00002903
Iteration 33/1000 | Loss: 0.00002903
Iteration 34/1000 | Loss: 0.00002902
Iteration 35/1000 | Loss: 0.00002901
Iteration 36/1000 | Loss: 0.00002899
Iteration 37/1000 | Loss: 0.00002899
Iteration 38/1000 | Loss: 0.00002899
Iteration 39/1000 | Loss: 0.00002898
Iteration 40/1000 | Loss: 0.00002897
Iteration 41/1000 | Loss: 0.00002896
Iteration 42/1000 | Loss: 0.00002895
Iteration 43/1000 | Loss: 0.00002895
Iteration 44/1000 | Loss: 0.00002894
Iteration 45/1000 | Loss: 0.00002894
Iteration 46/1000 | Loss: 0.00002893
Iteration 47/1000 | Loss: 0.00002893
Iteration 48/1000 | Loss: 0.00002893
Iteration 49/1000 | Loss: 0.00002892
Iteration 50/1000 | Loss: 0.00002892
Iteration 51/1000 | Loss: 0.00002891
Iteration 52/1000 | Loss: 0.00002891
Iteration 53/1000 | Loss: 0.00002891
Iteration 54/1000 | Loss: 0.00002890
Iteration 55/1000 | Loss: 0.00002890
Iteration 56/1000 | Loss: 0.00002890
Iteration 57/1000 | Loss: 0.00002889
Iteration 58/1000 | Loss: 0.00002889
Iteration 59/1000 | Loss: 0.00002889
Iteration 60/1000 | Loss: 0.00002888
Iteration 61/1000 | Loss: 0.00002888
Iteration 62/1000 | Loss: 0.00002887
Iteration 63/1000 | Loss: 0.00002887
Iteration 64/1000 | Loss: 0.00002887
Iteration 65/1000 | Loss: 0.00002885
Iteration 66/1000 | Loss: 0.00002885
Iteration 67/1000 | Loss: 0.00002885
Iteration 68/1000 | Loss: 0.00002885
Iteration 69/1000 | Loss: 0.00002885
Iteration 70/1000 | Loss: 0.00002885
Iteration 71/1000 | Loss: 0.00002885
Iteration 72/1000 | Loss: 0.00002885
Iteration 73/1000 | Loss: 0.00002884
Iteration 74/1000 | Loss: 0.00002884
Iteration 75/1000 | Loss: 0.00002884
Iteration 76/1000 | Loss: 0.00002883
Iteration 77/1000 | Loss: 0.00002882
Iteration 78/1000 | Loss: 0.00002882
Iteration 79/1000 | Loss: 0.00002881
Iteration 80/1000 | Loss: 0.00002881
Iteration 81/1000 | Loss: 0.00002881
Iteration 82/1000 | Loss: 0.00002880
Iteration 83/1000 | Loss: 0.00002880
Iteration 84/1000 | Loss: 0.00002880
Iteration 85/1000 | Loss: 0.00002880
Iteration 86/1000 | Loss: 0.00002880
Iteration 87/1000 | Loss: 0.00002880
Iteration 88/1000 | Loss: 0.00002880
Iteration 89/1000 | Loss: 0.00002879
Iteration 90/1000 | Loss: 0.00002879
Iteration 91/1000 | Loss: 0.00002879
Iteration 92/1000 | Loss: 0.00002879
Iteration 93/1000 | Loss: 0.00002879
Iteration 94/1000 | Loss: 0.00002879
Iteration 95/1000 | Loss: 0.00002879
Iteration 96/1000 | Loss: 0.00002879
Iteration 97/1000 | Loss: 0.00002879
Iteration 98/1000 | Loss: 0.00002878
Iteration 99/1000 | Loss: 0.00002877
Iteration 100/1000 | Loss: 0.00002877
Iteration 101/1000 | Loss: 0.00002877
Iteration 102/1000 | Loss: 0.00002876
Iteration 103/1000 | Loss: 0.00002876
Iteration 104/1000 | Loss: 0.00002876
Iteration 105/1000 | Loss: 0.00002875
Iteration 106/1000 | Loss: 0.00002875
Iteration 107/1000 | Loss: 0.00002875
Iteration 108/1000 | Loss: 0.00002875
Iteration 109/1000 | Loss: 0.00002874
Iteration 110/1000 | Loss: 0.00002874
Iteration 111/1000 | Loss: 0.00002874
Iteration 112/1000 | Loss: 0.00002873
Iteration 113/1000 | Loss: 0.00002873
Iteration 114/1000 | Loss: 0.00002873
Iteration 115/1000 | Loss: 0.00002873
Iteration 116/1000 | Loss: 0.00002873
Iteration 117/1000 | Loss: 0.00002872
Iteration 118/1000 | Loss: 0.00002872
Iteration 119/1000 | Loss: 0.00002872
Iteration 120/1000 | Loss: 0.00002872
Iteration 121/1000 | Loss: 0.00002872
Iteration 122/1000 | Loss: 0.00002872
Iteration 123/1000 | Loss: 0.00002871
Iteration 124/1000 | Loss: 0.00002871
Iteration 125/1000 | Loss: 0.00002871
Iteration 126/1000 | Loss: 0.00002871
Iteration 127/1000 | Loss: 0.00002870
Iteration 128/1000 | Loss: 0.00002870
Iteration 129/1000 | Loss: 0.00002870
Iteration 130/1000 | Loss: 0.00002870
Iteration 131/1000 | Loss: 0.00002870
Iteration 132/1000 | Loss: 0.00002870
Iteration 133/1000 | Loss: 0.00002870
Iteration 134/1000 | Loss: 0.00002870
Iteration 135/1000 | Loss: 0.00002870
Iteration 136/1000 | Loss: 0.00002869
Iteration 137/1000 | Loss: 0.00002869
Iteration 138/1000 | Loss: 0.00002869
Iteration 139/1000 | Loss: 0.00002869
Iteration 140/1000 | Loss: 0.00002869
Iteration 141/1000 | Loss: 0.00002869
Iteration 142/1000 | Loss: 0.00002869
Iteration 143/1000 | Loss: 0.00002868
Iteration 144/1000 | Loss: 0.00002868
Iteration 145/1000 | Loss: 0.00002868
Iteration 146/1000 | Loss: 0.00002868
Iteration 147/1000 | Loss: 0.00002868
Iteration 148/1000 | Loss: 0.00002868
Iteration 149/1000 | Loss: 0.00002868
Iteration 150/1000 | Loss: 0.00002868
Iteration 151/1000 | Loss: 0.00002868
Iteration 152/1000 | Loss: 0.00002868
Iteration 153/1000 | Loss: 0.00002868
Iteration 154/1000 | Loss: 0.00002868
Iteration 155/1000 | Loss: 0.00002868
Iteration 156/1000 | Loss: 0.00002867
Iteration 157/1000 | Loss: 0.00002867
Iteration 158/1000 | Loss: 0.00002867
Iteration 159/1000 | Loss: 0.00002866
Iteration 160/1000 | Loss: 0.00002866
Iteration 161/1000 | Loss: 0.00002866
Iteration 162/1000 | Loss: 0.00002866
Iteration 163/1000 | Loss: 0.00002866
Iteration 164/1000 | Loss: 0.00002866
Iteration 165/1000 | Loss: 0.00002866
Iteration 166/1000 | Loss: 0.00002866
Iteration 167/1000 | Loss: 0.00002866
Iteration 168/1000 | Loss: 0.00002866
Iteration 169/1000 | Loss: 0.00002866
Iteration 170/1000 | Loss: 0.00002866
Iteration 171/1000 | Loss: 0.00002866
Iteration 172/1000 | Loss: 0.00002865
Iteration 173/1000 | Loss: 0.00002865
Iteration 174/1000 | Loss: 0.00002865
Iteration 175/1000 | Loss: 0.00002865
Iteration 176/1000 | Loss: 0.00002864
Iteration 177/1000 | Loss: 0.00002864
Iteration 178/1000 | Loss: 0.00002864
Iteration 179/1000 | Loss: 0.00002864
Iteration 180/1000 | Loss: 0.00002864
Iteration 181/1000 | Loss: 0.00002864
Iteration 182/1000 | Loss: 0.00002864
Iteration 183/1000 | Loss: 0.00002863
Iteration 184/1000 | Loss: 0.00002863
Iteration 185/1000 | Loss: 0.00002863
Iteration 186/1000 | Loss: 0.00002863
Iteration 187/1000 | Loss: 0.00002863
Iteration 188/1000 | Loss: 0.00002863
Iteration 189/1000 | Loss: 0.00002863
Iteration 190/1000 | Loss: 0.00002862
Iteration 191/1000 | Loss: 0.00002862
Iteration 192/1000 | Loss: 0.00002862
Iteration 193/1000 | Loss: 0.00002862
Iteration 194/1000 | Loss: 0.00002862
Iteration 195/1000 | Loss: 0.00002862
Iteration 196/1000 | Loss: 0.00002862
Iteration 197/1000 | Loss: 0.00002862
Iteration 198/1000 | Loss: 0.00002862
Iteration 199/1000 | Loss: 0.00002862
Iteration 200/1000 | Loss: 0.00002862
Iteration 201/1000 | Loss: 0.00002862
Iteration 202/1000 | Loss: 0.00002862
Iteration 203/1000 | Loss: 0.00002862
Iteration 204/1000 | Loss: 0.00002861
Iteration 205/1000 | Loss: 0.00002861
Iteration 206/1000 | Loss: 0.00002861
Iteration 207/1000 | Loss: 0.00002861
Iteration 208/1000 | Loss: 0.00002861
Iteration 209/1000 | Loss: 0.00002861
Iteration 210/1000 | Loss: 0.00002861
Iteration 211/1000 | Loss: 0.00002861
Iteration 212/1000 | Loss: 0.00002861
Iteration 213/1000 | Loss: 0.00002861
Iteration 214/1000 | Loss: 0.00002861
Iteration 215/1000 | Loss: 0.00002861
Iteration 216/1000 | Loss: 0.00002861
Iteration 217/1000 | Loss: 0.00002861
Iteration 218/1000 | Loss: 0.00002860
Iteration 219/1000 | Loss: 0.00002860
Iteration 220/1000 | Loss: 0.00002860
Iteration 221/1000 | Loss: 0.00002860
Iteration 222/1000 | Loss: 0.00002860
Iteration 223/1000 | Loss: 0.00002860
Iteration 224/1000 | Loss: 0.00002860
Iteration 225/1000 | Loss: 0.00002860
Iteration 226/1000 | Loss: 0.00002860
Iteration 227/1000 | Loss: 0.00002860
Iteration 228/1000 | Loss: 0.00002860
Iteration 229/1000 | Loss: 0.00002860
Iteration 230/1000 | Loss: 0.00002860
Iteration 231/1000 | Loss: 0.00002860
Iteration 232/1000 | Loss: 0.00002860
Iteration 233/1000 | Loss: 0.00002860
Iteration 234/1000 | Loss: 0.00002860
Iteration 235/1000 | Loss: 0.00002860
Iteration 236/1000 | Loss: 0.00002860
Iteration 237/1000 | Loss: 0.00002860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [2.8598142307600938e-05, 2.8598142307600938e-05, 2.8598142307600938e-05, 2.8598142307600938e-05, 2.8598142307600938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8598142307600938e-05

Optimization complete. Final v2v error: 4.3422770500183105 mm

Highest mean error: 4.861219882965088 mm for frame 66

Lowest mean error: 3.813976526260376 mm for frame 40

Saving results

Total time: 48.651880502700806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041403
Iteration 2/25 | Loss: 0.00251101
Iteration 3/25 | Loss: 0.00260693
Iteration 4/25 | Loss: 0.00221071
Iteration 5/25 | Loss: 0.00186685
Iteration 6/25 | Loss: 0.00172878
Iteration 7/25 | Loss: 0.00170331
Iteration 8/25 | Loss: 0.00146824
Iteration 9/25 | Loss: 0.00134622
Iteration 10/25 | Loss: 0.00121193
Iteration 11/25 | Loss: 0.00118116
Iteration 12/25 | Loss: 0.00119422
Iteration 13/25 | Loss: 0.00114175
Iteration 14/25 | Loss: 0.00111440
Iteration 15/25 | Loss: 0.00108179
Iteration 16/25 | Loss: 0.00108919
Iteration 17/25 | Loss: 0.00108175
Iteration 18/25 | Loss: 0.00106727
Iteration 19/25 | Loss: 0.00106921
Iteration 20/25 | Loss: 0.00106981
Iteration 21/25 | Loss: 0.00106392
Iteration 22/25 | Loss: 0.00105653
Iteration 23/25 | Loss: 0.00105050
Iteration 24/25 | Loss: 0.00105135
Iteration 25/25 | Loss: 0.00104874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52708590
Iteration 2/25 | Loss: 0.00148073
Iteration 3/25 | Loss: 0.00122332
Iteration 4/25 | Loss: 0.00122332
Iteration 5/25 | Loss: 0.00122331
Iteration 6/25 | Loss: 0.00122331
Iteration 7/25 | Loss: 0.00122331
Iteration 8/25 | Loss: 0.00122331
Iteration 9/25 | Loss: 0.00122331
Iteration 10/25 | Loss: 0.00122331
Iteration 11/25 | Loss: 0.00122331
Iteration 12/25 | Loss: 0.00122331
Iteration 13/25 | Loss: 0.00122331
Iteration 14/25 | Loss: 0.00122331
Iteration 15/25 | Loss: 0.00122331
Iteration 16/25 | Loss: 0.00122331
Iteration 17/25 | Loss: 0.00122331
Iteration 18/25 | Loss: 0.00122331
Iteration 19/25 | Loss: 0.00122331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012233132729306817, 0.0012233132729306817, 0.0012233132729306817, 0.0012233132729306817, 0.0012233132729306817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012233132729306817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122331
Iteration 2/1000 | Loss: 0.00177316
Iteration 3/1000 | Loss: 0.00052359
Iteration 4/1000 | Loss: 0.00091042
Iteration 5/1000 | Loss: 0.00114986
Iteration 6/1000 | Loss: 0.00086868
Iteration 7/1000 | Loss: 0.00040852
Iteration 8/1000 | Loss: 0.00012214
Iteration 9/1000 | Loss: 0.00008277
Iteration 10/1000 | Loss: 0.00072322
Iteration 11/1000 | Loss: 0.00005477
Iteration 12/1000 | Loss: 0.00043781
Iteration 13/1000 | Loss: 0.00024077
Iteration 14/1000 | Loss: 0.00022341
Iteration 15/1000 | Loss: 0.00004162
Iteration 16/1000 | Loss: 0.00046380
Iteration 17/1000 | Loss: 0.00078698
Iteration 18/1000 | Loss: 0.00160566
Iteration 19/1000 | Loss: 0.00189716
Iteration 20/1000 | Loss: 0.00041190
Iteration 21/1000 | Loss: 0.00158076
Iteration 22/1000 | Loss: 0.00051855
Iteration 23/1000 | Loss: 0.00045621
Iteration 24/1000 | Loss: 0.00124407
Iteration 25/1000 | Loss: 0.00101590
Iteration 26/1000 | Loss: 0.00034507
Iteration 27/1000 | Loss: 0.00004056
Iteration 28/1000 | Loss: 0.00003264
Iteration 29/1000 | Loss: 0.00017618
Iteration 30/1000 | Loss: 0.00003470
Iteration 31/1000 | Loss: 0.00009605
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003357
Iteration 34/1000 | Loss: 0.00002475
Iteration 35/1000 | Loss: 0.00002415
Iteration 36/1000 | Loss: 0.00074659
Iteration 37/1000 | Loss: 0.00061055
Iteration 38/1000 | Loss: 0.00080623
Iteration 39/1000 | Loss: 0.00016365
Iteration 40/1000 | Loss: 0.00042373
Iteration 41/1000 | Loss: 0.00091679
Iteration 42/1000 | Loss: 0.00003021
Iteration 43/1000 | Loss: 0.00006829
Iteration 44/1000 | Loss: 0.00003493
Iteration 45/1000 | Loss: 0.00005516
Iteration 46/1000 | Loss: 0.00028217
Iteration 47/1000 | Loss: 0.00015552
Iteration 48/1000 | Loss: 0.00002142
Iteration 49/1000 | Loss: 0.00014606
Iteration 50/1000 | Loss: 0.00010856
Iteration 51/1000 | Loss: 0.00023001
Iteration 52/1000 | Loss: 0.00015302
Iteration 53/1000 | Loss: 0.00025832
Iteration 54/1000 | Loss: 0.00016627
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00026350
Iteration 57/1000 | Loss: 0.00010117
Iteration 58/1000 | Loss: 0.00012018
Iteration 59/1000 | Loss: 0.00009322
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00005223
Iteration 62/1000 | Loss: 0.00014383
Iteration 63/1000 | Loss: 0.00005196
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002101
Iteration 66/1000 | Loss: 0.00001963
Iteration 67/1000 | Loss: 0.00001953
Iteration 68/1000 | Loss: 0.00001950
Iteration 69/1000 | Loss: 0.00001950
Iteration 70/1000 | Loss: 0.00001950
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00006817
Iteration 76/1000 | Loss: 0.00001958
Iteration 77/1000 | Loss: 0.00001932
Iteration 78/1000 | Loss: 0.00001932
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00006484
Iteration 86/1000 | Loss: 0.00003189
Iteration 87/1000 | Loss: 0.00019328
Iteration 88/1000 | Loss: 0.00002376
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001922
Iteration 96/1000 | Loss: 0.00001922
Iteration 97/1000 | Loss: 0.00001922
Iteration 98/1000 | Loss: 0.00001922
Iteration 99/1000 | Loss: 0.00001922
Iteration 100/1000 | Loss: 0.00001922
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Iteration 105/1000 | Loss: 0.00001921
Iteration 106/1000 | Loss: 0.00001921
Iteration 107/1000 | Loss: 0.00001921
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001920
Iteration 110/1000 | Loss: 0.00001920
Iteration 111/1000 | Loss: 0.00001920
Iteration 112/1000 | Loss: 0.00001920
Iteration 113/1000 | Loss: 0.00001920
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001919
Iteration 120/1000 | Loss: 0.00001919
Iteration 121/1000 | Loss: 0.00001919
Iteration 122/1000 | Loss: 0.00001919
Iteration 123/1000 | Loss: 0.00001919
Iteration 124/1000 | Loss: 0.00001919
Iteration 125/1000 | Loss: 0.00001919
Iteration 126/1000 | Loss: 0.00001919
Iteration 127/1000 | Loss: 0.00001918
Iteration 128/1000 | Loss: 0.00001918
Iteration 129/1000 | Loss: 0.00001918
Iteration 130/1000 | Loss: 0.00001918
Iteration 131/1000 | Loss: 0.00001918
Iteration 132/1000 | Loss: 0.00001918
Iteration 133/1000 | Loss: 0.00001918
Iteration 134/1000 | Loss: 0.00001918
Iteration 135/1000 | Loss: 0.00001918
Iteration 136/1000 | Loss: 0.00001917
Iteration 137/1000 | Loss: 0.00001917
Iteration 138/1000 | Loss: 0.00001917
Iteration 139/1000 | Loss: 0.00001917
Iteration 140/1000 | Loss: 0.00001917
Iteration 141/1000 | Loss: 0.00001917
Iteration 142/1000 | Loss: 0.00001917
Iteration 143/1000 | Loss: 0.00001917
Iteration 144/1000 | Loss: 0.00001917
Iteration 145/1000 | Loss: 0.00001917
Iteration 146/1000 | Loss: 0.00001917
Iteration 147/1000 | Loss: 0.00001917
Iteration 148/1000 | Loss: 0.00001917
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001917
Iteration 152/1000 | Loss: 0.00001917
Iteration 153/1000 | Loss: 0.00001917
Iteration 154/1000 | Loss: 0.00001917
Iteration 155/1000 | Loss: 0.00001917
Iteration 156/1000 | Loss: 0.00001917
Iteration 157/1000 | Loss: 0.00001917
Iteration 158/1000 | Loss: 0.00001917
Iteration 159/1000 | Loss: 0.00001917
Iteration 160/1000 | Loss: 0.00001917
Iteration 161/1000 | Loss: 0.00001917
Iteration 162/1000 | Loss: 0.00001917
Iteration 163/1000 | Loss: 0.00001917
Iteration 164/1000 | Loss: 0.00001917
Iteration 165/1000 | Loss: 0.00001917
Iteration 166/1000 | Loss: 0.00001917
Iteration 167/1000 | Loss: 0.00001917
Iteration 168/1000 | Loss: 0.00001917
Iteration 169/1000 | Loss: 0.00001917
Iteration 170/1000 | Loss: 0.00001917
Iteration 171/1000 | Loss: 0.00001917
Iteration 172/1000 | Loss: 0.00001917
Iteration 173/1000 | Loss: 0.00001917
Iteration 174/1000 | Loss: 0.00001917
Iteration 175/1000 | Loss: 0.00001917
Iteration 176/1000 | Loss: 0.00001917
Iteration 177/1000 | Loss: 0.00001917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.916754808917176e-05, 1.916754808917176e-05, 1.916754808917176e-05, 1.916754808917176e-05, 1.916754808917176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.916754808917176e-05

Optimization complete. Final v2v error: 3.6738805770874023 mm

Highest mean error: 4.073544025421143 mm for frame 196

Lowest mean error: 3.3375070095062256 mm for frame 100

Saving results

Total time: 165.75298738479614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601887
Iteration 2/25 | Loss: 0.00139805
Iteration 3/25 | Loss: 0.00094059
Iteration 4/25 | Loss: 0.00087447
Iteration 5/25 | Loss: 0.00086002
Iteration 6/25 | Loss: 0.00085691
Iteration 7/25 | Loss: 0.00085632
Iteration 8/25 | Loss: 0.00085632
Iteration 9/25 | Loss: 0.00085632
Iteration 10/25 | Loss: 0.00085632
Iteration 11/25 | Loss: 0.00085632
Iteration 12/25 | Loss: 0.00085632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008563174051232636, 0.0008563174051232636, 0.0008563174051232636, 0.0008563174051232636, 0.0008563174051232636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008563174051232636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36247230
Iteration 2/25 | Loss: 0.00058342
Iteration 3/25 | Loss: 0.00058342
Iteration 4/25 | Loss: 0.00058342
Iteration 5/25 | Loss: 0.00058342
Iteration 6/25 | Loss: 0.00058342
Iteration 7/25 | Loss: 0.00058342
Iteration 8/25 | Loss: 0.00058342
Iteration 9/25 | Loss: 0.00058342
Iteration 10/25 | Loss: 0.00058342
Iteration 11/25 | Loss: 0.00058342
Iteration 12/25 | Loss: 0.00058342
Iteration 13/25 | Loss: 0.00058342
Iteration 14/25 | Loss: 0.00058342
Iteration 15/25 | Loss: 0.00058342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005834223120473325, 0.0005834223120473325, 0.0005834223120473325, 0.0005834223120473325, 0.0005834223120473325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005834223120473325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058342
Iteration 2/1000 | Loss: 0.00002613
Iteration 3/1000 | Loss: 0.00002099
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001556
Iteration 8/1000 | Loss: 0.00001515
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001469
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001457
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001437
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001436
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001434
Iteration 21/1000 | Loss: 0.00001433
Iteration 22/1000 | Loss: 0.00001433
Iteration 23/1000 | Loss: 0.00001433
Iteration 24/1000 | Loss: 0.00001432
Iteration 25/1000 | Loss: 0.00001432
Iteration 26/1000 | Loss: 0.00001432
Iteration 27/1000 | Loss: 0.00001431
Iteration 28/1000 | Loss: 0.00001431
Iteration 29/1000 | Loss: 0.00001431
Iteration 30/1000 | Loss: 0.00001430
Iteration 31/1000 | Loss: 0.00001430
Iteration 32/1000 | Loss: 0.00001429
Iteration 33/1000 | Loss: 0.00001429
Iteration 34/1000 | Loss: 0.00001428
Iteration 35/1000 | Loss: 0.00001428
Iteration 36/1000 | Loss: 0.00001428
Iteration 37/1000 | Loss: 0.00001428
Iteration 38/1000 | Loss: 0.00001427
Iteration 39/1000 | Loss: 0.00001427
Iteration 40/1000 | Loss: 0.00001426
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00001426
Iteration 43/1000 | Loss: 0.00001425
Iteration 44/1000 | Loss: 0.00001425
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001425
Iteration 48/1000 | Loss: 0.00001425
Iteration 49/1000 | Loss: 0.00001425
Iteration 50/1000 | Loss: 0.00001424
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001424
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001422
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001422
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001421
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001419
Iteration 80/1000 | Loss: 0.00001419
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001419
Iteration 86/1000 | Loss: 0.00001419
Iteration 87/1000 | Loss: 0.00001419
Iteration 88/1000 | Loss: 0.00001419
Iteration 89/1000 | Loss: 0.00001419
Iteration 90/1000 | Loss: 0.00001419
Iteration 91/1000 | Loss: 0.00001419
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.4193956303643063e-05, 1.4193956303643063e-05, 1.4193956303643063e-05, 1.4193956303643063e-05, 1.4193956303643063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4193956303643063e-05

Optimization complete. Final v2v error: 3.192420244216919 mm

Highest mean error: 3.8368313312530518 mm for frame 80

Lowest mean error: 3.0604960918426514 mm for frame 178

Saving results

Total time: 34.1581027507782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859090
Iteration 2/25 | Loss: 0.00140594
Iteration 3/25 | Loss: 0.00102258
Iteration 4/25 | Loss: 0.00095307
Iteration 5/25 | Loss: 0.00092365
Iteration 6/25 | Loss: 0.00092073
Iteration 7/25 | Loss: 0.00092047
Iteration 8/25 | Loss: 0.00092047
Iteration 9/25 | Loss: 0.00092047
Iteration 10/25 | Loss: 0.00092047
Iteration 11/25 | Loss: 0.00092047
Iteration 12/25 | Loss: 0.00092047
Iteration 13/25 | Loss: 0.00092047
Iteration 14/25 | Loss: 0.00092047
Iteration 15/25 | Loss: 0.00092047
Iteration 16/25 | Loss: 0.00092047
Iteration 17/25 | Loss: 0.00092047
Iteration 18/25 | Loss: 0.00092047
Iteration 19/25 | Loss: 0.00092047
Iteration 20/25 | Loss: 0.00092047
Iteration 21/25 | Loss: 0.00092047
Iteration 22/25 | Loss: 0.00092047
Iteration 23/25 | Loss: 0.00092047
Iteration 24/25 | Loss: 0.00092047
Iteration 25/25 | Loss: 0.00092047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.83205485
Iteration 2/25 | Loss: 0.00059174
Iteration 3/25 | Loss: 0.00059173
Iteration 4/25 | Loss: 0.00059173
Iteration 5/25 | Loss: 0.00059173
Iteration 6/25 | Loss: 0.00059173
Iteration 7/25 | Loss: 0.00059173
Iteration 8/25 | Loss: 0.00059173
Iteration 9/25 | Loss: 0.00059173
Iteration 10/25 | Loss: 0.00059173
Iteration 11/25 | Loss: 0.00059173
Iteration 12/25 | Loss: 0.00059173
Iteration 13/25 | Loss: 0.00059173
Iteration 14/25 | Loss: 0.00059173
Iteration 15/25 | Loss: 0.00059173
Iteration 16/25 | Loss: 0.00059173
Iteration 17/25 | Loss: 0.00059173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005917312810197473, 0.0005917312810197473, 0.0005917312810197473, 0.0005917312810197473, 0.0005917312810197473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005917312810197473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059173
Iteration 2/1000 | Loss: 0.00003760
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002314
Iteration 6/1000 | Loss: 0.00002236
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002102
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002034
Iteration 17/1000 | Loss: 0.00002032
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002025
Iteration 21/1000 | Loss: 0.00002025
Iteration 22/1000 | Loss: 0.00002022
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002021
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002020
Iteration 28/1000 | Loss: 0.00002020
Iteration 29/1000 | Loss: 0.00002020
Iteration 30/1000 | Loss: 0.00002019
Iteration 31/1000 | Loss: 0.00002019
Iteration 32/1000 | Loss: 0.00002019
Iteration 33/1000 | Loss: 0.00002019
Iteration 34/1000 | Loss: 0.00002018
Iteration 35/1000 | Loss: 0.00002018
Iteration 36/1000 | Loss: 0.00002018
Iteration 37/1000 | Loss: 0.00002018
Iteration 38/1000 | Loss: 0.00002018
Iteration 39/1000 | Loss: 0.00002018
Iteration 40/1000 | Loss: 0.00002018
Iteration 41/1000 | Loss: 0.00002018
Iteration 42/1000 | Loss: 0.00002018
Iteration 43/1000 | Loss: 0.00002018
Iteration 44/1000 | Loss: 0.00002018
Iteration 45/1000 | Loss: 0.00002017
Iteration 46/1000 | Loss: 0.00002017
Iteration 47/1000 | Loss: 0.00002017
Iteration 48/1000 | Loss: 0.00002017
Iteration 49/1000 | Loss: 0.00002017
Iteration 50/1000 | Loss: 0.00002016
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002016
Iteration 53/1000 | Loss: 0.00002015
Iteration 54/1000 | Loss: 0.00002015
Iteration 55/1000 | Loss: 0.00002015
Iteration 56/1000 | Loss: 0.00002014
Iteration 57/1000 | Loss: 0.00002014
Iteration 58/1000 | Loss: 0.00002014
Iteration 59/1000 | Loss: 0.00002013
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00002012
Iteration 62/1000 | Loss: 0.00002012
Iteration 63/1000 | Loss: 0.00002011
Iteration 64/1000 | Loss: 0.00002011
Iteration 65/1000 | Loss: 0.00002011
Iteration 66/1000 | Loss: 0.00002011
Iteration 67/1000 | Loss: 0.00002010
Iteration 68/1000 | Loss: 0.00002010
Iteration 69/1000 | Loss: 0.00002010
Iteration 70/1000 | Loss: 0.00002010
Iteration 71/1000 | Loss: 0.00002009
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002009
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002009
Iteration 80/1000 | Loss: 0.00002009
Iteration 81/1000 | Loss: 0.00002008
Iteration 82/1000 | Loss: 0.00002008
Iteration 83/1000 | Loss: 0.00002008
Iteration 84/1000 | Loss: 0.00002008
Iteration 85/1000 | Loss: 0.00002008
Iteration 86/1000 | Loss: 0.00002008
Iteration 87/1000 | Loss: 0.00002008
Iteration 88/1000 | Loss: 0.00002008
Iteration 89/1000 | Loss: 0.00002008
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002007
Iteration 94/1000 | Loss: 0.00002007
Iteration 95/1000 | Loss: 0.00002007
Iteration 96/1000 | Loss: 0.00002007
Iteration 97/1000 | Loss: 0.00002007
Iteration 98/1000 | Loss: 0.00002007
Iteration 99/1000 | Loss: 0.00002007
Iteration 100/1000 | Loss: 0.00002007
Iteration 101/1000 | Loss: 0.00002007
Iteration 102/1000 | Loss: 0.00002007
Iteration 103/1000 | Loss: 0.00002007
Iteration 104/1000 | Loss: 0.00002006
Iteration 105/1000 | Loss: 0.00002006
Iteration 106/1000 | Loss: 0.00002006
Iteration 107/1000 | Loss: 0.00002006
Iteration 108/1000 | Loss: 0.00002006
Iteration 109/1000 | Loss: 0.00002005
Iteration 110/1000 | Loss: 0.00002005
Iteration 111/1000 | Loss: 0.00002005
Iteration 112/1000 | Loss: 0.00002005
Iteration 113/1000 | Loss: 0.00002005
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002004
Iteration 116/1000 | Loss: 0.00002004
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002004
Iteration 119/1000 | Loss: 0.00002004
Iteration 120/1000 | Loss: 0.00002004
Iteration 121/1000 | Loss: 0.00002004
Iteration 122/1000 | Loss: 0.00002004
Iteration 123/1000 | Loss: 0.00002003
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00002003
Iteration 126/1000 | Loss: 0.00002003
Iteration 127/1000 | Loss: 0.00002003
Iteration 128/1000 | Loss: 0.00002003
Iteration 129/1000 | Loss: 0.00002003
Iteration 130/1000 | Loss: 0.00002003
Iteration 131/1000 | Loss: 0.00002003
Iteration 132/1000 | Loss: 0.00002003
Iteration 133/1000 | Loss: 0.00002003
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002001
Iteration 140/1000 | Loss: 0.00002001
Iteration 141/1000 | Loss: 0.00002001
Iteration 142/1000 | Loss: 0.00002001
Iteration 143/1000 | Loss: 0.00002001
Iteration 144/1000 | Loss: 0.00002001
Iteration 145/1000 | Loss: 0.00002001
Iteration 146/1000 | Loss: 0.00002001
Iteration 147/1000 | Loss: 0.00002001
Iteration 148/1000 | Loss: 0.00002001
Iteration 149/1000 | Loss: 0.00002001
Iteration 150/1000 | Loss: 0.00002001
Iteration 151/1000 | Loss: 0.00002001
Iteration 152/1000 | Loss: 0.00002001
Iteration 153/1000 | Loss: 0.00002001
Iteration 154/1000 | Loss: 0.00002000
Iteration 155/1000 | Loss: 0.00002000
Iteration 156/1000 | Loss: 0.00002000
Iteration 157/1000 | Loss: 0.00002000
Iteration 158/1000 | Loss: 0.00002000
Iteration 159/1000 | Loss: 0.00002000
Iteration 160/1000 | Loss: 0.00002000
Iteration 161/1000 | Loss: 0.00002000
Iteration 162/1000 | Loss: 0.00002000
Iteration 163/1000 | Loss: 0.00002000
Iteration 164/1000 | Loss: 0.00002000
Iteration 165/1000 | Loss: 0.00002000
Iteration 166/1000 | Loss: 0.00002000
Iteration 167/1000 | Loss: 0.00002000
Iteration 168/1000 | Loss: 0.00002000
Iteration 169/1000 | Loss: 0.00001999
Iteration 170/1000 | Loss: 0.00001999
Iteration 171/1000 | Loss: 0.00001999
Iteration 172/1000 | Loss: 0.00001999
Iteration 173/1000 | Loss: 0.00001999
Iteration 174/1000 | Loss: 0.00001999
Iteration 175/1000 | Loss: 0.00001999
Iteration 176/1000 | Loss: 0.00001999
Iteration 177/1000 | Loss: 0.00001999
Iteration 178/1000 | Loss: 0.00001999
Iteration 179/1000 | Loss: 0.00001999
Iteration 180/1000 | Loss: 0.00001999
Iteration 181/1000 | Loss: 0.00001999
Iteration 182/1000 | Loss: 0.00001999
Iteration 183/1000 | Loss: 0.00001999
Iteration 184/1000 | Loss: 0.00001999
Iteration 185/1000 | Loss: 0.00001999
Iteration 186/1000 | Loss: 0.00001999
Iteration 187/1000 | Loss: 0.00001999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.9988885469501838e-05, 1.9988885469501838e-05, 1.9988885469501838e-05, 1.9988885469501838e-05, 1.9988885469501838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9988885469501838e-05

Optimization complete. Final v2v error: 3.743553400039673 mm

Highest mean error: 4.002218246459961 mm for frame 62

Lowest mean error: 3.5509417057037354 mm for frame 111

Saving results

Total time: 39.199275732040405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859224
Iteration 2/25 | Loss: 0.00152417
Iteration 3/25 | Loss: 0.00103023
Iteration 4/25 | Loss: 0.00096859
Iteration 5/25 | Loss: 0.00095896
Iteration 6/25 | Loss: 0.00095220
Iteration 7/25 | Loss: 0.00093631
Iteration 8/25 | Loss: 0.00091982
Iteration 9/25 | Loss: 0.00091697
Iteration 10/25 | Loss: 0.00091641
Iteration 11/25 | Loss: 0.00091609
Iteration 12/25 | Loss: 0.00091664
Iteration 13/25 | Loss: 0.00092009
Iteration 14/25 | Loss: 0.00091410
Iteration 15/25 | Loss: 0.00091206
Iteration 16/25 | Loss: 0.00091157
Iteration 17/25 | Loss: 0.00091135
Iteration 18/25 | Loss: 0.00091073
Iteration 19/25 | Loss: 0.00090966
Iteration 20/25 | Loss: 0.00090952
Iteration 21/25 | Loss: 0.00090951
Iteration 22/25 | Loss: 0.00090951
Iteration 23/25 | Loss: 0.00090951
Iteration 24/25 | Loss: 0.00090951
Iteration 25/25 | Loss: 0.00090951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.62971687
Iteration 2/25 | Loss: 0.00082411
Iteration 3/25 | Loss: 0.00078226
Iteration 4/25 | Loss: 0.00078226
Iteration 5/25 | Loss: 0.00078226
Iteration 6/25 | Loss: 0.00078226
Iteration 7/25 | Loss: 0.00078226
Iteration 8/25 | Loss: 0.00078226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0007822588086128235, 0.0007822588086128235, 0.0007822588086128235, 0.0007822588086128235, 0.0007822588086128235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007822588086128235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078226
Iteration 2/1000 | Loss: 0.00017062
Iteration 3/1000 | Loss: 0.00003091
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002096
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001903
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001812
Iteration 10/1000 | Loss: 0.00001769
Iteration 11/1000 | Loss: 0.00001750
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001725
Iteration 18/1000 | Loss: 0.00001723
Iteration 19/1000 | Loss: 0.00001723
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001723
Iteration 24/1000 | Loss: 0.00001723
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001720
Iteration 31/1000 | Loss: 0.00001720
Iteration 32/1000 | Loss: 0.00001720
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001717
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001708
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.7080154066206887e-05, 1.7080154066206887e-05, 1.7080154066206887e-05, 1.7080154066206887e-05, 1.7080154066206887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7080154066206887e-05

Optimization complete. Final v2v error: 3.510976791381836 mm

Highest mean error: 3.977832794189453 mm for frame 200

Lowest mean error: 3.0199902057647705 mm for frame 203

Saving results

Total time: 66.42365455627441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995506
Iteration 2/25 | Loss: 0.00995506
Iteration 3/25 | Loss: 0.00470682
Iteration 4/25 | Loss: 0.00336260
Iteration 5/25 | Loss: 0.00300488
Iteration 6/25 | Loss: 0.00271140
Iteration 7/25 | Loss: 0.00242261
Iteration 8/25 | Loss: 0.00243993
Iteration 9/25 | Loss: 0.00218535
Iteration 10/25 | Loss: 0.00202243
Iteration 11/25 | Loss: 0.00188001
Iteration 12/25 | Loss: 0.00178138
Iteration 13/25 | Loss: 0.00177460
Iteration 14/25 | Loss: 0.00173333
Iteration 15/25 | Loss: 0.00173184
Iteration 16/25 | Loss: 0.00169184
Iteration 17/25 | Loss: 0.00168981
Iteration 18/25 | Loss: 0.00165717
Iteration 19/25 | Loss: 0.00164239
Iteration 20/25 | Loss: 0.00163123
Iteration 21/25 | Loss: 0.00162799
Iteration 22/25 | Loss: 0.00162304
Iteration 23/25 | Loss: 0.00162794
Iteration 24/25 | Loss: 0.00162144
Iteration 25/25 | Loss: 0.00162119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44488537
Iteration 2/25 | Loss: 0.01822997
Iteration 3/25 | Loss: 0.00514662
Iteration 4/25 | Loss: 0.00472960
Iteration 5/25 | Loss: 0.00472960
Iteration 6/25 | Loss: 0.00472960
Iteration 7/25 | Loss: 0.00472960
Iteration 8/25 | Loss: 0.00472959
Iteration 9/25 | Loss: 0.00472959
Iteration 10/25 | Loss: 0.00472959
Iteration 11/25 | Loss: 0.00472959
Iteration 12/25 | Loss: 0.00472959
Iteration 13/25 | Loss: 0.00472959
Iteration 14/25 | Loss: 0.00472959
Iteration 15/25 | Loss: 0.00472959
Iteration 16/25 | Loss: 0.00472959
Iteration 17/25 | Loss: 0.00472959
Iteration 18/25 | Loss: 0.00472959
Iteration 19/25 | Loss: 0.00472959
Iteration 20/25 | Loss: 0.00472959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004729592707008123, 0.004729592707008123, 0.004729592707008123, 0.004729592707008123, 0.004729592707008123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004729592707008123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00472959
Iteration 2/1000 | Loss: 0.00721324
Iteration 3/1000 | Loss: 0.00072624
Iteration 4/1000 | Loss: 0.00540711
Iteration 5/1000 | Loss: 0.00469530
Iteration 6/1000 | Loss: 0.00099067
Iteration 7/1000 | Loss: 0.00153628
Iteration 8/1000 | Loss: 0.00299269
Iteration 9/1000 | Loss: 0.00235877
Iteration 10/1000 | Loss: 0.00083241
Iteration 11/1000 | Loss: 0.00107882
Iteration 12/1000 | Loss: 0.00806524
Iteration 13/1000 | Loss: 0.01918229
Iteration 14/1000 | Loss: 0.00228213
Iteration 15/1000 | Loss: 0.00267371
Iteration 16/1000 | Loss: 0.00062132
Iteration 17/1000 | Loss: 0.00195894
Iteration 18/1000 | Loss: 0.00432435
Iteration 19/1000 | Loss: 0.00194744
Iteration 20/1000 | Loss: 0.00137486
Iteration 21/1000 | Loss: 0.00467491
Iteration 22/1000 | Loss: 0.00302711
Iteration 23/1000 | Loss: 0.00846293
Iteration 24/1000 | Loss: 0.00073448
Iteration 25/1000 | Loss: 0.00029986
Iteration 26/1000 | Loss: 0.00129671
Iteration 27/1000 | Loss: 0.00006222
Iteration 28/1000 | Loss: 0.00157496
Iteration 29/1000 | Loss: 0.00403306
Iteration 30/1000 | Loss: 0.00008586
Iteration 31/1000 | Loss: 0.00230949
Iteration 32/1000 | Loss: 0.00011384
Iteration 33/1000 | Loss: 0.00021387
Iteration 34/1000 | Loss: 0.00003153
Iteration 35/1000 | Loss: 0.00038396
Iteration 36/1000 | Loss: 0.00002676
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002150
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00041815
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00001885
Iteration 43/1000 | Loss: 0.00016935
Iteration 44/1000 | Loss: 0.00001822
Iteration 45/1000 | Loss: 0.00001762
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001708
Iteration 49/1000 | Loss: 0.00001697
Iteration 50/1000 | Loss: 0.00001694
Iteration 51/1000 | Loss: 0.00001693
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001674
Iteration 56/1000 | Loss: 0.00001674
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001673
Iteration 63/1000 | Loss: 0.00001673
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00042338
Iteration 77/1000 | Loss: 0.00002913
Iteration 78/1000 | Loss: 0.00004380
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001667
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001663
Iteration 85/1000 | Loss: 0.00001663
Iteration 86/1000 | Loss: 0.00001662
Iteration 87/1000 | Loss: 0.00001662
Iteration 88/1000 | Loss: 0.00001662
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001661
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001661
Iteration 96/1000 | Loss: 0.00001661
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001660
Iteration 99/1000 | Loss: 0.00001660
Iteration 100/1000 | Loss: 0.00001660
Iteration 101/1000 | Loss: 0.00001660
Iteration 102/1000 | Loss: 0.00001660
Iteration 103/1000 | Loss: 0.00001660
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001660
Iteration 106/1000 | Loss: 0.00001660
Iteration 107/1000 | Loss: 0.00001660
Iteration 108/1000 | Loss: 0.00001660
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001659
Iteration 111/1000 | Loss: 0.00001659
Iteration 112/1000 | Loss: 0.00001659
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001659
Iteration 115/1000 | Loss: 0.00001659
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001659
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001658
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Iteration 139/1000 | Loss: 0.00001658
Iteration 140/1000 | Loss: 0.00001658
Iteration 141/1000 | Loss: 0.00001658
Iteration 142/1000 | Loss: 0.00001658
Iteration 143/1000 | Loss: 0.00001658
Iteration 144/1000 | Loss: 0.00001658
Iteration 145/1000 | Loss: 0.00001658
Iteration 146/1000 | Loss: 0.00001658
Iteration 147/1000 | Loss: 0.00001658
Iteration 148/1000 | Loss: 0.00001658
Iteration 149/1000 | Loss: 0.00001658
Iteration 150/1000 | Loss: 0.00001657
Iteration 151/1000 | Loss: 0.00001657
Iteration 152/1000 | Loss: 0.00001657
Iteration 153/1000 | Loss: 0.00001657
Iteration 154/1000 | Loss: 0.00001657
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001657
Iteration 157/1000 | Loss: 0.00001657
Iteration 158/1000 | Loss: 0.00001657
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001657
Iteration 162/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.6572750610066578e-05, 1.6572750610066578e-05, 1.6572750610066578e-05, 1.6572750610066578e-05, 1.6572750610066578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6572750610066578e-05

Optimization complete. Final v2v error: 3.522481918334961 mm

Highest mean error: 3.7632246017456055 mm for frame 174

Lowest mean error: 3.309326410293579 mm for frame 94

Saving results

Total time: 147.35422778129578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00343635
Iteration 2/25 | Loss: 0.00102232
Iteration 3/25 | Loss: 0.00086866
Iteration 4/25 | Loss: 0.00083810
Iteration 5/25 | Loss: 0.00082989
Iteration 6/25 | Loss: 0.00082768
Iteration 7/25 | Loss: 0.00082714
Iteration 8/25 | Loss: 0.00082714
Iteration 9/25 | Loss: 0.00082706
Iteration 10/25 | Loss: 0.00082706
Iteration 11/25 | Loss: 0.00082706
Iteration 12/25 | Loss: 0.00082706
Iteration 13/25 | Loss: 0.00082706
Iteration 14/25 | Loss: 0.00082706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008270627586171031, 0.0008270627586171031, 0.0008270627586171031, 0.0008270627586171031, 0.0008270627586171031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008270627586171031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51281285
Iteration 2/25 | Loss: 0.00055948
Iteration 3/25 | Loss: 0.00055947
Iteration 4/25 | Loss: 0.00055947
Iteration 5/25 | Loss: 0.00055947
Iteration 6/25 | Loss: 0.00055947
Iteration 7/25 | Loss: 0.00055947
Iteration 8/25 | Loss: 0.00055947
Iteration 9/25 | Loss: 0.00055947
Iteration 10/25 | Loss: 0.00055947
Iteration 11/25 | Loss: 0.00055947
Iteration 12/25 | Loss: 0.00055947
Iteration 13/25 | Loss: 0.00055947
Iteration 14/25 | Loss: 0.00055947
Iteration 15/25 | Loss: 0.00055947
Iteration 16/25 | Loss: 0.00055947
Iteration 17/25 | Loss: 0.00055947
Iteration 18/25 | Loss: 0.00055947
Iteration 19/25 | Loss: 0.00055947
Iteration 20/25 | Loss: 0.00055947
Iteration 21/25 | Loss: 0.00055947
Iteration 22/25 | Loss: 0.00055947
Iteration 23/25 | Loss: 0.00055947
Iteration 24/25 | Loss: 0.00055947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005594700342044234, 0.0005594700342044234, 0.0005594700342044234, 0.0005594700342044234, 0.0005594700342044234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005594700342044234

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055947
Iteration 2/1000 | Loss: 0.00003612
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00002025
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001769
Iteration 7/1000 | Loss: 0.00001700
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001625
Iteration 10/1000 | Loss: 0.00001622
Iteration 11/1000 | Loss: 0.00001621
Iteration 12/1000 | Loss: 0.00001619
Iteration 13/1000 | Loss: 0.00001602
Iteration 14/1000 | Loss: 0.00001598
Iteration 15/1000 | Loss: 0.00001597
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001586
Iteration 18/1000 | Loss: 0.00001586
Iteration 19/1000 | Loss: 0.00001584
Iteration 20/1000 | Loss: 0.00001584
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001550
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001547
Iteration 32/1000 | Loss: 0.00001544
Iteration 33/1000 | Loss: 0.00001541
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001529
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001528
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001525
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001524
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001519
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001517
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001516
Iteration 84/1000 | Loss: 0.00001516
Iteration 85/1000 | Loss: 0.00001516
Iteration 86/1000 | Loss: 0.00001516
Iteration 87/1000 | Loss: 0.00001516
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001515
Iteration 97/1000 | Loss: 0.00001515
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001511
Iteration 104/1000 | Loss: 0.00001511
Iteration 105/1000 | Loss: 0.00001511
Iteration 106/1000 | Loss: 0.00001511
Iteration 107/1000 | Loss: 0.00001511
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001510
Iteration 112/1000 | Loss: 0.00001510
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001506
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001504
Iteration 136/1000 | Loss: 0.00001504
Iteration 137/1000 | Loss: 0.00001504
Iteration 138/1000 | Loss: 0.00001504
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001503
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001501
Iteration 173/1000 | Loss: 0.00001501
Iteration 174/1000 | Loss: 0.00001501
Iteration 175/1000 | Loss: 0.00001501
Iteration 176/1000 | Loss: 0.00001501
Iteration 177/1000 | Loss: 0.00001501
Iteration 178/1000 | Loss: 0.00001501
Iteration 179/1000 | Loss: 0.00001501
Iteration 180/1000 | Loss: 0.00001501
Iteration 181/1000 | Loss: 0.00001500
Iteration 182/1000 | Loss: 0.00001500
Iteration 183/1000 | Loss: 0.00001500
Iteration 184/1000 | Loss: 0.00001500
Iteration 185/1000 | Loss: 0.00001500
Iteration 186/1000 | Loss: 0.00001500
Iteration 187/1000 | Loss: 0.00001500
Iteration 188/1000 | Loss: 0.00001500
Iteration 189/1000 | Loss: 0.00001500
Iteration 190/1000 | Loss: 0.00001500
Iteration 191/1000 | Loss: 0.00001500
Iteration 192/1000 | Loss: 0.00001500
Iteration 193/1000 | Loss: 0.00001500
Iteration 194/1000 | Loss: 0.00001500
Iteration 195/1000 | Loss: 0.00001500
Iteration 196/1000 | Loss: 0.00001499
Iteration 197/1000 | Loss: 0.00001499
Iteration 198/1000 | Loss: 0.00001499
Iteration 199/1000 | Loss: 0.00001499
Iteration 200/1000 | Loss: 0.00001499
Iteration 201/1000 | Loss: 0.00001499
Iteration 202/1000 | Loss: 0.00001499
Iteration 203/1000 | Loss: 0.00001499
Iteration 204/1000 | Loss: 0.00001499
Iteration 205/1000 | Loss: 0.00001499
Iteration 206/1000 | Loss: 0.00001499
Iteration 207/1000 | Loss: 0.00001499
Iteration 208/1000 | Loss: 0.00001499
Iteration 209/1000 | Loss: 0.00001499
Iteration 210/1000 | Loss: 0.00001499
Iteration 211/1000 | Loss: 0.00001499
Iteration 212/1000 | Loss: 0.00001499
Iteration 213/1000 | Loss: 0.00001499
Iteration 214/1000 | Loss: 0.00001499
Iteration 215/1000 | Loss: 0.00001499
Iteration 216/1000 | Loss: 0.00001499
Iteration 217/1000 | Loss: 0.00001499
Iteration 218/1000 | Loss: 0.00001499
Iteration 219/1000 | Loss: 0.00001499
Iteration 220/1000 | Loss: 0.00001499
Iteration 221/1000 | Loss: 0.00001499
Iteration 222/1000 | Loss: 0.00001499
Iteration 223/1000 | Loss: 0.00001499
Iteration 224/1000 | Loss: 0.00001499
Iteration 225/1000 | Loss: 0.00001499
Iteration 226/1000 | Loss: 0.00001499
Iteration 227/1000 | Loss: 0.00001499
Iteration 228/1000 | Loss: 0.00001499
Iteration 229/1000 | Loss: 0.00001499
Iteration 230/1000 | Loss: 0.00001499
Iteration 231/1000 | Loss: 0.00001499
Iteration 232/1000 | Loss: 0.00001499
Iteration 233/1000 | Loss: 0.00001499
Iteration 234/1000 | Loss: 0.00001499
Iteration 235/1000 | Loss: 0.00001499
Iteration 236/1000 | Loss: 0.00001499
Iteration 237/1000 | Loss: 0.00001499
Iteration 238/1000 | Loss: 0.00001499
Iteration 239/1000 | Loss: 0.00001499
Iteration 240/1000 | Loss: 0.00001499
Iteration 241/1000 | Loss: 0.00001499
Iteration 242/1000 | Loss: 0.00001499
Iteration 243/1000 | Loss: 0.00001499
Iteration 244/1000 | Loss: 0.00001499
Iteration 245/1000 | Loss: 0.00001499
Iteration 246/1000 | Loss: 0.00001499
Iteration 247/1000 | Loss: 0.00001499
Iteration 248/1000 | Loss: 0.00001499
Iteration 249/1000 | Loss: 0.00001499
Iteration 250/1000 | Loss: 0.00001499
Iteration 251/1000 | Loss: 0.00001499
Iteration 252/1000 | Loss: 0.00001499
Iteration 253/1000 | Loss: 0.00001499
Iteration 254/1000 | Loss: 0.00001499
Iteration 255/1000 | Loss: 0.00001499
Iteration 256/1000 | Loss: 0.00001499
Iteration 257/1000 | Loss: 0.00001499
Iteration 258/1000 | Loss: 0.00001499
Iteration 259/1000 | Loss: 0.00001499
Iteration 260/1000 | Loss: 0.00001499
Iteration 261/1000 | Loss: 0.00001499
Iteration 262/1000 | Loss: 0.00001499
Iteration 263/1000 | Loss: 0.00001499
Iteration 264/1000 | Loss: 0.00001499
Iteration 265/1000 | Loss: 0.00001499
Iteration 266/1000 | Loss: 0.00001499
Iteration 267/1000 | Loss: 0.00001499
Iteration 268/1000 | Loss: 0.00001499
Iteration 269/1000 | Loss: 0.00001499
Iteration 270/1000 | Loss: 0.00001499
Iteration 271/1000 | Loss: 0.00001499
Iteration 272/1000 | Loss: 0.00001499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.4990780982770957e-05, 1.4990780982770957e-05, 1.4990780982770957e-05, 1.4990780982770957e-05, 1.4990780982770957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4990780982770957e-05

Optimization complete. Final v2v error: 3.29957914352417 mm

Highest mean error: 3.3942675590515137 mm for frame 77

Lowest mean error: 3.1842823028564453 mm for frame 175

Saving results

Total time: 48.47853684425354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053783
Iteration 2/25 | Loss: 0.00206828
Iteration 3/25 | Loss: 0.00130101
Iteration 4/25 | Loss: 0.00112362
Iteration 5/25 | Loss: 0.00108390
Iteration 6/25 | Loss: 0.00118914
Iteration 7/25 | Loss: 0.00120694
Iteration 8/25 | Loss: 0.00111699
Iteration 9/25 | Loss: 0.00105441
Iteration 10/25 | Loss: 0.00098463
Iteration 11/25 | Loss: 0.00093176
Iteration 12/25 | Loss: 0.00090218
Iteration 13/25 | Loss: 0.00088480
Iteration 14/25 | Loss: 0.00087729
Iteration 15/25 | Loss: 0.00086483
Iteration 16/25 | Loss: 0.00086789
Iteration 17/25 | Loss: 0.00086508
Iteration 18/25 | Loss: 0.00086028
Iteration 19/25 | Loss: 0.00085329
Iteration 20/25 | Loss: 0.00085276
Iteration 21/25 | Loss: 0.00084865
Iteration 22/25 | Loss: 0.00084072
Iteration 23/25 | Loss: 0.00083621
Iteration 24/25 | Loss: 0.00083344
Iteration 25/25 | Loss: 0.00082637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.38348293
Iteration 2/25 | Loss: 0.00076669
Iteration 3/25 | Loss: 0.00076668
Iteration 4/25 | Loss: 0.00076668
Iteration 5/25 | Loss: 0.00076668
Iteration 6/25 | Loss: 0.00076668
Iteration 7/25 | Loss: 0.00076668
Iteration 8/25 | Loss: 0.00076668
Iteration 9/25 | Loss: 0.00076668
Iteration 10/25 | Loss: 0.00076668
Iteration 11/25 | Loss: 0.00076668
Iteration 12/25 | Loss: 0.00076668
Iteration 13/25 | Loss: 0.00076668
Iteration 14/25 | Loss: 0.00076668
Iteration 15/25 | Loss: 0.00076668
Iteration 16/25 | Loss: 0.00076668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007666816236451268, 0.0007666816236451268, 0.0007666816236451268, 0.0007666816236451268, 0.0007666816236451268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007666816236451268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076668
Iteration 2/1000 | Loss: 0.00012169
Iteration 3/1000 | Loss: 0.00003806
Iteration 4/1000 | Loss: 0.00009127
Iteration 5/1000 | Loss: 0.00021911
Iteration 6/1000 | Loss: 0.00003235
Iteration 7/1000 | Loss: 0.00003071
Iteration 8/1000 | Loss: 0.00004427
Iteration 9/1000 | Loss: 0.00002891
Iteration 10/1000 | Loss: 0.00006196
Iteration 11/1000 | Loss: 0.00011051
Iteration 12/1000 | Loss: 0.00006590
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001283
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001222
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001142
Iteration 24/1000 | Loss: 0.00008231
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001652
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001121
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001121
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001117
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001114
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001108
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001107
Iteration 54/1000 | Loss: 0.00001107
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001103
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001102
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001101
Iteration 70/1000 | Loss: 0.00001101
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001100
Iteration 74/1000 | Loss: 0.00001100
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001096
Iteration 85/1000 | Loss: 0.00001096
Iteration 86/1000 | Loss: 0.00001096
Iteration 87/1000 | Loss: 0.00001095
Iteration 88/1000 | Loss: 0.00001095
Iteration 89/1000 | Loss: 0.00001095
Iteration 90/1000 | Loss: 0.00001094
Iteration 91/1000 | Loss: 0.00001094
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001093
Iteration 97/1000 | Loss: 0.00001093
Iteration 98/1000 | Loss: 0.00001092
Iteration 99/1000 | Loss: 0.00001092
Iteration 100/1000 | Loss: 0.00001092
Iteration 101/1000 | Loss: 0.00001092
Iteration 102/1000 | Loss: 0.00001091
Iteration 103/1000 | Loss: 0.00001091
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001090
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001089
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001088
Iteration 114/1000 | Loss: 0.00001088
Iteration 115/1000 | Loss: 0.00001088
Iteration 116/1000 | Loss: 0.00001088
Iteration 117/1000 | Loss: 0.00001088
Iteration 118/1000 | Loss: 0.00001087
Iteration 119/1000 | Loss: 0.00001087
Iteration 120/1000 | Loss: 0.00001086
Iteration 121/1000 | Loss: 0.00001086
Iteration 122/1000 | Loss: 0.00001085
Iteration 123/1000 | Loss: 0.00001085
Iteration 124/1000 | Loss: 0.00001085
Iteration 125/1000 | Loss: 0.00001085
Iteration 126/1000 | Loss: 0.00001085
Iteration 127/1000 | Loss: 0.00001085
Iteration 128/1000 | Loss: 0.00001084
Iteration 129/1000 | Loss: 0.00001084
Iteration 130/1000 | Loss: 0.00001084
Iteration 131/1000 | Loss: 0.00001083
Iteration 132/1000 | Loss: 0.00001083
Iteration 133/1000 | Loss: 0.00001083
Iteration 134/1000 | Loss: 0.00001083
Iteration 135/1000 | Loss: 0.00001082
Iteration 136/1000 | Loss: 0.00001082
Iteration 137/1000 | Loss: 0.00001082
Iteration 138/1000 | Loss: 0.00001082
Iteration 139/1000 | Loss: 0.00001082
Iteration 140/1000 | Loss: 0.00001081
Iteration 141/1000 | Loss: 0.00001081
Iteration 142/1000 | Loss: 0.00001081
Iteration 143/1000 | Loss: 0.00001081
Iteration 144/1000 | Loss: 0.00001081
Iteration 145/1000 | Loss: 0.00001081
Iteration 146/1000 | Loss: 0.00001080
Iteration 147/1000 | Loss: 0.00001080
Iteration 148/1000 | Loss: 0.00001080
Iteration 149/1000 | Loss: 0.00012605
Iteration 150/1000 | Loss: 0.00001092
Iteration 151/1000 | Loss: 0.00006312
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001087
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001089
Iteration 158/1000 | Loss: 0.00001089
Iteration 159/1000 | Loss: 0.00001089
Iteration 160/1000 | Loss: 0.00001086
Iteration 161/1000 | Loss: 0.00001085
Iteration 162/1000 | Loss: 0.00001083
Iteration 163/1000 | Loss: 0.00001083
Iteration 164/1000 | Loss: 0.00001082
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001080
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001079
Iteration 169/1000 | Loss: 0.00001079
Iteration 170/1000 | Loss: 0.00001079
Iteration 171/1000 | Loss: 0.00001078
Iteration 172/1000 | Loss: 0.00001078
Iteration 173/1000 | Loss: 0.00001078
Iteration 174/1000 | Loss: 0.00001077
Iteration 175/1000 | Loss: 0.00001076
Iteration 176/1000 | Loss: 0.00001076
Iteration 177/1000 | Loss: 0.00001076
Iteration 178/1000 | Loss: 0.00001076
Iteration 179/1000 | Loss: 0.00001076
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001076
Iteration 183/1000 | Loss: 0.00001076
Iteration 184/1000 | Loss: 0.00001075
Iteration 185/1000 | Loss: 0.00001075
Iteration 186/1000 | Loss: 0.00001075
Iteration 187/1000 | Loss: 0.00001074
Iteration 188/1000 | Loss: 0.00001074
Iteration 189/1000 | Loss: 0.00001073
Iteration 190/1000 | Loss: 0.00001073
Iteration 191/1000 | Loss: 0.00001073
Iteration 192/1000 | Loss: 0.00001073
Iteration 193/1000 | Loss: 0.00001073
Iteration 194/1000 | Loss: 0.00001073
Iteration 195/1000 | Loss: 0.00001073
Iteration 196/1000 | Loss: 0.00001073
Iteration 197/1000 | Loss: 0.00001072
Iteration 198/1000 | Loss: 0.00001072
Iteration 199/1000 | Loss: 0.00001072
Iteration 200/1000 | Loss: 0.00001072
Iteration 201/1000 | Loss: 0.00001072
Iteration 202/1000 | Loss: 0.00001072
Iteration 203/1000 | Loss: 0.00001072
Iteration 204/1000 | Loss: 0.00001072
Iteration 205/1000 | Loss: 0.00001072
Iteration 206/1000 | Loss: 0.00001072
Iteration 207/1000 | Loss: 0.00001072
Iteration 208/1000 | Loss: 0.00001072
Iteration 209/1000 | Loss: 0.00001072
Iteration 210/1000 | Loss: 0.00001072
Iteration 211/1000 | Loss: 0.00001072
Iteration 212/1000 | Loss: 0.00001072
Iteration 213/1000 | Loss: 0.00001072
Iteration 214/1000 | Loss: 0.00001072
Iteration 215/1000 | Loss: 0.00001072
Iteration 216/1000 | Loss: 0.00001072
Iteration 217/1000 | Loss: 0.00001072
Iteration 218/1000 | Loss: 0.00001072
Iteration 219/1000 | Loss: 0.00001072
Iteration 220/1000 | Loss: 0.00001072
Iteration 221/1000 | Loss: 0.00001072
Iteration 222/1000 | Loss: 0.00001072
Iteration 223/1000 | Loss: 0.00001072
Iteration 224/1000 | Loss: 0.00001072
Iteration 225/1000 | Loss: 0.00001072
Iteration 226/1000 | Loss: 0.00001072
Iteration 227/1000 | Loss: 0.00001072
Iteration 228/1000 | Loss: 0.00001072
Iteration 229/1000 | Loss: 0.00001072
Iteration 230/1000 | Loss: 0.00001072
Iteration 231/1000 | Loss: 0.00001072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.0716526048781816e-05, 1.0716526048781816e-05, 1.0716526048781816e-05, 1.0716526048781816e-05, 1.0716526048781816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0716526048781816e-05

Optimization complete. Final v2v error: 2.827585458755493 mm

Highest mean error: 3.42132830619812 mm for frame 39

Lowest mean error: 2.613839626312256 mm for frame 89

Saving results

Total time: 108.64148473739624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020919
Iteration 2/25 | Loss: 0.00115152
Iteration 3/25 | Loss: 0.00090786
Iteration 4/25 | Loss: 0.00087639
Iteration 5/25 | Loss: 0.00086806
Iteration 6/25 | Loss: 0.00086622
Iteration 7/25 | Loss: 0.00086569
Iteration 8/25 | Loss: 0.00086553
Iteration 9/25 | Loss: 0.00086552
Iteration 10/25 | Loss: 0.00086552
Iteration 11/25 | Loss: 0.00086552
Iteration 12/25 | Loss: 0.00086552
Iteration 13/25 | Loss: 0.00086552
Iteration 14/25 | Loss: 0.00086552
Iteration 15/25 | Loss: 0.00086552
Iteration 16/25 | Loss: 0.00086551
Iteration 17/25 | Loss: 0.00086551
Iteration 18/25 | Loss: 0.00086551
Iteration 19/25 | Loss: 0.00086551
Iteration 20/25 | Loss: 0.00086551
Iteration 21/25 | Loss: 0.00086551
Iteration 22/25 | Loss: 0.00086551
Iteration 23/25 | Loss: 0.00086551
Iteration 24/25 | Loss: 0.00086551
Iteration 25/25 | Loss: 0.00086551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.20209599
Iteration 2/25 | Loss: 0.00053634
Iteration 3/25 | Loss: 0.00053634
Iteration 4/25 | Loss: 0.00053634
Iteration 5/25 | Loss: 0.00053634
Iteration 6/25 | Loss: 0.00053634
Iteration 7/25 | Loss: 0.00053634
Iteration 8/25 | Loss: 0.00053633
Iteration 9/25 | Loss: 0.00053633
Iteration 10/25 | Loss: 0.00053633
Iteration 11/25 | Loss: 0.00053633
Iteration 12/25 | Loss: 0.00053633
Iteration 13/25 | Loss: 0.00053633
Iteration 14/25 | Loss: 0.00053633
Iteration 15/25 | Loss: 0.00053633
Iteration 16/25 | Loss: 0.00053633
Iteration 17/25 | Loss: 0.00053633
Iteration 18/25 | Loss: 0.00053633
Iteration 19/25 | Loss: 0.00053633
Iteration 20/25 | Loss: 0.00053633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005363340605981648, 0.0005363340605981648, 0.0005363340605981648, 0.0005363340605981648, 0.0005363340605981648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005363340605981648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053633
Iteration 2/1000 | Loss: 0.00002842
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001713
Iteration 5/1000 | Loss: 0.00001619
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001509
Iteration 8/1000 | Loss: 0.00001475
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001418
Iteration 13/1000 | Loss: 0.00001414
Iteration 14/1000 | Loss: 0.00001412
Iteration 15/1000 | Loss: 0.00001409
Iteration 16/1000 | Loss: 0.00001407
Iteration 17/1000 | Loss: 0.00001405
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001399
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001395
Iteration 27/1000 | Loss: 0.00001394
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001386
Iteration 35/1000 | Loss: 0.00001386
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001385
Iteration 40/1000 | Loss: 0.00001385
Iteration 41/1000 | Loss: 0.00001384
Iteration 42/1000 | Loss: 0.00001384
Iteration 43/1000 | Loss: 0.00001384
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001381
Iteration 51/1000 | Loss: 0.00001381
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001380
Iteration 54/1000 | Loss: 0.00001380
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001377
Iteration 64/1000 | Loss: 0.00001377
Iteration 65/1000 | Loss: 0.00001377
Iteration 66/1000 | Loss: 0.00001376
Iteration 67/1000 | Loss: 0.00001376
Iteration 68/1000 | Loss: 0.00001376
Iteration 69/1000 | Loss: 0.00001376
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001374
Iteration 73/1000 | Loss: 0.00001374
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001373
Iteration 76/1000 | Loss: 0.00001373
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001372
Iteration 80/1000 | Loss: 0.00001372
Iteration 81/1000 | Loss: 0.00001372
Iteration 82/1000 | Loss: 0.00001372
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001371
Iteration 85/1000 | Loss: 0.00001371
Iteration 86/1000 | Loss: 0.00001370
Iteration 87/1000 | Loss: 0.00001369
Iteration 88/1000 | Loss: 0.00001369
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001369
Iteration 91/1000 | Loss: 0.00001369
Iteration 92/1000 | Loss: 0.00001369
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001367
Iteration 103/1000 | Loss: 0.00001367
Iteration 104/1000 | Loss: 0.00001367
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001367
Iteration 108/1000 | Loss: 0.00001367
Iteration 109/1000 | Loss: 0.00001367
Iteration 110/1000 | Loss: 0.00001367
Iteration 111/1000 | Loss: 0.00001367
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001366
Iteration 114/1000 | Loss: 0.00001366
Iteration 115/1000 | Loss: 0.00001366
Iteration 116/1000 | Loss: 0.00001365
Iteration 117/1000 | Loss: 0.00001365
Iteration 118/1000 | Loss: 0.00001365
Iteration 119/1000 | Loss: 0.00001365
Iteration 120/1000 | Loss: 0.00001365
Iteration 121/1000 | Loss: 0.00001365
Iteration 122/1000 | Loss: 0.00001364
Iteration 123/1000 | Loss: 0.00001364
Iteration 124/1000 | Loss: 0.00001364
Iteration 125/1000 | Loss: 0.00001364
Iteration 126/1000 | Loss: 0.00001364
Iteration 127/1000 | Loss: 0.00001364
Iteration 128/1000 | Loss: 0.00001364
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001364
Iteration 132/1000 | Loss: 0.00001364
Iteration 133/1000 | Loss: 0.00001364
Iteration 134/1000 | Loss: 0.00001363
Iteration 135/1000 | Loss: 0.00001363
Iteration 136/1000 | Loss: 0.00001363
Iteration 137/1000 | Loss: 0.00001363
Iteration 138/1000 | Loss: 0.00001363
Iteration 139/1000 | Loss: 0.00001363
Iteration 140/1000 | Loss: 0.00001363
Iteration 141/1000 | Loss: 0.00001363
Iteration 142/1000 | Loss: 0.00001363
Iteration 143/1000 | Loss: 0.00001363
Iteration 144/1000 | Loss: 0.00001363
Iteration 145/1000 | Loss: 0.00001363
Iteration 146/1000 | Loss: 0.00001363
Iteration 147/1000 | Loss: 0.00001363
Iteration 148/1000 | Loss: 0.00001363
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001363
Iteration 152/1000 | Loss: 0.00001363
Iteration 153/1000 | Loss: 0.00001363
Iteration 154/1000 | Loss: 0.00001363
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001363
Iteration 157/1000 | Loss: 0.00001363
Iteration 158/1000 | Loss: 0.00001363
Iteration 159/1000 | Loss: 0.00001363
Iteration 160/1000 | Loss: 0.00001363
Iteration 161/1000 | Loss: 0.00001363
Iteration 162/1000 | Loss: 0.00001363
Iteration 163/1000 | Loss: 0.00001363
Iteration 164/1000 | Loss: 0.00001363
Iteration 165/1000 | Loss: 0.00001363
Iteration 166/1000 | Loss: 0.00001363
Iteration 167/1000 | Loss: 0.00001363
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00001363
Iteration 177/1000 | Loss: 0.00001363
Iteration 178/1000 | Loss: 0.00001363
Iteration 179/1000 | Loss: 0.00001363
Iteration 180/1000 | Loss: 0.00001363
Iteration 181/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.3631001820613164e-05, 1.3631001820613164e-05, 1.3631001820613164e-05, 1.3631001820613164e-05, 1.3631001820613164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3631001820613164e-05

Optimization complete. Final v2v error: 3.132319927215576 mm

Highest mean error: 3.413891553878784 mm for frame 15

Lowest mean error: 2.8426320552825928 mm for frame 138

Saving results

Total time: 41.415512800216675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785512
Iteration 2/25 | Loss: 0.00126705
Iteration 3/25 | Loss: 0.00106685
Iteration 4/25 | Loss: 0.00099814
Iteration 5/25 | Loss: 0.00097261
Iteration 6/25 | Loss: 0.00096734
Iteration 7/25 | Loss: 0.00096797
Iteration 8/25 | Loss: 0.00096311
Iteration 9/25 | Loss: 0.00096167
Iteration 10/25 | Loss: 0.00096108
Iteration 11/25 | Loss: 0.00096094
Iteration 12/25 | Loss: 0.00096094
Iteration 13/25 | Loss: 0.00096094
Iteration 14/25 | Loss: 0.00096094
Iteration 15/25 | Loss: 0.00096094
Iteration 16/25 | Loss: 0.00096094
Iteration 17/25 | Loss: 0.00096094
Iteration 18/25 | Loss: 0.00096093
Iteration 19/25 | Loss: 0.00096093
Iteration 20/25 | Loss: 0.00096093
Iteration 21/25 | Loss: 0.00096093
Iteration 22/25 | Loss: 0.00096093
Iteration 23/25 | Loss: 0.00096093
Iteration 24/25 | Loss: 0.00096093
Iteration 25/25 | Loss: 0.00096093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.00421047
Iteration 2/25 | Loss: 0.00072752
Iteration 3/25 | Loss: 0.00072752
Iteration 4/25 | Loss: 0.00072752
Iteration 5/25 | Loss: 0.00072752
Iteration 6/25 | Loss: 0.00072752
Iteration 7/25 | Loss: 0.00072752
Iteration 8/25 | Loss: 0.00072752
Iteration 9/25 | Loss: 0.00072752
Iteration 10/25 | Loss: 0.00072752
Iteration 11/25 | Loss: 0.00072752
Iteration 12/25 | Loss: 0.00072752
Iteration 13/25 | Loss: 0.00072752
Iteration 14/25 | Loss: 0.00072752
Iteration 15/25 | Loss: 0.00072752
Iteration 16/25 | Loss: 0.00072752
Iteration 17/25 | Loss: 0.00072752
Iteration 18/25 | Loss: 0.00072752
Iteration 19/25 | Loss: 0.00072752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007275203824974597, 0.0007275203824974597, 0.0007275203824974597, 0.0007275203824974597, 0.0007275203824974597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007275203824974597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072752
Iteration 2/1000 | Loss: 0.00007340
Iteration 3/1000 | Loss: 0.00005058
Iteration 4/1000 | Loss: 0.00004320
Iteration 5/1000 | Loss: 0.00004028
Iteration 6/1000 | Loss: 0.00003814
Iteration 7/1000 | Loss: 0.00003685
Iteration 8/1000 | Loss: 0.00003578
Iteration 9/1000 | Loss: 0.00003498
Iteration 10/1000 | Loss: 0.00003438
Iteration 11/1000 | Loss: 0.00003386
Iteration 12/1000 | Loss: 0.00003346
Iteration 13/1000 | Loss: 0.00003311
Iteration 14/1000 | Loss: 0.00003284
Iteration 15/1000 | Loss: 0.00003262
Iteration 16/1000 | Loss: 0.00003242
Iteration 17/1000 | Loss: 0.00003230
Iteration 18/1000 | Loss: 0.00003229
Iteration 19/1000 | Loss: 0.00003228
Iteration 20/1000 | Loss: 0.00003226
Iteration 21/1000 | Loss: 0.00018816
Iteration 22/1000 | Loss: 0.00003221
Iteration 23/1000 | Loss: 0.00003158
Iteration 24/1000 | Loss: 0.00003108
Iteration 25/1000 | Loss: 0.00003065
Iteration 26/1000 | Loss: 0.00003036
Iteration 27/1000 | Loss: 0.00003020
Iteration 28/1000 | Loss: 0.00003020
Iteration 29/1000 | Loss: 0.00003017
Iteration 30/1000 | Loss: 0.00003016
Iteration 31/1000 | Loss: 0.00003004
Iteration 32/1000 | Loss: 0.00003002
Iteration 33/1000 | Loss: 0.00002998
Iteration 34/1000 | Loss: 0.00002998
Iteration 35/1000 | Loss: 0.00002998
Iteration 36/1000 | Loss: 0.00002997
Iteration 37/1000 | Loss: 0.00002996
Iteration 38/1000 | Loss: 0.00002995
Iteration 39/1000 | Loss: 0.00002995
Iteration 40/1000 | Loss: 0.00002995
Iteration 41/1000 | Loss: 0.00002994
Iteration 42/1000 | Loss: 0.00002994
Iteration 43/1000 | Loss: 0.00002994
Iteration 44/1000 | Loss: 0.00002993
Iteration 45/1000 | Loss: 0.00002993
Iteration 46/1000 | Loss: 0.00002993
Iteration 47/1000 | Loss: 0.00002992
Iteration 48/1000 | Loss: 0.00002992
Iteration 49/1000 | Loss: 0.00002992
Iteration 50/1000 | Loss: 0.00002992
Iteration 51/1000 | Loss: 0.00002991
Iteration 52/1000 | Loss: 0.00002991
Iteration 53/1000 | Loss: 0.00002991
Iteration 54/1000 | Loss: 0.00002991
Iteration 55/1000 | Loss: 0.00002991
Iteration 56/1000 | Loss: 0.00002991
Iteration 57/1000 | Loss: 0.00002991
Iteration 58/1000 | Loss: 0.00002990
Iteration 59/1000 | Loss: 0.00002990
Iteration 60/1000 | Loss: 0.00002990
Iteration 61/1000 | Loss: 0.00002990
Iteration 62/1000 | Loss: 0.00002990
Iteration 63/1000 | Loss: 0.00002990
Iteration 64/1000 | Loss: 0.00002989
Iteration 65/1000 | Loss: 0.00002989
Iteration 66/1000 | Loss: 0.00002989
Iteration 67/1000 | Loss: 0.00002989
Iteration 68/1000 | Loss: 0.00002989
Iteration 69/1000 | Loss: 0.00002989
Iteration 70/1000 | Loss: 0.00002988
Iteration 71/1000 | Loss: 0.00002988
Iteration 72/1000 | Loss: 0.00002988
Iteration 73/1000 | Loss: 0.00002988
Iteration 74/1000 | Loss: 0.00002987
Iteration 75/1000 | Loss: 0.00002987
Iteration 76/1000 | Loss: 0.00002987
Iteration 77/1000 | Loss: 0.00002987
Iteration 78/1000 | Loss: 0.00002987
Iteration 79/1000 | Loss: 0.00002987
Iteration 80/1000 | Loss: 0.00002987
Iteration 81/1000 | Loss: 0.00002987
Iteration 82/1000 | Loss: 0.00002987
Iteration 83/1000 | Loss: 0.00002987
Iteration 84/1000 | Loss: 0.00002987
Iteration 85/1000 | Loss: 0.00002986
Iteration 86/1000 | Loss: 0.00002986
Iteration 87/1000 | Loss: 0.00002986
Iteration 88/1000 | Loss: 0.00002986
Iteration 89/1000 | Loss: 0.00002986
Iteration 90/1000 | Loss: 0.00002986
Iteration 91/1000 | Loss: 0.00002986
Iteration 92/1000 | Loss: 0.00002986
Iteration 93/1000 | Loss: 0.00002986
Iteration 94/1000 | Loss: 0.00002986
Iteration 95/1000 | Loss: 0.00002985
Iteration 96/1000 | Loss: 0.00002985
Iteration 97/1000 | Loss: 0.00002985
Iteration 98/1000 | Loss: 0.00002985
Iteration 99/1000 | Loss: 0.00002985
Iteration 100/1000 | Loss: 0.00002985
Iteration 101/1000 | Loss: 0.00002985
Iteration 102/1000 | Loss: 0.00002985
Iteration 103/1000 | Loss: 0.00002985
Iteration 104/1000 | Loss: 0.00002985
Iteration 105/1000 | Loss: 0.00002985
Iteration 106/1000 | Loss: 0.00002985
Iteration 107/1000 | Loss: 0.00002985
Iteration 108/1000 | Loss: 0.00002985
Iteration 109/1000 | Loss: 0.00002985
Iteration 110/1000 | Loss: 0.00002985
Iteration 111/1000 | Loss: 0.00002985
Iteration 112/1000 | Loss: 0.00002985
Iteration 113/1000 | Loss: 0.00002985
Iteration 114/1000 | Loss: 0.00002985
Iteration 115/1000 | Loss: 0.00002985
Iteration 116/1000 | Loss: 0.00002985
Iteration 117/1000 | Loss: 0.00002985
Iteration 118/1000 | Loss: 0.00002985
Iteration 119/1000 | Loss: 0.00002985
Iteration 120/1000 | Loss: 0.00002985
Iteration 121/1000 | Loss: 0.00002985
Iteration 122/1000 | Loss: 0.00002985
Iteration 123/1000 | Loss: 0.00002985
Iteration 124/1000 | Loss: 0.00002985
Iteration 125/1000 | Loss: 0.00002985
Iteration 126/1000 | Loss: 0.00002985
Iteration 127/1000 | Loss: 0.00002985
Iteration 128/1000 | Loss: 0.00002985
Iteration 129/1000 | Loss: 0.00002985
Iteration 130/1000 | Loss: 0.00002985
Iteration 131/1000 | Loss: 0.00002985
Iteration 132/1000 | Loss: 0.00002985
Iteration 133/1000 | Loss: 0.00002985
Iteration 134/1000 | Loss: 0.00002985
Iteration 135/1000 | Loss: 0.00002985
Iteration 136/1000 | Loss: 0.00002985
Iteration 137/1000 | Loss: 0.00002985
Iteration 138/1000 | Loss: 0.00002985
Iteration 139/1000 | Loss: 0.00002985
Iteration 140/1000 | Loss: 0.00002985
Iteration 141/1000 | Loss: 0.00002985
Iteration 142/1000 | Loss: 0.00002985
Iteration 143/1000 | Loss: 0.00002985
Iteration 144/1000 | Loss: 0.00002985
Iteration 145/1000 | Loss: 0.00002985
Iteration 146/1000 | Loss: 0.00002985
Iteration 147/1000 | Loss: 0.00002985
Iteration 148/1000 | Loss: 0.00002985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.9850827559130266e-05, 2.9850827559130266e-05, 2.9850827559130266e-05, 2.9850827559130266e-05, 2.9850827559130266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9850827559130266e-05

Optimization complete. Final v2v error: 4.509273052215576 mm

Highest mean error: 6.462581634521484 mm for frame 9

Lowest mean error: 3.422686815261841 mm for frame 231

Saving results

Total time: 68.48766851425171
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835051
Iteration 2/25 | Loss: 0.00101651
Iteration 3/25 | Loss: 0.00084654
Iteration 4/25 | Loss: 0.00082673
Iteration 5/25 | Loss: 0.00082003
Iteration 6/25 | Loss: 0.00081833
Iteration 7/25 | Loss: 0.00081788
Iteration 8/25 | Loss: 0.00081788
Iteration 9/25 | Loss: 0.00081788
Iteration 10/25 | Loss: 0.00081788
Iteration 11/25 | Loss: 0.00081788
Iteration 12/25 | Loss: 0.00081788
Iteration 13/25 | Loss: 0.00081788
Iteration 14/25 | Loss: 0.00081788
Iteration 15/25 | Loss: 0.00081788
Iteration 16/25 | Loss: 0.00081788
Iteration 17/25 | Loss: 0.00081788
Iteration 18/25 | Loss: 0.00081788
Iteration 19/25 | Loss: 0.00081788
Iteration 20/25 | Loss: 0.00081788
Iteration 21/25 | Loss: 0.00081788
Iteration 22/25 | Loss: 0.00081788
Iteration 23/25 | Loss: 0.00081788
Iteration 24/25 | Loss: 0.00081788
Iteration 25/25 | Loss: 0.00081788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.52769756
Iteration 2/25 | Loss: 0.00055037
Iteration 3/25 | Loss: 0.00055036
Iteration 4/25 | Loss: 0.00055036
Iteration 5/25 | Loss: 0.00055036
Iteration 6/25 | Loss: 0.00055036
Iteration 7/25 | Loss: 0.00055036
Iteration 8/25 | Loss: 0.00055036
Iteration 9/25 | Loss: 0.00055036
Iteration 10/25 | Loss: 0.00055036
Iteration 11/25 | Loss: 0.00055036
Iteration 12/25 | Loss: 0.00055036
Iteration 13/25 | Loss: 0.00055036
Iteration 14/25 | Loss: 0.00055036
Iteration 15/25 | Loss: 0.00055036
Iteration 16/25 | Loss: 0.00055036
Iteration 17/25 | Loss: 0.00055036
Iteration 18/25 | Loss: 0.00055036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005503574502654374, 0.0005503574502654374, 0.0005503574502654374, 0.0005503574502654374, 0.0005503574502654374]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005503574502654374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055036
Iteration 2/1000 | Loss: 0.00002737
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001561
Iteration 5/1000 | Loss: 0.00001473
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001336
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001308
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001279
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001265
Iteration 16/1000 | Loss: 0.00001263
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001259
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001249
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001236
Iteration 53/1000 | Loss: 0.00001236
Iteration 54/1000 | Loss: 0.00001236
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001233
Iteration 59/1000 | Loss: 0.00001233
Iteration 60/1000 | Loss: 0.00001232
Iteration 61/1000 | Loss: 0.00001232
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001228
Iteration 66/1000 | Loss: 0.00001228
Iteration 67/1000 | Loss: 0.00001228
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001226
Iteration 72/1000 | Loss: 0.00001226
Iteration 73/1000 | Loss: 0.00001225
Iteration 74/1000 | Loss: 0.00001225
Iteration 75/1000 | Loss: 0.00001225
Iteration 76/1000 | Loss: 0.00001225
Iteration 77/1000 | Loss: 0.00001224
Iteration 78/1000 | Loss: 0.00001224
Iteration 79/1000 | Loss: 0.00001224
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001224
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001223
Iteration 87/1000 | Loss: 0.00001223
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001220
Iteration 102/1000 | Loss: 0.00001220
Iteration 103/1000 | Loss: 0.00001220
Iteration 104/1000 | Loss: 0.00001220
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001220
Iteration 113/1000 | Loss: 0.00001220
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001220
Iteration 119/1000 | Loss: 0.00001220
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2197548130643554e-05, 1.2197548130643554e-05, 1.2197548130643554e-05, 1.2197548130643554e-05, 1.2197548130643554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2197548130643554e-05

Optimization complete. Final v2v error: 2.9520866870880127 mm

Highest mean error: 3.938042163848877 mm for frame 86

Lowest mean error: 2.610666513442993 mm for frame 114

Saving results

Total time: 37.376158237457275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537054
Iteration 2/25 | Loss: 0.00115485
Iteration 3/25 | Loss: 0.00093465
Iteration 4/25 | Loss: 0.00089298
Iteration 5/25 | Loss: 0.00087560
Iteration 6/25 | Loss: 0.00087270
Iteration 7/25 | Loss: 0.00087258
Iteration 8/25 | Loss: 0.00087258
Iteration 9/25 | Loss: 0.00087258
Iteration 10/25 | Loss: 0.00087258
Iteration 11/25 | Loss: 0.00087258
Iteration 12/25 | Loss: 0.00087258
Iteration 13/25 | Loss: 0.00087258
Iteration 14/25 | Loss: 0.00087258
Iteration 15/25 | Loss: 0.00087258
Iteration 16/25 | Loss: 0.00087258
Iteration 17/25 | Loss: 0.00087258
Iteration 18/25 | Loss: 0.00087258
Iteration 19/25 | Loss: 0.00087258
Iteration 20/25 | Loss: 0.00087258
Iteration 21/25 | Loss: 0.00087258
Iteration 22/25 | Loss: 0.00087258
Iteration 23/25 | Loss: 0.00087258
Iteration 24/25 | Loss: 0.00087258
Iteration 25/25 | Loss: 0.00087258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78969866
Iteration 2/25 | Loss: 0.00044988
Iteration 3/25 | Loss: 0.00044988
Iteration 4/25 | Loss: 0.00044988
Iteration 5/25 | Loss: 0.00044988
Iteration 6/25 | Loss: 0.00044988
Iteration 7/25 | Loss: 0.00044988
Iteration 8/25 | Loss: 0.00044988
Iteration 9/25 | Loss: 0.00044988
Iteration 10/25 | Loss: 0.00044988
Iteration 11/25 | Loss: 0.00044988
Iteration 12/25 | Loss: 0.00044988
Iteration 13/25 | Loss: 0.00044988
Iteration 14/25 | Loss: 0.00044988
Iteration 15/25 | Loss: 0.00044988
Iteration 16/25 | Loss: 0.00044988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004498759808484465, 0.0004498759808484465, 0.0004498759808484465, 0.0004498759808484465, 0.0004498759808484465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004498759808484465

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044988
Iteration 2/1000 | Loss: 0.00004056
Iteration 3/1000 | Loss: 0.00002858
Iteration 4/1000 | Loss: 0.00002681
Iteration 5/1000 | Loss: 0.00002543
Iteration 6/1000 | Loss: 0.00002461
Iteration 7/1000 | Loss: 0.00002400
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00002330
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002278
Iteration 12/1000 | Loss: 0.00002252
Iteration 13/1000 | Loss: 0.00002242
Iteration 14/1000 | Loss: 0.00002241
Iteration 15/1000 | Loss: 0.00002227
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002213
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002207
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002199
Iteration 29/1000 | Loss: 0.00002199
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002196
Iteration 33/1000 | Loss: 0.00002196
Iteration 34/1000 | Loss: 0.00002195
Iteration 35/1000 | Loss: 0.00002194
Iteration 36/1000 | Loss: 0.00002194
Iteration 37/1000 | Loss: 0.00002194
Iteration 38/1000 | Loss: 0.00002194
Iteration 39/1000 | Loss: 0.00002193
Iteration 40/1000 | Loss: 0.00002193
Iteration 41/1000 | Loss: 0.00002193
Iteration 42/1000 | Loss: 0.00002193
Iteration 43/1000 | Loss: 0.00002193
Iteration 44/1000 | Loss: 0.00002193
Iteration 45/1000 | Loss: 0.00002193
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002193
Iteration 53/1000 | Loss: 0.00002193
Iteration 54/1000 | Loss: 0.00002193
Iteration 55/1000 | Loss: 0.00002193
Iteration 56/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [2.1930802176939324e-05, 2.1930802176939324e-05, 2.1930802176939324e-05, 2.1930802176939324e-05, 2.1930802176939324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1930802176939324e-05

Optimization complete. Final v2v error: 3.937591791152954 mm

Highest mean error: 4.616257667541504 mm for frame 266

Lowest mean error: 3.8373701572418213 mm for frame 4

Saving results

Total time: 40.37356686592102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935126
Iteration 2/25 | Loss: 0.00126717
Iteration 3/25 | Loss: 0.00099369
Iteration 4/25 | Loss: 0.00095345
Iteration 5/25 | Loss: 0.00093416
Iteration 6/25 | Loss: 0.00093009
Iteration 7/25 | Loss: 0.00092906
Iteration 8/25 | Loss: 0.00092900
Iteration 9/25 | Loss: 0.00092900
Iteration 10/25 | Loss: 0.00092900
Iteration 11/25 | Loss: 0.00092900
Iteration 12/25 | Loss: 0.00092900
Iteration 13/25 | Loss: 0.00092900
Iteration 14/25 | Loss: 0.00092900
Iteration 15/25 | Loss: 0.00092900
Iteration 16/25 | Loss: 0.00092900
Iteration 17/25 | Loss: 0.00092900
Iteration 18/25 | Loss: 0.00092900
Iteration 19/25 | Loss: 0.00092900
Iteration 20/25 | Loss: 0.00092900
Iteration 21/25 | Loss: 0.00092900
Iteration 22/25 | Loss: 0.00092900
Iteration 23/25 | Loss: 0.00092900
Iteration 24/25 | Loss: 0.00092900
Iteration 25/25 | Loss: 0.00092900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51590824
Iteration 2/25 | Loss: 0.00059713
Iteration 3/25 | Loss: 0.00059711
Iteration 4/25 | Loss: 0.00059711
Iteration 5/25 | Loss: 0.00059710
Iteration 6/25 | Loss: 0.00059710
Iteration 7/25 | Loss: 0.00059710
Iteration 8/25 | Loss: 0.00059710
Iteration 9/25 | Loss: 0.00059710
Iteration 10/25 | Loss: 0.00059710
Iteration 11/25 | Loss: 0.00059710
Iteration 12/25 | Loss: 0.00059710
Iteration 13/25 | Loss: 0.00059710
Iteration 14/25 | Loss: 0.00059710
Iteration 15/25 | Loss: 0.00059710
Iteration 16/25 | Loss: 0.00059710
Iteration 17/25 | Loss: 0.00059710
Iteration 18/25 | Loss: 0.00059710
Iteration 19/25 | Loss: 0.00059710
Iteration 20/25 | Loss: 0.00059710
Iteration 21/25 | Loss: 0.00059710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005971030332148075, 0.0005971030332148075, 0.0005971030332148075, 0.0005971030332148075, 0.0005971030332148075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005971030332148075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059710
Iteration 2/1000 | Loss: 0.00005869
Iteration 3/1000 | Loss: 0.00004094
Iteration 4/1000 | Loss: 0.00003618
Iteration 5/1000 | Loss: 0.00003465
Iteration 6/1000 | Loss: 0.00003324
Iteration 7/1000 | Loss: 0.00003207
Iteration 8/1000 | Loss: 0.00003109
Iteration 9/1000 | Loss: 0.00003045
Iteration 10/1000 | Loss: 0.00003001
Iteration 11/1000 | Loss: 0.00002977
Iteration 12/1000 | Loss: 0.00002953
Iteration 13/1000 | Loss: 0.00002946
Iteration 14/1000 | Loss: 0.00002934
Iteration 15/1000 | Loss: 0.00002930
Iteration 16/1000 | Loss: 0.00002928
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002921
Iteration 19/1000 | Loss: 0.00002917
Iteration 20/1000 | Loss: 0.00002916
Iteration 21/1000 | Loss: 0.00002916
Iteration 22/1000 | Loss: 0.00002914
Iteration 23/1000 | Loss: 0.00002913
Iteration 24/1000 | Loss: 0.00002912
Iteration 25/1000 | Loss: 0.00002910
Iteration 26/1000 | Loss: 0.00002910
Iteration 27/1000 | Loss: 0.00002907
Iteration 28/1000 | Loss: 0.00002907
Iteration 29/1000 | Loss: 0.00002905
Iteration 30/1000 | Loss: 0.00002905
Iteration 31/1000 | Loss: 0.00002904
Iteration 32/1000 | Loss: 0.00002903
Iteration 33/1000 | Loss: 0.00002902
Iteration 34/1000 | Loss: 0.00002901
Iteration 35/1000 | Loss: 0.00002901
Iteration 36/1000 | Loss: 0.00002901
Iteration 37/1000 | Loss: 0.00002901
Iteration 38/1000 | Loss: 0.00002900
Iteration 39/1000 | Loss: 0.00002900
Iteration 40/1000 | Loss: 0.00002900
Iteration 41/1000 | Loss: 0.00002900
Iteration 42/1000 | Loss: 0.00002899
Iteration 43/1000 | Loss: 0.00002899
Iteration 44/1000 | Loss: 0.00002899
Iteration 45/1000 | Loss: 0.00002898
Iteration 46/1000 | Loss: 0.00002898
Iteration 47/1000 | Loss: 0.00002898
Iteration 48/1000 | Loss: 0.00002898
Iteration 49/1000 | Loss: 0.00002897
Iteration 50/1000 | Loss: 0.00002897
Iteration 51/1000 | Loss: 0.00002897
Iteration 52/1000 | Loss: 0.00002897
Iteration 53/1000 | Loss: 0.00002897
Iteration 54/1000 | Loss: 0.00002897
Iteration 55/1000 | Loss: 0.00002896
Iteration 56/1000 | Loss: 0.00002896
Iteration 57/1000 | Loss: 0.00002895
Iteration 58/1000 | Loss: 0.00002895
Iteration 59/1000 | Loss: 0.00002894
Iteration 60/1000 | Loss: 0.00002893
Iteration 61/1000 | Loss: 0.00002893
Iteration 62/1000 | Loss: 0.00002892
Iteration 63/1000 | Loss: 0.00002892
Iteration 64/1000 | Loss: 0.00002892
Iteration 65/1000 | Loss: 0.00002892
Iteration 66/1000 | Loss: 0.00002891
Iteration 67/1000 | Loss: 0.00002891
Iteration 68/1000 | Loss: 0.00002891
Iteration 69/1000 | Loss: 0.00002891
Iteration 70/1000 | Loss: 0.00002891
Iteration 71/1000 | Loss: 0.00002890
Iteration 72/1000 | Loss: 0.00002890
Iteration 73/1000 | Loss: 0.00002890
Iteration 74/1000 | Loss: 0.00002890
Iteration 75/1000 | Loss: 0.00002890
Iteration 76/1000 | Loss: 0.00002890
Iteration 77/1000 | Loss: 0.00002890
Iteration 78/1000 | Loss: 0.00002890
Iteration 79/1000 | Loss: 0.00002890
Iteration 80/1000 | Loss: 0.00002890
Iteration 81/1000 | Loss: 0.00002890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.889659663196653e-05, 2.889659663196653e-05, 2.889659663196653e-05, 2.889659663196653e-05, 2.889659663196653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.889659663196653e-05

Optimization complete. Final v2v error: 4.486520767211914 mm

Highest mean error: 5.996887683868408 mm for frame 67

Lowest mean error: 3.854740619659424 mm for frame 44

Saving results

Total time: 36.90678429603577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382729
Iteration 2/25 | Loss: 0.00087224
Iteration 3/25 | Loss: 0.00080360
Iteration 4/25 | Loss: 0.00078549
Iteration 5/25 | Loss: 0.00078296
Iteration 6/25 | Loss: 0.00078268
Iteration 7/25 | Loss: 0.00078268
Iteration 8/25 | Loss: 0.00078268
Iteration 9/25 | Loss: 0.00078268
Iteration 10/25 | Loss: 0.00078268
Iteration 11/25 | Loss: 0.00078268
Iteration 12/25 | Loss: 0.00078268
Iteration 13/25 | Loss: 0.00078268
Iteration 14/25 | Loss: 0.00078268
Iteration 15/25 | Loss: 0.00078268
Iteration 16/25 | Loss: 0.00078268
Iteration 17/25 | Loss: 0.00078268
Iteration 18/25 | Loss: 0.00078268
Iteration 19/25 | Loss: 0.00078268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007826825603842735, 0.0007826825603842735, 0.0007826825603842735, 0.0007826825603842735, 0.0007826825603842735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007826825603842735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58992195
Iteration 2/25 | Loss: 0.00052466
Iteration 3/25 | Loss: 0.00052466
Iteration 4/25 | Loss: 0.00052466
Iteration 5/25 | Loss: 0.00052466
Iteration 6/25 | Loss: 0.00052466
Iteration 7/25 | Loss: 0.00052466
Iteration 8/25 | Loss: 0.00052466
Iteration 9/25 | Loss: 0.00052466
Iteration 10/25 | Loss: 0.00052466
Iteration 11/25 | Loss: 0.00052466
Iteration 12/25 | Loss: 0.00052466
Iteration 13/25 | Loss: 0.00052466
Iteration 14/25 | Loss: 0.00052466
Iteration 15/25 | Loss: 0.00052466
Iteration 16/25 | Loss: 0.00052466
Iteration 17/25 | Loss: 0.00052466
Iteration 18/25 | Loss: 0.00052466
Iteration 19/25 | Loss: 0.00052466
Iteration 20/25 | Loss: 0.00052466
Iteration 21/25 | Loss: 0.00052466
Iteration 22/25 | Loss: 0.00052466
Iteration 23/25 | Loss: 0.00052466
Iteration 24/25 | Loss: 0.00052466
Iteration 25/25 | Loss: 0.00052466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052466
Iteration 2/1000 | Loss: 0.00001848
Iteration 3/1000 | Loss: 0.00001443
Iteration 4/1000 | Loss: 0.00001334
Iteration 5/1000 | Loss: 0.00001240
Iteration 6/1000 | Loss: 0.00001192
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001166
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001145
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001130
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001114
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001107
Iteration 25/1000 | Loss: 0.00001107
Iteration 26/1000 | Loss: 0.00001106
Iteration 27/1000 | Loss: 0.00001106
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001106
Iteration 30/1000 | Loss: 0.00001105
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001102
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001102
Iteration 37/1000 | Loss: 0.00001101
Iteration 38/1000 | Loss: 0.00001101
Iteration 39/1000 | Loss: 0.00001101
Iteration 40/1000 | Loss: 0.00001100
Iteration 41/1000 | Loss: 0.00001100
Iteration 42/1000 | Loss: 0.00001100
Iteration 43/1000 | Loss: 0.00001100
Iteration 44/1000 | Loss: 0.00001099
Iteration 45/1000 | Loss: 0.00001099
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001098
Iteration 48/1000 | Loss: 0.00001098
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001097
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001094
Iteration 55/1000 | Loss: 0.00001094
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001084
Iteration 62/1000 | Loss: 0.00001084
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001081
Iteration 81/1000 | Loss: 0.00001081
Iteration 82/1000 | Loss: 0.00001081
Iteration 83/1000 | Loss: 0.00001081
Iteration 84/1000 | Loss: 0.00001081
Iteration 85/1000 | Loss: 0.00001081
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001080
Iteration 88/1000 | Loss: 0.00001080
Iteration 89/1000 | Loss: 0.00001080
Iteration 90/1000 | Loss: 0.00001080
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001079
Iteration 108/1000 | Loss: 0.00001078
Iteration 109/1000 | Loss: 0.00001078
Iteration 110/1000 | Loss: 0.00001078
Iteration 111/1000 | Loss: 0.00001078
Iteration 112/1000 | Loss: 0.00001078
Iteration 113/1000 | Loss: 0.00001078
Iteration 114/1000 | Loss: 0.00001078
Iteration 115/1000 | Loss: 0.00001078
Iteration 116/1000 | Loss: 0.00001078
Iteration 117/1000 | Loss: 0.00001078
Iteration 118/1000 | Loss: 0.00001078
Iteration 119/1000 | Loss: 0.00001078
Iteration 120/1000 | Loss: 0.00001078
Iteration 121/1000 | Loss: 0.00001078
Iteration 122/1000 | Loss: 0.00001078
Iteration 123/1000 | Loss: 0.00001078
Iteration 124/1000 | Loss: 0.00001078
Iteration 125/1000 | Loss: 0.00001078
Iteration 126/1000 | Loss: 0.00001078
Iteration 127/1000 | Loss: 0.00001078
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001078
Iteration 140/1000 | Loss: 0.00001078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.0782037861645222e-05, 1.0782037861645222e-05, 1.0782037861645222e-05, 1.0782037861645222e-05, 1.0782037861645222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0782037861645222e-05

Optimization complete. Final v2v error: 2.8110291957855225 mm

Highest mean error: 2.9149036407470703 mm for frame 24

Lowest mean error: 2.7234859466552734 mm for frame 161

Saving results

Total time: 33.85932731628418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00730814
Iteration 2/25 | Loss: 0.00138083
Iteration 3/25 | Loss: 0.00099740
Iteration 4/25 | Loss: 0.00093727
Iteration 5/25 | Loss: 0.00092415
Iteration 6/25 | Loss: 0.00092210
Iteration 7/25 | Loss: 0.00091167
Iteration 8/25 | Loss: 0.00090193
Iteration 9/25 | Loss: 0.00089743
Iteration 10/25 | Loss: 0.00089425
Iteration 11/25 | Loss: 0.00089207
Iteration 12/25 | Loss: 0.00089146
Iteration 13/25 | Loss: 0.00089125
Iteration 14/25 | Loss: 0.00089120
Iteration 15/25 | Loss: 0.00089120
Iteration 16/25 | Loss: 0.00089120
Iteration 17/25 | Loss: 0.00089120
Iteration 18/25 | Loss: 0.00089120
Iteration 19/25 | Loss: 0.00089120
Iteration 20/25 | Loss: 0.00089120
Iteration 21/25 | Loss: 0.00089120
Iteration 22/25 | Loss: 0.00089120
Iteration 23/25 | Loss: 0.00089119
Iteration 24/25 | Loss: 0.00089119
Iteration 25/25 | Loss: 0.00089119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44968522
Iteration 2/25 | Loss: 0.00054380
Iteration 3/25 | Loss: 0.00054377
Iteration 4/25 | Loss: 0.00054377
Iteration 5/25 | Loss: 0.00054377
Iteration 6/25 | Loss: 0.00054377
Iteration 7/25 | Loss: 0.00054377
Iteration 8/25 | Loss: 0.00054377
Iteration 9/25 | Loss: 0.00054377
Iteration 10/25 | Loss: 0.00054377
Iteration 11/25 | Loss: 0.00054377
Iteration 12/25 | Loss: 0.00054377
Iteration 13/25 | Loss: 0.00054377
Iteration 14/25 | Loss: 0.00054377
Iteration 15/25 | Loss: 0.00054377
Iteration 16/25 | Loss: 0.00054377
Iteration 17/25 | Loss: 0.00054377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005437658983282745, 0.0005437658983282745, 0.0005437658983282745, 0.0005437658983282745, 0.0005437658983282745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005437658983282745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054377
Iteration 2/1000 | Loss: 0.00003670
Iteration 3/1000 | Loss: 0.00002504
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002121
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001905
Iteration 10/1000 | Loss: 0.00001888
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001871
Iteration 13/1000 | Loss: 0.00001866
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001862
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001858
Iteration 23/1000 | Loss: 0.00001858
Iteration 24/1000 | Loss: 0.00001858
Iteration 25/1000 | Loss: 0.00001857
Iteration 26/1000 | Loss: 0.00001857
Iteration 27/1000 | Loss: 0.00001857
Iteration 28/1000 | Loss: 0.00001857
Iteration 29/1000 | Loss: 0.00001857
Iteration 30/1000 | Loss: 0.00001857
Iteration 31/1000 | Loss: 0.00001857
Iteration 32/1000 | Loss: 0.00001857
Iteration 33/1000 | Loss: 0.00001857
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001853
Iteration 39/1000 | Loss: 0.00001853
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001853
Iteration 42/1000 | Loss: 0.00001853
Iteration 43/1000 | Loss: 0.00001853
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001853
Iteration 46/1000 | Loss: 0.00001853
Iteration 47/1000 | Loss: 0.00001852
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001851
Iteration 52/1000 | Loss: 0.00001851
Iteration 53/1000 | Loss: 0.00001851
Iteration 54/1000 | Loss: 0.00001851
Iteration 55/1000 | Loss: 0.00001850
Iteration 56/1000 | Loss: 0.00001850
Iteration 57/1000 | Loss: 0.00001850
Iteration 58/1000 | Loss: 0.00001850
Iteration 59/1000 | Loss: 0.00001850
Iteration 60/1000 | Loss: 0.00001850
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001848
Iteration 69/1000 | Loss: 0.00001848
Iteration 70/1000 | Loss: 0.00001848
Iteration 71/1000 | Loss: 0.00001848
Iteration 72/1000 | Loss: 0.00001848
Iteration 73/1000 | Loss: 0.00001848
Iteration 74/1000 | Loss: 0.00001848
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001848
Iteration 77/1000 | Loss: 0.00001848
Iteration 78/1000 | Loss: 0.00001847
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001846
Iteration 85/1000 | Loss: 0.00001846
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001845
Iteration 89/1000 | Loss: 0.00001845
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001845
Iteration 92/1000 | Loss: 0.00001845
Iteration 93/1000 | Loss: 0.00001845
Iteration 94/1000 | Loss: 0.00001845
Iteration 95/1000 | Loss: 0.00001845
Iteration 96/1000 | Loss: 0.00001845
Iteration 97/1000 | Loss: 0.00001844
Iteration 98/1000 | Loss: 0.00001844
Iteration 99/1000 | Loss: 0.00001844
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001843
Iteration 106/1000 | Loss: 0.00001843
Iteration 107/1000 | Loss: 0.00001842
Iteration 108/1000 | Loss: 0.00001842
Iteration 109/1000 | Loss: 0.00001842
Iteration 110/1000 | Loss: 0.00001842
Iteration 111/1000 | Loss: 0.00001842
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001841
Iteration 115/1000 | Loss: 0.00001840
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001840
Iteration 118/1000 | Loss: 0.00001840
Iteration 119/1000 | Loss: 0.00001840
Iteration 120/1000 | Loss: 0.00001839
Iteration 121/1000 | Loss: 0.00001839
Iteration 122/1000 | Loss: 0.00001839
Iteration 123/1000 | Loss: 0.00001839
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001838
Iteration 128/1000 | Loss: 0.00001838
Iteration 129/1000 | Loss: 0.00001838
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001837
Iteration 133/1000 | Loss: 0.00001837
Iteration 134/1000 | Loss: 0.00001837
Iteration 135/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.837477793742437e-05, 1.837477793742437e-05, 1.837477793742437e-05, 1.837477793742437e-05, 1.837477793742437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.837477793742437e-05

Optimization complete. Final v2v error: 3.542383909225464 mm

Highest mean error: 4.089116096496582 mm for frame 119

Lowest mean error: 2.9628896713256836 mm for frame 16

Saving results

Total time: 56.39513278007507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368506
Iteration 2/25 | Loss: 0.00108960
Iteration 3/25 | Loss: 0.00088688
Iteration 4/25 | Loss: 0.00085375
Iteration 5/25 | Loss: 0.00084394
Iteration 6/25 | Loss: 0.00084212
Iteration 7/25 | Loss: 0.00084173
Iteration 8/25 | Loss: 0.00084173
Iteration 9/25 | Loss: 0.00084173
Iteration 10/25 | Loss: 0.00084173
Iteration 11/25 | Loss: 0.00084173
Iteration 12/25 | Loss: 0.00084173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008417253848165274, 0.0008417253848165274, 0.0008417253848165274, 0.0008417253848165274, 0.0008417253848165274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008417253848165274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90949905
Iteration 2/25 | Loss: 0.00061552
Iteration 3/25 | Loss: 0.00061552
Iteration 4/25 | Loss: 0.00061552
Iteration 5/25 | Loss: 0.00061552
Iteration 6/25 | Loss: 0.00061552
Iteration 7/25 | Loss: 0.00061552
Iteration 8/25 | Loss: 0.00061552
Iteration 9/25 | Loss: 0.00061552
Iteration 10/25 | Loss: 0.00061552
Iteration 11/25 | Loss: 0.00061552
Iteration 12/25 | Loss: 0.00061552
Iteration 13/25 | Loss: 0.00061552
Iteration 14/25 | Loss: 0.00061552
Iteration 15/25 | Loss: 0.00061552
Iteration 16/25 | Loss: 0.00061552
Iteration 17/25 | Loss: 0.00061552
Iteration 18/25 | Loss: 0.00061552
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006155167939141393, 0.0006155167939141393, 0.0006155167939141393, 0.0006155167939141393, 0.0006155167939141393]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006155167939141393

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061552
Iteration 2/1000 | Loss: 0.00002996
Iteration 3/1000 | Loss: 0.00002058
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001714
Iteration 7/1000 | Loss: 0.00001667
Iteration 8/1000 | Loss: 0.00001631
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001545
Iteration 13/1000 | Loss: 0.00001531
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001505
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00001503
Iteration 26/1000 | Loss: 0.00001503
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001503
Iteration 29/1000 | Loss: 0.00001502
Iteration 30/1000 | Loss: 0.00001502
Iteration 31/1000 | Loss: 0.00001501
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001500
Iteration 36/1000 | Loss: 0.00001500
Iteration 37/1000 | Loss: 0.00001499
Iteration 38/1000 | Loss: 0.00001499
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001498
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001497
Iteration 43/1000 | Loss: 0.00001497
Iteration 44/1000 | Loss: 0.00001496
Iteration 45/1000 | Loss: 0.00001496
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001490
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001488
Iteration 66/1000 | Loss: 0.00001488
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001487
Iteration 70/1000 | Loss: 0.00001487
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001484
Iteration 87/1000 | Loss: 0.00001483
Iteration 88/1000 | Loss: 0.00001483
Iteration 89/1000 | Loss: 0.00001483
Iteration 90/1000 | Loss: 0.00001483
Iteration 91/1000 | Loss: 0.00001483
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001482
Iteration 104/1000 | Loss: 0.00001482
Iteration 105/1000 | Loss: 0.00001482
Iteration 106/1000 | Loss: 0.00001482
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001481
Iteration 120/1000 | Loss: 0.00001481
Iteration 121/1000 | Loss: 0.00001480
Iteration 122/1000 | Loss: 0.00001480
Iteration 123/1000 | Loss: 0.00001480
Iteration 124/1000 | Loss: 0.00001480
Iteration 125/1000 | Loss: 0.00001480
Iteration 126/1000 | Loss: 0.00001480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.4803808880969882e-05, 1.4803808880969882e-05, 1.4803808880969882e-05, 1.4803808880969882e-05, 1.4803808880969882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4803808880969882e-05

Optimization complete. Final v2v error: 3.288266897201538 mm

Highest mean error: 3.834550619125366 mm for frame 75

Lowest mean error: 2.895620107650757 mm for frame 230

Saving results

Total time: 42.98293709754944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610592
Iteration 2/25 | Loss: 0.00135953
Iteration 3/25 | Loss: 0.00096563
Iteration 4/25 | Loss: 0.00090773
Iteration 5/25 | Loss: 0.00089366
Iteration 6/25 | Loss: 0.00088972
Iteration 7/25 | Loss: 0.00088874
Iteration 8/25 | Loss: 0.00088849
Iteration 9/25 | Loss: 0.00088849
Iteration 10/25 | Loss: 0.00088849
Iteration 11/25 | Loss: 0.00088849
Iteration 12/25 | Loss: 0.00088849
Iteration 13/25 | Loss: 0.00088849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008884858107194304, 0.0008884858107194304, 0.0008884858107194304, 0.0008884858107194304, 0.0008884858107194304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008884858107194304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32075369
Iteration 2/25 | Loss: 0.00062237
Iteration 3/25 | Loss: 0.00062237
Iteration 4/25 | Loss: 0.00062237
Iteration 5/25 | Loss: 0.00062237
Iteration 6/25 | Loss: 0.00062237
Iteration 7/25 | Loss: 0.00062237
Iteration 8/25 | Loss: 0.00062237
Iteration 9/25 | Loss: 0.00062237
Iteration 10/25 | Loss: 0.00062237
Iteration 11/25 | Loss: 0.00062237
Iteration 12/25 | Loss: 0.00062237
Iteration 13/25 | Loss: 0.00062237
Iteration 14/25 | Loss: 0.00062237
Iteration 15/25 | Loss: 0.00062237
Iteration 16/25 | Loss: 0.00062237
Iteration 17/25 | Loss: 0.00062237
Iteration 18/25 | Loss: 0.00062237
Iteration 19/25 | Loss: 0.00062237
Iteration 20/25 | Loss: 0.00062237
Iteration 21/25 | Loss: 0.00062237
Iteration 22/25 | Loss: 0.00062237
Iteration 23/25 | Loss: 0.00062237
Iteration 24/25 | Loss: 0.00062237
Iteration 25/25 | Loss: 0.00062237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062237
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00002531
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002147
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001983
Iteration 9/1000 | Loss: 0.00001951
Iteration 10/1000 | Loss: 0.00001928
Iteration 11/1000 | Loss: 0.00001918
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001889
Iteration 19/1000 | Loss: 0.00001883
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001882
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001861
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001860
Iteration 35/1000 | Loss: 0.00001860
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001856
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001855
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001854
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001854
Iteration 56/1000 | Loss: 0.00001854
Iteration 57/1000 | Loss: 0.00001854
Iteration 58/1000 | Loss: 0.00001853
Iteration 59/1000 | Loss: 0.00001853
Iteration 60/1000 | Loss: 0.00001853
Iteration 61/1000 | Loss: 0.00001853
Iteration 62/1000 | Loss: 0.00001853
Iteration 63/1000 | Loss: 0.00001853
Iteration 64/1000 | Loss: 0.00001853
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00001853
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001852
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001852
Iteration 71/1000 | Loss: 0.00001852
Iteration 72/1000 | Loss: 0.00001852
Iteration 73/1000 | Loss: 0.00001852
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001852
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001851
Iteration 80/1000 | Loss: 0.00001851
Iteration 81/1000 | Loss: 0.00001851
Iteration 82/1000 | Loss: 0.00001851
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001848
Iteration 111/1000 | Loss: 0.00001848
Iteration 112/1000 | Loss: 0.00001848
Iteration 113/1000 | Loss: 0.00001848
Iteration 114/1000 | Loss: 0.00001848
Iteration 115/1000 | Loss: 0.00001848
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001848
Iteration 119/1000 | Loss: 0.00001848
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001847
Iteration 122/1000 | Loss: 0.00001847
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001847
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001846
Iteration 130/1000 | Loss: 0.00001846
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001846
Iteration 133/1000 | Loss: 0.00001846
Iteration 134/1000 | Loss: 0.00001846
Iteration 135/1000 | Loss: 0.00001846
Iteration 136/1000 | Loss: 0.00001846
Iteration 137/1000 | Loss: 0.00001846
Iteration 138/1000 | Loss: 0.00001846
Iteration 139/1000 | Loss: 0.00001846
Iteration 140/1000 | Loss: 0.00001846
Iteration 141/1000 | Loss: 0.00001846
Iteration 142/1000 | Loss: 0.00001846
Iteration 143/1000 | Loss: 0.00001846
Iteration 144/1000 | Loss: 0.00001846
Iteration 145/1000 | Loss: 0.00001846
Iteration 146/1000 | Loss: 0.00001846
Iteration 147/1000 | Loss: 0.00001846
Iteration 148/1000 | Loss: 0.00001846
Iteration 149/1000 | Loss: 0.00001846
Iteration 150/1000 | Loss: 0.00001846
Iteration 151/1000 | Loss: 0.00001846
Iteration 152/1000 | Loss: 0.00001846
Iteration 153/1000 | Loss: 0.00001846
Iteration 154/1000 | Loss: 0.00001846
Iteration 155/1000 | Loss: 0.00001846
Iteration 156/1000 | Loss: 0.00001846
Iteration 157/1000 | Loss: 0.00001846
Iteration 158/1000 | Loss: 0.00001846
Iteration 159/1000 | Loss: 0.00001846
Iteration 160/1000 | Loss: 0.00001846
Iteration 161/1000 | Loss: 0.00001846
Iteration 162/1000 | Loss: 0.00001846
Iteration 163/1000 | Loss: 0.00001846
Iteration 164/1000 | Loss: 0.00001846
Iteration 165/1000 | Loss: 0.00001846
Iteration 166/1000 | Loss: 0.00001846
Iteration 167/1000 | Loss: 0.00001846
Iteration 168/1000 | Loss: 0.00001846
Iteration 169/1000 | Loss: 0.00001846
Iteration 170/1000 | Loss: 0.00001846
Iteration 171/1000 | Loss: 0.00001846
Iteration 172/1000 | Loss: 0.00001846
Iteration 173/1000 | Loss: 0.00001846
Iteration 174/1000 | Loss: 0.00001846
Iteration 175/1000 | Loss: 0.00001846
Iteration 176/1000 | Loss: 0.00001846
Iteration 177/1000 | Loss: 0.00001846
Iteration 178/1000 | Loss: 0.00001846
Iteration 179/1000 | Loss: 0.00001846
Iteration 180/1000 | Loss: 0.00001846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8459091734257527e-05, 1.8459091734257527e-05, 1.8459091734257527e-05, 1.8459091734257527e-05, 1.8459091734257527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8459091734257527e-05

Optimization complete. Final v2v error: 3.543454170227051 mm

Highest mean error: 5.464182376861572 mm for frame 58

Lowest mean error: 3.1064579486846924 mm for frame 130

Saving results

Total time: 40.68212652206421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820607
Iteration 2/25 | Loss: 0.00163095
Iteration 3/25 | Loss: 0.00099424
Iteration 4/25 | Loss: 0.00090440
Iteration 5/25 | Loss: 0.00089326
Iteration 6/25 | Loss: 0.00089134
Iteration 7/25 | Loss: 0.00089107
Iteration 8/25 | Loss: 0.00089097
Iteration 9/25 | Loss: 0.00089097
Iteration 10/25 | Loss: 0.00089097
Iteration 11/25 | Loss: 0.00089097
Iteration 12/25 | Loss: 0.00089097
Iteration 13/25 | Loss: 0.00089097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008909660391509533, 0.0008909660391509533, 0.0008909660391509533, 0.0008909660391509533, 0.0008909660391509533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008909660391509533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41007638
Iteration 2/25 | Loss: 0.00056676
Iteration 3/25 | Loss: 0.00056673
Iteration 4/25 | Loss: 0.00056673
Iteration 5/25 | Loss: 0.00056673
Iteration 6/25 | Loss: 0.00056673
Iteration 7/25 | Loss: 0.00056673
Iteration 8/25 | Loss: 0.00056673
Iteration 9/25 | Loss: 0.00056673
Iteration 10/25 | Loss: 0.00056673
Iteration 11/25 | Loss: 0.00056673
Iteration 12/25 | Loss: 0.00056673
Iteration 13/25 | Loss: 0.00056673
Iteration 14/25 | Loss: 0.00056673
Iteration 15/25 | Loss: 0.00056673
Iteration 16/25 | Loss: 0.00056673
Iteration 17/25 | Loss: 0.00056673
Iteration 18/25 | Loss: 0.00056673
Iteration 19/25 | Loss: 0.00056673
Iteration 20/25 | Loss: 0.00056673
Iteration 21/25 | Loss: 0.00056673
Iteration 22/25 | Loss: 0.00056673
Iteration 23/25 | Loss: 0.00056673
Iteration 24/25 | Loss: 0.00056673
Iteration 25/25 | Loss: 0.00056673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056673
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00002024
Iteration 7/1000 | Loss: 0.00001997
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00001924
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001898
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001893
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001880
Iteration 18/1000 | Loss: 0.00001880
Iteration 19/1000 | Loss: 0.00001880
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001878
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001875
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001870
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001862
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001861
Iteration 92/1000 | Loss: 0.00001861
Iteration 93/1000 | Loss: 0.00001861
Iteration 94/1000 | Loss: 0.00001861
Iteration 95/1000 | Loss: 0.00001861
Iteration 96/1000 | Loss: 0.00001861
Iteration 97/1000 | Loss: 0.00001861
Iteration 98/1000 | Loss: 0.00001861
Iteration 99/1000 | Loss: 0.00001861
Iteration 100/1000 | Loss: 0.00001861
Iteration 101/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.8607259335112758e-05, 1.8607259335112758e-05, 1.8607259335112758e-05, 1.8607259335112758e-05, 1.8607259335112758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8607259335112758e-05

Optimization complete. Final v2v error: 3.5823893547058105 mm

Highest mean error: 3.9146504402160645 mm for frame 117

Lowest mean error: 3.350613832473755 mm for frame 45

Saving results

Total time: 32.075239419937134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006700
Iteration 2/25 | Loss: 0.00312565
Iteration 3/25 | Loss: 0.00230805
Iteration 4/25 | Loss: 0.00203992
Iteration 5/25 | Loss: 0.00199047
Iteration 6/25 | Loss: 0.00194027
Iteration 7/25 | Loss: 0.00165946
Iteration 8/25 | Loss: 0.00139537
Iteration 9/25 | Loss: 0.00130377
Iteration 10/25 | Loss: 0.00124293
Iteration 11/25 | Loss: 0.00120738
Iteration 12/25 | Loss: 0.00119410
Iteration 13/25 | Loss: 0.00117058
Iteration 14/25 | Loss: 0.00116110
Iteration 15/25 | Loss: 0.00117679
Iteration 16/25 | Loss: 0.00114255
Iteration 17/25 | Loss: 0.00112607
Iteration 18/25 | Loss: 0.00111444
Iteration 19/25 | Loss: 0.00109343
Iteration 20/25 | Loss: 0.00108693
Iteration 21/25 | Loss: 0.00108662
Iteration 22/25 | Loss: 0.00109922
Iteration 23/25 | Loss: 0.00109980
Iteration 24/25 | Loss: 0.00107466
Iteration 25/25 | Loss: 0.00109081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49278057
Iteration 2/25 | Loss: 0.00248673
Iteration 3/25 | Loss: 0.00197101
Iteration 4/25 | Loss: 0.00197101
Iteration 5/25 | Loss: 0.00197101
Iteration 6/25 | Loss: 0.00197101
Iteration 7/25 | Loss: 0.00197101
Iteration 8/25 | Loss: 0.00197101
Iteration 9/25 | Loss: 0.00197101
Iteration 10/25 | Loss: 0.00197101
Iteration 11/25 | Loss: 0.00197101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0019710075575858355, 0.0019710075575858355, 0.0019710075575858355, 0.0019710075575858355, 0.0019710075575858355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019710075575858355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197101
Iteration 2/1000 | Loss: 0.00125943
Iteration 3/1000 | Loss: 0.00135216
Iteration 4/1000 | Loss: 0.00039014
Iteration 5/1000 | Loss: 0.00017527
Iteration 6/1000 | Loss: 0.00055028
Iteration 7/1000 | Loss: 0.00015627
Iteration 8/1000 | Loss: 0.00012425
Iteration 9/1000 | Loss: 0.00030927
Iteration 10/1000 | Loss: 0.00030434
Iteration 11/1000 | Loss: 0.00020025
Iteration 12/1000 | Loss: 0.00012270
Iteration 13/1000 | Loss: 0.00009557
Iteration 14/1000 | Loss: 0.00009081
Iteration 15/1000 | Loss: 0.00011500
Iteration 16/1000 | Loss: 0.00043467
Iteration 17/1000 | Loss: 0.00133988
Iteration 18/1000 | Loss: 0.00087653
Iteration 19/1000 | Loss: 0.00031315
Iteration 20/1000 | Loss: 0.00075527
Iteration 21/1000 | Loss: 0.00047525
Iteration 22/1000 | Loss: 0.00044135
Iteration 23/1000 | Loss: 0.00027338
Iteration 24/1000 | Loss: 0.00025094
Iteration 25/1000 | Loss: 0.00037410
Iteration 26/1000 | Loss: 0.00011240
Iteration 27/1000 | Loss: 0.00025538
Iteration 28/1000 | Loss: 0.00046235
Iteration 29/1000 | Loss: 0.00016065
Iteration 30/1000 | Loss: 0.00030622
Iteration 31/1000 | Loss: 0.00068251
Iteration 32/1000 | Loss: 0.00046708
Iteration 33/1000 | Loss: 0.00015104
Iteration 34/1000 | Loss: 0.00022988
Iteration 35/1000 | Loss: 0.00009610
Iteration 36/1000 | Loss: 0.00008001
Iteration 37/1000 | Loss: 0.00010374
Iteration 38/1000 | Loss: 0.00008360
Iteration 39/1000 | Loss: 0.00022035
Iteration 40/1000 | Loss: 0.00019527
Iteration 41/1000 | Loss: 0.00008866
Iteration 42/1000 | Loss: 0.00009306
Iteration 43/1000 | Loss: 0.00007800
Iteration 44/1000 | Loss: 0.00007879
Iteration 45/1000 | Loss: 0.00009416
Iteration 46/1000 | Loss: 0.00043727
Iteration 47/1000 | Loss: 0.00023905
Iteration 48/1000 | Loss: 0.00015905
Iteration 49/1000 | Loss: 0.00007335
Iteration 50/1000 | Loss: 0.00009106
Iteration 51/1000 | Loss: 0.00008898
Iteration 52/1000 | Loss: 0.00009036
Iteration 53/1000 | Loss: 0.00008957
Iteration 54/1000 | Loss: 0.00024760
Iteration 55/1000 | Loss: 0.00015448
Iteration 56/1000 | Loss: 0.00023642
Iteration 57/1000 | Loss: 0.00014880
Iteration 58/1000 | Loss: 0.00026813
Iteration 59/1000 | Loss: 0.00007362
Iteration 60/1000 | Loss: 0.00008465
Iteration 61/1000 | Loss: 0.00035830
Iteration 62/1000 | Loss: 0.00006195
Iteration 63/1000 | Loss: 0.00019303
Iteration 64/1000 | Loss: 0.00013197
Iteration 65/1000 | Loss: 0.00019822
Iteration 66/1000 | Loss: 0.00009105
Iteration 67/1000 | Loss: 0.00031421
Iteration 68/1000 | Loss: 0.00006465
Iteration 69/1000 | Loss: 0.00006635
Iteration 70/1000 | Loss: 0.00016503
Iteration 71/1000 | Loss: 0.00006667
Iteration 72/1000 | Loss: 0.00006423
Iteration 73/1000 | Loss: 0.00007735
Iteration 74/1000 | Loss: 0.00006092
Iteration 75/1000 | Loss: 0.00006018
Iteration 76/1000 | Loss: 0.00007155
Iteration 77/1000 | Loss: 0.00017290
Iteration 78/1000 | Loss: 0.00034893
Iteration 79/1000 | Loss: 0.00015550
Iteration 80/1000 | Loss: 0.00027768
Iteration 81/1000 | Loss: 0.00024997
Iteration 82/1000 | Loss: 0.00020400
Iteration 83/1000 | Loss: 0.00027451
Iteration 84/1000 | Loss: 0.00016617
Iteration 85/1000 | Loss: 0.00005958
Iteration 86/1000 | Loss: 0.00030700
Iteration 87/1000 | Loss: 0.00005171
Iteration 88/1000 | Loss: 0.00004987
Iteration 89/1000 | Loss: 0.00004864
Iteration 90/1000 | Loss: 0.00004737
Iteration 91/1000 | Loss: 0.00004575
Iteration 92/1000 | Loss: 0.00004486
Iteration 93/1000 | Loss: 0.00004409
Iteration 94/1000 | Loss: 0.00004374
Iteration 95/1000 | Loss: 0.00004348
Iteration 96/1000 | Loss: 0.00004319
Iteration 97/1000 | Loss: 0.00004304
Iteration 98/1000 | Loss: 0.00020984
Iteration 99/1000 | Loss: 0.00006649
Iteration 100/1000 | Loss: 0.00004283
Iteration 101/1000 | Loss: 0.00014752
Iteration 102/1000 | Loss: 0.00028547
Iteration 103/1000 | Loss: 0.00019915
Iteration 104/1000 | Loss: 0.00004556
Iteration 105/1000 | Loss: 0.00032551
Iteration 106/1000 | Loss: 0.00040128
Iteration 107/1000 | Loss: 0.00035569
Iteration 108/1000 | Loss: 0.00014808
Iteration 109/1000 | Loss: 0.00011438
Iteration 110/1000 | Loss: 0.00004679
Iteration 111/1000 | Loss: 0.00004446
Iteration 112/1000 | Loss: 0.00004326
Iteration 113/1000 | Loss: 0.00004222
Iteration 114/1000 | Loss: 0.00004159
Iteration 115/1000 | Loss: 0.00004112
Iteration 116/1000 | Loss: 0.00004061
Iteration 117/1000 | Loss: 0.00004034
Iteration 118/1000 | Loss: 0.00004029
Iteration 119/1000 | Loss: 0.00004024
Iteration 120/1000 | Loss: 0.00004023
Iteration 121/1000 | Loss: 0.00004023
Iteration 122/1000 | Loss: 0.00004022
Iteration 123/1000 | Loss: 0.00004021
Iteration 124/1000 | Loss: 0.00004020
Iteration 125/1000 | Loss: 0.00004016
Iteration 126/1000 | Loss: 0.00004015
Iteration 127/1000 | Loss: 0.00004011
Iteration 128/1000 | Loss: 0.00004011
Iteration 129/1000 | Loss: 0.00004010
Iteration 130/1000 | Loss: 0.00004009
Iteration 131/1000 | Loss: 0.00004008
Iteration 132/1000 | Loss: 0.00004007
Iteration 133/1000 | Loss: 0.00004007
Iteration 134/1000 | Loss: 0.00004007
Iteration 135/1000 | Loss: 0.00004007
Iteration 136/1000 | Loss: 0.00004007
Iteration 137/1000 | Loss: 0.00004006
Iteration 138/1000 | Loss: 0.00004006
Iteration 139/1000 | Loss: 0.00004006
Iteration 140/1000 | Loss: 0.00004006
Iteration 141/1000 | Loss: 0.00004006
Iteration 142/1000 | Loss: 0.00004006
Iteration 143/1000 | Loss: 0.00004005
Iteration 144/1000 | Loss: 0.00004005
Iteration 145/1000 | Loss: 0.00004005
Iteration 146/1000 | Loss: 0.00004005
Iteration 147/1000 | Loss: 0.00004005
Iteration 148/1000 | Loss: 0.00004005
Iteration 149/1000 | Loss: 0.00004004
Iteration 150/1000 | Loss: 0.00004004
Iteration 151/1000 | Loss: 0.00004003
Iteration 152/1000 | Loss: 0.00004003
Iteration 153/1000 | Loss: 0.00004003
Iteration 154/1000 | Loss: 0.00004003
Iteration 155/1000 | Loss: 0.00004001
Iteration 156/1000 | Loss: 0.00004000
Iteration 157/1000 | Loss: 0.00004000
Iteration 158/1000 | Loss: 0.00004000
Iteration 159/1000 | Loss: 0.00003999
Iteration 160/1000 | Loss: 0.00003999
Iteration 161/1000 | Loss: 0.00003998
Iteration 162/1000 | Loss: 0.00003998
Iteration 163/1000 | Loss: 0.00003997
Iteration 164/1000 | Loss: 0.00003997
Iteration 165/1000 | Loss: 0.00003996
Iteration 166/1000 | Loss: 0.00003996
Iteration 167/1000 | Loss: 0.00003995
Iteration 168/1000 | Loss: 0.00003995
Iteration 169/1000 | Loss: 0.00003995
Iteration 170/1000 | Loss: 0.00003994
Iteration 171/1000 | Loss: 0.00003994
Iteration 172/1000 | Loss: 0.00003993
Iteration 173/1000 | Loss: 0.00003993
Iteration 174/1000 | Loss: 0.00003993
Iteration 175/1000 | Loss: 0.00003992
Iteration 176/1000 | Loss: 0.00003992
Iteration 177/1000 | Loss: 0.00003992
Iteration 178/1000 | Loss: 0.00003991
Iteration 179/1000 | Loss: 0.00003991
Iteration 180/1000 | Loss: 0.00003991
Iteration 181/1000 | Loss: 0.00003990
Iteration 182/1000 | Loss: 0.00003990
Iteration 183/1000 | Loss: 0.00003990
Iteration 184/1000 | Loss: 0.00003990
Iteration 185/1000 | Loss: 0.00003990
Iteration 186/1000 | Loss: 0.00003990
Iteration 187/1000 | Loss: 0.00003990
Iteration 188/1000 | Loss: 0.00003990
Iteration 189/1000 | Loss: 0.00003990
Iteration 190/1000 | Loss: 0.00003990
Iteration 191/1000 | Loss: 0.00003990
Iteration 192/1000 | Loss: 0.00003989
Iteration 193/1000 | Loss: 0.00003989
Iteration 194/1000 | Loss: 0.00003989
Iteration 195/1000 | Loss: 0.00003989
Iteration 196/1000 | Loss: 0.00003989
Iteration 197/1000 | Loss: 0.00003989
Iteration 198/1000 | Loss: 0.00003989
Iteration 199/1000 | Loss: 0.00003989
Iteration 200/1000 | Loss: 0.00003989
Iteration 201/1000 | Loss: 0.00003989
Iteration 202/1000 | Loss: 0.00003989
Iteration 203/1000 | Loss: 0.00003989
Iteration 204/1000 | Loss: 0.00003989
Iteration 205/1000 | Loss: 0.00003989
Iteration 206/1000 | Loss: 0.00003988
Iteration 207/1000 | Loss: 0.00003988
Iteration 208/1000 | Loss: 0.00003988
Iteration 209/1000 | Loss: 0.00003988
Iteration 210/1000 | Loss: 0.00003988
Iteration 211/1000 | Loss: 0.00003987
Iteration 212/1000 | Loss: 0.00003987
Iteration 213/1000 | Loss: 0.00003987
Iteration 214/1000 | Loss: 0.00003987
Iteration 215/1000 | Loss: 0.00003987
Iteration 216/1000 | Loss: 0.00003986
Iteration 217/1000 | Loss: 0.00003986
Iteration 218/1000 | Loss: 0.00003986
Iteration 219/1000 | Loss: 0.00003986
Iteration 220/1000 | Loss: 0.00003986
Iteration 221/1000 | Loss: 0.00003986
Iteration 222/1000 | Loss: 0.00003986
Iteration 223/1000 | Loss: 0.00003986
Iteration 224/1000 | Loss: 0.00003986
Iteration 225/1000 | Loss: 0.00003986
Iteration 226/1000 | Loss: 0.00003986
Iteration 227/1000 | Loss: 0.00003986
Iteration 228/1000 | Loss: 0.00003986
Iteration 229/1000 | Loss: 0.00003986
Iteration 230/1000 | Loss: 0.00003985
Iteration 231/1000 | Loss: 0.00003985
Iteration 232/1000 | Loss: 0.00003985
Iteration 233/1000 | Loss: 0.00003985
Iteration 234/1000 | Loss: 0.00003985
Iteration 235/1000 | Loss: 0.00003985
Iteration 236/1000 | Loss: 0.00003985
Iteration 237/1000 | Loss: 0.00003985
Iteration 238/1000 | Loss: 0.00003985
Iteration 239/1000 | Loss: 0.00003985
Iteration 240/1000 | Loss: 0.00003985
Iteration 241/1000 | Loss: 0.00003985
Iteration 242/1000 | Loss: 0.00003985
Iteration 243/1000 | Loss: 0.00003985
Iteration 244/1000 | Loss: 0.00003985
Iteration 245/1000 | Loss: 0.00003985
Iteration 246/1000 | Loss: 0.00003985
Iteration 247/1000 | Loss: 0.00003985
Iteration 248/1000 | Loss: 0.00003985
Iteration 249/1000 | Loss: 0.00003985
Iteration 250/1000 | Loss: 0.00003985
Iteration 251/1000 | Loss: 0.00003985
Iteration 252/1000 | Loss: 0.00003985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.985280272900127e-05, 3.985280272900127e-05, 3.985280272900127e-05, 3.985280272900127e-05, 3.985280272900127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.985280272900127e-05

Optimization complete. Final v2v error: 4.2574896812438965 mm

Highest mean error: 10.77387809753418 mm for frame 182

Lowest mean error: 3.6440014839172363 mm for frame 6

Saving results

Total time: 251.71353793144226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841585
Iteration 2/25 | Loss: 0.00102825
Iteration 3/25 | Loss: 0.00084043
Iteration 4/25 | Loss: 0.00081404
Iteration 5/25 | Loss: 0.00080819
Iteration 6/25 | Loss: 0.00080585
Iteration 7/25 | Loss: 0.00080530
Iteration 8/25 | Loss: 0.00080530
Iteration 9/25 | Loss: 0.00080530
Iteration 10/25 | Loss: 0.00080530
Iteration 11/25 | Loss: 0.00080530
Iteration 12/25 | Loss: 0.00080530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008053007768467069, 0.0008053007768467069, 0.0008053007768467069, 0.0008053007768467069, 0.0008053007768467069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008053007768467069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50575149
Iteration 2/25 | Loss: 0.00055210
Iteration 3/25 | Loss: 0.00055207
Iteration 4/25 | Loss: 0.00055207
Iteration 5/25 | Loss: 0.00055207
Iteration 6/25 | Loss: 0.00055207
Iteration 7/25 | Loss: 0.00055207
Iteration 8/25 | Loss: 0.00055207
Iteration 9/25 | Loss: 0.00055207
Iteration 10/25 | Loss: 0.00055207
Iteration 11/25 | Loss: 0.00055207
Iteration 12/25 | Loss: 0.00055207
Iteration 13/25 | Loss: 0.00055207
Iteration 14/25 | Loss: 0.00055207
Iteration 15/25 | Loss: 0.00055207
Iteration 16/25 | Loss: 0.00055207
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005520726554095745, 0.0005520726554095745, 0.0005520726554095745, 0.0005520726554095745, 0.0005520726554095745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005520726554095745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055207
Iteration 2/1000 | Loss: 0.00001977
Iteration 3/1000 | Loss: 0.00001488
Iteration 4/1000 | Loss: 0.00001365
Iteration 5/1000 | Loss: 0.00001272
Iteration 6/1000 | Loss: 0.00001199
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001141
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001129
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001123
Iteration 16/1000 | Loss: 0.00001122
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001117
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001114
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001107
Iteration 24/1000 | Loss: 0.00001106
Iteration 25/1000 | Loss: 0.00001104
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001102
Iteration 28/1000 | Loss: 0.00001102
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001101
Iteration 32/1000 | Loss: 0.00001100
Iteration 33/1000 | Loss: 0.00001100
Iteration 34/1000 | Loss: 0.00001099
Iteration 35/1000 | Loss: 0.00001098
Iteration 36/1000 | Loss: 0.00001098
Iteration 37/1000 | Loss: 0.00001098
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001097
Iteration 41/1000 | Loss: 0.00001097
Iteration 42/1000 | Loss: 0.00001097
Iteration 43/1000 | Loss: 0.00001096
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001095
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001094
Iteration 52/1000 | Loss: 0.00001094
Iteration 53/1000 | Loss: 0.00001094
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001091
Iteration 64/1000 | Loss: 0.00001091
Iteration 65/1000 | Loss: 0.00001091
Iteration 66/1000 | Loss: 0.00001091
Iteration 67/1000 | Loss: 0.00001091
Iteration 68/1000 | Loss: 0.00001091
Iteration 69/1000 | Loss: 0.00001091
Iteration 70/1000 | Loss: 0.00001091
Iteration 71/1000 | Loss: 0.00001090
Iteration 72/1000 | Loss: 0.00001090
Iteration 73/1000 | Loss: 0.00001090
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001088
Iteration 79/1000 | Loss: 0.00001088
Iteration 80/1000 | Loss: 0.00001088
Iteration 81/1000 | Loss: 0.00001088
Iteration 82/1000 | Loss: 0.00001088
Iteration 83/1000 | Loss: 0.00001088
Iteration 84/1000 | Loss: 0.00001088
Iteration 85/1000 | Loss: 0.00001088
Iteration 86/1000 | Loss: 0.00001088
Iteration 87/1000 | Loss: 0.00001088
Iteration 88/1000 | Loss: 0.00001088
Iteration 89/1000 | Loss: 0.00001088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.0881735761358868e-05, 1.0881735761358868e-05, 1.0881735761358868e-05, 1.0881735761358868e-05, 1.0881735761358868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0881735761358868e-05

Optimization complete. Final v2v error: 2.807424545288086 mm

Highest mean error: 3.046215057373047 mm for frame 6

Lowest mean error: 2.6822526454925537 mm for frame 111

Saving results

Total time: 29.700998783111572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821422
Iteration 2/25 | Loss: 0.00100729
Iteration 3/25 | Loss: 0.00087278
Iteration 4/25 | Loss: 0.00084483
Iteration 5/25 | Loss: 0.00083796
Iteration 6/25 | Loss: 0.00083605
Iteration 7/25 | Loss: 0.00083569
Iteration 8/25 | Loss: 0.00083569
Iteration 9/25 | Loss: 0.00083569
Iteration 10/25 | Loss: 0.00083569
Iteration 11/25 | Loss: 0.00083569
Iteration 12/25 | Loss: 0.00083569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008356945472769439, 0.0008356945472769439, 0.0008356945472769439, 0.0008356945472769439, 0.0008356945472769439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008356945472769439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54874504
Iteration 2/25 | Loss: 0.00053305
Iteration 3/25 | Loss: 0.00053305
Iteration 4/25 | Loss: 0.00053304
Iteration 5/25 | Loss: 0.00053304
Iteration 6/25 | Loss: 0.00053304
Iteration 7/25 | Loss: 0.00053304
Iteration 8/25 | Loss: 0.00053304
Iteration 9/25 | Loss: 0.00053304
Iteration 10/25 | Loss: 0.00053304
Iteration 11/25 | Loss: 0.00053304
Iteration 12/25 | Loss: 0.00053304
Iteration 13/25 | Loss: 0.00053304
Iteration 14/25 | Loss: 0.00053304
Iteration 15/25 | Loss: 0.00053304
Iteration 16/25 | Loss: 0.00053304
Iteration 17/25 | Loss: 0.00053304
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005330426502041519, 0.0005330426502041519, 0.0005330426502041519, 0.0005330426502041519, 0.0005330426502041519]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005330426502041519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053304
Iteration 2/1000 | Loss: 0.00003836
Iteration 3/1000 | Loss: 0.00002540
Iteration 4/1000 | Loss: 0.00002311
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00001998
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001894
Iteration 11/1000 | Loss: 0.00001893
Iteration 12/1000 | Loss: 0.00001881
Iteration 13/1000 | Loss: 0.00001874
Iteration 14/1000 | Loss: 0.00001874
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001871
Iteration 18/1000 | Loss: 0.00001865
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001860
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001859
Iteration 23/1000 | Loss: 0.00001858
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00001854
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00001854
Iteration 30/1000 | Loss: 0.00001854
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001854
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001853
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001851
Iteration 42/1000 | Loss: 0.00001851
Iteration 43/1000 | Loss: 0.00001851
Iteration 44/1000 | Loss: 0.00001850
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001846
Iteration 56/1000 | Loss: 0.00001846
Iteration 57/1000 | Loss: 0.00001846
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001845
Iteration 60/1000 | Loss: 0.00001845
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001842
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001838
Iteration 86/1000 | Loss: 0.00001838
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001837
Iteration 90/1000 | Loss: 0.00001837
Iteration 91/1000 | Loss: 0.00001837
Iteration 92/1000 | Loss: 0.00001837
Iteration 93/1000 | Loss: 0.00001836
Iteration 94/1000 | Loss: 0.00001836
Iteration 95/1000 | Loss: 0.00001836
Iteration 96/1000 | Loss: 0.00001835
Iteration 97/1000 | Loss: 0.00001835
Iteration 98/1000 | Loss: 0.00001835
Iteration 99/1000 | Loss: 0.00001835
Iteration 100/1000 | Loss: 0.00001835
Iteration 101/1000 | Loss: 0.00001835
Iteration 102/1000 | Loss: 0.00001834
Iteration 103/1000 | Loss: 0.00001834
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00001834
Iteration 106/1000 | Loss: 0.00001834
Iteration 107/1000 | Loss: 0.00001833
Iteration 108/1000 | Loss: 0.00001833
Iteration 109/1000 | Loss: 0.00001833
Iteration 110/1000 | Loss: 0.00001832
Iteration 111/1000 | Loss: 0.00001832
Iteration 112/1000 | Loss: 0.00001832
Iteration 113/1000 | Loss: 0.00001832
Iteration 114/1000 | Loss: 0.00001832
Iteration 115/1000 | Loss: 0.00001832
Iteration 116/1000 | Loss: 0.00001832
Iteration 117/1000 | Loss: 0.00001832
Iteration 118/1000 | Loss: 0.00001832
Iteration 119/1000 | Loss: 0.00001831
Iteration 120/1000 | Loss: 0.00001831
Iteration 121/1000 | Loss: 0.00001831
Iteration 122/1000 | Loss: 0.00001831
Iteration 123/1000 | Loss: 0.00001831
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001830
Iteration 128/1000 | Loss: 0.00001830
Iteration 129/1000 | Loss: 0.00001830
Iteration 130/1000 | Loss: 0.00001830
Iteration 131/1000 | Loss: 0.00001830
Iteration 132/1000 | Loss: 0.00001830
Iteration 133/1000 | Loss: 0.00001830
Iteration 134/1000 | Loss: 0.00001830
Iteration 135/1000 | Loss: 0.00001830
Iteration 136/1000 | Loss: 0.00001830
Iteration 137/1000 | Loss: 0.00001829
Iteration 138/1000 | Loss: 0.00001829
Iteration 139/1000 | Loss: 0.00001829
Iteration 140/1000 | Loss: 0.00001829
Iteration 141/1000 | Loss: 0.00001829
Iteration 142/1000 | Loss: 0.00001829
Iteration 143/1000 | Loss: 0.00001829
Iteration 144/1000 | Loss: 0.00001828
Iteration 145/1000 | Loss: 0.00001828
Iteration 146/1000 | Loss: 0.00001828
Iteration 147/1000 | Loss: 0.00001828
Iteration 148/1000 | Loss: 0.00001828
Iteration 149/1000 | Loss: 0.00001828
Iteration 150/1000 | Loss: 0.00001828
Iteration 151/1000 | Loss: 0.00001828
Iteration 152/1000 | Loss: 0.00001828
Iteration 153/1000 | Loss: 0.00001828
Iteration 154/1000 | Loss: 0.00001828
Iteration 155/1000 | Loss: 0.00001828
Iteration 156/1000 | Loss: 0.00001828
Iteration 157/1000 | Loss: 0.00001828
Iteration 158/1000 | Loss: 0.00001828
Iteration 159/1000 | Loss: 0.00001828
Iteration 160/1000 | Loss: 0.00001828
Iteration 161/1000 | Loss: 0.00001828
Iteration 162/1000 | Loss: 0.00001828
Iteration 163/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.8277467461302876e-05, 1.8277467461302876e-05, 1.8277467461302876e-05, 1.8277467461302876e-05, 1.8277467461302876e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8277467461302876e-05

Optimization complete. Final v2v error: 3.6185197830200195 mm

Highest mean error: 3.9731240272521973 mm for frame 74

Lowest mean error: 3.4516067504882812 mm for frame 57

Saving results

Total time: 43.58051824569702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454072
Iteration 2/25 | Loss: 0.00118999
Iteration 3/25 | Loss: 0.00090082
Iteration 4/25 | Loss: 0.00085759
Iteration 5/25 | Loss: 0.00085029
Iteration 6/25 | Loss: 0.00084857
Iteration 7/25 | Loss: 0.00084850
Iteration 8/25 | Loss: 0.00084850
Iteration 9/25 | Loss: 0.00084850
Iteration 10/25 | Loss: 0.00084850
Iteration 11/25 | Loss: 0.00084850
Iteration 12/25 | Loss: 0.00084850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008485008729621768, 0.0008485008729621768, 0.0008485008729621768, 0.0008485008729621768, 0.0008485008729621768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008485008729621768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92180598
Iteration 2/25 | Loss: 0.00051313
Iteration 3/25 | Loss: 0.00051312
Iteration 4/25 | Loss: 0.00051312
Iteration 5/25 | Loss: 0.00051312
Iteration 6/25 | Loss: 0.00051312
Iteration 7/25 | Loss: 0.00051312
Iteration 8/25 | Loss: 0.00051312
Iteration 9/25 | Loss: 0.00051312
Iteration 10/25 | Loss: 0.00051312
Iteration 11/25 | Loss: 0.00051312
Iteration 12/25 | Loss: 0.00051312
Iteration 13/25 | Loss: 0.00051312
Iteration 14/25 | Loss: 0.00051312
Iteration 15/25 | Loss: 0.00051312
Iteration 16/25 | Loss: 0.00051312
Iteration 17/25 | Loss: 0.00051312
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005131210200488567, 0.0005131210200488567, 0.0005131210200488567, 0.0005131210200488567, 0.0005131210200488567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005131210200488567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051312
Iteration 2/1000 | Loss: 0.00003164
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001890
Iteration 5/1000 | Loss: 0.00001803
Iteration 6/1000 | Loss: 0.00001715
Iteration 7/1000 | Loss: 0.00001668
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001587
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001534
Iteration 16/1000 | Loss: 0.00001529
Iteration 17/1000 | Loss: 0.00001527
Iteration 18/1000 | Loss: 0.00001527
Iteration 19/1000 | Loss: 0.00001526
Iteration 20/1000 | Loss: 0.00001526
Iteration 21/1000 | Loss: 0.00001525
Iteration 22/1000 | Loss: 0.00001525
Iteration 23/1000 | Loss: 0.00001524
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001522
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001514
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001513
Iteration 37/1000 | Loss: 0.00001512
Iteration 38/1000 | Loss: 0.00001510
Iteration 39/1000 | Loss: 0.00001510
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001507
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00001507
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001506
Iteration 48/1000 | Loss: 0.00001506
Iteration 49/1000 | Loss: 0.00001506
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001506
Iteration 55/1000 | Loss: 0.00001506
Iteration 56/1000 | Loss: 0.00001505
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001505
Iteration 59/1000 | Loss: 0.00001504
Iteration 60/1000 | Loss: 0.00001504
Iteration 61/1000 | Loss: 0.00001503
Iteration 62/1000 | Loss: 0.00001503
Iteration 63/1000 | Loss: 0.00001503
Iteration 64/1000 | Loss: 0.00001503
Iteration 65/1000 | Loss: 0.00001502
Iteration 66/1000 | Loss: 0.00001502
Iteration 67/1000 | Loss: 0.00001502
Iteration 68/1000 | Loss: 0.00001502
Iteration 69/1000 | Loss: 0.00001501
Iteration 70/1000 | Loss: 0.00001501
Iteration 71/1000 | Loss: 0.00001501
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001500
Iteration 74/1000 | Loss: 0.00001500
Iteration 75/1000 | Loss: 0.00001499
Iteration 76/1000 | Loss: 0.00001499
Iteration 77/1000 | Loss: 0.00001499
Iteration 78/1000 | Loss: 0.00001499
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001498
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001497
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001496
Iteration 92/1000 | Loss: 0.00001496
Iteration 93/1000 | Loss: 0.00001496
Iteration 94/1000 | Loss: 0.00001496
Iteration 95/1000 | Loss: 0.00001496
Iteration 96/1000 | Loss: 0.00001496
Iteration 97/1000 | Loss: 0.00001496
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001496
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001495
Iteration 102/1000 | Loss: 0.00001495
Iteration 103/1000 | Loss: 0.00001495
Iteration 104/1000 | Loss: 0.00001495
Iteration 105/1000 | Loss: 0.00001495
Iteration 106/1000 | Loss: 0.00001495
Iteration 107/1000 | Loss: 0.00001494
Iteration 108/1000 | Loss: 0.00001494
Iteration 109/1000 | Loss: 0.00001494
Iteration 110/1000 | Loss: 0.00001494
Iteration 111/1000 | Loss: 0.00001494
Iteration 112/1000 | Loss: 0.00001494
Iteration 113/1000 | Loss: 0.00001494
Iteration 114/1000 | Loss: 0.00001494
Iteration 115/1000 | Loss: 0.00001494
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001494
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001493
Iteration 122/1000 | Loss: 0.00001493
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001493
Iteration 128/1000 | Loss: 0.00001493
Iteration 129/1000 | Loss: 0.00001493
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.4929591998225078e-05, 1.4929591998225078e-05, 1.4929591998225078e-05, 1.4929591998225078e-05, 1.4929591998225078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4929591998225078e-05

Optimization complete. Final v2v error: 3.3318521976470947 mm

Highest mean error: 3.5952866077423096 mm for frame 237

Lowest mean error: 3.196272850036621 mm for frame 189

Saving results

Total time: 45.3771915435791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011821
Iteration 2/25 | Loss: 0.00156996
Iteration 3/25 | Loss: 0.00107461
Iteration 4/25 | Loss: 0.00102258
Iteration 5/25 | Loss: 0.00100968
Iteration 6/25 | Loss: 0.00100703
Iteration 7/25 | Loss: 0.00100675
Iteration 8/25 | Loss: 0.00100675
Iteration 9/25 | Loss: 0.00100675
Iteration 10/25 | Loss: 0.00100675
Iteration 11/25 | Loss: 0.00100675
Iteration 12/25 | Loss: 0.00100675
Iteration 13/25 | Loss: 0.00100675
Iteration 14/25 | Loss: 0.00100675
Iteration 15/25 | Loss: 0.00100675
Iteration 16/25 | Loss: 0.00100675
Iteration 17/25 | Loss: 0.00100675
Iteration 18/25 | Loss: 0.00100675
Iteration 19/25 | Loss: 0.00100675
Iteration 20/25 | Loss: 0.00100675
Iteration 21/25 | Loss: 0.00100675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010067492257803679, 0.0010067492257803679, 0.0010067492257803679, 0.0010067492257803679, 0.0010067492257803679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010067492257803679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96670955
Iteration 2/25 | Loss: 0.00045095
Iteration 3/25 | Loss: 0.00045095
Iteration 4/25 | Loss: 0.00045095
Iteration 5/25 | Loss: 0.00045095
Iteration 6/25 | Loss: 0.00045095
Iteration 7/25 | Loss: 0.00045095
Iteration 8/25 | Loss: 0.00045095
Iteration 9/25 | Loss: 0.00045095
Iteration 10/25 | Loss: 0.00045095
Iteration 11/25 | Loss: 0.00045095
Iteration 12/25 | Loss: 0.00045095
Iteration 13/25 | Loss: 0.00045095
Iteration 14/25 | Loss: 0.00045095
Iteration 15/25 | Loss: 0.00045095
Iteration 16/25 | Loss: 0.00045095
Iteration 17/25 | Loss: 0.00045095
Iteration 18/25 | Loss: 0.00045095
Iteration 19/25 | Loss: 0.00045095
Iteration 20/25 | Loss: 0.00045095
Iteration 21/25 | Loss: 0.00045095
Iteration 22/25 | Loss: 0.00045095
Iteration 23/25 | Loss: 0.00045095
Iteration 24/25 | Loss: 0.00045095
Iteration 25/25 | Loss: 0.00045095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00045094842789694667, 0.00045094842789694667, 0.00045094842789694667, 0.00045094842789694667, 0.00045094842789694667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045094842789694667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045095
Iteration 2/1000 | Loss: 0.00006142
Iteration 3/1000 | Loss: 0.00003911
Iteration 4/1000 | Loss: 0.00003610
Iteration 5/1000 | Loss: 0.00003470
Iteration 6/1000 | Loss: 0.00003371
Iteration 7/1000 | Loss: 0.00003303
Iteration 8/1000 | Loss: 0.00003251
Iteration 9/1000 | Loss: 0.00003218
Iteration 10/1000 | Loss: 0.00003191
Iteration 11/1000 | Loss: 0.00003169
Iteration 12/1000 | Loss: 0.00003151
Iteration 13/1000 | Loss: 0.00003134
Iteration 14/1000 | Loss: 0.00003127
Iteration 15/1000 | Loss: 0.00003122
Iteration 16/1000 | Loss: 0.00003117
Iteration 17/1000 | Loss: 0.00003104
Iteration 18/1000 | Loss: 0.00003103
Iteration 19/1000 | Loss: 0.00003096
Iteration 20/1000 | Loss: 0.00003090
Iteration 21/1000 | Loss: 0.00003089
Iteration 22/1000 | Loss: 0.00003088
Iteration 23/1000 | Loss: 0.00003087
Iteration 24/1000 | Loss: 0.00003087
Iteration 25/1000 | Loss: 0.00003086
Iteration 26/1000 | Loss: 0.00003082
Iteration 27/1000 | Loss: 0.00003080
Iteration 28/1000 | Loss: 0.00003078
Iteration 29/1000 | Loss: 0.00003078
Iteration 30/1000 | Loss: 0.00003078
Iteration 31/1000 | Loss: 0.00003078
Iteration 32/1000 | Loss: 0.00003078
Iteration 33/1000 | Loss: 0.00003078
Iteration 34/1000 | Loss: 0.00003077
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00003077
Iteration 37/1000 | Loss: 0.00003077
Iteration 38/1000 | Loss: 0.00003077
Iteration 39/1000 | Loss: 0.00003077
Iteration 40/1000 | Loss: 0.00003077
Iteration 41/1000 | Loss: 0.00003077
Iteration 42/1000 | Loss: 0.00003077
Iteration 43/1000 | Loss: 0.00003077
Iteration 44/1000 | Loss: 0.00003076
Iteration 45/1000 | Loss: 0.00003076
Iteration 46/1000 | Loss: 0.00003076
Iteration 47/1000 | Loss: 0.00003075
Iteration 48/1000 | Loss: 0.00003075
Iteration 49/1000 | Loss: 0.00003075
Iteration 50/1000 | Loss: 0.00003074
Iteration 51/1000 | Loss: 0.00003074
Iteration 52/1000 | Loss: 0.00003073
Iteration 53/1000 | Loss: 0.00003073
Iteration 54/1000 | Loss: 0.00003073
Iteration 55/1000 | Loss: 0.00003073
Iteration 56/1000 | Loss: 0.00003072
Iteration 57/1000 | Loss: 0.00003072
Iteration 58/1000 | Loss: 0.00003072
Iteration 59/1000 | Loss: 0.00003072
Iteration 60/1000 | Loss: 0.00003071
Iteration 61/1000 | Loss: 0.00003071
Iteration 62/1000 | Loss: 0.00003071
Iteration 63/1000 | Loss: 0.00003071
Iteration 64/1000 | Loss: 0.00003071
Iteration 65/1000 | Loss: 0.00003071
Iteration 66/1000 | Loss: 0.00003070
Iteration 67/1000 | Loss: 0.00003070
Iteration 68/1000 | Loss: 0.00003070
Iteration 69/1000 | Loss: 0.00003070
Iteration 70/1000 | Loss: 0.00003070
Iteration 71/1000 | Loss: 0.00003069
Iteration 72/1000 | Loss: 0.00003069
Iteration 73/1000 | Loss: 0.00003069
Iteration 74/1000 | Loss: 0.00003069
Iteration 75/1000 | Loss: 0.00003069
Iteration 76/1000 | Loss: 0.00003069
Iteration 77/1000 | Loss: 0.00003068
Iteration 78/1000 | Loss: 0.00003068
Iteration 79/1000 | Loss: 0.00003068
Iteration 80/1000 | Loss: 0.00003068
Iteration 81/1000 | Loss: 0.00003068
Iteration 82/1000 | Loss: 0.00003068
Iteration 83/1000 | Loss: 0.00003067
Iteration 84/1000 | Loss: 0.00003067
Iteration 85/1000 | Loss: 0.00003066
Iteration 86/1000 | Loss: 0.00003066
Iteration 87/1000 | Loss: 0.00003065
Iteration 88/1000 | Loss: 0.00003065
Iteration 89/1000 | Loss: 0.00003065
Iteration 90/1000 | Loss: 0.00003065
Iteration 91/1000 | Loss: 0.00003065
Iteration 92/1000 | Loss: 0.00003065
Iteration 93/1000 | Loss: 0.00003065
Iteration 94/1000 | Loss: 0.00003065
Iteration 95/1000 | Loss: 0.00003065
Iteration 96/1000 | Loss: 0.00003065
Iteration 97/1000 | Loss: 0.00003064
Iteration 98/1000 | Loss: 0.00003064
Iteration 99/1000 | Loss: 0.00003064
Iteration 100/1000 | Loss: 0.00003063
Iteration 101/1000 | Loss: 0.00003063
Iteration 102/1000 | Loss: 0.00003063
Iteration 103/1000 | Loss: 0.00003063
Iteration 104/1000 | Loss: 0.00003063
Iteration 105/1000 | Loss: 0.00003063
Iteration 106/1000 | Loss: 0.00003062
Iteration 107/1000 | Loss: 0.00003062
Iteration 108/1000 | Loss: 0.00003062
Iteration 109/1000 | Loss: 0.00003061
Iteration 110/1000 | Loss: 0.00003061
Iteration 111/1000 | Loss: 0.00003061
Iteration 112/1000 | Loss: 0.00003061
Iteration 113/1000 | Loss: 0.00003061
Iteration 114/1000 | Loss: 0.00003061
Iteration 115/1000 | Loss: 0.00003061
Iteration 116/1000 | Loss: 0.00003061
Iteration 117/1000 | Loss: 0.00003061
Iteration 118/1000 | Loss: 0.00003060
Iteration 119/1000 | Loss: 0.00003060
Iteration 120/1000 | Loss: 0.00003060
Iteration 121/1000 | Loss: 0.00003060
Iteration 122/1000 | Loss: 0.00003060
Iteration 123/1000 | Loss: 0.00003060
Iteration 124/1000 | Loss: 0.00003059
Iteration 125/1000 | Loss: 0.00003059
Iteration 126/1000 | Loss: 0.00003059
Iteration 127/1000 | Loss: 0.00003059
Iteration 128/1000 | Loss: 0.00003059
Iteration 129/1000 | Loss: 0.00003059
Iteration 130/1000 | Loss: 0.00003059
Iteration 131/1000 | Loss: 0.00003059
Iteration 132/1000 | Loss: 0.00003059
Iteration 133/1000 | Loss: 0.00003059
Iteration 134/1000 | Loss: 0.00003059
Iteration 135/1000 | Loss: 0.00003059
Iteration 136/1000 | Loss: 0.00003058
Iteration 137/1000 | Loss: 0.00003058
Iteration 138/1000 | Loss: 0.00003058
Iteration 139/1000 | Loss: 0.00003058
Iteration 140/1000 | Loss: 0.00003058
Iteration 141/1000 | Loss: 0.00003058
Iteration 142/1000 | Loss: 0.00003058
Iteration 143/1000 | Loss: 0.00003058
Iteration 144/1000 | Loss: 0.00003058
Iteration 145/1000 | Loss: 0.00003058
Iteration 146/1000 | Loss: 0.00003058
Iteration 147/1000 | Loss: 0.00003057
Iteration 148/1000 | Loss: 0.00003057
Iteration 149/1000 | Loss: 0.00003057
Iteration 150/1000 | Loss: 0.00003057
Iteration 151/1000 | Loss: 0.00003057
Iteration 152/1000 | Loss: 0.00003057
Iteration 153/1000 | Loss: 0.00003056
Iteration 154/1000 | Loss: 0.00003056
Iteration 155/1000 | Loss: 0.00003056
Iteration 156/1000 | Loss: 0.00003056
Iteration 157/1000 | Loss: 0.00003056
Iteration 158/1000 | Loss: 0.00003056
Iteration 159/1000 | Loss: 0.00003056
Iteration 160/1000 | Loss: 0.00003056
Iteration 161/1000 | Loss: 0.00003055
Iteration 162/1000 | Loss: 0.00003055
Iteration 163/1000 | Loss: 0.00003055
Iteration 164/1000 | Loss: 0.00003055
Iteration 165/1000 | Loss: 0.00003055
Iteration 166/1000 | Loss: 0.00003055
Iteration 167/1000 | Loss: 0.00003055
Iteration 168/1000 | Loss: 0.00003055
Iteration 169/1000 | Loss: 0.00003055
Iteration 170/1000 | Loss: 0.00003055
Iteration 171/1000 | Loss: 0.00003055
Iteration 172/1000 | Loss: 0.00003055
Iteration 173/1000 | Loss: 0.00003054
Iteration 174/1000 | Loss: 0.00003054
Iteration 175/1000 | Loss: 0.00003054
Iteration 176/1000 | Loss: 0.00003054
Iteration 177/1000 | Loss: 0.00003054
Iteration 178/1000 | Loss: 0.00003054
Iteration 179/1000 | Loss: 0.00003054
Iteration 180/1000 | Loss: 0.00003054
Iteration 181/1000 | Loss: 0.00003054
Iteration 182/1000 | Loss: 0.00003054
Iteration 183/1000 | Loss: 0.00003054
Iteration 184/1000 | Loss: 0.00003053
Iteration 185/1000 | Loss: 0.00003053
Iteration 186/1000 | Loss: 0.00003053
Iteration 187/1000 | Loss: 0.00003053
Iteration 188/1000 | Loss: 0.00003053
Iteration 189/1000 | Loss: 0.00003053
Iteration 190/1000 | Loss: 0.00003053
Iteration 191/1000 | Loss: 0.00003053
Iteration 192/1000 | Loss: 0.00003053
Iteration 193/1000 | Loss: 0.00003053
Iteration 194/1000 | Loss: 0.00003053
Iteration 195/1000 | Loss: 0.00003053
Iteration 196/1000 | Loss: 0.00003053
Iteration 197/1000 | Loss: 0.00003053
Iteration 198/1000 | Loss: 0.00003052
Iteration 199/1000 | Loss: 0.00003052
Iteration 200/1000 | Loss: 0.00003052
Iteration 201/1000 | Loss: 0.00003052
Iteration 202/1000 | Loss: 0.00003052
Iteration 203/1000 | Loss: 0.00003052
Iteration 204/1000 | Loss: 0.00003052
Iteration 205/1000 | Loss: 0.00003052
Iteration 206/1000 | Loss: 0.00003052
Iteration 207/1000 | Loss: 0.00003052
Iteration 208/1000 | Loss: 0.00003052
Iteration 209/1000 | Loss: 0.00003052
Iteration 210/1000 | Loss: 0.00003052
Iteration 211/1000 | Loss: 0.00003052
Iteration 212/1000 | Loss: 0.00003052
Iteration 213/1000 | Loss: 0.00003052
Iteration 214/1000 | Loss: 0.00003052
Iteration 215/1000 | Loss: 0.00003052
Iteration 216/1000 | Loss: 0.00003052
Iteration 217/1000 | Loss: 0.00003052
Iteration 218/1000 | Loss: 0.00003052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [3.0516886909026653e-05, 3.0516886909026653e-05, 3.0516886909026653e-05, 3.0516886909026653e-05, 3.0516886909026653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0516886909026653e-05

Optimization complete. Final v2v error: 4.510298252105713 mm

Highest mean error: 5.351791858673096 mm for frame 100

Lowest mean error: 3.502223253250122 mm for frame 53

Saving results

Total time: 53.206467390060425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956390
Iteration 2/25 | Loss: 0.00370453
Iteration 3/25 | Loss: 0.00292918
Iteration 4/25 | Loss: 0.00231202
Iteration 5/25 | Loss: 0.00189223
Iteration 6/25 | Loss: 0.00193528
Iteration 7/25 | Loss: 0.00175266
Iteration 8/25 | Loss: 0.00175716
Iteration 9/25 | Loss: 0.00170772
Iteration 10/25 | Loss: 0.00144965
Iteration 11/25 | Loss: 0.00199518
Iteration 12/25 | Loss: 0.00098925
Iteration 13/25 | Loss: 0.00096812
Iteration 14/25 | Loss: 0.00093146
Iteration 15/25 | Loss: 0.00093001
Iteration 16/25 | Loss: 0.00092973
Iteration 17/25 | Loss: 0.00092967
Iteration 18/25 | Loss: 0.00092967
Iteration 19/25 | Loss: 0.00092966
Iteration 20/25 | Loss: 0.00092966
Iteration 21/25 | Loss: 0.00092966
Iteration 22/25 | Loss: 0.00092966
Iteration 23/25 | Loss: 0.00092966
Iteration 24/25 | Loss: 0.00092966
Iteration 25/25 | Loss: 0.00092966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49104321
Iteration 2/25 | Loss: 0.00061498
Iteration 3/25 | Loss: 0.00061498
Iteration 4/25 | Loss: 0.00061498
Iteration 5/25 | Loss: 0.00061498
Iteration 6/25 | Loss: 0.00061498
Iteration 7/25 | Loss: 0.00061498
Iteration 8/25 | Loss: 0.00061498
Iteration 9/25 | Loss: 0.00061498
Iteration 10/25 | Loss: 0.00061498
Iteration 11/25 | Loss: 0.00061498
Iteration 12/25 | Loss: 0.00061498
Iteration 13/25 | Loss: 0.00061498
Iteration 14/25 | Loss: 0.00061498
Iteration 15/25 | Loss: 0.00061498
Iteration 16/25 | Loss: 0.00061498
Iteration 17/25 | Loss: 0.00061498
Iteration 18/25 | Loss: 0.00061498
Iteration 19/25 | Loss: 0.00061498
Iteration 20/25 | Loss: 0.00061498
Iteration 21/25 | Loss: 0.00061498
Iteration 22/25 | Loss: 0.00061498
Iteration 23/25 | Loss: 0.00061498
Iteration 24/25 | Loss: 0.00061498
Iteration 25/25 | Loss: 0.00061498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061498
Iteration 2/1000 | Loss: 0.00004152
Iteration 3/1000 | Loss: 0.00003106
Iteration 4/1000 | Loss: 0.00085684
Iteration 5/1000 | Loss: 0.00002873
Iteration 6/1000 | Loss: 0.00002648
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002447
Iteration 9/1000 | Loss: 0.00002398
Iteration 10/1000 | Loss: 0.00002360
Iteration 11/1000 | Loss: 0.00109215
Iteration 12/1000 | Loss: 0.00170202
Iteration 13/1000 | Loss: 0.00011940
Iteration 14/1000 | Loss: 0.00006792
Iteration 15/1000 | Loss: 0.00002801
Iteration 16/1000 | Loss: 0.00002368
Iteration 17/1000 | Loss: 0.00002029
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00093879
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001746
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001720
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001715
Iteration 33/1000 | Loss: 0.00001715
Iteration 34/1000 | Loss: 0.00001715
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001715
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001714
Iteration 40/1000 | Loss: 0.00001714
Iteration 41/1000 | Loss: 0.00001714
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001712
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.7115195078076795e-05, 1.7115195078076795e-05, 1.7115195078076795e-05, 1.7115195078076795e-05, 1.7115195078076795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7115195078076795e-05

Optimization complete. Final v2v error: 3.5143797397613525 mm

Highest mean error: 3.8697562217712402 mm for frame 2

Lowest mean error: 3.414263963699341 mm for frame 67

Saving results

Total time: 68.07873392105103
