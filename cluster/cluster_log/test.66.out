Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=66, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3696-3751
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838697
Iteration 2/25 | Loss: 0.00144095
Iteration 3/25 | Loss: 0.00133455
Iteration 4/25 | Loss: 0.00132572
Iteration 5/25 | Loss: 0.00132457
Iteration 6/25 | Loss: 0.00132457
Iteration 7/25 | Loss: 0.00132457
Iteration 8/25 | Loss: 0.00132457
Iteration 9/25 | Loss: 0.00132457
Iteration 10/25 | Loss: 0.00132457
Iteration 11/25 | Loss: 0.00132457
Iteration 12/25 | Loss: 0.00132457
Iteration 13/25 | Loss: 0.00132457
Iteration 14/25 | Loss: 0.00132457
Iteration 15/25 | Loss: 0.00132457
Iteration 16/25 | Loss: 0.00132457
Iteration 17/25 | Loss: 0.00132457
Iteration 18/25 | Loss: 0.00132457
Iteration 19/25 | Loss: 0.00132457
Iteration 20/25 | Loss: 0.00132457
Iteration 21/25 | Loss: 0.00132457
Iteration 22/25 | Loss: 0.00132457
Iteration 23/25 | Loss: 0.00132457
Iteration 24/25 | Loss: 0.00132457
Iteration 25/25 | Loss: 0.00132457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21301377
Iteration 2/25 | Loss: 0.00411346
Iteration 3/25 | Loss: 0.00411346
Iteration 4/25 | Loss: 0.00411346
Iteration 5/25 | Loss: 0.00411345
Iteration 6/25 | Loss: 0.00411345
Iteration 7/25 | Loss: 0.00411345
Iteration 8/25 | Loss: 0.00411345
Iteration 9/25 | Loss: 0.00411345
Iteration 10/25 | Loss: 0.00411345
Iteration 11/25 | Loss: 0.00411345
Iteration 12/25 | Loss: 0.00411345
Iteration 13/25 | Loss: 0.00411345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004113452974706888, 0.004113452974706888, 0.004113452974706888, 0.004113452974706888, 0.004113452974706888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004113452974706888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411345
Iteration 2/1000 | Loss: 0.00003402
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001556
Iteration 5/1000 | Loss: 0.00001442
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001110
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001080
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001040
Iteration 19/1000 | Loss: 0.00001039
Iteration 20/1000 | Loss: 0.00001038
Iteration 21/1000 | Loss: 0.00001033
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001024
Iteration 25/1000 | Loss: 0.00001016
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001015
Iteration 30/1000 | Loss: 0.00001014
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001013
Iteration 33/1000 | Loss: 0.00001013
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001012
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001012
Iteration 40/1000 | Loss: 0.00001011
Iteration 41/1000 | Loss: 0.00001011
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001006
Iteration 66/1000 | Loss: 0.00001006
Iteration 67/1000 | Loss: 0.00001006
Iteration 68/1000 | Loss: 0.00001006
Iteration 69/1000 | Loss: 0.00001006
Iteration 70/1000 | Loss: 0.00001006
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001006
Iteration 75/1000 | Loss: 0.00001006
Iteration 76/1000 | Loss: 0.00001006
Iteration 77/1000 | Loss: 0.00001006
Iteration 78/1000 | Loss: 0.00001006
Iteration 79/1000 | Loss: 0.00001006
Iteration 80/1000 | Loss: 0.00001006
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001006
Iteration 87/1000 | Loss: 0.00001006
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001006
Iteration 93/1000 | Loss: 0.00001006
Iteration 94/1000 | Loss: 0.00001006
Iteration 95/1000 | Loss: 0.00001006
Iteration 96/1000 | Loss: 0.00001006
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0063615263788961e-05, 1.0063615263788961e-05, 1.0063615263788961e-05, 1.0063615263788961e-05, 1.0063615263788961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0063615263788961e-05

Optimization complete. Final v2v error: 2.764993190765381 mm

Highest mean error: 3.434030294418335 mm for frame 119

Lowest mean error: 2.4364328384399414 mm for frame 180

Saving results

Total time: 35.888103008270264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758123
Iteration 2/25 | Loss: 0.00223965
Iteration 3/25 | Loss: 0.00173399
Iteration 4/25 | Loss: 0.00175304
Iteration 5/25 | Loss: 0.00159263
Iteration 6/25 | Loss: 0.00153901
Iteration 7/25 | Loss: 0.00148870
Iteration 8/25 | Loss: 0.00145907
Iteration 9/25 | Loss: 0.00145420
Iteration 10/25 | Loss: 0.00145070
Iteration 11/25 | Loss: 0.00144317
Iteration 12/25 | Loss: 0.00143873
Iteration 13/25 | Loss: 0.00143696
Iteration 14/25 | Loss: 0.00143948
Iteration 15/25 | Loss: 0.00143497
Iteration 16/25 | Loss: 0.00143683
Iteration 17/25 | Loss: 0.00143578
Iteration 18/25 | Loss: 0.00143097
Iteration 19/25 | Loss: 0.00142873
Iteration 20/25 | Loss: 0.00142786
Iteration 21/25 | Loss: 0.00142969
Iteration 22/25 | Loss: 0.00142607
Iteration 23/25 | Loss: 0.00142856
Iteration 24/25 | Loss: 0.00142445
Iteration 25/25 | Loss: 0.00142337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99428999
Iteration 2/25 | Loss: 0.00433841
Iteration 3/25 | Loss: 0.00433840
Iteration 4/25 | Loss: 0.00433840
Iteration 5/25 | Loss: 0.00433840
Iteration 6/25 | Loss: 0.00433839
Iteration 7/25 | Loss: 0.00433839
Iteration 8/25 | Loss: 0.00433839
Iteration 9/25 | Loss: 0.00433839
Iteration 10/25 | Loss: 0.00433839
Iteration 11/25 | Loss: 0.00433839
Iteration 12/25 | Loss: 0.00433839
Iteration 13/25 | Loss: 0.00433839
Iteration 14/25 | Loss: 0.00433839
Iteration 15/25 | Loss: 0.00433839
Iteration 16/25 | Loss: 0.00433839
Iteration 17/25 | Loss: 0.00433839
Iteration 18/25 | Loss: 0.00433839
Iteration 19/25 | Loss: 0.00433839
Iteration 20/25 | Loss: 0.00433839
Iteration 21/25 | Loss: 0.00433839
Iteration 22/25 | Loss: 0.00433839
Iteration 23/25 | Loss: 0.00433839
Iteration 24/25 | Loss: 0.00433839
Iteration 25/25 | Loss: 0.00433839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00433839
Iteration 2/1000 | Loss: 0.00017470
Iteration 3/1000 | Loss: 0.00005329
Iteration 4/1000 | Loss: 0.00004227
Iteration 5/1000 | Loss: 0.00003722
Iteration 6/1000 | Loss: 0.00003400
Iteration 7/1000 | Loss: 0.00003175
Iteration 8/1000 | Loss: 0.00003034
Iteration 9/1000 | Loss: 0.00002915
Iteration 10/1000 | Loss: 0.00021016
Iteration 11/1000 | Loss: 0.00035938
Iteration 12/1000 | Loss: 0.00002865
Iteration 13/1000 | Loss: 0.00043149
Iteration 14/1000 | Loss: 0.00002872
Iteration 15/1000 | Loss: 0.00002473
Iteration 16/1000 | Loss: 0.00002205
Iteration 17/1000 | Loss: 0.00002013
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001687
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001554
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001500
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001477
Iteration 29/1000 | Loss: 0.00001476
Iteration 30/1000 | Loss: 0.00001476
Iteration 31/1000 | Loss: 0.00001475
Iteration 32/1000 | Loss: 0.00001475
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001473
Iteration 36/1000 | Loss: 0.00001473
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001472
Iteration 42/1000 | Loss: 0.00001472
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001472
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001469
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001468
Iteration 60/1000 | Loss: 0.00001468
Iteration 61/1000 | Loss: 0.00001467
Iteration 62/1000 | Loss: 0.00001467
Iteration 63/1000 | Loss: 0.00001467
Iteration 64/1000 | Loss: 0.00001467
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001466
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001465
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001465
Iteration 72/1000 | Loss: 0.00001464
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001464
Iteration 77/1000 | Loss: 0.00001464
Iteration 78/1000 | Loss: 0.00001464
Iteration 79/1000 | Loss: 0.00001464
Iteration 80/1000 | Loss: 0.00001463
Iteration 81/1000 | Loss: 0.00001463
Iteration 82/1000 | Loss: 0.00001463
Iteration 83/1000 | Loss: 0.00001463
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001462
Iteration 86/1000 | Loss: 0.00001462
Iteration 87/1000 | Loss: 0.00001462
Iteration 88/1000 | Loss: 0.00001462
Iteration 89/1000 | Loss: 0.00001462
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001461
Iteration 92/1000 | Loss: 0.00001461
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001453
Iteration 129/1000 | Loss: 0.00001453
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001452
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001451
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001451
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001449
Iteration 153/1000 | Loss: 0.00001449
Iteration 154/1000 | Loss: 0.00001449
Iteration 155/1000 | Loss: 0.00001449
Iteration 156/1000 | Loss: 0.00001449
Iteration 157/1000 | Loss: 0.00001449
Iteration 158/1000 | Loss: 0.00001449
Iteration 159/1000 | Loss: 0.00001448
Iteration 160/1000 | Loss: 0.00001448
Iteration 161/1000 | Loss: 0.00001448
Iteration 162/1000 | Loss: 0.00001448
Iteration 163/1000 | Loss: 0.00001447
Iteration 164/1000 | Loss: 0.00001447
Iteration 165/1000 | Loss: 0.00001447
Iteration 166/1000 | Loss: 0.00001446
Iteration 167/1000 | Loss: 0.00001446
Iteration 168/1000 | Loss: 0.00001446
Iteration 169/1000 | Loss: 0.00001446
Iteration 170/1000 | Loss: 0.00001446
Iteration 171/1000 | Loss: 0.00001446
Iteration 172/1000 | Loss: 0.00001446
Iteration 173/1000 | Loss: 0.00001446
Iteration 174/1000 | Loss: 0.00001446
Iteration 175/1000 | Loss: 0.00001446
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001445
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Iteration 182/1000 | Loss: 0.00001445
Iteration 183/1000 | Loss: 0.00001445
Iteration 184/1000 | Loss: 0.00001444
Iteration 185/1000 | Loss: 0.00001444
Iteration 186/1000 | Loss: 0.00001444
Iteration 187/1000 | Loss: 0.00001444
Iteration 188/1000 | Loss: 0.00001444
Iteration 189/1000 | Loss: 0.00001444
Iteration 190/1000 | Loss: 0.00001444
Iteration 191/1000 | Loss: 0.00001444
Iteration 192/1000 | Loss: 0.00001444
Iteration 193/1000 | Loss: 0.00001443
Iteration 194/1000 | Loss: 0.00001443
Iteration 195/1000 | Loss: 0.00001443
Iteration 196/1000 | Loss: 0.00001443
Iteration 197/1000 | Loss: 0.00001443
Iteration 198/1000 | Loss: 0.00001443
Iteration 199/1000 | Loss: 0.00001442
Iteration 200/1000 | Loss: 0.00001442
Iteration 201/1000 | Loss: 0.00001442
Iteration 202/1000 | Loss: 0.00001442
Iteration 203/1000 | Loss: 0.00001442
Iteration 204/1000 | Loss: 0.00001442
Iteration 205/1000 | Loss: 0.00001442
Iteration 206/1000 | Loss: 0.00001442
Iteration 207/1000 | Loss: 0.00001442
Iteration 208/1000 | Loss: 0.00001441
Iteration 209/1000 | Loss: 0.00001441
Iteration 210/1000 | Loss: 0.00001441
Iteration 211/1000 | Loss: 0.00001441
Iteration 212/1000 | Loss: 0.00001441
Iteration 213/1000 | Loss: 0.00001441
Iteration 214/1000 | Loss: 0.00001441
Iteration 215/1000 | Loss: 0.00001441
Iteration 216/1000 | Loss: 0.00001441
Iteration 217/1000 | Loss: 0.00001441
Iteration 218/1000 | Loss: 0.00001441
Iteration 219/1000 | Loss: 0.00001441
Iteration 220/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.4414164979825728e-05, 1.4414164979825728e-05, 1.4414164979825728e-05, 1.4414164979825728e-05, 1.4414164979825728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4414164979825728e-05

Optimization complete. Final v2v error: 3.2582452297210693 mm

Highest mean error: 3.5344791412353516 mm for frame 0

Lowest mean error: 2.944218635559082 mm for frame 236

Saving results

Total time: 108.573801279068
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953202
Iteration 2/25 | Loss: 0.00337586
Iteration 3/25 | Loss: 0.00268370
Iteration 4/25 | Loss: 0.00238428
Iteration 5/25 | Loss: 0.00224987
Iteration 6/25 | Loss: 0.00211680
Iteration 7/25 | Loss: 0.00208087
Iteration 8/25 | Loss: 0.00207779
Iteration 9/25 | Loss: 0.00196204
Iteration 10/25 | Loss: 0.00183183
Iteration 11/25 | Loss: 0.00179115
Iteration 12/25 | Loss: 0.00179299
Iteration 13/25 | Loss: 0.00176609
Iteration 14/25 | Loss: 0.00175823
Iteration 15/25 | Loss: 0.00175223
Iteration 16/25 | Loss: 0.00175816
Iteration 17/25 | Loss: 0.00175529
Iteration 18/25 | Loss: 0.00174113
Iteration 19/25 | Loss: 0.00173565
Iteration 20/25 | Loss: 0.00172592
Iteration 21/25 | Loss: 0.00172359
Iteration 22/25 | Loss: 0.00172514
Iteration 23/25 | Loss: 0.00172161
Iteration 24/25 | Loss: 0.00171928
Iteration 25/25 | Loss: 0.00172215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05077672
Iteration 2/25 | Loss: 0.00765385
Iteration 3/25 | Loss: 0.00738914
Iteration 4/25 | Loss: 0.00738914
Iteration 5/25 | Loss: 0.00738914
Iteration 6/25 | Loss: 0.00738913
Iteration 7/25 | Loss: 0.00738913
Iteration 8/25 | Loss: 0.00738913
Iteration 9/25 | Loss: 0.00738913
Iteration 10/25 | Loss: 0.00738913
Iteration 11/25 | Loss: 0.00738913
Iteration 12/25 | Loss: 0.00738913
Iteration 13/25 | Loss: 0.00738913
Iteration 14/25 | Loss: 0.00738913
Iteration 15/25 | Loss: 0.00738913
Iteration 16/25 | Loss: 0.00738913
Iteration 17/25 | Loss: 0.00738913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.007389131002128124, 0.007389131002128124, 0.007389131002128124, 0.007389131002128124, 0.007389131002128124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007389131002128124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00738913
Iteration 2/1000 | Loss: 0.00058715
Iteration 3/1000 | Loss: 0.00086321
Iteration 4/1000 | Loss: 0.00169118
Iteration 5/1000 | Loss: 0.00164901
Iteration 6/1000 | Loss: 0.00060197
Iteration 7/1000 | Loss: 0.00049261
Iteration 8/1000 | Loss: 0.00026775
Iteration 9/1000 | Loss: 0.00046650
Iteration 10/1000 | Loss: 0.00042397
Iteration 11/1000 | Loss: 0.00023327
Iteration 12/1000 | Loss: 0.00026282
Iteration 13/1000 | Loss: 0.00023101
Iteration 14/1000 | Loss: 0.00030755
Iteration 15/1000 | Loss: 0.00043046
Iteration 16/1000 | Loss: 0.00044816
Iteration 17/1000 | Loss: 0.00023461
Iteration 18/1000 | Loss: 0.00049225
Iteration 19/1000 | Loss: 0.00042857
Iteration 20/1000 | Loss: 0.00026100
Iteration 21/1000 | Loss: 0.00025616
Iteration 22/1000 | Loss: 0.00033805
Iteration 23/1000 | Loss: 0.00182090
Iteration 24/1000 | Loss: 0.00026363
Iteration 25/1000 | Loss: 0.00023809
Iteration 26/1000 | Loss: 0.00026887
Iteration 27/1000 | Loss: 0.00029388
Iteration 28/1000 | Loss: 0.00024714
Iteration 29/1000 | Loss: 0.00042187
Iteration 30/1000 | Loss: 0.00046966
Iteration 31/1000 | Loss: 0.00051894
Iteration 32/1000 | Loss: 0.00021531
Iteration 33/1000 | Loss: 0.00021335
Iteration 34/1000 | Loss: 0.00054809
Iteration 35/1000 | Loss: 0.00022004
Iteration 36/1000 | Loss: 0.00021598
Iteration 37/1000 | Loss: 0.00018345
Iteration 38/1000 | Loss: 0.00020348
Iteration 39/1000 | Loss: 0.00019460
Iteration 40/1000 | Loss: 0.00019125
Iteration 41/1000 | Loss: 0.00055287
Iteration 42/1000 | Loss: 0.00027782
Iteration 43/1000 | Loss: 0.00020756
Iteration 44/1000 | Loss: 0.00041005
Iteration 45/1000 | Loss: 0.00024952
Iteration 46/1000 | Loss: 0.00027147
Iteration 47/1000 | Loss: 0.00018424
Iteration 48/1000 | Loss: 0.00018925
Iteration 49/1000 | Loss: 0.00031499
Iteration 50/1000 | Loss: 0.00018041
Iteration 51/1000 | Loss: 0.00025744
Iteration 52/1000 | Loss: 0.00018949
Iteration 53/1000 | Loss: 0.00017701
Iteration 54/1000 | Loss: 0.00017835
Iteration 55/1000 | Loss: 0.00018425
Iteration 56/1000 | Loss: 0.00019739
Iteration 57/1000 | Loss: 0.00018753
Iteration 58/1000 | Loss: 0.00018525
Iteration 59/1000 | Loss: 0.00023300
Iteration 60/1000 | Loss: 0.00022624
Iteration 61/1000 | Loss: 0.00018642
Iteration 62/1000 | Loss: 0.00019254
Iteration 63/1000 | Loss: 0.00016773
Iteration 64/1000 | Loss: 0.00017287
Iteration 65/1000 | Loss: 0.00017329
Iteration 66/1000 | Loss: 0.00017890
Iteration 67/1000 | Loss: 0.00020746
Iteration 68/1000 | Loss: 0.00027220
Iteration 69/1000 | Loss: 0.00019750
Iteration 70/1000 | Loss: 0.00019528
Iteration 71/1000 | Loss: 0.00019716
Iteration 72/1000 | Loss: 0.00019387
Iteration 73/1000 | Loss: 0.00017678
Iteration 74/1000 | Loss: 0.00017004
Iteration 75/1000 | Loss: 0.00018282
Iteration 76/1000 | Loss: 0.00024896
Iteration 77/1000 | Loss: 0.00018019
Iteration 78/1000 | Loss: 0.00026212
Iteration 79/1000 | Loss: 0.00014728
Iteration 80/1000 | Loss: 0.00021417
Iteration 81/1000 | Loss: 0.00015610
Iteration 82/1000 | Loss: 0.00018417
Iteration 83/1000 | Loss: 0.00014891
Iteration 84/1000 | Loss: 0.00021202
Iteration 85/1000 | Loss: 0.00017078
Iteration 86/1000 | Loss: 0.00025711
Iteration 87/1000 | Loss: 0.00019262
Iteration 88/1000 | Loss: 0.00018175
Iteration 89/1000 | Loss: 0.00017441
Iteration 90/1000 | Loss: 0.00016984
Iteration 91/1000 | Loss: 0.00013580
Iteration 92/1000 | Loss: 0.00014904
Iteration 93/1000 | Loss: 0.00013071
Iteration 94/1000 | Loss: 0.00017023
Iteration 95/1000 | Loss: 0.00013242
Iteration 96/1000 | Loss: 0.00012997
Iteration 97/1000 | Loss: 0.00014445
Iteration 98/1000 | Loss: 0.00013671
Iteration 99/1000 | Loss: 0.00013596
Iteration 100/1000 | Loss: 0.00013027
Iteration 101/1000 | Loss: 0.00014066
Iteration 102/1000 | Loss: 0.00012660
Iteration 103/1000 | Loss: 0.00012770
Iteration 104/1000 | Loss: 0.00030108
Iteration 105/1000 | Loss: 0.00201943
Iteration 106/1000 | Loss: 0.00097215
Iteration 107/1000 | Loss: 0.00021926
Iteration 108/1000 | Loss: 0.00016656
Iteration 109/1000 | Loss: 0.00038199
Iteration 110/1000 | Loss: 0.00030850
Iteration 111/1000 | Loss: 0.00012016
Iteration 112/1000 | Loss: 0.00011178
Iteration 113/1000 | Loss: 0.00011694
Iteration 114/1000 | Loss: 0.00010117
Iteration 115/1000 | Loss: 0.00012352
Iteration 116/1000 | Loss: 0.00010395
Iteration 117/1000 | Loss: 0.00010013
Iteration 118/1000 | Loss: 0.00009374
Iteration 119/1000 | Loss: 0.00010239
Iteration 120/1000 | Loss: 0.00009262
Iteration 121/1000 | Loss: 0.00010651
Iteration 122/1000 | Loss: 0.00009160
Iteration 123/1000 | Loss: 0.00009113
Iteration 124/1000 | Loss: 0.00044548
Iteration 125/1000 | Loss: 0.00010206
Iteration 126/1000 | Loss: 0.00009525
Iteration 127/1000 | Loss: 0.00009186
Iteration 128/1000 | Loss: 0.00008931
Iteration 129/1000 | Loss: 0.00008765
Iteration 130/1000 | Loss: 0.00008695
Iteration 131/1000 | Loss: 0.00032873
Iteration 132/1000 | Loss: 0.00011201
Iteration 133/1000 | Loss: 0.00009033
Iteration 134/1000 | Loss: 0.00008773
Iteration 135/1000 | Loss: 0.00008604
Iteration 136/1000 | Loss: 0.00012126
Iteration 137/1000 | Loss: 0.00008848
Iteration 138/1000 | Loss: 0.00008428
Iteration 139/1000 | Loss: 0.00008398
Iteration 140/1000 | Loss: 0.00026866
Iteration 141/1000 | Loss: 0.00009399
Iteration 142/1000 | Loss: 0.00026683
Iteration 143/1000 | Loss: 0.00028460
Iteration 144/1000 | Loss: 0.00013251
Iteration 145/1000 | Loss: 0.00017380
Iteration 146/1000 | Loss: 0.00012180
Iteration 147/1000 | Loss: 0.00008742
Iteration 148/1000 | Loss: 0.00008371
Iteration 149/1000 | Loss: 0.00008187
Iteration 150/1000 | Loss: 0.00010198
Iteration 151/1000 | Loss: 0.00043418
Iteration 152/1000 | Loss: 0.00095921
Iteration 153/1000 | Loss: 0.00026703
Iteration 154/1000 | Loss: 0.00013720
Iteration 155/1000 | Loss: 0.00048577
Iteration 156/1000 | Loss: 0.00028112
Iteration 157/1000 | Loss: 0.00025626
Iteration 158/1000 | Loss: 0.00016231
Iteration 159/1000 | Loss: 0.00009688
Iteration 160/1000 | Loss: 0.00008993
Iteration 161/1000 | Loss: 0.00008258
Iteration 162/1000 | Loss: 0.00007716
Iteration 163/1000 | Loss: 0.00007463
Iteration 164/1000 | Loss: 0.00007304
Iteration 165/1000 | Loss: 0.00007221
Iteration 166/1000 | Loss: 0.00007155
Iteration 167/1000 | Loss: 0.00007124
Iteration 168/1000 | Loss: 0.00007086
Iteration 169/1000 | Loss: 0.00007059
Iteration 170/1000 | Loss: 0.00007058
Iteration 171/1000 | Loss: 0.00007056
Iteration 172/1000 | Loss: 0.00007055
Iteration 173/1000 | Loss: 0.00007055
Iteration 174/1000 | Loss: 0.00007038
Iteration 175/1000 | Loss: 0.00007031
Iteration 176/1000 | Loss: 0.00007029
Iteration 177/1000 | Loss: 0.00007025
Iteration 178/1000 | Loss: 0.00007024
Iteration 179/1000 | Loss: 0.00007023
Iteration 180/1000 | Loss: 0.00007022
Iteration 181/1000 | Loss: 0.00007021
Iteration 182/1000 | Loss: 0.00007021
Iteration 183/1000 | Loss: 0.00007021
Iteration 184/1000 | Loss: 0.00007021
Iteration 185/1000 | Loss: 0.00007021
Iteration 186/1000 | Loss: 0.00007020
Iteration 187/1000 | Loss: 0.00007020
Iteration 188/1000 | Loss: 0.00007020
Iteration 189/1000 | Loss: 0.00007020
Iteration 190/1000 | Loss: 0.00007020
Iteration 191/1000 | Loss: 0.00007020
Iteration 192/1000 | Loss: 0.00007019
Iteration 193/1000 | Loss: 0.00007019
Iteration 194/1000 | Loss: 0.00007019
Iteration 195/1000 | Loss: 0.00007018
Iteration 196/1000 | Loss: 0.00007018
Iteration 197/1000 | Loss: 0.00007017
Iteration 198/1000 | Loss: 0.00007017
Iteration 199/1000 | Loss: 0.00007016
Iteration 200/1000 | Loss: 0.00007016
Iteration 201/1000 | Loss: 0.00007016
Iteration 202/1000 | Loss: 0.00007016
Iteration 203/1000 | Loss: 0.00007016
Iteration 204/1000 | Loss: 0.00007015
Iteration 205/1000 | Loss: 0.00007015
Iteration 206/1000 | Loss: 0.00007015
Iteration 207/1000 | Loss: 0.00007015
Iteration 208/1000 | Loss: 0.00007015
Iteration 209/1000 | Loss: 0.00007015
Iteration 210/1000 | Loss: 0.00007015
Iteration 211/1000 | Loss: 0.00007015
Iteration 212/1000 | Loss: 0.00007015
Iteration 213/1000 | Loss: 0.00007015
Iteration 214/1000 | Loss: 0.00007015
Iteration 215/1000 | Loss: 0.00007015
Iteration 216/1000 | Loss: 0.00007014
Iteration 217/1000 | Loss: 0.00007014
Iteration 218/1000 | Loss: 0.00007014
Iteration 219/1000 | Loss: 0.00007014
Iteration 220/1000 | Loss: 0.00007014
Iteration 221/1000 | Loss: 0.00007014
Iteration 222/1000 | Loss: 0.00007014
Iteration 223/1000 | Loss: 0.00007014
Iteration 224/1000 | Loss: 0.00007013
Iteration 225/1000 | Loss: 0.00007013
Iteration 226/1000 | Loss: 0.00007013
Iteration 227/1000 | Loss: 0.00007013
Iteration 228/1000 | Loss: 0.00007013
Iteration 229/1000 | Loss: 0.00007013
Iteration 230/1000 | Loss: 0.00007013
Iteration 231/1000 | Loss: 0.00007013
Iteration 232/1000 | Loss: 0.00007013
Iteration 233/1000 | Loss: 0.00007013
Iteration 234/1000 | Loss: 0.00007013
Iteration 235/1000 | Loss: 0.00007012
Iteration 236/1000 | Loss: 0.00007012
Iteration 237/1000 | Loss: 0.00007012
Iteration 238/1000 | Loss: 0.00007012
Iteration 239/1000 | Loss: 0.00007012
Iteration 240/1000 | Loss: 0.00007011
Iteration 241/1000 | Loss: 0.00007011
Iteration 242/1000 | Loss: 0.00007011
Iteration 243/1000 | Loss: 0.00007011
Iteration 244/1000 | Loss: 0.00007011
Iteration 245/1000 | Loss: 0.00007010
Iteration 246/1000 | Loss: 0.00007010
Iteration 247/1000 | Loss: 0.00007010
Iteration 248/1000 | Loss: 0.00007010
Iteration 249/1000 | Loss: 0.00007010
Iteration 250/1000 | Loss: 0.00007010
Iteration 251/1000 | Loss: 0.00007010
Iteration 252/1000 | Loss: 0.00007010
Iteration 253/1000 | Loss: 0.00007010
Iteration 254/1000 | Loss: 0.00007009
Iteration 255/1000 | Loss: 0.00007009
Iteration 256/1000 | Loss: 0.00007009
Iteration 257/1000 | Loss: 0.00007009
Iteration 258/1000 | Loss: 0.00007009
Iteration 259/1000 | Loss: 0.00007009
Iteration 260/1000 | Loss: 0.00007008
Iteration 261/1000 | Loss: 0.00007008
Iteration 262/1000 | Loss: 0.00007008
Iteration 263/1000 | Loss: 0.00007008
Iteration 264/1000 | Loss: 0.00007008
Iteration 265/1000 | Loss: 0.00007008
Iteration 266/1000 | Loss: 0.00007008
Iteration 267/1000 | Loss: 0.00007008
Iteration 268/1000 | Loss: 0.00007008
Iteration 269/1000 | Loss: 0.00007008
Iteration 270/1000 | Loss: 0.00007008
Iteration 271/1000 | Loss: 0.00007008
Iteration 272/1000 | Loss: 0.00007008
Iteration 273/1000 | Loss: 0.00007008
Iteration 274/1000 | Loss: 0.00007008
Iteration 275/1000 | Loss: 0.00007008
Iteration 276/1000 | Loss: 0.00007008
Iteration 277/1000 | Loss: 0.00007008
Iteration 278/1000 | Loss: 0.00007008
Iteration 279/1000 | Loss: 0.00007008
Iteration 280/1000 | Loss: 0.00007008
Iteration 281/1000 | Loss: 0.00007008
Iteration 282/1000 | Loss: 0.00007008
Iteration 283/1000 | Loss: 0.00007008
Iteration 284/1000 | Loss: 0.00007008
Iteration 285/1000 | Loss: 0.00007008
Iteration 286/1000 | Loss: 0.00007008
Iteration 287/1000 | Loss: 0.00007008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [7.00786113156937e-05, 7.00786113156937e-05, 7.00786113156937e-05, 7.00786113156937e-05, 7.00786113156937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.00786113156937e-05

Optimization complete. Final v2v error: 4.596015453338623 mm

Highest mean error: 11.926000595092773 mm for frame 123

Lowest mean error: 3.1377408504486084 mm for frame 15

Saving results

Total time: 286.62563395500183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791057
Iteration 2/25 | Loss: 0.00151278
Iteration 3/25 | Loss: 0.00137940
Iteration 4/25 | Loss: 0.00136838
Iteration 5/25 | Loss: 0.00136503
Iteration 6/25 | Loss: 0.00136503
Iteration 7/25 | Loss: 0.00136503
Iteration 8/25 | Loss: 0.00136503
Iteration 9/25 | Loss: 0.00136503
Iteration 10/25 | Loss: 0.00136503
Iteration 11/25 | Loss: 0.00136503
Iteration 12/25 | Loss: 0.00136503
Iteration 13/25 | Loss: 0.00136503
Iteration 14/25 | Loss: 0.00136503
Iteration 15/25 | Loss: 0.00136503
Iteration 16/25 | Loss: 0.00136503
Iteration 17/25 | Loss: 0.00136503
Iteration 18/25 | Loss: 0.00136503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013650283217430115, 0.0013650283217430115, 0.0013650283217430115, 0.0013650283217430115, 0.0013650283217430115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013650283217430115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46495223
Iteration 2/25 | Loss: 0.00490616
Iteration 3/25 | Loss: 0.00490615
Iteration 4/25 | Loss: 0.00490615
Iteration 5/25 | Loss: 0.00490615
Iteration 6/25 | Loss: 0.00490615
Iteration 7/25 | Loss: 0.00490615
Iteration 8/25 | Loss: 0.00490615
Iteration 9/25 | Loss: 0.00490615
Iteration 10/25 | Loss: 0.00490615
Iteration 11/25 | Loss: 0.00490615
Iteration 12/25 | Loss: 0.00490615
Iteration 13/25 | Loss: 0.00490615
Iteration 14/25 | Loss: 0.00490615
Iteration 15/25 | Loss: 0.00490615
Iteration 16/25 | Loss: 0.00490615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0049061523750424385, 0.0049061523750424385, 0.0049061523750424385, 0.0049061523750424385, 0.0049061523750424385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0049061523750424385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00490615
Iteration 2/1000 | Loss: 0.00004405
Iteration 3/1000 | Loss: 0.00002665
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002098
Iteration 6/1000 | Loss: 0.00001948
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001769
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001580
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001497
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001462
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001461
Iteration 24/1000 | Loss: 0.00001461
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001449
Iteration 30/1000 | Loss: 0.00001447
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001439
Iteration 33/1000 | Loss: 0.00001438
Iteration 34/1000 | Loss: 0.00001437
Iteration 35/1000 | Loss: 0.00001437
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001431
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001429
Iteration 42/1000 | Loss: 0.00001425
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001418
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001412
Iteration 55/1000 | Loss: 0.00001412
Iteration 56/1000 | Loss: 0.00001412
Iteration 57/1000 | Loss: 0.00001411
Iteration 58/1000 | Loss: 0.00001411
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001409
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001408
Iteration 72/1000 | Loss: 0.00001408
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001403
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.4027234101376962e-05, 1.4027234101376962e-05, 1.4027234101376962e-05, 1.4027234101376962e-05, 1.4027234101376962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4027234101376962e-05

Optimization complete. Final v2v error: 3.2842018604278564 mm

Highest mean error: 3.8508992195129395 mm for frame 221

Lowest mean error: 2.8832430839538574 mm for frame 43

Saving results

Total time: 46.984713077545166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01133532
Iteration 2/25 | Loss: 0.00264076
Iteration 3/25 | Loss: 0.00177007
Iteration 4/25 | Loss: 0.00168732
Iteration 5/25 | Loss: 0.00170755
Iteration 6/25 | Loss: 0.00170631
Iteration 7/25 | Loss: 0.00160226
Iteration 8/25 | Loss: 0.00157486
Iteration 9/25 | Loss: 0.00153496
Iteration 10/25 | Loss: 0.00150682
Iteration 11/25 | Loss: 0.00149758
Iteration 12/25 | Loss: 0.00146902
Iteration 13/25 | Loss: 0.00145174
Iteration 14/25 | Loss: 0.00144355
Iteration 15/25 | Loss: 0.00143615
Iteration 16/25 | Loss: 0.00143303
Iteration 17/25 | Loss: 0.00142161
Iteration 18/25 | Loss: 0.00142654
Iteration 19/25 | Loss: 0.00142332
Iteration 20/25 | Loss: 0.00142412
Iteration 21/25 | Loss: 0.00141411
Iteration 22/25 | Loss: 0.00141281
Iteration 23/25 | Loss: 0.00141200
Iteration 24/25 | Loss: 0.00140548
Iteration 25/25 | Loss: 0.00140568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87879229
Iteration 2/25 | Loss: 0.00799807
Iteration 3/25 | Loss: 0.00777861
Iteration 4/25 | Loss: 0.00777861
Iteration 5/25 | Loss: 0.00777861
Iteration 6/25 | Loss: 0.00777861
Iteration 7/25 | Loss: 0.00777861
Iteration 8/25 | Loss: 0.00777861
Iteration 9/25 | Loss: 0.00777861
Iteration 10/25 | Loss: 0.00777861
Iteration 11/25 | Loss: 0.00777861
Iteration 12/25 | Loss: 0.00777861
Iteration 13/25 | Loss: 0.00777861
Iteration 14/25 | Loss: 0.00777861
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007778611034154892, 0.007778611034154892, 0.007778611034154892, 0.007778611034154892, 0.007778611034154892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007778611034154892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00777861
Iteration 2/1000 | Loss: 0.00188206
Iteration 3/1000 | Loss: 0.00109149
Iteration 4/1000 | Loss: 0.00084857
Iteration 5/1000 | Loss: 0.00112164
Iteration 6/1000 | Loss: 0.00089958
Iteration 7/1000 | Loss: 0.00147110
Iteration 8/1000 | Loss: 0.00169654
Iteration 9/1000 | Loss: 0.00154842
Iteration 10/1000 | Loss: 0.00141103
Iteration 11/1000 | Loss: 0.00085333
Iteration 12/1000 | Loss: 0.00092195
Iteration 13/1000 | Loss: 0.00080921
Iteration 14/1000 | Loss: 0.00141944
Iteration 15/1000 | Loss: 0.00103771
Iteration 16/1000 | Loss: 0.00099728
Iteration 17/1000 | Loss: 0.00119538
Iteration 18/1000 | Loss: 0.00069708
Iteration 19/1000 | Loss: 0.00120260
Iteration 20/1000 | Loss: 0.00096646
Iteration 21/1000 | Loss: 0.00192469
Iteration 22/1000 | Loss: 0.00388461
Iteration 23/1000 | Loss: 0.00497818
Iteration 24/1000 | Loss: 0.00360117
Iteration 25/1000 | Loss: 0.00082884
Iteration 26/1000 | Loss: 0.00095952
Iteration 27/1000 | Loss: 0.00080578
Iteration 28/1000 | Loss: 0.00070762
Iteration 29/1000 | Loss: 0.00155028
Iteration 30/1000 | Loss: 0.00090106
Iteration 31/1000 | Loss: 0.00120511
Iteration 32/1000 | Loss: 0.00099111
Iteration 33/1000 | Loss: 0.00094088
Iteration 34/1000 | Loss: 0.00122797
Iteration 35/1000 | Loss: 0.00238496
Iteration 36/1000 | Loss: 0.00168434
Iteration 37/1000 | Loss: 0.00065954
Iteration 38/1000 | Loss: 0.00082388
Iteration 39/1000 | Loss: 0.00097971
Iteration 40/1000 | Loss: 0.00127399
Iteration 41/1000 | Loss: 0.00260711
Iteration 42/1000 | Loss: 0.00231148
Iteration 43/1000 | Loss: 0.00077353
Iteration 44/1000 | Loss: 0.00086956
Iteration 45/1000 | Loss: 0.00086216
Iteration 46/1000 | Loss: 0.00186862
Iteration 47/1000 | Loss: 0.00082762
Iteration 48/1000 | Loss: 0.00101056
Iteration 49/1000 | Loss: 0.00051985
Iteration 50/1000 | Loss: 0.00108468
Iteration 51/1000 | Loss: 0.00132226
Iteration 52/1000 | Loss: 0.00134865
Iteration 53/1000 | Loss: 0.00194528
Iteration 54/1000 | Loss: 0.00047393
Iteration 55/1000 | Loss: 0.00099343
Iteration 56/1000 | Loss: 0.00023934
Iteration 57/1000 | Loss: 0.00038402
Iteration 58/1000 | Loss: 0.00025553
Iteration 59/1000 | Loss: 0.00045281
Iteration 60/1000 | Loss: 0.00050152
Iteration 61/1000 | Loss: 0.00057609
Iteration 62/1000 | Loss: 0.00043201
Iteration 63/1000 | Loss: 0.00095418
Iteration 64/1000 | Loss: 0.00058383
Iteration 65/1000 | Loss: 0.00124816
Iteration 66/1000 | Loss: 0.00189911
Iteration 67/1000 | Loss: 0.00022437
Iteration 68/1000 | Loss: 0.00015308
Iteration 69/1000 | Loss: 0.00023933
Iteration 70/1000 | Loss: 0.00030477
Iteration 71/1000 | Loss: 0.00029874
Iteration 72/1000 | Loss: 0.00027118
Iteration 73/1000 | Loss: 0.00046235
Iteration 74/1000 | Loss: 0.00074862
Iteration 75/1000 | Loss: 0.00036245
Iteration 76/1000 | Loss: 0.00047030
Iteration 77/1000 | Loss: 0.00053693
Iteration 78/1000 | Loss: 0.00021399
Iteration 79/1000 | Loss: 0.00130333
Iteration 80/1000 | Loss: 0.00052034
Iteration 81/1000 | Loss: 0.00026502
Iteration 82/1000 | Loss: 0.00021335
Iteration 83/1000 | Loss: 0.00019456
Iteration 84/1000 | Loss: 0.00022088
Iteration 85/1000 | Loss: 0.00047989
Iteration 86/1000 | Loss: 0.00109259
Iteration 87/1000 | Loss: 0.00073727
Iteration 88/1000 | Loss: 0.00075710
Iteration 89/1000 | Loss: 0.00084825
Iteration 90/1000 | Loss: 0.00095217
Iteration 91/1000 | Loss: 0.00074971
Iteration 92/1000 | Loss: 0.00086822
Iteration 93/1000 | Loss: 0.00084229
Iteration 94/1000 | Loss: 0.00076744
Iteration 95/1000 | Loss: 0.00082636
Iteration 96/1000 | Loss: 0.00090711
Iteration 97/1000 | Loss: 0.00043859
Iteration 98/1000 | Loss: 0.00058638
Iteration 99/1000 | Loss: 0.00079316
Iteration 100/1000 | Loss: 0.00053672
Iteration 101/1000 | Loss: 0.00055480
Iteration 102/1000 | Loss: 0.00079630
Iteration 103/1000 | Loss: 0.00101331
Iteration 104/1000 | Loss: 0.00065372
Iteration 105/1000 | Loss: 0.00147400
Iteration 106/1000 | Loss: 0.00095380
Iteration 107/1000 | Loss: 0.00074011
Iteration 108/1000 | Loss: 0.00050097
Iteration 109/1000 | Loss: 0.00071485
Iteration 110/1000 | Loss: 0.00095274
Iteration 111/1000 | Loss: 0.00182000
Iteration 112/1000 | Loss: 0.00161263
Iteration 113/1000 | Loss: 0.00036019
Iteration 114/1000 | Loss: 0.00047994
Iteration 115/1000 | Loss: 0.00046812
Iteration 116/1000 | Loss: 0.00044286
Iteration 117/1000 | Loss: 0.00039274
Iteration 118/1000 | Loss: 0.00035156
Iteration 119/1000 | Loss: 0.00080455
Iteration 120/1000 | Loss: 0.00087234
Iteration 121/1000 | Loss: 0.00088985
Iteration 122/1000 | Loss: 0.00096892
Iteration 123/1000 | Loss: 0.00074736
Iteration 124/1000 | Loss: 0.00139325
Iteration 125/1000 | Loss: 0.00139911
Iteration 126/1000 | Loss: 0.00112395
Iteration 127/1000 | Loss: 0.00095240
Iteration 128/1000 | Loss: 0.00124461
Iteration 129/1000 | Loss: 0.00112771
Iteration 130/1000 | Loss: 0.00191887
Iteration 131/1000 | Loss: 0.00095231
Iteration 132/1000 | Loss: 0.00126109
Iteration 133/1000 | Loss: 0.00206034
Iteration 134/1000 | Loss: 0.00264914
Iteration 135/1000 | Loss: 0.00233774
Iteration 136/1000 | Loss: 0.00117510
Iteration 137/1000 | Loss: 0.00056911
Iteration 138/1000 | Loss: 0.00061245
Iteration 139/1000 | Loss: 0.00067559
Iteration 140/1000 | Loss: 0.00087664
Iteration 141/1000 | Loss: 0.00066831
Iteration 142/1000 | Loss: 0.00061088
Iteration 143/1000 | Loss: 0.00057724
Iteration 144/1000 | Loss: 0.00088437
Iteration 145/1000 | Loss: 0.00099496
Iteration 146/1000 | Loss: 0.00195887
Iteration 147/1000 | Loss: 0.00098909
Iteration 148/1000 | Loss: 0.00068291
Iteration 149/1000 | Loss: 0.00066665
Iteration 150/1000 | Loss: 0.00057944
Iteration 151/1000 | Loss: 0.00072046
Iteration 152/1000 | Loss: 0.00103004
Iteration 153/1000 | Loss: 0.00083263
Iteration 154/1000 | Loss: 0.00049751
Iteration 155/1000 | Loss: 0.00061126
Iteration 156/1000 | Loss: 0.00053478
Iteration 157/1000 | Loss: 0.00111836
Iteration 158/1000 | Loss: 0.00099016
Iteration 159/1000 | Loss: 0.00120174
Iteration 160/1000 | Loss: 0.00050125
Iteration 161/1000 | Loss: 0.00045282
Iteration 162/1000 | Loss: 0.00037240
Iteration 163/1000 | Loss: 0.00102169
Iteration 164/1000 | Loss: 0.00107380
Iteration 165/1000 | Loss: 0.00070111
Iteration 166/1000 | Loss: 0.00061150
Iteration 167/1000 | Loss: 0.00040159
Iteration 168/1000 | Loss: 0.00034384
Iteration 169/1000 | Loss: 0.00059496
Iteration 170/1000 | Loss: 0.00025597
Iteration 171/1000 | Loss: 0.00048145
Iteration 172/1000 | Loss: 0.00077348
Iteration 173/1000 | Loss: 0.00039006
Iteration 174/1000 | Loss: 0.00032237
Iteration 175/1000 | Loss: 0.00105201
Iteration 176/1000 | Loss: 0.00024627
Iteration 177/1000 | Loss: 0.00086572
Iteration 178/1000 | Loss: 0.00069113
Iteration 179/1000 | Loss: 0.00128626
Iteration 180/1000 | Loss: 0.00065606
Iteration 181/1000 | Loss: 0.00016877
Iteration 182/1000 | Loss: 0.00016570
Iteration 183/1000 | Loss: 0.00012749
Iteration 184/1000 | Loss: 0.00010383
Iteration 185/1000 | Loss: 0.00023192
Iteration 186/1000 | Loss: 0.00068121
Iteration 187/1000 | Loss: 0.00023678
Iteration 188/1000 | Loss: 0.00038706
Iteration 189/1000 | Loss: 0.00034439
Iteration 190/1000 | Loss: 0.00042962
Iteration 191/1000 | Loss: 0.00029055
Iteration 192/1000 | Loss: 0.00021562
Iteration 193/1000 | Loss: 0.00025612
Iteration 194/1000 | Loss: 0.00025940
Iteration 195/1000 | Loss: 0.00033357
Iteration 196/1000 | Loss: 0.00114727
Iteration 197/1000 | Loss: 0.00254223
Iteration 198/1000 | Loss: 0.00276927
Iteration 199/1000 | Loss: 0.00076676
Iteration 200/1000 | Loss: 0.00396628
Iteration 201/1000 | Loss: 0.00200895
Iteration 202/1000 | Loss: 0.00010124
Iteration 203/1000 | Loss: 0.00072517
Iteration 204/1000 | Loss: 0.00024950
Iteration 205/1000 | Loss: 0.00025344
Iteration 206/1000 | Loss: 0.00016428
Iteration 207/1000 | Loss: 0.00005771
Iteration 208/1000 | Loss: 0.00006423
Iteration 209/1000 | Loss: 0.00008458
Iteration 210/1000 | Loss: 0.00005586
Iteration 211/1000 | Loss: 0.00005240
Iteration 212/1000 | Loss: 0.00024379
Iteration 213/1000 | Loss: 0.00005416
Iteration 214/1000 | Loss: 0.00005291
Iteration 215/1000 | Loss: 0.00004537
Iteration 216/1000 | Loss: 0.00059128
Iteration 217/1000 | Loss: 0.00006120
Iteration 218/1000 | Loss: 0.00025237
Iteration 219/1000 | Loss: 0.00020198
Iteration 220/1000 | Loss: 0.00022163
Iteration 221/1000 | Loss: 0.00010139
Iteration 222/1000 | Loss: 0.00012837
Iteration 223/1000 | Loss: 0.00004605
Iteration 224/1000 | Loss: 0.00005220
Iteration 225/1000 | Loss: 0.00005393
Iteration 226/1000 | Loss: 0.00005041
Iteration 227/1000 | Loss: 0.00004340
Iteration 228/1000 | Loss: 0.00067068
Iteration 229/1000 | Loss: 0.00021439
Iteration 230/1000 | Loss: 0.00005386
Iteration 231/1000 | Loss: 0.00004028
Iteration 232/1000 | Loss: 0.00005044
Iteration 233/1000 | Loss: 0.00023055
Iteration 234/1000 | Loss: 0.00006408
Iteration 235/1000 | Loss: 0.00004122
Iteration 236/1000 | Loss: 0.00004802
Iteration 237/1000 | Loss: 0.00004443
Iteration 238/1000 | Loss: 0.00004487
Iteration 239/1000 | Loss: 0.00003880
Iteration 240/1000 | Loss: 0.00004986
Iteration 241/1000 | Loss: 0.00004332
Iteration 242/1000 | Loss: 0.00046317
Iteration 243/1000 | Loss: 0.00013925
Iteration 244/1000 | Loss: 0.00007358
Iteration 245/1000 | Loss: 0.00006809
Iteration 246/1000 | Loss: 0.00004294
Iteration 247/1000 | Loss: 0.00004751
Iteration 248/1000 | Loss: 0.00004965
Iteration 249/1000 | Loss: 0.00004315
Iteration 250/1000 | Loss: 0.00004446
Iteration 251/1000 | Loss: 0.00004071
Iteration 252/1000 | Loss: 0.00004014
Iteration 253/1000 | Loss: 0.00004281
Iteration 254/1000 | Loss: 0.00003912
Iteration 255/1000 | Loss: 0.00004167
Iteration 256/1000 | Loss: 0.00003320
Iteration 257/1000 | Loss: 0.00003367
Iteration 258/1000 | Loss: 0.00003058
Iteration 259/1000 | Loss: 0.00003050
Iteration 260/1000 | Loss: 0.00004002
Iteration 261/1000 | Loss: 0.00003560
Iteration 262/1000 | Loss: 0.00064632
Iteration 263/1000 | Loss: 0.00005513
Iteration 264/1000 | Loss: 0.00005053
Iteration 265/1000 | Loss: 0.00003887
Iteration 266/1000 | Loss: 0.00004198
Iteration 267/1000 | Loss: 0.00003875
Iteration 268/1000 | Loss: 0.00003922
Iteration 269/1000 | Loss: 0.00003711
Iteration 270/1000 | Loss: 0.00003800
Iteration 271/1000 | Loss: 0.00003498
Iteration 272/1000 | Loss: 0.00003623
Iteration 273/1000 | Loss: 0.00003024
Iteration 274/1000 | Loss: 0.00005035
Iteration 275/1000 | Loss: 0.00003831
Iteration 276/1000 | Loss: 0.00004510
Iteration 277/1000 | Loss: 0.00004232
Iteration 278/1000 | Loss: 0.00003671
Iteration 279/1000 | Loss: 0.00003851
Iteration 280/1000 | Loss: 0.00003639
Iteration 281/1000 | Loss: 0.00003911
Iteration 282/1000 | Loss: 0.00004072
Iteration 283/1000 | Loss: 0.00003858
Iteration 284/1000 | Loss: 0.00004065
Iteration 285/1000 | Loss: 0.00003986
Iteration 286/1000 | Loss: 0.00002790
Iteration 287/1000 | Loss: 0.00002684
Iteration 288/1000 | Loss: 0.00002543
Iteration 289/1000 | Loss: 0.00002432
Iteration 290/1000 | Loss: 0.00002363
Iteration 291/1000 | Loss: 0.00002667
Iteration 292/1000 | Loss: 0.00002426
Iteration 293/1000 | Loss: 0.00002311
Iteration 294/1000 | Loss: 0.00002308
Iteration 295/1000 | Loss: 0.00002359
Iteration 296/1000 | Loss: 0.00045432
Iteration 297/1000 | Loss: 0.00004609
Iteration 298/1000 | Loss: 0.00002899
Iteration 299/1000 | Loss: 0.00002404
Iteration 300/1000 | Loss: 0.00002317
Iteration 301/1000 | Loss: 0.00002308
Iteration 302/1000 | Loss: 0.00002146
Iteration 303/1000 | Loss: 0.00002098
Iteration 304/1000 | Loss: 0.00002076
Iteration 305/1000 | Loss: 0.00002065
Iteration 306/1000 | Loss: 0.00002276
Iteration 307/1000 | Loss: 0.00002064
Iteration 308/1000 | Loss: 0.00002094
Iteration 309/1000 | Loss: 0.00002053
Iteration 310/1000 | Loss: 0.00002052
Iteration 311/1000 | Loss: 0.00002052
Iteration 312/1000 | Loss: 0.00002051
Iteration 313/1000 | Loss: 0.00002051
Iteration 314/1000 | Loss: 0.00002049
Iteration 315/1000 | Loss: 0.00002048
Iteration 316/1000 | Loss: 0.00002047
Iteration 317/1000 | Loss: 0.00002047
Iteration 318/1000 | Loss: 0.00002046
Iteration 319/1000 | Loss: 0.00002285
Iteration 320/1000 | Loss: 0.00002045
Iteration 321/1000 | Loss: 0.00002040
Iteration 322/1000 | Loss: 0.00002039
Iteration 323/1000 | Loss: 0.00002039
Iteration 324/1000 | Loss: 0.00002039
Iteration 325/1000 | Loss: 0.00002038
Iteration 326/1000 | Loss: 0.00002038
Iteration 327/1000 | Loss: 0.00002037
Iteration 328/1000 | Loss: 0.00002037
Iteration 329/1000 | Loss: 0.00002037
Iteration 330/1000 | Loss: 0.00002035
Iteration 331/1000 | Loss: 0.00002034
Iteration 332/1000 | Loss: 0.00002034
Iteration 333/1000 | Loss: 0.00002034
Iteration 334/1000 | Loss: 0.00002034
Iteration 335/1000 | Loss: 0.00002033
Iteration 336/1000 | Loss: 0.00002033
Iteration 337/1000 | Loss: 0.00002033
Iteration 338/1000 | Loss: 0.00002032
Iteration 339/1000 | Loss: 0.00002150
Iteration 340/1000 | Loss: 0.00042347
Iteration 341/1000 | Loss: 0.00003277
Iteration 342/1000 | Loss: 0.00002694
Iteration 343/1000 | Loss: 0.00002451
Iteration 344/1000 | Loss: 0.00002107
Iteration 345/1000 | Loss: 0.00002172
Iteration 346/1000 | Loss: 0.00001932
Iteration 347/1000 | Loss: 0.00002076
Iteration 348/1000 | Loss: 0.00001841
Iteration 349/1000 | Loss: 0.00001820
Iteration 350/1000 | Loss: 0.00001818
Iteration 351/1000 | Loss: 0.00001815
Iteration 352/1000 | Loss: 0.00001815
Iteration 353/1000 | Loss: 0.00001814
Iteration 354/1000 | Loss: 0.00001814
Iteration 355/1000 | Loss: 0.00002124
Iteration 356/1000 | Loss: 0.00001812
Iteration 357/1000 | Loss: 0.00001811
Iteration 358/1000 | Loss: 0.00001811
Iteration 359/1000 | Loss: 0.00001811
Iteration 360/1000 | Loss: 0.00001811
Iteration 361/1000 | Loss: 0.00001811
Iteration 362/1000 | Loss: 0.00001811
Iteration 363/1000 | Loss: 0.00001811
Iteration 364/1000 | Loss: 0.00001809
Iteration 365/1000 | Loss: 0.00001807
Iteration 366/1000 | Loss: 0.00001806
Iteration 367/1000 | Loss: 0.00001802
Iteration 368/1000 | Loss: 0.00001802
Iteration 369/1000 | Loss: 0.00001802
Iteration 370/1000 | Loss: 0.00001802
Iteration 371/1000 | Loss: 0.00001802
Iteration 372/1000 | Loss: 0.00001801
Iteration 373/1000 | Loss: 0.00001799
Iteration 374/1000 | Loss: 0.00001796
Iteration 375/1000 | Loss: 0.00001796
Iteration 376/1000 | Loss: 0.00001793
Iteration 377/1000 | Loss: 0.00001793
Iteration 378/1000 | Loss: 0.00001792
Iteration 379/1000 | Loss: 0.00001792
Iteration 380/1000 | Loss: 0.00001792
Iteration 381/1000 | Loss: 0.00001791
Iteration 382/1000 | Loss: 0.00001791
Iteration 383/1000 | Loss: 0.00001790
Iteration 384/1000 | Loss: 0.00001790
Iteration 385/1000 | Loss: 0.00001790
Iteration 386/1000 | Loss: 0.00001789
Iteration 387/1000 | Loss: 0.00001789
Iteration 388/1000 | Loss: 0.00001789
Iteration 389/1000 | Loss: 0.00001789
Iteration 390/1000 | Loss: 0.00001788
Iteration 391/1000 | Loss: 0.00001788
Iteration 392/1000 | Loss: 0.00001788
Iteration 393/1000 | Loss: 0.00001788
Iteration 394/1000 | Loss: 0.00001788
Iteration 395/1000 | Loss: 0.00001788
Iteration 396/1000 | Loss: 0.00001788
Iteration 397/1000 | Loss: 0.00001788
Iteration 398/1000 | Loss: 0.00001788
Iteration 399/1000 | Loss: 0.00001788
Iteration 400/1000 | Loss: 0.00001788
Iteration 401/1000 | Loss: 0.00001788
Iteration 402/1000 | Loss: 0.00001788
Iteration 403/1000 | Loss: 0.00002388
Iteration 404/1000 | Loss: 0.00001786
Iteration 405/1000 | Loss: 0.00001785
Iteration 406/1000 | Loss: 0.00001785
Iteration 407/1000 | Loss: 0.00001785
Iteration 408/1000 | Loss: 0.00001785
Iteration 409/1000 | Loss: 0.00001785
Iteration 410/1000 | Loss: 0.00001785
Iteration 411/1000 | Loss: 0.00001785
Iteration 412/1000 | Loss: 0.00001785
Iteration 413/1000 | Loss: 0.00001785
Iteration 414/1000 | Loss: 0.00001785
Iteration 415/1000 | Loss: 0.00001785
Iteration 416/1000 | Loss: 0.00001784
Iteration 417/1000 | Loss: 0.00001784
Iteration 418/1000 | Loss: 0.00001784
Iteration 419/1000 | Loss: 0.00001784
Iteration 420/1000 | Loss: 0.00001784
Iteration 421/1000 | Loss: 0.00001784
Iteration 422/1000 | Loss: 0.00001783
Iteration 423/1000 | Loss: 0.00001783
Iteration 424/1000 | Loss: 0.00001783
Iteration 425/1000 | Loss: 0.00001783
Iteration 426/1000 | Loss: 0.00001783
Iteration 427/1000 | Loss: 0.00001783
Iteration 428/1000 | Loss: 0.00001782
Iteration 429/1000 | Loss: 0.00001782
Iteration 430/1000 | Loss: 0.00001782
Iteration 431/1000 | Loss: 0.00001782
Iteration 432/1000 | Loss: 0.00001782
Iteration 433/1000 | Loss: 0.00001782
Iteration 434/1000 | Loss: 0.00002227
Iteration 435/1000 | Loss: 0.00001979
Iteration 436/1000 | Loss: 0.00001819
Iteration 437/1000 | Loss: 0.00001781
Iteration 438/1000 | Loss: 0.00001781
Iteration 439/1000 | Loss: 0.00001781
Iteration 440/1000 | Loss: 0.00001781
Iteration 441/1000 | Loss: 0.00001781
Iteration 442/1000 | Loss: 0.00001781
Iteration 443/1000 | Loss: 0.00001781
Iteration 444/1000 | Loss: 0.00001781
Iteration 445/1000 | Loss: 0.00001781
Iteration 446/1000 | Loss: 0.00001781
Iteration 447/1000 | Loss: 0.00001781
Iteration 448/1000 | Loss: 0.00001781
Iteration 449/1000 | Loss: 0.00001781
Iteration 450/1000 | Loss: 0.00001781
Iteration 451/1000 | Loss: 0.00001781
Iteration 452/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 452. Stopping optimization.
Last 5 losses: [1.780712955223862e-05, 1.780712955223862e-05, 1.780712955223862e-05, 1.780712955223862e-05, 1.780712955223862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.780712955223862e-05

Optimization complete. Final v2v error: 3.5992448329925537 mm

Highest mean error: 12.207316398620605 mm for frame 37

Lowest mean error: 3.018543243408203 mm for frame 187

Saving results

Total time: 577.1289055347443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905075
Iteration 2/25 | Loss: 0.00161398
Iteration 3/25 | Loss: 0.00133971
Iteration 4/25 | Loss: 0.00131176
Iteration 5/25 | Loss: 0.00130374
Iteration 6/25 | Loss: 0.00130173
Iteration 7/25 | Loss: 0.00130153
Iteration 8/25 | Loss: 0.00130153
Iteration 9/25 | Loss: 0.00130153
Iteration 10/25 | Loss: 0.00130153
Iteration 11/25 | Loss: 0.00130153
Iteration 12/25 | Loss: 0.00130153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013015300501137972, 0.0013015300501137972, 0.0013015300501137972, 0.0013015300501137972, 0.0013015300501137972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013015300501137972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54240644
Iteration 2/25 | Loss: 0.00519622
Iteration 3/25 | Loss: 0.00519621
Iteration 4/25 | Loss: 0.00519621
Iteration 5/25 | Loss: 0.00519621
Iteration 6/25 | Loss: 0.00519621
Iteration 7/25 | Loss: 0.00519621
Iteration 8/25 | Loss: 0.00519621
Iteration 9/25 | Loss: 0.00519621
Iteration 10/25 | Loss: 0.00519621
Iteration 11/25 | Loss: 0.00519621
Iteration 12/25 | Loss: 0.00519621
Iteration 13/25 | Loss: 0.00519621
Iteration 14/25 | Loss: 0.00519621
Iteration 15/25 | Loss: 0.00519621
Iteration 16/25 | Loss: 0.00519621
Iteration 17/25 | Loss: 0.00519621
Iteration 18/25 | Loss: 0.00519621
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00519620580598712, 0.00519620580598712, 0.00519620580598712, 0.00519620580598712, 0.00519620580598712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00519620580598712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00519621
Iteration 2/1000 | Loss: 0.00005074
Iteration 3/1000 | Loss: 0.00003951
Iteration 4/1000 | Loss: 0.00003269
Iteration 5/1000 | Loss: 0.00002922
Iteration 6/1000 | Loss: 0.00002766
Iteration 7/1000 | Loss: 0.00002681
Iteration 8/1000 | Loss: 0.00002609
Iteration 9/1000 | Loss: 0.00002538
Iteration 10/1000 | Loss: 0.00002489
Iteration 11/1000 | Loss: 0.00002454
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002379
Iteration 15/1000 | Loss: 0.00002364
Iteration 16/1000 | Loss: 0.00002346
Iteration 17/1000 | Loss: 0.00002341
Iteration 18/1000 | Loss: 0.00002333
Iteration 19/1000 | Loss: 0.00002320
Iteration 20/1000 | Loss: 0.00002316
Iteration 21/1000 | Loss: 0.00002316
Iteration 22/1000 | Loss: 0.00002312
Iteration 23/1000 | Loss: 0.00002311
Iteration 24/1000 | Loss: 0.00002309
Iteration 25/1000 | Loss: 0.00002309
Iteration 26/1000 | Loss: 0.00002308
Iteration 27/1000 | Loss: 0.00002308
Iteration 28/1000 | Loss: 0.00002304
Iteration 29/1000 | Loss: 0.00002303
Iteration 30/1000 | Loss: 0.00002303
Iteration 31/1000 | Loss: 0.00002299
Iteration 32/1000 | Loss: 0.00002296
Iteration 33/1000 | Loss: 0.00002295
Iteration 34/1000 | Loss: 0.00002295
Iteration 35/1000 | Loss: 0.00002295
Iteration 36/1000 | Loss: 0.00002294
Iteration 37/1000 | Loss: 0.00002293
Iteration 38/1000 | Loss: 0.00002293
Iteration 39/1000 | Loss: 0.00002293
Iteration 40/1000 | Loss: 0.00002293
Iteration 41/1000 | Loss: 0.00002293
Iteration 42/1000 | Loss: 0.00002293
Iteration 43/1000 | Loss: 0.00002293
Iteration 44/1000 | Loss: 0.00002292
Iteration 45/1000 | Loss: 0.00002292
Iteration 46/1000 | Loss: 0.00002292
Iteration 47/1000 | Loss: 0.00002292
Iteration 48/1000 | Loss: 0.00002292
Iteration 49/1000 | Loss: 0.00002292
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002292
Iteration 52/1000 | Loss: 0.00002292
Iteration 53/1000 | Loss: 0.00002292
Iteration 54/1000 | Loss: 0.00002291
Iteration 55/1000 | Loss: 0.00002290
Iteration 56/1000 | Loss: 0.00002290
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002288
Iteration 62/1000 | Loss: 0.00002288
Iteration 63/1000 | Loss: 0.00002288
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002287
Iteration 66/1000 | Loss: 0.00002287
Iteration 67/1000 | Loss: 0.00002287
Iteration 68/1000 | Loss: 0.00002286
Iteration 69/1000 | Loss: 0.00002285
Iteration 70/1000 | Loss: 0.00002285
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002284
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002283
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002280
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002280
Iteration 89/1000 | Loss: 0.00002280
Iteration 90/1000 | Loss: 0.00002279
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002278
Iteration 93/1000 | Loss: 0.00002278
Iteration 94/1000 | Loss: 0.00002278
Iteration 95/1000 | Loss: 0.00002278
Iteration 96/1000 | Loss: 0.00002278
Iteration 97/1000 | Loss: 0.00002278
Iteration 98/1000 | Loss: 0.00002278
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002277
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002276
Iteration 106/1000 | Loss: 0.00002276
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002276
Iteration 111/1000 | Loss: 0.00002276
Iteration 112/1000 | Loss: 0.00002275
Iteration 113/1000 | Loss: 0.00002275
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002275
Iteration 116/1000 | Loss: 0.00002275
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002273
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002272
Iteration 127/1000 | Loss: 0.00002272
Iteration 128/1000 | Loss: 0.00002272
Iteration 129/1000 | Loss: 0.00002272
Iteration 130/1000 | Loss: 0.00002272
Iteration 131/1000 | Loss: 0.00002272
Iteration 132/1000 | Loss: 0.00002272
Iteration 133/1000 | Loss: 0.00002271
Iteration 134/1000 | Loss: 0.00002271
Iteration 135/1000 | Loss: 0.00002271
Iteration 136/1000 | Loss: 0.00002271
Iteration 137/1000 | Loss: 0.00002271
Iteration 138/1000 | Loss: 0.00002271
Iteration 139/1000 | Loss: 0.00002271
Iteration 140/1000 | Loss: 0.00002271
Iteration 141/1000 | Loss: 0.00002271
Iteration 142/1000 | Loss: 0.00002271
Iteration 143/1000 | Loss: 0.00002271
Iteration 144/1000 | Loss: 0.00002271
Iteration 145/1000 | Loss: 0.00002270
Iteration 146/1000 | Loss: 0.00002270
Iteration 147/1000 | Loss: 0.00002270
Iteration 148/1000 | Loss: 0.00002270
Iteration 149/1000 | Loss: 0.00002270
Iteration 150/1000 | Loss: 0.00002270
Iteration 151/1000 | Loss: 0.00002270
Iteration 152/1000 | Loss: 0.00002270
Iteration 153/1000 | Loss: 0.00002270
Iteration 154/1000 | Loss: 0.00002270
Iteration 155/1000 | Loss: 0.00002270
Iteration 156/1000 | Loss: 0.00002270
Iteration 157/1000 | Loss: 0.00002270
Iteration 158/1000 | Loss: 0.00002270
Iteration 159/1000 | Loss: 0.00002270
Iteration 160/1000 | Loss: 0.00002270
Iteration 161/1000 | Loss: 0.00002270
Iteration 162/1000 | Loss: 0.00002270
Iteration 163/1000 | Loss: 0.00002270
Iteration 164/1000 | Loss: 0.00002270
Iteration 165/1000 | Loss: 0.00002270
Iteration 166/1000 | Loss: 0.00002270
Iteration 167/1000 | Loss: 0.00002270
Iteration 168/1000 | Loss: 0.00002270
Iteration 169/1000 | Loss: 0.00002270
Iteration 170/1000 | Loss: 0.00002270
Iteration 171/1000 | Loss: 0.00002270
Iteration 172/1000 | Loss: 0.00002270
Iteration 173/1000 | Loss: 0.00002270
Iteration 174/1000 | Loss: 0.00002270
Iteration 175/1000 | Loss: 0.00002270
Iteration 176/1000 | Loss: 0.00002270
Iteration 177/1000 | Loss: 0.00002270
Iteration 178/1000 | Loss: 0.00002270
Iteration 179/1000 | Loss: 0.00002270
Iteration 180/1000 | Loss: 0.00002270
Iteration 181/1000 | Loss: 0.00002270
Iteration 182/1000 | Loss: 0.00002270
Iteration 183/1000 | Loss: 0.00002270
Iteration 184/1000 | Loss: 0.00002270
Iteration 185/1000 | Loss: 0.00002270
Iteration 186/1000 | Loss: 0.00002270
Iteration 187/1000 | Loss: 0.00002270
Iteration 188/1000 | Loss: 0.00002270
Iteration 189/1000 | Loss: 0.00002270
Iteration 190/1000 | Loss: 0.00002270
Iteration 191/1000 | Loss: 0.00002270
Iteration 192/1000 | Loss: 0.00002270
Iteration 193/1000 | Loss: 0.00002270
Iteration 194/1000 | Loss: 0.00002270
Iteration 195/1000 | Loss: 0.00002270
Iteration 196/1000 | Loss: 0.00002270
Iteration 197/1000 | Loss: 0.00002270
Iteration 198/1000 | Loss: 0.00002270
Iteration 199/1000 | Loss: 0.00002270
Iteration 200/1000 | Loss: 0.00002270
Iteration 201/1000 | Loss: 0.00002270
Iteration 202/1000 | Loss: 0.00002270
Iteration 203/1000 | Loss: 0.00002270
Iteration 204/1000 | Loss: 0.00002270
Iteration 205/1000 | Loss: 0.00002270
Iteration 206/1000 | Loss: 0.00002270
Iteration 207/1000 | Loss: 0.00002270
Iteration 208/1000 | Loss: 0.00002270
Iteration 209/1000 | Loss: 0.00002270
Iteration 210/1000 | Loss: 0.00002270
Iteration 211/1000 | Loss: 0.00002270
Iteration 212/1000 | Loss: 0.00002270
Iteration 213/1000 | Loss: 0.00002270
Iteration 214/1000 | Loss: 0.00002270
Iteration 215/1000 | Loss: 0.00002270
Iteration 216/1000 | Loss: 0.00002270
Iteration 217/1000 | Loss: 0.00002270
Iteration 218/1000 | Loss: 0.00002270
Iteration 219/1000 | Loss: 0.00002270
Iteration 220/1000 | Loss: 0.00002270
Iteration 221/1000 | Loss: 0.00002270
Iteration 222/1000 | Loss: 0.00002270
Iteration 223/1000 | Loss: 0.00002270
Iteration 224/1000 | Loss: 0.00002270
Iteration 225/1000 | Loss: 0.00002270
Iteration 226/1000 | Loss: 0.00002270
Iteration 227/1000 | Loss: 0.00002270
Iteration 228/1000 | Loss: 0.00002270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [2.270331788167823e-05, 2.270331788167823e-05, 2.270331788167823e-05, 2.270331788167823e-05, 2.270331788167823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.270331788167823e-05

Optimization complete. Final v2v error: 4.075453281402588 mm

Highest mean error: 5.555810451507568 mm for frame 65

Lowest mean error: 3.4624884128570557 mm for frame 94

Saving results

Total time: 47.429261684417725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898824
Iteration 2/25 | Loss: 0.00148939
Iteration 3/25 | Loss: 0.00135083
Iteration 4/25 | Loss: 0.00133076
Iteration 5/25 | Loss: 0.00132396
Iteration 6/25 | Loss: 0.00132174
Iteration 7/25 | Loss: 0.00132164
Iteration 8/25 | Loss: 0.00132164
Iteration 9/25 | Loss: 0.00132164
Iteration 10/25 | Loss: 0.00132164
Iteration 11/25 | Loss: 0.00132164
Iteration 12/25 | Loss: 0.00132164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001321642892435193, 0.001321642892435193, 0.001321642892435193, 0.001321642892435193, 0.001321642892435193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001321642892435193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59460390
Iteration 2/25 | Loss: 0.00460345
Iteration 3/25 | Loss: 0.00460341
Iteration 4/25 | Loss: 0.00460341
Iteration 5/25 | Loss: 0.00460341
Iteration 6/25 | Loss: 0.00460341
Iteration 7/25 | Loss: 0.00460341
Iteration 8/25 | Loss: 0.00460341
Iteration 9/25 | Loss: 0.00460341
Iteration 10/25 | Loss: 0.00460341
Iteration 11/25 | Loss: 0.00460341
Iteration 12/25 | Loss: 0.00460341
Iteration 13/25 | Loss: 0.00460341
Iteration 14/25 | Loss: 0.00460341
Iteration 15/25 | Loss: 0.00460341
Iteration 16/25 | Loss: 0.00460341
Iteration 17/25 | Loss: 0.00460341
Iteration 18/25 | Loss: 0.00460341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004603410605341196, 0.004603410605341196, 0.004603410605341196, 0.004603410605341196, 0.004603410605341196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004603410605341196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00460341
Iteration 2/1000 | Loss: 0.00005075
Iteration 3/1000 | Loss: 0.00003430
Iteration 4/1000 | Loss: 0.00002831
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002471
Iteration 7/1000 | Loss: 0.00002354
Iteration 8/1000 | Loss: 0.00002273
Iteration 9/1000 | Loss: 0.00002225
Iteration 10/1000 | Loss: 0.00002192
Iteration 11/1000 | Loss: 0.00002164
Iteration 12/1000 | Loss: 0.00002161
Iteration 13/1000 | Loss: 0.00002151
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002132
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002121
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002116
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00002116
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002114
Iteration 29/1000 | Loss: 0.00002114
Iteration 30/1000 | Loss: 0.00002114
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002113
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002113
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002112
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00002109
Iteration 50/1000 | Loss: 0.00002109
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002108
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002108
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002107
Iteration 59/1000 | Loss: 0.00002107
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002106
Iteration 62/1000 | Loss: 0.00002106
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002105
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002105
Iteration 68/1000 | Loss: 0.00002105
Iteration 69/1000 | Loss: 0.00002105
Iteration 70/1000 | Loss: 0.00002105
Iteration 71/1000 | Loss: 0.00002105
Iteration 72/1000 | Loss: 0.00002105
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002104
Iteration 76/1000 | Loss: 0.00002104
Iteration 77/1000 | Loss: 0.00002104
Iteration 78/1000 | Loss: 0.00002104
Iteration 79/1000 | Loss: 0.00002103
Iteration 80/1000 | Loss: 0.00002103
Iteration 81/1000 | Loss: 0.00002103
Iteration 82/1000 | Loss: 0.00002103
Iteration 83/1000 | Loss: 0.00002103
Iteration 84/1000 | Loss: 0.00002103
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002102
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002100
Iteration 98/1000 | Loss: 0.00002100
Iteration 99/1000 | Loss: 0.00002100
Iteration 100/1000 | Loss: 0.00002100
Iteration 101/1000 | Loss: 0.00002100
Iteration 102/1000 | Loss: 0.00002100
Iteration 103/1000 | Loss: 0.00002100
Iteration 104/1000 | Loss: 0.00002100
Iteration 105/1000 | Loss: 0.00002100
Iteration 106/1000 | Loss: 0.00002100
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002100
Iteration 109/1000 | Loss: 0.00002100
Iteration 110/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.1002932044211775e-05, 2.1002932044211775e-05, 2.1002932044211775e-05, 2.1002932044211775e-05, 2.1002932044211775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1002932044211775e-05

Optimization complete. Final v2v error: 3.9885010719299316 mm

Highest mean error: 4.641905307769775 mm for frame 52

Lowest mean error: 3.705512046813965 mm for frame 22

Saving results

Total time: 40.20837664604187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01252904
Iteration 2/25 | Loss: 0.01252903
Iteration 3/25 | Loss: 0.00216967
Iteration 4/25 | Loss: 0.00139848
Iteration 5/25 | Loss: 0.00134140
Iteration 6/25 | Loss: 0.00135191
Iteration 7/25 | Loss: 0.00134543
Iteration 8/25 | Loss: 0.00132760
Iteration 9/25 | Loss: 0.00134230
Iteration 10/25 | Loss: 0.00132521
Iteration 11/25 | Loss: 0.00132348
Iteration 12/25 | Loss: 0.00133731
Iteration 13/25 | Loss: 0.00131793
Iteration 14/25 | Loss: 0.00130797
Iteration 15/25 | Loss: 0.00130310
Iteration 16/25 | Loss: 0.00130175
Iteration 17/25 | Loss: 0.00130058
Iteration 18/25 | Loss: 0.00129980
Iteration 19/25 | Loss: 0.00129790
Iteration 20/25 | Loss: 0.00129757
Iteration 21/25 | Loss: 0.00129754
Iteration 22/25 | Loss: 0.00129753
Iteration 23/25 | Loss: 0.00129752
Iteration 24/25 | Loss: 0.00129752
Iteration 25/25 | Loss: 0.00129752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76715887
Iteration 2/25 | Loss: 0.00389507
Iteration 3/25 | Loss: 0.00389507
Iteration 4/25 | Loss: 0.00389507
Iteration 5/25 | Loss: 0.00389507
Iteration 6/25 | Loss: 0.00389507
Iteration 7/25 | Loss: 0.00389507
Iteration 8/25 | Loss: 0.00389507
Iteration 9/25 | Loss: 0.00389507
Iteration 10/25 | Loss: 0.00389507
Iteration 11/25 | Loss: 0.00389507
Iteration 12/25 | Loss: 0.00389507
Iteration 13/25 | Loss: 0.00389507
Iteration 14/25 | Loss: 0.00389507
Iteration 15/25 | Loss: 0.00389507
Iteration 16/25 | Loss: 0.00389507
Iteration 17/25 | Loss: 0.00389507
Iteration 18/25 | Loss: 0.00389507
Iteration 19/25 | Loss: 0.00389507
Iteration 20/25 | Loss: 0.00389507
Iteration 21/25 | Loss: 0.00389507
Iteration 22/25 | Loss: 0.00389507
Iteration 23/25 | Loss: 0.00389507
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0038950687739998102, 0.0038950687739998102, 0.0038950687739998102, 0.0038950687739998102, 0.0038950687739998102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038950687739998102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389507
Iteration 2/1000 | Loss: 0.00007334
Iteration 3/1000 | Loss: 0.00004899
Iteration 4/1000 | Loss: 0.00004359
Iteration 5/1000 | Loss: 0.00004141
Iteration 6/1000 | Loss: 0.00004010
Iteration 7/1000 | Loss: 0.00003884
Iteration 8/1000 | Loss: 0.00003813
Iteration 9/1000 | Loss: 0.00003762
Iteration 10/1000 | Loss: 0.00003718
Iteration 11/1000 | Loss: 0.00003686
Iteration 12/1000 | Loss: 0.00003656
Iteration 13/1000 | Loss: 0.00003636
Iteration 14/1000 | Loss: 0.00003630
Iteration 15/1000 | Loss: 0.00003623
Iteration 16/1000 | Loss: 0.00003620
Iteration 17/1000 | Loss: 0.00003618
Iteration 18/1000 | Loss: 0.00003617
Iteration 19/1000 | Loss: 0.00003617
Iteration 20/1000 | Loss: 0.00003616
Iteration 21/1000 | Loss: 0.00003615
Iteration 22/1000 | Loss: 0.00003612
Iteration 23/1000 | Loss: 0.00003612
Iteration 24/1000 | Loss: 0.00003612
Iteration 25/1000 | Loss: 0.00003611
Iteration 26/1000 | Loss: 0.00003611
Iteration 27/1000 | Loss: 0.00003609
Iteration 28/1000 | Loss: 0.00003608
Iteration 29/1000 | Loss: 0.00003608
Iteration 30/1000 | Loss: 0.00003608
Iteration 31/1000 | Loss: 0.00003608
Iteration 32/1000 | Loss: 0.00003606
Iteration 33/1000 | Loss: 0.00003606
Iteration 34/1000 | Loss: 0.00003606
Iteration 35/1000 | Loss: 0.00003606
Iteration 36/1000 | Loss: 0.00003605
Iteration 37/1000 | Loss: 0.00003605
Iteration 38/1000 | Loss: 0.00003605
Iteration 39/1000 | Loss: 0.00003605
Iteration 40/1000 | Loss: 0.00003604
Iteration 41/1000 | Loss: 0.00003603
Iteration 42/1000 | Loss: 0.00003603
Iteration 43/1000 | Loss: 0.00003603
Iteration 44/1000 | Loss: 0.00003603
Iteration 45/1000 | Loss: 0.00003603
Iteration 46/1000 | Loss: 0.00003603
Iteration 47/1000 | Loss: 0.00003602
Iteration 48/1000 | Loss: 0.00003602
Iteration 49/1000 | Loss: 0.00003602
Iteration 50/1000 | Loss: 0.00003602
Iteration 51/1000 | Loss: 0.00003602
Iteration 52/1000 | Loss: 0.00003602
Iteration 53/1000 | Loss: 0.00003601
Iteration 54/1000 | Loss: 0.00003601
Iteration 55/1000 | Loss: 0.00003601
Iteration 56/1000 | Loss: 0.00003601
Iteration 57/1000 | Loss: 0.00003600
Iteration 58/1000 | Loss: 0.00003600
Iteration 59/1000 | Loss: 0.00003600
Iteration 60/1000 | Loss: 0.00003600
Iteration 61/1000 | Loss: 0.00003600
Iteration 62/1000 | Loss: 0.00003600
Iteration 63/1000 | Loss: 0.00003600
Iteration 64/1000 | Loss: 0.00003600
Iteration 65/1000 | Loss: 0.00003599
Iteration 66/1000 | Loss: 0.00003599
Iteration 67/1000 | Loss: 0.00003599
Iteration 68/1000 | Loss: 0.00003599
Iteration 69/1000 | Loss: 0.00003599
Iteration 70/1000 | Loss: 0.00003599
Iteration 71/1000 | Loss: 0.00003599
Iteration 72/1000 | Loss: 0.00003599
Iteration 73/1000 | Loss: 0.00003598
Iteration 74/1000 | Loss: 0.00003598
Iteration 75/1000 | Loss: 0.00003598
Iteration 76/1000 | Loss: 0.00003598
Iteration 77/1000 | Loss: 0.00003598
Iteration 78/1000 | Loss: 0.00003598
Iteration 79/1000 | Loss: 0.00003598
Iteration 80/1000 | Loss: 0.00003597
Iteration 81/1000 | Loss: 0.00003597
Iteration 82/1000 | Loss: 0.00003597
Iteration 83/1000 | Loss: 0.00003597
Iteration 84/1000 | Loss: 0.00003597
Iteration 85/1000 | Loss: 0.00003597
Iteration 86/1000 | Loss: 0.00003596
Iteration 87/1000 | Loss: 0.00003596
Iteration 88/1000 | Loss: 0.00003596
Iteration 89/1000 | Loss: 0.00003596
Iteration 90/1000 | Loss: 0.00003596
Iteration 91/1000 | Loss: 0.00003596
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00003596
Iteration 94/1000 | Loss: 0.00003595
Iteration 95/1000 | Loss: 0.00003595
Iteration 96/1000 | Loss: 0.00003595
Iteration 97/1000 | Loss: 0.00003595
Iteration 98/1000 | Loss: 0.00003595
Iteration 99/1000 | Loss: 0.00003595
Iteration 100/1000 | Loss: 0.00003595
Iteration 101/1000 | Loss: 0.00003595
Iteration 102/1000 | Loss: 0.00003595
Iteration 103/1000 | Loss: 0.00003595
Iteration 104/1000 | Loss: 0.00003595
Iteration 105/1000 | Loss: 0.00003595
Iteration 106/1000 | Loss: 0.00003595
Iteration 107/1000 | Loss: 0.00003595
Iteration 108/1000 | Loss: 0.00003595
Iteration 109/1000 | Loss: 0.00003595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [3.594979352783412e-05, 3.594979352783412e-05, 3.594979352783412e-05, 3.594979352783412e-05, 3.594979352783412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.594979352783412e-05

Optimization complete. Final v2v error: 4.781564235687256 mm

Highest mean error: 21.1038818359375 mm for frame 31

Lowest mean error: 4.162716388702393 mm for frame 129

Saving results

Total time: 69.76242017745972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01196991
Iteration 2/25 | Loss: 0.01196991
Iteration 3/25 | Loss: 0.00259327
Iteration 4/25 | Loss: 0.00188916
Iteration 5/25 | Loss: 0.00178281
Iteration 6/25 | Loss: 0.00174527
Iteration 7/25 | Loss: 0.00166580
Iteration 8/25 | Loss: 0.00155475
Iteration 9/25 | Loss: 0.00149863
Iteration 10/25 | Loss: 0.00151875
Iteration 11/25 | Loss: 0.00150113
Iteration 12/25 | Loss: 0.00147906
Iteration 13/25 | Loss: 0.00146411
Iteration 14/25 | Loss: 0.00145073
Iteration 15/25 | Loss: 0.00143559
Iteration 16/25 | Loss: 0.00145036
Iteration 17/25 | Loss: 0.00144408
Iteration 18/25 | Loss: 0.00141892
Iteration 19/25 | Loss: 0.00140822
Iteration 20/25 | Loss: 0.00140404
Iteration 21/25 | Loss: 0.00142116
Iteration 22/25 | Loss: 0.00141101
Iteration 23/25 | Loss: 0.00138255
Iteration 24/25 | Loss: 0.00137384
Iteration 25/25 | Loss: 0.00137969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97207785
Iteration 2/25 | Loss: 0.00607505
Iteration 3/25 | Loss: 0.00607505
Iteration 4/25 | Loss: 0.00607505
Iteration 5/25 | Loss: 0.00607505
Iteration 6/25 | Loss: 0.00607505
Iteration 7/25 | Loss: 0.00607505
Iteration 8/25 | Loss: 0.00607505
Iteration 9/25 | Loss: 0.00607505
Iteration 10/25 | Loss: 0.00607505
Iteration 11/25 | Loss: 0.00607505
Iteration 12/25 | Loss: 0.00607505
Iteration 13/25 | Loss: 0.00607505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.006075046490877867, 0.006075046490877867, 0.006075046490877867, 0.006075046490877867, 0.006075046490877867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006075046490877867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00607505
Iteration 2/1000 | Loss: 0.00045395
Iteration 3/1000 | Loss: 0.00007054
Iteration 4/1000 | Loss: 0.00005189
Iteration 5/1000 | Loss: 0.00044390
Iteration 6/1000 | Loss: 0.00029158
Iteration 7/1000 | Loss: 0.00046816
Iteration 8/1000 | Loss: 0.00017041
Iteration 9/1000 | Loss: 0.00025744
Iteration 10/1000 | Loss: 0.00012534
Iteration 11/1000 | Loss: 0.00007711
Iteration 12/1000 | Loss: 0.00047441
Iteration 13/1000 | Loss: 0.00030179
Iteration 14/1000 | Loss: 0.00054042
Iteration 15/1000 | Loss: 0.00025429
Iteration 16/1000 | Loss: 0.00040276
Iteration 17/1000 | Loss: 0.00008187
Iteration 18/1000 | Loss: 0.00046940
Iteration 19/1000 | Loss: 0.00050056
Iteration 20/1000 | Loss: 0.00050768
Iteration 21/1000 | Loss: 0.00039996
Iteration 22/1000 | Loss: 0.00009627
Iteration 23/1000 | Loss: 0.00009280
Iteration 24/1000 | Loss: 0.00008301
Iteration 25/1000 | Loss: 0.00005102
Iteration 26/1000 | Loss: 0.00008019
Iteration 27/1000 | Loss: 0.00029982
Iteration 28/1000 | Loss: 0.00004916
Iteration 29/1000 | Loss: 0.00011400
Iteration 30/1000 | Loss: 0.00003792
Iteration 31/1000 | Loss: 0.00037437
Iteration 32/1000 | Loss: 0.00018643
Iteration 33/1000 | Loss: 0.00003457
Iteration 34/1000 | Loss: 0.00003297
Iteration 35/1000 | Loss: 0.00003180
Iteration 36/1000 | Loss: 0.00027833
Iteration 37/1000 | Loss: 0.00032748
Iteration 38/1000 | Loss: 0.00048104
Iteration 39/1000 | Loss: 0.00111962
Iteration 40/1000 | Loss: 0.00116687
Iteration 41/1000 | Loss: 0.00019841
Iteration 42/1000 | Loss: 0.00031520
Iteration 43/1000 | Loss: 0.00020111
Iteration 44/1000 | Loss: 0.00004431
Iteration 45/1000 | Loss: 0.00003539
Iteration 46/1000 | Loss: 0.00027323
Iteration 47/1000 | Loss: 0.00016897
Iteration 48/1000 | Loss: 0.00023112
Iteration 49/1000 | Loss: 0.00026331
Iteration 50/1000 | Loss: 0.00006573
Iteration 51/1000 | Loss: 0.00027599
Iteration 52/1000 | Loss: 0.00003941
Iteration 53/1000 | Loss: 0.00006932
Iteration 54/1000 | Loss: 0.00004546
Iteration 55/1000 | Loss: 0.00003203
Iteration 56/1000 | Loss: 0.00002866
Iteration 57/1000 | Loss: 0.00002685
Iteration 58/1000 | Loss: 0.00002602
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00003248
Iteration 61/1000 | Loss: 0.00003479
Iteration 62/1000 | Loss: 0.00002874
Iteration 63/1000 | Loss: 0.00003527
Iteration 64/1000 | Loss: 0.00003320
Iteration 65/1000 | Loss: 0.00003156
Iteration 66/1000 | Loss: 0.00003459
Iteration 67/1000 | Loss: 0.00003554
Iteration 68/1000 | Loss: 0.00003665
Iteration 69/1000 | Loss: 0.00002423
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00003605
Iteration 72/1000 | Loss: 0.00002769
Iteration 73/1000 | Loss: 0.00003229
Iteration 74/1000 | Loss: 0.00002779
Iteration 75/1000 | Loss: 0.00002678
Iteration 76/1000 | Loss: 0.00002622
Iteration 77/1000 | Loss: 0.00002574
Iteration 78/1000 | Loss: 0.00002535
Iteration 79/1000 | Loss: 0.00003645
Iteration 80/1000 | Loss: 0.00003486
Iteration 81/1000 | Loss: 0.00002850
Iteration 82/1000 | Loss: 0.00002528
Iteration 83/1000 | Loss: 0.00002394
Iteration 84/1000 | Loss: 0.00002259
Iteration 85/1000 | Loss: 0.00002203
Iteration 86/1000 | Loss: 0.00002171
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002144
Iteration 89/1000 | Loss: 0.00002143
Iteration 90/1000 | Loss: 0.00002143
Iteration 91/1000 | Loss: 0.00002143
Iteration 92/1000 | Loss: 0.00002142
Iteration 93/1000 | Loss: 0.00002142
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002141
Iteration 96/1000 | Loss: 0.00002141
Iteration 97/1000 | Loss: 0.00002141
Iteration 98/1000 | Loss: 0.00002141
Iteration 99/1000 | Loss: 0.00002141
Iteration 100/1000 | Loss: 0.00002141
Iteration 101/1000 | Loss: 0.00002140
Iteration 102/1000 | Loss: 0.00002140
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002138
Iteration 107/1000 | Loss: 0.00002138
Iteration 108/1000 | Loss: 0.00002137
Iteration 109/1000 | Loss: 0.00002137
Iteration 110/1000 | Loss: 0.00002137
Iteration 111/1000 | Loss: 0.00002137
Iteration 112/1000 | Loss: 0.00002137
Iteration 113/1000 | Loss: 0.00002137
Iteration 114/1000 | Loss: 0.00002137
Iteration 115/1000 | Loss: 0.00002137
Iteration 116/1000 | Loss: 0.00002136
Iteration 117/1000 | Loss: 0.00002136
Iteration 118/1000 | Loss: 0.00002136
Iteration 119/1000 | Loss: 0.00002136
Iteration 120/1000 | Loss: 0.00002136
Iteration 121/1000 | Loss: 0.00002136
Iteration 122/1000 | Loss: 0.00002136
Iteration 123/1000 | Loss: 0.00002136
Iteration 124/1000 | Loss: 0.00002136
Iteration 125/1000 | Loss: 0.00002136
Iteration 126/1000 | Loss: 0.00002136
Iteration 127/1000 | Loss: 0.00002135
Iteration 128/1000 | Loss: 0.00002135
Iteration 129/1000 | Loss: 0.00002135
Iteration 130/1000 | Loss: 0.00002135
Iteration 131/1000 | Loss: 0.00002135
Iteration 132/1000 | Loss: 0.00002135
Iteration 133/1000 | Loss: 0.00002135
Iteration 134/1000 | Loss: 0.00002135
Iteration 135/1000 | Loss: 0.00002135
Iteration 136/1000 | Loss: 0.00002134
Iteration 137/1000 | Loss: 0.00002134
Iteration 138/1000 | Loss: 0.00002134
Iteration 139/1000 | Loss: 0.00002134
Iteration 140/1000 | Loss: 0.00002134
Iteration 141/1000 | Loss: 0.00002134
Iteration 142/1000 | Loss: 0.00002134
Iteration 143/1000 | Loss: 0.00002134
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002134
Iteration 148/1000 | Loss: 0.00002133
Iteration 149/1000 | Loss: 0.00002133
Iteration 150/1000 | Loss: 0.00002133
Iteration 151/1000 | Loss: 0.00002133
Iteration 152/1000 | Loss: 0.00002133
Iteration 153/1000 | Loss: 0.00002133
Iteration 154/1000 | Loss: 0.00002133
Iteration 155/1000 | Loss: 0.00002133
Iteration 156/1000 | Loss: 0.00002133
Iteration 157/1000 | Loss: 0.00002133
Iteration 158/1000 | Loss: 0.00002132
Iteration 159/1000 | Loss: 0.00002132
Iteration 160/1000 | Loss: 0.00002132
Iteration 161/1000 | Loss: 0.00002132
Iteration 162/1000 | Loss: 0.00002132
Iteration 163/1000 | Loss: 0.00002131
Iteration 164/1000 | Loss: 0.00002131
Iteration 165/1000 | Loss: 0.00002131
Iteration 166/1000 | Loss: 0.00002131
Iteration 167/1000 | Loss: 0.00002131
Iteration 168/1000 | Loss: 0.00002131
Iteration 169/1000 | Loss: 0.00002131
Iteration 170/1000 | Loss: 0.00002131
Iteration 171/1000 | Loss: 0.00002131
Iteration 172/1000 | Loss: 0.00002130
Iteration 173/1000 | Loss: 0.00002130
Iteration 174/1000 | Loss: 0.00002130
Iteration 175/1000 | Loss: 0.00002130
Iteration 176/1000 | Loss: 0.00002130
Iteration 177/1000 | Loss: 0.00002130
Iteration 178/1000 | Loss: 0.00002130
Iteration 179/1000 | Loss: 0.00002130
Iteration 180/1000 | Loss: 0.00002130
Iteration 181/1000 | Loss: 0.00002130
Iteration 182/1000 | Loss: 0.00002130
Iteration 183/1000 | Loss: 0.00002130
Iteration 184/1000 | Loss: 0.00002130
Iteration 185/1000 | Loss: 0.00002130
Iteration 186/1000 | Loss: 0.00002130
Iteration 187/1000 | Loss: 0.00002130
Iteration 188/1000 | Loss: 0.00002130
Iteration 189/1000 | Loss: 0.00002130
Iteration 190/1000 | Loss: 0.00002130
Iteration 191/1000 | Loss: 0.00002130
Iteration 192/1000 | Loss: 0.00002130
Iteration 193/1000 | Loss: 0.00002130
Iteration 194/1000 | Loss: 0.00002130
Iteration 195/1000 | Loss: 0.00002130
Iteration 196/1000 | Loss: 0.00002130
Iteration 197/1000 | Loss: 0.00002130
Iteration 198/1000 | Loss: 0.00002130
Iteration 199/1000 | Loss: 0.00002130
Iteration 200/1000 | Loss: 0.00002130
Iteration 201/1000 | Loss: 0.00002130
Iteration 202/1000 | Loss: 0.00002130
Iteration 203/1000 | Loss: 0.00002130
Iteration 204/1000 | Loss: 0.00002130
Iteration 205/1000 | Loss: 0.00002130
Iteration 206/1000 | Loss: 0.00002130
Iteration 207/1000 | Loss: 0.00002130
Iteration 208/1000 | Loss: 0.00002130
Iteration 209/1000 | Loss: 0.00002130
Iteration 210/1000 | Loss: 0.00002130
Iteration 211/1000 | Loss: 0.00002130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.1303789253579453e-05, 2.1303789253579453e-05, 2.1303789253579453e-05, 2.1303789253579453e-05, 2.1303789253579453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1303789253579453e-05

Optimization complete. Final v2v error: 3.9362003803253174 mm

Highest mean error: 4.811529159545898 mm for frame 134

Lowest mean error: 3.7377376556396484 mm for frame 63

Saving results

Total time: 167.5176911354065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01198946
Iteration 2/25 | Loss: 0.00203675
Iteration 3/25 | Loss: 0.00163891
Iteration 4/25 | Loss: 0.00162096
Iteration 5/25 | Loss: 0.00155513
Iteration 6/25 | Loss: 0.00151020
Iteration 7/25 | Loss: 0.00144958
Iteration 8/25 | Loss: 0.00144194
Iteration 9/25 | Loss: 0.00141529
Iteration 10/25 | Loss: 0.00141360
Iteration 11/25 | Loss: 0.00140836
Iteration 12/25 | Loss: 0.00140893
Iteration 13/25 | Loss: 0.00139967
Iteration 14/25 | Loss: 0.00139935
Iteration 15/25 | Loss: 0.00139917
Iteration 16/25 | Loss: 0.00139906
Iteration 17/25 | Loss: 0.00139905
Iteration 18/25 | Loss: 0.00139904
Iteration 19/25 | Loss: 0.00139904
Iteration 20/25 | Loss: 0.00139903
Iteration 21/25 | Loss: 0.00139903
Iteration 22/25 | Loss: 0.00139902
Iteration 23/25 | Loss: 0.00139902
Iteration 24/25 | Loss: 0.00139902
Iteration 25/25 | Loss: 0.00139902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69464529
Iteration 2/25 | Loss: 0.00500527
Iteration 3/25 | Loss: 0.00491839
Iteration 4/25 | Loss: 0.00491839
Iteration 5/25 | Loss: 0.00491839
Iteration 6/25 | Loss: 0.00491839
Iteration 7/25 | Loss: 0.00491839
Iteration 8/25 | Loss: 0.00491839
Iteration 9/25 | Loss: 0.00491839
Iteration 10/25 | Loss: 0.00491839
Iteration 11/25 | Loss: 0.00491839
Iteration 12/25 | Loss: 0.00491839
Iteration 13/25 | Loss: 0.00491839
Iteration 14/25 | Loss: 0.00491839
Iteration 15/25 | Loss: 0.00491839
Iteration 16/25 | Loss: 0.00491839
Iteration 17/25 | Loss: 0.00491839
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004918386228382587, 0.004918386228382587, 0.004918386228382587, 0.004918386228382587, 0.004918386228382587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004918386228382587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00491839
Iteration 2/1000 | Loss: 0.00011220
Iteration 3/1000 | Loss: 0.00008347
Iteration 4/1000 | Loss: 0.00006713
Iteration 5/1000 | Loss: 0.00012862
Iteration 6/1000 | Loss: 0.00005509
Iteration 7/1000 | Loss: 0.00005120
Iteration 8/1000 | Loss: 0.00307981
Iteration 9/1000 | Loss: 0.00083271
Iteration 10/1000 | Loss: 0.00007263
Iteration 11/1000 | Loss: 0.00015173
Iteration 12/1000 | Loss: 0.00004819
Iteration 13/1000 | Loss: 0.00009251
Iteration 14/1000 | Loss: 0.00003982
Iteration 15/1000 | Loss: 0.00003653
Iteration 16/1000 | Loss: 0.00003500
Iteration 17/1000 | Loss: 0.00003412
Iteration 18/1000 | Loss: 0.00003344
Iteration 19/1000 | Loss: 0.00003310
Iteration 20/1000 | Loss: 0.00008876
Iteration 21/1000 | Loss: 0.00003277
Iteration 22/1000 | Loss: 0.00003246
Iteration 23/1000 | Loss: 0.00003241
Iteration 24/1000 | Loss: 0.00003238
Iteration 25/1000 | Loss: 0.00003228
Iteration 26/1000 | Loss: 0.00003221
Iteration 27/1000 | Loss: 0.00003209
Iteration 28/1000 | Loss: 0.00003206
Iteration 29/1000 | Loss: 0.00003204
Iteration 30/1000 | Loss: 0.00003203
Iteration 31/1000 | Loss: 0.00003203
Iteration 32/1000 | Loss: 0.00003199
Iteration 33/1000 | Loss: 0.00003199
Iteration 34/1000 | Loss: 0.00003199
Iteration 35/1000 | Loss: 0.00003199
Iteration 36/1000 | Loss: 0.00003199
Iteration 37/1000 | Loss: 0.00003199
Iteration 38/1000 | Loss: 0.00003199
Iteration 39/1000 | Loss: 0.00003199
Iteration 40/1000 | Loss: 0.00003199
Iteration 41/1000 | Loss: 0.00003199
Iteration 42/1000 | Loss: 0.00003199
Iteration 43/1000 | Loss: 0.00003198
Iteration 44/1000 | Loss: 0.00003198
Iteration 45/1000 | Loss: 0.00003198
Iteration 46/1000 | Loss: 0.00003197
Iteration 47/1000 | Loss: 0.00003197
Iteration 48/1000 | Loss: 0.00003196
Iteration 49/1000 | Loss: 0.00003196
Iteration 50/1000 | Loss: 0.00003196
Iteration 51/1000 | Loss: 0.00003196
Iteration 52/1000 | Loss: 0.00003196
Iteration 53/1000 | Loss: 0.00003196
Iteration 54/1000 | Loss: 0.00003195
Iteration 55/1000 | Loss: 0.00003195
Iteration 56/1000 | Loss: 0.00003195
Iteration 57/1000 | Loss: 0.00003195
Iteration 58/1000 | Loss: 0.00003195
Iteration 59/1000 | Loss: 0.00003195
Iteration 60/1000 | Loss: 0.00003195
Iteration 61/1000 | Loss: 0.00003195
Iteration 62/1000 | Loss: 0.00003195
Iteration 63/1000 | Loss: 0.00003194
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003193
Iteration 66/1000 | Loss: 0.00003193
Iteration 67/1000 | Loss: 0.00003193
Iteration 68/1000 | Loss: 0.00003193
Iteration 69/1000 | Loss: 0.00003193
Iteration 70/1000 | Loss: 0.00003193
Iteration 71/1000 | Loss: 0.00003193
Iteration 72/1000 | Loss: 0.00003193
Iteration 73/1000 | Loss: 0.00003192
Iteration 74/1000 | Loss: 0.00003192
Iteration 75/1000 | Loss: 0.00003192
Iteration 76/1000 | Loss: 0.00003192
Iteration 77/1000 | Loss: 0.00003191
Iteration 78/1000 | Loss: 0.00003191
Iteration 79/1000 | Loss: 0.00003191
Iteration 80/1000 | Loss: 0.00003190
Iteration 81/1000 | Loss: 0.00003190
Iteration 82/1000 | Loss: 0.00003190
Iteration 83/1000 | Loss: 0.00003190
Iteration 84/1000 | Loss: 0.00003190
Iteration 85/1000 | Loss: 0.00003190
Iteration 86/1000 | Loss: 0.00003190
Iteration 87/1000 | Loss: 0.00003190
Iteration 88/1000 | Loss: 0.00003190
Iteration 89/1000 | Loss: 0.00003190
Iteration 90/1000 | Loss: 0.00003189
Iteration 91/1000 | Loss: 0.00003189
Iteration 92/1000 | Loss: 0.00003189
Iteration 93/1000 | Loss: 0.00003189
Iteration 94/1000 | Loss: 0.00003189
Iteration 95/1000 | Loss: 0.00003189
Iteration 96/1000 | Loss: 0.00003189
Iteration 97/1000 | Loss: 0.00003189
Iteration 98/1000 | Loss: 0.00003189
Iteration 99/1000 | Loss: 0.00003189
Iteration 100/1000 | Loss: 0.00003189
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003189
Iteration 104/1000 | Loss: 0.00003189
Iteration 105/1000 | Loss: 0.00003189
Iteration 106/1000 | Loss: 0.00003189
Iteration 107/1000 | Loss: 0.00003189
Iteration 108/1000 | Loss: 0.00003189
Iteration 109/1000 | Loss: 0.00003189
Iteration 110/1000 | Loss: 0.00003189
Iteration 111/1000 | Loss: 0.00003189
Iteration 112/1000 | Loss: 0.00003189
Iteration 113/1000 | Loss: 0.00003188
Iteration 114/1000 | Loss: 0.00003188
Iteration 115/1000 | Loss: 0.00003188
Iteration 116/1000 | Loss: 0.00003188
Iteration 117/1000 | Loss: 0.00003188
Iteration 118/1000 | Loss: 0.00003188
Iteration 119/1000 | Loss: 0.00003188
Iteration 120/1000 | Loss: 0.00003188
Iteration 121/1000 | Loss: 0.00003187
Iteration 122/1000 | Loss: 0.00003187
Iteration 123/1000 | Loss: 0.00003187
Iteration 124/1000 | Loss: 0.00003187
Iteration 125/1000 | Loss: 0.00003187
Iteration 126/1000 | Loss: 0.00003186
Iteration 127/1000 | Loss: 0.00003186
Iteration 128/1000 | Loss: 0.00003186
Iteration 129/1000 | Loss: 0.00003186
Iteration 130/1000 | Loss: 0.00003186
Iteration 131/1000 | Loss: 0.00003186
Iteration 132/1000 | Loss: 0.00003186
Iteration 133/1000 | Loss: 0.00003186
Iteration 134/1000 | Loss: 0.00003186
Iteration 135/1000 | Loss: 0.00003186
Iteration 136/1000 | Loss: 0.00003186
Iteration 137/1000 | Loss: 0.00003186
Iteration 138/1000 | Loss: 0.00003186
Iteration 139/1000 | Loss: 0.00003186
Iteration 140/1000 | Loss: 0.00003186
Iteration 141/1000 | Loss: 0.00003185
Iteration 142/1000 | Loss: 0.00003185
Iteration 143/1000 | Loss: 0.00003185
Iteration 144/1000 | Loss: 0.00003185
Iteration 145/1000 | Loss: 0.00003185
Iteration 146/1000 | Loss: 0.00003185
Iteration 147/1000 | Loss: 0.00003185
Iteration 148/1000 | Loss: 0.00003185
Iteration 149/1000 | Loss: 0.00003185
Iteration 150/1000 | Loss: 0.00003185
Iteration 151/1000 | Loss: 0.00003185
Iteration 152/1000 | Loss: 0.00003185
Iteration 153/1000 | Loss: 0.00003185
Iteration 154/1000 | Loss: 0.00003185
Iteration 155/1000 | Loss: 0.00003185
Iteration 156/1000 | Loss: 0.00003185
Iteration 157/1000 | Loss: 0.00003185
Iteration 158/1000 | Loss: 0.00003185
Iteration 159/1000 | Loss: 0.00003185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [3.184927481925115e-05, 3.184927481925115e-05, 3.184927481925115e-05, 3.184927481925115e-05, 3.184927481925115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.184927481925115e-05

Optimization complete. Final v2v error: 4.010729789733887 mm

Highest mean error: 14.164067268371582 mm for frame 63

Lowest mean error: 3.7366433143615723 mm for frame 47

Saving results

Total time: 70.40875363349915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885726
Iteration 2/25 | Loss: 0.00165029
Iteration 3/25 | Loss: 0.00137949
Iteration 4/25 | Loss: 0.00135285
Iteration 5/25 | Loss: 0.00134691
Iteration 6/25 | Loss: 0.00134691
Iteration 7/25 | Loss: 0.00134691
Iteration 8/25 | Loss: 0.00134691
Iteration 9/25 | Loss: 0.00134691
Iteration 10/25 | Loss: 0.00134691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013469090918079019, 0.0013469090918079019, 0.0013469090918079019, 0.0013469090918079019, 0.0013469090918079019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013469090918079019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79206049
Iteration 2/25 | Loss: 0.00584061
Iteration 3/25 | Loss: 0.00584059
Iteration 4/25 | Loss: 0.00584059
Iteration 5/25 | Loss: 0.00584059
Iteration 6/25 | Loss: 0.00584059
Iteration 7/25 | Loss: 0.00584059
Iteration 8/25 | Loss: 0.00584059
Iteration 9/25 | Loss: 0.00584059
Iteration 10/25 | Loss: 0.00584059
Iteration 11/25 | Loss: 0.00584059
Iteration 12/25 | Loss: 0.00584059
Iteration 13/25 | Loss: 0.00584059
Iteration 14/25 | Loss: 0.00584059
Iteration 15/25 | Loss: 0.00584059
Iteration 16/25 | Loss: 0.00584059
Iteration 17/25 | Loss: 0.00584059
Iteration 18/25 | Loss: 0.00584059
Iteration 19/25 | Loss: 0.00584059
Iteration 20/25 | Loss: 0.00584059
Iteration 21/25 | Loss: 0.00584059
Iteration 22/25 | Loss: 0.00584059
Iteration 23/25 | Loss: 0.00584059
Iteration 24/25 | Loss: 0.00584059
Iteration 25/25 | Loss: 0.00584059
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0058405897580087185, 0.0058405897580087185, 0.0058405897580087185, 0.0058405897580087185, 0.0058405897580087185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0058405897580087185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00584059
Iteration 2/1000 | Loss: 0.00005094
Iteration 3/1000 | Loss: 0.00003849
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003227
Iteration 6/1000 | Loss: 0.00003128
Iteration 7/1000 | Loss: 0.00003011
Iteration 8/1000 | Loss: 0.00002940
Iteration 9/1000 | Loss: 0.00002904
Iteration 10/1000 | Loss: 0.00002867
Iteration 11/1000 | Loss: 0.00002836
Iteration 12/1000 | Loss: 0.00002817
Iteration 13/1000 | Loss: 0.00002801
Iteration 14/1000 | Loss: 0.00002799
Iteration 15/1000 | Loss: 0.00002799
Iteration 16/1000 | Loss: 0.00002799
Iteration 17/1000 | Loss: 0.00002798
Iteration 18/1000 | Loss: 0.00002798
Iteration 19/1000 | Loss: 0.00002798
Iteration 20/1000 | Loss: 0.00002798
Iteration 21/1000 | Loss: 0.00002798
Iteration 22/1000 | Loss: 0.00002798
Iteration 23/1000 | Loss: 0.00002797
Iteration 24/1000 | Loss: 0.00002797
Iteration 25/1000 | Loss: 0.00002797
Iteration 26/1000 | Loss: 0.00002797
Iteration 27/1000 | Loss: 0.00002796
Iteration 28/1000 | Loss: 0.00002796
Iteration 29/1000 | Loss: 0.00002796
Iteration 30/1000 | Loss: 0.00002795
Iteration 31/1000 | Loss: 0.00002795
Iteration 32/1000 | Loss: 0.00002795
Iteration 33/1000 | Loss: 0.00002794
Iteration 34/1000 | Loss: 0.00002794
Iteration 35/1000 | Loss: 0.00002794
Iteration 36/1000 | Loss: 0.00002794
Iteration 37/1000 | Loss: 0.00002794
Iteration 38/1000 | Loss: 0.00002793
Iteration 39/1000 | Loss: 0.00002793
Iteration 40/1000 | Loss: 0.00002793
Iteration 41/1000 | Loss: 0.00002792
Iteration 42/1000 | Loss: 0.00002792
Iteration 43/1000 | Loss: 0.00002792
Iteration 44/1000 | Loss: 0.00002792
Iteration 45/1000 | Loss: 0.00002792
Iteration 46/1000 | Loss: 0.00002791
Iteration 47/1000 | Loss: 0.00002791
Iteration 48/1000 | Loss: 0.00002791
Iteration 49/1000 | Loss: 0.00002791
Iteration 50/1000 | Loss: 0.00002790
Iteration 51/1000 | Loss: 0.00002790
Iteration 52/1000 | Loss: 0.00002790
Iteration 53/1000 | Loss: 0.00002789
Iteration 54/1000 | Loss: 0.00002789
Iteration 55/1000 | Loss: 0.00002789
Iteration 56/1000 | Loss: 0.00002788
Iteration 57/1000 | Loss: 0.00002787
Iteration 58/1000 | Loss: 0.00002787
Iteration 59/1000 | Loss: 0.00002787
Iteration 60/1000 | Loss: 0.00002787
Iteration 61/1000 | Loss: 0.00002786
Iteration 62/1000 | Loss: 0.00002786
Iteration 63/1000 | Loss: 0.00002786
Iteration 64/1000 | Loss: 0.00002786
Iteration 65/1000 | Loss: 0.00002786
Iteration 66/1000 | Loss: 0.00002786
Iteration 67/1000 | Loss: 0.00002785
Iteration 68/1000 | Loss: 0.00002785
Iteration 69/1000 | Loss: 0.00002785
Iteration 70/1000 | Loss: 0.00002785
Iteration 71/1000 | Loss: 0.00002785
Iteration 72/1000 | Loss: 0.00002785
Iteration 73/1000 | Loss: 0.00002785
Iteration 74/1000 | Loss: 0.00002784
Iteration 75/1000 | Loss: 0.00002784
Iteration 76/1000 | Loss: 0.00002784
Iteration 77/1000 | Loss: 0.00002784
Iteration 78/1000 | Loss: 0.00002783
Iteration 79/1000 | Loss: 0.00002783
Iteration 80/1000 | Loss: 0.00002783
Iteration 81/1000 | Loss: 0.00002783
Iteration 82/1000 | Loss: 0.00002783
Iteration 83/1000 | Loss: 0.00002783
Iteration 84/1000 | Loss: 0.00002783
Iteration 85/1000 | Loss: 0.00002783
Iteration 86/1000 | Loss: 0.00002783
Iteration 87/1000 | Loss: 0.00002783
Iteration 88/1000 | Loss: 0.00002783
Iteration 89/1000 | Loss: 0.00002782
Iteration 90/1000 | Loss: 0.00002782
Iteration 91/1000 | Loss: 0.00002782
Iteration 92/1000 | Loss: 0.00002782
Iteration 93/1000 | Loss: 0.00002782
Iteration 94/1000 | Loss: 0.00002782
Iteration 95/1000 | Loss: 0.00002782
Iteration 96/1000 | Loss: 0.00002782
Iteration 97/1000 | Loss: 0.00002782
Iteration 98/1000 | Loss: 0.00002782
Iteration 99/1000 | Loss: 0.00002782
Iteration 100/1000 | Loss: 0.00002782
Iteration 101/1000 | Loss: 0.00002781
Iteration 102/1000 | Loss: 0.00002781
Iteration 103/1000 | Loss: 0.00002781
Iteration 104/1000 | Loss: 0.00002781
Iteration 105/1000 | Loss: 0.00002781
Iteration 106/1000 | Loss: 0.00002781
Iteration 107/1000 | Loss: 0.00002781
Iteration 108/1000 | Loss: 0.00002781
Iteration 109/1000 | Loss: 0.00002781
Iteration 110/1000 | Loss: 0.00002781
Iteration 111/1000 | Loss: 0.00002781
Iteration 112/1000 | Loss: 0.00002781
Iteration 113/1000 | Loss: 0.00002781
Iteration 114/1000 | Loss: 0.00002781
Iteration 115/1000 | Loss: 0.00002781
Iteration 116/1000 | Loss: 0.00002781
Iteration 117/1000 | Loss: 0.00002781
Iteration 118/1000 | Loss: 0.00002780
Iteration 119/1000 | Loss: 0.00002780
Iteration 120/1000 | Loss: 0.00002779
Iteration 121/1000 | Loss: 0.00002779
Iteration 122/1000 | Loss: 0.00002779
Iteration 123/1000 | Loss: 0.00002779
Iteration 124/1000 | Loss: 0.00002779
Iteration 125/1000 | Loss: 0.00002779
Iteration 126/1000 | Loss: 0.00002779
Iteration 127/1000 | Loss: 0.00002779
Iteration 128/1000 | Loss: 0.00002779
Iteration 129/1000 | Loss: 0.00002779
Iteration 130/1000 | Loss: 0.00002779
Iteration 131/1000 | Loss: 0.00002779
Iteration 132/1000 | Loss: 0.00002779
Iteration 133/1000 | Loss: 0.00002779
Iteration 134/1000 | Loss: 0.00002779
Iteration 135/1000 | Loss: 0.00002779
Iteration 136/1000 | Loss: 0.00002779
Iteration 137/1000 | Loss: 0.00002779
Iteration 138/1000 | Loss: 0.00002779
Iteration 139/1000 | Loss: 0.00002779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.779392889351584e-05, 2.779392889351584e-05, 2.779392889351584e-05, 2.779392889351584e-05, 2.779392889351584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.779392889351584e-05

Optimization complete. Final v2v error: 4.586785316467285 mm

Highest mean error: 4.823784828186035 mm for frame 237

Lowest mean error: 4.176453113555908 mm for frame 0

Saving results

Total time: 38.33439087867737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094547
Iteration 2/25 | Loss: 0.00441049
Iteration 3/25 | Loss: 0.00304978
Iteration 4/25 | Loss: 0.00274636
Iteration 5/25 | Loss: 0.00254081
Iteration 6/25 | Loss: 0.00232410
Iteration 7/25 | Loss: 0.00230598
Iteration 8/25 | Loss: 0.00223999
Iteration 9/25 | Loss: 0.00210992
Iteration 10/25 | Loss: 0.00210646
Iteration 11/25 | Loss: 0.00205921
Iteration 12/25 | Loss: 0.00206204
Iteration 13/25 | Loss: 0.00206519
Iteration 14/25 | Loss: 0.00207842
Iteration 15/25 | Loss: 0.00205522
Iteration 16/25 | Loss: 0.00204168
Iteration 17/25 | Loss: 0.00204949
Iteration 18/25 | Loss: 0.00203617
Iteration 19/25 | Loss: 0.00202375
Iteration 20/25 | Loss: 0.00202514
Iteration 21/25 | Loss: 0.00202301
Iteration 22/25 | Loss: 0.00202405
Iteration 23/25 | Loss: 0.00202439
Iteration 24/25 | Loss: 0.00202126
Iteration 25/25 | Loss: 0.00201623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76101208
Iteration 2/25 | Loss: 0.01048668
Iteration 3/25 | Loss: 0.00983187
Iteration 4/25 | Loss: 0.00983187
Iteration 5/25 | Loss: 0.00983186
Iteration 6/25 | Loss: 0.00983186
Iteration 7/25 | Loss: 0.00983186
Iteration 8/25 | Loss: 0.00983186
Iteration 9/25 | Loss: 0.00983186
Iteration 10/25 | Loss: 0.00983186
Iteration 11/25 | Loss: 0.00983186
Iteration 12/25 | Loss: 0.00983186
Iteration 13/25 | Loss: 0.00983186
Iteration 14/25 | Loss: 0.00983186
Iteration 15/25 | Loss: 0.00983186
Iteration 16/25 | Loss: 0.00983186
Iteration 17/25 | Loss: 0.00983186
Iteration 18/25 | Loss: 0.00983186
Iteration 19/25 | Loss: 0.00983186
Iteration 20/25 | Loss: 0.00983186
Iteration 21/25 | Loss: 0.00983186
Iteration 22/25 | Loss: 0.00983186
Iteration 23/25 | Loss: 0.00983186
Iteration 24/25 | Loss: 0.00983186
Iteration 25/25 | Loss: 0.00983186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00983186
Iteration 2/1000 | Loss: 0.00404749
Iteration 3/1000 | Loss: 0.00251597
Iteration 4/1000 | Loss: 0.00106345
Iteration 5/1000 | Loss: 0.00281839
Iteration 6/1000 | Loss: 0.00052095
Iteration 7/1000 | Loss: 0.00046050
Iteration 8/1000 | Loss: 0.00080296
Iteration 9/1000 | Loss: 0.01589966
Iteration 10/1000 | Loss: 0.01886861
Iteration 11/1000 | Loss: 0.00090633
Iteration 12/1000 | Loss: 0.00072261
Iteration 13/1000 | Loss: 0.00145785
Iteration 14/1000 | Loss: 0.00056656
Iteration 15/1000 | Loss: 0.00073777
Iteration 16/1000 | Loss: 0.00348191
Iteration 17/1000 | Loss: 0.00814566
Iteration 18/1000 | Loss: 0.00577128
Iteration 19/1000 | Loss: 0.00280545
Iteration 20/1000 | Loss: 0.00049902
Iteration 21/1000 | Loss: 0.00260495
Iteration 22/1000 | Loss: 0.00142440
Iteration 23/1000 | Loss: 0.00283035
Iteration 24/1000 | Loss: 0.00251153
Iteration 25/1000 | Loss: 0.00082223
Iteration 26/1000 | Loss: 0.00065703
Iteration 27/1000 | Loss: 0.00055542
Iteration 28/1000 | Loss: 0.00028463
Iteration 29/1000 | Loss: 0.00056508
Iteration 30/1000 | Loss: 0.00035389
Iteration 31/1000 | Loss: 0.00098189
Iteration 32/1000 | Loss: 0.00076151
Iteration 33/1000 | Loss: 0.00331617
Iteration 34/1000 | Loss: 0.00024008
Iteration 35/1000 | Loss: 0.00014683
Iteration 36/1000 | Loss: 0.00228452
Iteration 37/1000 | Loss: 0.00009177
Iteration 38/1000 | Loss: 0.00012401
Iteration 39/1000 | Loss: 0.00023279
Iteration 40/1000 | Loss: 0.00003184
Iteration 41/1000 | Loss: 0.00002976
Iteration 42/1000 | Loss: 0.00002787
Iteration 43/1000 | Loss: 0.00002651
Iteration 44/1000 | Loss: 0.00002863
Iteration 45/1000 | Loss: 0.00007784
Iteration 46/1000 | Loss: 0.00014291
Iteration 47/1000 | Loss: 0.00059660
Iteration 48/1000 | Loss: 0.00015806
Iteration 49/1000 | Loss: 0.00004453
Iteration 50/1000 | Loss: 0.00002289
Iteration 51/1000 | Loss: 0.00005484
Iteration 52/1000 | Loss: 0.00015315
Iteration 53/1000 | Loss: 0.00007351
Iteration 54/1000 | Loss: 0.00003484
Iteration 55/1000 | Loss: 0.00002247
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002245
Iteration 58/1000 | Loss: 0.00003629
Iteration 59/1000 | Loss: 0.00002248
Iteration 60/1000 | Loss: 0.00002243
Iteration 61/1000 | Loss: 0.00002243
Iteration 62/1000 | Loss: 0.00002243
Iteration 63/1000 | Loss: 0.00002243
Iteration 64/1000 | Loss: 0.00002243
Iteration 65/1000 | Loss: 0.00002243
Iteration 66/1000 | Loss: 0.00002243
Iteration 67/1000 | Loss: 0.00002243
Iteration 68/1000 | Loss: 0.00002242
Iteration 69/1000 | Loss: 0.00002242
Iteration 70/1000 | Loss: 0.00002242
Iteration 71/1000 | Loss: 0.00002242
Iteration 72/1000 | Loss: 0.00002241
Iteration 73/1000 | Loss: 0.00002241
Iteration 74/1000 | Loss: 0.00002241
Iteration 75/1000 | Loss: 0.00002240
Iteration 76/1000 | Loss: 0.00002240
Iteration 77/1000 | Loss: 0.00002239
Iteration 78/1000 | Loss: 0.00002239
Iteration 79/1000 | Loss: 0.00004098
Iteration 80/1000 | Loss: 0.00002243
Iteration 81/1000 | Loss: 0.00002242
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002236
Iteration 91/1000 | Loss: 0.00002236
Iteration 92/1000 | Loss: 0.00002236
Iteration 93/1000 | Loss: 0.00002235
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002233
Iteration 102/1000 | Loss: 0.00002233
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002231
Iteration 107/1000 | Loss: 0.00002231
Iteration 108/1000 | Loss: 0.00002231
Iteration 109/1000 | Loss: 0.00002231
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002229
Iteration 114/1000 | Loss: 0.00002229
Iteration 115/1000 | Loss: 0.00002228
Iteration 116/1000 | Loss: 0.00002227
Iteration 117/1000 | Loss: 0.00002226
Iteration 118/1000 | Loss: 0.00002226
Iteration 119/1000 | Loss: 0.00002226
Iteration 120/1000 | Loss: 0.00002226
Iteration 121/1000 | Loss: 0.00002226
Iteration 122/1000 | Loss: 0.00002226
Iteration 123/1000 | Loss: 0.00002226
Iteration 124/1000 | Loss: 0.00002226
Iteration 125/1000 | Loss: 0.00002226
Iteration 126/1000 | Loss: 0.00002226
Iteration 127/1000 | Loss: 0.00002226
Iteration 128/1000 | Loss: 0.00002226
Iteration 129/1000 | Loss: 0.00002225
Iteration 130/1000 | Loss: 0.00002225
Iteration 131/1000 | Loss: 0.00002224
Iteration 132/1000 | Loss: 0.00002224
Iteration 133/1000 | Loss: 0.00002224
Iteration 134/1000 | Loss: 0.00002224
Iteration 135/1000 | Loss: 0.00002224
Iteration 136/1000 | Loss: 0.00002224
Iteration 137/1000 | Loss: 0.00002224
Iteration 138/1000 | Loss: 0.00002224
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00002223
Iteration 141/1000 | Loss: 0.00008315
Iteration 142/1000 | Loss: 0.00008315
Iteration 143/1000 | Loss: 0.00236915
Iteration 144/1000 | Loss: 0.00016857
Iteration 145/1000 | Loss: 0.00021477
Iteration 146/1000 | Loss: 0.00003295
Iteration 147/1000 | Loss: 0.00005203
Iteration 148/1000 | Loss: 0.00002252
Iteration 149/1000 | Loss: 0.00011415
Iteration 150/1000 | Loss: 0.00009334
Iteration 151/1000 | Loss: 0.00002374
Iteration 152/1000 | Loss: 0.00002870
Iteration 153/1000 | Loss: 0.00014478
Iteration 154/1000 | Loss: 0.00041993
Iteration 155/1000 | Loss: 0.00030611
Iteration 156/1000 | Loss: 0.00011167
Iteration 157/1000 | Loss: 0.00003362
Iteration 158/1000 | Loss: 0.00002237
Iteration 159/1000 | Loss: 0.00004051
Iteration 160/1000 | Loss: 0.00002239
Iteration 161/1000 | Loss: 0.00002221
Iteration 162/1000 | Loss: 0.00002220
Iteration 163/1000 | Loss: 0.00002220
Iteration 164/1000 | Loss: 0.00002220
Iteration 165/1000 | Loss: 0.00002220
Iteration 166/1000 | Loss: 0.00002220
Iteration 167/1000 | Loss: 0.00002220
Iteration 168/1000 | Loss: 0.00002220
Iteration 169/1000 | Loss: 0.00002220
Iteration 170/1000 | Loss: 0.00002220
Iteration 171/1000 | Loss: 0.00002219
Iteration 172/1000 | Loss: 0.00002219
Iteration 173/1000 | Loss: 0.00002218
Iteration 174/1000 | Loss: 0.00002217
Iteration 175/1000 | Loss: 0.00002216
Iteration 176/1000 | Loss: 0.00002216
Iteration 177/1000 | Loss: 0.00002216
Iteration 178/1000 | Loss: 0.00002216
Iteration 179/1000 | Loss: 0.00002216
Iteration 180/1000 | Loss: 0.00002216
Iteration 181/1000 | Loss: 0.00002216
Iteration 182/1000 | Loss: 0.00002216
Iteration 183/1000 | Loss: 0.00002215
Iteration 184/1000 | Loss: 0.00002215
Iteration 185/1000 | Loss: 0.00002215
Iteration 186/1000 | Loss: 0.00002215
Iteration 187/1000 | Loss: 0.00002215
Iteration 188/1000 | Loss: 0.00002215
Iteration 189/1000 | Loss: 0.00002215
Iteration 190/1000 | Loss: 0.00002215
Iteration 191/1000 | Loss: 0.00002215
Iteration 192/1000 | Loss: 0.00002215
Iteration 193/1000 | Loss: 0.00002215
Iteration 194/1000 | Loss: 0.00002215
Iteration 195/1000 | Loss: 0.00002215
Iteration 196/1000 | Loss: 0.00002215
Iteration 197/1000 | Loss: 0.00002215
Iteration 198/1000 | Loss: 0.00002215
Iteration 199/1000 | Loss: 0.00002215
Iteration 200/1000 | Loss: 0.00002215
Iteration 201/1000 | Loss: 0.00002215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.2154179532662965e-05, 2.2154179532662965e-05, 2.2154179532662965e-05, 2.2154179532662965e-05, 2.2154179532662965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2154179532662965e-05

Optimization complete. Final v2v error: 4.062841892242432 mm

Highest mean error: 4.27656364440918 mm for frame 110

Lowest mean error: 3.8010945320129395 mm for frame 45

Saving results

Total time: 156.48341870307922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01168042
Iteration 2/25 | Loss: 0.00279542
Iteration 3/25 | Loss: 0.00249743
Iteration 4/25 | Loss: 0.00232290
Iteration 5/25 | Loss: 0.00224586
Iteration 6/25 | Loss: 0.00216688
Iteration 7/25 | Loss: 0.00193838
Iteration 8/25 | Loss: 0.00188843
Iteration 9/25 | Loss: 0.00186781
Iteration 10/25 | Loss: 0.00184987
Iteration 11/25 | Loss: 0.00186685
Iteration 12/25 | Loss: 0.00181008
Iteration 13/25 | Loss: 0.00175832
Iteration 14/25 | Loss: 0.00175105
Iteration 15/25 | Loss: 0.00173400
Iteration 16/25 | Loss: 0.00172881
Iteration 17/25 | Loss: 0.00171807
Iteration 18/25 | Loss: 0.00171542
Iteration 19/25 | Loss: 0.00171494
Iteration 20/25 | Loss: 0.00171470
Iteration 21/25 | Loss: 0.00171371
Iteration 22/25 | Loss: 0.00171154
Iteration 23/25 | Loss: 0.00171561
Iteration 24/25 | Loss: 0.00171510
Iteration 25/25 | Loss: 0.00171052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83678532
Iteration 2/25 | Loss: 0.00714472
Iteration 3/25 | Loss: 0.00714472
Iteration 4/25 | Loss: 0.00714472
Iteration 5/25 | Loss: 0.00714472
Iteration 6/25 | Loss: 0.00714472
Iteration 7/25 | Loss: 0.00714472
Iteration 8/25 | Loss: 0.00714472
Iteration 9/25 | Loss: 0.00714472
Iteration 10/25 | Loss: 0.00714472
Iteration 11/25 | Loss: 0.00714472
Iteration 12/25 | Loss: 0.00714472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.007144718896597624, 0.007144718896597624, 0.007144718896597624, 0.007144718896597624, 0.007144718896597624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007144718896597624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00714472
Iteration 2/1000 | Loss: 0.00106437
Iteration 3/1000 | Loss: 0.00081355
Iteration 4/1000 | Loss: 0.00145052
Iteration 5/1000 | Loss: 0.00261828
Iteration 6/1000 | Loss: 0.00047998
Iteration 7/1000 | Loss: 0.00053704
Iteration 8/1000 | Loss: 0.00109749
Iteration 9/1000 | Loss: 0.00270676
Iteration 10/1000 | Loss: 0.00340589
Iteration 11/1000 | Loss: 0.00114790
Iteration 12/1000 | Loss: 0.00084397
Iteration 13/1000 | Loss: 0.00139063
Iteration 14/1000 | Loss: 0.00034908
Iteration 15/1000 | Loss: 0.00192853
Iteration 16/1000 | Loss: 0.00159765
Iteration 17/1000 | Loss: 0.00086853
Iteration 18/1000 | Loss: 0.00062794
Iteration 19/1000 | Loss: 0.00028769
Iteration 20/1000 | Loss: 0.00060243
Iteration 21/1000 | Loss: 0.00042872
Iteration 22/1000 | Loss: 0.00039447
Iteration 23/1000 | Loss: 0.00056548
Iteration 24/1000 | Loss: 0.00039259
Iteration 25/1000 | Loss: 0.00022353
Iteration 26/1000 | Loss: 0.00161314
Iteration 27/1000 | Loss: 0.00079527
Iteration 28/1000 | Loss: 0.00054776
Iteration 29/1000 | Loss: 0.00038996
Iteration 30/1000 | Loss: 0.00023209
Iteration 31/1000 | Loss: 0.00024523
Iteration 32/1000 | Loss: 0.00040802
Iteration 33/1000 | Loss: 0.00215952
Iteration 34/1000 | Loss: 0.00560077
Iteration 35/1000 | Loss: 0.00674415
Iteration 36/1000 | Loss: 0.00491412
Iteration 37/1000 | Loss: 0.00331171
Iteration 38/1000 | Loss: 0.00524222
Iteration 39/1000 | Loss: 0.00274779
Iteration 40/1000 | Loss: 0.00362488
Iteration 41/1000 | Loss: 0.00368990
Iteration 42/1000 | Loss: 0.00248685
Iteration 43/1000 | Loss: 0.00159352
Iteration 44/1000 | Loss: 0.00189822
Iteration 45/1000 | Loss: 0.00115337
Iteration 46/1000 | Loss: 0.00286256
Iteration 47/1000 | Loss: 0.00164748
Iteration 48/1000 | Loss: 0.00135459
Iteration 49/1000 | Loss: 0.00106514
Iteration 50/1000 | Loss: 0.00104511
Iteration 51/1000 | Loss: 0.00129174
Iteration 52/1000 | Loss: 0.00104446
Iteration 53/1000 | Loss: 0.00107088
Iteration 54/1000 | Loss: 0.00187541
Iteration 55/1000 | Loss: 0.00354909
Iteration 56/1000 | Loss: 0.00156057
Iteration 57/1000 | Loss: 0.00154294
Iteration 58/1000 | Loss: 0.00184345
Iteration 59/1000 | Loss: 0.00228438
Iteration 60/1000 | Loss: 0.00201346
Iteration 61/1000 | Loss: 0.00123090
Iteration 62/1000 | Loss: 0.00218512
Iteration 63/1000 | Loss: 0.00273263
Iteration 64/1000 | Loss: 0.00190986
Iteration 65/1000 | Loss: 0.00244163
Iteration 66/1000 | Loss: 0.00194920
Iteration 67/1000 | Loss: 0.00121579
Iteration 68/1000 | Loss: 0.00105701
Iteration 69/1000 | Loss: 0.00099573
Iteration 70/1000 | Loss: 0.00117856
Iteration 71/1000 | Loss: 0.00155169
Iteration 72/1000 | Loss: 0.00157786
Iteration 73/1000 | Loss: 0.00250260
Iteration 74/1000 | Loss: 0.00161610
Iteration 75/1000 | Loss: 0.00449432
Iteration 76/1000 | Loss: 0.00175394
Iteration 77/1000 | Loss: 0.00123269
Iteration 78/1000 | Loss: 0.00099729
Iteration 79/1000 | Loss: 0.00107796
Iteration 80/1000 | Loss: 0.00090137
Iteration 81/1000 | Loss: 0.00120062
Iteration 82/1000 | Loss: 0.00212304
Iteration 83/1000 | Loss: 0.00110449
Iteration 84/1000 | Loss: 0.00139052
Iteration 85/1000 | Loss: 0.00120454
Iteration 86/1000 | Loss: 0.00129925
Iteration 87/1000 | Loss: 0.00078495
Iteration 88/1000 | Loss: 0.00076043
Iteration 89/1000 | Loss: 0.00087075
Iteration 90/1000 | Loss: 0.00063031
Iteration 91/1000 | Loss: 0.00068517
Iteration 92/1000 | Loss: 0.00052436
Iteration 93/1000 | Loss: 0.00107937
Iteration 94/1000 | Loss: 0.00090807
Iteration 95/1000 | Loss: 0.00051079
Iteration 96/1000 | Loss: 0.00093666
Iteration 97/1000 | Loss: 0.00166774
Iteration 98/1000 | Loss: 0.00038554
Iteration 99/1000 | Loss: 0.00063534
Iteration 100/1000 | Loss: 0.00037731
Iteration 101/1000 | Loss: 0.00040680
Iteration 102/1000 | Loss: 0.00024192
Iteration 103/1000 | Loss: 0.00039434
Iteration 104/1000 | Loss: 0.00054460
Iteration 105/1000 | Loss: 0.00046791
Iteration 106/1000 | Loss: 0.00021574
Iteration 107/1000 | Loss: 0.00031624
Iteration 108/1000 | Loss: 0.00102909
Iteration 109/1000 | Loss: 0.00033691
Iteration 110/1000 | Loss: 0.00040379
Iteration 111/1000 | Loss: 0.00055132
Iteration 112/1000 | Loss: 0.00036668
Iteration 113/1000 | Loss: 0.00039561
Iteration 114/1000 | Loss: 0.00038062
Iteration 115/1000 | Loss: 0.00024395
Iteration 116/1000 | Loss: 0.00064733
Iteration 117/1000 | Loss: 0.00010008
Iteration 118/1000 | Loss: 0.00050806
Iteration 119/1000 | Loss: 0.00029810
Iteration 120/1000 | Loss: 0.00016518
Iteration 121/1000 | Loss: 0.00008080
Iteration 122/1000 | Loss: 0.00007099
Iteration 123/1000 | Loss: 0.00015330
Iteration 124/1000 | Loss: 0.00005800
Iteration 125/1000 | Loss: 0.00005202
Iteration 126/1000 | Loss: 0.00004767
Iteration 127/1000 | Loss: 0.00032407
Iteration 128/1000 | Loss: 0.00007352
Iteration 129/1000 | Loss: 0.00004674
Iteration 130/1000 | Loss: 0.00004085
Iteration 131/1000 | Loss: 0.00003824
Iteration 132/1000 | Loss: 0.00003623
Iteration 133/1000 | Loss: 0.00003489
Iteration 134/1000 | Loss: 0.00003410
Iteration 135/1000 | Loss: 0.00003360
Iteration 136/1000 | Loss: 0.00003319
Iteration 137/1000 | Loss: 0.00003296
Iteration 138/1000 | Loss: 0.00003292
Iteration 139/1000 | Loss: 0.00003292
Iteration 140/1000 | Loss: 0.00003272
Iteration 141/1000 | Loss: 0.00003268
Iteration 142/1000 | Loss: 0.00003267
Iteration 143/1000 | Loss: 0.00003263
Iteration 144/1000 | Loss: 0.00003263
Iteration 145/1000 | Loss: 0.00003260
Iteration 146/1000 | Loss: 0.00003260
Iteration 147/1000 | Loss: 0.00003259
Iteration 148/1000 | Loss: 0.00003258
Iteration 149/1000 | Loss: 0.00003257
Iteration 150/1000 | Loss: 0.00003257
Iteration 151/1000 | Loss: 0.00003257
Iteration 152/1000 | Loss: 0.00003257
Iteration 153/1000 | Loss: 0.00003257
Iteration 154/1000 | Loss: 0.00003257
Iteration 155/1000 | Loss: 0.00003257
Iteration 156/1000 | Loss: 0.00003257
Iteration 157/1000 | Loss: 0.00003256
Iteration 158/1000 | Loss: 0.00003256
Iteration 159/1000 | Loss: 0.00003256
Iteration 160/1000 | Loss: 0.00003256
Iteration 161/1000 | Loss: 0.00003256
Iteration 162/1000 | Loss: 0.00003256
Iteration 163/1000 | Loss: 0.00003256
Iteration 164/1000 | Loss: 0.00003256
Iteration 165/1000 | Loss: 0.00003256
Iteration 166/1000 | Loss: 0.00003256
Iteration 167/1000 | Loss: 0.00003256
Iteration 168/1000 | Loss: 0.00003255
Iteration 169/1000 | Loss: 0.00003254
Iteration 170/1000 | Loss: 0.00003254
Iteration 171/1000 | Loss: 0.00003252
Iteration 172/1000 | Loss: 0.00003252
Iteration 173/1000 | Loss: 0.00003252
Iteration 174/1000 | Loss: 0.00003252
Iteration 175/1000 | Loss: 0.00003252
Iteration 176/1000 | Loss: 0.00003251
Iteration 177/1000 | Loss: 0.00003249
Iteration 178/1000 | Loss: 0.00003249
Iteration 179/1000 | Loss: 0.00003246
Iteration 180/1000 | Loss: 0.00003246
Iteration 181/1000 | Loss: 0.00003246
Iteration 182/1000 | Loss: 0.00003245
Iteration 183/1000 | Loss: 0.00003245
Iteration 184/1000 | Loss: 0.00003245
Iteration 185/1000 | Loss: 0.00003245
Iteration 186/1000 | Loss: 0.00003244
Iteration 187/1000 | Loss: 0.00003244
Iteration 188/1000 | Loss: 0.00003244
Iteration 189/1000 | Loss: 0.00003244
Iteration 190/1000 | Loss: 0.00003244
Iteration 191/1000 | Loss: 0.00003244
Iteration 192/1000 | Loss: 0.00003243
Iteration 193/1000 | Loss: 0.00003243
Iteration 194/1000 | Loss: 0.00003243
Iteration 195/1000 | Loss: 0.00003243
Iteration 196/1000 | Loss: 0.00003243
Iteration 197/1000 | Loss: 0.00003243
Iteration 198/1000 | Loss: 0.00003243
Iteration 199/1000 | Loss: 0.00003242
Iteration 200/1000 | Loss: 0.00003242
Iteration 201/1000 | Loss: 0.00003242
Iteration 202/1000 | Loss: 0.00003242
Iteration 203/1000 | Loss: 0.00003242
Iteration 204/1000 | Loss: 0.00003241
Iteration 205/1000 | Loss: 0.00003241
Iteration 206/1000 | Loss: 0.00003241
Iteration 207/1000 | Loss: 0.00003241
Iteration 208/1000 | Loss: 0.00003241
Iteration 209/1000 | Loss: 0.00003241
Iteration 210/1000 | Loss: 0.00003241
Iteration 211/1000 | Loss: 0.00003241
Iteration 212/1000 | Loss: 0.00003241
Iteration 213/1000 | Loss: 0.00003241
Iteration 214/1000 | Loss: 0.00003241
Iteration 215/1000 | Loss: 0.00003240
Iteration 216/1000 | Loss: 0.00003240
Iteration 217/1000 | Loss: 0.00003240
Iteration 218/1000 | Loss: 0.00003240
Iteration 219/1000 | Loss: 0.00003240
Iteration 220/1000 | Loss: 0.00003240
Iteration 221/1000 | Loss: 0.00003240
Iteration 222/1000 | Loss: 0.00003240
Iteration 223/1000 | Loss: 0.00003240
Iteration 224/1000 | Loss: 0.00003240
Iteration 225/1000 | Loss: 0.00003240
Iteration 226/1000 | Loss: 0.00003239
Iteration 227/1000 | Loss: 0.00003239
Iteration 228/1000 | Loss: 0.00003239
Iteration 229/1000 | Loss: 0.00003239
Iteration 230/1000 | Loss: 0.00003239
Iteration 231/1000 | Loss: 0.00003239
Iteration 232/1000 | Loss: 0.00003239
Iteration 233/1000 | Loss: 0.00003239
Iteration 234/1000 | Loss: 0.00003239
Iteration 235/1000 | Loss: 0.00003239
Iteration 236/1000 | Loss: 0.00003239
Iteration 237/1000 | Loss: 0.00003239
Iteration 238/1000 | Loss: 0.00003239
Iteration 239/1000 | Loss: 0.00003238
Iteration 240/1000 | Loss: 0.00003238
Iteration 241/1000 | Loss: 0.00003238
Iteration 242/1000 | Loss: 0.00003238
Iteration 243/1000 | Loss: 0.00003238
Iteration 244/1000 | Loss: 0.00003238
Iteration 245/1000 | Loss: 0.00003238
Iteration 246/1000 | Loss: 0.00003238
Iteration 247/1000 | Loss: 0.00003238
Iteration 248/1000 | Loss: 0.00003238
Iteration 249/1000 | Loss: 0.00003238
Iteration 250/1000 | Loss: 0.00003238
Iteration 251/1000 | Loss: 0.00003238
Iteration 252/1000 | Loss: 0.00003238
Iteration 253/1000 | Loss: 0.00003238
Iteration 254/1000 | Loss: 0.00003238
Iteration 255/1000 | Loss: 0.00003238
Iteration 256/1000 | Loss: 0.00003238
Iteration 257/1000 | Loss: 0.00003238
Iteration 258/1000 | Loss: 0.00003238
Iteration 259/1000 | Loss: 0.00003238
Iteration 260/1000 | Loss: 0.00003238
Iteration 261/1000 | Loss: 0.00003238
Iteration 262/1000 | Loss: 0.00003238
Iteration 263/1000 | Loss: 0.00003238
Iteration 264/1000 | Loss: 0.00003238
Iteration 265/1000 | Loss: 0.00003238
Iteration 266/1000 | Loss: 0.00003238
Iteration 267/1000 | Loss: 0.00003238
Iteration 268/1000 | Loss: 0.00003238
Iteration 269/1000 | Loss: 0.00003238
Iteration 270/1000 | Loss: 0.00003238
Iteration 271/1000 | Loss: 0.00003238
Iteration 272/1000 | Loss: 0.00003238
Iteration 273/1000 | Loss: 0.00003238
Iteration 274/1000 | Loss: 0.00003238
Iteration 275/1000 | Loss: 0.00003238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [3.2376108720200136e-05, 3.2376108720200136e-05, 3.2376108720200136e-05, 3.2376108720200136e-05, 3.2376108720200136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2376108720200136e-05

Optimization complete. Final v2v error: 4.401416778564453 mm

Highest mean error: 13.626428604125977 mm for frame 124

Lowest mean error: 4.0918779373168945 mm for frame 15

Saving results

Total time: 245.97306847572327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928199
Iteration 2/25 | Loss: 0.00137378
Iteration 3/25 | Loss: 0.00125634
Iteration 4/25 | Loss: 0.00124414
Iteration 5/25 | Loss: 0.00123965
Iteration 6/25 | Loss: 0.00123878
Iteration 7/25 | Loss: 0.00123878
Iteration 8/25 | Loss: 0.00123878
Iteration 9/25 | Loss: 0.00123878
Iteration 10/25 | Loss: 0.00123878
Iteration 11/25 | Loss: 0.00123878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012387755559757352, 0.0012387755559757352, 0.0012387755559757352, 0.0012387755559757352, 0.0012387755559757352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012387755559757352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79080641
Iteration 2/25 | Loss: 0.00517457
Iteration 3/25 | Loss: 0.00517457
Iteration 4/25 | Loss: 0.00517457
Iteration 5/25 | Loss: 0.00517457
Iteration 6/25 | Loss: 0.00517457
Iteration 7/25 | Loss: 0.00517457
Iteration 8/25 | Loss: 0.00517457
Iteration 9/25 | Loss: 0.00517457
Iteration 10/25 | Loss: 0.00517457
Iteration 11/25 | Loss: 0.00517457
Iteration 12/25 | Loss: 0.00517457
Iteration 13/25 | Loss: 0.00517457
Iteration 14/25 | Loss: 0.00517457
Iteration 15/25 | Loss: 0.00517457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005174568388611078, 0.005174568388611078, 0.005174568388611078, 0.005174568388611078, 0.005174568388611078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005174568388611078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00517457
Iteration 2/1000 | Loss: 0.00003165
Iteration 3/1000 | Loss: 0.00002159
Iteration 4/1000 | Loss: 0.00001848
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001451
Iteration 23/1000 | Loss: 0.00001451
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001450
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001448
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001447
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001447
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001447
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001444
Iteration 55/1000 | Loss: 0.00001444
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001441
Iteration 61/1000 | Loss: 0.00001441
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001440
Iteration 64/1000 | Loss: 0.00001440
Iteration 65/1000 | Loss: 0.00001439
Iteration 66/1000 | Loss: 0.00001439
Iteration 67/1000 | Loss: 0.00001439
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001437
Iteration 72/1000 | Loss: 0.00001437
Iteration 73/1000 | Loss: 0.00001436
Iteration 74/1000 | Loss: 0.00001436
Iteration 75/1000 | Loss: 0.00001435
Iteration 76/1000 | Loss: 0.00001435
Iteration 77/1000 | Loss: 0.00001435
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001434
Iteration 80/1000 | Loss: 0.00001434
Iteration 81/1000 | Loss: 0.00001434
Iteration 82/1000 | Loss: 0.00001434
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001433
Iteration 86/1000 | Loss: 0.00001433
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001430
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001429
Iteration 109/1000 | Loss: 0.00001429
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001427
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001427
Iteration 123/1000 | Loss: 0.00001427
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001422
Iteration 156/1000 | Loss: 0.00001422
Iteration 157/1000 | Loss: 0.00001422
Iteration 158/1000 | Loss: 0.00001422
Iteration 159/1000 | Loss: 0.00001422
Iteration 160/1000 | Loss: 0.00001422
Iteration 161/1000 | Loss: 0.00001422
Iteration 162/1000 | Loss: 0.00001422
Iteration 163/1000 | Loss: 0.00001422
Iteration 164/1000 | Loss: 0.00001422
Iteration 165/1000 | Loss: 0.00001422
Iteration 166/1000 | Loss: 0.00001422
Iteration 167/1000 | Loss: 0.00001422
Iteration 168/1000 | Loss: 0.00001422
Iteration 169/1000 | Loss: 0.00001422
Iteration 170/1000 | Loss: 0.00001422
Iteration 171/1000 | Loss: 0.00001422
Iteration 172/1000 | Loss: 0.00001422
Iteration 173/1000 | Loss: 0.00001422
Iteration 174/1000 | Loss: 0.00001422
Iteration 175/1000 | Loss: 0.00001422
Iteration 176/1000 | Loss: 0.00001422
Iteration 177/1000 | Loss: 0.00001421
Iteration 178/1000 | Loss: 0.00001421
Iteration 179/1000 | Loss: 0.00001421
Iteration 180/1000 | Loss: 0.00001421
Iteration 181/1000 | Loss: 0.00001421
Iteration 182/1000 | Loss: 0.00001421
Iteration 183/1000 | Loss: 0.00001421
Iteration 184/1000 | Loss: 0.00001421
Iteration 185/1000 | Loss: 0.00001421
Iteration 186/1000 | Loss: 0.00001421
Iteration 187/1000 | Loss: 0.00001421
Iteration 188/1000 | Loss: 0.00001421
Iteration 189/1000 | Loss: 0.00001421
Iteration 190/1000 | Loss: 0.00001421
Iteration 191/1000 | Loss: 0.00001421
Iteration 192/1000 | Loss: 0.00001421
Iteration 193/1000 | Loss: 0.00001421
Iteration 194/1000 | Loss: 0.00001421
Iteration 195/1000 | Loss: 0.00001421
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Iteration 207/1000 | Loss: 0.00001421
Iteration 208/1000 | Loss: 0.00001421
Iteration 209/1000 | Loss: 0.00001421
Iteration 210/1000 | Loss: 0.00001421
Iteration 211/1000 | Loss: 0.00001421
Iteration 212/1000 | Loss: 0.00001421
Iteration 213/1000 | Loss: 0.00001421
Iteration 214/1000 | Loss: 0.00001421
Iteration 215/1000 | Loss: 0.00001421
Iteration 216/1000 | Loss: 0.00001421
Iteration 217/1000 | Loss: 0.00001421
Iteration 218/1000 | Loss: 0.00001421
Iteration 219/1000 | Loss: 0.00001421
Iteration 220/1000 | Loss: 0.00001421
Iteration 221/1000 | Loss: 0.00001421
Iteration 222/1000 | Loss: 0.00001421
Iteration 223/1000 | Loss: 0.00001421
Iteration 224/1000 | Loss: 0.00001421
Iteration 225/1000 | Loss: 0.00001421
Iteration 226/1000 | Loss: 0.00001421
Iteration 227/1000 | Loss: 0.00001421
Iteration 228/1000 | Loss: 0.00001421
Iteration 229/1000 | Loss: 0.00001421
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001421
Iteration 233/1000 | Loss: 0.00001421
Iteration 234/1000 | Loss: 0.00001421
Iteration 235/1000 | Loss: 0.00001421
Iteration 236/1000 | Loss: 0.00001421
Iteration 237/1000 | Loss: 0.00001421
Iteration 238/1000 | Loss: 0.00001421
Iteration 239/1000 | Loss: 0.00001421
Iteration 240/1000 | Loss: 0.00001421
Iteration 241/1000 | Loss: 0.00001421
Iteration 242/1000 | Loss: 0.00001421
Iteration 243/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.420933404006064e-05, 1.420933404006064e-05, 1.420933404006064e-05, 1.420933404006064e-05, 1.420933404006064e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.420933404006064e-05

Optimization complete. Final v2v error: 3.2622015476226807 mm

Highest mean error: 3.429340124130249 mm for frame 64

Lowest mean error: 3.1087865829467773 mm for frame 3

Saving results

Total time: 40.147178173065186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932791
Iteration 2/25 | Loss: 0.00167664
Iteration 3/25 | Loss: 0.00143213
Iteration 4/25 | Loss: 0.00140399
Iteration 5/25 | Loss: 0.00140017
Iteration 6/25 | Loss: 0.00140002
Iteration 7/25 | Loss: 0.00140002
Iteration 8/25 | Loss: 0.00140002
Iteration 9/25 | Loss: 0.00140002
Iteration 10/25 | Loss: 0.00140002
Iteration 11/25 | Loss: 0.00140002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014000164810568094, 0.0014000164810568094, 0.0014000164810568094, 0.0014000164810568094, 0.0014000164810568094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014000164810568094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29718041
Iteration 2/25 | Loss: 0.00457140
Iteration 3/25 | Loss: 0.00457139
Iteration 4/25 | Loss: 0.00457139
Iteration 5/25 | Loss: 0.00457138
Iteration 6/25 | Loss: 0.00457138
Iteration 7/25 | Loss: 0.00457138
Iteration 8/25 | Loss: 0.00457138
Iteration 9/25 | Loss: 0.00457138
Iteration 10/25 | Loss: 0.00457138
Iteration 11/25 | Loss: 0.00457138
Iteration 12/25 | Loss: 0.00457138
Iteration 13/25 | Loss: 0.00457138
Iteration 14/25 | Loss: 0.00457138
Iteration 15/25 | Loss: 0.00457138
Iteration 16/25 | Loss: 0.00457138
Iteration 17/25 | Loss: 0.00457138
Iteration 18/25 | Loss: 0.00457138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0045713819563388824, 0.0045713819563388824, 0.0045713819563388824, 0.0045713819563388824, 0.0045713819563388824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045713819563388824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00457138
Iteration 2/1000 | Loss: 0.00003667
Iteration 3/1000 | Loss: 0.00002945
Iteration 4/1000 | Loss: 0.00002714
Iteration 5/1000 | Loss: 0.00002572
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002382
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00002314
Iteration 10/1000 | Loss: 0.00002290
Iteration 11/1000 | Loss: 0.00002278
Iteration 12/1000 | Loss: 0.00002273
Iteration 13/1000 | Loss: 0.00002273
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002273
Iteration 16/1000 | Loss: 0.00002273
Iteration 17/1000 | Loss: 0.00002272
Iteration 18/1000 | Loss: 0.00002272
Iteration 19/1000 | Loss: 0.00002272
Iteration 20/1000 | Loss: 0.00002267
Iteration 21/1000 | Loss: 0.00002267
Iteration 22/1000 | Loss: 0.00002266
Iteration 23/1000 | Loss: 0.00002265
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002264
Iteration 27/1000 | Loss: 0.00002264
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002262
Iteration 32/1000 | Loss: 0.00002262
Iteration 33/1000 | Loss: 0.00002262
Iteration 34/1000 | Loss: 0.00002261
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002259
Iteration 37/1000 | Loss: 0.00002259
Iteration 38/1000 | Loss: 0.00002259
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002258
Iteration 41/1000 | Loss: 0.00002258
Iteration 42/1000 | Loss: 0.00002258
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00002258
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002258
Iteration 51/1000 | Loss: 0.00002258
Iteration 52/1000 | Loss: 0.00002258
Iteration 53/1000 | Loss: 0.00002258
Iteration 54/1000 | Loss: 0.00002258
Iteration 55/1000 | Loss: 0.00002258
Iteration 56/1000 | Loss: 0.00002258
Iteration 57/1000 | Loss: 0.00002258
Iteration 58/1000 | Loss: 0.00002258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.257950654893648e-05, 2.257950654893648e-05, 2.257950654893648e-05, 2.257950654893648e-05, 2.257950654893648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.257950654893648e-05

Optimization complete. Final v2v error: 4.052760124206543 mm

Highest mean error: 4.478795051574707 mm for frame 1

Lowest mean error: 3.7119195461273193 mm for frame 46

Saving results

Total time: 27.321990251541138
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01127475
Iteration 2/25 | Loss: 0.00266198
Iteration 3/25 | Loss: 0.00175244
Iteration 4/25 | Loss: 0.00160366
Iteration 5/25 | Loss: 0.00160800
Iteration 6/25 | Loss: 0.00148272
Iteration 7/25 | Loss: 0.00138776
Iteration 8/25 | Loss: 0.00134515
Iteration 9/25 | Loss: 0.00129831
Iteration 10/25 | Loss: 0.00127822
Iteration 11/25 | Loss: 0.00127771
Iteration 12/25 | Loss: 0.00127180
Iteration 13/25 | Loss: 0.00127201
Iteration 14/25 | Loss: 0.00126737
Iteration 15/25 | Loss: 0.00126544
Iteration 16/25 | Loss: 0.00126175
Iteration 17/25 | Loss: 0.00126331
Iteration 18/25 | Loss: 0.00126349
Iteration 19/25 | Loss: 0.00126138
Iteration 20/25 | Loss: 0.00125788
Iteration 21/25 | Loss: 0.00125602
Iteration 22/25 | Loss: 0.00125528
Iteration 23/25 | Loss: 0.00125457
Iteration 24/25 | Loss: 0.00125495
Iteration 25/25 | Loss: 0.00125432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93323040
Iteration 2/25 | Loss: 0.00599238
Iteration 3/25 | Loss: 0.00599237
Iteration 4/25 | Loss: 0.00599237
Iteration 5/25 | Loss: 0.00599237
Iteration 6/25 | Loss: 0.00599237
Iteration 7/25 | Loss: 0.00599237
Iteration 8/25 | Loss: 0.00599237
Iteration 9/25 | Loss: 0.00599237
Iteration 10/25 | Loss: 0.00599237
Iteration 11/25 | Loss: 0.00599237
Iteration 12/25 | Loss: 0.00599237
Iteration 13/25 | Loss: 0.00599237
Iteration 14/25 | Loss: 0.00599237
Iteration 15/25 | Loss: 0.00599237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005992371588945389, 0.005992371588945389, 0.005992371588945389, 0.005992371588945389, 0.005992371588945389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005992371588945389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00599237
Iteration 2/1000 | Loss: 0.00011438
Iteration 3/1000 | Loss: 0.00024996
Iteration 4/1000 | Loss: 0.00023818
Iteration 5/1000 | Loss: 0.00007806
Iteration 6/1000 | Loss: 0.00005959
Iteration 7/1000 | Loss: 0.00036135
Iteration 8/1000 | Loss: 0.00008958
Iteration 9/1000 | Loss: 0.00023555
Iteration 10/1000 | Loss: 0.00005800
Iteration 11/1000 | Loss: 0.00004911
Iteration 12/1000 | Loss: 0.00093635
Iteration 13/1000 | Loss: 0.00007581
Iteration 14/1000 | Loss: 0.00006941
Iteration 15/1000 | Loss: 0.00090207
Iteration 16/1000 | Loss: 0.00005182
Iteration 17/1000 | Loss: 0.00090667
Iteration 18/1000 | Loss: 0.00029288
Iteration 19/1000 | Loss: 0.00016884
Iteration 20/1000 | Loss: 0.00023198
Iteration 21/1000 | Loss: 0.00040127
Iteration 22/1000 | Loss: 0.00032784
Iteration 23/1000 | Loss: 0.00013578
Iteration 24/1000 | Loss: 0.00038400
Iteration 25/1000 | Loss: 0.00026820
Iteration 26/1000 | Loss: 0.00041989
Iteration 27/1000 | Loss: 0.00017287
Iteration 28/1000 | Loss: 0.00006961
Iteration 29/1000 | Loss: 0.00005506
Iteration 30/1000 | Loss: 0.00004484
Iteration 31/1000 | Loss: 0.00057188
Iteration 32/1000 | Loss: 0.00018538
Iteration 33/1000 | Loss: 0.00036570
Iteration 34/1000 | Loss: 0.00011747
Iteration 35/1000 | Loss: 0.00035102
Iteration 36/1000 | Loss: 0.00050450
Iteration 37/1000 | Loss: 0.00017572
Iteration 38/1000 | Loss: 0.00019684
Iteration 39/1000 | Loss: 0.00019320
Iteration 40/1000 | Loss: 0.00019825
Iteration 41/1000 | Loss: 0.00017860
Iteration 42/1000 | Loss: 0.00069894
Iteration 43/1000 | Loss: 0.00018475
Iteration 44/1000 | Loss: 0.00006788
Iteration 45/1000 | Loss: 0.00006079
Iteration 46/1000 | Loss: 0.00004520
Iteration 47/1000 | Loss: 0.00006582
Iteration 48/1000 | Loss: 0.00007242
Iteration 49/1000 | Loss: 0.00002884
Iteration 50/1000 | Loss: 0.00004313
Iteration 51/1000 | Loss: 0.00003480
Iteration 52/1000 | Loss: 0.00005442
Iteration 53/1000 | Loss: 0.00003236
Iteration 54/1000 | Loss: 0.00003404
Iteration 55/1000 | Loss: 0.00003860
Iteration 56/1000 | Loss: 0.00006298
Iteration 57/1000 | Loss: 0.00003289
Iteration 58/1000 | Loss: 0.00002967
Iteration 59/1000 | Loss: 0.00005465
Iteration 60/1000 | Loss: 0.00004330
Iteration 61/1000 | Loss: 0.00005330
Iteration 62/1000 | Loss: 0.00003908
Iteration 63/1000 | Loss: 0.00003274
Iteration 64/1000 | Loss: 0.00005521
Iteration 65/1000 | Loss: 0.00003823
Iteration 66/1000 | Loss: 0.00003725
Iteration 67/1000 | Loss: 0.00006304
Iteration 68/1000 | Loss: 0.00003016
Iteration 69/1000 | Loss: 0.00003973
Iteration 70/1000 | Loss: 0.00004558
Iteration 71/1000 | Loss: 0.00003390
Iteration 72/1000 | Loss: 0.00004263
Iteration 73/1000 | Loss: 0.00003730
Iteration 74/1000 | Loss: 0.00002672
Iteration 75/1000 | Loss: 0.00005041
Iteration 76/1000 | Loss: 0.00003603
Iteration 77/1000 | Loss: 0.00003727
Iteration 78/1000 | Loss: 0.00003404
Iteration 79/1000 | Loss: 0.00002965
Iteration 80/1000 | Loss: 0.00004196
Iteration 81/1000 | Loss: 0.00003632
Iteration 82/1000 | Loss: 0.00004979
Iteration 83/1000 | Loss: 0.00003934
Iteration 84/1000 | Loss: 0.00004683
Iteration 85/1000 | Loss: 0.00002944
Iteration 86/1000 | Loss: 0.00002327
Iteration 87/1000 | Loss: 0.00002987
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002593
Iteration 90/1000 | Loss: 0.00002082
Iteration 91/1000 | Loss: 0.00002394
Iteration 92/1000 | Loss: 0.00003353
Iteration 93/1000 | Loss: 0.00002574
Iteration 94/1000 | Loss: 0.00002822
Iteration 95/1000 | Loss: 0.00002317
Iteration 96/1000 | Loss: 0.00002187
Iteration 97/1000 | Loss: 0.00002166
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002048
Iteration 103/1000 | Loss: 0.00002048
Iteration 104/1000 | Loss: 0.00002048
Iteration 105/1000 | Loss: 0.00002048
Iteration 106/1000 | Loss: 0.00002048
Iteration 107/1000 | Loss: 0.00002048
Iteration 108/1000 | Loss: 0.00002048
Iteration 109/1000 | Loss: 0.00002048
Iteration 110/1000 | Loss: 0.00002048
Iteration 111/1000 | Loss: 0.00002048
Iteration 112/1000 | Loss: 0.00002048
Iteration 113/1000 | Loss: 0.00002048
Iteration 114/1000 | Loss: 0.00002048
Iteration 115/1000 | Loss: 0.00002048
Iteration 116/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.0479432350839488e-05, 2.0479432350839488e-05, 2.0479432350839488e-05, 2.0479432350839488e-05, 2.0479432350839488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0479432350839488e-05

Optimization complete. Final v2v error: 3.9395318031311035 mm

Highest mean error: 10.510281562805176 mm for frame 8

Lowest mean error: 3.38205623626709 mm for frame 81

Saving results

Total time: 203.2581820487976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145873
Iteration 2/25 | Loss: 0.00317450
Iteration 3/25 | Loss: 0.00224058
Iteration 4/25 | Loss: 0.00190660
Iteration 5/25 | Loss: 0.00165330
Iteration 6/25 | Loss: 0.00161554
Iteration 7/25 | Loss: 0.00143801
Iteration 8/25 | Loss: 0.00138982
Iteration 9/25 | Loss: 0.00134010
Iteration 10/25 | Loss: 0.00132097
Iteration 11/25 | Loss: 0.00129692
Iteration 12/25 | Loss: 0.00130188
Iteration 13/25 | Loss: 0.00127888
Iteration 14/25 | Loss: 0.00127623
Iteration 15/25 | Loss: 0.00127016
Iteration 16/25 | Loss: 0.00126930
Iteration 17/25 | Loss: 0.00126875
Iteration 18/25 | Loss: 0.00126868
Iteration 19/25 | Loss: 0.00126867
Iteration 20/25 | Loss: 0.00126867
Iteration 21/25 | Loss: 0.00126867
Iteration 22/25 | Loss: 0.00126866
Iteration 23/25 | Loss: 0.00126866
Iteration 24/25 | Loss: 0.00126866
Iteration 25/25 | Loss: 0.00126866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86692226
Iteration 2/25 | Loss: 0.00527698
Iteration 3/25 | Loss: 0.00527698
Iteration 4/25 | Loss: 0.00527698
Iteration 5/25 | Loss: 0.00527698
Iteration 6/25 | Loss: 0.00527697
Iteration 7/25 | Loss: 0.00527697
Iteration 8/25 | Loss: 0.00527697
Iteration 9/25 | Loss: 0.00527697
Iteration 10/25 | Loss: 0.00527697
Iteration 11/25 | Loss: 0.00527697
Iteration 12/25 | Loss: 0.00527697
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00527697429060936, 0.00527697429060936, 0.00527697429060936, 0.00527697429060936, 0.00527697429060936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00527697429060936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00527697
Iteration 2/1000 | Loss: 0.00009999
Iteration 3/1000 | Loss: 0.00007400
Iteration 4/1000 | Loss: 0.00005919
Iteration 5/1000 | Loss: 0.00005247
Iteration 6/1000 | Loss: 0.00004919
Iteration 7/1000 | Loss: 0.00133409
Iteration 8/1000 | Loss: 0.00166134
Iteration 9/1000 | Loss: 0.00031745
Iteration 10/1000 | Loss: 0.00033899
Iteration 11/1000 | Loss: 0.00012297
Iteration 12/1000 | Loss: 0.00004169
Iteration 13/1000 | Loss: 0.00003502
Iteration 14/1000 | Loss: 0.00045039
Iteration 15/1000 | Loss: 0.00157861
Iteration 16/1000 | Loss: 0.00008194
Iteration 17/1000 | Loss: 0.00012912
Iteration 18/1000 | Loss: 0.00002946
Iteration 19/1000 | Loss: 0.00013817
Iteration 20/1000 | Loss: 0.00005474
Iteration 21/1000 | Loss: 0.00008752
Iteration 22/1000 | Loss: 0.00004505
Iteration 23/1000 | Loss: 0.00003916
Iteration 24/1000 | Loss: 0.00008052
Iteration 25/1000 | Loss: 0.00003429
Iteration 26/1000 | Loss: 0.00002470
Iteration 27/1000 | Loss: 0.00002431
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00020249
Iteration 30/1000 | Loss: 0.00012487
Iteration 31/1000 | Loss: 0.00018716
Iteration 32/1000 | Loss: 0.00008005
Iteration 33/1000 | Loss: 0.00061980
Iteration 34/1000 | Loss: 0.00002914
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00002325
Iteration 38/1000 | Loss: 0.00011585
Iteration 39/1000 | Loss: 0.00016574
Iteration 40/1000 | Loss: 0.00002653
Iteration 41/1000 | Loss: 0.00007374
Iteration 42/1000 | Loss: 0.00008747
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002314
Iteration 45/1000 | Loss: 0.00002313
Iteration 46/1000 | Loss: 0.00002312
Iteration 47/1000 | Loss: 0.00002312
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002310
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00002309
Iteration 54/1000 | Loss: 0.00002309
Iteration 55/1000 | Loss: 0.00002308
Iteration 56/1000 | Loss: 0.00002308
Iteration 57/1000 | Loss: 0.00002308
Iteration 58/1000 | Loss: 0.00002307
Iteration 59/1000 | Loss: 0.00002307
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002306
Iteration 62/1000 | Loss: 0.00002306
Iteration 63/1000 | Loss: 0.00002306
Iteration 64/1000 | Loss: 0.00002306
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002305
Iteration 69/1000 | Loss: 0.00002305
Iteration 70/1000 | Loss: 0.00002305
Iteration 71/1000 | Loss: 0.00002304
Iteration 72/1000 | Loss: 0.00002304
Iteration 73/1000 | Loss: 0.00002304
Iteration 74/1000 | Loss: 0.00002303
Iteration 75/1000 | Loss: 0.00002303
Iteration 76/1000 | Loss: 0.00002303
Iteration 77/1000 | Loss: 0.00002303
Iteration 78/1000 | Loss: 0.00002303
Iteration 79/1000 | Loss: 0.00002302
Iteration 80/1000 | Loss: 0.00002302
Iteration 81/1000 | Loss: 0.00002302
Iteration 82/1000 | Loss: 0.00002302
Iteration 83/1000 | Loss: 0.00002302
Iteration 84/1000 | Loss: 0.00002302
Iteration 85/1000 | Loss: 0.00002302
Iteration 86/1000 | Loss: 0.00002302
Iteration 87/1000 | Loss: 0.00002302
Iteration 88/1000 | Loss: 0.00002302
Iteration 89/1000 | Loss: 0.00002302
Iteration 90/1000 | Loss: 0.00002302
Iteration 91/1000 | Loss: 0.00002302
Iteration 92/1000 | Loss: 0.00002301
Iteration 93/1000 | Loss: 0.00002301
Iteration 94/1000 | Loss: 0.00002301
Iteration 95/1000 | Loss: 0.00002301
Iteration 96/1000 | Loss: 0.00002301
Iteration 97/1000 | Loss: 0.00002301
Iteration 98/1000 | Loss: 0.00002301
Iteration 99/1000 | Loss: 0.00002301
Iteration 100/1000 | Loss: 0.00002301
Iteration 101/1000 | Loss: 0.00002301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.3013355530565605e-05, 2.3013355530565605e-05, 2.3013355530565605e-05, 2.3013355530565605e-05, 2.3013355530565605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3013355530565605e-05

Optimization complete. Final v2v error: 3.9824750423431396 mm

Highest mean error: 10.00177001953125 mm for frame 209

Lowest mean error: 3.6643309593200684 mm for frame 128

Saving results

Total time: 107.6947774887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459580
Iteration 2/25 | Loss: 0.00154668
Iteration 3/25 | Loss: 0.00129258
Iteration 4/25 | Loss: 0.00126817
Iteration 5/25 | Loss: 0.00126196
Iteration 6/25 | Loss: 0.00126051
Iteration 7/25 | Loss: 0.00126028
Iteration 8/25 | Loss: 0.00126028
Iteration 9/25 | Loss: 0.00126028
Iteration 10/25 | Loss: 0.00126028
Iteration 11/25 | Loss: 0.00126028
Iteration 12/25 | Loss: 0.00126028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012602750211954117, 0.0012602750211954117, 0.0012602750211954117, 0.0012602750211954117, 0.0012602750211954117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012602750211954117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78906345
Iteration 2/25 | Loss: 0.00525320
Iteration 3/25 | Loss: 0.00525319
Iteration 4/25 | Loss: 0.00525319
Iteration 5/25 | Loss: 0.00525319
Iteration 6/25 | Loss: 0.00525319
Iteration 7/25 | Loss: 0.00525319
Iteration 8/25 | Loss: 0.00525319
Iteration 9/25 | Loss: 0.00525319
Iteration 10/25 | Loss: 0.00525319
Iteration 11/25 | Loss: 0.00525319
Iteration 12/25 | Loss: 0.00525319
Iteration 13/25 | Loss: 0.00525319
Iteration 14/25 | Loss: 0.00525319
Iteration 15/25 | Loss: 0.00525319
Iteration 16/25 | Loss: 0.00525319
Iteration 17/25 | Loss: 0.00525319
Iteration 18/25 | Loss: 0.00525319
Iteration 19/25 | Loss: 0.00525319
Iteration 20/25 | Loss: 0.00525319
Iteration 21/25 | Loss: 0.00525319
Iteration 22/25 | Loss: 0.00525319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0052531869150698185, 0.0052531869150698185, 0.0052531869150698185, 0.0052531869150698185, 0.0052531869150698185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0052531869150698185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00525319
Iteration 2/1000 | Loss: 0.00004877
Iteration 3/1000 | Loss: 0.00002871
Iteration 4/1000 | Loss: 0.00002398
Iteration 5/1000 | Loss: 0.00002185
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00001987
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001880
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001867
Iteration 16/1000 | Loss: 0.00001864
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001840
Iteration 20/1000 | Loss: 0.00001830
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001825
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001814
Iteration 27/1000 | Loss: 0.00001814
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001813
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001813
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001808
Iteration 66/1000 | Loss: 0.00001808
Iteration 67/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.8075390471494757e-05, 1.8075390471494757e-05, 1.8075390471494757e-05, 1.8075390471494757e-05, 1.8075390471494757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8075390471494757e-05

Optimization complete. Final v2v error: 3.7712652683258057 mm

Highest mean error: 3.9055795669555664 mm for frame 75

Lowest mean error: 3.222709894180298 mm for frame 0

Saving results

Total time: 30.989251613616943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398411
Iteration 2/25 | Loss: 0.00148253
Iteration 3/25 | Loss: 0.00135343
Iteration 4/25 | Loss: 0.00132239
Iteration 5/25 | Loss: 0.00131711
Iteration 6/25 | Loss: 0.00131533
Iteration 7/25 | Loss: 0.00131463
Iteration 8/25 | Loss: 0.00131463
Iteration 9/25 | Loss: 0.00131463
Iteration 10/25 | Loss: 0.00131463
Iteration 11/25 | Loss: 0.00131463
Iteration 12/25 | Loss: 0.00131463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001314628985710442, 0.001314628985710442, 0.001314628985710442, 0.001314628985710442, 0.001314628985710442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001314628985710442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89311540
Iteration 2/25 | Loss: 0.00711964
Iteration 3/25 | Loss: 0.00711964
Iteration 4/25 | Loss: 0.00711964
Iteration 5/25 | Loss: 0.00711963
Iteration 6/25 | Loss: 0.00711963
Iteration 7/25 | Loss: 0.00711963
Iteration 8/25 | Loss: 0.00711963
Iteration 9/25 | Loss: 0.00711963
Iteration 10/25 | Loss: 0.00711963
Iteration 11/25 | Loss: 0.00711963
Iteration 12/25 | Loss: 0.00711963
Iteration 13/25 | Loss: 0.00711963
Iteration 14/25 | Loss: 0.00711963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007119631860405207, 0.007119631860405207, 0.007119631860405207, 0.007119631860405207, 0.007119631860405207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007119631860405207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00711963
Iteration 2/1000 | Loss: 0.00007761
Iteration 3/1000 | Loss: 0.00004332
Iteration 4/1000 | Loss: 0.00003275
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002515
Iteration 7/1000 | Loss: 0.00002372
Iteration 8/1000 | Loss: 0.00002287
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002159
Iteration 11/1000 | Loss: 0.00002122
Iteration 12/1000 | Loss: 0.00002083
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002025
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001976
Iteration 17/1000 | Loss: 0.00001957
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001902
Iteration 24/1000 | Loss: 0.00001900
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001890
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001889
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001886
Iteration 48/1000 | Loss: 0.00001886
Iteration 49/1000 | Loss: 0.00001886
Iteration 50/1000 | Loss: 0.00001886
Iteration 51/1000 | Loss: 0.00001886
Iteration 52/1000 | Loss: 0.00001886
Iteration 53/1000 | Loss: 0.00001886
Iteration 54/1000 | Loss: 0.00001885
Iteration 55/1000 | Loss: 0.00001885
Iteration 56/1000 | Loss: 0.00001885
Iteration 57/1000 | Loss: 0.00001885
Iteration 58/1000 | Loss: 0.00001885
Iteration 59/1000 | Loss: 0.00001885
Iteration 60/1000 | Loss: 0.00001884
Iteration 61/1000 | Loss: 0.00001884
Iteration 62/1000 | Loss: 0.00001884
Iteration 63/1000 | Loss: 0.00001884
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001884
Iteration 66/1000 | Loss: 0.00001883
Iteration 67/1000 | Loss: 0.00001883
Iteration 68/1000 | Loss: 0.00001883
Iteration 69/1000 | Loss: 0.00001883
Iteration 70/1000 | Loss: 0.00001883
Iteration 71/1000 | Loss: 0.00001883
Iteration 72/1000 | Loss: 0.00001883
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001883
Iteration 75/1000 | Loss: 0.00001882
Iteration 76/1000 | Loss: 0.00001882
Iteration 77/1000 | Loss: 0.00001882
Iteration 78/1000 | Loss: 0.00001882
Iteration 79/1000 | Loss: 0.00001882
Iteration 80/1000 | Loss: 0.00001882
Iteration 81/1000 | Loss: 0.00001882
Iteration 82/1000 | Loss: 0.00001881
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001881
Iteration 85/1000 | Loss: 0.00001881
Iteration 86/1000 | Loss: 0.00001881
Iteration 87/1000 | Loss: 0.00001881
Iteration 88/1000 | Loss: 0.00001881
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001881
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001881
Iteration 113/1000 | Loss: 0.00001881
Iteration 114/1000 | Loss: 0.00001881
Iteration 115/1000 | Loss: 0.00001881
Iteration 116/1000 | Loss: 0.00001881
Iteration 117/1000 | Loss: 0.00001881
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001881
Iteration 127/1000 | Loss: 0.00001881
Iteration 128/1000 | Loss: 0.00001881
Iteration 129/1000 | Loss: 0.00001881
Iteration 130/1000 | Loss: 0.00001881
Iteration 131/1000 | Loss: 0.00001881
Iteration 132/1000 | Loss: 0.00001881
Iteration 133/1000 | Loss: 0.00001881
Iteration 134/1000 | Loss: 0.00001881
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001881
Iteration 143/1000 | Loss: 0.00001881
Iteration 144/1000 | Loss: 0.00001881
Iteration 145/1000 | Loss: 0.00001881
Iteration 146/1000 | Loss: 0.00001881
Iteration 147/1000 | Loss: 0.00001881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.880628587969113e-05, 1.880628587969113e-05, 1.880628587969113e-05, 1.880628587969113e-05, 1.880628587969113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.880628587969113e-05

Optimization complete. Final v2v error: 3.731466054916382 mm

Highest mean error: 4.741973400115967 mm for frame 151

Lowest mean error: 3.1193764209747314 mm for frame 47

Saving results

Total time: 49.2782883644104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891135
Iteration 2/25 | Loss: 0.00186452
Iteration 3/25 | Loss: 0.00143665
Iteration 4/25 | Loss: 0.00138155
Iteration 5/25 | Loss: 0.00136508
Iteration 6/25 | Loss: 0.00136548
Iteration 7/25 | Loss: 0.00135316
Iteration 8/25 | Loss: 0.00134890
Iteration 9/25 | Loss: 0.00134783
Iteration 10/25 | Loss: 0.00135288
Iteration 11/25 | Loss: 0.00134377
Iteration 12/25 | Loss: 0.00134177
Iteration 13/25 | Loss: 0.00134161
Iteration 14/25 | Loss: 0.00134161
Iteration 15/25 | Loss: 0.00134161
Iteration 16/25 | Loss: 0.00134161
Iteration 17/25 | Loss: 0.00134161
Iteration 18/25 | Loss: 0.00134160
Iteration 19/25 | Loss: 0.00134160
Iteration 20/25 | Loss: 0.00134160
Iteration 21/25 | Loss: 0.00134160
Iteration 22/25 | Loss: 0.00134160
Iteration 23/25 | Loss: 0.00134160
Iteration 24/25 | Loss: 0.00134160
Iteration 25/25 | Loss: 0.00134160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.09165239
Iteration 2/25 | Loss: 0.00523539
Iteration 3/25 | Loss: 0.00523539
Iteration 4/25 | Loss: 0.00523539
Iteration 5/25 | Loss: 0.00523539
Iteration 6/25 | Loss: 0.00523539
Iteration 7/25 | Loss: 0.00523539
Iteration 8/25 | Loss: 0.00523539
Iteration 9/25 | Loss: 0.00523539
Iteration 10/25 | Loss: 0.00523539
Iteration 11/25 | Loss: 0.00523539
Iteration 12/25 | Loss: 0.00523539
Iteration 13/25 | Loss: 0.00523539
Iteration 14/25 | Loss: 0.00523539
Iteration 15/25 | Loss: 0.00523539
Iteration 16/25 | Loss: 0.00523539
Iteration 17/25 | Loss: 0.00523539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005235385615378618, 0.005235385615378618, 0.005235385615378618, 0.005235385615378618, 0.005235385615378618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005235385615378618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00523539
Iteration 2/1000 | Loss: 0.00004556
Iteration 3/1000 | Loss: 0.00003377
Iteration 4/1000 | Loss: 0.00003044
Iteration 5/1000 | Loss: 0.00002842
Iteration 6/1000 | Loss: 0.00002744
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002612
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002558
Iteration 11/1000 | Loss: 0.00002540
Iteration 12/1000 | Loss: 0.00002521
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002512
Iteration 15/1000 | Loss: 0.00002506
Iteration 16/1000 | Loss: 0.00002504
Iteration 17/1000 | Loss: 0.00002503
Iteration 18/1000 | Loss: 0.00002502
Iteration 19/1000 | Loss: 0.00002502
Iteration 20/1000 | Loss: 0.00002502
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002501
Iteration 23/1000 | Loss: 0.00002501
Iteration 24/1000 | Loss: 0.00002501
Iteration 25/1000 | Loss: 0.00002500
Iteration 26/1000 | Loss: 0.00002500
Iteration 27/1000 | Loss: 0.00002499
Iteration 28/1000 | Loss: 0.00002499
Iteration 29/1000 | Loss: 0.00002499
Iteration 30/1000 | Loss: 0.00002498
Iteration 31/1000 | Loss: 0.00002498
Iteration 32/1000 | Loss: 0.00002498
Iteration 33/1000 | Loss: 0.00002497
Iteration 34/1000 | Loss: 0.00002496
Iteration 35/1000 | Loss: 0.00002496
Iteration 36/1000 | Loss: 0.00002495
Iteration 37/1000 | Loss: 0.00002495
Iteration 38/1000 | Loss: 0.00002495
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002493
Iteration 43/1000 | Loss: 0.00002493
Iteration 44/1000 | Loss: 0.00002493
Iteration 45/1000 | Loss: 0.00002493
Iteration 46/1000 | Loss: 0.00002492
Iteration 47/1000 | Loss: 0.00002492
Iteration 48/1000 | Loss: 0.00002492
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002492
Iteration 51/1000 | Loss: 0.00002492
Iteration 52/1000 | Loss: 0.00002492
Iteration 53/1000 | Loss: 0.00002492
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002491
Iteration 56/1000 | Loss: 0.00002490
Iteration 57/1000 | Loss: 0.00002490
Iteration 58/1000 | Loss: 0.00002490
Iteration 59/1000 | Loss: 0.00002490
Iteration 60/1000 | Loss: 0.00002490
Iteration 61/1000 | Loss: 0.00002490
Iteration 62/1000 | Loss: 0.00002489
Iteration 63/1000 | Loss: 0.00002489
Iteration 64/1000 | Loss: 0.00002489
Iteration 65/1000 | Loss: 0.00002489
Iteration 66/1000 | Loss: 0.00002489
Iteration 67/1000 | Loss: 0.00002489
Iteration 68/1000 | Loss: 0.00002488
Iteration 69/1000 | Loss: 0.00002488
Iteration 70/1000 | Loss: 0.00002488
Iteration 71/1000 | Loss: 0.00002488
Iteration 72/1000 | Loss: 0.00002488
Iteration 73/1000 | Loss: 0.00002488
Iteration 74/1000 | Loss: 0.00002488
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002488
Iteration 77/1000 | Loss: 0.00002488
Iteration 78/1000 | Loss: 0.00002488
Iteration 79/1000 | Loss: 0.00002487
Iteration 80/1000 | Loss: 0.00002487
Iteration 81/1000 | Loss: 0.00002487
Iteration 82/1000 | Loss: 0.00002487
Iteration 83/1000 | Loss: 0.00002487
Iteration 84/1000 | Loss: 0.00002487
Iteration 85/1000 | Loss: 0.00002487
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002486
Iteration 90/1000 | Loss: 0.00002486
Iteration 91/1000 | Loss: 0.00002486
Iteration 92/1000 | Loss: 0.00002486
Iteration 93/1000 | Loss: 0.00002486
Iteration 94/1000 | Loss: 0.00002486
Iteration 95/1000 | Loss: 0.00002486
Iteration 96/1000 | Loss: 0.00002486
Iteration 97/1000 | Loss: 0.00002486
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002485
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002485
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Iteration 114/1000 | Loss: 0.00002484
Iteration 115/1000 | Loss: 0.00002484
Iteration 116/1000 | Loss: 0.00002484
Iteration 117/1000 | Loss: 0.00002484
Iteration 118/1000 | Loss: 0.00002484
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002484
Iteration 121/1000 | Loss: 0.00002483
Iteration 122/1000 | Loss: 0.00002483
Iteration 123/1000 | Loss: 0.00002483
Iteration 124/1000 | Loss: 0.00002483
Iteration 125/1000 | Loss: 0.00002483
Iteration 126/1000 | Loss: 0.00002483
Iteration 127/1000 | Loss: 0.00002483
Iteration 128/1000 | Loss: 0.00002483
Iteration 129/1000 | Loss: 0.00002483
Iteration 130/1000 | Loss: 0.00002483
Iteration 131/1000 | Loss: 0.00002483
Iteration 132/1000 | Loss: 0.00002483
Iteration 133/1000 | Loss: 0.00002483
Iteration 134/1000 | Loss: 0.00002483
Iteration 135/1000 | Loss: 0.00002483
Iteration 136/1000 | Loss: 0.00002482
Iteration 137/1000 | Loss: 0.00002482
Iteration 138/1000 | Loss: 0.00002482
Iteration 139/1000 | Loss: 0.00002482
Iteration 140/1000 | Loss: 0.00002482
Iteration 141/1000 | Loss: 0.00002482
Iteration 142/1000 | Loss: 0.00002482
Iteration 143/1000 | Loss: 0.00002482
Iteration 144/1000 | Loss: 0.00002482
Iteration 145/1000 | Loss: 0.00002482
Iteration 146/1000 | Loss: 0.00002482
Iteration 147/1000 | Loss: 0.00002482
Iteration 148/1000 | Loss: 0.00002482
Iteration 149/1000 | Loss: 0.00002482
Iteration 150/1000 | Loss: 0.00002482
Iteration 151/1000 | Loss: 0.00002482
Iteration 152/1000 | Loss: 0.00002482
Iteration 153/1000 | Loss: 0.00002482
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002482
Iteration 161/1000 | Loss: 0.00002482
Iteration 162/1000 | Loss: 0.00002482
Iteration 163/1000 | Loss: 0.00002482
Iteration 164/1000 | Loss: 0.00002482
Iteration 165/1000 | Loss: 0.00002482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.4819599275360815e-05, 2.4819599275360815e-05, 2.4819599275360815e-05, 2.4819599275360815e-05, 2.4819599275360815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4819599275360815e-05

Optimization complete. Final v2v error: 4.353668689727783 mm

Highest mean error: 4.9827046394348145 mm for frame 60

Lowest mean error: 3.7509024143218994 mm for frame 178

Saving results

Total time: 50.37524199485779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01171338
Iteration 2/25 | Loss: 0.00236592
Iteration 3/25 | Loss: 0.00173728
Iteration 4/25 | Loss: 0.00149631
Iteration 5/25 | Loss: 0.00145870
Iteration 6/25 | Loss: 0.00141064
Iteration 7/25 | Loss: 0.00141302
Iteration 8/25 | Loss: 0.00138526
Iteration 9/25 | Loss: 0.00137211
Iteration 10/25 | Loss: 0.00136066
Iteration 11/25 | Loss: 0.00135622
Iteration 12/25 | Loss: 0.00135500
Iteration 13/25 | Loss: 0.00134733
Iteration 14/25 | Loss: 0.00135128
Iteration 15/25 | Loss: 0.00135737
Iteration 16/25 | Loss: 0.00135114
Iteration 17/25 | Loss: 0.00134956
Iteration 18/25 | Loss: 0.00135276
Iteration 19/25 | Loss: 0.00134763
Iteration 20/25 | Loss: 0.00134060
Iteration 21/25 | Loss: 0.00133417
Iteration 22/25 | Loss: 0.00133723
Iteration 23/25 | Loss: 0.00133632
Iteration 24/25 | Loss: 0.00132996
Iteration 25/25 | Loss: 0.00133056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77375221
Iteration 2/25 | Loss: 0.00532908
Iteration 3/25 | Loss: 0.00483558
Iteration 4/25 | Loss: 0.00483407
Iteration 5/25 | Loss: 0.00483407
Iteration 6/25 | Loss: 0.00483407
Iteration 7/25 | Loss: 0.00483407
Iteration 8/25 | Loss: 0.00483406
Iteration 9/25 | Loss: 0.00483406
Iteration 10/25 | Loss: 0.00483406
Iteration 11/25 | Loss: 0.00483406
Iteration 12/25 | Loss: 0.00483406
Iteration 13/25 | Loss: 0.00483406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004834064748138189, 0.004834064748138189, 0.004834064748138189, 0.004834064748138189, 0.004834064748138189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004834064748138189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00483407
Iteration 2/1000 | Loss: 0.00044418
Iteration 3/1000 | Loss: 0.00082506
Iteration 4/1000 | Loss: 0.00064928
Iteration 5/1000 | Loss: 0.00289369
Iteration 6/1000 | Loss: 0.00021519
Iteration 7/1000 | Loss: 0.00006885
Iteration 8/1000 | Loss: 0.00009062
Iteration 9/1000 | Loss: 0.00016381
Iteration 10/1000 | Loss: 0.00010452
Iteration 11/1000 | Loss: 0.00009839
Iteration 12/1000 | Loss: 0.00057616
Iteration 13/1000 | Loss: 0.00007394
Iteration 14/1000 | Loss: 0.00009693
Iteration 15/1000 | Loss: 0.00003849
Iteration 16/1000 | Loss: 0.00005066
Iteration 17/1000 | Loss: 0.00003772
Iteration 18/1000 | Loss: 0.00017252
Iteration 19/1000 | Loss: 0.00003751
Iteration 20/1000 | Loss: 0.00009867
Iteration 21/1000 | Loss: 0.00019778
Iteration 22/1000 | Loss: 0.00019366
Iteration 23/1000 | Loss: 0.00010047
Iteration 24/1000 | Loss: 0.00004389
Iteration 25/1000 | Loss: 0.00003825
Iteration 26/1000 | Loss: 0.00003702
Iteration 27/1000 | Loss: 0.00003699
Iteration 28/1000 | Loss: 0.00008001
Iteration 29/1000 | Loss: 0.00038922
Iteration 30/1000 | Loss: 0.00005449
Iteration 31/1000 | Loss: 0.00008178
Iteration 32/1000 | Loss: 0.00009498
Iteration 33/1000 | Loss: 0.00005735
Iteration 34/1000 | Loss: 0.00007230
Iteration 35/1000 | Loss: 0.00004448
Iteration 36/1000 | Loss: 0.00009231
Iteration 37/1000 | Loss: 0.00003884
Iteration 38/1000 | Loss: 0.00003693
Iteration 39/1000 | Loss: 0.00007563
Iteration 40/1000 | Loss: 0.00003672
Iteration 41/1000 | Loss: 0.00010431
Iteration 42/1000 | Loss: 0.00003687
Iteration 43/1000 | Loss: 0.00003664
Iteration 44/1000 | Loss: 0.00003661
Iteration 45/1000 | Loss: 0.00003661
Iteration 46/1000 | Loss: 0.00003660
Iteration 47/1000 | Loss: 0.00006353
Iteration 48/1000 | Loss: 0.00009418
Iteration 49/1000 | Loss: 0.00004119
Iteration 50/1000 | Loss: 0.00004071
Iteration 51/1000 | Loss: 0.00008073
Iteration 52/1000 | Loss: 0.00003787
Iteration 53/1000 | Loss: 0.00003930
Iteration 54/1000 | Loss: 0.00003666
Iteration 55/1000 | Loss: 0.00003665
Iteration 56/1000 | Loss: 0.00003665
Iteration 57/1000 | Loss: 0.00003665
Iteration 58/1000 | Loss: 0.00003665
Iteration 59/1000 | Loss: 0.00005580
Iteration 60/1000 | Loss: 0.00003664
Iteration 61/1000 | Loss: 0.00005806
Iteration 62/1000 | Loss: 0.00003770
Iteration 63/1000 | Loss: 0.00003947
Iteration 64/1000 | Loss: 0.00003648
Iteration 65/1000 | Loss: 0.00003648
Iteration 66/1000 | Loss: 0.00003648
Iteration 67/1000 | Loss: 0.00003648
Iteration 68/1000 | Loss: 0.00003648
Iteration 69/1000 | Loss: 0.00003648
Iteration 70/1000 | Loss: 0.00003648
Iteration 71/1000 | Loss: 0.00003648
Iteration 72/1000 | Loss: 0.00003648
Iteration 73/1000 | Loss: 0.00003648
Iteration 74/1000 | Loss: 0.00003648
Iteration 75/1000 | Loss: 0.00003648
Iteration 76/1000 | Loss: 0.00003647
Iteration 77/1000 | Loss: 0.00003647
Iteration 78/1000 | Loss: 0.00003647
Iteration 79/1000 | Loss: 0.00003647
Iteration 80/1000 | Loss: 0.00003647
Iteration 81/1000 | Loss: 0.00003647
Iteration 82/1000 | Loss: 0.00003647
Iteration 83/1000 | Loss: 0.00003647
Iteration 84/1000 | Loss: 0.00003647
Iteration 85/1000 | Loss: 0.00003647
Iteration 86/1000 | Loss: 0.00003647
Iteration 87/1000 | Loss: 0.00003647
Iteration 88/1000 | Loss: 0.00003647
Iteration 89/1000 | Loss: 0.00003647
Iteration 90/1000 | Loss: 0.00004287
Iteration 91/1000 | Loss: 0.00003648
Iteration 92/1000 | Loss: 0.00003646
Iteration 93/1000 | Loss: 0.00003646
Iteration 94/1000 | Loss: 0.00003644
Iteration 95/1000 | Loss: 0.00003642
Iteration 96/1000 | Loss: 0.00003642
Iteration 97/1000 | Loss: 0.00003641
Iteration 98/1000 | Loss: 0.00003641
Iteration 99/1000 | Loss: 0.00003641
Iteration 100/1000 | Loss: 0.00003640
Iteration 101/1000 | Loss: 0.00003640
Iteration 102/1000 | Loss: 0.00003640
Iteration 103/1000 | Loss: 0.00003639
Iteration 104/1000 | Loss: 0.00003639
Iteration 105/1000 | Loss: 0.00003639
Iteration 106/1000 | Loss: 0.00003639
Iteration 107/1000 | Loss: 0.00003639
Iteration 108/1000 | Loss: 0.00003639
Iteration 109/1000 | Loss: 0.00003638
Iteration 110/1000 | Loss: 0.00003638
Iteration 111/1000 | Loss: 0.00003638
Iteration 112/1000 | Loss: 0.00004461
Iteration 113/1000 | Loss: 0.00003640
Iteration 114/1000 | Loss: 0.00003640
Iteration 115/1000 | Loss: 0.00003639
Iteration 116/1000 | Loss: 0.00003639
Iteration 117/1000 | Loss: 0.00003639
Iteration 118/1000 | Loss: 0.00004209
Iteration 119/1000 | Loss: 0.00014857
Iteration 120/1000 | Loss: 0.00014996
Iteration 121/1000 | Loss: 0.00006007
Iteration 122/1000 | Loss: 0.00004202
Iteration 123/1000 | Loss: 0.00004638
Iteration 124/1000 | Loss: 0.00003861
Iteration 125/1000 | Loss: 0.00003648
Iteration 126/1000 | Loss: 0.00003999
Iteration 127/1000 | Loss: 0.00005030
Iteration 128/1000 | Loss: 0.00003642
Iteration 129/1000 | Loss: 0.00004952
Iteration 130/1000 | Loss: 0.00003635
Iteration 131/1000 | Loss: 0.00003633
Iteration 132/1000 | Loss: 0.00003632
Iteration 133/1000 | Loss: 0.00003631
Iteration 134/1000 | Loss: 0.00003630
Iteration 135/1000 | Loss: 0.00004684
Iteration 136/1000 | Loss: 0.00003636
Iteration 137/1000 | Loss: 0.00003630
Iteration 138/1000 | Loss: 0.00003630
Iteration 139/1000 | Loss: 0.00003630
Iteration 140/1000 | Loss: 0.00003630
Iteration 141/1000 | Loss: 0.00003630
Iteration 142/1000 | Loss: 0.00003630
Iteration 143/1000 | Loss: 0.00003630
Iteration 144/1000 | Loss: 0.00003630
Iteration 145/1000 | Loss: 0.00003630
Iteration 146/1000 | Loss: 0.00003630
Iteration 147/1000 | Loss: 0.00003630
Iteration 148/1000 | Loss: 0.00003629
Iteration 149/1000 | Loss: 0.00003629
Iteration 150/1000 | Loss: 0.00003629
Iteration 151/1000 | Loss: 0.00003629
Iteration 152/1000 | Loss: 0.00003629
Iteration 153/1000 | Loss: 0.00003629
Iteration 154/1000 | Loss: 0.00003629
Iteration 155/1000 | Loss: 0.00003629
Iteration 156/1000 | Loss: 0.00003629
Iteration 157/1000 | Loss: 0.00003629
Iteration 158/1000 | Loss: 0.00003629
Iteration 159/1000 | Loss: 0.00003628
Iteration 160/1000 | Loss: 0.00003628
Iteration 161/1000 | Loss: 0.00003628
Iteration 162/1000 | Loss: 0.00003628
Iteration 163/1000 | Loss: 0.00003628
Iteration 164/1000 | Loss: 0.00003628
Iteration 165/1000 | Loss: 0.00003628
Iteration 166/1000 | Loss: 0.00003628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.6284262023400515e-05, 3.6284262023400515e-05, 3.6284262023400515e-05, 3.6284262023400515e-05, 3.6284262023400515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6284262023400515e-05

Optimization complete. Final v2v error: 5.044600963592529 mm

Highest mean error: 10.664124488830566 mm for frame 49

Lowest mean error: 4.612155437469482 mm for frame 129

Saving results

Total time: 139.23378038406372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01158846
Iteration 2/25 | Loss: 0.00222873
Iteration 3/25 | Loss: 0.00176984
Iteration 4/25 | Loss: 0.00171399
Iteration 5/25 | Loss: 0.00159210
Iteration 6/25 | Loss: 0.00166160
Iteration 7/25 | Loss: 0.00153661
Iteration 8/25 | Loss: 0.00142151
Iteration 9/25 | Loss: 0.00138327
Iteration 10/25 | Loss: 0.00145656
Iteration 11/25 | Loss: 0.00150896
Iteration 12/25 | Loss: 0.00143471
Iteration 13/25 | Loss: 0.00143268
Iteration 14/25 | Loss: 0.00136287
Iteration 15/25 | Loss: 0.00130305
Iteration 16/25 | Loss: 0.00137340
Iteration 17/25 | Loss: 0.00129784
Iteration 18/25 | Loss: 0.00128559
Iteration 19/25 | Loss: 0.00128042
Iteration 20/25 | Loss: 0.00127761
Iteration 21/25 | Loss: 0.00135700
Iteration 22/25 | Loss: 0.00146938
Iteration 23/25 | Loss: 0.00132698
Iteration 24/25 | Loss: 0.00130555
Iteration 25/25 | Loss: 0.00134181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93662179
Iteration 2/25 | Loss: 0.00707615
Iteration 3/25 | Loss: 0.00707615
Iteration 4/25 | Loss: 0.00707615
Iteration 5/25 | Loss: 0.00707615
Iteration 6/25 | Loss: 0.00707615
Iteration 7/25 | Loss: 0.00707615
Iteration 8/25 | Loss: 0.00707615
Iteration 9/25 | Loss: 0.00707615
Iteration 10/25 | Loss: 0.00707615
Iteration 11/25 | Loss: 0.00707615
Iteration 12/25 | Loss: 0.00707615
Iteration 13/25 | Loss: 0.00707615
Iteration 14/25 | Loss: 0.00707615
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.007076145149767399, 0.007076145149767399, 0.007076145149767399, 0.007076145149767399, 0.007076145149767399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007076145149767399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00707615
Iteration 2/1000 | Loss: 0.00693381
Iteration 3/1000 | Loss: 0.00342790
Iteration 4/1000 | Loss: 0.00277961
Iteration 5/1000 | Loss: 0.00219000
Iteration 6/1000 | Loss: 0.00160228
Iteration 7/1000 | Loss: 0.00402182
Iteration 8/1000 | Loss: 0.00344603
Iteration 9/1000 | Loss: 0.00579520
Iteration 10/1000 | Loss: 0.00601859
Iteration 11/1000 | Loss: 0.00539417
Iteration 12/1000 | Loss: 0.00932703
Iteration 13/1000 | Loss: 0.01014782
Iteration 14/1000 | Loss: 0.00512569
Iteration 15/1000 | Loss: 0.00327157
Iteration 16/1000 | Loss: 0.00208175
Iteration 17/1000 | Loss: 0.00251231
Iteration 18/1000 | Loss: 0.00282692
Iteration 19/1000 | Loss: 0.00014545
Iteration 20/1000 | Loss: 0.00338067
Iteration 21/1000 | Loss: 0.00265042
Iteration 22/1000 | Loss: 0.00211789
Iteration 23/1000 | Loss: 0.00340474
Iteration 24/1000 | Loss: 0.00157442
Iteration 25/1000 | Loss: 0.00140716
Iteration 26/1000 | Loss: 0.00194969
Iteration 27/1000 | Loss: 0.00147903
Iteration 28/1000 | Loss: 0.00352018
Iteration 29/1000 | Loss: 0.00231618
Iteration 30/1000 | Loss: 0.00215772
Iteration 31/1000 | Loss: 0.00334643
Iteration 32/1000 | Loss: 0.00192980
Iteration 33/1000 | Loss: 0.00175165
Iteration 34/1000 | Loss: 0.00114016
Iteration 35/1000 | Loss: 0.00169758
Iteration 36/1000 | Loss: 0.00335136
Iteration 37/1000 | Loss: 0.00358026
Iteration 38/1000 | Loss: 0.00382790
Iteration 39/1000 | Loss: 0.00304895
Iteration 40/1000 | Loss: 0.00132899
Iteration 41/1000 | Loss: 0.00280012
Iteration 42/1000 | Loss: 0.00174443
Iteration 43/1000 | Loss: 0.00265329
Iteration 44/1000 | Loss: 0.00229147
Iteration 45/1000 | Loss: 0.00178901
Iteration 46/1000 | Loss: 0.00161699
Iteration 47/1000 | Loss: 0.00103569
Iteration 48/1000 | Loss: 0.00253074
Iteration 49/1000 | Loss: 0.00099076
Iteration 50/1000 | Loss: 0.00198510
Iteration 51/1000 | Loss: 0.00201578
Iteration 52/1000 | Loss: 0.00227771
Iteration 53/1000 | Loss: 0.00209865
Iteration 54/1000 | Loss: 0.00186992
Iteration 55/1000 | Loss: 0.00269127
Iteration 56/1000 | Loss: 0.00209278
Iteration 57/1000 | Loss: 0.00142744
Iteration 58/1000 | Loss: 0.00317750
Iteration 59/1000 | Loss: 0.00153854
Iteration 60/1000 | Loss: 0.00053298
Iteration 61/1000 | Loss: 0.00062132
Iteration 62/1000 | Loss: 0.00029411
Iteration 63/1000 | Loss: 0.00061143
Iteration 64/1000 | Loss: 0.00245540
Iteration 65/1000 | Loss: 0.00245685
Iteration 66/1000 | Loss: 0.00240089
Iteration 67/1000 | Loss: 0.00253402
Iteration 68/1000 | Loss: 0.00028288
Iteration 69/1000 | Loss: 0.00152551
Iteration 70/1000 | Loss: 0.00044164
Iteration 71/1000 | Loss: 0.00114346
Iteration 72/1000 | Loss: 0.00155204
Iteration 73/1000 | Loss: 0.00101943
Iteration 74/1000 | Loss: 0.00135593
Iteration 75/1000 | Loss: 0.00079080
Iteration 76/1000 | Loss: 0.00040934
Iteration 77/1000 | Loss: 0.00087566
Iteration 78/1000 | Loss: 0.00030928
Iteration 79/1000 | Loss: 0.00081269
Iteration 80/1000 | Loss: 0.00051035
Iteration 81/1000 | Loss: 0.00068321
Iteration 82/1000 | Loss: 0.00048042
Iteration 83/1000 | Loss: 0.00051940
Iteration 84/1000 | Loss: 0.00045061
Iteration 85/1000 | Loss: 0.00143407
Iteration 86/1000 | Loss: 0.00043188
Iteration 87/1000 | Loss: 0.00136371
Iteration 88/1000 | Loss: 0.00055180
Iteration 89/1000 | Loss: 0.00031148
Iteration 90/1000 | Loss: 0.00007493
Iteration 91/1000 | Loss: 0.00016439
Iteration 92/1000 | Loss: 0.00005446
Iteration 93/1000 | Loss: 0.00004568
Iteration 94/1000 | Loss: 0.00003290
Iteration 95/1000 | Loss: 0.00027230
Iteration 96/1000 | Loss: 0.00006323
Iteration 97/1000 | Loss: 0.00008569
Iteration 98/1000 | Loss: 0.00003148
Iteration 99/1000 | Loss: 0.00002860
Iteration 100/1000 | Loss: 0.00002721
Iteration 101/1000 | Loss: 0.00002650
Iteration 102/1000 | Loss: 0.00005902
Iteration 103/1000 | Loss: 0.00035571
Iteration 104/1000 | Loss: 0.00012180
Iteration 105/1000 | Loss: 0.00020443
Iteration 106/1000 | Loss: 0.00080681
Iteration 107/1000 | Loss: 0.00015786
Iteration 108/1000 | Loss: 0.00003205
Iteration 109/1000 | Loss: 0.00003684
Iteration 110/1000 | Loss: 0.00005777
Iteration 111/1000 | Loss: 0.00002515
Iteration 112/1000 | Loss: 0.00003685
Iteration 113/1000 | Loss: 0.00002895
Iteration 114/1000 | Loss: 0.00040164
Iteration 115/1000 | Loss: 0.00023279
Iteration 116/1000 | Loss: 0.00002978
Iteration 117/1000 | Loss: 0.00038200
Iteration 118/1000 | Loss: 0.00034143
Iteration 119/1000 | Loss: 0.00034747
Iteration 120/1000 | Loss: 0.00003432
Iteration 121/1000 | Loss: 0.00002781
Iteration 122/1000 | Loss: 0.00002436
Iteration 123/1000 | Loss: 0.00002268
Iteration 124/1000 | Loss: 0.00038603
Iteration 125/1000 | Loss: 0.00030356
Iteration 126/1000 | Loss: 0.00003580
Iteration 127/1000 | Loss: 0.00043529
Iteration 128/1000 | Loss: 0.00031302
Iteration 129/1000 | Loss: 0.00031909
Iteration 130/1000 | Loss: 0.00028664
Iteration 131/1000 | Loss: 0.00032063
Iteration 132/1000 | Loss: 0.00004004
Iteration 133/1000 | Loss: 0.00002218
Iteration 134/1000 | Loss: 0.00002059
Iteration 135/1000 | Loss: 0.00002014
Iteration 136/1000 | Loss: 0.00001997
Iteration 137/1000 | Loss: 0.00002726
Iteration 138/1000 | Loss: 0.00002040
Iteration 139/1000 | Loss: 0.00001970
Iteration 140/1000 | Loss: 0.00001970
Iteration 141/1000 | Loss: 0.00001970
Iteration 142/1000 | Loss: 0.00001970
Iteration 143/1000 | Loss: 0.00001970
Iteration 144/1000 | Loss: 0.00001970
Iteration 145/1000 | Loss: 0.00001970
Iteration 146/1000 | Loss: 0.00001970
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00001961
Iteration 149/1000 | Loss: 0.00001959
Iteration 150/1000 | Loss: 0.00001959
Iteration 151/1000 | Loss: 0.00001959
Iteration 152/1000 | Loss: 0.00001959
Iteration 153/1000 | Loss: 0.00001959
Iteration 154/1000 | Loss: 0.00001958
Iteration 155/1000 | Loss: 0.00001958
Iteration 156/1000 | Loss: 0.00001958
Iteration 157/1000 | Loss: 0.00001958
Iteration 158/1000 | Loss: 0.00001958
Iteration 159/1000 | Loss: 0.00001958
Iteration 160/1000 | Loss: 0.00001958
Iteration 161/1000 | Loss: 0.00001958
Iteration 162/1000 | Loss: 0.00001958
Iteration 163/1000 | Loss: 0.00001958
Iteration 164/1000 | Loss: 0.00001958
Iteration 165/1000 | Loss: 0.00001958
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00001957
Iteration 168/1000 | Loss: 0.00001957
Iteration 169/1000 | Loss: 0.00001957
Iteration 170/1000 | Loss: 0.00001957
Iteration 171/1000 | Loss: 0.00001957
Iteration 172/1000 | Loss: 0.00001957
Iteration 173/1000 | Loss: 0.00001957
Iteration 174/1000 | Loss: 0.00001957
Iteration 175/1000 | Loss: 0.00001957
Iteration 176/1000 | Loss: 0.00001956
Iteration 177/1000 | Loss: 0.00001956
Iteration 178/1000 | Loss: 0.00001956
Iteration 179/1000 | Loss: 0.00001956
Iteration 180/1000 | Loss: 0.00001956
Iteration 181/1000 | Loss: 0.00001956
Iteration 182/1000 | Loss: 0.00001956
Iteration 183/1000 | Loss: 0.00001956
Iteration 184/1000 | Loss: 0.00001956
Iteration 185/1000 | Loss: 0.00001956
Iteration 186/1000 | Loss: 0.00001956
Iteration 187/1000 | Loss: 0.00001956
Iteration 188/1000 | Loss: 0.00001955
Iteration 189/1000 | Loss: 0.00001955
Iteration 190/1000 | Loss: 0.00001955
Iteration 191/1000 | Loss: 0.00001955
Iteration 192/1000 | Loss: 0.00001955
Iteration 193/1000 | Loss: 0.00001955
Iteration 194/1000 | Loss: 0.00001955
Iteration 195/1000 | Loss: 0.00001954
Iteration 196/1000 | Loss: 0.00001954
Iteration 197/1000 | Loss: 0.00001954
Iteration 198/1000 | Loss: 0.00001954
Iteration 199/1000 | Loss: 0.00001954
Iteration 200/1000 | Loss: 0.00001954
Iteration 201/1000 | Loss: 0.00001954
Iteration 202/1000 | Loss: 0.00001954
Iteration 203/1000 | Loss: 0.00001954
Iteration 204/1000 | Loss: 0.00001954
Iteration 205/1000 | Loss: 0.00001954
Iteration 206/1000 | Loss: 0.00001954
Iteration 207/1000 | Loss: 0.00001954
Iteration 208/1000 | Loss: 0.00001954
Iteration 209/1000 | Loss: 0.00001954
Iteration 210/1000 | Loss: 0.00001953
Iteration 211/1000 | Loss: 0.00001953
Iteration 212/1000 | Loss: 0.00001953
Iteration 213/1000 | Loss: 0.00001953
Iteration 214/1000 | Loss: 0.00001953
Iteration 215/1000 | Loss: 0.00001953
Iteration 216/1000 | Loss: 0.00001953
Iteration 217/1000 | Loss: 0.00001953
Iteration 218/1000 | Loss: 0.00001952
Iteration 219/1000 | Loss: 0.00001952
Iteration 220/1000 | Loss: 0.00001952
Iteration 221/1000 | Loss: 0.00001952
Iteration 222/1000 | Loss: 0.00001951
Iteration 223/1000 | Loss: 0.00001951
Iteration 224/1000 | Loss: 0.00001951
Iteration 225/1000 | Loss: 0.00001951
Iteration 226/1000 | Loss: 0.00001951
Iteration 227/1000 | Loss: 0.00001951
Iteration 228/1000 | Loss: 0.00001951
Iteration 229/1000 | Loss: 0.00001951
Iteration 230/1000 | Loss: 0.00001951
Iteration 231/1000 | Loss: 0.00001951
Iteration 232/1000 | Loss: 0.00001951
Iteration 233/1000 | Loss: 0.00001951
Iteration 234/1000 | Loss: 0.00001951
Iteration 235/1000 | Loss: 0.00001951
Iteration 236/1000 | Loss: 0.00001951
Iteration 237/1000 | Loss: 0.00001951
Iteration 238/1000 | Loss: 0.00001951
Iteration 239/1000 | Loss: 0.00001951
Iteration 240/1000 | Loss: 0.00001951
Iteration 241/1000 | Loss: 0.00001950
Iteration 242/1000 | Loss: 0.00001950
Iteration 243/1000 | Loss: 0.00001950
Iteration 244/1000 | Loss: 0.00001950
Iteration 245/1000 | Loss: 0.00001950
Iteration 246/1000 | Loss: 0.00001950
Iteration 247/1000 | Loss: 0.00001950
Iteration 248/1000 | Loss: 0.00001950
Iteration 249/1000 | Loss: 0.00001950
Iteration 250/1000 | Loss: 0.00001950
Iteration 251/1000 | Loss: 0.00003909
Iteration 252/1000 | Loss: 0.00003908
Iteration 253/1000 | Loss: 0.00001969
Iteration 254/1000 | Loss: 0.00001948
Iteration 255/1000 | Loss: 0.00001948
Iteration 256/1000 | Loss: 0.00001948
Iteration 257/1000 | Loss: 0.00001948
Iteration 258/1000 | Loss: 0.00001948
Iteration 259/1000 | Loss: 0.00001947
Iteration 260/1000 | Loss: 0.00001947
Iteration 261/1000 | Loss: 0.00001947
Iteration 262/1000 | Loss: 0.00001947
Iteration 263/1000 | Loss: 0.00001947
Iteration 264/1000 | Loss: 0.00001946
Iteration 265/1000 | Loss: 0.00001946
Iteration 266/1000 | Loss: 0.00001946
Iteration 267/1000 | Loss: 0.00001946
Iteration 268/1000 | Loss: 0.00001946
Iteration 269/1000 | Loss: 0.00001946
Iteration 270/1000 | Loss: 0.00001945
Iteration 271/1000 | Loss: 0.00001945
Iteration 272/1000 | Loss: 0.00001945
Iteration 273/1000 | Loss: 0.00001945
Iteration 274/1000 | Loss: 0.00001945
Iteration 275/1000 | Loss: 0.00001945
Iteration 276/1000 | Loss: 0.00001945
Iteration 277/1000 | Loss: 0.00001945
Iteration 278/1000 | Loss: 0.00001945
Iteration 279/1000 | Loss: 0.00001945
Iteration 280/1000 | Loss: 0.00001945
Iteration 281/1000 | Loss: 0.00001945
Iteration 282/1000 | Loss: 0.00001945
Iteration 283/1000 | Loss: 0.00001945
Iteration 284/1000 | Loss: 0.00001945
Iteration 285/1000 | Loss: 0.00001945
Iteration 286/1000 | Loss: 0.00001945
Iteration 287/1000 | Loss: 0.00001945
Iteration 288/1000 | Loss: 0.00001945
Iteration 289/1000 | Loss: 0.00001944
Iteration 290/1000 | Loss: 0.00001944
Iteration 291/1000 | Loss: 0.00001944
Iteration 292/1000 | Loss: 0.00001944
Iteration 293/1000 | Loss: 0.00001944
Iteration 294/1000 | Loss: 0.00001944
Iteration 295/1000 | Loss: 0.00001944
Iteration 296/1000 | Loss: 0.00001944
Iteration 297/1000 | Loss: 0.00001944
Iteration 298/1000 | Loss: 0.00001944
Iteration 299/1000 | Loss: 0.00001944
Iteration 300/1000 | Loss: 0.00001944
Iteration 301/1000 | Loss: 0.00001944
Iteration 302/1000 | Loss: 0.00001944
Iteration 303/1000 | Loss: 0.00001944
Iteration 304/1000 | Loss: 0.00001944
Iteration 305/1000 | Loss: 0.00001944
Iteration 306/1000 | Loss: 0.00001944
Iteration 307/1000 | Loss: 0.00001944
Iteration 308/1000 | Loss: 0.00001944
Iteration 309/1000 | Loss: 0.00001944
Iteration 310/1000 | Loss: 0.00001944
Iteration 311/1000 | Loss: 0.00001944
Iteration 312/1000 | Loss: 0.00001944
Iteration 313/1000 | Loss: 0.00001944
Iteration 314/1000 | Loss: 0.00001944
Iteration 315/1000 | Loss: 0.00001944
Iteration 316/1000 | Loss: 0.00001944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 316. Stopping optimization.
Last 5 losses: [1.943747338373214e-05, 1.943747338373214e-05, 1.943747338373214e-05, 1.943747338373214e-05, 1.943747338373214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.943747338373214e-05

Optimization complete. Final v2v error: 3.640383005142212 mm

Highest mean error: 9.825236320495605 mm for frame 68

Lowest mean error: 3.2150914669036865 mm for frame 148

Saving results

Total time: 248.11940503120422
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00752908
Iteration 2/25 | Loss: 0.00176911
Iteration 3/25 | Loss: 0.00141016
Iteration 4/25 | Loss: 0.00135146
Iteration 5/25 | Loss: 0.00134285
Iteration 6/25 | Loss: 0.00134065
Iteration 7/25 | Loss: 0.00134065
Iteration 8/25 | Loss: 0.00134065
Iteration 9/25 | Loss: 0.00134065
Iteration 10/25 | Loss: 0.00134065
Iteration 11/25 | Loss: 0.00134065
Iteration 12/25 | Loss: 0.00134065
Iteration 13/25 | Loss: 0.00134065
Iteration 14/25 | Loss: 0.00134065
Iteration 15/25 | Loss: 0.00134065
Iteration 16/25 | Loss: 0.00134065
Iteration 17/25 | Loss: 0.00134065
Iteration 18/25 | Loss: 0.00134065
Iteration 19/25 | Loss: 0.00134065
Iteration 20/25 | Loss: 0.00134065
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001340649090707302, 0.001340649090707302, 0.001340649090707302, 0.001340649090707302, 0.001340649090707302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001340649090707302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19166541
Iteration 2/25 | Loss: 0.00426930
Iteration 3/25 | Loss: 0.00426928
Iteration 4/25 | Loss: 0.00426928
Iteration 5/25 | Loss: 0.00426928
Iteration 6/25 | Loss: 0.00426928
Iteration 7/25 | Loss: 0.00426928
Iteration 8/25 | Loss: 0.00426928
Iteration 9/25 | Loss: 0.00426927
Iteration 10/25 | Loss: 0.00426927
Iteration 11/25 | Loss: 0.00426927
Iteration 12/25 | Loss: 0.00426928
Iteration 13/25 | Loss: 0.00426928
Iteration 14/25 | Loss: 0.00426927
Iteration 15/25 | Loss: 0.00426927
Iteration 16/25 | Loss: 0.00426927
Iteration 17/25 | Loss: 0.00426927
Iteration 18/25 | Loss: 0.00426927
Iteration 19/25 | Loss: 0.00426927
Iteration 20/25 | Loss: 0.00426927
Iteration 21/25 | Loss: 0.00426927
Iteration 22/25 | Loss: 0.00426927
Iteration 23/25 | Loss: 0.00426927
Iteration 24/25 | Loss: 0.00426927
Iteration 25/25 | Loss: 0.00426927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00426927
Iteration 2/1000 | Loss: 0.00005647
Iteration 3/1000 | Loss: 0.00003932
Iteration 4/1000 | Loss: 0.00003231
Iteration 5/1000 | Loss: 0.00002974
Iteration 6/1000 | Loss: 0.00002835
Iteration 7/1000 | Loss: 0.00002716
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00002571
Iteration 10/1000 | Loss: 0.00002532
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002483
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00002457
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002445
Iteration 17/1000 | Loss: 0.00002442
Iteration 18/1000 | Loss: 0.00002441
Iteration 19/1000 | Loss: 0.00002441
Iteration 20/1000 | Loss: 0.00002437
Iteration 21/1000 | Loss: 0.00002433
Iteration 22/1000 | Loss: 0.00002428
Iteration 23/1000 | Loss: 0.00002428
Iteration 24/1000 | Loss: 0.00002427
Iteration 25/1000 | Loss: 0.00002427
Iteration 26/1000 | Loss: 0.00002426
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00002425
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002424
Iteration 31/1000 | Loss: 0.00002424
Iteration 32/1000 | Loss: 0.00002423
Iteration 33/1000 | Loss: 0.00002423
Iteration 34/1000 | Loss: 0.00002423
Iteration 35/1000 | Loss: 0.00002422
Iteration 36/1000 | Loss: 0.00002422
Iteration 37/1000 | Loss: 0.00002421
Iteration 38/1000 | Loss: 0.00002421
Iteration 39/1000 | Loss: 0.00002420
Iteration 40/1000 | Loss: 0.00002420
Iteration 41/1000 | Loss: 0.00002420
Iteration 42/1000 | Loss: 0.00002420
Iteration 43/1000 | Loss: 0.00002420
Iteration 44/1000 | Loss: 0.00002419
Iteration 45/1000 | Loss: 0.00002419
Iteration 46/1000 | Loss: 0.00002419
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002418
Iteration 49/1000 | Loss: 0.00002417
Iteration 50/1000 | Loss: 0.00002417
Iteration 51/1000 | Loss: 0.00002416
Iteration 52/1000 | Loss: 0.00002416
Iteration 53/1000 | Loss: 0.00002416
Iteration 54/1000 | Loss: 0.00002416
Iteration 55/1000 | Loss: 0.00002415
Iteration 56/1000 | Loss: 0.00002415
Iteration 57/1000 | Loss: 0.00002415
Iteration 58/1000 | Loss: 0.00002414
Iteration 59/1000 | Loss: 0.00002414
Iteration 60/1000 | Loss: 0.00002414
Iteration 61/1000 | Loss: 0.00002413
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002413
Iteration 65/1000 | Loss: 0.00002413
Iteration 66/1000 | Loss: 0.00002413
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002412
Iteration 69/1000 | Loss: 0.00002412
Iteration 70/1000 | Loss: 0.00002412
Iteration 71/1000 | Loss: 0.00002412
Iteration 72/1000 | Loss: 0.00002412
Iteration 73/1000 | Loss: 0.00002412
Iteration 74/1000 | Loss: 0.00002412
Iteration 75/1000 | Loss: 0.00002411
Iteration 76/1000 | Loss: 0.00002411
Iteration 77/1000 | Loss: 0.00002411
Iteration 78/1000 | Loss: 0.00002411
Iteration 79/1000 | Loss: 0.00002411
Iteration 80/1000 | Loss: 0.00002411
Iteration 81/1000 | Loss: 0.00002411
Iteration 82/1000 | Loss: 0.00002410
Iteration 83/1000 | Loss: 0.00002410
Iteration 84/1000 | Loss: 0.00002410
Iteration 85/1000 | Loss: 0.00002410
Iteration 86/1000 | Loss: 0.00002409
Iteration 87/1000 | Loss: 0.00002409
Iteration 88/1000 | Loss: 0.00002409
Iteration 89/1000 | Loss: 0.00002409
Iteration 90/1000 | Loss: 0.00002408
Iteration 91/1000 | Loss: 0.00002408
Iteration 92/1000 | Loss: 0.00002408
Iteration 93/1000 | Loss: 0.00002408
Iteration 94/1000 | Loss: 0.00002408
Iteration 95/1000 | Loss: 0.00002407
Iteration 96/1000 | Loss: 0.00002407
Iteration 97/1000 | Loss: 0.00002407
Iteration 98/1000 | Loss: 0.00002407
Iteration 99/1000 | Loss: 0.00002407
Iteration 100/1000 | Loss: 0.00002407
Iteration 101/1000 | Loss: 0.00002406
Iteration 102/1000 | Loss: 0.00002406
Iteration 103/1000 | Loss: 0.00002406
Iteration 104/1000 | Loss: 0.00002406
Iteration 105/1000 | Loss: 0.00002406
Iteration 106/1000 | Loss: 0.00002406
Iteration 107/1000 | Loss: 0.00002406
Iteration 108/1000 | Loss: 0.00002406
Iteration 109/1000 | Loss: 0.00002406
Iteration 110/1000 | Loss: 0.00002406
Iteration 111/1000 | Loss: 0.00002405
Iteration 112/1000 | Loss: 0.00002405
Iteration 113/1000 | Loss: 0.00002405
Iteration 114/1000 | Loss: 0.00002405
Iteration 115/1000 | Loss: 0.00002404
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002404
Iteration 119/1000 | Loss: 0.00002404
Iteration 120/1000 | Loss: 0.00002404
Iteration 121/1000 | Loss: 0.00002404
Iteration 122/1000 | Loss: 0.00002403
Iteration 123/1000 | Loss: 0.00002403
Iteration 124/1000 | Loss: 0.00002403
Iteration 125/1000 | Loss: 0.00002403
Iteration 126/1000 | Loss: 0.00002402
Iteration 127/1000 | Loss: 0.00002402
Iteration 128/1000 | Loss: 0.00002402
Iteration 129/1000 | Loss: 0.00002402
Iteration 130/1000 | Loss: 0.00002402
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002402
Iteration 133/1000 | Loss: 0.00002402
Iteration 134/1000 | Loss: 0.00002402
Iteration 135/1000 | Loss: 0.00002402
Iteration 136/1000 | Loss: 0.00002401
Iteration 137/1000 | Loss: 0.00002401
Iteration 138/1000 | Loss: 0.00002401
Iteration 139/1000 | Loss: 0.00002401
Iteration 140/1000 | Loss: 0.00002401
Iteration 141/1000 | Loss: 0.00002401
Iteration 142/1000 | Loss: 0.00002401
Iteration 143/1000 | Loss: 0.00002401
Iteration 144/1000 | Loss: 0.00002400
Iteration 145/1000 | Loss: 0.00002400
Iteration 146/1000 | Loss: 0.00002400
Iteration 147/1000 | Loss: 0.00002400
Iteration 148/1000 | Loss: 0.00002400
Iteration 149/1000 | Loss: 0.00002400
Iteration 150/1000 | Loss: 0.00002400
Iteration 151/1000 | Loss: 0.00002400
Iteration 152/1000 | Loss: 0.00002400
Iteration 153/1000 | Loss: 0.00002400
Iteration 154/1000 | Loss: 0.00002400
Iteration 155/1000 | Loss: 0.00002400
Iteration 156/1000 | Loss: 0.00002400
Iteration 157/1000 | Loss: 0.00002400
Iteration 158/1000 | Loss: 0.00002400
Iteration 159/1000 | Loss: 0.00002400
Iteration 160/1000 | Loss: 0.00002400
Iteration 161/1000 | Loss: 0.00002400
Iteration 162/1000 | Loss: 0.00002400
Iteration 163/1000 | Loss: 0.00002400
Iteration 164/1000 | Loss: 0.00002400
Iteration 165/1000 | Loss: 0.00002400
Iteration 166/1000 | Loss: 0.00002400
Iteration 167/1000 | Loss: 0.00002400
Iteration 168/1000 | Loss: 0.00002400
Iteration 169/1000 | Loss: 0.00002400
Iteration 170/1000 | Loss: 0.00002400
Iteration 171/1000 | Loss: 0.00002400
Iteration 172/1000 | Loss: 0.00002400
Iteration 173/1000 | Loss: 0.00002400
Iteration 174/1000 | Loss: 0.00002400
Iteration 175/1000 | Loss: 0.00002400
Iteration 176/1000 | Loss: 0.00002400
Iteration 177/1000 | Loss: 0.00002400
Iteration 178/1000 | Loss: 0.00002400
Iteration 179/1000 | Loss: 0.00002400
Iteration 180/1000 | Loss: 0.00002400
Iteration 181/1000 | Loss: 0.00002400
Iteration 182/1000 | Loss: 0.00002400
Iteration 183/1000 | Loss: 0.00002400
Iteration 184/1000 | Loss: 0.00002400
Iteration 185/1000 | Loss: 0.00002400
Iteration 186/1000 | Loss: 0.00002400
Iteration 187/1000 | Loss: 0.00002400
Iteration 188/1000 | Loss: 0.00002400
Iteration 189/1000 | Loss: 0.00002400
Iteration 190/1000 | Loss: 0.00002400
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002400
Iteration 193/1000 | Loss: 0.00002400
Iteration 194/1000 | Loss: 0.00002400
Iteration 195/1000 | Loss: 0.00002400
Iteration 196/1000 | Loss: 0.00002400
Iteration 197/1000 | Loss: 0.00002400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.400246012257412e-05, 2.400246012257412e-05, 2.400246012257412e-05, 2.400246012257412e-05, 2.400246012257412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.400246012257412e-05

Optimization complete. Final v2v error: 4.118703365325928 mm

Highest mean error: 5.142200946807861 mm for frame 96

Lowest mean error: 3.4777257442474365 mm for frame 55

Saving results

Total time: 49.2295823097229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154084
Iteration 2/25 | Loss: 0.01154084
Iteration 3/25 | Loss: 0.01154084
Iteration 4/25 | Loss: 0.01154084
Iteration 5/25 | Loss: 0.01154084
Iteration 6/25 | Loss: 0.01154084
Iteration 7/25 | Loss: 0.01154084
Iteration 8/25 | Loss: 0.01154083
Iteration 9/25 | Loss: 0.01154083
Iteration 10/25 | Loss: 0.01154083
Iteration 11/25 | Loss: 0.01154083
Iteration 12/25 | Loss: 0.01154083
Iteration 13/25 | Loss: 0.01154083
Iteration 14/25 | Loss: 0.01154083
Iteration 15/25 | Loss: 0.01154083
Iteration 16/25 | Loss: 0.01154083
Iteration 17/25 | Loss: 0.01154083
Iteration 18/25 | Loss: 0.01154083
Iteration 19/25 | Loss: 0.01154083
Iteration 20/25 | Loss: 0.01154083
Iteration 21/25 | Loss: 0.01154082
Iteration 22/25 | Loss: 0.01154082
Iteration 23/25 | Loss: 0.01154082
Iteration 24/25 | Loss: 0.01154082
Iteration 25/25 | Loss: 0.01154082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13575578
Iteration 2/25 | Loss: 0.10594542
Iteration 3/25 | Loss: 0.10343784
Iteration 4/25 | Loss: 0.10343782
Iteration 5/25 | Loss: 0.10343782
Iteration 6/25 | Loss: 0.10343782
Iteration 7/25 | Loss: 0.10343781
Iteration 8/25 | Loss: 0.10343781
Iteration 9/25 | Loss: 0.10343781
Iteration 10/25 | Loss: 0.10343780
Iteration 11/25 | Loss: 0.10343780
Iteration 12/25 | Loss: 0.10343780
Iteration 13/25 | Loss: 0.10343780
Iteration 14/25 | Loss: 0.10343780
Iteration 15/25 | Loss: 0.10343780
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.10343780368566513, 0.10343780368566513, 0.10343780368566513, 0.10343780368566513, 0.10343780368566513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10343780368566513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10343780
Iteration 2/1000 | Loss: 0.00450395
Iteration 3/1000 | Loss: 0.00883146
Iteration 4/1000 | Loss: 0.00216222
Iteration 5/1000 | Loss: 0.00528468
Iteration 6/1000 | Loss: 0.00450245
Iteration 7/1000 | Loss: 0.00027412
Iteration 8/1000 | Loss: 0.00052300
Iteration 9/1000 | Loss: 0.00008863
Iteration 10/1000 | Loss: 0.00042574
Iteration 11/1000 | Loss: 0.00083058
Iteration 12/1000 | Loss: 0.00029879
Iteration 13/1000 | Loss: 0.00015080
Iteration 14/1000 | Loss: 0.00008890
Iteration 15/1000 | Loss: 0.00008725
Iteration 16/1000 | Loss: 0.00019397
Iteration 17/1000 | Loss: 0.00012741
Iteration 18/1000 | Loss: 0.00025986
Iteration 19/1000 | Loss: 0.00010097
Iteration 20/1000 | Loss: 0.00009468
Iteration 21/1000 | Loss: 0.00007329
Iteration 22/1000 | Loss: 0.00009565
Iteration 23/1000 | Loss: 0.00034681
Iteration 24/1000 | Loss: 0.00004969
Iteration 25/1000 | Loss: 0.00012861
Iteration 26/1000 | Loss: 0.00053401
Iteration 27/1000 | Loss: 0.00019952
Iteration 28/1000 | Loss: 0.00004303
Iteration 29/1000 | Loss: 0.00003274
Iteration 30/1000 | Loss: 0.00003763
Iteration 31/1000 | Loss: 0.00003789
Iteration 32/1000 | Loss: 0.00006315
Iteration 33/1000 | Loss: 0.00003260
Iteration 34/1000 | Loss: 0.00024164
Iteration 35/1000 | Loss: 0.00005930
Iteration 36/1000 | Loss: 0.00003141
Iteration 37/1000 | Loss: 0.00004552
Iteration 38/1000 | Loss: 0.00003516
Iteration 39/1000 | Loss: 0.00002998
Iteration 40/1000 | Loss: 0.00004117
Iteration 41/1000 | Loss: 0.00003609
Iteration 42/1000 | Loss: 0.00008412
Iteration 43/1000 | Loss: 0.00003769
Iteration 44/1000 | Loss: 0.00003198
Iteration 45/1000 | Loss: 0.00003196
Iteration 46/1000 | Loss: 0.00002483
Iteration 47/1000 | Loss: 0.00002903
Iteration 48/1000 | Loss: 0.00003473
Iteration 49/1000 | Loss: 0.00002883
Iteration 50/1000 | Loss: 0.00003387
Iteration 51/1000 | Loss: 0.00003063
Iteration 52/1000 | Loss: 0.00003474
Iteration 53/1000 | Loss: 0.00023588
Iteration 54/1000 | Loss: 0.00003890
Iteration 55/1000 | Loss: 0.00003563
Iteration 56/1000 | Loss: 0.00008684
Iteration 57/1000 | Loss: 0.00002798
Iteration 58/1000 | Loss: 0.00002985
Iteration 59/1000 | Loss: 0.00002452
Iteration 60/1000 | Loss: 0.00002435
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002435
Iteration 63/1000 | Loss: 0.00002851
Iteration 64/1000 | Loss: 0.00002632
Iteration 65/1000 | Loss: 0.00002478
Iteration 66/1000 | Loss: 0.00002862
Iteration 67/1000 | Loss: 0.00002910
Iteration 68/1000 | Loss: 0.00002668
Iteration 69/1000 | Loss: 0.00002467
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00003530
Iteration 72/1000 | Loss: 0.00002754
Iteration 73/1000 | Loss: 0.00004213
Iteration 74/1000 | Loss: 0.00002440
Iteration 75/1000 | Loss: 0.00002476
Iteration 76/1000 | Loss: 0.00003656
Iteration 77/1000 | Loss: 0.00005869
Iteration 78/1000 | Loss: 0.00002763
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00003995
Iteration 81/1000 | Loss: 0.00002537
Iteration 82/1000 | Loss: 0.00002537
Iteration 83/1000 | Loss: 0.00004889
Iteration 84/1000 | Loss: 0.00002574
Iteration 85/1000 | Loss: 0.00003091
Iteration 86/1000 | Loss: 0.00002665
Iteration 87/1000 | Loss: 0.00002446
Iteration 88/1000 | Loss: 0.00002419
Iteration 89/1000 | Loss: 0.00002419
Iteration 90/1000 | Loss: 0.00002419
Iteration 91/1000 | Loss: 0.00002419
Iteration 92/1000 | Loss: 0.00002419
Iteration 93/1000 | Loss: 0.00002419
Iteration 94/1000 | Loss: 0.00002419
Iteration 95/1000 | Loss: 0.00002419
Iteration 96/1000 | Loss: 0.00002419
Iteration 97/1000 | Loss: 0.00002419
Iteration 98/1000 | Loss: 0.00002419
Iteration 99/1000 | Loss: 0.00002419
Iteration 100/1000 | Loss: 0.00002419
Iteration 101/1000 | Loss: 0.00002419
Iteration 102/1000 | Loss: 0.00002419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.4189914256567135e-05, 2.4189914256567135e-05, 2.4189914256567135e-05, 2.4189914256567135e-05, 2.4189914256567135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4189914256567135e-05

Optimization complete. Final v2v error: 4.2335896492004395 mm

Highest mean error: 4.5825886726379395 mm for frame 111

Lowest mean error: 3.9217700958251953 mm for frame 190

Saving results

Total time: 123.33567118644714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884142
Iteration 2/25 | Loss: 0.00184537
Iteration 3/25 | Loss: 0.00147341
Iteration 4/25 | Loss: 0.00140764
Iteration 5/25 | Loss: 0.00139561
Iteration 6/25 | Loss: 0.00139374
Iteration 7/25 | Loss: 0.00139322
Iteration 8/25 | Loss: 0.00139305
Iteration 9/25 | Loss: 0.00139305
Iteration 10/25 | Loss: 0.00139305
Iteration 11/25 | Loss: 0.00139305
Iteration 12/25 | Loss: 0.00139305
Iteration 13/25 | Loss: 0.00139305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013930494897067547, 0.0013930494897067547, 0.0013930494897067547, 0.0013930494897067547, 0.0013930494897067547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013930494897067547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72738898
Iteration 2/25 | Loss: 0.00630296
Iteration 3/25 | Loss: 0.00630296
Iteration 4/25 | Loss: 0.00630296
Iteration 5/25 | Loss: 0.00630296
Iteration 6/25 | Loss: 0.00630296
Iteration 7/25 | Loss: 0.00630296
Iteration 8/25 | Loss: 0.00630296
Iteration 9/25 | Loss: 0.00630296
Iteration 10/25 | Loss: 0.00630296
Iteration 11/25 | Loss: 0.00630296
Iteration 12/25 | Loss: 0.00630296
Iteration 13/25 | Loss: 0.00630296
Iteration 14/25 | Loss: 0.00630296
Iteration 15/25 | Loss: 0.00630296
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006302959751337767, 0.006302959751337767, 0.006302959751337767, 0.006302959751337767, 0.006302959751337767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006302959751337767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00630296
Iteration 2/1000 | Loss: 0.00008053
Iteration 3/1000 | Loss: 0.00005223
Iteration 4/1000 | Loss: 0.00004308
Iteration 5/1000 | Loss: 0.00004007
Iteration 6/1000 | Loss: 0.00003779
Iteration 7/1000 | Loss: 0.00003608
Iteration 8/1000 | Loss: 0.00003470
Iteration 9/1000 | Loss: 0.00003386
Iteration 10/1000 | Loss: 0.00022791
Iteration 11/1000 | Loss: 0.00003924
Iteration 12/1000 | Loss: 0.00003571
Iteration 13/1000 | Loss: 0.00003284
Iteration 14/1000 | Loss: 0.00003177
Iteration 15/1000 | Loss: 0.00003139
Iteration 16/1000 | Loss: 0.00003115
Iteration 17/1000 | Loss: 0.00003087
Iteration 18/1000 | Loss: 0.00025653
Iteration 19/1000 | Loss: 0.00003869
Iteration 20/1000 | Loss: 0.00003627
Iteration 21/1000 | Loss: 0.00003251
Iteration 22/1000 | Loss: 0.00003053
Iteration 23/1000 | Loss: 0.00002999
Iteration 24/1000 | Loss: 0.00002957
Iteration 25/1000 | Loss: 0.00002929
Iteration 26/1000 | Loss: 0.00002902
Iteration 27/1000 | Loss: 0.00002881
Iteration 28/1000 | Loss: 0.00002862
Iteration 29/1000 | Loss: 0.00002853
Iteration 30/1000 | Loss: 0.00002834
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00002821
Iteration 33/1000 | Loss: 0.00002820
Iteration 34/1000 | Loss: 0.00002817
Iteration 35/1000 | Loss: 0.00002810
Iteration 36/1000 | Loss: 0.00002806
Iteration 37/1000 | Loss: 0.00002806
Iteration 38/1000 | Loss: 0.00002806
Iteration 39/1000 | Loss: 0.00002802
Iteration 40/1000 | Loss: 0.00002802
Iteration 41/1000 | Loss: 0.00002802
Iteration 42/1000 | Loss: 0.00002801
Iteration 43/1000 | Loss: 0.00002801
Iteration 44/1000 | Loss: 0.00002801
Iteration 45/1000 | Loss: 0.00002801
Iteration 46/1000 | Loss: 0.00002801
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002801
Iteration 49/1000 | Loss: 0.00002801
Iteration 50/1000 | Loss: 0.00002800
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002800
Iteration 53/1000 | Loss: 0.00002800
Iteration 54/1000 | Loss: 0.00002800
Iteration 55/1000 | Loss: 0.00002800
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002800
Iteration 58/1000 | Loss: 0.00002800
Iteration 59/1000 | Loss: 0.00002800
Iteration 60/1000 | Loss: 0.00002800
Iteration 61/1000 | Loss: 0.00002799
Iteration 62/1000 | Loss: 0.00002799
Iteration 63/1000 | Loss: 0.00002799
Iteration 64/1000 | Loss: 0.00002799
Iteration 65/1000 | Loss: 0.00002799
Iteration 66/1000 | Loss: 0.00002799
Iteration 67/1000 | Loss: 0.00002798
Iteration 68/1000 | Loss: 0.00002798
Iteration 69/1000 | Loss: 0.00002798
Iteration 70/1000 | Loss: 0.00002798
Iteration 71/1000 | Loss: 0.00002798
Iteration 72/1000 | Loss: 0.00002798
Iteration 73/1000 | Loss: 0.00002798
Iteration 74/1000 | Loss: 0.00002798
Iteration 75/1000 | Loss: 0.00002798
Iteration 76/1000 | Loss: 0.00002798
Iteration 77/1000 | Loss: 0.00002798
Iteration 78/1000 | Loss: 0.00002798
Iteration 79/1000 | Loss: 0.00002797
Iteration 80/1000 | Loss: 0.00002797
Iteration 81/1000 | Loss: 0.00002797
Iteration 82/1000 | Loss: 0.00002797
Iteration 83/1000 | Loss: 0.00002797
Iteration 84/1000 | Loss: 0.00002797
Iteration 85/1000 | Loss: 0.00002797
Iteration 86/1000 | Loss: 0.00002797
Iteration 87/1000 | Loss: 0.00002797
Iteration 88/1000 | Loss: 0.00002797
Iteration 89/1000 | Loss: 0.00002797
Iteration 90/1000 | Loss: 0.00002797
Iteration 91/1000 | Loss: 0.00002797
Iteration 92/1000 | Loss: 0.00002797
Iteration 93/1000 | Loss: 0.00002797
Iteration 94/1000 | Loss: 0.00002797
Iteration 95/1000 | Loss: 0.00002797
Iteration 96/1000 | Loss: 0.00002797
Iteration 97/1000 | Loss: 0.00002797
Iteration 98/1000 | Loss: 0.00002797
Iteration 99/1000 | Loss: 0.00002797
Iteration 100/1000 | Loss: 0.00002797
Iteration 101/1000 | Loss: 0.00002797
Iteration 102/1000 | Loss: 0.00002796
Iteration 103/1000 | Loss: 0.00002796
Iteration 104/1000 | Loss: 0.00002796
Iteration 105/1000 | Loss: 0.00002796
Iteration 106/1000 | Loss: 0.00002796
Iteration 107/1000 | Loss: 0.00002796
Iteration 108/1000 | Loss: 0.00002796
Iteration 109/1000 | Loss: 0.00002796
Iteration 110/1000 | Loss: 0.00002796
Iteration 111/1000 | Loss: 0.00002796
Iteration 112/1000 | Loss: 0.00002796
Iteration 113/1000 | Loss: 0.00002796
Iteration 114/1000 | Loss: 0.00002796
Iteration 115/1000 | Loss: 0.00002796
Iteration 116/1000 | Loss: 0.00002796
Iteration 117/1000 | Loss: 0.00002796
Iteration 118/1000 | Loss: 0.00002796
Iteration 119/1000 | Loss: 0.00002796
Iteration 120/1000 | Loss: 0.00002796
Iteration 121/1000 | Loss: 0.00002796
Iteration 122/1000 | Loss: 0.00002796
Iteration 123/1000 | Loss: 0.00002796
Iteration 124/1000 | Loss: 0.00002796
Iteration 125/1000 | Loss: 0.00002796
Iteration 126/1000 | Loss: 0.00002796
Iteration 127/1000 | Loss: 0.00002796
Iteration 128/1000 | Loss: 0.00002796
Iteration 129/1000 | Loss: 0.00002796
Iteration 130/1000 | Loss: 0.00002796
Iteration 131/1000 | Loss: 0.00002796
Iteration 132/1000 | Loss: 0.00002796
Iteration 133/1000 | Loss: 0.00002796
Iteration 134/1000 | Loss: 0.00002796
Iteration 135/1000 | Loss: 0.00002796
Iteration 136/1000 | Loss: 0.00002796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.7963635147898458e-05, 2.7963635147898458e-05, 2.7963635147898458e-05, 2.7963635147898458e-05, 2.7963635147898458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7963635147898458e-05

Optimization complete. Final v2v error: 4.586667537689209 mm

Highest mean error: 5.364346504211426 mm for frame 65

Lowest mean error: 4.025671482086182 mm for frame 189

Saving results

Total time: 69.71543860435486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426559
Iteration 2/25 | Loss: 0.00143357
Iteration 3/25 | Loss: 0.00129474
Iteration 4/25 | Loss: 0.00128358
Iteration 5/25 | Loss: 0.00127754
Iteration 6/25 | Loss: 0.00127618
Iteration 7/25 | Loss: 0.00127618
Iteration 8/25 | Loss: 0.00127618
Iteration 9/25 | Loss: 0.00127618
Iteration 10/25 | Loss: 0.00127618
Iteration 11/25 | Loss: 0.00127618
Iteration 12/25 | Loss: 0.00127618
Iteration 13/25 | Loss: 0.00127618
Iteration 14/25 | Loss: 0.00127618
Iteration 15/25 | Loss: 0.00127618
Iteration 16/25 | Loss: 0.00127618
Iteration 17/25 | Loss: 0.00127618
Iteration 18/25 | Loss: 0.00127618
Iteration 19/25 | Loss: 0.00127618
Iteration 20/25 | Loss: 0.00127618
Iteration 21/25 | Loss: 0.00127618
Iteration 22/25 | Loss: 0.00127618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012761821271851659, 0.0012761821271851659, 0.0012761821271851659, 0.0012761821271851659, 0.0012761821271851659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012761821271851659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96408081
Iteration 2/25 | Loss: 0.00587888
Iteration 3/25 | Loss: 0.00587888
Iteration 4/25 | Loss: 0.00587888
Iteration 5/25 | Loss: 0.00587888
Iteration 6/25 | Loss: 0.00587888
Iteration 7/25 | Loss: 0.00587888
Iteration 8/25 | Loss: 0.00587888
Iteration 9/25 | Loss: 0.00587887
Iteration 10/25 | Loss: 0.00587887
Iteration 11/25 | Loss: 0.00587887
Iteration 12/25 | Loss: 0.00587887
Iteration 13/25 | Loss: 0.00587887
Iteration 14/25 | Loss: 0.00587887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.005878874566406012, 0.005878874566406012, 0.005878874566406012, 0.005878874566406012, 0.005878874566406012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005878874566406012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00587887
Iteration 2/1000 | Loss: 0.00004611
Iteration 3/1000 | Loss: 0.00002867
Iteration 4/1000 | Loss: 0.00002408
Iteration 5/1000 | Loss: 0.00002228
Iteration 6/1000 | Loss: 0.00002087
Iteration 7/1000 | Loss: 0.00002033
Iteration 8/1000 | Loss: 0.00001984
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001915
Iteration 11/1000 | Loss: 0.00001888
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001836
Iteration 16/1000 | Loss: 0.00001833
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001822
Iteration 19/1000 | Loss: 0.00001815
Iteration 20/1000 | Loss: 0.00001815
Iteration 21/1000 | Loss: 0.00001813
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001812
Iteration 24/1000 | Loss: 0.00001807
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001805
Iteration 27/1000 | Loss: 0.00001805
Iteration 28/1000 | Loss: 0.00001804
Iteration 29/1000 | Loss: 0.00001803
Iteration 30/1000 | Loss: 0.00001803
Iteration 31/1000 | Loss: 0.00001803
Iteration 32/1000 | Loss: 0.00001802
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001801
Iteration 36/1000 | Loss: 0.00001800
Iteration 37/1000 | Loss: 0.00001800
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001798
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001797
Iteration 44/1000 | Loss: 0.00001796
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001795
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001793
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001792
Iteration 58/1000 | Loss: 0.00001791
Iteration 59/1000 | Loss: 0.00001791
Iteration 60/1000 | Loss: 0.00001791
Iteration 61/1000 | Loss: 0.00001790
Iteration 62/1000 | Loss: 0.00001789
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001789
Iteration 65/1000 | Loss: 0.00001789
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001788
Iteration 70/1000 | Loss: 0.00001788
Iteration 71/1000 | Loss: 0.00001788
Iteration 72/1000 | Loss: 0.00001788
Iteration 73/1000 | Loss: 0.00001788
Iteration 74/1000 | Loss: 0.00001788
Iteration 75/1000 | Loss: 0.00001787
Iteration 76/1000 | Loss: 0.00001787
Iteration 77/1000 | Loss: 0.00001787
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001784
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001783
Iteration 101/1000 | Loss: 0.00001783
Iteration 102/1000 | Loss: 0.00001783
Iteration 103/1000 | Loss: 0.00001783
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001780
Iteration 121/1000 | Loss: 0.00001780
Iteration 122/1000 | Loss: 0.00001780
Iteration 123/1000 | Loss: 0.00001780
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001780
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Iteration 147/1000 | Loss: 0.00001780
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.780490856617689e-05, 1.780490856617689e-05, 1.780490856617689e-05, 1.780490856617689e-05, 1.780490856617689e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.780490856617689e-05

Optimization complete. Final v2v error: 3.7197234630584717 mm

Highest mean error: 4.131684303283691 mm for frame 75

Lowest mean error: 3.3135106563568115 mm for frame 0

Saving results

Total time: 46.188201665878296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943818
Iteration 2/25 | Loss: 0.00134415
Iteration 3/25 | Loss: 0.00126080
Iteration 4/25 | Loss: 0.00124541
Iteration 5/25 | Loss: 0.00124048
Iteration 6/25 | Loss: 0.00123938
Iteration 7/25 | Loss: 0.00123932
Iteration 8/25 | Loss: 0.00123932
Iteration 9/25 | Loss: 0.00123932
Iteration 10/25 | Loss: 0.00123932
Iteration 11/25 | Loss: 0.00123932
Iteration 12/25 | Loss: 0.00123932
Iteration 13/25 | Loss: 0.00123932
Iteration 14/25 | Loss: 0.00123932
Iteration 15/25 | Loss: 0.00123932
Iteration 16/25 | Loss: 0.00123932
Iteration 17/25 | Loss: 0.00123932
Iteration 18/25 | Loss: 0.00123932
Iteration 19/25 | Loss: 0.00123932
Iteration 20/25 | Loss: 0.00123932
Iteration 21/25 | Loss: 0.00123932
Iteration 22/25 | Loss: 0.00123932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012393216602504253, 0.0012393216602504253, 0.0012393216602504253, 0.0012393216602504253, 0.0012393216602504253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012393216602504253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84900236
Iteration 2/25 | Loss: 0.00493662
Iteration 3/25 | Loss: 0.00493662
Iteration 4/25 | Loss: 0.00493662
Iteration 5/25 | Loss: 0.00493662
Iteration 6/25 | Loss: 0.00493662
Iteration 7/25 | Loss: 0.00493662
Iteration 8/25 | Loss: 0.00493662
Iteration 9/25 | Loss: 0.00493662
Iteration 10/25 | Loss: 0.00493662
Iteration 11/25 | Loss: 0.00493662
Iteration 12/25 | Loss: 0.00493662
Iteration 13/25 | Loss: 0.00493662
Iteration 14/25 | Loss: 0.00493662
Iteration 15/25 | Loss: 0.00493662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004936615005135536, 0.004936615005135536, 0.004936615005135536, 0.004936615005135536, 0.004936615005135536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004936615005135536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00493662
Iteration 2/1000 | Loss: 0.00003768
Iteration 3/1000 | Loss: 0.00002435
Iteration 4/1000 | Loss: 0.00002073
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001824
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001712
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001657
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001653
Iteration 15/1000 | Loss: 0.00001650
Iteration 16/1000 | Loss: 0.00001649
Iteration 17/1000 | Loss: 0.00001649
Iteration 18/1000 | Loss: 0.00001648
Iteration 19/1000 | Loss: 0.00001647
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001630
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001628
Iteration 26/1000 | Loss: 0.00001628
Iteration 27/1000 | Loss: 0.00001628
Iteration 28/1000 | Loss: 0.00001627
Iteration 29/1000 | Loss: 0.00001627
Iteration 30/1000 | Loss: 0.00001627
Iteration 31/1000 | Loss: 0.00001626
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001626
Iteration 35/1000 | Loss: 0.00001625
Iteration 36/1000 | Loss: 0.00001624
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001623
Iteration 39/1000 | Loss: 0.00001623
Iteration 40/1000 | Loss: 0.00001623
Iteration 41/1000 | Loss: 0.00001623
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001623
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001621
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001621
Iteration 50/1000 | Loss: 0.00001620
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001619
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001619
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001618
Iteration 60/1000 | Loss: 0.00001618
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001614
Iteration 77/1000 | Loss: 0.00001614
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001613
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00001613
Iteration 98/1000 | Loss: 0.00001613
Iteration 99/1000 | Loss: 0.00001613
Iteration 100/1000 | Loss: 0.00001613
Iteration 101/1000 | Loss: 0.00001613
Iteration 102/1000 | Loss: 0.00001613
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001613
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001613
Iteration 120/1000 | Loss: 0.00001613
Iteration 121/1000 | Loss: 0.00001613
Iteration 122/1000 | Loss: 0.00001613
Iteration 123/1000 | Loss: 0.00001613
Iteration 124/1000 | Loss: 0.00001613
Iteration 125/1000 | Loss: 0.00001613
Iteration 126/1000 | Loss: 0.00001613
Iteration 127/1000 | Loss: 0.00001613
Iteration 128/1000 | Loss: 0.00001613
Iteration 129/1000 | Loss: 0.00001613
Iteration 130/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.6131551092257723e-05, 1.6131551092257723e-05, 1.6131551092257723e-05, 1.6131551092257723e-05, 1.6131551092257723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6131551092257723e-05

Optimization complete. Final v2v error: 3.5389180183410645 mm

Highest mean error: 3.9199464321136475 mm for frame 4

Lowest mean error: 3.326399087905884 mm for frame 86

Saving results

Total time: 31.89833378791809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935842
Iteration 2/25 | Loss: 0.00191190
Iteration 3/25 | Loss: 0.00146847
Iteration 4/25 | Loss: 0.00141438
Iteration 5/25 | Loss: 0.00142778
Iteration 6/25 | Loss: 0.00138710
Iteration 7/25 | Loss: 0.00135509
Iteration 8/25 | Loss: 0.00135139
Iteration 9/25 | Loss: 0.00135090
Iteration 10/25 | Loss: 0.00134697
Iteration 11/25 | Loss: 0.00133998
Iteration 12/25 | Loss: 0.00133882
Iteration 13/25 | Loss: 0.00133839
Iteration 14/25 | Loss: 0.00133772
Iteration 15/25 | Loss: 0.00133747
Iteration 16/25 | Loss: 0.00133735
Iteration 17/25 | Loss: 0.00133734
Iteration 18/25 | Loss: 0.00133734
Iteration 19/25 | Loss: 0.00133734
Iteration 20/25 | Loss: 0.00133734
Iteration 21/25 | Loss: 0.00133734
Iteration 22/25 | Loss: 0.00133734
Iteration 23/25 | Loss: 0.00133734
Iteration 24/25 | Loss: 0.00133734
Iteration 25/25 | Loss: 0.00133734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06031370
Iteration 2/25 | Loss: 0.00527096
Iteration 3/25 | Loss: 0.00527094
Iteration 4/25 | Loss: 0.00527094
Iteration 5/25 | Loss: 0.00527094
Iteration 6/25 | Loss: 0.00527094
Iteration 7/25 | Loss: 0.00527094
Iteration 8/25 | Loss: 0.00527093
Iteration 9/25 | Loss: 0.00527093
Iteration 10/25 | Loss: 0.00527093
Iteration 11/25 | Loss: 0.00527093
Iteration 12/25 | Loss: 0.00527093
Iteration 13/25 | Loss: 0.00527093
Iteration 14/25 | Loss: 0.00527093
Iteration 15/25 | Loss: 0.00527093
Iteration 16/25 | Loss: 0.00527093
Iteration 17/25 | Loss: 0.00527093
Iteration 18/25 | Loss: 0.00527093
Iteration 19/25 | Loss: 0.00527093
Iteration 20/25 | Loss: 0.00527093
Iteration 21/25 | Loss: 0.00527093
Iteration 22/25 | Loss: 0.00527093
Iteration 23/25 | Loss: 0.00527093
Iteration 24/25 | Loss: 0.00527093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005270934663712978, 0.005270934663712978, 0.005270934663712978, 0.005270934663712978, 0.005270934663712978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005270934663712978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00527093
Iteration 2/1000 | Loss: 0.00005628
Iteration 3/1000 | Loss: 0.00003893
Iteration 4/1000 | Loss: 0.00003375
Iteration 5/1000 | Loss: 0.00003138
Iteration 6/1000 | Loss: 0.00002988
Iteration 7/1000 | Loss: 0.00008765
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00002780
Iteration 10/1000 | Loss: 0.00002650
Iteration 11/1000 | Loss: 0.00002570
Iteration 12/1000 | Loss: 0.00002538
Iteration 13/1000 | Loss: 0.00002519
Iteration 14/1000 | Loss: 0.00002508
Iteration 15/1000 | Loss: 0.00002506
Iteration 16/1000 | Loss: 0.00002502
Iteration 17/1000 | Loss: 0.00002496
Iteration 18/1000 | Loss: 0.00002487
Iteration 19/1000 | Loss: 0.00002485
Iteration 20/1000 | Loss: 0.00002476
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002475
Iteration 23/1000 | Loss: 0.00002474
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002474
Iteration 26/1000 | Loss: 0.00002473
Iteration 27/1000 | Loss: 0.00002472
Iteration 28/1000 | Loss: 0.00002472
Iteration 29/1000 | Loss: 0.00002472
Iteration 30/1000 | Loss: 0.00002471
Iteration 31/1000 | Loss: 0.00002471
Iteration 32/1000 | Loss: 0.00002471
Iteration 33/1000 | Loss: 0.00002471
Iteration 34/1000 | Loss: 0.00002471
Iteration 35/1000 | Loss: 0.00002471
Iteration 36/1000 | Loss: 0.00002471
Iteration 37/1000 | Loss: 0.00002470
Iteration 38/1000 | Loss: 0.00002470
Iteration 39/1000 | Loss: 0.00002470
Iteration 40/1000 | Loss: 0.00002469
Iteration 41/1000 | Loss: 0.00002469
Iteration 42/1000 | Loss: 0.00002469
Iteration 43/1000 | Loss: 0.00002469
Iteration 44/1000 | Loss: 0.00002469
Iteration 45/1000 | Loss: 0.00002469
Iteration 46/1000 | Loss: 0.00002469
Iteration 47/1000 | Loss: 0.00002469
Iteration 48/1000 | Loss: 0.00002469
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002469
Iteration 51/1000 | Loss: 0.00002469
Iteration 52/1000 | Loss: 0.00002469
Iteration 53/1000 | Loss: 0.00002469
Iteration 54/1000 | Loss: 0.00002469
Iteration 55/1000 | Loss: 0.00002469
Iteration 56/1000 | Loss: 0.00002469
Iteration 57/1000 | Loss: 0.00002469
Iteration 58/1000 | Loss: 0.00002469
Iteration 59/1000 | Loss: 0.00002469
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002469
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002469
Iteration 66/1000 | Loss: 0.00002469
Iteration 67/1000 | Loss: 0.00002469
Iteration 68/1000 | Loss: 0.00002469
Iteration 69/1000 | Loss: 0.00002469
Iteration 70/1000 | Loss: 0.00002469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.4685759854037315e-05, 2.4685759854037315e-05, 2.4685759854037315e-05, 2.4685759854037315e-05, 2.4685759854037315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4685759854037315e-05

Optimization complete. Final v2v error: 4.357153415679932 mm

Highest mean error: 5.193391799926758 mm for frame 196

Lowest mean error: 3.772193670272827 mm for frame 6

Saving results

Total time: 59.80374312400818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_nl_1207/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_nl_1207/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696987
Iteration 2/25 | Loss: 0.00193024
Iteration 3/25 | Loss: 0.00162012
Iteration 4/25 | Loss: 0.00156725
Iteration 5/25 | Loss: 0.00154521
Iteration 6/25 | Loss: 0.00152662
Iteration 7/25 | Loss: 0.00152166
Iteration 8/25 | Loss: 0.00151439
Iteration 9/25 | Loss: 0.00151061
Iteration 10/25 | Loss: 0.00151779
Iteration 11/25 | Loss: 0.00153080
Iteration 12/25 | Loss: 0.00151848
Iteration 13/25 | Loss: 0.00151694
Iteration 14/25 | Loss: 0.00152891
Iteration 15/25 | Loss: 0.00152048
Iteration 16/25 | Loss: 0.00150161
Iteration 17/25 | Loss: 0.00149277
Iteration 18/25 | Loss: 0.00149006
Iteration 19/25 | Loss: 0.00148964
Iteration 20/25 | Loss: 0.00148952
Iteration 21/25 | Loss: 0.00148944
Iteration 22/25 | Loss: 0.00148943
Iteration 23/25 | Loss: 0.00148943
Iteration 24/25 | Loss: 0.00148942
Iteration 25/25 | Loss: 0.00148942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77285039
Iteration 2/25 | Loss: 0.00714212
Iteration 3/25 | Loss: 0.00714212
Iteration 4/25 | Loss: 0.00714212
Iteration 5/25 | Loss: 0.00714211
Iteration 6/25 | Loss: 0.00714211
Iteration 7/25 | Loss: 0.00714211
Iteration 8/25 | Loss: 0.00714211
Iteration 9/25 | Loss: 0.00714211
Iteration 10/25 | Loss: 0.00714211
Iteration 11/25 | Loss: 0.00714211
Iteration 12/25 | Loss: 0.00714211
Iteration 13/25 | Loss: 0.00714211
Iteration 14/25 | Loss: 0.00714211
Iteration 15/25 | Loss: 0.00714211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00714211305603385, 0.00714211305603385, 0.00714211305603385, 0.00714211305603385, 0.00714211305603385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00714211305603385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00714211
Iteration 2/1000 | Loss: 0.00096482
Iteration 3/1000 | Loss: 0.00095001
Iteration 4/1000 | Loss: 0.00049109
Iteration 5/1000 | Loss: 0.00062227
Iteration 6/1000 | Loss: 0.00101594
Iteration 7/1000 | Loss: 0.00144212
Iteration 8/1000 | Loss: 0.00142011
Iteration 9/1000 | Loss: 0.00338995
Iteration 10/1000 | Loss: 0.00093478
Iteration 11/1000 | Loss: 0.00111389
Iteration 12/1000 | Loss: 0.00199052
Iteration 13/1000 | Loss: 0.00070036
Iteration 14/1000 | Loss: 0.00061056
Iteration 15/1000 | Loss: 0.00021011
Iteration 16/1000 | Loss: 0.00067589
Iteration 17/1000 | Loss: 0.00021679
Iteration 18/1000 | Loss: 0.00064410
Iteration 19/1000 | Loss: 0.00013563
Iteration 20/1000 | Loss: 0.00264368
Iteration 21/1000 | Loss: 0.00092842
Iteration 22/1000 | Loss: 0.00255042
Iteration 23/1000 | Loss: 0.00310209
Iteration 24/1000 | Loss: 0.00119666
Iteration 25/1000 | Loss: 0.00024540
Iteration 26/1000 | Loss: 0.00068768
Iteration 27/1000 | Loss: 0.00060747
Iteration 28/1000 | Loss: 0.00049792
Iteration 29/1000 | Loss: 0.00030335
Iteration 30/1000 | Loss: 0.00026894
Iteration 31/1000 | Loss: 0.00053098
Iteration 32/1000 | Loss: 0.00039996
Iteration 33/1000 | Loss: 0.00027090
Iteration 34/1000 | Loss: 0.00161856
Iteration 35/1000 | Loss: 0.00191432
Iteration 36/1000 | Loss: 0.00048864
Iteration 37/1000 | Loss: 0.00028464
Iteration 38/1000 | Loss: 0.00024768
Iteration 39/1000 | Loss: 0.00026960
Iteration 40/1000 | Loss: 0.00027819
Iteration 41/1000 | Loss: 0.00085518
Iteration 42/1000 | Loss: 0.00025368
Iteration 43/1000 | Loss: 0.00022521
Iteration 44/1000 | Loss: 0.00090548
Iteration 45/1000 | Loss: 0.00016111
Iteration 46/1000 | Loss: 0.00018232
Iteration 47/1000 | Loss: 0.00060841
Iteration 48/1000 | Loss: 0.00016783
Iteration 49/1000 | Loss: 0.00020137
Iteration 50/1000 | Loss: 0.00030999
Iteration 51/1000 | Loss: 0.00013828
Iteration 52/1000 | Loss: 0.00055131
Iteration 53/1000 | Loss: 0.00122852
Iteration 54/1000 | Loss: 0.00016391
Iteration 55/1000 | Loss: 0.00050320
Iteration 56/1000 | Loss: 0.00010731
Iteration 57/1000 | Loss: 0.00093995
Iteration 58/1000 | Loss: 0.00033739
Iteration 59/1000 | Loss: 0.00089082
Iteration 60/1000 | Loss: 0.00056303
Iteration 61/1000 | Loss: 0.00025291
Iteration 62/1000 | Loss: 0.00037612
Iteration 63/1000 | Loss: 0.00023103
Iteration 64/1000 | Loss: 0.00009866
Iteration 65/1000 | Loss: 0.00012287
Iteration 66/1000 | Loss: 0.00008794
Iteration 67/1000 | Loss: 0.00138405
Iteration 68/1000 | Loss: 0.00073886
Iteration 69/1000 | Loss: 0.00069698
Iteration 70/1000 | Loss: 0.00059921
Iteration 71/1000 | Loss: 0.00041450
Iteration 72/1000 | Loss: 0.00088634
Iteration 73/1000 | Loss: 0.00083850
Iteration 74/1000 | Loss: 0.00068239
Iteration 75/1000 | Loss: 0.00011527
Iteration 76/1000 | Loss: 0.00046564
Iteration 77/1000 | Loss: 0.00038104
Iteration 78/1000 | Loss: 0.00008180
Iteration 79/1000 | Loss: 0.00062750
Iteration 80/1000 | Loss: 0.00008365
Iteration 81/1000 | Loss: 0.00014005
Iteration 82/1000 | Loss: 0.00007285
Iteration 83/1000 | Loss: 0.00046034
Iteration 84/1000 | Loss: 0.00011119
Iteration 85/1000 | Loss: 0.00007791
Iteration 86/1000 | Loss: 0.00006692
Iteration 87/1000 | Loss: 0.00006192
Iteration 88/1000 | Loss: 0.00005857
Iteration 89/1000 | Loss: 0.00098901
Iteration 90/1000 | Loss: 0.00392649
Iteration 91/1000 | Loss: 0.00236104
Iteration 92/1000 | Loss: 0.00172525
Iteration 93/1000 | Loss: 0.00064277
Iteration 94/1000 | Loss: 0.00008510
Iteration 95/1000 | Loss: 0.00018300
Iteration 96/1000 | Loss: 0.00006427
Iteration 97/1000 | Loss: 0.00005524
Iteration 98/1000 | Loss: 0.00004760
Iteration 99/1000 | Loss: 0.00004373
Iteration 100/1000 | Loss: 0.00004116
Iteration 101/1000 | Loss: 0.00003917
Iteration 102/1000 | Loss: 0.00003809
Iteration 103/1000 | Loss: 0.00003721
Iteration 104/1000 | Loss: 0.00136130
Iteration 105/1000 | Loss: 0.00015278
Iteration 106/1000 | Loss: 0.00010952
Iteration 107/1000 | Loss: 0.00135506
Iteration 108/1000 | Loss: 0.00008530
Iteration 109/1000 | Loss: 0.00005167
Iteration 110/1000 | Loss: 0.00004519
Iteration 111/1000 | Loss: 0.00004142
Iteration 112/1000 | Loss: 0.00003809
Iteration 113/1000 | Loss: 0.00003587
Iteration 114/1000 | Loss: 0.00003317
Iteration 115/1000 | Loss: 0.00003197
Iteration 116/1000 | Loss: 0.00003127
Iteration 117/1000 | Loss: 0.00003062
Iteration 118/1000 | Loss: 0.00050638
Iteration 119/1000 | Loss: 0.00019134
Iteration 120/1000 | Loss: 0.00016056
Iteration 121/1000 | Loss: 0.00050104
Iteration 122/1000 | Loss: 0.00038969
Iteration 123/1000 | Loss: 0.00036755
Iteration 124/1000 | Loss: 0.00048352
Iteration 125/1000 | Loss: 0.00040250
Iteration 126/1000 | Loss: 0.00041132
Iteration 127/1000 | Loss: 0.00044693
Iteration 128/1000 | Loss: 0.00045475
Iteration 129/1000 | Loss: 0.00053948
Iteration 130/1000 | Loss: 0.00003532
Iteration 131/1000 | Loss: 0.00002943
Iteration 132/1000 | Loss: 0.00002815
Iteration 133/1000 | Loss: 0.00002750
Iteration 134/1000 | Loss: 0.00002705
Iteration 135/1000 | Loss: 0.00002682
Iteration 136/1000 | Loss: 0.00002682
Iteration 137/1000 | Loss: 0.00002681
Iteration 138/1000 | Loss: 0.00002681
Iteration 139/1000 | Loss: 0.00002679
Iteration 140/1000 | Loss: 0.00002679
Iteration 141/1000 | Loss: 0.00002679
Iteration 142/1000 | Loss: 0.00002679
Iteration 143/1000 | Loss: 0.00002678
Iteration 144/1000 | Loss: 0.00002678
Iteration 145/1000 | Loss: 0.00002678
Iteration 146/1000 | Loss: 0.00002678
Iteration 147/1000 | Loss: 0.00002678
Iteration 148/1000 | Loss: 0.00002678
Iteration 149/1000 | Loss: 0.00002678
Iteration 150/1000 | Loss: 0.00002678
Iteration 151/1000 | Loss: 0.00002677
Iteration 152/1000 | Loss: 0.00002677
Iteration 153/1000 | Loss: 0.00002676
Iteration 154/1000 | Loss: 0.00002676
Iteration 155/1000 | Loss: 0.00002676
Iteration 156/1000 | Loss: 0.00002676
Iteration 157/1000 | Loss: 0.00002675
Iteration 158/1000 | Loss: 0.00002675
Iteration 159/1000 | Loss: 0.00002674
Iteration 160/1000 | Loss: 0.00002674
Iteration 161/1000 | Loss: 0.00002674
Iteration 162/1000 | Loss: 0.00002674
Iteration 163/1000 | Loss: 0.00002674
Iteration 164/1000 | Loss: 0.00002674
Iteration 165/1000 | Loss: 0.00002674
Iteration 166/1000 | Loss: 0.00002674
Iteration 167/1000 | Loss: 0.00002674
Iteration 168/1000 | Loss: 0.00002673
Iteration 169/1000 | Loss: 0.00002673
Iteration 170/1000 | Loss: 0.00002673
Iteration 171/1000 | Loss: 0.00002673
Iteration 172/1000 | Loss: 0.00002673
Iteration 173/1000 | Loss: 0.00002673
Iteration 174/1000 | Loss: 0.00002673
Iteration 175/1000 | Loss: 0.00002673
Iteration 176/1000 | Loss: 0.00002673
Iteration 177/1000 | Loss: 0.00002673
Iteration 178/1000 | Loss: 0.00002673
Iteration 179/1000 | Loss: 0.00002673
Iteration 180/1000 | Loss: 0.00002673
Iteration 181/1000 | Loss: 0.00002672
Iteration 182/1000 | Loss: 0.00002672
Iteration 183/1000 | Loss: 0.00002672
Iteration 184/1000 | Loss: 0.00002672
Iteration 185/1000 | Loss: 0.00002672
Iteration 186/1000 | Loss: 0.00002672
Iteration 187/1000 | Loss: 0.00002672
Iteration 188/1000 | Loss: 0.00002672
Iteration 189/1000 | Loss: 0.00002672
Iteration 190/1000 | Loss: 0.00002671
Iteration 191/1000 | Loss: 0.00002671
Iteration 192/1000 | Loss: 0.00002671
Iteration 193/1000 | Loss: 0.00002671
Iteration 194/1000 | Loss: 0.00002671
Iteration 195/1000 | Loss: 0.00002670
Iteration 196/1000 | Loss: 0.00002670
Iteration 197/1000 | Loss: 0.00002670
Iteration 198/1000 | Loss: 0.00002670
Iteration 199/1000 | Loss: 0.00002669
Iteration 200/1000 | Loss: 0.00002669
Iteration 201/1000 | Loss: 0.00002669
Iteration 202/1000 | Loss: 0.00002669
Iteration 203/1000 | Loss: 0.00002669
Iteration 204/1000 | Loss: 0.00002669
Iteration 205/1000 | Loss: 0.00002669
Iteration 206/1000 | Loss: 0.00002668
Iteration 207/1000 | Loss: 0.00002668
Iteration 208/1000 | Loss: 0.00002668
Iteration 209/1000 | Loss: 0.00002668
Iteration 210/1000 | Loss: 0.00002668
Iteration 211/1000 | Loss: 0.00002668
Iteration 212/1000 | Loss: 0.00002668
Iteration 213/1000 | Loss: 0.00002668
Iteration 214/1000 | Loss: 0.00002668
Iteration 215/1000 | Loss: 0.00002668
Iteration 216/1000 | Loss: 0.00002668
Iteration 217/1000 | Loss: 0.00002668
Iteration 218/1000 | Loss: 0.00002668
Iteration 219/1000 | Loss: 0.00002668
Iteration 220/1000 | Loss: 0.00002668
Iteration 221/1000 | Loss: 0.00002668
Iteration 222/1000 | Loss: 0.00002668
Iteration 223/1000 | Loss: 0.00002668
Iteration 224/1000 | Loss: 0.00002668
Iteration 225/1000 | Loss: 0.00002668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.6679599613999017e-05, 2.6679599613999017e-05, 2.6679599613999017e-05, 2.6679599613999017e-05, 2.6679599613999017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6679599613999017e-05

Optimization complete. Final v2v error: 4.452156066894531 mm

Highest mean error: 5.827504634857178 mm for frame 177

Lowest mean error: 3.757361650466919 mm for frame 207

Saving results

Total time: 260.8981807231903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033664
Iteration 2/25 | Loss: 0.01033664
Iteration 3/25 | Loss: 0.01033664
Iteration 4/25 | Loss: 0.01033663
Iteration 5/25 | Loss: 0.01033663
Iteration 6/25 | Loss: 0.01033663
Iteration 7/25 | Loss: 0.01033663
Iteration 8/25 | Loss: 0.01033663
Iteration 9/25 | Loss: 0.00342653
Iteration 10/25 | Loss: 0.00201777
Iteration 11/25 | Loss: 0.00194438
Iteration 12/25 | Loss: 0.00183959
Iteration 13/25 | Loss: 0.00180053
Iteration 14/25 | Loss: 0.00171858
Iteration 15/25 | Loss: 0.00170124
Iteration 16/25 | Loss: 0.00163751
Iteration 17/25 | Loss: 0.00161754
Iteration 18/25 | Loss: 0.00160452
Iteration 19/25 | Loss: 0.00158963
Iteration 20/25 | Loss: 0.00156231
Iteration 21/25 | Loss: 0.00155522
Iteration 22/25 | Loss: 0.00155053
Iteration 23/25 | Loss: 0.00155031
Iteration 24/25 | Loss: 0.00155687
Iteration 25/25 | Loss: 0.00153633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31590486
Iteration 2/25 | Loss: 0.00355124
Iteration 3/25 | Loss: 0.00355123
Iteration 4/25 | Loss: 0.00337464
Iteration 5/25 | Loss: 0.00337464
Iteration 6/25 | Loss: 0.00337464
Iteration 7/25 | Loss: 0.00337464
Iteration 8/25 | Loss: 0.00337464
Iteration 9/25 | Loss: 0.00337464
Iteration 10/25 | Loss: 0.00337464
Iteration 11/25 | Loss: 0.00337464
Iteration 12/25 | Loss: 0.00337464
Iteration 13/25 | Loss: 0.00337464
Iteration 14/25 | Loss: 0.00337464
Iteration 15/25 | Loss: 0.00337464
Iteration 16/25 | Loss: 0.00337464
Iteration 17/25 | Loss: 0.00337464
Iteration 18/25 | Loss: 0.00337464
Iteration 19/25 | Loss: 0.00337464
Iteration 20/25 | Loss: 0.00337464
Iteration 21/25 | Loss: 0.00337464
Iteration 22/25 | Loss: 0.00337464
Iteration 23/25 | Loss: 0.00337464
Iteration 24/25 | Loss: 0.00337464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003374640131369233, 0.003374640131369233, 0.003374640131369233, 0.003374640131369233, 0.003374640131369233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003374640131369233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337464
Iteration 2/1000 | Loss: 0.00123369
Iteration 3/1000 | Loss: 0.00162291
Iteration 4/1000 | Loss: 0.00246821
Iteration 5/1000 | Loss: 0.00075624
Iteration 6/1000 | Loss: 0.00231735
Iteration 7/1000 | Loss: 0.00192032
Iteration 8/1000 | Loss: 0.00159805
Iteration 9/1000 | Loss: 0.02253225
Iteration 10/1000 | Loss: 0.01230724
Iteration 11/1000 | Loss: 0.00660508
Iteration 12/1000 | Loss: 0.00427531
Iteration 13/1000 | Loss: 0.00446626
Iteration 14/1000 | Loss: 0.00294376
Iteration 15/1000 | Loss: 0.00312337
Iteration 16/1000 | Loss: 0.00280152
Iteration 17/1000 | Loss: 0.00463054
Iteration 18/1000 | Loss: 0.00283915
Iteration 19/1000 | Loss: 0.00101950
Iteration 20/1000 | Loss: 0.00111980
Iteration 21/1000 | Loss: 0.00142674
Iteration 22/1000 | Loss: 0.00101054
Iteration 23/1000 | Loss: 0.00022274
Iteration 24/1000 | Loss: 0.00023778
Iteration 25/1000 | Loss: 0.00016292
Iteration 26/1000 | Loss: 0.00015818
Iteration 27/1000 | Loss: 0.00016185
Iteration 28/1000 | Loss: 0.00081796
Iteration 29/1000 | Loss: 0.00813567
Iteration 30/1000 | Loss: 0.00498700
Iteration 31/1000 | Loss: 0.00128009
Iteration 32/1000 | Loss: 0.00713640
Iteration 33/1000 | Loss: 0.00560611
Iteration 34/1000 | Loss: 0.01086771
Iteration 35/1000 | Loss: 0.00593195
Iteration 36/1000 | Loss: 0.00759537
Iteration 37/1000 | Loss: 0.00551380
Iteration 38/1000 | Loss: 0.00950295
Iteration 39/1000 | Loss: 0.00565467
Iteration 40/1000 | Loss: 0.01379736
Iteration 41/1000 | Loss: 0.00313255
Iteration 42/1000 | Loss: 0.00273414
Iteration 43/1000 | Loss: 0.00301696
Iteration 44/1000 | Loss: 0.00249109
Iteration 45/1000 | Loss: 0.00204956
Iteration 46/1000 | Loss: 0.00109066
Iteration 47/1000 | Loss: 0.00084964
Iteration 48/1000 | Loss: 0.00060746
Iteration 49/1000 | Loss: 0.00105506
Iteration 50/1000 | Loss: 0.00045664
Iteration 51/1000 | Loss: 0.00127553
Iteration 52/1000 | Loss: 0.00044985
Iteration 53/1000 | Loss: 0.00048197
Iteration 54/1000 | Loss: 0.00009210
Iteration 55/1000 | Loss: 0.00011369
Iteration 56/1000 | Loss: 0.00054001
Iteration 57/1000 | Loss: 0.00073945
Iteration 58/1000 | Loss: 0.00045614
Iteration 59/1000 | Loss: 0.00034933
Iteration 60/1000 | Loss: 0.00013766
Iteration 61/1000 | Loss: 0.00006180
Iteration 62/1000 | Loss: 0.00089055
Iteration 63/1000 | Loss: 0.00070230
Iteration 64/1000 | Loss: 0.00019313
Iteration 65/1000 | Loss: 0.00005272
Iteration 66/1000 | Loss: 0.00004497
Iteration 67/1000 | Loss: 0.00004592
Iteration 68/1000 | Loss: 0.00074425
Iteration 69/1000 | Loss: 0.00229531
Iteration 70/1000 | Loss: 0.00022871
Iteration 71/1000 | Loss: 0.00023434
Iteration 72/1000 | Loss: 0.00044930
Iteration 73/1000 | Loss: 0.00020284
Iteration 74/1000 | Loss: 0.00027045
Iteration 75/1000 | Loss: 0.00023736
Iteration 76/1000 | Loss: 0.00019337
Iteration 77/1000 | Loss: 0.00042673
Iteration 78/1000 | Loss: 0.00037306
Iteration 79/1000 | Loss: 0.00037979
Iteration 80/1000 | Loss: 0.00022689
Iteration 81/1000 | Loss: 0.00036001
Iteration 82/1000 | Loss: 0.00176093
Iteration 83/1000 | Loss: 0.00030024
Iteration 84/1000 | Loss: 0.00023492
Iteration 85/1000 | Loss: 0.00028318
Iteration 86/1000 | Loss: 0.00019612
Iteration 87/1000 | Loss: 0.00022610
Iteration 88/1000 | Loss: 0.00015626
Iteration 89/1000 | Loss: 0.00014448
Iteration 90/1000 | Loss: 0.00015880
Iteration 91/1000 | Loss: 0.00008412
Iteration 92/1000 | Loss: 0.00003022
Iteration 93/1000 | Loss: 0.00003374
Iteration 94/1000 | Loss: 0.00003700
Iteration 95/1000 | Loss: 0.00003632
Iteration 96/1000 | Loss: 0.00003479
Iteration 97/1000 | Loss: 0.00002550
Iteration 98/1000 | Loss: 0.00039602
Iteration 99/1000 | Loss: 0.00004238
Iteration 100/1000 | Loss: 0.00003367
Iteration 101/1000 | Loss: 0.00003505
Iteration 102/1000 | Loss: 0.00050570
Iteration 103/1000 | Loss: 0.00004140
Iteration 104/1000 | Loss: 0.00002595
Iteration 105/1000 | Loss: 0.00003648
Iteration 106/1000 | Loss: 0.00003564
Iteration 107/1000 | Loss: 0.00003476
Iteration 108/1000 | Loss: 0.00003299
Iteration 109/1000 | Loss: 0.00002778
Iteration 110/1000 | Loss: 0.00005805
Iteration 111/1000 | Loss: 0.00006279
Iteration 112/1000 | Loss: 0.00003616
Iteration 113/1000 | Loss: 0.00003880
Iteration 114/1000 | Loss: 0.00003573
Iteration 115/1000 | Loss: 0.00003547
Iteration 116/1000 | Loss: 0.00003463
Iteration 117/1000 | Loss: 0.00003363
Iteration 118/1000 | Loss: 0.00007161
Iteration 119/1000 | Loss: 0.00002615
Iteration 120/1000 | Loss: 0.00003105
Iteration 121/1000 | Loss: 0.00003109
Iteration 122/1000 | Loss: 0.00003562
Iteration 123/1000 | Loss: 0.00003286
Iteration 124/1000 | Loss: 0.00003068
Iteration 125/1000 | Loss: 0.00008417
Iteration 126/1000 | Loss: 0.00006455
Iteration 127/1000 | Loss: 0.00003075
Iteration 128/1000 | Loss: 0.00003237
Iteration 129/1000 | Loss: 0.00002916
Iteration 130/1000 | Loss: 0.00004111
Iteration 131/1000 | Loss: 0.00051850
Iteration 132/1000 | Loss: 0.00040463
Iteration 133/1000 | Loss: 0.00004948
Iteration 134/1000 | Loss: 0.00003185
Iteration 135/1000 | Loss: 0.00003141
Iteration 136/1000 | Loss: 0.00003019
Iteration 137/1000 | Loss: 0.00050523
Iteration 138/1000 | Loss: 0.00013780
Iteration 139/1000 | Loss: 0.00043177
Iteration 140/1000 | Loss: 0.00047373
Iteration 141/1000 | Loss: 0.00037600
Iteration 142/1000 | Loss: 0.00005040
Iteration 143/1000 | Loss: 0.00004180
Iteration 144/1000 | Loss: 0.00003723
Iteration 145/1000 | Loss: 0.00002474
Iteration 146/1000 | Loss: 0.00002308
Iteration 147/1000 | Loss: 0.00002813
Iteration 148/1000 | Loss: 0.00002086
Iteration 149/1000 | Loss: 0.00002706
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00002195
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00002086
Iteration 155/1000 | Loss: 0.00002653
Iteration 156/1000 | Loss: 0.00002793
Iteration 157/1000 | Loss: 0.00001871
Iteration 158/1000 | Loss: 0.00001870
Iteration 159/1000 | Loss: 0.00001869
Iteration 160/1000 | Loss: 0.00001869
Iteration 161/1000 | Loss: 0.00002476
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001871
Iteration 167/1000 | Loss: 0.00001846
Iteration 168/1000 | Loss: 0.00001846
Iteration 169/1000 | Loss: 0.00001846
Iteration 170/1000 | Loss: 0.00001846
Iteration 171/1000 | Loss: 0.00001846
Iteration 172/1000 | Loss: 0.00001846
Iteration 173/1000 | Loss: 0.00001846
Iteration 174/1000 | Loss: 0.00001846
Iteration 175/1000 | Loss: 0.00001846
Iteration 176/1000 | Loss: 0.00001845
Iteration 177/1000 | Loss: 0.00002002
Iteration 178/1000 | Loss: 0.00001843
Iteration 179/1000 | Loss: 0.00002218
Iteration 180/1000 | Loss: 0.00001838
Iteration 181/1000 | Loss: 0.00001838
Iteration 182/1000 | Loss: 0.00001838
Iteration 183/1000 | Loss: 0.00001838
Iteration 184/1000 | Loss: 0.00001916
Iteration 185/1000 | Loss: 0.00001837
Iteration 186/1000 | Loss: 0.00001837
Iteration 187/1000 | Loss: 0.00001837
Iteration 188/1000 | Loss: 0.00001837
Iteration 189/1000 | Loss: 0.00001837
Iteration 190/1000 | Loss: 0.00001837
Iteration 191/1000 | Loss: 0.00001837
Iteration 192/1000 | Loss: 0.00001837
Iteration 193/1000 | Loss: 0.00001837
Iteration 194/1000 | Loss: 0.00001837
Iteration 195/1000 | Loss: 0.00001837
Iteration 196/1000 | Loss: 0.00001837
Iteration 197/1000 | Loss: 0.00001837
Iteration 198/1000 | Loss: 0.00001837
Iteration 199/1000 | Loss: 0.00001837
Iteration 200/1000 | Loss: 0.00001837
Iteration 201/1000 | Loss: 0.00001837
Iteration 202/1000 | Loss: 0.00001837
Iteration 203/1000 | Loss: 0.00001837
Iteration 204/1000 | Loss: 0.00001837
Iteration 205/1000 | Loss: 0.00001837
Iteration 206/1000 | Loss: 0.00001837
Iteration 207/1000 | Loss: 0.00001837
Iteration 208/1000 | Loss: 0.00001837
Iteration 209/1000 | Loss: 0.00001837
Iteration 210/1000 | Loss: 0.00001837
Iteration 211/1000 | Loss: 0.00001837
Iteration 212/1000 | Loss: 0.00001837
Iteration 213/1000 | Loss: 0.00001837
Iteration 214/1000 | Loss: 0.00001837
Iteration 215/1000 | Loss: 0.00001837
Iteration 216/1000 | Loss: 0.00001837
Iteration 217/1000 | Loss: 0.00001837
Iteration 218/1000 | Loss: 0.00001837
Iteration 219/1000 | Loss: 0.00001837
Iteration 220/1000 | Loss: 0.00001837
Iteration 221/1000 | Loss: 0.00001837
Iteration 222/1000 | Loss: 0.00001837
Iteration 223/1000 | Loss: 0.00001837
Iteration 224/1000 | Loss: 0.00001837
Iteration 225/1000 | Loss: 0.00001837
Iteration 226/1000 | Loss: 0.00001837
Iteration 227/1000 | Loss: 0.00001837
Iteration 228/1000 | Loss: 0.00001837
Iteration 229/1000 | Loss: 0.00001837
Iteration 230/1000 | Loss: 0.00001837
Iteration 231/1000 | Loss: 0.00001837
Iteration 232/1000 | Loss: 0.00001837
Iteration 233/1000 | Loss: 0.00001837
Iteration 234/1000 | Loss: 0.00001837
Iteration 235/1000 | Loss: 0.00001837
Iteration 236/1000 | Loss: 0.00001837
Iteration 237/1000 | Loss: 0.00001837
Iteration 238/1000 | Loss: 0.00001837
Iteration 239/1000 | Loss: 0.00001837
Iteration 240/1000 | Loss: 0.00001837
Iteration 241/1000 | Loss: 0.00001837
Iteration 242/1000 | Loss: 0.00001837
Iteration 243/1000 | Loss: 0.00001837
Iteration 244/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.8367118173046038e-05, 1.8367118173046038e-05, 1.8367118173046038e-05, 1.8367118173046038e-05, 1.8367118173046038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8367118173046038e-05

Optimization complete. Final v2v error: 3.323814630508423 mm

Highest mean error: 12.290292739868164 mm for frame 143

Lowest mean error: 2.7414047718048096 mm for frame 119

Saving results

Total time: 292.4405972957611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398405
Iteration 2/25 | Loss: 0.00133782
Iteration 3/25 | Loss: 0.00121665
Iteration 4/25 | Loss: 0.00120098
Iteration 5/25 | Loss: 0.00119855
Iteration 6/25 | Loss: 0.00119836
Iteration 7/25 | Loss: 0.00119836
Iteration 8/25 | Loss: 0.00119836
Iteration 9/25 | Loss: 0.00119836
Iteration 10/25 | Loss: 0.00119836
Iteration 11/25 | Loss: 0.00119836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011983630247414112, 0.0011983630247414112, 0.0011983630247414112, 0.0011983630247414112, 0.0011983630247414112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011983630247414112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33757722
Iteration 2/25 | Loss: 0.00087429
Iteration 3/25 | Loss: 0.00087429
Iteration 4/25 | Loss: 0.00087429
Iteration 5/25 | Loss: 0.00087429
Iteration 6/25 | Loss: 0.00087429
Iteration 7/25 | Loss: 0.00087429
Iteration 8/25 | Loss: 0.00087428
Iteration 9/25 | Loss: 0.00087428
Iteration 10/25 | Loss: 0.00087428
Iteration 11/25 | Loss: 0.00087428
Iteration 12/25 | Loss: 0.00087428
Iteration 13/25 | Loss: 0.00087428
Iteration 14/25 | Loss: 0.00087428
Iteration 15/25 | Loss: 0.00087428
Iteration 16/25 | Loss: 0.00087428
Iteration 17/25 | Loss: 0.00087428
Iteration 18/25 | Loss: 0.00087428
Iteration 19/25 | Loss: 0.00087428
Iteration 20/25 | Loss: 0.00087428
Iteration 21/25 | Loss: 0.00087428
Iteration 22/25 | Loss: 0.00087428
Iteration 23/25 | Loss: 0.00087428
Iteration 24/25 | Loss: 0.00087428
Iteration 25/25 | Loss: 0.00087428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087428
Iteration 2/1000 | Loss: 0.00002410
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001520
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001255
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001206
Iteration 26/1000 | Loss: 0.00001206
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001186
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001186
Iteration 41/1000 | Loss: 0.00001186
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001186
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001185
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001185
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001184
Iteration 51/1000 | Loss: 0.00001184
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001177
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001175
Iteration 89/1000 | Loss: 0.00001175
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001173
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001167
Iteration 125/1000 | Loss: 0.00001167
Iteration 126/1000 | Loss: 0.00001167
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001166
Iteration 131/1000 | Loss: 0.00001166
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001165
Iteration 134/1000 | Loss: 0.00001165
Iteration 135/1000 | Loss: 0.00001165
Iteration 136/1000 | Loss: 0.00001165
Iteration 137/1000 | Loss: 0.00001165
Iteration 138/1000 | Loss: 0.00001165
Iteration 139/1000 | Loss: 0.00001165
Iteration 140/1000 | Loss: 0.00001165
Iteration 141/1000 | Loss: 0.00001165
Iteration 142/1000 | Loss: 0.00001165
Iteration 143/1000 | Loss: 0.00001165
Iteration 144/1000 | Loss: 0.00001165
Iteration 145/1000 | Loss: 0.00001165
Iteration 146/1000 | Loss: 0.00001165
Iteration 147/1000 | Loss: 0.00001165
Iteration 148/1000 | Loss: 0.00001164
Iteration 149/1000 | Loss: 0.00001164
Iteration 150/1000 | Loss: 0.00001164
Iteration 151/1000 | Loss: 0.00001164
Iteration 152/1000 | Loss: 0.00001163
Iteration 153/1000 | Loss: 0.00001163
Iteration 154/1000 | Loss: 0.00001163
Iteration 155/1000 | Loss: 0.00001163
Iteration 156/1000 | Loss: 0.00001163
Iteration 157/1000 | Loss: 0.00001163
Iteration 158/1000 | Loss: 0.00001163
Iteration 159/1000 | Loss: 0.00001163
Iteration 160/1000 | Loss: 0.00001163
Iteration 161/1000 | Loss: 0.00001163
Iteration 162/1000 | Loss: 0.00001163
Iteration 163/1000 | Loss: 0.00001163
Iteration 164/1000 | Loss: 0.00001163
Iteration 165/1000 | Loss: 0.00001163
Iteration 166/1000 | Loss: 0.00001163
Iteration 167/1000 | Loss: 0.00001163
Iteration 168/1000 | Loss: 0.00001163
Iteration 169/1000 | Loss: 0.00001163
Iteration 170/1000 | Loss: 0.00001162
Iteration 171/1000 | Loss: 0.00001162
Iteration 172/1000 | Loss: 0.00001162
Iteration 173/1000 | Loss: 0.00001162
Iteration 174/1000 | Loss: 0.00001162
Iteration 175/1000 | Loss: 0.00001162
Iteration 176/1000 | Loss: 0.00001162
Iteration 177/1000 | Loss: 0.00001162
Iteration 178/1000 | Loss: 0.00001162
Iteration 179/1000 | Loss: 0.00001162
Iteration 180/1000 | Loss: 0.00001162
Iteration 181/1000 | Loss: 0.00001162
Iteration 182/1000 | Loss: 0.00001162
Iteration 183/1000 | Loss: 0.00001162
Iteration 184/1000 | Loss: 0.00001162
Iteration 185/1000 | Loss: 0.00001162
Iteration 186/1000 | Loss: 0.00001162
Iteration 187/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.162457738246303e-05, 1.162457738246303e-05, 1.162457738246303e-05, 1.162457738246303e-05, 1.162457738246303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.162457738246303e-05

Optimization complete. Final v2v error: 2.9298744201660156 mm

Highest mean error: 3.2599198818206787 mm for frame 83

Lowest mean error: 2.723522663116455 mm for frame 18

Saving results

Total time: 41.722869873046875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042864
Iteration 2/25 | Loss: 0.00303020
Iteration 3/25 | Loss: 0.00209927
Iteration 4/25 | Loss: 0.00199591
Iteration 5/25 | Loss: 0.00191672
Iteration 6/25 | Loss: 0.00181370
Iteration 7/25 | Loss: 0.00186635
Iteration 8/25 | Loss: 0.00176427
Iteration 9/25 | Loss: 0.00169717
Iteration 10/25 | Loss: 0.00167172
Iteration 11/25 | Loss: 0.00164036
Iteration 12/25 | Loss: 0.00161296
Iteration 13/25 | Loss: 0.00162428
Iteration 14/25 | Loss: 0.00160075
Iteration 15/25 | Loss: 0.00159912
Iteration 16/25 | Loss: 0.00159247
Iteration 17/25 | Loss: 0.00160207
Iteration 18/25 | Loss: 0.00157902
Iteration 19/25 | Loss: 0.00156905
Iteration 20/25 | Loss: 0.00159187
Iteration 21/25 | Loss: 0.00159842
Iteration 22/25 | Loss: 0.00158310
Iteration 23/25 | Loss: 0.00156721
Iteration 24/25 | Loss: 0.00156636
Iteration 25/25 | Loss: 0.00156816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08410144
Iteration 2/25 | Loss: 0.00247767
Iteration 3/25 | Loss: 0.00247767
Iteration 4/25 | Loss: 0.00247767
Iteration 5/25 | Loss: 0.00247767
Iteration 6/25 | Loss: 0.00247767
Iteration 7/25 | Loss: 0.00247767
Iteration 8/25 | Loss: 0.00247767
Iteration 9/25 | Loss: 0.00247767
Iteration 10/25 | Loss: 0.00247767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.002477666363120079, 0.002477666363120079, 0.002477666363120079, 0.002477666363120079, 0.002477666363120079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002477666363120079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247767
Iteration 2/1000 | Loss: 0.00117570
Iteration 3/1000 | Loss: 0.00035962
Iteration 4/1000 | Loss: 0.00022053
Iteration 5/1000 | Loss: 0.00035331
Iteration 6/1000 | Loss: 0.00021973
Iteration 7/1000 | Loss: 0.00014508
Iteration 8/1000 | Loss: 0.00043105
Iteration 9/1000 | Loss: 0.00038380
Iteration 10/1000 | Loss: 0.00013101
Iteration 11/1000 | Loss: 0.00023293
Iteration 12/1000 | Loss: 0.00012627
Iteration 13/1000 | Loss: 0.00047072
Iteration 14/1000 | Loss: 0.00015766
Iteration 15/1000 | Loss: 0.00012288
Iteration 16/1000 | Loss: 0.00022766
Iteration 17/1000 | Loss: 0.00050329
Iteration 18/1000 | Loss: 0.00019531
Iteration 19/1000 | Loss: 0.00013134
Iteration 20/1000 | Loss: 0.00036351
Iteration 21/1000 | Loss: 0.00015201
Iteration 22/1000 | Loss: 0.00036005
Iteration 23/1000 | Loss: 0.00033291
Iteration 24/1000 | Loss: 0.00048552
Iteration 25/1000 | Loss: 0.00016407
Iteration 26/1000 | Loss: 0.00011697
Iteration 27/1000 | Loss: 0.00018760
Iteration 28/1000 | Loss: 0.00011397
Iteration 29/1000 | Loss: 0.00015922
Iteration 30/1000 | Loss: 0.00011255
Iteration 31/1000 | Loss: 0.00011084
Iteration 32/1000 | Loss: 0.00021447
Iteration 33/1000 | Loss: 0.00010858
Iteration 34/1000 | Loss: 0.00010695
Iteration 35/1000 | Loss: 0.00018407
Iteration 36/1000 | Loss: 0.00010512
Iteration 37/1000 | Loss: 0.00018746
Iteration 38/1000 | Loss: 0.00028323
Iteration 39/1000 | Loss: 0.00065268
Iteration 40/1000 | Loss: 0.00022494
Iteration 41/1000 | Loss: 0.00044331
Iteration 42/1000 | Loss: 0.00075007
Iteration 43/1000 | Loss: 0.00179049
Iteration 44/1000 | Loss: 0.00160590
Iteration 45/1000 | Loss: 0.00178329
Iteration 46/1000 | Loss: 0.00080691
Iteration 47/1000 | Loss: 0.00041311
Iteration 48/1000 | Loss: 0.00068919
Iteration 49/1000 | Loss: 0.00032455
Iteration 50/1000 | Loss: 0.00053180
Iteration 51/1000 | Loss: 0.00029102
Iteration 52/1000 | Loss: 0.00044200
Iteration 53/1000 | Loss: 0.00022419
Iteration 54/1000 | Loss: 0.00021819
Iteration 55/1000 | Loss: 0.00009610
Iteration 56/1000 | Loss: 0.00045672
Iteration 57/1000 | Loss: 0.00049569
Iteration 58/1000 | Loss: 0.00037121
Iteration 59/1000 | Loss: 0.00091815
Iteration 60/1000 | Loss: 0.00103423
Iteration 61/1000 | Loss: 0.00012613
Iteration 62/1000 | Loss: 0.00008942
Iteration 63/1000 | Loss: 0.00054784
Iteration 64/1000 | Loss: 0.00048762
Iteration 65/1000 | Loss: 0.00047035
Iteration 66/1000 | Loss: 0.00046640
Iteration 67/1000 | Loss: 0.00024496
Iteration 68/1000 | Loss: 0.00022732
Iteration 69/1000 | Loss: 0.00006982
Iteration 70/1000 | Loss: 0.00006075
Iteration 71/1000 | Loss: 0.00008323
Iteration 72/1000 | Loss: 0.00005575
Iteration 73/1000 | Loss: 0.00005240
Iteration 74/1000 | Loss: 0.00013093
Iteration 75/1000 | Loss: 0.00011745
Iteration 76/1000 | Loss: 0.00006239
Iteration 77/1000 | Loss: 0.00006953
Iteration 78/1000 | Loss: 0.00004586
Iteration 79/1000 | Loss: 0.00005114
Iteration 80/1000 | Loss: 0.00004474
Iteration 81/1000 | Loss: 0.00004413
Iteration 82/1000 | Loss: 0.00011673
Iteration 83/1000 | Loss: 0.00004384
Iteration 84/1000 | Loss: 0.00004359
Iteration 85/1000 | Loss: 0.00004341
Iteration 86/1000 | Loss: 0.00005289
Iteration 87/1000 | Loss: 0.00004879
Iteration 88/1000 | Loss: 0.00004307
Iteration 89/1000 | Loss: 0.00004291
Iteration 90/1000 | Loss: 0.00004280
Iteration 91/1000 | Loss: 0.00004280
Iteration 92/1000 | Loss: 0.00004279
Iteration 93/1000 | Loss: 0.00006798
Iteration 94/1000 | Loss: 0.00004263
Iteration 95/1000 | Loss: 0.00004254
Iteration 96/1000 | Loss: 0.00004254
Iteration 97/1000 | Loss: 0.00004242
Iteration 98/1000 | Loss: 0.00004241
Iteration 99/1000 | Loss: 0.00004240
Iteration 100/1000 | Loss: 0.00004240
Iteration 101/1000 | Loss: 0.00004237
Iteration 102/1000 | Loss: 0.00009356
Iteration 103/1000 | Loss: 0.00005416
Iteration 104/1000 | Loss: 0.00004240
Iteration 105/1000 | Loss: 0.00004235
Iteration 106/1000 | Loss: 0.00004233
Iteration 107/1000 | Loss: 0.00004232
Iteration 108/1000 | Loss: 0.00004231
Iteration 109/1000 | Loss: 0.00004231
Iteration 110/1000 | Loss: 0.00004229
Iteration 111/1000 | Loss: 0.00004229
Iteration 112/1000 | Loss: 0.00004229
Iteration 113/1000 | Loss: 0.00004229
Iteration 114/1000 | Loss: 0.00004229
Iteration 115/1000 | Loss: 0.00004229
Iteration 116/1000 | Loss: 0.00004229
Iteration 117/1000 | Loss: 0.00004229
Iteration 118/1000 | Loss: 0.00004228
Iteration 119/1000 | Loss: 0.00004228
Iteration 120/1000 | Loss: 0.00004228
Iteration 121/1000 | Loss: 0.00004228
Iteration 122/1000 | Loss: 0.00004228
Iteration 123/1000 | Loss: 0.00004227
Iteration 124/1000 | Loss: 0.00004227
Iteration 125/1000 | Loss: 0.00004227
Iteration 126/1000 | Loss: 0.00004226
Iteration 127/1000 | Loss: 0.00004226
Iteration 128/1000 | Loss: 0.00004226
Iteration 129/1000 | Loss: 0.00004226
Iteration 130/1000 | Loss: 0.00004226
Iteration 131/1000 | Loss: 0.00004226
Iteration 132/1000 | Loss: 0.00004226
Iteration 133/1000 | Loss: 0.00004226
Iteration 134/1000 | Loss: 0.00004225
Iteration 135/1000 | Loss: 0.00004225
Iteration 136/1000 | Loss: 0.00004225
Iteration 137/1000 | Loss: 0.00004225
Iteration 138/1000 | Loss: 0.00004225
Iteration 139/1000 | Loss: 0.00004225
Iteration 140/1000 | Loss: 0.00004225
Iteration 141/1000 | Loss: 0.00004225
Iteration 142/1000 | Loss: 0.00004224
Iteration 143/1000 | Loss: 0.00004224
Iteration 144/1000 | Loss: 0.00004224
Iteration 145/1000 | Loss: 0.00004224
Iteration 146/1000 | Loss: 0.00004224
Iteration 147/1000 | Loss: 0.00004224
Iteration 148/1000 | Loss: 0.00004223
Iteration 149/1000 | Loss: 0.00004223
Iteration 150/1000 | Loss: 0.00004223
Iteration 151/1000 | Loss: 0.00004223
Iteration 152/1000 | Loss: 0.00004223
Iteration 153/1000 | Loss: 0.00004223
Iteration 154/1000 | Loss: 0.00004223
Iteration 155/1000 | Loss: 0.00004223
Iteration 156/1000 | Loss: 0.00004223
Iteration 157/1000 | Loss: 0.00004223
Iteration 158/1000 | Loss: 0.00004223
Iteration 159/1000 | Loss: 0.00004223
Iteration 160/1000 | Loss: 0.00004222
Iteration 161/1000 | Loss: 0.00004222
Iteration 162/1000 | Loss: 0.00004222
Iteration 163/1000 | Loss: 0.00004222
Iteration 164/1000 | Loss: 0.00004222
Iteration 165/1000 | Loss: 0.00004222
Iteration 166/1000 | Loss: 0.00004222
Iteration 167/1000 | Loss: 0.00004221
Iteration 168/1000 | Loss: 0.00004221
Iteration 169/1000 | Loss: 0.00004221
Iteration 170/1000 | Loss: 0.00004221
Iteration 171/1000 | Loss: 0.00004221
Iteration 172/1000 | Loss: 0.00004221
Iteration 173/1000 | Loss: 0.00004221
Iteration 174/1000 | Loss: 0.00004221
Iteration 175/1000 | Loss: 0.00004221
Iteration 176/1000 | Loss: 0.00004221
Iteration 177/1000 | Loss: 0.00004221
Iteration 178/1000 | Loss: 0.00004220
Iteration 179/1000 | Loss: 0.00004220
Iteration 180/1000 | Loss: 0.00004220
Iteration 181/1000 | Loss: 0.00004220
Iteration 182/1000 | Loss: 0.00004220
Iteration 183/1000 | Loss: 0.00004219
Iteration 184/1000 | Loss: 0.00004219
Iteration 185/1000 | Loss: 0.00004219
Iteration 186/1000 | Loss: 0.00004218
Iteration 187/1000 | Loss: 0.00004218
Iteration 188/1000 | Loss: 0.00004218
Iteration 189/1000 | Loss: 0.00004218
Iteration 190/1000 | Loss: 0.00004218
Iteration 191/1000 | Loss: 0.00004218
Iteration 192/1000 | Loss: 0.00004218
Iteration 193/1000 | Loss: 0.00004218
Iteration 194/1000 | Loss: 0.00004218
Iteration 195/1000 | Loss: 0.00004217
Iteration 196/1000 | Loss: 0.00004217
Iteration 197/1000 | Loss: 0.00004217
Iteration 198/1000 | Loss: 0.00004217
Iteration 199/1000 | Loss: 0.00004217
Iteration 200/1000 | Loss: 0.00004217
Iteration 201/1000 | Loss: 0.00004217
Iteration 202/1000 | Loss: 0.00004216
Iteration 203/1000 | Loss: 0.00004216
Iteration 204/1000 | Loss: 0.00004216
Iteration 205/1000 | Loss: 0.00004216
Iteration 206/1000 | Loss: 0.00004216
Iteration 207/1000 | Loss: 0.00004216
Iteration 208/1000 | Loss: 0.00004215
Iteration 209/1000 | Loss: 0.00004215
Iteration 210/1000 | Loss: 0.00004215
Iteration 211/1000 | Loss: 0.00004215
Iteration 212/1000 | Loss: 0.00004215
Iteration 213/1000 | Loss: 0.00004215
Iteration 214/1000 | Loss: 0.00004215
Iteration 215/1000 | Loss: 0.00004215
Iteration 216/1000 | Loss: 0.00004215
Iteration 217/1000 | Loss: 0.00004214
Iteration 218/1000 | Loss: 0.00004214
Iteration 219/1000 | Loss: 0.00004214
Iteration 220/1000 | Loss: 0.00004214
Iteration 221/1000 | Loss: 0.00004214
Iteration 222/1000 | Loss: 0.00004214
Iteration 223/1000 | Loss: 0.00004214
Iteration 224/1000 | Loss: 0.00004214
Iteration 225/1000 | Loss: 0.00004214
Iteration 226/1000 | Loss: 0.00004214
Iteration 227/1000 | Loss: 0.00004214
Iteration 228/1000 | Loss: 0.00004213
Iteration 229/1000 | Loss: 0.00004213
Iteration 230/1000 | Loss: 0.00004213
Iteration 231/1000 | Loss: 0.00004212
Iteration 232/1000 | Loss: 0.00004212
Iteration 233/1000 | Loss: 0.00004212
Iteration 234/1000 | Loss: 0.00004212
Iteration 235/1000 | Loss: 0.00004212
Iteration 236/1000 | Loss: 0.00004212
Iteration 237/1000 | Loss: 0.00004211
Iteration 238/1000 | Loss: 0.00004211
Iteration 239/1000 | Loss: 0.00004211
Iteration 240/1000 | Loss: 0.00004211
Iteration 241/1000 | Loss: 0.00004211
Iteration 242/1000 | Loss: 0.00004211
Iteration 243/1000 | Loss: 0.00004211
Iteration 244/1000 | Loss: 0.00004211
Iteration 245/1000 | Loss: 0.00004211
Iteration 246/1000 | Loss: 0.00004210
Iteration 247/1000 | Loss: 0.00004210
Iteration 248/1000 | Loss: 0.00004210
Iteration 249/1000 | Loss: 0.00004210
Iteration 250/1000 | Loss: 0.00004210
Iteration 251/1000 | Loss: 0.00004210
Iteration 252/1000 | Loss: 0.00004210
Iteration 253/1000 | Loss: 0.00004209
Iteration 254/1000 | Loss: 0.00004209
Iteration 255/1000 | Loss: 0.00004209
Iteration 256/1000 | Loss: 0.00004209
Iteration 257/1000 | Loss: 0.00004209
Iteration 258/1000 | Loss: 0.00004209
Iteration 259/1000 | Loss: 0.00004209
Iteration 260/1000 | Loss: 0.00004209
Iteration 261/1000 | Loss: 0.00004209
Iteration 262/1000 | Loss: 0.00004208
Iteration 263/1000 | Loss: 0.00005926
Iteration 264/1000 | Loss: 0.00009840
Iteration 265/1000 | Loss: 0.00004233
Iteration 266/1000 | Loss: 0.00004208
Iteration 267/1000 | Loss: 0.00004205
Iteration 268/1000 | Loss: 0.00004205
Iteration 269/1000 | Loss: 0.00004205
Iteration 270/1000 | Loss: 0.00004204
Iteration 271/1000 | Loss: 0.00004204
Iteration 272/1000 | Loss: 0.00004204
Iteration 273/1000 | Loss: 0.00004204
Iteration 274/1000 | Loss: 0.00004204
Iteration 275/1000 | Loss: 0.00004204
Iteration 276/1000 | Loss: 0.00004204
Iteration 277/1000 | Loss: 0.00004204
Iteration 278/1000 | Loss: 0.00004203
Iteration 279/1000 | Loss: 0.00004203
Iteration 280/1000 | Loss: 0.00004203
Iteration 281/1000 | Loss: 0.00004203
Iteration 282/1000 | Loss: 0.00004203
Iteration 283/1000 | Loss: 0.00004203
Iteration 284/1000 | Loss: 0.00004203
Iteration 285/1000 | Loss: 0.00004203
Iteration 286/1000 | Loss: 0.00004203
Iteration 287/1000 | Loss: 0.00004203
Iteration 288/1000 | Loss: 0.00004203
Iteration 289/1000 | Loss: 0.00004203
Iteration 290/1000 | Loss: 0.00004203
Iteration 291/1000 | Loss: 0.00004203
Iteration 292/1000 | Loss: 0.00004203
Iteration 293/1000 | Loss: 0.00004203
Iteration 294/1000 | Loss: 0.00004203
Iteration 295/1000 | Loss: 0.00004203
Iteration 296/1000 | Loss: 0.00004203
Iteration 297/1000 | Loss: 0.00004203
Iteration 298/1000 | Loss: 0.00004203
Iteration 299/1000 | Loss: 0.00004203
Iteration 300/1000 | Loss: 0.00004203
Iteration 301/1000 | Loss: 0.00004203
Iteration 302/1000 | Loss: 0.00004203
Iteration 303/1000 | Loss: 0.00004203
Iteration 304/1000 | Loss: 0.00004203
Iteration 305/1000 | Loss: 0.00004203
Iteration 306/1000 | Loss: 0.00004203
Iteration 307/1000 | Loss: 0.00004203
Iteration 308/1000 | Loss: 0.00004203
Iteration 309/1000 | Loss: 0.00004203
Iteration 310/1000 | Loss: 0.00004203
Iteration 311/1000 | Loss: 0.00004203
Iteration 312/1000 | Loss: 0.00004203
Iteration 313/1000 | Loss: 0.00004203
Iteration 314/1000 | Loss: 0.00004203
Iteration 315/1000 | Loss: 0.00004203
Iteration 316/1000 | Loss: 0.00004203
Iteration 317/1000 | Loss: 0.00004203
Iteration 318/1000 | Loss: 0.00004203
Iteration 319/1000 | Loss: 0.00004203
Iteration 320/1000 | Loss: 0.00004203
Iteration 321/1000 | Loss: 0.00004203
Iteration 322/1000 | Loss: 0.00004203
Iteration 323/1000 | Loss: 0.00004203
Iteration 324/1000 | Loss: 0.00004203
Iteration 325/1000 | Loss: 0.00004203
Iteration 326/1000 | Loss: 0.00004203
Iteration 327/1000 | Loss: 0.00004203
Iteration 328/1000 | Loss: 0.00004203
Iteration 329/1000 | Loss: 0.00004203
Iteration 330/1000 | Loss: 0.00004203
Iteration 331/1000 | Loss: 0.00004203
Iteration 332/1000 | Loss: 0.00004203
Iteration 333/1000 | Loss: 0.00004203
Iteration 334/1000 | Loss: 0.00004203
Iteration 335/1000 | Loss: 0.00004203
Iteration 336/1000 | Loss: 0.00004203
Iteration 337/1000 | Loss: 0.00004203
Iteration 338/1000 | Loss: 0.00004203
Iteration 339/1000 | Loss: 0.00004203
Iteration 340/1000 | Loss: 0.00004203
Iteration 341/1000 | Loss: 0.00004203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [4.203016578685492e-05, 4.203016578685492e-05, 4.203016578685492e-05, 4.203016578685492e-05, 4.203016578685492e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.203016578685492e-05

Optimization complete. Final v2v error: 4.647336483001709 mm

Highest mean error: 11.22607707977295 mm for frame 116

Lowest mean error: 3.3422739505767822 mm for frame 1

Saving results

Total time: 190.81045246124268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817370
Iteration 2/25 | Loss: 0.00129229
Iteration 3/25 | Loss: 0.00121248
Iteration 4/25 | Loss: 0.00120376
Iteration 5/25 | Loss: 0.00120220
Iteration 6/25 | Loss: 0.00120220
Iteration 7/25 | Loss: 0.00120220
Iteration 8/25 | Loss: 0.00120220
Iteration 9/25 | Loss: 0.00120220
Iteration 10/25 | Loss: 0.00120220
Iteration 11/25 | Loss: 0.00120217
Iteration 12/25 | Loss: 0.00120217
Iteration 13/25 | Loss: 0.00120217
Iteration 14/25 | Loss: 0.00120217
Iteration 15/25 | Loss: 0.00120217
Iteration 16/25 | Loss: 0.00120217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012021661968901753, 0.0012021661968901753, 0.0012021661968901753, 0.0012021661968901753, 0.0012021661968901753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012021661968901753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34042037
Iteration 2/25 | Loss: 0.00088595
Iteration 3/25 | Loss: 0.00088593
Iteration 4/25 | Loss: 0.00088593
Iteration 5/25 | Loss: 0.00088593
Iteration 6/25 | Loss: 0.00088593
Iteration 7/25 | Loss: 0.00088593
Iteration 8/25 | Loss: 0.00088593
Iteration 9/25 | Loss: 0.00088593
Iteration 10/25 | Loss: 0.00088593
Iteration 11/25 | Loss: 0.00088593
Iteration 12/25 | Loss: 0.00088593
Iteration 13/25 | Loss: 0.00088593
Iteration 14/25 | Loss: 0.00088593
Iteration 15/25 | Loss: 0.00088593
Iteration 16/25 | Loss: 0.00088593
Iteration 17/25 | Loss: 0.00088593
Iteration 18/25 | Loss: 0.00088593
Iteration 19/25 | Loss: 0.00088593
Iteration 20/25 | Loss: 0.00088593
Iteration 21/25 | Loss: 0.00088593
Iteration 22/25 | Loss: 0.00088593
Iteration 23/25 | Loss: 0.00088593
Iteration 24/25 | Loss: 0.00088593
Iteration 25/25 | Loss: 0.00088593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088593
Iteration 2/1000 | Loss: 0.00002004
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001329
Iteration 5/1000 | Loss: 0.00001252
Iteration 6/1000 | Loss: 0.00001192
Iteration 7/1000 | Loss: 0.00001145
Iteration 8/1000 | Loss: 0.00001118
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001087
Iteration 11/1000 | Loss: 0.00001067
Iteration 12/1000 | Loss: 0.00001053
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001040
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001023
Iteration 17/1000 | Loss: 0.00001022
Iteration 18/1000 | Loss: 0.00001022
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001017
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001016
Iteration 25/1000 | Loss: 0.00001015
Iteration 26/1000 | Loss: 0.00001013
Iteration 27/1000 | Loss: 0.00001013
Iteration 28/1000 | Loss: 0.00001012
Iteration 29/1000 | Loss: 0.00001011
Iteration 30/1000 | Loss: 0.00001011
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001011
Iteration 34/1000 | Loss: 0.00001011
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001011
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001008
Iteration 41/1000 | Loss: 0.00001008
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000995
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000993
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000990
Iteration 87/1000 | Loss: 0.00000990
Iteration 88/1000 | Loss: 0.00000989
Iteration 89/1000 | Loss: 0.00000989
Iteration 90/1000 | Loss: 0.00000988
Iteration 91/1000 | Loss: 0.00000988
Iteration 92/1000 | Loss: 0.00000988
Iteration 93/1000 | Loss: 0.00000988
Iteration 94/1000 | Loss: 0.00000988
Iteration 95/1000 | Loss: 0.00000987
Iteration 96/1000 | Loss: 0.00000987
Iteration 97/1000 | Loss: 0.00000987
Iteration 98/1000 | Loss: 0.00000986
Iteration 99/1000 | Loss: 0.00000986
Iteration 100/1000 | Loss: 0.00000986
Iteration 101/1000 | Loss: 0.00000986
Iteration 102/1000 | Loss: 0.00000986
Iteration 103/1000 | Loss: 0.00000986
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000985
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000984
Iteration 109/1000 | Loss: 0.00000984
Iteration 110/1000 | Loss: 0.00000984
Iteration 111/1000 | Loss: 0.00000984
Iteration 112/1000 | Loss: 0.00000984
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000983
Iteration 115/1000 | Loss: 0.00000983
Iteration 116/1000 | Loss: 0.00000983
Iteration 117/1000 | Loss: 0.00000982
Iteration 118/1000 | Loss: 0.00000982
Iteration 119/1000 | Loss: 0.00000982
Iteration 120/1000 | Loss: 0.00000982
Iteration 121/1000 | Loss: 0.00000982
Iteration 122/1000 | Loss: 0.00000981
Iteration 123/1000 | Loss: 0.00000981
Iteration 124/1000 | Loss: 0.00000981
Iteration 125/1000 | Loss: 0.00000981
Iteration 126/1000 | Loss: 0.00000981
Iteration 127/1000 | Loss: 0.00000980
Iteration 128/1000 | Loss: 0.00000980
Iteration 129/1000 | Loss: 0.00000980
Iteration 130/1000 | Loss: 0.00000980
Iteration 131/1000 | Loss: 0.00000980
Iteration 132/1000 | Loss: 0.00000980
Iteration 133/1000 | Loss: 0.00000980
Iteration 134/1000 | Loss: 0.00000980
Iteration 135/1000 | Loss: 0.00000980
Iteration 136/1000 | Loss: 0.00000980
Iteration 137/1000 | Loss: 0.00000980
Iteration 138/1000 | Loss: 0.00000979
Iteration 139/1000 | Loss: 0.00000979
Iteration 140/1000 | Loss: 0.00000979
Iteration 141/1000 | Loss: 0.00000979
Iteration 142/1000 | Loss: 0.00000979
Iteration 143/1000 | Loss: 0.00000979
Iteration 144/1000 | Loss: 0.00000979
Iteration 145/1000 | Loss: 0.00000978
Iteration 146/1000 | Loss: 0.00000978
Iteration 147/1000 | Loss: 0.00000978
Iteration 148/1000 | Loss: 0.00000978
Iteration 149/1000 | Loss: 0.00000978
Iteration 150/1000 | Loss: 0.00000977
Iteration 151/1000 | Loss: 0.00000977
Iteration 152/1000 | Loss: 0.00000977
Iteration 153/1000 | Loss: 0.00000977
Iteration 154/1000 | Loss: 0.00000977
Iteration 155/1000 | Loss: 0.00000977
Iteration 156/1000 | Loss: 0.00000977
Iteration 157/1000 | Loss: 0.00000976
Iteration 158/1000 | Loss: 0.00000976
Iteration 159/1000 | Loss: 0.00000976
Iteration 160/1000 | Loss: 0.00000976
Iteration 161/1000 | Loss: 0.00000976
Iteration 162/1000 | Loss: 0.00000976
Iteration 163/1000 | Loss: 0.00000976
Iteration 164/1000 | Loss: 0.00000976
Iteration 165/1000 | Loss: 0.00000976
Iteration 166/1000 | Loss: 0.00000976
Iteration 167/1000 | Loss: 0.00000976
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000976
Iteration 171/1000 | Loss: 0.00000976
Iteration 172/1000 | Loss: 0.00000976
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000976
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Iteration 179/1000 | Loss: 0.00000976
Iteration 180/1000 | Loss: 0.00000976
Iteration 181/1000 | Loss: 0.00000976
Iteration 182/1000 | Loss: 0.00000976
Iteration 183/1000 | Loss: 0.00000976
Iteration 184/1000 | Loss: 0.00000976
Iteration 185/1000 | Loss: 0.00000976
Iteration 186/1000 | Loss: 0.00000976
Iteration 187/1000 | Loss: 0.00000976
Iteration 188/1000 | Loss: 0.00000976
Iteration 189/1000 | Loss: 0.00000976
Iteration 190/1000 | Loss: 0.00000976
Iteration 191/1000 | Loss: 0.00000976
Iteration 192/1000 | Loss: 0.00000976
Iteration 193/1000 | Loss: 0.00000976
Iteration 194/1000 | Loss: 0.00000976
Iteration 195/1000 | Loss: 0.00000976
Iteration 196/1000 | Loss: 0.00000976
Iteration 197/1000 | Loss: 0.00000976
Iteration 198/1000 | Loss: 0.00000976
Iteration 199/1000 | Loss: 0.00000976
Iteration 200/1000 | Loss: 0.00000976
Iteration 201/1000 | Loss: 0.00000976
Iteration 202/1000 | Loss: 0.00000976
Iteration 203/1000 | Loss: 0.00000976
Iteration 204/1000 | Loss: 0.00000976
Iteration 205/1000 | Loss: 0.00000976
Iteration 206/1000 | Loss: 0.00000976
Iteration 207/1000 | Loss: 0.00000976
Iteration 208/1000 | Loss: 0.00000976
Iteration 209/1000 | Loss: 0.00000976
Iteration 210/1000 | Loss: 0.00000976
Iteration 211/1000 | Loss: 0.00000976
Iteration 212/1000 | Loss: 0.00000976
Iteration 213/1000 | Loss: 0.00000976
Iteration 214/1000 | Loss: 0.00000976
Iteration 215/1000 | Loss: 0.00000976
Iteration 216/1000 | Loss: 0.00000976
Iteration 217/1000 | Loss: 0.00000976
Iteration 218/1000 | Loss: 0.00000976
Iteration 219/1000 | Loss: 0.00000976
Iteration 220/1000 | Loss: 0.00000976
Iteration 221/1000 | Loss: 0.00000976
Iteration 222/1000 | Loss: 0.00000976
Iteration 223/1000 | Loss: 0.00000976
Iteration 224/1000 | Loss: 0.00000976
Iteration 225/1000 | Loss: 0.00000976
Iteration 226/1000 | Loss: 0.00000976
Iteration 227/1000 | Loss: 0.00000976
Iteration 228/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [9.758722626429517e-06, 9.758722626429517e-06, 9.758722626429517e-06, 9.758722626429517e-06, 9.758722626429517e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.758722626429517e-06

Optimization complete. Final v2v error: 2.6878201961517334 mm

Highest mean error: 2.8091044425964355 mm for frame 1

Lowest mean error: 2.599832057952881 mm for frame 43

Saving results

Total time: 38.5966272354126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997592
Iteration 2/25 | Loss: 0.00997592
Iteration 3/25 | Loss: 0.00250250
Iteration 4/25 | Loss: 0.00199009
Iteration 5/25 | Loss: 0.00188876
Iteration 6/25 | Loss: 0.00195552
Iteration 7/25 | Loss: 0.00181224
Iteration 8/25 | Loss: 0.00173789
Iteration 9/25 | Loss: 0.00169867
Iteration 10/25 | Loss: 0.00166531
Iteration 11/25 | Loss: 0.00164382
Iteration 12/25 | Loss: 0.00163124
Iteration 13/25 | Loss: 0.00162693
Iteration 14/25 | Loss: 0.00161947
Iteration 15/25 | Loss: 0.00161435
Iteration 16/25 | Loss: 0.00161225
Iteration 17/25 | Loss: 0.00161123
Iteration 18/25 | Loss: 0.00161071
Iteration 19/25 | Loss: 0.00161046
Iteration 20/25 | Loss: 0.00161033
Iteration 21/25 | Loss: 0.00161025
Iteration 22/25 | Loss: 0.00161024
Iteration 23/25 | Loss: 0.00161024
Iteration 24/25 | Loss: 0.00161023
Iteration 25/25 | Loss: 0.00161023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27782416
Iteration 2/25 | Loss: 0.00573206
Iteration 3/25 | Loss: 0.00573205
Iteration 4/25 | Loss: 0.00573205
Iteration 5/25 | Loss: 0.00573205
Iteration 6/25 | Loss: 0.00573205
Iteration 7/25 | Loss: 0.00573205
Iteration 8/25 | Loss: 0.00573205
Iteration 9/25 | Loss: 0.00573205
Iteration 10/25 | Loss: 0.00573205
Iteration 11/25 | Loss: 0.00573205
Iteration 12/25 | Loss: 0.00573205
Iteration 13/25 | Loss: 0.00573205
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.005732052028179169, 0.005732052028179169, 0.005732052028179169, 0.005732052028179169, 0.005732052028179169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005732052028179169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00573205
Iteration 2/1000 | Loss: 0.00068073
Iteration 3/1000 | Loss: 0.00067745
Iteration 4/1000 | Loss: 0.00038627
Iteration 5/1000 | Loss: 0.00033930
Iteration 6/1000 | Loss: 0.00031124
Iteration 7/1000 | Loss: 0.00027503
Iteration 8/1000 | Loss: 0.00025925
Iteration 9/1000 | Loss: 0.00024986
Iteration 10/1000 | Loss: 0.00024056
Iteration 11/1000 | Loss: 0.00023618
Iteration 12/1000 | Loss: 0.00023369
Iteration 13/1000 | Loss: 0.00023205
Iteration 14/1000 | Loss: 0.00023072
Iteration 15/1000 | Loss: 0.00022973
Iteration 16/1000 | Loss: 0.00022886
Iteration 17/1000 | Loss: 0.00022840
Iteration 18/1000 | Loss: 0.00022798
Iteration 19/1000 | Loss: 0.00022760
Iteration 20/1000 | Loss: 0.00022717
Iteration 21/1000 | Loss: 0.00022692
Iteration 22/1000 | Loss: 0.00022673
Iteration 23/1000 | Loss: 0.00022670
Iteration 24/1000 | Loss: 0.00022649
Iteration 25/1000 | Loss: 0.00022626
Iteration 26/1000 | Loss: 0.00022883
Iteration 27/1000 | Loss: 0.00022591
Iteration 28/1000 | Loss: 0.00022483
Iteration 29/1000 | Loss: 0.00022301
Iteration 30/1000 | Loss: 0.00022082
Iteration 31/1000 | Loss: 0.00021685
Iteration 32/1000 | Loss: 0.00071864
Iteration 33/1000 | Loss: 0.00208165
Iteration 34/1000 | Loss: 0.00391061
Iteration 35/1000 | Loss: 0.00030651
Iteration 36/1000 | Loss: 0.00071397
Iteration 37/1000 | Loss: 0.00068314
Iteration 38/1000 | Loss: 0.00164026
Iteration 39/1000 | Loss: 0.00109954
Iteration 40/1000 | Loss: 0.00131647
Iteration 41/1000 | Loss: 0.00059501
Iteration 42/1000 | Loss: 0.00018977
Iteration 43/1000 | Loss: 0.00017805
Iteration 44/1000 | Loss: 0.00016799
Iteration 45/1000 | Loss: 0.00016131
Iteration 46/1000 | Loss: 0.00015708
Iteration 47/1000 | Loss: 0.00015355
Iteration 48/1000 | Loss: 0.00015120
Iteration 49/1000 | Loss: 0.00014910
Iteration 50/1000 | Loss: 0.00014691
Iteration 51/1000 | Loss: 0.00014560
Iteration 52/1000 | Loss: 0.00014429
Iteration 53/1000 | Loss: 0.00014361
Iteration 54/1000 | Loss: 0.00014206
Iteration 55/1000 | Loss: 0.00114042
Iteration 56/1000 | Loss: 0.01284028
Iteration 57/1000 | Loss: 0.01351452
Iteration 58/1000 | Loss: 0.00058815
Iteration 59/1000 | Loss: 0.00046453
Iteration 60/1000 | Loss: 0.00055708
Iteration 61/1000 | Loss: 0.00041562
Iteration 62/1000 | Loss: 0.00032117
Iteration 63/1000 | Loss: 0.00070284
Iteration 64/1000 | Loss: 0.00063294
Iteration 65/1000 | Loss: 0.00055165
Iteration 66/1000 | Loss: 0.00026557
Iteration 67/1000 | Loss: 0.00009833
Iteration 68/1000 | Loss: 0.00006246
Iteration 69/1000 | Loss: 0.00005046
Iteration 70/1000 | Loss: 0.00004270
Iteration 71/1000 | Loss: 0.00003793
Iteration 72/1000 | Loss: 0.00003369
Iteration 73/1000 | Loss: 0.00003052
Iteration 74/1000 | Loss: 0.00002884
Iteration 75/1000 | Loss: 0.00002688
Iteration 76/1000 | Loss: 0.00002529
Iteration 77/1000 | Loss: 0.00082385
Iteration 78/1000 | Loss: 0.00004417
Iteration 79/1000 | Loss: 0.00030430
Iteration 80/1000 | Loss: 0.00003022
Iteration 81/1000 | Loss: 0.00002321
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00070950
Iteration 84/1000 | Loss: 0.00004189
Iteration 85/1000 | Loss: 0.00021568
Iteration 86/1000 | Loss: 0.00002247
Iteration 87/1000 | Loss: 0.00002078
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001784
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001658
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001645
Iteration 102/1000 | Loss: 0.00001642
Iteration 103/1000 | Loss: 0.00001641
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001639
Iteration 106/1000 | Loss: 0.00001639
Iteration 107/1000 | Loss: 0.00001634
Iteration 108/1000 | Loss: 0.00001634
Iteration 109/1000 | Loss: 0.00001633
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001627
Iteration 116/1000 | Loss: 0.00001626
Iteration 117/1000 | Loss: 0.00001626
Iteration 118/1000 | Loss: 0.00001626
Iteration 119/1000 | Loss: 0.00001626
Iteration 120/1000 | Loss: 0.00001626
Iteration 121/1000 | Loss: 0.00001626
Iteration 122/1000 | Loss: 0.00001625
Iteration 123/1000 | Loss: 0.00001625
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001622
Iteration 126/1000 | Loss: 0.00001622
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001612
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001609
Iteration 153/1000 | Loss: 0.00001609
Iteration 154/1000 | Loss: 0.00001609
Iteration 155/1000 | Loss: 0.00001609
Iteration 156/1000 | Loss: 0.00001609
Iteration 157/1000 | Loss: 0.00001609
Iteration 158/1000 | Loss: 0.00001609
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001608
Iteration 166/1000 | Loss: 0.00001608
Iteration 167/1000 | Loss: 0.00001608
Iteration 168/1000 | Loss: 0.00001608
Iteration 169/1000 | Loss: 0.00001608
Iteration 170/1000 | Loss: 0.00001608
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001607
Iteration 182/1000 | Loss: 0.00001607
Iteration 183/1000 | Loss: 0.00001607
Iteration 184/1000 | Loss: 0.00001607
Iteration 185/1000 | Loss: 0.00001607
Iteration 186/1000 | Loss: 0.00001607
Iteration 187/1000 | Loss: 0.00001607
Iteration 188/1000 | Loss: 0.00001607
Iteration 189/1000 | Loss: 0.00001607
Iteration 190/1000 | Loss: 0.00001607
Iteration 191/1000 | Loss: 0.00001607
Iteration 192/1000 | Loss: 0.00001607
Iteration 193/1000 | Loss: 0.00001607
Iteration 194/1000 | Loss: 0.00001607
Iteration 195/1000 | Loss: 0.00001607
Iteration 196/1000 | Loss: 0.00001607
Iteration 197/1000 | Loss: 0.00001607
Iteration 198/1000 | Loss: 0.00001607
Iteration 199/1000 | Loss: 0.00001606
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001606
Iteration 207/1000 | Loss: 0.00001606
Iteration 208/1000 | Loss: 0.00001606
Iteration 209/1000 | Loss: 0.00001606
Iteration 210/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.60622348630568e-05, 1.60622348630568e-05, 1.60622348630568e-05, 1.60622348630568e-05, 1.60622348630568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.60622348630568e-05

Optimization complete. Final v2v error: 3.349034309387207 mm

Highest mean error: 4.276951313018799 mm for frame 212

Lowest mean error: 3.194582223892212 mm for frame 69

Saving results

Total time: 197.12264704704285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395600
Iteration 2/25 | Loss: 0.00125496
Iteration 3/25 | Loss: 0.00119281
Iteration 4/25 | Loss: 0.00118408
Iteration 5/25 | Loss: 0.00118187
Iteration 6/25 | Loss: 0.00118187
Iteration 7/25 | Loss: 0.00118187
Iteration 8/25 | Loss: 0.00118187
Iteration 9/25 | Loss: 0.00118187
Iteration 10/25 | Loss: 0.00118187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011818708153441548, 0.0011818708153441548, 0.0011818708153441548, 0.0011818708153441548, 0.0011818708153441548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011818708153441548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39709663
Iteration 2/25 | Loss: 0.00099725
Iteration 3/25 | Loss: 0.00099725
Iteration 4/25 | Loss: 0.00099725
Iteration 5/25 | Loss: 0.00099725
Iteration 6/25 | Loss: 0.00099725
Iteration 7/25 | Loss: 0.00099725
Iteration 8/25 | Loss: 0.00099725
Iteration 9/25 | Loss: 0.00099725
Iteration 10/25 | Loss: 0.00099725
Iteration 11/25 | Loss: 0.00099725
Iteration 12/25 | Loss: 0.00099725
Iteration 13/25 | Loss: 0.00099725
Iteration 14/25 | Loss: 0.00099725
Iteration 15/25 | Loss: 0.00099725
Iteration 16/25 | Loss: 0.00099725
Iteration 17/25 | Loss: 0.00099725
Iteration 18/25 | Loss: 0.00099725
Iteration 19/25 | Loss: 0.00099725
Iteration 20/25 | Loss: 0.00099725
Iteration 21/25 | Loss: 0.00099725
Iteration 22/25 | Loss: 0.00099725
Iteration 23/25 | Loss: 0.00099725
Iteration 24/25 | Loss: 0.00099725
Iteration 25/25 | Loss: 0.00099725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099725
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001594
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001154
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001131
Iteration 12/1000 | Loss: 0.00001128
Iteration 13/1000 | Loss: 0.00001108
Iteration 14/1000 | Loss: 0.00001105
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001094
Iteration 18/1000 | Loss: 0.00001093
Iteration 19/1000 | Loss: 0.00001090
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001078
Iteration 23/1000 | Loss: 0.00001064
Iteration 24/1000 | Loss: 0.00001063
Iteration 25/1000 | Loss: 0.00001062
Iteration 26/1000 | Loss: 0.00001060
Iteration 27/1000 | Loss: 0.00001055
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001049
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001047
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001046
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001042
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001040
Iteration 45/1000 | Loss: 0.00001040
Iteration 46/1000 | Loss: 0.00001040
Iteration 47/1000 | Loss: 0.00001039
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001038
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001037
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001035
Iteration 58/1000 | Loss: 0.00001033
Iteration 59/1000 | Loss: 0.00001032
Iteration 60/1000 | Loss: 0.00001032
Iteration 61/1000 | Loss: 0.00001031
Iteration 62/1000 | Loss: 0.00001031
Iteration 63/1000 | Loss: 0.00001031
Iteration 64/1000 | Loss: 0.00001030
Iteration 65/1000 | Loss: 0.00001030
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001026
Iteration 72/1000 | Loss: 0.00001026
Iteration 73/1000 | Loss: 0.00001025
Iteration 74/1000 | Loss: 0.00001025
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001025
Iteration 77/1000 | Loss: 0.00001025
Iteration 78/1000 | Loss: 0.00001025
Iteration 79/1000 | Loss: 0.00001025
Iteration 80/1000 | Loss: 0.00001025
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001024
Iteration 84/1000 | Loss: 0.00001024
Iteration 85/1000 | Loss: 0.00001023
Iteration 86/1000 | Loss: 0.00001022
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001021
Iteration 91/1000 | Loss: 0.00001021
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001019
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001018
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001016
Iteration 105/1000 | Loss: 0.00001016
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001016
Iteration 109/1000 | Loss: 0.00001016
Iteration 110/1000 | Loss: 0.00001016
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001015
Iteration 115/1000 | Loss: 0.00001015
Iteration 116/1000 | Loss: 0.00001015
Iteration 117/1000 | Loss: 0.00001015
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001014
Iteration 121/1000 | Loss: 0.00001014
Iteration 122/1000 | Loss: 0.00001014
Iteration 123/1000 | Loss: 0.00001014
Iteration 124/1000 | Loss: 0.00001014
Iteration 125/1000 | Loss: 0.00001014
Iteration 126/1000 | Loss: 0.00001014
Iteration 127/1000 | Loss: 0.00001014
Iteration 128/1000 | Loss: 0.00001014
Iteration 129/1000 | Loss: 0.00001013
Iteration 130/1000 | Loss: 0.00001013
Iteration 131/1000 | Loss: 0.00001013
Iteration 132/1000 | Loss: 0.00001013
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001013
Iteration 138/1000 | Loss: 0.00001013
Iteration 139/1000 | Loss: 0.00001013
Iteration 140/1000 | Loss: 0.00001013
Iteration 141/1000 | Loss: 0.00001013
Iteration 142/1000 | Loss: 0.00001013
Iteration 143/1000 | Loss: 0.00001013
Iteration 144/1000 | Loss: 0.00001013
Iteration 145/1000 | Loss: 0.00001013
Iteration 146/1000 | Loss: 0.00001013
Iteration 147/1000 | Loss: 0.00001013
Iteration 148/1000 | Loss: 0.00001013
Iteration 149/1000 | Loss: 0.00001013
Iteration 150/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.0127166206075344e-05, 1.0127166206075344e-05, 1.0127166206075344e-05, 1.0127166206075344e-05, 1.0127166206075344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0127166206075344e-05

Optimization complete. Final v2v error: 2.7648236751556396 mm

Highest mean error: 2.9603111743927 mm for frame 126

Lowest mean error: 2.663829803466797 mm for frame 0

Saving results

Total time: 42.828147172927856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807094
Iteration 2/25 | Loss: 0.00151543
Iteration 3/25 | Loss: 0.00133291
Iteration 4/25 | Loss: 0.00130818
Iteration 5/25 | Loss: 0.00130892
Iteration 6/25 | Loss: 0.00131475
Iteration 7/25 | Loss: 0.00131469
Iteration 8/25 | Loss: 0.00129502
Iteration 9/25 | Loss: 0.00128373
Iteration 10/25 | Loss: 0.00128113
Iteration 11/25 | Loss: 0.00128027
Iteration 12/25 | Loss: 0.00128008
Iteration 13/25 | Loss: 0.00127999
Iteration 14/25 | Loss: 0.00127994
Iteration 15/25 | Loss: 0.00127994
Iteration 16/25 | Loss: 0.00127994
Iteration 17/25 | Loss: 0.00127994
Iteration 18/25 | Loss: 0.00127994
Iteration 19/25 | Loss: 0.00127994
Iteration 20/25 | Loss: 0.00127994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001279938267543912, 0.001279938267543912, 0.001279938267543912, 0.001279938267543912, 0.001279938267543912]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001279938267543912

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28502512
Iteration 2/25 | Loss: 0.00114792
Iteration 3/25 | Loss: 0.00114792
Iteration 4/25 | Loss: 0.00114792
Iteration 5/25 | Loss: 0.00114792
Iteration 6/25 | Loss: 0.00114792
Iteration 7/25 | Loss: 0.00114792
Iteration 8/25 | Loss: 0.00114792
Iteration 9/25 | Loss: 0.00114792
Iteration 10/25 | Loss: 0.00114792
Iteration 11/25 | Loss: 0.00114792
Iteration 12/25 | Loss: 0.00114792
Iteration 13/25 | Loss: 0.00114792
Iteration 14/25 | Loss: 0.00114792
Iteration 15/25 | Loss: 0.00114792
Iteration 16/25 | Loss: 0.00114792
Iteration 17/25 | Loss: 0.00114792
Iteration 18/25 | Loss: 0.00114792
Iteration 19/25 | Loss: 0.00114792
Iteration 20/25 | Loss: 0.00114792
Iteration 21/25 | Loss: 0.00114792
Iteration 22/25 | Loss: 0.00114792
Iteration 23/25 | Loss: 0.00114792
Iteration 24/25 | Loss: 0.00114792
Iteration 25/25 | Loss: 0.00114792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114792
Iteration 2/1000 | Loss: 0.00004909
Iteration 3/1000 | Loss: 0.00003206
Iteration 4/1000 | Loss: 0.00002831
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002543
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002389
Iteration 9/1000 | Loss: 0.00002346
Iteration 10/1000 | Loss: 0.00002307
Iteration 11/1000 | Loss: 0.00002276
Iteration 12/1000 | Loss: 0.00002259
Iteration 13/1000 | Loss: 0.00002253
Iteration 14/1000 | Loss: 0.00002245
Iteration 15/1000 | Loss: 0.00002238
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002228
Iteration 18/1000 | Loss: 0.00002227
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002225
Iteration 21/1000 | Loss: 0.00002221
Iteration 22/1000 | Loss: 0.00002215
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002211
Iteration 25/1000 | Loss: 0.00002211
Iteration 26/1000 | Loss: 0.00002210
Iteration 27/1000 | Loss: 0.00002209
Iteration 28/1000 | Loss: 0.00002208
Iteration 29/1000 | Loss: 0.00002207
Iteration 30/1000 | Loss: 0.00002206
Iteration 31/1000 | Loss: 0.00002206
Iteration 32/1000 | Loss: 0.00002203
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002199
Iteration 37/1000 | Loss: 0.00002199
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002196
Iteration 44/1000 | Loss: 0.00002196
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002195
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002191
Iteration 56/1000 | Loss: 0.00002191
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002190
Iteration 59/1000 | Loss: 0.00002190
Iteration 60/1000 | Loss: 0.00002190
Iteration 61/1000 | Loss: 0.00002189
Iteration 62/1000 | Loss: 0.00002189
Iteration 63/1000 | Loss: 0.00002189
Iteration 64/1000 | Loss: 0.00002189
Iteration 65/1000 | Loss: 0.00002189
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002188
Iteration 68/1000 | Loss: 0.00002188
Iteration 69/1000 | Loss: 0.00002188
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002188
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002188
Iteration 76/1000 | Loss: 0.00002188
Iteration 77/1000 | Loss: 0.00002188
Iteration 78/1000 | Loss: 0.00002188
Iteration 79/1000 | Loss: 0.00002188
Iteration 80/1000 | Loss: 0.00002188
Iteration 81/1000 | Loss: 0.00002188
Iteration 82/1000 | Loss: 0.00002188
Iteration 83/1000 | Loss: 0.00002188
Iteration 84/1000 | Loss: 0.00002188
Iteration 85/1000 | Loss: 0.00002188
Iteration 86/1000 | Loss: 0.00002188
Iteration 87/1000 | Loss: 0.00002188
Iteration 88/1000 | Loss: 0.00002188
Iteration 89/1000 | Loss: 0.00002188
Iteration 90/1000 | Loss: 0.00002188
Iteration 91/1000 | Loss: 0.00002188
Iteration 92/1000 | Loss: 0.00002188
Iteration 93/1000 | Loss: 0.00002188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.188035250583198e-05, 2.188035250583198e-05, 2.188035250583198e-05, 2.188035250583198e-05, 2.188035250583198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.188035250583198e-05

Optimization complete. Final v2v error: 3.867835283279419 mm

Highest mean error: 4.80815315246582 mm for frame 162

Lowest mean error: 3.142575740814209 mm for frame 101

Saving results

Total time: 56.03924202919006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982805
Iteration 2/25 | Loss: 0.00267381
Iteration 3/25 | Loss: 0.00199640
Iteration 4/25 | Loss: 0.00187382
Iteration 5/25 | Loss: 0.00176548
Iteration 6/25 | Loss: 0.00175314
Iteration 7/25 | Loss: 0.00171689
Iteration 8/25 | Loss: 0.00167666
Iteration 9/25 | Loss: 0.00163526
Iteration 10/25 | Loss: 0.00159066
Iteration 11/25 | Loss: 0.00153892
Iteration 12/25 | Loss: 0.00150896
Iteration 13/25 | Loss: 0.00153268
Iteration 14/25 | Loss: 0.00147276
Iteration 15/25 | Loss: 0.00142842
Iteration 16/25 | Loss: 0.00142189
Iteration 17/25 | Loss: 0.00141405
Iteration 18/25 | Loss: 0.00141601
Iteration 19/25 | Loss: 0.00141810
Iteration 20/25 | Loss: 0.00141414
Iteration 21/25 | Loss: 0.00140374
Iteration 22/25 | Loss: 0.00139992
Iteration 23/25 | Loss: 0.00139890
Iteration 24/25 | Loss: 0.00139813
Iteration 25/25 | Loss: 0.00139791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33182800
Iteration 2/25 | Loss: 0.00184498
Iteration 3/25 | Loss: 0.00163207
Iteration 4/25 | Loss: 0.00163198
Iteration 5/25 | Loss: 0.00163198
Iteration 6/25 | Loss: 0.00163198
Iteration 7/25 | Loss: 0.00163198
Iteration 8/25 | Loss: 0.00163198
Iteration 9/25 | Loss: 0.00163198
Iteration 10/25 | Loss: 0.00163198
Iteration 11/25 | Loss: 0.00163198
Iteration 12/25 | Loss: 0.00163197
Iteration 13/25 | Loss: 0.00163197
Iteration 14/25 | Loss: 0.00163197
Iteration 15/25 | Loss: 0.00163197
Iteration 16/25 | Loss: 0.00163197
Iteration 17/25 | Loss: 0.00163197
Iteration 18/25 | Loss: 0.00163197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016319748247042298, 0.0016319748247042298, 0.0016319748247042298, 0.0016319748247042298, 0.0016319748247042298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016319748247042298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163197
Iteration 2/1000 | Loss: 0.00111811
Iteration 3/1000 | Loss: 0.00070901
Iteration 4/1000 | Loss: 0.00012780
Iteration 5/1000 | Loss: 0.00020830
Iteration 6/1000 | Loss: 0.00026092
Iteration 7/1000 | Loss: 0.00007960
Iteration 8/1000 | Loss: 0.00007039
Iteration 9/1000 | Loss: 0.00006492
Iteration 10/1000 | Loss: 0.00014805
Iteration 11/1000 | Loss: 0.00020073
Iteration 12/1000 | Loss: 0.00061548
Iteration 13/1000 | Loss: 0.00089550
Iteration 14/1000 | Loss: 0.00011151
Iteration 15/1000 | Loss: 0.00008484
Iteration 16/1000 | Loss: 0.00018526
Iteration 17/1000 | Loss: 0.00025146
Iteration 18/1000 | Loss: 0.00012159
Iteration 19/1000 | Loss: 0.00021894
Iteration 20/1000 | Loss: 0.00034618
Iteration 21/1000 | Loss: 0.00053515
Iteration 22/1000 | Loss: 0.00035504
Iteration 23/1000 | Loss: 0.00020119
Iteration 24/1000 | Loss: 0.00024663
Iteration 25/1000 | Loss: 0.00089859
Iteration 26/1000 | Loss: 0.00038860
Iteration 27/1000 | Loss: 0.00009586
Iteration 28/1000 | Loss: 0.00033633
Iteration 29/1000 | Loss: 0.00052137
Iteration 30/1000 | Loss: 0.00026514
Iteration 31/1000 | Loss: 0.00007438
Iteration 32/1000 | Loss: 0.00006989
Iteration 33/1000 | Loss: 0.00008837
Iteration 34/1000 | Loss: 0.00018566
Iteration 35/1000 | Loss: 0.00030568
Iteration 36/1000 | Loss: 0.00007713
Iteration 37/1000 | Loss: 0.00006327
Iteration 38/1000 | Loss: 0.00005999
Iteration 39/1000 | Loss: 0.00005712
Iteration 40/1000 | Loss: 0.00005530
Iteration 41/1000 | Loss: 0.00005330
Iteration 42/1000 | Loss: 0.00005200
Iteration 43/1000 | Loss: 0.00005095
Iteration 44/1000 | Loss: 0.00004999
Iteration 45/1000 | Loss: 0.00004933
Iteration 46/1000 | Loss: 0.00004887
Iteration 47/1000 | Loss: 0.00004827
Iteration 48/1000 | Loss: 0.00004784
Iteration 49/1000 | Loss: 0.00004739
Iteration 50/1000 | Loss: 0.00004696
Iteration 51/1000 | Loss: 0.00004656
Iteration 52/1000 | Loss: 0.00004609
Iteration 53/1000 | Loss: 0.00004551
Iteration 54/1000 | Loss: 0.00004509
Iteration 55/1000 | Loss: 0.00004488
Iteration 56/1000 | Loss: 0.00004475
Iteration 57/1000 | Loss: 0.00004470
Iteration 58/1000 | Loss: 0.00004464
Iteration 59/1000 | Loss: 0.00004456
Iteration 60/1000 | Loss: 0.00004453
Iteration 61/1000 | Loss: 0.00004451
Iteration 62/1000 | Loss: 0.00004450
Iteration 63/1000 | Loss: 0.00004450
Iteration 64/1000 | Loss: 0.00004447
Iteration 65/1000 | Loss: 0.00004447
Iteration 66/1000 | Loss: 0.00004447
Iteration 67/1000 | Loss: 0.00004447
Iteration 68/1000 | Loss: 0.00004446
Iteration 69/1000 | Loss: 0.00004443
Iteration 70/1000 | Loss: 0.00004441
Iteration 71/1000 | Loss: 0.00004441
Iteration 72/1000 | Loss: 0.00004440
Iteration 73/1000 | Loss: 0.00004434
Iteration 74/1000 | Loss: 0.00004434
Iteration 75/1000 | Loss: 0.00004432
Iteration 76/1000 | Loss: 0.00004429
Iteration 77/1000 | Loss: 0.00004428
Iteration 78/1000 | Loss: 0.00004427
Iteration 79/1000 | Loss: 0.00004426
Iteration 80/1000 | Loss: 0.00004420
Iteration 81/1000 | Loss: 0.00004417
Iteration 82/1000 | Loss: 0.00004414
Iteration 83/1000 | Loss: 0.00004414
Iteration 84/1000 | Loss: 0.00004414
Iteration 85/1000 | Loss: 0.00004413
Iteration 86/1000 | Loss: 0.00004408
Iteration 87/1000 | Loss: 0.00004408
Iteration 88/1000 | Loss: 0.00004407
Iteration 89/1000 | Loss: 0.00004407
Iteration 90/1000 | Loss: 0.00004406
Iteration 91/1000 | Loss: 0.00004405
Iteration 92/1000 | Loss: 0.00004405
Iteration 93/1000 | Loss: 0.00004405
Iteration 94/1000 | Loss: 0.00004405
Iteration 95/1000 | Loss: 0.00004405
Iteration 96/1000 | Loss: 0.00004405
Iteration 97/1000 | Loss: 0.00004405
Iteration 98/1000 | Loss: 0.00004405
Iteration 99/1000 | Loss: 0.00004404
Iteration 100/1000 | Loss: 0.00004404
Iteration 101/1000 | Loss: 0.00004404
Iteration 102/1000 | Loss: 0.00004403
Iteration 103/1000 | Loss: 0.00004402
Iteration 104/1000 | Loss: 0.00004402
Iteration 105/1000 | Loss: 0.00004402
Iteration 106/1000 | Loss: 0.00004401
Iteration 107/1000 | Loss: 0.00004401
Iteration 108/1000 | Loss: 0.00004401
Iteration 109/1000 | Loss: 0.00004400
Iteration 110/1000 | Loss: 0.00004400
Iteration 111/1000 | Loss: 0.00004400
Iteration 112/1000 | Loss: 0.00004399
Iteration 113/1000 | Loss: 0.00004399
Iteration 114/1000 | Loss: 0.00004398
Iteration 115/1000 | Loss: 0.00004397
Iteration 116/1000 | Loss: 0.00004397
Iteration 117/1000 | Loss: 0.00004397
Iteration 118/1000 | Loss: 0.00004397
Iteration 119/1000 | Loss: 0.00004397
Iteration 120/1000 | Loss: 0.00004397
Iteration 121/1000 | Loss: 0.00004397
Iteration 122/1000 | Loss: 0.00004396
Iteration 123/1000 | Loss: 0.00004395
Iteration 124/1000 | Loss: 0.00004395
Iteration 125/1000 | Loss: 0.00004395
Iteration 126/1000 | Loss: 0.00004394
Iteration 127/1000 | Loss: 0.00004394
Iteration 128/1000 | Loss: 0.00004393
Iteration 129/1000 | Loss: 0.00004393
Iteration 130/1000 | Loss: 0.00004393
Iteration 131/1000 | Loss: 0.00004392
Iteration 132/1000 | Loss: 0.00004392
Iteration 133/1000 | Loss: 0.00004392
Iteration 134/1000 | Loss: 0.00004392
Iteration 135/1000 | Loss: 0.00004391
Iteration 136/1000 | Loss: 0.00004391
Iteration 137/1000 | Loss: 0.00004391
Iteration 138/1000 | Loss: 0.00004391
Iteration 139/1000 | Loss: 0.00004390
Iteration 140/1000 | Loss: 0.00004390
Iteration 141/1000 | Loss: 0.00004389
Iteration 142/1000 | Loss: 0.00004389
Iteration 143/1000 | Loss: 0.00004389
Iteration 144/1000 | Loss: 0.00004389
Iteration 145/1000 | Loss: 0.00004388
Iteration 146/1000 | Loss: 0.00004388
Iteration 147/1000 | Loss: 0.00004387
Iteration 148/1000 | Loss: 0.00004387
Iteration 149/1000 | Loss: 0.00004386
Iteration 150/1000 | Loss: 0.00004386
Iteration 151/1000 | Loss: 0.00004386
Iteration 152/1000 | Loss: 0.00004385
Iteration 153/1000 | Loss: 0.00004385
Iteration 154/1000 | Loss: 0.00004385
Iteration 155/1000 | Loss: 0.00004385
Iteration 156/1000 | Loss: 0.00004385
Iteration 157/1000 | Loss: 0.00004384
Iteration 158/1000 | Loss: 0.00004384
Iteration 159/1000 | Loss: 0.00004384
Iteration 160/1000 | Loss: 0.00004383
Iteration 161/1000 | Loss: 0.00004383
Iteration 162/1000 | Loss: 0.00004383
Iteration 163/1000 | Loss: 0.00004383
Iteration 164/1000 | Loss: 0.00004383
Iteration 165/1000 | Loss: 0.00004383
Iteration 166/1000 | Loss: 0.00004382
Iteration 167/1000 | Loss: 0.00004382
Iteration 168/1000 | Loss: 0.00004382
Iteration 169/1000 | Loss: 0.00004382
Iteration 170/1000 | Loss: 0.00004382
Iteration 171/1000 | Loss: 0.00004382
Iteration 172/1000 | Loss: 0.00004381
Iteration 173/1000 | Loss: 0.00004381
Iteration 174/1000 | Loss: 0.00004381
Iteration 175/1000 | Loss: 0.00004381
Iteration 176/1000 | Loss: 0.00004380
Iteration 177/1000 | Loss: 0.00004380
Iteration 178/1000 | Loss: 0.00004380
Iteration 179/1000 | Loss: 0.00004380
Iteration 180/1000 | Loss: 0.00004380
Iteration 181/1000 | Loss: 0.00004379
Iteration 182/1000 | Loss: 0.00004379
Iteration 183/1000 | Loss: 0.00004379
Iteration 184/1000 | Loss: 0.00004378
Iteration 185/1000 | Loss: 0.00004378
Iteration 186/1000 | Loss: 0.00004378
Iteration 187/1000 | Loss: 0.00004378
Iteration 188/1000 | Loss: 0.00004377
Iteration 189/1000 | Loss: 0.00004377
Iteration 190/1000 | Loss: 0.00004377
Iteration 191/1000 | Loss: 0.00004377
Iteration 192/1000 | Loss: 0.00004376
Iteration 193/1000 | Loss: 0.00004376
Iteration 194/1000 | Loss: 0.00004376
Iteration 195/1000 | Loss: 0.00004373
Iteration 196/1000 | Loss: 0.00004373
Iteration 197/1000 | Loss: 0.00004373
Iteration 198/1000 | Loss: 0.00004372
Iteration 199/1000 | Loss: 0.00004370
Iteration 200/1000 | Loss: 0.00004370
Iteration 201/1000 | Loss: 0.00004369
Iteration 202/1000 | Loss: 0.00004369
Iteration 203/1000 | Loss: 0.00004368
Iteration 204/1000 | Loss: 0.00004367
Iteration 205/1000 | Loss: 0.00004367
Iteration 206/1000 | Loss: 0.00004366
Iteration 207/1000 | Loss: 0.00004346
Iteration 208/1000 | Loss: 0.00004314
Iteration 209/1000 | Loss: 0.00004284
Iteration 210/1000 | Loss: 0.00004255
Iteration 211/1000 | Loss: 0.00022973
Iteration 212/1000 | Loss: 0.00024760
Iteration 213/1000 | Loss: 0.00005669
Iteration 214/1000 | Loss: 0.00005001
Iteration 215/1000 | Loss: 0.00004661
Iteration 216/1000 | Loss: 0.00004542
Iteration 217/1000 | Loss: 0.00004460
Iteration 218/1000 | Loss: 0.00019213
Iteration 219/1000 | Loss: 0.00029139
Iteration 220/1000 | Loss: 0.00041116
Iteration 221/1000 | Loss: 0.00020543
Iteration 222/1000 | Loss: 0.00024083
Iteration 223/1000 | Loss: 0.00021463
Iteration 224/1000 | Loss: 0.00007239
Iteration 225/1000 | Loss: 0.00007559
Iteration 226/1000 | Loss: 0.00006568
Iteration 227/1000 | Loss: 0.00004870
Iteration 228/1000 | Loss: 0.00032759
Iteration 229/1000 | Loss: 0.00018533
Iteration 230/1000 | Loss: 0.00025961
Iteration 231/1000 | Loss: 0.00004576
Iteration 232/1000 | Loss: 0.00004469
Iteration 233/1000 | Loss: 0.00004398
Iteration 234/1000 | Loss: 0.00004355
Iteration 235/1000 | Loss: 0.00004324
Iteration 236/1000 | Loss: 0.00004290
Iteration 237/1000 | Loss: 0.00018883
Iteration 238/1000 | Loss: 0.00014791
Iteration 239/1000 | Loss: 0.00019176
Iteration 240/1000 | Loss: 0.00015995
Iteration 241/1000 | Loss: 0.00018755
Iteration 242/1000 | Loss: 0.00005827
Iteration 243/1000 | Loss: 0.00005114
Iteration 244/1000 | Loss: 0.00004583
Iteration 245/1000 | Loss: 0.00004413
Iteration 246/1000 | Loss: 0.00004350
Iteration 247/1000 | Loss: 0.00004332
Iteration 248/1000 | Loss: 0.00024988
Iteration 249/1000 | Loss: 0.00014907
Iteration 250/1000 | Loss: 0.00004663
Iteration 251/1000 | Loss: 0.00025449
Iteration 252/1000 | Loss: 0.00015195
Iteration 253/1000 | Loss: 0.00006178
Iteration 254/1000 | Loss: 0.00005462
Iteration 255/1000 | Loss: 0.00004482
Iteration 256/1000 | Loss: 0.00004357
Iteration 257/1000 | Loss: 0.00004320
Iteration 258/1000 | Loss: 0.00026260
Iteration 259/1000 | Loss: 0.00011745
Iteration 260/1000 | Loss: 0.00004572
Iteration 261/1000 | Loss: 0.00027099
Iteration 262/1000 | Loss: 0.00011183
Iteration 263/1000 | Loss: 0.00068178
Iteration 264/1000 | Loss: 0.00033295
Iteration 265/1000 | Loss: 0.00005097
Iteration 266/1000 | Loss: 0.00004790
Iteration 267/1000 | Loss: 0.00004586
Iteration 268/1000 | Loss: 0.00004458
Iteration 269/1000 | Loss: 0.00004370
Iteration 270/1000 | Loss: 0.00004324
Iteration 271/1000 | Loss: 0.00004283
Iteration 272/1000 | Loss: 0.00004237
Iteration 273/1000 | Loss: 0.00004178
Iteration 274/1000 | Loss: 0.00004151
Iteration 275/1000 | Loss: 0.00004136
Iteration 276/1000 | Loss: 0.00004119
Iteration 277/1000 | Loss: 0.00004114
Iteration 278/1000 | Loss: 0.00004112
Iteration 279/1000 | Loss: 0.00004104
Iteration 280/1000 | Loss: 0.00004104
Iteration 281/1000 | Loss: 0.00004104
Iteration 282/1000 | Loss: 0.00004104
Iteration 283/1000 | Loss: 0.00004104
Iteration 284/1000 | Loss: 0.00004104
Iteration 285/1000 | Loss: 0.00004103
Iteration 286/1000 | Loss: 0.00004103
Iteration 287/1000 | Loss: 0.00004103
Iteration 288/1000 | Loss: 0.00004103
Iteration 289/1000 | Loss: 0.00004103
Iteration 290/1000 | Loss: 0.00004103
Iteration 291/1000 | Loss: 0.00004103
Iteration 292/1000 | Loss: 0.00004102
Iteration 293/1000 | Loss: 0.00004102
Iteration 294/1000 | Loss: 0.00004102
Iteration 295/1000 | Loss: 0.00004101
Iteration 296/1000 | Loss: 0.00004101
Iteration 297/1000 | Loss: 0.00004100
Iteration 298/1000 | Loss: 0.00004098
Iteration 299/1000 | Loss: 0.00004094
Iteration 300/1000 | Loss: 0.00004091
Iteration 301/1000 | Loss: 0.00004091
Iteration 302/1000 | Loss: 0.00004090
Iteration 303/1000 | Loss: 0.00004090
Iteration 304/1000 | Loss: 0.00004089
Iteration 305/1000 | Loss: 0.00004088
Iteration 306/1000 | Loss: 0.00004087
Iteration 307/1000 | Loss: 0.00004087
Iteration 308/1000 | Loss: 0.00004086
Iteration 309/1000 | Loss: 0.00004086
Iteration 310/1000 | Loss: 0.00004086
Iteration 311/1000 | Loss: 0.00004085
Iteration 312/1000 | Loss: 0.00004085
Iteration 313/1000 | Loss: 0.00004084
Iteration 314/1000 | Loss: 0.00004084
Iteration 315/1000 | Loss: 0.00004084
Iteration 316/1000 | Loss: 0.00004083
Iteration 317/1000 | Loss: 0.00004083
Iteration 318/1000 | Loss: 0.00004083
Iteration 319/1000 | Loss: 0.00004083
Iteration 320/1000 | Loss: 0.00004083
Iteration 321/1000 | Loss: 0.00004083
Iteration 322/1000 | Loss: 0.00004082
Iteration 323/1000 | Loss: 0.00004082
Iteration 324/1000 | Loss: 0.00004082
Iteration 325/1000 | Loss: 0.00004082
Iteration 326/1000 | Loss: 0.00004082
Iteration 327/1000 | Loss: 0.00004082
Iteration 328/1000 | Loss: 0.00004082
Iteration 329/1000 | Loss: 0.00004082
Iteration 330/1000 | Loss: 0.00004082
Iteration 331/1000 | Loss: 0.00004082
Iteration 332/1000 | Loss: 0.00004082
Iteration 333/1000 | Loss: 0.00004082
Iteration 334/1000 | Loss: 0.00004082
Iteration 335/1000 | Loss: 0.00004082
Iteration 336/1000 | Loss: 0.00004082
Iteration 337/1000 | Loss: 0.00004082
Iteration 338/1000 | Loss: 0.00004082
Iteration 339/1000 | Loss: 0.00004082
Iteration 340/1000 | Loss: 0.00004082
Iteration 341/1000 | Loss: 0.00004082
Iteration 342/1000 | Loss: 0.00004081
Iteration 343/1000 | Loss: 0.00004081
Iteration 344/1000 | Loss: 0.00004081
Iteration 345/1000 | Loss: 0.00004081
Iteration 346/1000 | Loss: 0.00004081
Iteration 347/1000 | Loss: 0.00004081
Iteration 348/1000 | Loss: 0.00004081
Iteration 349/1000 | Loss: 0.00004081
Iteration 350/1000 | Loss: 0.00004081
Iteration 351/1000 | Loss: 0.00004081
Iteration 352/1000 | Loss: 0.00004081
Iteration 353/1000 | Loss: 0.00004081
Iteration 354/1000 | Loss: 0.00004081
Iteration 355/1000 | Loss: 0.00004081
Iteration 356/1000 | Loss: 0.00004081
Iteration 357/1000 | Loss: 0.00004081
Iteration 358/1000 | Loss: 0.00004081
Iteration 359/1000 | Loss: 0.00004081
Iteration 360/1000 | Loss: 0.00004081
Iteration 361/1000 | Loss: 0.00004081
Iteration 362/1000 | Loss: 0.00004081
Iteration 363/1000 | Loss: 0.00004081
Iteration 364/1000 | Loss: 0.00004081
Iteration 365/1000 | Loss: 0.00004081
Iteration 366/1000 | Loss: 0.00004081
Iteration 367/1000 | Loss: 0.00004081
Iteration 368/1000 | Loss: 0.00004081
Iteration 369/1000 | Loss: 0.00004081
Iteration 370/1000 | Loss: 0.00004081
Iteration 371/1000 | Loss: 0.00004081
Iteration 372/1000 | Loss: 0.00004081
Iteration 373/1000 | Loss: 0.00004081
Iteration 374/1000 | Loss: 0.00004081
Iteration 375/1000 | Loss: 0.00004081
Iteration 376/1000 | Loss: 0.00004081
Iteration 377/1000 | Loss: 0.00004081
Iteration 378/1000 | Loss: 0.00004081
Iteration 379/1000 | Loss: 0.00004081
Iteration 380/1000 | Loss: 0.00004081
Iteration 381/1000 | Loss: 0.00004081
Iteration 382/1000 | Loss: 0.00004081
Iteration 383/1000 | Loss: 0.00004081
Iteration 384/1000 | Loss: 0.00004081
Iteration 385/1000 | Loss: 0.00004081
Iteration 386/1000 | Loss: 0.00004081
Iteration 387/1000 | Loss: 0.00004081
Iteration 388/1000 | Loss: 0.00004081
Iteration 389/1000 | Loss: 0.00004081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 389. Stopping optimization.
Last 5 losses: [4.081005317857489e-05, 4.081005317857489e-05, 4.081005317857489e-05, 4.081005317857489e-05, 4.081005317857489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.081005317857489e-05

Optimization complete. Final v2v error: 3.9952080249786377 mm

Highest mean error: 9.99899673461914 mm for frame 105

Lowest mean error: 2.901341199874878 mm for frame 148

Saving results

Total time: 246.4444921016693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649040
Iteration 2/25 | Loss: 0.00144185
Iteration 3/25 | Loss: 0.00130908
Iteration 4/25 | Loss: 0.00129534
Iteration 5/25 | Loss: 0.00129389
Iteration 6/25 | Loss: 0.00129389
Iteration 7/25 | Loss: 0.00129389
Iteration 8/25 | Loss: 0.00129389
Iteration 9/25 | Loss: 0.00129389
Iteration 10/25 | Loss: 0.00129389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012938869185745716, 0.0012938869185745716, 0.0012938869185745716, 0.0012938869185745716, 0.0012938869185745716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012938869185745716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28562140
Iteration 2/25 | Loss: 0.00120437
Iteration 3/25 | Loss: 0.00120436
Iteration 4/25 | Loss: 0.00120436
Iteration 5/25 | Loss: 0.00120436
Iteration 6/25 | Loss: 0.00120436
Iteration 7/25 | Loss: 0.00120436
Iteration 8/25 | Loss: 0.00120436
Iteration 9/25 | Loss: 0.00120436
Iteration 10/25 | Loss: 0.00120436
Iteration 11/25 | Loss: 0.00120436
Iteration 12/25 | Loss: 0.00120436
Iteration 13/25 | Loss: 0.00120436
Iteration 14/25 | Loss: 0.00120436
Iteration 15/25 | Loss: 0.00120436
Iteration 16/25 | Loss: 0.00120436
Iteration 17/25 | Loss: 0.00120436
Iteration 18/25 | Loss: 0.00120436
Iteration 19/25 | Loss: 0.00120436
Iteration 20/25 | Loss: 0.00120436
Iteration 21/25 | Loss: 0.00120436
Iteration 22/25 | Loss: 0.00120436
Iteration 23/25 | Loss: 0.00120436
Iteration 24/25 | Loss: 0.00120436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012043624883517623, 0.0012043624883517623, 0.0012043624883517623, 0.0012043624883517623, 0.0012043624883517623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012043624883517623

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120436
Iteration 2/1000 | Loss: 0.00004345
Iteration 3/1000 | Loss: 0.00002890
Iteration 4/1000 | Loss: 0.00002215
Iteration 5/1000 | Loss: 0.00001975
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001785
Iteration 8/1000 | Loss: 0.00001742
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001646
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001614
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001599
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001588
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001584
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001569
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001568
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001565
Iteration 46/1000 | Loss: 0.00001563
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001563
Iteration 52/1000 | Loss: 0.00001563
Iteration 53/1000 | Loss: 0.00001563
Iteration 54/1000 | Loss: 0.00001563
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001561
Iteration 58/1000 | Loss: 0.00001561
Iteration 59/1000 | Loss: 0.00001561
Iteration 60/1000 | Loss: 0.00001560
Iteration 61/1000 | Loss: 0.00001560
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001559
Iteration 64/1000 | Loss: 0.00001559
Iteration 65/1000 | Loss: 0.00001558
Iteration 66/1000 | Loss: 0.00001558
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001555
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001554
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001552
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001551
Iteration 89/1000 | Loss: 0.00001551
Iteration 90/1000 | Loss: 0.00001551
Iteration 91/1000 | Loss: 0.00001550
Iteration 92/1000 | Loss: 0.00001550
Iteration 93/1000 | Loss: 0.00001550
Iteration 94/1000 | Loss: 0.00001550
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001549
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001547
Iteration 105/1000 | Loss: 0.00001547
Iteration 106/1000 | Loss: 0.00001547
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001546
Iteration 113/1000 | Loss: 0.00001545
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001544
Iteration 122/1000 | Loss: 0.00001544
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001542
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001542
Iteration 134/1000 | Loss: 0.00001542
Iteration 135/1000 | Loss: 0.00001542
Iteration 136/1000 | Loss: 0.00001542
Iteration 137/1000 | Loss: 0.00001542
Iteration 138/1000 | Loss: 0.00001541
Iteration 139/1000 | Loss: 0.00001541
Iteration 140/1000 | Loss: 0.00001541
Iteration 141/1000 | Loss: 0.00001541
Iteration 142/1000 | Loss: 0.00001541
Iteration 143/1000 | Loss: 0.00001541
Iteration 144/1000 | Loss: 0.00001541
Iteration 145/1000 | Loss: 0.00001541
Iteration 146/1000 | Loss: 0.00001541
Iteration 147/1000 | Loss: 0.00001541
Iteration 148/1000 | Loss: 0.00001541
Iteration 149/1000 | Loss: 0.00001541
Iteration 150/1000 | Loss: 0.00001541
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001540
Iteration 156/1000 | Loss: 0.00001540
Iteration 157/1000 | Loss: 0.00001540
Iteration 158/1000 | Loss: 0.00001540
Iteration 159/1000 | Loss: 0.00001540
Iteration 160/1000 | Loss: 0.00001540
Iteration 161/1000 | Loss: 0.00001540
Iteration 162/1000 | Loss: 0.00001540
Iteration 163/1000 | Loss: 0.00001540
Iteration 164/1000 | Loss: 0.00001540
Iteration 165/1000 | Loss: 0.00001540
Iteration 166/1000 | Loss: 0.00001540
Iteration 167/1000 | Loss: 0.00001540
Iteration 168/1000 | Loss: 0.00001540
Iteration 169/1000 | Loss: 0.00001540
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.5399937183246948e-05, 1.5399937183246948e-05, 1.5399937183246948e-05, 1.5399937183246948e-05, 1.5399937183246948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5399937183246948e-05

Optimization complete. Final v2v error: 3.303924322128296 mm

Highest mean error: 3.6640987396240234 mm for frame 108

Lowest mean error: 2.8228824138641357 mm for frame 92

Saving results

Total time: 40.88094162940979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00627187
Iteration 2/25 | Loss: 0.00151592
Iteration 3/25 | Loss: 0.00134232
Iteration 4/25 | Loss: 0.00130890
Iteration 5/25 | Loss: 0.00129274
Iteration 6/25 | Loss: 0.00128876
Iteration 7/25 | Loss: 0.00128759
Iteration 8/25 | Loss: 0.00128703
Iteration 9/25 | Loss: 0.00128675
Iteration 10/25 | Loss: 0.00128653
Iteration 11/25 | Loss: 0.00128626
Iteration 12/25 | Loss: 0.00128571
Iteration 13/25 | Loss: 0.00129209
Iteration 14/25 | Loss: 0.00128915
Iteration 15/25 | Loss: 0.00128893
Iteration 16/25 | Loss: 0.00128612
Iteration 17/25 | Loss: 0.00128641
Iteration 18/25 | Loss: 0.00128479
Iteration 19/25 | Loss: 0.00128117
Iteration 20/25 | Loss: 0.00128309
Iteration 21/25 | Loss: 0.00128097
Iteration 22/25 | Loss: 0.00127798
Iteration 23/25 | Loss: 0.00127752
Iteration 24/25 | Loss: 0.00127747
Iteration 25/25 | Loss: 0.00127747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64339066
Iteration 2/25 | Loss: 0.00152558
Iteration 3/25 | Loss: 0.00152558
Iteration 4/25 | Loss: 0.00152558
Iteration 5/25 | Loss: 0.00152558
Iteration 6/25 | Loss: 0.00152558
Iteration 7/25 | Loss: 0.00152558
Iteration 8/25 | Loss: 0.00152558
Iteration 9/25 | Loss: 0.00152558
Iteration 10/25 | Loss: 0.00152558
Iteration 11/25 | Loss: 0.00152558
Iteration 12/25 | Loss: 0.00152558
Iteration 13/25 | Loss: 0.00152558
Iteration 14/25 | Loss: 0.00152558
Iteration 15/25 | Loss: 0.00152558
Iteration 16/25 | Loss: 0.00152558
Iteration 17/25 | Loss: 0.00152558
Iteration 18/25 | Loss: 0.00152558
Iteration 19/25 | Loss: 0.00152558
Iteration 20/25 | Loss: 0.00152558
Iteration 21/25 | Loss: 0.00152558
Iteration 22/25 | Loss: 0.00152558
Iteration 23/25 | Loss: 0.00152558
Iteration 24/25 | Loss: 0.00152558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015255754115059972, 0.0015255754115059972, 0.0015255754115059972, 0.0015255754115059972, 0.0015255754115059972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015255754115059972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152558
Iteration 2/1000 | Loss: 0.00005454
Iteration 3/1000 | Loss: 0.00003732
Iteration 4/1000 | Loss: 0.00003262
Iteration 5/1000 | Loss: 0.00003041
Iteration 6/1000 | Loss: 0.00002901
Iteration 7/1000 | Loss: 0.00002778
Iteration 8/1000 | Loss: 0.00020615
Iteration 9/1000 | Loss: 0.00002670
Iteration 10/1000 | Loss: 0.00002611
Iteration 11/1000 | Loss: 0.00002566
Iteration 12/1000 | Loss: 0.00002535
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002483
Iteration 15/1000 | Loss: 0.00002465
Iteration 16/1000 | Loss: 0.00002446
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002416
Iteration 19/1000 | Loss: 0.00002416
Iteration 20/1000 | Loss: 0.00002406
Iteration 21/1000 | Loss: 0.00002406
Iteration 22/1000 | Loss: 0.00002403
Iteration 23/1000 | Loss: 0.00002403
Iteration 24/1000 | Loss: 0.00002400
Iteration 25/1000 | Loss: 0.00002400
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002394
Iteration 30/1000 | Loss: 0.00002391
Iteration 31/1000 | Loss: 0.00002389
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002388
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00002387
Iteration 37/1000 | Loss: 0.00002387
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002383
Iteration 42/1000 | Loss: 0.00002382
Iteration 43/1000 | Loss: 0.00002379
Iteration 44/1000 | Loss: 0.00002379
Iteration 45/1000 | Loss: 0.00002378
Iteration 46/1000 | Loss: 0.00002378
Iteration 47/1000 | Loss: 0.00002377
Iteration 48/1000 | Loss: 0.00002377
Iteration 49/1000 | Loss: 0.00002377
Iteration 50/1000 | Loss: 0.00002377
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002377
Iteration 53/1000 | Loss: 0.00002376
Iteration 54/1000 | Loss: 0.00002376
Iteration 55/1000 | Loss: 0.00002375
Iteration 56/1000 | Loss: 0.00002375
Iteration 57/1000 | Loss: 0.00002374
Iteration 58/1000 | Loss: 0.00002374
Iteration 59/1000 | Loss: 0.00002374
Iteration 60/1000 | Loss: 0.00002374
Iteration 61/1000 | Loss: 0.00002374
Iteration 62/1000 | Loss: 0.00002373
Iteration 63/1000 | Loss: 0.00002373
Iteration 64/1000 | Loss: 0.00002372
Iteration 65/1000 | Loss: 0.00002372
Iteration 66/1000 | Loss: 0.00002372
Iteration 67/1000 | Loss: 0.00002372
Iteration 68/1000 | Loss: 0.00002371
Iteration 69/1000 | Loss: 0.00002371
Iteration 70/1000 | Loss: 0.00002371
Iteration 71/1000 | Loss: 0.00002370
Iteration 72/1000 | Loss: 0.00002370
Iteration 73/1000 | Loss: 0.00002370
Iteration 74/1000 | Loss: 0.00002370
Iteration 75/1000 | Loss: 0.00002370
Iteration 76/1000 | Loss: 0.00002370
Iteration 77/1000 | Loss: 0.00002370
Iteration 78/1000 | Loss: 0.00002370
Iteration 79/1000 | Loss: 0.00002370
Iteration 80/1000 | Loss: 0.00002370
Iteration 81/1000 | Loss: 0.00002369
Iteration 82/1000 | Loss: 0.00002369
Iteration 83/1000 | Loss: 0.00002368
Iteration 84/1000 | Loss: 0.00002368
Iteration 85/1000 | Loss: 0.00002367
Iteration 86/1000 | Loss: 0.00002367
Iteration 87/1000 | Loss: 0.00002366
Iteration 88/1000 | Loss: 0.00002366
Iteration 89/1000 | Loss: 0.00002366
Iteration 90/1000 | Loss: 0.00002366
Iteration 91/1000 | Loss: 0.00002366
Iteration 92/1000 | Loss: 0.00002366
Iteration 93/1000 | Loss: 0.00002365
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002365
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002365
Iteration 98/1000 | Loss: 0.00002365
Iteration 99/1000 | Loss: 0.00002364
Iteration 100/1000 | Loss: 0.00002364
Iteration 101/1000 | Loss: 0.00002364
Iteration 102/1000 | Loss: 0.00002363
Iteration 103/1000 | Loss: 0.00002363
Iteration 104/1000 | Loss: 0.00002363
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002362
Iteration 109/1000 | Loss: 0.00002361
Iteration 110/1000 | Loss: 0.00002361
Iteration 111/1000 | Loss: 0.00002361
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002360
Iteration 114/1000 | Loss: 0.00002360
Iteration 115/1000 | Loss: 0.00002360
Iteration 116/1000 | Loss: 0.00002360
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002358
Iteration 122/1000 | Loss: 0.00002358
Iteration 123/1000 | Loss: 0.00002358
Iteration 124/1000 | Loss: 0.00002358
Iteration 125/1000 | Loss: 0.00002358
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002358
Iteration 128/1000 | Loss: 0.00002357
Iteration 129/1000 | Loss: 0.00002357
Iteration 130/1000 | Loss: 0.00002357
Iteration 131/1000 | Loss: 0.00002357
Iteration 132/1000 | Loss: 0.00002356
Iteration 133/1000 | Loss: 0.00002356
Iteration 134/1000 | Loss: 0.00002356
Iteration 135/1000 | Loss: 0.00002356
Iteration 136/1000 | Loss: 0.00002355
Iteration 137/1000 | Loss: 0.00002355
Iteration 138/1000 | Loss: 0.00002355
Iteration 139/1000 | Loss: 0.00002355
Iteration 140/1000 | Loss: 0.00002355
Iteration 141/1000 | Loss: 0.00002355
Iteration 142/1000 | Loss: 0.00002354
Iteration 143/1000 | Loss: 0.00002354
Iteration 144/1000 | Loss: 0.00002354
Iteration 145/1000 | Loss: 0.00002354
Iteration 146/1000 | Loss: 0.00002354
Iteration 147/1000 | Loss: 0.00002353
Iteration 148/1000 | Loss: 0.00002353
Iteration 149/1000 | Loss: 0.00002353
Iteration 150/1000 | Loss: 0.00002353
Iteration 151/1000 | Loss: 0.00002353
Iteration 152/1000 | Loss: 0.00002353
Iteration 153/1000 | Loss: 0.00002353
Iteration 154/1000 | Loss: 0.00002353
Iteration 155/1000 | Loss: 0.00002353
Iteration 156/1000 | Loss: 0.00002352
Iteration 157/1000 | Loss: 0.00002352
Iteration 158/1000 | Loss: 0.00002352
Iteration 159/1000 | Loss: 0.00002352
Iteration 160/1000 | Loss: 0.00002352
Iteration 161/1000 | Loss: 0.00002352
Iteration 162/1000 | Loss: 0.00002352
Iteration 163/1000 | Loss: 0.00002351
Iteration 164/1000 | Loss: 0.00002351
Iteration 165/1000 | Loss: 0.00002351
Iteration 166/1000 | Loss: 0.00002351
Iteration 167/1000 | Loss: 0.00002351
Iteration 168/1000 | Loss: 0.00002351
Iteration 169/1000 | Loss: 0.00002351
Iteration 170/1000 | Loss: 0.00002351
Iteration 171/1000 | Loss: 0.00002351
Iteration 172/1000 | Loss: 0.00002351
Iteration 173/1000 | Loss: 0.00002351
Iteration 174/1000 | Loss: 0.00002351
Iteration 175/1000 | Loss: 0.00002351
Iteration 176/1000 | Loss: 0.00002351
Iteration 177/1000 | Loss: 0.00002351
Iteration 178/1000 | Loss: 0.00002351
Iteration 179/1000 | Loss: 0.00002351
Iteration 180/1000 | Loss: 0.00002351
Iteration 181/1000 | Loss: 0.00002351
Iteration 182/1000 | Loss: 0.00002351
Iteration 183/1000 | Loss: 0.00002351
Iteration 184/1000 | Loss: 0.00002351
Iteration 185/1000 | Loss: 0.00002351
Iteration 186/1000 | Loss: 0.00002351
Iteration 187/1000 | Loss: 0.00002351
Iteration 188/1000 | Loss: 0.00002351
Iteration 189/1000 | Loss: 0.00002351
Iteration 190/1000 | Loss: 0.00002351
Iteration 191/1000 | Loss: 0.00002351
Iteration 192/1000 | Loss: 0.00002351
Iteration 193/1000 | Loss: 0.00002351
Iteration 194/1000 | Loss: 0.00002351
Iteration 195/1000 | Loss: 0.00002351
Iteration 196/1000 | Loss: 0.00002351
Iteration 197/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.350749673496466e-05, 2.350749673496466e-05, 2.350749673496466e-05, 2.350749673496466e-05, 2.350749673496466e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.350749673496466e-05

Optimization complete. Final v2v error: 4.035513401031494 mm

Highest mean error: 4.983139514923096 mm for frame 59

Lowest mean error: 3.345677614212036 mm for frame 78

Saving results

Total time: 79.67756533622742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440671
Iteration 2/25 | Loss: 0.00130979
Iteration 3/25 | Loss: 0.00124223
Iteration 4/25 | Loss: 0.00122651
Iteration 5/25 | Loss: 0.00122105
Iteration 6/25 | Loss: 0.00122032
Iteration 7/25 | Loss: 0.00122032
Iteration 8/25 | Loss: 0.00122032
Iteration 9/25 | Loss: 0.00122032
Iteration 10/25 | Loss: 0.00122032
Iteration 11/25 | Loss: 0.00122032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012203248916193843, 0.0012203248916193843, 0.0012203248916193843, 0.0012203248916193843, 0.0012203248916193843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012203248916193843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50866079
Iteration 2/25 | Loss: 0.00100579
Iteration 3/25 | Loss: 0.00100579
Iteration 4/25 | Loss: 0.00100579
Iteration 5/25 | Loss: 0.00100578
Iteration 6/25 | Loss: 0.00100578
Iteration 7/25 | Loss: 0.00100578
Iteration 8/25 | Loss: 0.00100578
Iteration 9/25 | Loss: 0.00100578
Iteration 10/25 | Loss: 0.00100578
Iteration 11/25 | Loss: 0.00100578
Iteration 12/25 | Loss: 0.00100578
Iteration 13/25 | Loss: 0.00100578
Iteration 14/25 | Loss: 0.00100578
Iteration 15/25 | Loss: 0.00100578
Iteration 16/25 | Loss: 0.00100578
Iteration 17/25 | Loss: 0.00100578
Iteration 18/25 | Loss: 0.00100578
Iteration 19/25 | Loss: 0.00100578
Iteration 20/25 | Loss: 0.00100578
Iteration 21/25 | Loss: 0.00100578
Iteration 22/25 | Loss: 0.00100578
Iteration 23/25 | Loss: 0.00100578
Iteration 24/25 | Loss: 0.00100578
Iteration 25/25 | Loss: 0.00100578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100578
Iteration 2/1000 | Loss: 0.00002747
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001735
Iteration 5/1000 | Loss: 0.00001656
Iteration 6/1000 | Loss: 0.00001586
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001489
Iteration 10/1000 | Loss: 0.00001458
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001430
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001396
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001388
Iteration 20/1000 | Loss: 0.00001388
Iteration 21/1000 | Loss: 0.00001388
Iteration 22/1000 | Loss: 0.00001388
Iteration 23/1000 | Loss: 0.00001387
Iteration 24/1000 | Loss: 0.00001387
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001379
Iteration 31/1000 | Loss: 0.00001377
Iteration 32/1000 | Loss: 0.00001376
Iteration 33/1000 | Loss: 0.00001376
Iteration 34/1000 | Loss: 0.00001376
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001374
Iteration 38/1000 | Loss: 0.00001374
Iteration 39/1000 | Loss: 0.00001373
Iteration 40/1000 | Loss: 0.00001372
Iteration 41/1000 | Loss: 0.00001372
Iteration 42/1000 | Loss: 0.00001371
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001368
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001356
Iteration 56/1000 | Loss: 0.00001356
Iteration 57/1000 | Loss: 0.00001356
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001352
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001346
Iteration 90/1000 | Loss: 0.00001346
Iteration 91/1000 | Loss: 0.00001346
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001345
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001343
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001343
Iteration 105/1000 | Loss: 0.00001343
Iteration 106/1000 | Loss: 0.00001343
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001342
Iteration 109/1000 | Loss: 0.00001342
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001341
Iteration 112/1000 | Loss: 0.00001341
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001340
Iteration 115/1000 | Loss: 0.00001340
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001340
Iteration 119/1000 | Loss: 0.00001340
Iteration 120/1000 | Loss: 0.00001340
Iteration 121/1000 | Loss: 0.00001340
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001338
Iteration 135/1000 | Loss: 0.00001338
Iteration 136/1000 | Loss: 0.00001338
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001338
Iteration 141/1000 | Loss: 0.00001338
Iteration 142/1000 | Loss: 0.00001338
Iteration 143/1000 | Loss: 0.00001338
Iteration 144/1000 | Loss: 0.00001338
Iteration 145/1000 | Loss: 0.00001338
Iteration 146/1000 | Loss: 0.00001338
Iteration 147/1000 | Loss: 0.00001338
Iteration 148/1000 | Loss: 0.00001338
Iteration 149/1000 | Loss: 0.00001337
Iteration 150/1000 | Loss: 0.00001337
Iteration 151/1000 | Loss: 0.00001337
Iteration 152/1000 | Loss: 0.00001337
Iteration 153/1000 | Loss: 0.00001337
Iteration 154/1000 | Loss: 0.00001337
Iteration 155/1000 | Loss: 0.00001337
Iteration 156/1000 | Loss: 0.00001337
Iteration 157/1000 | Loss: 0.00001337
Iteration 158/1000 | Loss: 0.00001337
Iteration 159/1000 | Loss: 0.00001337
Iteration 160/1000 | Loss: 0.00001337
Iteration 161/1000 | Loss: 0.00001336
Iteration 162/1000 | Loss: 0.00001336
Iteration 163/1000 | Loss: 0.00001336
Iteration 164/1000 | Loss: 0.00001336
Iteration 165/1000 | Loss: 0.00001336
Iteration 166/1000 | Loss: 0.00001336
Iteration 167/1000 | Loss: 0.00001335
Iteration 168/1000 | Loss: 0.00001335
Iteration 169/1000 | Loss: 0.00001335
Iteration 170/1000 | Loss: 0.00001335
Iteration 171/1000 | Loss: 0.00001335
Iteration 172/1000 | Loss: 0.00001335
Iteration 173/1000 | Loss: 0.00001335
Iteration 174/1000 | Loss: 0.00001335
Iteration 175/1000 | Loss: 0.00001335
Iteration 176/1000 | Loss: 0.00001335
Iteration 177/1000 | Loss: 0.00001335
Iteration 178/1000 | Loss: 0.00001335
Iteration 179/1000 | Loss: 0.00001335
Iteration 180/1000 | Loss: 0.00001335
Iteration 181/1000 | Loss: 0.00001335
Iteration 182/1000 | Loss: 0.00001335
Iteration 183/1000 | Loss: 0.00001335
Iteration 184/1000 | Loss: 0.00001335
Iteration 185/1000 | Loss: 0.00001335
Iteration 186/1000 | Loss: 0.00001335
Iteration 187/1000 | Loss: 0.00001334
Iteration 188/1000 | Loss: 0.00001334
Iteration 189/1000 | Loss: 0.00001334
Iteration 190/1000 | Loss: 0.00001334
Iteration 191/1000 | Loss: 0.00001334
Iteration 192/1000 | Loss: 0.00001334
Iteration 193/1000 | Loss: 0.00001334
Iteration 194/1000 | Loss: 0.00001334
Iteration 195/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3344801118364558e-05, 1.3344801118364558e-05, 1.3344801118364558e-05, 1.3344801118364558e-05, 1.3344801118364558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3344801118364558e-05

Optimization complete. Final v2v error: 3.125385284423828 mm

Highest mean error: 3.5136325359344482 mm for frame 113

Lowest mean error: 3.0260860919952393 mm for frame 34

Saving results

Total time: 40.941325664520264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426157
Iteration 2/25 | Loss: 0.00138545
Iteration 3/25 | Loss: 0.00125210
Iteration 4/25 | Loss: 0.00124257
Iteration 5/25 | Loss: 0.00124117
Iteration 6/25 | Loss: 0.00124099
Iteration 7/25 | Loss: 0.00124099
Iteration 8/25 | Loss: 0.00124099
Iteration 9/25 | Loss: 0.00124099
Iteration 10/25 | Loss: 0.00124099
Iteration 11/25 | Loss: 0.00124099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012409926857799292, 0.0012409926857799292, 0.0012409926857799292, 0.0012409926857799292, 0.0012409926857799292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012409926857799292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15526533
Iteration 2/25 | Loss: 0.00084475
Iteration 3/25 | Loss: 0.00084473
Iteration 4/25 | Loss: 0.00084473
Iteration 5/25 | Loss: 0.00084473
Iteration 6/25 | Loss: 0.00084473
Iteration 7/25 | Loss: 0.00084473
Iteration 8/25 | Loss: 0.00084473
Iteration 9/25 | Loss: 0.00084473
Iteration 10/25 | Loss: 0.00084473
Iteration 11/25 | Loss: 0.00084473
Iteration 12/25 | Loss: 0.00084473
Iteration 13/25 | Loss: 0.00084473
Iteration 14/25 | Loss: 0.00084473
Iteration 15/25 | Loss: 0.00084473
Iteration 16/25 | Loss: 0.00084473
Iteration 17/25 | Loss: 0.00084473
Iteration 18/25 | Loss: 0.00084473
Iteration 19/25 | Loss: 0.00084473
Iteration 20/25 | Loss: 0.00084473
Iteration 21/25 | Loss: 0.00084473
Iteration 22/25 | Loss: 0.00084473
Iteration 23/25 | Loss: 0.00084473
Iteration 24/25 | Loss: 0.00084473
Iteration 25/25 | Loss: 0.00084473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084473
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00002037
Iteration 4/1000 | Loss: 0.00001893
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001587
Iteration 9/1000 | Loss: 0.00001554
Iteration 10/1000 | Loss: 0.00001527
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001488
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001472
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001465
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001464
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001457
Iteration 32/1000 | Loss: 0.00001455
Iteration 33/1000 | Loss: 0.00001452
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001447
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001443
Iteration 44/1000 | Loss: 0.00001442
Iteration 45/1000 | Loss: 0.00001442
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001442
Iteration 48/1000 | Loss: 0.00001441
Iteration 49/1000 | Loss: 0.00001441
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001440
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001440
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001438
Iteration 57/1000 | Loss: 0.00001438
Iteration 58/1000 | Loss: 0.00001438
Iteration 59/1000 | Loss: 0.00001437
Iteration 60/1000 | Loss: 0.00001437
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001436
Iteration 63/1000 | Loss: 0.00001436
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001435
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001434
Iteration 71/1000 | Loss: 0.00001434
Iteration 72/1000 | Loss: 0.00001434
Iteration 73/1000 | Loss: 0.00001434
Iteration 74/1000 | Loss: 0.00001434
Iteration 75/1000 | Loss: 0.00001433
Iteration 76/1000 | Loss: 0.00001433
Iteration 77/1000 | Loss: 0.00001433
Iteration 78/1000 | Loss: 0.00001433
Iteration 79/1000 | Loss: 0.00001433
Iteration 80/1000 | Loss: 0.00001433
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001433
Iteration 83/1000 | Loss: 0.00001433
Iteration 84/1000 | Loss: 0.00001433
Iteration 85/1000 | Loss: 0.00001432
Iteration 86/1000 | Loss: 0.00001432
Iteration 87/1000 | Loss: 0.00001432
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001432
Iteration 91/1000 | Loss: 0.00001432
Iteration 92/1000 | Loss: 0.00001432
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001430
Iteration 97/1000 | Loss: 0.00001430
Iteration 98/1000 | Loss: 0.00001430
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00001430
Iteration 102/1000 | Loss: 0.00001430
Iteration 103/1000 | Loss: 0.00001430
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001429
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4279867173172534e-05, 1.4279867173172534e-05, 1.4279867173172534e-05, 1.4279867173172534e-05, 1.4279867173172534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4279867173172534e-05

Optimization complete. Final v2v error: 3.2131378650665283 mm

Highest mean error: 3.7291955947875977 mm for frame 72

Lowest mean error: 2.9147398471832275 mm for frame 123

Saving results

Total time: 35.695805072784424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865509
Iteration 2/25 | Loss: 0.00255473
Iteration 3/25 | Loss: 0.00179511
Iteration 4/25 | Loss: 0.00158740
Iteration 5/25 | Loss: 0.00166004
Iteration 6/25 | Loss: 0.00156114
Iteration 7/25 | Loss: 0.00144327
Iteration 8/25 | Loss: 0.00140321
Iteration 9/25 | Loss: 0.00140955
Iteration 10/25 | Loss: 0.00140773
Iteration 11/25 | Loss: 0.00141176
Iteration 12/25 | Loss: 0.00138221
Iteration 13/25 | Loss: 0.00137446
Iteration 14/25 | Loss: 0.00137373
Iteration 15/25 | Loss: 0.00135643
Iteration 16/25 | Loss: 0.00134486
Iteration 17/25 | Loss: 0.00135440
Iteration 18/25 | Loss: 0.00135249
Iteration 19/25 | Loss: 0.00134115
Iteration 20/25 | Loss: 0.00133590
Iteration 21/25 | Loss: 0.00133957
Iteration 22/25 | Loss: 0.00133717
Iteration 23/25 | Loss: 0.00134042
Iteration 24/25 | Loss: 0.00133571
Iteration 25/25 | Loss: 0.00132712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45525479
Iteration 2/25 | Loss: 0.00106413
Iteration 3/25 | Loss: 0.00100027
Iteration 4/25 | Loss: 0.00100027
Iteration 5/25 | Loss: 0.00100027
Iteration 6/25 | Loss: 0.00100027
Iteration 7/25 | Loss: 0.00100027
Iteration 8/25 | Loss: 0.00100027
Iteration 9/25 | Loss: 0.00100027
Iteration 10/25 | Loss: 0.00100027
Iteration 11/25 | Loss: 0.00100027
Iteration 12/25 | Loss: 0.00100027
Iteration 13/25 | Loss: 0.00100027
Iteration 14/25 | Loss: 0.00100027
Iteration 15/25 | Loss: 0.00100027
Iteration 16/25 | Loss: 0.00100027
Iteration 17/25 | Loss: 0.00100027
Iteration 18/25 | Loss: 0.00100027
Iteration 19/25 | Loss: 0.00100027
Iteration 20/25 | Loss: 0.00100027
Iteration 21/25 | Loss: 0.00100027
Iteration 22/25 | Loss: 0.00100027
Iteration 23/25 | Loss: 0.00100027
Iteration 24/25 | Loss: 0.00100027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010002697817981243, 0.0010002697817981243, 0.0010002697817981243, 0.0010002697817981243, 0.0010002697817981243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010002697817981243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100027
Iteration 2/1000 | Loss: 0.00006954
Iteration 3/1000 | Loss: 0.00010567
Iteration 4/1000 | Loss: 0.00018297
Iteration 5/1000 | Loss: 0.00004275
Iteration 6/1000 | Loss: 0.00003918
Iteration 7/1000 | Loss: 0.00003616
Iteration 8/1000 | Loss: 0.00019734
Iteration 9/1000 | Loss: 0.00004183
Iteration 10/1000 | Loss: 0.00003694
Iteration 11/1000 | Loss: 0.00017883
Iteration 12/1000 | Loss: 0.00003913
Iteration 13/1000 | Loss: 0.00003378
Iteration 14/1000 | Loss: 0.00007844
Iteration 15/1000 | Loss: 0.00003273
Iteration 16/1000 | Loss: 0.00003199
Iteration 17/1000 | Loss: 0.00003158
Iteration 18/1000 | Loss: 0.00008926
Iteration 19/1000 | Loss: 0.00003110
Iteration 20/1000 | Loss: 0.00006367
Iteration 21/1000 | Loss: 0.00003080
Iteration 22/1000 | Loss: 0.00003066
Iteration 23/1000 | Loss: 0.00003058
Iteration 24/1000 | Loss: 0.00003043
Iteration 25/1000 | Loss: 0.00003038
Iteration 26/1000 | Loss: 0.00007704
Iteration 27/1000 | Loss: 0.00003018
Iteration 28/1000 | Loss: 0.00003015
Iteration 29/1000 | Loss: 0.00003015
Iteration 30/1000 | Loss: 0.00003015
Iteration 31/1000 | Loss: 0.00003014
Iteration 32/1000 | Loss: 0.00003014
Iteration 33/1000 | Loss: 0.00003014
Iteration 34/1000 | Loss: 0.00003012
Iteration 35/1000 | Loss: 0.00003010
Iteration 36/1000 | Loss: 0.00003009
Iteration 37/1000 | Loss: 0.00003009
Iteration 38/1000 | Loss: 0.00003008
Iteration 39/1000 | Loss: 0.00003008
Iteration 40/1000 | Loss: 0.00003008
Iteration 41/1000 | Loss: 0.00003008
Iteration 42/1000 | Loss: 0.00003008
Iteration 43/1000 | Loss: 0.00003007
Iteration 44/1000 | Loss: 0.00003007
Iteration 45/1000 | Loss: 0.00003007
Iteration 46/1000 | Loss: 0.00010539
Iteration 47/1000 | Loss: 0.00003054
Iteration 48/1000 | Loss: 0.00002958
Iteration 49/1000 | Loss: 0.00008473
Iteration 50/1000 | Loss: 0.00003835
Iteration 51/1000 | Loss: 0.00003314
Iteration 52/1000 | Loss: 0.00003642
Iteration 53/1000 | Loss: 0.00004221
Iteration 54/1000 | Loss: 0.00002847
Iteration 55/1000 | Loss: 0.00002838
Iteration 56/1000 | Loss: 0.00002834
Iteration 57/1000 | Loss: 0.00002833
Iteration 58/1000 | Loss: 0.00002832
Iteration 59/1000 | Loss: 0.00002832
Iteration 60/1000 | Loss: 0.00002822
Iteration 61/1000 | Loss: 0.00002822
Iteration 62/1000 | Loss: 0.00002821
Iteration 63/1000 | Loss: 0.00002820
Iteration 64/1000 | Loss: 0.00002820
Iteration 65/1000 | Loss: 0.00002817
Iteration 66/1000 | Loss: 0.00002812
Iteration 67/1000 | Loss: 0.00002810
Iteration 68/1000 | Loss: 0.00002810
Iteration 69/1000 | Loss: 0.00002809
Iteration 70/1000 | Loss: 0.00002809
Iteration 71/1000 | Loss: 0.00002808
Iteration 72/1000 | Loss: 0.00002808
Iteration 73/1000 | Loss: 0.00002808
Iteration 74/1000 | Loss: 0.00002808
Iteration 75/1000 | Loss: 0.00002808
Iteration 76/1000 | Loss: 0.00002807
Iteration 77/1000 | Loss: 0.00002807
Iteration 78/1000 | Loss: 0.00002807
Iteration 79/1000 | Loss: 0.00002807
Iteration 80/1000 | Loss: 0.00002807
Iteration 81/1000 | Loss: 0.00002806
Iteration 82/1000 | Loss: 0.00002806
Iteration 83/1000 | Loss: 0.00002806
Iteration 84/1000 | Loss: 0.00002803
Iteration 85/1000 | Loss: 0.00002800
Iteration 86/1000 | Loss: 0.00002800
Iteration 87/1000 | Loss: 0.00002800
Iteration 88/1000 | Loss: 0.00002800
Iteration 89/1000 | Loss: 0.00002799
Iteration 90/1000 | Loss: 0.00002799
Iteration 91/1000 | Loss: 0.00002798
Iteration 92/1000 | Loss: 0.00002793
Iteration 93/1000 | Loss: 0.00002792
Iteration 94/1000 | Loss: 0.00002791
Iteration 95/1000 | Loss: 0.00002791
Iteration 96/1000 | Loss: 0.00002791
Iteration 97/1000 | Loss: 0.00002791
Iteration 98/1000 | Loss: 0.00002790
Iteration 99/1000 | Loss: 0.00002790
Iteration 100/1000 | Loss: 0.00002790
Iteration 101/1000 | Loss: 0.00002789
Iteration 102/1000 | Loss: 0.00002789
Iteration 103/1000 | Loss: 0.00002789
Iteration 104/1000 | Loss: 0.00002789
Iteration 105/1000 | Loss: 0.00022288
Iteration 106/1000 | Loss: 0.00030028
Iteration 107/1000 | Loss: 0.00003530
Iteration 108/1000 | Loss: 0.00002886
Iteration 109/1000 | Loss: 0.00002806
Iteration 110/1000 | Loss: 0.00005784
Iteration 111/1000 | Loss: 0.00002785
Iteration 112/1000 | Loss: 0.00002756
Iteration 113/1000 | Loss: 0.00002752
Iteration 114/1000 | Loss: 0.00002752
Iteration 115/1000 | Loss: 0.00002751
Iteration 116/1000 | Loss: 0.00002751
Iteration 117/1000 | Loss: 0.00002751
Iteration 118/1000 | Loss: 0.00002751
Iteration 119/1000 | Loss: 0.00002751
Iteration 120/1000 | Loss: 0.00002751
Iteration 121/1000 | Loss: 0.00002751
Iteration 122/1000 | Loss: 0.00002751
Iteration 123/1000 | Loss: 0.00002744
Iteration 124/1000 | Loss: 0.00002744
Iteration 125/1000 | Loss: 0.00002744
Iteration 126/1000 | Loss: 0.00002744
Iteration 127/1000 | Loss: 0.00002744
Iteration 128/1000 | Loss: 0.00002744
Iteration 129/1000 | Loss: 0.00002744
Iteration 130/1000 | Loss: 0.00002744
Iteration 131/1000 | Loss: 0.00002743
Iteration 132/1000 | Loss: 0.00002743
Iteration 133/1000 | Loss: 0.00002743
Iteration 134/1000 | Loss: 0.00002743
Iteration 135/1000 | Loss: 0.00002743
Iteration 136/1000 | Loss: 0.00002743
Iteration 137/1000 | Loss: 0.00002743
Iteration 138/1000 | Loss: 0.00002743
Iteration 139/1000 | Loss: 0.00002743
Iteration 140/1000 | Loss: 0.00002742
Iteration 141/1000 | Loss: 0.00002742
Iteration 142/1000 | Loss: 0.00002742
Iteration 143/1000 | Loss: 0.00002741
Iteration 144/1000 | Loss: 0.00002741
Iteration 145/1000 | Loss: 0.00002741
Iteration 146/1000 | Loss: 0.00002741
Iteration 147/1000 | Loss: 0.00002740
Iteration 148/1000 | Loss: 0.00002740
Iteration 149/1000 | Loss: 0.00002740
Iteration 150/1000 | Loss: 0.00002739
Iteration 151/1000 | Loss: 0.00002739
Iteration 152/1000 | Loss: 0.00002739
Iteration 153/1000 | Loss: 0.00002739
Iteration 154/1000 | Loss: 0.00002738
Iteration 155/1000 | Loss: 0.00002738
Iteration 156/1000 | Loss: 0.00002738
Iteration 157/1000 | Loss: 0.00002738
Iteration 158/1000 | Loss: 0.00002738
Iteration 159/1000 | Loss: 0.00002738
Iteration 160/1000 | Loss: 0.00002738
Iteration 161/1000 | Loss: 0.00002738
Iteration 162/1000 | Loss: 0.00002738
Iteration 163/1000 | Loss: 0.00002738
Iteration 164/1000 | Loss: 0.00002737
Iteration 165/1000 | Loss: 0.00002737
Iteration 166/1000 | Loss: 0.00002737
Iteration 167/1000 | Loss: 0.00002737
Iteration 168/1000 | Loss: 0.00002737
Iteration 169/1000 | Loss: 0.00002736
Iteration 170/1000 | Loss: 0.00002736
Iteration 171/1000 | Loss: 0.00002736
Iteration 172/1000 | Loss: 0.00002736
Iteration 173/1000 | Loss: 0.00002736
Iteration 174/1000 | Loss: 0.00002736
Iteration 175/1000 | Loss: 0.00002735
Iteration 176/1000 | Loss: 0.00002735
Iteration 177/1000 | Loss: 0.00002735
Iteration 178/1000 | Loss: 0.00002735
Iteration 179/1000 | Loss: 0.00002735
Iteration 180/1000 | Loss: 0.00002735
Iteration 181/1000 | Loss: 0.00002734
Iteration 182/1000 | Loss: 0.00002734
Iteration 183/1000 | Loss: 0.00002734
Iteration 184/1000 | Loss: 0.00002734
Iteration 185/1000 | Loss: 0.00002734
Iteration 186/1000 | Loss: 0.00002734
Iteration 187/1000 | Loss: 0.00002734
Iteration 188/1000 | Loss: 0.00002734
Iteration 189/1000 | Loss: 0.00002734
Iteration 190/1000 | Loss: 0.00002734
Iteration 191/1000 | Loss: 0.00002734
Iteration 192/1000 | Loss: 0.00002734
Iteration 193/1000 | Loss: 0.00002733
Iteration 194/1000 | Loss: 0.00002733
Iteration 195/1000 | Loss: 0.00002733
Iteration 196/1000 | Loss: 0.00002733
Iteration 197/1000 | Loss: 0.00002733
Iteration 198/1000 | Loss: 0.00002733
Iteration 199/1000 | Loss: 0.00002733
Iteration 200/1000 | Loss: 0.00002733
Iteration 201/1000 | Loss: 0.00002733
Iteration 202/1000 | Loss: 0.00002733
Iteration 203/1000 | Loss: 0.00002733
Iteration 204/1000 | Loss: 0.00002733
Iteration 205/1000 | Loss: 0.00002733
Iteration 206/1000 | Loss: 0.00002733
Iteration 207/1000 | Loss: 0.00002733
Iteration 208/1000 | Loss: 0.00002733
Iteration 209/1000 | Loss: 0.00002733
Iteration 210/1000 | Loss: 0.00002733
Iteration 211/1000 | Loss: 0.00002733
Iteration 212/1000 | Loss: 0.00002733
Iteration 213/1000 | Loss: 0.00002733
Iteration 214/1000 | Loss: 0.00002733
Iteration 215/1000 | Loss: 0.00002733
Iteration 216/1000 | Loss: 0.00002733
Iteration 217/1000 | Loss: 0.00002733
Iteration 218/1000 | Loss: 0.00002733
Iteration 219/1000 | Loss: 0.00002733
Iteration 220/1000 | Loss: 0.00002733
Iteration 221/1000 | Loss: 0.00002733
Iteration 222/1000 | Loss: 0.00002733
Iteration 223/1000 | Loss: 0.00002733
Iteration 224/1000 | Loss: 0.00002733
Iteration 225/1000 | Loss: 0.00002733
Iteration 226/1000 | Loss: 0.00002733
Iteration 227/1000 | Loss: 0.00002733
Iteration 228/1000 | Loss: 0.00002733
Iteration 229/1000 | Loss: 0.00002733
Iteration 230/1000 | Loss: 0.00002733
Iteration 231/1000 | Loss: 0.00002733
Iteration 232/1000 | Loss: 0.00002733
Iteration 233/1000 | Loss: 0.00002733
Iteration 234/1000 | Loss: 0.00002733
Iteration 235/1000 | Loss: 0.00002733
Iteration 236/1000 | Loss: 0.00002733
Iteration 237/1000 | Loss: 0.00002733
Iteration 238/1000 | Loss: 0.00002733
Iteration 239/1000 | Loss: 0.00002733
Iteration 240/1000 | Loss: 0.00002733
Iteration 241/1000 | Loss: 0.00002733
Iteration 242/1000 | Loss: 0.00002733
Iteration 243/1000 | Loss: 0.00002733
Iteration 244/1000 | Loss: 0.00002733
Iteration 245/1000 | Loss: 0.00002733
Iteration 246/1000 | Loss: 0.00002733
Iteration 247/1000 | Loss: 0.00002733
Iteration 248/1000 | Loss: 0.00002733
Iteration 249/1000 | Loss: 0.00002733
Iteration 250/1000 | Loss: 0.00002733
Iteration 251/1000 | Loss: 0.00002733
Iteration 252/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [2.732613756961655e-05, 2.732613756961655e-05, 2.732613756961655e-05, 2.732613756961655e-05, 2.732613756961655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.732613756961655e-05

Optimization complete. Final v2v error: 4.299305438995361 mm

Highest mean error: 6.668639659881592 mm for frame 34

Lowest mean error: 3.64473557472229 mm for frame 59

Saving results

Total time: 136.8089566230774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401366
Iteration 2/25 | Loss: 0.00133939
Iteration 3/25 | Loss: 0.00125520
Iteration 4/25 | Loss: 0.00124609
Iteration 5/25 | Loss: 0.00124385
Iteration 6/25 | Loss: 0.00124356
Iteration 7/25 | Loss: 0.00124356
Iteration 8/25 | Loss: 0.00124356
Iteration 9/25 | Loss: 0.00124356
Iteration 10/25 | Loss: 0.00124356
Iteration 11/25 | Loss: 0.00124356
Iteration 12/25 | Loss: 0.00124356
Iteration 13/25 | Loss: 0.00124356
Iteration 14/25 | Loss: 0.00124356
Iteration 15/25 | Loss: 0.00124356
Iteration 16/25 | Loss: 0.00124356
Iteration 17/25 | Loss: 0.00124356
Iteration 18/25 | Loss: 0.00124356
Iteration 19/25 | Loss: 0.00124356
Iteration 20/25 | Loss: 0.00124356
Iteration 21/25 | Loss: 0.00124356
Iteration 22/25 | Loss: 0.00124356
Iteration 23/25 | Loss: 0.00124356
Iteration 24/25 | Loss: 0.00124356
Iteration 25/25 | Loss: 0.00124356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35359132
Iteration 2/25 | Loss: 0.00104003
Iteration 3/25 | Loss: 0.00104003
Iteration 4/25 | Loss: 0.00104003
Iteration 5/25 | Loss: 0.00104002
Iteration 6/25 | Loss: 0.00104002
Iteration 7/25 | Loss: 0.00104002
Iteration 8/25 | Loss: 0.00104002
Iteration 9/25 | Loss: 0.00104002
Iteration 10/25 | Loss: 0.00104002
Iteration 11/25 | Loss: 0.00104002
Iteration 12/25 | Loss: 0.00104002
Iteration 13/25 | Loss: 0.00104002
Iteration 14/25 | Loss: 0.00104002
Iteration 15/25 | Loss: 0.00104002
Iteration 16/25 | Loss: 0.00104002
Iteration 17/25 | Loss: 0.00104002
Iteration 18/25 | Loss: 0.00104002
Iteration 19/25 | Loss: 0.00104002
Iteration 20/25 | Loss: 0.00104002
Iteration 21/25 | Loss: 0.00104002
Iteration 22/25 | Loss: 0.00104002
Iteration 23/25 | Loss: 0.00104002
Iteration 24/25 | Loss: 0.00104002
Iteration 25/25 | Loss: 0.00104002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104002
Iteration 2/1000 | Loss: 0.00003505
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00001908
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001644
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001592
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001553
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001533
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001519
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001516
Iteration 21/1000 | Loss: 0.00001516
Iteration 22/1000 | Loss: 0.00001513
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001512
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001508
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001502
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001501
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001500
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001498
Iteration 44/1000 | Loss: 0.00001497
Iteration 45/1000 | Loss: 0.00001497
Iteration 46/1000 | Loss: 0.00001497
Iteration 47/1000 | Loss: 0.00001497
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001496
Iteration 51/1000 | Loss: 0.00001496
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001496
Iteration 55/1000 | Loss: 0.00001496
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001493
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001492
Iteration 62/1000 | Loss: 0.00001492
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001491
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00001490
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001488
Iteration 70/1000 | Loss: 0.00001488
Iteration 71/1000 | Loss: 0.00001487
Iteration 72/1000 | Loss: 0.00001487
Iteration 73/1000 | Loss: 0.00001487
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001486
Iteration 76/1000 | Loss: 0.00001486
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001483
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001473
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001473
Iteration 129/1000 | Loss: 0.00001473
Iteration 130/1000 | Loss: 0.00001473
Iteration 131/1000 | Loss: 0.00001473
Iteration 132/1000 | Loss: 0.00001473
Iteration 133/1000 | Loss: 0.00001473
Iteration 134/1000 | Loss: 0.00001473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4729862414242234e-05, 1.4729862414242234e-05, 1.4729862414242234e-05, 1.4729862414242234e-05, 1.4729862414242234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4729862414242234e-05

Optimization complete. Final v2v error: 3.1916439533233643 mm

Highest mean error: 3.4108996391296387 mm for frame 21

Lowest mean error: 2.8361001014709473 mm for frame 65

Saving results

Total time: 36.42912769317627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539170
Iteration 2/25 | Loss: 0.00149391
Iteration 3/25 | Loss: 0.00132056
Iteration 4/25 | Loss: 0.00131125
Iteration 5/25 | Loss: 0.00130918
Iteration 6/25 | Loss: 0.00130880
Iteration 7/25 | Loss: 0.00130880
Iteration 8/25 | Loss: 0.00130880
Iteration 9/25 | Loss: 0.00130880
Iteration 10/25 | Loss: 0.00130880
Iteration 11/25 | Loss: 0.00130880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001308801001869142, 0.001308801001869142, 0.001308801001869142, 0.001308801001869142, 0.001308801001869142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001308801001869142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76809710
Iteration 2/25 | Loss: 0.00101657
Iteration 3/25 | Loss: 0.00101656
Iteration 4/25 | Loss: 0.00101656
Iteration 5/25 | Loss: 0.00101656
Iteration 6/25 | Loss: 0.00101656
Iteration 7/25 | Loss: 0.00101656
Iteration 8/25 | Loss: 0.00101656
Iteration 9/25 | Loss: 0.00101656
Iteration 10/25 | Loss: 0.00101656
Iteration 11/25 | Loss: 0.00101656
Iteration 12/25 | Loss: 0.00101656
Iteration 13/25 | Loss: 0.00101656
Iteration 14/25 | Loss: 0.00101656
Iteration 15/25 | Loss: 0.00101656
Iteration 16/25 | Loss: 0.00101656
Iteration 17/25 | Loss: 0.00101656
Iteration 18/25 | Loss: 0.00101656
Iteration 19/25 | Loss: 0.00101656
Iteration 20/25 | Loss: 0.00101656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010165551211684942, 0.0010165551211684942, 0.0010165551211684942, 0.0010165551211684942, 0.0010165551211684942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010165551211684942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101656
Iteration 2/1000 | Loss: 0.00003299
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001844
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001765
Iteration 10/1000 | Loss: 0.00001742
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001687
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001663
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001642
Iteration 19/1000 | Loss: 0.00001640
Iteration 20/1000 | Loss: 0.00001623
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001606
Iteration 23/1000 | Loss: 0.00001596
Iteration 24/1000 | Loss: 0.00001595
Iteration 25/1000 | Loss: 0.00001588
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001582
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001574
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001574
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001569
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001558
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001558
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001553
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Iteration 117/1000 | Loss: 0.00001552
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001552
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001551
Iteration 127/1000 | Loss: 0.00001551
Iteration 128/1000 | Loss: 0.00001551
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001551
Iteration 131/1000 | Loss: 0.00001551
Iteration 132/1000 | Loss: 0.00001551
Iteration 133/1000 | Loss: 0.00001551
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001551
Iteration 139/1000 | Loss: 0.00001551
Iteration 140/1000 | Loss: 0.00001550
Iteration 141/1000 | Loss: 0.00001550
Iteration 142/1000 | Loss: 0.00001550
Iteration 143/1000 | Loss: 0.00001550
Iteration 144/1000 | Loss: 0.00001550
Iteration 145/1000 | Loss: 0.00001550
Iteration 146/1000 | Loss: 0.00001550
Iteration 147/1000 | Loss: 0.00001550
Iteration 148/1000 | Loss: 0.00001550
Iteration 149/1000 | Loss: 0.00001550
Iteration 150/1000 | Loss: 0.00001550
Iteration 151/1000 | Loss: 0.00001550
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001549
Iteration 154/1000 | Loss: 0.00001549
Iteration 155/1000 | Loss: 0.00001549
Iteration 156/1000 | Loss: 0.00001549
Iteration 157/1000 | Loss: 0.00001549
Iteration 158/1000 | Loss: 0.00001549
Iteration 159/1000 | Loss: 0.00001549
Iteration 160/1000 | Loss: 0.00001549
Iteration 161/1000 | Loss: 0.00001549
Iteration 162/1000 | Loss: 0.00001549
Iteration 163/1000 | Loss: 0.00001549
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001548
Iteration 168/1000 | Loss: 0.00001548
Iteration 169/1000 | Loss: 0.00001548
Iteration 170/1000 | Loss: 0.00001548
Iteration 171/1000 | Loss: 0.00001548
Iteration 172/1000 | Loss: 0.00001548
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Iteration 178/1000 | Loss: 0.00001548
Iteration 179/1000 | Loss: 0.00001548
Iteration 180/1000 | Loss: 0.00001548
Iteration 181/1000 | Loss: 0.00001548
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001548
Iteration 184/1000 | Loss: 0.00001548
Iteration 185/1000 | Loss: 0.00001548
Iteration 186/1000 | Loss: 0.00001548
Iteration 187/1000 | Loss: 0.00001548
Iteration 188/1000 | Loss: 0.00001547
Iteration 189/1000 | Loss: 0.00001547
Iteration 190/1000 | Loss: 0.00001547
Iteration 191/1000 | Loss: 0.00001547
Iteration 192/1000 | Loss: 0.00001547
Iteration 193/1000 | Loss: 0.00001547
Iteration 194/1000 | Loss: 0.00001547
Iteration 195/1000 | Loss: 0.00001547
Iteration 196/1000 | Loss: 0.00001547
Iteration 197/1000 | Loss: 0.00001547
Iteration 198/1000 | Loss: 0.00001547
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001547
Iteration 201/1000 | Loss: 0.00001547
Iteration 202/1000 | Loss: 0.00001547
Iteration 203/1000 | Loss: 0.00001547
Iteration 204/1000 | Loss: 0.00001547
Iteration 205/1000 | Loss: 0.00001546
Iteration 206/1000 | Loss: 0.00001546
Iteration 207/1000 | Loss: 0.00001546
Iteration 208/1000 | Loss: 0.00001546
Iteration 209/1000 | Loss: 0.00001546
Iteration 210/1000 | Loss: 0.00001546
Iteration 211/1000 | Loss: 0.00001546
Iteration 212/1000 | Loss: 0.00001546
Iteration 213/1000 | Loss: 0.00001546
Iteration 214/1000 | Loss: 0.00001546
Iteration 215/1000 | Loss: 0.00001546
Iteration 216/1000 | Loss: 0.00001546
Iteration 217/1000 | Loss: 0.00001546
Iteration 218/1000 | Loss: 0.00001545
Iteration 219/1000 | Loss: 0.00001545
Iteration 220/1000 | Loss: 0.00001545
Iteration 221/1000 | Loss: 0.00001545
Iteration 222/1000 | Loss: 0.00001545
Iteration 223/1000 | Loss: 0.00001545
Iteration 224/1000 | Loss: 0.00001545
Iteration 225/1000 | Loss: 0.00001545
Iteration 226/1000 | Loss: 0.00001545
Iteration 227/1000 | Loss: 0.00001545
Iteration 228/1000 | Loss: 0.00001545
Iteration 229/1000 | Loss: 0.00001545
Iteration 230/1000 | Loss: 0.00001545
Iteration 231/1000 | Loss: 0.00001545
Iteration 232/1000 | Loss: 0.00001545
Iteration 233/1000 | Loss: 0.00001545
Iteration 234/1000 | Loss: 0.00001545
Iteration 235/1000 | Loss: 0.00001545
Iteration 236/1000 | Loss: 0.00001544
Iteration 237/1000 | Loss: 0.00001544
Iteration 238/1000 | Loss: 0.00001544
Iteration 239/1000 | Loss: 0.00001544
Iteration 240/1000 | Loss: 0.00001544
Iteration 241/1000 | Loss: 0.00001544
Iteration 242/1000 | Loss: 0.00001544
Iteration 243/1000 | Loss: 0.00001544
Iteration 244/1000 | Loss: 0.00001544
Iteration 245/1000 | Loss: 0.00001544
Iteration 246/1000 | Loss: 0.00001544
Iteration 247/1000 | Loss: 0.00001544
Iteration 248/1000 | Loss: 0.00001544
Iteration 249/1000 | Loss: 0.00001544
Iteration 250/1000 | Loss: 0.00001544
Iteration 251/1000 | Loss: 0.00001544
Iteration 252/1000 | Loss: 0.00001544
Iteration 253/1000 | Loss: 0.00001544
Iteration 254/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.5442801668541506e-05, 1.5442801668541506e-05, 1.5442801668541506e-05, 1.5442801668541506e-05, 1.5442801668541506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5442801668541506e-05

Optimization complete. Final v2v error: 3.265596866607666 mm

Highest mean error: 3.591407299041748 mm for frame 15

Lowest mean error: 3.1318187713623047 mm for frame 60

Saving results

Total time: 49.306195974349976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780854
Iteration 2/25 | Loss: 0.00140752
Iteration 3/25 | Loss: 0.00130679
Iteration 4/25 | Loss: 0.00129269
Iteration 5/25 | Loss: 0.00128814
Iteration 6/25 | Loss: 0.00128737
Iteration 7/25 | Loss: 0.00128737
Iteration 8/25 | Loss: 0.00128737
Iteration 9/25 | Loss: 0.00128737
Iteration 10/25 | Loss: 0.00128737
Iteration 11/25 | Loss: 0.00128737
Iteration 12/25 | Loss: 0.00128737
Iteration 13/25 | Loss: 0.00128737
Iteration 14/25 | Loss: 0.00128737
Iteration 15/25 | Loss: 0.00128737
Iteration 16/25 | Loss: 0.00128737
Iteration 17/25 | Loss: 0.00128737
Iteration 18/25 | Loss: 0.00128737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012873733649030328, 0.0012873733649030328, 0.0012873733649030328, 0.0012873733649030328, 0.0012873733649030328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012873733649030328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32927072
Iteration 2/25 | Loss: 0.00130197
Iteration 3/25 | Loss: 0.00130197
Iteration 4/25 | Loss: 0.00130197
Iteration 5/25 | Loss: 0.00130197
Iteration 6/25 | Loss: 0.00130197
Iteration 7/25 | Loss: 0.00130196
Iteration 8/25 | Loss: 0.00130196
Iteration 9/25 | Loss: 0.00130196
Iteration 10/25 | Loss: 0.00130196
Iteration 11/25 | Loss: 0.00130196
Iteration 12/25 | Loss: 0.00130196
Iteration 13/25 | Loss: 0.00130196
Iteration 14/25 | Loss: 0.00130196
Iteration 15/25 | Loss: 0.00130196
Iteration 16/25 | Loss: 0.00130196
Iteration 17/25 | Loss: 0.00130196
Iteration 18/25 | Loss: 0.00130196
Iteration 19/25 | Loss: 0.00130196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013019638136029243, 0.0013019638136029243, 0.0013019638136029243, 0.0013019638136029243, 0.0013019638136029243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013019638136029243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130196
Iteration 2/1000 | Loss: 0.00004855
Iteration 3/1000 | Loss: 0.00003321
Iteration 4/1000 | Loss: 0.00002803
Iteration 5/1000 | Loss: 0.00002630
Iteration 6/1000 | Loss: 0.00002508
Iteration 7/1000 | Loss: 0.00002421
Iteration 8/1000 | Loss: 0.00002352
Iteration 9/1000 | Loss: 0.00002313
Iteration 10/1000 | Loss: 0.00002280
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002245
Iteration 13/1000 | Loss: 0.00002234
Iteration 14/1000 | Loss: 0.00002226
Iteration 15/1000 | Loss: 0.00002219
Iteration 16/1000 | Loss: 0.00002218
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002217
Iteration 19/1000 | Loss: 0.00002217
Iteration 20/1000 | Loss: 0.00002215
Iteration 21/1000 | Loss: 0.00002214
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002211
Iteration 24/1000 | Loss: 0.00002210
Iteration 25/1000 | Loss: 0.00002210
Iteration 26/1000 | Loss: 0.00002209
Iteration 27/1000 | Loss: 0.00002207
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002202
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002195
Iteration 36/1000 | Loss: 0.00002195
Iteration 37/1000 | Loss: 0.00002193
Iteration 38/1000 | Loss: 0.00002192
Iteration 39/1000 | Loss: 0.00002192
Iteration 40/1000 | Loss: 0.00002192
Iteration 41/1000 | Loss: 0.00002191
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002190
Iteration 44/1000 | Loss: 0.00002190
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002189
Iteration 47/1000 | Loss: 0.00002189
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002186
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002184
Iteration 56/1000 | Loss: 0.00002184
Iteration 57/1000 | Loss: 0.00002184
Iteration 58/1000 | Loss: 0.00002183
Iteration 59/1000 | Loss: 0.00002183
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002182
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002181
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002176
Iteration 85/1000 | Loss: 0.00002176
Iteration 86/1000 | Loss: 0.00002176
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002175
Iteration 90/1000 | Loss: 0.00002175
Iteration 91/1000 | Loss: 0.00002175
Iteration 92/1000 | Loss: 0.00002174
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002174
Iteration 95/1000 | Loss: 0.00002174
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002171
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002170
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002170
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002169
Iteration 119/1000 | Loss: 0.00002169
Iteration 120/1000 | Loss: 0.00002169
Iteration 121/1000 | Loss: 0.00002169
Iteration 122/1000 | Loss: 0.00002169
Iteration 123/1000 | Loss: 0.00002169
Iteration 124/1000 | Loss: 0.00002168
Iteration 125/1000 | Loss: 0.00002168
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002168
Iteration 128/1000 | Loss: 0.00002168
Iteration 129/1000 | Loss: 0.00002168
Iteration 130/1000 | Loss: 0.00002168
Iteration 131/1000 | Loss: 0.00002168
Iteration 132/1000 | Loss: 0.00002168
Iteration 133/1000 | Loss: 0.00002168
Iteration 134/1000 | Loss: 0.00002168
Iteration 135/1000 | Loss: 0.00002168
Iteration 136/1000 | Loss: 0.00002168
Iteration 137/1000 | Loss: 0.00002168
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002167
Iteration 142/1000 | Loss: 0.00002167
Iteration 143/1000 | Loss: 0.00002167
Iteration 144/1000 | Loss: 0.00002167
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002166
Iteration 149/1000 | Loss: 0.00002166
Iteration 150/1000 | Loss: 0.00002165
Iteration 151/1000 | Loss: 0.00002165
Iteration 152/1000 | Loss: 0.00002165
Iteration 153/1000 | Loss: 0.00002165
Iteration 154/1000 | Loss: 0.00002165
Iteration 155/1000 | Loss: 0.00002165
Iteration 156/1000 | Loss: 0.00002165
Iteration 157/1000 | Loss: 0.00002165
Iteration 158/1000 | Loss: 0.00002165
Iteration 159/1000 | Loss: 0.00002165
Iteration 160/1000 | Loss: 0.00002165
Iteration 161/1000 | Loss: 0.00002165
Iteration 162/1000 | Loss: 0.00002165
Iteration 163/1000 | Loss: 0.00002165
Iteration 164/1000 | Loss: 0.00002164
Iteration 165/1000 | Loss: 0.00002164
Iteration 166/1000 | Loss: 0.00002164
Iteration 167/1000 | Loss: 0.00002164
Iteration 168/1000 | Loss: 0.00002164
Iteration 169/1000 | Loss: 0.00002164
Iteration 170/1000 | Loss: 0.00002164
Iteration 171/1000 | Loss: 0.00002164
Iteration 172/1000 | Loss: 0.00002164
Iteration 173/1000 | Loss: 0.00002164
Iteration 174/1000 | Loss: 0.00002164
Iteration 175/1000 | Loss: 0.00002164
Iteration 176/1000 | Loss: 0.00002164
Iteration 177/1000 | Loss: 0.00002164
Iteration 178/1000 | Loss: 0.00002163
Iteration 179/1000 | Loss: 0.00002163
Iteration 180/1000 | Loss: 0.00002163
Iteration 181/1000 | Loss: 0.00002163
Iteration 182/1000 | Loss: 0.00002163
Iteration 183/1000 | Loss: 0.00002163
Iteration 184/1000 | Loss: 0.00002163
Iteration 185/1000 | Loss: 0.00002163
Iteration 186/1000 | Loss: 0.00002163
Iteration 187/1000 | Loss: 0.00002163
Iteration 188/1000 | Loss: 0.00002163
Iteration 189/1000 | Loss: 0.00002163
Iteration 190/1000 | Loss: 0.00002162
Iteration 191/1000 | Loss: 0.00002162
Iteration 192/1000 | Loss: 0.00002162
Iteration 193/1000 | Loss: 0.00002162
Iteration 194/1000 | Loss: 0.00002162
Iteration 195/1000 | Loss: 0.00002162
Iteration 196/1000 | Loss: 0.00002162
Iteration 197/1000 | Loss: 0.00002162
Iteration 198/1000 | Loss: 0.00002161
Iteration 199/1000 | Loss: 0.00002161
Iteration 200/1000 | Loss: 0.00002161
Iteration 201/1000 | Loss: 0.00002161
Iteration 202/1000 | Loss: 0.00002161
Iteration 203/1000 | Loss: 0.00002161
Iteration 204/1000 | Loss: 0.00002161
Iteration 205/1000 | Loss: 0.00002161
Iteration 206/1000 | Loss: 0.00002161
Iteration 207/1000 | Loss: 0.00002161
Iteration 208/1000 | Loss: 0.00002161
Iteration 209/1000 | Loss: 0.00002161
Iteration 210/1000 | Loss: 0.00002161
Iteration 211/1000 | Loss: 0.00002161
Iteration 212/1000 | Loss: 0.00002161
Iteration 213/1000 | Loss: 0.00002161
Iteration 214/1000 | Loss: 0.00002161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.1605812435154803e-05, 2.1605812435154803e-05, 2.1605812435154803e-05, 2.1605812435154803e-05, 2.1605812435154803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1605812435154803e-05

Optimization complete. Final v2v error: 3.822202682495117 mm

Highest mean error: 4.744901657104492 mm for frame 96

Lowest mean error: 3.007693290710449 mm for frame 2

Saving results

Total time: 42.18531322479248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459513
Iteration 2/25 | Loss: 0.00136230
Iteration 3/25 | Loss: 0.00129507
Iteration 4/25 | Loss: 0.00128940
Iteration 5/25 | Loss: 0.00128798
Iteration 6/25 | Loss: 0.00128798
Iteration 7/25 | Loss: 0.00128798
Iteration 8/25 | Loss: 0.00128798
Iteration 9/25 | Loss: 0.00128798
Iteration 10/25 | Loss: 0.00128798
Iteration 11/25 | Loss: 0.00128798
Iteration 12/25 | Loss: 0.00128798
Iteration 13/25 | Loss: 0.00128798
Iteration 14/25 | Loss: 0.00128798
Iteration 15/25 | Loss: 0.00128798
Iteration 16/25 | Loss: 0.00128798
Iteration 17/25 | Loss: 0.00128798
Iteration 18/25 | Loss: 0.00128798
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012879791902378201, 0.0012879791902378201, 0.0012879791902378201, 0.0012879791902378201, 0.0012879791902378201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012879791902378201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36618757
Iteration 2/25 | Loss: 0.00129195
Iteration 3/25 | Loss: 0.00129194
Iteration 4/25 | Loss: 0.00129194
Iteration 5/25 | Loss: 0.00129194
Iteration 6/25 | Loss: 0.00129194
Iteration 7/25 | Loss: 0.00129194
Iteration 8/25 | Loss: 0.00129194
Iteration 9/25 | Loss: 0.00129194
Iteration 10/25 | Loss: 0.00129194
Iteration 11/25 | Loss: 0.00129194
Iteration 12/25 | Loss: 0.00129194
Iteration 13/25 | Loss: 0.00129194
Iteration 14/25 | Loss: 0.00129194
Iteration 15/25 | Loss: 0.00129194
Iteration 16/25 | Loss: 0.00129194
Iteration 17/25 | Loss: 0.00129194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012919383589178324, 0.0012919383589178324, 0.0012919383589178324, 0.0012919383589178324, 0.0012919383589178324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012919383589178324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129194
Iteration 2/1000 | Loss: 0.00004103
Iteration 3/1000 | Loss: 0.00002556
Iteration 4/1000 | Loss: 0.00002154
Iteration 5/1000 | Loss: 0.00002024
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001832
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001772
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001727
Iteration 13/1000 | Loss: 0.00001723
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001717
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001708
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001708
Iteration 30/1000 | Loss: 0.00001708
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001702
Iteration 43/1000 | Loss: 0.00001701
Iteration 44/1000 | Loss: 0.00001701
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001700
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001697
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001694
Iteration 55/1000 | Loss: 0.00001693
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001685
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001672
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001671
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001669
Iteration 75/1000 | Loss: 0.00001662
Iteration 76/1000 | Loss: 0.00001661
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001653
Iteration 99/1000 | Loss: 0.00001653
Iteration 100/1000 | Loss: 0.00001653
Iteration 101/1000 | Loss: 0.00001653
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001652
Iteration 105/1000 | Loss: 0.00001652
Iteration 106/1000 | Loss: 0.00001651
Iteration 107/1000 | Loss: 0.00001651
Iteration 108/1000 | Loss: 0.00001651
Iteration 109/1000 | Loss: 0.00001651
Iteration 110/1000 | Loss: 0.00001651
Iteration 111/1000 | Loss: 0.00001651
Iteration 112/1000 | Loss: 0.00001651
Iteration 113/1000 | Loss: 0.00001651
Iteration 114/1000 | Loss: 0.00001651
Iteration 115/1000 | Loss: 0.00001650
Iteration 116/1000 | Loss: 0.00001650
Iteration 117/1000 | Loss: 0.00001650
Iteration 118/1000 | Loss: 0.00001650
Iteration 119/1000 | Loss: 0.00001650
Iteration 120/1000 | Loss: 0.00001650
Iteration 121/1000 | Loss: 0.00001650
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001648
Iteration 129/1000 | Loss: 0.00001648
Iteration 130/1000 | Loss: 0.00001648
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001647
Iteration 134/1000 | Loss: 0.00001647
Iteration 135/1000 | Loss: 0.00001647
Iteration 136/1000 | Loss: 0.00001647
Iteration 137/1000 | Loss: 0.00001646
Iteration 138/1000 | Loss: 0.00001646
Iteration 139/1000 | Loss: 0.00001646
Iteration 140/1000 | Loss: 0.00001646
Iteration 141/1000 | Loss: 0.00001646
Iteration 142/1000 | Loss: 0.00001646
Iteration 143/1000 | Loss: 0.00001645
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001645
Iteration 146/1000 | Loss: 0.00001645
Iteration 147/1000 | Loss: 0.00001644
Iteration 148/1000 | Loss: 0.00001644
Iteration 149/1000 | Loss: 0.00001644
Iteration 150/1000 | Loss: 0.00001644
Iteration 151/1000 | Loss: 0.00001644
Iteration 152/1000 | Loss: 0.00001644
Iteration 153/1000 | Loss: 0.00001643
Iteration 154/1000 | Loss: 0.00001642
Iteration 155/1000 | Loss: 0.00001642
Iteration 156/1000 | Loss: 0.00001642
Iteration 157/1000 | Loss: 0.00001642
Iteration 158/1000 | Loss: 0.00001642
Iteration 159/1000 | Loss: 0.00001641
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001641
Iteration 162/1000 | Loss: 0.00001641
Iteration 163/1000 | Loss: 0.00001640
Iteration 164/1000 | Loss: 0.00001640
Iteration 165/1000 | Loss: 0.00001640
Iteration 166/1000 | Loss: 0.00001640
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001639
Iteration 169/1000 | Loss: 0.00001639
Iteration 170/1000 | Loss: 0.00001639
Iteration 171/1000 | Loss: 0.00001639
Iteration 172/1000 | Loss: 0.00001639
Iteration 173/1000 | Loss: 0.00001639
Iteration 174/1000 | Loss: 0.00001639
Iteration 175/1000 | Loss: 0.00001638
Iteration 176/1000 | Loss: 0.00001638
Iteration 177/1000 | Loss: 0.00001638
Iteration 178/1000 | Loss: 0.00001638
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Iteration 181/1000 | Loss: 0.00001637
Iteration 182/1000 | Loss: 0.00001636
Iteration 183/1000 | Loss: 0.00001636
Iteration 184/1000 | Loss: 0.00001636
Iteration 185/1000 | Loss: 0.00001636
Iteration 186/1000 | Loss: 0.00001636
Iteration 187/1000 | Loss: 0.00001636
Iteration 188/1000 | Loss: 0.00001636
Iteration 189/1000 | Loss: 0.00001636
Iteration 190/1000 | Loss: 0.00001636
Iteration 191/1000 | Loss: 0.00001636
Iteration 192/1000 | Loss: 0.00001636
Iteration 193/1000 | Loss: 0.00001635
Iteration 194/1000 | Loss: 0.00001635
Iteration 195/1000 | Loss: 0.00001635
Iteration 196/1000 | Loss: 0.00001635
Iteration 197/1000 | Loss: 0.00001635
Iteration 198/1000 | Loss: 0.00001635
Iteration 199/1000 | Loss: 0.00001635
Iteration 200/1000 | Loss: 0.00001635
Iteration 201/1000 | Loss: 0.00001635
Iteration 202/1000 | Loss: 0.00001634
Iteration 203/1000 | Loss: 0.00001634
Iteration 204/1000 | Loss: 0.00001634
Iteration 205/1000 | Loss: 0.00001634
Iteration 206/1000 | Loss: 0.00001634
Iteration 207/1000 | Loss: 0.00001634
Iteration 208/1000 | Loss: 0.00001634
Iteration 209/1000 | Loss: 0.00001634
Iteration 210/1000 | Loss: 0.00001634
Iteration 211/1000 | Loss: 0.00001634
Iteration 212/1000 | Loss: 0.00001634
Iteration 213/1000 | Loss: 0.00001634
Iteration 214/1000 | Loss: 0.00001634
Iteration 215/1000 | Loss: 0.00001634
Iteration 216/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.6336769476765767e-05, 1.6336769476765767e-05, 1.6336769476765767e-05, 1.6336769476765767e-05, 1.6336769476765767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6336769476765767e-05

Optimization complete. Final v2v error: 3.38336181640625 mm

Highest mean error: 3.710721015930176 mm for frame 9

Lowest mean error: 2.8301236629486084 mm for frame 147

Saving results

Total time: 42.86737060546875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787786
Iteration 2/25 | Loss: 0.00178300
Iteration 3/25 | Loss: 0.00146397
Iteration 4/25 | Loss: 0.00141981
Iteration 5/25 | Loss: 0.00139573
Iteration 6/25 | Loss: 0.00138696
Iteration 7/25 | Loss: 0.00138664
Iteration 8/25 | Loss: 0.00142198
Iteration 9/25 | Loss: 0.00140083
Iteration 10/25 | Loss: 0.00137479
Iteration 11/25 | Loss: 0.00136535
Iteration 12/25 | Loss: 0.00136508
Iteration 13/25 | Loss: 0.00136366
Iteration 14/25 | Loss: 0.00135969
Iteration 15/25 | Loss: 0.00136131
Iteration 16/25 | Loss: 0.00135948
Iteration 17/25 | Loss: 0.00135945
Iteration 18/25 | Loss: 0.00135944
Iteration 19/25 | Loss: 0.00135944
Iteration 20/25 | Loss: 0.00135944
Iteration 21/25 | Loss: 0.00135944
Iteration 22/25 | Loss: 0.00135944
Iteration 23/25 | Loss: 0.00135944
Iteration 24/25 | Loss: 0.00135944
Iteration 25/25 | Loss: 0.00135944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.44980621
Iteration 2/25 | Loss: 0.00098122
Iteration 3/25 | Loss: 0.00094489
Iteration 4/25 | Loss: 0.00094489
Iteration 5/25 | Loss: 0.00094489
Iteration 6/25 | Loss: 0.00094489
Iteration 7/25 | Loss: 0.00094489
Iteration 8/25 | Loss: 0.00094489
Iteration 9/25 | Loss: 0.00094489
Iteration 10/25 | Loss: 0.00094489
Iteration 11/25 | Loss: 0.00094489
Iteration 12/25 | Loss: 0.00094489
Iteration 13/25 | Loss: 0.00094489
Iteration 14/25 | Loss: 0.00094489
Iteration 15/25 | Loss: 0.00094489
Iteration 16/25 | Loss: 0.00094489
Iteration 17/25 | Loss: 0.00094489
Iteration 18/25 | Loss: 0.00094489
Iteration 19/25 | Loss: 0.00094489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009448899072594941, 0.0009448899072594941, 0.0009448899072594941, 0.0009448899072594941, 0.0009448899072594941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009448899072594941

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094489
Iteration 2/1000 | Loss: 0.00004381
Iteration 3/1000 | Loss: 0.00003123
Iteration 4/1000 | Loss: 0.00002848
Iteration 5/1000 | Loss: 0.00002732
Iteration 6/1000 | Loss: 0.00002665
Iteration 7/1000 | Loss: 0.00002609
Iteration 8/1000 | Loss: 0.00002558
Iteration 9/1000 | Loss: 0.00002520
Iteration 10/1000 | Loss: 0.00002493
Iteration 11/1000 | Loss: 0.00002468
Iteration 12/1000 | Loss: 0.00002451
Iteration 13/1000 | Loss: 0.00002446
Iteration 14/1000 | Loss: 0.00002431
Iteration 15/1000 | Loss: 0.00002424
Iteration 16/1000 | Loss: 0.00002421
Iteration 17/1000 | Loss: 0.00002418
Iteration 18/1000 | Loss: 0.00002417
Iteration 19/1000 | Loss: 0.00002415
Iteration 20/1000 | Loss: 0.00002414
Iteration 21/1000 | Loss: 0.00002414
Iteration 22/1000 | Loss: 0.00002413
Iteration 23/1000 | Loss: 0.00002413
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002412
Iteration 26/1000 | Loss: 0.00002406
Iteration 27/1000 | Loss: 0.00002406
Iteration 28/1000 | Loss: 0.00002406
Iteration 29/1000 | Loss: 0.00002404
Iteration 30/1000 | Loss: 0.00002403
Iteration 31/1000 | Loss: 0.00002403
Iteration 32/1000 | Loss: 0.00002403
Iteration 33/1000 | Loss: 0.00002403
Iteration 34/1000 | Loss: 0.00002402
Iteration 35/1000 | Loss: 0.00002402
Iteration 36/1000 | Loss: 0.00002402
Iteration 37/1000 | Loss: 0.00002401
Iteration 38/1000 | Loss: 0.00002401
Iteration 39/1000 | Loss: 0.00002401
Iteration 40/1000 | Loss: 0.00002400
Iteration 41/1000 | Loss: 0.00002399
Iteration 42/1000 | Loss: 0.00002399
Iteration 43/1000 | Loss: 0.00002398
Iteration 44/1000 | Loss: 0.00002398
Iteration 45/1000 | Loss: 0.00002397
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002396
Iteration 50/1000 | Loss: 0.00002395
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002395
Iteration 56/1000 | Loss: 0.00002394
Iteration 57/1000 | Loss: 0.00002394
Iteration 58/1000 | Loss: 0.00002394
Iteration 59/1000 | Loss: 0.00002393
Iteration 60/1000 | Loss: 0.00002392
Iteration 61/1000 | Loss: 0.00002391
Iteration 62/1000 | Loss: 0.00002391
Iteration 63/1000 | Loss: 0.00002390
Iteration 64/1000 | Loss: 0.00002390
Iteration 65/1000 | Loss: 0.00002390
Iteration 66/1000 | Loss: 0.00002390
Iteration 67/1000 | Loss: 0.00002390
Iteration 68/1000 | Loss: 0.00002390
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002389
Iteration 71/1000 | Loss: 0.00002389
Iteration 72/1000 | Loss: 0.00002389
Iteration 73/1000 | Loss: 0.00002389
Iteration 74/1000 | Loss: 0.00002389
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002388
Iteration 80/1000 | Loss: 0.00002387
Iteration 81/1000 | Loss: 0.00002387
Iteration 82/1000 | Loss: 0.00002387
Iteration 83/1000 | Loss: 0.00002387
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002386
Iteration 90/1000 | Loss: 0.00002386
Iteration 91/1000 | Loss: 0.00002386
Iteration 92/1000 | Loss: 0.00002386
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002385
Iteration 95/1000 | Loss: 0.00002385
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002384
Iteration 104/1000 | Loss: 0.00002384
Iteration 105/1000 | Loss: 0.00002384
Iteration 106/1000 | Loss: 0.00002383
Iteration 107/1000 | Loss: 0.00002383
Iteration 108/1000 | Loss: 0.00002383
Iteration 109/1000 | Loss: 0.00002383
Iteration 110/1000 | Loss: 0.00002383
Iteration 111/1000 | Loss: 0.00002383
Iteration 112/1000 | Loss: 0.00002383
Iteration 113/1000 | Loss: 0.00002383
Iteration 114/1000 | Loss: 0.00002383
Iteration 115/1000 | Loss: 0.00002383
Iteration 116/1000 | Loss: 0.00002383
Iteration 117/1000 | Loss: 0.00002382
Iteration 118/1000 | Loss: 0.00002382
Iteration 119/1000 | Loss: 0.00002382
Iteration 120/1000 | Loss: 0.00002382
Iteration 121/1000 | Loss: 0.00002382
Iteration 122/1000 | Loss: 0.00002382
Iteration 123/1000 | Loss: 0.00002382
Iteration 124/1000 | Loss: 0.00002382
Iteration 125/1000 | Loss: 0.00002382
Iteration 126/1000 | Loss: 0.00002382
Iteration 127/1000 | Loss: 0.00002382
Iteration 128/1000 | Loss: 0.00002382
Iteration 129/1000 | Loss: 0.00002382
Iteration 130/1000 | Loss: 0.00002382
Iteration 131/1000 | Loss: 0.00002382
Iteration 132/1000 | Loss: 0.00002382
Iteration 133/1000 | Loss: 0.00002382
Iteration 134/1000 | Loss: 0.00002382
Iteration 135/1000 | Loss: 0.00002382
Iteration 136/1000 | Loss: 0.00002382
Iteration 137/1000 | Loss: 0.00002382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.3817261535441503e-05, 2.3817261535441503e-05, 2.3817261535441503e-05, 2.3817261535441503e-05, 2.3817261535441503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3817261535441503e-05

Optimization complete. Final v2v error: 3.8723630905151367 mm

Highest mean error: 11.337942123413086 mm for frame 103

Lowest mean error: 3.197888135910034 mm for frame 92

Saving results

Total time: 66.34291076660156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045731
Iteration 2/25 | Loss: 0.01045731
Iteration 3/25 | Loss: 0.00165873
Iteration 4/25 | Loss: 0.00133259
Iteration 5/25 | Loss: 0.00125547
Iteration 6/25 | Loss: 0.00127241
Iteration 7/25 | Loss: 0.00119701
Iteration 8/25 | Loss: 0.00119570
Iteration 9/25 | Loss: 0.00119137
Iteration 10/25 | Loss: 0.00118115
Iteration 11/25 | Loss: 0.00118786
Iteration 12/25 | Loss: 0.00117341
Iteration 13/25 | Loss: 0.00118075
Iteration 14/25 | Loss: 0.00116952
Iteration 15/25 | Loss: 0.00116913
Iteration 16/25 | Loss: 0.00116803
Iteration 17/25 | Loss: 0.00116395
Iteration 18/25 | Loss: 0.00116431
Iteration 19/25 | Loss: 0.00116466
Iteration 20/25 | Loss: 0.00116481
Iteration 21/25 | Loss: 0.00116353
Iteration 22/25 | Loss: 0.00116353
Iteration 23/25 | Loss: 0.00116352
Iteration 24/25 | Loss: 0.00116352
Iteration 25/25 | Loss: 0.00116352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26697969
Iteration 2/25 | Loss: 0.00142637
Iteration 3/25 | Loss: 0.00132553
Iteration 4/25 | Loss: 0.00132553
Iteration 5/25 | Loss: 0.00132553
Iteration 6/25 | Loss: 0.00132553
Iteration 7/25 | Loss: 0.00132553
Iteration 8/25 | Loss: 0.00132553
Iteration 9/25 | Loss: 0.00132553
Iteration 10/25 | Loss: 0.00132553
Iteration 11/25 | Loss: 0.00132553
Iteration 12/25 | Loss: 0.00132553
Iteration 13/25 | Loss: 0.00132553
Iteration 14/25 | Loss: 0.00132553
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00132553034927696, 0.00132553034927696, 0.00132553034927696, 0.00132553034927696, 0.00132553034927696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00132553034927696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132553
Iteration 2/1000 | Loss: 0.00009103
Iteration 3/1000 | Loss: 0.00017207
Iteration 4/1000 | Loss: 0.00001978
Iteration 5/1000 | Loss: 0.00001721
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00001558
Iteration 8/1000 | Loss: 0.00002493
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00005363
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001393
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00002773
Iteration 21/1000 | Loss: 0.00004386
Iteration 22/1000 | Loss: 0.00002592
Iteration 23/1000 | Loss: 0.00002422
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001278
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001273
Iteration 35/1000 | Loss: 0.00001273
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001265
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001250
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001249
Iteration 81/1000 | Loss: 0.00001246
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001245
Iteration 89/1000 | Loss: 0.00001245
Iteration 90/1000 | Loss: 0.00001245
Iteration 91/1000 | Loss: 0.00001245
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001244
Iteration 95/1000 | Loss: 0.00001244
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001243
Iteration 98/1000 | Loss: 0.00001243
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001243
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001242
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001245
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001239
Iteration 112/1000 | Loss: 0.00001239
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001239
Iteration 120/1000 | Loss: 0.00001239
Iteration 121/1000 | Loss: 0.00001239
Iteration 122/1000 | Loss: 0.00001239
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2390361007419415e-05, 1.2390361007419415e-05, 1.2390361007419415e-05, 1.2390361007419415e-05, 1.2390361007419415e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2390361007419415e-05

Optimization complete. Final v2v error: 2.9921414852142334 mm

Highest mean error: 5.302116870880127 mm for frame 148

Lowest mean error: 2.669879198074341 mm for frame 93

Saving results

Total time: 76.15179562568665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030717
Iteration 2/25 | Loss: 0.01030716
Iteration 3/25 | Loss: 0.01030716
Iteration 4/25 | Loss: 0.01030716
Iteration 5/25 | Loss: 0.01030716
Iteration 6/25 | Loss: 0.01030716
Iteration 7/25 | Loss: 0.01030716
Iteration 8/25 | Loss: 0.01030716
Iteration 9/25 | Loss: 0.01030716
Iteration 10/25 | Loss: 0.01030716
Iteration 11/25 | Loss: 0.01030715
Iteration 12/25 | Loss: 0.01030715
Iteration 13/25 | Loss: 0.01030715
Iteration 14/25 | Loss: 0.01030715
Iteration 15/25 | Loss: 0.01030715
Iteration 16/25 | Loss: 0.01030715
Iteration 17/25 | Loss: 0.01030715
Iteration 18/25 | Loss: 0.01030714
Iteration 19/25 | Loss: 0.01030714
Iteration 20/25 | Loss: 0.01030714
Iteration 21/25 | Loss: 0.01030714
Iteration 22/25 | Loss: 0.01030714
Iteration 23/25 | Loss: 0.01030714
Iteration 24/25 | Loss: 0.01030714
Iteration 25/25 | Loss: 0.01030713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29902077
Iteration 2/25 | Loss: 0.18779308
Iteration 3/25 | Loss: 0.17966822
Iteration 4/25 | Loss: 0.17931925
Iteration 5/25 | Loss: 0.17931923
Iteration 6/25 | Loss: 0.17931920
Iteration 7/25 | Loss: 0.17931919
Iteration 8/25 | Loss: 0.17931919
Iteration 9/25 | Loss: 0.17931919
Iteration 10/25 | Loss: 0.17931919
Iteration 11/25 | Loss: 0.17931919
Iteration 12/25 | Loss: 0.17931919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.17931918799877167, 0.17931918799877167, 0.17931918799877167, 0.17931918799877167, 0.17931918799877167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17931918799877167

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17931919
Iteration 2/1000 | Loss: 0.00292275
Iteration 3/1000 | Loss: 0.00090349
Iteration 4/1000 | Loss: 0.00177730
Iteration 5/1000 | Loss: 0.00038536
Iteration 6/1000 | Loss: 0.00020444
Iteration 7/1000 | Loss: 0.00006726
Iteration 8/1000 | Loss: 0.00004096
Iteration 9/1000 | Loss: 0.00005835
Iteration 10/1000 | Loss: 0.00002751
Iteration 11/1000 | Loss: 0.00002515
Iteration 12/1000 | Loss: 0.00012723
Iteration 13/1000 | Loss: 0.00060664
Iteration 14/1000 | Loss: 0.00002557
Iteration 15/1000 | Loss: 0.00027275
Iteration 16/1000 | Loss: 0.00002213
Iteration 17/1000 | Loss: 0.00002112
Iteration 18/1000 | Loss: 0.00011899
Iteration 19/1000 | Loss: 0.00072469
Iteration 20/1000 | Loss: 0.00002040
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00071947
Iteration 23/1000 | Loss: 0.00025267
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001862
Iteration 26/1000 | Loss: 0.00001815
Iteration 27/1000 | Loss: 0.00025182
Iteration 28/1000 | Loss: 0.00170579
Iteration 29/1000 | Loss: 0.00001901
Iteration 30/1000 | Loss: 0.00041727
Iteration 31/1000 | Loss: 0.00010237
Iteration 32/1000 | Loss: 0.00002123
Iteration 33/1000 | Loss: 0.00001895
Iteration 34/1000 | Loss: 0.00019455
Iteration 35/1000 | Loss: 0.00008863
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001664
Iteration 39/1000 | Loss: 0.00011169
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001590
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001569
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001568
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001562
Iteration 53/1000 | Loss: 0.00001562
Iteration 54/1000 | Loss: 0.00001562
Iteration 55/1000 | Loss: 0.00001562
Iteration 56/1000 | Loss: 0.00001562
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001562
Iteration 59/1000 | Loss: 0.00001560
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001550
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00001549
Iteration 76/1000 | Loss: 0.00001549
Iteration 77/1000 | Loss: 0.00001549
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001548
Iteration 80/1000 | Loss: 0.00001548
Iteration 81/1000 | Loss: 0.00001547
Iteration 82/1000 | Loss: 0.00001547
Iteration 83/1000 | Loss: 0.00001547
Iteration 84/1000 | Loss: 0.00001547
Iteration 85/1000 | Loss: 0.00001547
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00001546
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001543
Iteration 113/1000 | Loss: 0.00001543
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001542
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001541
Iteration 118/1000 | Loss: 0.00001541
Iteration 119/1000 | Loss: 0.00001541
Iteration 120/1000 | Loss: 0.00001541
Iteration 121/1000 | Loss: 0.00001541
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001540
Iteration 124/1000 | Loss: 0.00001540
Iteration 125/1000 | Loss: 0.00001540
Iteration 126/1000 | Loss: 0.00001540
Iteration 127/1000 | Loss: 0.00001540
Iteration 128/1000 | Loss: 0.00001540
Iteration 129/1000 | Loss: 0.00001540
Iteration 130/1000 | Loss: 0.00001540
Iteration 131/1000 | Loss: 0.00001540
Iteration 132/1000 | Loss: 0.00001540
Iteration 133/1000 | Loss: 0.00001540
Iteration 134/1000 | Loss: 0.00001540
Iteration 135/1000 | Loss: 0.00001540
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001540
Iteration 140/1000 | Loss: 0.00001540
Iteration 141/1000 | Loss: 0.00001540
Iteration 142/1000 | Loss: 0.00001540
Iteration 143/1000 | Loss: 0.00001540
Iteration 144/1000 | Loss: 0.00001540
Iteration 145/1000 | Loss: 0.00001540
Iteration 146/1000 | Loss: 0.00001540
Iteration 147/1000 | Loss: 0.00001540
Iteration 148/1000 | Loss: 0.00001540
Iteration 149/1000 | Loss: 0.00001540
Iteration 150/1000 | Loss: 0.00001540
Iteration 151/1000 | Loss: 0.00001540
Iteration 152/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.540164157631807e-05, 1.540164157631807e-05, 1.540164157631807e-05, 1.540164157631807e-05, 1.540164157631807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.540164157631807e-05

Optimization complete. Final v2v error: 3.399522304534912 mm

Highest mean error: 3.66267466545105 mm for frame 140

Lowest mean error: 3.250584125518799 mm for frame 211

Saving results

Total time: 83.96416211128235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448875
Iteration 2/25 | Loss: 0.00131227
Iteration 3/25 | Loss: 0.00122216
Iteration 4/25 | Loss: 0.00120463
Iteration 5/25 | Loss: 0.00119897
Iteration 6/25 | Loss: 0.00119734
Iteration 7/25 | Loss: 0.00119700
Iteration 8/25 | Loss: 0.00119700
Iteration 9/25 | Loss: 0.00119700
Iteration 10/25 | Loss: 0.00119700
Iteration 11/25 | Loss: 0.00119700
Iteration 12/25 | Loss: 0.00119700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001196998986415565, 0.001196998986415565, 0.001196998986415565, 0.001196998986415565, 0.001196998986415565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001196998986415565

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22859371
Iteration 2/25 | Loss: 0.00102482
Iteration 3/25 | Loss: 0.00102479
Iteration 4/25 | Loss: 0.00102478
Iteration 5/25 | Loss: 0.00102478
Iteration 6/25 | Loss: 0.00102478
Iteration 7/25 | Loss: 0.00102478
Iteration 8/25 | Loss: 0.00102478
Iteration 9/25 | Loss: 0.00102478
Iteration 10/25 | Loss: 0.00102478
Iteration 11/25 | Loss: 0.00102478
Iteration 12/25 | Loss: 0.00102478
Iteration 13/25 | Loss: 0.00102478
Iteration 14/25 | Loss: 0.00102478
Iteration 15/25 | Loss: 0.00102478
Iteration 16/25 | Loss: 0.00102478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010247827740386128, 0.0010247827740386128, 0.0010247827740386128, 0.0010247827740386128, 0.0010247827740386128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010247827740386128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102478
Iteration 2/1000 | Loss: 0.00004350
Iteration 3/1000 | Loss: 0.00002600
Iteration 4/1000 | Loss: 0.00001942
Iteration 5/1000 | Loss: 0.00001711
Iteration 6/1000 | Loss: 0.00001591
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001471
Iteration 9/1000 | Loss: 0.00001436
Iteration 10/1000 | Loss: 0.00001408
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001379
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001375
Iteration 19/1000 | Loss: 0.00001365
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001332
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001322
Iteration 26/1000 | Loss: 0.00001316
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001303
Iteration 32/1000 | Loss: 0.00001303
Iteration 33/1000 | Loss: 0.00001303
Iteration 34/1000 | Loss: 0.00001303
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001301
Iteration 38/1000 | Loss: 0.00001301
Iteration 39/1000 | Loss: 0.00001301
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001296
Iteration 52/1000 | Loss: 0.00001296
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001296
Iteration 56/1000 | Loss: 0.00001296
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001296
Iteration 59/1000 | Loss: 0.00001295
Iteration 60/1000 | Loss: 0.00001295
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001294
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001293
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001293
Iteration 68/1000 | Loss: 0.00001293
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001293
Iteration 71/1000 | Loss: 0.00001293
Iteration 72/1000 | Loss: 0.00001293
Iteration 73/1000 | Loss: 0.00001292
Iteration 74/1000 | Loss: 0.00001292
Iteration 75/1000 | Loss: 0.00001292
Iteration 76/1000 | Loss: 0.00001292
Iteration 77/1000 | Loss: 0.00001292
Iteration 78/1000 | Loss: 0.00001292
Iteration 79/1000 | Loss: 0.00001292
Iteration 80/1000 | Loss: 0.00001291
Iteration 81/1000 | Loss: 0.00001291
Iteration 82/1000 | Loss: 0.00001290
Iteration 83/1000 | Loss: 0.00001290
Iteration 84/1000 | Loss: 0.00001290
Iteration 85/1000 | Loss: 0.00001290
Iteration 86/1000 | Loss: 0.00001290
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001289
Iteration 89/1000 | Loss: 0.00001289
Iteration 90/1000 | Loss: 0.00001289
Iteration 91/1000 | Loss: 0.00001289
Iteration 92/1000 | Loss: 0.00001289
Iteration 93/1000 | Loss: 0.00001289
Iteration 94/1000 | Loss: 0.00001288
Iteration 95/1000 | Loss: 0.00001288
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001286
Iteration 105/1000 | Loss: 0.00001286
Iteration 106/1000 | Loss: 0.00001286
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001285
Iteration 109/1000 | Loss: 0.00001285
Iteration 110/1000 | Loss: 0.00001285
Iteration 111/1000 | Loss: 0.00001285
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001282
Iteration 123/1000 | Loss: 0.00001282
Iteration 124/1000 | Loss: 0.00001282
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001282
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001281
Iteration 130/1000 | Loss: 0.00001281
Iteration 131/1000 | Loss: 0.00001281
Iteration 132/1000 | Loss: 0.00001281
Iteration 133/1000 | Loss: 0.00001281
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001280
Iteration 136/1000 | Loss: 0.00001280
Iteration 137/1000 | Loss: 0.00001280
Iteration 138/1000 | Loss: 0.00001280
Iteration 139/1000 | Loss: 0.00001279
Iteration 140/1000 | Loss: 0.00001279
Iteration 141/1000 | Loss: 0.00001279
Iteration 142/1000 | Loss: 0.00001279
Iteration 143/1000 | Loss: 0.00001279
Iteration 144/1000 | Loss: 0.00001279
Iteration 145/1000 | Loss: 0.00001279
Iteration 146/1000 | Loss: 0.00001279
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001278
Iteration 156/1000 | Loss: 0.00001278
Iteration 157/1000 | Loss: 0.00001278
Iteration 158/1000 | Loss: 0.00001278
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001277
Iteration 162/1000 | Loss: 0.00001277
Iteration 163/1000 | Loss: 0.00001277
Iteration 164/1000 | Loss: 0.00001277
Iteration 165/1000 | Loss: 0.00001277
Iteration 166/1000 | Loss: 0.00001277
Iteration 167/1000 | Loss: 0.00001277
Iteration 168/1000 | Loss: 0.00001277
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Iteration 175/1000 | Loss: 0.00001276
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001276
Iteration 179/1000 | Loss: 0.00001276
Iteration 180/1000 | Loss: 0.00001276
Iteration 181/1000 | Loss: 0.00001276
Iteration 182/1000 | Loss: 0.00001276
Iteration 183/1000 | Loss: 0.00001276
Iteration 184/1000 | Loss: 0.00001276
Iteration 185/1000 | Loss: 0.00001275
Iteration 186/1000 | Loss: 0.00001275
Iteration 187/1000 | Loss: 0.00001275
Iteration 188/1000 | Loss: 0.00001275
Iteration 189/1000 | Loss: 0.00001275
Iteration 190/1000 | Loss: 0.00001275
Iteration 191/1000 | Loss: 0.00001275
Iteration 192/1000 | Loss: 0.00001275
Iteration 193/1000 | Loss: 0.00001275
Iteration 194/1000 | Loss: 0.00001274
Iteration 195/1000 | Loss: 0.00001274
Iteration 196/1000 | Loss: 0.00001274
Iteration 197/1000 | Loss: 0.00001274
Iteration 198/1000 | Loss: 0.00001274
Iteration 199/1000 | Loss: 0.00001274
Iteration 200/1000 | Loss: 0.00001274
Iteration 201/1000 | Loss: 0.00001274
Iteration 202/1000 | Loss: 0.00001274
Iteration 203/1000 | Loss: 0.00001274
Iteration 204/1000 | Loss: 0.00001274
Iteration 205/1000 | Loss: 0.00001273
Iteration 206/1000 | Loss: 0.00001273
Iteration 207/1000 | Loss: 0.00001273
Iteration 208/1000 | Loss: 0.00001273
Iteration 209/1000 | Loss: 0.00001273
Iteration 210/1000 | Loss: 0.00001273
Iteration 211/1000 | Loss: 0.00001273
Iteration 212/1000 | Loss: 0.00001273
Iteration 213/1000 | Loss: 0.00001273
Iteration 214/1000 | Loss: 0.00001273
Iteration 215/1000 | Loss: 0.00001273
Iteration 216/1000 | Loss: 0.00001273
Iteration 217/1000 | Loss: 0.00001273
Iteration 218/1000 | Loss: 0.00001273
Iteration 219/1000 | Loss: 0.00001273
Iteration 220/1000 | Loss: 0.00001273
Iteration 221/1000 | Loss: 0.00001272
Iteration 222/1000 | Loss: 0.00001272
Iteration 223/1000 | Loss: 0.00001272
Iteration 224/1000 | Loss: 0.00001272
Iteration 225/1000 | Loss: 0.00001272
Iteration 226/1000 | Loss: 0.00001272
Iteration 227/1000 | Loss: 0.00001272
Iteration 228/1000 | Loss: 0.00001272
Iteration 229/1000 | Loss: 0.00001272
Iteration 230/1000 | Loss: 0.00001272
Iteration 231/1000 | Loss: 0.00001272
Iteration 232/1000 | Loss: 0.00001272
Iteration 233/1000 | Loss: 0.00001272
Iteration 234/1000 | Loss: 0.00001272
Iteration 235/1000 | Loss: 0.00001272
Iteration 236/1000 | Loss: 0.00001272
Iteration 237/1000 | Loss: 0.00001272
Iteration 238/1000 | Loss: 0.00001272
Iteration 239/1000 | Loss: 0.00001272
Iteration 240/1000 | Loss: 0.00001272
Iteration 241/1000 | Loss: 0.00001272
Iteration 242/1000 | Loss: 0.00001272
Iteration 243/1000 | Loss: 0.00001272
Iteration 244/1000 | Loss: 0.00001272
Iteration 245/1000 | Loss: 0.00001272
Iteration 246/1000 | Loss: 0.00001272
Iteration 247/1000 | Loss: 0.00001271
Iteration 248/1000 | Loss: 0.00001271
Iteration 249/1000 | Loss: 0.00001271
Iteration 250/1000 | Loss: 0.00001271
Iteration 251/1000 | Loss: 0.00001271
Iteration 252/1000 | Loss: 0.00001271
Iteration 253/1000 | Loss: 0.00001271
Iteration 254/1000 | Loss: 0.00001271
Iteration 255/1000 | Loss: 0.00001271
Iteration 256/1000 | Loss: 0.00001271
Iteration 257/1000 | Loss: 0.00001271
Iteration 258/1000 | Loss: 0.00001271
Iteration 259/1000 | Loss: 0.00001271
Iteration 260/1000 | Loss: 0.00001271
Iteration 261/1000 | Loss: 0.00001271
Iteration 262/1000 | Loss: 0.00001271
Iteration 263/1000 | Loss: 0.00001271
Iteration 264/1000 | Loss: 0.00001271
Iteration 265/1000 | Loss: 0.00001271
Iteration 266/1000 | Loss: 0.00001271
Iteration 267/1000 | Loss: 0.00001271
Iteration 268/1000 | Loss: 0.00001271
Iteration 269/1000 | Loss: 0.00001271
Iteration 270/1000 | Loss: 0.00001271
Iteration 271/1000 | Loss: 0.00001271
Iteration 272/1000 | Loss: 0.00001271
Iteration 273/1000 | Loss: 0.00001271
Iteration 274/1000 | Loss: 0.00001271
Iteration 275/1000 | Loss: 0.00001271
Iteration 276/1000 | Loss: 0.00001271
Iteration 277/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.2714755030174274e-05, 1.2714755030174274e-05, 1.2714755030174274e-05, 1.2714755030174274e-05, 1.2714755030174274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2714755030174274e-05

Optimization complete. Final v2v error: 3.076280117034912 mm

Highest mean error: 3.5963990688323975 mm for frame 57

Lowest mean error: 2.755692720413208 mm for frame 132

Saving results

Total time: 46.14004731178284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792863
Iteration 2/25 | Loss: 0.00144120
Iteration 3/25 | Loss: 0.00129845
Iteration 4/25 | Loss: 0.00128094
Iteration 5/25 | Loss: 0.00127714
Iteration 6/25 | Loss: 0.00127707
Iteration 7/25 | Loss: 0.00127707
Iteration 8/25 | Loss: 0.00127707
Iteration 9/25 | Loss: 0.00127707
Iteration 10/25 | Loss: 0.00127707
Iteration 11/25 | Loss: 0.00127707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012770728208124638, 0.0012770728208124638, 0.0012770728208124638, 0.0012770728208124638, 0.0012770728208124638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012770728208124638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30221152
Iteration 2/25 | Loss: 0.00093342
Iteration 3/25 | Loss: 0.00093338
Iteration 4/25 | Loss: 0.00093338
Iteration 5/25 | Loss: 0.00093338
Iteration 6/25 | Loss: 0.00093338
Iteration 7/25 | Loss: 0.00093338
Iteration 8/25 | Loss: 0.00093337
Iteration 9/25 | Loss: 0.00093337
Iteration 10/25 | Loss: 0.00093337
Iteration 11/25 | Loss: 0.00093337
Iteration 12/25 | Loss: 0.00093337
Iteration 13/25 | Loss: 0.00093337
Iteration 14/25 | Loss: 0.00093337
Iteration 15/25 | Loss: 0.00093337
Iteration 16/25 | Loss: 0.00093337
Iteration 17/25 | Loss: 0.00093337
Iteration 18/25 | Loss: 0.00093337
Iteration 19/25 | Loss: 0.00093337
Iteration 20/25 | Loss: 0.00093337
Iteration 21/25 | Loss: 0.00093337
Iteration 22/25 | Loss: 0.00093337
Iteration 23/25 | Loss: 0.00093337
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009333736961707473, 0.0009333736961707473, 0.0009333736961707473, 0.0009333736961707473, 0.0009333736961707473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009333736961707473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093337
Iteration 2/1000 | Loss: 0.00003535
Iteration 3/1000 | Loss: 0.00002475
Iteration 4/1000 | Loss: 0.00002190
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001894
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001719
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001717
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001708
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001698
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001696
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001695
Iteration 32/1000 | Loss: 0.00001695
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001694
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001690
Iteration 41/1000 | Loss: 0.00001690
Iteration 42/1000 | Loss: 0.00001689
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001677
Iteration 71/1000 | Loss: 0.00001677
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001676
Iteration 75/1000 | Loss: 0.00001676
Iteration 76/1000 | Loss: 0.00001676
Iteration 77/1000 | Loss: 0.00001676
Iteration 78/1000 | Loss: 0.00001676
Iteration 79/1000 | Loss: 0.00001676
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001675
Iteration 83/1000 | Loss: 0.00001675
Iteration 84/1000 | Loss: 0.00001675
Iteration 85/1000 | Loss: 0.00001675
Iteration 86/1000 | Loss: 0.00001675
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001673
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001672
Iteration 103/1000 | Loss: 0.00001672
Iteration 104/1000 | Loss: 0.00001672
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001670
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001669
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001669
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001667
Iteration 128/1000 | Loss: 0.00001667
Iteration 129/1000 | Loss: 0.00001667
Iteration 130/1000 | Loss: 0.00001667
Iteration 131/1000 | Loss: 0.00001667
Iteration 132/1000 | Loss: 0.00001667
Iteration 133/1000 | Loss: 0.00001667
Iteration 134/1000 | Loss: 0.00001667
Iteration 135/1000 | Loss: 0.00001667
Iteration 136/1000 | Loss: 0.00001666
Iteration 137/1000 | Loss: 0.00001666
Iteration 138/1000 | Loss: 0.00001666
Iteration 139/1000 | Loss: 0.00001666
Iteration 140/1000 | Loss: 0.00001666
Iteration 141/1000 | Loss: 0.00001666
Iteration 142/1000 | Loss: 0.00001665
Iteration 143/1000 | Loss: 0.00001665
Iteration 144/1000 | Loss: 0.00001665
Iteration 145/1000 | Loss: 0.00001665
Iteration 146/1000 | Loss: 0.00001664
Iteration 147/1000 | Loss: 0.00001664
Iteration 148/1000 | Loss: 0.00001664
Iteration 149/1000 | Loss: 0.00001663
Iteration 150/1000 | Loss: 0.00001663
Iteration 151/1000 | Loss: 0.00001663
Iteration 152/1000 | Loss: 0.00001663
Iteration 153/1000 | Loss: 0.00001663
Iteration 154/1000 | Loss: 0.00001663
Iteration 155/1000 | Loss: 0.00001662
Iteration 156/1000 | Loss: 0.00001662
Iteration 157/1000 | Loss: 0.00001662
Iteration 158/1000 | Loss: 0.00001662
Iteration 159/1000 | Loss: 0.00001662
Iteration 160/1000 | Loss: 0.00001662
Iteration 161/1000 | Loss: 0.00001662
Iteration 162/1000 | Loss: 0.00001661
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001661
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001660
Iteration 169/1000 | Loss: 0.00001660
Iteration 170/1000 | Loss: 0.00001660
Iteration 171/1000 | Loss: 0.00001660
Iteration 172/1000 | Loss: 0.00001660
Iteration 173/1000 | Loss: 0.00001660
Iteration 174/1000 | Loss: 0.00001660
Iteration 175/1000 | Loss: 0.00001660
Iteration 176/1000 | Loss: 0.00001660
Iteration 177/1000 | Loss: 0.00001659
Iteration 178/1000 | Loss: 0.00001659
Iteration 179/1000 | Loss: 0.00001659
Iteration 180/1000 | Loss: 0.00001659
Iteration 181/1000 | Loss: 0.00001659
Iteration 182/1000 | Loss: 0.00001659
Iteration 183/1000 | Loss: 0.00001659
Iteration 184/1000 | Loss: 0.00001659
Iteration 185/1000 | Loss: 0.00001659
Iteration 186/1000 | Loss: 0.00001658
Iteration 187/1000 | Loss: 0.00001658
Iteration 188/1000 | Loss: 0.00001658
Iteration 189/1000 | Loss: 0.00001658
Iteration 190/1000 | Loss: 0.00001658
Iteration 191/1000 | Loss: 0.00001658
Iteration 192/1000 | Loss: 0.00001658
Iteration 193/1000 | Loss: 0.00001658
Iteration 194/1000 | Loss: 0.00001658
Iteration 195/1000 | Loss: 0.00001658
Iteration 196/1000 | Loss: 0.00001658
Iteration 197/1000 | Loss: 0.00001658
Iteration 198/1000 | Loss: 0.00001658
Iteration 199/1000 | Loss: 0.00001658
Iteration 200/1000 | Loss: 0.00001658
Iteration 201/1000 | Loss: 0.00001657
Iteration 202/1000 | Loss: 0.00001657
Iteration 203/1000 | Loss: 0.00001657
Iteration 204/1000 | Loss: 0.00001657
Iteration 205/1000 | Loss: 0.00001657
Iteration 206/1000 | Loss: 0.00001657
Iteration 207/1000 | Loss: 0.00001657
Iteration 208/1000 | Loss: 0.00001657
Iteration 209/1000 | Loss: 0.00001657
Iteration 210/1000 | Loss: 0.00001657
Iteration 211/1000 | Loss: 0.00001657
Iteration 212/1000 | Loss: 0.00001657
Iteration 213/1000 | Loss: 0.00001657
Iteration 214/1000 | Loss: 0.00001657
Iteration 215/1000 | Loss: 0.00001657
Iteration 216/1000 | Loss: 0.00001657
Iteration 217/1000 | Loss: 0.00001657
Iteration 218/1000 | Loss: 0.00001656
Iteration 219/1000 | Loss: 0.00001656
Iteration 220/1000 | Loss: 0.00001656
Iteration 221/1000 | Loss: 0.00001656
Iteration 222/1000 | Loss: 0.00001656
Iteration 223/1000 | Loss: 0.00001656
Iteration 224/1000 | Loss: 0.00001656
Iteration 225/1000 | Loss: 0.00001656
Iteration 226/1000 | Loss: 0.00001656
Iteration 227/1000 | Loss: 0.00001656
Iteration 228/1000 | Loss: 0.00001655
Iteration 229/1000 | Loss: 0.00001655
Iteration 230/1000 | Loss: 0.00001655
Iteration 231/1000 | Loss: 0.00001655
Iteration 232/1000 | Loss: 0.00001655
Iteration 233/1000 | Loss: 0.00001655
Iteration 234/1000 | Loss: 0.00001655
Iteration 235/1000 | Loss: 0.00001655
Iteration 236/1000 | Loss: 0.00001655
Iteration 237/1000 | Loss: 0.00001655
Iteration 238/1000 | Loss: 0.00001655
Iteration 239/1000 | Loss: 0.00001655
Iteration 240/1000 | Loss: 0.00001655
Iteration 241/1000 | Loss: 0.00001655
Iteration 242/1000 | Loss: 0.00001655
Iteration 243/1000 | Loss: 0.00001655
Iteration 244/1000 | Loss: 0.00001654
Iteration 245/1000 | Loss: 0.00001654
Iteration 246/1000 | Loss: 0.00001654
Iteration 247/1000 | Loss: 0.00001654
Iteration 248/1000 | Loss: 0.00001654
Iteration 249/1000 | Loss: 0.00001654
Iteration 250/1000 | Loss: 0.00001654
Iteration 251/1000 | Loss: 0.00001654
Iteration 252/1000 | Loss: 0.00001654
Iteration 253/1000 | Loss: 0.00001654
Iteration 254/1000 | Loss: 0.00001654
Iteration 255/1000 | Loss: 0.00001654
Iteration 256/1000 | Loss: 0.00001654
Iteration 257/1000 | Loss: 0.00001654
Iteration 258/1000 | Loss: 0.00001654
Iteration 259/1000 | Loss: 0.00001654
Iteration 260/1000 | Loss: 0.00001654
Iteration 261/1000 | Loss: 0.00001654
Iteration 262/1000 | Loss: 0.00001654
Iteration 263/1000 | Loss: 0.00001654
Iteration 264/1000 | Loss: 0.00001654
Iteration 265/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.6540103388251737e-05, 1.6540103388251737e-05, 1.6540103388251737e-05, 1.6540103388251737e-05, 1.6540103388251737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6540103388251737e-05

Optimization complete. Final v2v error: 3.3851308822631836 mm

Highest mean error: 4.725973606109619 mm for frame 23

Lowest mean error: 2.833850622177124 mm for frame 206

Saving results

Total time: 51.81974482536316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466270
Iteration 2/25 | Loss: 0.00163632
Iteration 3/25 | Loss: 0.00142488
Iteration 4/25 | Loss: 0.00135911
Iteration 5/25 | Loss: 0.00134846
Iteration 6/25 | Loss: 0.00131954
Iteration 7/25 | Loss: 0.00131671
Iteration 8/25 | Loss: 0.00131193
Iteration 9/25 | Loss: 0.00131038
Iteration 10/25 | Loss: 0.00131011
Iteration 11/25 | Loss: 0.00131005
Iteration 12/25 | Loss: 0.00131005
Iteration 13/25 | Loss: 0.00131004
Iteration 14/25 | Loss: 0.00131004
Iteration 15/25 | Loss: 0.00131004
Iteration 16/25 | Loss: 0.00131004
Iteration 17/25 | Loss: 0.00131004
Iteration 18/25 | Loss: 0.00131004
Iteration 19/25 | Loss: 0.00131004
Iteration 20/25 | Loss: 0.00131004
Iteration 21/25 | Loss: 0.00131004
Iteration 22/25 | Loss: 0.00131004
Iteration 23/25 | Loss: 0.00131004
Iteration 24/25 | Loss: 0.00131003
Iteration 25/25 | Loss: 0.00131003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33632612
Iteration 2/25 | Loss: 0.00160118
Iteration 3/25 | Loss: 0.00160115
Iteration 4/25 | Loss: 0.00160115
Iteration 5/25 | Loss: 0.00160115
Iteration 6/25 | Loss: 0.00160115
Iteration 7/25 | Loss: 0.00160115
Iteration 8/25 | Loss: 0.00160115
Iteration 9/25 | Loss: 0.00160115
Iteration 10/25 | Loss: 0.00160115
Iteration 11/25 | Loss: 0.00160115
Iteration 12/25 | Loss: 0.00160115
Iteration 13/25 | Loss: 0.00160115
Iteration 14/25 | Loss: 0.00160115
Iteration 15/25 | Loss: 0.00160115
Iteration 16/25 | Loss: 0.00160115
Iteration 17/25 | Loss: 0.00160115
Iteration 18/25 | Loss: 0.00160115
Iteration 19/25 | Loss: 0.00160115
Iteration 20/25 | Loss: 0.00160115
Iteration 21/25 | Loss: 0.00160115
Iteration 22/25 | Loss: 0.00160115
Iteration 23/25 | Loss: 0.00160115
Iteration 24/25 | Loss: 0.00160115
Iteration 25/25 | Loss: 0.00160115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160115
Iteration 2/1000 | Loss: 0.00009910
Iteration 3/1000 | Loss: 0.00008368
Iteration 4/1000 | Loss: 0.00003837
Iteration 5/1000 | Loss: 0.00004030
Iteration 6/1000 | Loss: 0.00004117
Iteration 7/1000 | Loss: 0.00003245
Iteration 8/1000 | Loss: 0.00004628
Iteration 9/1000 | Loss: 0.00005009
Iteration 10/1000 | Loss: 0.00004870
Iteration 11/1000 | Loss: 0.00003975
Iteration 12/1000 | Loss: 0.00004698
Iteration 13/1000 | Loss: 0.00003757
Iteration 14/1000 | Loss: 0.00003907
Iteration 15/1000 | Loss: 0.00003132
Iteration 16/1000 | Loss: 0.00003750
Iteration 17/1000 | Loss: 0.00004015
Iteration 18/1000 | Loss: 0.00003721
Iteration 19/1000 | Loss: 0.00002829
Iteration 20/1000 | Loss: 0.00002883
Iteration 21/1000 | Loss: 0.00002785
Iteration 22/1000 | Loss: 0.00002618
Iteration 23/1000 | Loss: 0.00003283
Iteration 24/1000 | Loss: 0.00003429
Iteration 25/1000 | Loss: 0.00003706
Iteration 26/1000 | Loss: 0.00003409
Iteration 27/1000 | Loss: 0.00004246
Iteration 28/1000 | Loss: 0.00002977
Iteration 29/1000 | Loss: 0.00002976
Iteration 30/1000 | Loss: 0.00002894
Iteration 31/1000 | Loss: 0.00002821
Iteration 32/1000 | Loss: 0.00003042
Iteration 33/1000 | Loss: 0.00002695
Iteration 34/1000 | Loss: 0.00003220
Iteration 35/1000 | Loss: 0.00002998
Iteration 36/1000 | Loss: 0.00002929
Iteration 37/1000 | Loss: 0.00002574
Iteration 38/1000 | Loss: 0.00002677
Iteration 39/1000 | Loss: 0.00002580
Iteration 40/1000 | Loss: 0.00002855
Iteration 41/1000 | Loss: 0.00002752
Iteration 42/1000 | Loss: 0.00002782
Iteration 43/1000 | Loss: 0.00002767
Iteration 44/1000 | Loss: 0.00003231
Iteration 45/1000 | Loss: 0.00003002
Iteration 46/1000 | Loss: 0.00003311
Iteration 47/1000 | Loss: 0.00003598
Iteration 48/1000 | Loss: 0.00004183
Iteration 49/1000 | Loss: 0.00004340
Iteration 50/1000 | Loss: 0.00003911
Iteration 51/1000 | Loss: 0.00003049
Iteration 52/1000 | Loss: 0.00004170
Iteration 53/1000 | Loss: 0.00005178
Iteration 54/1000 | Loss: 0.00005029
Iteration 55/1000 | Loss: 0.00003756
Iteration 56/1000 | Loss: 0.00004350
Iteration 57/1000 | Loss: 0.00004929
Iteration 58/1000 | Loss: 0.00005129
Iteration 59/1000 | Loss: 0.00004829
Iteration 60/1000 | Loss: 0.00003473
Iteration 61/1000 | Loss: 0.00002675
Iteration 62/1000 | Loss: 0.00003513
Iteration 63/1000 | Loss: 0.00004830
Iteration 64/1000 | Loss: 0.00004297
Iteration 65/1000 | Loss: 0.00005183
Iteration 66/1000 | Loss: 0.00004828
Iteration 67/1000 | Loss: 0.00006177
Iteration 68/1000 | Loss: 0.00005269
Iteration 69/1000 | Loss: 0.00006126
Iteration 70/1000 | Loss: 0.00004936
Iteration 71/1000 | Loss: 0.00006095
Iteration 72/1000 | Loss: 0.00004758
Iteration 73/1000 | Loss: 0.00005429
Iteration 74/1000 | Loss: 0.00005269
Iteration 75/1000 | Loss: 0.00004980
Iteration 76/1000 | Loss: 0.00004715
Iteration 77/1000 | Loss: 0.00004765
Iteration 78/1000 | Loss: 0.00004628
Iteration 79/1000 | Loss: 0.00005622
Iteration 80/1000 | Loss: 0.00006082
Iteration 81/1000 | Loss: 0.00005519
Iteration 82/1000 | Loss: 0.00004670
Iteration 83/1000 | Loss: 0.00002884
Iteration 84/1000 | Loss: 0.00003767
Iteration 85/1000 | Loss: 0.00003413
Iteration 86/1000 | Loss: 0.00003667
Iteration 87/1000 | Loss: 0.00002735
Iteration 88/1000 | Loss: 0.00004779
Iteration 89/1000 | Loss: 0.00004715
Iteration 90/1000 | Loss: 0.00006097
Iteration 91/1000 | Loss: 0.00004956
Iteration 92/1000 | Loss: 0.00005995
Iteration 93/1000 | Loss: 0.00004664
Iteration 94/1000 | Loss: 0.00005422
Iteration 95/1000 | Loss: 0.00003981
Iteration 96/1000 | Loss: 0.00003336
Iteration 97/1000 | Loss: 0.00003465
Iteration 98/1000 | Loss: 0.00003644
Iteration 99/1000 | Loss: 0.00004617
Iteration 100/1000 | Loss: 0.00005283
Iteration 101/1000 | Loss: 0.00004679
Iteration 102/1000 | Loss: 0.00004111
Iteration 103/1000 | Loss: 0.00004794
Iteration 104/1000 | Loss: 0.00003167
Iteration 105/1000 | Loss: 0.00002805
Iteration 106/1000 | Loss: 0.00003254
Iteration 107/1000 | Loss: 0.00003830
Iteration 108/1000 | Loss: 0.00003837
Iteration 109/1000 | Loss: 0.00003909
Iteration 110/1000 | Loss: 0.00004623
Iteration 111/1000 | Loss: 0.00004270
Iteration 112/1000 | Loss: 0.00004469
Iteration 113/1000 | Loss: 0.00005226
Iteration 114/1000 | Loss: 0.00004498
Iteration 115/1000 | Loss: 0.00003941
Iteration 116/1000 | Loss: 0.00004497
Iteration 117/1000 | Loss: 0.00003075
Iteration 118/1000 | Loss: 0.00003265
Iteration 119/1000 | Loss: 0.00003293
Iteration 120/1000 | Loss: 0.00004440
Iteration 121/1000 | Loss: 0.00004577
Iteration 122/1000 | Loss: 0.00004154
Iteration 123/1000 | Loss: 0.00003589
Iteration 124/1000 | Loss: 0.00003008
Iteration 125/1000 | Loss: 0.00002602
Iteration 126/1000 | Loss: 0.00003337
Iteration 127/1000 | Loss: 0.00003167
Iteration 128/1000 | Loss: 0.00003361
Iteration 129/1000 | Loss: 0.00002710
Iteration 130/1000 | Loss: 0.00002512
Iteration 131/1000 | Loss: 0.00002622
Iteration 132/1000 | Loss: 0.00003018
Iteration 133/1000 | Loss: 0.00003001
Iteration 134/1000 | Loss: 0.00003109
Iteration 135/1000 | Loss: 0.00003143
Iteration 136/1000 | Loss: 0.00002891
Iteration 137/1000 | Loss: 0.00002747
Iteration 138/1000 | Loss: 0.00003184
Iteration 139/1000 | Loss: 0.00002767
Iteration 140/1000 | Loss: 0.00003686
Iteration 141/1000 | Loss: 0.00003128
Iteration 142/1000 | Loss: 0.00003410
Iteration 143/1000 | Loss: 0.00003833
Iteration 144/1000 | Loss: 0.00003303
Iteration 145/1000 | Loss: 0.00002582
Iteration 146/1000 | Loss: 0.00003137
Iteration 147/1000 | Loss: 0.00004049
Iteration 148/1000 | Loss: 0.00004040
Iteration 149/1000 | Loss: 0.00003149
Iteration 150/1000 | Loss: 0.00002967
Iteration 151/1000 | Loss: 0.00003238
Iteration 152/1000 | Loss: 0.00003522
Iteration 153/1000 | Loss: 0.00003424
Iteration 154/1000 | Loss: 0.00003704
Iteration 155/1000 | Loss: 0.00004377
Iteration 156/1000 | Loss: 0.00004541
Iteration 157/1000 | Loss: 0.00005305
Iteration 158/1000 | Loss: 0.00005331
Iteration 159/1000 | Loss: 0.00003741
Iteration 160/1000 | Loss: 0.00003414
Iteration 161/1000 | Loss: 0.00004854
Iteration 162/1000 | Loss: 0.00004477
Iteration 163/1000 | Loss: 0.00005861
Iteration 164/1000 | Loss: 0.00005736
Iteration 165/1000 | Loss: 0.00004981
Iteration 166/1000 | Loss: 0.00003940
Iteration 167/1000 | Loss: 0.00005965
Iteration 168/1000 | Loss: 0.00005169
Iteration 169/1000 | Loss: 0.00023276
Iteration 170/1000 | Loss: 0.00005242
Iteration 171/1000 | Loss: 0.00006785
Iteration 172/1000 | Loss: 0.00005364
Iteration 173/1000 | Loss: 0.00005841
Iteration 174/1000 | Loss: 0.00006181
Iteration 175/1000 | Loss: 0.00004112
Iteration 176/1000 | Loss: 0.00003744
Iteration 177/1000 | Loss: 0.00003869
Iteration 178/1000 | Loss: 0.00004375
Iteration 179/1000 | Loss: 0.00004404
Iteration 180/1000 | Loss: 0.00004019
Iteration 181/1000 | Loss: 0.00002883
Iteration 182/1000 | Loss: 0.00004239
Iteration 183/1000 | Loss: 0.00003297
Iteration 184/1000 | Loss: 0.00003025
Iteration 185/1000 | Loss: 0.00003055
Iteration 186/1000 | Loss: 0.00002782
Iteration 187/1000 | Loss: 0.00002671
Iteration 188/1000 | Loss: 0.00003449
Iteration 189/1000 | Loss: 0.00002904
Iteration 190/1000 | Loss: 0.00003486
Iteration 191/1000 | Loss: 0.00002922
Iteration 192/1000 | Loss: 0.00003570
Iteration 193/1000 | Loss: 0.00003018
Iteration 194/1000 | Loss: 0.00003770
Iteration 195/1000 | Loss: 0.00003056
Iteration 196/1000 | Loss: 0.00004136
Iteration 197/1000 | Loss: 0.00003021
Iteration 198/1000 | Loss: 0.00004542
Iteration 199/1000 | Loss: 0.00003357
Iteration 200/1000 | Loss: 0.00005271
Iteration 201/1000 | Loss: 0.00003478
Iteration 202/1000 | Loss: 0.00005141
Iteration 203/1000 | Loss: 0.00003755
Iteration 204/1000 | Loss: 0.00005886
Iteration 205/1000 | Loss: 0.00004103
Iteration 206/1000 | Loss: 0.00005945
Iteration 207/1000 | Loss: 0.00004129
Iteration 208/1000 | Loss: 0.00004124
Iteration 209/1000 | Loss: 0.00003479
Iteration 210/1000 | Loss: 0.00003237
Iteration 211/1000 | Loss: 0.00003992
Iteration 212/1000 | Loss: 0.00002937
Iteration 213/1000 | Loss: 0.00004336
Iteration 214/1000 | Loss: 0.00003400
Iteration 215/1000 | Loss: 0.00003784
Iteration 216/1000 | Loss: 0.00003644
Iteration 217/1000 | Loss: 0.00004594
Iteration 218/1000 | Loss: 0.00004157
Iteration 219/1000 | Loss: 0.00005199
Iteration 220/1000 | Loss: 0.00004354
Iteration 221/1000 | Loss: 0.00004975
Iteration 222/1000 | Loss: 0.00004470
Iteration 223/1000 | Loss: 0.00005614
Iteration 224/1000 | Loss: 0.00004509
Iteration 225/1000 | Loss: 0.00005892
Iteration 226/1000 | Loss: 0.00004884
Iteration 227/1000 | Loss: 0.00006158
Iteration 228/1000 | Loss: 0.00004814
Iteration 229/1000 | Loss: 0.00005814
Iteration 230/1000 | Loss: 0.00004724
Iteration 231/1000 | Loss: 0.00006308
Iteration 232/1000 | Loss: 0.00005261
Iteration 233/1000 | Loss: 0.00007119
Iteration 234/1000 | Loss: 0.00005924
Iteration 235/1000 | Loss: 0.00006563
Iteration 236/1000 | Loss: 0.00003741
Iteration 237/1000 | Loss: 0.00003085
Iteration 238/1000 | Loss: 0.00004023
Iteration 239/1000 | Loss: 0.00005037
Iteration 240/1000 | Loss: 0.00004072
Iteration 241/1000 | Loss: 0.00006248
Iteration 242/1000 | Loss: 0.00004716
Iteration 243/1000 | Loss: 0.00005366
Iteration 244/1000 | Loss: 0.00004359
Iteration 245/1000 | Loss: 0.00005607
Iteration 246/1000 | Loss: 0.00004769
Iteration 247/1000 | Loss: 0.00006338
Iteration 248/1000 | Loss: 0.00004756
Iteration 249/1000 | Loss: 0.00006021
Iteration 250/1000 | Loss: 0.00004487
Iteration 251/1000 | Loss: 0.00002941
Iteration 252/1000 | Loss: 0.00002497
Iteration 253/1000 | Loss: 0.00002432
Iteration 254/1000 | Loss: 0.00002974
Iteration 255/1000 | Loss: 0.00003717
Iteration 256/1000 | Loss: 0.00003377
Iteration 257/1000 | Loss: 0.00003614
Iteration 258/1000 | Loss: 0.00003005
Iteration 259/1000 | Loss: 0.00002827
Iteration 260/1000 | Loss: 0.00003210
Iteration 261/1000 | Loss: 0.00003582
Iteration 262/1000 | Loss: 0.00003513
Iteration 263/1000 | Loss: 0.00003992
Iteration 264/1000 | Loss: 0.00003533
Iteration 265/1000 | Loss: 0.00003338
Iteration 266/1000 | Loss: 0.00002819
Iteration 267/1000 | Loss: 0.00002513
Iteration 268/1000 | Loss: 0.00002862
Iteration 269/1000 | Loss: 0.00003059
Iteration 270/1000 | Loss: 0.00003789
Iteration 271/1000 | Loss: 0.00004188
Iteration 272/1000 | Loss: 0.00004141
Iteration 273/1000 | Loss: 0.00002872
Iteration 274/1000 | Loss: 0.00002366
Iteration 275/1000 | Loss: 0.00002647
Iteration 276/1000 | Loss: 0.00002965
Iteration 277/1000 | Loss: 0.00003789
Iteration 278/1000 | Loss: 0.00003339
Iteration 279/1000 | Loss: 0.00003480
Iteration 280/1000 | Loss: 0.00003178
Iteration 281/1000 | Loss: 0.00002938
Iteration 282/1000 | Loss: 0.00002546
Iteration 283/1000 | Loss: 0.00002658
Iteration 284/1000 | Loss: 0.00003135
Iteration 285/1000 | Loss: 0.00003584
Iteration 286/1000 | Loss: 0.00003208
Iteration 287/1000 | Loss: 0.00002894
Iteration 288/1000 | Loss: 0.00002741
Iteration 289/1000 | Loss: 0.00002910
Iteration 290/1000 | Loss: 0.00003289
Iteration 291/1000 | Loss: 0.00003847
Iteration 292/1000 | Loss: 0.00003681
Iteration 293/1000 | Loss: 0.00003865
Iteration 294/1000 | Loss: 0.00004173
Iteration 295/1000 | Loss: 0.00004534
Iteration 296/1000 | Loss: 0.00004565
Iteration 297/1000 | Loss: 0.00005070
Iteration 298/1000 | Loss: 0.00004924
Iteration 299/1000 | Loss: 0.00003048
Iteration 300/1000 | Loss: 0.00002818
Iteration 301/1000 | Loss: 0.00003174
Iteration 302/1000 | Loss: 0.00005149
Iteration 303/1000 | Loss: 0.00004255
Iteration 304/1000 | Loss: 0.00005692
Iteration 305/1000 | Loss: 0.00004721
Iteration 306/1000 | Loss: 0.00005953
Iteration 307/1000 | Loss: 0.00004254
Iteration 308/1000 | Loss: 0.00006433
Iteration 309/1000 | Loss: 0.00005675
Iteration 310/1000 | Loss: 0.00007060
Iteration 311/1000 | Loss: 0.00005267
Iteration 312/1000 | Loss: 0.00005793
Iteration 313/1000 | Loss: 0.00005905
Iteration 314/1000 | Loss: 0.00006575
Iteration 315/1000 | Loss: 0.00006174
Iteration 316/1000 | Loss: 0.00006635
Iteration 317/1000 | Loss: 0.00004976
Iteration 318/1000 | Loss: 0.00005951
Iteration 319/1000 | Loss: 0.00004794
Iteration 320/1000 | Loss: 0.00006447
Iteration 321/1000 | Loss: 0.00005449
Iteration 322/1000 | Loss: 0.00006873
Iteration 323/1000 | Loss: 0.00005457
Iteration 324/1000 | Loss: 0.00006943
Iteration 325/1000 | Loss: 0.00006404
Iteration 326/1000 | Loss: 0.00008335
Iteration 327/1000 | Loss: 0.00007771
Iteration 328/1000 | Loss: 0.00013700
Iteration 329/1000 | Loss: 0.00005712
Iteration 330/1000 | Loss: 0.00008297
Iteration 331/1000 | Loss: 0.00005085
Iteration 332/1000 | Loss: 0.00003637
Iteration 333/1000 | Loss: 0.00004370
Iteration 334/1000 | Loss: 0.00003477
Iteration 335/1000 | Loss: 0.00006645
Iteration 336/1000 | Loss: 0.00005266
Iteration 337/1000 | Loss: 0.00005759
Iteration 338/1000 | Loss: 0.00003719
Iteration 339/1000 | Loss: 0.00002933
Iteration 340/1000 | Loss: 0.00002276
Iteration 341/1000 | Loss: 0.00002659
Iteration 342/1000 | Loss: 0.00004195
Iteration 343/1000 | Loss: 0.00003494
Iteration 344/1000 | Loss: 0.00002547
Iteration 345/1000 | Loss: 0.00003303
Iteration 346/1000 | Loss: 0.00003298
Iteration 347/1000 | Loss: 0.00003850
Iteration 348/1000 | Loss: 0.00003531
Iteration 349/1000 | Loss: 0.00003060
Iteration 350/1000 | Loss: 0.00003006
Iteration 351/1000 | Loss: 0.00002977
Iteration 352/1000 | Loss: 0.00002949
Iteration 353/1000 | Loss: 0.00002943
Iteration 354/1000 | Loss: 0.00002953
Iteration 355/1000 | Loss: 0.00002982
Iteration 356/1000 | Loss: 0.00003131
Iteration 357/1000 | Loss: 0.00003307
Iteration 358/1000 | Loss: 0.00003275
Iteration 359/1000 | Loss: 0.00002947
Iteration 360/1000 | Loss: 0.00002882
Iteration 361/1000 | Loss: 0.00002957
Iteration 362/1000 | Loss: 0.00002822
Iteration 363/1000 | Loss: 0.00003113
Iteration 364/1000 | Loss: 0.00003683
Iteration 365/1000 | Loss: 0.00003593
Iteration 366/1000 | Loss: 0.00004156
Iteration 367/1000 | Loss: 0.00004070
Iteration 368/1000 | Loss: 0.00004493
Iteration 369/1000 | Loss: 0.00003485
Iteration 370/1000 | Loss: 0.00003285
Iteration 371/1000 | Loss: 0.00003573
Iteration 372/1000 | Loss: 0.00002915
Iteration 373/1000 | Loss: 0.00002600
Iteration 374/1000 | Loss: 0.00003233
Iteration 375/1000 | Loss: 0.00004156
Iteration 376/1000 | Loss: 0.00003073
Iteration 377/1000 | Loss: 0.00003324
Iteration 378/1000 | Loss: 0.00004311
Iteration 379/1000 | Loss: 0.00004179
Iteration 380/1000 | Loss: 0.00004602
Iteration 381/1000 | Loss: 0.00004428
Iteration 382/1000 | Loss: 0.00003306
Iteration 383/1000 | Loss: 0.00003196
Iteration 384/1000 | Loss: 0.00003945
Iteration 385/1000 | Loss: 0.00003952
Iteration 386/1000 | Loss: 0.00004566
Iteration 387/1000 | Loss: 0.00004829
Iteration 388/1000 | Loss: 0.00004972
Iteration 389/1000 | Loss: 0.00005754
Iteration 390/1000 | Loss: 0.00004214
Iteration 391/1000 | Loss: 0.00004052
Iteration 392/1000 | Loss: 0.00003903
Iteration 393/1000 | Loss: 0.00004352
Iteration 394/1000 | Loss: 0.00004542
Iteration 395/1000 | Loss: 0.00003458
Iteration 396/1000 | Loss: 0.00004527
Iteration 397/1000 | Loss: 0.00004688
Iteration 398/1000 | Loss: 0.00005159
Iteration 399/1000 | Loss: 0.00003701
Iteration 400/1000 | Loss: 0.00003165
Iteration 401/1000 | Loss: 0.00003516
Iteration 402/1000 | Loss: 0.00003513
Iteration 403/1000 | Loss: 0.00004348
Iteration 404/1000 | Loss: 0.00004954
Iteration 405/1000 | Loss: 0.00004807
Iteration 406/1000 | Loss: 0.00004365
Iteration 407/1000 | Loss: 0.00004871
Iteration 408/1000 | Loss: 0.00004739
Iteration 409/1000 | Loss: 0.00005304
Iteration 410/1000 | Loss: 0.00004622
Iteration 411/1000 | Loss: 0.00004938
Iteration 412/1000 | Loss: 0.00005526
Iteration 413/1000 | Loss: 0.00005807
Iteration 414/1000 | Loss: 0.00005150
Iteration 415/1000 | Loss: 0.00005245
Iteration 416/1000 | Loss: 0.00005883
Iteration 417/1000 | Loss: 0.00005910
Iteration 418/1000 | Loss: 0.00006537
Iteration 419/1000 | Loss: 0.00005796
Iteration 420/1000 | Loss: 0.00004150
Iteration 421/1000 | Loss: 0.00003413
Iteration 422/1000 | Loss: 0.00003762
Iteration 423/1000 | Loss: 0.00004620
Iteration 424/1000 | Loss: 0.00004433
Iteration 425/1000 | Loss: 0.00003583
Iteration 426/1000 | Loss: 0.00005327
Iteration 427/1000 | Loss: 0.00005462
Iteration 428/1000 | Loss: 0.00004760
Iteration 429/1000 | Loss: 0.00003022
Iteration 430/1000 | Loss: 0.00003351
Iteration 431/1000 | Loss: 0.00003252
Iteration 432/1000 | Loss: 0.00002947
Iteration 433/1000 | Loss: 0.00002721
Iteration 434/1000 | Loss: 0.00003823
Iteration 435/1000 | Loss: 0.00004437
Iteration 436/1000 | Loss: 0.00005472
Iteration 437/1000 | Loss: 0.00004990
Iteration 438/1000 | Loss: 0.00005804
Iteration 439/1000 | Loss: 0.00006043
Iteration 440/1000 | Loss: 0.00004741
Iteration 441/1000 | Loss: 0.00005620
Iteration 442/1000 | Loss: 0.00005952
Iteration 443/1000 | Loss: 0.00006097
Iteration 444/1000 | Loss: 0.00006741
Iteration 445/1000 | Loss: 0.00005563
Iteration 446/1000 | Loss: 0.00002922
Iteration 447/1000 | Loss: 0.00002320
Iteration 448/1000 | Loss: 0.00002287
Iteration 449/1000 | Loss: 0.00002238
Iteration 450/1000 | Loss: 0.00003287
Iteration 451/1000 | Loss: 0.00003168
Iteration 452/1000 | Loss: 0.00003356
Iteration 453/1000 | Loss: 0.00003706
Iteration 454/1000 | Loss: 0.00003271
Iteration 455/1000 | Loss: 0.00003001
Iteration 456/1000 | Loss: 0.00002430
Iteration 457/1000 | Loss: 0.00002918
Iteration 458/1000 | Loss: 0.00002500
Iteration 459/1000 | Loss: 0.00002723
Iteration 460/1000 | Loss: 0.00002686
Iteration 461/1000 | Loss: 0.00003042
Iteration 462/1000 | Loss: 0.00002546
Iteration 463/1000 | Loss: 0.00002667
Iteration 464/1000 | Loss: 0.00003145
Iteration 465/1000 | Loss: 0.00002989
Iteration 466/1000 | Loss: 0.00002812
Iteration 467/1000 | Loss: 0.00003746
Iteration 468/1000 | Loss: 0.00003016
Iteration 469/1000 | Loss: 0.00002795
Iteration 470/1000 | Loss: 0.00002841
Iteration 471/1000 | Loss: 0.00002729
Iteration 472/1000 | Loss: 0.00002998
Iteration 473/1000 | Loss: 0.00003087
Iteration 474/1000 | Loss: 0.00003156
Iteration 475/1000 | Loss: 0.00002898
Iteration 476/1000 | Loss: 0.00002442
Iteration 477/1000 | Loss: 0.00002607
Iteration 478/1000 | Loss: 0.00003103
Iteration 479/1000 | Loss: 0.00002905
Iteration 480/1000 | Loss: 0.00002979
Iteration 481/1000 | Loss: 0.00002725
Iteration 482/1000 | Loss: 0.00002837
Iteration 483/1000 | Loss: 0.00003476
Iteration 484/1000 | Loss: 0.00002837
Iteration 485/1000 | Loss: 0.00002881
Iteration 486/1000 | Loss: 0.00003041
Iteration 487/1000 | Loss: 0.00003181
Iteration 488/1000 | Loss: 0.00003093
Iteration 489/1000 | Loss: 0.00002818
Iteration 490/1000 | Loss: 0.00002589
Iteration 491/1000 | Loss: 0.00002525
Iteration 492/1000 | Loss: 0.00002837
Iteration 493/1000 | Loss: 0.00003457
Iteration 494/1000 | Loss: 0.00003312
Iteration 495/1000 | Loss: 0.00004118
Iteration 496/1000 | Loss: 0.00003904
Iteration 497/1000 | Loss: 0.00004323
Iteration 498/1000 | Loss: 0.00004225
Iteration 499/1000 | Loss: 0.00004495
Iteration 500/1000 | Loss: 0.00004448
Iteration 501/1000 | Loss: 0.00005581
Iteration 502/1000 | Loss: 0.00004228
Iteration 503/1000 | Loss: 0.00004635
Iteration 504/1000 | Loss: 0.00004234
Iteration 505/1000 | Loss: 0.00004689
Iteration 506/1000 | Loss: 0.00004263
Iteration 507/1000 | Loss: 0.00005437
Iteration 508/1000 | Loss: 0.00004238
Iteration 509/1000 | Loss: 0.00004939
Iteration 510/1000 | Loss: 0.00004273
Iteration 511/1000 | Loss: 0.00004945
Iteration 512/1000 | Loss: 0.00003569
Iteration 513/1000 | Loss: 0.00003115
Iteration 514/1000 | Loss: 0.00002958
Iteration 515/1000 | Loss: 0.00002913
Iteration 516/1000 | Loss: 0.00003377
Iteration 517/1000 | Loss: 0.00003143
Iteration 518/1000 | Loss: 0.00003479
Iteration 519/1000 | Loss: 0.00003141
Iteration 520/1000 | Loss: 0.00003932
Iteration 521/1000 | Loss: 0.00004240
Iteration 522/1000 | Loss: 0.00003745
Iteration 523/1000 | Loss: 0.00003829
Iteration 524/1000 | Loss: 0.00003860
Iteration 525/1000 | Loss: 0.00002932
Iteration 526/1000 | Loss: 0.00003122
Iteration 527/1000 | Loss: 0.00003286
Iteration 528/1000 | Loss: 0.00003152
Iteration 529/1000 | Loss: 0.00002373
Iteration 530/1000 | Loss: 0.00002714
Iteration 531/1000 | Loss: 0.00003205
Iteration 532/1000 | Loss: 0.00003740
Iteration 533/1000 | Loss: 0.00003976
Iteration 534/1000 | Loss: 0.00003774
Iteration 535/1000 | Loss: 0.00003671
Iteration 536/1000 | Loss: 0.00003421
Iteration 537/1000 | Loss: 0.00003699
Iteration 538/1000 | Loss: 0.00004009
Iteration 539/1000 | Loss: 0.00004432
Iteration 540/1000 | Loss: 0.00004385
Iteration 541/1000 | Loss: 0.00003049
Iteration 542/1000 | Loss: 0.00002620
Iteration 543/1000 | Loss: 0.00002957
Iteration 544/1000 | Loss: 0.00003158
Iteration 545/1000 | Loss: 0.00002509
Iteration 546/1000 | Loss: 0.00002619
Iteration 547/1000 | Loss: 0.00002706
Iteration 548/1000 | Loss: 0.00002493
Iteration 549/1000 | Loss: 0.00002453
Iteration 550/1000 | Loss: 0.00002866
Iteration 551/1000 | Loss: 0.00002936
Iteration 552/1000 | Loss: 0.00003198
Iteration 553/1000 | Loss: 0.00002747
Iteration 554/1000 | Loss: 0.00002443
Iteration 555/1000 | Loss: 0.00003093
Iteration 556/1000 | Loss: 0.00003287
Iteration 557/1000 | Loss: 0.00003233
Iteration 558/1000 | Loss: 0.00002900
Iteration 559/1000 | Loss: 0.00003348
Iteration 560/1000 | Loss: 0.00003711
Iteration 561/1000 | Loss: 0.00004216
Iteration 562/1000 | Loss: 0.00004430
Iteration 563/1000 | Loss: 0.00004857
Iteration 564/1000 | Loss: 0.00004000
Iteration 565/1000 | Loss: 0.00003013
Iteration 566/1000 | Loss: 0.00003356
Iteration 567/1000 | Loss: 0.00003594
Iteration 568/1000 | Loss: 0.00004472
Iteration 569/1000 | Loss: 0.00003828
Iteration 570/1000 | Loss: 0.00004890
Iteration 571/1000 | Loss: 0.00004609
Iteration 572/1000 | Loss: 0.00004380
Iteration 573/1000 | Loss: 0.00003980
Iteration 574/1000 | Loss: 0.00005197
Iteration 575/1000 | Loss: 0.00004552
Iteration 576/1000 | Loss: 0.00003513
Iteration 577/1000 | Loss: 0.00003651
Iteration 578/1000 | Loss: 0.00003951
Iteration 579/1000 | Loss: 0.00004261
Iteration 580/1000 | Loss: 0.00004335
Iteration 581/1000 | Loss: 0.00004078
Iteration 582/1000 | Loss: 0.00004603
Iteration 583/1000 | Loss: 0.00004674
Iteration 584/1000 | Loss: 0.00004355
Iteration 585/1000 | Loss: 0.00005077
Iteration 586/1000 | Loss: 0.00004880
Iteration 587/1000 | Loss: 0.00005284
Iteration 588/1000 | Loss: 0.00004922
Iteration 589/1000 | Loss: 0.00006495
Iteration 590/1000 | Loss: 0.00004536
Iteration 591/1000 | Loss: 0.00004251
Iteration 592/1000 | Loss: 0.00003374
Iteration 593/1000 | Loss: 0.00003724
Iteration 594/1000 | Loss: 0.00003680
Iteration 595/1000 | Loss: 0.00002946
Iteration 596/1000 | Loss: 0.00002366
Iteration 597/1000 | Loss: 0.00003281
Iteration 598/1000 | Loss: 0.00003777
Iteration 599/1000 | Loss: 0.00004029
Iteration 600/1000 | Loss: 0.00004702
Iteration 601/1000 | Loss: 0.00003789
Iteration 602/1000 | Loss: 0.00002885
Iteration 603/1000 | Loss: 0.00002946
Iteration 604/1000 | Loss: 0.00002648
Iteration 605/1000 | Loss: 0.00004067
Iteration 606/1000 | Loss: 0.00004449
Iteration 607/1000 | Loss: 0.00003484
Iteration 608/1000 | Loss: 0.00003133
Iteration 609/1000 | Loss: 0.00002692
Iteration 610/1000 | Loss: 0.00003328
Iteration 611/1000 | Loss: 0.00003924
Iteration 612/1000 | Loss: 0.00004229
Iteration 613/1000 | Loss: 0.00004651
Iteration 614/1000 | Loss: 0.00004811
Iteration 615/1000 | Loss: 0.00004569
Iteration 616/1000 | Loss: 0.00002496
Iteration 617/1000 | Loss: 0.00003238
Iteration 618/1000 | Loss: 0.00003052
Iteration 619/1000 | Loss: 0.00005014
Iteration 620/1000 | Loss: 0.00004980
Iteration 621/1000 | Loss: 0.00005284
Iteration 622/1000 | Loss: 0.00005321
Iteration 623/1000 | Loss: 0.00005948
Iteration 624/1000 | Loss: 0.00005818
Iteration 625/1000 | Loss: 0.00005003
Iteration 626/1000 | Loss: 0.00004297
Iteration 627/1000 | Loss: 0.00006344
Iteration 628/1000 | Loss: 0.00004314
Iteration 629/1000 | Loss: 0.00002771
Iteration 630/1000 | Loss: 0.00002516
Iteration 631/1000 | Loss: 0.00002519
Iteration 632/1000 | Loss: 0.00002729
Iteration 633/1000 | Loss: 0.00003040
Iteration 634/1000 | Loss: 0.00002919
Iteration 635/1000 | Loss: 0.00002467
Iteration 636/1000 | Loss: 0.00003292
Iteration 637/1000 | Loss: 0.00003023
Iteration 638/1000 | Loss: 0.00002858
Iteration 639/1000 | Loss: 0.00002618
Iteration 640/1000 | Loss: 0.00002461
Iteration 641/1000 | Loss: 0.00002681
Iteration 642/1000 | Loss: 0.00003030
Iteration 643/1000 | Loss: 0.00003067
Iteration 644/1000 | Loss: 0.00002730
Iteration 645/1000 | Loss: 0.00002721
Iteration 646/1000 | Loss: 0.00002837
Iteration 647/1000 | Loss: 0.00002928
Iteration 648/1000 | Loss: 0.00003597
Iteration 649/1000 | Loss: 0.00003388
Iteration 650/1000 | Loss: 0.00002708
Iteration 651/1000 | Loss: 0.00002657
Iteration 652/1000 | Loss: 0.00003368
Iteration 653/1000 | Loss: 0.00003477
Iteration 654/1000 | Loss: 0.00003773
Iteration 655/1000 | Loss: 0.00003650
Iteration 656/1000 | Loss: 0.00004181
Iteration 657/1000 | Loss: 0.00002905
Iteration 658/1000 | Loss: 0.00002440
Iteration 659/1000 | Loss: 0.00002560
Iteration 660/1000 | Loss: 0.00002820
Iteration 661/1000 | Loss: 0.00003553
Iteration 662/1000 | Loss: 0.00003642
Iteration 663/1000 | Loss: 0.00003041
Iteration 664/1000 | Loss: 0.00003164
Iteration 665/1000 | Loss: 0.00003276
Iteration 666/1000 | Loss: 0.00003755
Iteration 667/1000 | Loss: 0.00004322
Iteration 668/1000 | Loss: 0.00004690
Iteration 669/1000 | Loss: 0.00004290
Iteration 670/1000 | Loss: 0.00004484
Iteration 671/1000 | Loss: 0.00004701
Iteration 672/1000 | Loss: 0.00005166
Iteration 673/1000 | Loss: 0.00004921
Iteration 674/1000 | Loss: 0.00005447
Iteration 675/1000 | Loss: 0.00005182
Iteration 676/1000 | Loss: 0.00005992
Iteration 677/1000 | Loss: 0.00005117
Iteration 678/1000 | Loss: 0.00005905
Iteration 679/1000 | Loss: 0.00005628
Iteration 680/1000 | Loss: 0.00005550
Iteration 681/1000 | Loss: 0.00005484
Iteration 682/1000 | Loss: 0.00006825
Iteration 683/1000 | Loss: 0.00003394
Iteration 684/1000 | Loss: 0.00002718
Iteration 685/1000 | Loss: 0.00004279
Iteration 686/1000 | Loss: 0.00004742
Iteration 687/1000 | Loss: 0.00002566
Iteration 688/1000 | Loss: 0.00002698
Iteration 689/1000 | Loss: 0.00002752
Iteration 690/1000 | Loss: 0.00003645
Iteration 691/1000 | Loss: 0.00005521
Iteration 692/1000 | Loss: 0.00005020
Iteration 693/1000 | Loss: 0.00006646
Iteration 694/1000 | Loss: 0.00002926
Iteration 695/1000 | Loss: 0.00002375
Iteration 696/1000 | Loss: 0.00002343
Iteration 697/1000 | Loss: 0.00003802
Iteration 698/1000 | Loss: 0.00003388
Iteration 699/1000 | Loss: 0.00003522
Iteration 700/1000 | Loss: 0.00003175
Iteration 701/1000 | Loss: 0.00004985
Iteration 702/1000 | Loss: 0.00004258
Iteration 703/1000 | Loss: 0.00005278
Iteration 704/1000 | Loss: 0.00005617
Iteration 705/1000 | Loss: 0.00005841
Iteration 706/1000 | Loss: 0.00006333
Iteration 707/1000 | Loss: 0.00006208
Iteration 708/1000 | Loss: 0.00006341
Iteration 709/1000 | Loss: 0.00003769
Iteration 710/1000 | Loss: 0.00005142
Iteration 711/1000 | Loss: 0.00004442
Iteration 712/1000 | Loss: 0.00006585
Iteration 713/1000 | Loss: 0.00004066
Iteration 714/1000 | Loss: 0.00002802
Iteration 715/1000 | Loss: 0.00002717
Iteration 716/1000 | Loss: 0.00002689
Iteration 717/1000 | Loss: 0.00003921
Iteration 718/1000 | Loss: 0.00004276
Iteration 719/1000 | Loss: 0.00004644
Iteration 720/1000 | Loss: 0.00005023
Iteration 721/1000 | Loss: 0.00005550
Iteration 722/1000 | Loss: 0.00003959
Iteration 723/1000 | Loss: 0.00004097
Iteration 724/1000 | Loss: 0.00004017
Iteration 725/1000 | Loss: 0.00004808
Iteration 726/1000 | Loss: 0.00004007
Iteration 727/1000 | Loss: 0.00002556
Iteration 728/1000 | Loss: 0.00002767
Iteration 729/1000 | Loss: 0.00003171
Iteration 730/1000 | Loss: 0.00003201
Iteration 731/1000 | Loss: 0.00002636
Iteration 732/1000 | Loss: 0.00002880
Iteration 733/1000 | Loss: 0.00002861
Iteration 734/1000 | Loss: 0.00003429
Iteration 735/1000 | Loss: 0.00003192
Iteration 736/1000 | Loss: 0.00002792
Iteration 737/1000 | Loss: 0.00002512
Iteration 738/1000 | Loss: 0.00002429
Iteration 739/1000 | Loss: 0.00002661
Iteration 740/1000 | Loss: 0.00002357
Iteration 741/1000 | Loss: 0.00002970
Iteration 742/1000 | Loss: 0.00002999
Iteration 743/1000 | Loss: 0.00002739
Iteration 744/1000 | Loss: 0.00002798
Iteration 745/1000 | Loss: 0.00002756
Iteration 746/1000 | Loss: 0.00003076
Iteration 747/1000 | Loss: 0.00002985
Iteration 748/1000 | Loss: 0.00002827
Iteration 749/1000 | Loss: 0.00002718
Iteration 750/1000 | Loss: 0.00002852
Iteration 751/1000 | Loss: 0.00003562
Iteration 752/1000 | Loss: 0.00003214
Iteration 753/1000 | Loss: 0.00003821
Iteration 754/1000 | Loss: 0.00003378
Iteration 755/1000 | Loss: 0.00004568
Iteration 756/1000 | Loss: 0.00003153
Iteration 757/1000 | Loss: 0.00003232
Iteration 758/1000 | Loss: 0.00003054
Iteration 759/1000 | Loss: 0.00004485
Iteration 760/1000 | Loss: 0.00003648
Iteration 761/1000 | Loss: 0.00004960
Iteration 762/1000 | Loss: 0.00003983
Iteration 763/1000 | Loss: 0.00005581
Iteration 764/1000 | Loss: 0.00004322
Iteration 765/1000 | Loss: 0.00006137
Iteration 766/1000 | Loss: 0.00004581
Iteration 767/1000 | Loss: 0.00005268
Iteration 768/1000 | Loss: 0.00004644
Iteration 769/1000 | Loss: 0.00005930
Iteration 770/1000 | Loss: 0.00005272
Iteration 771/1000 | Loss: 0.00006477
Iteration 772/1000 | Loss: 0.00006194
Iteration 773/1000 | Loss: 0.00006890
Iteration 774/1000 | Loss: 0.00006522
Iteration 775/1000 | Loss: 0.00005490
Iteration 776/1000 | Loss: 0.00005788
Iteration 777/1000 | Loss: 0.00005837
Iteration 778/1000 | Loss: 0.00005186
Iteration 779/1000 | Loss: 0.00003138
Iteration 780/1000 | Loss: 0.00002576
Iteration 781/1000 | Loss: 0.00003394
Iteration 782/1000 | Loss: 0.00003986
Iteration 783/1000 | Loss: 0.00004479
Iteration 784/1000 | Loss: 0.00005149
Iteration 785/1000 | Loss: 0.00004361
Iteration 786/1000 | Loss: 0.00004270
Iteration 787/1000 | Loss: 0.00004472
Iteration 788/1000 | Loss: 0.00005170
Iteration 789/1000 | Loss: 0.00005427
Iteration 790/1000 | Loss: 0.00005379
Iteration 791/1000 | Loss: 0.00004862
Iteration 792/1000 | Loss: 0.00003273
Iteration 793/1000 | Loss: 0.00003164
Iteration 794/1000 | Loss: 0.00002488
Iteration 795/1000 | Loss: 0.00002225
Iteration 796/1000 | Loss: 0.00002971
Iteration 797/1000 | Loss: 0.00003245
Iteration 798/1000 | Loss: 0.00002819
Iteration 799/1000 | Loss: 0.00003241
Iteration 800/1000 | Loss: 0.00002968
Iteration 801/1000 | Loss: 0.00003913
Iteration 802/1000 | Loss: 0.00003580
Iteration 803/1000 | Loss: 0.00004369
Iteration 804/1000 | Loss: 0.00003725
Iteration 805/1000 | Loss: 0.00003806
Iteration 806/1000 | Loss: 0.00003763
Iteration 807/1000 | Loss: 0.00002818
Iteration 808/1000 | Loss: 0.00002477
Iteration 809/1000 | Loss: 0.00002478
Iteration 810/1000 | Loss: 0.00003017
Iteration 811/1000 | Loss: 0.00003059
Iteration 812/1000 | Loss: 0.00003160
Iteration 813/1000 | Loss: 0.00003143
Iteration 814/1000 | Loss: 0.00002884
Iteration 815/1000 | Loss: 0.00002951
Iteration 816/1000 | Loss: 0.00002762
Iteration 817/1000 | Loss: 0.00002582
Iteration 818/1000 | Loss: 0.00003114
Iteration 819/1000 | Loss: 0.00002922
Iteration 820/1000 | Loss: 0.00002637
Iteration 821/1000 | Loss: 0.00003787
Iteration 822/1000 | Loss: 0.00002739
Iteration 823/1000 | Loss: 0.00002483
Iteration 824/1000 | Loss: 0.00002299
Iteration 825/1000 | Loss: 0.00002722
Iteration 826/1000 | Loss: 0.00002754
Iteration 827/1000 | Loss: 0.00003106
Iteration 828/1000 | Loss: 0.00003140
Iteration 829/1000 | Loss: 0.00003557
Iteration 830/1000 | Loss: 0.00003511
Iteration 831/1000 | Loss: 0.00004201
Iteration 832/1000 | Loss: 0.00003915
Iteration 833/1000 | Loss: 0.00005035
Iteration 834/1000 | Loss: 0.00004632
Iteration 835/1000 | Loss: 0.00006245
Iteration 836/1000 | Loss: 0.00005163
Iteration 837/1000 | Loss: 0.00004871
Iteration 838/1000 | Loss: 0.00005338
Iteration 839/1000 | Loss: 0.00006920
Iteration 840/1000 | Loss: 0.00005822
Iteration 841/1000 | Loss: 0.00007426
Iteration 842/1000 | Loss: 0.00005807
Iteration 843/1000 | Loss: 0.00008209
Iteration 844/1000 | Loss: 0.00005657
Iteration 845/1000 | Loss: 0.00007303
Iteration 846/1000 | Loss: 0.00003158
Iteration 847/1000 | Loss: 0.00003075
Iteration 848/1000 | Loss: 0.00005021
Iteration 849/1000 | Loss: 0.00004408
Iteration 850/1000 | Loss: 0.00004758
Iteration 851/1000 | Loss: 0.00004883
Iteration 852/1000 | Loss: 0.00005467
Iteration 853/1000 | Loss: 0.00006288
Iteration 854/1000 | Loss: 0.00006410
Iteration 855/1000 | Loss: 0.00005279
Iteration 856/1000 | Loss: 0.00005277
Iteration 857/1000 | Loss: 0.00004250
Iteration 858/1000 | Loss: 0.00003396
Iteration 859/1000 | Loss: 0.00002342
Iteration 860/1000 | Loss: 0.00002447
Iteration 861/1000 | Loss: 0.00002885
Iteration 862/1000 | Loss: 0.00003031
Iteration 863/1000 | Loss: 0.00002777
Iteration 864/1000 | Loss: 0.00002597
Iteration 865/1000 | Loss: 0.00002578
Iteration 866/1000 | Loss: 0.00003024
Iteration 867/1000 | Loss: 0.00003783
Iteration 868/1000 | Loss: 0.00003451
Iteration 869/1000 | Loss: 0.00004218
Iteration 870/1000 | Loss: 0.00003659
Iteration 871/1000 | Loss: 0.00004024
Iteration 872/1000 | Loss: 0.00003933
Iteration 873/1000 | Loss: 0.00004935
Iteration 874/1000 | Loss: 0.00004462
Iteration 875/1000 | Loss: 0.00005263
Iteration 876/1000 | Loss: 0.00004334
Iteration 877/1000 | Loss: 0.00004912
Iteration 878/1000 | Loss: 0.00004273
Iteration 879/1000 | Loss: 0.00002924
Iteration 880/1000 | Loss: 0.00003548
Iteration 881/1000 | Loss: 0.00002593
Iteration 882/1000 | Loss: 0.00002264
Iteration 883/1000 | Loss: 0.00002316
Iteration 884/1000 | Loss: 0.00002340
Iteration 885/1000 | Loss: 0.00002992
Iteration 886/1000 | Loss: 0.00003236
Iteration 887/1000 | Loss: 0.00004025
Iteration 888/1000 | Loss: 0.00003795
Iteration 889/1000 | Loss: 0.00004673
Iteration 890/1000 | Loss: 0.00004329
Iteration 891/1000 | Loss: 0.00005070
Iteration 892/1000 | Loss: 0.00003724
Iteration 893/1000 | Loss: 0.00004275
Iteration 894/1000 | Loss: 0.00004189
Iteration 895/1000 | Loss: 0.00003995
Iteration 896/1000 | Loss: 0.00004696
Iteration 897/1000 | Loss: 0.00005424
Iteration 898/1000 | Loss: 0.00005631
Iteration 899/1000 | Loss: 0.00005214
Iteration 900/1000 | Loss: 0.00002960
Iteration 901/1000 | Loss: 0.00002834
Iteration 902/1000 | Loss: 0.00002356
Iteration 903/1000 | Loss: 0.00003136
Iteration 904/1000 | Loss: 0.00005277
Iteration 905/1000 | Loss: 0.00004713
Iteration 906/1000 | Loss: 0.00005066
Iteration 907/1000 | Loss: 0.00005018
Iteration 908/1000 | Loss: 0.00004924
Iteration 909/1000 | Loss: 0.00003283
Iteration 910/1000 | Loss: 0.00002558
Iteration 911/1000 | Loss: 0.00003840
Iteration 912/1000 | Loss: 0.00005542
Iteration 913/1000 | Loss: 0.00004603
Iteration 914/1000 | Loss: 0.00005855
Iteration 915/1000 | Loss: 0.00005323
Iteration 916/1000 | Loss: 0.00005104
Iteration 917/1000 | Loss: 0.00003978
Iteration 918/1000 | Loss: 0.00005358
Iteration 919/1000 | Loss: 0.00005852
Iteration 920/1000 | Loss: 0.00004055
Iteration 921/1000 | Loss: 0.00002770
Iteration 922/1000 | Loss: 0.00004033
Iteration 923/1000 | Loss: 0.00003134
Iteration 924/1000 | Loss: 0.00004855
Iteration 925/1000 | Loss: 0.00003976
Iteration 926/1000 | Loss: 0.00005883
Iteration 927/1000 | Loss: 0.00004322
Iteration 928/1000 | Loss: 0.00004928
Iteration 929/1000 | Loss: 0.00004180
Iteration 930/1000 | Loss: 0.00006245
Iteration 931/1000 | Loss: 0.00004522
Iteration 932/1000 | Loss: 0.00006549
Iteration 933/1000 | Loss: 0.00004496
Iteration 934/1000 | Loss: 0.00006521
Iteration 935/1000 | Loss: 0.00005266
Iteration 936/1000 | Loss: 0.00006605
Iteration 937/1000 | Loss: 0.00003719
Iteration 938/1000 | Loss: 0.00002817
Iteration 939/1000 | Loss: 0.00003203
Iteration 940/1000 | Loss: 0.00002491
Iteration 941/1000 | Loss: 0.00003405
Iteration 942/1000 | Loss: 0.00002697
Iteration 943/1000 | Loss: 0.00002947
Iteration 944/1000 | Loss: 0.00002746
Iteration 945/1000 | Loss: 0.00002591
Iteration 946/1000 | Loss: 0.00002778
Iteration 947/1000 | Loss: 0.00003518
Iteration 948/1000 | Loss: 0.00003028
Iteration 949/1000 | Loss: 0.00002838
Iteration 950/1000 | Loss: 0.00002529
Iteration 951/1000 | Loss: 0.00002421
Iteration 952/1000 | Loss: 0.00002431
Iteration 953/1000 | Loss: 0.00002394
Iteration 954/1000 | Loss: 0.00002367
Iteration 955/1000 | Loss: 0.00003327
Iteration 956/1000 | Loss: 0.00002652
Iteration 957/1000 | Loss: 0.00002473
Iteration 958/1000 | Loss: 0.00002400
Iteration 959/1000 | Loss: 0.00002636
Iteration 960/1000 | Loss: 0.00003047
Iteration 961/1000 | Loss: 0.00002904
Iteration 962/1000 | Loss: 0.00002398
Iteration 963/1000 | Loss: 0.00002446
Iteration 964/1000 | Loss: 0.00003918
Iteration 965/1000 | Loss: 0.00003457
Iteration 966/1000 | Loss: 0.00003348
Iteration 967/1000 | Loss: 0.00003181
Iteration 968/1000 | Loss: 0.00003614
Iteration 969/1000 | Loss: 0.00003450
Iteration 970/1000 | Loss: 0.00003527
Iteration 971/1000 | Loss: 0.00003558
Iteration 972/1000 | Loss: 0.00004769
Iteration 973/1000 | Loss: 0.00004381
Iteration 974/1000 | Loss: 0.00003879
Iteration 975/1000 | Loss: 0.00003271
Iteration 976/1000 | Loss: 0.00002939
Iteration 977/1000 | Loss: 0.00002872
Iteration 978/1000 | Loss: 0.00004443
Iteration 979/1000 | Loss: 0.00004139
Iteration 980/1000 | Loss: 0.00005177
Iteration 981/1000 | Loss: 0.00004423
Iteration 982/1000 | Loss: 0.00005084
Iteration 983/1000 | Loss: 0.00004450
Iteration 984/1000 | Loss: 0.00005998
Iteration 985/1000 | Loss: 0.00005577
Iteration 986/1000 | Loss: 0.00006727
Iteration 987/1000 | Loss: 0.00003992
Iteration 988/1000 | Loss: 0.00003043
Iteration 989/1000 | Loss: 0.00003868
Iteration 990/1000 | Loss: 0.00004598
Iteration 991/1000 | Loss: 0.00005580
Iteration 992/1000 | Loss: 0.00004440
Iteration 993/1000 | Loss: 0.00004038
Iteration 994/1000 | Loss: 0.00004393
Iteration 995/1000 | Loss: 0.00004266
Iteration 996/1000 | Loss: 0.00004813
Iteration 997/1000 | Loss: 0.00005139
Iteration 998/1000 | Loss: 0.00005584
Iteration 999/1000 | Loss: 0.00005864
Iteration 1000/1000 | Loss: 0.00004311

Optimization complete. Final v2v error: 4.000763893127441 mm

Highest mean error: 9.528265953063965 mm for frame 111

Lowest mean error: 2.6492183208465576 mm for frame 30

Saving results

Total time: 1371.0681772232056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365056
Iteration 2/25 | Loss: 0.00127565
Iteration 3/25 | Loss: 0.00120703
Iteration 4/25 | Loss: 0.00119878
Iteration 5/25 | Loss: 0.00119624
Iteration 6/25 | Loss: 0.00119550
Iteration 7/25 | Loss: 0.00119550
Iteration 8/25 | Loss: 0.00119550
Iteration 9/25 | Loss: 0.00119550
Iteration 10/25 | Loss: 0.00119550
Iteration 11/25 | Loss: 0.00119550
Iteration 12/25 | Loss: 0.00119550
Iteration 13/25 | Loss: 0.00119550
Iteration 14/25 | Loss: 0.00119550
Iteration 15/25 | Loss: 0.00119550
Iteration 16/25 | Loss: 0.00119550
Iteration 17/25 | Loss: 0.00119550
Iteration 18/25 | Loss: 0.00119550
Iteration 19/25 | Loss: 0.00119550
Iteration 20/25 | Loss: 0.00119550
Iteration 21/25 | Loss: 0.00119550
Iteration 22/25 | Loss: 0.00119550
Iteration 23/25 | Loss: 0.00119550
Iteration 24/25 | Loss: 0.00119550
Iteration 25/25 | Loss: 0.00119550

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70218468
Iteration 2/25 | Loss: 0.00110990
Iteration 3/25 | Loss: 0.00110990
Iteration 4/25 | Loss: 0.00110990
Iteration 5/25 | Loss: 0.00110990
Iteration 6/25 | Loss: 0.00110990
Iteration 7/25 | Loss: 0.00110989
Iteration 8/25 | Loss: 0.00110989
Iteration 9/25 | Loss: 0.00110989
Iteration 10/25 | Loss: 0.00110989
Iteration 11/25 | Loss: 0.00110989
Iteration 12/25 | Loss: 0.00110989
Iteration 13/25 | Loss: 0.00110989
Iteration 14/25 | Loss: 0.00110989
Iteration 15/25 | Loss: 0.00110989
Iteration 16/25 | Loss: 0.00110989
Iteration 17/25 | Loss: 0.00110989
Iteration 18/25 | Loss: 0.00110989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011098937829956412, 0.0011098937829956412, 0.0011098937829956412, 0.0011098937829956412, 0.0011098937829956412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011098937829956412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110989
Iteration 2/1000 | Loss: 0.00002735
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001241
Iteration 6/1000 | Loss: 0.00001147
Iteration 7/1000 | Loss: 0.00001090
Iteration 8/1000 | Loss: 0.00001040
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001011
Iteration 11/1000 | Loss: 0.00000993
Iteration 12/1000 | Loss: 0.00000978
Iteration 13/1000 | Loss: 0.00000976
Iteration 14/1000 | Loss: 0.00000973
Iteration 15/1000 | Loss: 0.00000969
Iteration 16/1000 | Loss: 0.00000963
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000960
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000957
Iteration 24/1000 | Loss: 0.00000956
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000953
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000953
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000952
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000951
Iteration 36/1000 | Loss: 0.00000950
Iteration 37/1000 | Loss: 0.00000949
Iteration 38/1000 | Loss: 0.00000949
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000949
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000948
Iteration 43/1000 | Loss: 0.00000948
Iteration 44/1000 | Loss: 0.00000948
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000946
Iteration 49/1000 | Loss: 0.00000945
Iteration 50/1000 | Loss: 0.00000945
Iteration 51/1000 | Loss: 0.00000945
Iteration 52/1000 | Loss: 0.00000944
Iteration 53/1000 | Loss: 0.00000944
Iteration 54/1000 | Loss: 0.00000944
Iteration 55/1000 | Loss: 0.00000943
Iteration 56/1000 | Loss: 0.00000943
Iteration 57/1000 | Loss: 0.00000943
Iteration 58/1000 | Loss: 0.00000942
Iteration 59/1000 | Loss: 0.00000942
Iteration 60/1000 | Loss: 0.00000942
Iteration 61/1000 | Loss: 0.00000942
Iteration 62/1000 | Loss: 0.00000942
Iteration 63/1000 | Loss: 0.00000941
Iteration 64/1000 | Loss: 0.00000941
Iteration 65/1000 | Loss: 0.00000941
Iteration 66/1000 | Loss: 0.00000941
Iteration 67/1000 | Loss: 0.00000941
Iteration 68/1000 | Loss: 0.00000941
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000940
Iteration 71/1000 | Loss: 0.00000940
Iteration 72/1000 | Loss: 0.00000940
Iteration 73/1000 | Loss: 0.00000939
Iteration 74/1000 | Loss: 0.00000939
Iteration 75/1000 | Loss: 0.00000939
Iteration 76/1000 | Loss: 0.00000938
Iteration 77/1000 | Loss: 0.00000938
Iteration 78/1000 | Loss: 0.00000938
Iteration 79/1000 | Loss: 0.00000938
Iteration 80/1000 | Loss: 0.00000937
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000936
Iteration 84/1000 | Loss: 0.00000935
Iteration 85/1000 | Loss: 0.00000935
Iteration 86/1000 | Loss: 0.00000935
Iteration 87/1000 | Loss: 0.00000934
Iteration 88/1000 | Loss: 0.00000934
Iteration 89/1000 | Loss: 0.00000934
Iteration 90/1000 | Loss: 0.00000933
Iteration 91/1000 | Loss: 0.00000933
Iteration 92/1000 | Loss: 0.00000933
Iteration 93/1000 | Loss: 0.00000932
Iteration 94/1000 | Loss: 0.00000932
Iteration 95/1000 | Loss: 0.00000932
Iteration 96/1000 | Loss: 0.00000932
Iteration 97/1000 | Loss: 0.00000932
Iteration 98/1000 | Loss: 0.00000931
Iteration 99/1000 | Loss: 0.00000931
Iteration 100/1000 | Loss: 0.00000931
Iteration 101/1000 | Loss: 0.00000931
Iteration 102/1000 | Loss: 0.00000931
Iteration 103/1000 | Loss: 0.00000931
Iteration 104/1000 | Loss: 0.00000931
Iteration 105/1000 | Loss: 0.00000931
Iteration 106/1000 | Loss: 0.00000931
Iteration 107/1000 | Loss: 0.00000930
Iteration 108/1000 | Loss: 0.00000930
Iteration 109/1000 | Loss: 0.00000930
Iteration 110/1000 | Loss: 0.00000930
Iteration 111/1000 | Loss: 0.00000930
Iteration 112/1000 | Loss: 0.00000930
Iteration 113/1000 | Loss: 0.00000929
Iteration 114/1000 | Loss: 0.00000928
Iteration 115/1000 | Loss: 0.00000928
Iteration 116/1000 | Loss: 0.00000928
Iteration 117/1000 | Loss: 0.00000928
Iteration 118/1000 | Loss: 0.00000928
Iteration 119/1000 | Loss: 0.00000928
Iteration 120/1000 | Loss: 0.00000928
Iteration 121/1000 | Loss: 0.00000928
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000927
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000926
Iteration 129/1000 | Loss: 0.00000926
Iteration 130/1000 | Loss: 0.00000926
Iteration 131/1000 | Loss: 0.00000925
Iteration 132/1000 | Loss: 0.00000924
Iteration 133/1000 | Loss: 0.00000924
Iteration 134/1000 | Loss: 0.00000924
Iteration 135/1000 | Loss: 0.00000924
Iteration 136/1000 | Loss: 0.00000923
Iteration 137/1000 | Loss: 0.00000923
Iteration 138/1000 | Loss: 0.00000922
Iteration 139/1000 | Loss: 0.00000922
Iteration 140/1000 | Loss: 0.00000922
Iteration 141/1000 | Loss: 0.00000922
Iteration 142/1000 | Loss: 0.00000922
Iteration 143/1000 | Loss: 0.00000922
Iteration 144/1000 | Loss: 0.00000922
Iteration 145/1000 | Loss: 0.00000922
Iteration 146/1000 | Loss: 0.00000922
Iteration 147/1000 | Loss: 0.00000922
Iteration 148/1000 | Loss: 0.00000921
Iteration 149/1000 | Loss: 0.00000921
Iteration 150/1000 | Loss: 0.00000921
Iteration 151/1000 | Loss: 0.00000921
Iteration 152/1000 | Loss: 0.00000921
Iteration 153/1000 | Loss: 0.00000921
Iteration 154/1000 | Loss: 0.00000921
Iteration 155/1000 | Loss: 0.00000921
Iteration 156/1000 | Loss: 0.00000921
Iteration 157/1000 | Loss: 0.00000921
Iteration 158/1000 | Loss: 0.00000921
Iteration 159/1000 | Loss: 0.00000921
Iteration 160/1000 | Loss: 0.00000920
Iteration 161/1000 | Loss: 0.00000920
Iteration 162/1000 | Loss: 0.00000920
Iteration 163/1000 | Loss: 0.00000920
Iteration 164/1000 | Loss: 0.00000920
Iteration 165/1000 | Loss: 0.00000920
Iteration 166/1000 | Loss: 0.00000920
Iteration 167/1000 | Loss: 0.00000920
Iteration 168/1000 | Loss: 0.00000920
Iteration 169/1000 | Loss: 0.00000920
Iteration 170/1000 | Loss: 0.00000920
Iteration 171/1000 | Loss: 0.00000920
Iteration 172/1000 | Loss: 0.00000919
Iteration 173/1000 | Loss: 0.00000919
Iteration 174/1000 | Loss: 0.00000919
Iteration 175/1000 | Loss: 0.00000919
Iteration 176/1000 | Loss: 0.00000919
Iteration 177/1000 | Loss: 0.00000919
Iteration 178/1000 | Loss: 0.00000919
Iteration 179/1000 | Loss: 0.00000919
Iteration 180/1000 | Loss: 0.00000919
Iteration 181/1000 | Loss: 0.00000919
Iteration 182/1000 | Loss: 0.00000919
Iteration 183/1000 | Loss: 0.00000918
Iteration 184/1000 | Loss: 0.00000918
Iteration 185/1000 | Loss: 0.00000918
Iteration 186/1000 | Loss: 0.00000918
Iteration 187/1000 | Loss: 0.00000918
Iteration 188/1000 | Loss: 0.00000918
Iteration 189/1000 | Loss: 0.00000918
Iteration 190/1000 | Loss: 0.00000918
Iteration 191/1000 | Loss: 0.00000918
Iteration 192/1000 | Loss: 0.00000918
Iteration 193/1000 | Loss: 0.00000917
Iteration 194/1000 | Loss: 0.00000917
Iteration 195/1000 | Loss: 0.00000917
Iteration 196/1000 | Loss: 0.00000917
Iteration 197/1000 | Loss: 0.00000917
Iteration 198/1000 | Loss: 0.00000917
Iteration 199/1000 | Loss: 0.00000917
Iteration 200/1000 | Loss: 0.00000917
Iteration 201/1000 | Loss: 0.00000917
Iteration 202/1000 | Loss: 0.00000917
Iteration 203/1000 | Loss: 0.00000917
Iteration 204/1000 | Loss: 0.00000917
Iteration 205/1000 | Loss: 0.00000917
Iteration 206/1000 | Loss: 0.00000917
Iteration 207/1000 | Loss: 0.00000917
Iteration 208/1000 | Loss: 0.00000917
Iteration 209/1000 | Loss: 0.00000917
Iteration 210/1000 | Loss: 0.00000917
Iteration 211/1000 | Loss: 0.00000917
Iteration 212/1000 | Loss: 0.00000916
Iteration 213/1000 | Loss: 0.00000916
Iteration 214/1000 | Loss: 0.00000916
Iteration 215/1000 | Loss: 0.00000916
Iteration 216/1000 | Loss: 0.00000916
Iteration 217/1000 | Loss: 0.00000916
Iteration 218/1000 | Loss: 0.00000916
Iteration 219/1000 | Loss: 0.00000916
Iteration 220/1000 | Loss: 0.00000916
Iteration 221/1000 | Loss: 0.00000916
Iteration 222/1000 | Loss: 0.00000916
Iteration 223/1000 | Loss: 0.00000916
Iteration 224/1000 | Loss: 0.00000916
Iteration 225/1000 | Loss: 0.00000916
Iteration 226/1000 | Loss: 0.00000916
Iteration 227/1000 | Loss: 0.00000916
Iteration 228/1000 | Loss: 0.00000916
Iteration 229/1000 | Loss: 0.00000916
Iteration 230/1000 | Loss: 0.00000916
Iteration 231/1000 | Loss: 0.00000916
Iteration 232/1000 | Loss: 0.00000916
Iteration 233/1000 | Loss: 0.00000916
Iteration 234/1000 | Loss: 0.00000916
Iteration 235/1000 | Loss: 0.00000916
Iteration 236/1000 | Loss: 0.00000916
Iteration 237/1000 | Loss: 0.00000916
Iteration 238/1000 | Loss: 0.00000916
Iteration 239/1000 | Loss: 0.00000916
Iteration 240/1000 | Loss: 0.00000916
Iteration 241/1000 | Loss: 0.00000916
Iteration 242/1000 | Loss: 0.00000916
Iteration 243/1000 | Loss: 0.00000916
Iteration 244/1000 | Loss: 0.00000916
Iteration 245/1000 | Loss: 0.00000916
Iteration 246/1000 | Loss: 0.00000916
Iteration 247/1000 | Loss: 0.00000916
Iteration 248/1000 | Loss: 0.00000916
Iteration 249/1000 | Loss: 0.00000916
Iteration 250/1000 | Loss: 0.00000916
Iteration 251/1000 | Loss: 0.00000916
Iteration 252/1000 | Loss: 0.00000916
Iteration 253/1000 | Loss: 0.00000916
Iteration 254/1000 | Loss: 0.00000916
Iteration 255/1000 | Loss: 0.00000916
Iteration 256/1000 | Loss: 0.00000916
Iteration 257/1000 | Loss: 0.00000916
Iteration 258/1000 | Loss: 0.00000916
Iteration 259/1000 | Loss: 0.00000916
Iteration 260/1000 | Loss: 0.00000916
Iteration 261/1000 | Loss: 0.00000916
Iteration 262/1000 | Loss: 0.00000916
Iteration 263/1000 | Loss: 0.00000916
Iteration 264/1000 | Loss: 0.00000916
Iteration 265/1000 | Loss: 0.00000916
Iteration 266/1000 | Loss: 0.00000916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [9.158955435850658e-06, 9.158955435850658e-06, 9.158955435850658e-06, 9.158955435850658e-06, 9.158955435850658e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.158955435850658e-06

Optimization complete. Final v2v error: 2.6018738746643066 mm

Highest mean error: 3.1019997596740723 mm for frame 75

Lowest mean error: 2.456568956375122 mm for frame 123

Saving results

Total time: 43.44270062446594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897516
Iteration 2/25 | Loss: 0.00263806
Iteration 3/25 | Loss: 0.00226195
Iteration 4/25 | Loss: 0.00168932
Iteration 5/25 | Loss: 0.00156212
Iteration 6/25 | Loss: 0.00161065
Iteration 7/25 | Loss: 0.00144212
Iteration 8/25 | Loss: 0.00137812
Iteration 9/25 | Loss: 0.00134717
Iteration 10/25 | Loss: 0.00133691
Iteration 11/25 | Loss: 0.00134346
Iteration 12/25 | Loss: 0.00133595
Iteration 13/25 | Loss: 0.00133012
Iteration 14/25 | Loss: 0.00131127
Iteration 15/25 | Loss: 0.00130280
Iteration 16/25 | Loss: 0.00130055
Iteration 17/25 | Loss: 0.00131925
Iteration 18/25 | Loss: 0.00129393
Iteration 19/25 | Loss: 0.00128879
Iteration 20/25 | Loss: 0.00129509
Iteration 21/25 | Loss: 0.00129036
Iteration 22/25 | Loss: 0.00128602
Iteration 23/25 | Loss: 0.00128498
Iteration 24/25 | Loss: 0.00128480
Iteration 25/25 | Loss: 0.00128477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31401896
Iteration 2/25 | Loss: 0.00091298
Iteration 3/25 | Loss: 0.00091296
Iteration 4/25 | Loss: 0.00091295
Iteration 5/25 | Loss: 0.00091295
Iteration 6/25 | Loss: 0.00091295
Iteration 7/25 | Loss: 0.00091295
Iteration 8/25 | Loss: 0.00091295
Iteration 9/25 | Loss: 0.00091295
Iteration 10/25 | Loss: 0.00091295
Iteration 11/25 | Loss: 0.00091295
Iteration 12/25 | Loss: 0.00091295
Iteration 13/25 | Loss: 0.00091295
Iteration 14/25 | Loss: 0.00091295
Iteration 15/25 | Loss: 0.00091295
Iteration 16/25 | Loss: 0.00091295
Iteration 17/25 | Loss: 0.00091295
Iteration 18/25 | Loss: 0.00091295
Iteration 19/25 | Loss: 0.00091295
Iteration 20/25 | Loss: 0.00091295
Iteration 21/25 | Loss: 0.00091295
Iteration 22/25 | Loss: 0.00091295
Iteration 23/25 | Loss: 0.00091295
Iteration 24/25 | Loss: 0.00091295
Iteration 25/25 | Loss: 0.00091295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091295
Iteration 2/1000 | Loss: 0.00004265
Iteration 3/1000 | Loss: 0.00003052
Iteration 4/1000 | Loss: 0.00002586
Iteration 5/1000 | Loss: 0.00002421
Iteration 6/1000 | Loss: 0.00002301
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002098
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002050
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002014
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001993
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001983
Iteration 20/1000 | Loss: 0.00001983
Iteration 21/1000 | Loss: 0.00001982
Iteration 22/1000 | Loss: 0.00001982
Iteration 23/1000 | Loss: 0.00001981
Iteration 24/1000 | Loss: 0.00001981
Iteration 25/1000 | Loss: 0.00001978
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001976
Iteration 32/1000 | Loss: 0.00001974
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001973
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001967
Iteration 51/1000 | Loss: 0.00001967
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001966
Iteration 54/1000 | Loss: 0.00001966
Iteration 55/1000 | Loss: 0.00001966
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001964
Iteration 68/1000 | Loss: 0.00001964
Iteration 69/1000 | Loss: 0.00001963
Iteration 70/1000 | Loss: 0.00001963
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001962
Iteration 73/1000 | Loss: 0.00001962
Iteration 74/1000 | Loss: 0.00001962
Iteration 75/1000 | Loss: 0.00001962
Iteration 76/1000 | Loss: 0.00001962
Iteration 77/1000 | Loss: 0.00001961
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001956
Iteration 88/1000 | Loss: 0.00001956
Iteration 89/1000 | Loss: 0.00001956
Iteration 90/1000 | Loss: 0.00001955
Iteration 91/1000 | Loss: 0.00001955
Iteration 92/1000 | Loss: 0.00001955
Iteration 93/1000 | Loss: 0.00001955
Iteration 94/1000 | Loss: 0.00001955
Iteration 95/1000 | Loss: 0.00001955
Iteration 96/1000 | Loss: 0.00001955
Iteration 97/1000 | Loss: 0.00001955
Iteration 98/1000 | Loss: 0.00001954
Iteration 99/1000 | Loss: 0.00001954
Iteration 100/1000 | Loss: 0.00001954
Iteration 101/1000 | Loss: 0.00001954
Iteration 102/1000 | Loss: 0.00001954
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001953
Iteration 108/1000 | Loss: 0.00001953
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001953
Iteration 117/1000 | Loss: 0.00001953
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001951
Iteration 134/1000 | Loss: 0.00001950
Iteration 135/1000 | Loss: 0.00001950
Iteration 136/1000 | Loss: 0.00001950
Iteration 137/1000 | Loss: 0.00001950
Iteration 138/1000 | Loss: 0.00001950
Iteration 139/1000 | Loss: 0.00001950
Iteration 140/1000 | Loss: 0.00001950
Iteration 141/1000 | Loss: 0.00001950
Iteration 142/1000 | Loss: 0.00001949
Iteration 143/1000 | Loss: 0.00001949
Iteration 144/1000 | Loss: 0.00001949
Iteration 145/1000 | Loss: 0.00001949
Iteration 146/1000 | Loss: 0.00001949
Iteration 147/1000 | Loss: 0.00001949
Iteration 148/1000 | Loss: 0.00001948
Iteration 149/1000 | Loss: 0.00001948
Iteration 150/1000 | Loss: 0.00001948
Iteration 151/1000 | Loss: 0.00001948
Iteration 152/1000 | Loss: 0.00001948
Iteration 153/1000 | Loss: 0.00001948
Iteration 154/1000 | Loss: 0.00001947
Iteration 155/1000 | Loss: 0.00001947
Iteration 156/1000 | Loss: 0.00001947
Iteration 157/1000 | Loss: 0.00001947
Iteration 158/1000 | Loss: 0.00001947
Iteration 159/1000 | Loss: 0.00001947
Iteration 160/1000 | Loss: 0.00001947
Iteration 161/1000 | Loss: 0.00001946
Iteration 162/1000 | Loss: 0.00001946
Iteration 163/1000 | Loss: 0.00001946
Iteration 164/1000 | Loss: 0.00001946
Iteration 165/1000 | Loss: 0.00001946
Iteration 166/1000 | Loss: 0.00001946
Iteration 167/1000 | Loss: 0.00001946
Iteration 168/1000 | Loss: 0.00001946
Iteration 169/1000 | Loss: 0.00001945
Iteration 170/1000 | Loss: 0.00001945
Iteration 171/1000 | Loss: 0.00001945
Iteration 172/1000 | Loss: 0.00001945
Iteration 173/1000 | Loss: 0.00001945
Iteration 174/1000 | Loss: 0.00001945
Iteration 175/1000 | Loss: 0.00001945
Iteration 176/1000 | Loss: 0.00001944
Iteration 177/1000 | Loss: 0.00001944
Iteration 178/1000 | Loss: 0.00001944
Iteration 179/1000 | Loss: 0.00001944
Iteration 180/1000 | Loss: 0.00001943
Iteration 181/1000 | Loss: 0.00001943
Iteration 182/1000 | Loss: 0.00001943
Iteration 183/1000 | Loss: 0.00001943
Iteration 184/1000 | Loss: 0.00001942
Iteration 185/1000 | Loss: 0.00001942
Iteration 186/1000 | Loss: 0.00001942
Iteration 187/1000 | Loss: 0.00001942
Iteration 188/1000 | Loss: 0.00001942
Iteration 189/1000 | Loss: 0.00001942
Iteration 190/1000 | Loss: 0.00001942
Iteration 191/1000 | Loss: 0.00001942
Iteration 192/1000 | Loss: 0.00001942
Iteration 193/1000 | Loss: 0.00001942
Iteration 194/1000 | Loss: 0.00001942
Iteration 195/1000 | Loss: 0.00001942
Iteration 196/1000 | Loss: 0.00001942
Iteration 197/1000 | Loss: 0.00001942
Iteration 198/1000 | Loss: 0.00001942
Iteration 199/1000 | Loss: 0.00001942
Iteration 200/1000 | Loss: 0.00001942
Iteration 201/1000 | Loss: 0.00001942
Iteration 202/1000 | Loss: 0.00001942
Iteration 203/1000 | Loss: 0.00001942
Iteration 204/1000 | Loss: 0.00001942
Iteration 205/1000 | Loss: 0.00001942
Iteration 206/1000 | Loss: 0.00001942
Iteration 207/1000 | Loss: 0.00001942
Iteration 208/1000 | Loss: 0.00001942
Iteration 209/1000 | Loss: 0.00001942
Iteration 210/1000 | Loss: 0.00001942
Iteration 211/1000 | Loss: 0.00001942
Iteration 212/1000 | Loss: 0.00001942
Iteration 213/1000 | Loss: 0.00001942
Iteration 214/1000 | Loss: 0.00001942
Iteration 215/1000 | Loss: 0.00001942
Iteration 216/1000 | Loss: 0.00001942
Iteration 217/1000 | Loss: 0.00001942
Iteration 218/1000 | Loss: 0.00001942
Iteration 219/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.941727168741636e-05, 1.941727168741636e-05, 1.941727168741636e-05, 1.941727168741636e-05, 1.941727168741636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.941727168741636e-05

Optimization complete. Final v2v error: 3.718386173248291 mm

Highest mean error: 4.755619525909424 mm for frame 68

Lowest mean error: 3.1394498348236084 mm for frame 46

Saving results

Total time: 76.31946611404419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575937
Iteration 2/25 | Loss: 0.00169607
Iteration 3/25 | Loss: 0.00142734
Iteration 4/25 | Loss: 0.00139951
Iteration 5/25 | Loss: 0.00140375
Iteration 6/25 | Loss: 0.00139825
Iteration 7/25 | Loss: 0.00139189
Iteration 8/25 | Loss: 0.00138414
Iteration 9/25 | Loss: 0.00138164
Iteration 10/25 | Loss: 0.00138132
Iteration 11/25 | Loss: 0.00138118
Iteration 12/25 | Loss: 0.00138116
Iteration 13/25 | Loss: 0.00138116
Iteration 14/25 | Loss: 0.00138115
Iteration 15/25 | Loss: 0.00138115
Iteration 16/25 | Loss: 0.00138115
Iteration 17/25 | Loss: 0.00138115
Iteration 18/25 | Loss: 0.00138114
Iteration 19/25 | Loss: 0.00138113
Iteration 20/25 | Loss: 0.00138112
Iteration 21/25 | Loss: 0.00138110
Iteration 22/25 | Loss: 0.00138110
Iteration 23/25 | Loss: 0.00138110
Iteration 24/25 | Loss: 0.00138110
Iteration 25/25 | Loss: 0.00138109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24061263
Iteration 2/25 | Loss: 0.00142825
Iteration 3/25 | Loss: 0.00142821
Iteration 4/25 | Loss: 0.00142821
Iteration 5/25 | Loss: 0.00142821
Iteration 6/25 | Loss: 0.00142821
Iteration 7/25 | Loss: 0.00142821
Iteration 8/25 | Loss: 0.00142821
Iteration 9/25 | Loss: 0.00142821
Iteration 10/25 | Loss: 0.00142821
Iteration 11/25 | Loss: 0.00142821
Iteration 12/25 | Loss: 0.00142821
Iteration 13/25 | Loss: 0.00142821
Iteration 14/25 | Loss: 0.00142821
Iteration 15/25 | Loss: 0.00142821
Iteration 16/25 | Loss: 0.00142821
Iteration 17/25 | Loss: 0.00142821
Iteration 18/25 | Loss: 0.00142821
Iteration 19/25 | Loss: 0.00142821
Iteration 20/25 | Loss: 0.00142821
Iteration 21/25 | Loss: 0.00142821
Iteration 22/25 | Loss: 0.00142821
Iteration 23/25 | Loss: 0.00142821
Iteration 24/25 | Loss: 0.00142821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014282099436968565, 0.0014282099436968565, 0.0014282099436968565, 0.0014282099436968565, 0.0014282099436968565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014282099436968565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142821
Iteration 2/1000 | Loss: 0.00013168
Iteration 3/1000 | Loss: 0.00008926
Iteration 4/1000 | Loss: 0.00007824
Iteration 5/1000 | Loss: 0.00007139
Iteration 6/1000 | Loss: 0.00006694
Iteration 7/1000 | Loss: 0.00038544
Iteration 8/1000 | Loss: 0.00022606
Iteration 9/1000 | Loss: 0.00036571
Iteration 10/1000 | Loss: 0.00036156
Iteration 11/1000 | Loss: 0.00006321
Iteration 12/1000 | Loss: 0.00034063
Iteration 13/1000 | Loss: 0.00038110
Iteration 14/1000 | Loss: 0.00006133
Iteration 15/1000 | Loss: 0.00005743
Iteration 16/1000 | Loss: 0.00005597
Iteration 17/1000 | Loss: 0.00005495
Iteration 18/1000 | Loss: 0.00005403
Iteration 19/1000 | Loss: 0.00064224
Iteration 20/1000 | Loss: 0.00037576
Iteration 21/1000 | Loss: 0.00005890
Iteration 22/1000 | Loss: 0.00005423
Iteration 23/1000 | Loss: 0.00005267
Iteration 24/1000 | Loss: 0.00005165
Iteration 25/1000 | Loss: 0.00039948
Iteration 26/1000 | Loss: 0.00094441
Iteration 27/1000 | Loss: 0.00034776
Iteration 28/1000 | Loss: 0.00046611
Iteration 29/1000 | Loss: 0.00013106
Iteration 30/1000 | Loss: 0.00017076
Iteration 31/1000 | Loss: 0.00029430
Iteration 32/1000 | Loss: 0.00018850
Iteration 33/1000 | Loss: 0.00024365
Iteration 34/1000 | Loss: 0.00005174
Iteration 35/1000 | Loss: 0.00004887
Iteration 36/1000 | Loss: 0.00041128
Iteration 37/1000 | Loss: 0.00005665
Iteration 38/1000 | Loss: 0.00004595
Iteration 39/1000 | Loss: 0.00004414
Iteration 40/1000 | Loss: 0.00004327
Iteration 41/1000 | Loss: 0.00004233
Iteration 42/1000 | Loss: 0.00004168
Iteration 43/1000 | Loss: 0.00004117
Iteration 44/1000 | Loss: 0.00004088
Iteration 45/1000 | Loss: 0.00004060
Iteration 46/1000 | Loss: 0.00004042
Iteration 47/1000 | Loss: 0.00004041
Iteration 48/1000 | Loss: 0.00004030
Iteration 49/1000 | Loss: 0.00004022
Iteration 50/1000 | Loss: 0.00004022
Iteration 51/1000 | Loss: 0.00004021
Iteration 52/1000 | Loss: 0.00004016
Iteration 53/1000 | Loss: 0.00004006
Iteration 54/1000 | Loss: 0.00004003
Iteration 55/1000 | Loss: 0.00004002
Iteration 56/1000 | Loss: 0.00004002
Iteration 57/1000 | Loss: 0.00004002
Iteration 58/1000 | Loss: 0.00004002
Iteration 59/1000 | Loss: 0.00004002
Iteration 60/1000 | Loss: 0.00004002
Iteration 61/1000 | Loss: 0.00004002
Iteration 62/1000 | Loss: 0.00004002
Iteration 63/1000 | Loss: 0.00004001
Iteration 64/1000 | Loss: 0.00004001
Iteration 65/1000 | Loss: 0.00004001
Iteration 66/1000 | Loss: 0.00004000
Iteration 67/1000 | Loss: 0.00003999
Iteration 68/1000 | Loss: 0.00003998
Iteration 69/1000 | Loss: 0.00003998
Iteration 70/1000 | Loss: 0.00003997
Iteration 71/1000 | Loss: 0.00003997
Iteration 72/1000 | Loss: 0.00003996
Iteration 73/1000 | Loss: 0.00003995
Iteration 74/1000 | Loss: 0.00003995
Iteration 75/1000 | Loss: 0.00003995
Iteration 76/1000 | Loss: 0.00003994
Iteration 77/1000 | Loss: 0.00003994
Iteration 78/1000 | Loss: 0.00003993
Iteration 79/1000 | Loss: 0.00003992
Iteration 80/1000 | Loss: 0.00003992
Iteration 81/1000 | Loss: 0.00003992
Iteration 82/1000 | Loss: 0.00003991
Iteration 83/1000 | Loss: 0.00003991
Iteration 84/1000 | Loss: 0.00003991
Iteration 85/1000 | Loss: 0.00003991
Iteration 86/1000 | Loss: 0.00003990
Iteration 87/1000 | Loss: 0.00003990
Iteration 88/1000 | Loss: 0.00003990
Iteration 89/1000 | Loss: 0.00003990
Iteration 90/1000 | Loss: 0.00003990
Iteration 91/1000 | Loss: 0.00003990
Iteration 92/1000 | Loss: 0.00003990
Iteration 93/1000 | Loss: 0.00003990
Iteration 94/1000 | Loss: 0.00003990
Iteration 95/1000 | Loss: 0.00003990
Iteration 96/1000 | Loss: 0.00003990
Iteration 97/1000 | Loss: 0.00003990
Iteration 98/1000 | Loss: 0.00003990
Iteration 99/1000 | Loss: 0.00003990
Iteration 100/1000 | Loss: 0.00003990
Iteration 101/1000 | Loss: 0.00003989
Iteration 102/1000 | Loss: 0.00003989
Iteration 103/1000 | Loss: 0.00003989
Iteration 104/1000 | Loss: 0.00003989
Iteration 105/1000 | Loss: 0.00003989
Iteration 106/1000 | Loss: 0.00003988
Iteration 107/1000 | Loss: 0.00003988
Iteration 108/1000 | Loss: 0.00003988
Iteration 109/1000 | Loss: 0.00003988
Iteration 110/1000 | Loss: 0.00003987
Iteration 111/1000 | Loss: 0.00003987
Iteration 112/1000 | Loss: 0.00003987
Iteration 113/1000 | Loss: 0.00003987
Iteration 114/1000 | Loss: 0.00003987
Iteration 115/1000 | Loss: 0.00003987
Iteration 116/1000 | Loss: 0.00003986
Iteration 117/1000 | Loss: 0.00003986
Iteration 118/1000 | Loss: 0.00003986
Iteration 119/1000 | Loss: 0.00003986
Iteration 120/1000 | Loss: 0.00003985
Iteration 121/1000 | Loss: 0.00003985
Iteration 122/1000 | Loss: 0.00003985
Iteration 123/1000 | Loss: 0.00003984
Iteration 124/1000 | Loss: 0.00003984
Iteration 125/1000 | Loss: 0.00003984
Iteration 126/1000 | Loss: 0.00003983
Iteration 127/1000 | Loss: 0.00003983
Iteration 128/1000 | Loss: 0.00003983
Iteration 129/1000 | Loss: 0.00003982
Iteration 130/1000 | Loss: 0.00003982
Iteration 131/1000 | Loss: 0.00003982
Iteration 132/1000 | Loss: 0.00003981
Iteration 133/1000 | Loss: 0.00003979
Iteration 134/1000 | Loss: 0.00003978
Iteration 135/1000 | Loss: 0.00003974
Iteration 136/1000 | Loss: 0.00003974
Iteration 137/1000 | Loss: 0.00003972
Iteration 138/1000 | Loss: 0.00003972
Iteration 139/1000 | Loss: 0.00003971
Iteration 140/1000 | Loss: 0.00003971
Iteration 141/1000 | Loss: 0.00003971
Iteration 142/1000 | Loss: 0.00003970
Iteration 143/1000 | Loss: 0.00003970
Iteration 144/1000 | Loss: 0.00003969
Iteration 145/1000 | Loss: 0.00003969
Iteration 146/1000 | Loss: 0.00003969
Iteration 147/1000 | Loss: 0.00003968
Iteration 148/1000 | Loss: 0.00003968
Iteration 149/1000 | Loss: 0.00003967
Iteration 150/1000 | Loss: 0.00003967
Iteration 151/1000 | Loss: 0.00003967
Iteration 152/1000 | Loss: 0.00003966
Iteration 153/1000 | Loss: 0.00003966
Iteration 154/1000 | Loss: 0.00003966
Iteration 155/1000 | Loss: 0.00003965
Iteration 156/1000 | Loss: 0.00003965
Iteration 157/1000 | Loss: 0.00003964
Iteration 158/1000 | Loss: 0.00003963
Iteration 159/1000 | Loss: 0.00003960
Iteration 160/1000 | Loss: 0.00003959
Iteration 161/1000 | Loss: 0.00003957
Iteration 162/1000 | Loss: 0.00003956
Iteration 163/1000 | Loss: 0.00003956
Iteration 164/1000 | Loss: 0.00003955
Iteration 165/1000 | Loss: 0.00003954
Iteration 166/1000 | Loss: 0.00003954
Iteration 167/1000 | Loss: 0.00003954
Iteration 168/1000 | Loss: 0.00003954
Iteration 169/1000 | Loss: 0.00003954
Iteration 170/1000 | Loss: 0.00003953
Iteration 171/1000 | Loss: 0.00003953
Iteration 172/1000 | Loss: 0.00003953
Iteration 173/1000 | Loss: 0.00003953
Iteration 174/1000 | Loss: 0.00003952
Iteration 175/1000 | Loss: 0.00003952
Iteration 176/1000 | Loss: 0.00003952
Iteration 177/1000 | Loss: 0.00003951
Iteration 178/1000 | Loss: 0.00003951
Iteration 179/1000 | Loss: 0.00003950
Iteration 180/1000 | Loss: 0.00003946
Iteration 181/1000 | Loss: 0.00003945
Iteration 182/1000 | Loss: 0.00003943
Iteration 183/1000 | Loss: 0.00003943
Iteration 184/1000 | Loss: 0.00003943
Iteration 185/1000 | Loss: 0.00003943
Iteration 186/1000 | Loss: 0.00003943
Iteration 187/1000 | Loss: 0.00003943
Iteration 188/1000 | Loss: 0.00003943
Iteration 189/1000 | Loss: 0.00003943
Iteration 190/1000 | Loss: 0.00003943
Iteration 191/1000 | Loss: 0.00003942
Iteration 192/1000 | Loss: 0.00003942
Iteration 193/1000 | Loss: 0.00003942
Iteration 194/1000 | Loss: 0.00003942
Iteration 195/1000 | Loss: 0.00003942
Iteration 196/1000 | Loss: 0.00003941
Iteration 197/1000 | Loss: 0.00003941
Iteration 198/1000 | Loss: 0.00003940
Iteration 199/1000 | Loss: 0.00003940
Iteration 200/1000 | Loss: 0.00003940
Iteration 201/1000 | Loss: 0.00003939
Iteration 202/1000 | Loss: 0.00003939
Iteration 203/1000 | Loss: 0.00003939
Iteration 204/1000 | Loss: 0.00003939
Iteration 205/1000 | Loss: 0.00003939
Iteration 206/1000 | Loss: 0.00003939
Iteration 207/1000 | Loss: 0.00003939
Iteration 208/1000 | Loss: 0.00003939
Iteration 209/1000 | Loss: 0.00003939
Iteration 210/1000 | Loss: 0.00003939
Iteration 211/1000 | Loss: 0.00003939
Iteration 212/1000 | Loss: 0.00003939
Iteration 213/1000 | Loss: 0.00003939
Iteration 214/1000 | Loss: 0.00003939
Iteration 215/1000 | Loss: 0.00003939
Iteration 216/1000 | Loss: 0.00003939
Iteration 217/1000 | Loss: 0.00003938
Iteration 218/1000 | Loss: 0.00003938
Iteration 219/1000 | Loss: 0.00003938
Iteration 220/1000 | Loss: 0.00003938
Iteration 221/1000 | Loss: 0.00003937
Iteration 222/1000 | Loss: 0.00003937
Iteration 223/1000 | Loss: 0.00003937
Iteration 224/1000 | Loss: 0.00003937
Iteration 225/1000 | Loss: 0.00003937
Iteration 226/1000 | Loss: 0.00003937
Iteration 227/1000 | Loss: 0.00003937
Iteration 228/1000 | Loss: 0.00003937
Iteration 229/1000 | Loss: 0.00003937
Iteration 230/1000 | Loss: 0.00003936
Iteration 231/1000 | Loss: 0.00003936
Iteration 232/1000 | Loss: 0.00003936
Iteration 233/1000 | Loss: 0.00003936
Iteration 234/1000 | Loss: 0.00003936
Iteration 235/1000 | Loss: 0.00003936
Iteration 236/1000 | Loss: 0.00003936
Iteration 237/1000 | Loss: 0.00003936
Iteration 238/1000 | Loss: 0.00003935
Iteration 239/1000 | Loss: 0.00003935
Iteration 240/1000 | Loss: 0.00003935
Iteration 241/1000 | Loss: 0.00003934
Iteration 242/1000 | Loss: 0.00003934
Iteration 243/1000 | Loss: 0.00003934
Iteration 244/1000 | Loss: 0.00003934
Iteration 245/1000 | Loss: 0.00003933
Iteration 246/1000 | Loss: 0.00003933
Iteration 247/1000 | Loss: 0.00003933
Iteration 248/1000 | Loss: 0.00003933
Iteration 249/1000 | Loss: 0.00003933
Iteration 250/1000 | Loss: 0.00003933
Iteration 251/1000 | Loss: 0.00003932
Iteration 252/1000 | Loss: 0.00003932
Iteration 253/1000 | Loss: 0.00003932
Iteration 254/1000 | Loss: 0.00003932
Iteration 255/1000 | Loss: 0.00003932
Iteration 256/1000 | Loss: 0.00003932
Iteration 257/1000 | Loss: 0.00003932
Iteration 258/1000 | Loss: 0.00003932
Iteration 259/1000 | Loss: 0.00003932
Iteration 260/1000 | Loss: 0.00003931
Iteration 261/1000 | Loss: 0.00003931
Iteration 262/1000 | Loss: 0.00003931
Iteration 263/1000 | Loss: 0.00003931
Iteration 264/1000 | Loss: 0.00003931
Iteration 265/1000 | Loss: 0.00003931
Iteration 266/1000 | Loss: 0.00003931
Iteration 267/1000 | Loss: 0.00003931
Iteration 268/1000 | Loss: 0.00003931
Iteration 269/1000 | Loss: 0.00003931
Iteration 270/1000 | Loss: 0.00003931
Iteration 271/1000 | Loss: 0.00003931
Iteration 272/1000 | Loss: 0.00003931
Iteration 273/1000 | Loss: 0.00003931
Iteration 274/1000 | Loss: 0.00003930
Iteration 275/1000 | Loss: 0.00003930
Iteration 276/1000 | Loss: 0.00003930
Iteration 277/1000 | Loss: 0.00003930
Iteration 278/1000 | Loss: 0.00003930
Iteration 279/1000 | Loss: 0.00003930
Iteration 280/1000 | Loss: 0.00003930
Iteration 281/1000 | Loss: 0.00003930
Iteration 282/1000 | Loss: 0.00003930
Iteration 283/1000 | Loss: 0.00003930
Iteration 284/1000 | Loss: 0.00003930
Iteration 285/1000 | Loss: 0.00003930
Iteration 286/1000 | Loss: 0.00003930
Iteration 287/1000 | Loss: 0.00003930
Iteration 288/1000 | Loss: 0.00003930
Iteration 289/1000 | Loss: 0.00003930
Iteration 290/1000 | Loss: 0.00003930
Iteration 291/1000 | Loss: 0.00003930
Iteration 292/1000 | Loss: 0.00003930
Iteration 293/1000 | Loss: 0.00003929
Iteration 294/1000 | Loss: 0.00003929
Iteration 295/1000 | Loss: 0.00003929
Iteration 296/1000 | Loss: 0.00003929
Iteration 297/1000 | Loss: 0.00003929
Iteration 298/1000 | Loss: 0.00003929
Iteration 299/1000 | Loss: 0.00003929
Iteration 300/1000 | Loss: 0.00003929
Iteration 301/1000 | Loss: 0.00003929
Iteration 302/1000 | Loss: 0.00003929
Iteration 303/1000 | Loss: 0.00003929
Iteration 304/1000 | Loss: 0.00003929
Iteration 305/1000 | Loss: 0.00003929
Iteration 306/1000 | Loss: 0.00003929
Iteration 307/1000 | Loss: 0.00003929
Iteration 308/1000 | Loss: 0.00003929
Iteration 309/1000 | Loss: 0.00003929
Iteration 310/1000 | Loss: 0.00003929
Iteration 311/1000 | Loss: 0.00003929
Iteration 312/1000 | Loss: 0.00003929
Iteration 313/1000 | Loss: 0.00003928
Iteration 314/1000 | Loss: 0.00003928
Iteration 315/1000 | Loss: 0.00003928
Iteration 316/1000 | Loss: 0.00003928
Iteration 317/1000 | Loss: 0.00003928
Iteration 318/1000 | Loss: 0.00003928
Iteration 319/1000 | Loss: 0.00003928
Iteration 320/1000 | Loss: 0.00003928
Iteration 321/1000 | Loss: 0.00003928
Iteration 322/1000 | Loss: 0.00003928
Iteration 323/1000 | Loss: 0.00003928
Iteration 324/1000 | Loss: 0.00003928
Iteration 325/1000 | Loss: 0.00003928
Iteration 326/1000 | Loss: 0.00003928
Iteration 327/1000 | Loss: 0.00003928
Iteration 328/1000 | Loss: 0.00003928
Iteration 329/1000 | Loss: 0.00003928
Iteration 330/1000 | Loss: 0.00003928
Iteration 331/1000 | Loss: 0.00003928
Iteration 332/1000 | Loss: 0.00003928
Iteration 333/1000 | Loss: 0.00003928
Iteration 334/1000 | Loss: 0.00003928
Iteration 335/1000 | Loss: 0.00003927
Iteration 336/1000 | Loss: 0.00003927
Iteration 337/1000 | Loss: 0.00003927
Iteration 338/1000 | Loss: 0.00003927
Iteration 339/1000 | Loss: 0.00003927
Iteration 340/1000 | Loss: 0.00003927
Iteration 341/1000 | Loss: 0.00003927
Iteration 342/1000 | Loss: 0.00003927
Iteration 343/1000 | Loss: 0.00003927
Iteration 344/1000 | Loss: 0.00003927
Iteration 345/1000 | Loss: 0.00003927
Iteration 346/1000 | Loss: 0.00003927
Iteration 347/1000 | Loss: 0.00003927
Iteration 348/1000 | Loss: 0.00003927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 348. Stopping optimization.
Last 5 losses: [3.927472425857559e-05, 3.927472425857559e-05, 3.927472425857559e-05, 3.927472425857559e-05, 3.927472425857559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.927472425857559e-05

Optimization complete. Final v2v error: 3.991788148880005 mm

Highest mean error: 11.029555320739746 mm for frame 91

Lowest mean error: 2.7272703647613525 mm for frame 127

Saving results

Total time: 127.90034031867981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_001/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_001/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856679
Iteration 2/25 | Loss: 0.00255575
Iteration 3/25 | Loss: 0.00202098
Iteration 4/25 | Loss: 0.00198747
Iteration 5/25 | Loss: 0.00175464
Iteration 6/25 | Loss: 0.00167386
Iteration 7/25 | Loss: 0.00165070
Iteration 8/25 | Loss: 0.00164524
Iteration 9/25 | Loss: 0.00164197
Iteration 10/25 | Loss: 0.00163841
Iteration 11/25 | Loss: 0.00163660
Iteration 12/25 | Loss: 0.00164266
Iteration 13/25 | Loss: 0.00163443
Iteration 14/25 | Loss: 0.00164321
Iteration 15/25 | Loss: 0.00164140
Iteration 16/25 | Loss: 0.00163226
Iteration 17/25 | Loss: 0.00163471
Iteration 18/25 | Loss: 0.00163012
Iteration 19/25 | Loss: 0.00162955
Iteration 20/25 | Loss: 0.00162935
Iteration 21/25 | Loss: 0.00163401
Iteration 22/25 | Loss: 0.00162349
Iteration 23/25 | Loss: 0.00161805
Iteration 24/25 | Loss: 0.00161687
Iteration 25/25 | Loss: 0.00161673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51909041
Iteration 2/25 | Loss: 0.00233516
Iteration 3/25 | Loss: 0.00233473
Iteration 4/25 | Loss: 0.00233473
Iteration 5/25 | Loss: 0.00233473
Iteration 6/25 | Loss: 0.00233473
Iteration 7/25 | Loss: 0.00233473
Iteration 8/25 | Loss: 0.00233473
Iteration 9/25 | Loss: 0.00233473
Iteration 10/25 | Loss: 0.00233473
Iteration 11/25 | Loss: 0.00233473
Iteration 12/25 | Loss: 0.00233473
Iteration 13/25 | Loss: 0.00233473
Iteration 14/25 | Loss: 0.00233473
Iteration 15/25 | Loss: 0.00233473
Iteration 16/25 | Loss: 0.00233473
Iteration 17/25 | Loss: 0.00233473
Iteration 18/25 | Loss: 0.00233473
Iteration 19/25 | Loss: 0.00233473
Iteration 20/25 | Loss: 0.00233473
Iteration 21/25 | Loss: 0.00233473
Iteration 22/25 | Loss: 0.00233473
Iteration 23/25 | Loss: 0.00233473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00233472790569067, 0.00233472790569067, 0.00233472790569067, 0.00233472790569067, 0.00233472790569067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00233472790569067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233473
Iteration 2/1000 | Loss: 0.00409264
Iteration 3/1000 | Loss: 0.00025113
Iteration 4/1000 | Loss: 0.00014190
Iteration 5/1000 | Loss: 0.00009929
Iteration 6/1000 | Loss: 0.00008338
Iteration 7/1000 | Loss: 0.00007375
Iteration 8/1000 | Loss: 0.00006858
Iteration 9/1000 | Loss: 0.00006481
Iteration 10/1000 | Loss: 0.00006296
Iteration 11/1000 | Loss: 0.00006114
Iteration 12/1000 | Loss: 0.00005987
Iteration 13/1000 | Loss: 0.00005861
Iteration 14/1000 | Loss: 0.00005782
Iteration 15/1000 | Loss: 0.00005688
Iteration 16/1000 | Loss: 0.00006862
Iteration 17/1000 | Loss: 0.00006502
Iteration 18/1000 | Loss: 0.00006434
Iteration 19/1000 | Loss: 0.00006266
Iteration 20/1000 | Loss: 0.00006125
Iteration 21/1000 | Loss: 0.00005591
Iteration 22/1000 | Loss: 0.00005523
Iteration 23/1000 | Loss: 0.00005478
Iteration 24/1000 | Loss: 0.00005433
Iteration 25/1000 | Loss: 0.00005405
Iteration 26/1000 | Loss: 0.00005375
Iteration 27/1000 | Loss: 0.00006829
Iteration 28/1000 | Loss: 0.00006350
Iteration 29/1000 | Loss: 0.00006816
Iteration 30/1000 | Loss: 0.00005501
Iteration 31/1000 | Loss: 0.00005429
Iteration 32/1000 | Loss: 0.00005371
Iteration 33/1000 | Loss: 0.00005319
Iteration 34/1000 | Loss: 0.00005294
Iteration 35/1000 | Loss: 0.00005287
Iteration 36/1000 | Loss: 0.00005287
Iteration 37/1000 | Loss: 0.00005284
Iteration 38/1000 | Loss: 0.00005283
Iteration 39/1000 | Loss: 0.00005283
Iteration 40/1000 | Loss: 0.00005282
Iteration 41/1000 | Loss: 0.00005271
Iteration 42/1000 | Loss: 0.00005266
Iteration 43/1000 | Loss: 0.00005250
Iteration 44/1000 | Loss: 0.00005248
Iteration 45/1000 | Loss: 0.00005248
Iteration 46/1000 | Loss: 0.00005248
Iteration 47/1000 | Loss: 0.00005247
Iteration 48/1000 | Loss: 0.00005247
Iteration 49/1000 | Loss: 0.00005247
Iteration 50/1000 | Loss: 0.00005247
Iteration 51/1000 | Loss: 0.00005247
Iteration 52/1000 | Loss: 0.00005246
Iteration 53/1000 | Loss: 0.00005246
Iteration 54/1000 | Loss: 0.00005246
Iteration 55/1000 | Loss: 0.00005246
Iteration 56/1000 | Loss: 0.00005245
Iteration 57/1000 | Loss: 0.00005245
Iteration 58/1000 | Loss: 0.00005245
Iteration 59/1000 | Loss: 0.00005245
Iteration 60/1000 | Loss: 0.00005245
Iteration 61/1000 | Loss: 0.00005244
Iteration 62/1000 | Loss: 0.00005244
Iteration 63/1000 | Loss: 0.00005244
Iteration 64/1000 | Loss: 0.00005244
Iteration 65/1000 | Loss: 0.00005244
Iteration 66/1000 | Loss: 0.00005244
Iteration 67/1000 | Loss: 0.00005244
Iteration 68/1000 | Loss: 0.00005244
Iteration 69/1000 | Loss: 0.00005243
Iteration 70/1000 | Loss: 0.00005243
Iteration 71/1000 | Loss: 0.00005243
Iteration 72/1000 | Loss: 0.00005243
Iteration 73/1000 | Loss: 0.00005242
Iteration 74/1000 | Loss: 0.00005242
Iteration 75/1000 | Loss: 0.00005242
Iteration 76/1000 | Loss: 0.00005241
Iteration 77/1000 | Loss: 0.00005241
Iteration 78/1000 | Loss: 0.00005241
Iteration 79/1000 | Loss: 0.00005240
Iteration 80/1000 | Loss: 0.00005240
Iteration 81/1000 | Loss: 0.00005240
Iteration 82/1000 | Loss: 0.00005240
Iteration 83/1000 | Loss: 0.00005239
Iteration 84/1000 | Loss: 0.00005239
Iteration 85/1000 | Loss: 0.00005239
Iteration 86/1000 | Loss: 0.00005238
Iteration 87/1000 | Loss: 0.00005238
Iteration 88/1000 | Loss: 0.00005238
Iteration 89/1000 | Loss: 0.00005238
Iteration 90/1000 | Loss: 0.00005237
Iteration 91/1000 | Loss: 0.00005237
Iteration 92/1000 | Loss: 0.00005237
Iteration 93/1000 | Loss: 0.00005237
Iteration 94/1000 | Loss: 0.00005236
Iteration 95/1000 | Loss: 0.00005236
Iteration 96/1000 | Loss: 0.00005236
Iteration 97/1000 | Loss: 0.00005236
Iteration 98/1000 | Loss: 0.00005236
Iteration 99/1000 | Loss: 0.00005235
Iteration 100/1000 | Loss: 0.00005235
Iteration 101/1000 | Loss: 0.00005235
Iteration 102/1000 | Loss: 0.00005235
Iteration 103/1000 | Loss: 0.00005235
Iteration 104/1000 | Loss: 0.00005235
Iteration 105/1000 | Loss: 0.00005235
Iteration 106/1000 | Loss: 0.00005234
Iteration 107/1000 | Loss: 0.00005234
Iteration 108/1000 | Loss: 0.00005234
Iteration 109/1000 | Loss: 0.00005234
Iteration 110/1000 | Loss: 0.00005234
Iteration 111/1000 | Loss: 0.00005234
Iteration 112/1000 | Loss: 0.00005234
Iteration 113/1000 | Loss: 0.00005234
Iteration 114/1000 | Loss: 0.00005234
Iteration 115/1000 | Loss: 0.00005234
Iteration 116/1000 | Loss: 0.00005234
Iteration 117/1000 | Loss: 0.00005234
Iteration 118/1000 | Loss: 0.00005234
Iteration 119/1000 | Loss: 0.00005234
Iteration 120/1000 | Loss: 0.00005234
Iteration 121/1000 | Loss: 0.00005234
Iteration 122/1000 | Loss: 0.00005233
Iteration 123/1000 | Loss: 0.00005233
Iteration 124/1000 | Loss: 0.00005233
Iteration 125/1000 | Loss: 0.00005233
Iteration 126/1000 | Loss: 0.00005233
Iteration 127/1000 | Loss: 0.00005233
Iteration 128/1000 | Loss: 0.00005233
Iteration 129/1000 | Loss: 0.00005233
Iteration 130/1000 | Loss: 0.00005233
Iteration 131/1000 | Loss: 0.00005233
Iteration 132/1000 | Loss: 0.00005232
Iteration 133/1000 | Loss: 0.00005232
Iteration 134/1000 | Loss: 0.00005232
Iteration 135/1000 | Loss: 0.00005232
Iteration 136/1000 | Loss: 0.00005232
Iteration 137/1000 | Loss: 0.00005232
Iteration 138/1000 | Loss: 0.00005231
Iteration 139/1000 | Loss: 0.00005231
Iteration 140/1000 | Loss: 0.00005231
Iteration 141/1000 | Loss: 0.00005231
Iteration 142/1000 | Loss: 0.00005231
Iteration 143/1000 | Loss: 0.00005231
Iteration 144/1000 | Loss: 0.00005231
Iteration 145/1000 | Loss: 0.00005231
Iteration 146/1000 | Loss: 0.00005231
Iteration 147/1000 | Loss: 0.00005231
Iteration 148/1000 | Loss: 0.00005231
Iteration 149/1000 | Loss: 0.00005230
Iteration 150/1000 | Loss: 0.00005230
Iteration 151/1000 | Loss: 0.00005230
Iteration 152/1000 | Loss: 0.00005230
Iteration 153/1000 | Loss: 0.00005230
Iteration 154/1000 | Loss: 0.00005230
Iteration 155/1000 | Loss: 0.00005230
Iteration 156/1000 | Loss: 0.00005230
Iteration 157/1000 | Loss: 0.00005230
Iteration 158/1000 | Loss: 0.00005230
Iteration 159/1000 | Loss: 0.00005230
Iteration 160/1000 | Loss: 0.00005230
Iteration 161/1000 | Loss: 0.00005230
Iteration 162/1000 | Loss: 0.00005230
Iteration 163/1000 | Loss: 0.00005230
Iteration 164/1000 | Loss: 0.00005230
Iteration 165/1000 | Loss: 0.00005230
Iteration 166/1000 | Loss: 0.00005230
Iteration 167/1000 | Loss: 0.00005230
Iteration 168/1000 | Loss: 0.00005230
Iteration 169/1000 | Loss: 0.00005230
Iteration 170/1000 | Loss: 0.00005230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [5.230117676546797e-05, 5.230117676546797e-05, 5.230117676546797e-05, 5.230117676546797e-05, 5.230117676546797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.230117676546797e-05

Optimization complete. Final v2v error: 5.440073013305664 mm

Highest mean error: 11.431406021118164 mm for frame 42

Lowest mean error: 3.438486099243164 mm for frame 83

Saving results

Total time: 117.46428656578064
