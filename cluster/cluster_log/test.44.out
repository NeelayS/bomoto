Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=44, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2464-2519
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465900
Iteration 2/25 | Loss: 0.00142001
Iteration 3/25 | Loss: 0.00136055
Iteration 4/25 | Loss: 0.00135102
Iteration 5/25 | Loss: 0.00134972
Iteration 6/25 | Loss: 0.00134972
Iteration 7/25 | Loss: 0.00134972
Iteration 8/25 | Loss: 0.00134972
Iteration 9/25 | Loss: 0.00134972
Iteration 10/25 | Loss: 0.00134972
Iteration 11/25 | Loss: 0.00134972
Iteration 12/25 | Loss: 0.00134972
Iteration 13/25 | Loss: 0.00134972
Iteration 14/25 | Loss: 0.00134972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013497185427695513, 0.0013497185427695513, 0.0013497185427695513, 0.0013497185427695513, 0.0013497185427695513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013497185427695513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25569737
Iteration 2/25 | Loss: 0.00218593
Iteration 3/25 | Loss: 0.00218592
Iteration 4/25 | Loss: 0.00218592
Iteration 5/25 | Loss: 0.00218592
Iteration 6/25 | Loss: 0.00218592
Iteration 7/25 | Loss: 0.00218592
Iteration 8/25 | Loss: 0.00218592
Iteration 9/25 | Loss: 0.00218592
Iteration 10/25 | Loss: 0.00218592
Iteration 11/25 | Loss: 0.00218592
Iteration 12/25 | Loss: 0.00218592
Iteration 13/25 | Loss: 0.00218592
Iteration 14/25 | Loss: 0.00218592
Iteration 15/25 | Loss: 0.00218592
Iteration 16/25 | Loss: 0.00218592
Iteration 17/25 | Loss: 0.00218592
Iteration 18/25 | Loss: 0.00218592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021859186235815287, 0.0021859186235815287, 0.0021859186235815287, 0.0021859186235815287, 0.0021859186235815287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021859186235815287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218592
Iteration 2/1000 | Loss: 0.00003318
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00002240
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002040
Iteration 7/1000 | Loss: 0.00001976
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001884
Iteration 10/1000 | Loss: 0.00001836
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001777
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001733
Iteration 15/1000 | Loss: 0.00001714
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001707
Iteration 19/1000 | Loss: 0.00001706
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001699
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00001674
Iteration 32/1000 | Loss: 0.00001669
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001666
Iteration 35/1000 | Loss: 0.00001665
Iteration 36/1000 | Loss: 0.00001664
Iteration 37/1000 | Loss: 0.00001664
Iteration 38/1000 | Loss: 0.00001663
Iteration 39/1000 | Loss: 0.00001663
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001656
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001655
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001653
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001651
Iteration 53/1000 | Loss: 0.00001651
Iteration 54/1000 | Loss: 0.00001651
Iteration 55/1000 | Loss: 0.00001650
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001650
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001646
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001644
Iteration 94/1000 | Loss: 0.00001644
Iteration 95/1000 | Loss: 0.00001644
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001642
Iteration 115/1000 | Loss: 0.00001642
Iteration 116/1000 | Loss: 0.00001642
Iteration 117/1000 | Loss: 0.00001642
Iteration 118/1000 | Loss: 0.00001642
Iteration 119/1000 | Loss: 0.00001642
Iteration 120/1000 | Loss: 0.00001642
Iteration 121/1000 | Loss: 0.00001642
Iteration 122/1000 | Loss: 0.00001642
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001642
Iteration 127/1000 | Loss: 0.00001642
Iteration 128/1000 | Loss: 0.00001642
Iteration 129/1000 | Loss: 0.00001642
Iteration 130/1000 | Loss: 0.00001642
Iteration 131/1000 | Loss: 0.00001642
Iteration 132/1000 | Loss: 0.00001642
Iteration 133/1000 | Loss: 0.00001642
Iteration 134/1000 | Loss: 0.00001642
Iteration 135/1000 | Loss: 0.00001642
Iteration 136/1000 | Loss: 0.00001642
Iteration 137/1000 | Loss: 0.00001642
Iteration 138/1000 | Loss: 0.00001642
Iteration 139/1000 | Loss: 0.00001642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.6420335668954067e-05, 1.6420335668954067e-05, 1.6420335668954067e-05, 1.6420335668954067e-05, 1.6420335668954067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6420335668954067e-05

Optimization complete. Final v2v error: 3.43845272064209 mm

Highest mean error: 3.8449532985687256 mm for frame 193

Lowest mean error: 2.893009901046753 mm for frame 65

Saving results

Total time: 49.296202182769775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411583
Iteration 2/25 | Loss: 0.00136233
Iteration 3/25 | Loss: 0.00130958
Iteration 4/25 | Loss: 0.00130229
Iteration 5/25 | Loss: 0.00130014
Iteration 6/25 | Loss: 0.00129989
Iteration 7/25 | Loss: 0.00129989
Iteration 8/25 | Loss: 0.00129989
Iteration 9/25 | Loss: 0.00129989
Iteration 10/25 | Loss: 0.00129989
Iteration 11/25 | Loss: 0.00129989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012998884776607156, 0.0012998884776607156, 0.0012998884776607156, 0.0012998884776607156, 0.0012998884776607156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012998884776607156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.99478698
Iteration 2/25 | Loss: 0.00201332
Iteration 3/25 | Loss: 0.00201332
Iteration 4/25 | Loss: 0.00201332
Iteration 5/25 | Loss: 0.00201332
Iteration 6/25 | Loss: 0.00201332
Iteration 7/25 | Loss: 0.00201332
Iteration 8/25 | Loss: 0.00201332
Iteration 9/25 | Loss: 0.00201332
Iteration 10/25 | Loss: 0.00201332
Iteration 11/25 | Loss: 0.00201332
Iteration 12/25 | Loss: 0.00201332
Iteration 13/25 | Loss: 0.00201332
Iteration 14/25 | Loss: 0.00201332
Iteration 15/25 | Loss: 0.00201332
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0020133156795054674, 0.0020133156795054674, 0.0020133156795054674, 0.0020133156795054674, 0.0020133156795054674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020133156795054674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201332
Iteration 2/1000 | Loss: 0.00002295
Iteration 3/1000 | Loss: 0.00001683
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001339
Iteration 8/1000 | Loss: 0.00001301
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001180
Iteration 14/1000 | Loss: 0.00001171
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001157
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001145
Iteration 19/1000 | Loss: 0.00001144
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001132
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001131
Iteration 47/1000 | Loss: 0.00001130
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001123
Iteration 67/1000 | Loss: 0.00001123
Iteration 68/1000 | Loss: 0.00001123
Iteration 69/1000 | Loss: 0.00001123
Iteration 70/1000 | Loss: 0.00001123
Iteration 71/1000 | Loss: 0.00001123
Iteration 72/1000 | Loss: 0.00001123
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001122
Iteration 76/1000 | Loss: 0.00001122
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001122
Iteration 79/1000 | Loss: 0.00001122
Iteration 80/1000 | Loss: 0.00001121
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001120
Iteration 85/1000 | Loss: 0.00001120
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001114
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001114
Iteration 112/1000 | Loss: 0.00001114
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001114
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001112
Iteration 128/1000 | Loss: 0.00001112
Iteration 129/1000 | Loss: 0.00001112
Iteration 130/1000 | Loss: 0.00001112
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001111
Iteration 133/1000 | Loss: 0.00001111
Iteration 134/1000 | Loss: 0.00001111
Iteration 135/1000 | Loss: 0.00001111
Iteration 136/1000 | Loss: 0.00001111
Iteration 137/1000 | Loss: 0.00001110
Iteration 138/1000 | Loss: 0.00001110
Iteration 139/1000 | Loss: 0.00001110
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001108
Iteration 152/1000 | Loss: 0.00001108
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001108
Iteration 159/1000 | Loss: 0.00001108
Iteration 160/1000 | Loss: 0.00001108
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001106
Iteration 188/1000 | Loss: 0.00001106
Iteration 189/1000 | Loss: 0.00001106
Iteration 190/1000 | Loss: 0.00001106
Iteration 191/1000 | Loss: 0.00001106
Iteration 192/1000 | Loss: 0.00001106
Iteration 193/1000 | Loss: 0.00001106
Iteration 194/1000 | Loss: 0.00001106
Iteration 195/1000 | Loss: 0.00001106
Iteration 196/1000 | Loss: 0.00001106
Iteration 197/1000 | Loss: 0.00001106
Iteration 198/1000 | Loss: 0.00001106
Iteration 199/1000 | Loss: 0.00001106
Iteration 200/1000 | Loss: 0.00001106
Iteration 201/1000 | Loss: 0.00001106
Iteration 202/1000 | Loss: 0.00001106
Iteration 203/1000 | Loss: 0.00001106
Iteration 204/1000 | Loss: 0.00001106
Iteration 205/1000 | Loss: 0.00001106
Iteration 206/1000 | Loss: 0.00001106
Iteration 207/1000 | Loss: 0.00001105
Iteration 208/1000 | Loss: 0.00001105
Iteration 209/1000 | Loss: 0.00001105
Iteration 210/1000 | Loss: 0.00001105
Iteration 211/1000 | Loss: 0.00001105
Iteration 212/1000 | Loss: 0.00001105
Iteration 213/1000 | Loss: 0.00001105
Iteration 214/1000 | Loss: 0.00001105
Iteration 215/1000 | Loss: 0.00001105
Iteration 216/1000 | Loss: 0.00001105
Iteration 217/1000 | Loss: 0.00001105
Iteration 218/1000 | Loss: 0.00001105
Iteration 219/1000 | Loss: 0.00001105
Iteration 220/1000 | Loss: 0.00001105
Iteration 221/1000 | Loss: 0.00001105
Iteration 222/1000 | Loss: 0.00001105
Iteration 223/1000 | Loss: 0.00001105
Iteration 224/1000 | Loss: 0.00001105
Iteration 225/1000 | Loss: 0.00001105
Iteration 226/1000 | Loss: 0.00001105
Iteration 227/1000 | Loss: 0.00001105
Iteration 228/1000 | Loss: 0.00001105
Iteration 229/1000 | Loss: 0.00001105
Iteration 230/1000 | Loss: 0.00001105
Iteration 231/1000 | Loss: 0.00001105
Iteration 232/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.1052948138967622e-05, 1.1052948138967622e-05, 1.1052948138967622e-05, 1.1052948138967622e-05, 1.1052948138967622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1052948138967622e-05

Optimization complete. Final v2v error: 2.900700807571411 mm

Highest mean error: 3.1338512897491455 mm for frame 112

Lowest mean error: 2.700716733932495 mm for frame 34

Saving results

Total time: 42.49804639816284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992836
Iteration 2/25 | Loss: 0.00992836
Iteration 3/25 | Loss: 0.00992836
Iteration 4/25 | Loss: 0.00992835
Iteration 5/25 | Loss: 0.00992835
Iteration 6/25 | Loss: 0.00992835
Iteration 7/25 | Loss: 0.00992835
Iteration 8/25 | Loss: 0.00992835
Iteration 9/25 | Loss: 0.00992835
Iteration 10/25 | Loss: 0.00992835
Iteration 11/25 | Loss: 0.00992835
Iteration 12/25 | Loss: 0.00992835
Iteration 13/25 | Loss: 0.00992835
Iteration 14/25 | Loss: 0.00992835
Iteration 15/25 | Loss: 0.00992835
Iteration 16/25 | Loss: 0.00992835
Iteration 17/25 | Loss: 0.00992835
Iteration 18/25 | Loss: 0.00992834
Iteration 19/25 | Loss: 0.00992834
Iteration 20/25 | Loss: 0.00992834
Iteration 21/25 | Loss: 0.00992834
Iteration 22/25 | Loss: 0.00992834
Iteration 23/25 | Loss: 0.00992834
Iteration 24/25 | Loss: 0.00992834
Iteration 25/25 | Loss: 0.00992834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45593238
Iteration 2/25 | Loss: 0.12108698
Iteration 3/25 | Loss: 0.12097797
Iteration 4/25 | Loss: 0.12067434
Iteration 5/25 | Loss: 0.12067432
Iteration 6/25 | Loss: 0.12067430
Iteration 7/25 | Loss: 0.12067430
Iteration 8/25 | Loss: 0.12067430
Iteration 9/25 | Loss: 0.12067430
Iteration 10/25 | Loss: 0.12067430
Iteration 11/25 | Loss: 0.12067430
Iteration 12/25 | Loss: 0.12067430
Iteration 13/25 | Loss: 0.12067430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.12067429721355438, 0.12067429721355438, 0.12067429721355438, 0.12067429721355438, 0.12067429721355438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12067429721355438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12067430
Iteration 2/1000 | Loss: 0.00267255
Iteration 3/1000 | Loss: 0.00812353
Iteration 4/1000 | Loss: 0.00198656
Iteration 5/1000 | Loss: 0.00154175
Iteration 6/1000 | Loss: 0.00277171
Iteration 7/1000 | Loss: 0.00037713
Iteration 8/1000 | Loss: 0.00080158
Iteration 9/1000 | Loss: 0.00022747
Iteration 10/1000 | Loss: 0.00018416
Iteration 11/1000 | Loss: 0.00029947
Iteration 12/1000 | Loss: 0.00015304
Iteration 13/1000 | Loss: 0.00005334
Iteration 14/1000 | Loss: 0.00012628
Iteration 15/1000 | Loss: 0.00025146
Iteration 16/1000 | Loss: 0.00013495
Iteration 17/1000 | Loss: 0.00003821
Iteration 18/1000 | Loss: 0.00034559
Iteration 19/1000 | Loss: 0.00011254
Iteration 20/1000 | Loss: 0.00004031
Iteration 21/1000 | Loss: 0.00013229
Iteration 22/1000 | Loss: 0.00037780
Iteration 23/1000 | Loss: 0.00011166
Iteration 24/1000 | Loss: 0.00003815
Iteration 25/1000 | Loss: 0.00009428
Iteration 26/1000 | Loss: 0.00009926
Iteration 27/1000 | Loss: 0.00004299
Iteration 28/1000 | Loss: 0.00010477
Iteration 29/1000 | Loss: 0.00002777
Iteration 30/1000 | Loss: 0.00005583
Iteration 31/1000 | Loss: 0.00008010
Iteration 32/1000 | Loss: 0.00004364
Iteration 33/1000 | Loss: 0.00005554
Iteration 34/1000 | Loss: 0.00117305
Iteration 35/1000 | Loss: 0.00004478
Iteration 36/1000 | Loss: 0.00004488
Iteration 37/1000 | Loss: 0.00006358
Iteration 38/1000 | Loss: 0.00001957
Iteration 39/1000 | Loss: 0.00001835
Iteration 40/1000 | Loss: 0.00008472
Iteration 41/1000 | Loss: 0.00005288
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001662
Iteration 44/1000 | Loss: 0.00001660
Iteration 45/1000 | Loss: 0.00003230
Iteration 46/1000 | Loss: 0.00007257
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00012868
Iteration 49/1000 | Loss: 0.00004531
Iteration 50/1000 | Loss: 0.00002173
Iteration 51/1000 | Loss: 0.00005885
Iteration 52/1000 | Loss: 0.00010450
Iteration 53/1000 | Loss: 0.00001608
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00008251
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00001659
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00004018
Iteration 60/1000 | Loss: 0.00001762
Iteration 61/1000 | Loss: 0.00001573
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001557
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001552
Iteration 74/1000 | Loss: 0.00001550
Iteration 75/1000 | Loss: 0.00003874
Iteration 76/1000 | Loss: 0.00018330
Iteration 77/1000 | Loss: 0.00003273
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00001799
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001541
Iteration 82/1000 | Loss: 0.00001541
Iteration 83/1000 | Loss: 0.00001541
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001541
Iteration 86/1000 | Loss: 0.00001760
Iteration 87/1000 | Loss: 0.00001557
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001539
Iteration 91/1000 | Loss: 0.00001539
Iteration 92/1000 | Loss: 0.00001539
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001538
Iteration 100/1000 | Loss: 0.00001538
Iteration 101/1000 | Loss: 0.00001538
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.5383853678940795e-05, 1.5383853678940795e-05, 1.5383853678940795e-05, 1.5383853678940795e-05, 1.5383853678940795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5383853678940795e-05

Optimization complete. Final v2v error: 3.3412749767303467 mm

Highest mean error: 3.6198043823242188 mm for frame 126

Lowest mean error: 3.2584400177001953 mm for frame 233

Saving results

Total time: 114.74124932289124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590322
Iteration 2/25 | Loss: 0.00152656
Iteration 3/25 | Loss: 0.00144013
Iteration 4/25 | Loss: 0.00143132
Iteration 5/25 | Loss: 0.00142868
Iteration 6/25 | Loss: 0.00142825
Iteration 7/25 | Loss: 0.00142816
Iteration 8/25 | Loss: 0.00142816
Iteration 9/25 | Loss: 0.00142816
Iteration 10/25 | Loss: 0.00142816
Iteration 11/25 | Loss: 0.00142816
Iteration 12/25 | Loss: 0.00142816
Iteration 13/25 | Loss: 0.00142816
Iteration 14/25 | Loss: 0.00142816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014281575568020344, 0.0014281575568020344, 0.0014281575568020344, 0.0014281575568020344, 0.0014281575568020344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014281575568020344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21251309
Iteration 2/25 | Loss: 0.00246498
Iteration 3/25 | Loss: 0.00246498
Iteration 4/25 | Loss: 0.00246498
Iteration 5/25 | Loss: 0.00246498
Iteration 6/25 | Loss: 0.00246498
Iteration 7/25 | Loss: 0.00246498
Iteration 8/25 | Loss: 0.00246498
Iteration 9/25 | Loss: 0.00246498
Iteration 10/25 | Loss: 0.00246498
Iteration 11/25 | Loss: 0.00246498
Iteration 12/25 | Loss: 0.00246498
Iteration 13/25 | Loss: 0.00246498
Iteration 14/25 | Loss: 0.00246498
Iteration 15/25 | Loss: 0.00246498
Iteration 16/25 | Loss: 0.00246498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00246498198248446, 0.00246498198248446, 0.00246498198248446, 0.00246498198248446, 0.00246498198248446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00246498198248446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246498
Iteration 2/1000 | Loss: 0.00003456
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002338
Iteration 5/1000 | Loss: 0.00002250
Iteration 6/1000 | Loss: 0.00002185
Iteration 7/1000 | Loss: 0.00002135
Iteration 8/1000 | Loss: 0.00002099
Iteration 9/1000 | Loss: 0.00002066
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002031
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00002018
Iteration 16/1000 | Loss: 0.00002014
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00002003
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001995
Iteration 21/1000 | Loss: 0.00001991
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001987
Iteration 24/1000 | Loss: 0.00001987
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001986
Iteration 27/1000 | Loss: 0.00001985
Iteration 28/1000 | Loss: 0.00001984
Iteration 29/1000 | Loss: 0.00001984
Iteration 30/1000 | Loss: 0.00001984
Iteration 31/1000 | Loss: 0.00001984
Iteration 32/1000 | Loss: 0.00001983
Iteration 33/1000 | Loss: 0.00001983
Iteration 34/1000 | Loss: 0.00001983
Iteration 35/1000 | Loss: 0.00001982
Iteration 36/1000 | Loss: 0.00001982
Iteration 37/1000 | Loss: 0.00001982
Iteration 38/1000 | Loss: 0.00001981
Iteration 39/1000 | Loss: 0.00001981
Iteration 40/1000 | Loss: 0.00001981
Iteration 41/1000 | Loss: 0.00001981
Iteration 42/1000 | Loss: 0.00001981
Iteration 43/1000 | Loss: 0.00001981
Iteration 44/1000 | Loss: 0.00001980
Iteration 45/1000 | Loss: 0.00001980
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001980
Iteration 48/1000 | Loss: 0.00001979
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001979
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001977
Iteration 56/1000 | Loss: 0.00001977
Iteration 57/1000 | Loss: 0.00001977
Iteration 58/1000 | Loss: 0.00001977
Iteration 59/1000 | Loss: 0.00001976
Iteration 60/1000 | Loss: 0.00001976
Iteration 61/1000 | Loss: 0.00001976
Iteration 62/1000 | Loss: 0.00001976
Iteration 63/1000 | Loss: 0.00001976
Iteration 64/1000 | Loss: 0.00001976
Iteration 65/1000 | Loss: 0.00001975
Iteration 66/1000 | Loss: 0.00001975
Iteration 67/1000 | Loss: 0.00001975
Iteration 68/1000 | Loss: 0.00001975
Iteration 69/1000 | Loss: 0.00001975
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001974
Iteration 72/1000 | Loss: 0.00001974
Iteration 73/1000 | Loss: 0.00001974
Iteration 74/1000 | Loss: 0.00001974
Iteration 75/1000 | Loss: 0.00001974
Iteration 76/1000 | Loss: 0.00001974
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001974
Iteration 80/1000 | Loss: 0.00001974
Iteration 81/1000 | Loss: 0.00001974
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001973
Iteration 84/1000 | Loss: 0.00001973
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001972
Iteration 87/1000 | Loss: 0.00001972
Iteration 88/1000 | Loss: 0.00001972
Iteration 89/1000 | Loss: 0.00001972
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001971
Iteration 93/1000 | Loss: 0.00001971
Iteration 94/1000 | Loss: 0.00001971
Iteration 95/1000 | Loss: 0.00001971
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001971
Iteration 99/1000 | Loss: 0.00001971
Iteration 100/1000 | Loss: 0.00001970
Iteration 101/1000 | Loss: 0.00001970
Iteration 102/1000 | Loss: 0.00001970
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00001970
Iteration 105/1000 | Loss: 0.00001970
Iteration 106/1000 | Loss: 0.00001970
Iteration 107/1000 | Loss: 0.00001970
Iteration 108/1000 | Loss: 0.00001970
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00001969
Iteration 111/1000 | Loss: 0.00001969
Iteration 112/1000 | Loss: 0.00001969
Iteration 113/1000 | Loss: 0.00001969
Iteration 114/1000 | Loss: 0.00001969
Iteration 115/1000 | Loss: 0.00001969
Iteration 116/1000 | Loss: 0.00001968
Iteration 117/1000 | Loss: 0.00001968
Iteration 118/1000 | Loss: 0.00001968
Iteration 119/1000 | Loss: 0.00001968
Iteration 120/1000 | Loss: 0.00001968
Iteration 121/1000 | Loss: 0.00001968
Iteration 122/1000 | Loss: 0.00001968
Iteration 123/1000 | Loss: 0.00001968
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00001967
Iteration 126/1000 | Loss: 0.00001967
Iteration 127/1000 | Loss: 0.00001967
Iteration 128/1000 | Loss: 0.00001967
Iteration 129/1000 | Loss: 0.00001967
Iteration 130/1000 | Loss: 0.00001967
Iteration 131/1000 | Loss: 0.00001967
Iteration 132/1000 | Loss: 0.00001967
Iteration 133/1000 | Loss: 0.00001967
Iteration 134/1000 | Loss: 0.00001967
Iteration 135/1000 | Loss: 0.00001967
Iteration 136/1000 | Loss: 0.00001967
Iteration 137/1000 | Loss: 0.00001967
Iteration 138/1000 | Loss: 0.00001966
Iteration 139/1000 | Loss: 0.00001966
Iteration 140/1000 | Loss: 0.00001966
Iteration 141/1000 | Loss: 0.00001966
Iteration 142/1000 | Loss: 0.00001966
Iteration 143/1000 | Loss: 0.00001966
Iteration 144/1000 | Loss: 0.00001965
Iteration 145/1000 | Loss: 0.00001965
Iteration 146/1000 | Loss: 0.00001964
Iteration 147/1000 | Loss: 0.00001964
Iteration 148/1000 | Loss: 0.00001963
Iteration 149/1000 | Loss: 0.00001963
Iteration 150/1000 | Loss: 0.00001963
Iteration 151/1000 | Loss: 0.00001963
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001962
Iteration 154/1000 | Loss: 0.00001962
Iteration 155/1000 | Loss: 0.00001962
Iteration 156/1000 | Loss: 0.00001962
Iteration 157/1000 | Loss: 0.00001961
Iteration 158/1000 | Loss: 0.00001961
Iteration 159/1000 | Loss: 0.00001961
Iteration 160/1000 | Loss: 0.00001960
Iteration 161/1000 | Loss: 0.00001960
Iteration 162/1000 | Loss: 0.00001960
Iteration 163/1000 | Loss: 0.00001960
Iteration 164/1000 | Loss: 0.00001959
Iteration 165/1000 | Loss: 0.00001959
Iteration 166/1000 | Loss: 0.00001959
Iteration 167/1000 | Loss: 0.00001959
Iteration 168/1000 | Loss: 0.00001959
Iteration 169/1000 | Loss: 0.00001958
Iteration 170/1000 | Loss: 0.00001958
Iteration 171/1000 | Loss: 0.00001958
Iteration 172/1000 | Loss: 0.00001958
Iteration 173/1000 | Loss: 0.00001958
Iteration 174/1000 | Loss: 0.00001958
Iteration 175/1000 | Loss: 0.00001958
Iteration 176/1000 | Loss: 0.00001958
Iteration 177/1000 | Loss: 0.00001957
Iteration 178/1000 | Loss: 0.00001957
Iteration 179/1000 | Loss: 0.00001957
Iteration 180/1000 | Loss: 0.00001956
Iteration 181/1000 | Loss: 0.00001956
Iteration 182/1000 | Loss: 0.00001956
Iteration 183/1000 | Loss: 0.00001956
Iteration 184/1000 | Loss: 0.00001956
Iteration 185/1000 | Loss: 0.00001956
Iteration 186/1000 | Loss: 0.00001956
Iteration 187/1000 | Loss: 0.00001956
Iteration 188/1000 | Loss: 0.00001956
Iteration 189/1000 | Loss: 0.00001956
Iteration 190/1000 | Loss: 0.00001956
Iteration 191/1000 | Loss: 0.00001956
Iteration 192/1000 | Loss: 0.00001956
Iteration 193/1000 | Loss: 0.00001956
Iteration 194/1000 | Loss: 0.00001956
Iteration 195/1000 | Loss: 0.00001956
Iteration 196/1000 | Loss: 0.00001956
Iteration 197/1000 | Loss: 0.00001956
Iteration 198/1000 | Loss: 0.00001956
Iteration 199/1000 | Loss: 0.00001956
Iteration 200/1000 | Loss: 0.00001956
Iteration 201/1000 | Loss: 0.00001956
Iteration 202/1000 | Loss: 0.00001956
Iteration 203/1000 | Loss: 0.00001956
Iteration 204/1000 | Loss: 0.00001956
Iteration 205/1000 | Loss: 0.00001956
Iteration 206/1000 | Loss: 0.00001956
Iteration 207/1000 | Loss: 0.00001956
Iteration 208/1000 | Loss: 0.00001956
Iteration 209/1000 | Loss: 0.00001956
Iteration 210/1000 | Loss: 0.00001956
Iteration 211/1000 | Loss: 0.00001956
Iteration 212/1000 | Loss: 0.00001956
Iteration 213/1000 | Loss: 0.00001956
Iteration 214/1000 | Loss: 0.00001956
Iteration 215/1000 | Loss: 0.00001956
Iteration 216/1000 | Loss: 0.00001956
Iteration 217/1000 | Loss: 0.00001956
Iteration 218/1000 | Loss: 0.00001956
Iteration 219/1000 | Loss: 0.00001956
Iteration 220/1000 | Loss: 0.00001956
Iteration 221/1000 | Loss: 0.00001956
Iteration 222/1000 | Loss: 0.00001956
Iteration 223/1000 | Loss: 0.00001956
Iteration 224/1000 | Loss: 0.00001956
Iteration 225/1000 | Loss: 0.00001956
Iteration 226/1000 | Loss: 0.00001956
Iteration 227/1000 | Loss: 0.00001956
Iteration 228/1000 | Loss: 0.00001956
Iteration 229/1000 | Loss: 0.00001956
Iteration 230/1000 | Loss: 0.00001956
Iteration 231/1000 | Loss: 0.00001956
Iteration 232/1000 | Loss: 0.00001956
Iteration 233/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.9559558495529927e-05, 1.9559558495529927e-05, 1.9559558495529927e-05, 1.9559558495529927e-05, 1.9559558495529927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9559558495529927e-05

Optimization complete. Final v2v error: 3.804063081741333 mm

Highest mean error: 4.361423492431641 mm for frame 60

Lowest mean error: 3.5330119132995605 mm for frame 110

Saving results

Total time: 47.537429332733154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850190
Iteration 2/25 | Loss: 0.00143791
Iteration 3/25 | Loss: 0.00137007
Iteration 4/25 | Loss: 0.00136234
Iteration 5/25 | Loss: 0.00136175
Iteration 6/25 | Loss: 0.00136175
Iteration 7/25 | Loss: 0.00136175
Iteration 8/25 | Loss: 0.00136175
Iteration 9/25 | Loss: 0.00136175
Iteration 10/25 | Loss: 0.00136175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013617471558973193, 0.0013617471558973193, 0.0013617471558973193, 0.0013617471558973193, 0.0013617471558973193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013617471558973193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18190849
Iteration 2/25 | Loss: 0.00183703
Iteration 3/25 | Loss: 0.00183695
Iteration 4/25 | Loss: 0.00183695
Iteration 5/25 | Loss: 0.00183695
Iteration 6/25 | Loss: 0.00183695
Iteration 7/25 | Loss: 0.00183695
Iteration 8/25 | Loss: 0.00183695
Iteration 9/25 | Loss: 0.00183695
Iteration 10/25 | Loss: 0.00183695
Iteration 11/25 | Loss: 0.00183695
Iteration 12/25 | Loss: 0.00183695
Iteration 13/25 | Loss: 0.00183695
Iteration 14/25 | Loss: 0.00183695
Iteration 15/25 | Loss: 0.00183695
Iteration 16/25 | Loss: 0.00183695
Iteration 17/25 | Loss: 0.00183695
Iteration 18/25 | Loss: 0.00183695
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001836945302784443, 0.001836945302784443, 0.001836945302784443, 0.001836945302784443, 0.001836945302784443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001836945302784443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183695
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00002127
Iteration 4/1000 | Loss: 0.00001935
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00001686
Iteration 7/1000 | Loss: 0.00001642
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00001533
Iteration 10/1000 | Loss: 0.00001493
Iteration 11/1000 | Loss: 0.00001492
Iteration 12/1000 | Loss: 0.00001462
Iteration 13/1000 | Loss: 0.00001438
Iteration 14/1000 | Loss: 0.00001410
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001368
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001352
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001342
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001341
Iteration 32/1000 | Loss: 0.00001338
Iteration 33/1000 | Loss: 0.00001336
Iteration 34/1000 | Loss: 0.00001328
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001320
Iteration 37/1000 | Loss: 0.00001319
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001311
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001304
Iteration 50/1000 | Loss: 0.00001301
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001300
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001300
Iteration 58/1000 | Loss: 0.00001300
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001300
Iteration 61/1000 | Loss: 0.00001299
Iteration 62/1000 | Loss: 0.00001299
Iteration 63/1000 | Loss: 0.00001296
Iteration 64/1000 | Loss: 0.00001296
Iteration 65/1000 | Loss: 0.00001296
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001295
Iteration 69/1000 | Loss: 0.00001293
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001291
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001290
Iteration 79/1000 | Loss: 0.00001290
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001287
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001286
Iteration 97/1000 | Loss: 0.00001286
Iteration 98/1000 | Loss: 0.00001286
Iteration 99/1000 | Loss: 0.00001286
Iteration 100/1000 | Loss: 0.00001286
Iteration 101/1000 | Loss: 0.00001286
Iteration 102/1000 | Loss: 0.00001286
Iteration 103/1000 | Loss: 0.00001285
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001285
Iteration 109/1000 | Loss: 0.00001285
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001282
Iteration 126/1000 | Loss: 0.00001282
Iteration 127/1000 | Loss: 0.00001282
Iteration 128/1000 | Loss: 0.00001282
Iteration 129/1000 | Loss: 0.00001282
Iteration 130/1000 | Loss: 0.00001282
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001282
Iteration 134/1000 | Loss: 0.00001282
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001282
Iteration 138/1000 | Loss: 0.00001282
Iteration 139/1000 | Loss: 0.00001282
Iteration 140/1000 | Loss: 0.00001281
Iteration 141/1000 | Loss: 0.00001281
Iteration 142/1000 | Loss: 0.00001281
Iteration 143/1000 | Loss: 0.00001281
Iteration 144/1000 | Loss: 0.00001281
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001280
Iteration 147/1000 | Loss: 0.00001280
Iteration 148/1000 | Loss: 0.00001280
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001280
Iteration 158/1000 | Loss: 0.00001280
Iteration 159/1000 | Loss: 0.00001280
Iteration 160/1000 | Loss: 0.00001280
Iteration 161/1000 | Loss: 0.00001280
Iteration 162/1000 | Loss: 0.00001280
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001280
Iteration 165/1000 | Loss: 0.00001280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2797997442248743e-05, 1.2797997442248743e-05, 1.2797997442248743e-05, 1.2797997442248743e-05, 1.2797997442248743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2797997442248743e-05

Optimization complete. Final v2v error: 3.100969076156616 mm

Highest mean error: 3.4094345569610596 mm for frame 218

Lowest mean error: 2.9260151386260986 mm for frame 78

Saving results

Total time: 50.020853757858276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430400
Iteration 2/25 | Loss: 0.00145476
Iteration 3/25 | Loss: 0.00135868
Iteration 4/25 | Loss: 0.00135222
Iteration 5/25 | Loss: 0.00135217
Iteration 6/25 | Loss: 0.00135217
Iteration 7/25 | Loss: 0.00135217
Iteration 8/25 | Loss: 0.00135217
Iteration 9/25 | Loss: 0.00135217
Iteration 10/25 | Loss: 0.00135217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013521689688786864, 0.0013521689688786864, 0.0013521689688786864, 0.0013521689688786864, 0.0013521689688786864]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013521689688786864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22027504
Iteration 2/25 | Loss: 0.00208135
Iteration 3/25 | Loss: 0.00208135
Iteration 4/25 | Loss: 0.00208135
Iteration 5/25 | Loss: 0.00208135
Iteration 6/25 | Loss: 0.00208135
Iteration 7/25 | Loss: 0.00208135
Iteration 8/25 | Loss: 0.00208135
Iteration 9/25 | Loss: 0.00208135
Iteration 10/25 | Loss: 0.00208135
Iteration 11/25 | Loss: 0.00208135
Iteration 12/25 | Loss: 0.00208134
Iteration 13/25 | Loss: 0.00208134
Iteration 14/25 | Loss: 0.00208134
Iteration 15/25 | Loss: 0.00208134
Iteration 16/25 | Loss: 0.00208134
Iteration 17/25 | Loss: 0.00208134
Iteration 18/25 | Loss: 0.00208134
Iteration 19/25 | Loss: 0.00208134
Iteration 20/25 | Loss: 0.00208134
Iteration 21/25 | Loss: 0.00208134
Iteration 22/25 | Loss: 0.00208134
Iteration 23/25 | Loss: 0.00208134
Iteration 24/25 | Loss: 0.00208134
Iteration 25/25 | Loss: 0.00208134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208134
Iteration 2/1000 | Loss: 0.00002783
Iteration 3/1000 | Loss: 0.00002118
Iteration 4/1000 | Loss: 0.00001929
Iteration 5/1000 | Loss: 0.00001834
Iteration 6/1000 | Loss: 0.00001753
Iteration 7/1000 | Loss: 0.00001705
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001557
Iteration 13/1000 | Loss: 0.00001552
Iteration 14/1000 | Loss: 0.00001549
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001534
Iteration 17/1000 | Loss: 0.00001532
Iteration 18/1000 | Loss: 0.00001530
Iteration 19/1000 | Loss: 0.00001514
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001506
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001495
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001495
Iteration 32/1000 | Loss: 0.00001494
Iteration 33/1000 | Loss: 0.00001494
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00001487
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001481
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001469
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001462
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001461
Iteration 57/1000 | Loss: 0.00001461
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001458
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001451
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001450
Iteration 82/1000 | Loss: 0.00001450
Iteration 83/1000 | Loss: 0.00001450
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001449
Iteration 87/1000 | Loss: 0.00001449
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001449
Iteration 91/1000 | Loss: 0.00001449
Iteration 92/1000 | Loss: 0.00001449
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001447
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001446
Iteration 107/1000 | Loss: 0.00001445
Iteration 108/1000 | Loss: 0.00001445
Iteration 109/1000 | Loss: 0.00001445
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001445
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001445
Iteration 115/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.4454903976002242e-05, 1.4454903976002242e-05, 1.4454903976002242e-05, 1.4454903976002242e-05, 1.4454903976002242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4454903976002242e-05

Optimization complete. Final v2v error: 3.304119825363159 mm

Highest mean error: 3.6333091259002686 mm for frame 98

Lowest mean error: 2.9762797355651855 mm for frame 13

Saving results

Total time: 39.295169830322266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459965
Iteration 2/25 | Loss: 0.00150998
Iteration 3/25 | Loss: 0.00137327
Iteration 4/25 | Loss: 0.00136121
Iteration 5/25 | Loss: 0.00135851
Iteration 6/25 | Loss: 0.00135813
Iteration 7/25 | Loss: 0.00135813
Iteration 8/25 | Loss: 0.00135813
Iteration 9/25 | Loss: 0.00135813
Iteration 10/25 | Loss: 0.00135813
Iteration 11/25 | Loss: 0.00135813
Iteration 12/25 | Loss: 0.00135813
Iteration 13/25 | Loss: 0.00135813
Iteration 14/25 | Loss: 0.00135813
Iteration 15/25 | Loss: 0.00135813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013581267558038235, 0.0013581267558038235, 0.0013581267558038235, 0.0013581267558038235, 0.0013581267558038235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013581267558038235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34645832
Iteration 2/25 | Loss: 0.00192505
Iteration 3/25 | Loss: 0.00192504
Iteration 4/25 | Loss: 0.00192504
Iteration 5/25 | Loss: 0.00192504
Iteration 6/25 | Loss: 0.00192504
Iteration 7/25 | Loss: 0.00192504
Iteration 8/25 | Loss: 0.00192503
Iteration 9/25 | Loss: 0.00192503
Iteration 10/25 | Loss: 0.00192503
Iteration 11/25 | Loss: 0.00192503
Iteration 12/25 | Loss: 0.00192503
Iteration 13/25 | Loss: 0.00192503
Iteration 14/25 | Loss: 0.00192503
Iteration 15/25 | Loss: 0.00192503
Iteration 16/25 | Loss: 0.00192503
Iteration 17/25 | Loss: 0.00192503
Iteration 18/25 | Loss: 0.00192503
Iteration 19/25 | Loss: 0.00192503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019250345649197698, 0.0019250345649197698, 0.0019250345649197698, 0.0019250345649197698, 0.0019250345649197698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019250345649197698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192503
Iteration 2/1000 | Loss: 0.00003021
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002127
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001931
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001803
Iteration 11/1000 | Loss: 0.00001780
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001743
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001723
Iteration 17/1000 | Loss: 0.00001710
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001689
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001676
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001670
Iteration 26/1000 | Loss: 0.00001669
Iteration 27/1000 | Loss: 0.00001669
Iteration 28/1000 | Loss: 0.00001669
Iteration 29/1000 | Loss: 0.00001669
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001665
Iteration 33/1000 | Loss: 0.00001665
Iteration 34/1000 | Loss: 0.00001665
Iteration 35/1000 | Loss: 0.00001664
Iteration 36/1000 | Loss: 0.00001662
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001661
Iteration 39/1000 | Loss: 0.00001661
Iteration 40/1000 | Loss: 0.00001661
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001661
Iteration 44/1000 | Loss: 0.00001661
Iteration 45/1000 | Loss: 0.00001661
Iteration 46/1000 | Loss: 0.00001661
Iteration 47/1000 | Loss: 0.00001660
Iteration 48/1000 | Loss: 0.00001659
Iteration 49/1000 | Loss: 0.00001659
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00001659
Iteration 52/1000 | Loss: 0.00001659
Iteration 53/1000 | Loss: 0.00001659
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001658
Iteration 57/1000 | Loss: 0.00001658
Iteration 58/1000 | Loss: 0.00001657
Iteration 59/1000 | Loss: 0.00001657
Iteration 60/1000 | Loss: 0.00001657
Iteration 61/1000 | Loss: 0.00001657
Iteration 62/1000 | Loss: 0.00001656
Iteration 63/1000 | Loss: 0.00001656
Iteration 64/1000 | Loss: 0.00001656
Iteration 65/1000 | Loss: 0.00001656
Iteration 66/1000 | Loss: 0.00001655
Iteration 67/1000 | Loss: 0.00001655
Iteration 68/1000 | Loss: 0.00001655
Iteration 69/1000 | Loss: 0.00001655
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001654
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001653
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001652
Iteration 79/1000 | Loss: 0.00001652
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001650
Iteration 85/1000 | Loss: 0.00001650
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001649
Iteration 89/1000 | Loss: 0.00001649
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001649
Iteration 94/1000 | Loss: 0.00001649
Iteration 95/1000 | Loss: 0.00001649
Iteration 96/1000 | Loss: 0.00001649
Iteration 97/1000 | Loss: 0.00001649
Iteration 98/1000 | Loss: 0.00001648
Iteration 99/1000 | Loss: 0.00001648
Iteration 100/1000 | Loss: 0.00001648
Iteration 101/1000 | Loss: 0.00001648
Iteration 102/1000 | Loss: 0.00001648
Iteration 103/1000 | Loss: 0.00001648
Iteration 104/1000 | Loss: 0.00001648
Iteration 105/1000 | Loss: 0.00001648
Iteration 106/1000 | Loss: 0.00001648
Iteration 107/1000 | Loss: 0.00001648
Iteration 108/1000 | Loss: 0.00001648
Iteration 109/1000 | Loss: 0.00001647
Iteration 110/1000 | Loss: 0.00001647
Iteration 111/1000 | Loss: 0.00001647
Iteration 112/1000 | Loss: 0.00001647
Iteration 113/1000 | Loss: 0.00001647
Iteration 114/1000 | Loss: 0.00001647
Iteration 115/1000 | Loss: 0.00001647
Iteration 116/1000 | Loss: 0.00001647
Iteration 117/1000 | Loss: 0.00001647
Iteration 118/1000 | Loss: 0.00001646
Iteration 119/1000 | Loss: 0.00001646
Iteration 120/1000 | Loss: 0.00001646
Iteration 121/1000 | Loss: 0.00001646
Iteration 122/1000 | Loss: 0.00001646
Iteration 123/1000 | Loss: 0.00001646
Iteration 124/1000 | Loss: 0.00001646
Iteration 125/1000 | Loss: 0.00001646
Iteration 126/1000 | Loss: 0.00001646
Iteration 127/1000 | Loss: 0.00001646
Iteration 128/1000 | Loss: 0.00001646
Iteration 129/1000 | Loss: 0.00001645
Iteration 130/1000 | Loss: 0.00001645
Iteration 131/1000 | Loss: 0.00001645
Iteration 132/1000 | Loss: 0.00001645
Iteration 133/1000 | Loss: 0.00001645
Iteration 134/1000 | Loss: 0.00001645
Iteration 135/1000 | Loss: 0.00001644
Iteration 136/1000 | Loss: 0.00001644
Iteration 137/1000 | Loss: 0.00001644
Iteration 138/1000 | Loss: 0.00001644
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001643
Iteration 141/1000 | Loss: 0.00001643
Iteration 142/1000 | Loss: 0.00001643
Iteration 143/1000 | Loss: 0.00001643
Iteration 144/1000 | Loss: 0.00001643
Iteration 145/1000 | Loss: 0.00001643
Iteration 146/1000 | Loss: 0.00001643
Iteration 147/1000 | Loss: 0.00001642
Iteration 148/1000 | Loss: 0.00001642
Iteration 149/1000 | Loss: 0.00001642
Iteration 150/1000 | Loss: 0.00001642
Iteration 151/1000 | Loss: 0.00001642
Iteration 152/1000 | Loss: 0.00001641
Iteration 153/1000 | Loss: 0.00001641
Iteration 154/1000 | Loss: 0.00001641
Iteration 155/1000 | Loss: 0.00001641
Iteration 156/1000 | Loss: 0.00001641
Iteration 157/1000 | Loss: 0.00001641
Iteration 158/1000 | Loss: 0.00001641
Iteration 159/1000 | Loss: 0.00001641
Iteration 160/1000 | Loss: 0.00001641
Iteration 161/1000 | Loss: 0.00001641
Iteration 162/1000 | Loss: 0.00001641
Iteration 163/1000 | Loss: 0.00001641
Iteration 164/1000 | Loss: 0.00001640
Iteration 165/1000 | Loss: 0.00001640
Iteration 166/1000 | Loss: 0.00001640
Iteration 167/1000 | Loss: 0.00001640
Iteration 168/1000 | Loss: 0.00001640
Iteration 169/1000 | Loss: 0.00001640
Iteration 170/1000 | Loss: 0.00001640
Iteration 171/1000 | Loss: 0.00001640
Iteration 172/1000 | Loss: 0.00001639
Iteration 173/1000 | Loss: 0.00001639
Iteration 174/1000 | Loss: 0.00001639
Iteration 175/1000 | Loss: 0.00001639
Iteration 176/1000 | Loss: 0.00001639
Iteration 177/1000 | Loss: 0.00001639
Iteration 178/1000 | Loss: 0.00001639
Iteration 179/1000 | Loss: 0.00001639
Iteration 180/1000 | Loss: 0.00001639
Iteration 181/1000 | Loss: 0.00001639
Iteration 182/1000 | Loss: 0.00001639
Iteration 183/1000 | Loss: 0.00001639
Iteration 184/1000 | Loss: 0.00001639
Iteration 185/1000 | Loss: 0.00001639
Iteration 186/1000 | Loss: 0.00001639
Iteration 187/1000 | Loss: 0.00001639
Iteration 188/1000 | Loss: 0.00001639
Iteration 189/1000 | Loss: 0.00001639
Iteration 190/1000 | Loss: 0.00001639
Iteration 191/1000 | Loss: 0.00001639
Iteration 192/1000 | Loss: 0.00001639
Iteration 193/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.638765024836175e-05, 1.638765024836175e-05, 1.638765024836175e-05, 1.638765024836175e-05, 1.638765024836175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.638765024836175e-05

Optimization complete. Final v2v error: 3.3748281002044678 mm

Highest mean error: 4.110450267791748 mm for frame 147

Lowest mean error: 2.6528584957122803 mm for frame 0

Saving results

Total time: 46.14795446395874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00760234
Iteration 2/25 | Loss: 0.00151361
Iteration 3/25 | Loss: 0.00137643
Iteration 4/25 | Loss: 0.00133580
Iteration 5/25 | Loss: 0.00132011
Iteration 6/25 | Loss: 0.00131143
Iteration 7/25 | Loss: 0.00131910
Iteration 8/25 | Loss: 0.00130933
Iteration 9/25 | Loss: 0.00130836
Iteration 10/25 | Loss: 0.00130817
Iteration 11/25 | Loss: 0.00131462
Iteration 12/25 | Loss: 0.00130810
Iteration 13/25 | Loss: 0.00130806
Iteration 14/25 | Loss: 0.00130806
Iteration 15/25 | Loss: 0.00130805
Iteration 16/25 | Loss: 0.00130805
Iteration 17/25 | Loss: 0.00130805
Iteration 18/25 | Loss: 0.00130805
Iteration 19/25 | Loss: 0.00130805
Iteration 20/25 | Loss: 0.00130805
Iteration 21/25 | Loss: 0.00130805
Iteration 22/25 | Loss: 0.00130805
Iteration 23/25 | Loss: 0.00130805
Iteration 24/25 | Loss: 0.00130805
Iteration 25/25 | Loss: 0.00130805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26591492
Iteration 2/25 | Loss: 0.00356698
Iteration 3/25 | Loss: 0.00356695
Iteration 4/25 | Loss: 0.00356695
Iteration 5/25 | Loss: 0.00356695
Iteration 6/25 | Loss: 0.00356695
Iteration 7/25 | Loss: 0.00356695
Iteration 8/25 | Loss: 0.00356694
Iteration 9/25 | Loss: 0.00356694
Iteration 10/25 | Loss: 0.00356694
Iteration 11/25 | Loss: 0.00356694
Iteration 12/25 | Loss: 0.00356694
Iteration 13/25 | Loss: 0.00356694
Iteration 14/25 | Loss: 0.00356694
Iteration 15/25 | Loss: 0.00356694
Iteration 16/25 | Loss: 0.00356694
Iteration 17/25 | Loss: 0.00356694
Iteration 18/25 | Loss: 0.00356694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003566944506019354, 0.003566944506019354, 0.003566944506019354, 0.003566944506019354, 0.003566944506019354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003566944506019354

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00356694
Iteration 2/1000 | Loss: 0.00007104
Iteration 3/1000 | Loss: 0.00002158
Iteration 4/1000 | Loss: 0.00001894
Iteration 5/1000 | Loss: 0.00001812
Iteration 6/1000 | Loss: 0.00001716
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001614
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001570
Iteration 11/1000 | Loss: 0.00001551
Iteration 12/1000 | Loss: 0.00001540
Iteration 13/1000 | Loss: 0.00001539
Iteration 14/1000 | Loss: 0.00001538
Iteration 15/1000 | Loss: 0.00001537
Iteration 16/1000 | Loss: 0.00001537
Iteration 17/1000 | Loss: 0.00001530
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001523
Iteration 20/1000 | Loss: 0.00001523
Iteration 21/1000 | Loss: 0.00001522
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001503
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001498
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001497
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001496
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001490
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001483
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001481
Iteration 96/1000 | Loss: 0.00001481
Iteration 97/1000 | Loss: 0.00001481
Iteration 98/1000 | Loss: 0.00001481
Iteration 99/1000 | Loss: 0.00001481
Iteration 100/1000 | Loss: 0.00001481
Iteration 101/1000 | Loss: 0.00001481
Iteration 102/1000 | Loss: 0.00001481
Iteration 103/1000 | Loss: 0.00001480
Iteration 104/1000 | Loss: 0.00001480
Iteration 105/1000 | Loss: 0.00001479
Iteration 106/1000 | Loss: 0.00001479
Iteration 107/1000 | Loss: 0.00001479
Iteration 108/1000 | Loss: 0.00001479
Iteration 109/1000 | Loss: 0.00001479
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001477
Iteration 116/1000 | Loss: 0.00001477
Iteration 117/1000 | Loss: 0.00001477
Iteration 118/1000 | Loss: 0.00001476
Iteration 119/1000 | Loss: 0.00001476
Iteration 120/1000 | Loss: 0.00001476
Iteration 121/1000 | Loss: 0.00001476
Iteration 122/1000 | Loss: 0.00001476
Iteration 123/1000 | Loss: 0.00001476
Iteration 124/1000 | Loss: 0.00001476
Iteration 125/1000 | Loss: 0.00001476
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001476
Iteration 129/1000 | Loss: 0.00001476
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001474
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001474
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001473
Iteration 141/1000 | Loss: 0.00001473
Iteration 142/1000 | Loss: 0.00001473
Iteration 143/1000 | Loss: 0.00001473
Iteration 144/1000 | Loss: 0.00001473
Iteration 145/1000 | Loss: 0.00001473
Iteration 146/1000 | Loss: 0.00001473
Iteration 147/1000 | Loss: 0.00001473
Iteration 148/1000 | Loss: 0.00001473
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001471
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001471
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001471
Iteration 164/1000 | Loss: 0.00001471
Iteration 165/1000 | Loss: 0.00001470
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001470
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001469
Iteration 171/1000 | Loss: 0.00001469
Iteration 172/1000 | Loss: 0.00001469
Iteration 173/1000 | Loss: 0.00001468
Iteration 174/1000 | Loss: 0.00001468
Iteration 175/1000 | Loss: 0.00001468
Iteration 176/1000 | Loss: 0.00001468
Iteration 177/1000 | Loss: 0.00001468
Iteration 178/1000 | Loss: 0.00001468
Iteration 179/1000 | Loss: 0.00001468
Iteration 180/1000 | Loss: 0.00001468
Iteration 181/1000 | Loss: 0.00001467
Iteration 182/1000 | Loss: 0.00001467
Iteration 183/1000 | Loss: 0.00001467
Iteration 184/1000 | Loss: 0.00001467
Iteration 185/1000 | Loss: 0.00001467
Iteration 186/1000 | Loss: 0.00001467
Iteration 187/1000 | Loss: 0.00001466
Iteration 188/1000 | Loss: 0.00001466
Iteration 189/1000 | Loss: 0.00001466
Iteration 190/1000 | Loss: 0.00001465
Iteration 191/1000 | Loss: 0.00001465
Iteration 192/1000 | Loss: 0.00001465
Iteration 193/1000 | Loss: 0.00001465
Iteration 194/1000 | Loss: 0.00001465
Iteration 195/1000 | Loss: 0.00001465
Iteration 196/1000 | Loss: 0.00001465
Iteration 197/1000 | Loss: 0.00001465
Iteration 198/1000 | Loss: 0.00001465
Iteration 199/1000 | Loss: 0.00001465
Iteration 200/1000 | Loss: 0.00001464
Iteration 201/1000 | Loss: 0.00001464
Iteration 202/1000 | Loss: 0.00001464
Iteration 203/1000 | Loss: 0.00001464
Iteration 204/1000 | Loss: 0.00001464
Iteration 205/1000 | Loss: 0.00001464
Iteration 206/1000 | Loss: 0.00001463
Iteration 207/1000 | Loss: 0.00001463
Iteration 208/1000 | Loss: 0.00001463
Iteration 209/1000 | Loss: 0.00001463
Iteration 210/1000 | Loss: 0.00001463
Iteration 211/1000 | Loss: 0.00001463
Iteration 212/1000 | Loss: 0.00001463
Iteration 213/1000 | Loss: 0.00001463
Iteration 214/1000 | Loss: 0.00001463
Iteration 215/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.4629889847128652e-05, 1.4629889847128652e-05, 1.4629889847128652e-05, 1.4629889847128652e-05, 1.4629889847128652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4629889847128652e-05

Optimization complete. Final v2v error: 3.32704496383667 mm

Highest mean error: 3.892343759536743 mm for frame 54

Lowest mean error: 2.978228807449341 mm for frame 117

Saving results

Total time: 57.34047484397888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00587666
Iteration 2/25 | Loss: 0.00185679
Iteration 3/25 | Loss: 0.00161337
Iteration 4/25 | Loss: 0.00161485
Iteration 5/25 | Loss: 0.00155168
Iteration 6/25 | Loss: 0.00150278
Iteration 7/25 | Loss: 0.00148138
Iteration 8/25 | Loss: 0.00146179
Iteration 9/25 | Loss: 0.00145864
Iteration 10/25 | Loss: 0.00145236
Iteration 11/25 | Loss: 0.00145154
Iteration 12/25 | Loss: 0.00145108
Iteration 13/25 | Loss: 0.00145188
Iteration 14/25 | Loss: 0.00145026
Iteration 15/25 | Loss: 0.00144591
Iteration 16/25 | Loss: 0.00144286
Iteration 17/25 | Loss: 0.00144250
Iteration 18/25 | Loss: 0.00144244
Iteration 19/25 | Loss: 0.00144244
Iteration 20/25 | Loss: 0.00144244
Iteration 21/25 | Loss: 0.00144244
Iteration 22/25 | Loss: 0.00144244
Iteration 23/25 | Loss: 0.00144244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014424362452700734, 0.0014424362452700734, 0.0014424362452700734, 0.0014424362452700734, 0.0014424362452700734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014424362452700734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72624493
Iteration 2/25 | Loss: 0.00241565
Iteration 3/25 | Loss: 0.00234931
Iteration 4/25 | Loss: 0.00234931
Iteration 5/25 | Loss: 0.00234931
Iteration 6/25 | Loss: 0.00234931
Iteration 7/25 | Loss: 0.00234931
Iteration 8/25 | Loss: 0.00234931
Iteration 9/25 | Loss: 0.00234931
Iteration 10/25 | Loss: 0.00234931
Iteration 11/25 | Loss: 0.00234931
Iteration 12/25 | Loss: 0.00234931
Iteration 13/25 | Loss: 0.00234931
Iteration 14/25 | Loss: 0.00234931
Iteration 15/25 | Loss: 0.00234931
Iteration 16/25 | Loss: 0.00234931
Iteration 17/25 | Loss: 0.00234931
Iteration 18/25 | Loss: 0.00234931
Iteration 19/25 | Loss: 0.00234931
Iteration 20/25 | Loss: 0.00234931
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023493091575801373, 0.0023493091575801373, 0.0023493091575801373, 0.0023493091575801373, 0.0023493091575801373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023493091575801373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234931
Iteration 2/1000 | Loss: 0.00067492
Iteration 3/1000 | Loss: 0.00051721
Iteration 4/1000 | Loss: 0.00025354
Iteration 5/1000 | Loss: 0.00009383
Iteration 6/1000 | Loss: 0.00007888
Iteration 7/1000 | Loss: 0.00005852
Iteration 8/1000 | Loss: 0.00005080
Iteration 9/1000 | Loss: 0.00006142
Iteration 10/1000 | Loss: 0.00004750
Iteration 11/1000 | Loss: 0.00004610
Iteration 12/1000 | Loss: 0.00004467
Iteration 13/1000 | Loss: 0.00004316
Iteration 14/1000 | Loss: 0.00004199
Iteration 15/1000 | Loss: 0.00004099
Iteration 16/1000 | Loss: 0.00004030
Iteration 17/1000 | Loss: 0.00003969
Iteration 18/1000 | Loss: 0.00003914
Iteration 19/1000 | Loss: 0.00003873
Iteration 20/1000 | Loss: 0.00003838
Iteration 21/1000 | Loss: 0.00010133
Iteration 22/1000 | Loss: 0.00031219
Iteration 23/1000 | Loss: 0.00004208
Iteration 24/1000 | Loss: 0.00003935
Iteration 25/1000 | Loss: 0.00003818
Iteration 26/1000 | Loss: 0.00003718
Iteration 27/1000 | Loss: 0.00003664
Iteration 28/1000 | Loss: 0.00003615
Iteration 29/1000 | Loss: 0.00003580
Iteration 30/1000 | Loss: 0.00003556
Iteration 31/1000 | Loss: 0.00003539
Iteration 32/1000 | Loss: 0.00003529
Iteration 33/1000 | Loss: 0.00003524
Iteration 34/1000 | Loss: 0.00003522
Iteration 35/1000 | Loss: 0.00006789
Iteration 36/1000 | Loss: 0.00003503
Iteration 37/1000 | Loss: 0.00003474
Iteration 38/1000 | Loss: 0.00003442
Iteration 39/1000 | Loss: 0.00015206
Iteration 40/1000 | Loss: 0.00003773
Iteration 41/1000 | Loss: 0.00003532
Iteration 42/1000 | Loss: 0.00003449
Iteration 43/1000 | Loss: 0.00003405
Iteration 44/1000 | Loss: 0.00003379
Iteration 45/1000 | Loss: 0.00003370
Iteration 46/1000 | Loss: 0.00003367
Iteration 47/1000 | Loss: 0.00003365
Iteration 48/1000 | Loss: 0.00003364
Iteration 49/1000 | Loss: 0.00003360
Iteration 50/1000 | Loss: 0.00003352
Iteration 51/1000 | Loss: 0.00003351
Iteration 52/1000 | Loss: 0.00003347
Iteration 53/1000 | Loss: 0.00003346
Iteration 54/1000 | Loss: 0.00003345
Iteration 55/1000 | Loss: 0.00003344
Iteration 56/1000 | Loss: 0.00017128
Iteration 57/1000 | Loss: 0.00051856
Iteration 58/1000 | Loss: 0.00005449
Iteration 59/1000 | Loss: 0.00042521
Iteration 60/1000 | Loss: 0.00023898
Iteration 61/1000 | Loss: 0.00003929
Iteration 62/1000 | Loss: 0.00040250
Iteration 63/1000 | Loss: 0.00004098
Iteration 64/1000 | Loss: 0.00003540
Iteration 65/1000 | Loss: 0.00003374
Iteration 66/1000 | Loss: 0.00005981
Iteration 67/1000 | Loss: 0.00003160
Iteration 68/1000 | Loss: 0.00003136
Iteration 69/1000 | Loss: 0.00003105
Iteration 70/1000 | Loss: 0.00003089
Iteration 71/1000 | Loss: 0.00003074
Iteration 72/1000 | Loss: 0.00003072
Iteration 73/1000 | Loss: 0.00003064
Iteration 74/1000 | Loss: 0.00003064
Iteration 75/1000 | Loss: 0.00003062
Iteration 76/1000 | Loss: 0.00003062
Iteration 77/1000 | Loss: 0.00003057
Iteration 78/1000 | Loss: 0.00003050
Iteration 79/1000 | Loss: 0.00003047
Iteration 80/1000 | Loss: 0.00003047
Iteration 81/1000 | Loss: 0.00003046
Iteration 82/1000 | Loss: 0.00003045
Iteration 83/1000 | Loss: 0.00003045
Iteration 84/1000 | Loss: 0.00003044
Iteration 85/1000 | Loss: 0.00003044
Iteration 86/1000 | Loss: 0.00003044
Iteration 87/1000 | Loss: 0.00003043
Iteration 88/1000 | Loss: 0.00003043
Iteration 89/1000 | Loss: 0.00003043
Iteration 90/1000 | Loss: 0.00003043
Iteration 91/1000 | Loss: 0.00003043
Iteration 92/1000 | Loss: 0.00003042
Iteration 93/1000 | Loss: 0.00003042
Iteration 94/1000 | Loss: 0.00003041
Iteration 95/1000 | Loss: 0.00003041
Iteration 96/1000 | Loss: 0.00003041
Iteration 97/1000 | Loss: 0.00003041
Iteration 98/1000 | Loss: 0.00003040
Iteration 99/1000 | Loss: 0.00003040
Iteration 100/1000 | Loss: 0.00003040
Iteration 101/1000 | Loss: 0.00003040
Iteration 102/1000 | Loss: 0.00003040
Iteration 103/1000 | Loss: 0.00003040
Iteration 104/1000 | Loss: 0.00003040
Iteration 105/1000 | Loss: 0.00003040
Iteration 106/1000 | Loss: 0.00003040
Iteration 107/1000 | Loss: 0.00003040
Iteration 108/1000 | Loss: 0.00003040
Iteration 109/1000 | Loss: 0.00003040
Iteration 110/1000 | Loss: 0.00003039
Iteration 111/1000 | Loss: 0.00003039
Iteration 112/1000 | Loss: 0.00003039
Iteration 113/1000 | Loss: 0.00003039
Iteration 114/1000 | Loss: 0.00003039
Iteration 115/1000 | Loss: 0.00003039
Iteration 116/1000 | Loss: 0.00003039
Iteration 117/1000 | Loss: 0.00003039
Iteration 118/1000 | Loss: 0.00003039
Iteration 119/1000 | Loss: 0.00003039
Iteration 120/1000 | Loss: 0.00003039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [3.0391749533009715e-05, 3.0391749533009715e-05, 3.0391749533009715e-05, 3.0391749533009715e-05, 3.0391749533009715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0391749533009715e-05

Optimization complete. Final v2v error: 3.9476966857910156 mm

Highest mean error: 11.914373397827148 mm for frame 104

Lowest mean error: 2.90910005569458 mm for frame 81

Saving results

Total time: 138.93626070022583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861870
Iteration 2/25 | Loss: 0.00150780
Iteration 3/25 | Loss: 0.00139171
Iteration 4/25 | Loss: 0.00137063
Iteration 5/25 | Loss: 0.00136413
Iteration 6/25 | Loss: 0.00136254
Iteration 7/25 | Loss: 0.00136254
Iteration 8/25 | Loss: 0.00136254
Iteration 9/25 | Loss: 0.00136254
Iteration 10/25 | Loss: 0.00136254
Iteration 11/25 | Loss: 0.00136254
Iteration 12/25 | Loss: 0.00136254
Iteration 13/25 | Loss: 0.00136254
Iteration 14/25 | Loss: 0.00136254
Iteration 15/25 | Loss: 0.00136254
Iteration 16/25 | Loss: 0.00136254
Iteration 17/25 | Loss: 0.00136254
Iteration 18/25 | Loss: 0.00136254
Iteration 19/25 | Loss: 0.00136254
Iteration 20/25 | Loss: 0.00136254
Iteration 21/25 | Loss: 0.00136254
Iteration 22/25 | Loss: 0.00136254
Iteration 23/25 | Loss: 0.00136254
Iteration 24/25 | Loss: 0.00136254
Iteration 25/25 | Loss: 0.00136254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26752508
Iteration 2/25 | Loss: 0.00200136
Iteration 3/25 | Loss: 0.00200134
Iteration 4/25 | Loss: 0.00200134
Iteration 5/25 | Loss: 0.00200134
Iteration 6/25 | Loss: 0.00200134
Iteration 7/25 | Loss: 0.00200134
Iteration 8/25 | Loss: 0.00200134
Iteration 9/25 | Loss: 0.00200134
Iteration 10/25 | Loss: 0.00200134
Iteration 11/25 | Loss: 0.00200134
Iteration 12/25 | Loss: 0.00200134
Iteration 13/25 | Loss: 0.00200134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0020013374742120504, 0.0020013374742120504, 0.0020013374742120504, 0.0020013374742120504, 0.0020013374742120504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020013374742120504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200134
Iteration 2/1000 | Loss: 0.00003926
Iteration 3/1000 | Loss: 0.00002951
Iteration 4/1000 | Loss: 0.00002493
Iteration 5/1000 | Loss: 0.00002360
Iteration 6/1000 | Loss: 0.00002274
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002075
Iteration 10/1000 | Loss: 0.00002047
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002013
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001979
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001957
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001946
Iteration 24/1000 | Loss: 0.00001940
Iteration 25/1000 | Loss: 0.00001940
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001938
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001937
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001935
Iteration 33/1000 | Loss: 0.00001935
Iteration 34/1000 | Loss: 0.00001934
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001931
Iteration 38/1000 | Loss: 0.00001931
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001926
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001925
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001925
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001913
Iteration 85/1000 | Loss: 0.00001913
Iteration 86/1000 | Loss: 0.00001913
Iteration 87/1000 | Loss: 0.00001913
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001912
Iteration 92/1000 | Loss: 0.00001912
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001911
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001909
Iteration 111/1000 | Loss: 0.00001909
Iteration 112/1000 | Loss: 0.00001909
Iteration 113/1000 | Loss: 0.00001909
Iteration 114/1000 | Loss: 0.00001909
Iteration 115/1000 | Loss: 0.00001909
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001908
Iteration 118/1000 | Loss: 0.00001908
Iteration 119/1000 | Loss: 0.00001908
Iteration 120/1000 | Loss: 0.00001908
Iteration 121/1000 | Loss: 0.00001908
Iteration 122/1000 | Loss: 0.00001907
Iteration 123/1000 | Loss: 0.00001907
Iteration 124/1000 | Loss: 0.00001907
Iteration 125/1000 | Loss: 0.00001907
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001906
Iteration 128/1000 | Loss: 0.00001906
Iteration 129/1000 | Loss: 0.00001905
Iteration 130/1000 | Loss: 0.00001905
Iteration 131/1000 | Loss: 0.00001905
Iteration 132/1000 | Loss: 0.00001905
Iteration 133/1000 | Loss: 0.00001905
Iteration 134/1000 | Loss: 0.00001905
Iteration 135/1000 | Loss: 0.00001905
Iteration 136/1000 | Loss: 0.00001905
Iteration 137/1000 | Loss: 0.00001905
Iteration 138/1000 | Loss: 0.00001904
Iteration 139/1000 | Loss: 0.00001904
Iteration 140/1000 | Loss: 0.00001904
Iteration 141/1000 | Loss: 0.00001904
Iteration 142/1000 | Loss: 0.00001904
Iteration 143/1000 | Loss: 0.00001903
Iteration 144/1000 | Loss: 0.00001903
Iteration 145/1000 | Loss: 0.00001902
Iteration 146/1000 | Loss: 0.00001902
Iteration 147/1000 | Loss: 0.00001902
Iteration 148/1000 | Loss: 0.00001902
Iteration 149/1000 | Loss: 0.00001902
Iteration 150/1000 | Loss: 0.00001902
Iteration 151/1000 | Loss: 0.00001902
Iteration 152/1000 | Loss: 0.00001902
Iteration 153/1000 | Loss: 0.00001902
Iteration 154/1000 | Loss: 0.00001902
Iteration 155/1000 | Loss: 0.00001902
Iteration 156/1000 | Loss: 0.00001902
Iteration 157/1000 | Loss: 0.00001902
Iteration 158/1000 | Loss: 0.00001902
Iteration 159/1000 | Loss: 0.00001902
Iteration 160/1000 | Loss: 0.00001902
Iteration 161/1000 | Loss: 0.00001902
Iteration 162/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.9017492377315648e-05, 1.9017492377315648e-05, 1.9017492377315648e-05, 1.9017492377315648e-05, 1.9017492377315648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9017492377315648e-05

Optimization complete. Final v2v error: 3.682706832885742 mm

Highest mean error: 5.328614711761475 mm for frame 67

Lowest mean error: 3.0669891834259033 mm for frame 98

Saving results

Total time: 41.3008668422699
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janett_posed_025/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janett_posed_025/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790198
Iteration 2/25 | Loss: 0.00204191
Iteration 3/25 | Loss: 0.00160598
Iteration 4/25 | Loss: 0.00167595
Iteration 5/25 | Loss: 0.00156398
Iteration 6/25 | Loss: 0.00152200
Iteration 7/25 | Loss: 0.00147414
Iteration 8/25 | Loss: 0.00145336
Iteration 9/25 | Loss: 0.00145556
Iteration 10/25 | Loss: 0.00144938
Iteration 11/25 | Loss: 0.00143664
Iteration 12/25 | Loss: 0.00143082
Iteration 13/25 | Loss: 0.00142690
Iteration 14/25 | Loss: 0.00143100
Iteration 15/25 | Loss: 0.00143234
Iteration 16/25 | Loss: 0.00143121
Iteration 17/25 | Loss: 0.00143260
Iteration 18/25 | Loss: 0.00143002
Iteration 19/25 | Loss: 0.00143216
Iteration 20/25 | Loss: 0.00142991
Iteration 21/25 | Loss: 0.00142330
Iteration 22/25 | Loss: 0.00142200
Iteration 23/25 | Loss: 0.00142178
Iteration 24/25 | Loss: 0.00142175
Iteration 25/25 | Loss: 0.00142175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39131045
Iteration 2/25 | Loss: 0.00209913
Iteration 3/25 | Loss: 0.00209913
Iteration 4/25 | Loss: 0.00209913
Iteration 5/25 | Loss: 0.00209913
Iteration 6/25 | Loss: 0.00209913
Iteration 7/25 | Loss: 0.00209913
Iteration 8/25 | Loss: 0.00209913
Iteration 9/25 | Loss: 0.00209913
Iteration 10/25 | Loss: 0.00209913
Iteration 11/25 | Loss: 0.00209913
Iteration 12/25 | Loss: 0.00209913
Iteration 13/25 | Loss: 0.00209913
Iteration 14/25 | Loss: 0.00209913
Iteration 15/25 | Loss: 0.00209913
Iteration 16/25 | Loss: 0.00209913
Iteration 17/25 | Loss: 0.00209913
Iteration 18/25 | Loss: 0.00209913
Iteration 19/25 | Loss: 0.00209913
Iteration 20/25 | Loss: 0.00209913
Iteration 21/25 | Loss: 0.00209913
Iteration 22/25 | Loss: 0.00209913
Iteration 23/25 | Loss: 0.00209913
Iteration 24/25 | Loss: 0.00209913
Iteration 25/25 | Loss: 0.00209913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209913
Iteration 2/1000 | Loss: 0.00006380
Iteration 3/1000 | Loss: 0.00004357
Iteration 4/1000 | Loss: 0.00003515
Iteration 5/1000 | Loss: 0.00003086
Iteration 6/1000 | Loss: 0.00002754
Iteration 7/1000 | Loss: 0.00236067
Iteration 8/1000 | Loss: 0.00003040
Iteration 9/1000 | Loss: 0.00002512
Iteration 10/1000 | Loss: 0.00002363
Iteration 11/1000 | Loss: 0.00002294
Iteration 12/1000 | Loss: 0.00002267
Iteration 13/1000 | Loss: 0.00002236
Iteration 14/1000 | Loss: 0.00002200
Iteration 15/1000 | Loss: 0.00002170
Iteration 16/1000 | Loss: 0.00002150
Iteration 17/1000 | Loss: 0.00002143
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002138
Iteration 20/1000 | Loss: 0.00002134
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002122
Iteration 23/1000 | Loss: 0.00002112
Iteration 24/1000 | Loss: 0.00002111
Iteration 25/1000 | Loss: 0.00002110
Iteration 26/1000 | Loss: 0.00002109
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002102
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002097
Iteration 32/1000 | Loss: 0.00002095
Iteration 33/1000 | Loss: 0.00002092
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002086
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002086
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002085
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002084
Iteration 46/1000 | Loss: 0.00002084
Iteration 47/1000 | Loss: 0.00002083
Iteration 48/1000 | Loss: 0.00002083
Iteration 49/1000 | Loss: 0.00002083
Iteration 50/1000 | Loss: 0.00002082
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002081
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002081
Iteration 56/1000 | Loss: 0.00002081
Iteration 57/1000 | Loss: 0.00002080
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002080
Iteration 61/1000 | Loss: 0.00002079
Iteration 62/1000 | Loss: 0.00002079
Iteration 63/1000 | Loss: 0.00002079
Iteration 64/1000 | Loss: 0.00002079
Iteration 65/1000 | Loss: 0.00002079
Iteration 66/1000 | Loss: 0.00002078
Iteration 67/1000 | Loss: 0.00002078
Iteration 68/1000 | Loss: 0.00002078
Iteration 69/1000 | Loss: 0.00002078
Iteration 70/1000 | Loss: 0.00002078
Iteration 71/1000 | Loss: 0.00002078
Iteration 72/1000 | Loss: 0.00002077
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002077
Iteration 75/1000 | Loss: 0.00002077
Iteration 76/1000 | Loss: 0.00002077
Iteration 77/1000 | Loss: 0.00002077
Iteration 78/1000 | Loss: 0.00002076
Iteration 79/1000 | Loss: 0.00002076
Iteration 80/1000 | Loss: 0.00002076
Iteration 81/1000 | Loss: 0.00002076
Iteration 82/1000 | Loss: 0.00002076
Iteration 83/1000 | Loss: 0.00002076
Iteration 84/1000 | Loss: 0.00002076
Iteration 85/1000 | Loss: 0.00002076
Iteration 86/1000 | Loss: 0.00002076
Iteration 87/1000 | Loss: 0.00002075
Iteration 88/1000 | Loss: 0.00002075
Iteration 89/1000 | Loss: 0.00002075
Iteration 90/1000 | Loss: 0.00002075
Iteration 91/1000 | Loss: 0.00002075
Iteration 92/1000 | Loss: 0.00002075
Iteration 93/1000 | Loss: 0.00002075
Iteration 94/1000 | Loss: 0.00002075
Iteration 95/1000 | Loss: 0.00002075
Iteration 96/1000 | Loss: 0.00002075
Iteration 97/1000 | Loss: 0.00002075
Iteration 98/1000 | Loss: 0.00002075
Iteration 99/1000 | Loss: 0.00002075
Iteration 100/1000 | Loss: 0.00002075
Iteration 101/1000 | Loss: 0.00002075
Iteration 102/1000 | Loss: 0.00002075
Iteration 103/1000 | Loss: 0.00002074
Iteration 104/1000 | Loss: 0.00002074
Iteration 105/1000 | Loss: 0.00002074
Iteration 106/1000 | Loss: 0.00002074
Iteration 107/1000 | Loss: 0.00002074
Iteration 108/1000 | Loss: 0.00002074
Iteration 109/1000 | Loss: 0.00002074
Iteration 110/1000 | Loss: 0.00002074
Iteration 111/1000 | Loss: 0.00002074
Iteration 112/1000 | Loss: 0.00002074
Iteration 113/1000 | Loss: 0.00002074
Iteration 114/1000 | Loss: 0.00002074
Iteration 115/1000 | Loss: 0.00002074
Iteration 116/1000 | Loss: 0.00002073
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002073
Iteration 119/1000 | Loss: 0.00002073
Iteration 120/1000 | Loss: 0.00002073
Iteration 121/1000 | Loss: 0.00002073
Iteration 122/1000 | Loss: 0.00002073
Iteration 123/1000 | Loss: 0.00002073
Iteration 124/1000 | Loss: 0.00002073
Iteration 125/1000 | Loss: 0.00002073
Iteration 126/1000 | Loss: 0.00002073
Iteration 127/1000 | Loss: 0.00002073
Iteration 128/1000 | Loss: 0.00002073
Iteration 129/1000 | Loss: 0.00002073
Iteration 130/1000 | Loss: 0.00002073
Iteration 131/1000 | Loss: 0.00002073
Iteration 132/1000 | Loss: 0.00002073
Iteration 133/1000 | Loss: 0.00002073
Iteration 134/1000 | Loss: 0.00002073
Iteration 135/1000 | Loss: 0.00002073
Iteration 136/1000 | Loss: 0.00002073
Iteration 137/1000 | Loss: 0.00002073
Iteration 138/1000 | Loss: 0.00002073
Iteration 139/1000 | Loss: 0.00002073
Iteration 140/1000 | Loss: 0.00002073
Iteration 141/1000 | Loss: 0.00002073
Iteration 142/1000 | Loss: 0.00002073
Iteration 143/1000 | Loss: 0.00002073
Iteration 144/1000 | Loss: 0.00002073
Iteration 145/1000 | Loss: 0.00002073
Iteration 146/1000 | Loss: 0.00002073
Iteration 147/1000 | Loss: 0.00002073
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002073
Iteration 150/1000 | Loss: 0.00002073
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002073
Iteration 153/1000 | Loss: 0.00002073
Iteration 154/1000 | Loss: 0.00002073
Iteration 155/1000 | Loss: 0.00002073
Iteration 156/1000 | Loss: 0.00002073
Iteration 157/1000 | Loss: 0.00002073
Iteration 158/1000 | Loss: 0.00002073
Iteration 159/1000 | Loss: 0.00002073
Iteration 160/1000 | Loss: 0.00002073
Iteration 161/1000 | Loss: 0.00002073
Iteration 162/1000 | Loss: 0.00002073
Iteration 163/1000 | Loss: 0.00002073
Iteration 164/1000 | Loss: 0.00002073
Iteration 165/1000 | Loss: 0.00002073
Iteration 166/1000 | Loss: 0.00002073
Iteration 167/1000 | Loss: 0.00002073
Iteration 168/1000 | Loss: 0.00002073
Iteration 169/1000 | Loss: 0.00002073
Iteration 170/1000 | Loss: 0.00002073
Iteration 171/1000 | Loss: 0.00002073
Iteration 172/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.0733703422592953e-05, 2.0733703422592953e-05, 2.0733703422592953e-05, 2.0733703422592953e-05, 2.0733703422592953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0733703422592953e-05

Optimization complete. Final v2v error: 3.7804789543151855 mm

Highest mean error: 4.550480842590332 mm for frame 41

Lowest mean error: 2.8906307220458984 mm for frame 2

Saving results

Total time: 76.30179238319397
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00691872
Iteration 2/25 | Loss: 0.00163648
Iteration 3/25 | Loss: 0.00127229
Iteration 4/25 | Loss: 0.00123081
Iteration 5/25 | Loss: 0.00123155
Iteration 6/25 | Loss: 0.00124506
Iteration 7/25 | Loss: 0.00123570
Iteration 8/25 | Loss: 0.00122778
Iteration 9/25 | Loss: 0.00121833
Iteration 10/25 | Loss: 0.00121198
Iteration 11/25 | Loss: 0.00121536
Iteration 12/25 | Loss: 0.00121192
Iteration 13/25 | Loss: 0.00120689
Iteration 14/25 | Loss: 0.00120623
Iteration 15/25 | Loss: 0.00120604
Iteration 16/25 | Loss: 0.00120603
Iteration 17/25 | Loss: 0.00120603
Iteration 18/25 | Loss: 0.00120603
Iteration 19/25 | Loss: 0.00120603
Iteration 20/25 | Loss: 0.00120603
Iteration 21/25 | Loss: 0.00120603
Iteration 22/25 | Loss: 0.00120603
Iteration 23/25 | Loss: 0.00120603
Iteration 24/25 | Loss: 0.00120603
Iteration 25/25 | Loss: 0.00120603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.51315403
Iteration 2/25 | Loss: 0.00134058
Iteration 3/25 | Loss: 0.00134057
Iteration 4/25 | Loss: 0.00134057
Iteration 5/25 | Loss: 0.00134057
Iteration 6/25 | Loss: 0.00134057
Iteration 7/25 | Loss: 0.00134057
Iteration 8/25 | Loss: 0.00134057
Iteration 9/25 | Loss: 0.00134057
Iteration 10/25 | Loss: 0.00134057
Iteration 11/25 | Loss: 0.00134057
Iteration 12/25 | Loss: 0.00134057
Iteration 13/25 | Loss: 0.00134057
Iteration 14/25 | Loss: 0.00134057
Iteration 15/25 | Loss: 0.00134057
Iteration 16/25 | Loss: 0.00134057
Iteration 17/25 | Loss: 0.00134057
Iteration 18/25 | Loss: 0.00134057
Iteration 19/25 | Loss: 0.00134057
Iteration 20/25 | Loss: 0.00134057
Iteration 21/25 | Loss: 0.00134057
Iteration 22/25 | Loss: 0.00134057
Iteration 23/25 | Loss: 0.00134057
Iteration 24/25 | Loss: 0.00134057
Iteration 25/25 | Loss: 0.00134057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134057
Iteration 2/1000 | Loss: 0.00002936
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001523
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001363
Iteration 7/1000 | Loss: 0.00001330
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001259
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001213
Iteration 12/1000 | Loss: 0.00001209
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001182
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001180
Iteration 22/1000 | Loss: 0.00001179
Iteration 23/1000 | Loss: 0.00001179
Iteration 24/1000 | Loss: 0.00001175
Iteration 25/1000 | Loss: 0.00001174
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001160
Iteration 47/1000 | Loss: 0.00001160
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001158
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001156
Iteration 59/1000 | Loss: 0.00001156
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001152
Iteration 76/1000 | Loss: 0.00001152
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001147
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001146
Iteration 92/1000 | Loss: 0.00001146
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001145
Iteration 95/1000 | Loss: 0.00001145
Iteration 96/1000 | Loss: 0.00001145
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001143
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001142
Iteration 110/1000 | Loss: 0.00001142
Iteration 111/1000 | Loss: 0.00001142
Iteration 112/1000 | Loss: 0.00001142
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001141
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001140
Iteration 121/1000 | Loss: 0.00001140
Iteration 122/1000 | Loss: 0.00001140
Iteration 123/1000 | Loss: 0.00001140
Iteration 124/1000 | Loss: 0.00001140
Iteration 125/1000 | Loss: 0.00001140
Iteration 126/1000 | Loss: 0.00001140
Iteration 127/1000 | Loss: 0.00001140
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Iteration 154/1000 | Loss: 0.00001139
Iteration 155/1000 | Loss: 0.00001139
Iteration 156/1000 | Loss: 0.00001139
Iteration 157/1000 | Loss: 0.00001139
Iteration 158/1000 | Loss: 0.00001139
Iteration 159/1000 | Loss: 0.00001139
Iteration 160/1000 | Loss: 0.00001139
Iteration 161/1000 | Loss: 0.00001139
Iteration 162/1000 | Loss: 0.00001139
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001139
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.138537481892854e-05, 1.138537481892854e-05, 1.138537481892854e-05, 1.138537481892854e-05, 1.138537481892854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.138537481892854e-05

Optimization complete. Final v2v error: 2.8710501194000244 mm

Highest mean error: 3.7129106521606445 mm for frame 3

Lowest mean error: 2.60988712310791 mm for frame 94

Saving results

Total time: 56.254183769226074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423527
Iteration 2/25 | Loss: 0.00130180
Iteration 3/25 | Loss: 0.00119761
Iteration 4/25 | Loss: 0.00118217
Iteration 5/25 | Loss: 0.00117856
Iteration 6/25 | Loss: 0.00117777
Iteration 7/25 | Loss: 0.00117777
Iteration 8/25 | Loss: 0.00117777
Iteration 9/25 | Loss: 0.00117777
Iteration 10/25 | Loss: 0.00117777
Iteration 11/25 | Loss: 0.00117777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011777719482779503, 0.0011777719482779503, 0.0011777719482779503, 0.0011777719482779503, 0.0011777719482779503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011777719482779503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26293099
Iteration 2/25 | Loss: 0.00203236
Iteration 3/25 | Loss: 0.00203236
Iteration 4/25 | Loss: 0.00203236
Iteration 5/25 | Loss: 0.00203236
Iteration 6/25 | Loss: 0.00203236
Iteration 7/25 | Loss: 0.00203236
Iteration 8/25 | Loss: 0.00203236
Iteration 9/25 | Loss: 0.00203236
Iteration 10/25 | Loss: 0.00203236
Iteration 11/25 | Loss: 0.00203236
Iteration 12/25 | Loss: 0.00203236
Iteration 13/25 | Loss: 0.00203235
Iteration 14/25 | Loss: 0.00203236
Iteration 15/25 | Loss: 0.00203235
Iteration 16/25 | Loss: 0.00203235
Iteration 17/25 | Loss: 0.00203236
Iteration 18/25 | Loss: 0.00203235
Iteration 19/25 | Loss: 0.00203235
Iteration 20/25 | Loss: 0.00203235
Iteration 21/25 | Loss: 0.00203235
Iteration 22/25 | Loss: 0.00203236
Iteration 23/25 | Loss: 0.00203235
Iteration 24/25 | Loss: 0.00203235
Iteration 25/25 | Loss: 0.00203235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00203235
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00001683
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001289
Iteration 8/1000 | Loss: 0.00001262
Iteration 9/1000 | Loss: 0.00001245
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001190
Iteration 14/1000 | Loss: 0.00001186
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001163
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001153
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001139
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001136
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001130
Iteration 41/1000 | Loss: 0.00001130
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001125
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001124
Iteration 62/1000 | Loss: 0.00001124
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001123
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001122
Iteration 70/1000 | Loss: 0.00001122
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001117
Iteration 89/1000 | Loss: 0.00001117
Iteration 90/1000 | Loss: 0.00001117
Iteration 91/1000 | Loss: 0.00001117
Iteration 92/1000 | Loss: 0.00001117
Iteration 93/1000 | Loss: 0.00001117
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001116
Iteration 96/1000 | Loss: 0.00001116
Iteration 97/1000 | Loss: 0.00001116
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001115
Iteration 113/1000 | Loss: 0.00001115
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001113
Iteration 118/1000 | Loss: 0.00001113
Iteration 119/1000 | Loss: 0.00001113
Iteration 120/1000 | Loss: 0.00001113
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001113
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001113
Iteration 130/1000 | Loss: 0.00001113
Iteration 131/1000 | Loss: 0.00001113
Iteration 132/1000 | Loss: 0.00001113
Iteration 133/1000 | Loss: 0.00001113
Iteration 134/1000 | Loss: 0.00001113
Iteration 135/1000 | Loss: 0.00001113
Iteration 136/1000 | Loss: 0.00001113
Iteration 137/1000 | Loss: 0.00001113
Iteration 138/1000 | Loss: 0.00001113
Iteration 139/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.1125395758426748e-05, 1.1125395758426748e-05, 1.1125395758426748e-05, 1.1125395758426748e-05, 1.1125395758426748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1125395758426748e-05

Optimization complete. Final v2v error: 2.8143017292022705 mm

Highest mean error: 3.177945613861084 mm for frame 19

Lowest mean error: 2.428633451461792 mm for frame 80

Saving results

Total time: 36.371870040893555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761024
Iteration 2/25 | Loss: 0.00138228
Iteration 3/25 | Loss: 0.00122010
Iteration 4/25 | Loss: 0.00119406
Iteration 5/25 | Loss: 0.00118886
Iteration 6/25 | Loss: 0.00118759
Iteration 7/25 | Loss: 0.00118759
Iteration 8/25 | Loss: 0.00118759
Iteration 9/25 | Loss: 0.00118759
Iteration 10/25 | Loss: 0.00118759
Iteration 11/25 | Loss: 0.00118759
Iteration 12/25 | Loss: 0.00118759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011875852942466736, 0.0011875852942466736, 0.0011875852942466736, 0.0011875852942466736, 0.0011875852942466736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011875852942466736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28471720
Iteration 2/25 | Loss: 0.00239387
Iteration 3/25 | Loss: 0.00239387
Iteration 4/25 | Loss: 0.00239387
Iteration 5/25 | Loss: 0.00239387
Iteration 6/25 | Loss: 0.00239387
Iteration 7/25 | Loss: 0.00239387
Iteration 8/25 | Loss: 0.00239387
Iteration 9/25 | Loss: 0.00239387
Iteration 10/25 | Loss: 0.00239387
Iteration 11/25 | Loss: 0.00239387
Iteration 12/25 | Loss: 0.00239387
Iteration 13/25 | Loss: 0.00239387
Iteration 14/25 | Loss: 0.00239387
Iteration 15/25 | Loss: 0.00239387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0023938650265336037, 0.0023938650265336037, 0.0023938650265336037, 0.0023938650265336037, 0.0023938650265336037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023938650265336037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239387
Iteration 2/1000 | Loss: 0.00002975
Iteration 3/1000 | Loss: 0.00001966
Iteration 4/1000 | Loss: 0.00001660
Iteration 5/1000 | Loss: 0.00001559
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001303
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001280
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001249
Iteration 17/1000 | Loss: 0.00001243
Iteration 18/1000 | Loss: 0.00001236
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001230
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001198
Iteration 40/1000 | Loss: 0.00001198
Iteration 41/1000 | Loss: 0.00001198
Iteration 42/1000 | Loss: 0.00001198
Iteration 43/1000 | Loss: 0.00001197
Iteration 44/1000 | Loss: 0.00001197
Iteration 45/1000 | Loss: 0.00001197
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001196
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001196
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001195
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001194
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001194
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001192
Iteration 69/1000 | Loss: 0.00001192
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001191
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001190
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001189
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001188
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001187
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001184
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001184
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001184
Iteration 105/1000 | Loss: 0.00001184
Iteration 106/1000 | Loss: 0.00001184
Iteration 107/1000 | Loss: 0.00001184
Iteration 108/1000 | Loss: 0.00001184
Iteration 109/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.1836439625767525e-05, 1.1836439625767525e-05, 1.1836439625767525e-05, 1.1836439625767525e-05, 1.1836439625767525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1836439625767525e-05

Optimization complete. Final v2v error: 2.962549924850464 mm

Highest mean error: 3.2874248027801514 mm for frame 16

Lowest mean error: 2.346398115158081 mm for frame 168

Saving results

Total time: 42.87395405769348
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024345
Iteration 2/25 | Loss: 0.00137739
Iteration 3/25 | Loss: 0.00121461
Iteration 4/25 | Loss: 0.00120054
Iteration 5/25 | Loss: 0.00119855
Iteration 6/25 | Loss: 0.00119824
Iteration 7/25 | Loss: 0.00119824
Iteration 8/25 | Loss: 0.00119824
Iteration 9/25 | Loss: 0.00119824
Iteration 10/25 | Loss: 0.00119824
Iteration 11/25 | Loss: 0.00119824
Iteration 12/25 | Loss: 0.00119824
Iteration 13/25 | Loss: 0.00119824
Iteration 14/25 | Loss: 0.00119824
Iteration 15/25 | Loss: 0.00119824
Iteration 16/25 | Loss: 0.00119824
Iteration 17/25 | Loss: 0.00119824
Iteration 18/25 | Loss: 0.00119824
Iteration 19/25 | Loss: 0.00119824
Iteration 20/25 | Loss: 0.00119824
Iteration 21/25 | Loss: 0.00119824
Iteration 22/25 | Loss: 0.00119824
Iteration 23/25 | Loss: 0.00119824
Iteration 24/25 | Loss: 0.00119824
Iteration 25/25 | Loss: 0.00119824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011982433497905731, 0.0011982433497905731, 0.0011982433497905731, 0.0011982433497905731, 0.0011982433497905731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011982433497905731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84673440
Iteration 2/25 | Loss: 0.00168490
Iteration 3/25 | Loss: 0.00168489
Iteration 4/25 | Loss: 0.00168489
Iteration 5/25 | Loss: 0.00168489
Iteration 6/25 | Loss: 0.00168489
Iteration 7/25 | Loss: 0.00168489
Iteration 8/25 | Loss: 0.00168489
Iteration 9/25 | Loss: 0.00168489
Iteration 10/25 | Loss: 0.00168489
Iteration 11/25 | Loss: 0.00168489
Iteration 12/25 | Loss: 0.00168489
Iteration 13/25 | Loss: 0.00168489
Iteration 14/25 | Loss: 0.00168489
Iteration 15/25 | Loss: 0.00168489
Iteration 16/25 | Loss: 0.00168489
Iteration 17/25 | Loss: 0.00168489
Iteration 18/25 | Loss: 0.00168489
Iteration 19/25 | Loss: 0.00168489
Iteration 20/25 | Loss: 0.00168489
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016848864033818245, 0.0016848864033818245, 0.0016848864033818245, 0.0016848864033818245, 0.0016848864033818245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016848864033818245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168489
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002484
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001476
Iteration 7/1000 | Loss: 0.00001431
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001332
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001288
Iteration 16/1000 | Loss: 0.00001283
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001262
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001261
Iteration 32/1000 | Loss: 0.00001261
Iteration 33/1000 | Loss: 0.00001261
Iteration 34/1000 | Loss: 0.00001260
Iteration 35/1000 | Loss: 0.00001260
Iteration 36/1000 | Loss: 0.00001260
Iteration 37/1000 | Loss: 0.00001260
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001260
Iteration 41/1000 | Loss: 0.00001258
Iteration 42/1000 | Loss: 0.00001257
Iteration 43/1000 | Loss: 0.00001257
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001253
Iteration 48/1000 | Loss: 0.00001253
Iteration 49/1000 | Loss: 0.00001253
Iteration 50/1000 | Loss: 0.00001253
Iteration 51/1000 | Loss: 0.00001252
Iteration 52/1000 | Loss: 0.00001252
Iteration 53/1000 | Loss: 0.00001251
Iteration 54/1000 | Loss: 0.00001250
Iteration 55/1000 | Loss: 0.00001250
Iteration 56/1000 | Loss: 0.00001250
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001249
Iteration 59/1000 | Loss: 0.00001249
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001248
Iteration 63/1000 | Loss: 0.00001248
Iteration 64/1000 | Loss: 0.00001248
Iteration 65/1000 | Loss: 0.00001248
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001247
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001244
Iteration 80/1000 | Loss: 0.00001244
Iteration 81/1000 | Loss: 0.00001244
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001242
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001235
Iteration 124/1000 | Loss: 0.00001235
Iteration 125/1000 | Loss: 0.00001235
Iteration 126/1000 | Loss: 0.00001235
Iteration 127/1000 | Loss: 0.00001235
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001235
Iteration 146/1000 | Loss: 0.00001235
Iteration 147/1000 | Loss: 0.00001235
Iteration 148/1000 | Loss: 0.00001235
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001235
Iteration 151/1000 | Loss: 0.00001235
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001235
Iteration 161/1000 | Loss: 0.00001235
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001235
Iteration 164/1000 | Loss: 0.00001235
Iteration 165/1000 | Loss: 0.00001235
Iteration 166/1000 | Loss: 0.00001235
Iteration 167/1000 | Loss: 0.00001235
Iteration 168/1000 | Loss: 0.00001235
Iteration 169/1000 | Loss: 0.00001235
Iteration 170/1000 | Loss: 0.00001235
Iteration 171/1000 | Loss: 0.00001235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2346047697064932e-05, 1.2346047697064932e-05, 1.2346047697064932e-05, 1.2346047697064932e-05, 1.2346047697064932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2346047697064932e-05

Optimization complete. Final v2v error: 2.9858579635620117 mm

Highest mean error: 3.4825117588043213 mm for frame 50

Lowest mean error: 2.688798427581787 mm for frame 15

Saving results

Total time: 38.9752995967865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793495
Iteration 2/25 | Loss: 0.00139217
Iteration 3/25 | Loss: 0.00115769
Iteration 4/25 | Loss: 0.00114196
Iteration 5/25 | Loss: 0.00113707
Iteration 6/25 | Loss: 0.00113527
Iteration 7/25 | Loss: 0.00113518
Iteration 8/25 | Loss: 0.00113518
Iteration 9/25 | Loss: 0.00113518
Iteration 10/25 | Loss: 0.00113518
Iteration 11/25 | Loss: 0.00113518
Iteration 12/25 | Loss: 0.00113518
Iteration 13/25 | Loss: 0.00113518
Iteration 14/25 | Loss: 0.00113518
Iteration 15/25 | Loss: 0.00113518
Iteration 16/25 | Loss: 0.00113518
Iteration 17/25 | Loss: 0.00113518
Iteration 18/25 | Loss: 0.00113518
Iteration 19/25 | Loss: 0.00113518
Iteration 20/25 | Loss: 0.00113518
Iteration 21/25 | Loss: 0.00113518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011351824505254626, 0.0011351824505254626, 0.0011351824505254626, 0.0011351824505254626, 0.0011351824505254626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011351824505254626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09289801
Iteration 2/25 | Loss: 0.00194021
Iteration 3/25 | Loss: 0.00194021
Iteration 4/25 | Loss: 0.00194021
Iteration 5/25 | Loss: 0.00194021
Iteration 6/25 | Loss: 0.00194021
Iteration 7/25 | Loss: 0.00194021
Iteration 8/25 | Loss: 0.00194021
Iteration 9/25 | Loss: 0.00194021
Iteration 10/25 | Loss: 0.00194021
Iteration 11/25 | Loss: 0.00194021
Iteration 12/25 | Loss: 0.00194021
Iteration 13/25 | Loss: 0.00194021
Iteration 14/25 | Loss: 0.00194021
Iteration 15/25 | Loss: 0.00194021
Iteration 16/25 | Loss: 0.00194021
Iteration 17/25 | Loss: 0.00194021
Iteration 18/25 | Loss: 0.00194021
Iteration 19/25 | Loss: 0.00194021
Iteration 20/25 | Loss: 0.00194021
Iteration 21/25 | Loss: 0.00194021
Iteration 22/25 | Loss: 0.00194021
Iteration 23/25 | Loss: 0.00194021
Iteration 24/25 | Loss: 0.00194021
Iteration 25/25 | Loss: 0.00194021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194021
Iteration 2/1000 | Loss: 0.00004266
Iteration 3/1000 | Loss: 0.00002550
Iteration 4/1000 | Loss: 0.00001948
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001568
Iteration 7/1000 | Loss: 0.00001470
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001357
Iteration 10/1000 | Loss: 0.00001319
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001242
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001214
Iteration 22/1000 | Loss: 0.00001213
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001190
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001189
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001180
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001179
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001178
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.171126768895192e-05, 1.171126768895192e-05, 1.171126768895192e-05, 1.171126768895192e-05, 1.171126768895192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.171126768895192e-05

Optimization complete. Final v2v error: 2.827833890914917 mm

Highest mean error: 4.12174654006958 mm for frame 69

Lowest mean error: 2.302002429962158 mm for frame 103

Saving results

Total time: 38.81699728965759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754235
Iteration 2/25 | Loss: 0.00175422
Iteration 3/25 | Loss: 0.00136109
Iteration 4/25 | Loss: 0.00127984
Iteration 5/25 | Loss: 0.00127705
Iteration 6/25 | Loss: 0.00129159
Iteration 7/25 | Loss: 0.00126608
Iteration 8/25 | Loss: 0.00123064
Iteration 9/25 | Loss: 0.00122289
Iteration 10/25 | Loss: 0.00122052
Iteration 11/25 | Loss: 0.00121485
Iteration 12/25 | Loss: 0.00120811
Iteration 13/25 | Loss: 0.00120498
Iteration 14/25 | Loss: 0.00120304
Iteration 15/25 | Loss: 0.00120267
Iteration 16/25 | Loss: 0.00120254
Iteration 17/25 | Loss: 0.00120254
Iteration 18/25 | Loss: 0.00120254
Iteration 19/25 | Loss: 0.00120254
Iteration 20/25 | Loss: 0.00120254
Iteration 21/25 | Loss: 0.00120254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012025402393192053, 0.0012025402393192053, 0.0012025402393192053, 0.0012025402393192053, 0.0012025402393192053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012025402393192053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19496107
Iteration 2/25 | Loss: 0.00222392
Iteration 3/25 | Loss: 0.00222389
Iteration 4/25 | Loss: 0.00222389
Iteration 5/25 | Loss: 0.00222389
Iteration 6/25 | Loss: 0.00222389
Iteration 7/25 | Loss: 0.00222389
Iteration 8/25 | Loss: 0.00222389
Iteration 9/25 | Loss: 0.00222389
Iteration 10/25 | Loss: 0.00222389
Iteration 11/25 | Loss: 0.00222389
Iteration 12/25 | Loss: 0.00222389
Iteration 13/25 | Loss: 0.00222389
Iteration 14/25 | Loss: 0.00222389
Iteration 15/25 | Loss: 0.00222389
Iteration 16/25 | Loss: 0.00222389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022238914389163256, 0.0022238914389163256, 0.0022238914389163256, 0.0022238914389163256, 0.0022238914389163256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022238914389163256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222389
Iteration 2/1000 | Loss: 0.00007147
Iteration 3/1000 | Loss: 0.00004974
Iteration 4/1000 | Loss: 0.00004201
Iteration 5/1000 | Loss: 0.00003898
Iteration 6/1000 | Loss: 0.00003719
Iteration 7/1000 | Loss: 0.00003605
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00003475
Iteration 10/1000 | Loss: 0.00003413
Iteration 11/1000 | Loss: 0.00003372
Iteration 12/1000 | Loss: 0.00003334
Iteration 13/1000 | Loss: 0.00003301
Iteration 14/1000 | Loss: 0.00003260
Iteration 15/1000 | Loss: 0.00003231
Iteration 16/1000 | Loss: 0.00003202
Iteration 17/1000 | Loss: 0.00068027
Iteration 18/1000 | Loss: 0.00100634
Iteration 19/1000 | Loss: 0.00019245
Iteration 20/1000 | Loss: 0.00005029
Iteration 21/1000 | Loss: 0.00004088
Iteration 22/1000 | Loss: 0.00003444
Iteration 23/1000 | Loss: 0.00036601
Iteration 24/1000 | Loss: 0.00005813
Iteration 25/1000 | Loss: 0.00033853
Iteration 26/1000 | Loss: 0.00038010
Iteration 27/1000 | Loss: 0.00004249
Iteration 28/1000 | Loss: 0.00003258
Iteration 29/1000 | Loss: 0.00002988
Iteration 30/1000 | Loss: 0.00002893
Iteration 31/1000 | Loss: 0.00002719
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002439
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002364
Iteration 37/1000 | Loss: 0.00002345
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002339
Iteration 40/1000 | Loss: 0.00002338
Iteration 41/1000 | Loss: 0.00002335
Iteration 42/1000 | Loss: 0.00002334
Iteration 43/1000 | Loss: 0.00002333
Iteration 44/1000 | Loss: 0.00002327
Iteration 45/1000 | Loss: 0.00002324
Iteration 46/1000 | Loss: 0.00002324
Iteration 47/1000 | Loss: 0.00002324
Iteration 48/1000 | Loss: 0.00002323
Iteration 49/1000 | Loss: 0.00002323
Iteration 50/1000 | Loss: 0.00002322
Iteration 51/1000 | Loss: 0.00002321
Iteration 52/1000 | Loss: 0.00002321
Iteration 53/1000 | Loss: 0.00002320
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002317
Iteration 56/1000 | Loss: 0.00002317
Iteration 57/1000 | Loss: 0.00002317
Iteration 58/1000 | Loss: 0.00002317
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002307
Iteration 62/1000 | Loss: 0.00002306
Iteration 63/1000 | Loss: 0.00002306
Iteration 64/1000 | Loss: 0.00002305
Iteration 65/1000 | Loss: 0.00002305
Iteration 66/1000 | Loss: 0.00002305
Iteration 67/1000 | Loss: 0.00002305
Iteration 68/1000 | Loss: 0.00002303
Iteration 69/1000 | Loss: 0.00002300
Iteration 70/1000 | Loss: 0.00002298
Iteration 71/1000 | Loss: 0.00002297
Iteration 72/1000 | Loss: 0.00002297
Iteration 73/1000 | Loss: 0.00002297
Iteration 74/1000 | Loss: 0.00002297
Iteration 75/1000 | Loss: 0.00002296
Iteration 76/1000 | Loss: 0.00002296
Iteration 77/1000 | Loss: 0.00002296
Iteration 78/1000 | Loss: 0.00002295
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002292
Iteration 81/1000 | Loss: 0.00002291
Iteration 82/1000 | Loss: 0.00002291
Iteration 83/1000 | Loss: 0.00002290
Iteration 84/1000 | Loss: 0.00002290
Iteration 85/1000 | Loss: 0.00002289
Iteration 86/1000 | Loss: 0.00002289
Iteration 87/1000 | Loss: 0.00002289
Iteration 88/1000 | Loss: 0.00002289
Iteration 89/1000 | Loss: 0.00002288
Iteration 90/1000 | Loss: 0.00002288
Iteration 91/1000 | Loss: 0.00002288
Iteration 92/1000 | Loss: 0.00002288
Iteration 93/1000 | Loss: 0.00002287
Iteration 94/1000 | Loss: 0.00002287
Iteration 95/1000 | Loss: 0.00002287
Iteration 96/1000 | Loss: 0.00002286
Iteration 97/1000 | Loss: 0.00002286
Iteration 98/1000 | Loss: 0.00002286
Iteration 99/1000 | Loss: 0.00002286
Iteration 100/1000 | Loss: 0.00002286
Iteration 101/1000 | Loss: 0.00002286
Iteration 102/1000 | Loss: 0.00002285
Iteration 103/1000 | Loss: 0.00002285
Iteration 104/1000 | Loss: 0.00002285
Iteration 105/1000 | Loss: 0.00002285
Iteration 106/1000 | Loss: 0.00002284
Iteration 107/1000 | Loss: 0.00002284
Iteration 108/1000 | Loss: 0.00002284
Iteration 109/1000 | Loss: 0.00002284
Iteration 110/1000 | Loss: 0.00002284
Iteration 111/1000 | Loss: 0.00002284
Iteration 112/1000 | Loss: 0.00002284
Iteration 113/1000 | Loss: 0.00002283
Iteration 114/1000 | Loss: 0.00002283
Iteration 115/1000 | Loss: 0.00002283
Iteration 116/1000 | Loss: 0.00002283
Iteration 117/1000 | Loss: 0.00002283
Iteration 118/1000 | Loss: 0.00002283
Iteration 119/1000 | Loss: 0.00002283
Iteration 120/1000 | Loss: 0.00002282
Iteration 121/1000 | Loss: 0.00002282
Iteration 122/1000 | Loss: 0.00002281
Iteration 123/1000 | Loss: 0.00002281
Iteration 124/1000 | Loss: 0.00002281
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Iteration 127/1000 | Loss: 0.00002280
Iteration 128/1000 | Loss: 0.00002280
Iteration 129/1000 | Loss: 0.00002280
Iteration 130/1000 | Loss: 0.00002280
Iteration 131/1000 | Loss: 0.00002280
Iteration 132/1000 | Loss: 0.00002280
Iteration 133/1000 | Loss: 0.00002280
Iteration 134/1000 | Loss: 0.00002280
Iteration 135/1000 | Loss: 0.00002279
Iteration 136/1000 | Loss: 0.00002279
Iteration 137/1000 | Loss: 0.00002279
Iteration 138/1000 | Loss: 0.00002279
Iteration 139/1000 | Loss: 0.00002279
Iteration 140/1000 | Loss: 0.00002279
Iteration 141/1000 | Loss: 0.00002279
Iteration 142/1000 | Loss: 0.00002279
Iteration 143/1000 | Loss: 0.00002279
Iteration 144/1000 | Loss: 0.00002279
Iteration 145/1000 | Loss: 0.00002279
Iteration 146/1000 | Loss: 0.00002278
Iteration 147/1000 | Loss: 0.00002278
Iteration 148/1000 | Loss: 0.00002278
Iteration 149/1000 | Loss: 0.00002278
Iteration 150/1000 | Loss: 0.00002278
Iteration 151/1000 | Loss: 0.00002278
Iteration 152/1000 | Loss: 0.00002277
Iteration 153/1000 | Loss: 0.00002277
Iteration 154/1000 | Loss: 0.00002277
Iteration 155/1000 | Loss: 0.00002277
Iteration 156/1000 | Loss: 0.00002277
Iteration 157/1000 | Loss: 0.00002277
Iteration 158/1000 | Loss: 0.00002277
Iteration 159/1000 | Loss: 0.00002277
Iteration 160/1000 | Loss: 0.00002277
Iteration 161/1000 | Loss: 0.00002277
Iteration 162/1000 | Loss: 0.00002277
Iteration 163/1000 | Loss: 0.00002276
Iteration 164/1000 | Loss: 0.00002276
Iteration 165/1000 | Loss: 0.00002276
Iteration 166/1000 | Loss: 0.00002276
Iteration 167/1000 | Loss: 0.00002276
Iteration 168/1000 | Loss: 0.00002276
Iteration 169/1000 | Loss: 0.00002276
Iteration 170/1000 | Loss: 0.00002275
Iteration 171/1000 | Loss: 0.00002275
Iteration 172/1000 | Loss: 0.00002275
Iteration 173/1000 | Loss: 0.00002275
Iteration 174/1000 | Loss: 0.00002275
Iteration 175/1000 | Loss: 0.00002275
Iteration 176/1000 | Loss: 0.00002275
Iteration 177/1000 | Loss: 0.00002275
Iteration 178/1000 | Loss: 0.00002275
Iteration 179/1000 | Loss: 0.00002275
Iteration 180/1000 | Loss: 0.00002275
Iteration 181/1000 | Loss: 0.00002275
Iteration 182/1000 | Loss: 0.00002275
Iteration 183/1000 | Loss: 0.00002275
Iteration 184/1000 | Loss: 0.00002275
Iteration 185/1000 | Loss: 0.00002275
Iteration 186/1000 | Loss: 0.00002275
Iteration 187/1000 | Loss: 0.00002275
Iteration 188/1000 | Loss: 0.00002275
Iteration 189/1000 | Loss: 0.00002275
Iteration 190/1000 | Loss: 0.00002275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.2745580281480215e-05, 2.2745580281480215e-05, 2.2745580281480215e-05, 2.2745580281480215e-05, 2.2745580281480215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2745580281480215e-05

Optimization complete. Final v2v error: 3.4594204425811768 mm

Highest mean error: 11.290678024291992 mm for frame 21

Lowest mean error: 2.4736995697021484 mm for frame 237

Saving results

Total time: 108.01687049865723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459424
Iteration 2/25 | Loss: 0.00130338
Iteration 3/25 | Loss: 0.00117413
Iteration 4/25 | Loss: 0.00114822
Iteration 5/25 | Loss: 0.00113975
Iteration 6/25 | Loss: 0.00113776
Iteration 7/25 | Loss: 0.00113776
Iteration 8/25 | Loss: 0.00113776
Iteration 9/25 | Loss: 0.00113776
Iteration 10/25 | Loss: 0.00113776
Iteration 11/25 | Loss: 0.00113776
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011377575574442744, 0.0011377575574442744, 0.0011377575574442744, 0.0011377575574442744, 0.0011377575574442744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011377575574442744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16021061
Iteration 2/25 | Loss: 0.00278119
Iteration 3/25 | Loss: 0.00278119
Iteration 4/25 | Loss: 0.00278118
Iteration 5/25 | Loss: 0.00278118
Iteration 6/25 | Loss: 0.00278118
Iteration 7/25 | Loss: 0.00278118
Iteration 8/25 | Loss: 0.00278118
Iteration 9/25 | Loss: 0.00278118
Iteration 10/25 | Loss: 0.00278118
Iteration 11/25 | Loss: 0.00278118
Iteration 12/25 | Loss: 0.00278118
Iteration 13/25 | Loss: 0.00278118
Iteration 14/25 | Loss: 0.00278118
Iteration 15/25 | Loss: 0.00278118
Iteration 16/25 | Loss: 0.00278118
Iteration 17/25 | Loss: 0.00278118
Iteration 18/25 | Loss: 0.00278118
Iteration 19/25 | Loss: 0.00278118
Iteration 20/25 | Loss: 0.00278118
Iteration 21/25 | Loss: 0.00278118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002781182061880827, 0.002781182061880827, 0.002781182061880827, 0.002781182061880827, 0.002781182061880827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002781182061880827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278118
Iteration 2/1000 | Loss: 0.00005688
Iteration 3/1000 | Loss: 0.00003734
Iteration 4/1000 | Loss: 0.00002578
Iteration 5/1000 | Loss: 0.00002404
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002120
Iteration 9/1000 | Loss: 0.00002094
Iteration 10/1000 | Loss: 0.00002089
Iteration 11/1000 | Loss: 0.00002064
Iteration 12/1000 | Loss: 0.00002042
Iteration 13/1000 | Loss: 0.00002028
Iteration 14/1000 | Loss: 0.00002025
Iteration 15/1000 | Loss: 0.00002021
Iteration 16/1000 | Loss: 0.00002018
Iteration 17/1000 | Loss: 0.00002006
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00002000
Iteration 20/1000 | Loss: 0.00001996
Iteration 21/1000 | Loss: 0.00001990
Iteration 22/1000 | Loss: 0.00001979
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001968
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001966
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001964
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00001961
Iteration 35/1000 | Loss: 0.00001961
Iteration 36/1000 | Loss: 0.00001960
Iteration 37/1000 | Loss: 0.00001960
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001959
Iteration 40/1000 | Loss: 0.00001959
Iteration 41/1000 | Loss: 0.00001958
Iteration 42/1000 | Loss: 0.00001957
Iteration 43/1000 | Loss: 0.00001957
Iteration 44/1000 | Loss: 0.00001957
Iteration 45/1000 | Loss: 0.00001956
Iteration 46/1000 | Loss: 0.00001956
Iteration 47/1000 | Loss: 0.00001955
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001953
Iteration 52/1000 | Loss: 0.00001953
Iteration 53/1000 | Loss: 0.00001953
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001949
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001949
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001948
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001946
Iteration 72/1000 | Loss: 0.00001946
Iteration 73/1000 | Loss: 0.00001946
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001944
Iteration 78/1000 | Loss: 0.00001944
Iteration 79/1000 | Loss: 0.00001944
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001944
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001943
Iteration 86/1000 | Loss: 0.00001943
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001943
Iteration 90/1000 | Loss: 0.00001943
Iteration 91/1000 | Loss: 0.00001943
Iteration 92/1000 | Loss: 0.00001943
Iteration 93/1000 | Loss: 0.00001943
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001942
Iteration 104/1000 | Loss: 0.00001942
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001941
Iteration 114/1000 | Loss: 0.00001941
Iteration 115/1000 | Loss: 0.00001941
Iteration 116/1000 | Loss: 0.00001941
Iteration 117/1000 | Loss: 0.00001941
Iteration 118/1000 | Loss: 0.00001941
Iteration 119/1000 | Loss: 0.00001941
Iteration 120/1000 | Loss: 0.00001941
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001940
Iteration 127/1000 | Loss: 0.00001940
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Iteration 132/1000 | Loss: 0.00001940
Iteration 133/1000 | Loss: 0.00001940
Iteration 134/1000 | Loss: 0.00001940
Iteration 135/1000 | Loss: 0.00001940
Iteration 136/1000 | Loss: 0.00001940
Iteration 137/1000 | Loss: 0.00001940
Iteration 138/1000 | Loss: 0.00001940
Iteration 139/1000 | Loss: 0.00001940
Iteration 140/1000 | Loss: 0.00001940
Iteration 141/1000 | Loss: 0.00001940
Iteration 142/1000 | Loss: 0.00001940
Iteration 143/1000 | Loss: 0.00001940
Iteration 144/1000 | Loss: 0.00001940
Iteration 145/1000 | Loss: 0.00001940
Iteration 146/1000 | Loss: 0.00001940
Iteration 147/1000 | Loss: 0.00001940
Iteration 148/1000 | Loss: 0.00001940
Iteration 149/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.939893809321802e-05, 1.939893809321802e-05, 1.939893809321802e-05, 1.939893809321802e-05, 1.939893809321802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.939893809321802e-05

Optimization complete. Final v2v error: 3.5732994079589844 mm

Highest mean error: 4.181142807006836 mm for frame 175

Lowest mean error: 3.259767532348633 mm for frame 166

Saving results

Total time: 41.40586543083191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837144
Iteration 2/25 | Loss: 0.00188433
Iteration 3/25 | Loss: 0.00141154
Iteration 4/25 | Loss: 0.00129011
Iteration 5/25 | Loss: 0.00127290
Iteration 6/25 | Loss: 0.00127044
Iteration 7/25 | Loss: 0.00127044
Iteration 8/25 | Loss: 0.00127044
Iteration 9/25 | Loss: 0.00127044
Iteration 10/25 | Loss: 0.00127044
Iteration 11/25 | Loss: 0.00127044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012704443652182817, 0.0012704443652182817, 0.0012704443652182817, 0.0012704443652182817, 0.0012704443652182817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012704443652182817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21699345
Iteration 2/25 | Loss: 0.00187325
Iteration 3/25 | Loss: 0.00187323
Iteration 4/25 | Loss: 0.00187323
Iteration 5/25 | Loss: 0.00187323
Iteration 6/25 | Loss: 0.00187322
Iteration 7/25 | Loss: 0.00187322
Iteration 8/25 | Loss: 0.00187322
Iteration 9/25 | Loss: 0.00187322
Iteration 10/25 | Loss: 0.00187322
Iteration 11/25 | Loss: 0.00187322
Iteration 12/25 | Loss: 0.00187322
Iteration 13/25 | Loss: 0.00187322
Iteration 14/25 | Loss: 0.00187322
Iteration 15/25 | Loss: 0.00187322
Iteration 16/25 | Loss: 0.00187322
Iteration 17/25 | Loss: 0.00187322
Iteration 18/25 | Loss: 0.00187322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001873223576694727, 0.001873223576694727, 0.001873223576694727, 0.001873223576694727, 0.001873223576694727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001873223576694727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187322
Iteration 2/1000 | Loss: 0.00005755
Iteration 3/1000 | Loss: 0.00003653
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002724
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002575
Iteration 8/1000 | Loss: 0.00002533
Iteration 9/1000 | Loss: 0.00002499
Iteration 10/1000 | Loss: 0.00002461
Iteration 11/1000 | Loss: 0.00002433
Iteration 12/1000 | Loss: 0.00002424
Iteration 13/1000 | Loss: 0.00002399
Iteration 14/1000 | Loss: 0.00002392
Iteration 15/1000 | Loss: 0.00002378
Iteration 16/1000 | Loss: 0.00002372
Iteration 17/1000 | Loss: 0.00002367
Iteration 18/1000 | Loss: 0.00002366
Iteration 19/1000 | Loss: 0.00002366
Iteration 20/1000 | Loss: 0.00002365
Iteration 21/1000 | Loss: 0.00002365
Iteration 22/1000 | Loss: 0.00002365
Iteration 23/1000 | Loss: 0.00002364
Iteration 24/1000 | Loss: 0.00002364
Iteration 25/1000 | Loss: 0.00002364
Iteration 26/1000 | Loss: 0.00002363
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002362
Iteration 29/1000 | Loss: 0.00002362
Iteration 30/1000 | Loss: 0.00002361
Iteration 31/1000 | Loss: 0.00002361
Iteration 32/1000 | Loss: 0.00002361
Iteration 33/1000 | Loss: 0.00002358
Iteration 34/1000 | Loss: 0.00002357
Iteration 35/1000 | Loss: 0.00002357
Iteration 36/1000 | Loss: 0.00002356
Iteration 37/1000 | Loss: 0.00002355
Iteration 38/1000 | Loss: 0.00002355
Iteration 39/1000 | Loss: 0.00002354
Iteration 40/1000 | Loss: 0.00002354
Iteration 41/1000 | Loss: 0.00002354
Iteration 42/1000 | Loss: 0.00002354
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002354
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002354
Iteration 47/1000 | Loss: 0.00002354
Iteration 48/1000 | Loss: 0.00002354
Iteration 49/1000 | Loss: 0.00002353
Iteration 50/1000 | Loss: 0.00002353
Iteration 51/1000 | Loss: 0.00002353
Iteration 52/1000 | Loss: 0.00002353
Iteration 53/1000 | Loss: 0.00002353
Iteration 54/1000 | Loss: 0.00002353
Iteration 55/1000 | Loss: 0.00002353
Iteration 56/1000 | Loss: 0.00002353
Iteration 57/1000 | Loss: 0.00002353
Iteration 58/1000 | Loss: 0.00002353
Iteration 59/1000 | Loss: 0.00002352
Iteration 60/1000 | Loss: 0.00002352
Iteration 61/1000 | Loss: 0.00002352
Iteration 62/1000 | Loss: 0.00002352
Iteration 63/1000 | Loss: 0.00002352
Iteration 64/1000 | Loss: 0.00002352
Iteration 65/1000 | Loss: 0.00002352
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00002352
Iteration 68/1000 | Loss: 0.00002352
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002351
Iteration 72/1000 | Loss: 0.00002351
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002351
Iteration 75/1000 | Loss: 0.00002351
Iteration 76/1000 | Loss: 0.00002351
Iteration 77/1000 | Loss: 0.00002351
Iteration 78/1000 | Loss: 0.00002351
Iteration 79/1000 | Loss: 0.00002351
Iteration 80/1000 | Loss: 0.00002351
Iteration 81/1000 | Loss: 0.00002351
Iteration 82/1000 | Loss: 0.00002350
Iteration 83/1000 | Loss: 0.00002350
Iteration 84/1000 | Loss: 0.00002350
Iteration 85/1000 | Loss: 0.00002350
Iteration 86/1000 | Loss: 0.00002350
Iteration 87/1000 | Loss: 0.00002350
Iteration 88/1000 | Loss: 0.00002350
Iteration 89/1000 | Loss: 0.00002350
Iteration 90/1000 | Loss: 0.00002350
Iteration 91/1000 | Loss: 0.00002349
Iteration 92/1000 | Loss: 0.00002349
Iteration 93/1000 | Loss: 0.00002349
Iteration 94/1000 | Loss: 0.00002349
Iteration 95/1000 | Loss: 0.00002349
Iteration 96/1000 | Loss: 0.00002349
Iteration 97/1000 | Loss: 0.00002349
Iteration 98/1000 | Loss: 0.00002349
Iteration 99/1000 | Loss: 0.00002348
Iteration 100/1000 | Loss: 0.00002348
Iteration 101/1000 | Loss: 0.00002348
Iteration 102/1000 | Loss: 0.00002348
Iteration 103/1000 | Loss: 0.00002347
Iteration 104/1000 | Loss: 0.00002347
Iteration 105/1000 | Loss: 0.00002347
Iteration 106/1000 | Loss: 0.00002347
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002347
Iteration 114/1000 | Loss: 0.00002347
Iteration 115/1000 | Loss: 0.00002347
Iteration 116/1000 | Loss: 0.00002347
Iteration 117/1000 | Loss: 0.00002347
Iteration 118/1000 | Loss: 0.00002347
Iteration 119/1000 | Loss: 0.00002347
Iteration 120/1000 | Loss: 0.00002347
Iteration 121/1000 | Loss: 0.00002347
Iteration 122/1000 | Loss: 0.00002347
Iteration 123/1000 | Loss: 0.00002347
Iteration 124/1000 | Loss: 0.00002347
Iteration 125/1000 | Loss: 0.00002347
Iteration 126/1000 | Loss: 0.00002347
Iteration 127/1000 | Loss: 0.00002347
Iteration 128/1000 | Loss: 0.00002347
Iteration 129/1000 | Loss: 0.00002347
Iteration 130/1000 | Loss: 0.00002347
Iteration 131/1000 | Loss: 0.00002347
Iteration 132/1000 | Loss: 0.00002347
Iteration 133/1000 | Loss: 0.00002347
Iteration 134/1000 | Loss: 0.00002347
Iteration 135/1000 | Loss: 0.00002347
Iteration 136/1000 | Loss: 0.00002347
Iteration 137/1000 | Loss: 0.00002347
Iteration 138/1000 | Loss: 0.00002347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.3467047867598012e-05, 2.3467047867598012e-05, 2.3467047867598012e-05, 2.3467047867598012e-05, 2.3467047867598012e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3467047867598012e-05

Optimization complete. Final v2v error: 4.1052422523498535 mm

Highest mean error: 4.717404842376709 mm for frame 29

Lowest mean error: 3.3369829654693604 mm for frame 127

Saving results

Total time: 37.71225166320801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919382
Iteration 2/25 | Loss: 0.00141224
Iteration 3/25 | Loss: 0.00123203
Iteration 4/25 | Loss: 0.00120448
Iteration 5/25 | Loss: 0.00119757
Iteration 6/25 | Loss: 0.00119557
Iteration 7/25 | Loss: 0.00119557
Iteration 8/25 | Loss: 0.00119557
Iteration 9/25 | Loss: 0.00119557
Iteration 10/25 | Loss: 0.00119557
Iteration 11/25 | Loss: 0.00119557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011955683585256338, 0.0011955683585256338, 0.0011955683585256338, 0.0011955683585256338, 0.0011955683585256338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011955683585256338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35030222
Iteration 2/25 | Loss: 0.00253043
Iteration 3/25 | Loss: 0.00253042
Iteration 4/25 | Loss: 0.00253042
Iteration 5/25 | Loss: 0.00253042
Iteration 6/25 | Loss: 0.00253042
Iteration 7/25 | Loss: 0.00253042
Iteration 8/25 | Loss: 0.00253042
Iteration 9/25 | Loss: 0.00253042
Iteration 10/25 | Loss: 0.00253042
Iteration 11/25 | Loss: 0.00253042
Iteration 12/25 | Loss: 0.00253042
Iteration 13/25 | Loss: 0.00253042
Iteration 14/25 | Loss: 0.00253042
Iteration 15/25 | Loss: 0.00253042
Iteration 16/25 | Loss: 0.00253042
Iteration 17/25 | Loss: 0.00253042
Iteration 18/25 | Loss: 0.00253042
Iteration 19/25 | Loss: 0.00253042
Iteration 20/25 | Loss: 0.00253042
Iteration 21/25 | Loss: 0.00253042
Iteration 22/25 | Loss: 0.00253042
Iteration 23/25 | Loss: 0.00253042
Iteration 24/25 | Loss: 0.00253042
Iteration 25/25 | Loss: 0.00253042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253042
Iteration 2/1000 | Loss: 0.00004594
Iteration 3/1000 | Loss: 0.00002832
Iteration 4/1000 | Loss: 0.00002037
Iteration 5/1000 | Loss: 0.00001879
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001681
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001629
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001577
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001553
Iteration 20/1000 | Loss: 0.00001552
Iteration 21/1000 | Loss: 0.00001551
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001537
Iteration 24/1000 | Loss: 0.00001537
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001531
Iteration 28/1000 | Loss: 0.00001531
Iteration 29/1000 | Loss: 0.00001530
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001523
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001521
Iteration 41/1000 | Loss: 0.00001517
Iteration 42/1000 | Loss: 0.00001517
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001513
Iteration 48/1000 | Loss: 0.00001513
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001510
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001508
Iteration 56/1000 | Loss: 0.00001508
Iteration 57/1000 | Loss: 0.00001508
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001506
Iteration 60/1000 | Loss: 0.00001506
Iteration 61/1000 | Loss: 0.00001505
Iteration 62/1000 | Loss: 0.00001504
Iteration 63/1000 | Loss: 0.00001504
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001503
Iteration 66/1000 | Loss: 0.00001503
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001503
Iteration 71/1000 | Loss: 0.00001503
Iteration 72/1000 | Loss: 0.00001503
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001503
Iteration 76/1000 | Loss: 0.00001503
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001496
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001495
Iteration 88/1000 | Loss: 0.00001495
Iteration 89/1000 | Loss: 0.00001495
Iteration 90/1000 | Loss: 0.00001494
Iteration 91/1000 | Loss: 0.00001493
Iteration 92/1000 | Loss: 0.00001493
Iteration 93/1000 | Loss: 0.00001493
Iteration 94/1000 | Loss: 0.00001493
Iteration 95/1000 | Loss: 0.00001492
Iteration 96/1000 | Loss: 0.00001492
Iteration 97/1000 | Loss: 0.00001492
Iteration 98/1000 | Loss: 0.00001490
Iteration 99/1000 | Loss: 0.00001489
Iteration 100/1000 | Loss: 0.00001489
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001483
Iteration 110/1000 | Loss: 0.00001483
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001482
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001480
Iteration 117/1000 | Loss: 0.00001479
Iteration 118/1000 | Loss: 0.00001479
Iteration 119/1000 | Loss: 0.00001479
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001477
Iteration 125/1000 | Loss: 0.00001476
Iteration 126/1000 | Loss: 0.00001476
Iteration 127/1000 | Loss: 0.00001476
Iteration 128/1000 | Loss: 0.00001475
Iteration 129/1000 | Loss: 0.00001475
Iteration 130/1000 | Loss: 0.00001475
Iteration 131/1000 | Loss: 0.00001475
Iteration 132/1000 | Loss: 0.00001474
Iteration 133/1000 | Loss: 0.00001474
Iteration 134/1000 | Loss: 0.00001474
Iteration 135/1000 | Loss: 0.00001474
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001472
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001470
Iteration 149/1000 | Loss: 0.00001470
Iteration 150/1000 | Loss: 0.00001469
Iteration 151/1000 | Loss: 0.00001469
Iteration 152/1000 | Loss: 0.00001469
Iteration 153/1000 | Loss: 0.00001469
Iteration 154/1000 | Loss: 0.00001469
Iteration 155/1000 | Loss: 0.00001469
Iteration 156/1000 | Loss: 0.00001469
Iteration 157/1000 | Loss: 0.00001469
Iteration 158/1000 | Loss: 0.00001469
Iteration 159/1000 | Loss: 0.00001469
Iteration 160/1000 | Loss: 0.00001468
Iteration 161/1000 | Loss: 0.00001468
Iteration 162/1000 | Loss: 0.00001468
Iteration 163/1000 | Loss: 0.00001468
Iteration 164/1000 | Loss: 0.00001468
Iteration 165/1000 | Loss: 0.00001467
Iteration 166/1000 | Loss: 0.00001467
Iteration 167/1000 | Loss: 0.00001467
Iteration 168/1000 | Loss: 0.00001467
Iteration 169/1000 | Loss: 0.00001467
Iteration 170/1000 | Loss: 0.00001467
Iteration 171/1000 | Loss: 0.00001467
Iteration 172/1000 | Loss: 0.00001467
Iteration 173/1000 | Loss: 0.00001467
Iteration 174/1000 | Loss: 0.00001467
Iteration 175/1000 | Loss: 0.00001467
Iteration 176/1000 | Loss: 0.00001467
Iteration 177/1000 | Loss: 0.00001467
Iteration 178/1000 | Loss: 0.00001466
Iteration 179/1000 | Loss: 0.00001466
Iteration 180/1000 | Loss: 0.00001466
Iteration 181/1000 | Loss: 0.00001466
Iteration 182/1000 | Loss: 0.00001466
Iteration 183/1000 | Loss: 0.00001466
Iteration 184/1000 | Loss: 0.00001466
Iteration 185/1000 | Loss: 0.00001466
Iteration 186/1000 | Loss: 0.00001466
Iteration 187/1000 | Loss: 0.00001466
Iteration 188/1000 | Loss: 0.00001465
Iteration 189/1000 | Loss: 0.00001465
Iteration 190/1000 | Loss: 0.00001465
Iteration 191/1000 | Loss: 0.00001465
Iteration 192/1000 | Loss: 0.00001465
Iteration 193/1000 | Loss: 0.00001465
Iteration 194/1000 | Loss: 0.00001465
Iteration 195/1000 | Loss: 0.00001465
Iteration 196/1000 | Loss: 0.00001465
Iteration 197/1000 | Loss: 0.00001464
Iteration 198/1000 | Loss: 0.00001464
Iteration 199/1000 | Loss: 0.00001464
Iteration 200/1000 | Loss: 0.00001464
Iteration 201/1000 | Loss: 0.00001464
Iteration 202/1000 | Loss: 0.00001464
Iteration 203/1000 | Loss: 0.00001464
Iteration 204/1000 | Loss: 0.00001464
Iteration 205/1000 | Loss: 0.00001464
Iteration 206/1000 | Loss: 0.00001463
Iteration 207/1000 | Loss: 0.00001463
Iteration 208/1000 | Loss: 0.00001463
Iteration 209/1000 | Loss: 0.00001463
Iteration 210/1000 | Loss: 0.00001463
Iteration 211/1000 | Loss: 0.00001463
Iteration 212/1000 | Loss: 0.00001463
Iteration 213/1000 | Loss: 0.00001463
Iteration 214/1000 | Loss: 0.00001463
Iteration 215/1000 | Loss: 0.00001463
Iteration 216/1000 | Loss: 0.00001463
Iteration 217/1000 | Loss: 0.00001463
Iteration 218/1000 | Loss: 0.00001463
Iteration 219/1000 | Loss: 0.00001463
Iteration 220/1000 | Loss: 0.00001463
Iteration 221/1000 | Loss: 0.00001463
Iteration 222/1000 | Loss: 0.00001463
Iteration 223/1000 | Loss: 0.00001463
Iteration 224/1000 | Loss: 0.00001463
Iteration 225/1000 | Loss: 0.00001463
Iteration 226/1000 | Loss: 0.00001463
Iteration 227/1000 | Loss: 0.00001463
Iteration 228/1000 | Loss: 0.00001463
Iteration 229/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.4625002222601324e-05, 1.4625002222601324e-05, 1.4625002222601324e-05, 1.4625002222601324e-05, 1.4625002222601324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4625002222601324e-05

Optimization complete. Final v2v error: 3.2510428428649902 mm

Highest mean error: 3.5655884742736816 mm for frame 168

Lowest mean error: 2.833268165588379 mm for frame 0

Saving results

Total time: 47.65372371673584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789445
Iteration 2/25 | Loss: 0.00135552
Iteration 3/25 | Loss: 0.00120001
Iteration 4/25 | Loss: 0.00118002
Iteration 5/25 | Loss: 0.00117660
Iteration 6/25 | Loss: 0.00117576
Iteration 7/25 | Loss: 0.00117576
Iteration 8/25 | Loss: 0.00117576
Iteration 9/25 | Loss: 0.00117576
Iteration 10/25 | Loss: 0.00117576
Iteration 11/25 | Loss: 0.00117576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001175756100565195, 0.001175756100565195, 0.001175756100565195, 0.001175756100565195, 0.001175756100565195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175756100565195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23143137
Iteration 2/25 | Loss: 0.00197349
Iteration 3/25 | Loss: 0.00197349
Iteration 4/25 | Loss: 0.00197349
Iteration 5/25 | Loss: 0.00197349
Iteration 6/25 | Loss: 0.00197349
Iteration 7/25 | Loss: 0.00197349
Iteration 8/25 | Loss: 0.00197349
Iteration 9/25 | Loss: 0.00197349
Iteration 10/25 | Loss: 0.00197349
Iteration 11/25 | Loss: 0.00197349
Iteration 12/25 | Loss: 0.00197349
Iteration 13/25 | Loss: 0.00197349
Iteration 14/25 | Loss: 0.00197349
Iteration 15/25 | Loss: 0.00197349
Iteration 16/25 | Loss: 0.00197349
Iteration 17/25 | Loss: 0.00197349
Iteration 18/25 | Loss: 0.00197349
Iteration 19/25 | Loss: 0.00197349
Iteration 20/25 | Loss: 0.00197349
Iteration 21/25 | Loss: 0.00197349
Iteration 22/25 | Loss: 0.00197349
Iteration 23/25 | Loss: 0.00197349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019734904635697603, 0.0019734904635697603, 0.0019734904635697603, 0.0019734904635697603, 0.0019734904635697603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019734904635697603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197349
Iteration 2/1000 | Loss: 0.00003429
Iteration 3/1000 | Loss: 0.00002519
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001945
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001708
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001575
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001480
Iteration 14/1000 | Loss: 0.00001468
Iteration 15/1000 | Loss: 0.00001460
Iteration 16/1000 | Loss: 0.00001452
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001442
Iteration 20/1000 | Loss: 0.00001440
Iteration 21/1000 | Loss: 0.00001435
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001434
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001432
Iteration 30/1000 | Loss: 0.00001429
Iteration 31/1000 | Loss: 0.00001429
Iteration 32/1000 | Loss: 0.00001429
Iteration 33/1000 | Loss: 0.00001428
Iteration 34/1000 | Loss: 0.00001426
Iteration 35/1000 | Loss: 0.00001425
Iteration 36/1000 | Loss: 0.00001425
Iteration 37/1000 | Loss: 0.00001424
Iteration 38/1000 | Loss: 0.00001424
Iteration 39/1000 | Loss: 0.00001424
Iteration 40/1000 | Loss: 0.00001424
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001423
Iteration 44/1000 | Loss: 0.00001423
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001423
Iteration 47/1000 | Loss: 0.00001423
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001422
Iteration 50/1000 | Loss: 0.00001422
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001421
Iteration 56/1000 | Loss: 0.00001421
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001417
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Iteration 79/1000 | Loss: 0.00001416
Iteration 80/1000 | Loss: 0.00001416
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001415
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001414
Iteration 89/1000 | Loss: 0.00001414
Iteration 90/1000 | Loss: 0.00001414
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001413
Iteration 94/1000 | Loss: 0.00001413
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001411
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001410
Iteration 104/1000 | Loss: 0.00001410
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001409
Iteration 112/1000 | Loss: 0.00001409
Iteration 113/1000 | Loss: 0.00001409
Iteration 114/1000 | Loss: 0.00001409
Iteration 115/1000 | Loss: 0.00001409
Iteration 116/1000 | Loss: 0.00001409
Iteration 117/1000 | Loss: 0.00001409
Iteration 118/1000 | Loss: 0.00001408
Iteration 119/1000 | Loss: 0.00001408
Iteration 120/1000 | Loss: 0.00001408
Iteration 121/1000 | Loss: 0.00001408
Iteration 122/1000 | Loss: 0.00001408
Iteration 123/1000 | Loss: 0.00001408
Iteration 124/1000 | Loss: 0.00001407
Iteration 125/1000 | Loss: 0.00001407
Iteration 126/1000 | Loss: 0.00001407
Iteration 127/1000 | Loss: 0.00001407
Iteration 128/1000 | Loss: 0.00001407
Iteration 129/1000 | Loss: 0.00001406
Iteration 130/1000 | Loss: 0.00001406
Iteration 131/1000 | Loss: 0.00001406
Iteration 132/1000 | Loss: 0.00001406
Iteration 133/1000 | Loss: 0.00001406
Iteration 134/1000 | Loss: 0.00001406
Iteration 135/1000 | Loss: 0.00001406
Iteration 136/1000 | Loss: 0.00001405
Iteration 137/1000 | Loss: 0.00001405
Iteration 138/1000 | Loss: 0.00001405
Iteration 139/1000 | Loss: 0.00001404
Iteration 140/1000 | Loss: 0.00001404
Iteration 141/1000 | Loss: 0.00001404
Iteration 142/1000 | Loss: 0.00001404
Iteration 143/1000 | Loss: 0.00001404
Iteration 144/1000 | Loss: 0.00001404
Iteration 145/1000 | Loss: 0.00001404
Iteration 146/1000 | Loss: 0.00001404
Iteration 147/1000 | Loss: 0.00001404
Iteration 148/1000 | Loss: 0.00001403
Iteration 149/1000 | Loss: 0.00001403
Iteration 150/1000 | Loss: 0.00001403
Iteration 151/1000 | Loss: 0.00001403
Iteration 152/1000 | Loss: 0.00001402
Iteration 153/1000 | Loss: 0.00001402
Iteration 154/1000 | Loss: 0.00001402
Iteration 155/1000 | Loss: 0.00001402
Iteration 156/1000 | Loss: 0.00001402
Iteration 157/1000 | Loss: 0.00001402
Iteration 158/1000 | Loss: 0.00001402
Iteration 159/1000 | Loss: 0.00001402
Iteration 160/1000 | Loss: 0.00001402
Iteration 161/1000 | Loss: 0.00001402
Iteration 162/1000 | Loss: 0.00001402
Iteration 163/1000 | Loss: 0.00001401
Iteration 164/1000 | Loss: 0.00001401
Iteration 165/1000 | Loss: 0.00001401
Iteration 166/1000 | Loss: 0.00001401
Iteration 167/1000 | Loss: 0.00001401
Iteration 168/1000 | Loss: 0.00001401
Iteration 169/1000 | Loss: 0.00001401
Iteration 170/1000 | Loss: 0.00001401
Iteration 171/1000 | Loss: 0.00001401
Iteration 172/1000 | Loss: 0.00001401
Iteration 173/1000 | Loss: 0.00001401
Iteration 174/1000 | Loss: 0.00001401
Iteration 175/1000 | Loss: 0.00001401
Iteration 176/1000 | Loss: 0.00001401
Iteration 177/1000 | Loss: 0.00001401
Iteration 178/1000 | Loss: 0.00001401
Iteration 179/1000 | Loss: 0.00001401
Iteration 180/1000 | Loss: 0.00001401
Iteration 181/1000 | Loss: 0.00001401
Iteration 182/1000 | Loss: 0.00001401
Iteration 183/1000 | Loss: 0.00001401
Iteration 184/1000 | Loss: 0.00001401
Iteration 185/1000 | Loss: 0.00001401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4010896848049015e-05, 1.4010896848049015e-05, 1.4010896848049015e-05, 1.4010896848049015e-05, 1.4010896848049015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4010896848049015e-05

Optimization complete. Final v2v error: 3.1798830032348633 mm

Highest mean error: 3.8515353202819824 mm for frame 89

Lowest mean error: 2.782799005508423 mm for frame 56

Saving results

Total time: 41.40977334976196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592185
Iteration 2/25 | Loss: 0.00135868
Iteration 3/25 | Loss: 0.00120377
Iteration 4/25 | Loss: 0.00118744
Iteration 5/25 | Loss: 0.00118418
Iteration 6/25 | Loss: 0.00118418
Iteration 7/25 | Loss: 0.00118418
Iteration 8/25 | Loss: 0.00118418
Iteration 9/25 | Loss: 0.00118418
Iteration 10/25 | Loss: 0.00118418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001184182008728385, 0.001184182008728385, 0.001184182008728385, 0.001184182008728385, 0.001184182008728385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184182008728385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30830860
Iteration 2/25 | Loss: 0.00139437
Iteration 3/25 | Loss: 0.00139437
Iteration 4/25 | Loss: 0.00139437
Iteration 5/25 | Loss: 0.00139437
Iteration 6/25 | Loss: 0.00139437
Iteration 7/25 | Loss: 0.00139436
Iteration 8/25 | Loss: 0.00139436
Iteration 9/25 | Loss: 0.00139436
Iteration 10/25 | Loss: 0.00139436
Iteration 11/25 | Loss: 0.00139436
Iteration 12/25 | Loss: 0.00139436
Iteration 13/25 | Loss: 0.00139436
Iteration 14/25 | Loss: 0.00139436
Iteration 15/25 | Loss: 0.00139436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013943641679361463, 0.0013943641679361463, 0.0013943641679361463, 0.0013943641679361463, 0.0013943641679361463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013943641679361463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139436
Iteration 2/1000 | Loss: 0.00002020
Iteration 3/1000 | Loss: 0.00001587
Iteration 4/1000 | Loss: 0.00001469
Iteration 5/1000 | Loss: 0.00001401
Iteration 6/1000 | Loss: 0.00001365
Iteration 7/1000 | Loss: 0.00001323
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001201
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001184
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001176
Iteration 27/1000 | Loss: 0.00001176
Iteration 28/1000 | Loss: 0.00001176
Iteration 29/1000 | Loss: 0.00001174
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001171
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001166
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001165
Iteration 63/1000 | Loss: 0.00001165
Iteration 64/1000 | Loss: 0.00001165
Iteration 65/1000 | Loss: 0.00001165
Iteration 66/1000 | Loss: 0.00001165
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001164
Iteration 69/1000 | Loss: 0.00001164
Iteration 70/1000 | Loss: 0.00001163
Iteration 71/1000 | Loss: 0.00001163
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001161
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001155
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001154
Iteration 101/1000 | Loss: 0.00001154
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001152
Iteration 112/1000 | Loss: 0.00001152
Iteration 113/1000 | Loss: 0.00001152
Iteration 114/1000 | Loss: 0.00001152
Iteration 115/1000 | Loss: 0.00001152
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001151
Iteration 120/1000 | Loss: 0.00001151
Iteration 121/1000 | Loss: 0.00001151
Iteration 122/1000 | Loss: 0.00001151
Iteration 123/1000 | Loss: 0.00001151
Iteration 124/1000 | Loss: 0.00001151
Iteration 125/1000 | Loss: 0.00001151
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001150
Iteration 128/1000 | Loss: 0.00001150
Iteration 129/1000 | Loss: 0.00001150
Iteration 130/1000 | Loss: 0.00001150
Iteration 131/1000 | Loss: 0.00001150
Iteration 132/1000 | Loss: 0.00001150
Iteration 133/1000 | Loss: 0.00001150
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1501667358970735e-05, 1.1501667358970735e-05, 1.1501667358970735e-05, 1.1501667358970735e-05, 1.1501667358970735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1501667358970735e-05

Optimization complete. Final v2v error: 2.8811745643615723 mm

Highest mean error: 3.7327985763549805 mm for frame 70

Lowest mean error: 2.522582769393921 mm for frame 30

Saving results

Total time: 40.94074845314026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434968
Iteration 2/25 | Loss: 0.00121241
Iteration 3/25 | Loss: 0.00116171
Iteration 4/25 | Loss: 0.00115323
Iteration 5/25 | Loss: 0.00115111
Iteration 6/25 | Loss: 0.00115111
Iteration 7/25 | Loss: 0.00115111
Iteration 8/25 | Loss: 0.00115111
Iteration 9/25 | Loss: 0.00115111
Iteration 10/25 | Loss: 0.00115111
Iteration 11/25 | Loss: 0.00115111
Iteration 12/25 | Loss: 0.00115111
Iteration 13/25 | Loss: 0.00115111
Iteration 14/25 | Loss: 0.00115111
Iteration 15/25 | Loss: 0.00115111
Iteration 16/25 | Loss: 0.00115111
Iteration 17/25 | Loss: 0.00115111
Iteration 18/25 | Loss: 0.00115111
Iteration 19/25 | Loss: 0.00115111
Iteration 20/25 | Loss: 0.00115111
Iteration 21/25 | Loss: 0.00115111
Iteration 22/25 | Loss: 0.00115111
Iteration 23/25 | Loss: 0.00115111
Iteration 24/25 | Loss: 0.00115111
Iteration 25/25 | Loss: 0.00115111

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25251484
Iteration 2/25 | Loss: 0.00198287
Iteration 3/25 | Loss: 0.00198287
Iteration 4/25 | Loss: 0.00198287
Iteration 5/25 | Loss: 0.00198287
Iteration 6/25 | Loss: 0.00198287
Iteration 7/25 | Loss: 0.00198287
Iteration 8/25 | Loss: 0.00198287
Iteration 9/25 | Loss: 0.00198287
Iteration 10/25 | Loss: 0.00198287
Iteration 11/25 | Loss: 0.00198287
Iteration 12/25 | Loss: 0.00198287
Iteration 13/25 | Loss: 0.00198287
Iteration 14/25 | Loss: 0.00198287
Iteration 15/25 | Loss: 0.00198287
Iteration 16/25 | Loss: 0.00198287
Iteration 17/25 | Loss: 0.00198287
Iteration 18/25 | Loss: 0.00198287
Iteration 19/25 | Loss: 0.00198287
Iteration 20/25 | Loss: 0.00198287
Iteration 21/25 | Loss: 0.00198287
Iteration 22/25 | Loss: 0.00198287
Iteration 23/25 | Loss: 0.00198287
Iteration 24/25 | Loss: 0.00198287
Iteration 25/25 | Loss: 0.00198287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198287
Iteration 2/1000 | Loss: 0.00001935
Iteration 3/1000 | Loss: 0.00001476
Iteration 4/1000 | Loss: 0.00001337
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001178
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001146
Iteration 10/1000 | Loss: 0.00001121
Iteration 11/1000 | Loss: 0.00001106
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001083
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001080
Iteration 17/1000 | Loss: 0.00001080
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001078
Iteration 20/1000 | Loss: 0.00001077
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001073
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001068
Iteration 27/1000 | Loss: 0.00001068
Iteration 28/1000 | Loss: 0.00001067
Iteration 29/1000 | Loss: 0.00001067
Iteration 30/1000 | Loss: 0.00001066
Iteration 31/1000 | Loss: 0.00001066
Iteration 32/1000 | Loss: 0.00001065
Iteration 33/1000 | Loss: 0.00001065
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001060
Iteration 36/1000 | Loss: 0.00001057
Iteration 37/1000 | Loss: 0.00001055
Iteration 38/1000 | Loss: 0.00001053
Iteration 39/1000 | Loss: 0.00001052
Iteration 40/1000 | Loss: 0.00001052
Iteration 41/1000 | Loss: 0.00001051
Iteration 42/1000 | Loss: 0.00001046
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001040
Iteration 48/1000 | Loss: 0.00001039
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001038
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001031
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001031
Iteration 60/1000 | Loss: 0.00001031
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001027
Iteration 67/1000 | Loss: 0.00001027
Iteration 68/1000 | Loss: 0.00001026
Iteration 69/1000 | Loss: 0.00001026
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001025
Iteration 72/1000 | Loss: 0.00001025
Iteration 73/1000 | Loss: 0.00001025
Iteration 74/1000 | Loss: 0.00001024
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001021
Iteration 89/1000 | Loss: 0.00001021
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001018
Iteration 95/1000 | Loss: 0.00001018
Iteration 96/1000 | Loss: 0.00001017
Iteration 97/1000 | Loss: 0.00001017
Iteration 98/1000 | Loss: 0.00001016
Iteration 99/1000 | Loss: 0.00001016
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001015
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001014
Iteration 107/1000 | Loss: 0.00001014
Iteration 108/1000 | Loss: 0.00001014
Iteration 109/1000 | Loss: 0.00001014
Iteration 110/1000 | Loss: 0.00001013
Iteration 111/1000 | Loss: 0.00001013
Iteration 112/1000 | Loss: 0.00001013
Iteration 113/1000 | Loss: 0.00001013
Iteration 114/1000 | Loss: 0.00001013
Iteration 115/1000 | Loss: 0.00001013
Iteration 116/1000 | Loss: 0.00001013
Iteration 117/1000 | Loss: 0.00001013
Iteration 118/1000 | Loss: 0.00001013
Iteration 119/1000 | Loss: 0.00001013
Iteration 120/1000 | Loss: 0.00001013
Iteration 121/1000 | Loss: 0.00001013
Iteration 122/1000 | Loss: 0.00001013
Iteration 123/1000 | Loss: 0.00001013
Iteration 124/1000 | Loss: 0.00001013
Iteration 125/1000 | Loss: 0.00001013
Iteration 126/1000 | Loss: 0.00001013
Iteration 127/1000 | Loss: 0.00001013
Iteration 128/1000 | Loss: 0.00001013
Iteration 129/1000 | Loss: 0.00001013
Iteration 130/1000 | Loss: 0.00001013
Iteration 131/1000 | Loss: 0.00001013
Iteration 132/1000 | Loss: 0.00001013
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.0133884643437341e-05, 1.0133884643437341e-05, 1.0133884643437341e-05, 1.0133884643437341e-05, 1.0133884643437341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0133884643437341e-05

Optimization complete. Final v2v error: 2.7575325965881348 mm

Highest mean error: 2.840879440307617 mm for frame 2

Lowest mean error: 2.665780782699585 mm for frame 189

Saving results

Total time: 36.888195276260376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900106
Iteration 2/25 | Loss: 0.00125731
Iteration 3/25 | Loss: 0.00117240
Iteration 4/25 | Loss: 0.00115709
Iteration 5/25 | Loss: 0.00115272
Iteration 6/25 | Loss: 0.00115272
Iteration 7/25 | Loss: 0.00115272
Iteration 8/25 | Loss: 0.00115272
Iteration 9/25 | Loss: 0.00115272
Iteration 10/25 | Loss: 0.00115272
Iteration 11/25 | Loss: 0.00115272
Iteration 12/25 | Loss: 0.00115272
Iteration 13/25 | Loss: 0.00115272
Iteration 14/25 | Loss: 0.00115272
Iteration 15/25 | Loss: 0.00115272
Iteration 16/25 | Loss: 0.00115272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011527184396982193, 0.0011527184396982193, 0.0011527184396982193, 0.0011527184396982193, 0.0011527184396982193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011527184396982193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33181131
Iteration 2/25 | Loss: 0.00180941
Iteration 3/25 | Loss: 0.00180941
Iteration 4/25 | Loss: 0.00180941
Iteration 5/25 | Loss: 0.00180941
Iteration 6/25 | Loss: 0.00180941
Iteration 7/25 | Loss: 0.00180941
Iteration 8/25 | Loss: 0.00180941
Iteration 9/25 | Loss: 0.00180941
Iteration 10/25 | Loss: 0.00180941
Iteration 11/25 | Loss: 0.00180941
Iteration 12/25 | Loss: 0.00180941
Iteration 13/25 | Loss: 0.00180941
Iteration 14/25 | Loss: 0.00180941
Iteration 15/25 | Loss: 0.00180941
Iteration 16/25 | Loss: 0.00180941
Iteration 17/25 | Loss: 0.00180941
Iteration 18/25 | Loss: 0.00180941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018094090046361089, 0.0018094090046361089, 0.0018094090046361089, 0.0018094090046361089, 0.0018094090046361089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018094090046361089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180941
Iteration 2/1000 | Loss: 0.00001659
Iteration 3/1000 | Loss: 0.00001411
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001223
Iteration 6/1000 | Loss: 0.00001179
Iteration 7/1000 | Loss: 0.00001130
Iteration 8/1000 | Loss: 0.00001106
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001070
Iteration 11/1000 | Loss: 0.00001066
Iteration 12/1000 | Loss: 0.00001064
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001061
Iteration 15/1000 | Loss: 0.00001060
Iteration 16/1000 | Loss: 0.00001051
Iteration 17/1000 | Loss: 0.00001051
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001043
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001040
Iteration 23/1000 | Loss: 0.00001040
Iteration 24/1000 | Loss: 0.00001036
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001025
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001025
Iteration 33/1000 | Loss: 0.00001024
Iteration 34/1000 | Loss: 0.00001024
Iteration 35/1000 | Loss: 0.00001024
Iteration 36/1000 | Loss: 0.00001024
Iteration 37/1000 | Loss: 0.00001024
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001024
Iteration 40/1000 | Loss: 0.00001024
Iteration 41/1000 | Loss: 0.00001024
Iteration 42/1000 | Loss: 0.00001023
Iteration 43/1000 | Loss: 0.00001021
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001018
Iteration 52/1000 | Loss: 0.00001018
Iteration 53/1000 | Loss: 0.00001017
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001016
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001015
Iteration 58/1000 | Loss: 0.00001015
Iteration 59/1000 | Loss: 0.00001015
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001014
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001011
Iteration 76/1000 | Loss: 0.00001011
Iteration 77/1000 | Loss: 0.00001011
Iteration 78/1000 | Loss: 0.00001010
Iteration 79/1000 | Loss: 0.00001010
Iteration 80/1000 | Loss: 0.00001010
Iteration 81/1000 | Loss: 0.00001009
Iteration 82/1000 | Loss: 0.00001009
Iteration 83/1000 | Loss: 0.00001008
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001007
Iteration 86/1000 | Loss: 0.00001007
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001005
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001005
Iteration 96/1000 | Loss: 0.00001005
Iteration 97/1000 | Loss: 0.00001005
Iteration 98/1000 | Loss: 0.00001005
Iteration 99/1000 | Loss: 0.00001005
Iteration 100/1000 | Loss: 0.00001005
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001004
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001002
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001002
Iteration 130/1000 | Loss: 0.00001002
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001002
Iteration 134/1000 | Loss: 0.00001002
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001001
Iteration 137/1000 | Loss: 0.00001001
Iteration 138/1000 | Loss: 0.00001001
Iteration 139/1000 | Loss: 0.00001001
Iteration 140/1000 | Loss: 0.00001001
Iteration 141/1000 | Loss: 0.00001001
Iteration 142/1000 | Loss: 0.00001001
Iteration 143/1000 | Loss: 0.00001001
Iteration 144/1000 | Loss: 0.00001001
Iteration 145/1000 | Loss: 0.00001001
Iteration 146/1000 | Loss: 0.00001001
Iteration 147/1000 | Loss: 0.00001000
Iteration 148/1000 | Loss: 0.00001000
Iteration 149/1000 | Loss: 0.00001000
Iteration 150/1000 | Loss: 0.00001000
Iteration 151/1000 | Loss: 0.00001000
Iteration 152/1000 | Loss: 0.00001000
Iteration 153/1000 | Loss: 0.00001000
Iteration 154/1000 | Loss: 0.00001000
Iteration 155/1000 | Loss: 0.00001000
Iteration 156/1000 | Loss: 0.00001000
Iteration 157/1000 | Loss: 0.00001000
Iteration 158/1000 | Loss: 0.00001000
Iteration 159/1000 | Loss: 0.00001000
Iteration 160/1000 | Loss: 0.00001000
Iteration 161/1000 | Loss: 0.00001000
Iteration 162/1000 | Loss: 0.00001000
Iteration 163/1000 | Loss: 0.00001000
Iteration 164/1000 | Loss: 0.00001000
Iteration 165/1000 | Loss: 0.00001000
Iteration 166/1000 | Loss: 0.00001000
Iteration 167/1000 | Loss: 0.00001000
Iteration 168/1000 | Loss: 0.00001000
Iteration 169/1000 | Loss: 0.00001000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [9.995775144489016e-06, 9.995775144489016e-06, 9.995775144489016e-06, 9.995775144489016e-06, 9.995775144489016e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.995775144489016e-06

Optimization complete. Final v2v error: 2.7374444007873535 mm

Highest mean error: 3.1665329933166504 mm for frame 146

Lowest mean error: 2.6592631340026855 mm for frame 211

Saving results

Total time: 41.44963264465332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00943217
Iteration 2/25 | Loss: 0.00234538
Iteration 3/25 | Loss: 0.00214138
Iteration 4/25 | Loss: 0.00163828
Iteration 5/25 | Loss: 0.00156750
Iteration 6/25 | Loss: 0.00154276
Iteration 7/25 | Loss: 0.00179640
Iteration 8/25 | Loss: 0.00179362
Iteration 9/25 | Loss: 0.00161979
Iteration 10/25 | Loss: 0.00173973
Iteration 11/25 | Loss: 0.00148297
Iteration 12/25 | Loss: 0.00130883
Iteration 13/25 | Loss: 0.00128674
Iteration 14/25 | Loss: 0.00128088
Iteration 15/25 | Loss: 0.00127957
Iteration 16/25 | Loss: 0.00127910
Iteration 17/25 | Loss: 0.00128160
Iteration 18/25 | Loss: 0.00128143
Iteration 19/25 | Loss: 0.00128122
Iteration 20/25 | Loss: 0.00128093
Iteration 21/25 | Loss: 0.00128078
Iteration 22/25 | Loss: 0.00128076
Iteration 23/25 | Loss: 0.00128069
Iteration 24/25 | Loss: 0.00128053
Iteration 25/25 | Loss: 0.00128012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19231141
Iteration 2/25 | Loss: 0.00117657
Iteration 3/25 | Loss: 0.00117657
Iteration 4/25 | Loss: 0.00117657
Iteration 5/25 | Loss: 0.00117657
Iteration 6/25 | Loss: 0.00117657
Iteration 7/25 | Loss: 0.00117657
Iteration 8/25 | Loss: 0.00117657
Iteration 9/25 | Loss: 0.00117657
Iteration 10/25 | Loss: 0.00117657
Iteration 11/25 | Loss: 0.00117657
Iteration 12/25 | Loss: 0.00117657
Iteration 13/25 | Loss: 0.00117657
Iteration 14/25 | Loss: 0.00117657
Iteration 15/25 | Loss: 0.00117657
Iteration 16/25 | Loss: 0.00117657
Iteration 17/25 | Loss: 0.00117657
Iteration 18/25 | Loss: 0.00117657
Iteration 19/25 | Loss: 0.00117657
Iteration 20/25 | Loss: 0.00117657
Iteration 21/25 | Loss: 0.00117657
Iteration 22/25 | Loss: 0.00117657
Iteration 23/25 | Loss: 0.00117657
Iteration 24/25 | Loss: 0.00117657
Iteration 25/25 | Loss: 0.00117657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117657
Iteration 2/1000 | Loss: 0.00041776
Iteration 3/1000 | Loss: 0.00040980
Iteration 4/1000 | Loss: 0.00042433
Iteration 5/1000 | Loss: 0.00024696
Iteration 6/1000 | Loss: 0.00016659
Iteration 7/1000 | Loss: 0.00007188
Iteration 8/1000 | Loss: 0.00007184
Iteration 9/1000 | Loss: 0.00005729
Iteration 10/1000 | Loss: 0.00038686
Iteration 11/1000 | Loss: 0.00008308
Iteration 12/1000 | Loss: 0.00004994
Iteration 13/1000 | Loss: 0.00004461
Iteration 14/1000 | Loss: 0.00004228
Iteration 15/1000 | Loss: 0.00005060
Iteration 16/1000 | Loss: 0.00004803
Iteration 17/1000 | Loss: 0.00003762
Iteration 18/1000 | Loss: 0.00003613
Iteration 19/1000 | Loss: 0.00003535
Iteration 20/1000 | Loss: 0.00003489
Iteration 21/1000 | Loss: 0.00003450
Iteration 22/1000 | Loss: 0.00003434
Iteration 23/1000 | Loss: 0.00003686
Iteration 24/1000 | Loss: 0.00003403
Iteration 25/1000 | Loss: 0.00003396
Iteration 26/1000 | Loss: 0.00003393
Iteration 27/1000 | Loss: 0.00003392
Iteration 28/1000 | Loss: 0.00003392
Iteration 29/1000 | Loss: 0.00003390
Iteration 30/1000 | Loss: 0.00003388
Iteration 31/1000 | Loss: 0.00003386
Iteration 32/1000 | Loss: 0.00003680
Iteration 33/1000 | Loss: 0.00003380
Iteration 34/1000 | Loss: 0.00003378
Iteration 35/1000 | Loss: 0.00003378
Iteration 36/1000 | Loss: 0.00003378
Iteration 37/1000 | Loss: 0.00003377
Iteration 38/1000 | Loss: 0.00003376
Iteration 39/1000 | Loss: 0.00003376
Iteration 40/1000 | Loss: 0.00003374
Iteration 41/1000 | Loss: 0.00003374
Iteration 42/1000 | Loss: 0.00003373
Iteration 43/1000 | Loss: 0.00003372
Iteration 44/1000 | Loss: 0.00003371
Iteration 45/1000 | Loss: 0.00003366
Iteration 46/1000 | Loss: 0.00003366
Iteration 47/1000 | Loss: 0.00003366
Iteration 48/1000 | Loss: 0.00003366
Iteration 49/1000 | Loss: 0.00003366
Iteration 50/1000 | Loss: 0.00003366
Iteration 51/1000 | Loss: 0.00003366
Iteration 52/1000 | Loss: 0.00003366
Iteration 53/1000 | Loss: 0.00003365
Iteration 54/1000 | Loss: 0.00003365
Iteration 55/1000 | Loss: 0.00003365
Iteration 56/1000 | Loss: 0.00003365
Iteration 57/1000 | Loss: 0.00003365
Iteration 58/1000 | Loss: 0.00003365
Iteration 59/1000 | Loss: 0.00003365
Iteration 60/1000 | Loss: 0.00003365
Iteration 61/1000 | Loss: 0.00003364
Iteration 62/1000 | Loss: 0.00003364
Iteration 63/1000 | Loss: 0.00003363
Iteration 64/1000 | Loss: 0.00003363
Iteration 65/1000 | Loss: 0.00003363
Iteration 66/1000 | Loss: 0.00003362
Iteration 67/1000 | Loss: 0.00004331
Iteration 68/1000 | Loss: 0.00005004
Iteration 69/1000 | Loss: 0.00003571
Iteration 70/1000 | Loss: 0.00004003
Iteration 71/1000 | Loss: 0.00003657
Iteration 72/1000 | Loss: 0.00003707
Iteration 73/1000 | Loss: 0.00003366
Iteration 74/1000 | Loss: 0.00004343
Iteration 75/1000 | Loss: 0.00038043
Iteration 76/1000 | Loss: 0.00009883
Iteration 77/1000 | Loss: 0.00004073
Iteration 78/1000 | Loss: 0.00004593
Iteration 79/1000 | Loss: 0.00004088
Iteration 80/1000 | Loss: 0.00004133
Iteration 81/1000 | Loss: 0.00003542
Iteration 82/1000 | Loss: 0.00005046
Iteration 83/1000 | Loss: 0.00003436
Iteration 84/1000 | Loss: 0.00003394
Iteration 85/1000 | Loss: 0.00003808
Iteration 86/1000 | Loss: 0.00003332
Iteration 87/1000 | Loss: 0.00004444
Iteration 88/1000 | Loss: 0.00003283
Iteration 89/1000 | Loss: 0.00004442
Iteration 90/1000 | Loss: 0.00003765
Iteration 91/1000 | Loss: 0.00003211
Iteration 92/1000 | Loss: 0.00003186
Iteration 93/1000 | Loss: 0.00003153
Iteration 94/1000 | Loss: 0.00003614
Iteration 95/1000 | Loss: 0.00003105
Iteration 96/1000 | Loss: 0.00017880
Iteration 97/1000 | Loss: 0.00065366
Iteration 98/1000 | Loss: 0.00036855
Iteration 99/1000 | Loss: 0.00049318
Iteration 100/1000 | Loss: 0.00017449
Iteration 101/1000 | Loss: 0.00012410
Iteration 102/1000 | Loss: 0.00007107
Iteration 103/1000 | Loss: 0.00004374
Iteration 104/1000 | Loss: 0.00005654
Iteration 105/1000 | Loss: 0.00016590
Iteration 106/1000 | Loss: 0.00004895
Iteration 107/1000 | Loss: 0.00004709
Iteration 108/1000 | Loss: 0.00003909
Iteration 109/1000 | Loss: 0.00004263
Iteration 110/1000 | Loss: 0.00005247
Iteration 111/1000 | Loss: 0.00003369
Iteration 112/1000 | Loss: 0.00017870
Iteration 113/1000 | Loss: 0.00004898
Iteration 114/1000 | Loss: 0.00003780
Iteration 115/1000 | Loss: 0.00004093
Iteration 116/1000 | Loss: 0.00003517
Iteration 117/1000 | Loss: 0.00003283
Iteration 118/1000 | Loss: 0.00003292
Iteration 119/1000 | Loss: 0.00003708
Iteration 120/1000 | Loss: 0.00003388
Iteration 121/1000 | Loss: 0.00003096
Iteration 122/1000 | Loss: 0.00003083
Iteration 123/1000 | Loss: 0.00003083
Iteration 124/1000 | Loss: 0.00003074
Iteration 125/1000 | Loss: 0.00003073
Iteration 126/1000 | Loss: 0.00003053
Iteration 127/1000 | Loss: 0.00003705
Iteration 128/1000 | Loss: 0.00003010
Iteration 129/1000 | Loss: 0.00002999
Iteration 130/1000 | Loss: 0.00003357
Iteration 131/1000 | Loss: 0.00003594
Iteration 132/1000 | Loss: 0.00003594
Iteration 133/1000 | Loss: 0.00003322
Iteration 134/1000 | Loss: 0.00004472
Iteration 135/1000 | Loss: 0.00020050
Iteration 136/1000 | Loss: 0.00004081
Iteration 137/1000 | Loss: 0.00003902
Iteration 138/1000 | Loss: 0.00003711
Iteration 139/1000 | Loss: 0.00017580
Iteration 140/1000 | Loss: 0.00023478
Iteration 141/1000 | Loss: 0.00007615
Iteration 142/1000 | Loss: 0.00016691
Iteration 143/1000 | Loss: 0.00004078
Iteration 144/1000 | Loss: 0.00014797
Iteration 145/1000 | Loss: 0.00010253
Iteration 146/1000 | Loss: 0.00011991
Iteration 147/1000 | Loss: 0.00008834
Iteration 148/1000 | Loss: 0.00011554
Iteration 149/1000 | Loss: 0.00017349
Iteration 150/1000 | Loss: 0.00004879
Iteration 151/1000 | Loss: 0.00004217
Iteration 152/1000 | Loss: 0.00003709
Iteration 153/1000 | Loss: 0.00003468
Iteration 154/1000 | Loss: 0.00004127
Iteration 155/1000 | Loss: 0.00003802
Iteration 156/1000 | Loss: 0.00003212
Iteration 157/1000 | Loss: 0.00003142
Iteration 158/1000 | Loss: 0.00004369
Iteration 159/1000 | Loss: 0.00003188
Iteration 160/1000 | Loss: 0.00003087
Iteration 161/1000 | Loss: 0.00017696
Iteration 162/1000 | Loss: 0.00004613
Iteration 163/1000 | Loss: 0.00003315
Iteration 164/1000 | Loss: 0.00003663
Iteration 165/1000 | Loss: 0.00003073
Iteration 166/1000 | Loss: 0.00003043
Iteration 167/1000 | Loss: 0.00003042
Iteration 168/1000 | Loss: 0.00003024
Iteration 169/1000 | Loss: 0.00003017
Iteration 170/1000 | Loss: 0.00003016
Iteration 171/1000 | Loss: 0.00003016
Iteration 172/1000 | Loss: 0.00003015
Iteration 173/1000 | Loss: 0.00003014
Iteration 174/1000 | Loss: 0.00003013
Iteration 175/1000 | Loss: 0.00003012
Iteration 176/1000 | Loss: 0.00003012
Iteration 177/1000 | Loss: 0.00003012
Iteration 178/1000 | Loss: 0.00003012
Iteration 179/1000 | Loss: 0.00003012
Iteration 180/1000 | Loss: 0.00003012
Iteration 181/1000 | Loss: 0.00003012
Iteration 182/1000 | Loss: 0.00003012
Iteration 183/1000 | Loss: 0.00003012
Iteration 184/1000 | Loss: 0.00003011
Iteration 185/1000 | Loss: 0.00003011
Iteration 186/1000 | Loss: 0.00003011
Iteration 187/1000 | Loss: 0.00003011
Iteration 188/1000 | Loss: 0.00003010
Iteration 189/1000 | Loss: 0.00003010
Iteration 190/1000 | Loss: 0.00003010
Iteration 191/1000 | Loss: 0.00003841
Iteration 192/1000 | Loss: 0.00003101
Iteration 193/1000 | Loss: 0.00003009
Iteration 194/1000 | Loss: 0.00003009
Iteration 195/1000 | Loss: 0.00003009
Iteration 196/1000 | Loss: 0.00003009
Iteration 197/1000 | Loss: 0.00003009
Iteration 198/1000 | Loss: 0.00003009
Iteration 199/1000 | Loss: 0.00003009
Iteration 200/1000 | Loss: 0.00003009
Iteration 201/1000 | Loss: 0.00003008
Iteration 202/1000 | Loss: 0.00003008
Iteration 203/1000 | Loss: 0.00003008
Iteration 204/1000 | Loss: 0.00003008
Iteration 205/1000 | Loss: 0.00003008
Iteration 206/1000 | Loss: 0.00003081
Iteration 207/1000 | Loss: 0.00003069
Iteration 208/1000 | Loss: 0.00003008
Iteration 209/1000 | Loss: 0.00003008
Iteration 210/1000 | Loss: 0.00003008
Iteration 211/1000 | Loss: 0.00003008
Iteration 212/1000 | Loss: 0.00003008
Iteration 213/1000 | Loss: 0.00003008
Iteration 214/1000 | Loss: 0.00003008
Iteration 215/1000 | Loss: 0.00003008
Iteration 216/1000 | Loss: 0.00003021
Iteration 217/1000 | Loss: 0.00003010
Iteration 218/1000 | Loss: 0.00003010
Iteration 219/1000 | Loss: 0.00003008
Iteration 220/1000 | Loss: 0.00003008
Iteration 221/1000 | Loss: 0.00003007
Iteration 222/1000 | Loss: 0.00003007
Iteration 223/1000 | Loss: 0.00003007
Iteration 224/1000 | Loss: 0.00003007
Iteration 225/1000 | Loss: 0.00003007
Iteration 226/1000 | Loss: 0.00003007
Iteration 227/1000 | Loss: 0.00003007
Iteration 228/1000 | Loss: 0.00003007
Iteration 229/1000 | Loss: 0.00003007
Iteration 230/1000 | Loss: 0.00003007
Iteration 231/1000 | Loss: 0.00003007
Iteration 232/1000 | Loss: 0.00003007
Iteration 233/1000 | Loss: 0.00003007
Iteration 234/1000 | Loss: 0.00003007
Iteration 235/1000 | Loss: 0.00003007
Iteration 236/1000 | Loss: 0.00003007
Iteration 237/1000 | Loss: 0.00003007
Iteration 238/1000 | Loss: 0.00003007
Iteration 239/1000 | Loss: 0.00003007
Iteration 240/1000 | Loss: 0.00003007
Iteration 241/1000 | Loss: 0.00003007
Iteration 242/1000 | Loss: 0.00003007
Iteration 243/1000 | Loss: 0.00003007
Iteration 244/1000 | Loss: 0.00003007
Iteration 245/1000 | Loss: 0.00003007
Iteration 246/1000 | Loss: 0.00003007
Iteration 247/1000 | Loss: 0.00003007
Iteration 248/1000 | Loss: 0.00003007
Iteration 249/1000 | Loss: 0.00003007
Iteration 250/1000 | Loss: 0.00003007
Iteration 251/1000 | Loss: 0.00003007
Iteration 252/1000 | Loss: 0.00003007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.0073377274675295e-05, 3.0073377274675295e-05, 3.0073377274675295e-05, 3.0073377274675295e-05, 3.0073377274675295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0073377274675295e-05

Optimization complete. Final v2v error: 4.1417107582092285 mm

Highest mean error: 10.020755767822266 mm for frame 164

Lowest mean error: 3.729116201400757 mm for frame 8

Saving results

Total time: 237.81401419639587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408835
Iteration 2/25 | Loss: 0.00122105
Iteration 3/25 | Loss: 0.00114563
Iteration 4/25 | Loss: 0.00113757
Iteration 5/25 | Loss: 0.00113525
Iteration 6/25 | Loss: 0.00113498
Iteration 7/25 | Loss: 0.00113498
Iteration 8/25 | Loss: 0.00113498
Iteration 9/25 | Loss: 0.00113498
Iteration 10/25 | Loss: 0.00113498
Iteration 11/25 | Loss: 0.00113498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011349784908816218, 0.0011349784908816218, 0.0011349784908816218, 0.0011349784908816218, 0.0011349784908816218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011349784908816218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27254057
Iteration 2/25 | Loss: 0.00180261
Iteration 3/25 | Loss: 0.00180260
Iteration 4/25 | Loss: 0.00180260
Iteration 5/25 | Loss: 0.00180260
Iteration 6/25 | Loss: 0.00180260
Iteration 7/25 | Loss: 0.00180260
Iteration 8/25 | Loss: 0.00180260
Iteration 9/25 | Loss: 0.00180260
Iteration 10/25 | Loss: 0.00180260
Iteration 11/25 | Loss: 0.00180260
Iteration 12/25 | Loss: 0.00180260
Iteration 13/25 | Loss: 0.00180260
Iteration 14/25 | Loss: 0.00180260
Iteration 15/25 | Loss: 0.00180260
Iteration 16/25 | Loss: 0.00180260
Iteration 17/25 | Loss: 0.00180260
Iteration 18/25 | Loss: 0.00180260
Iteration 19/25 | Loss: 0.00180260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018025998724624515, 0.0018025998724624515, 0.0018025998724624515, 0.0018025998724624515, 0.0018025998724624515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018025998724624515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180260
Iteration 2/1000 | Loss: 0.00002242
Iteration 3/1000 | Loss: 0.00001575
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001154
Iteration 7/1000 | Loss: 0.00001111
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001041
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001006
Iteration 12/1000 | Loss: 0.00000992
Iteration 13/1000 | Loss: 0.00000977
Iteration 14/1000 | Loss: 0.00000969
Iteration 15/1000 | Loss: 0.00000964
Iteration 16/1000 | Loss: 0.00000963
Iteration 17/1000 | Loss: 0.00000963
Iteration 18/1000 | Loss: 0.00000962
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000960
Iteration 21/1000 | Loss: 0.00000959
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000957
Iteration 24/1000 | Loss: 0.00000957
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000944
Iteration 29/1000 | Loss: 0.00000942
Iteration 30/1000 | Loss: 0.00000941
Iteration 31/1000 | Loss: 0.00000941
Iteration 32/1000 | Loss: 0.00000941
Iteration 33/1000 | Loss: 0.00000940
Iteration 34/1000 | Loss: 0.00000940
Iteration 35/1000 | Loss: 0.00000939
Iteration 36/1000 | Loss: 0.00000939
Iteration 37/1000 | Loss: 0.00000938
Iteration 38/1000 | Loss: 0.00000938
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000937
Iteration 41/1000 | Loss: 0.00000937
Iteration 42/1000 | Loss: 0.00000936
Iteration 43/1000 | Loss: 0.00000936
Iteration 44/1000 | Loss: 0.00000936
Iteration 45/1000 | Loss: 0.00000935
Iteration 46/1000 | Loss: 0.00000934
Iteration 47/1000 | Loss: 0.00000934
Iteration 48/1000 | Loss: 0.00000934
Iteration 49/1000 | Loss: 0.00000934
Iteration 50/1000 | Loss: 0.00000934
Iteration 51/1000 | Loss: 0.00000934
Iteration 52/1000 | Loss: 0.00000933
Iteration 53/1000 | Loss: 0.00000933
Iteration 54/1000 | Loss: 0.00000933
Iteration 55/1000 | Loss: 0.00000932
Iteration 56/1000 | Loss: 0.00000932
Iteration 57/1000 | Loss: 0.00000932
Iteration 58/1000 | Loss: 0.00000932
Iteration 59/1000 | Loss: 0.00000931
Iteration 60/1000 | Loss: 0.00000931
Iteration 61/1000 | Loss: 0.00000931
Iteration 62/1000 | Loss: 0.00000931
Iteration 63/1000 | Loss: 0.00000930
Iteration 64/1000 | Loss: 0.00000930
Iteration 65/1000 | Loss: 0.00000930
Iteration 66/1000 | Loss: 0.00000930
Iteration 67/1000 | Loss: 0.00000929
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000929
Iteration 70/1000 | Loss: 0.00000928
Iteration 71/1000 | Loss: 0.00000928
Iteration 72/1000 | Loss: 0.00000928
Iteration 73/1000 | Loss: 0.00000927
Iteration 74/1000 | Loss: 0.00000927
Iteration 75/1000 | Loss: 0.00000927
Iteration 76/1000 | Loss: 0.00000927
Iteration 77/1000 | Loss: 0.00000927
Iteration 78/1000 | Loss: 0.00000927
Iteration 79/1000 | Loss: 0.00000927
Iteration 80/1000 | Loss: 0.00000927
Iteration 81/1000 | Loss: 0.00000927
Iteration 82/1000 | Loss: 0.00000927
Iteration 83/1000 | Loss: 0.00000926
Iteration 84/1000 | Loss: 0.00000926
Iteration 85/1000 | Loss: 0.00000926
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000926
Iteration 89/1000 | Loss: 0.00000926
Iteration 90/1000 | Loss: 0.00000926
Iteration 91/1000 | Loss: 0.00000925
Iteration 92/1000 | Loss: 0.00000925
Iteration 93/1000 | Loss: 0.00000924
Iteration 94/1000 | Loss: 0.00000924
Iteration 95/1000 | Loss: 0.00000924
Iteration 96/1000 | Loss: 0.00000924
Iteration 97/1000 | Loss: 0.00000924
Iteration 98/1000 | Loss: 0.00000924
Iteration 99/1000 | Loss: 0.00000924
Iteration 100/1000 | Loss: 0.00000923
Iteration 101/1000 | Loss: 0.00000923
Iteration 102/1000 | Loss: 0.00000923
Iteration 103/1000 | Loss: 0.00000923
Iteration 104/1000 | Loss: 0.00000923
Iteration 105/1000 | Loss: 0.00000923
Iteration 106/1000 | Loss: 0.00000923
Iteration 107/1000 | Loss: 0.00000923
Iteration 108/1000 | Loss: 0.00000922
Iteration 109/1000 | Loss: 0.00000922
Iteration 110/1000 | Loss: 0.00000922
Iteration 111/1000 | Loss: 0.00000922
Iteration 112/1000 | Loss: 0.00000922
Iteration 113/1000 | Loss: 0.00000922
Iteration 114/1000 | Loss: 0.00000922
Iteration 115/1000 | Loss: 0.00000922
Iteration 116/1000 | Loss: 0.00000921
Iteration 117/1000 | Loss: 0.00000921
Iteration 118/1000 | Loss: 0.00000921
Iteration 119/1000 | Loss: 0.00000921
Iteration 120/1000 | Loss: 0.00000921
Iteration 121/1000 | Loss: 0.00000921
Iteration 122/1000 | Loss: 0.00000920
Iteration 123/1000 | Loss: 0.00000920
Iteration 124/1000 | Loss: 0.00000920
Iteration 125/1000 | Loss: 0.00000920
Iteration 126/1000 | Loss: 0.00000920
Iteration 127/1000 | Loss: 0.00000920
Iteration 128/1000 | Loss: 0.00000920
Iteration 129/1000 | Loss: 0.00000919
Iteration 130/1000 | Loss: 0.00000919
Iteration 131/1000 | Loss: 0.00000919
Iteration 132/1000 | Loss: 0.00000919
Iteration 133/1000 | Loss: 0.00000919
Iteration 134/1000 | Loss: 0.00000919
Iteration 135/1000 | Loss: 0.00000919
Iteration 136/1000 | Loss: 0.00000919
Iteration 137/1000 | Loss: 0.00000919
Iteration 138/1000 | Loss: 0.00000919
Iteration 139/1000 | Loss: 0.00000919
Iteration 140/1000 | Loss: 0.00000919
Iteration 141/1000 | Loss: 0.00000919
Iteration 142/1000 | Loss: 0.00000919
Iteration 143/1000 | Loss: 0.00000919
Iteration 144/1000 | Loss: 0.00000918
Iteration 145/1000 | Loss: 0.00000918
Iteration 146/1000 | Loss: 0.00000918
Iteration 147/1000 | Loss: 0.00000918
Iteration 148/1000 | Loss: 0.00000918
Iteration 149/1000 | Loss: 0.00000918
Iteration 150/1000 | Loss: 0.00000918
Iteration 151/1000 | Loss: 0.00000918
Iteration 152/1000 | Loss: 0.00000918
Iteration 153/1000 | Loss: 0.00000918
Iteration 154/1000 | Loss: 0.00000918
Iteration 155/1000 | Loss: 0.00000918
Iteration 156/1000 | Loss: 0.00000918
Iteration 157/1000 | Loss: 0.00000918
Iteration 158/1000 | Loss: 0.00000918
Iteration 159/1000 | Loss: 0.00000918
Iteration 160/1000 | Loss: 0.00000918
Iteration 161/1000 | Loss: 0.00000918
Iteration 162/1000 | Loss: 0.00000918
Iteration 163/1000 | Loss: 0.00000917
Iteration 164/1000 | Loss: 0.00000917
Iteration 165/1000 | Loss: 0.00000917
Iteration 166/1000 | Loss: 0.00000917
Iteration 167/1000 | Loss: 0.00000917
Iteration 168/1000 | Loss: 0.00000917
Iteration 169/1000 | Loss: 0.00000917
Iteration 170/1000 | Loss: 0.00000917
Iteration 171/1000 | Loss: 0.00000917
Iteration 172/1000 | Loss: 0.00000917
Iteration 173/1000 | Loss: 0.00000917
Iteration 174/1000 | Loss: 0.00000917
Iteration 175/1000 | Loss: 0.00000917
Iteration 176/1000 | Loss: 0.00000917
Iteration 177/1000 | Loss: 0.00000917
Iteration 178/1000 | Loss: 0.00000917
Iteration 179/1000 | Loss: 0.00000917
Iteration 180/1000 | Loss: 0.00000917
Iteration 181/1000 | Loss: 0.00000917
Iteration 182/1000 | Loss: 0.00000917
Iteration 183/1000 | Loss: 0.00000917
Iteration 184/1000 | Loss: 0.00000917
Iteration 185/1000 | Loss: 0.00000917
Iteration 186/1000 | Loss: 0.00000917
Iteration 187/1000 | Loss: 0.00000917
Iteration 188/1000 | Loss: 0.00000917
Iteration 189/1000 | Loss: 0.00000917
Iteration 190/1000 | Loss: 0.00000917
Iteration 191/1000 | Loss: 0.00000917
Iteration 192/1000 | Loss: 0.00000917
Iteration 193/1000 | Loss: 0.00000917
Iteration 194/1000 | Loss: 0.00000917
Iteration 195/1000 | Loss: 0.00000917
Iteration 196/1000 | Loss: 0.00000917
Iteration 197/1000 | Loss: 0.00000917
Iteration 198/1000 | Loss: 0.00000917
Iteration 199/1000 | Loss: 0.00000917
Iteration 200/1000 | Loss: 0.00000917
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [9.166084964817856e-06, 9.166084964817856e-06, 9.166084964817856e-06, 9.166084964817856e-06, 9.166084964817856e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.166084964817856e-06

Optimization complete. Final v2v error: 2.6245315074920654 mm

Highest mean error: 3.1383559703826904 mm for frame 52

Lowest mean error: 2.399951696395874 mm for frame 127

Saving results

Total time: 40.686468839645386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412599
Iteration 2/25 | Loss: 0.00121019
Iteration 3/25 | Loss: 0.00114947
Iteration 4/25 | Loss: 0.00114001
Iteration 5/25 | Loss: 0.00113746
Iteration 6/25 | Loss: 0.00113728
Iteration 7/25 | Loss: 0.00113728
Iteration 8/25 | Loss: 0.00113728
Iteration 9/25 | Loss: 0.00113728
Iteration 10/25 | Loss: 0.00113728
Iteration 11/25 | Loss: 0.00113728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011372833978384733, 0.0011372833978384733, 0.0011372833978384733, 0.0011372833978384733, 0.0011372833978384733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011372833978384733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27095461
Iteration 2/25 | Loss: 0.00172640
Iteration 3/25 | Loss: 0.00172640
Iteration 4/25 | Loss: 0.00172640
Iteration 5/25 | Loss: 0.00172640
Iteration 6/25 | Loss: 0.00172640
Iteration 7/25 | Loss: 0.00172640
Iteration 8/25 | Loss: 0.00172640
Iteration 9/25 | Loss: 0.00172640
Iteration 10/25 | Loss: 0.00172640
Iteration 11/25 | Loss: 0.00172640
Iteration 12/25 | Loss: 0.00172640
Iteration 13/25 | Loss: 0.00172640
Iteration 14/25 | Loss: 0.00172640
Iteration 15/25 | Loss: 0.00172640
Iteration 16/25 | Loss: 0.00172640
Iteration 17/25 | Loss: 0.00172640
Iteration 18/25 | Loss: 0.00172640
Iteration 19/25 | Loss: 0.00172640
Iteration 20/25 | Loss: 0.00172640
Iteration 21/25 | Loss: 0.00172640
Iteration 22/25 | Loss: 0.00172640
Iteration 23/25 | Loss: 0.00172640
Iteration 24/25 | Loss: 0.00172640
Iteration 25/25 | Loss: 0.00172640
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017263992922380567, 0.0017263992922380567, 0.0017263992922380567, 0.0017263992922380567, 0.0017263992922380567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017263992922380567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172640
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001514
Iteration 4/1000 | Loss: 0.00001378
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001159
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001128
Iteration 12/1000 | Loss: 0.00001124
Iteration 13/1000 | Loss: 0.00001111
Iteration 14/1000 | Loss: 0.00001107
Iteration 15/1000 | Loss: 0.00001099
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001085
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001077
Iteration 22/1000 | Loss: 0.00001075
Iteration 23/1000 | Loss: 0.00001074
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001074
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001073
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001073
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001070
Iteration 33/1000 | Loss: 0.00001070
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001065
Iteration 36/1000 | Loss: 0.00001065
Iteration 37/1000 | Loss: 0.00001065
Iteration 38/1000 | Loss: 0.00001064
Iteration 39/1000 | Loss: 0.00001063
Iteration 40/1000 | Loss: 0.00001063
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001062
Iteration 43/1000 | Loss: 0.00001062
Iteration 44/1000 | Loss: 0.00001062
Iteration 45/1000 | Loss: 0.00001062
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001061
Iteration 48/1000 | Loss: 0.00001061
Iteration 49/1000 | Loss: 0.00001061
Iteration 50/1000 | Loss: 0.00001061
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001061
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001060
Iteration 55/1000 | Loss: 0.00001060
Iteration 56/1000 | Loss: 0.00001058
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001058
Iteration 62/1000 | Loss: 0.00001058
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001057
Iteration 68/1000 | Loss: 0.00001057
Iteration 69/1000 | Loss: 0.00001057
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001056
Iteration 72/1000 | Loss: 0.00001056
Iteration 73/1000 | Loss: 0.00001055
Iteration 74/1000 | Loss: 0.00001055
Iteration 75/1000 | Loss: 0.00001054
Iteration 76/1000 | Loss: 0.00001054
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001052
Iteration 86/1000 | Loss: 0.00001052
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001051
Iteration 90/1000 | Loss: 0.00001051
Iteration 91/1000 | Loss: 0.00001051
Iteration 92/1000 | Loss: 0.00001051
Iteration 93/1000 | Loss: 0.00001051
Iteration 94/1000 | Loss: 0.00001051
Iteration 95/1000 | Loss: 0.00001050
Iteration 96/1000 | Loss: 0.00001050
Iteration 97/1000 | Loss: 0.00001049
Iteration 98/1000 | Loss: 0.00001049
Iteration 99/1000 | Loss: 0.00001049
Iteration 100/1000 | Loss: 0.00001049
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001049
Iteration 106/1000 | Loss: 0.00001049
Iteration 107/1000 | Loss: 0.00001049
Iteration 108/1000 | Loss: 0.00001049
Iteration 109/1000 | Loss: 0.00001049
Iteration 110/1000 | Loss: 0.00001049
Iteration 111/1000 | Loss: 0.00001049
Iteration 112/1000 | Loss: 0.00001049
Iteration 113/1000 | Loss: 0.00001049
Iteration 114/1000 | Loss: 0.00001049
Iteration 115/1000 | Loss: 0.00001048
Iteration 116/1000 | Loss: 0.00001048
Iteration 117/1000 | Loss: 0.00001048
Iteration 118/1000 | Loss: 0.00001048
Iteration 119/1000 | Loss: 0.00001048
Iteration 120/1000 | Loss: 0.00001048
Iteration 121/1000 | Loss: 0.00001048
Iteration 122/1000 | Loss: 0.00001048
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Iteration 125/1000 | Loss: 0.00001047
Iteration 126/1000 | Loss: 0.00001047
Iteration 127/1000 | Loss: 0.00001047
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001047
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001045
Iteration 134/1000 | Loss: 0.00001045
Iteration 135/1000 | Loss: 0.00001045
Iteration 136/1000 | Loss: 0.00001045
Iteration 137/1000 | Loss: 0.00001045
Iteration 138/1000 | Loss: 0.00001045
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001044
Iteration 141/1000 | Loss: 0.00001044
Iteration 142/1000 | Loss: 0.00001044
Iteration 143/1000 | Loss: 0.00001044
Iteration 144/1000 | Loss: 0.00001044
Iteration 145/1000 | Loss: 0.00001044
Iteration 146/1000 | Loss: 0.00001044
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001043
Iteration 152/1000 | Loss: 0.00001043
Iteration 153/1000 | Loss: 0.00001043
Iteration 154/1000 | Loss: 0.00001043
Iteration 155/1000 | Loss: 0.00001043
Iteration 156/1000 | Loss: 0.00001043
Iteration 157/1000 | Loss: 0.00001043
Iteration 158/1000 | Loss: 0.00001043
Iteration 159/1000 | Loss: 0.00001043
Iteration 160/1000 | Loss: 0.00001043
Iteration 161/1000 | Loss: 0.00001043
Iteration 162/1000 | Loss: 0.00001043
Iteration 163/1000 | Loss: 0.00001043
Iteration 164/1000 | Loss: 0.00001043
Iteration 165/1000 | Loss: 0.00001043
Iteration 166/1000 | Loss: 0.00001043
Iteration 167/1000 | Loss: 0.00001043
Iteration 168/1000 | Loss: 0.00001043
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001043
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001043
Iteration 177/1000 | Loss: 0.00001043
Iteration 178/1000 | Loss: 0.00001043
Iteration 179/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.0432101589685772e-05, 1.0432101589685772e-05, 1.0432101589685772e-05, 1.0432101589685772e-05, 1.0432101589685772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0432101589685772e-05

Optimization complete. Final v2v error: 2.8173344135284424 mm

Highest mean error: 2.995431900024414 mm for frame 6

Lowest mean error: 2.7088840007781982 mm for frame 59

Saving results

Total time: 39.173155307769775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022879
Iteration 2/25 | Loss: 0.00247593
Iteration 3/25 | Loss: 0.00214950
Iteration 4/25 | Loss: 0.00207409
Iteration 5/25 | Loss: 0.00205799
Iteration 6/25 | Loss: 0.00196823
Iteration 7/25 | Loss: 0.00191398
Iteration 8/25 | Loss: 0.00185721
Iteration 9/25 | Loss: 0.00181590
Iteration 10/25 | Loss: 0.00180358
Iteration 11/25 | Loss: 0.00178019
Iteration 12/25 | Loss: 0.00177274
Iteration 13/25 | Loss: 0.00176720
Iteration 14/25 | Loss: 0.00176522
Iteration 15/25 | Loss: 0.00176477
Iteration 16/25 | Loss: 0.00176460
Iteration 17/25 | Loss: 0.00176450
Iteration 18/25 | Loss: 0.00176436
Iteration 19/25 | Loss: 0.00176424
Iteration 20/25 | Loss: 0.00176360
Iteration 21/25 | Loss: 0.00176749
Iteration 22/25 | Loss: 0.00175908
Iteration 23/25 | Loss: 0.00175800
Iteration 24/25 | Loss: 0.00175769
Iteration 25/25 | Loss: 0.00175768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14529061
Iteration 2/25 | Loss: 0.00981713
Iteration 3/25 | Loss: 0.00981712
Iteration 4/25 | Loss: 0.00981712
Iteration 5/25 | Loss: 0.00981712
Iteration 6/25 | Loss: 0.00981712
Iteration 7/25 | Loss: 0.00981712
Iteration 8/25 | Loss: 0.00981712
Iteration 9/25 | Loss: 0.00981712
Iteration 10/25 | Loss: 0.00981712
Iteration 11/25 | Loss: 0.00981712
Iteration 12/25 | Loss: 0.00981712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.009817121550440788, 0.009817121550440788, 0.009817121550440788, 0.009817121550440788, 0.009817121550440788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009817121550440788

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00981712
Iteration 2/1000 | Loss: 0.00096332
Iteration 3/1000 | Loss: 0.00069135
Iteration 4/1000 | Loss: 0.00053375
Iteration 5/1000 | Loss: 0.00048964
Iteration 6/1000 | Loss: 0.00045256
Iteration 7/1000 | Loss: 0.00040760
Iteration 8/1000 | Loss: 0.00037983
Iteration 9/1000 | Loss: 0.00036706
Iteration 10/1000 | Loss: 0.00035672
Iteration 11/1000 | Loss: 0.00035006
Iteration 12/1000 | Loss: 0.00034554
Iteration 13/1000 | Loss: 0.00034244
Iteration 14/1000 | Loss: 0.00034015
Iteration 15/1000 | Loss: 0.00033829
Iteration 16/1000 | Loss: 0.00033687
Iteration 17/1000 | Loss: 0.00033589
Iteration 18/1000 | Loss: 0.00033489
Iteration 19/1000 | Loss: 0.00033413
Iteration 20/1000 | Loss: 0.00033358
Iteration 21/1000 | Loss: 0.00033326
Iteration 22/1000 | Loss: 0.00033309
Iteration 23/1000 | Loss: 0.00033289
Iteration 24/1000 | Loss: 0.00033280
Iteration 25/1000 | Loss: 0.00033267
Iteration 26/1000 | Loss: 0.00033255
Iteration 27/1000 | Loss: 0.00033248
Iteration 28/1000 | Loss: 0.00033245
Iteration 29/1000 | Loss: 0.00033243
Iteration 30/1000 | Loss: 0.00033242
Iteration 31/1000 | Loss: 0.00033239
Iteration 32/1000 | Loss: 0.00033237
Iteration 33/1000 | Loss: 0.00033237
Iteration 34/1000 | Loss: 0.00033236
Iteration 35/1000 | Loss: 0.00033236
Iteration 36/1000 | Loss: 0.00033236
Iteration 37/1000 | Loss: 0.00033236
Iteration 38/1000 | Loss: 0.00033236
Iteration 39/1000 | Loss: 0.00033236
Iteration 40/1000 | Loss: 0.00033235
Iteration 41/1000 | Loss: 0.00033235
Iteration 42/1000 | Loss: 0.00033235
Iteration 43/1000 | Loss: 0.00033234
Iteration 44/1000 | Loss: 0.00033234
Iteration 45/1000 | Loss: 0.00033233
Iteration 46/1000 | Loss: 0.00033233
Iteration 47/1000 | Loss: 0.00033233
Iteration 48/1000 | Loss: 0.00033233
Iteration 49/1000 | Loss: 0.00033232
Iteration 50/1000 | Loss: 0.00033232
Iteration 51/1000 | Loss: 0.00033232
Iteration 52/1000 | Loss: 0.00033232
Iteration 53/1000 | Loss: 0.00033232
Iteration 54/1000 | Loss: 0.00033232
Iteration 55/1000 | Loss: 0.00033232
Iteration 56/1000 | Loss: 0.00033231
Iteration 57/1000 | Loss: 0.00033230
Iteration 58/1000 | Loss: 0.00033229
Iteration 59/1000 | Loss: 0.00033229
Iteration 60/1000 | Loss: 0.00033229
Iteration 61/1000 | Loss: 0.00033228
Iteration 62/1000 | Loss: 0.00033228
Iteration 63/1000 | Loss: 0.00033228
Iteration 64/1000 | Loss: 0.00033227
Iteration 65/1000 | Loss: 0.00033227
Iteration 66/1000 | Loss: 0.00033227
Iteration 67/1000 | Loss: 0.00033227
Iteration 68/1000 | Loss: 0.00033227
Iteration 69/1000 | Loss: 0.00033227
Iteration 70/1000 | Loss: 0.00033227
Iteration 71/1000 | Loss: 0.00033227
Iteration 72/1000 | Loss: 0.00033226
Iteration 73/1000 | Loss: 0.00033226
Iteration 74/1000 | Loss: 0.00033226
Iteration 75/1000 | Loss: 0.00033226
Iteration 76/1000 | Loss: 0.00033226
Iteration 77/1000 | Loss: 0.00033225
Iteration 78/1000 | Loss: 0.00033225
Iteration 79/1000 | Loss: 0.00033224
Iteration 80/1000 | Loss: 0.00033224
Iteration 81/1000 | Loss: 0.00033224
Iteration 82/1000 | Loss: 0.00033224
Iteration 83/1000 | Loss: 0.00033224
Iteration 84/1000 | Loss: 0.00033224
Iteration 85/1000 | Loss: 0.00033224
Iteration 86/1000 | Loss: 0.00033224
Iteration 87/1000 | Loss: 0.00033224
Iteration 88/1000 | Loss: 0.00033224
Iteration 89/1000 | Loss: 0.00033222
Iteration 90/1000 | Loss: 0.00033222
Iteration 91/1000 | Loss: 0.00033222
Iteration 92/1000 | Loss: 0.00033222
Iteration 93/1000 | Loss: 0.00033222
Iteration 94/1000 | Loss: 0.00033222
Iteration 95/1000 | Loss: 0.00033222
Iteration 96/1000 | Loss: 0.00033222
Iteration 97/1000 | Loss: 0.00033222
Iteration 98/1000 | Loss: 0.00033222
Iteration 99/1000 | Loss: 0.00033222
Iteration 100/1000 | Loss: 0.00033221
Iteration 101/1000 | Loss: 0.00033221
Iteration 102/1000 | Loss: 0.00033221
Iteration 103/1000 | Loss: 0.00033221
Iteration 104/1000 | Loss: 0.00033221
Iteration 105/1000 | Loss: 0.00033221
Iteration 106/1000 | Loss: 0.00033221
Iteration 107/1000 | Loss: 0.00033220
Iteration 108/1000 | Loss: 0.00033220
Iteration 109/1000 | Loss: 0.00033220
Iteration 110/1000 | Loss: 0.00033220
Iteration 111/1000 | Loss: 0.00033220
Iteration 112/1000 | Loss: 0.00033219
Iteration 113/1000 | Loss: 0.00033219
Iteration 114/1000 | Loss: 0.00033219
Iteration 115/1000 | Loss: 0.00033219
Iteration 116/1000 | Loss: 0.00033218
Iteration 117/1000 | Loss: 0.00033218
Iteration 118/1000 | Loss: 0.00033218
Iteration 119/1000 | Loss: 0.00033218
Iteration 120/1000 | Loss: 0.00033218
Iteration 121/1000 | Loss: 0.00033218
Iteration 122/1000 | Loss: 0.00033218
Iteration 123/1000 | Loss: 0.00033218
Iteration 124/1000 | Loss: 0.00033218
Iteration 125/1000 | Loss: 0.00033218
Iteration 126/1000 | Loss: 0.00033218
Iteration 127/1000 | Loss: 0.00033218
Iteration 128/1000 | Loss: 0.00033218
Iteration 129/1000 | Loss: 0.00033218
Iteration 130/1000 | Loss: 0.00033218
Iteration 131/1000 | Loss: 0.00033217
Iteration 132/1000 | Loss: 0.00033217
Iteration 133/1000 | Loss: 0.00033217
Iteration 134/1000 | Loss: 0.00033217
Iteration 135/1000 | Loss: 0.00033217
Iteration 136/1000 | Loss: 0.00033217
Iteration 137/1000 | Loss: 0.00033216
Iteration 138/1000 | Loss: 0.00033216
Iteration 139/1000 | Loss: 0.00033216
Iteration 140/1000 | Loss: 0.00033216
Iteration 141/1000 | Loss: 0.00033216
Iteration 142/1000 | Loss: 0.00033216
Iteration 143/1000 | Loss: 0.00033216
Iteration 144/1000 | Loss: 0.00033216
Iteration 145/1000 | Loss: 0.00033216
Iteration 146/1000 | Loss: 0.00033216
Iteration 147/1000 | Loss: 0.00033216
Iteration 148/1000 | Loss: 0.00033216
Iteration 149/1000 | Loss: 0.00033216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [0.00033215980511158705, 0.00033215980511158705, 0.00033215980511158705, 0.00033215980511158705, 0.00033215980511158705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033215980511158705

Optimization complete. Final v2v error: 9.865964889526367 mm

Highest mean error: 12.08663272857666 mm for frame 57

Lowest mean error: 4.9095916748046875 mm for frame 142

Saving results

Total time: 100.4664478302002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769520
Iteration 2/25 | Loss: 0.00145611
Iteration 3/25 | Loss: 0.00120461
Iteration 4/25 | Loss: 0.00118391
Iteration 5/25 | Loss: 0.00118269
Iteration 6/25 | Loss: 0.00118269
Iteration 7/25 | Loss: 0.00118269
Iteration 8/25 | Loss: 0.00118269
Iteration 9/25 | Loss: 0.00118269
Iteration 10/25 | Loss: 0.00118269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001182687352411449, 0.001182687352411449, 0.001182687352411449, 0.001182687352411449, 0.001182687352411449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001182687352411449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19654155
Iteration 2/25 | Loss: 0.00151176
Iteration 3/25 | Loss: 0.00151174
Iteration 4/25 | Loss: 0.00151174
Iteration 5/25 | Loss: 0.00151174
Iteration 6/25 | Loss: 0.00151174
Iteration 7/25 | Loss: 0.00151174
Iteration 8/25 | Loss: 0.00151174
Iteration 9/25 | Loss: 0.00151174
Iteration 10/25 | Loss: 0.00151174
Iteration 11/25 | Loss: 0.00151174
Iteration 12/25 | Loss: 0.00151174
Iteration 13/25 | Loss: 0.00151174
Iteration 14/25 | Loss: 0.00151174
Iteration 15/25 | Loss: 0.00151174
Iteration 16/25 | Loss: 0.00151174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015117354923859239, 0.0015117354923859239, 0.0015117354923859239, 0.0015117354923859239, 0.0015117354923859239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015117354923859239

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151174
Iteration 2/1000 | Loss: 0.00002468
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001443
Iteration 5/1000 | Loss: 0.00001355
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001132
Iteration 11/1000 | Loss: 0.00001121
Iteration 12/1000 | Loss: 0.00001111
Iteration 13/1000 | Loss: 0.00001110
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001087
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001079
Iteration 19/1000 | Loss: 0.00001070
Iteration 20/1000 | Loss: 0.00001069
Iteration 21/1000 | Loss: 0.00001067
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001065
Iteration 24/1000 | Loss: 0.00001064
Iteration 25/1000 | Loss: 0.00001064
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001063
Iteration 28/1000 | Loss: 0.00001061
Iteration 29/1000 | Loss: 0.00001060
Iteration 30/1000 | Loss: 0.00001059
Iteration 31/1000 | Loss: 0.00001058
Iteration 32/1000 | Loss: 0.00001057
Iteration 33/1000 | Loss: 0.00001056
Iteration 34/1000 | Loss: 0.00001054
Iteration 35/1000 | Loss: 0.00001051
Iteration 36/1000 | Loss: 0.00001051
Iteration 37/1000 | Loss: 0.00001051
Iteration 38/1000 | Loss: 0.00001048
Iteration 39/1000 | Loss: 0.00001048
Iteration 40/1000 | Loss: 0.00001047
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001040
Iteration 44/1000 | Loss: 0.00001037
Iteration 45/1000 | Loss: 0.00001036
Iteration 46/1000 | Loss: 0.00001036
Iteration 47/1000 | Loss: 0.00001036
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001034
Iteration 50/1000 | Loss: 0.00001033
Iteration 51/1000 | Loss: 0.00001033
Iteration 52/1000 | Loss: 0.00001033
Iteration 53/1000 | Loss: 0.00001030
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001028
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001025
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001024
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001021
Iteration 70/1000 | Loss: 0.00001021
Iteration 71/1000 | Loss: 0.00001021
Iteration 72/1000 | Loss: 0.00001021
Iteration 73/1000 | Loss: 0.00001021
Iteration 74/1000 | Loss: 0.00001021
Iteration 75/1000 | Loss: 0.00001021
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001019
Iteration 81/1000 | Loss: 0.00001018
Iteration 82/1000 | Loss: 0.00001018
Iteration 83/1000 | Loss: 0.00001018
Iteration 84/1000 | Loss: 0.00001018
Iteration 85/1000 | Loss: 0.00001018
Iteration 86/1000 | Loss: 0.00001017
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001016
Iteration 90/1000 | Loss: 0.00001016
Iteration 91/1000 | Loss: 0.00001016
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001015
Iteration 94/1000 | Loss: 0.00001015
Iteration 95/1000 | Loss: 0.00001015
Iteration 96/1000 | Loss: 0.00001015
Iteration 97/1000 | Loss: 0.00001015
Iteration 98/1000 | Loss: 0.00001015
Iteration 99/1000 | Loss: 0.00001015
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001015
Iteration 103/1000 | Loss: 0.00001015
Iteration 104/1000 | Loss: 0.00001015
Iteration 105/1000 | Loss: 0.00001015
Iteration 106/1000 | Loss: 0.00001015
Iteration 107/1000 | Loss: 0.00001015
Iteration 108/1000 | Loss: 0.00001014
Iteration 109/1000 | Loss: 0.00001014
Iteration 110/1000 | Loss: 0.00001014
Iteration 111/1000 | Loss: 0.00001014
Iteration 112/1000 | Loss: 0.00001014
Iteration 113/1000 | Loss: 0.00001014
Iteration 114/1000 | Loss: 0.00001014
Iteration 115/1000 | Loss: 0.00001014
Iteration 116/1000 | Loss: 0.00001014
Iteration 117/1000 | Loss: 0.00001014
Iteration 118/1000 | Loss: 0.00001014
Iteration 119/1000 | Loss: 0.00001014
Iteration 120/1000 | Loss: 0.00001014
Iteration 121/1000 | Loss: 0.00001014
Iteration 122/1000 | Loss: 0.00001014
Iteration 123/1000 | Loss: 0.00001014
Iteration 124/1000 | Loss: 0.00001014
Iteration 125/1000 | Loss: 0.00001014
Iteration 126/1000 | Loss: 0.00001013
Iteration 127/1000 | Loss: 0.00001013
Iteration 128/1000 | Loss: 0.00001013
Iteration 129/1000 | Loss: 0.00001013
Iteration 130/1000 | Loss: 0.00001013
Iteration 131/1000 | Loss: 0.00001013
Iteration 132/1000 | Loss: 0.00001013
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001012
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001012
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Iteration 146/1000 | Loss: 0.00001012
Iteration 147/1000 | Loss: 0.00001012
Iteration 148/1000 | Loss: 0.00001012
Iteration 149/1000 | Loss: 0.00001012
Iteration 150/1000 | Loss: 0.00001012
Iteration 151/1000 | Loss: 0.00001012
Iteration 152/1000 | Loss: 0.00001012
Iteration 153/1000 | Loss: 0.00001012
Iteration 154/1000 | Loss: 0.00001012
Iteration 155/1000 | Loss: 0.00001012
Iteration 156/1000 | Loss: 0.00001012
Iteration 157/1000 | Loss: 0.00001012
Iteration 158/1000 | Loss: 0.00001012
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.0122908861376345e-05, 1.0122908861376345e-05, 1.0122908861376345e-05, 1.0122908861376345e-05, 1.0122908861376345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0122908861376345e-05

Optimization complete. Final v2v error: 2.712360143661499 mm

Highest mean error: 2.98124098777771 mm for frame 108

Lowest mean error: 2.547898769378662 mm for frame 50

Saving results

Total time: 38.040377140045166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710871
Iteration 2/25 | Loss: 0.00150584
Iteration 3/25 | Loss: 0.00130502
Iteration 4/25 | Loss: 0.00128031
Iteration 5/25 | Loss: 0.00127594
Iteration 6/25 | Loss: 0.00127374
Iteration 7/25 | Loss: 0.00127255
Iteration 8/25 | Loss: 0.00127162
Iteration 9/25 | Loss: 0.00127078
Iteration 10/25 | Loss: 0.00127603
Iteration 11/25 | Loss: 0.00127102
Iteration 12/25 | Loss: 0.00126600
Iteration 13/25 | Loss: 0.00126445
Iteration 14/25 | Loss: 0.00126424
Iteration 15/25 | Loss: 0.00126418
Iteration 16/25 | Loss: 0.00126418
Iteration 17/25 | Loss: 0.00126417
Iteration 18/25 | Loss: 0.00126417
Iteration 19/25 | Loss: 0.00126417
Iteration 20/25 | Loss: 0.00126417
Iteration 21/25 | Loss: 0.00126417
Iteration 22/25 | Loss: 0.00126417
Iteration 23/25 | Loss: 0.00126417
Iteration 24/25 | Loss: 0.00126417
Iteration 25/25 | Loss: 0.00126417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15751529
Iteration 2/25 | Loss: 0.00201967
Iteration 3/25 | Loss: 0.00201964
Iteration 4/25 | Loss: 0.00201964
Iteration 5/25 | Loss: 0.00201964
Iteration 6/25 | Loss: 0.00201964
Iteration 7/25 | Loss: 0.00201964
Iteration 8/25 | Loss: 0.00201964
Iteration 9/25 | Loss: 0.00201964
Iteration 10/25 | Loss: 0.00201964
Iteration 11/25 | Loss: 0.00201964
Iteration 12/25 | Loss: 0.00201964
Iteration 13/25 | Loss: 0.00201964
Iteration 14/25 | Loss: 0.00201964
Iteration 15/25 | Loss: 0.00201964
Iteration 16/25 | Loss: 0.00201964
Iteration 17/25 | Loss: 0.00201964
Iteration 18/25 | Loss: 0.00201964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00201963703148067, 0.00201963703148067, 0.00201963703148067, 0.00201963703148067, 0.00201963703148067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00201963703148067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201964
Iteration 2/1000 | Loss: 0.00007606
Iteration 3/1000 | Loss: 0.00005321
Iteration 4/1000 | Loss: 0.00004671
Iteration 5/1000 | Loss: 0.00004269
Iteration 6/1000 | Loss: 0.00004095
Iteration 7/1000 | Loss: 0.00003981
Iteration 8/1000 | Loss: 0.00003915
Iteration 9/1000 | Loss: 0.00003860
Iteration 10/1000 | Loss: 0.00003823
Iteration 11/1000 | Loss: 0.00003793
Iteration 12/1000 | Loss: 0.00003767
Iteration 13/1000 | Loss: 0.00003738
Iteration 14/1000 | Loss: 0.00003711
Iteration 15/1000 | Loss: 0.00003690
Iteration 16/1000 | Loss: 0.00003681
Iteration 17/1000 | Loss: 0.00003677
Iteration 18/1000 | Loss: 0.00003670
Iteration 19/1000 | Loss: 0.00003669
Iteration 20/1000 | Loss: 0.00003661
Iteration 21/1000 | Loss: 0.00003660
Iteration 22/1000 | Loss: 0.00003653
Iteration 23/1000 | Loss: 0.00003647
Iteration 24/1000 | Loss: 0.00003642
Iteration 25/1000 | Loss: 0.00003642
Iteration 26/1000 | Loss: 0.00003642
Iteration 27/1000 | Loss: 0.00003641
Iteration 28/1000 | Loss: 0.00003639
Iteration 29/1000 | Loss: 0.00003633
Iteration 30/1000 | Loss: 0.00003632
Iteration 31/1000 | Loss: 0.00003627
Iteration 32/1000 | Loss: 0.00003626
Iteration 33/1000 | Loss: 0.00003626
Iteration 34/1000 | Loss: 0.00003626
Iteration 35/1000 | Loss: 0.00003625
Iteration 36/1000 | Loss: 0.00003624
Iteration 37/1000 | Loss: 0.00003624
Iteration 38/1000 | Loss: 0.00003623
Iteration 39/1000 | Loss: 0.00003623
Iteration 40/1000 | Loss: 0.00003623
Iteration 41/1000 | Loss: 0.00003622
Iteration 42/1000 | Loss: 0.00003622
Iteration 43/1000 | Loss: 0.00003621
Iteration 44/1000 | Loss: 0.00003621
Iteration 45/1000 | Loss: 0.00003620
Iteration 46/1000 | Loss: 0.00003620
Iteration 47/1000 | Loss: 0.00003620
Iteration 48/1000 | Loss: 0.00003620
Iteration 49/1000 | Loss: 0.00003620
Iteration 50/1000 | Loss: 0.00003620
Iteration 51/1000 | Loss: 0.00003619
Iteration 52/1000 | Loss: 0.00003619
Iteration 53/1000 | Loss: 0.00003617
Iteration 54/1000 | Loss: 0.00003617
Iteration 55/1000 | Loss: 0.00003617
Iteration 56/1000 | Loss: 0.00003616
Iteration 57/1000 | Loss: 0.00003616
Iteration 58/1000 | Loss: 0.00003616
Iteration 59/1000 | Loss: 0.00003616
Iteration 60/1000 | Loss: 0.00003616
Iteration 61/1000 | Loss: 0.00003616
Iteration 62/1000 | Loss: 0.00003615
Iteration 63/1000 | Loss: 0.00003615
Iteration 64/1000 | Loss: 0.00003615
Iteration 65/1000 | Loss: 0.00003615
Iteration 66/1000 | Loss: 0.00003615
Iteration 67/1000 | Loss: 0.00003614
Iteration 68/1000 | Loss: 0.00003614
Iteration 69/1000 | Loss: 0.00003614
Iteration 70/1000 | Loss: 0.00003612
Iteration 71/1000 | Loss: 0.00003612
Iteration 72/1000 | Loss: 0.00003611
Iteration 73/1000 | Loss: 0.00003611
Iteration 74/1000 | Loss: 0.00003611
Iteration 75/1000 | Loss: 0.00003611
Iteration 76/1000 | Loss: 0.00003611
Iteration 77/1000 | Loss: 0.00003611
Iteration 78/1000 | Loss: 0.00003611
Iteration 79/1000 | Loss: 0.00003611
Iteration 80/1000 | Loss: 0.00003611
Iteration 81/1000 | Loss: 0.00003611
Iteration 82/1000 | Loss: 0.00003610
Iteration 83/1000 | Loss: 0.00003610
Iteration 84/1000 | Loss: 0.00003610
Iteration 85/1000 | Loss: 0.00003609
Iteration 86/1000 | Loss: 0.00003609
Iteration 87/1000 | Loss: 0.00003609
Iteration 88/1000 | Loss: 0.00003608
Iteration 89/1000 | Loss: 0.00003608
Iteration 90/1000 | Loss: 0.00003607
Iteration 91/1000 | Loss: 0.00003607
Iteration 92/1000 | Loss: 0.00003607
Iteration 93/1000 | Loss: 0.00003606
Iteration 94/1000 | Loss: 0.00003606
Iteration 95/1000 | Loss: 0.00003606
Iteration 96/1000 | Loss: 0.00003606
Iteration 97/1000 | Loss: 0.00003606
Iteration 98/1000 | Loss: 0.00003605
Iteration 99/1000 | Loss: 0.00003605
Iteration 100/1000 | Loss: 0.00003605
Iteration 101/1000 | Loss: 0.00003605
Iteration 102/1000 | Loss: 0.00003605
Iteration 103/1000 | Loss: 0.00003605
Iteration 104/1000 | Loss: 0.00003605
Iteration 105/1000 | Loss: 0.00003605
Iteration 106/1000 | Loss: 0.00003605
Iteration 107/1000 | Loss: 0.00003604
Iteration 108/1000 | Loss: 0.00003604
Iteration 109/1000 | Loss: 0.00003604
Iteration 110/1000 | Loss: 0.00003604
Iteration 111/1000 | Loss: 0.00003603
Iteration 112/1000 | Loss: 0.00003603
Iteration 113/1000 | Loss: 0.00003603
Iteration 114/1000 | Loss: 0.00003602
Iteration 115/1000 | Loss: 0.00003602
Iteration 116/1000 | Loss: 0.00003602
Iteration 117/1000 | Loss: 0.00003602
Iteration 118/1000 | Loss: 0.00003601
Iteration 119/1000 | Loss: 0.00003601
Iteration 120/1000 | Loss: 0.00003601
Iteration 121/1000 | Loss: 0.00003601
Iteration 122/1000 | Loss: 0.00003601
Iteration 123/1000 | Loss: 0.00003601
Iteration 124/1000 | Loss: 0.00003601
Iteration 125/1000 | Loss: 0.00003601
Iteration 126/1000 | Loss: 0.00003601
Iteration 127/1000 | Loss: 0.00003601
Iteration 128/1000 | Loss: 0.00003601
Iteration 129/1000 | Loss: 0.00003601
Iteration 130/1000 | Loss: 0.00003601
Iteration 131/1000 | Loss: 0.00003601
Iteration 132/1000 | Loss: 0.00003601
Iteration 133/1000 | Loss: 0.00003601
Iteration 134/1000 | Loss: 0.00003601
Iteration 135/1000 | Loss: 0.00003601
Iteration 136/1000 | Loss: 0.00003601
Iteration 137/1000 | Loss: 0.00003601
Iteration 138/1000 | Loss: 0.00003601
Iteration 139/1000 | Loss: 0.00003601
Iteration 140/1000 | Loss: 0.00003601
Iteration 141/1000 | Loss: 0.00003601
Iteration 142/1000 | Loss: 0.00003601
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.6007462767884135e-05, 3.6007462767884135e-05, 3.6007462767884135e-05, 3.6007462767884135e-05, 3.6007462767884135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6007462767884135e-05

Optimization complete. Final v2v error: 3.7267513275146484 mm

Highest mean error: 11.870349884033203 mm for frame 19

Lowest mean error: 2.6341545581817627 mm for frame 231

Saving results

Total time: 70.30955815315247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923565
Iteration 2/25 | Loss: 0.00138524
Iteration 3/25 | Loss: 0.00126893
Iteration 4/25 | Loss: 0.00124939
Iteration 5/25 | Loss: 0.00124181
Iteration 6/25 | Loss: 0.00123955
Iteration 7/25 | Loss: 0.00123912
Iteration 8/25 | Loss: 0.00123912
Iteration 9/25 | Loss: 0.00123912
Iteration 10/25 | Loss: 0.00123912
Iteration 11/25 | Loss: 0.00123912
Iteration 12/25 | Loss: 0.00123912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012391159543767571, 0.0012391159543767571, 0.0012391159543767571, 0.0012391159543767571, 0.0012391159543767571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012391159543767571

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15732975
Iteration 2/25 | Loss: 0.00159900
Iteration 3/25 | Loss: 0.00159899
Iteration 4/25 | Loss: 0.00159899
Iteration 5/25 | Loss: 0.00159899
Iteration 6/25 | Loss: 0.00159899
Iteration 7/25 | Loss: 0.00159899
Iteration 8/25 | Loss: 0.00159899
Iteration 9/25 | Loss: 0.00159899
Iteration 10/25 | Loss: 0.00159899
Iteration 11/25 | Loss: 0.00159899
Iteration 12/25 | Loss: 0.00159899
Iteration 13/25 | Loss: 0.00159899
Iteration 14/25 | Loss: 0.00159899
Iteration 15/25 | Loss: 0.00159899
Iteration 16/25 | Loss: 0.00159899
Iteration 17/25 | Loss: 0.00159899
Iteration 18/25 | Loss: 0.00159899
Iteration 19/25 | Loss: 0.00159899
Iteration 20/25 | Loss: 0.00159899
Iteration 21/25 | Loss: 0.00159899
Iteration 22/25 | Loss: 0.00159899
Iteration 23/25 | Loss: 0.00159899
Iteration 24/25 | Loss: 0.00159899
Iteration 25/25 | Loss: 0.00159899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159899
Iteration 2/1000 | Loss: 0.00005561
Iteration 3/1000 | Loss: 0.00003834
Iteration 4/1000 | Loss: 0.00003211
Iteration 5/1000 | Loss: 0.00002942
Iteration 6/1000 | Loss: 0.00002828
Iteration 7/1000 | Loss: 0.00002721
Iteration 8/1000 | Loss: 0.00002670
Iteration 9/1000 | Loss: 0.00002632
Iteration 10/1000 | Loss: 0.00002589
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002526
Iteration 13/1000 | Loss: 0.00002525
Iteration 14/1000 | Loss: 0.00002524
Iteration 15/1000 | Loss: 0.00002499
Iteration 16/1000 | Loss: 0.00002489
Iteration 17/1000 | Loss: 0.00002483
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002473
Iteration 20/1000 | Loss: 0.00002473
Iteration 21/1000 | Loss: 0.00002470
Iteration 22/1000 | Loss: 0.00002469
Iteration 23/1000 | Loss: 0.00002469
Iteration 24/1000 | Loss: 0.00002468
Iteration 25/1000 | Loss: 0.00002452
Iteration 26/1000 | Loss: 0.00002446
Iteration 27/1000 | Loss: 0.00002445
Iteration 28/1000 | Loss: 0.00002445
Iteration 29/1000 | Loss: 0.00002441
Iteration 30/1000 | Loss: 0.00002440
Iteration 31/1000 | Loss: 0.00002440
Iteration 32/1000 | Loss: 0.00002439
Iteration 33/1000 | Loss: 0.00002435
Iteration 34/1000 | Loss: 0.00002435
Iteration 35/1000 | Loss: 0.00002434
Iteration 36/1000 | Loss: 0.00002432
Iteration 37/1000 | Loss: 0.00002426
Iteration 38/1000 | Loss: 0.00002426
Iteration 39/1000 | Loss: 0.00002425
Iteration 40/1000 | Loss: 0.00002421
Iteration 41/1000 | Loss: 0.00002421
Iteration 42/1000 | Loss: 0.00002419
Iteration 43/1000 | Loss: 0.00002418
Iteration 44/1000 | Loss: 0.00002418
Iteration 45/1000 | Loss: 0.00002418
Iteration 46/1000 | Loss: 0.00002414
Iteration 47/1000 | Loss: 0.00002414
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00002412
Iteration 51/1000 | Loss: 0.00002412
Iteration 52/1000 | Loss: 0.00002411
Iteration 53/1000 | Loss: 0.00002411
Iteration 54/1000 | Loss: 0.00002410
Iteration 55/1000 | Loss: 0.00002410
Iteration 56/1000 | Loss: 0.00002410
Iteration 57/1000 | Loss: 0.00002410
Iteration 58/1000 | Loss: 0.00002410
Iteration 59/1000 | Loss: 0.00002410
Iteration 60/1000 | Loss: 0.00002410
Iteration 61/1000 | Loss: 0.00002410
Iteration 62/1000 | Loss: 0.00002409
Iteration 63/1000 | Loss: 0.00002409
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002408
Iteration 68/1000 | Loss: 0.00002408
Iteration 69/1000 | Loss: 0.00002408
Iteration 70/1000 | Loss: 0.00002408
Iteration 71/1000 | Loss: 0.00002407
Iteration 72/1000 | Loss: 0.00002407
Iteration 73/1000 | Loss: 0.00002407
Iteration 74/1000 | Loss: 0.00002407
Iteration 75/1000 | Loss: 0.00002407
Iteration 76/1000 | Loss: 0.00002406
Iteration 77/1000 | Loss: 0.00002406
Iteration 78/1000 | Loss: 0.00002406
Iteration 79/1000 | Loss: 0.00002406
Iteration 80/1000 | Loss: 0.00002406
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002405
Iteration 85/1000 | Loss: 0.00002405
Iteration 86/1000 | Loss: 0.00002404
Iteration 87/1000 | Loss: 0.00002404
Iteration 88/1000 | Loss: 0.00002404
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002404
Iteration 91/1000 | Loss: 0.00002404
Iteration 92/1000 | Loss: 0.00002403
Iteration 93/1000 | Loss: 0.00002403
Iteration 94/1000 | Loss: 0.00002403
Iteration 95/1000 | Loss: 0.00002403
Iteration 96/1000 | Loss: 0.00002403
Iteration 97/1000 | Loss: 0.00002403
Iteration 98/1000 | Loss: 0.00002403
Iteration 99/1000 | Loss: 0.00002403
Iteration 100/1000 | Loss: 0.00002402
Iteration 101/1000 | Loss: 0.00002402
Iteration 102/1000 | Loss: 0.00002402
Iteration 103/1000 | Loss: 0.00002402
Iteration 104/1000 | Loss: 0.00002402
Iteration 105/1000 | Loss: 0.00002402
Iteration 106/1000 | Loss: 0.00002402
Iteration 107/1000 | Loss: 0.00002401
Iteration 108/1000 | Loss: 0.00002401
Iteration 109/1000 | Loss: 0.00002401
Iteration 110/1000 | Loss: 0.00002401
Iteration 111/1000 | Loss: 0.00002401
Iteration 112/1000 | Loss: 0.00002401
Iteration 113/1000 | Loss: 0.00002401
Iteration 114/1000 | Loss: 0.00002401
Iteration 115/1000 | Loss: 0.00002401
Iteration 116/1000 | Loss: 0.00002401
Iteration 117/1000 | Loss: 0.00002401
Iteration 118/1000 | Loss: 0.00002401
Iteration 119/1000 | Loss: 0.00002401
Iteration 120/1000 | Loss: 0.00002401
Iteration 121/1000 | Loss: 0.00002400
Iteration 122/1000 | Loss: 0.00002400
Iteration 123/1000 | Loss: 0.00002400
Iteration 124/1000 | Loss: 0.00002400
Iteration 125/1000 | Loss: 0.00002400
Iteration 126/1000 | Loss: 0.00002400
Iteration 127/1000 | Loss: 0.00002400
Iteration 128/1000 | Loss: 0.00002400
Iteration 129/1000 | Loss: 0.00002400
Iteration 130/1000 | Loss: 0.00002400
Iteration 131/1000 | Loss: 0.00002399
Iteration 132/1000 | Loss: 0.00002399
Iteration 133/1000 | Loss: 0.00002399
Iteration 134/1000 | Loss: 0.00002399
Iteration 135/1000 | Loss: 0.00002399
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002399
Iteration 140/1000 | Loss: 0.00002399
Iteration 141/1000 | Loss: 0.00002399
Iteration 142/1000 | Loss: 0.00002399
Iteration 143/1000 | Loss: 0.00002399
Iteration 144/1000 | Loss: 0.00002399
Iteration 145/1000 | Loss: 0.00002399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.3990854970179498e-05, 2.3990854970179498e-05, 2.3990854970179498e-05, 2.3990854970179498e-05, 2.3990854970179498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3990854970179498e-05

Optimization complete. Final v2v error: 4.069035530090332 mm

Highest mean error: 4.345171928405762 mm for frame 47

Lowest mean error: 3.648430585861206 mm for frame 8

Saving results

Total time: 41.3878436088562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830893
Iteration 2/25 | Loss: 0.00203200
Iteration 3/25 | Loss: 0.00156532
Iteration 4/25 | Loss: 0.00149965
Iteration 5/25 | Loss: 0.00146723
Iteration 6/25 | Loss: 0.00144749
Iteration 7/25 | Loss: 0.00138754
Iteration 8/25 | Loss: 0.00137743
Iteration 9/25 | Loss: 0.00136754
Iteration 10/25 | Loss: 0.00136206
Iteration 11/25 | Loss: 0.00136250
Iteration 12/25 | Loss: 0.00136136
Iteration 13/25 | Loss: 0.00136169
Iteration 14/25 | Loss: 0.00136134
Iteration 15/25 | Loss: 0.00136049
Iteration 16/25 | Loss: 0.00136198
Iteration 17/25 | Loss: 0.00136312
Iteration 18/25 | Loss: 0.00136408
Iteration 19/25 | Loss: 0.00136271
Iteration 20/25 | Loss: 0.00136033
Iteration 21/25 | Loss: 0.00136027
Iteration 22/25 | Loss: 0.00136193
Iteration 23/25 | Loss: 0.00136028
Iteration 24/25 | Loss: 0.00136014
Iteration 25/25 | Loss: 0.00135933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25180292
Iteration 2/25 | Loss: 0.00156993
Iteration 3/25 | Loss: 0.00156991
Iteration 4/25 | Loss: 0.00156991
Iteration 5/25 | Loss: 0.00156991
Iteration 6/25 | Loss: 0.00156991
Iteration 7/25 | Loss: 0.00156990
Iteration 8/25 | Loss: 0.00156990
Iteration 9/25 | Loss: 0.00156990
Iteration 10/25 | Loss: 0.00156990
Iteration 11/25 | Loss: 0.00156990
Iteration 12/25 | Loss: 0.00156990
Iteration 13/25 | Loss: 0.00156990
Iteration 14/25 | Loss: 0.00156990
Iteration 15/25 | Loss: 0.00156990
Iteration 16/25 | Loss: 0.00156990
Iteration 17/25 | Loss: 0.00156990
Iteration 18/25 | Loss: 0.00156990
Iteration 19/25 | Loss: 0.00156990
Iteration 20/25 | Loss: 0.00156990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015699032228440046, 0.0015699032228440046, 0.0015699032228440046, 0.0015699032228440046, 0.0015699032228440046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015699032228440046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156990
Iteration 2/1000 | Loss: 0.00009586
Iteration 3/1000 | Loss: 0.00008097
Iteration 4/1000 | Loss: 0.00008497
Iteration 5/1000 | Loss: 0.00009080
Iteration 6/1000 | Loss: 0.00008284
Iteration 7/1000 | Loss: 0.00010616
Iteration 8/1000 | Loss: 0.00010607
Iteration 9/1000 | Loss: 0.00011411
Iteration 10/1000 | Loss: 0.00008589
Iteration 11/1000 | Loss: 0.00008684
Iteration 12/1000 | Loss: 0.00008629
Iteration 13/1000 | Loss: 0.00008094
Iteration 14/1000 | Loss: 0.00009506
Iteration 15/1000 | Loss: 0.00010749
Iteration 16/1000 | Loss: 0.00009274
Iteration 17/1000 | Loss: 0.00007994
Iteration 18/1000 | Loss: 0.00011207
Iteration 19/1000 | Loss: 0.00008763
Iteration 20/1000 | Loss: 0.00010839
Iteration 21/1000 | Loss: 0.00011085
Iteration 22/1000 | Loss: 0.00011675
Iteration 23/1000 | Loss: 0.00012473
Iteration 24/1000 | Loss: 0.00013133
Iteration 25/1000 | Loss: 0.00013058
Iteration 26/1000 | Loss: 0.00020326
Iteration 27/1000 | Loss: 0.00014189
Iteration 28/1000 | Loss: 0.00012688
Iteration 29/1000 | Loss: 0.00018218
Iteration 30/1000 | Loss: 0.00008068
Iteration 31/1000 | Loss: 0.00022163
Iteration 32/1000 | Loss: 0.00007008
Iteration 33/1000 | Loss: 0.00009516
Iteration 34/1000 | Loss: 0.00010568
Iteration 35/1000 | Loss: 0.00011106
Iteration 36/1000 | Loss: 0.00005220
Iteration 37/1000 | Loss: 0.00007271
Iteration 38/1000 | Loss: 0.00006178
Iteration 39/1000 | Loss: 0.00006451
Iteration 40/1000 | Loss: 0.00007393
Iteration 41/1000 | Loss: 0.00007815
Iteration 42/1000 | Loss: 0.00006783
Iteration 43/1000 | Loss: 0.00006867
Iteration 44/1000 | Loss: 0.00007619
Iteration 45/1000 | Loss: 0.00009795
Iteration 46/1000 | Loss: 0.00008913
Iteration 47/1000 | Loss: 0.00006372
Iteration 48/1000 | Loss: 0.00007876
Iteration 49/1000 | Loss: 0.00008411
Iteration 50/1000 | Loss: 0.00007193
Iteration 51/1000 | Loss: 0.00008565
Iteration 52/1000 | Loss: 0.00008838
Iteration 53/1000 | Loss: 0.00013132
Iteration 54/1000 | Loss: 0.00007797
Iteration 55/1000 | Loss: 0.00008207
Iteration 56/1000 | Loss: 0.00007222
Iteration 57/1000 | Loss: 0.00009643
Iteration 58/1000 | Loss: 0.00007911
Iteration 59/1000 | Loss: 0.00008512
Iteration 60/1000 | Loss: 0.00006873
Iteration 61/1000 | Loss: 0.00006997
Iteration 62/1000 | Loss: 0.00007149
Iteration 63/1000 | Loss: 0.00008952
Iteration 64/1000 | Loss: 0.00008506
Iteration 65/1000 | Loss: 0.00008284
Iteration 66/1000 | Loss: 0.00007813
Iteration 67/1000 | Loss: 0.00005987
Iteration 68/1000 | Loss: 0.00006604
Iteration 69/1000 | Loss: 0.00007740
Iteration 70/1000 | Loss: 0.00009683
Iteration 71/1000 | Loss: 0.00008467
Iteration 72/1000 | Loss: 0.00008656
Iteration 73/1000 | Loss: 0.00008267
Iteration 74/1000 | Loss: 0.00008188
Iteration 75/1000 | Loss: 0.00005598
Iteration 76/1000 | Loss: 0.00007099
Iteration 77/1000 | Loss: 0.00008738
Iteration 78/1000 | Loss: 0.00008696
Iteration 79/1000 | Loss: 0.00006840
Iteration 80/1000 | Loss: 0.00006829
Iteration 81/1000 | Loss: 0.00006759
Iteration 82/1000 | Loss: 0.00007336
Iteration 83/1000 | Loss: 0.00005958
Iteration 84/1000 | Loss: 0.00006699
Iteration 85/1000 | Loss: 0.00005678
Iteration 86/1000 | Loss: 0.00004550
Iteration 87/1000 | Loss: 0.00006974
Iteration 88/1000 | Loss: 0.00006423
Iteration 89/1000 | Loss: 0.00007277
Iteration 90/1000 | Loss: 0.00006630
Iteration 91/1000 | Loss: 0.00006802
Iteration 92/1000 | Loss: 0.00007780
Iteration 93/1000 | Loss: 0.00009795
Iteration 94/1000 | Loss: 0.00005910
Iteration 95/1000 | Loss: 0.00009212
Iteration 96/1000 | Loss: 0.00009192
Iteration 97/1000 | Loss: 0.00006119
Iteration 98/1000 | Loss: 0.00008226
Iteration 99/1000 | Loss: 0.00004844
Iteration 100/1000 | Loss: 0.00003064
Iteration 101/1000 | Loss: 0.00003578
Iteration 102/1000 | Loss: 0.00004565
Iteration 103/1000 | Loss: 0.00004907
Iteration 104/1000 | Loss: 0.00005341
Iteration 105/1000 | Loss: 0.00005424
Iteration 106/1000 | Loss: 0.00006749
Iteration 107/1000 | Loss: 0.00004642
Iteration 108/1000 | Loss: 0.00003971
Iteration 109/1000 | Loss: 0.00004217
Iteration 110/1000 | Loss: 0.00003956
Iteration 111/1000 | Loss: 0.00003806
Iteration 112/1000 | Loss: 0.00004693
Iteration 113/1000 | Loss: 0.00004131
Iteration 114/1000 | Loss: 0.00004274
Iteration 115/1000 | Loss: 0.00004276
Iteration 116/1000 | Loss: 0.00003961
Iteration 117/1000 | Loss: 0.00004819
Iteration 118/1000 | Loss: 0.00004687
Iteration 119/1000 | Loss: 0.00003520
Iteration 120/1000 | Loss: 0.00005022
Iteration 121/1000 | Loss: 0.00004039
Iteration 122/1000 | Loss: 0.00004222
Iteration 123/1000 | Loss: 0.00004155
Iteration 124/1000 | Loss: 0.00004152
Iteration 125/1000 | Loss: 0.00003447
Iteration 126/1000 | Loss: 0.00003415
Iteration 127/1000 | Loss: 0.00003200
Iteration 128/1000 | Loss: 0.00003064
Iteration 129/1000 | Loss: 0.00003120
Iteration 130/1000 | Loss: 0.00004840
Iteration 131/1000 | Loss: 0.00005005
Iteration 132/1000 | Loss: 0.00005245
Iteration 133/1000 | Loss: 0.00005314
Iteration 134/1000 | Loss: 0.00004750
Iteration 135/1000 | Loss: 0.00005859
Iteration 136/1000 | Loss: 0.00005046
Iteration 137/1000 | Loss: 0.00006659
Iteration 138/1000 | Loss: 0.00005057
Iteration 139/1000 | Loss: 0.00006301
Iteration 140/1000 | Loss: 0.00005385
Iteration 141/1000 | Loss: 0.00004998
Iteration 142/1000 | Loss: 0.00005340
Iteration 143/1000 | Loss: 0.00007896
Iteration 144/1000 | Loss: 0.00005471
Iteration 145/1000 | Loss: 0.00005441
Iteration 146/1000 | Loss: 0.00004080
Iteration 147/1000 | Loss: 0.00003829
Iteration 148/1000 | Loss: 0.00003261
Iteration 149/1000 | Loss: 0.00002560
Iteration 150/1000 | Loss: 0.00003575
Iteration 151/1000 | Loss: 0.00003717
Iteration 152/1000 | Loss: 0.00004875
Iteration 153/1000 | Loss: 0.00004145
Iteration 154/1000 | Loss: 0.00003873
Iteration 155/1000 | Loss: 0.00003800
Iteration 156/1000 | Loss: 0.00003371
Iteration 157/1000 | Loss: 0.00003898
Iteration 158/1000 | Loss: 0.00003837
Iteration 159/1000 | Loss: 0.00003861
Iteration 160/1000 | Loss: 0.00003852
Iteration 161/1000 | Loss: 0.00003385
Iteration 162/1000 | Loss: 0.00003248
Iteration 163/1000 | Loss: 0.00003512
Iteration 164/1000 | Loss: 0.00004061
Iteration 165/1000 | Loss: 0.00003904
Iteration 166/1000 | Loss: 0.00003768
Iteration 167/1000 | Loss: 0.00003949
Iteration 168/1000 | Loss: 0.00004394
Iteration 169/1000 | Loss: 0.00004309
Iteration 170/1000 | Loss: 0.00004596
Iteration 171/1000 | Loss: 0.00004265
Iteration 172/1000 | Loss: 0.00002942
Iteration 173/1000 | Loss: 0.00002583
Iteration 174/1000 | Loss: 0.00004191
Iteration 175/1000 | Loss: 0.00003934
Iteration 176/1000 | Loss: 0.00002507
Iteration 177/1000 | Loss: 0.00003332
Iteration 178/1000 | Loss: 0.00002915
Iteration 179/1000 | Loss: 0.00002869
Iteration 180/1000 | Loss: 0.00003113
Iteration 181/1000 | Loss: 0.00002610
Iteration 182/1000 | Loss: 0.00002733
Iteration 183/1000 | Loss: 0.00002063
Iteration 184/1000 | Loss: 0.00002982
Iteration 185/1000 | Loss: 0.00002232
Iteration 186/1000 | Loss: 0.00002288
Iteration 187/1000 | Loss: 0.00003338
Iteration 188/1000 | Loss: 0.00002774
Iteration 189/1000 | Loss: 0.00002656
Iteration 190/1000 | Loss: 0.00002702
Iteration 191/1000 | Loss: 0.00002514
Iteration 192/1000 | Loss: 0.00002175
Iteration 193/1000 | Loss: 0.00003506
Iteration 194/1000 | Loss: 0.00002893
Iteration 195/1000 | Loss: 0.00003176
Iteration 196/1000 | Loss: 0.00002733
Iteration 197/1000 | Loss: 0.00002474
Iteration 198/1000 | Loss: 0.00002980
Iteration 199/1000 | Loss: 0.00002301
Iteration 200/1000 | Loss: 0.00002163
Iteration 201/1000 | Loss: 0.00002039
Iteration 202/1000 | Loss: 0.00001990
Iteration 203/1000 | Loss: 0.00001934
Iteration 204/1000 | Loss: 0.00001912
Iteration 205/1000 | Loss: 0.00001907
Iteration 206/1000 | Loss: 0.00001906
Iteration 207/1000 | Loss: 0.00001906
Iteration 208/1000 | Loss: 0.00001905
Iteration 209/1000 | Loss: 0.00001904
Iteration 210/1000 | Loss: 0.00001904
Iteration 211/1000 | Loss: 0.00001900
Iteration 212/1000 | Loss: 0.00001898
Iteration 213/1000 | Loss: 0.00001898
Iteration 214/1000 | Loss: 0.00001897
Iteration 215/1000 | Loss: 0.00001897
Iteration 216/1000 | Loss: 0.00001896
Iteration 217/1000 | Loss: 0.00001895
Iteration 218/1000 | Loss: 0.00001895
Iteration 219/1000 | Loss: 0.00001894
Iteration 220/1000 | Loss: 0.00001894
Iteration 221/1000 | Loss: 0.00001893
Iteration 222/1000 | Loss: 0.00001892
Iteration 223/1000 | Loss: 0.00001892
Iteration 224/1000 | Loss: 0.00001892
Iteration 225/1000 | Loss: 0.00001891
Iteration 226/1000 | Loss: 0.00001891
Iteration 227/1000 | Loss: 0.00001890
Iteration 228/1000 | Loss: 0.00001890
Iteration 229/1000 | Loss: 0.00001890
Iteration 230/1000 | Loss: 0.00001890
Iteration 231/1000 | Loss: 0.00001890
Iteration 232/1000 | Loss: 0.00001890
Iteration 233/1000 | Loss: 0.00001890
Iteration 234/1000 | Loss: 0.00001890
Iteration 235/1000 | Loss: 0.00001889
Iteration 236/1000 | Loss: 0.00001889
Iteration 237/1000 | Loss: 0.00001889
Iteration 238/1000 | Loss: 0.00001889
Iteration 239/1000 | Loss: 0.00001889
Iteration 240/1000 | Loss: 0.00001889
Iteration 241/1000 | Loss: 0.00001889
Iteration 242/1000 | Loss: 0.00001889
Iteration 243/1000 | Loss: 0.00001888
Iteration 244/1000 | Loss: 0.00001888
Iteration 245/1000 | Loss: 0.00001888
Iteration 246/1000 | Loss: 0.00001888
Iteration 247/1000 | Loss: 0.00001888
Iteration 248/1000 | Loss: 0.00001888
Iteration 249/1000 | Loss: 0.00001888
Iteration 250/1000 | Loss: 0.00001888
Iteration 251/1000 | Loss: 0.00001888
Iteration 252/1000 | Loss: 0.00001888
Iteration 253/1000 | Loss: 0.00001888
Iteration 254/1000 | Loss: 0.00001888
Iteration 255/1000 | Loss: 0.00001887
Iteration 256/1000 | Loss: 0.00001887
Iteration 257/1000 | Loss: 0.00001887
Iteration 258/1000 | Loss: 0.00001887
Iteration 259/1000 | Loss: 0.00001887
Iteration 260/1000 | Loss: 0.00001887
Iteration 261/1000 | Loss: 0.00001886
Iteration 262/1000 | Loss: 0.00001886
Iteration 263/1000 | Loss: 0.00001886
Iteration 264/1000 | Loss: 0.00001886
Iteration 265/1000 | Loss: 0.00001886
Iteration 266/1000 | Loss: 0.00001886
Iteration 267/1000 | Loss: 0.00001886
Iteration 268/1000 | Loss: 0.00001886
Iteration 269/1000 | Loss: 0.00001886
Iteration 270/1000 | Loss: 0.00001886
Iteration 271/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.8862450815504417e-05, 1.8862450815504417e-05, 1.8862450815504417e-05, 1.8862450815504417e-05, 1.8862450815504417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8862450815504417e-05

Optimization complete. Final v2v error: 3.680722236633301 mm

Highest mean error: 4.893847942352295 mm for frame 235

Lowest mean error: 3.293663740158081 mm for frame 74

Saving results

Total time: 382.2936797142029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643369
Iteration 2/25 | Loss: 0.00130574
Iteration 3/25 | Loss: 0.00122782
Iteration 4/25 | Loss: 0.00121725
Iteration 5/25 | Loss: 0.00121494
Iteration 6/25 | Loss: 0.00121494
Iteration 7/25 | Loss: 0.00121494
Iteration 8/25 | Loss: 0.00121494
Iteration 9/25 | Loss: 0.00121494
Iteration 10/25 | Loss: 0.00121494
Iteration 11/25 | Loss: 0.00121494
Iteration 12/25 | Loss: 0.00121494
Iteration 13/25 | Loss: 0.00121494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001214938354678452, 0.001214938354678452, 0.001214938354678452, 0.001214938354678452, 0.001214938354678452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001214938354678452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00417757
Iteration 2/25 | Loss: 0.00135728
Iteration 3/25 | Loss: 0.00135725
Iteration 4/25 | Loss: 0.00135725
Iteration 5/25 | Loss: 0.00135725
Iteration 6/25 | Loss: 0.00135725
Iteration 7/25 | Loss: 0.00135725
Iteration 8/25 | Loss: 0.00135725
Iteration 9/25 | Loss: 0.00135725
Iteration 10/25 | Loss: 0.00135725
Iteration 11/25 | Loss: 0.00135725
Iteration 12/25 | Loss: 0.00135725
Iteration 13/25 | Loss: 0.00135725
Iteration 14/25 | Loss: 0.00135725
Iteration 15/25 | Loss: 0.00135725
Iteration 16/25 | Loss: 0.00135725
Iteration 17/25 | Loss: 0.00135725
Iteration 18/25 | Loss: 0.00135725
Iteration 19/25 | Loss: 0.00135725
Iteration 20/25 | Loss: 0.00135725
Iteration 21/25 | Loss: 0.00135725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013572497991845012, 0.0013572497991845012, 0.0013572497991845012, 0.0013572497991845012, 0.0013572497991845012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013572497991845012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135725
Iteration 2/1000 | Loss: 0.00003040
Iteration 3/1000 | Loss: 0.00002240
Iteration 4/1000 | Loss: 0.00002081
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00001791
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001686
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001675
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001656
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001643
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001639
Iteration 32/1000 | Loss: 0.00001639
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001638
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001637
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001633
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001632
Iteration 47/1000 | Loss: 0.00001631
Iteration 48/1000 | Loss: 0.00001631
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001626
Iteration 56/1000 | Loss: 0.00001624
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001620
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001619
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001617
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001610
Iteration 90/1000 | Loss: 0.00001610
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001609
Iteration 95/1000 | Loss: 0.00001609
Iteration 96/1000 | Loss: 0.00001609
Iteration 97/1000 | Loss: 0.00001609
Iteration 98/1000 | Loss: 0.00001609
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001608
Iteration 101/1000 | Loss: 0.00001608
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001606
Iteration 106/1000 | Loss: 0.00001606
Iteration 107/1000 | Loss: 0.00001606
Iteration 108/1000 | Loss: 0.00001606
Iteration 109/1000 | Loss: 0.00001606
Iteration 110/1000 | Loss: 0.00001606
Iteration 111/1000 | Loss: 0.00001606
Iteration 112/1000 | Loss: 0.00001606
Iteration 113/1000 | Loss: 0.00001606
Iteration 114/1000 | Loss: 0.00001606
Iteration 115/1000 | Loss: 0.00001606
Iteration 116/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.6057476386777125e-05, 1.6057476386777125e-05, 1.6057476386777125e-05, 1.6057476386777125e-05, 1.6057476386777125e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6057476386777125e-05

Optimization complete. Final v2v error: 3.370908498764038 mm

Highest mean error: 3.92425799369812 mm for frame 85

Lowest mean error: 2.9191319942474365 mm for frame 14

Saving results

Total time: 41.0890634059906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768916
Iteration 2/25 | Loss: 0.00180350
Iteration 3/25 | Loss: 0.00134128
Iteration 4/25 | Loss: 0.00128239
Iteration 5/25 | Loss: 0.00126757
Iteration 6/25 | Loss: 0.00127218
Iteration 7/25 | Loss: 0.00125553
Iteration 8/25 | Loss: 0.00124741
Iteration 9/25 | Loss: 0.00124273
Iteration 10/25 | Loss: 0.00124226
Iteration 11/25 | Loss: 0.00124208
Iteration 12/25 | Loss: 0.00124267
Iteration 13/25 | Loss: 0.00124266
Iteration 14/25 | Loss: 0.00124266
Iteration 15/25 | Loss: 0.00124266
Iteration 16/25 | Loss: 0.00124254
Iteration 17/25 | Loss: 0.00124215
Iteration 18/25 | Loss: 0.00124237
Iteration 19/25 | Loss: 0.00124257
Iteration 20/25 | Loss: 0.00124230
Iteration 21/25 | Loss: 0.00124253
Iteration 22/25 | Loss: 0.00124232
Iteration 23/25 | Loss: 0.00124248
Iteration 24/25 | Loss: 0.00124229
Iteration 25/25 | Loss: 0.00124249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37941575
Iteration 2/25 | Loss: 0.00131716
Iteration 3/25 | Loss: 0.00131716
Iteration 4/25 | Loss: 0.00131716
Iteration 5/25 | Loss: 0.00131716
Iteration 6/25 | Loss: 0.00131716
Iteration 7/25 | Loss: 0.00131715
Iteration 8/25 | Loss: 0.00131715
Iteration 9/25 | Loss: 0.00131715
Iteration 10/25 | Loss: 0.00131715
Iteration 11/25 | Loss: 0.00131715
Iteration 12/25 | Loss: 0.00131715
Iteration 13/25 | Loss: 0.00131715
Iteration 14/25 | Loss: 0.00131715
Iteration 15/25 | Loss: 0.00131715
Iteration 16/25 | Loss: 0.00131715
Iteration 17/25 | Loss: 0.00131715
Iteration 18/25 | Loss: 0.00131715
Iteration 19/25 | Loss: 0.00131715
Iteration 20/25 | Loss: 0.00131715
Iteration 21/25 | Loss: 0.00131715
Iteration 22/25 | Loss: 0.00131715
Iteration 23/25 | Loss: 0.00131715
Iteration 24/25 | Loss: 0.00131715
Iteration 25/25 | Loss: 0.00131715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131715
Iteration 2/1000 | Loss: 0.00002841
Iteration 3/1000 | Loss: 0.00002191
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00012452
Iteration 8/1000 | Loss: 0.00001846
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001787
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001778
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001757
Iteration 20/1000 | Loss: 0.00001756
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001751
Iteration 23/1000 | Loss: 0.00001778
Iteration 24/1000 | Loss: 0.00001754
Iteration 25/1000 | Loss: 0.00001768
Iteration 26/1000 | Loss: 0.00001742
Iteration 27/1000 | Loss: 0.00001742
Iteration 28/1000 | Loss: 0.00001742
Iteration 29/1000 | Loss: 0.00001742
Iteration 30/1000 | Loss: 0.00001741
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001739
Iteration 33/1000 | Loss: 0.00001739
Iteration 34/1000 | Loss: 0.00001739
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001739
Iteration 37/1000 | Loss: 0.00001738
Iteration 38/1000 | Loss: 0.00001738
Iteration 39/1000 | Loss: 0.00001738
Iteration 40/1000 | Loss: 0.00001738
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00001738
Iteration 43/1000 | Loss: 0.00001735
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001727
Iteration 68/1000 | Loss: 0.00001727
Iteration 69/1000 | Loss: 0.00001727
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001725
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001723
Iteration 82/1000 | Loss: 0.00001723
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001721
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001720
Iteration 93/1000 | Loss: 0.00001720
Iteration 94/1000 | Loss: 0.00001720
Iteration 95/1000 | Loss: 0.00001720
Iteration 96/1000 | Loss: 0.00001720
Iteration 97/1000 | Loss: 0.00001720
Iteration 98/1000 | Loss: 0.00001720
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001717
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001717
Iteration 114/1000 | Loss: 0.00001717
Iteration 115/1000 | Loss: 0.00001717
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001716
Iteration 120/1000 | Loss: 0.00001716
Iteration 121/1000 | Loss: 0.00001716
Iteration 122/1000 | Loss: 0.00001716
Iteration 123/1000 | Loss: 0.00001716
Iteration 124/1000 | Loss: 0.00001716
Iteration 125/1000 | Loss: 0.00001716
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001715
Iteration 133/1000 | Loss: 0.00001715
Iteration 134/1000 | Loss: 0.00001715
Iteration 135/1000 | Loss: 0.00001715
Iteration 136/1000 | Loss: 0.00001715
Iteration 137/1000 | Loss: 0.00001715
Iteration 138/1000 | Loss: 0.00001715
Iteration 139/1000 | Loss: 0.00001715
Iteration 140/1000 | Loss: 0.00001715
Iteration 141/1000 | Loss: 0.00001715
Iteration 142/1000 | Loss: 0.00001715
Iteration 143/1000 | Loss: 0.00001715
Iteration 144/1000 | Loss: 0.00001715
Iteration 145/1000 | Loss: 0.00001715
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001715
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001715
Iteration 175/1000 | Loss: 0.00001715
Iteration 176/1000 | Loss: 0.00001715
Iteration 177/1000 | Loss: 0.00001715
Iteration 178/1000 | Loss: 0.00001715
Iteration 179/1000 | Loss: 0.00001715
Iteration 180/1000 | Loss: 0.00001715
Iteration 181/1000 | Loss: 0.00001715
Iteration 182/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.7148866390925832e-05, 1.7148866390925832e-05, 1.7148866390925832e-05, 1.7148866390925832e-05, 1.7148866390925832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7148866390925832e-05

Optimization complete. Final v2v error: 3.1438300609588623 mm

Highest mean error: 19.595186233520508 mm for frame 63

Lowest mean error: 2.84962797164917 mm for frame 228

Saving results

Total time: 82.11865520477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782198
Iteration 2/25 | Loss: 0.00120936
Iteration 3/25 | Loss: 0.00113822
Iteration 4/25 | Loss: 0.00112935
Iteration 5/25 | Loss: 0.00112731
Iteration 6/25 | Loss: 0.00112731
Iteration 7/25 | Loss: 0.00112731
Iteration 8/25 | Loss: 0.00112731
Iteration 9/25 | Loss: 0.00112731
Iteration 10/25 | Loss: 0.00112731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001127308001741767, 0.001127308001741767, 0.001127308001741767, 0.001127308001741767, 0.001127308001741767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127308001741767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24887598
Iteration 2/25 | Loss: 0.00182283
Iteration 3/25 | Loss: 0.00182283
Iteration 4/25 | Loss: 0.00182283
Iteration 5/25 | Loss: 0.00182283
Iteration 6/25 | Loss: 0.00182282
Iteration 7/25 | Loss: 0.00182282
Iteration 8/25 | Loss: 0.00182282
Iteration 9/25 | Loss: 0.00182282
Iteration 10/25 | Loss: 0.00182282
Iteration 11/25 | Loss: 0.00182282
Iteration 12/25 | Loss: 0.00182282
Iteration 13/25 | Loss: 0.00182282
Iteration 14/25 | Loss: 0.00182282
Iteration 15/25 | Loss: 0.00182282
Iteration 16/25 | Loss: 0.00182282
Iteration 17/25 | Loss: 0.00182282
Iteration 18/25 | Loss: 0.00182282
Iteration 19/25 | Loss: 0.00182282
Iteration 20/25 | Loss: 0.00182282
Iteration 21/25 | Loss: 0.00182282
Iteration 22/25 | Loss: 0.00182282
Iteration 23/25 | Loss: 0.00182282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0018228227272629738, 0.0018228227272629738, 0.0018228227272629738, 0.0018228227272629738, 0.0018228227272629738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018228227272629738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182282
Iteration 2/1000 | Loss: 0.00002247
Iteration 3/1000 | Loss: 0.00001355
Iteration 4/1000 | Loss: 0.00001185
Iteration 5/1000 | Loss: 0.00001083
Iteration 6/1000 | Loss: 0.00001026
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000937
Iteration 9/1000 | Loss: 0.00000903
Iteration 10/1000 | Loss: 0.00000881
Iteration 11/1000 | Loss: 0.00000875
Iteration 12/1000 | Loss: 0.00000871
Iteration 13/1000 | Loss: 0.00000868
Iteration 14/1000 | Loss: 0.00000866
Iteration 15/1000 | Loss: 0.00000854
Iteration 16/1000 | Loss: 0.00000852
Iteration 17/1000 | Loss: 0.00000851
Iteration 18/1000 | Loss: 0.00000851
Iteration 19/1000 | Loss: 0.00000851
Iteration 20/1000 | Loss: 0.00000851
Iteration 21/1000 | Loss: 0.00000844
Iteration 22/1000 | Loss: 0.00000841
Iteration 23/1000 | Loss: 0.00000841
Iteration 24/1000 | Loss: 0.00000839
Iteration 25/1000 | Loss: 0.00000839
Iteration 26/1000 | Loss: 0.00000833
Iteration 27/1000 | Loss: 0.00000830
Iteration 28/1000 | Loss: 0.00000825
Iteration 29/1000 | Loss: 0.00000818
Iteration 30/1000 | Loss: 0.00000816
Iteration 31/1000 | Loss: 0.00000815
Iteration 32/1000 | Loss: 0.00000814
Iteration 33/1000 | Loss: 0.00000814
Iteration 34/1000 | Loss: 0.00000813
Iteration 35/1000 | Loss: 0.00000813
Iteration 36/1000 | Loss: 0.00000813
Iteration 37/1000 | Loss: 0.00000813
Iteration 38/1000 | Loss: 0.00000809
Iteration 39/1000 | Loss: 0.00000809
Iteration 40/1000 | Loss: 0.00000809
Iteration 41/1000 | Loss: 0.00000809
Iteration 42/1000 | Loss: 0.00000809
Iteration 43/1000 | Loss: 0.00000809
Iteration 44/1000 | Loss: 0.00000809
Iteration 45/1000 | Loss: 0.00000809
Iteration 46/1000 | Loss: 0.00000809
Iteration 47/1000 | Loss: 0.00000809
Iteration 48/1000 | Loss: 0.00000809
Iteration 49/1000 | Loss: 0.00000809
Iteration 50/1000 | Loss: 0.00000808
Iteration 51/1000 | Loss: 0.00000808
Iteration 52/1000 | Loss: 0.00000808
Iteration 53/1000 | Loss: 0.00000808
Iteration 54/1000 | Loss: 0.00000807
Iteration 55/1000 | Loss: 0.00000807
Iteration 56/1000 | Loss: 0.00000806
Iteration 57/1000 | Loss: 0.00000806
Iteration 58/1000 | Loss: 0.00000806
Iteration 59/1000 | Loss: 0.00000805
Iteration 60/1000 | Loss: 0.00000805
Iteration 61/1000 | Loss: 0.00000805
Iteration 62/1000 | Loss: 0.00000805
Iteration 63/1000 | Loss: 0.00000805
Iteration 64/1000 | Loss: 0.00000805
Iteration 65/1000 | Loss: 0.00000805
Iteration 66/1000 | Loss: 0.00000805
Iteration 67/1000 | Loss: 0.00000805
Iteration 68/1000 | Loss: 0.00000805
Iteration 69/1000 | Loss: 0.00000804
Iteration 70/1000 | Loss: 0.00000804
Iteration 71/1000 | Loss: 0.00000804
Iteration 72/1000 | Loss: 0.00000803
Iteration 73/1000 | Loss: 0.00000803
Iteration 74/1000 | Loss: 0.00000802
Iteration 75/1000 | Loss: 0.00000802
Iteration 76/1000 | Loss: 0.00000801
Iteration 77/1000 | Loss: 0.00000801
Iteration 78/1000 | Loss: 0.00000801
Iteration 79/1000 | Loss: 0.00000801
Iteration 80/1000 | Loss: 0.00000801
Iteration 81/1000 | Loss: 0.00000801
Iteration 82/1000 | Loss: 0.00000800
Iteration 83/1000 | Loss: 0.00000800
Iteration 84/1000 | Loss: 0.00000800
Iteration 85/1000 | Loss: 0.00000799
Iteration 86/1000 | Loss: 0.00000799
Iteration 87/1000 | Loss: 0.00000799
Iteration 88/1000 | Loss: 0.00000799
Iteration 89/1000 | Loss: 0.00000798
Iteration 90/1000 | Loss: 0.00000798
Iteration 91/1000 | Loss: 0.00000798
Iteration 92/1000 | Loss: 0.00000798
Iteration 93/1000 | Loss: 0.00000798
Iteration 94/1000 | Loss: 0.00000798
Iteration 95/1000 | Loss: 0.00000798
Iteration 96/1000 | Loss: 0.00000798
Iteration 97/1000 | Loss: 0.00000798
Iteration 98/1000 | Loss: 0.00000798
Iteration 99/1000 | Loss: 0.00000798
Iteration 100/1000 | Loss: 0.00000798
Iteration 101/1000 | Loss: 0.00000797
Iteration 102/1000 | Loss: 0.00000797
Iteration 103/1000 | Loss: 0.00000797
Iteration 104/1000 | Loss: 0.00000797
Iteration 105/1000 | Loss: 0.00000797
Iteration 106/1000 | Loss: 0.00000797
Iteration 107/1000 | Loss: 0.00000797
Iteration 108/1000 | Loss: 0.00000796
Iteration 109/1000 | Loss: 0.00000796
Iteration 110/1000 | Loss: 0.00000796
Iteration 111/1000 | Loss: 0.00000796
Iteration 112/1000 | Loss: 0.00000796
Iteration 113/1000 | Loss: 0.00000796
Iteration 114/1000 | Loss: 0.00000796
Iteration 115/1000 | Loss: 0.00000796
Iteration 116/1000 | Loss: 0.00000796
Iteration 117/1000 | Loss: 0.00000795
Iteration 118/1000 | Loss: 0.00000795
Iteration 119/1000 | Loss: 0.00000795
Iteration 120/1000 | Loss: 0.00000795
Iteration 121/1000 | Loss: 0.00000795
Iteration 122/1000 | Loss: 0.00000795
Iteration 123/1000 | Loss: 0.00000795
Iteration 124/1000 | Loss: 0.00000795
Iteration 125/1000 | Loss: 0.00000795
Iteration 126/1000 | Loss: 0.00000795
Iteration 127/1000 | Loss: 0.00000795
Iteration 128/1000 | Loss: 0.00000795
Iteration 129/1000 | Loss: 0.00000795
Iteration 130/1000 | Loss: 0.00000795
Iteration 131/1000 | Loss: 0.00000795
Iteration 132/1000 | Loss: 0.00000795
Iteration 133/1000 | Loss: 0.00000795
Iteration 134/1000 | Loss: 0.00000795
Iteration 135/1000 | Loss: 0.00000795
Iteration 136/1000 | Loss: 0.00000795
Iteration 137/1000 | Loss: 0.00000795
Iteration 138/1000 | Loss: 0.00000795
Iteration 139/1000 | Loss: 0.00000795
Iteration 140/1000 | Loss: 0.00000795
Iteration 141/1000 | Loss: 0.00000795
Iteration 142/1000 | Loss: 0.00000795
Iteration 143/1000 | Loss: 0.00000795
Iteration 144/1000 | Loss: 0.00000795
Iteration 145/1000 | Loss: 0.00000795
Iteration 146/1000 | Loss: 0.00000795
Iteration 147/1000 | Loss: 0.00000795
Iteration 148/1000 | Loss: 0.00000795
Iteration 149/1000 | Loss: 0.00000795
Iteration 150/1000 | Loss: 0.00000795
Iteration 151/1000 | Loss: 0.00000795
Iteration 152/1000 | Loss: 0.00000795
Iteration 153/1000 | Loss: 0.00000795
Iteration 154/1000 | Loss: 0.00000795
Iteration 155/1000 | Loss: 0.00000795
Iteration 156/1000 | Loss: 0.00000795
Iteration 157/1000 | Loss: 0.00000795
Iteration 158/1000 | Loss: 0.00000795
Iteration 159/1000 | Loss: 0.00000795
Iteration 160/1000 | Loss: 0.00000795
Iteration 161/1000 | Loss: 0.00000795
Iteration 162/1000 | Loss: 0.00000795
Iteration 163/1000 | Loss: 0.00000795
Iteration 164/1000 | Loss: 0.00000795
Iteration 165/1000 | Loss: 0.00000795
Iteration 166/1000 | Loss: 0.00000795
Iteration 167/1000 | Loss: 0.00000795
Iteration 168/1000 | Loss: 0.00000795
Iteration 169/1000 | Loss: 0.00000795
Iteration 170/1000 | Loss: 0.00000795
Iteration 171/1000 | Loss: 0.00000795
Iteration 172/1000 | Loss: 0.00000795
Iteration 173/1000 | Loss: 0.00000795
Iteration 174/1000 | Loss: 0.00000795
Iteration 175/1000 | Loss: 0.00000795
Iteration 176/1000 | Loss: 0.00000795
Iteration 177/1000 | Loss: 0.00000795
Iteration 178/1000 | Loss: 0.00000795
Iteration 179/1000 | Loss: 0.00000795
Iteration 180/1000 | Loss: 0.00000795
Iteration 181/1000 | Loss: 0.00000795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [7.946286132209934e-06, 7.946286132209934e-06, 7.946286132209934e-06, 7.946286132209934e-06, 7.946286132209934e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.946286132209934e-06

Optimization complete. Final v2v error: 2.441790819168091 mm

Highest mean error: 2.6239430904388428 mm for frame 59

Lowest mean error: 2.3171865940093994 mm for frame 172

Saving results

Total time: 38.190723180770874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831592
Iteration 2/25 | Loss: 0.00123041
Iteration 3/25 | Loss: 0.00115837
Iteration 4/25 | Loss: 0.00114550
Iteration 5/25 | Loss: 0.00114145
Iteration 6/25 | Loss: 0.00114083
Iteration 7/25 | Loss: 0.00114083
Iteration 8/25 | Loss: 0.00114083
Iteration 9/25 | Loss: 0.00114083
Iteration 10/25 | Loss: 0.00114083
Iteration 11/25 | Loss: 0.00114083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011408284772187471, 0.0011408284772187471, 0.0011408284772187471, 0.0011408284772187471, 0.0011408284772187471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011408284772187471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26674545
Iteration 2/25 | Loss: 0.00186734
Iteration 3/25 | Loss: 0.00186734
Iteration 4/25 | Loss: 0.00186734
Iteration 5/25 | Loss: 0.00186734
Iteration 6/25 | Loss: 0.00186734
Iteration 7/25 | Loss: 0.00186734
Iteration 8/25 | Loss: 0.00186734
Iteration 9/25 | Loss: 0.00186734
Iteration 10/25 | Loss: 0.00186734
Iteration 11/25 | Loss: 0.00186734
Iteration 12/25 | Loss: 0.00186734
Iteration 13/25 | Loss: 0.00186734
Iteration 14/25 | Loss: 0.00186734
Iteration 15/25 | Loss: 0.00186734
Iteration 16/25 | Loss: 0.00186734
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00186733843293041, 0.00186733843293041, 0.00186733843293041, 0.00186733843293041, 0.00186733843293041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00186733843293041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186734
Iteration 2/1000 | Loss: 0.00001945
Iteration 3/1000 | Loss: 0.00001477
Iteration 4/1000 | Loss: 0.00001342
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001206
Iteration 7/1000 | Loss: 0.00001156
Iteration 8/1000 | Loss: 0.00001127
Iteration 9/1000 | Loss: 0.00001104
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001056
Iteration 16/1000 | Loss: 0.00001045
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001025
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001012
Iteration 24/1000 | Loss: 0.00001011
Iteration 25/1000 | Loss: 0.00001010
Iteration 26/1000 | Loss: 0.00001009
Iteration 27/1000 | Loss: 0.00001008
Iteration 28/1000 | Loss: 0.00001008
Iteration 29/1000 | Loss: 0.00001007
Iteration 30/1000 | Loss: 0.00001007
Iteration 31/1000 | Loss: 0.00001007
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001006
Iteration 35/1000 | Loss: 0.00001006
Iteration 36/1000 | Loss: 0.00001004
Iteration 37/1000 | Loss: 0.00001004
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001004
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001004
Iteration 42/1000 | Loss: 0.00001003
Iteration 43/1000 | Loss: 0.00001003
Iteration 44/1000 | Loss: 0.00001003
Iteration 45/1000 | Loss: 0.00001003
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001003
Iteration 49/1000 | Loss: 0.00001003
Iteration 50/1000 | Loss: 0.00001003
Iteration 51/1000 | Loss: 0.00001003
Iteration 52/1000 | Loss: 0.00001003
Iteration 53/1000 | Loss: 0.00001002
Iteration 54/1000 | Loss: 0.00001002
Iteration 55/1000 | Loss: 0.00001002
Iteration 56/1000 | Loss: 0.00001002
Iteration 57/1000 | Loss: 0.00001002
Iteration 58/1000 | Loss: 0.00001002
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001001
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000997
Iteration 83/1000 | Loss: 0.00000997
Iteration 84/1000 | Loss: 0.00000997
Iteration 85/1000 | Loss: 0.00000997
Iteration 86/1000 | Loss: 0.00000996
Iteration 87/1000 | Loss: 0.00000996
Iteration 88/1000 | Loss: 0.00000996
Iteration 89/1000 | Loss: 0.00000996
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000993
Iteration 98/1000 | Loss: 0.00000993
Iteration 99/1000 | Loss: 0.00000993
Iteration 100/1000 | Loss: 0.00000993
Iteration 101/1000 | Loss: 0.00000993
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000992
Iteration 104/1000 | Loss: 0.00000992
Iteration 105/1000 | Loss: 0.00000992
Iteration 106/1000 | Loss: 0.00000992
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000990
Iteration 112/1000 | Loss: 0.00000990
Iteration 113/1000 | Loss: 0.00000990
Iteration 114/1000 | Loss: 0.00000990
Iteration 115/1000 | Loss: 0.00000990
Iteration 116/1000 | Loss: 0.00000990
Iteration 117/1000 | Loss: 0.00000990
Iteration 118/1000 | Loss: 0.00000989
Iteration 119/1000 | Loss: 0.00000989
Iteration 120/1000 | Loss: 0.00000989
Iteration 121/1000 | Loss: 0.00000989
Iteration 122/1000 | Loss: 0.00000989
Iteration 123/1000 | Loss: 0.00000989
Iteration 124/1000 | Loss: 0.00000989
Iteration 125/1000 | Loss: 0.00000989
Iteration 126/1000 | Loss: 0.00000988
Iteration 127/1000 | Loss: 0.00000988
Iteration 128/1000 | Loss: 0.00000988
Iteration 129/1000 | Loss: 0.00000988
Iteration 130/1000 | Loss: 0.00000988
Iteration 131/1000 | Loss: 0.00000988
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000987
Iteration 134/1000 | Loss: 0.00000987
Iteration 135/1000 | Loss: 0.00000987
Iteration 136/1000 | Loss: 0.00000987
Iteration 137/1000 | Loss: 0.00000987
Iteration 138/1000 | Loss: 0.00000987
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000986
Iteration 144/1000 | Loss: 0.00000986
Iteration 145/1000 | Loss: 0.00000986
Iteration 146/1000 | Loss: 0.00000986
Iteration 147/1000 | Loss: 0.00000986
Iteration 148/1000 | Loss: 0.00000986
Iteration 149/1000 | Loss: 0.00000986
Iteration 150/1000 | Loss: 0.00000986
Iteration 151/1000 | Loss: 0.00000985
Iteration 152/1000 | Loss: 0.00000985
Iteration 153/1000 | Loss: 0.00000985
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000984
Iteration 165/1000 | Loss: 0.00000984
Iteration 166/1000 | Loss: 0.00000984
Iteration 167/1000 | Loss: 0.00000984
Iteration 168/1000 | Loss: 0.00000984
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000983
Iteration 171/1000 | Loss: 0.00000983
Iteration 172/1000 | Loss: 0.00000983
Iteration 173/1000 | Loss: 0.00000983
Iteration 174/1000 | Loss: 0.00000983
Iteration 175/1000 | Loss: 0.00000983
Iteration 176/1000 | Loss: 0.00000982
Iteration 177/1000 | Loss: 0.00000982
Iteration 178/1000 | Loss: 0.00000982
Iteration 179/1000 | Loss: 0.00000982
Iteration 180/1000 | Loss: 0.00000982
Iteration 181/1000 | Loss: 0.00000982
Iteration 182/1000 | Loss: 0.00000982
Iteration 183/1000 | Loss: 0.00000982
Iteration 184/1000 | Loss: 0.00000982
Iteration 185/1000 | Loss: 0.00000982
Iteration 186/1000 | Loss: 0.00000982
Iteration 187/1000 | Loss: 0.00000981
Iteration 188/1000 | Loss: 0.00000981
Iteration 189/1000 | Loss: 0.00000981
Iteration 190/1000 | Loss: 0.00000981
Iteration 191/1000 | Loss: 0.00000981
Iteration 192/1000 | Loss: 0.00000981
Iteration 193/1000 | Loss: 0.00000981
Iteration 194/1000 | Loss: 0.00000981
Iteration 195/1000 | Loss: 0.00000981
Iteration 196/1000 | Loss: 0.00000981
Iteration 197/1000 | Loss: 0.00000981
Iteration 198/1000 | Loss: 0.00000980
Iteration 199/1000 | Loss: 0.00000980
Iteration 200/1000 | Loss: 0.00000980
Iteration 201/1000 | Loss: 0.00000980
Iteration 202/1000 | Loss: 0.00000980
Iteration 203/1000 | Loss: 0.00000980
Iteration 204/1000 | Loss: 0.00000980
Iteration 205/1000 | Loss: 0.00000980
Iteration 206/1000 | Loss: 0.00000980
Iteration 207/1000 | Loss: 0.00000980
Iteration 208/1000 | Loss: 0.00000980
Iteration 209/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.802197382668965e-06, 9.802197382668965e-06, 9.802197382668965e-06, 9.802197382668965e-06, 9.802197382668965e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.802197382668965e-06

Optimization complete. Final v2v error: 2.6918599605560303 mm

Highest mean error: 3.0859310626983643 mm for frame 97

Lowest mean error: 2.5566322803497314 mm for frame 144

Saving results

Total time: 42.08325791358948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835781
Iteration 2/25 | Loss: 0.00130567
Iteration 3/25 | Loss: 0.00118389
Iteration 4/25 | Loss: 0.00117242
Iteration 5/25 | Loss: 0.00117008
Iteration 6/25 | Loss: 0.00116986
Iteration 7/25 | Loss: 0.00116986
Iteration 8/25 | Loss: 0.00116986
Iteration 9/25 | Loss: 0.00116986
Iteration 10/25 | Loss: 0.00116986
Iteration 11/25 | Loss: 0.00116986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011698621092364192, 0.0011698621092364192, 0.0011698621092364192, 0.0011698621092364192, 0.0011698621092364192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011698621092364192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34648550
Iteration 2/25 | Loss: 0.00167200
Iteration 3/25 | Loss: 0.00167200
Iteration 4/25 | Loss: 0.00167200
Iteration 5/25 | Loss: 0.00167200
Iteration 6/25 | Loss: 0.00167200
Iteration 7/25 | Loss: 0.00167200
Iteration 8/25 | Loss: 0.00167200
Iteration 9/25 | Loss: 0.00167200
Iteration 10/25 | Loss: 0.00167200
Iteration 11/25 | Loss: 0.00167200
Iteration 12/25 | Loss: 0.00167200
Iteration 13/25 | Loss: 0.00167200
Iteration 14/25 | Loss: 0.00167200
Iteration 15/25 | Loss: 0.00167200
Iteration 16/25 | Loss: 0.00167200
Iteration 17/25 | Loss: 0.00167200
Iteration 18/25 | Loss: 0.00167200
Iteration 19/25 | Loss: 0.00167200
Iteration 20/25 | Loss: 0.00167200
Iteration 21/25 | Loss: 0.00167200
Iteration 22/25 | Loss: 0.00167200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001671999809332192, 0.001671999809332192, 0.001671999809332192, 0.001671999809332192, 0.001671999809332192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001671999809332192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167200
Iteration 2/1000 | Loss: 0.00002963
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001398
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001219
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001100
Iteration 10/1000 | Loss: 0.00001075
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001022
Iteration 14/1000 | Loss: 0.00001020
Iteration 15/1000 | Loss: 0.00001015
Iteration 16/1000 | Loss: 0.00001012
Iteration 17/1000 | Loss: 0.00001011
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001010
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001004
Iteration 24/1000 | Loss: 0.00001002
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00001000
Iteration 27/1000 | Loss: 0.00000999
Iteration 28/1000 | Loss: 0.00000999
Iteration 29/1000 | Loss: 0.00000998
Iteration 30/1000 | Loss: 0.00000997
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000995
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000990
Iteration 46/1000 | Loss: 0.00000990
Iteration 47/1000 | Loss: 0.00000989
Iteration 48/1000 | Loss: 0.00000989
Iteration 49/1000 | Loss: 0.00000988
Iteration 50/1000 | Loss: 0.00000988
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000986
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000985
Iteration 62/1000 | Loss: 0.00000985
Iteration 63/1000 | Loss: 0.00000985
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000985
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000985
Iteration 68/1000 | Loss: 0.00000985
Iteration 69/1000 | Loss: 0.00000985
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000984
Iteration 72/1000 | Loss: 0.00000984
Iteration 73/1000 | Loss: 0.00000984
Iteration 74/1000 | Loss: 0.00000984
Iteration 75/1000 | Loss: 0.00000984
Iteration 76/1000 | Loss: 0.00000984
Iteration 77/1000 | Loss: 0.00000984
Iteration 78/1000 | Loss: 0.00000984
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000983
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000983
Iteration 85/1000 | Loss: 0.00000983
Iteration 86/1000 | Loss: 0.00000983
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000980
Iteration 94/1000 | Loss: 0.00000980
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000979
Iteration 99/1000 | Loss: 0.00000979
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000975
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000974
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000974
Iteration 118/1000 | Loss: 0.00000974
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000972
Iteration 124/1000 | Loss: 0.00000972
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000972
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000972
Iteration 131/1000 | Loss: 0.00000972
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Iteration 138/1000 | Loss: 0.00000971
Iteration 139/1000 | Loss: 0.00000971
Iteration 140/1000 | Loss: 0.00000971
Iteration 141/1000 | Loss: 0.00000971
Iteration 142/1000 | Loss: 0.00000971
Iteration 143/1000 | Loss: 0.00000971
Iteration 144/1000 | Loss: 0.00000970
Iteration 145/1000 | Loss: 0.00000970
Iteration 146/1000 | Loss: 0.00000970
Iteration 147/1000 | Loss: 0.00000970
Iteration 148/1000 | Loss: 0.00000970
Iteration 149/1000 | Loss: 0.00000970
Iteration 150/1000 | Loss: 0.00000970
Iteration 151/1000 | Loss: 0.00000970
Iteration 152/1000 | Loss: 0.00000970
Iteration 153/1000 | Loss: 0.00000970
Iteration 154/1000 | Loss: 0.00000970
Iteration 155/1000 | Loss: 0.00000970
Iteration 156/1000 | Loss: 0.00000970
Iteration 157/1000 | Loss: 0.00000970
Iteration 158/1000 | Loss: 0.00000970
Iteration 159/1000 | Loss: 0.00000970
Iteration 160/1000 | Loss: 0.00000970
Iteration 161/1000 | Loss: 0.00000970
Iteration 162/1000 | Loss: 0.00000970
Iteration 163/1000 | Loss: 0.00000969
Iteration 164/1000 | Loss: 0.00000969
Iteration 165/1000 | Loss: 0.00000969
Iteration 166/1000 | Loss: 0.00000969
Iteration 167/1000 | Loss: 0.00000969
Iteration 168/1000 | Loss: 0.00000969
Iteration 169/1000 | Loss: 0.00000969
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000968
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000968
Iteration 178/1000 | Loss: 0.00000968
Iteration 179/1000 | Loss: 0.00000968
Iteration 180/1000 | Loss: 0.00000968
Iteration 181/1000 | Loss: 0.00000968
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000968
Iteration 187/1000 | Loss: 0.00000968
Iteration 188/1000 | Loss: 0.00000967
Iteration 189/1000 | Loss: 0.00000967
Iteration 190/1000 | Loss: 0.00000967
Iteration 191/1000 | Loss: 0.00000967
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00000967
Iteration 194/1000 | Loss: 0.00000967
Iteration 195/1000 | Loss: 0.00000967
Iteration 196/1000 | Loss: 0.00000967
Iteration 197/1000 | Loss: 0.00000967
Iteration 198/1000 | Loss: 0.00000967
Iteration 199/1000 | Loss: 0.00000967
Iteration 200/1000 | Loss: 0.00000967
Iteration 201/1000 | Loss: 0.00000967
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000966
Iteration 206/1000 | Loss: 0.00000966
Iteration 207/1000 | Loss: 0.00000966
Iteration 208/1000 | Loss: 0.00000966
Iteration 209/1000 | Loss: 0.00000966
Iteration 210/1000 | Loss: 0.00000966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.661926014814526e-06, 9.661926014814526e-06, 9.661926014814526e-06, 9.661926014814526e-06, 9.661926014814526e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.661926014814526e-06

Optimization complete. Final v2v error: 2.636928081512451 mm

Highest mean error: 3.5870721340179443 mm for frame 119

Lowest mean error: 2.336378812789917 mm for frame 38

Saving results

Total time: 42.54641509056091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827314
Iteration 2/25 | Loss: 0.00142088
Iteration 3/25 | Loss: 0.00125906
Iteration 4/25 | Loss: 0.00121889
Iteration 5/25 | Loss: 0.00121077
Iteration 6/25 | Loss: 0.00120983
Iteration 7/25 | Loss: 0.00120983
Iteration 8/25 | Loss: 0.00120983
Iteration 9/25 | Loss: 0.00120983
Iteration 10/25 | Loss: 0.00120983
Iteration 11/25 | Loss: 0.00120983
Iteration 12/25 | Loss: 0.00120983
Iteration 13/25 | Loss: 0.00120983
Iteration 14/25 | Loss: 0.00120983
Iteration 15/25 | Loss: 0.00120983
Iteration 16/25 | Loss: 0.00120983
Iteration 17/25 | Loss: 0.00120983
Iteration 18/25 | Loss: 0.00120983
Iteration 19/25 | Loss: 0.00120983
Iteration 20/25 | Loss: 0.00120983
Iteration 21/25 | Loss: 0.00120983
Iteration 22/25 | Loss: 0.00120983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012098336592316628, 0.0012098336592316628, 0.0012098336592316628, 0.0012098336592316628, 0.0012098336592316628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012098336592316628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20238912
Iteration 2/25 | Loss: 0.00217526
Iteration 3/25 | Loss: 0.00217526
Iteration 4/25 | Loss: 0.00217526
Iteration 5/25 | Loss: 0.00217526
Iteration 6/25 | Loss: 0.00217526
Iteration 7/25 | Loss: 0.00217526
Iteration 8/25 | Loss: 0.00217526
Iteration 9/25 | Loss: 0.00217526
Iteration 10/25 | Loss: 0.00217526
Iteration 11/25 | Loss: 0.00217526
Iteration 12/25 | Loss: 0.00217526
Iteration 13/25 | Loss: 0.00217526
Iteration 14/25 | Loss: 0.00217526
Iteration 15/25 | Loss: 0.00217526
Iteration 16/25 | Loss: 0.00217526
Iteration 17/25 | Loss: 0.00217526
Iteration 18/25 | Loss: 0.00217526
Iteration 19/25 | Loss: 0.00217526
Iteration 20/25 | Loss: 0.00217526
Iteration 21/25 | Loss: 0.00217526
Iteration 22/25 | Loss: 0.00217526
Iteration 23/25 | Loss: 0.00217526
Iteration 24/25 | Loss: 0.00217526
Iteration 25/25 | Loss: 0.00217526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217526
Iteration 2/1000 | Loss: 0.00003728
Iteration 3/1000 | Loss: 0.00002391
Iteration 4/1000 | Loss: 0.00002137
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001810
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001723
Iteration 12/1000 | Loss: 0.00001695
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001674
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001666
Iteration 25/1000 | Loss: 0.00001665
Iteration 26/1000 | Loss: 0.00001665
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001663
Iteration 29/1000 | Loss: 0.00001662
Iteration 30/1000 | Loss: 0.00001662
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001659
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001648
Iteration 52/1000 | Loss: 0.00001648
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001647
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001646
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001646
Iteration 60/1000 | Loss: 0.00001646
Iteration 61/1000 | Loss: 0.00001646
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001645
Iteration 66/1000 | Loss: 0.00001645
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001644
Iteration 69/1000 | Loss: 0.00001644
Iteration 70/1000 | Loss: 0.00001644
Iteration 71/1000 | Loss: 0.00001644
Iteration 72/1000 | Loss: 0.00001644
Iteration 73/1000 | Loss: 0.00001644
Iteration 74/1000 | Loss: 0.00001644
Iteration 75/1000 | Loss: 0.00001644
Iteration 76/1000 | Loss: 0.00001643
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00001643
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001643
Iteration 94/1000 | Loss: 0.00001643
Iteration 95/1000 | Loss: 0.00001643
Iteration 96/1000 | Loss: 0.00001643
Iteration 97/1000 | Loss: 0.00001643
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6426458387286402e-05, 1.6426458387286402e-05, 1.6426458387286402e-05, 1.6426458387286402e-05, 1.6426458387286402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6426458387286402e-05

Optimization complete. Final v2v error: 3.50457501411438 mm

Highest mean error: 3.766232967376709 mm for frame 143

Lowest mean error: 3.3585846424102783 mm for frame 117

Saving results

Total time: 33.819921255111694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805255
Iteration 2/25 | Loss: 0.00132445
Iteration 3/25 | Loss: 0.00119005
Iteration 4/25 | Loss: 0.00117588
Iteration 5/25 | Loss: 0.00117193
Iteration 6/25 | Loss: 0.00117193
Iteration 7/25 | Loss: 0.00117193
Iteration 8/25 | Loss: 0.00117193
Iteration 9/25 | Loss: 0.00117193
Iteration 10/25 | Loss: 0.00117193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011719338363036513, 0.0011719338363036513, 0.0011719338363036513, 0.0011719338363036513, 0.0011719338363036513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011719338363036513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12586129
Iteration 2/25 | Loss: 0.00162710
Iteration 3/25 | Loss: 0.00162710
Iteration 4/25 | Loss: 0.00162710
Iteration 5/25 | Loss: 0.00162710
Iteration 6/25 | Loss: 0.00162710
Iteration 7/25 | Loss: 0.00162710
Iteration 8/25 | Loss: 0.00162710
Iteration 9/25 | Loss: 0.00162710
Iteration 10/25 | Loss: 0.00162710
Iteration 11/25 | Loss: 0.00162710
Iteration 12/25 | Loss: 0.00162710
Iteration 13/25 | Loss: 0.00162710
Iteration 14/25 | Loss: 0.00162710
Iteration 15/25 | Loss: 0.00162710
Iteration 16/25 | Loss: 0.00162710
Iteration 17/25 | Loss: 0.00162710
Iteration 18/25 | Loss: 0.00162710
Iteration 19/25 | Loss: 0.00162710
Iteration 20/25 | Loss: 0.00162710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001627099234610796, 0.001627099234610796, 0.001627099234610796, 0.001627099234610796, 0.001627099234610796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001627099234610796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162710
Iteration 2/1000 | Loss: 0.00003768
Iteration 3/1000 | Loss: 0.00002537
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001853
Iteration 6/1000 | Loss: 0.00001719
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00001593
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001478
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001447
Iteration 14/1000 | Loss: 0.00001439
Iteration 15/1000 | Loss: 0.00001432
Iteration 16/1000 | Loss: 0.00001425
Iteration 17/1000 | Loss: 0.00001422
Iteration 18/1000 | Loss: 0.00001421
Iteration 19/1000 | Loss: 0.00001407
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001384
Iteration 26/1000 | Loss: 0.00001384
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001381
Iteration 32/1000 | Loss: 0.00001380
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001373
Iteration 35/1000 | Loss: 0.00001373
Iteration 36/1000 | Loss: 0.00001372
Iteration 37/1000 | Loss: 0.00001372
Iteration 38/1000 | Loss: 0.00001371
Iteration 39/1000 | Loss: 0.00001371
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001370
Iteration 42/1000 | Loss: 0.00001370
Iteration 43/1000 | Loss: 0.00001370
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001369
Iteration 49/1000 | Loss: 0.00001369
Iteration 50/1000 | Loss: 0.00001369
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001366
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001365
Iteration 61/1000 | Loss: 0.00001365
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001364
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001360
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001359
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001356
Iteration 91/1000 | Loss: 0.00001356
Iteration 92/1000 | Loss: 0.00001355
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001355
Iteration 95/1000 | Loss: 0.00001355
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001354
Iteration 99/1000 | Loss: 0.00001354
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.3543223758460954e-05, 1.3543223758460954e-05, 1.3543223758460954e-05, 1.3543223758460954e-05, 1.3543223758460954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3543223758460954e-05

Optimization complete. Final v2v error: 3.1026711463928223 mm

Highest mean error: 3.8748087882995605 mm for frame 62

Lowest mean error: 2.768523931503296 mm for frame 198

Saving results

Total time: 42.552058696746826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463269
Iteration 2/25 | Loss: 0.00137504
Iteration 3/25 | Loss: 0.00121438
Iteration 4/25 | Loss: 0.00119855
Iteration 5/25 | Loss: 0.00119596
Iteration 6/25 | Loss: 0.00119522
Iteration 7/25 | Loss: 0.00119522
Iteration 8/25 | Loss: 0.00119522
Iteration 9/25 | Loss: 0.00119522
Iteration 10/25 | Loss: 0.00119522
Iteration 11/25 | Loss: 0.00119522
Iteration 12/25 | Loss: 0.00119522
Iteration 13/25 | Loss: 0.00119522
Iteration 14/25 | Loss: 0.00119522
Iteration 15/25 | Loss: 0.00119522
Iteration 16/25 | Loss: 0.00119522
Iteration 17/25 | Loss: 0.00119522
Iteration 18/25 | Loss: 0.00119522
Iteration 19/25 | Loss: 0.00119522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011952167842537165, 0.0011952167842537165, 0.0011952167842537165, 0.0011952167842537165, 0.0011952167842537165]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011952167842537165

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29794955
Iteration 2/25 | Loss: 0.00160912
Iteration 3/25 | Loss: 0.00160911
Iteration 4/25 | Loss: 0.00160910
Iteration 5/25 | Loss: 0.00160910
Iteration 6/25 | Loss: 0.00160910
Iteration 7/25 | Loss: 0.00160910
Iteration 8/25 | Loss: 0.00160910
Iteration 9/25 | Loss: 0.00160910
Iteration 10/25 | Loss: 0.00160910
Iteration 11/25 | Loss: 0.00160910
Iteration 12/25 | Loss: 0.00160910
Iteration 13/25 | Loss: 0.00160910
Iteration 14/25 | Loss: 0.00160910
Iteration 15/25 | Loss: 0.00160910
Iteration 16/25 | Loss: 0.00160910
Iteration 17/25 | Loss: 0.00160910
Iteration 18/25 | Loss: 0.00160910
Iteration 19/25 | Loss: 0.00160910
Iteration 20/25 | Loss: 0.00160910
Iteration 21/25 | Loss: 0.00160910
Iteration 22/25 | Loss: 0.00160910
Iteration 23/25 | Loss: 0.00160910
Iteration 24/25 | Loss: 0.00160910
Iteration 25/25 | Loss: 0.00160910

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160910
Iteration 2/1000 | Loss: 0.00002678
Iteration 3/1000 | Loss: 0.00001929
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001465
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001374
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001343
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001327
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001314
Iteration 23/1000 | Loss: 0.00001313
Iteration 24/1000 | Loss: 0.00001312
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001304
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001302
Iteration 30/1000 | Loss: 0.00001302
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001300
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001289
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001287
Iteration 57/1000 | Loss: 0.00001287
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001284
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001283
Iteration 66/1000 | Loss: 0.00001283
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001280
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001278
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001273
Iteration 104/1000 | Loss: 0.00001273
Iteration 105/1000 | Loss: 0.00001272
Iteration 106/1000 | Loss: 0.00001272
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001271
Iteration 112/1000 | Loss: 0.00001271
Iteration 113/1000 | Loss: 0.00001271
Iteration 114/1000 | Loss: 0.00001270
Iteration 115/1000 | Loss: 0.00001270
Iteration 116/1000 | Loss: 0.00001270
Iteration 117/1000 | Loss: 0.00001270
Iteration 118/1000 | Loss: 0.00001270
Iteration 119/1000 | Loss: 0.00001270
Iteration 120/1000 | Loss: 0.00001270
Iteration 121/1000 | Loss: 0.00001269
Iteration 122/1000 | Loss: 0.00001269
Iteration 123/1000 | Loss: 0.00001269
Iteration 124/1000 | Loss: 0.00001269
Iteration 125/1000 | Loss: 0.00001269
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00001269
Iteration 128/1000 | Loss: 0.00001269
Iteration 129/1000 | Loss: 0.00001269
Iteration 130/1000 | Loss: 0.00001268
Iteration 131/1000 | Loss: 0.00001268
Iteration 132/1000 | Loss: 0.00001268
Iteration 133/1000 | Loss: 0.00001268
Iteration 134/1000 | Loss: 0.00001268
Iteration 135/1000 | Loss: 0.00001268
Iteration 136/1000 | Loss: 0.00001268
Iteration 137/1000 | Loss: 0.00001268
Iteration 138/1000 | Loss: 0.00001268
Iteration 139/1000 | Loss: 0.00001268
Iteration 140/1000 | Loss: 0.00001268
Iteration 141/1000 | Loss: 0.00001267
Iteration 142/1000 | Loss: 0.00001267
Iteration 143/1000 | Loss: 0.00001267
Iteration 144/1000 | Loss: 0.00001267
Iteration 145/1000 | Loss: 0.00001267
Iteration 146/1000 | Loss: 0.00001267
Iteration 147/1000 | Loss: 0.00001267
Iteration 148/1000 | Loss: 0.00001267
Iteration 149/1000 | Loss: 0.00001267
Iteration 150/1000 | Loss: 0.00001267
Iteration 151/1000 | Loss: 0.00001267
Iteration 152/1000 | Loss: 0.00001267
Iteration 153/1000 | Loss: 0.00001267
Iteration 154/1000 | Loss: 0.00001267
Iteration 155/1000 | Loss: 0.00001266
Iteration 156/1000 | Loss: 0.00001266
Iteration 157/1000 | Loss: 0.00001266
Iteration 158/1000 | Loss: 0.00001266
Iteration 159/1000 | Loss: 0.00001266
Iteration 160/1000 | Loss: 0.00001266
Iteration 161/1000 | Loss: 0.00001266
Iteration 162/1000 | Loss: 0.00001266
Iteration 163/1000 | Loss: 0.00001266
Iteration 164/1000 | Loss: 0.00001266
Iteration 165/1000 | Loss: 0.00001265
Iteration 166/1000 | Loss: 0.00001265
Iteration 167/1000 | Loss: 0.00001265
Iteration 168/1000 | Loss: 0.00001265
Iteration 169/1000 | Loss: 0.00001265
Iteration 170/1000 | Loss: 0.00001265
Iteration 171/1000 | Loss: 0.00001265
Iteration 172/1000 | Loss: 0.00001265
Iteration 173/1000 | Loss: 0.00001265
Iteration 174/1000 | Loss: 0.00001265
Iteration 175/1000 | Loss: 0.00001265
Iteration 176/1000 | Loss: 0.00001264
Iteration 177/1000 | Loss: 0.00001264
Iteration 178/1000 | Loss: 0.00001264
Iteration 179/1000 | Loss: 0.00001264
Iteration 180/1000 | Loss: 0.00001264
Iteration 181/1000 | Loss: 0.00001264
Iteration 182/1000 | Loss: 0.00001264
Iteration 183/1000 | Loss: 0.00001264
Iteration 184/1000 | Loss: 0.00001263
Iteration 185/1000 | Loss: 0.00001263
Iteration 186/1000 | Loss: 0.00001263
Iteration 187/1000 | Loss: 0.00001263
Iteration 188/1000 | Loss: 0.00001263
Iteration 189/1000 | Loss: 0.00001263
Iteration 190/1000 | Loss: 0.00001263
Iteration 191/1000 | Loss: 0.00001263
Iteration 192/1000 | Loss: 0.00001262
Iteration 193/1000 | Loss: 0.00001262
Iteration 194/1000 | Loss: 0.00001262
Iteration 195/1000 | Loss: 0.00001262
Iteration 196/1000 | Loss: 0.00001262
Iteration 197/1000 | Loss: 0.00001262
Iteration 198/1000 | Loss: 0.00001262
Iteration 199/1000 | Loss: 0.00001262
Iteration 200/1000 | Loss: 0.00001262
Iteration 201/1000 | Loss: 0.00001262
Iteration 202/1000 | Loss: 0.00001262
Iteration 203/1000 | Loss: 0.00001262
Iteration 204/1000 | Loss: 0.00001262
Iteration 205/1000 | Loss: 0.00001262
Iteration 206/1000 | Loss: 0.00001262
Iteration 207/1000 | Loss: 0.00001262
Iteration 208/1000 | Loss: 0.00001262
Iteration 209/1000 | Loss: 0.00001262
Iteration 210/1000 | Loss: 0.00001262
Iteration 211/1000 | Loss: 0.00001262
Iteration 212/1000 | Loss: 0.00001262
Iteration 213/1000 | Loss: 0.00001262
Iteration 214/1000 | Loss: 0.00001262
Iteration 215/1000 | Loss: 0.00001262
Iteration 216/1000 | Loss: 0.00001262
Iteration 217/1000 | Loss: 0.00001262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.262066507479176e-05, 1.262066507479176e-05, 1.262066507479176e-05, 1.262066507479176e-05, 1.262066507479176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.262066507479176e-05

Optimization complete. Final v2v error: 2.9370901584625244 mm

Highest mean error: 3.629096269607544 mm for frame 70

Lowest mean error: 2.381221294403076 mm for frame 148

Saving results

Total time: 43.5229856967926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454111
Iteration 2/25 | Loss: 0.00127367
Iteration 3/25 | Loss: 0.00118090
Iteration 4/25 | Loss: 0.00117125
Iteration 5/25 | Loss: 0.00116855
Iteration 6/25 | Loss: 0.00116855
Iteration 7/25 | Loss: 0.00116855
Iteration 8/25 | Loss: 0.00116855
Iteration 9/25 | Loss: 0.00116855
Iteration 10/25 | Loss: 0.00116855
Iteration 11/25 | Loss: 0.00116855
Iteration 12/25 | Loss: 0.00116855
Iteration 13/25 | Loss: 0.00116855
Iteration 14/25 | Loss: 0.00116855
Iteration 15/25 | Loss: 0.00116855
Iteration 16/25 | Loss: 0.00116855
Iteration 17/25 | Loss: 0.00116855
Iteration 18/25 | Loss: 0.00116855
Iteration 19/25 | Loss: 0.00116855
Iteration 20/25 | Loss: 0.00116855
Iteration 21/25 | Loss: 0.00116855
Iteration 22/25 | Loss: 0.00116855
Iteration 23/25 | Loss: 0.00116855
Iteration 24/25 | Loss: 0.00116855
Iteration 25/25 | Loss: 0.00116855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79275846
Iteration 2/25 | Loss: 0.00192224
Iteration 3/25 | Loss: 0.00192223
Iteration 4/25 | Loss: 0.00192223
Iteration 5/25 | Loss: 0.00192223
Iteration 6/25 | Loss: 0.00192223
Iteration 7/25 | Loss: 0.00192222
Iteration 8/25 | Loss: 0.00192222
Iteration 9/25 | Loss: 0.00192222
Iteration 10/25 | Loss: 0.00192222
Iteration 11/25 | Loss: 0.00192222
Iteration 12/25 | Loss: 0.00192222
Iteration 13/25 | Loss: 0.00192222
Iteration 14/25 | Loss: 0.00192222
Iteration 15/25 | Loss: 0.00192222
Iteration 16/25 | Loss: 0.00192222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019222244154661894, 0.0019222244154661894, 0.0019222244154661894, 0.0019222244154661894, 0.0019222244154661894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019222244154661894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192222
Iteration 2/1000 | Loss: 0.00002591
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001288
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001151
Iteration 8/1000 | Loss: 0.00001111
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001051
Iteration 11/1000 | Loss: 0.00001026
Iteration 12/1000 | Loss: 0.00001013
Iteration 13/1000 | Loss: 0.00001000
Iteration 14/1000 | Loss: 0.00000997
Iteration 15/1000 | Loss: 0.00000995
Iteration 16/1000 | Loss: 0.00000995
Iteration 17/1000 | Loss: 0.00000994
Iteration 18/1000 | Loss: 0.00000993
Iteration 19/1000 | Loss: 0.00000992
Iteration 20/1000 | Loss: 0.00000990
Iteration 21/1000 | Loss: 0.00000988
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000986
Iteration 24/1000 | Loss: 0.00000985
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000975
Iteration 27/1000 | Loss: 0.00000972
Iteration 28/1000 | Loss: 0.00000968
Iteration 29/1000 | Loss: 0.00000963
Iteration 30/1000 | Loss: 0.00000963
Iteration 31/1000 | Loss: 0.00000962
Iteration 32/1000 | Loss: 0.00000961
Iteration 33/1000 | Loss: 0.00000961
Iteration 34/1000 | Loss: 0.00000960
Iteration 35/1000 | Loss: 0.00000959
Iteration 36/1000 | Loss: 0.00000958
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000957
Iteration 39/1000 | Loss: 0.00000955
Iteration 40/1000 | Loss: 0.00000955
Iteration 41/1000 | Loss: 0.00000955
Iteration 42/1000 | Loss: 0.00000955
Iteration 43/1000 | Loss: 0.00000955
Iteration 44/1000 | Loss: 0.00000955
Iteration 45/1000 | Loss: 0.00000955
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000954
Iteration 50/1000 | Loss: 0.00000954
Iteration 51/1000 | Loss: 0.00000954
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000954
Iteration 55/1000 | Loss: 0.00000954
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000953
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000952
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000952
Iteration 62/1000 | Loss: 0.00000951
Iteration 63/1000 | Loss: 0.00000951
Iteration 64/1000 | Loss: 0.00000951
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000950
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000948
Iteration 72/1000 | Loss: 0.00000948
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000947
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000946
Iteration 80/1000 | Loss: 0.00000946
Iteration 81/1000 | Loss: 0.00000946
Iteration 82/1000 | Loss: 0.00000945
Iteration 83/1000 | Loss: 0.00000945
Iteration 84/1000 | Loss: 0.00000945
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000945
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000944
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000941
Iteration 102/1000 | Loss: 0.00000941
Iteration 103/1000 | Loss: 0.00000941
Iteration 104/1000 | Loss: 0.00000941
Iteration 105/1000 | Loss: 0.00000941
Iteration 106/1000 | Loss: 0.00000941
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000940
Iteration 109/1000 | Loss: 0.00000940
Iteration 110/1000 | Loss: 0.00000940
Iteration 111/1000 | Loss: 0.00000940
Iteration 112/1000 | Loss: 0.00000939
Iteration 113/1000 | Loss: 0.00000939
Iteration 114/1000 | Loss: 0.00000939
Iteration 115/1000 | Loss: 0.00000938
Iteration 116/1000 | Loss: 0.00000938
Iteration 117/1000 | Loss: 0.00000938
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000936
Iteration 121/1000 | Loss: 0.00000936
Iteration 122/1000 | Loss: 0.00000936
Iteration 123/1000 | Loss: 0.00000935
Iteration 124/1000 | Loss: 0.00000935
Iteration 125/1000 | Loss: 0.00000935
Iteration 126/1000 | Loss: 0.00000935
Iteration 127/1000 | Loss: 0.00000934
Iteration 128/1000 | Loss: 0.00000934
Iteration 129/1000 | Loss: 0.00000934
Iteration 130/1000 | Loss: 0.00000934
Iteration 131/1000 | Loss: 0.00000934
Iteration 132/1000 | Loss: 0.00000933
Iteration 133/1000 | Loss: 0.00000933
Iteration 134/1000 | Loss: 0.00000933
Iteration 135/1000 | Loss: 0.00000933
Iteration 136/1000 | Loss: 0.00000932
Iteration 137/1000 | Loss: 0.00000932
Iteration 138/1000 | Loss: 0.00000932
Iteration 139/1000 | Loss: 0.00000931
Iteration 140/1000 | Loss: 0.00000931
Iteration 141/1000 | Loss: 0.00000931
Iteration 142/1000 | Loss: 0.00000931
Iteration 143/1000 | Loss: 0.00000931
Iteration 144/1000 | Loss: 0.00000931
Iteration 145/1000 | Loss: 0.00000930
Iteration 146/1000 | Loss: 0.00000930
Iteration 147/1000 | Loss: 0.00000930
Iteration 148/1000 | Loss: 0.00000930
Iteration 149/1000 | Loss: 0.00000929
Iteration 150/1000 | Loss: 0.00000929
Iteration 151/1000 | Loss: 0.00000929
Iteration 152/1000 | Loss: 0.00000929
Iteration 153/1000 | Loss: 0.00000929
Iteration 154/1000 | Loss: 0.00000929
Iteration 155/1000 | Loss: 0.00000928
Iteration 156/1000 | Loss: 0.00000928
Iteration 157/1000 | Loss: 0.00000928
Iteration 158/1000 | Loss: 0.00000928
Iteration 159/1000 | Loss: 0.00000928
Iteration 160/1000 | Loss: 0.00000928
Iteration 161/1000 | Loss: 0.00000927
Iteration 162/1000 | Loss: 0.00000927
Iteration 163/1000 | Loss: 0.00000927
Iteration 164/1000 | Loss: 0.00000927
Iteration 165/1000 | Loss: 0.00000927
Iteration 166/1000 | Loss: 0.00000927
Iteration 167/1000 | Loss: 0.00000927
Iteration 168/1000 | Loss: 0.00000927
Iteration 169/1000 | Loss: 0.00000927
Iteration 170/1000 | Loss: 0.00000927
Iteration 171/1000 | Loss: 0.00000927
Iteration 172/1000 | Loss: 0.00000927
Iteration 173/1000 | Loss: 0.00000927
Iteration 174/1000 | Loss: 0.00000927
Iteration 175/1000 | Loss: 0.00000926
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000926
Iteration 180/1000 | Loss: 0.00000926
Iteration 181/1000 | Loss: 0.00000926
Iteration 182/1000 | Loss: 0.00000926
Iteration 183/1000 | Loss: 0.00000926
Iteration 184/1000 | Loss: 0.00000926
Iteration 185/1000 | Loss: 0.00000926
Iteration 186/1000 | Loss: 0.00000926
Iteration 187/1000 | Loss: 0.00000925
Iteration 188/1000 | Loss: 0.00000925
Iteration 189/1000 | Loss: 0.00000925
Iteration 190/1000 | Loss: 0.00000925
Iteration 191/1000 | Loss: 0.00000925
Iteration 192/1000 | Loss: 0.00000925
Iteration 193/1000 | Loss: 0.00000925
Iteration 194/1000 | Loss: 0.00000925
Iteration 195/1000 | Loss: 0.00000925
Iteration 196/1000 | Loss: 0.00000925
Iteration 197/1000 | Loss: 0.00000925
Iteration 198/1000 | Loss: 0.00000925
Iteration 199/1000 | Loss: 0.00000925
Iteration 200/1000 | Loss: 0.00000925
Iteration 201/1000 | Loss: 0.00000925
Iteration 202/1000 | Loss: 0.00000924
Iteration 203/1000 | Loss: 0.00000924
Iteration 204/1000 | Loss: 0.00000924
Iteration 205/1000 | Loss: 0.00000924
Iteration 206/1000 | Loss: 0.00000924
Iteration 207/1000 | Loss: 0.00000924
Iteration 208/1000 | Loss: 0.00000924
Iteration 209/1000 | Loss: 0.00000924
Iteration 210/1000 | Loss: 0.00000924
Iteration 211/1000 | Loss: 0.00000924
Iteration 212/1000 | Loss: 0.00000924
Iteration 213/1000 | Loss: 0.00000924
Iteration 214/1000 | Loss: 0.00000924
Iteration 215/1000 | Loss: 0.00000924
Iteration 216/1000 | Loss: 0.00000924
Iteration 217/1000 | Loss: 0.00000924
Iteration 218/1000 | Loss: 0.00000924
Iteration 219/1000 | Loss: 0.00000924
Iteration 220/1000 | Loss: 0.00000924
Iteration 221/1000 | Loss: 0.00000924
Iteration 222/1000 | Loss: 0.00000924
Iteration 223/1000 | Loss: 0.00000924
Iteration 224/1000 | Loss: 0.00000924
Iteration 225/1000 | Loss: 0.00000923
Iteration 226/1000 | Loss: 0.00000923
Iteration 227/1000 | Loss: 0.00000923
Iteration 228/1000 | Loss: 0.00000923
Iteration 229/1000 | Loss: 0.00000923
Iteration 230/1000 | Loss: 0.00000923
Iteration 231/1000 | Loss: 0.00000923
Iteration 232/1000 | Loss: 0.00000923
Iteration 233/1000 | Loss: 0.00000923
Iteration 234/1000 | Loss: 0.00000923
Iteration 235/1000 | Loss: 0.00000923
Iteration 236/1000 | Loss: 0.00000923
Iteration 237/1000 | Loss: 0.00000923
Iteration 238/1000 | Loss: 0.00000923
Iteration 239/1000 | Loss: 0.00000923
Iteration 240/1000 | Loss: 0.00000923
Iteration 241/1000 | Loss: 0.00000923
Iteration 242/1000 | Loss: 0.00000923
Iteration 243/1000 | Loss: 0.00000923
Iteration 244/1000 | Loss: 0.00000923
Iteration 245/1000 | Loss: 0.00000923
Iteration 246/1000 | Loss: 0.00000923
Iteration 247/1000 | Loss: 0.00000923
Iteration 248/1000 | Loss: 0.00000923
Iteration 249/1000 | Loss: 0.00000923
Iteration 250/1000 | Loss: 0.00000923
Iteration 251/1000 | Loss: 0.00000923
Iteration 252/1000 | Loss: 0.00000923
Iteration 253/1000 | Loss: 0.00000923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [9.230540854332503e-06, 9.230540854332503e-06, 9.230540854332503e-06, 9.230540854332503e-06, 9.230540854332503e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.230540854332503e-06

Optimization complete. Final v2v error: 2.6089797019958496 mm

Highest mean error: 3.5118701457977295 mm for frame 171

Lowest mean error: 2.3995676040649414 mm for frame 153

Saving results

Total time: 50.84094309806824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829674
Iteration 2/25 | Loss: 0.00118259
Iteration 3/25 | Loss: 0.00113790
Iteration 4/25 | Loss: 0.00113028
Iteration 5/25 | Loss: 0.00112837
Iteration 6/25 | Loss: 0.00112801
Iteration 7/25 | Loss: 0.00112801
Iteration 8/25 | Loss: 0.00112801
Iteration 9/25 | Loss: 0.00112801
Iteration 10/25 | Loss: 0.00112801
Iteration 11/25 | Loss: 0.00112801
Iteration 12/25 | Loss: 0.00112801
Iteration 13/25 | Loss: 0.00112801
Iteration 14/25 | Loss: 0.00112801
Iteration 15/25 | Loss: 0.00112801
Iteration 16/25 | Loss: 0.00112801
Iteration 17/25 | Loss: 0.00112801
Iteration 18/25 | Loss: 0.00112801
Iteration 19/25 | Loss: 0.00112801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011280113831162453, 0.0011280113831162453, 0.0011280113831162453, 0.0011280113831162453, 0.0011280113831162453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011280113831162453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.36790419
Iteration 2/25 | Loss: 0.00180797
Iteration 3/25 | Loss: 0.00180797
Iteration 4/25 | Loss: 0.00180797
Iteration 5/25 | Loss: 0.00180796
Iteration 6/25 | Loss: 0.00180796
Iteration 7/25 | Loss: 0.00180796
Iteration 8/25 | Loss: 0.00180796
Iteration 9/25 | Loss: 0.00180796
Iteration 10/25 | Loss: 0.00180796
Iteration 11/25 | Loss: 0.00180796
Iteration 12/25 | Loss: 0.00180796
Iteration 13/25 | Loss: 0.00180796
Iteration 14/25 | Loss: 0.00180796
Iteration 15/25 | Loss: 0.00180796
Iteration 16/25 | Loss: 0.00180796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018079630099236965, 0.0018079630099236965, 0.0018079630099236965, 0.0018079630099236965, 0.0018079630099236965]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018079630099236965

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180796
Iteration 2/1000 | Loss: 0.00002104
Iteration 3/1000 | Loss: 0.00001557
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001327
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001207
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001171
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001123
Iteration 14/1000 | Loss: 0.00001113
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001092
Iteration 21/1000 | Loss: 0.00001090
Iteration 22/1000 | Loss: 0.00001089
Iteration 23/1000 | Loss: 0.00001089
Iteration 24/1000 | Loss: 0.00001084
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001084
Iteration 27/1000 | Loss: 0.00001084
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001084
Iteration 30/1000 | Loss: 0.00001084
Iteration 31/1000 | Loss: 0.00001084
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001083
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001083
Iteration 37/1000 | Loss: 0.00001083
Iteration 38/1000 | Loss: 0.00001083
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001081
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001077
Iteration 46/1000 | Loss: 0.00001076
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001073
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001071
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001065
Iteration 77/1000 | Loss: 0.00001064
Iteration 78/1000 | Loss: 0.00001064
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001063
Iteration 81/1000 | Loss: 0.00001063
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001058
Iteration 95/1000 | Loss: 0.00001058
Iteration 96/1000 | Loss: 0.00001058
Iteration 97/1000 | Loss: 0.00001058
Iteration 98/1000 | Loss: 0.00001057
Iteration 99/1000 | Loss: 0.00001057
Iteration 100/1000 | Loss: 0.00001057
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001054
Iteration 108/1000 | Loss: 0.00001054
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001053
Iteration 114/1000 | Loss: 0.00001053
Iteration 115/1000 | Loss: 0.00001053
Iteration 116/1000 | Loss: 0.00001053
Iteration 117/1000 | Loss: 0.00001053
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001052
Iteration 120/1000 | Loss: 0.00001052
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001051
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001050
Iteration 127/1000 | Loss: 0.00001050
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001049
Iteration 130/1000 | Loss: 0.00001049
Iteration 131/1000 | Loss: 0.00001049
Iteration 132/1000 | Loss: 0.00001048
Iteration 133/1000 | Loss: 0.00001048
Iteration 134/1000 | Loss: 0.00001048
Iteration 135/1000 | Loss: 0.00001048
Iteration 136/1000 | Loss: 0.00001048
Iteration 137/1000 | Loss: 0.00001048
Iteration 138/1000 | Loss: 0.00001047
Iteration 139/1000 | Loss: 0.00001047
Iteration 140/1000 | Loss: 0.00001047
Iteration 141/1000 | Loss: 0.00001047
Iteration 142/1000 | Loss: 0.00001046
Iteration 143/1000 | Loss: 0.00001046
Iteration 144/1000 | Loss: 0.00001046
Iteration 145/1000 | Loss: 0.00001046
Iteration 146/1000 | Loss: 0.00001045
Iteration 147/1000 | Loss: 0.00001045
Iteration 148/1000 | Loss: 0.00001045
Iteration 149/1000 | Loss: 0.00001045
Iteration 150/1000 | Loss: 0.00001045
Iteration 151/1000 | Loss: 0.00001045
Iteration 152/1000 | Loss: 0.00001045
Iteration 153/1000 | Loss: 0.00001045
Iteration 154/1000 | Loss: 0.00001045
Iteration 155/1000 | Loss: 0.00001044
Iteration 156/1000 | Loss: 0.00001044
Iteration 157/1000 | Loss: 0.00001044
Iteration 158/1000 | Loss: 0.00001044
Iteration 159/1000 | Loss: 0.00001044
Iteration 160/1000 | Loss: 0.00001044
Iteration 161/1000 | Loss: 0.00001044
Iteration 162/1000 | Loss: 0.00001043
Iteration 163/1000 | Loss: 0.00001043
Iteration 164/1000 | Loss: 0.00001043
Iteration 165/1000 | Loss: 0.00001043
Iteration 166/1000 | Loss: 0.00001043
Iteration 167/1000 | Loss: 0.00001043
Iteration 168/1000 | Loss: 0.00001043
Iteration 169/1000 | Loss: 0.00001043
Iteration 170/1000 | Loss: 0.00001043
Iteration 171/1000 | Loss: 0.00001043
Iteration 172/1000 | Loss: 0.00001043
Iteration 173/1000 | Loss: 0.00001043
Iteration 174/1000 | Loss: 0.00001043
Iteration 175/1000 | Loss: 0.00001043
Iteration 176/1000 | Loss: 0.00001042
Iteration 177/1000 | Loss: 0.00001042
Iteration 178/1000 | Loss: 0.00001042
Iteration 179/1000 | Loss: 0.00001042
Iteration 180/1000 | Loss: 0.00001042
Iteration 181/1000 | Loss: 0.00001042
Iteration 182/1000 | Loss: 0.00001042
Iteration 183/1000 | Loss: 0.00001042
Iteration 184/1000 | Loss: 0.00001041
Iteration 185/1000 | Loss: 0.00001041
Iteration 186/1000 | Loss: 0.00001041
Iteration 187/1000 | Loss: 0.00001041
Iteration 188/1000 | Loss: 0.00001041
Iteration 189/1000 | Loss: 0.00001041
Iteration 190/1000 | Loss: 0.00001041
Iteration 191/1000 | Loss: 0.00001041
Iteration 192/1000 | Loss: 0.00001041
Iteration 193/1000 | Loss: 0.00001041
Iteration 194/1000 | Loss: 0.00001041
Iteration 195/1000 | Loss: 0.00001041
Iteration 196/1000 | Loss: 0.00001041
Iteration 197/1000 | Loss: 0.00001041
Iteration 198/1000 | Loss: 0.00001041
Iteration 199/1000 | Loss: 0.00001041
Iteration 200/1000 | Loss: 0.00001041
Iteration 201/1000 | Loss: 0.00001041
Iteration 202/1000 | Loss: 0.00001041
Iteration 203/1000 | Loss: 0.00001041
Iteration 204/1000 | Loss: 0.00001041
Iteration 205/1000 | Loss: 0.00001041
Iteration 206/1000 | Loss: 0.00001041
Iteration 207/1000 | Loss: 0.00001041
Iteration 208/1000 | Loss: 0.00001041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.0409864444227424e-05, 1.0409864444227424e-05, 1.0409864444227424e-05, 1.0409864444227424e-05, 1.0409864444227424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0409864444227424e-05

Optimization complete. Final v2v error: 2.7739126682281494 mm

Highest mean error: 3.3845653533935547 mm for frame 101

Lowest mean error: 2.4970314502716064 mm for frame 47

Saving results

Total time: 39.41014075279236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758075
Iteration 2/25 | Loss: 0.00137580
Iteration 3/25 | Loss: 0.00125035
Iteration 4/25 | Loss: 0.00122690
Iteration 5/25 | Loss: 0.00121981
Iteration 6/25 | Loss: 0.00121777
Iteration 7/25 | Loss: 0.00121777
Iteration 8/25 | Loss: 0.00121777
Iteration 9/25 | Loss: 0.00121777
Iteration 10/25 | Loss: 0.00121777
Iteration 11/25 | Loss: 0.00121777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012177721364423633, 0.0012177721364423633, 0.0012177721364423633, 0.0012177721364423633, 0.0012177721364423633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012177721364423633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22730005
Iteration 2/25 | Loss: 0.00234491
Iteration 3/25 | Loss: 0.00234490
Iteration 4/25 | Loss: 0.00234490
Iteration 5/25 | Loss: 0.00234490
Iteration 6/25 | Loss: 0.00234490
Iteration 7/25 | Loss: 0.00234490
Iteration 8/25 | Loss: 0.00234490
Iteration 9/25 | Loss: 0.00234490
Iteration 10/25 | Loss: 0.00234490
Iteration 11/25 | Loss: 0.00234490
Iteration 12/25 | Loss: 0.00234490
Iteration 13/25 | Loss: 0.00234490
Iteration 14/25 | Loss: 0.00234490
Iteration 15/25 | Loss: 0.00234490
Iteration 16/25 | Loss: 0.00234490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002344896085560322, 0.002344896085560322, 0.002344896085560322, 0.002344896085560322, 0.002344896085560322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002344896085560322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234490
Iteration 2/1000 | Loss: 0.00005038
Iteration 3/1000 | Loss: 0.00003486
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002743
Iteration 6/1000 | Loss: 0.00002602
Iteration 7/1000 | Loss: 0.00002526
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002401
Iteration 11/1000 | Loss: 0.00002390
Iteration 12/1000 | Loss: 0.00002367
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002350
Iteration 15/1000 | Loss: 0.00002339
Iteration 16/1000 | Loss: 0.00002333
Iteration 17/1000 | Loss: 0.00002324
Iteration 18/1000 | Loss: 0.00002315
Iteration 19/1000 | Loss: 0.00002304
Iteration 20/1000 | Loss: 0.00002296
Iteration 21/1000 | Loss: 0.00002293
Iteration 22/1000 | Loss: 0.00002293
Iteration 23/1000 | Loss: 0.00002291
Iteration 24/1000 | Loss: 0.00002291
Iteration 25/1000 | Loss: 0.00002288
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002283
Iteration 28/1000 | Loss: 0.00002281
Iteration 29/1000 | Loss: 0.00002281
Iteration 30/1000 | Loss: 0.00002281
Iteration 31/1000 | Loss: 0.00002279
Iteration 32/1000 | Loss: 0.00002278
Iteration 33/1000 | Loss: 0.00002277
Iteration 34/1000 | Loss: 0.00002277
Iteration 35/1000 | Loss: 0.00002277
Iteration 36/1000 | Loss: 0.00002276
Iteration 37/1000 | Loss: 0.00002276
Iteration 38/1000 | Loss: 0.00002276
Iteration 39/1000 | Loss: 0.00002275
Iteration 40/1000 | Loss: 0.00002275
Iteration 41/1000 | Loss: 0.00002274
Iteration 42/1000 | Loss: 0.00002274
Iteration 43/1000 | Loss: 0.00002274
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002272
Iteration 47/1000 | Loss: 0.00002271
Iteration 48/1000 | Loss: 0.00002271
Iteration 49/1000 | Loss: 0.00002271
Iteration 50/1000 | Loss: 0.00002271
Iteration 51/1000 | Loss: 0.00002271
Iteration 52/1000 | Loss: 0.00002271
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002270
Iteration 55/1000 | Loss: 0.00002270
Iteration 56/1000 | Loss: 0.00002269
Iteration 57/1000 | Loss: 0.00002269
Iteration 58/1000 | Loss: 0.00002268
Iteration 59/1000 | Loss: 0.00002268
Iteration 60/1000 | Loss: 0.00002268
Iteration 61/1000 | Loss: 0.00002267
Iteration 62/1000 | Loss: 0.00002267
Iteration 63/1000 | Loss: 0.00002267
Iteration 64/1000 | Loss: 0.00002266
Iteration 65/1000 | Loss: 0.00002266
Iteration 66/1000 | Loss: 0.00002266
Iteration 67/1000 | Loss: 0.00002266
Iteration 68/1000 | Loss: 0.00002266
Iteration 69/1000 | Loss: 0.00002266
Iteration 70/1000 | Loss: 0.00002266
Iteration 71/1000 | Loss: 0.00002266
Iteration 72/1000 | Loss: 0.00002265
Iteration 73/1000 | Loss: 0.00002265
Iteration 74/1000 | Loss: 0.00002265
Iteration 75/1000 | Loss: 0.00002264
Iteration 76/1000 | Loss: 0.00002264
Iteration 77/1000 | Loss: 0.00002264
Iteration 78/1000 | Loss: 0.00002263
Iteration 79/1000 | Loss: 0.00002263
Iteration 80/1000 | Loss: 0.00002263
Iteration 81/1000 | Loss: 0.00002263
Iteration 82/1000 | Loss: 0.00002263
Iteration 83/1000 | Loss: 0.00002263
Iteration 84/1000 | Loss: 0.00002263
Iteration 85/1000 | Loss: 0.00002263
Iteration 86/1000 | Loss: 0.00002262
Iteration 87/1000 | Loss: 0.00002262
Iteration 88/1000 | Loss: 0.00002262
Iteration 89/1000 | Loss: 0.00002262
Iteration 90/1000 | Loss: 0.00002262
Iteration 91/1000 | Loss: 0.00002262
Iteration 92/1000 | Loss: 0.00002261
Iteration 93/1000 | Loss: 0.00002261
Iteration 94/1000 | Loss: 0.00002261
Iteration 95/1000 | Loss: 0.00002260
Iteration 96/1000 | Loss: 0.00002260
Iteration 97/1000 | Loss: 0.00002260
Iteration 98/1000 | Loss: 0.00002260
Iteration 99/1000 | Loss: 0.00002259
Iteration 100/1000 | Loss: 0.00002259
Iteration 101/1000 | Loss: 0.00002259
Iteration 102/1000 | Loss: 0.00002259
Iteration 103/1000 | Loss: 0.00002258
Iteration 104/1000 | Loss: 0.00002258
Iteration 105/1000 | Loss: 0.00002258
Iteration 106/1000 | Loss: 0.00002258
Iteration 107/1000 | Loss: 0.00002257
Iteration 108/1000 | Loss: 0.00002257
Iteration 109/1000 | Loss: 0.00002257
Iteration 110/1000 | Loss: 0.00002256
Iteration 111/1000 | Loss: 0.00002256
Iteration 112/1000 | Loss: 0.00002256
Iteration 113/1000 | Loss: 0.00002255
Iteration 114/1000 | Loss: 0.00002255
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002255
Iteration 117/1000 | Loss: 0.00002255
Iteration 118/1000 | Loss: 0.00002255
Iteration 119/1000 | Loss: 0.00002254
Iteration 120/1000 | Loss: 0.00002254
Iteration 121/1000 | Loss: 0.00002254
Iteration 122/1000 | Loss: 0.00002254
Iteration 123/1000 | Loss: 0.00002254
Iteration 124/1000 | Loss: 0.00002253
Iteration 125/1000 | Loss: 0.00002253
Iteration 126/1000 | Loss: 0.00002253
Iteration 127/1000 | Loss: 0.00002253
Iteration 128/1000 | Loss: 0.00002253
Iteration 129/1000 | Loss: 0.00002253
Iteration 130/1000 | Loss: 0.00002253
Iteration 131/1000 | Loss: 0.00002252
Iteration 132/1000 | Loss: 0.00002252
Iteration 133/1000 | Loss: 0.00002252
Iteration 134/1000 | Loss: 0.00002251
Iteration 135/1000 | Loss: 0.00002251
Iteration 136/1000 | Loss: 0.00002251
Iteration 137/1000 | Loss: 0.00002251
Iteration 138/1000 | Loss: 0.00002251
Iteration 139/1000 | Loss: 0.00002251
Iteration 140/1000 | Loss: 0.00002251
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002250
Iteration 143/1000 | Loss: 0.00002250
Iteration 144/1000 | Loss: 0.00002250
Iteration 145/1000 | Loss: 0.00002250
Iteration 146/1000 | Loss: 0.00002250
Iteration 147/1000 | Loss: 0.00002250
Iteration 148/1000 | Loss: 0.00002250
Iteration 149/1000 | Loss: 0.00002250
Iteration 150/1000 | Loss: 0.00002250
Iteration 151/1000 | Loss: 0.00002249
Iteration 152/1000 | Loss: 0.00002249
Iteration 153/1000 | Loss: 0.00002249
Iteration 154/1000 | Loss: 0.00002249
Iteration 155/1000 | Loss: 0.00002249
Iteration 156/1000 | Loss: 0.00002249
Iteration 157/1000 | Loss: 0.00002249
Iteration 158/1000 | Loss: 0.00002249
Iteration 159/1000 | Loss: 0.00002249
Iteration 160/1000 | Loss: 0.00002249
Iteration 161/1000 | Loss: 0.00002249
Iteration 162/1000 | Loss: 0.00002249
Iteration 163/1000 | Loss: 0.00002248
Iteration 164/1000 | Loss: 0.00002248
Iteration 165/1000 | Loss: 0.00002248
Iteration 166/1000 | Loss: 0.00002248
Iteration 167/1000 | Loss: 0.00002248
Iteration 168/1000 | Loss: 0.00002248
Iteration 169/1000 | Loss: 0.00002248
Iteration 170/1000 | Loss: 0.00002248
Iteration 171/1000 | Loss: 0.00002248
Iteration 172/1000 | Loss: 0.00002248
Iteration 173/1000 | Loss: 0.00002248
Iteration 174/1000 | Loss: 0.00002248
Iteration 175/1000 | Loss: 0.00002248
Iteration 176/1000 | Loss: 0.00002248
Iteration 177/1000 | Loss: 0.00002248
Iteration 178/1000 | Loss: 0.00002247
Iteration 179/1000 | Loss: 0.00002247
Iteration 180/1000 | Loss: 0.00002247
Iteration 181/1000 | Loss: 0.00002247
Iteration 182/1000 | Loss: 0.00002247
Iteration 183/1000 | Loss: 0.00002247
Iteration 184/1000 | Loss: 0.00002247
Iteration 185/1000 | Loss: 0.00002247
Iteration 186/1000 | Loss: 0.00002247
Iteration 187/1000 | Loss: 0.00002246
Iteration 188/1000 | Loss: 0.00002246
Iteration 189/1000 | Loss: 0.00002246
Iteration 190/1000 | Loss: 0.00002246
Iteration 191/1000 | Loss: 0.00002246
Iteration 192/1000 | Loss: 0.00002246
Iteration 193/1000 | Loss: 0.00002246
Iteration 194/1000 | Loss: 0.00002246
Iteration 195/1000 | Loss: 0.00002246
Iteration 196/1000 | Loss: 0.00002246
Iteration 197/1000 | Loss: 0.00002246
Iteration 198/1000 | Loss: 0.00002246
Iteration 199/1000 | Loss: 0.00002246
Iteration 200/1000 | Loss: 0.00002246
Iteration 201/1000 | Loss: 0.00002246
Iteration 202/1000 | Loss: 0.00002245
Iteration 203/1000 | Loss: 0.00002245
Iteration 204/1000 | Loss: 0.00002245
Iteration 205/1000 | Loss: 0.00002245
Iteration 206/1000 | Loss: 0.00002245
Iteration 207/1000 | Loss: 0.00002245
Iteration 208/1000 | Loss: 0.00002245
Iteration 209/1000 | Loss: 0.00002245
Iteration 210/1000 | Loss: 0.00002245
Iteration 211/1000 | Loss: 0.00002245
Iteration 212/1000 | Loss: 0.00002245
Iteration 213/1000 | Loss: 0.00002245
Iteration 214/1000 | Loss: 0.00002245
Iteration 215/1000 | Loss: 0.00002245
Iteration 216/1000 | Loss: 0.00002244
Iteration 217/1000 | Loss: 0.00002244
Iteration 218/1000 | Loss: 0.00002244
Iteration 219/1000 | Loss: 0.00002244
Iteration 220/1000 | Loss: 0.00002244
Iteration 221/1000 | Loss: 0.00002244
Iteration 222/1000 | Loss: 0.00002244
Iteration 223/1000 | Loss: 0.00002244
Iteration 224/1000 | Loss: 0.00002244
Iteration 225/1000 | Loss: 0.00002244
Iteration 226/1000 | Loss: 0.00002244
Iteration 227/1000 | Loss: 0.00002244
Iteration 228/1000 | Loss: 0.00002244
Iteration 229/1000 | Loss: 0.00002244
Iteration 230/1000 | Loss: 0.00002244
Iteration 231/1000 | Loss: 0.00002244
Iteration 232/1000 | Loss: 0.00002244
Iteration 233/1000 | Loss: 0.00002244
Iteration 234/1000 | Loss: 0.00002244
Iteration 235/1000 | Loss: 0.00002244
Iteration 236/1000 | Loss: 0.00002244
Iteration 237/1000 | Loss: 0.00002244
Iteration 238/1000 | Loss: 0.00002244
Iteration 239/1000 | Loss: 0.00002244
Iteration 240/1000 | Loss: 0.00002244
Iteration 241/1000 | Loss: 0.00002244
Iteration 242/1000 | Loss: 0.00002244
Iteration 243/1000 | Loss: 0.00002244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.24361883738311e-05, 2.24361883738311e-05, 2.24361883738311e-05, 2.24361883738311e-05, 2.24361883738311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.24361883738311e-05

Optimization complete. Final v2v error: 3.9414284229278564 mm

Highest mean error: 5.00056791305542 mm for frame 21

Lowest mean error: 3.010312080383301 mm for frame 130

Saving results

Total time: 47.73146963119507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964584
Iteration 2/25 | Loss: 0.00165859
Iteration 3/25 | Loss: 0.00140364
Iteration 4/25 | Loss: 0.00135298
Iteration 5/25 | Loss: 0.00133261
Iteration 6/25 | Loss: 0.00132640
Iteration 7/25 | Loss: 0.00132442
Iteration 8/25 | Loss: 0.00132254
Iteration 9/25 | Loss: 0.00132535
Iteration 10/25 | Loss: 0.00132658
Iteration 11/25 | Loss: 0.00132501
Iteration 12/25 | Loss: 0.00132239
Iteration 13/25 | Loss: 0.00132122
Iteration 14/25 | Loss: 0.00132043
Iteration 15/25 | Loss: 0.00132008
Iteration 16/25 | Loss: 0.00132000
Iteration 17/25 | Loss: 0.00132000
Iteration 18/25 | Loss: 0.00132000
Iteration 19/25 | Loss: 0.00132000
Iteration 20/25 | Loss: 0.00132000
Iteration 21/25 | Loss: 0.00132000
Iteration 22/25 | Loss: 0.00132000
Iteration 23/25 | Loss: 0.00132000
Iteration 24/25 | Loss: 0.00132000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001319996314123273, 0.001319996314123273, 0.001319996314123273, 0.001319996314123273, 0.001319996314123273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001319996314123273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21248662
Iteration 2/25 | Loss: 0.00259964
Iteration 3/25 | Loss: 0.00259957
Iteration 4/25 | Loss: 0.00259957
Iteration 5/25 | Loss: 0.00259957
Iteration 6/25 | Loss: 0.00259957
Iteration 7/25 | Loss: 0.00259957
Iteration 8/25 | Loss: 0.00259957
Iteration 9/25 | Loss: 0.00259957
Iteration 10/25 | Loss: 0.00259956
Iteration 11/25 | Loss: 0.00259956
Iteration 12/25 | Loss: 0.00259956
Iteration 13/25 | Loss: 0.00259956
Iteration 14/25 | Loss: 0.00259956
Iteration 15/25 | Loss: 0.00259956
Iteration 16/25 | Loss: 0.00259956
Iteration 17/25 | Loss: 0.00259956
Iteration 18/25 | Loss: 0.00259956
Iteration 19/25 | Loss: 0.00259956
Iteration 20/25 | Loss: 0.00259956
Iteration 21/25 | Loss: 0.00259956
Iteration 22/25 | Loss: 0.00259956
Iteration 23/25 | Loss: 0.00259956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002599564380943775, 0.002599564380943775, 0.002599564380943775, 0.002599564380943775, 0.002599564380943775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002599564380943775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259956
Iteration 2/1000 | Loss: 0.00045371
Iteration 3/1000 | Loss: 0.00013071
Iteration 4/1000 | Loss: 0.00010671
Iteration 5/1000 | Loss: 0.00009387
Iteration 6/1000 | Loss: 0.00008668
Iteration 7/1000 | Loss: 0.00008155
Iteration 8/1000 | Loss: 0.00054035
Iteration 9/1000 | Loss: 0.00028940
Iteration 10/1000 | Loss: 0.00093957
Iteration 11/1000 | Loss: 0.00036009
Iteration 12/1000 | Loss: 0.00100610
Iteration 13/1000 | Loss: 0.00087423
Iteration 14/1000 | Loss: 0.00132101
Iteration 15/1000 | Loss: 0.00046479
Iteration 16/1000 | Loss: 0.00101857
Iteration 17/1000 | Loss: 0.00058231
Iteration 18/1000 | Loss: 0.00093715
Iteration 19/1000 | Loss: 0.00017176
Iteration 20/1000 | Loss: 0.00020776
Iteration 21/1000 | Loss: 0.00018903
Iteration 22/1000 | Loss: 0.00017948
Iteration 23/1000 | Loss: 0.00010237
Iteration 24/1000 | Loss: 0.00006890
Iteration 25/1000 | Loss: 0.00039530
Iteration 26/1000 | Loss: 0.00006759
Iteration 27/1000 | Loss: 0.00035937
Iteration 28/1000 | Loss: 0.00006865
Iteration 29/1000 | Loss: 0.00006530
Iteration 30/1000 | Loss: 0.00006380
Iteration 31/1000 | Loss: 0.00009929
Iteration 32/1000 | Loss: 0.00038343
Iteration 33/1000 | Loss: 0.00010347
Iteration 34/1000 | Loss: 0.00032574
Iteration 35/1000 | Loss: 0.00022362
Iteration 36/1000 | Loss: 0.00020294
Iteration 37/1000 | Loss: 0.00019092
Iteration 38/1000 | Loss: 0.00028140
Iteration 39/1000 | Loss: 0.00021844
Iteration 40/1000 | Loss: 0.00034511
Iteration 41/1000 | Loss: 0.00023426
Iteration 42/1000 | Loss: 0.00022169
Iteration 43/1000 | Loss: 0.00006676
Iteration 44/1000 | Loss: 0.00016698
Iteration 45/1000 | Loss: 0.00012966
Iteration 46/1000 | Loss: 0.00006590
Iteration 47/1000 | Loss: 0.00006208
Iteration 48/1000 | Loss: 0.00006127
Iteration 49/1000 | Loss: 0.00014823
Iteration 50/1000 | Loss: 0.00034621
Iteration 51/1000 | Loss: 0.00025547
Iteration 52/1000 | Loss: 0.00027125
Iteration 53/1000 | Loss: 0.00038439
Iteration 54/1000 | Loss: 0.00039199
Iteration 55/1000 | Loss: 0.00007621
Iteration 56/1000 | Loss: 0.00033846
Iteration 57/1000 | Loss: 0.00006357
Iteration 58/1000 | Loss: 0.00006135
Iteration 59/1000 | Loss: 0.00006058
Iteration 60/1000 | Loss: 0.00006023
Iteration 61/1000 | Loss: 0.00012514
Iteration 62/1000 | Loss: 0.00053876
Iteration 63/1000 | Loss: 0.00183091
Iteration 64/1000 | Loss: 0.00219729
Iteration 65/1000 | Loss: 0.00107443
Iteration 66/1000 | Loss: 0.00033640
Iteration 67/1000 | Loss: 0.00007871
Iteration 68/1000 | Loss: 0.00089568
Iteration 69/1000 | Loss: 0.00009987
Iteration 70/1000 | Loss: 0.00006335
Iteration 71/1000 | Loss: 0.00005452
Iteration 72/1000 | Loss: 0.00005022
Iteration 73/1000 | Loss: 0.00075888
Iteration 74/1000 | Loss: 0.00152242
Iteration 75/1000 | Loss: 0.00111215
Iteration 76/1000 | Loss: 0.00051933
Iteration 77/1000 | Loss: 0.00024780
Iteration 78/1000 | Loss: 0.00005547
Iteration 79/1000 | Loss: 0.00004866
Iteration 80/1000 | Loss: 0.00004685
Iteration 81/1000 | Loss: 0.00004564
Iteration 82/1000 | Loss: 0.00070918
Iteration 83/1000 | Loss: 0.00005525
Iteration 84/1000 | Loss: 0.00004504
Iteration 85/1000 | Loss: 0.00004236
Iteration 86/1000 | Loss: 0.00004049
Iteration 87/1000 | Loss: 0.00003963
Iteration 88/1000 | Loss: 0.00003871
Iteration 89/1000 | Loss: 0.00003784
Iteration 90/1000 | Loss: 0.00003736
Iteration 91/1000 | Loss: 0.00003704
Iteration 92/1000 | Loss: 0.00003677
Iteration 93/1000 | Loss: 0.00003670
Iteration 94/1000 | Loss: 0.00003661
Iteration 95/1000 | Loss: 0.00003660
Iteration 96/1000 | Loss: 0.00003659
Iteration 97/1000 | Loss: 0.00003659
Iteration 98/1000 | Loss: 0.00003653
Iteration 99/1000 | Loss: 0.00003652
Iteration 100/1000 | Loss: 0.00003643
Iteration 101/1000 | Loss: 0.00003641
Iteration 102/1000 | Loss: 0.00003639
Iteration 103/1000 | Loss: 0.00003620
Iteration 104/1000 | Loss: 0.00003599
Iteration 105/1000 | Loss: 0.00003586
Iteration 106/1000 | Loss: 0.00003578
Iteration 107/1000 | Loss: 0.00003572
Iteration 108/1000 | Loss: 0.00003572
Iteration 109/1000 | Loss: 0.00003571
Iteration 110/1000 | Loss: 0.00003569
Iteration 111/1000 | Loss: 0.00003568
Iteration 112/1000 | Loss: 0.00003568
Iteration 113/1000 | Loss: 0.00003566
Iteration 114/1000 | Loss: 0.00003565
Iteration 115/1000 | Loss: 0.00003565
Iteration 116/1000 | Loss: 0.00003564
Iteration 117/1000 | Loss: 0.00003564
Iteration 118/1000 | Loss: 0.00003563
Iteration 119/1000 | Loss: 0.00003563
Iteration 120/1000 | Loss: 0.00003562
Iteration 121/1000 | Loss: 0.00003562
Iteration 122/1000 | Loss: 0.00003562
Iteration 123/1000 | Loss: 0.00003562
Iteration 124/1000 | Loss: 0.00003561
Iteration 125/1000 | Loss: 0.00003561
Iteration 126/1000 | Loss: 0.00003561
Iteration 127/1000 | Loss: 0.00003561
Iteration 128/1000 | Loss: 0.00003561
Iteration 129/1000 | Loss: 0.00003561
Iteration 130/1000 | Loss: 0.00003560
Iteration 131/1000 | Loss: 0.00003560
Iteration 132/1000 | Loss: 0.00003560
Iteration 133/1000 | Loss: 0.00003560
Iteration 134/1000 | Loss: 0.00003560
Iteration 135/1000 | Loss: 0.00003560
Iteration 136/1000 | Loss: 0.00003559
Iteration 137/1000 | Loss: 0.00003559
Iteration 138/1000 | Loss: 0.00003559
Iteration 139/1000 | Loss: 0.00003558
Iteration 140/1000 | Loss: 0.00003558
Iteration 141/1000 | Loss: 0.00003558
Iteration 142/1000 | Loss: 0.00003558
Iteration 143/1000 | Loss: 0.00003557
Iteration 144/1000 | Loss: 0.00003557
Iteration 145/1000 | Loss: 0.00003557
Iteration 146/1000 | Loss: 0.00003556
Iteration 147/1000 | Loss: 0.00003556
Iteration 148/1000 | Loss: 0.00003556
Iteration 149/1000 | Loss: 0.00003555
Iteration 150/1000 | Loss: 0.00003555
Iteration 151/1000 | Loss: 0.00003555
Iteration 152/1000 | Loss: 0.00003554
Iteration 153/1000 | Loss: 0.00003554
Iteration 154/1000 | Loss: 0.00003554
Iteration 155/1000 | Loss: 0.00003554
Iteration 156/1000 | Loss: 0.00003553
Iteration 157/1000 | Loss: 0.00003553
Iteration 158/1000 | Loss: 0.00003553
Iteration 159/1000 | Loss: 0.00003553
Iteration 160/1000 | Loss: 0.00003553
Iteration 161/1000 | Loss: 0.00003553
Iteration 162/1000 | Loss: 0.00003553
Iteration 163/1000 | Loss: 0.00003552
Iteration 164/1000 | Loss: 0.00003552
Iteration 165/1000 | Loss: 0.00003552
Iteration 166/1000 | Loss: 0.00003552
Iteration 167/1000 | Loss: 0.00003552
Iteration 168/1000 | Loss: 0.00003551
Iteration 169/1000 | Loss: 0.00003551
Iteration 170/1000 | Loss: 0.00003551
Iteration 171/1000 | Loss: 0.00003551
Iteration 172/1000 | Loss: 0.00003551
Iteration 173/1000 | Loss: 0.00003551
Iteration 174/1000 | Loss: 0.00003550
Iteration 175/1000 | Loss: 0.00003550
Iteration 176/1000 | Loss: 0.00003550
Iteration 177/1000 | Loss: 0.00003550
Iteration 178/1000 | Loss: 0.00003550
Iteration 179/1000 | Loss: 0.00003550
Iteration 180/1000 | Loss: 0.00003550
Iteration 181/1000 | Loss: 0.00003550
Iteration 182/1000 | Loss: 0.00003550
Iteration 183/1000 | Loss: 0.00003550
Iteration 184/1000 | Loss: 0.00003549
Iteration 185/1000 | Loss: 0.00003549
Iteration 186/1000 | Loss: 0.00003549
Iteration 187/1000 | Loss: 0.00003549
Iteration 188/1000 | Loss: 0.00003549
Iteration 189/1000 | Loss: 0.00003549
Iteration 190/1000 | Loss: 0.00003549
Iteration 191/1000 | Loss: 0.00003549
Iteration 192/1000 | Loss: 0.00003549
Iteration 193/1000 | Loss: 0.00003549
Iteration 194/1000 | Loss: 0.00003549
Iteration 195/1000 | Loss: 0.00003548
Iteration 196/1000 | Loss: 0.00003548
Iteration 197/1000 | Loss: 0.00003548
Iteration 198/1000 | Loss: 0.00003548
Iteration 199/1000 | Loss: 0.00003548
Iteration 200/1000 | Loss: 0.00003548
Iteration 201/1000 | Loss: 0.00003548
Iteration 202/1000 | Loss: 0.00003548
Iteration 203/1000 | Loss: 0.00003548
Iteration 204/1000 | Loss: 0.00003548
Iteration 205/1000 | Loss: 0.00003548
Iteration 206/1000 | Loss: 0.00003547
Iteration 207/1000 | Loss: 0.00003547
Iteration 208/1000 | Loss: 0.00003547
Iteration 209/1000 | Loss: 0.00003547
Iteration 210/1000 | Loss: 0.00003547
Iteration 211/1000 | Loss: 0.00003547
Iteration 212/1000 | Loss: 0.00003547
Iteration 213/1000 | Loss: 0.00003547
Iteration 214/1000 | Loss: 0.00003547
Iteration 215/1000 | Loss: 0.00003547
Iteration 216/1000 | Loss: 0.00003547
Iteration 217/1000 | Loss: 0.00003547
Iteration 218/1000 | Loss: 0.00003546
Iteration 219/1000 | Loss: 0.00003546
Iteration 220/1000 | Loss: 0.00003546
Iteration 221/1000 | Loss: 0.00003546
Iteration 222/1000 | Loss: 0.00003546
Iteration 223/1000 | Loss: 0.00003546
Iteration 224/1000 | Loss: 0.00003546
Iteration 225/1000 | Loss: 0.00003546
Iteration 226/1000 | Loss: 0.00003546
Iteration 227/1000 | Loss: 0.00003546
Iteration 228/1000 | Loss: 0.00003546
Iteration 229/1000 | Loss: 0.00003546
Iteration 230/1000 | Loss: 0.00003546
Iteration 231/1000 | Loss: 0.00003546
Iteration 232/1000 | Loss: 0.00003546
Iteration 233/1000 | Loss: 0.00003546
Iteration 234/1000 | Loss: 0.00003546
Iteration 235/1000 | Loss: 0.00003546
Iteration 236/1000 | Loss: 0.00003545
Iteration 237/1000 | Loss: 0.00003545
Iteration 238/1000 | Loss: 0.00003545
Iteration 239/1000 | Loss: 0.00003545
Iteration 240/1000 | Loss: 0.00003545
Iteration 241/1000 | Loss: 0.00003545
Iteration 242/1000 | Loss: 0.00003545
Iteration 243/1000 | Loss: 0.00003545
Iteration 244/1000 | Loss: 0.00003545
Iteration 245/1000 | Loss: 0.00003545
Iteration 246/1000 | Loss: 0.00003545
Iteration 247/1000 | Loss: 0.00003545
Iteration 248/1000 | Loss: 0.00003545
Iteration 249/1000 | Loss: 0.00003545
Iteration 250/1000 | Loss: 0.00003545
Iteration 251/1000 | Loss: 0.00003545
Iteration 252/1000 | Loss: 0.00003545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.544830542523414e-05, 3.544830542523414e-05, 3.544830542523414e-05, 3.544830542523414e-05, 3.544830542523414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.544830542523414e-05

Optimization complete. Final v2v error: 4.7636494636535645 mm

Highest mean error: 6.446669101715088 mm for frame 138

Lowest mean error: 2.809749126434326 mm for frame 38

Saving results

Total time: 198.56841588020325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995601
Iteration 2/25 | Loss: 0.00254170
Iteration 3/25 | Loss: 0.00188397
Iteration 4/25 | Loss: 0.00180904
Iteration 5/25 | Loss: 0.00164809
Iteration 6/25 | Loss: 0.00147436
Iteration 7/25 | Loss: 0.00143766
Iteration 8/25 | Loss: 0.00141594
Iteration 9/25 | Loss: 0.00140787
Iteration 10/25 | Loss: 0.00140045
Iteration 11/25 | Loss: 0.00139745
Iteration 12/25 | Loss: 0.00140057
Iteration 13/25 | Loss: 0.00139671
Iteration 14/25 | Loss: 0.00139769
Iteration 15/25 | Loss: 0.00139240
Iteration 16/25 | Loss: 0.00139119
Iteration 17/25 | Loss: 0.00139287
Iteration 18/25 | Loss: 0.00139446
Iteration 19/25 | Loss: 0.00139101
Iteration 20/25 | Loss: 0.00138989
Iteration 21/25 | Loss: 0.00138632
Iteration 22/25 | Loss: 0.00138322
Iteration 23/25 | Loss: 0.00138417
Iteration 24/25 | Loss: 0.00138235
Iteration 25/25 | Loss: 0.00138234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23222578
Iteration 2/25 | Loss: 0.00390828
Iteration 3/25 | Loss: 0.00390828
Iteration 4/25 | Loss: 0.00390828
Iteration 5/25 | Loss: 0.00388967
Iteration 6/25 | Loss: 0.00388965
Iteration 7/25 | Loss: 0.00388965
Iteration 8/25 | Loss: 0.00388965
Iteration 9/25 | Loss: 0.00388965
Iteration 10/25 | Loss: 0.00388965
Iteration 11/25 | Loss: 0.00388965
Iteration 12/25 | Loss: 0.00388965
Iteration 13/25 | Loss: 0.00388965
Iteration 14/25 | Loss: 0.00388965
Iteration 15/25 | Loss: 0.00388965
Iteration 16/25 | Loss: 0.00388965
Iteration 17/25 | Loss: 0.00388965
Iteration 18/25 | Loss: 0.00388965
Iteration 19/25 | Loss: 0.00388965
Iteration 20/25 | Loss: 0.00388965
Iteration 21/25 | Loss: 0.00388965
Iteration 22/25 | Loss: 0.00388965
Iteration 23/25 | Loss: 0.00388965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0038896482437849045, 0.0038896482437849045, 0.0038896482437849045, 0.0038896482437849045, 0.0038896482437849045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038896482437849045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00388965
Iteration 2/1000 | Loss: 0.00039450
Iteration 3/1000 | Loss: 0.00081200
Iteration 4/1000 | Loss: 0.00039807
Iteration 5/1000 | Loss: 0.00092915
Iteration 6/1000 | Loss: 0.00021852
Iteration 7/1000 | Loss: 0.00024179
Iteration 8/1000 | Loss: 0.00034762
Iteration 9/1000 | Loss: 0.00013907
Iteration 10/1000 | Loss: 0.00048669
Iteration 11/1000 | Loss: 0.00044270
Iteration 12/1000 | Loss: 0.00142582
Iteration 13/1000 | Loss: 0.00014050
Iteration 14/1000 | Loss: 0.00034132
Iteration 15/1000 | Loss: 0.00019915
Iteration 16/1000 | Loss: 0.00055522
Iteration 17/1000 | Loss: 0.00176512
Iteration 18/1000 | Loss: 0.00203974
Iteration 19/1000 | Loss: 0.00223802
Iteration 20/1000 | Loss: 0.00224086
Iteration 21/1000 | Loss: 0.00036079
Iteration 22/1000 | Loss: 0.00136133
Iteration 23/1000 | Loss: 0.00101958
Iteration 24/1000 | Loss: 0.00063812
Iteration 25/1000 | Loss: 0.00031524
Iteration 26/1000 | Loss: 0.00064950
Iteration 27/1000 | Loss: 0.00040454
Iteration 28/1000 | Loss: 0.00027259
Iteration 29/1000 | Loss: 0.00062370
Iteration 30/1000 | Loss: 0.00059144
Iteration 31/1000 | Loss: 0.00087104
Iteration 32/1000 | Loss: 0.00029663
Iteration 33/1000 | Loss: 0.00008246
Iteration 34/1000 | Loss: 0.00016208
Iteration 35/1000 | Loss: 0.00006306
Iteration 36/1000 | Loss: 0.00004621
Iteration 37/1000 | Loss: 0.00012634
Iteration 38/1000 | Loss: 0.00082073
Iteration 39/1000 | Loss: 0.00013889
Iteration 40/1000 | Loss: 0.00015149
Iteration 41/1000 | Loss: 0.00018646
Iteration 42/1000 | Loss: 0.00015100
Iteration 43/1000 | Loss: 0.00021695
Iteration 44/1000 | Loss: 0.00012479
Iteration 45/1000 | Loss: 0.00008444
Iteration 46/1000 | Loss: 0.00004868
Iteration 47/1000 | Loss: 0.00011721
Iteration 48/1000 | Loss: 0.00016428
Iteration 49/1000 | Loss: 0.00020859
Iteration 50/1000 | Loss: 0.00042583
Iteration 51/1000 | Loss: 0.00032000
Iteration 52/1000 | Loss: 0.00016076
Iteration 53/1000 | Loss: 0.00006625
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00012551
Iteration 56/1000 | Loss: 0.00010556
Iteration 57/1000 | Loss: 0.00008391
Iteration 58/1000 | Loss: 0.00004247
Iteration 59/1000 | Loss: 0.00023264
Iteration 60/1000 | Loss: 0.00005383
Iteration 61/1000 | Loss: 0.00002526
Iteration 62/1000 | Loss: 0.00013870
Iteration 63/1000 | Loss: 0.00001963
Iteration 64/1000 | Loss: 0.00004769
Iteration 65/1000 | Loss: 0.00008388
Iteration 66/1000 | Loss: 0.00002531
Iteration 67/1000 | Loss: 0.00002607
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00004496
Iteration 70/1000 | Loss: 0.00003738
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00003607
Iteration 73/1000 | Loss: 0.00001690
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00002993
Iteration 81/1000 | Loss: 0.00001672
Iteration 82/1000 | Loss: 0.00001672
Iteration 83/1000 | Loss: 0.00001672
Iteration 84/1000 | Loss: 0.00001672
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001672
Iteration 87/1000 | Loss: 0.00001672
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001671
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001671
Iteration 98/1000 | Loss: 0.00001671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.671376776357647e-05, 1.671376776357647e-05, 1.671376776357647e-05, 1.671376776357647e-05, 1.671376776357647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.671376776357647e-05

Optimization complete. Final v2v error: 3.492727041244507 mm

Highest mean error: 4.892963886260986 mm for frame 165

Lowest mean error: 3.3162922859191895 mm for frame 9

Saving results

Total time: 167.67295670509338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070395
Iteration 2/25 | Loss: 0.00580185
Iteration 3/25 | Loss: 0.00348652
Iteration 4/25 | Loss: 0.00336557
Iteration 5/25 | Loss: 0.00260791
Iteration 6/25 | Loss: 0.00201640
Iteration 7/25 | Loss: 0.00172517
Iteration 8/25 | Loss: 0.00159274
Iteration 9/25 | Loss: 0.00153815
Iteration 10/25 | Loss: 0.00145691
Iteration 11/25 | Loss: 0.00149947
Iteration 12/25 | Loss: 0.00140147
Iteration 13/25 | Loss: 0.00139298
Iteration 14/25 | Loss: 0.00136573
Iteration 15/25 | Loss: 0.00135817
Iteration 16/25 | Loss: 0.00135025
Iteration 17/25 | Loss: 0.00134712
Iteration 18/25 | Loss: 0.00134646
Iteration 19/25 | Loss: 0.00134633
Iteration 20/25 | Loss: 0.00134856
Iteration 21/25 | Loss: 0.00134443
Iteration 22/25 | Loss: 0.00134483
Iteration 23/25 | Loss: 0.00134430
Iteration 24/25 | Loss: 0.00134430
Iteration 25/25 | Loss: 0.00134430

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52337962
Iteration 2/25 | Loss: 0.00161981
Iteration 3/25 | Loss: 0.00161406
Iteration 4/25 | Loss: 0.00161406
Iteration 5/25 | Loss: 0.00161406
Iteration 6/25 | Loss: 0.00161406
Iteration 7/25 | Loss: 0.00161406
Iteration 8/25 | Loss: 0.00161406
Iteration 9/25 | Loss: 0.00161406
Iteration 10/25 | Loss: 0.00161406
Iteration 11/25 | Loss: 0.00161406
Iteration 12/25 | Loss: 0.00161405
Iteration 13/25 | Loss: 0.00161405
Iteration 14/25 | Loss: 0.00161405
Iteration 15/25 | Loss: 0.00161405
Iteration 16/25 | Loss: 0.00161405
Iteration 17/25 | Loss: 0.00161405
Iteration 18/25 | Loss: 0.00161405
Iteration 19/25 | Loss: 0.00161405
Iteration 20/25 | Loss: 0.00161405
Iteration 21/25 | Loss: 0.00161405
Iteration 22/25 | Loss: 0.00161405
Iteration 23/25 | Loss: 0.00161405
Iteration 24/25 | Loss: 0.00161405
Iteration 25/25 | Loss: 0.00161405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161405
Iteration 2/1000 | Loss: 0.00009854
Iteration 3/1000 | Loss: 0.00006191
Iteration 4/1000 | Loss: 0.00005137
Iteration 5/1000 | Loss: 0.00005156
Iteration 6/1000 | Loss: 0.00007336
Iteration 7/1000 | Loss: 0.00004573
Iteration 8/1000 | Loss: 0.00004430
Iteration 9/1000 | Loss: 0.00004351
Iteration 10/1000 | Loss: 0.00004270
Iteration 11/1000 | Loss: 0.00004218
Iteration 12/1000 | Loss: 0.00004172
Iteration 13/1000 | Loss: 0.00284053
Iteration 14/1000 | Loss: 0.00258498
Iteration 15/1000 | Loss: 0.00006588
Iteration 16/1000 | Loss: 0.00004489
Iteration 17/1000 | Loss: 0.00004133
Iteration 18/1000 | Loss: 0.00004123
Iteration 19/1000 | Loss: 0.00003724
Iteration 20/1000 | Loss: 0.00004088
Iteration 21/1000 | Loss: 0.00003161
Iteration 22/1000 | Loss: 0.00002354
Iteration 23/1000 | Loss: 0.00012615
Iteration 24/1000 | Loss: 0.00002104
Iteration 25/1000 | Loss: 0.00001968
Iteration 26/1000 | Loss: 0.00001864
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001764
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001591
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001589
Iteration 47/1000 | Loss: 0.00001588
Iteration 48/1000 | Loss: 0.00001588
Iteration 49/1000 | Loss: 0.00001587
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001587
Iteration 52/1000 | Loss: 0.00001586
Iteration 53/1000 | Loss: 0.00001586
Iteration 54/1000 | Loss: 0.00001586
Iteration 55/1000 | Loss: 0.00001586
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001585
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001584
Iteration 65/1000 | Loss: 0.00001584
Iteration 66/1000 | Loss: 0.00001584
Iteration 67/1000 | Loss: 0.00001584
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001580
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001578
Iteration 118/1000 | Loss: 0.00001578
Iteration 119/1000 | Loss: 0.00001578
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.578032060933765e-05, 1.578032060933765e-05, 1.578032060933765e-05, 1.578032060933765e-05, 1.578032060933765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.578032060933765e-05

Optimization complete. Final v2v error: 3.445753335952759 mm

Highest mean error: 3.6335790157318115 mm for frame 54

Lowest mean error: 3.3373007774353027 mm for frame 35

Saving results

Total time: 104.00481176376343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534804
Iteration 2/25 | Loss: 0.00147148
Iteration 3/25 | Loss: 0.00131581
Iteration 4/25 | Loss: 0.00130270
Iteration 5/25 | Loss: 0.00130067
Iteration 6/25 | Loss: 0.00130067
Iteration 7/25 | Loss: 0.00130067
Iteration 8/25 | Loss: 0.00130067
Iteration 9/25 | Loss: 0.00130067
Iteration 10/25 | Loss: 0.00130067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013006733497604728, 0.0013006733497604728, 0.0013006733497604728, 0.0013006733497604728, 0.0013006733497604728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013006733497604728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20688593
Iteration 2/25 | Loss: 0.00160741
Iteration 3/25 | Loss: 0.00160738
Iteration 4/25 | Loss: 0.00160738
Iteration 5/25 | Loss: 0.00160738
Iteration 6/25 | Loss: 0.00160738
Iteration 7/25 | Loss: 0.00160738
Iteration 8/25 | Loss: 0.00160738
Iteration 9/25 | Loss: 0.00160738
Iteration 10/25 | Loss: 0.00160738
Iteration 11/25 | Loss: 0.00160738
Iteration 12/25 | Loss: 0.00160738
Iteration 13/25 | Loss: 0.00160738
Iteration 14/25 | Loss: 0.00160737
Iteration 15/25 | Loss: 0.00160738
Iteration 16/25 | Loss: 0.00160737
Iteration 17/25 | Loss: 0.00160738
Iteration 18/25 | Loss: 0.00160737
Iteration 19/25 | Loss: 0.00160737
Iteration 20/25 | Loss: 0.00160737
Iteration 21/25 | Loss: 0.00160737
Iteration 22/25 | Loss: 0.00160737
Iteration 23/25 | Loss: 0.00160737
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016073749866336584, 0.0016073749866336584, 0.0016073749866336584, 0.0016073749866336584, 0.0016073749866336584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016073749866336584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160737
Iteration 2/1000 | Loss: 0.00005050
Iteration 3/1000 | Loss: 0.00003357
Iteration 4/1000 | Loss: 0.00002807
Iteration 5/1000 | Loss: 0.00002556
Iteration 6/1000 | Loss: 0.00002384
Iteration 7/1000 | Loss: 0.00002273
Iteration 8/1000 | Loss: 0.00002200
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002083
Iteration 12/1000 | Loss: 0.00002068
Iteration 13/1000 | Loss: 0.00002049
Iteration 14/1000 | Loss: 0.00002042
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002035
Iteration 18/1000 | Loss: 0.00002032
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002030
Iteration 21/1000 | Loss: 0.00002029
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002021
Iteration 25/1000 | Loss: 0.00002020
Iteration 26/1000 | Loss: 0.00002019
Iteration 27/1000 | Loss: 0.00002019
Iteration 28/1000 | Loss: 0.00002018
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00002018
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002015
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002015
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002013
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00002012
Iteration 46/1000 | Loss: 0.00002012
Iteration 47/1000 | Loss: 0.00002011
Iteration 48/1000 | Loss: 0.00002011
Iteration 49/1000 | Loss: 0.00002011
Iteration 50/1000 | Loss: 0.00002011
Iteration 51/1000 | Loss: 0.00002011
Iteration 52/1000 | Loss: 0.00002011
Iteration 53/1000 | Loss: 0.00002010
Iteration 54/1000 | Loss: 0.00002010
Iteration 55/1000 | Loss: 0.00002010
Iteration 56/1000 | Loss: 0.00002010
Iteration 57/1000 | Loss: 0.00002009
Iteration 58/1000 | Loss: 0.00002009
Iteration 59/1000 | Loss: 0.00002009
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002008
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002007
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002006
Iteration 68/1000 | Loss: 0.00002006
Iteration 69/1000 | Loss: 0.00002006
Iteration 70/1000 | Loss: 0.00002006
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002005
Iteration 76/1000 | Loss: 0.00002005
Iteration 77/1000 | Loss: 0.00002005
Iteration 78/1000 | Loss: 0.00002005
Iteration 79/1000 | Loss: 0.00002004
Iteration 80/1000 | Loss: 0.00002004
Iteration 81/1000 | Loss: 0.00002004
Iteration 82/1000 | Loss: 0.00002004
Iteration 83/1000 | Loss: 0.00002004
Iteration 84/1000 | Loss: 0.00002003
Iteration 85/1000 | Loss: 0.00002003
Iteration 86/1000 | Loss: 0.00002003
Iteration 87/1000 | Loss: 0.00002003
Iteration 88/1000 | Loss: 0.00002003
Iteration 89/1000 | Loss: 0.00002003
Iteration 90/1000 | Loss: 0.00002002
Iteration 91/1000 | Loss: 0.00002002
Iteration 92/1000 | Loss: 0.00002002
Iteration 93/1000 | Loss: 0.00002002
Iteration 94/1000 | Loss: 0.00002002
Iteration 95/1000 | Loss: 0.00002002
Iteration 96/1000 | Loss: 0.00002001
Iteration 97/1000 | Loss: 0.00002001
Iteration 98/1000 | Loss: 0.00002001
Iteration 99/1000 | Loss: 0.00002001
Iteration 100/1000 | Loss: 0.00002001
Iteration 101/1000 | Loss: 0.00002001
Iteration 102/1000 | Loss: 0.00002001
Iteration 103/1000 | Loss: 0.00002001
Iteration 104/1000 | Loss: 0.00002001
Iteration 105/1000 | Loss: 0.00002001
Iteration 106/1000 | Loss: 0.00002001
Iteration 107/1000 | Loss: 0.00002001
Iteration 108/1000 | Loss: 0.00002001
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002001
Iteration 112/1000 | Loss: 0.00002001
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00002001
Iteration 116/1000 | Loss: 0.00002001
Iteration 117/1000 | Loss: 0.00002001
Iteration 118/1000 | Loss: 0.00002001
Iteration 119/1000 | Loss: 0.00002001
Iteration 120/1000 | Loss: 0.00002001
Iteration 121/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.000580025196541e-05, 2.000580025196541e-05, 2.000580025196541e-05, 2.000580025196541e-05, 2.000580025196541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000580025196541e-05

Optimization complete. Final v2v error: 3.748359441757202 mm

Highest mean error: 4.380209445953369 mm for frame 141

Lowest mean error: 3.0461530685424805 mm for frame 19

Saving results

Total time: 34.96973514556885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017202
Iteration 2/25 | Loss: 0.00237008
Iteration 3/25 | Loss: 0.00154364
Iteration 4/25 | Loss: 0.00151848
Iteration 5/25 | Loss: 0.00143831
Iteration 6/25 | Loss: 0.00143112
Iteration 7/25 | Loss: 0.00136342
Iteration 8/25 | Loss: 0.00137815
Iteration 9/25 | Loss: 0.00133146
Iteration 10/25 | Loss: 0.00132231
Iteration 11/25 | Loss: 0.00133067
Iteration 12/25 | Loss: 0.00131371
Iteration 13/25 | Loss: 0.00130941
Iteration 14/25 | Loss: 0.00131384
Iteration 15/25 | Loss: 0.00131200
Iteration 16/25 | Loss: 0.00131050
Iteration 17/25 | Loss: 0.00131765
Iteration 18/25 | Loss: 0.00131168
Iteration 19/25 | Loss: 0.00130946
Iteration 20/25 | Loss: 0.00130647
Iteration 21/25 | Loss: 0.00130896
Iteration 22/25 | Loss: 0.00130486
Iteration 23/25 | Loss: 0.00130519
Iteration 24/25 | Loss: 0.00130226
Iteration 25/25 | Loss: 0.00130402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24505460
Iteration 2/25 | Loss: 0.00328592
Iteration 3/25 | Loss: 0.00311367
Iteration 4/25 | Loss: 0.00311367
Iteration 5/25 | Loss: 0.00311366
Iteration 6/25 | Loss: 0.00311366
Iteration 7/25 | Loss: 0.00311366
Iteration 8/25 | Loss: 0.00311366
Iteration 9/25 | Loss: 0.00311366
Iteration 10/25 | Loss: 0.00311366
Iteration 11/25 | Loss: 0.00311366
Iteration 12/25 | Loss: 0.00311366
Iteration 13/25 | Loss: 0.00311366
Iteration 14/25 | Loss: 0.00311366
Iteration 15/25 | Loss: 0.00311366
Iteration 16/25 | Loss: 0.00311366
Iteration 17/25 | Loss: 0.00311366
Iteration 18/25 | Loss: 0.00311366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0031136618927121162, 0.0031136618927121162, 0.0031136618927121162, 0.0031136618927121162, 0.0031136618927121162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031136618927121162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311366
Iteration 2/1000 | Loss: 0.00025772
Iteration 3/1000 | Loss: 0.00029747
Iteration 4/1000 | Loss: 0.00072205
Iteration 5/1000 | Loss: 0.00026262
Iteration 6/1000 | Loss: 0.00044367
Iteration 7/1000 | Loss: 0.00018060
Iteration 8/1000 | Loss: 0.00050480
Iteration 9/1000 | Loss: 0.00061805
Iteration 10/1000 | Loss: 0.00022285
Iteration 11/1000 | Loss: 0.00034518
Iteration 12/1000 | Loss: 0.00016811
Iteration 13/1000 | Loss: 0.00042269
Iteration 14/1000 | Loss: 0.00046906
Iteration 15/1000 | Loss: 0.00019830
Iteration 16/1000 | Loss: 0.00027137
Iteration 17/1000 | Loss: 0.00040322
Iteration 18/1000 | Loss: 0.00048810
Iteration 19/1000 | Loss: 0.00046389
Iteration 20/1000 | Loss: 0.00067402
Iteration 21/1000 | Loss: 0.00027561
Iteration 22/1000 | Loss: 0.00076971
Iteration 23/1000 | Loss: 0.00015230
Iteration 24/1000 | Loss: 0.00066516
Iteration 25/1000 | Loss: 0.00030124
Iteration 26/1000 | Loss: 0.00039068
Iteration 27/1000 | Loss: 0.00011635
Iteration 28/1000 | Loss: 0.00011090
Iteration 29/1000 | Loss: 0.00011041
Iteration 30/1000 | Loss: 0.00032069
Iteration 31/1000 | Loss: 0.00027883
Iteration 32/1000 | Loss: 0.00028956
Iteration 33/1000 | Loss: 0.00020932
Iteration 34/1000 | Loss: 0.00028709
Iteration 35/1000 | Loss: 0.00021456
Iteration 36/1000 | Loss: 0.00029910
Iteration 37/1000 | Loss: 0.00022837
Iteration 38/1000 | Loss: 0.00063615
Iteration 39/1000 | Loss: 0.00023931
Iteration 40/1000 | Loss: 0.00031089
Iteration 41/1000 | Loss: 0.00095931
Iteration 42/1000 | Loss: 0.00042518
Iteration 43/1000 | Loss: 0.00032090
Iteration 44/1000 | Loss: 0.00013562
Iteration 45/1000 | Loss: 0.00011050
Iteration 46/1000 | Loss: 0.00010598
Iteration 47/1000 | Loss: 0.00009090
Iteration 48/1000 | Loss: 0.00007912
Iteration 49/1000 | Loss: 0.00009325
Iteration 50/1000 | Loss: 0.00036701
Iteration 51/1000 | Loss: 0.00031236
Iteration 52/1000 | Loss: 0.00031006
Iteration 53/1000 | Loss: 0.00010254
Iteration 54/1000 | Loss: 0.00009049
Iteration 55/1000 | Loss: 0.00010538
Iteration 56/1000 | Loss: 0.00007167
Iteration 57/1000 | Loss: 0.00007672
Iteration 58/1000 | Loss: 0.00008474
Iteration 59/1000 | Loss: 0.00007462
Iteration 60/1000 | Loss: 0.00007439
Iteration 61/1000 | Loss: 0.00061672
Iteration 62/1000 | Loss: 0.00077954
Iteration 63/1000 | Loss: 0.00014342
Iteration 64/1000 | Loss: 0.00026681
Iteration 65/1000 | Loss: 0.00020495
Iteration 66/1000 | Loss: 0.00012667
Iteration 67/1000 | Loss: 0.00009625
Iteration 68/1000 | Loss: 0.00050858
Iteration 69/1000 | Loss: 0.00018711
Iteration 70/1000 | Loss: 0.00011791
Iteration 71/1000 | Loss: 0.00012448
Iteration 72/1000 | Loss: 0.00007769
Iteration 73/1000 | Loss: 0.00007025
Iteration 74/1000 | Loss: 0.00005782
Iteration 75/1000 | Loss: 0.00007165
Iteration 76/1000 | Loss: 0.00017256
Iteration 77/1000 | Loss: 0.00062164
Iteration 78/1000 | Loss: 0.00092299
Iteration 79/1000 | Loss: 0.00011619
Iteration 80/1000 | Loss: 0.00009176
Iteration 81/1000 | Loss: 0.00010185
Iteration 82/1000 | Loss: 0.00005498
Iteration 83/1000 | Loss: 0.00005806
Iteration 84/1000 | Loss: 0.00009238
Iteration 85/1000 | Loss: 0.00005168
Iteration 86/1000 | Loss: 0.00004940
Iteration 87/1000 | Loss: 0.00066615
Iteration 88/1000 | Loss: 0.00101813
Iteration 89/1000 | Loss: 0.00017277
Iteration 90/1000 | Loss: 0.00017431
Iteration 91/1000 | Loss: 0.00013145
Iteration 92/1000 | Loss: 0.00013960
Iteration 93/1000 | Loss: 0.00007199
Iteration 94/1000 | Loss: 0.00006404
Iteration 95/1000 | Loss: 0.00005766
Iteration 96/1000 | Loss: 0.00015442
Iteration 97/1000 | Loss: 0.00040045
Iteration 98/1000 | Loss: 0.00049563
Iteration 99/1000 | Loss: 0.00037780
Iteration 100/1000 | Loss: 0.00007508
Iteration 101/1000 | Loss: 0.00006437
Iteration 102/1000 | Loss: 0.00004494
Iteration 103/1000 | Loss: 0.00004630
Iteration 104/1000 | Loss: 0.00051481
Iteration 105/1000 | Loss: 0.00051533
Iteration 106/1000 | Loss: 0.00007739
Iteration 107/1000 | Loss: 0.00005286
Iteration 108/1000 | Loss: 0.00018108
Iteration 109/1000 | Loss: 0.00154811
Iteration 110/1000 | Loss: 0.00009658
Iteration 111/1000 | Loss: 0.00005886
Iteration 112/1000 | Loss: 0.00005875
Iteration 113/1000 | Loss: 0.00005071
Iteration 114/1000 | Loss: 0.00019965
Iteration 115/1000 | Loss: 0.00077175
Iteration 116/1000 | Loss: 0.00008593
Iteration 117/1000 | Loss: 0.00007353
Iteration 118/1000 | Loss: 0.00006248
Iteration 119/1000 | Loss: 0.00003915
Iteration 120/1000 | Loss: 0.00005655
Iteration 121/1000 | Loss: 0.00004530
Iteration 122/1000 | Loss: 0.00030506
Iteration 123/1000 | Loss: 0.00007270
Iteration 124/1000 | Loss: 0.00004079
Iteration 125/1000 | Loss: 0.00033760
Iteration 126/1000 | Loss: 0.00018970
Iteration 127/1000 | Loss: 0.00038476
Iteration 128/1000 | Loss: 0.00025848
Iteration 129/1000 | Loss: 0.00044135
Iteration 130/1000 | Loss: 0.00020638
Iteration 131/1000 | Loss: 0.00010670
Iteration 132/1000 | Loss: 0.00003423
Iteration 133/1000 | Loss: 0.00004387
Iteration 134/1000 | Loss: 0.00025693
Iteration 135/1000 | Loss: 0.00047091
Iteration 136/1000 | Loss: 0.00021694
Iteration 137/1000 | Loss: 0.00046400
Iteration 138/1000 | Loss: 0.00022775
Iteration 139/1000 | Loss: 0.00029902
Iteration 140/1000 | Loss: 0.00008049
Iteration 141/1000 | Loss: 0.00003416
Iteration 142/1000 | Loss: 0.00005980
Iteration 143/1000 | Loss: 0.00003206
Iteration 144/1000 | Loss: 0.00002398
Iteration 145/1000 | Loss: 0.00006958
Iteration 146/1000 | Loss: 0.00018271
Iteration 147/1000 | Loss: 0.00018387
Iteration 148/1000 | Loss: 0.00002475
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00015179
Iteration 151/1000 | Loss: 0.00018152
Iteration 152/1000 | Loss: 0.00017645
Iteration 153/1000 | Loss: 0.00003700
Iteration 154/1000 | Loss: 0.00003488
Iteration 155/1000 | Loss: 0.00002393
Iteration 156/1000 | Loss: 0.00002147
Iteration 157/1000 | Loss: 0.00002007
Iteration 158/1000 | Loss: 0.00002298
Iteration 159/1000 | Loss: 0.00030616
Iteration 160/1000 | Loss: 0.00002404
Iteration 161/1000 | Loss: 0.00001787
Iteration 162/1000 | Loss: 0.00001662
Iteration 163/1000 | Loss: 0.00001836
Iteration 164/1000 | Loss: 0.00001554
Iteration 165/1000 | Loss: 0.00001465
Iteration 166/1000 | Loss: 0.00003250
Iteration 167/1000 | Loss: 0.00002226
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00004045
Iteration 171/1000 | Loss: 0.00001542
Iteration 172/1000 | Loss: 0.00001339
Iteration 173/1000 | Loss: 0.00001919
Iteration 174/1000 | Loss: 0.00001335
Iteration 175/1000 | Loss: 0.00001326
Iteration 176/1000 | Loss: 0.00001325
Iteration 177/1000 | Loss: 0.00001320
Iteration 178/1000 | Loss: 0.00001320
Iteration 179/1000 | Loss: 0.00001318
Iteration 180/1000 | Loss: 0.00001318
Iteration 181/1000 | Loss: 0.00001314
Iteration 182/1000 | Loss: 0.00003276
Iteration 183/1000 | Loss: 0.00001467
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001304
Iteration 195/1000 | Loss: 0.00001304
Iteration 196/1000 | Loss: 0.00001304
Iteration 197/1000 | Loss: 0.00001304
Iteration 198/1000 | Loss: 0.00001304
Iteration 199/1000 | Loss: 0.00001304
Iteration 200/1000 | Loss: 0.00001303
Iteration 201/1000 | Loss: 0.00001303
Iteration 202/1000 | Loss: 0.00001303
Iteration 203/1000 | Loss: 0.00001303
Iteration 204/1000 | Loss: 0.00001303
Iteration 205/1000 | Loss: 0.00001303
Iteration 206/1000 | Loss: 0.00001302
Iteration 207/1000 | Loss: 0.00001301
Iteration 208/1000 | Loss: 0.00001989
Iteration 209/1000 | Loss: 0.00001299
Iteration 210/1000 | Loss: 0.00001299
Iteration 211/1000 | Loss: 0.00001298
Iteration 212/1000 | Loss: 0.00001298
Iteration 213/1000 | Loss: 0.00001298
Iteration 214/1000 | Loss: 0.00001297
Iteration 215/1000 | Loss: 0.00001296
Iteration 216/1000 | Loss: 0.00001296
Iteration 217/1000 | Loss: 0.00001296
Iteration 218/1000 | Loss: 0.00001296
Iteration 219/1000 | Loss: 0.00001296
Iteration 220/1000 | Loss: 0.00001296
Iteration 221/1000 | Loss: 0.00001295
Iteration 222/1000 | Loss: 0.00001295
Iteration 223/1000 | Loss: 0.00001295
Iteration 224/1000 | Loss: 0.00002092
Iteration 225/1000 | Loss: 0.00001299
Iteration 226/1000 | Loss: 0.00001296
Iteration 227/1000 | Loss: 0.00001296
Iteration 228/1000 | Loss: 0.00001296
Iteration 229/1000 | Loss: 0.00001296
Iteration 230/1000 | Loss: 0.00001296
Iteration 231/1000 | Loss: 0.00001296
Iteration 232/1000 | Loss: 0.00001296
Iteration 233/1000 | Loss: 0.00001296
Iteration 234/1000 | Loss: 0.00001296
Iteration 235/1000 | Loss: 0.00001295
Iteration 236/1000 | Loss: 0.00001295
Iteration 237/1000 | Loss: 0.00001293
Iteration 238/1000 | Loss: 0.00001370
Iteration 239/1000 | Loss: 0.00001291
Iteration 240/1000 | Loss: 0.00001291
Iteration 241/1000 | Loss: 0.00001291
Iteration 242/1000 | Loss: 0.00001291
Iteration 243/1000 | Loss: 0.00001291
Iteration 244/1000 | Loss: 0.00001291
Iteration 245/1000 | Loss: 0.00001291
Iteration 246/1000 | Loss: 0.00001291
Iteration 247/1000 | Loss: 0.00001291
Iteration 248/1000 | Loss: 0.00001291
Iteration 249/1000 | Loss: 0.00001291
Iteration 250/1000 | Loss: 0.00001290
Iteration 251/1000 | Loss: 0.00001290
Iteration 252/1000 | Loss: 0.00001290
Iteration 253/1000 | Loss: 0.00001290
Iteration 254/1000 | Loss: 0.00001290
Iteration 255/1000 | Loss: 0.00001290
Iteration 256/1000 | Loss: 0.00001290
Iteration 257/1000 | Loss: 0.00001290
Iteration 258/1000 | Loss: 0.00001290
Iteration 259/1000 | Loss: 0.00001289
Iteration 260/1000 | Loss: 0.00001289
Iteration 261/1000 | Loss: 0.00001289
Iteration 262/1000 | Loss: 0.00001289
Iteration 263/1000 | Loss: 0.00001289
Iteration 264/1000 | Loss: 0.00001289
Iteration 265/1000 | Loss: 0.00001289
Iteration 266/1000 | Loss: 0.00001288
Iteration 267/1000 | Loss: 0.00001288
Iteration 268/1000 | Loss: 0.00001288
Iteration 269/1000 | Loss: 0.00001288
Iteration 270/1000 | Loss: 0.00001288
Iteration 271/1000 | Loss: 0.00001288
Iteration 272/1000 | Loss: 0.00001288
Iteration 273/1000 | Loss: 0.00001288
Iteration 274/1000 | Loss: 0.00001288
Iteration 275/1000 | Loss: 0.00001288
Iteration 276/1000 | Loss: 0.00001288
Iteration 277/1000 | Loss: 0.00001288
Iteration 278/1000 | Loss: 0.00001288
Iteration 279/1000 | Loss: 0.00001287
Iteration 280/1000 | Loss: 0.00001287
Iteration 281/1000 | Loss: 0.00001287
Iteration 282/1000 | Loss: 0.00001287
Iteration 283/1000 | Loss: 0.00001859
Iteration 284/1000 | Loss: 0.00001373
Iteration 285/1000 | Loss: 0.00001352
Iteration 286/1000 | Loss: 0.00001312
Iteration 287/1000 | Loss: 0.00001298
Iteration 288/1000 | Loss: 0.00001296
Iteration 289/1000 | Loss: 0.00002767
Iteration 290/1000 | Loss: 0.00001519
Iteration 291/1000 | Loss: 0.00001493
Iteration 292/1000 | Loss: 0.00001285
Iteration 293/1000 | Loss: 0.00001285
Iteration 294/1000 | Loss: 0.00001284
Iteration 295/1000 | Loss: 0.00001284
Iteration 296/1000 | Loss: 0.00001283
Iteration 297/1000 | Loss: 0.00001283
Iteration 298/1000 | Loss: 0.00001283
Iteration 299/1000 | Loss: 0.00001283
Iteration 300/1000 | Loss: 0.00001283
Iteration 301/1000 | Loss: 0.00001283
Iteration 302/1000 | Loss: 0.00001283
Iteration 303/1000 | Loss: 0.00001283
Iteration 304/1000 | Loss: 0.00001283
Iteration 305/1000 | Loss: 0.00001283
Iteration 306/1000 | Loss: 0.00001282
Iteration 307/1000 | Loss: 0.00001282
Iteration 308/1000 | Loss: 0.00001282
Iteration 309/1000 | Loss: 0.00001282
Iteration 310/1000 | Loss: 0.00001282
Iteration 311/1000 | Loss: 0.00001282
Iteration 312/1000 | Loss: 0.00001282
Iteration 313/1000 | Loss: 0.00001282
Iteration 314/1000 | Loss: 0.00001282
Iteration 315/1000 | Loss: 0.00001282
Iteration 316/1000 | Loss: 0.00001282
Iteration 317/1000 | Loss: 0.00001282
Iteration 318/1000 | Loss: 0.00001282
Iteration 319/1000 | Loss: 0.00001282
Iteration 320/1000 | Loss: 0.00001282
Iteration 321/1000 | Loss: 0.00001282
Iteration 322/1000 | Loss: 0.00001282
Iteration 323/1000 | Loss: 0.00001282
Iteration 324/1000 | Loss: 0.00001282
Iteration 325/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.2824402801925316e-05, 1.2824402801925316e-05, 1.2824402801925316e-05, 1.2824402801925316e-05, 1.2824402801925316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2824402801925316e-05

Optimization complete. Final v2v error: 2.6665918827056885 mm

Highest mean error: 11.34296703338623 mm for frame 131

Lowest mean error: 2.2484488487243652 mm for frame 177

Saving results

Total time: 355.3520736694336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783494
Iteration 2/25 | Loss: 0.00168319
Iteration 3/25 | Loss: 0.00138048
Iteration 4/25 | Loss: 0.00131795
Iteration 5/25 | Loss: 0.00131496
Iteration 6/25 | Loss: 0.00131333
Iteration 7/25 | Loss: 0.00130380
Iteration 8/25 | Loss: 0.00129489
Iteration 9/25 | Loss: 0.00129474
Iteration 10/25 | Loss: 0.00129600
Iteration 11/25 | Loss: 0.00128804
Iteration 12/25 | Loss: 0.00128104
Iteration 13/25 | Loss: 0.00127730
Iteration 14/25 | Loss: 0.00127926
Iteration 15/25 | Loss: 0.00127886
Iteration 16/25 | Loss: 0.00127759
Iteration 17/25 | Loss: 0.00127743
Iteration 18/25 | Loss: 0.00127683
Iteration 19/25 | Loss: 0.00128013
Iteration 20/25 | Loss: 0.00127887
Iteration 21/25 | Loss: 0.00127815
Iteration 22/25 | Loss: 0.00127725
Iteration 23/25 | Loss: 0.00127780
Iteration 24/25 | Loss: 0.00127378
Iteration 25/25 | Loss: 0.00127081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67186856
Iteration 2/25 | Loss: 0.00171115
Iteration 3/25 | Loss: 0.00171112
Iteration 4/25 | Loss: 0.00171112
Iteration 5/25 | Loss: 0.00171112
Iteration 6/25 | Loss: 0.00171112
Iteration 7/25 | Loss: 0.00171112
Iteration 8/25 | Loss: 0.00171111
Iteration 9/25 | Loss: 0.00171111
Iteration 10/25 | Loss: 0.00171111
Iteration 11/25 | Loss: 0.00171111
Iteration 12/25 | Loss: 0.00171111
Iteration 13/25 | Loss: 0.00171111
Iteration 14/25 | Loss: 0.00171111
Iteration 15/25 | Loss: 0.00171111
Iteration 16/25 | Loss: 0.00171111
Iteration 17/25 | Loss: 0.00171111
Iteration 18/25 | Loss: 0.00171111
Iteration 19/25 | Loss: 0.00171111
Iteration 20/25 | Loss: 0.00171111
Iteration 21/25 | Loss: 0.00171111
Iteration 22/25 | Loss: 0.00171111
Iteration 23/25 | Loss: 0.00171111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017111130291596055, 0.0017111130291596055, 0.0017111130291596055, 0.0017111130291596055, 0.0017111130291596055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017111130291596055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171111
Iteration 2/1000 | Loss: 0.00003115
Iteration 3/1000 | Loss: 0.00002563
Iteration 4/1000 | Loss: 0.00005852
Iteration 5/1000 | Loss: 0.00002364
Iteration 6/1000 | Loss: 0.00002234
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00005329
Iteration 9/1000 | Loss: 0.00002129
Iteration 10/1000 | Loss: 0.00002097
Iteration 11/1000 | Loss: 0.00004373
Iteration 12/1000 | Loss: 0.00002049
Iteration 13/1000 | Loss: 0.00004613
Iteration 14/1000 | Loss: 0.00002070
Iteration 15/1000 | Loss: 0.00002012
Iteration 16/1000 | Loss: 0.00001997
Iteration 17/1000 | Loss: 0.00008670
Iteration 18/1000 | Loss: 0.00003140
Iteration 19/1000 | Loss: 0.00002482
Iteration 20/1000 | Loss: 0.00013009
Iteration 21/1000 | Loss: 0.00010234
Iteration 22/1000 | Loss: 0.00004222
Iteration 23/1000 | Loss: 0.00010941
Iteration 24/1000 | Loss: 0.00010455
Iteration 25/1000 | Loss: 0.00015901
Iteration 26/1000 | Loss: 0.00004608
Iteration 27/1000 | Loss: 0.00005734
Iteration 28/1000 | Loss: 0.00015705
Iteration 29/1000 | Loss: 0.00009126
Iteration 30/1000 | Loss: 0.00010536
Iteration 31/1000 | Loss: 0.00011203
Iteration 32/1000 | Loss: 0.00023460
Iteration 33/1000 | Loss: 0.00007661
Iteration 34/1000 | Loss: 0.00009469
Iteration 35/1000 | Loss: 0.00010472
Iteration 36/1000 | Loss: 0.00014199
Iteration 37/1000 | Loss: 0.00012946
Iteration 38/1000 | Loss: 0.00015443
Iteration 39/1000 | Loss: 0.00012452
Iteration 40/1000 | Loss: 0.00021530
Iteration 41/1000 | Loss: 0.00012855
Iteration 42/1000 | Loss: 0.00006993
Iteration 43/1000 | Loss: 0.00014027
Iteration 44/1000 | Loss: 0.00006700
Iteration 45/1000 | Loss: 0.00005391
Iteration 46/1000 | Loss: 0.00004390
Iteration 47/1000 | Loss: 0.00004851
Iteration 48/1000 | Loss: 0.00003107
Iteration 49/1000 | Loss: 0.00006794
Iteration 50/1000 | Loss: 0.00003854
Iteration 51/1000 | Loss: 0.00003003
Iteration 52/1000 | Loss: 0.00005345
Iteration 53/1000 | Loss: 0.00003671
Iteration 54/1000 | Loss: 0.00004081
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00003097
Iteration 57/1000 | Loss: 0.00002921
Iteration 58/1000 | Loss: 0.00006015
Iteration 59/1000 | Loss: 0.00003218
Iteration 60/1000 | Loss: 0.00006398
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001963
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001948
Iteration 67/1000 | Loss: 0.00001948
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001947
Iteration 79/1000 | Loss: 0.00001947
Iteration 80/1000 | Loss: 0.00001947
Iteration 81/1000 | Loss: 0.00001947
Iteration 82/1000 | Loss: 0.00001947
Iteration 83/1000 | Loss: 0.00001947
Iteration 84/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.9469302060315385e-05, 1.9469302060315385e-05, 1.9469302060315385e-05, 1.9469302060315385e-05, 1.9469302060315385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9469302060315385e-05

Optimization complete. Final v2v error: 3.637923002243042 mm

Highest mean error: 6.612112998962402 mm for frame 20

Lowest mean error: 3.054950475692749 mm for frame 60

Saving results

Total time: 151.0567090511322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850965
Iteration 2/25 | Loss: 0.00125211
Iteration 3/25 | Loss: 0.00116387
Iteration 4/25 | Loss: 0.00115224
Iteration 5/25 | Loss: 0.00114930
Iteration 6/25 | Loss: 0.00114875
Iteration 7/25 | Loss: 0.00114875
Iteration 8/25 | Loss: 0.00114875
Iteration 9/25 | Loss: 0.00114871
Iteration 10/25 | Loss: 0.00114871
Iteration 11/25 | Loss: 0.00114871
Iteration 12/25 | Loss: 0.00114871
Iteration 13/25 | Loss: 0.00114871
Iteration 14/25 | Loss: 0.00114871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011487067677080631, 0.0011487067677080631, 0.0011487067677080631, 0.0011487067677080631, 0.0011487067677080631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011487067677080631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36782241
Iteration 2/25 | Loss: 0.00195612
Iteration 3/25 | Loss: 0.00195612
Iteration 4/25 | Loss: 0.00195612
Iteration 5/25 | Loss: 0.00195612
Iteration 6/25 | Loss: 0.00195611
Iteration 7/25 | Loss: 0.00195611
Iteration 8/25 | Loss: 0.00195611
Iteration 9/25 | Loss: 0.00195611
Iteration 10/25 | Loss: 0.00195611
Iteration 11/25 | Loss: 0.00195611
Iteration 12/25 | Loss: 0.00195611
Iteration 13/25 | Loss: 0.00195611
Iteration 14/25 | Loss: 0.00195611
Iteration 15/25 | Loss: 0.00195611
Iteration 16/25 | Loss: 0.00195611
Iteration 17/25 | Loss: 0.00195611
Iteration 18/25 | Loss: 0.00195611
Iteration 19/25 | Loss: 0.00195611
Iteration 20/25 | Loss: 0.00195611
Iteration 21/25 | Loss: 0.00195611
Iteration 22/25 | Loss: 0.00195611
Iteration 23/25 | Loss: 0.00195611
Iteration 24/25 | Loss: 0.00195611
Iteration 25/25 | Loss: 0.00195611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195611
Iteration 2/1000 | Loss: 0.00002520
Iteration 3/1000 | Loss: 0.00001752
Iteration 4/1000 | Loss: 0.00001418
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001201
Iteration 7/1000 | Loss: 0.00001138
Iteration 8/1000 | Loss: 0.00001102
Iteration 9/1000 | Loss: 0.00001071
Iteration 10/1000 | Loss: 0.00001043
Iteration 11/1000 | Loss: 0.00001035
Iteration 12/1000 | Loss: 0.00001032
Iteration 13/1000 | Loss: 0.00001031
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001010
Iteration 16/1000 | Loss: 0.00000997
Iteration 17/1000 | Loss: 0.00000994
Iteration 18/1000 | Loss: 0.00000990
Iteration 19/1000 | Loss: 0.00000989
Iteration 20/1000 | Loss: 0.00000989
Iteration 21/1000 | Loss: 0.00000987
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000986
Iteration 24/1000 | Loss: 0.00000983
Iteration 25/1000 | Loss: 0.00000982
Iteration 26/1000 | Loss: 0.00000982
Iteration 27/1000 | Loss: 0.00000981
Iteration 28/1000 | Loss: 0.00000981
Iteration 29/1000 | Loss: 0.00000981
Iteration 30/1000 | Loss: 0.00000980
Iteration 31/1000 | Loss: 0.00000978
Iteration 32/1000 | Loss: 0.00000977
Iteration 33/1000 | Loss: 0.00000974
Iteration 34/1000 | Loss: 0.00000973
Iteration 35/1000 | Loss: 0.00000972
Iteration 36/1000 | Loss: 0.00000972
Iteration 37/1000 | Loss: 0.00000971
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000968
Iteration 40/1000 | Loss: 0.00000966
Iteration 41/1000 | Loss: 0.00000966
Iteration 42/1000 | Loss: 0.00000966
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000966
Iteration 45/1000 | Loss: 0.00000966
Iteration 46/1000 | Loss: 0.00000966
Iteration 47/1000 | Loss: 0.00000966
Iteration 48/1000 | Loss: 0.00000966
Iteration 49/1000 | Loss: 0.00000966
Iteration 50/1000 | Loss: 0.00000965
Iteration 51/1000 | Loss: 0.00000964
Iteration 52/1000 | Loss: 0.00000964
Iteration 53/1000 | Loss: 0.00000962
Iteration 54/1000 | Loss: 0.00000962
Iteration 55/1000 | Loss: 0.00000962
Iteration 56/1000 | Loss: 0.00000962
Iteration 57/1000 | Loss: 0.00000962
Iteration 58/1000 | Loss: 0.00000962
Iteration 59/1000 | Loss: 0.00000962
Iteration 60/1000 | Loss: 0.00000962
Iteration 61/1000 | Loss: 0.00000962
Iteration 62/1000 | Loss: 0.00000962
Iteration 63/1000 | Loss: 0.00000962
Iteration 64/1000 | Loss: 0.00000961
Iteration 65/1000 | Loss: 0.00000961
Iteration 66/1000 | Loss: 0.00000960
Iteration 67/1000 | Loss: 0.00000960
Iteration 68/1000 | Loss: 0.00000960
Iteration 69/1000 | Loss: 0.00000959
Iteration 70/1000 | Loss: 0.00000959
Iteration 71/1000 | Loss: 0.00000958
Iteration 72/1000 | Loss: 0.00000958
Iteration 73/1000 | Loss: 0.00000957
Iteration 74/1000 | Loss: 0.00000957
Iteration 75/1000 | Loss: 0.00000957
Iteration 76/1000 | Loss: 0.00000957
Iteration 77/1000 | Loss: 0.00000956
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000954
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000953
Iteration 86/1000 | Loss: 0.00000953
Iteration 87/1000 | Loss: 0.00000953
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000952
Iteration 90/1000 | Loss: 0.00000952
Iteration 91/1000 | Loss: 0.00000952
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000951
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000950
Iteration 100/1000 | Loss: 0.00000950
Iteration 101/1000 | Loss: 0.00000950
Iteration 102/1000 | Loss: 0.00000950
Iteration 103/1000 | Loss: 0.00000950
Iteration 104/1000 | Loss: 0.00000950
Iteration 105/1000 | Loss: 0.00000950
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000949
Iteration 108/1000 | Loss: 0.00000949
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000949
Iteration 113/1000 | Loss: 0.00000949
Iteration 114/1000 | Loss: 0.00000949
Iteration 115/1000 | Loss: 0.00000949
Iteration 116/1000 | Loss: 0.00000949
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000948
Iteration 119/1000 | Loss: 0.00000948
Iteration 120/1000 | Loss: 0.00000948
Iteration 121/1000 | Loss: 0.00000948
Iteration 122/1000 | Loss: 0.00000948
Iteration 123/1000 | Loss: 0.00000948
Iteration 124/1000 | Loss: 0.00000948
Iteration 125/1000 | Loss: 0.00000948
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000947
Iteration 129/1000 | Loss: 0.00000947
Iteration 130/1000 | Loss: 0.00000947
Iteration 131/1000 | Loss: 0.00000947
Iteration 132/1000 | Loss: 0.00000947
Iteration 133/1000 | Loss: 0.00000947
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000946
Iteration 136/1000 | Loss: 0.00000946
Iteration 137/1000 | Loss: 0.00000946
Iteration 138/1000 | Loss: 0.00000946
Iteration 139/1000 | Loss: 0.00000946
Iteration 140/1000 | Loss: 0.00000946
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000945
Iteration 143/1000 | Loss: 0.00000945
Iteration 144/1000 | Loss: 0.00000944
Iteration 145/1000 | Loss: 0.00000944
Iteration 146/1000 | Loss: 0.00000944
Iteration 147/1000 | Loss: 0.00000944
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000943
Iteration 150/1000 | Loss: 0.00000943
Iteration 151/1000 | Loss: 0.00000943
Iteration 152/1000 | Loss: 0.00000943
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000943
Iteration 155/1000 | Loss: 0.00000943
Iteration 156/1000 | Loss: 0.00000943
Iteration 157/1000 | Loss: 0.00000943
Iteration 158/1000 | Loss: 0.00000942
Iteration 159/1000 | Loss: 0.00000942
Iteration 160/1000 | Loss: 0.00000942
Iteration 161/1000 | Loss: 0.00000941
Iteration 162/1000 | Loss: 0.00000941
Iteration 163/1000 | Loss: 0.00000941
Iteration 164/1000 | Loss: 0.00000941
Iteration 165/1000 | Loss: 0.00000941
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000940
Iteration 172/1000 | Loss: 0.00000940
Iteration 173/1000 | Loss: 0.00000940
Iteration 174/1000 | Loss: 0.00000940
Iteration 175/1000 | Loss: 0.00000940
Iteration 176/1000 | Loss: 0.00000940
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000939
Iteration 182/1000 | Loss: 0.00000939
Iteration 183/1000 | Loss: 0.00000939
Iteration 184/1000 | Loss: 0.00000939
Iteration 185/1000 | Loss: 0.00000939
Iteration 186/1000 | Loss: 0.00000939
Iteration 187/1000 | Loss: 0.00000939
Iteration 188/1000 | Loss: 0.00000939
Iteration 189/1000 | Loss: 0.00000938
Iteration 190/1000 | Loss: 0.00000938
Iteration 191/1000 | Loss: 0.00000938
Iteration 192/1000 | Loss: 0.00000938
Iteration 193/1000 | Loss: 0.00000937
Iteration 194/1000 | Loss: 0.00000937
Iteration 195/1000 | Loss: 0.00000937
Iteration 196/1000 | Loss: 0.00000937
Iteration 197/1000 | Loss: 0.00000937
Iteration 198/1000 | Loss: 0.00000937
Iteration 199/1000 | Loss: 0.00000937
Iteration 200/1000 | Loss: 0.00000937
Iteration 201/1000 | Loss: 0.00000937
Iteration 202/1000 | Loss: 0.00000936
Iteration 203/1000 | Loss: 0.00000936
Iteration 204/1000 | Loss: 0.00000936
Iteration 205/1000 | Loss: 0.00000936
Iteration 206/1000 | Loss: 0.00000936
Iteration 207/1000 | Loss: 0.00000936
Iteration 208/1000 | Loss: 0.00000936
Iteration 209/1000 | Loss: 0.00000936
Iteration 210/1000 | Loss: 0.00000936
Iteration 211/1000 | Loss: 0.00000936
Iteration 212/1000 | Loss: 0.00000935
Iteration 213/1000 | Loss: 0.00000935
Iteration 214/1000 | Loss: 0.00000935
Iteration 215/1000 | Loss: 0.00000935
Iteration 216/1000 | Loss: 0.00000935
Iteration 217/1000 | Loss: 0.00000935
Iteration 218/1000 | Loss: 0.00000935
Iteration 219/1000 | Loss: 0.00000935
Iteration 220/1000 | Loss: 0.00000935
Iteration 221/1000 | Loss: 0.00000935
Iteration 222/1000 | Loss: 0.00000935
Iteration 223/1000 | Loss: 0.00000935
Iteration 224/1000 | Loss: 0.00000935
Iteration 225/1000 | Loss: 0.00000935
Iteration 226/1000 | Loss: 0.00000935
Iteration 227/1000 | Loss: 0.00000935
Iteration 228/1000 | Loss: 0.00000935
Iteration 229/1000 | Loss: 0.00000935
Iteration 230/1000 | Loss: 0.00000934
Iteration 231/1000 | Loss: 0.00000934
Iteration 232/1000 | Loss: 0.00000934
Iteration 233/1000 | Loss: 0.00000934
Iteration 234/1000 | Loss: 0.00000934
Iteration 235/1000 | Loss: 0.00000934
Iteration 236/1000 | Loss: 0.00000934
Iteration 237/1000 | Loss: 0.00000934
Iteration 238/1000 | Loss: 0.00000934
Iteration 239/1000 | Loss: 0.00000934
Iteration 240/1000 | Loss: 0.00000934
Iteration 241/1000 | Loss: 0.00000934
Iteration 242/1000 | Loss: 0.00000934
Iteration 243/1000 | Loss: 0.00000934
Iteration 244/1000 | Loss: 0.00000934
Iteration 245/1000 | Loss: 0.00000934
Iteration 246/1000 | Loss: 0.00000934
Iteration 247/1000 | Loss: 0.00000934
Iteration 248/1000 | Loss: 0.00000934
Iteration 249/1000 | Loss: 0.00000934
Iteration 250/1000 | Loss: 0.00000934
Iteration 251/1000 | Loss: 0.00000934
Iteration 252/1000 | Loss: 0.00000934
Iteration 253/1000 | Loss: 0.00000934
Iteration 254/1000 | Loss: 0.00000934
Iteration 255/1000 | Loss: 0.00000934
Iteration 256/1000 | Loss: 0.00000934
Iteration 257/1000 | Loss: 0.00000934
Iteration 258/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [9.335590220871381e-06, 9.335590220871381e-06, 9.335590220871381e-06, 9.335590220871381e-06, 9.335590220871381e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.335590220871381e-06

Optimization complete. Final v2v error: 2.5727765560150146 mm

Highest mean error: 3.2051169872283936 mm for frame 45

Lowest mean error: 2.3889148235321045 mm for frame 12

Saving results

Total time: 42.9979362487793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822132
Iteration 2/25 | Loss: 0.00143720
Iteration 3/25 | Loss: 0.00119938
Iteration 4/25 | Loss: 0.00117505
Iteration 5/25 | Loss: 0.00117303
Iteration 6/25 | Loss: 0.00117291
Iteration 7/25 | Loss: 0.00117291
Iteration 8/25 | Loss: 0.00117291
Iteration 9/25 | Loss: 0.00117291
Iteration 10/25 | Loss: 0.00117291
Iteration 11/25 | Loss: 0.00117291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011729078833013773, 0.0011729078833013773, 0.0011729078833013773, 0.0011729078833013773, 0.0011729078833013773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011729078833013773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88001251
Iteration 2/25 | Loss: 0.00099429
Iteration 3/25 | Loss: 0.00099428
Iteration 4/25 | Loss: 0.00099428
Iteration 5/25 | Loss: 0.00099428
Iteration 6/25 | Loss: 0.00099428
Iteration 7/25 | Loss: 0.00099428
Iteration 8/25 | Loss: 0.00099428
Iteration 9/25 | Loss: 0.00099428
Iteration 10/25 | Loss: 0.00099428
Iteration 11/25 | Loss: 0.00099428
Iteration 12/25 | Loss: 0.00099428
Iteration 13/25 | Loss: 0.00099428
Iteration 14/25 | Loss: 0.00099428
Iteration 15/25 | Loss: 0.00099428
Iteration 16/25 | Loss: 0.00099428
Iteration 17/25 | Loss: 0.00099428
Iteration 18/25 | Loss: 0.00099428
Iteration 19/25 | Loss: 0.00099428
Iteration 20/25 | Loss: 0.00099428
Iteration 21/25 | Loss: 0.00099428
Iteration 22/25 | Loss: 0.00099428
Iteration 23/25 | Loss: 0.00099428
Iteration 24/25 | Loss: 0.00099428
Iteration 25/25 | Loss: 0.00099428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099428
Iteration 2/1000 | Loss: 0.00002876
Iteration 3/1000 | Loss: 0.00002156
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001794
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001640
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001622
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001603
Iteration 17/1000 | Loss: 0.00001599
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001591
Iteration 24/1000 | Loss: 0.00001591
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001589
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001588
Iteration 31/1000 | Loss: 0.00001585
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001577
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001576
Iteration 51/1000 | Loss: 0.00001576
Iteration 52/1000 | Loss: 0.00001576
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001575
Iteration 55/1000 | Loss: 0.00001575
Iteration 56/1000 | Loss: 0.00001575
Iteration 57/1000 | Loss: 0.00001575
Iteration 58/1000 | Loss: 0.00001575
Iteration 59/1000 | Loss: 0.00001574
Iteration 60/1000 | Loss: 0.00001574
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001573
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001568
Iteration 68/1000 | Loss: 0.00001568
Iteration 69/1000 | Loss: 0.00001568
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001567
Iteration 72/1000 | Loss: 0.00001567
Iteration 73/1000 | Loss: 0.00001567
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001566
Iteration 82/1000 | Loss: 0.00001566
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001566
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001565
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001558
Iteration 98/1000 | Loss: 0.00001558
Iteration 99/1000 | Loss: 0.00001558
Iteration 100/1000 | Loss: 0.00001558
Iteration 101/1000 | Loss: 0.00001558
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001557
Iteration 104/1000 | Loss: 0.00001557
Iteration 105/1000 | Loss: 0.00001556
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001556
Iteration 109/1000 | Loss: 0.00001556
Iteration 110/1000 | Loss: 0.00001556
Iteration 111/1000 | Loss: 0.00001556
Iteration 112/1000 | Loss: 0.00001556
Iteration 113/1000 | Loss: 0.00001556
Iteration 114/1000 | Loss: 0.00001556
Iteration 115/1000 | Loss: 0.00001555
Iteration 116/1000 | Loss: 0.00001555
Iteration 117/1000 | Loss: 0.00001555
Iteration 118/1000 | Loss: 0.00001555
Iteration 119/1000 | Loss: 0.00001555
Iteration 120/1000 | Loss: 0.00001555
Iteration 121/1000 | Loss: 0.00001555
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001553
Iteration 133/1000 | Loss: 0.00001553
Iteration 134/1000 | Loss: 0.00001553
Iteration 135/1000 | Loss: 0.00001553
Iteration 136/1000 | Loss: 0.00001553
Iteration 137/1000 | Loss: 0.00001553
Iteration 138/1000 | Loss: 0.00001553
Iteration 139/1000 | Loss: 0.00001553
Iteration 140/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05, 1.553265428810846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.553265428810846e-05

Optimization complete. Final v2v error: 3.3436779975891113 mm

Highest mean error: 3.6872079372406006 mm for frame 26

Lowest mean error: 3.169545888900757 mm for frame 141

Saving results

Total time: 37.294273853302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094982
Iteration 2/25 | Loss: 0.00351652
Iteration 3/25 | Loss: 0.00292313
Iteration 4/25 | Loss: 0.00203298
Iteration 5/25 | Loss: 0.00182985
Iteration 6/25 | Loss: 0.00190567
Iteration 7/25 | Loss: 0.00195839
Iteration 8/25 | Loss: 0.00170768
Iteration 9/25 | Loss: 0.00173605
Iteration 10/25 | Loss: 0.00165430
Iteration 11/25 | Loss: 0.00163592
Iteration 12/25 | Loss: 0.00163342
Iteration 13/25 | Loss: 0.00163301
Iteration 14/25 | Loss: 0.00163262
Iteration 15/25 | Loss: 0.00163230
Iteration 16/25 | Loss: 0.00163219
Iteration 17/25 | Loss: 0.00163214
Iteration 18/25 | Loss: 0.00163214
Iteration 19/25 | Loss: 0.00163213
Iteration 20/25 | Loss: 0.00163213
Iteration 21/25 | Loss: 0.00163213
Iteration 22/25 | Loss: 0.00163213
Iteration 23/25 | Loss: 0.00163213
Iteration 24/25 | Loss: 0.00163213
Iteration 25/25 | Loss: 0.00163212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.40322414
Iteration 2/25 | Loss: 0.01345769
Iteration 3/25 | Loss: 0.00194636
Iteration 4/25 | Loss: 0.00194636
Iteration 5/25 | Loss: 0.00194636
Iteration 6/25 | Loss: 0.00194636
Iteration 7/25 | Loss: 0.00194635
Iteration 8/25 | Loss: 0.00194635
Iteration 9/25 | Loss: 0.00194635
Iteration 10/25 | Loss: 0.00194635
Iteration 11/25 | Loss: 0.00194635
Iteration 12/25 | Loss: 0.00194635
Iteration 13/25 | Loss: 0.00194635
Iteration 14/25 | Loss: 0.00194635
Iteration 15/25 | Loss: 0.00194635
Iteration 16/25 | Loss: 0.00194635
Iteration 17/25 | Loss: 0.00194635
Iteration 18/25 | Loss: 0.00194635
Iteration 19/25 | Loss: 0.00194635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019463530043140054, 0.0019463530043140054, 0.0019463530043140054, 0.0019463530043140054, 0.0019463530043140054]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019463530043140054

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194635
Iteration 2/1000 | Loss: 0.00292374
Iteration 3/1000 | Loss: 0.00261269
Iteration 4/1000 | Loss: 0.00289057
Iteration 5/1000 | Loss: 0.00265671
Iteration 6/1000 | Loss: 0.00272151
Iteration 7/1000 | Loss: 0.00241304
Iteration 8/1000 | Loss: 0.00205390
Iteration 9/1000 | Loss: 0.00234889
Iteration 10/1000 | Loss: 0.00193106
Iteration 11/1000 | Loss: 0.00266584
Iteration 12/1000 | Loss: 0.00252941
Iteration 13/1000 | Loss: 0.00284801
Iteration 14/1000 | Loss: 0.00252722
Iteration 15/1000 | Loss: 0.00228479
Iteration 16/1000 | Loss: 0.00221140
Iteration 17/1000 | Loss: 0.00207911
Iteration 18/1000 | Loss: 0.00201322
Iteration 19/1000 | Loss: 0.00203438
Iteration 20/1000 | Loss: 0.00047221
Iteration 21/1000 | Loss: 0.00031624
Iteration 22/1000 | Loss: 0.00025275
Iteration 23/1000 | Loss: 0.00018067
Iteration 24/1000 | Loss: 0.00013917
Iteration 25/1000 | Loss: 0.00021140
Iteration 26/1000 | Loss: 0.00011422
Iteration 27/1000 | Loss: 0.00049841
Iteration 28/1000 | Loss: 0.00011973
Iteration 29/1000 | Loss: 0.00010013
Iteration 30/1000 | Loss: 0.00008779
Iteration 31/1000 | Loss: 0.00008088
Iteration 32/1000 | Loss: 0.00007516
Iteration 33/1000 | Loss: 0.00007030
Iteration 34/1000 | Loss: 0.00006815
Iteration 35/1000 | Loss: 0.00024294
Iteration 36/1000 | Loss: 0.00015353
Iteration 37/1000 | Loss: 0.00032183
Iteration 38/1000 | Loss: 0.00015625
Iteration 39/1000 | Loss: 0.00042432
Iteration 40/1000 | Loss: 0.00020320
Iteration 41/1000 | Loss: 0.00006554
Iteration 42/1000 | Loss: 0.00006459
Iteration 43/1000 | Loss: 0.00023786
Iteration 44/1000 | Loss: 0.00016257
Iteration 45/1000 | Loss: 0.00044561
Iteration 46/1000 | Loss: 0.00025839
Iteration 47/1000 | Loss: 0.00023522
Iteration 48/1000 | Loss: 0.00024169
Iteration 49/1000 | Loss: 0.00021471
Iteration 50/1000 | Loss: 0.00019351
Iteration 51/1000 | Loss: 0.00012483
Iteration 52/1000 | Loss: 0.00006872
Iteration 53/1000 | Loss: 0.00006582
Iteration 54/1000 | Loss: 0.00006476
Iteration 55/1000 | Loss: 0.00017781
Iteration 56/1000 | Loss: 0.00007222
Iteration 57/1000 | Loss: 0.00006918
Iteration 58/1000 | Loss: 0.00006518
Iteration 59/1000 | Loss: 0.00006349
Iteration 60/1000 | Loss: 0.00006167
Iteration 61/1000 | Loss: 0.00006048
Iteration 62/1000 | Loss: 0.00006012
Iteration 63/1000 | Loss: 0.00005994
Iteration 64/1000 | Loss: 0.00005978
Iteration 65/1000 | Loss: 0.00005956
Iteration 66/1000 | Loss: 0.00005930
Iteration 67/1000 | Loss: 0.00005912
Iteration 68/1000 | Loss: 0.00005893
Iteration 69/1000 | Loss: 0.00005875
Iteration 70/1000 | Loss: 0.00005852
Iteration 71/1000 | Loss: 0.00005835
Iteration 72/1000 | Loss: 0.00005831
Iteration 73/1000 | Loss: 0.00005826
Iteration 74/1000 | Loss: 0.00005825
Iteration 75/1000 | Loss: 0.00005824
Iteration 76/1000 | Loss: 0.00005824
Iteration 77/1000 | Loss: 0.00005824
Iteration 78/1000 | Loss: 0.00005824
Iteration 79/1000 | Loss: 0.00005824
Iteration 80/1000 | Loss: 0.00005824
Iteration 81/1000 | Loss: 0.00005823
Iteration 82/1000 | Loss: 0.00005823
Iteration 83/1000 | Loss: 0.00005823
Iteration 84/1000 | Loss: 0.00005823
Iteration 85/1000 | Loss: 0.00005823
Iteration 86/1000 | Loss: 0.00005823
Iteration 87/1000 | Loss: 0.00005823
Iteration 88/1000 | Loss: 0.00005822
Iteration 89/1000 | Loss: 0.00005822
Iteration 90/1000 | Loss: 0.00005822
Iteration 91/1000 | Loss: 0.00005822
Iteration 92/1000 | Loss: 0.00005822
Iteration 93/1000 | Loss: 0.00005822
Iteration 94/1000 | Loss: 0.00005821
Iteration 95/1000 | Loss: 0.00005821
Iteration 96/1000 | Loss: 0.00005821
Iteration 97/1000 | Loss: 0.00005821
Iteration 98/1000 | Loss: 0.00005821
Iteration 99/1000 | Loss: 0.00005821
Iteration 100/1000 | Loss: 0.00005821
Iteration 101/1000 | Loss: 0.00005821
Iteration 102/1000 | Loss: 0.00005821
Iteration 103/1000 | Loss: 0.00005821
Iteration 104/1000 | Loss: 0.00005821
Iteration 105/1000 | Loss: 0.00005821
Iteration 106/1000 | Loss: 0.00005821
Iteration 107/1000 | Loss: 0.00005821
Iteration 108/1000 | Loss: 0.00005820
Iteration 109/1000 | Loss: 0.00005820
Iteration 110/1000 | Loss: 0.00005820
Iteration 111/1000 | Loss: 0.00005820
Iteration 112/1000 | Loss: 0.00005820
Iteration 113/1000 | Loss: 0.00005820
Iteration 114/1000 | Loss: 0.00005820
Iteration 115/1000 | Loss: 0.00005820
Iteration 116/1000 | Loss: 0.00005820
Iteration 117/1000 | Loss: 0.00005820
Iteration 118/1000 | Loss: 0.00005820
Iteration 119/1000 | Loss: 0.00005820
Iteration 120/1000 | Loss: 0.00005820
Iteration 121/1000 | Loss: 0.00005819
Iteration 122/1000 | Loss: 0.00005819
Iteration 123/1000 | Loss: 0.00005819
Iteration 124/1000 | Loss: 0.00005819
Iteration 125/1000 | Loss: 0.00005819
Iteration 126/1000 | Loss: 0.00005819
Iteration 127/1000 | Loss: 0.00005819
Iteration 128/1000 | Loss: 0.00005819
Iteration 129/1000 | Loss: 0.00005819
Iteration 130/1000 | Loss: 0.00005819
Iteration 131/1000 | Loss: 0.00005819
Iteration 132/1000 | Loss: 0.00005819
Iteration 133/1000 | Loss: 0.00005819
Iteration 134/1000 | Loss: 0.00005819
Iteration 135/1000 | Loss: 0.00005819
Iteration 136/1000 | Loss: 0.00005819
Iteration 137/1000 | Loss: 0.00005819
Iteration 138/1000 | Loss: 0.00005819
Iteration 139/1000 | Loss: 0.00005819
Iteration 140/1000 | Loss: 0.00005819
Iteration 141/1000 | Loss: 0.00005819
Iteration 142/1000 | Loss: 0.00005819
Iteration 143/1000 | Loss: 0.00005819
Iteration 144/1000 | Loss: 0.00005819
Iteration 145/1000 | Loss: 0.00005819
Iteration 146/1000 | Loss: 0.00005819
Iteration 147/1000 | Loss: 0.00005819
Iteration 148/1000 | Loss: 0.00005819
Iteration 149/1000 | Loss: 0.00005819
Iteration 150/1000 | Loss: 0.00005819
Iteration 151/1000 | Loss: 0.00005819
Iteration 152/1000 | Loss: 0.00005819
Iteration 153/1000 | Loss: 0.00005819
Iteration 154/1000 | Loss: 0.00005819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [5.8188212278764695e-05, 5.8188212278764695e-05, 5.8188212278764695e-05, 5.8188212278764695e-05, 5.8188212278764695e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.8188212278764695e-05

Optimization complete. Final v2v error: 5.8873772621154785 mm

Highest mean error: 7.359981060028076 mm for frame 66

Lowest mean error: 4.612396240234375 mm for frame 121

Saving results

Total time: 131.09589266777039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588073
Iteration 2/25 | Loss: 0.00120207
Iteration 3/25 | Loss: 0.00113237
Iteration 4/25 | Loss: 0.00112237
Iteration 5/25 | Loss: 0.00111960
Iteration 6/25 | Loss: 0.00111938
Iteration 7/25 | Loss: 0.00111938
Iteration 8/25 | Loss: 0.00111938
Iteration 9/25 | Loss: 0.00111938
Iteration 10/25 | Loss: 0.00111938
Iteration 11/25 | Loss: 0.00111938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011193802347406745, 0.0011193802347406745, 0.0011193802347406745, 0.0011193802347406745, 0.0011193802347406745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011193802347406745

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95363402
Iteration 2/25 | Loss: 0.00185244
Iteration 3/25 | Loss: 0.00185243
Iteration 4/25 | Loss: 0.00185243
Iteration 5/25 | Loss: 0.00185243
Iteration 6/25 | Loss: 0.00185243
Iteration 7/25 | Loss: 0.00185243
Iteration 8/25 | Loss: 0.00185243
Iteration 9/25 | Loss: 0.00185243
Iteration 10/25 | Loss: 0.00185243
Iteration 11/25 | Loss: 0.00185243
Iteration 12/25 | Loss: 0.00185243
Iteration 13/25 | Loss: 0.00185243
Iteration 14/25 | Loss: 0.00185243
Iteration 15/25 | Loss: 0.00185243
Iteration 16/25 | Loss: 0.00185243
Iteration 17/25 | Loss: 0.00185243
Iteration 18/25 | Loss: 0.00185243
Iteration 19/25 | Loss: 0.00185243
Iteration 20/25 | Loss: 0.00185243
Iteration 21/25 | Loss: 0.00185243
Iteration 22/25 | Loss: 0.00185243
Iteration 23/25 | Loss: 0.00185243
Iteration 24/25 | Loss: 0.00185243
Iteration 25/25 | Loss: 0.00185243
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018524297047406435, 0.0018524297047406435, 0.0018524297047406435, 0.0018524297047406435, 0.0018524297047406435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018524297047406435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185243
Iteration 2/1000 | Loss: 0.00001676
Iteration 3/1000 | Loss: 0.00001316
Iteration 4/1000 | Loss: 0.00001173
Iteration 5/1000 | Loss: 0.00001091
Iteration 6/1000 | Loss: 0.00001030
Iteration 7/1000 | Loss: 0.00000988
Iteration 8/1000 | Loss: 0.00000967
Iteration 9/1000 | Loss: 0.00000950
Iteration 10/1000 | Loss: 0.00000928
Iteration 11/1000 | Loss: 0.00000925
Iteration 12/1000 | Loss: 0.00000925
Iteration 13/1000 | Loss: 0.00000924
Iteration 14/1000 | Loss: 0.00000906
Iteration 15/1000 | Loss: 0.00000901
Iteration 16/1000 | Loss: 0.00000900
Iteration 17/1000 | Loss: 0.00000896
Iteration 18/1000 | Loss: 0.00000896
Iteration 19/1000 | Loss: 0.00000895
Iteration 20/1000 | Loss: 0.00000895
Iteration 21/1000 | Loss: 0.00000890
Iteration 22/1000 | Loss: 0.00000887
Iteration 23/1000 | Loss: 0.00000876
Iteration 24/1000 | Loss: 0.00000870
Iteration 25/1000 | Loss: 0.00000867
Iteration 26/1000 | Loss: 0.00000865
Iteration 27/1000 | Loss: 0.00000861
Iteration 28/1000 | Loss: 0.00000858
Iteration 29/1000 | Loss: 0.00000858
Iteration 30/1000 | Loss: 0.00000857
Iteration 31/1000 | Loss: 0.00000854
Iteration 32/1000 | Loss: 0.00000854
Iteration 33/1000 | Loss: 0.00000853
Iteration 34/1000 | Loss: 0.00000853
Iteration 35/1000 | Loss: 0.00000853
Iteration 36/1000 | Loss: 0.00000852
Iteration 37/1000 | Loss: 0.00000852
Iteration 38/1000 | Loss: 0.00000852
Iteration 39/1000 | Loss: 0.00000852
Iteration 40/1000 | Loss: 0.00000852
Iteration 41/1000 | Loss: 0.00000852
Iteration 42/1000 | Loss: 0.00000851
Iteration 43/1000 | Loss: 0.00000851
Iteration 44/1000 | Loss: 0.00000851
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000850
Iteration 47/1000 | Loss: 0.00000850
Iteration 48/1000 | Loss: 0.00000849
Iteration 49/1000 | Loss: 0.00000849
Iteration 50/1000 | Loss: 0.00000849
Iteration 51/1000 | Loss: 0.00000847
Iteration 52/1000 | Loss: 0.00000847
Iteration 53/1000 | Loss: 0.00000846
Iteration 54/1000 | Loss: 0.00000846
Iteration 55/1000 | Loss: 0.00000846
Iteration 56/1000 | Loss: 0.00000845
Iteration 57/1000 | Loss: 0.00000845
Iteration 58/1000 | Loss: 0.00000845
Iteration 59/1000 | Loss: 0.00000844
Iteration 60/1000 | Loss: 0.00000844
Iteration 61/1000 | Loss: 0.00000844
Iteration 62/1000 | Loss: 0.00000843
Iteration 63/1000 | Loss: 0.00000843
Iteration 64/1000 | Loss: 0.00000843
Iteration 65/1000 | Loss: 0.00000843
Iteration 66/1000 | Loss: 0.00000842
Iteration 67/1000 | Loss: 0.00000842
Iteration 68/1000 | Loss: 0.00000842
Iteration 69/1000 | Loss: 0.00000842
Iteration 70/1000 | Loss: 0.00000842
Iteration 71/1000 | Loss: 0.00000842
Iteration 72/1000 | Loss: 0.00000841
Iteration 73/1000 | Loss: 0.00000841
Iteration 74/1000 | Loss: 0.00000841
Iteration 75/1000 | Loss: 0.00000841
Iteration 76/1000 | Loss: 0.00000840
Iteration 77/1000 | Loss: 0.00000840
Iteration 78/1000 | Loss: 0.00000840
Iteration 79/1000 | Loss: 0.00000840
Iteration 80/1000 | Loss: 0.00000839
Iteration 81/1000 | Loss: 0.00000839
Iteration 82/1000 | Loss: 0.00000839
Iteration 83/1000 | Loss: 0.00000839
Iteration 84/1000 | Loss: 0.00000839
Iteration 85/1000 | Loss: 0.00000839
Iteration 86/1000 | Loss: 0.00000839
Iteration 87/1000 | Loss: 0.00000839
Iteration 88/1000 | Loss: 0.00000839
Iteration 89/1000 | Loss: 0.00000838
Iteration 90/1000 | Loss: 0.00000838
Iteration 91/1000 | Loss: 0.00000838
Iteration 92/1000 | Loss: 0.00000838
Iteration 93/1000 | Loss: 0.00000837
Iteration 94/1000 | Loss: 0.00000837
Iteration 95/1000 | Loss: 0.00000837
Iteration 96/1000 | Loss: 0.00000837
Iteration 97/1000 | Loss: 0.00000837
Iteration 98/1000 | Loss: 0.00000837
Iteration 99/1000 | Loss: 0.00000837
Iteration 100/1000 | Loss: 0.00000837
Iteration 101/1000 | Loss: 0.00000837
Iteration 102/1000 | Loss: 0.00000836
Iteration 103/1000 | Loss: 0.00000836
Iteration 104/1000 | Loss: 0.00000836
Iteration 105/1000 | Loss: 0.00000836
Iteration 106/1000 | Loss: 0.00000836
Iteration 107/1000 | Loss: 0.00000836
Iteration 108/1000 | Loss: 0.00000836
Iteration 109/1000 | Loss: 0.00000835
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000835
Iteration 112/1000 | Loss: 0.00000835
Iteration 113/1000 | Loss: 0.00000835
Iteration 114/1000 | Loss: 0.00000834
Iteration 115/1000 | Loss: 0.00000834
Iteration 116/1000 | Loss: 0.00000834
Iteration 117/1000 | Loss: 0.00000834
Iteration 118/1000 | Loss: 0.00000833
Iteration 119/1000 | Loss: 0.00000833
Iteration 120/1000 | Loss: 0.00000833
Iteration 121/1000 | Loss: 0.00000833
Iteration 122/1000 | Loss: 0.00000833
Iteration 123/1000 | Loss: 0.00000833
Iteration 124/1000 | Loss: 0.00000833
Iteration 125/1000 | Loss: 0.00000833
Iteration 126/1000 | Loss: 0.00000833
Iteration 127/1000 | Loss: 0.00000833
Iteration 128/1000 | Loss: 0.00000833
Iteration 129/1000 | Loss: 0.00000833
Iteration 130/1000 | Loss: 0.00000832
Iteration 131/1000 | Loss: 0.00000832
Iteration 132/1000 | Loss: 0.00000832
Iteration 133/1000 | Loss: 0.00000832
Iteration 134/1000 | Loss: 0.00000832
Iteration 135/1000 | Loss: 0.00000832
Iteration 136/1000 | Loss: 0.00000832
Iteration 137/1000 | Loss: 0.00000832
Iteration 138/1000 | Loss: 0.00000832
Iteration 139/1000 | Loss: 0.00000831
Iteration 140/1000 | Loss: 0.00000831
Iteration 141/1000 | Loss: 0.00000831
Iteration 142/1000 | Loss: 0.00000831
Iteration 143/1000 | Loss: 0.00000831
Iteration 144/1000 | Loss: 0.00000831
Iteration 145/1000 | Loss: 0.00000831
Iteration 146/1000 | Loss: 0.00000831
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000830
Iteration 153/1000 | Loss: 0.00000830
Iteration 154/1000 | Loss: 0.00000830
Iteration 155/1000 | Loss: 0.00000830
Iteration 156/1000 | Loss: 0.00000830
Iteration 157/1000 | Loss: 0.00000830
Iteration 158/1000 | Loss: 0.00000829
Iteration 159/1000 | Loss: 0.00000829
Iteration 160/1000 | Loss: 0.00000829
Iteration 161/1000 | Loss: 0.00000829
Iteration 162/1000 | Loss: 0.00000829
Iteration 163/1000 | Loss: 0.00000829
Iteration 164/1000 | Loss: 0.00000829
Iteration 165/1000 | Loss: 0.00000829
Iteration 166/1000 | Loss: 0.00000829
Iteration 167/1000 | Loss: 0.00000829
Iteration 168/1000 | Loss: 0.00000829
Iteration 169/1000 | Loss: 0.00000829
Iteration 170/1000 | Loss: 0.00000829
Iteration 171/1000 | Loss: 0.00000829
Iteration 172/1000 | Loss: 0.00000829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [8.289752258860972e-06, 8.289752258860972e-06, 8.289752258860972e-06, 8.289752258860972e-06, 8.289752258860972e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.289752258860972e-06

Optimization complete. Final v2v error: 2.5347301959991455 mm

Highest mean error: 2.7835211753845215 mm for frame 111

Lowest mean error: 2.4065425395965576 mm for frame 170

Saving results

Total time: 39.526684045791626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502508
Iteration 2/25 | Loss: 0.00136794
Iteration 3/25 | Loss: 0.00123118
Iteration 4/25 | Loss: 0.00121864
Iteration 5/25 | Loss: 0.00121620
Iteration 6/25 | Loss: 0.00121620
Iteration 7/25 | Loss: 0.00121620
Iteration 8/25 | Loss: 0.00121620
Iteration 9/25 | Loss: 0.00121620
Iteration 10/25 | Loss: 0.00121620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001216204254887998, 0.001216204254887998, 0.001216204254887998, 0.001216204254887998, 0.001216204254887998]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001216204254887998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25036466
Iteration 2/25 | Loss: 0.00197326
Iteration 3/25 | Loss: 0.00197325
Iteration 4/25 | Loss: 0.00197325
Iteration 5/25 | Loss: 0.00197325
Iteration 6/25 | Loss: 0.00197325
Iteration 7/25 | Loss: 0.00197325
Iteration 8/25 | Loss: 0.00197325
Iteration 9/25 | Loss: 0.00197325
Iteration 10/25 | Loss: 0.00197325
Iteration 11/25 | Loss: 0.00197325
Iteration 12/25 | Loss: 0.00197325
Iteration 13/25 | Loss: 0.00197325
Iteration 14/25 | Loss: 0.00197325
Iteration 15/25 | Loss: 0.00197325
Iteration 16/25 | Loss: 0.00197325
Iteration 17/25 | Loss: 0.00197325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0019732459913939238, 0.0019732459913939238, 0.0019732459913939238, 0.0019732459913939238, 0.0019732459913939238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019732459913939238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197325
Iteration 2/1000 | Loss: 0.00003536
Iteration 3/1000 | Loss: 0.00002332
Iteration 4/1000 | Loss: 0.00002033
Iteration 5/1000 | Loss: 0.00001917
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001779
Iteration 8/1000 | Loss: 0.00001740
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001679
Iteration 11/1000 | Loss: 0.00001654
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001612
Iteration 14/1000 | Loss: 0.00001593
Iteration 15/1000 | Loss: 0.00001590
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001574
Iteration 20/1000 | Loss: 0.00001573
Iteration 21/1000 | Loss: 0.00001572
Iteration 22/1000 | Loss: 0.00001571
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001567
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001562
Iteration 28/1000 | Loss: 0.00001560
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001556
Iteration 36/1000 | Loss: 0.00001556
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001551
Iteration 48/1000 | Loss: 0.00001551
Iteration 49/1000 | Loss: 0.00001550
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001550
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001550
Iteration 58/1000 | Loss: 0.00001550
Iteration 59/1000 | Loss: 0.00001550
Iteration 60/1000 | Loss: 0.00001549
Iteration 61/1000 | Loss: 0.00001549
Iteration 62/1000 | Loss: 0.00001549
Iteration 63/1000 | Loss: 0.00001549
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001546
Iteration 69/1000 | Loss: 0.00001546
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001544
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001541
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001540
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001539
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001538
Iteration 100/1000 | Loss: 0.00001538
Iteration 101/1000 | Loss: 0.00001538
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001537
Iteration 106/1000 | Loss: 0.00001537
Iteration 107/1000 | Loss: 0.00001537
Iteration 108/1000 | Loss: 0.00001537
Iteration 109/1000 | Loss: 0.00001537
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001536
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Iteration 117/1000 | Loss: 0.00001535
Iteration 118/1000 | Loss: 0.00001535
Iteration 119/1000 | Loss: 0.00001534
Iteration 120/1000 | Loss: 0.00001534
Iteration 121/1000 | Loss: 0.00001534
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001534
Iteration 124/1000 | Loss: 0.00001534
Iteration 125/1000 | Loss: 0.00001534
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001532
Iteration 136/1000 | Loss: 0.00001532
Iteration 137/1000 | Loss: 0.00001532
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001532
Iteration 140/1000 | Loss: 0.00001532
Iteration 141/1000 | Loss: 0.00001532
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001531
Iteration 145/1000 | Loss: 0.00001531
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001531
Iteration 149/1000 | Loss: 0.00001531
Iteration 150/1000 | Loss: 0.00001531
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001530
Iteration 154/1000 | Loss: 0.00001530
Iteration 155/1000 | Loss: 0.00001530
Iteration 156/1000 | Loss: 0.00001530
Iteration 157/1000 | Loss: 0.00001530
Iteration 158/1000 | Loss: 0.00001530
Iteration 159/1000 | Loss: 0.00001530
Iteration 160/1000 | Loss: 0.00001530
Iteration 161/1000 | Loss: 0.00001530
Iteration 162/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5300220184144564e-05, 1.5300220184144564e-05, 1.5300220184144564e-05, 1.5300220184144564e-05, 1.5300220184144564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5300220184144564e-05

Optimization complete. Final v2v error: 3.2174293994903564 mm

Highest mean error: 3.8831143379211426 mm for frame 209

Lowest mean error: 2.640544891357422 mm for frame 102

Saving results

Total time: 47.33745241165161
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005483
Iteration 2/25 | Loss: 0.00398395
Iteration 3/25 | Loss: 0.00227495
Iteration 4/25 | Loss: 0.00176656
Iteration 5/25 | Loss: 0.00170316
Iteration 6/25 | Loss: 0.00163626
Iteration 7/25 | Loss: 0.00162013
Iteration 8/25 | Loss: 0.00159995
Iteration 9/25 | Loss: 0.00157982
Iteration 10/25 | Loss: 0.00157040
Iteration 11/25 | Loss: 0.00156484
Iteration 12/25 | Loss: 0.00155297
Iteration 13/25 | Loss: 0.00155999
Iteration 14/25 | Loss: 0.00154612
Iteration 15/25 | Loss: 0.00154268
Iteration 16/25 | Loss: 0.00154141
Iteration 17/25 | Loss: 0.00154026
Iteration 18/25 | Loss: 0.00153884
Iteration 19/25 | Loss: 0.00153766
Iteration 20/25 | Loss: 0.00153624
Iteration 21/25 | Loss: 0.00153536
Iteration 22/25 | Loss: 0.00153482
Iteration 23/25 | Loss: 0.00153427
Iteration 24/25 | Loss: 0.00153391
Iteration 25/25 | Loss: 0.00153349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.85959101
Iteration 2/25 | Loss: 0.00588654
Iteration 3/25 | Loss: 0.00588649
Iteration 4/25 | Loss: 0.00588649
Iteration 5/25 | Loss: 0.00588649
Iteration 6/25 | Loss: 0.00588649
Iteration 7/25 | Loss: 0.00588649
Iteration 8/25 | Loss: 0.00588649
Iteration 9/25 | Loss: 0.00588649
Iteration 10/25 | Loss: 0.00588649
Iteration 11/25 | Loss: 0.00588649
Iteration 12/25 | Loss: 0.00588649
Iteration 13/25 | Loss: 0.00588649
Iteration 14/25 | Loss: 0.00588649
Iteration 15/25 | Loss: 0.00588649
Iteration 16/25 | Loss: 0.00588649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005886487662792206, 0.005886487662792206, 0.005886487662792206, 0.005886487662792206, 0.005886487662792206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005886487662792206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00588649
Iteration 2/1000 | Loss: 0.00186179
Iteration 3/1000 | Loss: 0.00109387
Iteration 4/1000 | Loss: 0.00185158
Iteration 5/1000 | Loss: 0.00172384
Iteration 6/1000 | Loss: 0.00150941
Iteration 7/1000 | Loss: 0.00095981
Iteration 8/1000 | Loss: 0.00126304
Iteration 9/1000 | Loss: 0.00022279
Iteration 10/1000 | Loss: 0.00053182
Iteration 11/1000 | Loss: 0.00010811
Iteration 12/1000 | Loss: 0.00009406
Iteration 13/1000 | Loss: 0.00107289
Iteration 14/1000 | Loss: 0.00010829
Iteration 15/1000 | Loss: 0.00007244
Iteration 16/1000 | Loss: 0.00005551
Iteration 17/1000 | Loss: 0.00004703
Iteration 18/1000 | Loss: 0.00004112
Iteration 19/1000 | Loss: 0.00003659
Iteration 20/1000 | Loss: 0.00003436
Iteration 21/1000 | Loss: 0.00003270
Iteration 22/1000 | Loss: 0.00003125
Iteration 23/1000 | Loss: 0.00003025
Iteration 24/1000 | Loss: 0.00002943
Iteration 25/1000 | Loss: 0.00002875
Iteration 26/1000 | Loss: 0.00002812
Iteration 27/1000 | Loss: 0.00041182
Iteration 28/1000 | Loss: 0.00060881
Iteration 29/1000 | Loss: 0.00004540
Iteration 30/1000 | Loss: 0.00003083
Iteration 31/1000 | Loss: 0.00002552
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00001914
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001758
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001686
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001666
Iteration 42/1000 | Loss: 0.00001661
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001645
Iteration 47/1000 | Loss: 0.00001645
Iteration 48/1000 | Loss: 0.00001645
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001643
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001640
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001639
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001638
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001636
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001634
Iteration 90/1000 | Loss: 0.00001634
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001632
Iteration 93/1000 | Loss: 0.00001632
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001632
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001632
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001630
Iteration 103/1000 | Loss: 0.00001630
Iteration 104/1000 | Loss: 0.00001630
Iteration 105/1000 | Loss: 0.00001630
Iteration 106/1000 | Loss: 0.00001629
Iteration 107/1000 | Loss: 0.00001629
Iteration 108/1000 | Loss: 0.00001629
Iteration 109/1000 | Loss: 0.00001629
Iteration 110/1000 | Loss: 0.00001628
Iteration 111/1000 | Loss: 0.00001628
Iteration 112/1000 | Loss: 0.00001628
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001627
Iteration 117/1000 | Loss: 0.00001627
Iteration 118/1000 | Loss: 0.00001627
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001626
Iteration 129/1000 | Loss: 0.00001625
Iteration 130/1000 | Loss: 0.00001625
Iteration 131/1000 | Loss: 0.00001625
Iteration 132/1000 | Loss: 0.00001625
Iteration 133/1000 | Loss: 0.00001625
Iteration 134/1000 | Loss: 0.00001625
Iteration 135/1000 | Loss: 0.00001625
Iteration 136/1000 | Loss: 0.00001625
Iteration 137/1000 | Loss: 0.00001624
Iteration 138/1000 | Loss: 0.00001624
Iteration 139/1000 | Loss: 0.00001624
Iteration 140/1000 | Loss: 0.00001624
Iteration 141/1000 | Loss: 0.00001623
Iteration 142/1000 | Loss: 0.00001623
Iteration 143/1000 | Loss: 0.00001623
Iteration 144/1000 | Loss: 0.00001623
Iteration 145/1000 | Loss: 0.00001622
Iteration 146/1000 | Loss: 0.00001622
Iteration 147/1000 | Loss: 0.00001622
Iteration 148/1000 | Loss: 0.00001622
Iteration 149/1000 | Loss: 0.00001622
Iteration 150/1000 | Loss: 0.00001622
Iteration 151/1000 | Loss: 0.00001622
Iteration 152/1000 | Loss: 0.00001622
Iteration 153/1000 | Loss: 0.00001622
Iteration 154/1000 | Loss: 0.00001621
Iteration 155/1000 | Loss: 0.00001621
Iteration 156/1000 | Loss: 0.00001621
Iteration 157/1000 | Loss: 0.00001621
Iteration 158/1000 | Loss: 0.00001621
Iteration 159/1000 | Loss: 0.00001621
Iteration 160/1000 | Loss: 0.00001621
Iteration 161/1000 | Loss: 0.00001621
Iteration 162/1000 | Loss: 0.00001621
Iteration 163/1000 | Loss: 0.00001621
Iteration 164/1000 | Loss: 0.00001621
Iteration 165/1000 | Loss: 0.00001621
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001621
Iteration 168/1000 | Loss: 0.00001621
Iteration 169/1000 | Loss: 0.00001621
Iteration 170/1000 | Loss: 0.00001621
Iteration 171/1000 | Loss: 0.00001621
Iteration 172/1000 | Loss: 0.00001621
Iteration 173/1000 | Loss: 0.00001621
Iteration 174/1000 | Loss: 0.00001621
Iteration 175/1000 | Loss: 0.00001621
Iteration 176/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.6208594388444908e-05, 1.6208594388444908e-05, 1.6208594388444908e-05, 1.6208594388444908e-05, 1.6208594388444908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6208594388444908e-05

Optimization complete. Final v2v error: 3.2081592082977295 mm

Highest mean error: 4.980239391326904 mm for frame 77

Lowest mean error: 2.477778196334839 mm for frame 4

Saving results

Total time: 111.29278445243835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436518
Iteration 2/25 | Loss: 0.00132092
Iteration 3/25 | Loss: 0.00119651
Iteration 4/25 | Loss: 0.00118448
Iteration 5/25 | Loss: 0.00118250
Iteration 6/25 | Loss: 0.00118237
Iteration 7/25 | Loss: 0.00118237
Iteration 8/25 | Loss: 0.00118237
Iteration 9/25 | Loss: 0.00118237
Iteration 10/25 | Loss: 0.00118237
Iteration 11/25 | Loss: 0.00118237
Iteration 12/25 | Loss: 0.00118237
Iteration 13/25 | Loss: 0.00118237
Iteration 14/25 | Loss: 0.00118237
Iteration 15/25 | Loss: 0.00118237
Iteration 16/25 | Loss: 0.00118237
Iteration 17/25 | Loss: 0.00118237
Iteration 18/25 | Loss: 0.00118237
Iteration 19/25 | Loss: 0.00118237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011823743116110563, 0.0011823743116110563, 0.0011823743116110563, 0.0011823743116110563, 0.0011823743116110563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011823743116110563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.66384172
Iteration 2/25 | Loss: 0.00145859
Iteration 3/25 | Loss: 0.00145854
Iteration 4/25 | Loss: 0.00145854
Iteration 5/25 | Loss: 0.00145854
Iteration 6/25 | Loss: 0.00145854
Iteration 7/25 | Loss: 0.00145854
Iteration 8/25 | Loss: 0.00145854
Iteration 9/25 | Loss: 0.00145854
Iteration 10/25 | Loss: 0.00145854
Iteration 11/25 | Loss: 0.00145854
Iteration 12/25 | Loss: 0.00145854
Iteration 13/25 | Loss: 0.00145854
Iteration 14/25 | Loss: 0.00145854
Iteration 15/25 | Loss: 0.00145854
Iteration 16/25 | Loss: 0.00145854
Iteration 17/25 | Loss: 0.00145854
Iteration 18/25 | Loss: 0.00145854
Iteration 19/25 | Loss: 0.00145854
Iteration 20/25 | Loss: 0.00145854
Iteration 21/25 | Loss: 0.00145854
Iteration 22/25 | Loss: 0.00145854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014585384633392096, 0.0014585384633392096, 0.0014585384633392096, 0.0014585384633392096, 0.0014585384633392096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014585384633392096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145854
Iteration 2/1000 | Loss: 0.00002127
Iteration 3/1000 | Loss: 0.00001489
Iteration 4/1000 | Loss: 0.00001356
Iteration 5/1000 | Loss: 0.00001282
Iteration 6/1000 | Loss: 0.00001232
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001161
Iteration 9/1000 | Loss: 0.00001129
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001099
Iteration 13/1000 | Loss: 0.00001095
Iteration 14/1000 | Loss: 0.00001095
Iteration 15/1000 | Loss: 0.00001094
Iteration 16/1000 | Loss: 0.00001089
Iteration 17/1000 | Loss: 0.00001089
Iteration 18/1000 | Loss: 0.00001085
Iteration 19/1000 | Loss: 0.00001084
Iteration 20/1000 | Loss: 0.00001084
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001083
Iteration 23/1000 | Loss: 0.00001082
Iteration 24/1000 | Loss: 0.00001081
Iteration 25/1000 | Loss: 0.00001079
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001078
Iteration 30/1000 | Loss: 0.00001078
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001078
Iteration 34/1000 | Loss: 0.00001076
Iteration 35/1000 | Loss: 0.00001075
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001074
Iteration 41/1000 | Loss: 0.00001069
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001065
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001062
Iteration 51/1000 | Loss: 0.00001062
Iteration 52/1000 | Loss: 0.00001059
Iteration 53/1000 | Loss: 0.00001059
Iteration 54/1000 | Loss: 0.00001058
Iteration 55/1000 | Loss: 0.00001058
Iteration 56/1000 | Loss: 0.00001057
Iteration 57/1000 | Loss: 0.00001057
Iteration 58/1000 | Loss: 0.00001056
Iteration 59/1000 | Loss: 0.00001056
Iteration 60/1000 | Loss: 0.00001055
Iteration 61/1000 | Loss: 0.00001054
Iteration 62/1000 | Loss: 0.00001054
Iteration 63/1000 | Loss: 0.00001054
Iteration 64/1000 | Loss: 0.00001053
Iteration 65/1000 | Loss: 0.00001053
Iteration 66/1000 | Loss: 0.00001053
Iteration 67/1000 | Loss: 0.00001052
Iteration 68/1000 | Loss: 0.00001052
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001051
Iteration 74/1000 | Loss: 0.00001051
Iteration 75/1000 | Loss: 0.00001051
Iteration 76/1000 | Loss: 0.00001051
Iteration 77/1000 | Loss: 0.00001051
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001051
Iteration 80/1000 | Loss: 0.00001051
Iteration 81/1000 | Loss: 0.00001050
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001050
Iteration 84/1000 | Loss: 0.00001050
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001047
Iteration 89/1000 | Loss: 0.00001047
Iteration 90/1000 | Loss: 0.00001047
Iteration 91/1000 | Loss: 0.00001047
Iteration 92/1000 | Loss: 0.00001047
Iteration 93/1000 | Loss: 0.00001046
Iteration 94/1000 | Loss: 0.00001046
Iteration 95/1000 | Loss: 0.00001046
Iteration 96/1000 | Loss: 0.00001046
Iteration 97/1000 | Loss: 0.00001046
Iteration 98/1000 | Loss: 0.00001046
Iteration 99/1000 | Loss: 0.00001046
Iteration 100/1000 | Loss: 0.00001044
Iteration 101/1000 | Loss: 0.00001044
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001043
Iteration 104/1000 | Loss: 0.00001043
Iteration 105/1000 | Loss: 0.00001043
Iteration 106/1000 | Loss: 0.00001043
Iteration 107/1000 | Loss: 0.00001043
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001040
Iteration 112/1000 | Loss: 0.00001040
Iteration 113/1000 | Loss: 0.00001040
Iteration 114/1000 | Loss: 0.00001040
Iteration 115/1000 | Loss: 0.00001040
Iteration 116/1000 | Loss: 0.00001040
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001039
Iteration 119/1000 | Loss: 0.00001039
Iteration 120/1000 | Loss: 0.00001039
Iteration 121/1000 | Loss: 0.00001039
Iteration 122/1000 | Loss: 0.00001039
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001039
Iteration 125/1000 | Loss: 0.00001039
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001038
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001037
Iteration 137/1000 | Loss: 0.00001037
Iteration 138/1000 | Loss: 0.00001037
Iteration 139/1000 | Loss: 0.00001037
Iteration 140/1000 | Loss: 0.00001037
Iteration 141/1000 | Loss: 0.00001037
Iteration 142/1000 | Loss: 0.00001037
Iteration 143/1000 | Loss: 0.00001037
Iteration 144/1000 | Loss: 0.00001037
Iteration 145/1000 | Loss: 0.00001037
Iteration 146/1000 | Loss: 0.00001037
Iteration 147/1000 | Loss: 0.00001037
Iteration 148/1000 | Loss: 0.00001037
Iteration 149/1000 | Loss: 0.00001037
Iteration 150/1000 | Loss: 0.00001036
Iteration 151/1000 | Loss: 0.00001036
Iteration 152/1000 | Loss: 0.00001036
Iteration 153/1000 | Loss: 0.00001036
Iteration 154/1000 | Loss: 0.00001036
Iteration 155/1000 | Loss: 0.00001036
Iteration 156/1000 | Loss: 0.00001036
Iteration 157/1000 | Loss: 0.00001036
Iteration 158/1000 | Loss: 0.00001036
Iteration 159/1000 | Loss: 0.00001036
Iteration 160/1000 | Loss: 0.00001036
Iteration 161/1000 | Loss: 0.00001035
Iteration 162/1000 | Loss: 0.00001035
Iteration 163/1000 | Loss: 0.00001035
Iteration 164/1000 | Loss: 0.00001035
Iteration 165/1000 | Loss: 0.00001035
Iteration 166/1000 | Loss: 0.00001035
Iteration 167/1000 | Loss: 0.00001035
Iteration 168/1000 | Loss: 0.00001035
Iteration 169/1000 | Loss: 0.00001035
Iteration 170/1000 | Loss: 0.00001035
Iteration 171/1000 | Loss: 0.00001035
Iteration 172/1000 | Loss: 0.00001035
Iteration 173/1000 | Loss: 0.00001035
Iteration 174/1000 | Loss: 0.00001035
Iteration 175/1000 | Loss: 0.00001035
Iteration 176/1000 | Loss: 0.00001035
Iteration 177/1000 | Loss: 0.00001035
Iteration 178/1000 | Loss: 0.00001035
Iteration 179/1000 | Loss: 0.00001035
Iteration 180/1000 | Loss: 0.00001035
Iteration 181/1000 | Loss: 0.00001035
Iteration 182/1000 | Loss: 0.00001035
Iteration 183/1000 | Loss: 0.00001034
Iteration 184/1000 | Loss: 0.00001034
Iteration 185/1000 | Loss: 0.00001034
Iteration 186/1000 | Loss: 0.00001034
Iteration 187/1000 | Loss: 0.00001034
Iteration 188/1000 | Loss: 0.00001034
Iteration 189/1000 | Loss: 0.00001034
Iteration 190/1000 | Loss: 0.00001034
Iteration 191/1000 | Loss: 0.00001034
Iteration 192/1000 | Loss: 0.00001034
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001034
Iteration 196/1000 | Loss: 0.00001033
Iteration 197/1000 | Loss: 0.00001033
Iteration 198/1000 | Loss: 0.00001033
Iteration 199/1000 | Loss: 0.00001033
Iteration 200/1000 | Loss: 0.00001033
Iteration 201/1000 | Loss: 0.00001033
Iteration 202/1000 | Loss: 0.00001033
Iteration 203/1000 | Loss: 0.00001033
Iteration 204/1000 | Loss: 0.00001033
Iteration 205/1000 | Loss: 0.00001033
Iteration 206/1000 | Loss: 0.00001033
Iteration 207/1000 | Loss: 0.00001033
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0333567843190394e-05, 1.0333567843190394e-05, 1.0333567843190394e-05, 1.0333567843190394e-05, 1.0333567843190394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0333567843190394e-05

Optimization complete. Final v2v error: 2.7159998416900635 mm

Highest mean error: 3.0616886615753174 mm for frame 76

Lowest mean error: 2.463639736175537 mm for frame 22

Saving results

Total time: 40.072460412979126
