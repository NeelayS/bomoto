Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=29, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1624-1679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013093
Iteration 2/25 | Loss: 0.01013092
Iteration 3/25 | Loss: 0.00296277
Iteration 4/25 | Loss: 0.00222364
Iteration 5/25 | Loss: 0.00222600
Iteration 6/25 | Loss: 0.00201535
Iteration 7/25 | Loss: 0.00188130
Iteration 8/25 | Loss: 0.00179204
Iteration 9/25 | Loss: 0.00170735
Iteration 10/25 | Loss: 0.00163448
Iteration 11/25 | Loss: 0.00162144
Iteration 12/25 | Loss: 0.00161024
Iteration 13/25 | Loss: 0.00158930
Iteration 14/25 | Loss: 0.00158095
Iteration 15/25 | Loss: 0.00157950
Iteration 16/25 | Loss: 0.00157571
Iteration 17/25 | Loss: 0.00157190
Iteration 18/25 | Loss: 0.00156919
Iteration 19/25 | Loss: 0.00156867
Iteration 20/25 | Loss: 0.00156833
Iteration 21/25 | Loss: 0.00156882
Iteration 22/25 | Loss: 0.00156965
Iteration 23/25 | Loss: 0.00156670
Iteration 24/25 | Loss: 0.00156588
Iteration 25/25 | Loss: 0.00156514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42778516
Iteration 2/25 | Loss: 0.00299214
Iteration 3/25 | Loss: 0.00269590
Iteration 4/25 | Loss: 0.00269589
Iteration 5/25 | Loss: 0.00269589
Iteration 6/25 | Loss: 0.00269589
Iteration 7/25 | Loss: 0.00269589
Iteration 8/25 | Loss: 0.00269589
Iteration 9/25 | Loss: 0.00269589
Iteration 10/25 | Loss: 0.00269589
Iteration 11/25 | Loss: 0.00269589
Iteration 12/25 | Loss: 0.00269589
Iteration 13/25 | Loss: 0.00269589
Iteration 14/25 | Loss: 0.00269589
Iteration 15/25 | Loss: 0.00269589
Iteration 16/25 | Loss: 0.00269589
Iteration 17/25 | Loss: 0.00269589
Iteration 18/25 | Loss: 0.00269589
Iteration 19/25 | Loss: 0.00269589
Iteration 20/25 | Loss: 0.00269589
Iteration 21/25 | Loss: 0.00269589
Iteration 22/25 | Loss: 0.00269589
Iteration 23/25 | Loss: 0.00269589
Iteration 24/25 | Loss: 0.00269589
Iteration 25/25 | Loss: 0.00269589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269589
Iteration 2/1000 | Loss: 0.00043626
Iteration 3/1000 | Loss: 0.00063567
Iteration 4/1000 | Loss: 0.00163604
Iteration 5/1000 | Loss: 0.00294329
Iteration 6/1000 | Loss: 0.00054543
Iteration 7/1000 | Loss: 0.00034929
Iteration 8/1000 | Loss: 0.00165284
Iteration 9/1000 | Loss: 0.00037808
Iteration 10/1000 | Loss: 0.00025873
Iteration 11/1000 | Loss: 0.00020900
Iteration 12/1000 | Loss: 0.00029467
Iteration 13/1000 | Loss: 0.00158960
Iteration 14/1000 | Loss: 0.00135743
Iteration 15/1000 | Loss: 0.00153864
Iteration 16/1000 | Loss: 0.00175367
Iteration 17/1000 | Loss: 0.00195941
Iteration 18/1000 | Loss: 0.00353677
Iteration 19/1000 | Loss: 0.00280794
Iteration 20/1000 | Loss: 0.00163165
Iteration 21/1000 | Loss: 0.00128594
Iteration 22/1000 | Loss: 0.00134696
Iteration 23/1000 | Loss: 0.00098061
Iteration 24/1000 | Loss: 0.00015931
Iteration 25/1000 | Loss: 0.00012891
Iteration 26/1000 | Loss: 0.00014406
Iteration 27/1000 | Loss: 0.00092710
Iteration 28/1000 | Loss: 0.00034660
Iteration 29/1000 | Loss: 0.00060165
Iteration 30/1000 | Loss: 0.00132339
Iteration 31/1000 | Loss: 0.00054743
Iteration 32/1000 | Loss: 0.00115500
Iteration 33/1000 | Loss: 0.00040849
Iteration 34/1000 | Loss: 0.00036700
Iteration 35/1000 | Loss: 0.00030513
Iteration 36/1000 | Loss: 0.00110779
Iteration 37/1000 | Loss: 0.00033677
Iteration 38/1000 | Loss: 0.00010838
Iteration 39/1000 | Loss: 0.00047120
Iteration 40/1000 | Loss: 0.00021306
Iteration 41/1000 | Loss: 0.00031556
Iteration 42/1000 | Loss: 0.00235950
Iteration 43/1000 | Loss: 0.00098000
Iteration 44/1000 | Loss: 0.00128416
Iteration 45/1000 | Loss: 0.00122436
Iteration 46/1000 | Loss: 0.00086914
Iteration 47/1000 | Loss: 0.00067974
Iteration 48/1000 | Loss: 0.00073980
Iteration 49/1000 | Loss: 0.00064640
Iteration 50/1000 | Loss: 0.00034256
Iteration 51/1000 | Loss: 0.00078547
Iteration 52/1000 | Loss: 0.00074390
Iteration 53/1000 | Loss: 0.00026393
Iteration 54/1000 | Loss: 0.00008140
Iteration 55/1000 | Loss: 0.00044127
Iteration 56/1000 | Loss: 0.00048732
Iteration 57/1000 | Loss: 0.00057781
Iteration 58/1000 | Loss: 0.00070142
Iteration 59/1000 | Loss: 0.00071446
Iteration 60/1000 | Loss: 0.00057096
Iteration 61/1000 | Loss: 0.00062495
Iteration 62/1000 | Loss: 0.00036584
Iteration 63/1000 | Loss: 0.00030520
Iteration 64/1000 | Loss: 0.00007568
Iteration 65/1000 | Loss: 0.00006447
Iteration 66/1000 | Loss: 0.00005866
Iteration 67/1000 | Loss: 0.00036294
Iteration 68/1000 | Loss: 0.00040628
Iteration 69/1000 | Loss: 0.00041614
Iteration 70/1000 | Loss: 0.00020043
Iteration 71/1000 | Loss: 0.00027590
Iteration 72/1000 | Loss: 0.00005010
Iteration 73/1000 | Loss: 0.00005118
Iteration 74/1000 | Loss: 0.00071300
Iteration 75/1000 | Loss: 0.00028333
Iteration 76/1000 | Loss: 0.00057737
Iteration 77/1000 | Loss: 0.00102615
Iteration 78/1000 | Loss: 0.00057057
Iteration 79/1000 | Loss: 0.00089723
Iteration 80/1000 | Loss: 0.00028381
Iteration 81/1000 | Loss: 0.00044399
Iteration 82/1000 | Loss: 0.00034610
Iteration 83/1000 | Loss: 0.00005140
Iteration 84/1000 | Loss: 0.00004985
Iteration 85/1000 | Loss: 0.00004372
Iteration 86/1000 | Loss: 0.00004598
Iteration 87/1000 | Loss: 0.00041671
Iteration 88/1000 | Loss: 0.00035931
Iteration 89/1000 | Loss: 0.00026059
Iteration 90/1000 | Loss: 0.00024600
Iteration 91/1000 | Loss: 0.00029270
Iteration 92/1000 | Loss: 0.00091425
Iteration 93/1000 | Loss: 0.00015990
Iteration 94/1000 | Loss: 0.00004539
Iteration 95/1000 | Loss: 0.00004073
Iteration 96/1000 | Loss: 0.00004002
Iteration 97/1000 | Loss: 0.00004096
Iteration 98/1000 | Loss: 0.00033734
Iteration 99/1000 | Loss: 0.00045277
Iteration 100/1000 | Loss: 0.00038067
Iteration 101/1000 | Loss: 0.00041449
Iteration 102/1000 | Loss: 0.00011847
Iteration 103/1000 | Loss: 0.00056675
Iteration 104/1000 | Loss: 0.00051433
Iteration 105/1000 | Loss: 0.00012034
Iteration 106/1000 | Loss: 0.00004017
Iteration 107/1000 | Loss: 0.00015842
Iteration 108/1000 | Loss: 0.00070091
Iteration 109/1000 | Loss: 0.00047538
Iteration 110/1000 | Loss: 0.00018587
Iteration 111/1000 | Loss: 0.00021985
Iteration 112/1000 | Loss: 0.00020262
Iteration 113/1000 | Loss: 0.00060459
Iteration 114/1000 | Loss: 0.00036442
Iteration 115/1000 | Loss: 0.00005717
Iteration 116/1000 | Loss: 0.00007992
Iteration 117/1000 | Loss: 0.00003651
Iteration 118/1000 | Loss: 0.00003505
Iteration 119/1000 | Loss: 0.00003356
Iteration 120/1000 | Loss: 0.00003215
Iteration 121/1000 | Loss: 0.00068724
Iteration 122/1000 | Loss: 0.00044518
Iteration 123/1000 | Loss: 0.00035754
Iteration 124/1000 | Loss: 0.00031921
Iteration 125/1000 | Loss: 0.00044533
Iteration 126/1000 | Loss: 0.00039979
Iteration 127/1000 | Loss: 0.00041364
Iteration 128/1000 | Loss: 0.00028827
Iteration 129/1000 | Loss: 0.00061036
Iteration 130/1000 | Loss: 0.00027224
Iteration 131/1000 | Loss: 0.00054975
Iteration 132/1000 | Loss: 0.00036810
Iteration 133/1000 | Loss: 0.00021525
Iteration 134/1000 | Loss: 0.00005219
Iteration 135/1000 | Loss: 0.00004276
Iteration 136/1000 | Loss: 0.00004905
Iteration 137/1000 | Loss: 0.00003638
Iteration 138/1000 | Loss: 0.00030284
Iteration 139/1000 | Loss: 0.00025520
Iteration 140/1000 | Loss: 0.00022684
Iteration 141/1000 | Loss: 0.00022631
Iteration 142/1000 | Loss: 0.00020178
Iteration 143/1000 | Loss: 0.00014311
Iteration 144/1000 | Loss: 0.00009455
Iteration 145/1000 | Loss: 0.00003107
Iteration 146/1000 | Loss: 0.00003096
Iteration 147/1000 | Loss: 0.00011583
Iteration 148/1000 | Loss: 0.00005563
Iteration 149/1000 | Loss: 0.00011697
Iteration 150/1000 | Loss: 0.00005033
Iteration 151/1000 | Loss: 0.00007910
Iteration 152/1000 | Loss: 0.00004397
Iteration 153/1000 | Loss: 0.00007103
Iteration 154/1000 | Loss: 0.00005271
Iteration 155/1000 | Loss: 0.00006573
Iteration 156/1000 | Loss: 0.00003118
Iteration 157/1000 | Loss: 0.00003593
Iteration 158/1000 | Loss: 0.00005796
Iteration 159/1000 | Loss: 0.00003345
Iteration 160/1000 | Loss: 0.00004560
Iteration 161/1000 | Loss: 0.00003037
Iteration 162/1000 | Loss: 0.00004239
Iteration 163/1000 | Loss: 0.00003083
Iteration 164/1000 | Loss: 0.00008326
Iteration 165/1000 | Loss: 0.00004355
Iteration 166/1000 | Loss: 0.00002893
Iteration 167/1000 | Loss: 0.00006820
Iteration 168/1000 | Loss: 0.00002876
Iteration 169/1000 | Loss: 0.00002746
Iteration 170/1000 | Loss: 0.00002632
Iteration 171/1000 | Loss: 0.00002596
Iteration 172/1000 | Loss: 0.00002545
Iteration 173/1000 | Loss: 0.00003072
Iteration 174/1000 | Loss: 0.00002497
Iteration 175/1000 | Loss: 0.00002493
Iteration 176/1000 | Loss: 0.00002489
Iteration 177/1000 | Loss: 0.00002475
Iteration 178/1000 | Loss: 0.00002936
Iteration 179/1000 | Loss: 0.00002450
Iteration 180/1000 | Loss: 0.00002448
Iteration 181/1000 | Loss: 0.00002448
Iteration 182/1000 | Loss: 0.00002448
Iteration 183/1000 | Loss: 0.00002448
Iteration 184/1000 | Loss: 0.00002448
Iteration 185/1000 | Loss: 0.00002448
Iteration 186/1000 | Loss: 0.00002552
Iteration 187/1000 | Loss: 0.00002447
Iteration 188/1000 | Loss: 0.00002446
Iteration 189/1000 | Loss: 0.00002445
Iteration 190/1000 | Loss: 0.00002445
Iteration 191/1000 | Loss: 0.00002445
Iteration 192/1000 | Loss: 0.00002444
Iteration 193/1000 | Loss: 0.00002444
Iteration 194/1000 | Loss: 0.00002443
Iteration 195/1000 | Loss: 0.00002443
Iteration 196/1000 | Loss: 0.00002442
Iteration 197/1000 | Loss: 0.00002442
Iteration 198/1000 | Loss: 0.00002441
Iteration 199/1000 | Loss: 0.00002441
Iteration 200/1000 | Loss: 0.00002440
Iteration 201/1000 | Loss: 0.00002440
Iteration 202/1000 | Loss: 0.00002485
Iteration 203/1000 | Loss: 0.00002435
Iteration 204/1000 | Loss: 0.00002433
Iteration 205/1000 | Loss: 0.00002433
Iteration 206/1000 | Loss: 0.00002433
Iteration 207/1000 | Loss: 0.00002433
Iteration 208/1000 | Loss: 0.00002433
Iteration 209/1000 | Loss: 0.00002433
Iteration 210/1000 | Loss: 0.00002433
Iteration 211/1000 | Loss: 0.00002432
Iteration 212/1000 | Loss: 0.00002432
Iteration 213/1000 | Loss: 0.00002432
Iteration 214/1000 | Loss: 0.00002432
Iteration 215/1000 | Loss: 0.00002432
Iteration 216/1000 | Loss: 0.00002432
Iteration 217/1000 | Loss: 0.00002432
Iteration 218/1000 | Loss: 0.00002432
Iteration 219/1000 | Loss: 0.00002432
Iteration 220/1000 | Loss: 0.00002432
Iteration 221/1000 | Loss: 0.00002432
Iteration 222/1000 | Loss: 0.00002431
Iteration 223/1000 | Loss: 0.00002431
Iteration 224/1000 | Loss: 0.00002431
Iteration 225/1000 | Loss: 0.00002431
Iteration 226/1000 | Loss: 0.00002431
Iteration 227/1000 | Loss: 0.00002431
Iteration 228/1000 | Loss: 0.00002431
Iteration 229/1000 | Loss: 0.00002431
Iteration 230/1000 | Loss: 0.00002431
Iteration 231/1000 | Loss: 0.00002430
Iteration 232/1000 | Loss: 0.00002430
Iteration 233/1000 | Loss: 0.00002430
Iteration 234/1000 | Loss: 0.00002430
Iteration 235/1000 | Loss: 0.00002427
Iteration 236/1000 | Loss: 0.00002424
Iteration 237/1000 | Loss: 0.00002423
Iteration 238/1000 | Loss: 0.00002422
Iteration 239/1000 | Loss: 0.00002422
Iteration 240/1000 | Loss: 0.00002422
Iteration 241/1000 | Loss: 0.00002422
Iteration 242/1000 | Loss: 0.00002421
Iteration 243/1000 | Loss: 0.00002421
Iteration 244/1000 | Loss: 0.00002421
Iteration 245/1000 | Loss: 0.00002421
Iteration 246/1000 | Loss: 0.00002421
Iteration 247/1000 | Loss: 0.00002421
Iteration 248/1000 | Loss: 0.00002420
Iteration 249/1000 | Loss: 0.00002420
Iteration 250/1000 | Loss: 0.00002420
Iteration 251/1000 | Loss: 0.00002420
Iteration 252/1000 | Loss: 0.00002419
Iteration 253/1000 | Loss: 0.00002419
Iteration 254/1000 | Loss: 0.00002419
Iteration 255/1000 | Loss: 0.00002816
Iteration 256/1000 | Loss: 0.00002816
Iteration 257/1000 | Loss: 0.00005004
Iteration 258/1000 | Loss: 0.00002718
Iteration 259/1000 | Loss: 0.00003126
Iteration 260/1000 | Loss: 0.00002442
Iteration 261/1000 | Loss: 0.00002415
Iteration 262/1000 | Loss: 0.00002415
Iteration 263/1000 | Loss: 0.00002415
Iteration 264/1000 | Loss: 0.00002415
Iteration 265/1000 | Loss: 0.00002415
Iteration 266/1000 | Loss: 0.00002415
Iteration 267/1000 | Loss: 0.00002415
Iteration 268/1000 | Loss: 0.00002415
Iteration 269/1000 | Loss: 0.00002415
Iteration 270/1000 | Loss: 0.00002415
Iteration 271/1000 | Loss: 0.00002415
Iteration 272/1000 | Loss: 0.00002415
Iteration 273/1000 | Loss: 0.00002415
Iteration 274/1000 | Loss: 0.00002415
Iteration 275/1000 | Loss: 0.00002415
Iteration 276/1000 | Loss: 0.00002415
Iteration 277/1000 | Loss: 0.00002415
Iteration 278/1000 | Loss: 0.00002415
Iteration 279/1000 | Loss: 0.00002415
Iteration 280/1000 | Loss: 0.00002415
Iteration 281/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [2.414522532490082e-05, 2.414522532490082e-05, 2.414522532490082e-05, 2.414522532490082e-05, 2.414522532490082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.414522532490082e-05

Optimization complete. Final v2v error: 3.737731456756592 mm

Highest mean error: 12.275431632995605 mm for frame 38

Lowest mean error: 2.968170642852783 mm for frame 125

Saving results

Total time: 347.3243532180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439436
Iteration 2/25 | Loss: 0.00130931
Iteration 3/25 | Loss: 0.00123237
Iteration 4/25 | Loss: 0.00122187
Iteration 5/25 | Loss: 0.00121918
Iteration 6/25 | Loss: 0.00121918
Iteration 7/25 | Loss: 0.00121918
Iteration 8/25 | Loss: 0.00121918
Iteration 9/25 | Loss: 0.00121918
Iteration 10/25 | Loss: 0.00121918
Iteration 11/25 | Loss: 0.00121918
Iteration 12/25 | Loss: 0.00121918
Iteration 13/25 | Loss: 0.00121918
Iteration 14/25 | Loss: 0.00121918
Iteration 15/25 | Loss: 0.00121918
Iteration 16/25 | Loss: 0.00121918
Iteration 17/25 | Loss: 0.00121918
Iteration 18/25 | Loss: 0.00121918
Iteration 19/25 | Loss: 0.00121918
Iteration 20/25 | Loss: 0.00121918
Iteration 21/25 | Loss: 0.00121918
Iteration 22/25 | Loss: 0.00121918
Iteration 23/25 | Loss: 0.00121918
Iteration 24/25 | Loss: 0.00121918
Iteration 25/25 | Loss: 0.00121918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.46125031
Iteration 2/25 | Loss: 0.00066363
Iteration 3/25 | Loss: 0.00066361
Iteration 4/25 | Loss: 0.00066361
Iteration 5/25 | Loss: 0.00066361
Iteration 6/25 | Loss: 0.00066361
Iteration 7/25 | Loss: 0.00066361
Iteration 8/25 | Loss: 0.00066361
Iteration 9/25 | Loss: 0.00066361
Iteration 10/25 | Loss: 0.00066361
Iteration 11/25 | Loss: 0.00066361
Iteration 12/25 | Loss: 0.00066361
Iteration 13/25 | Loss: 0.00066361
Iteration 14/25 | Loss: 0.00066361
Iteration 15/25 | Loss: 0.00066361
Iteration 16/25 | Loss: 0.00066361
Iteration 17/25 | Loss: 0.00066361
Iteration 18/25 | Loss: 0.00066361
Iteration 19/25 | Loss: 0.00066361
Iteration 20/25 | Loss: 0.00066361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006636107573285699, 0.0006636107573285699, 0.0006636107573285699, 0.0006636107573285699, 0.0006636107573285699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006636107573285699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066361
Iteration 2/1000 | Loss: 0.00002760
Iteration 3/1000 | Loss: 0.00001913
Iteration 4/1000 | Loss: 0.00001770
Iteration 5/1000 | Loss: 0.00001678
Iteration 6/1000 | Loss: 0.00001614
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001536
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001469
Iteration 12/1000 | Loss: 0.00001467
Iteration 13/1000 | Loss: 0.00001463
Iteration 14/1000 | Loss: 0.00001461
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001458
Iteration 17/1000 | Loss: 0.00001457
Iteration 18/1000 | Loss: 0.00001454
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001449
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001446
Iteration 27/1000 | Loss: 0.00001445
Iteration 28/1000 | Loss: 0.00001445
Iteration 29/1000 | Loss: 0.00001441
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001432
Iteration 34/1000 | Loss: 0.00001429
Iteration 35/1000 | Loss: 0.00001423
Iteration 36/1000 | Loss: 0.00001423
Iteration 37/1000 | Loss: 0.00001423
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001416
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001415
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001409
Iteration 47/1000 | Loss: 0.00001409
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001405
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001402
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001398
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001397
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001393
Iteration 74/1000 | Loss: 0.00001393
Iteration 75/1000 | Loss: 0.00001392
Iteration 76/1000 | Loss: 0.00001392
Iteration 77/1000 | Loss: 0.00001392
Iteration 78/1000 | Loss: 0.00001392
Iteration 79/1000 | Loss: 0.00001391
Iteration 80/1000 | Loss: 0.00001391
Iteration 81/1000 | Loss: 0.00001391
Iteration 82/1000 | Loss: 0.00001391
Iteration 83/1000 | Loss: 0.00001391
Iteration 84/1000 | Loss: 0.00001391
Iteration 85/1000 | Loss: 0.00001391
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001390
Iteration 89/1000 | Loss: 0.00001390
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001389
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001389
Iteration 103/1000 | Loss: 0.00001389
Iteration 104/1000 | Loss: 0.00001389
Iteration 105/1000 | Loss: 0.00001389
Iteration 106/1000 | Loss: 0.00001389
Iteration 107/1000 | Loss: 0.00001389
Iteration 108/1000 | Loss: 0.00001389
Iteration 109/1000 | Loss: 0.00001389
Iteration 110/1000 | Loss: 0.00001389
Iteration 111/1000 | Loss: 0.00001389
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.3889558431401383e-05, 1.3889558431401383e-05, 1.3889558431401383e-05, 1.3889558431401383e-05, 1.3889558431401383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3889558431401383e-05

Optimization complete. Final v2v error: 3.1295411586761475 mm

Highest mean error: 3.5561022758483887 mm for frame 107

Lowest mean error: 2.9378738403320312 mm for frame 53

Saving results

Total time: 35.46931481361389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366550
Iteration 2/25 | Loss: 0.00135067
Iteration 3/25 | Loss: 0.00122853
Iteration 4/25 | Loss: 0.00120159
Iteration 5/25 | Loss: 0.00119254
Iteration 6/25 | Loss: 0.00119032
Iteration 7/25 | Loss: 0.00118979
Iteration 8/25 | Loss: 0.00118979
Iteration 9/25 | Loss: 0.00118979
Iteration 10/25 | Loss: 0.00118979
Iteration 11/25 | Loss: 0.00118979
Iteration 12/25 | Loss: 0.00118979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011897868243977427, 0.0011897868243977427, 0.0011897868243977427, 0.0011897868243977427, 0.0011897868243977427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011897868243977427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46078432
Iteration 2/25 | Loss: 0.00070743
Iteration 3/25 | Loss: 0.00070743
Iteration 4/25 | Loss: 0.00070743
Iteration 5/25 | Loss: 0.00070743
Iteration 6/25 | Loss: 0.00070743
Iteration 7/25 | Loss: 0.00070743
Iteration 8/25 | Loss: 0.00070743
Iteration 9/25 | Loss: 0.00070743
Iteration 10/25 | Loss: 0.00070743
Iteration 11/25 | Loss: 0.00070743
Iteration 12/25 | Loss: 0.00070743
Iteration 13/25 | Loss: 0.00070743
Iteration 14/25 | Loss: 0.00070743
Iteration 15/25 | Loss: 0.00070743
Iteration 16/25 | Loss: 0.00070743
Iteration 17/25 | Loss: 0.00070743
Iteration 18/25 | Loss: 0.00070743
Iteration 19/25 | Loss: 0.00070743
Iteration 20/25 | Loss: 0.00070743
Iteration 21/25 | Loss: 0.00070743
Iteration 22/25 | Loss: 0.00070743
Iteration 23/25 | Loss: 0.00070743
Iteration 24/25 | Loss: 0.00070743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007074304157868028, 0.0007074304157868028, 0.0007074304157868028, 0.0007074304157868028, 0.0007074304157868028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007074304157868028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070743
Iteration 2/1000 | Loss: 0.00005017
Iteration 3/1000 | Loss: 0.00003291
Iteration 4/1000 | Loss: 0.00002512
Iteration 5/1000 | Loss: 0.00002297
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001903
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001813
Iteration 13/1000 | Loss: 0.00001795
Iteration 14/1000 | Loss: 0.00001792
Iteration 15/1000 | Loss: 0.00001788
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001785
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00001771
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001753
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001749
Iteration 29/1000 | Loss: 0.00001748
Iteration 30/1000 | Loss: 0.00001748
Iteration 31/1000 | Loss: 0.00001748
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001748
Iteration 35/1000 | Loss: 0.00001748
Iteration 36/1000 | Loss: 0.00001747
Iteration 37/1000 | Loss: 0.00001747
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001747
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001745
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001744
Iteration 52/1000 | Loss: 0.00001744
Iteration 53/1000 | Loss: 0.00001744
Iteration 54/1000 | Loss: 0.00001743
Iteration 55/1000 | Loss: 0.00001743
Iteration 56/1000 | Loss: 0.00001743
Iteration 57/1000 | Loss: 0.00001743
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001743
Iteration 60/1000 | Loss: 0.00001742
Iteration 61/1000 | Loss: 0.00001742
Iteration 62/1000 | Loss: 0.00001742
Iteration 63/1000 | Loss: 0.00001742
Iteration 64/1000 | Loss: 0.00001742
Iteration 65/1000 | Loss: 0.00001741
Iteration 66/1000 | Loss: 0.00001741
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00001741
Iteration 70/1000 | Loss: 0.00001741
Iteration 71/1000 | Loss: 0.00001740
Iteration 72/1000 | Loss: 0.00001740
Iteration 73/1000 | Loss: 0.00001740
Iteration 74/1000 | Loss: 0.00001740
Iteration 75/1000 | Loss: 0.00001740
Iteration 76/1000 | Loss: 0.00001739
Iteration 77/1000 | Loss: 0.00001739
Iteration 78/1000 | Loss: 0.00001739
Iteration 79/1000 | Loss: 0.00001739
Iteration 80/1000 | Loss: 0.00001739
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001738
Iteration 85/1000 | Loss: 0.00001738
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001737
Iteration 91/1000 | Loss: 0.00001737
Iteration 92/1000 | Loss: 0.00001737
Iteration 93/1000 | Loss: 0.00001737
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001736
Iteration 96/1000 | Loss: 0.00001736
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001735
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001735
Iteration 106/1000 | Loss: 0.00001735
Iteration 107/1000 | Loss: 0.00001735
Iteration 108/1000 | Loss: 0.00001734
Iteration 109/1000 | Loss: 0.00001734
Iteration 110/1000 | Loss: 0.00001734
Iteration 111/1000 | Loss: 0.00001734
Iteration 112/1000 | Loss: 0.00001734
Iteration 113/1000 | Loss: 0.00001734
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001733
Iteration 116/1000 | Loss: 0.00001733
Iteration 117/1000 | Loss: 0.00001733
Iteration 118/1000 | Loss: 0.00001733
Iteration 119/1000 | Loss: 0.00001733
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001731
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001730
Iteration 130/1000 | Loss: 0.00001730
Iteration 131/1000 | Loss: 0.00001730
Iteration 132/1000 | Loss: 0.00001730
Iteration 133/1000 | Loss: 0.00001730
Iteration 134/1000 | Loss: 0.00001730
Iteration 135/1000 | Loss: 0.00001730
Iteration 136/1000 | Loss: 0.00001729
Iteration 137/1000 | Loss: 0.00001729
Iteration 138/1000 | Loss: 0.00001729
Iteration 139/1000 | Loss: 0.00001729
Iteration 140/1000 | Loss: 0.00001729
Iteration 141/1000 | Loss: 0.00001729
Iteration 142/1000 | Loss: 0.00001729
Iteration 143/1000 | Loss: 0.00001728
Iteration 144/1000 | Loss: 0.00001728
Iteration 145/1000 | Loss: 0.00001728
Iteration 146/1000 | Loss: 0.00001728
Iteration 147/1000 | Loss: 0.00001728
Iteration 148/1000 | Loss: 0.00001728
Iteration 149/1000 | Loss: 0.00001728
Iteration 150/1000 | Loss: 0.00001728
Iteration 151/1000 | Loss: 0.00001728
Iteration 152/1000 | Loss: 0.00001728
Iteration 153/1000 | Loss: 0.00001728
Iteration 154/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7278171071666293e-05, 1.7278171071666293e-05, 1.7278171071666293e-05, 1.7278171071666293e-05, 1.7278171071666293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7278171071666293e-05

Optimization complete. Final v2v error: 3.4596967697143555 mm

Highest mean error: 5.032705307006836 mm for frame 151

Lowest mean error: 2.553701639175415 mm for frame 165

Saving results

Total time: 42.43911123275757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874226
Iteration 2/25 | Loss: 0.00141580
Iteration 3/25 | Loss: 0.00124628
Iteration 4/25 | Loss: 0.00123127
Iteration 5/25 | Loss: 0.00122630
Iteration 6/25 | Loss: 0.00122598
Iteration 7/25 | Loss: 0.00122598
Iteration 8/25 | Loss: 0.00122598
Iteration 9/25 | Loss: 0.00122598
Iteration 10/25 | Loss: 0.00122598
Iteration 11/25 | Loss: 0.00122598
Iteration 12/25 | Loss: 0.00122598
Iteration 13/25 | Loss: 0.00122598
Iteration 14/25 | Loss: 0.00122598
Iteration 15/25 | Loss: 0.00122598
Iteration 16/25 | Loss: 0.00122598
Iteration 17/25 | Loss: 0.00122598
Iteration 18/25 | Loss: 0.00122598
Iteration 19/25 | Loss: 0.00122598
Iteration 20/25 | Loss: 0.00122598
Iteration 21/25 | Loss: 0.00122598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001225981512106955, 0.001225981512106955, 0.001225981512106955, 0.001225981512106955, 0.001225981512106955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225981512106955

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47312737
Iteration 2/25 | Loss: 0.00083516
Iteration 3/25 | Loss: 0.00083516
Iteration 4/25 | Loss: 0.00083516
Iteration 5/25 | Loss: 0.00083516
Iteration 6/25 | Loss: 0.00083516
Iteration 7/25 | Loss: 0.00083516
Iteration 8/25 | Loss: 0.00083516
Iteration 9/25 | Loss: 0.00083516
Iteration 10/25 | Loss: 0.00083516
Iteration 11/25 | Loss: 0.00083516
Iteration 12/25 | Loss: 0.00083516
Iteration 13/25 | Loss: 0.00083516
Iteration 14/25 | Loss: 0.00083516
Iteration 15/25 | Loss: 0.00083516
Iteration 16/25 | Loss: 0.00083516
Iteration 17/25 | Loss: 0.00083516
Iteration 18/25 | Loss: 0.00083516
Iteration 19/25 | Loss: 0.00083516
Iteration 20/25 | Loss: 0.00083516
Iteration 21/25 | Loss: 0.00083516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000835159735288471, 0.000835159735288471, 0.000835159735288471, 0.000835159735288471, 0.000835159735288471]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000835159735288471

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083516
Iteration 2/1000 | Loss: 0.00003033
Iteration 3/1000 | Loss: 0.00002235
Iteration 4/1000 | Loss: 0.00002062
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001884
Iteration 8/1000 | Loss: 0.00001851
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001788
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001783
Iteration 17/1000 | Loss: 0.00001783
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001780
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001776
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001771
Iteration 28/1000 | Loss: 0.00001769
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001767
Iteration 32/1000 | Loss: 0.00001767
Iteration 33/1000 | Loss: 0.00001766
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001764
Iteration 40/1000 | Loss: 0.00001764
Iteration 41/1000 | Loss: 0.00001764
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001762
Iteration 51/1000 | Loss: 0.00001761
Iteration 52/1000 | Loss: 0.00001760
Iteration 53/1000 | Loss: 0.00001759
Iteration 54/1000 | Loss: 0.00001759
Iteration 55/1000 | Loss: 0.00001759
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001758
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001754
Iteration 66/1000 | Loss: 0.00001754
Iteration 67/1000 | Loss: 0.00001754
Iteration 68/1000 | Loss: 0.00001754
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001750
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001748
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001747
Iteration 84/1000 | Loss: 0.00001747
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001745
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001744
Iteration 89/1000 | Loss: 0.00001744
Iteration 90/1000 | Loss: 0.00001744
Iteration 91/1000 | Loss: 0.00001743
Iteration 92/1000 | Loss: 0.00001743
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001736
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001735
Iteration 104/1000 | Loss: 0.00001735
Iteration 105/1000 | Loss: 0.00001734
Iteration 106/1000 | Loss: 0.00001734
Iteration 107/1000 | Loss: 0.00001734
Iteration 108/1000 | Loss: 0.00001733
Iteration 109/1000 | Loss: 0.00001733
Iteration 110/1000 | Loss: 0.00001733
Iteration 111/1000 | Loss: 0.00001733
Iteration 112/1000 | Loss: 0.00001733
Iteration 113/1000 | Loss: 0.00001733
Iteration 114/1000 | Loss: 0.00001733
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001731
Iteration 125/1000 | Loss: 0.00001731
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001731
Iteration 130/1000 | Loss: 0.00001731
Iteration 131/1000 | Loss: 0.00001731
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7312009731540456e-05, 1.7312009731540456e-05, 1.7312009731540456e-05, 1.7312009731540456e-05, 1.7312009731540456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7312009731540456e-05

Optimization complete. Final v2v error: 3.395608425140381 mm

Highest mean error: 4.015740871429443 mm for frame 184

Lowest mean error: 2.8003337383270264 mm for frame 212

Saving results

Total time: 40.875582456588745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793519
Iteration 2/25 | Loss: 0.00141885
Iteration 3/25 | Loss: 0.00123853
Iteration 4/25 | Loss: 0.00121416
Iteration 5/25 | Loss: 0.00120880
Iteration 6/25 | Loss: 0.00120797
Iteration 7/25 | Loss: 0.00120797
Iteration 8/25 | Loss: 0.00120797
Iteration 9/25 | Loss: 0.00120797
Iteration 10/25 | Loss: 0.00120797
Iteration 11/25 | Loss: 0.00120797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012079677544534206, 0.0012079677544534206, 0.0012079677544534206, 0.0012079677544534206, 0.0012079677544534206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012079677544534206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36140203
Iteration 2/25 | Loss: 0.00069862
Iteration 3/25 | Loss: 0.00069859
Iteration 4/25 | Loss: 0.00069859
Iteration 5/25 | Loss: 0.00069859
Iteration 6/25 | Loss: 0.00069859
Iteration 7/25 | Loss: 0.00069859
Iteration 8/25 | Loss: 0.00069859
Iteration 9/25 | Loss: 0.00069859
Iteration 10/25 | Loss: 0.00069859
Iteration 11/25 | Loss: 0.00069859
Iteration 12/25 | Loss: 0.00069859
Iteration 13/25 | Loss: 0.00069859
Iteration 14/25 | Loss: 0.00069859
Iteration 15/25 | Loss: 0.00069859
Iteration 16/25 | Loss: 0.00069859
Iteration 17/25 | Loss: 0.00069859
Iteration 18/25 | Loss: 0.00069859
Iteration 19/25 | Loss: 0.00069859
Iteration 20/25 | Loss: 0.00069859
Iteration 21/25 | Loss: 0.00069859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000698585354257375, 0.000698585354257375, 0.000698585354257375, 0.000698585354257375, 0.000698585354257375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000698585354257375

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069859
Iteration 2/1000 | Loss: 0.00004297
Iteration 3/1000 | Loss: 0.00003124
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002399
Iteration 6/1000 | Loss: 0.00002283
Iteration 7/1000 | Loss: 0.00002205
Iteration 8/1000 | Loss: 0.00002154
Iteration 9/1000 | Loss: 0.00002115
Iteration 10/1000 | Loss: 0.00002084
Iteration 11/1000 | Loss: 0.00002045
Iteration 12/1000 | Loss: 0.00002020
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002012
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001990
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001980
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001974
Iteration 23/1000 | Loss: 0.00001971
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001967
Iteration 29/1000 | Loss: 0.00001967
Iteration 30/1000 | Loss: 0.00001967
Iteration 31/1000 | Loss: 0.00001967
Iteration 32/1000 | Loss: 0.00001964
Iteration 33/1000 | Loss: 0.00001964
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001960
Iteration 38/1000 | Loss: 0.00001957
Iteration 39/1000 | Loss: 0.00001956
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001953
Iteration 45/1000 | Loss: 0.00001953
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001952
Iteration 48/1000 | Loss: 0.00001952
Iteration 49/1000 | Loss: 0.00001951
Iteration 50/1000 | Loss: 0.00001951
Iteration 51/1000 | Loss: 0.00001951
Iteration 52/1000 | Loss: 0.00001950
Iteration 53/1000 | Loss: 0.00001950
Iteration 54/1000 | Loss: 0.00001950
Iteration 55/1000 | Loss: 0.00001950
Iteration 56/1000 | Loss: 0.00001949
Iteration 57/1000 | Loss: 0.00001949
Iteration 58/1000 | Loss: 0.00001949
Iteration 59/1000 | Loss: 0.00001949
Iteration 60/1000 | Loss: 0.00001949
Iteration 61/1000 | Loss: 0.00001948
Iteration 62/1000 | Loss: 0.00001948
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001946
Iteration 68/1000 | Loss: 0.00001946
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001946
Iteration 71/1000 | Loss: 0.00001945
Iteration 72/1000 | Loss: 0.00001945
Iteration 73/1000 | Loss: 0.00001945
Iteration 74/1000 | Loss: 0.00001945
Iteration 75/1000 | Loss: 0.00001945
Iteration 76/1000 | Loss: 0.00001945
Iteration 77/1000 | Loss: 0.00001945
Iteration 78/1000 | Loss: 0.00001945
Iteration 79/1000 | Loss: 0.00001944
Iteration 80/1000 | Loss: 0.00001944
Iteration 81/1000 | Loss: 0.00001944
Iteration 82/1000 | Loss: 0.00001943
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Iteration 85/1000 | Loss: 0.00001942
Iteration 86/1000 | Loss: 0.00001942
Iteration 87/1000 | Loss: 0.00001942
Iteration 88/1000 | Loss: 0.00001942
Iteration 89/1000 | Loss: 0.00001941
Iteration 90/1000 | Loss: 0.00001941
Iteration 91/1000 | Loss: 0.00001941
Iteration 92/1000 | Loss: 0.00001941
Iteration 93/1000 | Loss: 0.00001940
Iteration 94/1000 | Loss: 0.00001940
Iteration 95/1000 | Loss: 0.00001940
Iteration 96/1000 | Loss: 0.00001940
Iteration 97/1000 | Loss: 0.00001940
Iteration 98/1000 | Loss: 0.00001940
Iteration 99/1000 | Loss: 0.00001940
Iteration 100/1000 | Loss: 0.00001940
Iteration 101/1000 | Loss: 0.00001940
Iteration 102/1000 | Loss: 0.00001940
Iteration 103/1000 | Loss: 0.00001940
Iteration 104/1000 | Loss: 0.00001940
Iteration 105/1000 | Loss: 0.00001940
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001940
Iteration 110/1000 | Loss: 0.00001940
Iteration 111/1000 | Loss: 0.00001940
Iteration 112/1000 | Loss: 0.00001940
Iteration 113/1000 | Loss: 0.00001940
Iteration 114/1000 | Loss: 0.00001940
Iteration 115/1000 | Loss: 0.00001940
Iteration 116/1000 | Loss: 0.00001940
Iteration 117/1000 | Loss: 0.00001940
Iteration 118/1000 | Loss: 0.00001940
Iteration 119/1000 | Loss: 0.00001940
Iteration 120/1000 | Loss: 0.00001940
Iteration 121/1000 | Loss: 0.00001940
Iteration 122/1000 | Loss: 0.00001940
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001940
Iteration 127/1000 | Loss: 0.00001940
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.939624962687958e-05, 1.939624962687958e-05, 1.939624962687958e-05, 1.939624962687958e-05, 1.939624962687958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.939624962687958e-05

Optimization complete. Final v2v error: 3.636613368988037 mm

Highest mean error: 4.956962585449219 mm for frame 66

Lowest mean error: 2.802116632461548 mm for frame 181

Saving results

Total time: 42.32436752319336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610655
Iteration 2/25 | Loss: 0.00171615
Iteration 3/25 | Loss: 0.00152524
Iteration 4/25 | Loss: 0.00147526
Iteration 5/25 | Loss: 0.00150576
Iteration 6/25 | Loss: 0.00144271
Iteration 7/25 | Loss: 0.00139612
Iteration 8/25 | Loss: 0.00138392
Iteration 9/25 | Loss: 0.00137706
Iteration 10/25 | Loss: 0.00137559
Iteration 11/25 | Loss: 0.00137461
Iteration 12/25 | Loss: 0.00136932
Iteration 13/25 | Loss: 0.00136734
Iteration 14/25 | Loss: 0.00136656
Iteration 15/25 | Loss: 0.00136615
Iteration 16/25 | Loss: 0.00136602
Iteration 17/25 | Loss: 0.00136600
Iteration 18/25 | Loss: 0.00136600
Iteration 19/25 | Loss: 0.00136600
Iteration 20/25 | Loss: 0.00136600
Iteration 21/25 | Loss: 0.00136600
Iteration 22/25 | Loss: 0.00136600
Iteration 23/25 | Loss: 0.00136600
Iteration 24/25 | Loss: 0.00136600
Iteration 25/25 | Loss: 0.00136600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32752752
Iteration 2/25 | Loss: 0.00191159
Iteration 3/25 | Loss: 0.00191151
Iteration 4/25 | Loss: 0.00191151
Iteration 5/25 | Loss: 0.00191151
Iteration 6/25 | Loss: 0.00191151
Iteration 7/25 | Loss: 0.00191151
Iteration 8/25 | Loss: 0.00191151
Iteration 9/25 | Loss: 0.00191151
Iteration 10/25 | Loss: 0.00191150
Iteration 11/25 | Loss: 0.00191150
Iteration 12/25 | Loss: 0.00191150
Iteration 13/25 | Loss: 0.00191150
Iteration 14/25 | Loss: 0.00191150
Iteration 15/25 | Loss: 0.00191150
Iteration 16/25 | Loss: 0.00191150
Iteration 17/25 | Loss: 0.00191150
Iteration 18/25 | Loss: 0.00191150
Iteration 19/25 | Loss: 0.00191150
Iteration 20/25 | Loss: 0.00191150
Iteration 21/25 | Loss: 0.00191150
Iteration 22/25 | Loss: 0.00191150
Iteration 23/25 | Loss: 0.00191150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001911503728479147, 0.001911503728479147, 0.001911503728479147, 0.001911503728479147, 0.001911503728479147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001911503728479147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191150
Iteration 2/1000 | Loss: 0.00030901
Iteration 3/1000 | Loss: 0.00011071
Iteration 4/1000 | Loss: 0.00006976
Iteration 5/1000 | Loss: 0.00005087
Iteration 6/1000 | Loss: 0.00004378
Iteration 7/1000 | Loss: 0.00004067
Iteration 8/1000 | Loss: 0.00003865
Iteration 9/1000 | Loss: 0.00003693
Iteration 10/1000 | Loss: 0.00003546
Iteration 11/1000 | Loss: 0.00003467
Iteration 12/1000 | Loss: 0.00003402
Iteration 13/1000 | Loss: 0.00003359
Iteration 14/1000 | Loss: 0.00003312
Iteration 15/1000 | Loss: 0.00003273
Iteration 16/1000 | Loss: 0.00003245
Iteration 17/1000 | Loss: 0.00003221
Iteration 18/1000 | Loss: 0.00003216
Iteration 19/1000 | Loss: 0.00003215
Iteration 20/1000 | Loss: 0.00003199
Iteration 21/1000 | Loss: 0.00003186
Iteration 22/1000 | Loss: 0.00003179
Iteration 23/1000 | Loss: 0.00003177
Iteration 24/1000 | Loss: 0.00003176
Iteration 25/1000 | Loss: 0.00003175
Iteration 26/1000 | Loss: 0.00003174
Iteration 27/1000 | Loss: 0.00003173
Iteration 28/1000 | Loss: 0.00003171
Iteration 29/1000 | Loss: 0.00003164
Iteration 30/1000 | Loss: 0.00003162
Iteration 31/1000 | Loss: 0.00003160
Iteration 32/1000 | Loss: 0.00003160
Iteration 33/1000 | Loss: 0.00003159
Iteration 34/1000 | Loss: 0.00003158
Iteration 35/1000 | Loss: 0.00003153
Iteration 36/1000 | Loss: 0.00003150
Iteration 37/1000 | Loss: 0.00003146
Iteration 38/1000 | Loss: 0.00003146
Iteration 39/1000 | Loss: 0.00003141
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00003130
Iteration 42/1000 | Loss: 0.00003130
Iteration 43/1000 | Loss: 0.00003129
Iteration 44/1000 | Loss: 0.00003129
Iteration 45/1000 | Loss: 0.00003128
Iteration 46/1000 | Loss: 0.00003127
Iteration 47/1000 | Loss: 0.00003127
Iteration 48/1000 | Loss: 0.00003127
Iteration 49/1000 | Loss: 0.00003126
Iteration 50/1000 | Loss: 0.00003126
Iteration 51/1000 | Loss: 0.00003125
Iteration 52/1000 | Loss: 0.00003125
Iteration 53/1000 | Loss: 0.00003124
Iteration 54/1000 | Loss: 0.00003124
Iteration 55/1000 | Loss: 0.00003124
Iteration 56/1000 | Loss: 0.00003123
Iteration 57/1000 | Loss: 0.00003123
Iteration 58/1000 | Loss: 0.00003123
Iteration 59/1000 | Loss: 0.00003123
Iteration 60/1000 | Loss: 0.00003123
Iteration 61/1000 | Loss: 0.00003123
Iteration 62/1000 | Loss: 0.00003123
Iteration 63/1000 | Loss: 0.00003122
Iteration 64/1000 | Loss: 0.00003122
Iteration 65/1000 | Loss: 0.00003122
Iteration 66/1000 | Loss: 0.00003122
Iteration 67/1000 | Loss: 0.00003121
Iteration 68/1000 | Loss: 0.00003121
Iteration 69/1000 | Loss: 0.00003121
Iteration 70/1000 | Loss: 0.00003120
Iteration 71/1000 | Loss: 0.00003119
Iteration 72/1000 | Loss: 0.00003119
Iteration 73/1000 | Loss: 0.00003118
Iteration 74/1000 | Loss: 0.00003118
Iteration 75/1000 | Loss: 0.00003118
Iteration 76/1000 | Loss: 0.00003117
Iteration 77/1000 | Loss: 0.00003116
Iteration 78/1000 | Loss: 0.00003116
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00003115
Iteration 81/1000 | Loss: 0.00003115
Iteration 82/1000 | Loss: 0.00003114
Iteration 83/1000 | Loss: 0.00003114
Iteration 84/1000 | Loss: 0.00003114
Iteration 85/1000 | Loss: 0.00003113
Iteration 86/1000 | Loss: 0.00003113
Iteration 87/1000 | Loss: 0.00003112
Iteration 88/1000 | Loss: 0.00003112
Iteration 89/1000 | Loss: 0.00003111
Iteration 90/1000 | Loss: 0.00003111
Iteration 91/1000 | Loss: 0.00003110
Iteration 92/1000 | Loss: 0.00003110
Iteration 93/1000 | Loss: 0.00003110
Iteration 94/1000 | Loss: 0.00003109
Iteration 95/1000 | Loss: 0.00003109
Iteration 96/1000 | Loss: 0.00003109
Iteration 97/1000 | Loss: 0.00003109
Iteration 98/1000 | Loss: 0.00003109
Iteration 99/1000 | Loss: 0.00003108
Iteration 100/1000 | Loss: 0.00003108
Iteration 101/1000 | Loss: 0.00003108
Iteration 102/1000 | Loss: 0.00003108
Iteration 103/1000 | Loss: 0.00003107
Iteration 104/1000 | Loss: 0.00003107
Iteration 105/1000 | Loss: 0.00003107
Iteration 106/1000 | Loss: 0.00003107
Iteration 107/1000 | Loss: 0.00003106
Iteration 108/1000 | Loss: 0.00003106
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00003106
Iteration 111/1000 | Loss: 0.00003106
Iteration 112/1000 | Loss: 0.00003105
Iteration 113/1000 | Loss: 0.00003105
Iteration 114/1000 | Loss: 0.00003105
Iteration 115/1000 | Loss: 0.00003105
Iteration 116/1000 | Loss: 0.00003105
Iteration 117/1000 | Loss: 0.00003104
Iteration 118/1000 | Loss: 0.00003104
Iteration 119/1000 | Loss: 0.00003104
Iteration 120/1000 | Loss: 0.00003104
Iteration 121/1000 | Loss: 0.00003104
Iteration 122/1000 | Loss: 0.00003104
Iteration 123/1000 | Loss: 0.00003104
Iteration 124/1000 | Loss: 0.00003103
Iteration 125/1000 | Loss: 0.00003103
Iteration 126/1000 | Loss: 0.00003103
Iteration 127/1000 | Loss: 0.00003103
Iteration 128/1000 | Loss: 0.00003103
Iteration 129/1000 | Loss: 0.00003103
Iteration 130/1000 | Loss: 0.00003103
Iteration 131/1000 | Loss: 0.00003103
Iteration 132/1000 | Loss: 0.00003103
Iteration 133/1000 | Loss: 0.00003103
Iteration 134/1000 | Loss: 0.00003103
Iteration 135/1000 | Loss: 0.00003103
Iteration 136/1000 | Loss: 0.00003103
Iteration 137/1000 | Loss: 0.00003103
Iteration 138/1000 | Loss: 0.00003103
Iteration 139/1000 | Loss: 0.00003102
Iteration 140/1000 | Loss: 0.00003102
Iteration 141/1000 | Loss: 0.00003102
Iteration 142/1000 | Loss: 0.00003102
Iteration 143/1000 | Loss: 0.00003102
Iteration 144/1000 | Loss: 0.00003102
Iteration 145/1000 | Loss: 0.00003102
Iteration 146/1000 | Loss: 0.00003102
Iteration 147/1000 | Loss: 0.00003102
Iteration 148/1000 | Loss: 0.00003101
Iteration 149/1000 | Loss: 0.00003101
Iteration 150/1000 | Loss: 0.00003101
Iteration 151/1000 | Loss: 0.00003101
Iteration 152/1000 | Loss: 0.00003101
Iteration 153/1000 | Loss: 0.00003101
Iteration 154/1000 | Loss: 0.00003101
Iteration 155/1000 | Loss: 0.00003101
Iteration 156/1000 | Loss: 0.00003101
Iteration 157/1000 | Loss: 0.00003101
Iteration 158/1000 | Loss: 0.00003101
Iteration 159/1000 | Loss: 0.00003101
Iteration 160/1000 | Loss: 0.00003101
Iteration 161/1000 | Loss: 0.00003101
Iteration 162/1000 | Loss: 0.00003101
Iteration 163/1000 | Loss: 0.00003101
Iteration 164/1000 | Loss: 0.00003101
Iteration 165/1000 | Loss: 0.00003101
Iteration 166/1000 | Loss: 0.00003101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.100706817349419e-05, 3.100706817349419e-05, 3.100706817349419e-05, 3.100706817349419e-05, 3.100706817349419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.100706817349419e-05

Optimization complete. Final v2v error: 4.046634197235107 mm

Highest mean error: 11.173279762268066 mm for frame 137

Lowest mean error: 3.193030595779419 mm for frame 77

Saving results

Total time: 70.42231726646423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009527
Iteration 2/25 | Loss: 0.00179249
Iteration 3/25 | Loss: 0.00158506
Iteration 4/25 | Loss: 0.00144756
Iteration 5/25 | Loss: 0.00142293
Iteration 6/25 | Loss: 0.00141486
Iteration 7/25 | Loss: 0.00140334
Iteration 8/25 | Loss: 0.00137877
Iteration 9/25 | Loss: 0.00134938
Iteration 10/25 | Loss: 0.00134374
Iteration 11/25 | Loss: 0.00133550
Iteration 12/25 | Loss: 0.00133898
Iteration 13/25 | Loss: 0.00133154
Iteration 14/25 | Loss: 0.00133065
Iteration 15/25 | Loss: 0.00132418
Iteration 16/25 | Loss: 0.00131939
Iteration 17/25 | Loss: 0.00133419
Iteration 18/25 | Loss: 0.00131987
Iteration 19/25 | Loss: 0.00130673
Iteration 20/25 | Loss: 0.00129542
Iteration 21/25 | Loss: 0.00129435
Iteration 22/25 | Loss: 0.00128829
Iteration 23/25 | Loss: 0.00127751
Iteration 24/25 | Loss: 0.00127510
Iteration 25/25 | Loss: 0.00127477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44405520
Iteration 2/25 | Loss: 0.00109254
Iteration 3/25 | Loss: 0.00109254
Iteration 4/25 | Loss: 0.00109254
Iteration 5/25 | Loss: 0.00109254
Iteration 6/25 | Loss: 0.00109254
Iteration 7/25 | Loss: 0.00109254
Iteration 8/25 | Loss: 0.00109254
Iteration 9/25 | Loss: 0.00109254
Iteration 10/25 | Loss: 0.00109254
Iteration 11/25 | Loss: 0.00109254
Iteration 12/25 | Loss: 0.00109254
Iteration 13/25 | Loss: 0.00109254
Iteration 14/25 | Loss: 0.00109254
Iteration 15/25 | Loss: 0.00109254
Iteration 16/25 | Loss: 0.00109254
Iteration 17/25 | Loss: 0.00109254
Iteration 18/25 | Loss: 0.00109254
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010925378883257508, 0.0010925378883257508, 0.0010925378883257508, 0.0010925378883257508, 0.0010925378883257508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010925378883257508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109254
Iteration 2/1000 | Loss: 0.00030774
Iteration 3/1000 | Loss: 0.00027250
Iteration 4/1000 | Loss: 0.00008920
Iteration 5/1000 | Loss: 0.00078556
Iteration 6/1000 | Loss: 0.00051064
Iteration 7/1000 | Loss: 0.00020555
Iteration 8/1000 | Loss: 0.00002994
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00006366
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00005209
Iteration 13/1000 | Loss: 0.00002079
Iteration 14/1000 | Loss: 0.00003616
Iteration 15/1000 | Loss: 0.00001917
Iteration 16/1000 | Loss: 0.00001864
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001771
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00006181
Iteration 24/1000 | Loss: 0.00025427
Iteration 25/1000 | Loss: 0.00006603
Iteration 26/1000 | Loss: 0.00004840
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001748
Iteration 30/1000 | Loss: 0.00001747
Iteration 31/1000 | Loss: 0.00001747
Iteration 32/1000 | Loss: 0.00001746
Iteration 33/1000 | Loss: 0.00001746
Iteration 34/1000 | Loss: 0.00001745
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001744
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001743
Iteration 40/1000 | Loss: 0.00002678
Iteration 41/1000 | Loss: 0.00001742
Iteration 42/1000 | Loss: 0.00001739
Iteration 43/1000 | Loss: 0.00001739
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001738
Iteration 49/1000 | Loss: 0.00001738
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001736
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001735
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001729
Iteration 92/1000 | Loss: 0.00001729
Iteration 93/1000 | Loss: 0.00001729
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001728
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001726
Iteration 112/1000 | Loss: 0.00001726
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001724
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001724
Iteration 125/1000 | Loss: 0.00001724
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001723
Iteration 130/1000 | Loss: 0.00001723
Iteration 131/1000 | Loss: 0.00001723
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001723
Iteration 135/1000 | Loss: 0.00001723
Iteration 136/1000 | Loss: 0.00001723
Iteration 137/1000 | Loss: 0.00001723
Iteration 138/1000 | Loss: 0.00001723
Iteration 139/1000 | Loss: 0.00001723
Iteration 140/1000 | Loss: 0.00001723
Iteration 141/1000 | Loss: 0.00001723
Iteration 142/1000 | Loss: 0.00001723
Iteration 143/1000 | Loss: 0.00001723
Iteration 144/1000 | Loss: 0.00001723
Iteration 145/1000 | Loss: 0.00001723
Iteration 146/1000 | Loss: 0.00001722
Iteration 147/1000 | Loss: 0.00001722
Iteration 148/1000 | Loss: 0.00001722
Iteration 149/1000 | Loss: 0.00001722
Iteration 150/1000 | Loss: 0.00001722
Iteration 151/1000 | Loss: 0.00001722
Iteration 152/1000 | Loss: 0.00001722
Iteration 153/1000 | Loss: 0.00001722
Iteration 154/1000 | Loss: 0.00001722
Iteration 155/1000 | Loss: 0.00001722
Iteration 156/1000 | Loss: 0.00001722
Iteration 157/1000 | Loss: 0.00001722
Iteration 158/1000 | Loss: 0.00001722
Iteration 159/1000 | Loss: 0.00001722
Iteration 160/1000 | Loss: 0.00001722
Iteration 161/1000 | Loss: 0.00001722
Iteration 162/1000 | Loss: 0.00001722
Iteration 163/1000 | Loss: 0.00001722
Iteration 164/1000 | Loss: 0.00001722
Iteration 165/1000 | Loss: 0.00001722
Iteration 166/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.7218422726728022e-05, 1.7218422726728022e-05, 1.7218422726728022e-05, 1.7218422726728022e-05, 1.7218422726728022e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7218422726728022e-05

Optimization complete. Final v2v error: 3.3912906646728516 mm

Highest mean error: 5.798754692077637 mm for frame 68

Lowest mean error: 2.9733431339263916 mm for frame 7

Saving results

Total time: 90.30839943885803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012060
Iteration 2/25 | Loss: 0.00213910
Iteration 3/25 | Loss: 0.00154986
Iteration 4/25 | Loss: 0.00144358
Iteration 5/25 | Loss: 0.00139671
Iteration 6/25 | Loss: 0.00138901
Iteration 7/25 | Loss: 0.00137087
Iteration 8/25 | Loss: 0.00136602
Iteration 9/25 | Loss: 0.00136231
Iteration 10/25 | Loss: 0.00135872
Iteration 11/25 | Loss: 0.00135740
Iteration 12/25 | Loss: 0.00135702
Iteration 13/25 | Loss: 0.00135702
Iteration 14/25 | Loss: 0.00135702
Iteration 15/25 | Loss: 0.00135702
Iteration 16/25 | Loss: 0.00135702
Iteration 17/25 | Loss: 0.00135702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013570233713835478, 0.0013570233713835478, 0.0013570233713835478, 0.0013570233713835478, 0.0013570233713835478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013570233713835478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33615291
Iteration 2/25 | Loss: 0.00108824
Iteration 3/25 | Loss: 0.00091017
Iteration 4/25 | Loss: 0.00091017
Iteration 5/25 | Loss: 0.00091017
Iteration 6/25 | Loss: 0.00091017
Iteration 7/25 | Loss: 0.00091016
Iteration 8/25 | Loss: 0.00091016
Iteration 9/25 | Loss: 0.00091016
Iteration 10/25 | Loss: 0.00091016
Iteration 11/25 | Loss: 0.00091016
Iteration 12/25 | Loss: 0.00091016
Iteration 13/25 | Loss: 0.00091016
Iteration 14/25 | Loss: 0.00091016
Iteration 15/25 | Loss: 0.00091016
Iteration 16/25 | Loss: 0.00091016
Iteration 17/25 | Loss: 0.00091016
Iteration 18/25 | Loss: 0.00091016
Iteration 19/25 | Loss: 0.00091016
Iteration 20/25 | Loss: 0.00091016
Iteration 21/25 | Loss: 0.00091016
Iteration 22/25 | Loss: 0.00091016
Iteration 23/25 | Loss: 0.00091016
Iteration 24/25 | Loss: 0.00091016
Iteration 25/25 | Loss: 0.00091016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091016
Iteration 2/1000 | Loss: 0.00016734
Iteration 3/1000 | Loss: 0.00009929
Iteration 4/1000 | Loss: 0.00003965
Iteration 5/1000 | Loss: 0.00002964
Iteration 6/1000 | Loss: 0.00002730
Iteration 7/1000 | Loss: 0.00002565
Iteration 8/1000 | Loss: 0.00002403
Iteration 9/1000 | Loss: 0.00002352
Iteration 10/1000 | Loss: 0.00002306
Iteration 11/1000 | Loss: 0.00002279
Iteration 12/1000 | Loss: 0.00002263
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002244
Iteration 15/1000 | Loss: 0.00002226
Iteration 16/1000 | Loss: 0.00002225
Iteration 17/1000 | Loss: 0.00002225
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002218
Iteration 20/1000 | Loss: 0.00002217
Iteration 21/1000 | Loss: 0.00002217
Iteration 22/1000 | Loss: 0.00002217
Iteration 23/1000 | Loss: 0.00002216
Iteration 24/1000 | Loss: 0.00002216
Iteration 25/1000 | Loss: 0.00002216
Iteration 26/1000 | Loss: 0.00002215
Iteration 27/1000 | Loss: 0.00002213
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002211
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002210
Iteration 32/1000 | Loss: 0.00002209
Iteration 33/1000 | Loss: 0.00002209
Iteration 34/1000 | Loss: 0.00002208
Iteration 35/1000 | Loss: 0.00002208
Iteration 36/1000 | Loss: 0.00002207
Iteration 37/1000 | Loss: 0.00002207
Iteration 38/1000 | Loss: 0.00002204
Iteration 39/1000 | Loss: 0.00002204
Iteration 40/1000 | Loss: 0.00002203
Iteration 41/1000 | Loss: 0.00002203
Iteration 42/1000 | Loss: 0.00002203
Iteration 43/1000 | Loss: 0.00002203
Iteration 44/1000 | Loss: 0.00002203
Iteration 45/1000 | Loss: 0.00002202
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002202
Iteration 48/1000 | Loss: 0.00002202
Iteration 49/1000 | Loss: 0.00002201
Iteration 50/1000 | Loss: 0.00002201
Iteration 51/1000 | Loss: 0.00002201
Iteration 52/1000 | Loss: 0.00002201
Iteration 53/1000 | Loss: 0.00002200
Iteration 54/1000 | Loss: 0.00002200
Iteration 55/1000 | Loss: 0.00002200
Iteration 56/1000 | Loss: 0.00002199
Iteration 57/1000 | Loss: 0.00002199
Iteration 58/1000 | Loss: 0.00002199
Iteration 59/1000 | Loss: 0.00002199
Iteration 60/1000 | Loss: 0.00002199
Iteration 61/1000 | Loss: 0.00002199
Iteration 62/1000 | Loss: 0.00002199
Iteration 63/1000 | Loss: 0.00002198
Iteration 64/1000 | Loss: 0.00002198
Iteration 65/1000 | Loss: 0.00002198
Iteration 66/1000 | Loss: 0.00002197
Iteration 67/1000 | Loss: 0.00002197
Iteration 68/1000 | Loss: 0.00002197
Iteration 69/1000 | Loss: 0.00002197
Iteration 70/1000 | Loss: 0.00002196
Iteration 71/1000 | Loss: 0.00002196
Iteration 72/1000 | Loss: 0.00002196
Iteration 73/1000 | Loss: 0.00002196
Iteration 74/1000 | Loss: 0.00002196
Iteration 75/1000 | Loss: 0.00002195
Iteration 76/1000 | Loss: 0.00002195
Iteration 77/1000 | Loss: 0.00002195
Iteration 78/1000 | Loss: 0.00002195
Iteration 79/1000 | Loss: 0.00002195
Iteration 80/1000 | Loss: 0.00002194
Iteration 81/1000 | Loss: 0.00002194
Iteration 82/1000 | Loss: 0.00002194
Iteration 83/1000 | Loss: 0.00002194
Iteration 84/1000 | Loss: 0.00002194
Iteration 85/1000 | Loss: 0.00002194
Iteration 86/1000 | Loss: 0.00002194
Iteration 87/1000 | Loss: 0.00002193
Iteration 88/1000 | Loss: 0.00002193
Iteration 89/1000 | Loss: 0.00002193
Iteration 90/1000 | Loss: 0.00002193
Iteration 91/1000 | Loss: 0.00002193
Iteration 92/1000 | Loss: 0.00002193
Iteration 93/1000 | Loss: 0.00002193
Iteration 94/1000 | Loss: 0.00002193
Iteration 95/1000 | Loss: 0.00002192
Iteration 96/1000 | Loss: 0.00002192
Iteration 97/1000 | Loss: 0.00002192
Iteration 98/1000 | Loss: 0.00002192
Iteration 99/1000 | Loss: 0.00002192
Iteration 100/1000 | Loss: 0.00002192
Iteration 101/1000 | Loss: 0.00002191
Iteration 102/1000 | Loss: 0.00002191
Iteration 103/1000 | Loss: 0.00002191
Iteration 104/1000 | Loss: 0.00002191
Iteration 105/1000 | Loss: 0.00002191
Iteration 106/1000 | Loss: 0.00002191
Iteration 107/1000 | Loss: 0.00002191
Iteration 108/1000 | Loss: 0.00002191
Iteration 109/1000 | Loss: 0.00002191
Iteration 110/1000 | Loss: 0.00002191
Iteration 111/1000 | Loss: 0.00002191
Iteration 112/1000 | Loss: 0.00002191
Iteration 113/1000 | Loss: 0.00002190
Iteration 114/1000 | Loss: 0.00002190
Iteration 115/1000 | Loss: 0.00002190
Iteration 116/1000 | Loss: 0.00002189
Iteration 117/1000 | Loss: 0.00002189
Iteration 118/1000 | Loss: 0.00002189
Iteration 119/1000 | Loss: 0.00002189
Iteration 120/1000 | Loss: 0.00002188
Iteration 121/1000 | Loss: 0.00002188
Iteration 122/1000 | Loss: 0.00002188
Iteration 123/1000 | Loss: 0.00002188
Iteration 124/1000 | Loss: 0.00002188
Iteration 125/1000 | Loss: 0.00002188
Iteration 126/1000 | Loss: 0.00002188
Iteration 127/1000 | Loss: 0.00002188
Iteration 128/1000 | Loss: 0.00002188
Iteration 129/1000 | Loss: 0.00002188
Iteration 130/1000 | Loss: 0.00002188
Iteration 131/1000 | Loss: 0.00002188
Iteration 132/1000 | Loss: 0.00002188
Iteration 133/1000 | Loss: 0.00002188
Iteration 134/1000 | Loss: 0.00002188
Iteration 135/1000 | Loss: 0.00002187
Iteration 136/1000 | Loss: 0.00002187
Iteration 137/1000 | Loss: 0.00002187
Iteration 138/1000 | Loss: 0.00002187
Iteration 139/1000 | Loss: 0.00002187
Iteration 140/1000 | Loss: 0.00002187
Iteration 141/1000 | Loss: 0.00002187
Iteration 142/1000 | Loss: 0.00002187
Iteration 143/1000 | Loss: 0.00002187
Iteration 144/1000 | Loss: 0.00002187
Iteration 145/1000 | Loss: 0.00002187
Iteration 146/1000 | Loss: 0.00002187
Iteration 147/1000 | Loss: 0.00002187
Iteration 148/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1870455384487286e-05, 2.1870455384487286e-05, 2.1870455384487286e-05, 2.1870455384487286e-05, 2.1870455384487286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1870455384487286e-05

Optimization complete. Final v2v error: 3.8313422203063965 mm

Highest mean error: 5.446589469909668 mm for frame 218

Lowest mean error: 3.40795636177063 mm for frame 67

Saving results

Total time: 56.657684326171875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843782
Iteration 2/25 | Loss: 0.00127165
Iteration 3/25 | Loss: 0.00120178
Iteration 4/25 | Loss: 0.00119210
Iteration 5/25 | Loss: 0.00118839
Iteration 6/25 | Loss: 0.00118774
Iteration 7/25 | Loss: 0.00118774
Iteration 8/25 | Loss: 0.00118774
Iteration 9/25 | Loss: 0.00118774
Iteration 10/25 | Loss: 0.00118774
Iteration 11/25 | Loss: 0.00118774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011877407087013125, 0.0011877407087013125, 0.0011877407087013125, 0.0011877407087013125, 0.0011877407087013125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011877407087013125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77078617
Iteration 2/25 | Loss: 0.00081259
Iteration 3/25 | Loss: 0.00081259
Iteration 4/25 | Loss: 0.00081259
Iteration 5/25 | Loss: 0.00081259
Iteration 6/25 | Loss: 0.00081259
Iteration 7/25 | Loss: 0.00081259
Iteration 8/25 | Loss: 0.00081259
Iteration 9/25 | Loss: 0.00081259
Iteration 10/25 | Loss: 0.00081259
Iteration 11/25 | Loss: 0.00081259
Iteration 12/25 | Loss: 0.00081259
Iteration 13/25 | Loss: 0.00081259
Iteration 14/25 | Loss: 0.00081259
Iteration 15/25 | Loss: 0.00081259
Iteration 16/25 | Loss: 0.00081259
Iteration 17/25 | Loss: 0.00081259
Iteration 18/25 | Loss: 0.00081259
Iteration 19/25 | Loss: 0.00081259
Iteration 20/25 | Loss: 0.00081259
Iteration 21/25 | Loss: 0.00081259
Iteration 22/25 | Loss: 0.00081259
Iteration 23/25 | Loss: 0.00081259
Iteration 24/25 | Loss: 0.00081259
Iteration 25/25 | Loss: 0.00081259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081259
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001400
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001201
Iteration 8/1000 | Loss: 0.00001166
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001147
Iteration 12/1000 | Loss: 0.00001126
Iteration 13/1000 | Loss: 0.00001120
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001112
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001110
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001108
Iteration 26/1000 | Loss: 0.00001103
Iteration 27/1000 | Loss: 0.00001098
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001094
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001092
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001090
Iteration 45/1000 | Loss: 0.00001090
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001090
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00001090
Iteration 52/1000 | Loss: 0.00001090
Iteration 53/1000 | Loss: 0.00001089
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001088
Iteration 58/1000 | Loss: 0.00001088
Iteration 59/1000 | Loss: 0.00001088
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001086
Iteration 63/1000 | Loss: 0.00001086
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001082
Iteration 69/1000 | Loss: 0.00001082
Iteration 70/1000 | Loss: 0.00001082
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001081
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001080
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001077
Iteration 83/1000 | Loss: 0.00001077
Iteration 84/1000 | Loss: 0.00001077
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001076
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001076
Iteration 121/1000 | Loss: 0.00001076
Iteration 122/1000 | Loss: 0.00001076
Iteration 123/1000 | Loss: 0.00001076
Iteration 124/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.0759646102087572e-05, 1.0759646102087572e-05, 1.0759646102087572e-05, 1.0759646102087572e-05, 1.0759646102087572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0759646102087572e-05

Optimization complete. Final v2v error: 2.8140201568603516 mm

Highest mean error: 3.2232320308685303 mm for frame 91

Lowest mean error: 2.6263632774353027 mm for frame 128

Saving results

Total time: 31.17107915878296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019622
Iteration 2/25 | Loss: 0.01019622
Iteration 3/25 | Loss: 0.00194161
Iteration 4/25 | Loss: 0.00153511
Iteration 5/25 | Loss: 0.00143223
Iteration 6/25 | Loss: 0.00148184
Iteration 7/25 | Loss: 0.00153489
Iteration 8/25 | Loss: 0.00141359
Iteration 9/25 | Loss: 0.00135184
Iteration 10/25 | Loss: 0.00131901
Iteration 11/25 | Loss: 0.00131236
Iteration 12/25 | Loss: 0.00131265
Iteration 13/25 | Loss: 0.00131700
Iteration 14/25 | Loss: 0.00130729
Iteration 15/25 | Loss: 0.00130752
Iteration 16/25 | Loss: 0.00129939
Iteration 17/25 | Loss: 0.00129544
Iteration 18/25 | Loss: 0.00129112
Iteration 19/25 | Loss: 0.00128224
Iteration 20/25 | Loss: 0.00128066
Iteration 21/25 | Loss: 0.00128024
Iteration 22/25 | Loss: 0.00127581
Iteration 23/25 | Loss: 0.00127086
Iteration 24/25 | Loss: 0.00126464
Iteration 25/25 | Loss: 0.00126123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46798086
Iteration 2/25 | Loss: 0.00111671
Iteration 3/25 | Loss: 0.00111670
Iteration 4/25 | Loss: 0.00111670
Iteration 5/25 | Loss: 0.00111670
Iteration 6/25 | Loss: 0.00111670
Iteration 7/25 | Loss: 0.00111670
Iteration 8/25 | Loss: 0.00111670
Iteration 9/25 | Loss: 0.00111670
Iteration 10/25 | Loss: 0.00111670
Iteration 11/25 | Loss: 0.00111670
Iteration 12/25 | Loss: 0.00111670
Iteration 13/25 | Loss: 0.00111670
Iteration 14/25 | Loss: 0.00111670
Iteration 15/25 | Loss: 0.00111670
Iteration 16/25 | Loss: 0.00111670
Iteration 17/25 | Loss: 0.00111670
Iteration 18/25 | Loss: 0.00111670
Iteration 19/25 | Loss: 0.00111670
Iteration 20/25 | Loss: 0.00111670
Iteration 21/25 | Loss: 0.00111670
Iteration 22/25 | Loss: 0.00111670
Iteration 23/25 | Loss: 0.00111670
Iteration 24/25 | Loss: 0.00111670
Iteration 25/25 | Loss: 0.00111670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111670
Iteration 2/1000 | Loss: 0.00006300
Iteration 3/1000 | Loss: 0.00010173
Iteration 4/1000 | Loss: 0.00008001
Iteration 5/1000 | Loss: 0.00005185
Iteration 6/1000 | Loss: 0.00015481
Iteration 7/1000 | Loss: 0.00018530
Iteration 8/1000 | Loss: 0.00044259
Iteration 9/1000 | Loss: 0.00032901
Iteration 10/1000 | Loss: 0.00016447
Iteration 11/1000 | Loss: 0.00023327
Iteration 12/1000 | Loss: 0.00025896
Iteration 13/1000 | Loss: 0.00009537
Iteration 14/1000 | Loss: 0.00020616
Iteration 15/1000 | Loss: 0.00038300
Iteration 16/1000 | Loss: 0.00020585
Iteration 17/1000 | Loss: 0.00015132
Iteration 18/1000 | Loss: 0.00022982
Iteration 19/1000 | Loss: 0.00053951
Iteration 20/1000 | Loss: 0.00014638
Iteration 21/1000 | Loss: 0.00011186
Iteration 22/1000 | Loss: 0.00013892
Iteration 23/1000 | Loss: 0.00008956
Iteration 24/1000 | Loss: 0.00035865
Iteration 25/1000 | Loss: 0.00019422
Iteration 26/1000 | Loss: 0.00018863
Iteration 27/1000 | Loss: 0.00003374
Iteration 28/1000 | Loss: 0.00035535
Iteration 29/1000 | Loss: 0.00021323
Iteration 30/1000 | Loss: 0.00011308
Iteration 31/1000 | Loss: 0.00018149
Iteration 32/1000 | Loss: 0.00009477
Iteration 33/1000 | Loss: 0.00009784
Iteration 34/1000 | Loss: 0.00007403
Iteration 35/1000 | Loss: 0.00012560
Iteration 36/1000 | Loss: 0.00014477
Iteration 37/1000 | Loss: 0.00004244
Iteration 38/1000 | Loss: 0.00009281
Iteration 39/1000 | Loss: 0.00007561
Iteration 40/1000 | Loss: 0.00011652
Iteration 41/1000 | Loss: 0.00008115
Iteration 42/1000 | Loss: 0.00025053
Iteration 43/1000 | Loss: 0.00027102
Iteration 44/1000 | Loss: 0.00016397
Iteration 45/1000 | Loss: 0.00004860
Iteration 46/1000 | Loss: 0.00003367
Iteration 47/1000 | Loss: 0.00013193
Iteration 48/1000 | Loss: 0.00003158
Iteration 49/1000 | Loss: 0.00003716
Iteration 50/1000 | Loss: 0.00002702
Iteration 51/1000 | Loss: 0.00003093
Iteration 52/1000 | Loss: 0.00002848
Iteration 53/1000 | Loss: 0.00003698
Iteration 54/1000 | Loss: 0.00003592
Iteration 55/1000 | Loss: 0.00003553
Iteration 56/1000 | Loss: 0.00004446
Iteration 57/1000 | Loss: 0.00004576
Iteration 58/1000 | Loss: 0.00003516
Iteration 59/1000 | Loss: 0.00004448
Iteration 60/1000 | Loss: 0.00003199
Iteration 61/1000 | Loss: 0.00003690
Iteration 62/1000 | Loss: 0.00003426
Iteration 63/1000 | Loss: 0.00003078
Iteration 64/1000 | Loss: 0.00003230
Iteration 65/1000 | Loss: 0.00003066
Iteration 66/1000 | Loss: 0.00054022
Iteration 67/1000 | Loss: 0.00028386
Iteration 68/1000 | Loss: 0.00026660
Iteration 69/1000 | Loss: 0.00066294
Iteration 70/1000 | Loss: 0.00020200
Iteration 71/1000 | Loss: 0.00054162
Iteration 72/1000 | Loss: 0.00063007
Iteration 73/1000 | Loss: 0.00046301
Iteration 74/1000 | Loss: 0.00006145
Iteration 75/1000 | Loss: 0.00002771
Iteration 76/1000 | Loss: 0.00002287
Iteration 77/1000 | Loss: 0.00002101
Iteration 78/1000 | Loss: 0.00017068
Iteration 79/1000 | Loss: 0.00007777
Iteration 80/1000 | Loss: 0.00005219
Iteration 81/1000 | Loss: 0.00002129
Iteration 82/1000 | Loss: 0.00001983
Iteration 83/1000 | Loss: 0.00012237
Iteration 84/1000 | Loss: 0.00012546
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00020012
Iteration 88/1000 | Loss: 0.00014863
Iteration 89/1000 | Loss: 0.00002676
Iteration 90/1000 | Loss: 0.00008816
Iteration 91/1000 | Loss: 0.00002381
Iteration 92/1000 | Loss: 0.00012145
Iteration 93/1000 | Loss: 0.00008637
Iteration 94/1000 | Loss: 0.00010044
Iteration 95/1000 | Loss: 0.00007605
Iteration 96/1000 | Loss: 0.00012984
Iteration 97/1000 | Loss: 0.00013411
Iteration 98/1000 | Loss: 0.00003621
Iteration 99/1000 | Loss: 0.00002227
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00012494
Iteration 102/1000 | Loss: 0.00005244
Iteration 103/1000 | Loss: 0.00011622
Iteration 104/1000 | Loss: 0.00011364
Iteration 105/1000 | Loss: 0.00011884
Iteration 106/1000 | Loss: 0.00014988
Iteration 107/1000 | Loss: 0.00009880
Iteration 108/1000 | Loss: 0.00011187
Iteration 109/1000 | Loss: 0.00012450
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00008780
Iteration 112/1000 | Loss: 0.00021800
Iteration 113/1000 | Loss: 0.00015984
Iteration 114/1000 | Loss: 0.00046905
Iteration 115/1000 | Loss: 0.00009091
Iteration 116/1000 | Loss: 0.00009604
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002033
Iteration 119/1000 | Loss: 0.00001911
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001576
Iteration 123/1000 | Loss: 0.00001515
Iteration 124/1000 | Loss: 0.00001481
Iteration 125/1000 | Loss: 0.00001460
Iteration 126/1000 | Loss: 0.00001457
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001453
Iteration 132/1000 | Loss: 0.00001453
Iteration 133/1000 | Loss: 0.00001453
Iteration 134/1000 | Loss: 0.00001452
Iteration 135/1000 | Loss: 0.00001452
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001450
Iteration 139/1000 | Loss: 0.00001450
Iteration 140/1000 | Loss: 0.00001449
Iteration 141/1000 | Loss: 0.00001449
Iteration 142/1000 | Loss: 0.00001449
Iteration 143/1000 | Loss: 0.00001449
Iteration 144/1000 | Loss: 0.00001449
Iteration 145/1000 | Loss: 0.00001449
Iteration 146/1000 | Loss: 0.00001449
Iteration 147/1000 | Loss: 0.00001449
Iteration 148/1000 | Loss: 0.00001449
Iteration 149/1000 | Loss: 0.00001449
Iteration 150/1000 | Loss: 0.00001448
Iteration 151/1000 | Loss: 0.00001448
Iteration 152/1000 | Loss: 0.00001448
Iteration 153/1000 | Loss: 0.00001448
Iteration 154/1000 | Loss: 0.00001447
Iteration 155/1000 | Loss: 0.00001447
Iteration 156/1000 | Loss: 0.00001447
Iteration 157/1000 | Loss: 0.00001447
Iteration 158/1000 | Loss: 0.00001447
Iteration 159/1000 | Loss: 0.00001447
Iteration 160/1000 | Loss: 0.00001446
Iteration 161/1000 | Loss: 0.00001446
Iteration 162/1000 | Loss: 0.00001446
Iteration 163/1000 | Loss: 0.00001446
Iteration 164/1000 | Loss: 0.00001446
Iteration 165/1000 | Loss: 0.00001446
Iteration 166/1000 | Loss: 0.00001446
Iteration 167/1000 | Loss: 0.00001446
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001445
Iteration 170/1000 | Loss: 0.00001445
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001444
Iteration 176/1000 | Loss: 0.00001444
Iteration 177/1000 | Loss: 0.00001444
Iteration 178/1000 | Loss: 0.00001444
Iteration 179/1000 | Loss: 0.00001444
Iteration 180/1000 | Loss: 0.00001444
Iteration 181/1000 | Loss: 0.00001444
Iteration 182/1000 | Loss: 0.00001443
Iteration 183/1000 | Loss: 0.00001443
Iteration 184/1000 | Loss: 0.00001443
Iteration 185/1000 | Loss: 0.00001443
Iteration 186/1000 | Loss: 0.00001443
Iteration 187/1000 | Loss: 0.00001443
Iteration 188/1000 | Loss: 0.00001443
Iteration 189/1000 | Loss: 0.00001443
Iteration 190/1000 | Loss: 0.00001443
Iteration 191/1000 | Loss: 0.00001443
Iteration 192/1000 | Loss: 0.00001442
Iteration 193/1000 | Loss: 0.00001442
Iteration 194/1000 | Loss: 0.00001442
Iteration 195/1000 | Loss: 0.00001442
Iteration 196/1000 | Loss: 0.00001442
Iteration 197/1000 | Loss: 0.00001442
Iteration 198/1000 | Loss: 0.00001441
Iteration 199/1000 | Loss: 0.00001441
Iteration 200/1000 | Loss: 0.00001441
Iteration 201/1000 | Loss: 0.00001441
Iteration 202/1000 | Loss: 0.00001440
Iteration 203/1000 | Loss: 0.00001440
Iteration 204/1000 | Loss: 0.00001440
Iteration 205/1000 | Loss: 0.00001439
Iteration 206/1000 | Loss: 0.00001439
Iteration 207/1000 | Loss: 0.00001439
Iteration 208/1000 | Loss: 0.00001438
Iteration 209/1000 | Loss: 0.00001438
Iteration 210/1000 | Loss: 0.00001438
Iteration 211/1000 | Loss: 0.00001437
Iteration 212/1000 | Loss: 0.00001437
Iteration 213/1000 | Loss: 0.00001437
Iteration 214/1000 | Loss: 0.00001437
Iteration 215/1000 | Loss: 0.00001437
Iteration 216/1000 | Loss: 0.00001437
Iteration 217/1000 | Loss: 0.00001437
Iteration 218/1000 | Loss: 0.00001437
Iteration 219/1000 | Loss: 0.00001437
Iteration 220/1000 | Loss: 0.00001437
Iteration 221/1000 | Loss: 0.00001437
Iteration 222/1000 | Loss: 0.00001437
Iteration 223/1000 | Loss: 0.00001437
Iteration 224/1000 | Loss: 0.00001437
Iteration 225/1000 | Loss: 0.00001437
Iteration 226/1000 | Loss: 0.00001437
Iteration 227/1000 | Loss: 0.00001437
Iteration 228/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [1.4367322364705615e-05, 1.4367322364705615e-05, 1.4367322364705615e-05, 1.4367322364705615e-05, 1.4367322364705615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4367322364705615e-05

Optimization complete. Final v2v error: 3.2016289234161377 mm

Highest mean error: 4.211031436920166 mm for frame 170

Lowest mean error: 2.900637626647949 mm for frame 142

Saving results

Total time: 254.16035652160645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840749
Iteration 2/25 | Loss: 0.00171901
Iteration 3/25 | Loss: 0.00143927
Iteration 4/25 | Loss: 0.00139671
Iteration 5/25 | Loss: 0.00137400
Iteration 6/25 | Loss: 0.00135936
Iteration 7/25 | Loss: 0.00136794
Iteration 8/25 | Loss: 0.00135445
Iteration 9/25 | Loss: 0.00131276
Iteration 10/25 | Loss: 0.00131304
Iteration 11/25 | Loss: 0.00129817
Iteration 12/25 | Loss: 0.00129645
Iteration 13/25 | Loss: 0.00129995
Iteration 14/25 | Loss: 0.00129339
Iteration 15/25 | Loss: 0.00129268
Iteration 16/25 | Loss: 0.00129250
Iteration 17/25 | Loss: 0.00129178
Iteration 18/25 | Loss: 0.00129553
Iteration 19/25 | Loss: 0.00128876
Iteration 20/25 | Loss: 0.00128776
Iteration 21/25 | Loss: 0.00128770
Iteration 22/25 | Loss: 0.00128770
Iteration 23/25 | Loss: 0.00128770
Iteration 24/25 | Loss: 0.00128770
Iteration 25/25 | Loss: 0.00128770

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98753017
Iteration 2/25 | Loss: 0.00075445
Iteration 3/25 | Loss: 0.00075444
Iteration 4/25 | Loss: 0.00075444
Iteration 5/25 | Loss: 0.00075444
Iteration 6/25 | Loss: 0.00075444
Iteration 7/25 | Loss: 0.00075444
Iteration 8/25 | Loss: 0.00075444
Iteration 9/25 | Loss: 0.00075444
Iteration 10/25 | Loss: 0.00075444
Iteration 11/25 | Loss: 0.00075444
Iteration 12/25 | Loss: 0.00075444
Iteration 13/25 | Loss: 0.00075444
Iteration 14/25 | Loss: 0.00075444
Iteration 15/25 | Loss: 0.00075444
Iteration 16/25 | Loss: 0.00075444
Iteration 17/25 | Loss: 0.00075444
Iteration 18/25 | Loss: 0.00075444
Iteration 19/25 | Loss: 0.00075444
Iteration 20/25 | Loss: 0.00075444
Iteration 21/25 | Loss: 0.00075444
Iteration 22/25 | Loss: 0.00075444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007544399122707546, 0.0007544399122707546, 0.0007544399122707546, 0.0007544399122707546, 0.0007544399122707546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007544399122707546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075444
Iteration 2/1000 | Loss: 0.00007548
Iteration 3/1000 | Loss: 0.00005705
Iteration 4/1000 | Loss: 0.00004908
Iteration 5/1000 | Loss: 0.00004635
Iteration 6/1000 | Loss: 0.00004342
Iteration 7/1000 | Loss: 0.00004170
Iteration 8/1000 | Loss: 0.00004046
Iteration 9/1000 | Loss: 0.00003938
Iteration 10/1000 | Loss: 0.00003827
Iteration 11/1000 | Loss: 0.00003739
Iteration 12/1000 | Loss: 0.00312266
Iteration 13/1000 | Loss: 0.00019262
Iteration 14/1000 | Loss: 0.00009506
Iteration 15/1000 | Loss: 0.00004764
Iteration 16/1000 | Loss: 0.00003884
Iteration 17/1000 | Loss: 0.00003217
Iteration 18/1000 | Loss: 0.00002607
Iteration 19/1000 | Loss: 0.00002389
Iteration 20/1000 | Loss: 0.00002237
Iteration 21/1000 | Loss: 0.00002188
Iteration 22/1000 | Loss: 0.00002157
Iteration 23/1000 | Loss: 0.00002133
Iteration 24/1000 | Loss: 0.00002112
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002067
Iteration 28/1000 | Loss: 0.00002066
Iteration 29/1000 | Loss: 0.00002066
Iteration 30/1000 | Loss: 0.00002065
Iteration 31/1000 | Loss: 0.00002065
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002064
Iteration 34/1000 | Loss: 0.00002064
Iteration 35/1000 | Loss: 0.00002064
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002064
Iteration 38/1000 | Loss: 0.00002064
Iteration 39/1000 | Loss: 0.00002064
Iteration 40/1000 | Loss: 0.00002064
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002064
Iteration 43/1000 | Loss: 0.00002064
Iteration 44/1000 | Loss: 0.00002063
Iteration 45/1000 | Loss: 0.00002063
Iteration 46/1000 | Loss: 0.00002063
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002063
Iteration 49/1000 | Loss: 0.00002063
Iteration 50/1000 | Loss: 0.00002063
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002063
Iteration 54/1000 | Loss: 0.00002063
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002062
Iteration 57/1000 | Loss: 0.00002062
Iteration 58/1000 | Loss: 0.00002062
Iteration 59/1000 | Loss: 0.00002062
Iteration 60/1000 | Loss: 0.00002062
Iteration 61/1000 | Loss: 0.00002062
Iteration 62/1000 | Loss: 0.00002062
Iteration 63/1000 | Loss: 0.00002062
Iteration 64/1000 | Loss: 0.00002061
Iteration 65/1000 | Loss: 0.00002061
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002060
Iteration 70/1000 | Loss: 0.00002060
Iteration 71/1000 | Loss: 0.00002060
Iteration 72/1000 | Loss: 0.00002060
Iteration 73/1000 | Loss: 0.00002059
Iteration 74/1000 | Loss: 0.00002059
Iteration 75/1000 | Loss: 0.00002059
Iteration 76/1000 | Loss: 0.00002059
Iteration 77/1000 | Loss: 0.00002059
Iteration 78/1000 | Loss: 0.00002059
Iteration 79/1000 | Loss: 0.00002059
Iteration 80/1000 | Loss: 0.00002059
Iteration 81/1000 | Loss: 0.00002059
Iteration 82/1000 | Loss: 0.00002059
Iteration 83/1000 | Loss: 0.00002059
Iteration 84/1000 | Loss: 0.00002059
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002059
Iteration 87/1000 | Loss: 0.00002059
Iteration 88/1000 | Loss: 0.00002059
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002059
Iteration 92/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.058611426036805e-05, 2.058611426036805e-05, 2.058611426036805e-05, 2.058611426036805e-05, 2.058611426036805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.058611426036805e-05

Optimization complete. Final v2v error: 3.794793128967285 mm

Highest mean error: 3.955303192138672 mm for frame 20

Lowest mean error: 3.662959337234497 mm for frame 0

Saving results

Total time: 74.57645034790039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714066
Iteration 2/25 | Loss: 0.00213815
Iteration 3/25 | Loss: 0.00159846
Iteration 4/25 | Loss: 0.00137394
Iteration 5/25 | Loss: 0.00132843
Iteration 6/25 | Loss: 0.00133269
Iteration 7/25 | Loss: 0.00126636
Iteration 8/25 | Loss: 0.00124938
Iteration 9/25 | Loss: 0.00120756
Iteration 10/25 | Loss: 0.00119830
Iteration 11/25 | Loss: 0.00118872
Iteration 12/25 | Loss: 0.00118930
Iteration 13/25 | Loss: 0.00118661
Iteration 14/25 | Loss: 0.00118634
Iteration 15/25 | Loss: 0.00118624
Iteration 16/25 | Loss: 0.00118623
Iteration 17/25 | Loss: 0.00118623
Iteration 18/25 | Loss: 0.00118623
Iteration 19/25 | Loss: 0.00118623
Iteration 20/25 | Loss: 0.00118623
Iteration 21/25 | Loss: 0.00118623
Iteration 22/25 | Loss: 0.00118623
Iteration 23/25 | Loss: 0.00118623
Iteration 24/25 | Loss: 0.00118623
Iteration 25/25 | Loss: 0.00118623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76138031
Iteration 2/25 | Loss: 0.00084075
Iteration 3/25 | Loss: 0.00082648
Iteration 4/25 | Loss: 0.00082648
Iteration 5/25 | Loss: 0.00082648
Iteration 6/25 | Loss: 0.00082648
Iteration 7/25 | Loss: 0.00082648
Iteration 8/25 | Loss: 0.00082648
Iteration 9/25 | Loss: 0.00082648
Iteration 10/25 | Loss: 0.00082648
Iteration 11/25 | Loss: 0.00082648
Iteration 12/25 | Loss: 0.00082648
Iteration 13/25 | Loss: 0.00082647
Iteration 14/25 | Loss: 0.00082647
Iteration 15/25 | Loss: 0.00082647
Iteration 16/25 | Loss: 0.00082647
Iteration 17/25 | Loss: 0.00082647
Iteration 18/25 | Loss: 0.00082647
Iteration 19/25 | Loss: 0.00082647
Iteration 20/25 | Loss: 0.00082647
Iteration 21/25 | Loss: 0.00082647
Iteration 22/25 | Loss: 0.00082647
Iteration 23/25 | Loss: 0.00082647
Iteration 24/25 | Loss: 0.00082647
Iteration 25/25 | Loss: 0.00082647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082647
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00008499
Iteration 4/1000 | Loss: 0.00001965
Iteration 5/1000 | Loss: 0.00003897
Iteration 6/1000 | Loss: 0.00003444
Iteration 7/1000 | Loss: 0.00001718
Iteration 8/1000 | Loss: 0.00001652
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00002943
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00091834
Iteration 17/1000 | Loss: 0.00004106
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00004401
Iteration 21/1000 | Loss: 0.00002523
Iteration 22/1000 | Loss: 0.00002116
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001628
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00003863
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00003082
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00003003
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00001156
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001155
Iteration 40/1000 | Loss: 0.00001155
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001151
Iteration 44/1000 | Loss: 0.00001150
Iteration 45/1000 | Loss: 0.00001150
Iteration 46/1000 | Loss: 0.00001150
Iteration 47/1000 | Loss: 0.00001149
Iteration 48/1000 | Loss: 0.00001148
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001145
Iteration 52/1000 | Loss: 0.00001145
Iteration 53/1000 | Loss: 0.00001145
Iteration 54/1000 | Loss: 0.00001145
Iteration 55/1000 | Loss: 0.00001145
Iteration 56/1000 | Loss: 0.00001145
Iteration 57/1000 | Loss: 0.00001145
Iteration 58/1000 | Loss: 0.00001145
Iteration 59/1000 | Loss: 0.00001144
Iteration 60/1000 | Loss: 0.00001144
Iteration 61/1000 | Loss: 0.00001144
Iteration 62/1000 | Loss: 0.00001143
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001138
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001135
Iteration 92/1000 | Loss: 0.00001135
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001134
Iteration 100/1000 | Loss: 0.00001134
Iteration 101/1000 | Loss: 0.00001134
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001133
Iteration 105/1000 | Loss: 0.00001133
Iteration 106/1000 | Loss: 0.00001133
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001133
Iteration 109/1000 | Loss: 0.00001133
Iteration 110/1000 | Loss: 0.00001133
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001132
Iteration 130/1000 | Loss: 0.00001132
Iteration 131/1000 | Loss: 0.00001132
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001131
Iteration 138/1000 | Loss: 0.00001131
Iteration 139/1000 | Loss: 0.00001131
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001130
Iteration 142/1000 | Loss: 0.00001130
Iteration 143/1000 | Loss: 0.00001130
Iteration 144/1000 | Loss: 0.00001130
Iteration 145/1000 | Loss: 0.00001130
Iteration 146/1000 | Loss: 0.00001130
Iteration 147/1000 | Loss: 0.00001130
Iteration 148/1000 | Loss: 0.00001130
Iteration 149/1000 | Loss: 0.00001130
Iteration 150/1000 | Loss: 0.00001130
Iteration 151/1000 | Loss: 0.00001130
Iteration 152/1000 | Loss: 0.00001130
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001128
Iteration 171/1000 | Loss: 0.00001128
Iteration 172/1000 | Loss: 0.00001128
Iteration 173/1000 | Loss: 0.00001128
Iteration 174/1000 | Loss: 0.00001128
Iteration 175/1000 | Loss: 0.00001128
Iteration 176/1000 | Loss: 0.00001128
Iteration 177/1000 | Loss: 0.00001128
Iteration 178/1000 | Loss: 0.00001128
Iteration 179/1000 | Loss: 0.00001128
Iteration 180/1000 | Loss: 0.00001128
Iteration 181/1000 | Loss: 0.00001128
Iteration 182/1000 | Loss: 0.00001128
Iteration 183/1000 | Loss: 0.00001128
Iteration 184/1000 | Loss: 0.00001128
Iteration 185/1000 | Loss: 0.00001128
Iteration 186/1000 | Loss: 0.00001128
Iteration 187/1000 | Loss: 0.00001128
Iteration 188/1000 | Loss: 0.00001128
Iteration 189/1000 | Loss: 0.00001128
Iteration 190/1000 | Loss: 0.00001128
Iteration 191/1000 | Loss: 0.00001128
Iteration 192/1000 | Loss: 0.00001128
Iteration 193/1000 | Loss: 0.00001128
Iteration 194/1000 | Loss: 0.00001128
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1277296835032757e-05, 1.1277296835032757e-05, 1.1277296835032757e-05, 1.1277296835032757e-05, 1.1277296835032757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1277296835032757e-05

Optimization complete. Final v2v error: 2.892853260040283 mm

Highest mean error: 3.35105037689209 mm for frame 105

Lowest mean error: 2.722342014312744 mm for frame 11

Saving results

Total time: 83.20861291885376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405190
Iteration 2/25 | Loss: 0.00123948
Iteration 3/25 | Loss: 0.00118830
Iteration 4/25 | Loss: 0.00117869
Iteration 5/25 | Loss: 0.00117591
Iteration 6/25 | Loss: 0.00117591
Iteration 7/25 | Loss: 0.00117591
Iteration 8/25 | Loss: 0.00117591
Iteration 9/25 | Loss: 0.00117591
Iteration 10/25 | Loss: 0.00117591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011759053450077772, 0.0011759053450077772, 0.0011759053450077772, 0.0011759053450077772, 0.0011759053450077772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011759053450077772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22547388
Iteration 2/25 | Loss: 0.00072165
Iteration 3/25 | Loss: 0.00072164
Iteration 4/25 | Loss: 0.00072164
Iteration 5/25 | Loss: 0.00072164
Iteration 6/25 | Loss: 0.00072164
Iteration 7/25 | Loss: 0.00072164
Iteration 8/25 | Loss: 0.00072164
Iteration 9/25 | Loss: 0.00072164
Iteration 10/25 | Loss: 0.00072164
Iteration 11/25 | Loss: 0.00072164
Iteration 12/25 | Loss: 0.00072164
Iteration 13/25 | Loss: 0.00072164
Iteration 14/25 | Loss: 0.00072164
Iteration 15/25 | Loss: 0.00072164
Iteration 16/25 | Loss: 0.00072164
Iteration 17/25 | Loss: 0.00072164
Iteration 18/25 | Loss: 0.00072164
Iteration 19/25 | Loss: 0.00072164
Iteration 20/25 | Loss: 0.00072164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007216382655315101, 0.0007216382655315101, 0.0007216382655315101, 0.0007216382655315101, 0.0007216382655315101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007216382655315101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072164
Iteration 2/1000 | Loss: 0.00002403
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001361
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001273
Iteration 8/1000 | Loss: 0.00001238
Iteration 9/1000 | Loss: 0.00001223
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001198
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001195
Iteration 16/1000 | Loss: 0.00001195
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001188
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001185
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001184
Iteration 31/1000 | Loss: 0.00001184
Iteration 32/1000 | Loss: 0.00001183
Iteration 33/1000 | Loss: 0.00001183
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001180
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001171
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001168
Iteration 48/1000 | Loss: 0.00001168
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001161
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001160
Iteration 59/1000 | Loss: 0.00001160
Iteration 60/1000 | Loss: 0.00001160
Iteration 61/1000 | Loss: 0.00001160
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001159
Iteration 64/1000 | Loss: 0.00001159
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001155
Iteration 95/1000 | Loss: 0.00001155
Iteration 96/1000 | Loss: 0.00001155
Iteration 97/1000 | Loss: 0.00001154
Iteration 98/1000 | Loss: 0.00001154
Iteration 99/1000 | Loss: 0.00001154
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001150
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001149
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001147
Iteration 120/1000 | Loss: 0.00001147
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001147
Iteration 124/1000 | Loss: 0.00001147
Iteration 125/1000 | Loss: 0.00001147
Iteration 126/1000 | Loss: 0.00001147
Iteration 127/1000 | Loss: 0.00001146
Iteration 128/1000 | Loss: 0.00001146
Iteration 129/1000 | Loss: 0.00001146
Iteration 130/1000 | Loss: 0.00001146
Iteration 131/1000 | Loss: 0.00001146
Iteration 132/1000 | Loss: 0.00001146
Iteration 133/1000 | Loss: 0.00001146
Iteration 134/1000 | Loss: 0.00001146
Iteration 135/1000 | Loss: 0.00001146
Iteration 136/1000 | Loss: 0.00001146
Iteration 137/1000 | Loss: 0.00001146
Iteration 138/1000 | Loss: 0.00001146
Iteration 139/1000 | Loss: 0.00001145
Iteration 140/1000 | Loss: 0.00001145
Iteration 141/1000 | Loss: 0.00001145
Iteration 142/1000 | Loss: 0.00001145
Iteration 143/1000 | Loss: 0.00001145
Iteration 144/1000 | Loss: 0.00001145
Iteration 145/1000 | Loss: 0.00001145
Iteration 146/1000 | Loss: 0.00001145
Iteration 147/1000 | Loss: 0.00001144
Iteration 148/1000 | Loss: 0.00001144
Iteration 149/1000 | Loss: 0.00001144
Iteration 150/1000 | Loss: 0.00001144
Iteration 151/1000 | Loss: 0.00001143
Iteration 152/1000 | Loss: 0.00001143
Iteration 153/1000 | Loss: 0.00001143
Iteration 154/1000 | Loss: 0.00001143
Iteration 155/1000 | Loss: 0.00001142
Iteration 156/1000 | Loss: 0.00001142
Iteration 157/1000 | Loss: 0.00001142
Iteration 158/1000 | Loss: 0.00001142
Iteration 159/1000 | Loss: 0.00001141
Iteration 160/1000 | Loss: 0.00001141
Iteration 161/1000 | Loss: 0.00001140
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001139
Iteration 164/1000 | Loss: 0.00001139
Iteration 165/1000 | Loss: 0.00001139
Iteration 166/1000 | Loss: 0.00001139
Iteration 167/1000 | Loss: 0.00001139
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001138
Iteration 172/1000 | Loss: 0.00001138
Iteration 173/1000 | Loss: 0.00001138
Iteration 174/1000 | Loss: 0.00001138
Iteration 175/1000 | Loss: 0.00001138
Iteration 176/1000 | Loss: 0.00001138
Iteration 177/1000 | Loss: 0.00001138
Iteration 178/1000 | Loss: 0.00001138
Iteration 179/1000 | Loss: 0.00001138
Iteration 180/1000 | Loss: 0.00001138
Iteration 181/1000 | Loss: 0.00001138
Iteration 182/1000 | Loss: 0.00001138
Iteration 183/1000 | Loss: 0.00001138
Iteration 184/1000 | Loss: 0.00001138
Iteration 185/1000 | Loss: 0.00001138
Iteration 186/1000 | Loss: 0.00001138
Iteration 187/1000 | Loss: 0.00001138
Iteration 188/1000 | Loss: 0.00001138
Iteration 189/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1376768270565663e-05, 1.1376768270565663e-05, 1.1376768270565663e-05, 1.1376768270565663e-05, 1.1376768270565663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1376768270565663e-05

Optimization complete. Final v2v error: 2.882601737976074 mm

Highest mean error: 3.3579118251800537 mm for frame 211

Lowest mean error: 2.7655444145202637 mm for frame 76

Saving results

Total time: 43.62275052070618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464809
Iteration 2/25 | Loss: 0.00128059
Iteration 3/25 | Loss: 0.00122552
Iteration 4/25 | Loss: 0.00121705
Iteration 5/25 | Loss: 0.00121462
Iteration 6/25 | Loss: 0.00121448
Iteration 7/25 | Loss: 0.00121448
Iteration 8/25 | Loss: 0.00121448
Iteration 9/25 | Loss: 0.00121448
Iteration 10/25 | Loss: 0.00121448
Iteration 11/25 | Loss: 0.00121448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012144764186814427, 0.0012144764186814427, 0.0012144764186814427, 0.0012144764186814427, 0.0012144764186814427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012144764186814427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40288353
Iteration 2/25 | Loss: 0.00070607
Iteration 3/25 | Loss: 0.00070606
Iteration 4/25 | Loss: 0.00070606
Iteration 5/25 | Loss: 0.00070606
Iteration 6/25 | Loss: 0.00070606
Iteration 7/25 | Loss: 0.00070606
Iteration 8/25 | Loss: 0.00070606
Iteration 9/25 | Loss: 0.00070606
Iteration 10/25 | Loss: 0.00070606
Iteration 11/25 | Loss: 0.00070606
Iteration 12/25 | Loss: 0.00070606
Iteration 13/25 | Loss: 0.00070606
Iteration 14/25 | Loss: 0.00070606
Iteration 15/25 | Loss: 0.00070606
Iteration 16/25 | Loss: 0.00070606
Iteration 17/25 | Loss: 0.00070606
Iteration 18/25 | Loss: 0.00070606
Iteration 19/25 | Loss: 0.00070606
Iteration 20/25 | Loss: 0.00070606
Iteration 21/25 | Loss: 0.00070606
Iteration 22/25 | Loss: 0.00070606
Iteration 23/25 | Loss: 0.00070606
Iteration 24/25 | Loss: 0.00070606
Iteration 25/25 | Loss: 0.00070606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070606
Iteration 2/1000 | Loss: 0.00002910
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001555
Iteration 8/1000 | Loss: 0.00001533
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001451
Iteration 14/1000 | Loss: 0.00001450
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001445
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001445
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00001443
Iteration 22/1000 | Loss: 0.00001443
Iteration 23/1000 | Loss: 0.00001442
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001433
Iteration 27/1000 | Loss: 0.00001433
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001433
Iteration 30/1000 | Loss: 0.00001433
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001433
Iteration 33/1000 | Loss: 0.00001433
Iteration 34/1000 | Loss: 0.00001432
Iteration 35/1000 | Loss: 0.00001432
Iteration 36/1000 | Loss: 0.00001432
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001427
Iteration 45/1000 | Loss: 0.00001427
Iteration 46/1000 | Loss: 0.00001427
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001427
Iteration 49/1000 | Loss: 0.00001427
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001427
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001427
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001426
Iteration 66/1000 | Loss: 0.00001426
Iteration 67/1000 | Loss: 0.00001426
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001426
Iteration 75/1000 | Loss: 0.00001426
Iteration 76/1000 | Loss: 0.00001426
Iteration 77/1000 | Loss: 0.00001426
Iteration 78/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.426396283932263e-05, 1.426396283932263e-05, 1.426396283932263e-05, 1.426396283932263e-05, 1.426396283932263e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.426396283932263e-05

Optimization complete. Final v2v error: 3.23250675201416 mm

Highest mean error: 3.379747152328491 mm for frame 23

Lowest mean error: 3.095215320587158 mm for frame 63

Saving results

Total time: 28.202725410461426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879398
Iteration 2/25 | Loss: 0.00134636
Iteration 3/25 | Loss: 0.00126628
Iteration 4/25 | Loss: 0.00125128
Iteration 5/25 | Loss: 0.00124573
Iteration 6/25 | Loss: 0.00124540
Iteration 7/25 | Loss: 0.00124540
Iteration 8/25 | Loss: 0.00124540
Iteration 9/25 | Loss: 0.00124540
Iteration 10/25 | Loss: 0.00124540
Iteration 11/25 | Loss: 0.00124540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012454031966626644, 0.0012454031966626644, 0.0012454031966626644, 0.0012454031966626644, 0.0012454031966626644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012454031966626644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44618630
Iteration 2/25 | Loss: 0.00083673
Iteration 3/25 | Loss: 0.00083672
Iteration 4/25 | Loss: 0.00083672
Iteration 5/25 | Loss: 0.00083672
Iteration 6/25 | Loss: 0.00083672
Iteration 7/25 | Loss: 0.00083672
Iteration 8/25 | Loss: 0.00083672
Iteration 9/25 | Loss: 0.00083672
Iteration 10/25 | Loss: 0.00083672
Iteration 11/25 | Loss: 0.00083672
Iteration 12/25 | Loss: 0.00083672
Iteration 13/25 | Loss: 0.00083672
Iteration 14/25 | Loss: 0.00083672
Iteration 15/25 | Loss: 0.00083672
Iteration 16/25 | Loss: 0.00083672
Iteration 17/25 | Loss: 0.00083672
Iteration 18/25 | Loss: 0.00083672
Iteration 19/25 | Loss: 0.00083672
Iteration 20/25 | Loss: 0.00083672
Iteration 21/25 | Loss: 0.00083672
Iteration 22/25 | Loss: 0.00083672
Iteration 23/25 | Loss: 0.00083672
Iteration 24/25 | Loss: 0.00083672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000836720981169492, 0.000836720981169492, 0.000836720981169492, 0.000836720981169492, 0.000836720981169492]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000836720981169492

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083672
Iteration 2/1000 | Loss: 0.00003533
Iteration 3/1000 | Loss: 0.00002181
Iteration 4/1000 | Loss: 0.00001926
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001674
Iteration 8/1000 | Loss: 0.00001646
Iteration 9/1000 | Loss: 0.00001619
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001576
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001575
Iteration 18/1000 | Loss: 0.00001571
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001566
Iteration 22/1000 | Loss: 0.00001564
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001560
Iteration 26/1000 | Loss: 0.00001560
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001558
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001557
Iteration 35/1000 | Loss: 0.00001557
Iteration 36/1000 | Loss: 0.00001557
Iteration 37/1000 | Loss: 0.00001556
Iteration 38/1000 | Loss: 0.00001556
Iteration 39/1000 | Loss: 0.00001556
Iteration 40/1000 | Loss: 0.00001556
Iteration 41/1000 | Loss: 0.00001555
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001554
Iteration 46/1000 | Loss: 0.00001554
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001551
Iteration 52/1000 | Loss: 0.00001551
Iteration 53/1000 | Loss: 0.00001551
Iteration 54/1000 | Loss: 0.00001550
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001550
Iteration 57/1000 | Loss: 0.00001549
Iteration 58/1000 | Loss: 0.00001549
Iteration 59/1000 | Loss: 0.00001549
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001548
Iteration 62/1000 | Loss: 0.00001548
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001546
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001544
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001544
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001539
Iteration 80/1000 | Loss: 0.00001539
Iteration 81/1000 | Loss: 0.00001539
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001538
Iteration 84/1000 | Loss: 0.00001538
Iteration 85/1000 | Loss: 0.00001538
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001537
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001537
Iteration 95/1000 | Loss: 0.00001537
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001535
Iteration 103/1000 | Loss: 0.00001535
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001534
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001534
Iteration 115/1000 | Loss: 0.00001534
Iteration 116/1000 | Loss: 0.00001534
Iteration 117/1000 | Loss: 0.00001534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5337654986069538e-05, 1.5337654986069538e-05, 1.5337654986069538e-05, 1.5337654986069538e-05, 1.5337654986069538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5337654986069538e-05

Optimization complete. Final v2v error: 3.3168301582336426 mm

Highest mean error: 3.878378391265869 mm for frame 37

Lowest mean error: 3.0378828048706055 mm for frame 30

Saving results

Total time: 37.526957273483276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775179
Iteration 2/25 | Loss: 0.00155193
Iteration 3/25 | Loss: 0.00130630
Iteration 4/25 | Loss: 0.00127528
Iteration 5/25 | Loss: 0.00127870
Iteration 6/25 | Loss: 0.00126570
Iteration 7/25 | Loss: 0.00125692
Iteration 8/25 | Loss: 0.00125554
Iteration 9/25 | Loss: 0.00125637
Iteration 10/25 | Loss: 0.00125403
Iteration 11/25 | Loss: 0.00125258
Iteration 12/25 | Loss: 0.00125479
Iteration 13/25 | Loss: 0.00125543
Iteration 14/25 | Loss: 0.00125471
Iteration 15/25 | Loss: 0.00125263
Iteration 16/25 | Loss: 0.00125232
Iteration 17/25 | Loss: 0.00125313
Iteration 18/25 | Loss: 0.00125334
Iteration 19/25 | Loss: 0.00125414
Iteration 20/25 | Loss: 0.00125330
Iteration 21/25 | Loss: 0.00125197
Iteration 22/25 | Loss: 0.00125319
Iteration 23/25 | Loss: 0.00125303
Iteration 24/25 | Loss: 0.00125047
Iteration 25/25 | Loss: 0.00125029

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97109675
Iteration 2/25 | Loss: 0.00087262
Iteration 3/25 | Loss: 0.00087261
Iteration 4/25 | Loss: 0.00087261
Iteration 5/25 | Loss: 0.00087261
Iteration 6/25 | Loss: 0.00087261
Iteration 7/25 | Loss: 0.00087261
Iteration 8/25 | Loss: 0.00087261
Iteration 9/25 | Loss: 0.00087261
Iteration 10/25 | Loss: 0.00087261
Iteration 11/25 | Loss: 0.00087261
Iteration 12/25 | Loss: 0.00087261
Iteration 13/25 | Loss: 0.00087261
Iteration 14/25 | Loss: 0.00087261
Iteration 15/25 | Loss: 0.00087261
Iteration 16/25 | Loss: 0.00087261
Iteration 17/25 | Loss: 0.00087261
Iteration 18/25 | Loss: 0.00087261
Iteration 19/25 | Loss: 0.00087261
Iteration 20/25 | Loss: 0.00087261
Iteration 21/25 | Loss: 0.00087261
Iteration 22/25 | Loss: 0.00087261
Iteration 23/25 | Loss: 0.00087261
Iteration 24/25 | Loss: 0.00087261
Iteration 25/25 | Loss: 0.00087261

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087261
Iteration 2/1000 | Loss: 0.00016211
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00013593
Iteration 5/1000 | Loss: 0.00012116
Iteration 6/1000 | Loss: 0.00012963
Iteration 7/1000 | Loss: 0.00002800
Iteration 8/1000 | Loss: 0.00002484
Iteration 9/1000 | Loss: 0.00018174
Iteration 10/1000 | Loss: 0.00015779
Iteration 11/1000 | Loss: 0.00030884
Iteration 12/1000 | Loss: 0.00002587
Iteration 13/1000 | Loss: 0.00002235
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00001980
Iteration 16/1000 | Loss: 0.00001905
Iteration 17/1000 | Loss: 0.00001855
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001724
Iteration 21/1000 | Loss: 0.00001700
Iteration 22/1000 | Loss: 0.00001683
Iteration 23/1000 | Loss: 0.00001676
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001668
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001653
Iteration 46/1000 | Loss: 0.00001653
Iteration 47/1000 | Loss: 0.00001653
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001651
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001650
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001649
Iteration 61/1000 | Loss: 0.00001649
Iteration 62/1000 | Loss: 0.00001649
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001648
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001642
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001641
Iteration 88/1000 | Loss: 0.00001641
Iteration 89/1000 | Loss: 0.00001641
Iteration 90/1000 | Loss: 0.00001641
Iteration 91/1000 | Loss: 0.00001640
Iteration 92/1000 | Loss: 0.00001640
Iteration 93/1000 | Loss: 0.00001640
Iteration 94/1000 | Loss: 0.00001640
Iteration 95/1000 | Loss: 0.00001640
Iteration 96/1000 | Loss: 0.00001639
Iteration 97/1000 | Loss: 0.00001639
Iteration 98/1000 | Loss: 0.00001639
Iteration 99/1000 | Loss: 0.00001639
Iteration 100/1000 | Loss: 0.00001639
Iteration 101/1000 | Loss: 0.00001639
Iteration 102/1000 | Loss: 0.00001639
Iteration 103/1000 | Loss: 0.00001638
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001638
Iteration 107/1000 | Loss: 0.00001638
Iteration 108/1000 | Loss: 0.00001638
Iteration 109/1000 | Loss: 0.00001638
Iteration 110/1000 | Loss: 0.00001638
Iteration 111/1000 | Loss: 0.00001637
Iteration 112/1000 | Loss: 0.00001637
Iteration 113/1000 | Loss: 0.00001637
Iteration 114/1000 | Loss: 0.00001637
Iteration 115/1000 | Loss: 0.00001637
Iteration 116/1000 | Loss: 0.00001637
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001636
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001635
Iteration 126/1000 | Loss: 0.00001635
Iteration 127/1000 | Loss: 0.00001635
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001634
Iteration 132/1000 | Loss: 0.00001634
Iteration 133/1000 | Loss: 0.00001634
Iteration 134/1000 | Loss: 0.00001634
Iteration 135/1000 | Loss: 0.00001634
Iteration 136/1000 | Loss: 0.00001634
Iteration 137/1000 | Loss: 0.00001634
Iteration 138/1000 | Loss: 0.00001634
Iteration 139/1000 | Loss: 0.00001634
Iteration 140/1000 | Loss: 0.00001634
Iteration 141/1000 | Loss: 0.00001634
Iteration 142/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.6341309674317017e-05, 1.6341309674317017e-05, 1.6341309674317017e-05, 1.6341309674317017e-05, 1.6341309674317017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6341309674317017e-05

Optimization complete. Final v2v error: 3.370483875274658 mm

Highest mean error: 4.288922309875488 mm for frame 39

Lowest mean error: 2.9898500442504883 mm for frame 188

Saving results

Total time: 98.27867817878723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989052
Iteration 2/25 | Loss: 0.00184500
Iteration 3/25 | Loss: 0.00152284
Iteration 4/25 | Loss: 0.00140580
Iteration 5/25 | Loss: 0.00140539
Iteration 6/25 | Loss: 0.00135398
Iteration 7/25 | Loss: 0.00135594
Iteration 8/25 | Loss: 0.00136318
Iteration 9/25 | Loss: 0.00133746
Iteration 10/25 | Loss: 0.00133270
Iteration 11/25 | Loss: 0.00132287
Iteration 12/25 | Loss: 0.00131437
Iteration 13/25 | Loss: 0.00129892
Iteration 14/25 | Loss: 0.00128962
Iteration 15/25 | Loss: 0.00128280
Iteration 16/25 | Loss: 0.00127875
Iteration 17/25 | Loss: 0.00128851
Iteration 18/25 | Loss: 0.00128171
Iteration 19/25 | Loss: 0.00128765
Iteration 20/25 | Loss: 0.00127434
Iteration 21/25 | Loss: 0.00125891
Iteration 22/25 | Loss: 0.00125369
Iteration 23/25 | Loss: 0.00124571
Iteration 24/25 | Loss: 0.00124360
Iteration 25/25 | Loss: 0.00124333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42650044
Iteration 2/25 | Loss: 0.00094224
Iteration 3/25 | Loss: 0.00072665
Iteration 4/25 | Loss: 0.00072665
Iteration 5/25 | Loss: 0.00072665
Iteration 6/25 | Loss: 0.00072665
Iteration 7/25 | Loss: 0.00072665
Iteration 8/25 | Loss: 0.00072665
Iteration 9/25 | Loss: 0.00072665
Iteration 10/25 | Loss: 0.00072665
Iteration 11/25 | Loss: 0.00072665
Iteration 12/25 | Loss: 0.00072665
Iteration 13/25 | Loss: 0.00072665
Iteration 14/25 | Loss: 0.00072665
Iteration 15/25 | Loss: 0.00072665
Iteration 16/25 | Loss: 0.00072665
Iteration 17/25 | Loss: 0.00072665
Iteration 18/25 | Loss: 0.00072665
Iteration 19/25 | Loss: 0.00072665
Iteration 20/25 | Loss: 0.00072665
Iteration 21/25 | Loss: 0.00072665
Iteration 22/25 | Loss: 0.00072665
Iteration 23/25 | Loss: 0.00072665
Iteration 24/25 | Loss: 0.00072665
Iteration 25/25 | Loss: 0.00072665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072665
Iteration 2/1000 | Loss: 0.00027027
Iteration 3/1000 | Loss: 0.00035322
Iteration 4/1000 | Loss: 0.00002236
Iteration 5/1000 | Loss: 0.00004833
Iteration 6/1000 | Loss: 0.00002826
Iteration 7/1000 | Loss: 0.00004660
Iteration 8/1000 | Loss: 0.00003022
Iteration 9/1000 | Loss: 0.00003093
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00005793
Iteration 12/1000 | Loss: 0.00002997
Iteration 13/1000 | Loss: 0.00008705
Iteration 14/1000 | Loss: 0.00001771
Iteration 15/1000 | Loss: 0.00002016
Iteration 16/1000 | Loss: 0.00002538
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00003646
Iteration 20/1000 | Loss: 0.00005730
Iteration 21/1000 | Loss: 0.00004211
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00002344
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001578
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001578
Iteration 37/1000 | Loss: 0.00001578
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001578
Iteration 40/1000 | Loss: 0.00001578
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001576
Iteration 66/1000 | Loss: 0.00001576
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001575
Iteration 70/1000 | Loss: 0.00001575
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001574
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001570
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001566
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00003302
Iteration 95/1000 | Loss: 0.00001590
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001736
Iteration 104/1000 | Loss: 0.00003871
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00002784
Iteration 107/1000 | Loss: 0.00001707
Iteration 108/1000 | Loss: 0.00005327
Iteration 109/1000 | Loss: 0.00024076
Iteration 110/1000 | Loss: 0.00001585
Iteration 111/1000 | Loss: 0.00006184
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00003719
Iteration 114/1000 | Loss: 0.00001548
Iteration 115/1000 | Loss: 0.00001548
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001547
Iteration 139/1000 | Loss: 0.00001547
Iteration 140/1000 | Loss: 0.00001547
Iteration 141/1000 | Loss: 0.00001547
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001546
Iteration 146/1000 | Loss: 0.00001546
Iteration 147/1000 | Loss: 0.00001546
Iteration 148/1000 | Loss: 0.00001546
Iteration 149/1000 | Loss: 0.00001546
Iteration 150/1000 | Loss: 0.00001546
Iteration 151/1000 | Loss: 0.00001546
Iteration 152/1000 | Loss: 0.00001546
Iteration 153/1000 | Loss: 0.00001545
Iteration 154/1000 | Loss: 0.00001545
Iteration 155/1000 | Loss: 0.00001545
Iteration 156/1000 | Loss: 0.00001545
Iteration 157/1000 | Loss: 0.00001545
Iteration 158/1000 | Loss: 0.00001545
Iteration 159/1000 | Loss: 0.00001545
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001545
Iteration 163/1000 | Loss: 0.00001545
Iteration 164/1000 | Loss: 0.00001545
Iteration 165/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.545303712191526e-05, 1.545303712191526e-05, 1.545303712191526e-05, 1.545303712191526e-05, 1.545303712191526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.545303712191526e-05

Optimization complete. Final v2v error: 3.315810203552246 mm

Highest mean error: 4.420615196228027 mm for frame 68

Lowest mean error: 3.00974178314209 mm for frame 36

Saving results

Total time: 97.51558566093445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520877
Iteration 2/25 | Loss: 0.00145989
Iteration 3/25 | Loss: 0.00129304
Iteration 4/25 | Loss: 0.00127994
Iteration 5/25 | Loss: 0.00127659
Iteration 6/25 | Loss: 0.00127528
Iteration 7/25 | Loss: 0.00127528
Iteration 8/25 | Loss: 0.00127528
Iteration 9/25 | Loss: 0.00127528
Iteration 10/25 | Loss: 0.00127528
Iteration 11/25 | Loss: 0.00127528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012752803741022944, 0.0012752803741022944, 0.0012752803741022944, 0.0012752803741022944, 0.0012752803741022944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012752803741022944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44939291
Iteration 2/25 | Loss: 0.00085908
Iteration 3/25 | Loss: 0.00085908
Iteration 4/25 | Loss: 0.00085908
Iteration 5/25 | Loss: 0.00085908
Iteration 6/25 | Loss: 0.00085908
Iteration 7/25 | Loss: 0.00085907
Iteration 8/25 | Loss: 0.00085907
Iteration 9/25 | Loss: 0.00085907
Iteration 10/25 | Loss: 0.00085907
Iteration 11/25 | Loss: 0.00085907
Iteration 12/25 | Loss: 0.00085907
Iteration 13/25 | Loss: 0.00085907
Iteration 14/25 | Loss: 0.00085907
Iteration 15/25 | Loss: 0.00085907
Iteration 16/25 | Loss: 0.00085907
Iteration 17/25 | Loss: 0.00085907
Iteration 18/25 | Loss: 0.00085907
Iteration 19/25 | Loss: 0.00085907
Iteration 20/25 | Loss: 0.00085907
Iteration 21/25 | Loss: 0.00085907
Iteration 22/25 | Loss: 0.00085907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008590730139985681, 0.0008590730139985681, 0.0008590730139985681, 0.0008590730139985681, 0.0008590730139985681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008590730139985681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085907
Iteration 2/1000 | Loss: 0.00004593
Iteration 3/1000 | Loss: 0.00003401
Iteration 4/1000 | Loss: 0.00003024
Iteration 5/1000 | Loss: 0.00002849
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002641
Iteration 8/1000 | Loss: 0.00002576
Iteration 9/1000 | Loss: 0.00002536
Iteration 10/1000 | Loss: 0.00002484
Iteration 11/1000 | Loss: 0.00002450
Iteration 12/1000 | Loss: 0.00002427
Iteration 13/1000 | Loss: 0.00002409
Iteration 14/1000 | Loss: 0.00002396
Iteration 15/1000 | Loss: 0.00002392
Iteration 16/1000 | Loss: 0.00002389
Iteration 17/1000 | Loss: 0.00002383
Iteration 18/1000 | Loss: 0.00002378
Iteration 19/1000 | Loss: 0.00002377
Iteration 20/1000 | Loss: 0.00002374
Iteration 21/1000 | Loss: 0.00002373
Iteration 22/1000 | Loss: 0.00002373
Iteration 23/1000 | Loss: 0.00002373
Iteration 24/1000 | Loss: 0.00002372
Iteration 25/1000 | Loss: 0.00002372
Iteration 26/1000 | Loss: 0.00002372
Iteration 27/1000 | Loss: 0.00002371
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002370
Iteration 30/1000 | Loss: 0.00002369
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00002369
Iteration 36/1000 | Loss: 0.00002369
Iteration 37/1000 | Loss: 0.00002369
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002368
Iteration 42/1000 | Loss: 0.00002367
Iteration 43/1000 | Loss: 0.00002367
Iteration 44/1000 | Loss: 0.00002366
Iteration 45/1000 | Loss: 0.00002366
Iteration 46/1000 | Loss: 0.00002366
Iteration 47/1000 | Loss: 0.00002366
Iteration 48/1000 | Loss: 0.00002365
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00002364
Iteration 51/1000 | Loss: 0.00002363
Iteration 52/1000 | Loss: 0.00002363
Iteration 53/1000 | Loss: 0.00002363
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002361
Iteration 56/1000 | Loss: 0.00002361
Iteration 57/1000 | Loss: 0.00002361
Iteration 58/1000 | Loss: 0.00002360
Iteration 59/1000 | Loss: 0.00002360
Iteration 60/1000 | Loss: 0.00002360
Iteration 61/1000 | Loss: 0.00002359
Iteration 62/1000 | Loss: 0.00002359
Iteration 63/1000 | Loss: 0.00002358
Iteration 64/1000 | Loss: 0.00002358
Iteration 65/1000 | Loss: 0.00002358
Iteration 66/1000 | Loss: 0.00002357
Iteration 67/1000 | Loss: 0.00002357
Iteration 68/1000 | Loss: 0.00002357
Iteration 69/1000 | Loss: 0.00002357
Iteration 70/1000 | Loss: 0.00002356
Iteration 71/1000 | Loss: 0.00002356
Iteration 72/1000 | Loss: 0.00002356
Iteration 73/1000 | Loss: 0.00002356
Iteration 74/1000 | Loss: 0.00002356
Iteration 75/1000 | Loss: 0.00002356
Iteration 76/1000 | Loss: 0.00002356
Iteration 77/1000 | Loss: 0.00002356
Iteration 78/1000 | Loss: 0.00002355
Iteration 79/1000 | Loss: 0.00002355
Iteration 80/1000 | Loss: 0.00002355
Iteration 81/1000 | Loss: 0.00002354
Iteration 82/1000 | Loss: 0.00002354
Iteration 83/1000 | Loss: 0.00002354
Iteration 84/1000 | Loss: 0.00002354
Iteration 85/1000 | Loss: 0.00002354
Iteration 86/1000 | Loss: 0.00002354
Iteration 87/1000 | Loss: 0.00002354
Iteration 88/1000 | Loss: 0.00002353
Iteration 89/1000 | Loss: 0.00002353
Iteration 90/1000 | Loss: 0.00002353
Iteration 91/1000 | Loss: 0.00002353
Iteration 92/1000 | Loss: 0.00002352
Iteration 93/1000 | Loss: 0.00002352
Iteration 94/1000 | Loss: 0.00002352
Iteration 95/1000 | Loss: 0.00002352
Iteration 96/1000 | Loss: 0.00002352
Iteration 97/1000 | Loss: 0.00002352
Iteration 98/1000 | Loss: 0.00002352
Iteration 99/1000 | Loss: 0.00002352
Iteration 100/1000 | Loss: 0.00002352
Iteration 101/1000 | Loss: 0.00002351
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002351
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002351
Iteration 106/1000 | Loss: 0.00002351
Iteration 107/1000 | Loss: 0.00002350
Iteration 108/1000 | Loss: 0.00002350
Iteration 109/1000 | Loss: 0.00002350
Iteration 110/1000 | Loss: 0.00002350
Iteration 111/1000 | Loss: 0.00002350
Iteration 112/1000 | Loss: 0.00002349
Iteration 113/1000 | Loss: 0.00002349
Iteration 114/1000 | Loss: 0.00002349
Iteration 115/1000 | Loss: 0.00002349
Iteration 116/1000 | Loss: 0.00002349
Iteration 117/1000 | Loss: 0.00002349
Iteration 118/1000 | Loss: 0.00002349
Iteration 119/1000 | Loss: 0.00002349
Iteration 120/1000 | Loss: 0.00002348
Iteration 121/1000 | Loss: 0.00002348
Iteration 122/1000 | Loss: 0.00002348
Iteration 123/1000 | Loss: 0.00002348
Iteration 124/1000 | Loss: 0.00002348
Iteration 125/1000 | Loss: 0.00002348
Iteration 126/1000 | Loss: 0.00002348
Iteration 127/1000 | Loss: 0.00002348
Iteration 128/1000 | Loss: 0.00002347
Iteration 129/1000 | Loss: 0.00002347
Iteration 130/1000 | Loss: 0.00002347
Iteration 131/1000 | Loss: 0.00002347
Iteration 132/1000 | Loss: 0.00002347
Iteration 133/1000 | Loss: 0.00002347
Iteration 134/1000 | Loss: 0.00002347
Iteration 135/1000 | Loss: 0.00002347
Iteration 136/1000 | Loss: 0.00002346
Iteration 137/1000 | Loss: 0.00002346
Iteration 138/1000 | Loss: 0.00002346
Iteration 139/1000 | Loss: 0.00002346
Iteration 140/1000 | Loss: 0.00002346
Iteration 141/1000 | Loss: 0.00002346
Iteration 142/1000 | Loss: 0.00002346
Iteration 143/1000 | Loss: 0.00002346
Iteration 144/1000 | Loss: 0.00002346
Iteration 145/1000 | Loss: 0.00002346
Iteration 146/1000 | Loss: 0.00002346
Iteration 147/1000 | Loss: 0.00002346
Iteration 148/1000 | Loss: 0.00002346
Iteration 149/1000 | Loss: 0.00002346
Iteration 150/1000 | Loss: 0.00002346
Iteration 151/1000 | Loss: 0.00002346
Iteration 152/1000 | Loss: 0.00002346
Iteration 153/1000 | Loss: 0.00002346
Iteration 154/1000 | Loss: 0.00002346
Iteration 155/1000 | Loss: 0.00002346
Iteration 156/1000 | Loss: 0.00002345
Iteration 157/1000 | Loss: 0.00002345
Iteration 158/1000 | Loss: 0.00002345
Iteration 159/1000 | Loss: 0.00002345
Iteration 160/1000 | Loss: 0.00002345
Iteration 161/1000 | Loss: 0.00002345
Iteration 162/1000 | Loss: 0.00002345
Iteration 163/1000 | Loss: 0.00002345
Iteration 164/1000 | Loss: 0.00002345
Iteration 165/1000 | Loss: 0.00002345
Iteration 166/1000 | Loss: 0.00002345
Iteration 167/1000 | Loss: 0.00002345
Iteration 168/1000 | Loss: 0.00002345
Iteration 169/1000 | Loss: 0.00002345
Iteration 170/1000 | Loss: 0.00002344
Iteration 171/1000 | Loss: 0.00002344
Iteration 172/1000 | Loss: 0.00002344
Iteration 173/1000 | Loss: 0.00002344
Iteration 174/1000 | Loss: 0.00002344
Iteration 175/1000 | Loss: 0.00002344
Iteration 176/1000 | Loss: 0.00002344
Iteration 177/1000 | Loss: 0.00002344
Iteration 178/1000 | Loss: 0.00002344
Iteration 179/1000 | Loss: 0.00002344
Iteration 180/1000 | Loss: 0.00002344
Iteration 181/1000 | Loss: 0.00002344
Iteration 182/1000 | Loss: 0.00002343
Iteration 183/1000 | Loss: 0.00002343
Iteration 184/1000 | Loss: 0.00002343
Iteration 185/1000 | Loss: 0.00002343
Iteration 186/1000 | Loss: 0.00002343
Iteration 187/1000 | Loss: 0.00002343
Iteration 188/1000 | Loss: 0.00002343
Iteration 189/1000 | Loss: 0.00002343
Iteration 190/1000 | Loss: 0.00002343
Iteration 191/1000 | Loss: 0.00002343
Iteration 192/1000 | Loss: 0.00002343
Iteration 193/1000 | Loss: 0.00002343
Iteration 194/1000 | Loss: 0.00002343
Iteration 195/1000 | Loss: 0.00002342
Iteration 196/1000 | Loss: 0.00002342
Iteration 197/1000 | Loss: 0.00002342
Iteration 198/1000 | Loss: 0.00002342
Iteration 199/1000 | Loss: 0.00002342
Iteration 200/1000 | Loss: 0.00002342
Iteration 201/1000 | Loss: 0.00002341
Iteration 202/1000 | Loss: 0.00002341
Iteration 203/1000 | Loss: 0.00002341
Iteration 204/1000 | Loss: 0.00002341
Iteration 205/1000 | Loss: 0.00002341
Iteration 206/1000 | Loss: 0.00002341
Iteration 207/1000 | Loss: 0.00002341
Iteration 208/1000 | Loss: 0.00002341
Iteration 209/1000 | Loss: 0.00002341
Iteration 210/1000 | Loss: 0.00002341
Iteration 211/1000 | Loss: 0.00002341
Iteration 212/1000 | Loss: 0.00002341
Iteration 213/1000 | Loss: 0.00002341
Iteration 214/1000 | Loss: 0.00002341
Iteration 215/1000 | Loss: 0.00002341
Iteration 216/1000 | Loss: 0.00002341
Iteration 217/1000 | Loss: 0.00002341
Iteration 218/1000 | Loss: 0.00002341
Iteration 219/1000 | Loss: 0.00002341
Iteration 220/1000 | Loss: 0.00002341
Iteration 221/1000 | Loss: 0.00002341
Iteration 222/1000 | Loss: 0.00002341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.340709397685714e-05, 2.340709397685714e-05, 2.340709397685714e-05, 2.340709397685714e-05, 2.340709397685714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.340709397685714e-05

Optimization complete. Final v2v error: 3.8626651763916016 mm

Highest mean error: 4.473576068878174 mm for frame 11

Lowest mean error: 3.0378074645996094 mm for frame 63

Saving results

Total time: 45.94995880126953
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801403
Iteration 2/25 | Loss: 0.00139353
Iteration 3/25 | Loss: 0.00126870
Iteration 4/25 | Loss: 0.00125586
Iteration 5/25 | Loss: 0.00126207
Iteration 6/25 | Loss: 0.00123790
Iteration 7/25 | Loss: 0.00123665
Iteration 8/25 | Loss: 0.00123646
Iteration 9/25 | Loss: 0.00123206
Iteration 10/25 | Loss: 0.00123077
Iteration 11/25 | Loss: 0.00123052
Iteration 12/25 | Loss: 0.00123046
Iteration 13/25 | Loss: 0.00123046
Iteration 14/25 | Loss: 0.00123046
Iteration 15/25 | Loss: 0.00123046
Iteration 16/25 | Loss: 0.00123046
Iteration 17/25 | Loss: 0.00123046
Iteration 18/25 | Loss: 0.00123045
Iteration 19/25 | Loss: 0.00123045
Iteration 20/25 | Loss: 0.00123045
Iteration 21/25 | Loss: 0.00123045
Iteration 22/25 | Loss: 0.00123045
Iteration 23/25 | Loss: 0.00123045
Iteration 24/25 | Loss: 0.00123045
Iteration 25/25 | Loss: 0.00123045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53243685
Iteration 2/25 | Loss: 0.00083576
Iteration 3/25 | Loss: 0.00083575
Iteration 4/25 | Loss: 0.00083575
Iteration 5/25 | Loss: 0.00083575
Iteration 6/25 | Loss: 0.00083575
Iteration 7/25 | Loss: 0.00083575
Iteration 8/25 | Loss: 0.00083575
Iteration 9/25 | Loss: 0.00083575
Iteration 10/25 | Loss: 0.00083575
Iteration 11/25 | Loss: 0.00083575
Iteration 12/25 | Loss: 0.00083575
Iteration 13/25 | Loss: 0.00083575
Iteration 14/25 | Loss: 0.00083575
Iteration 15/25 | Loss: 0.00083575
Iteration 16/25 | Loss: 0.00083575
Iteration 17/25 | Loss: 0.00083575
Iteration 18/25 | Loss: 0.00083575
Iteration 19/25 | Loss: 0.00083575
Iteration 20/25 | Loss: 0.00083575
Iteration 21/25 | Loss: 0.00083575
Iteration 22/25 | Loss: 0.00083575
Iteration 23/25 | Loss: 0.00083575
Iteration 24/25 | Loss: 0.00083575
Iteration 25/25 | Loss: 0.00083575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083575
Iteration 2/1000 | Loss: 0.00003158
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001675
Iteration 5/1000 | Loss: 0.00001578
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001378
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001352
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001336
Iteration 18/1000 | Loss: 0.00001335
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001333
Iteration 21/1000 | Loss: 0.00001332
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001320
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001317
Iteration 28/1000 | Loss: 0.00001317
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001316
Iteration 31/1000 | Loss: 0.00001316
Iteration 32/1000 | Loss: 0.00001315
Iteration 33/1000 | Loss: 0.00001315
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001311
Iteration 37/1000 | Loss: 0.00001310
Iteration 38/1000 | Loss: 0.00001310
Iteration 39/1000 | Loss: 0.00001309
Iteration 40/1000 | Loss: 0.00001309
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001308
Iteration 44/1000 | Loss: 0.00001308
Iteration 45/1000 | Loss: 0.00001308
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001307
Iteration 57/1000 | Loss: 0.00001307
Iteration 58/1000 | Loss: 0.00001307
Iteration 59/1000 | Loss: 0.00001307
Iteration 60/1000 | Loss: 0.00001306
Iteration 61/1000 | Loss: 0.00001306
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001304
Iteration 66/1000 | Loss: 0.00001303
Iteration 67/1000 | Loss: 0.00001303
Iteration 68/1000 | Loss: 0.00001303
Iteration 69/1000 | Loss: 0.00001303
Iteration 70/1000 | Loss: 0.00001303
Iteration 71/1000 | Loss: 0.00001302
Iteration 72/1000 | Loss: 0.00001302
Iteration 73/1000 | Loss: 0.00001302
Iteration 74/1000 | Loss: 0.00001301
Iteration 75/1000 | Loss: 0.00001301
Iteration 76/1000 | Loss: 0.00001301
Iteration 77/1000 | Loss: 0.00001301
Iteration 78/1000 | Loss: 0.00001301
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001300
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001300
Iteration 85/1000 | Loss: 0.00001300
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001296
Iteration 101/1000 | Loss: 0.00001296
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001295
Iteration 105/1000 | Loss: 0.00001295
Iteration 106/1000 | Loss: 0.00001295
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001293
Iteration 113/1000 | Loss: 0.00001293
Iteration 114/1000 | Loss: 0.00001293
Iteration 115/1000 | Loss: 0.00001292
Iteration 116/1000 | Loss: 0.00001292
Iteration 117/1000 | Loss: 0.00001292
Iteration 118/1000 | Loss: 0.00001291
Iteration 119/1000 | Loss: 0.00001291
Iteration 120/1000 | Loss: 0.00001290
Iteration 121/1000 | Loss: 0.00001290
Iteration 122/1000 | Loss: 0.00001290
Iteration 123/1000 | Loss: 0.00001290
Iteration 124/1000 | Loss: 0.00001290
Iteration 125/1000 | Loss: 0.00001290
Iteration 126/1000 | Loss: 0.00001290
Iteration 127/1000 | Loss: 0.00001290
Iteration 128/1000 | Loss: 0.00001290
Iteration 129/1000 | Loss: 0.00001289
Iteration 130/1000 | Loss: 0.00001289
Iteration 131/1000 | Loss: 0.00001289
Iteration 132/1000 | Loss: 0.00001289
Iteration 133/1000 | Loss: 0.00001289
Iteration 134/1000 | Loss: 0.00001289
Iteration 135/1000 | Loss: 0.00001289
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001288
Iteration 140/1000 | Loss: 0.00001288
Iteration 141/1000 | Loss: 0.00001288
Iteration 142/1000 | Loss: 0.00001288
Iteration 143/1000 | Loss: 0.00001288
Iteration 144/1000 | Loss: 0.00001288
Iteration 145/1000 | Loss: 0.00001288
Iteration 146/1000 | Loss: 0.00001288
Iteration 147/1000 | Loss: 0.00001288
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001288
Iteration 157/1000 | Loss: 0.00001288
Iteration 158/1000 | Loss: 0.00001288
Iteration 159/1000 | Loss: 0.00001288
Iteration 160/1000 | Loss: 0.00001288
Iteration 161/1000 | Loss: 0.00001288
Iteration 162/1000 | Loss: 0.00001288
Iteration 163/1000 | Loss: 0.00001288
Iteration 164/1000 | Loss: 0.00001288
Iteration 165/1000 | Loss: 0.00001288
Iteration 166/1000 | Loss: 0.00001288
Iteration 167/1000 | Loss: 0.00001288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.2876313121523708e-05, 1.2876313121523708e-05, 1.2876313121523708e-05, 1.2876313121523708e-05, 1.2876313121523708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2876313121523708e-05

Optimization complete. Final v2v error: 3.007821798324585 mm

Highest mean error: 3.8023765087127686 mm for frame 49

Lowest mean error: 2.658142566680908 mm for frame 5

Saving results

Total time: 47.7517147064209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780906
Iteration 2/25 | Loss: 0.00127518
Iteration 3/25 | Loss: 0.00120383
Iteration 4/25 | Loss: 0.00119526
Iteration 5/25 | Loss: 0.00119333
Iteration 6/25 | Loss: 0.00119333
Iteration 7/25 | Loss: 0.00119333
Iteration 8/25 | Loss: 0.00119333
Iteration 9/25 | Loss: 0.00119333
Iteration 10/25 | Loss: 0.00119333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011933344649150968, 0.0011933344649150968, 0.0011933344649150968, 0.0011933344649150968, 0.0011933344649150968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011933344649150968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44055796
Iteration 2/25 | Loss: 0.00070322
Iteration 3/25 | Loss: 0.00070322
Iteration 4/25 | Loss: 0.00070322
Iteration 5/25 | Loss: 0.00070322
Iteration 6/25 | Loss: 0.00070322
Iteration 7/25 | Loss: 0.00070322
Iteration 8/25 | Loss: 0.00070322
Iteration 9/25 | Loss: 0.00070322
Iteration 10/25 | Loss: 0.00070322
Iteration 11/25 | Loss: 0.00070322
Iteration 12/25 | Loss: 0.00070322
Iteration 13/25 | Loss: 0.00070322
Iteration 14/25 | Loss: 0.00070322
Iteration 15/25 | Loss: 0.00070322
Iteration 16/25 | Loss: 0.00070322
Iteration 17/25 | Loss: 0.00070322
Iteration 18/25 | Loss: 0.00070322
Iteration 19/25 | Loss: 0.00070322
Iteration 20/25 | Loss: 0.00070322
Iteration 21/25 | Loss: 0.00070322
Iteration 22/25 | Loss: 0.00070322
Iteration 23/25 | Loss: 0.00070322
Iteration 24/25 | Loss: 0.00070322
Iteration 25/25 | Loss: 0.00070322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070322
Iteration 2/1000 | Loss: 0.00002691
Iteration 3/1000 | Loss: 0.00001776
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001257
Iteration 8/1000 | Loss: 0.00001230
Iteration 9/1000 | Loss: 0.00001202
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001168
Iteration 15/1000 | Loss: 0.00001162
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001157
Iteration 18/1000 | Loss: 0.00001156
Iteration 19/1000 | Loss: 0.00001156
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001145
Iteration 42/1000 | Loss: 0.00001145
Iteration 43/1000 | Loss: 0.00001144
Iteration 44/1000 | Loss: 0.00001144
Iteration 45/1000 | Loss: 0.00001144
Iteration 46/1000 | Loss: 0.00001143
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001131
Iteration 72/1000 | Loss: 0.00001130
Iteration 73/1000 | Loss: 0.00001130
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001124
Iteration 92/1000 | Loss: 0.00001124
Iteration 93/1000 | Loss: 0.00001124
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001123
Iteration 101/1000 | Loss: 0.00001123
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001121
Iteration 108/1000 | Loss: 0.00001121
Iteration 109/1000 | Loss: 0.00001121
Iteration 110/1000 | Loss: 0.00001121
Iteration 111/1000 | Loss: 0.00001121
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001120
Iteration 114/1000 | Loss: 0.00001120
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001119
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00001119
Iteration 119/1000 | Loss: 0.00001119
Iteration 120/1000 | Loss: 0.00001118
Iteration 121/1000 | Loss: 0.00001118
Iteration 122/1000 | Loss: 0.00001118
Iteration 123/1000 | Loss: 0.00001118
Iteration 124/1000 | Loss: 0.00001118
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001113
Iteration 149/1000 | Loss: 0.00001113
Iteration 150/1000 | Loss: 0.00001113
Iteration 151/1000 | Loss: 0.00001113
Iteration 152/1000 | Loss: 0.00001113
Iteration 153/1000 | Loss: 0.00001112
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001112
Iteration 156/1000 | Loss: 0.00001112
Iteration 157/1000 | Loss: 0.00001112
Iteration 158/1000 | Loss: 0.00001112
Iteration 159/1000 | Loss: 0.00001112
Iteration 160/1000 | Loss: 0.00001112
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Iteration 164/1000 | Loss: 0.00001112
Iteration 165/1000 | Loss: 0.00001112
Iteration 166/1000 | Loss: 0.00001112
Iteration 167/1000 | Loss: 0.00001112
Iteration 168/1000 | Loss: 0.00001111
Iteration 169/1000 | Loss: 0.00001111
Iteration 170/1000 | Loss: 0.00001111
Iteration 171/1000 | Loss: 0.00001111
Iteration 172/1000 | Loss: 0.00001111
Iteration 173/1000 | Loss: 0.00001111
Iteration 174/1000 | Loss: 0.00001111
Iteration 175/1000 | Loss: 0.00001111
Iteration 176/1000 | Loss: 0.00001111
Iteration 177/1000 | Loss: 0.00001111
Iteration 178/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.1113680557173211e-05, 1.1113680557173211e-05, 1.1113680557173211e-05, 1.1113680557173211e-05, 1.1113680557173211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1113680557173211e-05

Optimization complete. Final v2v error: 2.844886064529419 mm

Highest mean error: 3.0600509643554688 mm for frame 76

Lowest mean error: 2.6995387077331543 mm for frame 192

Saving results

Total time: 38.745601177215576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822350
Iteration 2/25 | Loss: 0.00140209
Iteration 3/25 | Loss: 0.00128655
Iteration 4/25 | Loss: 0.00127380
Iteration 5/25 | Loss: 0.00126960
Iteration 6/25 | Loss: 0.00126892
Iteration 7/25 | Loss: 0.00126892
Iteration 8/25 | Loss: 0.00126892
Iteration 9/25 | Loss: 0.00126892
Iteration 10/25 | Loss: 0.00126892
Iteration 11/25 | Loss: 0.00126892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001268921885639429, 0.001268921885639429, 0.001268921885639429, 0.001268921885639429, 0.001268921885639429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268921885639429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75316441
Iteration 2/25 | Loss: 0.00090190
Iteration 3/25 | Loss: 0.00090190
Iteration 4/25 | Loss: 0.00090190
Iteration 5/25 | Loss: 0.00090189
Iteration 6/25 | Loss: 0.00090189
Iteration 7/25 | Loss: 0.00090189
Iteration 8/25 | Loss: 0.00090189
Iteration 9/25 | Loss: 0.00090189
Iteration 10/25 | Loss: 0.00090189
Iteration 11/25 | Loss: 0.00090189
Iteration 12/25 | Loss: 0.00090189
Iteration 13/25 | Loss: 0.00090189
Iteration 14/25 | Loss: 0.00090189
Iteration 15/25 | Loss: 0.00090189
Iteration 16/25 | Loss: 0.00090189
Iteration 17/25 | Loss: 0.00090189
Iteration 18/25 | Loss: 0.00090189
Iteration 19/25 | Loss: 0.00090189
Iteration 20/25 | Loss: 0.00090189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009018924320116639, 0.0009018924320116639, 0.0009018924320116639, 0.0009018924320116639, 0.0009018924320116639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009018924320116639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090189
Iteration 2/1000 | Loss: 0.00003087
Iteration 3/1000 | Loss: 0.00002313
Iteration 4/1000 | Loss: 0.00002112
Iteration 5/1000 | Loss: 0.00002002
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001882
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001827
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001790
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001764
Iteration 15/1000 | Loss: 0.00001762
Iteration 16/1000 | Loss: 0.00001761
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001757
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001756
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001754
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001752
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00001751
Iteration 38/1000 | Loss: 0.00001751
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001745
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001742
Iteration 47/1000 | Loss: 0.00001742
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001741
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001740
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001737
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001736
Iteration 62/1000 | Loss: 0.00001736
Iteration 63/1000 | Loss: 0.00001736
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001726
Iteration 102/1000 | Loss: 0.00001726
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001726
Iteration 109/1000 | Loss: 0.00001726
Iteration 110/1000 | Loss: 0.00001726
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001724
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.7244003174710087e-05, 1.7244003174710087e-05, 1.7244003174710087e-05, 1.7244003174710087e-05, 1.7244003174710087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7244003174710087e-05

Optimization complete. Final v2v error: 3.3700339794158936 mm

Highest mean error: 4.872793197631836 mm for frame 166

Lowest mean error: 2.8663244247436523 mm for frame 27

Saving results

Total time: 40.35240721702576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432740
Iteration 2/25 | Loss: 0.00141093
Iteration 3/25 | Loss: 0.00125850
Iteration 4/25 | Loss: 0.00123794
Iteration 5/25 | Loss: 0.00123189
Iteration 6/25 | Loss: 0.00123129
Iteration 7/25 | Loss: 0.00123129
Iteration 8/25 | Loss: 0.00123116
Iteration 9/25 | Loss: 0.00123116
Iteration 10/25 | Loss: 0.00123116
Iteration 11/25 | Loss: 0.00123116
Iteration 12/25 | Loss: 0.00123116
Iteration 13/25 | Loss: 0.00123116
Iteration 14/25 | Loss: 0.00123116
Iteration 15/25 | Loss: 0.00123116
Iteration 16/25 | Loss: 0.00123116
Iteration 17/25 | Loss: 0.00123116
Iteration 18/25 | Loss: 0.00123116
Iteration 19/25 | Loss: 0.00123116
Iteration 20/25 | Loss: 0.00123116
Iteration 21/25 | Loss: 0.00123116
Iteration 22/25 | Loss: 0.00123116
Iteration 23/25 | Loss: 0.00123116
Iteration 24/25 | Loss: 0.00123116
Iteration 25/25 | Loss: 0.00123116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48231065
Iteration 2/25 | Loss: 0.00087297
Iteration 3/25 | Loss: 0.00087297
Iteration 4/25 | Loss: 0.00087297
Iteration 5/25 | Loss: 0.00087297
Iteration 6/25 | Loss: 0.00087297
Iteration 7/25 | Loss: 0.00087297
Iteration 8/25 | Loss: 0.00087297
Iteration 9/25 | Loss: 0.00087297
Iteration 10/25 | Loss: 0.00087297
Iteration 11/25 | Loss: 0.00087297
Iteration 12/25 | Loss: 0.00087297
Iteration 13/25 | Loss: 0.00087297
Iteration 14/25 | Loss: 0.00087297
Iteration 15/25 | Loss: 0.00087297
Iteration 16/25 | Loss: 0.00087297
Iteration 17/25 | Loss: 0.00087297
Iteration 18/25 | Loss: 0.00087297
Iteration 19/25 | Loss: 0.00087297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008729667752049863, 0.0008729667752049863, 0.0008729667752049863, 0.0008729667752049863, 0.0008729667752049863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008729667752049863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087297
Iteration 2/1000 | Loss: 0.00003574
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002195
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00001980
Iteration 7/1000 | Loss: 0.00001908
Iteration 8/1000 | Loss: 0.00001867
Iteration 9/1000 | Loss: 0.00001830
Iteration 10/1000 | Loss: 0.00001791
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001760
Iteration 13/1000 | Loss: 0.00001758
Iteration 14/1000 | Loss: 0.00001745
Iteration 15/1000 | Loss: 0.00001744
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001733
Iteration 25/1000 | Loss: 0.00001732
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001726
Iteration 32/1000 | Loss: 0.00001725
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001707
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001706
Iteration 56/1000 | Loss: 0.00001706
Iteration 57/1000 | Loss: 0.00001706
Iteration 58/1000 | Loss: 0.00001704
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001703
Iteration 61/1000 | Loss: 0.00001703
Iteration 62/1000 | Loss: 0.00001703
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001700
Iteration 72/1000 | Loss: 0.00001700
Iteration 73/1000 | Loss: 0.00001700
Iteration 74/1000 | Loss: 0.00001700
Iteration 75/1000 | Loss: 0.00001700
Iteration 76/1000 | Loss: 0.00001700
Iteration 77/1000 | Loss: 0.00001700
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001700
Iteration 82/1000 | Loss: 0.00001700
Iteration 83/1000 | Loss: 0.00001699
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001699
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001696
Iteration 99/1000 | Loss: 0.00001696
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001696
Iteration 104/1000 | Loss: 0.00001695
Iteration 105/1000 | Loss: 0.00001695
Iteration 106/1000 | Loss: 0.00001695
Iteration 107/1000 | Loss: 0.00001695
Iteration 108/1000 | Loss: 0.00001695
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001693
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001691
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001682
Iteration 174/1000 | Loss: 0.00001682
Iteration 175/1000 | Loss: 0.00001682
Iteration 176/1000 | Loss: 0.00001682
Iteration 177/1000 | Loss: 0.00001682
Iteration 178/1000 | Loss: 0.00001682
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00001682
Iteration 185/1000 | Loss: 0.00001681
Iteration 186/1000 | Loss: 0.00001681
Iteration 187/1000 | Loss: 0.00001681
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001681
Iteration 192/1000 | Loss: 0.00001681
Iteration 193/1000 | Loss: 0.00001681
Iteration 194/1000 | Loss: 0.00001681
Iteration 195/1000 | Loss: 0.00001681
Iteration 196/1000 | Loss: 0.00001681
Iteration 197/1000 | Loss: 0.00001681
Iteration 198/1000 | Loss: 0.00001681
Iteration 199/1000 | Loss: 0.00001681
Iteration 200/1000 | Loss: 0.00001681
Iteration 201/1000 | Loss: 0.00001681
Iteration 202/1000 | Loss: 0.00001681
Iteration 203/1000 | Loss: 0.00001681
Iteration 204/1000 | Loss: 0.00001681
Iteration 205/1000 | Loss: 0.00001681
Iteration 206/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.6814719856483862e-05, 1.6814719856483862e-05, 1.6814719856483862e-05, 1.6814719856483862e-05, 1.6814719856483862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6814719856483862e-05

Optimization complete. Final v2v error: 3.4579241275787354 mm

Highest mean error: 4.081974983215332 mm for frame 15

Lowest mean error: 2.9625580310821533 mm for frame 168

Saving results

Total time: 47.695860385894775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688653
Iteration 2/25 | Loss: 0.00135386
Iteration 3/25 | Loss: 0.00123508
Iteration 4/25 | Loss: 0.00121176
Iteration 5/25 | Loss: 0.00120463
Iteration 6/25 | Loss: 0.00120345
Iteration 7/25 | Loss: 0.00120345
Iteration 8/25 | Loss: 0.00120345
Iteration 9/25 | Loss: 0.00120345
Iteration 10/25 | Loss: 0.00120345
Iteration 11/25 | Loss: 0.00120345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012034516548737884, 0.0012034516548737884, 0.0012034516548737884, 0.0012034516548737884, 0.0012034516548737884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012034516548737884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.07648373
Iteration 2/25 | Loss: 0.00073275
Iteration 3/25 | Loss: 0.00073275
Iteration 4/25 | Loss: 0.00073275
Iteration 5/25 | Loss: 0.00073275
Iteration 6/25 | Loss: 0.00073275
Iteration 7/25 | Loss: 0.00073275
Iteration 8/25 | Loss: 0.00073275
Iteration 9/25 | Loss: 0.00073275
Iteration 10/25 | Loss: 0.00073275
Iteration 11/25 | Loss: 0.00073275
Iteration 12/25 | Loss: 0.00073275
Iteration 13/25 | Loss: 0.00073275
Iteration 14/25 | Loss: 0.00073275
Iteration 15/25 | Loss: 0.00073275
Iteration 16/25 | Loss: 0.00073275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007327497587539256, 0.0007327497587539256, 0.0007327497587539256, 0.0007327497587539256, 0.0007327497587539256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007327497587539256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073275
Iteration 2/1000 | Loss: 0.00003292
Iteration 3/1000 | Loss: 0.00002325
Iteration 4/1000 | Loss: 0.00002080
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001869
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001747
Iteration 9/1000 | Loss: 0.00001713
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001595
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00001574
Iteration 25/1000 | Loss: 0.00001574
Iteration 26/1000 | Loss: 0.00001573
Iteration 27/1000 | Loss: 0.00001572
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001572
Iteration 30/1000 | Loss: 0.00001571
Iteration 31/1000 | Loss: 0.00001571
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001570
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001566
Iteration 41/1000 | Loss: 0.00001566
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001563
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001562
Iteration 50/1000 | Loss: 0.00001562
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001561
Iteration 56/1000 | Loss: 0.00001560
Iteration 57/1000 | Loss: 0.00001560
Iteration 58/1000 | Loss: 0.00001560
Iteration 59/1000 | Loss: 0.00001559
Iteration 60/1000 | Loss: 0.00001559
Iteration 61/1000 | Loss: 0.00001559
Iteration 62/1000 | Loss: 0.00001559
Iteration 63/1000 | Loss: 0.00001558
Iteration 64/1000 | Loss: 0.00001558
Iteration 65/1000 | Loss: 0.00001557
Iteration 66/1000 | Loss: 0.00001557
Iteration 67/1000 | Loss: 0.00001557
Iteration 68/1000 | Loss: 0.00001557
Iteration 69/1000 | Loss: 0.00001557
Iteration 70/1000 | Loss: 0.00001557
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001557
Iteration 73/1000 | Loss: 0.00001557
Iteration 74/1000 | Loss: 0.00001557
Iteration 75/1000 | Loss: 0.00001557
Iteration 76/1000 | Loss: 0.00001557
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001555
Iteration 83/1000 | Loss: 0.00001555
Iteration 84/1000 | Loss: 0.00001554
Iteration 85/1000 | Loss: 0.00001554
Iteration 86/1000 | Loss: 0.00001554
Iteration 87/1000 | Loss: 0.00001554
Iteration 88/1000 | Loss: 0.00001554
Iteration 89/1000 | Loss: 0.00001554
Iteration 90/1000 | Loss: 0.00001554
Iteration 91/1000 | Loss: 0.00001554
Iteration 92/1000 | Loss: 0.00001554
Iteration 93/1000 | Loss: 0.00001554
Iteration 94/1000 | Loss: 0.00001554
Iteration 95/1000 | Loss: 0.00001554
Iteration 96/1000 | Loss: 0.00001554
Iteration 97/1000 | Loss: 0.00001553
Iteration 98/1000 | Loss: 0.00001553
Iteration 99/1000 | Loss: 0.00001553
Iteration 100/1000 | Loss: 0.00001553
Iteration 101/1000 | Loss: 0.00001553
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001552
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001552
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001551
Iteration 121/1000 | Loss: 0.00001551
Iteration 122/1000 | Loss: 0.00001551
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001551
Iteration 127/1000 | Loss: 0.00001551
Iteration 128/1000 | Loss: 0.00001551
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001551
Iteration 131/1000 | Loss: 0.00001551
Iteration 132/1000 | Loss: 0.00001551
Iteration 133/1000 | Loss: 0.00001551
Iteration 134/1000 | Loss: 0.00001551
Iteration 135/1000 | Loss: 0.00001551
Iteration 136/1000 | Loss: 0.00001551
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001550
Iteration 139/1000 | Loss: 0.00001550
Iteration 140/1000 | Loss: 0.00001550
Iteration 141/1000 | Loss: 0.00001550
Iteration 142/1000 | Loss: 0.00001550
Iteration 143/1000 | Loss: 0.00001550
Iteration 144/1000 | Loss: 0.00001550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5504818293266e-05, 1.5504818293266e-05, 1.5504818293266e-05, 1.5504818293266e-05, 1.5504818293266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5504818293266e-05

Optimization complete. Final v2v error: 3.3692188262939453 mm

Highest mean error: 3.6947803497314453 mm for frame 70

Lowest mean error: 3.067694902420044 mm for frame 43

Saving results

Total time: 44.830376386642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471267
Iteration 2/25 | Loss: 0.00141546
Iteration 3/25 | Loss: 0.00127680
Iteration 4/25 | Loss: 0.00126362
Iteration 5/25 | Loss: 0.00125960
Iteration 6/25 | Loss: 0.00125946
Iteration 7/25 | Loss: 0.00125946
Iteration 8/25 | Loss: 0.00125946
Iteration 9/25 | Loss: 0.00125946
Iteration 10/25 | Loss: 0.00125946
Iteration 11/25 | Loss: 0.00125946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012594558065757155, 0.0012594558065757155, 0.0012594558065757155, 0.0012594558065757155, 0.0012594558065757155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012594558065757155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45706844
Iteration 2/25 | Loss: 0.00074027
Iteration 3/25 | Loss: 0.00074027
Iteration 4/25 | Loss: 0.00074027
Iteration 5/25 | Loss: 0.00074027
Iteration 6/25 | Loss: 0.00074027
Iteration 7/25 | Loss: 0.00074027
Iteration 8/25 | Loss: 0.00074027
Iteration 9/25 | Loss: 0.00074027
Iteration 10/25 | Loss: 0.00074027
Iteration 11/25 | Loss: 0.00074027
Iteration 12/25 | Loss: 0.00074027
Iteration 13/25 | Loss: 0.00074027
Iteration 14/25 | Loss: 0.00074027
Iteration 15/25 | Loss: 0.00074027
Iteration 16/25 | Loss: 0.00074027
Iteration 17/25 | Loss: 0.00074027
Iteration 18/25 | Loss: 0.00074027
Iteration 19/25 | Loss: 0.00074027
Iteration 20/25 | Loss: 0.00074027
Iteration 21/25 | Loss: 0.00074027
Iteration 22/25 | Loss: 0.00074027
Iteration 23/25 | Loss: 0.00074027
Iteration 24/25 | Loss: 0.00074027
Iteration 25/25 | Loss: 0.00074027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074027
Iteration 2/1000 | Loss: 0.00003548
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002396
Iteration 5/1000 | Loss: 0.00002302
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002119
Iteration 8/1000 | Loss: 0.00002068
Iteration 9/1000 | Loss: 0.00002031
Iteration 10/1000 | Loss: 0.00002004
Iteration 11/1000 | Loss: 0.00002000
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001983
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001961
Iteration 17/1000 | Loss: 0.00001961
Iteration 18/1000 | Loss: 0.00001960
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001958
Iteration 21/1000 | Loss: 0.00001957
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001956
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001952
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001950
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001948
Iteration 34/1000 | Loss: 0.00001948
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001944
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001944
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001939
Iteration 53/1000 | Loss: 0.00001931
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001926
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001925
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001924
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001924
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001920
Iteration 71/1000 | Loss: 0.00001920
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001920
Iteration 76/1000 | Loss: 0.00001919
Iteration 77/1000 | Loss: 0.00001919
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001919
Iteration 80/1000 | Loss: 0.00001919
Iteration 81/1000 | Loss: 0.00001918
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001917
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001917
Iteration 89/1000 | Loss: 0.00001917
Iteration 90/1000 | Loss: 0.00001917
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001915
Iteration 102/1000 | Loss: 0.00001915
Iteration 103/1000 | Loss: 0.00001915
Iteration 104/1000 | Loss: 0.00001915
Iteration 105/1000 | Loss: 0.00001915
Iteration 106/1000 | Loss: 0.00001915
Iteration 107/1000 | Loss: 0.00001915
Iteration 108/1000 | Loss: 0.00001915
Iteration 109/1000 | Loss: 0.00001915
Iteration 110/1000 | Loss: 0.00001915
Iteration 111/1000 | Loss: 0.00001914
Iteration 112/1000 | Loss: 0.00001914
Iteration 113/1000 | Loss: 0.00001914
Iteration 114/1000 | Loss: 0.00001914
Iteration 115/1000 | Loss: 0.00001914
Iteration 116/1000 | Loss: 0.00001914
Iteration 117/1000 | Loss: 0.00001914
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001913
Iteration 123/1000 | Loss: 0.00001913
Iteration 124/1000 | Loss: 0.00001913
Iteration 125/1000 | Loss: 0.00001912
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001912
Iteration 128/1000 | Loss: 0.00001912
Iteration 129/1000 | Loss: 0.00001912
Iteration 130/1000 | Loss: 0.00001912
Iteration 131/1000 | Loss: 0.00001912
Iteration 132/1000 | Loss: 0.00001912
Iteration 133/1000 | Loss: 0.00001912
Iteration 134/1000 | Loss: 0.00001912
Iteration 135/1000 | Loss: 0.00001912
Iteration 136/1000 | Loss: 0.00001912
Iteration 137/1000 | Loss: 0.00001911
Iteration 138/1000 | Loss: 0.00001911
Iteration 139/1000 | Loss: 0.00001911
Iteration 140/1000 | Loss: 0.00001911
Iteration 141/1000 | Loss: 0.00001911
Iteration 142/1000 | Loss: 0.00001911
Iteration 143/1000 | Loss: 0.00001911
Iteration 144/1000 | Loss: 0.00001911
Iteration 145/1000 | Loss: 0.00001910
Iteration 146/1000 | Loss: 0.00001910
Iteration 147/1000 | Loss: 0.00001910
Iteration 148/1000 | Loss: 0.00001910
Iteration 149/1000 | Loss: 0.00001910
Iteration 150/1000 | Loss: 0.00001910
Iteration 151/1000 | Loss: 0.00001910
Iteration 152/1000 | Loss: 0.00001910
Iteration 153/1000 | Loss: 0.00001910
Iteration 154/1000 | Loss: 0.00001910
Iteration 155/1000 | Loss: 0.00001910
Iteration 156/1000 | Loss: 0.00001910
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001908
Iteration 162/1000 | Loss: 0.00001908
Iteration 163/1000 | Loss: 0.00001908
Iteration 164/1000 | Loss: 0.00001908
Iteration 165/1000 | Loss: 0.00001908
Iteration 166/1000 | Loss: 0.00001908
Iteration 167/1000 | Loss: 0.00001908
Iteration 168/1000 | Loss: 0.00001908
Iteration 169/1000 | Loss: 0.00001908
Iteration 170/1000 | Loss: 0.00001907
Iteration 171/1000 | Loss: 0.00001907
Iteration 172/1000 | Loss: 0.00001907
Iteration 173/1000 | Loss: 0.00001907
Iteration 174/1000 | Loss: 0.00001907
Iteration 175/1000 | Loss: 0.00001907
Iteration 176/1000 | Loss: 0.00001907
Iteration 177/1000 | Loss: 0.00001907
Iteration 178/1000 | Loss: 0.00001907
Iteration 179/1000 | Loss: 0.00001907
Iteration 180/1000 | Loss: 0.00001907
Iteration 181/1000 | Loss: 0.00001907
Iteration 182/1000 | Loss: 0.00001907
Iteration 183/1000 | Loss: 0.00001907
Iteration 184/1000 | Loss: 0.00001907
Iteration 185/1000 | Loss: 0.00001907
Iteration 186/1000 | Loss: 0.00001907
Iteration 187/1000 | Loss: 0.00001907
Iteration 188/1000 | Loss: 0.00001907
Iteration 189/1000 | Loss: 0.00001907
Iteration 190/1000 | Loss: 0.00001907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.9065100786974654e-05, 1.9065100786974654e-05, 1.9065100786974654e-05, 1.9065100786974654e-05, 1.9065100786974654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9065100786974654e-05

Optimization complete. Final v2v error: 3.6671595573425293 mm

Highest mean error: 4.024845600128174 mm for frame 15

Lowest mean error: 3.206770181655884 mm for frame 191

Saving results

Total time: 46.00177454948425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490054
Iteration 2/25 | Loss: 0.00136488
Iteration 3/25 | Loss: 0.00128972
Iteration 4/25 | Loss: 0.00127830
Iteration 5/25 | Loss: 0.00127376
Iteration 6/25 | Loss: 0.00127322
Iteration 7/25 | Loss: 0.00127322
Iteration 8/25 | Loss: 0.00127322
Iteration 9/25 | Loss: 0.00127322
Iteration 10/25 | Loss: 0.00127322
Iteration 11/25 | Loss: 0.00127322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012732192408293486, 0.0012732192408293486, 0.0012732192408293486, 0.0012732192408293486, 0.0012732192408293486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012732192408293486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50361025
Iteration 2/25 | Loss: 0.00084918
Iteration 3/25 | Loss: 0.00084916
Iteration 4/25 | Loss: 0.00084916
Iteration 5/25 | Loss: 0.00084916
Iteration 6/25 | Loss: 0.00084916
Iteration 7/25 | Loss: 0.00084916
Iteration 8/25 | Loss: 0.00084916
Iteration 9/25 | Loss: 0.00084916
Iteration 10/25 | Loss: 0.00084916
Iteration 11/25 | Loss: 0.00084916
Iteration 12/25 | Loss: 0.00084916
Iteration 13/25 | Loss: 0.00084916
Iteration 14/25 | Loss: 0.00084916
Iteration 15/25 | Loss: 0.00084916
Iteration 16/25 | Loss: 0.00084916
Iteration 17/25 | Loss: 0.00084916
Iteration 18/25 | Loss: 0.00084916
Iteration 19/25 | Loss: 0.00084916
Iteration 20/25 | Loss: 0.00084916
Iteration 21/25 | Loss: 0.00084916
Iteration 22/25 | Loss: 0.00084916
Iteration 23/25 | Loss: 0.00084916
Iteration 24/25 | Loss: 0.00084916
Iteration 25/25 | Loss: 0.00084916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084916
Iteration 2/1000 | Loss: 0.00004150
Iteration 3/1000 | Loss: 0.00002334
Iteration 4/1000 | Loss: 0.00001974
Iteration 5/1000 | Loss: 0.00001856
Iteration 6/1000 | Loss: 0.00001773
Iteration 7/1000 | Loss: 0.00001720
Iteration 8/1000 | Loss: 0.00001696
Iteration 9/1000 | Loss: 0.00001669
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001640
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001622
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001613
Iteration 21/1000 | Loss: 0.00001612
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001610
Iteration 24/1000 | Loss: 0.00001610
Iteration 25/1000 | Loss: 0.00001609
Iteration 26/1000 | Loss: 0.00001609
Iteration 27/1000 | Loss: 0.00001609
Iteration 28/1000 | Loss: 0.00001609
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001605
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001602
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00001602
Iteration 37/1000 | Loss: 0.00001601
Iteration 38/1000 | Loss: 0.00001601
Iteration 39/1000 | Loss: 0.00001600
Iteration 40/1000 | Loss: 0.00001599
Iteration 41/1000 | Loss: 0.00001598
Iteration 42/1000 | Loss: 0.00001598
Iteration 43/1000 | Loss: 0.00001598
Iteration 44/1000 | Loss: 0.00001598
Iteration 45/1000 | Loss: 0.00001597
Iteration 46/1000 | Loss: 0.00001597
Iteration 47/1000 | Loss: 0.00001597
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001597
Iteration 51/1000 | Loss: 0.00001597
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001596
Iteration 54/1000 | Loss: 0.00001595
Iteration 55/1000 | Loss: 0.00001595
Iteration 56/1000 | Loss: 0.00001594
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001593
Iteration 59/1000 | Loss: 0.00001592
Iteration 60/1000 | Loss: 0.00001592
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001591
Iteration 63/1000 | Loss: 0.00001591
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001585
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001578
Iteration 70/1000 | Loss: 0.00001578
Iteration 71/1000 | Loss: 0.00001577
Iteration 72/1000 | Loss: 0.00001574
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001567
Iteration 75/1000 | Loss: 0.00001567
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001566
Iteration 79/1000 | Loss: 0.00001566
Iteration 80/1000 | Loss: 0.00001566
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001565
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001562
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001561
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001561
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001559
Iteration 130/1000 | Loss: 0.00001559
Iteration 131/1000 | Loss: 0.00001559
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001557
Iteration 148/1000 | Loss: 0.00001557
Iteration 149/1000 | Loss: 0.00001557
Iteration 150/1000 | Loss: 0.00001557
Iteration 151/1000 | Loss: 0.00001557
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001557
Iteration 158/1000 | Loss: 0.00001557
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001556
Iteration 166/1000 | Loss: 0.00001556
Iteration 167/1000 | Loss: 0.00001556
Iteration 168/1000 | Loss: 0.00001556
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Iteration 173/1000 | Loss: 0.00001555
Iteration 174/1000 | Loss: 0.00001555
Iteration 175/1000 | Loss: 0.00001555
Iteration 176/1000 | Loss: 0.00001555
Iteration 177/1000 | Loss: 0.00001555
Iteration 178/1000 | Loss: 0.00001554
Iteration 179/1000 | Loss: 0.00001554
Iteration 180/1000 | Loss: 0.00001554
Iteration 181/1000 | Loss: 0.00001554
Iteration 182/1000 | Loss: 0.00001554
Iteration 183/1000 | Loss: 0.00001554
Iteration 184/1000 | Loss: 0.00001554
Iteration 185/1000 | Loss: 0.00001554
Iteration 186/1000 | Loss: 0.00001554
Iteration 187/1000 | Loss: 0.00001554
Iteration 188/1000 | Loss: 0.00001554
Iteration 189/1000 | Loss: 0.00001554
Iteration 190/1000 | Loss: 0.00001553
Iteration 191/1000 | Loss: 0.00001553
Iteration 192/1000 | Loss: 0.00001553
Iteration 193/1000 | Loss: 0.00001553
Iteration 194/1000 | Loss: 0.00001553
Iteration 195/1000 | Loss: 0.00001553
Iteration 196/1000 | Loss: 0.00001553
Iteration 197/1000 | Loss: 0.00001553
Iteration 198/1000 | Loss: 0.00001553
Iteration 199/1000 | Loss: 0.00001553
Iteration 200/1000 | Loss: 0.00001553
Iteration 201/1000 | Loss: 0.00001553
Iteration 202/1000 | Loss: 0.00001553
Iteration 203/1000 | Loss: 0.00001553
Iteration 204/1000 | Loss: 0.00001553
Iteration 205/1000 | Loss: 0.00001553
Iteration 206/1000 | Loss: 0.00001553
Iteration 207/1000 | Loss: 0.00001553
Iteration 208/1000 | Loss: 0.00001553
Iteration 209/1000 | Loss: 0.00001553
Iteration 210/1000 | Loss: 0.00001553
Iteration 211/1000 | Loss: 0.00001552
Iteration 212/1000 | Loss: 0.00001552
Iteration 213/1000 | Loss: 0.00001552
Iteration 214/1000 | Loss: 0.00001552
Iteration 215/1000 | Loss: 0.00001552
Iteration 216/1000 | Loss: 0.00001552
Iteration 217/1000 | Loss: 0.00001552
Iteration 218/1000 | Loss: 0.00001552
Iteration 219/1000 | Loss: 0.00001552
Iteration 220/1000 | Loss: 0.00001552
Iteration 221/1000 | Loss: 0.00001552
Iteration 222/1000 | Loss: 0.00001552
Iteration 223/1000 | Loss: 0.00001552
Iteration 224/1000 | Loss: 0.00001552
Iteration 225/1000 | Loss: 0.00001552
Iteration 226/1000 | Loss: 0.00001552
Iteration 227/1000 | Loss: 0.00001551
Iteration 228/1000 | Loss: 0.00001551
Iteration 229/1000 | Loss: 0.00001551
Iteration 230/1000 | Loss: 0.00001551
Iteration 231/1000 | Loss: 0.00001551
Iteration 232/1000 | Loss: 0.00001551
Iteration 233/1000 | Loss: 0.00001551
Iteration 234/1000 | Loss: 0.00001551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.5513456673943438e-05, 1.5513456673943438e-05, 1.5513456673943438e-05, 1.5513456673943438e-05, 1.5513456673943438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5513456673943438e-05

Optimization complete. Final v2v error: 3.3223299980163574 mm

Highest mean error: 3.795948028564453 mm for frame 105

Lowest mean error: 3.107393980026245 mm for frame 64

Saving results

Total time: 48.9539897441864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_023/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_023/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00727465
Iteration 2/25 | Loss: 0.00134139
Iteration 3/25 | Loss: 0.00125386
Iteration 4/25 | Loss: 0.00124540
Iteration 5/25 | Loss: 0.00124376
Iteration 6/25 | Loss: 0.00124376
Iteration 7/25 | Loss: 0.00124376
Iteration 8/25 | Loss: 0.00124376
Iteration 9/25 | Loss: 0.00124376
Iteration 10/25 | Loss: 0.00124376
Iteration 11/25 | Loss: 0.00124376
Iteration 12/25 | Loss: 0.00124376
Iteration 13/25 | Loss: 0.00124376
Iteration 14/25 | Loss: 0.00124376
Iteration 15/25 | Loss: 0.00124376
Iteration 16/25 | Loss: 0.00124376
Iteration 17/25 | Loss: 0.00124376
Iteration 18/25 | Loss: 0.00124376
Iteration 19/25 | Loss: 0.00124376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012437583645805717, 0.0012437583645805717, 0.0012437583645805717, 0.0012437583645805717, 0.0012437583645805717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012437583645805717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43495774
Iteration 2/25 | Loss: 0.00063362
Iteration 3/25 | Loss: 0.00063356
Iteration 4/25 | Loss: 0.00063356
Iteration 5/25 | Loss: 0.00063356
Iteration 6/25 | Loss: 0.00063356
Iteration 7/25 | Loss: 0.00063356
Iteration 8/25 | Loss: 0.00063356
Iteration 9/25 | Loss: 0.00063356
Iteration 10/25 | Loss: 0.00063356
Iteration 11/25 | Loss: 0.00063356
Iteration 12/25 | Loss: 0.00063356
Iteration 13/25 | Loss: 0.00063356
Iteration 14/25 | Loss: 0.00063356
Iteration 15/25 | Loss: 0.00063356
Iteration 16/25 | Loss: 0.00063356
Iteration 17/25 | Loss: 0.00063356
Iteration 18/25 | Loss: 0.00063356
Iteration 19/25 | Loss: 0.00063356
Iteration 20/25 | Loss: 0.00063356
Iteration 21/25 | Loss: 0.00063356
Iteration 22/25 | Loss: 0.00063356
Iteration 23/25 | Loss: 0.00063356
Iteration 24/25 | Loss: 0.00063356
Iteration 25/25 | Loss: 0.00063356

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063356
Iteration 2/1000 | Loss: 0.00003373
Iteration 3/1000 | Loss: 0.00002202
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001732
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001516
Iteration 9/1000 | Loss: 0.00001482
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001385
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001373
Iteration 16/1000 | Loss: 0.00001369
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001344
Iteration 24/1000 | Loss: 0.00001344
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001344
Iteration 27/1000 | Loss: 0.00001344
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001343
Iteration 30/1000 | Loss: 0.00001343
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001339
Iteration 34/1000 | Loss: 0.00001339
Iteration 35/1000 | Loss: 0.00001339
Iteration 36/1000 | Loss: 0.00001338
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001336
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001334
Iteration 41/1000 | Loss: 0.00001334
Iteration 42/1000 | Loss: 0.00001334
Iteration 43/1000 | Loss: 0.00001334
Iteration 44/1000 | Loss: 0.00001334
Iteration 45/1000 | Loss: 0.00001333
Iteration 46/1000 | Loss: 0.00001333
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001333
Iteration 50/1000 | Loss: 0.00001332
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001328
Iteration 69/1000 | Loss: 0.00001328
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001325
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001324
Iteration 80/1000 | Loss: 0.00001324
Iteration 81/1000 | Loss: 0.00001324
Iteration 82/1000 | Loss: 0.00001324
Iteration 83/1000 | Loss: 0.00001324
Iteration 84/1000 | Loss: 0.00001324
Iteration 85/1000 | Loss: 0.00001324
Iteration 86/1000 | Loss: 0.00001323
Iteration 87/1000 | Loss: 0.00001323
Iteration 88/1000 | Loss: 0.00001323
Iteration 89/1000 | Loss: 0.00001323
Iteration 90/1000 | Loss: 0.00001322
Iteration 91/1000 | Loss: 0.00001322
Iteration 92/1000 | Loss: 0.00001322
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001321
Iteration 95/1000 | Loss: 0.00001321
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001315
Iteration 111/1000 | Loss: 0.00001315
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001314
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001314
Iteration 117/1000 | Loss: 0.00001314
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.313602024310967e-05, 1.313602024310967e-05, 1.313602024310967e-05, 1.313602024310967e-05, 1.313602024310967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.313602024310967e-05

Optimization complete. Final v2v error: 3.093783140182495 mm

Highest mean error: 3.3385140895843506 mm for frame 73

Lowest mean error: 2.915966749191284 mm for frame 225

Saving results

Total time: 44.04711961746216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422245
Iteration 2/25 | Loss: 0.00110282
Iteration 3/25 | Loss: 0.00096510
Iteration 4/25 | Loss: 0.00094991
Iteration 5/25 | Loss: 0.00094491
Iteration 6/25 | Loss: 0.00094342
Iteration 7/25 | Loss: 0.00094304
Iteration 8/25 | Loss: 0.00094300
Iteration 9/25 | Loss: 0.00094300
Iteration 10/25 | Loss: 0.00094300
Iteration 11/25 | Loss: 0.00094300
Iteration 12/25 | Loss: 0.00094300
Iteration 13/25 | Loss: 0.00094300
Iteration 14/25 | Loss: 0.00094300
Iteration 15/25 | Loss: 0.00094300
Iteration 16/25 | Loss: 0.00094300
Iteration 17/25 | Loss: 0.00094300
Iteration 18/25 | Loss: 0.00094300
Iteration 19/25 | Loss: 0.00094300
Iteration 20/25 | Loss: 0.00094300
Iteration 21/25 | Loss: 0.00094300
Iteration 22/25 | Loss: 0.00094300
Iteration 23/25 | Loss: 0.00094300
Iteration 24/25 | Loss: 0.00094300
Iteration 25/25 | Loss: 0.00094300

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46532917
Iteration 2/25 | Loss: 0.00057727
Iteration 3/25 | Loss: 0.00057727
Iteration 4/25 | Loss: 0.00057727
Iteration 5/25 | Loss: 0.00057727
Iteration 6/25 | Loss: 0.00057727
Iteration 7/25 | Loss: 0.00057727
Iteration 8/25 | Loss: 0.00057727
Iteration 9/25 | Loss: 0.00057727
Iteration 10/25 | Loss: 0.00057727
Iteration 11/25 | Loss: 0.00057727
Iteration 12/25 | Loss: 0.00057727
Iteration 13/25 | Loss: 0.00057727
Iteration 14/25 | Loss: 0.00057727
Iteration 15/25 | Loss: 0.00057727
Iteration 16/25 | Loss: 0.00057727
Iteration 17/25 | Loss: 0.00057727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005772682488895953, 0.0005772682488895953, 0.0005772682488895953, 0.0005772682488895953, 0.0005772682488895953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005772682488895953

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057727
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00001648
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001238
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001117
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001080
Iteration 13/1000 | Loss: 0.00001079
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001076
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001069
Iteration 23/1000 | Loss: 0.00001066
Iteration 24/1000 | Loss: 0.00001065
Iteration 25/1000 | Loss: 0.00001065
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001058
Iteration 30/1000 | Loss: 0.00001057
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001055
Iteration 33/1000 | Loss: 0.00001054
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001050
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001049
Iteration 39/1000 | Loss: 0.00001049
Iteration 40/1000 | Loss: 0.00001049
Iteration 41/1000 | Loss: 0.00001049
Iteration 42/1000 | Loss: 0.00001048
Iteration 43/1000 | Loss: 0.00001048
Iteration 44/1000 | Loss: 0.00001048
Iteration 45/1000 | Loss: 0.00001048
Iteration 46/1000 | Loss: 0.00001047
Iteration 47/1000 | Loss: 0.00001047
Iteration 48/1000 | Loss: 0.00001046
Iteration 49/1000 | Loss: 0.00001046
Iteration 50/1000 | Loss: 0.00001046
Iteration 51/1000 | Loss: 0.00001046
Iteration 52/1000 | Loss: 0.00001046
Iteration 53/1000 | Loss: 0.00001046
Iteration 54/1000 | Loss: 0.00001046
Iteration 55/1000 | Loss: 0.00001046
Iteration 56/1000 | Loss: 0.00001046
Iteration 57/1000 | Loss: 0.00001046
Iteration 58/1000 | Loss: 0.00001046
Iteration 59/1000 | Loss: 0.00001046
Iteration 60/1000 | Loss: 0.00001046
Iteration 61/1000 | Loss: 0.00001046
Iteration 62/1000 | Loss: 0.00001045
Iteration 63/1000 | Loss: 0.00001045
Iteration 64/1000 | Loss: 0.00001045
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00001044
Iteration 67/1000 | Loss: 0.00001044
Iteration 68/1000 | Loss: 0.00001044
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001043
Iteration 71/1000 | Loss: 0.00001043
Iteration 72/1000 | Loss: 0.00001043
Iteration 73/1000 | Loss: 0.00001043
Iteration 74/1000 | Loss: 0.00001043
Iteration 75/1000 | Loss: 0.00001043
Iteration 76/1000 | Loss: 0.00001043
Iteration 77/1000 | Loss: 0.00001043
Iteration 78/1000 | Loss: 0.00001042
Iteration 79/1000 | Loss: 0.00001042
Iteration 80/1000 | Loss: 0.00001042
Iteration 81/1000 | Loss: 0.00001041
Iteration 82/1000 | Loss: 0.00001041
Iteration 83/1000 | Loss: 0.00001041
Iteration 84/1000 | Loss: 0.00001041
Iteration 85/1000 | Loss: 0.00001041
Iteration 86/1000 | Loss: 0.00001041
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Iteration 89/1000 | Loss: 0.00001040
Iteration 90/1000 | Loss: 0.00001040
Iteration 91/1000 | Loss: 0.00001040
Iteration 92/1000 | Loss: 0.00001039
Iteration 93/1000 | Loss: 0.00001039
Iteration 94/1000 | Loss: 0.00001039
Iteration 95/1000 | Loss: 0.00001039
Iteration 96/1000 | Loss: 0.00001039
Iteration 97/1000 | Loss: 0.00001038
Iteration 98/1000 | Loss: 0.00001038
Iteration 99/1000 | Loss: 0.00001038
Iteration 100/1000 | Loss: 0.00001038
Iteration 101/1000 | Loss: 0.00001038
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001037
Iteration 109/1000 | Loss: 0.00001037
Iteration 110/1000 | Loss: 0.00001037
Iteration 111/1000 | Loss: 0.00001037
Iteration 112/1000 | Loss: 0.00001037
Iteration 113/1000 | Loss: 0.00001037
Iteration 114/1000 | Loss: 0.00001037
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Iteration 124/1000 | Loss: 0.00001036
Iteration 125/1000 | Loss: 0.00001036
Iteration 126/1000 | Loss: 0.00001036
Iteration 127/1000 | Loss: 0.00001036
Iteration 128/1000 | Loss: 0.00001036
Iteration 129/1000 | Loss: 0.00001036
Iteration 130/1000 | Loss: 0.00001036
Iteration 131/1000 | Loss: 0.00001036
Iteration 132/1000 | Loss: 0.00001036
Iteration 133/1000 | Loss: 0.00001035
Iteration 134/1000 | Loss: 0.00001035
Iteration 135/1000 | Loss: 0.00001035
Iteration 136/1000 | Loss: 0.00001035
Iteration 137/1000 | Loss: 0.00001035
Iteration 138/1000 | Loss: 0.00001035
Iteration 139/1000 | Loss: 0.00001035
Iteration 140/1000 | Loss: 0.00001035
Iteration 141/1000 | Loss: 0.00001034
Iteration 142/1000 | Loss: 0.00001034
Iteration 143/1000 | Loss: 0.00001034
Iteration 144/1000 | Loss: 0.00001034
Iteration 145/1000 | Loss: 0.00001034
Iteration 146/1000 | Loss: 0.00001034
Iteration 147/1000 | Loss: 0.00001034
Iteration 148/1000 | Loss: 0.00001034
Iteration 149/1000 | Loss: 0.00001034
Iteration 150/1000 | Loss: 0.00001034
Iteration 151/1000 | Loss: 0.00001034
Iteration 152/1000 | Loss: 0.00001034
Iteration 153/1000 | Loss: 0.00001034
Iteration 154/1000 | Loss: 0.00001034
Iteration 155/1000 | Loss: 0.00001034
Iteration 156/1000 | Loss: 0.00001034
Iteration 157/1000 | Loss: 0.00001034
Iteration 158/1000 | Loss: 0.00001033
Iteration 159/1000 | Loss: 0.00001033
Iteration 160/1000 | Loss: 0.00001033
Iteration 161/1000 | Loss: 0.00001033
Iteration 162/1000 | Loss: 0.00001033
Iteration 163/1000 | Loss: 0.00001033
Iteration 164/1000 | Loss: 0.00001033
Iteration 165/1000 | Loss: 0.00001033
Iteration 166/1000 | Loss: 0.00001033
Iteration 167/1000 | Loss: 0.00001033
Iteration 168/1000 | Loss: 0.00001033
Iteration 169/1000 | Loss: 0.00001033
Iteration 170/1000 | Loss: 0.00001033
Iteration 171/1000 | Loss: 0.00001033
Iteration 172/1000 | Loss: 0.00001032
Iteration 173/1000 | Loss: 0.00001032
Iteration 174/1000 | Loss: 0.00001032
Iteration 175/1000 | Loss: 0.00001032
Iteration 176/1000 | Loss: 0.00001032
Iteration 177/1000 | Loss: 0.00001032
Iteration 178/1000 | Loss: 0.00001032
Iteration 179/1000 | Loss: 0.00001032
Iteration 180/1000 | Loss: 0.00001032
Iteration 181/1000 | Loss: 0.00001032
Iteration 182/1000 | Loss: 0.00001032
Iteration 183/1000 | Loss: 0.00001031
Iteration 184/1000 | Loss: 0.00001031
Iteration 185/1000 | Loss: 0.00001031
Iteration 186/1000 | Loss: 0.00001031
Iteration 187/1000 | Loss: 0.00001031
Iteration 188/1000 | Loss: 0.00001031
Iteration 189/1000 | Loss: 0.00001031
Iteration 190/1000 | Loss: 0.00001031
Iteration 191/1000 | Loss: 0.00001030
Iteration 192/1000 | Loss: 0.00001030
Iteration 193/1000 | Loss: 0.00001030
Iteration 194/1000 | Loss: 0.00001030
Iteration 195/1000 | Loss: 0.00001030
Iteration 196/1000 | Loss: 0.00001030
Iteration 197/1000 | Loss: 0.00001030
Iteration 198/1000 | Loss: 0.00001030
Iteration 199/1000 | Loss: 0.00001030
Iteration 200/1000 | Loss: 0.00001030
Iteration 201/1000 | Loss: 0.00001030
Iteration 202/1000 | Loss: 0.00001029
Iteration 203/1000 | Loss: 0.00001029
Iteration 204/1000 | Loss: 0.00001029
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001029
Iteration 207/1000 | Loss: 0.00001029
Iteration 208/1000 | Loss: 0.00001029
Iteration 209/1000 | Loss: 0.00001028
Iteration 210/1000 | Loss: 0.00001028
Iteration 211/1000 | Loss: 0.00001028
Iteration 212/1000 | Loss: 0.00001028
Iteration 213/1000 | Loss: 0.00001028
Iteration 214/1000 | Loss: 0.00001028
Iteration 215/1000 | Loss: 0.00001028
Iteration 216/1000 | Loss: 0.00001028
Iteration 217/1000 | Loss: 0.00001028
Iteration 218/1000 | Loss: 0.00001027
Iteration 219/1000 | Loss: 0.00001027
Iteration 220/1000 | Loss: 0.00001027
Iteration 221/1000 | Loss: 0.00001027
Iteration 222/1000 | Loss: 0.00001027
Iteration 223/1000 | Loss: 0.00001027
Iteration 224/1000 | Loss: 0.00001027
Iteration 225/1000 | Loss: 0.00001027
Iteration 226/1000 | Loss: 0.00001027
Iteration 227/1000 | Loss: 0.00001027
Iteration 228/1000 | Loss: 0.00001027
Iteration 229/1000 | Loss: 0.00001027
Iteration 230/1000 | Loss: 0.00001027
Iteration 231/1000 | Loss: 0.00001027
Iteration 232/1000 | Loss: 0.00001027
Iteration 233/1000 | Loss: 0.00001027
Iteration 234/1000 | Loss: 0.00001027
Iteration 235/1000 | Loss: 0.00001027
Iteration 236/1000 | Loss: 0.00001027
Iteration 237/1000 | Loss: 0.00001026
Iteration 238/1000 | Loss: 0.00001026
Iteration 239/1000 | Loss: 0.00001026
Iteration 240/1000 | Loss: 0.00001026
Iteration 241/1000 | Loss: 0.00001026
Iteration 242/1000 | Loss: 0.00001026
Iteration 243/1000 | Loss: 0.00001026
Iteration 244/1000 | Loss: 0.00001026
Iteration 245/1000 | Loss: 0.00001026
Iteration 246/1000 | Loss: 0.00001026
Iteration 247/1000 | Loss: 0.00001026
Iteration 248/1000 | Loss: 0.00001026
Iteration 249/1000 | Loss: 0.00001026
Iteration 250/1000 | Loss: 0.00001026
Iteration 251/1000 | Loss: 0.00001026
Iteration 252/1000 | Loss: 0.00001026
Iteration 253/1000 | Loss: 0.00001026
Iteration 254/1000 | Loss: 0.00001026
Iteration 255/1000 | Loss: 0.00001026
Iteration 256/1000 | Loss: 0.00001026
Iteration 257/1000 | Loss: 0.00001026
Iteration 258/1000 | Loss: 0.00001026
Iteration 259/1000 | Loss: 0.00001026
Iteration 260/1000 | Loss: 0.00001026
Iteration 261/1000 | Loss: 0.00001026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [1.0257287613057997e-05, 1.0257287613057997e-05, 1.0257287613057997e-05, 1.0257287613057997e-05, 1.0257287613057997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0257287613057997e-05

Optimization complete. Final v2v error: 2.6798360347747803 mm

Highest mean error: 4.1470723152160645 mm for frame 30

Lowest mean error: 2.2484798431396484 mm for frame 17

Saving results

Total time: 41.75705122947693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862596
Iteration 2/25 | Loss: 0.00109742
Iteration 3/25 | Loss: 0.00095409
Iteration 4/25 | Loss: 0.00094452
Iteration 5/25 | Loss: 0.00094227
Iteration 6/25 | Loss: 0.00094148
Iteration 7/25 | Loss: 0.00094148
Iteration 8/25 | Loss: 0.00094148
Iteration 9/25 | Loss: 0.00094148
Iteration 10/25 | Loss: 0.00094148
Iteration 11/25 | Loss: 0.00094148
Iteration 12/25 | Loss: 0.00094148
Iteration 13/25 | Loss: 0.00094148
Iteration 14/25 | Loss: 0.00094148
Iteration 15/25 | Loss: 0.00094148
Iteration 16/25 | Loss: 0.00094148
Iteration 17/25 | Loss: 0.00094148
Iteration 18/25 | Loss: 0.00094148
Iteration 19/25 | Loss: 0.00094148
Iteration 20/25 | Loss: 0.00094148
Iteration 21/25 | Loss: 0.00094148
Iteration 22/25 | Loss: 0.00094148
Iteration 23/25 | Loss: 0.00094148
Iteration 24/25 | Loss: 0.00094148
Iteration 25/25 | Loss: 0.00094148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34190571
Iteration 2/25 | Loss: 0.00063121
Iteration 3/25 | Loss: 0.00063121
Iteration 4/25 | Loss: 0.00063121
Iteration 5/25 | Loss: 0.00063121
Iteration 6/25 | Loss: 0.00063121
Iteration 7/25 | Loss: 0.00063121
Iteration 8/25 | Loss: 0.00063121
Iteration 9/25 | Loss: 0.00063121
Iteration 10/25 | Loss: 0.00063121
Iteration 11/25 | Loss: 0.00063121
Iteration 12/25 | Loss: 0.00063121
Iteration 13/25 | Loss: 0.00063121
Iteration 14/25 | Loss: 0.00063121
Iteration 15/25 | Loss: 0.00063121
Iteration 16/25 | Loss: 0.00063121
Iteration 17/25 | Loss: 0.00063121
Iteration 18/25 | Loss: 0.00063121
Iteration 19/25 | Loss: 0.00063121
Iteration 20/25 | Loss: 0.00063121
Iteration 21/25 | Loss: 0.00063121
Iteration 22/25 | Loss: 0.00063121
Iteration 23/25 | Loss: 0.00063121
Iteration 24/25 | Loss: 0.00063121
Iteration 25/25 | Loss: 0.00063121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063121
Iteration 2/1000 | Loss: 0.00003278
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001309
Iteration 5/1000 | Loss: 0.00001210
Iteration 6/1000 | Loss: 0.00001140
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001074
Iteration 9/1000 | Loss: 0.00001052
Iteration 10/1000 | Loss: 0.00001038
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001020
Iteration 15/1000 | Loss: 0.00001012
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001008
Iteration 19/1000 | Loss: 0.00001008
Iteration 20/1000 | Loss: 0.00001006
Iteration 21/1000 | Loss: 0.00001006
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001003
Iteration 26/1000 | Loss: 0.00001003
Iteration 27/1000 | Loss: 0.00001003
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00000998
Iteration 31/1000 | Loss: 0.00000997
Iteration 32/1000 | Loss: 0.00000996
Iteration 33/1000 | Loss: 0.00000995
Iteration 34/1000 | Loss: 0.00000995
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000993
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000993
Iteration 42/1000 | Loss: 0.00000993
Iteration 43/1000 | Loss: 0.00000992
Iteration 44/1000 | Loss: 0.00000992
Iteration 45/1000 | Loss: 0.00000991
Iteration 46/1000 | Loss: 0.00000991
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000990
Iteration 49/1000 | Loss: 0.00000990
Iteration 50/1000 | Loss: 0.00000989
Iteration 51/1000 | Loss: 0.00000988
Iteration 52/1000 | Loss: 0.00000988
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000985
Iteration 55/1000 | Loss: 0.00000985
Iteration 56/1000 | Loss: 0.00000985
Iteration 57/1000 | Loss: 0.00000985
Iteration 58/1000 | Loss: 0.00000985
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000985
Iteration 62/1000 | Loss: 0.00000984
Iteration 63/1000 | Loss: 0.00000984
Iteration 64/1000 | Loss: 0.00000984
Iteration 65/1000 | Loss: 0.00000983
Iteration 66/1000 | Loss: 0.00000983
Iteration 67/1000 | Loss: 0.00000982
Iteration 68/1000 | Loss: 0.00000982
Iteration 69/1000 | Loss: 0.00000982
Iteration 70/1000 | Loss: 0.00000982
Iteration 71/1000 | Loss: 0.00000981
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000981
Iteration 76/1000 | Loss: 0.00000981
Iteration 77/1000 | Loss: 0.00000981
Iteration 78/1000 | Loss: 0.00000980
Iteration 79/1000 | Loss: 0.00000980
Iteration 80/1000 | Loss: 0.00000980
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000980
Iteration 85/1000 | Loss: 0.00000980
Iteration 86/1000 | Loss: 0.00000979
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000976
Iteration 130/1000 | Loss: 0.00000976
Iteration 131/1000 | Loss: 0.00000976
Iteration 132/1000 | Loss: 0.00000976
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [9.762390618561767e-06, 9.762390618561767e-06, 9.762390618561767e-06, 9.762390618561767e-06, 9.762390618561767e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.762390618561767e-06

Optimization complete. Final v2v error: 2.587381601333618 mm

Highest mean error: 3.373732089996338 mm for frame 63

Lowest mean error: 2.293673038482666 mm for frame 122

Saving results

Total time: 34.53463649749756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376576
Iteration 2/25 | Loss: 0.00099101
Iteration 3/25 | Loss: 0.00090912
Iteration 4/25 | Loss: 0.00090301
Iteration 5/25 | Loss: 0.00090143
Iteration 6/25 | Loss: 0.00090112
Iteration 7/25 | Loss: 0.00090112
Iteration 8/25 | Loss: 0.00090112
Iteration 9/25 | Loss: 0.00090112
Iteration 10/25 | Loss: 0.00090112
Iteration 11/25 | Loss: 0.00090112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009011243819259107, 0.0009011243819259107, 0.0009011243819259107, 0.0009011243819259107, 0.0009011243819259107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009011243819259107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34380579
Iteration 2/25 | Loss: 0.00064487
Iteration 3/25 | Loss: 0.00064487
Iteration 4/25 | Loss: 0.00064487
Iteration 5/25 | Loss: 0.00064487
Iteration 6/25 | Loss: 0.00064487
Iteration 7/25 | Loss: 0.00064487
Iteration 8/25 | Loss: 0.00064487
Iteration 9/25 | Loss: 0.00064487
Iteration 10/25 | Loss: 0.00064487
Iteration 11/25 | Loss: 0.00064487
Iteration 12/25 | Loss: 0.00064487
Iteration 13/25 | Loss: 0.00064487
Iteration 14/25 | Loss: 0.00064487
Iteration 15/25 | Loss: 0.00064487
Iteration 16/25 | Loss: 0.00064487
Iteration 17/25 | Loss: 0.00064487
Iteration 18/25 | Loss: 0.00064487
Iteration 19/25 | Loss: 0.00064487
Iteration 20/25 | Loss: 0.00064487
Iteration 21/25 | Loss: 0.00064487
Iteration 22/25 | Loss: 0.00064487
Iteration 23/25 | Loss: 0.00064487
Iteration 24/25 | Loss: 0.00064487
Iteration 25/25 | Loss: 0.00064487
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006448650383390486, 0.0006448650383390486, 0.0006448650383390486, 0.0006448650383390486, 0.0006448650383390486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006448650383390486

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064486
Iteration 2/1000 | Loss: 0.00002478
Iteration 3/1000 | Loss: 0.00001270
Iteration 4/1000 | Loss: 0.00000939
Iteration 5/1000 | Loss: 0.00000874
Iteration 6/1000 | Loss: 0.00000821
Iteration 7/1000 | Loss: 0.00000807
Iteration 8/1000 | Loss: 0.00000787
Iteration 9/1000 | Loss: 0.00000781
Iteration 10/1000 | Loss: 0.00000776
Iteration 11/1000 | Loss: 0.00000773
Iteration 12/1000 | Loss: 0.00000772
Iteration 13/1000 | Loss: 0.00000771
Iteration 14/1000 | Loss: 0.00000767
Iteration 15/1000 | Loss: 0.00000767
Iteration 16/1000 | Loss: 0.00000767
Iteration 17/1000 | Loss: 0.00000766
Iteration 18/1000 | Loss: 0.00000766
Iteration 19/1000 | Loss: 0.00000766
Iteration 20/1000 | Loss: 0.00000765
Iteration 21/1000 | Loss: 0.00000765
Iteration 22/1000 | Loss: 0.00000764
Iteration 23/1000 | Loss: 0.00000764
Iteration 24/1000 | Loss: 0.00000764
Iteration 25/1000 | Loss: 0.00000764
Iteration 26/1000 | Loss: 0.00000763
Iteration 27/1000 | Loss: 0.00000763
Iteration 28/1000 | Loss: 0.00000762
Iteration 29/1000 | Loss: 0.00000761
Iteration 30/1000 | Loss: 0.00000761
Iteration 31/1000 | Loss: 0.00000761
Iteration 32/1000 | Loss: 0.00000761
Iteration 33/1000 | Loss: 0.00000760
Iteration 34/1000 | Loss: 0.00000760
Iteration 35/1000 | Loss: 0.00000760
Iteration 36/1000 | Loss: 0.00000760
Iteration 37/1000 | Loss: 0.00000760
Iteration 38/1000 | Loss: 0.00000759
Iteration 39/1000 | Loss: 0.00000759
Iteration 40/1000 | Loss: 0.00000758
Iteration 41/1000 | Loss: 0.00000758
Iteration 42/1000 | Loss: 0.00000757
Iteration 43/1000 | Loss: 0.00000757
Iteration 44/1000 | Loss: 0.00000757
Iteration 45/1000 | Loss: 0.00000756
Iteration 46/1000 | Loss: 0.00000756
Iteration 47/1000 | Loss: 0.00000756
Iteration 48/1000 | Loss: 0.00000755
Iteration 49/1000 | Loss: 0.00000754
Iteration 50/1000 | Loss: 0.00000754
Iteration 51/1000 | Loss: 0.00000752
Iteration 52/1000 | Loss: 0.00000752
Iteration 53/1000 | Loss: 0.00000752
Iteration 54/1000 | Loss: 0.00000752
Iteration 55/1000 | Loss: 0.00000752
Iteration 56/1000 | Loss: 0.00000752
Iteration 57/1000 | Loss: 0.00000752
Iteration 58/1000 | Loss: 0.00000752
Iteration 59/1000 | Loss: 0.00000751
Iteration 60/1000 | Loss: 0.00000751
Iteration 61/1000 | Loss: 0.00000751
Iteration 62/1000 | Loss: 0.00000751
Iteration 63/1000 | Loss: 0.00000751
Iteration 64/1000 | Loss: 0.00000750
Iteration 65/1000 | Loss: 0.00000749
Iteration 66/1000 | Loss: 0.00000748
Iteration 67/1000 | Loss: 0.00000748
Iteration 68/1000 | Loss: 0.00000746
Iteration 69/1000 | Loss: 0.00000746
Iteration 70/1000 | Loss: 0.00000746
Iteration 71/1000 | Loss: 0.00000746
Iteration 72/1000 | Loss: 0.00000746
Iteration 73/1000 | Loss: 0.00000746
Iteration 74/1000 | Loss: 0.00000745
Iteration 75/1000 | Loss: 0.00000745
Iteration 76/1000 | Loss: 0.00000745
Iteration 77/1000 | Loss: 0.00000744
Iteration 78/1000 | Loss: 0.00000744
Iteration 79/1000 | Loss: 0.00000744
Iteration 80/1000 | Loss: 0.00000744
Iteration 81/1000 | Loss: 0.00000743
Iteration 82/1000 | Loss: 0.00000743
Iteration 83/1000 | Loss: 0.00000743
Iteration 84/1000 | Loss: 0.00000743
Iteration 85/1000 | Loss: 0.00000742
Iteration 86/1000 | Loss: 0.00000742
Iteration 87/1000 | Loss: 0.00000742
Iteration 88/1000 | Loss: 0.00000742
Iteration 89/1000 | Loss: 0.00000742
Iteration 90/1000 | Loss: 0.00000742
Iteration 91/1000 | Loss: 0.00000742
Iteration 92/1000 | Loss: 0.00000742
Iteration 93/1000 | Loss: 0.00000741
Iteration 94/1000 | Loss: 0.00000741
Iteration 95/1000 | Loss: 0.00000741
Iteration 96/1000 | Loss: 0.00000740
Iteration 97/1000 | Loss: 0.00000740
Iteration 98/1000 | Loss: 0.00000740
Iteration 99/1000 | Loss: 0.00000740
Iteration 100/1000 | Loss: 0.00000740
Iteration 101/1000 | Loss: 0.00000740
Iteration 102/1000 | Loss: 0.00000740
Iteration 103/1000 | Loss: 0.00000740
Iteration 104/1000 | Loss: 0.00000739
Iteration 105/1000 | Loss: 0.00000739
Iteration 106/1000 | Loss: 0.00000739
Iteration 107/1000 | Loss: 0.00000739
Iteration 108/1000 | Loss: 0.00000739
Iteration 109/1000 | Loss: 0.00000739
Iteration 110/1000 | Loss: 0.00000739
Iteration 111/1000 | Loss: 0.00000739
Iteration 112/1000 | Loss: 0.00000739
Iteration 113/1000 | Loss: 0.00000739
Iteration 114/1000 | Loss: 0.00000739
Iteration 115/1000 | Loss: 0.00000739
Iteration 116/1000 | Loss: 0.00000739
Iteration 117/1000 | Loss: 0.00000739
Iteration 118/1000 | Loss: 0.00000739
Iteration 119/1000 | Loss: 0.00000739
Iteration 120/1000 | Loss: 0.00000739
Iteration 121/1000 | Loss: 0.00000739
Iteration 122/1000 | Loss: 0.00000739
Iteration 123/1000 | Loss: 0.00000739
Iteration 124/1000 | Loss: 0.00000739
Iteration 125/1000 | Loss: 0.00000739
Iteration 126/1000 | Loss: 0.00000739
Iteration 127/1000 | Loss: 0.00000739
Iteration 128/1000 | Loss: 0.00000739
Iteration 129/1000 | Loss: 0.00000739
Iteration 130/1000 | Loss: 0.00000739
Iteration 131/1000 | Loss: 0.00000739
Iteration 132/1000 | Loss: 0.00000739
Iteration 133/1000 | Loss: 0.00000739
Iteration 134/1000 | Loss: 0.00000739
Iteration 135/1000 | Loss: 0.00000739
Iteration 136/1000 | Loss: 0.00000739
Iteration 137/1000 | Loss: 0.00000739
Iteration 138/1000 | Loss: 0.00000739
Iteration 139/1000 | Loss: 0.00000739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [7.392719453491736e-06, 7.392719453491736e-06, 7.392719453491736e-06, 7.392719453491736e-06, 7.392719453491736e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.392719453491736e-06

Optimization complete. Final v2v error: 2.2969322204589844 mm

Highest mean error: 2.457993745803833 mm for frame 79

Lowest mean error: 2.2092440128326416 mm for frame 42

Saving results

Total time: 30.56793522834778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01085591
Iteration 2/25 | Loss: 0.01085591
Iteration 3/25 | Loss: 0.01085591
Iteration 4/25 | Loss: 0.01085591
Iteration 5/25 | Loss: 0.01085591
Iteration 6/25 | Loss: 0.01085591
Iteration 7/25 | Loss: 0.01085591
Iteration 8/25 | Loss: 0.01085591
Iteration 9/25 | Loss: 0.01085590
Iteration 10/25 | Loss: 0.01085590
Iteration 11/25 | Loss: 0.01085590
Iteration 12/25 | Loss: 0.01085590
Iteration 13/25 | Loss: 0.01085590
Iteration 14/25 | Loss: 0.01085589
Iteration 15/25 | Loss: 0.01085589
Iteration 16/25 | Loss: 0.01085589
Iteration 17/25 | Loss: 0.01085589
Iteration 18/25 | Loss: 0.01085589
Iteration 19/25 | Loss: 0.01085589
Iteration 20/25 | Loss: 0.01085588
Iteration 21/25 | Loss: 0.01085588
Iteration 22/25 | Loss: 0.01085588
Iteration 23/25 | Loss: 0.01085588
Iteration 24/25 | Loss: 0.01085588
Iteration 25/25 | Loss: 0.01085588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44406462
Iteration 2/25 | Loss: 0.17603280
Iteration 3/25 | Loss: 0.16600703
Iteration 4/25 | Loss: 0.16514978
Iteration 5/25 | Loss: 0.16514973
Iteration 6/25 | Loss: 0.16514973
Iteration 7/25 | Loss: 0.16514973
Iteration 8/25 | Loss: 0.16514973
Iteration 9/25 | Loss: 0.16514973
Iteration 10/25 | Loss: 0.16514973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.1651497334241867, 0.1651497334241867, 0.1651497334241867, 0.1651497334241867, 0.1651497334241867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1651497334241867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16514973
Iteration 2/1000 | Loss: 0.01265496
Iteration 3/1000 | Loss: 0.00170489
Iteration 4/1000 | Loss: 0.00084231
Iteration 5/1000 | Loss: 0.00129691
Iteration 6/1000 | Loss: 0.00132074
Iteration 7/1000 | Loss: 0.00044788
Iteration 8/1000 | Loss: 0.00014918
Iteration 9/1000 | Loss: 0.00017683
Iteration 10/1000 | Loss: 0.00007066
Iteration 11/1000 | Loss: 0.00005309
Iteration 12/1000 | Loss: 0.00014506
Iteration 13/1000 | Loss: 0.00027397
Iteration 14/1000 | Loss: 0.00021333
Iteration 15/1000 | Loss: 0.00013302
Iteration 16/1000 | Loss: 0.00004336
Iteration 17/1000 | Loss: 0.00036903
Iteration 18/1000 | Loss: 0.00002749
Iteration 19/1000 | Loss: 0.00002389
Iteration 20/1000 | Loss: 0.00002167
Iteration 21/1000 | Loss: 0.00019514
Iteration 22/1000 | Loss: 0.00003897
Iteration 23/1000 | Loss: 0.00003570
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00004438
Iteration 26/1000 | Loss: 0.00003073
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00017670
Iteration 29/1000 | Loss: 0.00001668
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00011984
Iteration 33/1000 | Loss: 0.00045488
Iteration 34/1000 | Loss: 0.00002856
Iteration 35/1000 | Loss: 0.00004929
Iteration 36/1000 | Loss: 0.00008251
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001457
Iteration 40/1000 | Loss: 0.00012979
Iteration 41/1000 | Loss: 0.00012210
Iteration 42/1000 | Loss: 0.00008934
Iteration 43/1000 | Loss: 0.00002931
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001434
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001430
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001429
Iteration 53/1000 | Loss: 0.00001429
Iteration 54/1000 | Loss: 0.00001429
Iteration 55/1000 | Loss: 0.00001429
Iteration 56/1000 | Loss: 0.00001428
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001428
Iteration 60/1000 | Loss: 0.00001428
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001423
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001422
Iteration 73/1000 | Loss: 0.00001421
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001420
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001418
Iteration 84/1000 | Loss: 0.00001418
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001417
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001416
Iteration 98/1000 | Loss: 0.00001416
Iteration 99/1000 | Loss: 0.00001416
Iteration 100/1000 | Loss: 0.00001416
Iteration 101/1000 | Loss: 0.00001416
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001414
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00001414
Iteration 113/1000 | Loss: 0.00001414
Iteration 114/1000 | Loss: 0.00001414
Iteration 115/1000 | Loss: 0.00001414
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001413
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001413
Iteration 121/1000 | Loss: 0.00001413
Iteration 122/1000 | Loss: 0.00001413
Iteration 123/1000 | Loss: 0.00001413
Iteration 124/1000 | Loss: 0.00001413
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001412
Iteration 132/1000 | Loss: 0.00001412
Iteration 133/1000 | Loss: 0.00001412
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001410
Iteration 142/1000 | Loss: 0.00001410
Iteration 143/1000 | Loss: 0.00001410
Iteration 144/1000 | Loss: 0.00001410
Iteration 145/1000 | Loss: 0.00001410
Iteration 146/1000 | Loss: 0.00001410
Iteration 147/1000 | Loss: 0.00001410
Iteration 148/1000 | Loss: 0.00001410
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001409
Iteration 152/1000 | Loss: 0.00001409
Iteration 153/1000 | Loss: 0.00001409
Iteration 154/1000 | Loss: 0.00001409
Iteration 155/1000 | Loss: 0.00001409
Iteration 156/1000 | Loss: 0.00001409
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001408
Iteration 159/1000 | Loss: 0.00001408
Iteration 160/1000 | Loss: 0.00001408
Iteration 161/1000 | Loss: 0.00001408
Iteration 162/1000 | Loss: 0.00001408
Iteration 163/1000 | Loss: 0.00001408
Iteration 164/1000 | Loss: 0.00001408
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001407
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001407
Iteration 170/1000 | Loss: 0.00001407
Iteration 171/1000 | Loss: 0.00001407
Iteration 172/1000 | Loss: 0.00001407
Iteration 173/1000 | Loss: 0.00001407
Iteration 174/1000 | Loss: 0.00001407
Iteration 175/1000 | Loss: 0.00001407
Iteration 176/1000 | Loss: 0.00001407
Iteration 177/1000 | Loss: 0.00001407
Iteration 178/1000 | Loss: 0.00001407
Iteration 179/1000 | Loss: 0.00001407
Iteration 180/1000 | Loss: 0.00001407
Iteration 181/1000 | Loss: 0.00001407
Iteration 182/1000 | Loss: 0.00001407
Iteration 183/1000 | Loss: 0.00001407
Iteration 184/1000 | Loss: 0.00001407
Iteration 185/1000 | Loss: 0.00001407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.407386389473686e-05, 1.407386389473686e-05, 1.407386389473686e-05, 1.407386389473686e-05, 1.407386389473686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.407386389473686e-05

Optimization complete. Final v2v error: 3.16129732131958 mm

Highest mean error: 8.26411247253418 mm for frame 228

Lowest mean error: 2.5797152519226074 mm for frame 222

Saving results

Total time: 88.17702507972717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739534
Iteration 2/25 | Loss: 0.00137736
Iteration 3/25 | Loss: 0.00102372
Iteration 4/25 | Loss: 0.00098005
Iteration 5/25 | Loss: 0.00097377
Iteration 6/25 | Loss: 0.00097194
Iteration 7/25 | Loss: 0.00097194
Iteration 8/25 | Loss: 0.00097194
Iteration 9/25 | Loss: 0.00097194
Iteration 10/25 | Loss: 0.00097194
Iteration 11/25 | Loss: 0.00097194
Iteration 12/25 | Loss: 0.00097194
Iteration 13/25 | Loss: 0.00097194
Iteration 14/25 | Loss: 0.00097194
Iteration 15/25 | Loss: 0.00097194
Iteration 16/25 | Loss: 0.00097194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009719412191770971, 0.0009719412191770971, 0.0009719412191770971, 0.0009719412191770971, 0.0009719412191770971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009719412191770971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34122741
Iteration 2/25 | Loss: 0.00061786
Iteration 3/25 | Loss: 0.00061785
Iteration 4/25 | Loss: 0.00061785
Iteration 5/25 | Loss: 0.00061785
Iteration 6/25 | Loss: 0.00061785
Iteration 7/25 | Loss: 0.00061785
Iteration 8/25 | Loss: 0.00061785
Iteration 9/25 | Loss: 0.00061785
Iteration 10/25 | Loss: 0.00061785
Iteration 11/25 | Loss: 0.00061785
Iteration 12/25 | Loss: 0.00061785
Iteration 13/25 | Loss: 0.00061785
Iteration 14/25 | Loss: 0.00061785
Iteration 15/25 | Loss: 0.00061785
Iteration 16/25 | Loss: 0.00061785
Iteration 17/25 | Loss: 0.00061785
Iteration 18/25 | Loss: 0.00061785
Iteration 19/25 | Loss: 0.00061785
Iteration 20/25 | Loss: 0.00061785
Iteration 21/25 | Loss: 0.00061785
Iteration 22/25 | Loss: 0.00061785
Iteration 23/25 | Loss: 0.00061785
Iteration 24/25 | Loss: 0.00061785
Iteration 25/25 | Loss: 0.00061785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061785
Iteration 2/1000 | Loss: 0.00002456
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001384
Iteration 5/1000 | Loss: 0.00001269
Iteration 6/1000 | Loss: 0.00001210
Iteration 7/1000 | Loss: 0.00001171
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001101
Iteration 15/1000 | Loss: 0.00001100
Iteration 16/1000 | Loss: 0.00001098
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001095
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001094
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001091
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001090
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001085
Iteration 34/1000 | Loss: 0.00001085
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001083
Iteration 39/1000 | Loss: 0.00001082
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001081
Iteration 43/1000 | Loss: 0.00001081
Iteration 44/1000 | Loss: 0.00001081
Iteration 45/1000 | Loss: 0.00001081
Iteration 46/1000 | Loss: 0.00001081
Iteration 47/1000 | Loss: 0.00001081
Iteration 48/1000 | Loss: 0.00001081
Iteration 49/1000 | Loss: 0.00001081
Iteration 50/1000 | Loss: 0.00001080
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001079
Iteration 55/1000 | Loss: 0.00001079
Iteration 56/1000 | Loss: 0.00001079
Iteration 57/1000 | Loss: 0.00001079
Iteration 58/1000 | Loss: 0.00001079
Iteration 59/1000 | Loss: 0.00001079
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001077
Iteration 62/1000 | Loss: 0.00001077
Iteration 63/1000 | Loss: 0.00001077
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001077
Iteration 66/1000 | Loss: 0.00001077
Iteration 67/1000 | Loss: 0.00001077
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001076
Iteration 72/1000 | Loss: 0.00001076
Iteration 73/1000 | Loss: 0.00001076
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001076
Iteration 76/1000 | Loss: 0.00001076
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001076
Iteration 80/1000 | Loss: 0.00001076
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001075
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001075
Iteration 87/1000 | Loss: 0.00001075
Iteration 88/1000 | Loss: 0.00001075
Iteration 89/1000 | Loss: 0.00001075
Iteration 90/1000 | Loss: 0.00001075
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001073
Iteration 95/1000 | Loss: 0.00001073
Iteration 96/1000 | Loss: 0.00001073
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001073
Iteration 101/1000 | Loss: 0.00001072
Iteration 102/1000 | Loss: 0.00001072
Iteration 103/1000 | Loss: 0.00001072
Iteration 104/1000 | Loss: 0.00001072
Iteration 105/1000 | Loss: 0.00001072
Iteration 106/1000 | Loss: 0.00001072
Iteration 107/1000 | Loss: 0.00001072
Iteration 108/1000 | Loss: 0.00001072
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001071
Iteration 117/1000 | Loss: 0.00001071
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.069728568836581e-05, 1.069728568836581e-05, 1.069728568836581e-05, 1.069728568836581e-05, 1.069728568836581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.069728568836581e-05

Optimization complete. Final v2v error: 2.7941160202026367 mm

Highest mean error: 3.2839879989624023 mm for frame 234

Lowest mean error: 2.5890464782714844 mm for frame 71

Saving results

Total time: 36.677247762680054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073766
Iteration 2/25 | Loss: 0.00300346
Iteration 3/25 | Loss: 0.00205239
Iteration 4/25 | Loss: 0.00177943
Iteration 5/25 | Loss: 0.00166725
Iteration 6/25 | Loss: 0.00166545
Iteration 7/25 | Loss: 0.00153184
Iteration 8/25 | Loss: 0.00147728
Iteration 9/25 | Loss: 0.00139000
Iteration 10/25 | Loss: 0.00135713
Iteration 11/25 | Loss: 0.00132443
Iteration 12/25 | Loss: 0.00130747
Iteration 13/25 | Loss: 0.00127541
Iteration 14/25 | Loss: 0.00124938
Iteration 15/25 | Loss: 0.00121956
Iteration 16/25 | Loss: 0.00120559
Iteration 17/25 | Loss: 0.00119574
Iteration 18/25 | Loss: 0.00118866
Iteration 19/25 | Loss: 0.00118076
Iteration 20/25 | Loss: 0.00117756
Iteration 21/25 | Loss: 0.00117614
Iteration 22/25 | Loss: 0.00117057
Iteration 23/25 | Loss: 0.00116836
Iteration 24/25 | Loss: 0.00116757
Iteration 25/25 | Loss: 0.00116435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34661496
Iteration 2/25 | Loss: 0.00330376
Iteration 3/25 | Loss: 0.00241370
Iteration 4/25 | Loss: 0.00241370
Iteration 5/25 | Loss: 0.00241370
Iteration 6/25 | Loss: 0.00241370
Iteration 7/25 | Loss: 0.00241370
Iteration 8/25 | Loss: 0.00241370
Iteration 9/25 | Loss: 0.00241369
Iteration 10/25 | Loss: 0.00241369
Iteration 11/25 | Loss: 0.00241369
Iteration 12/25 | Loss: 0.00241369
Iteration 13/25 | Loss: 0.00241369
Iteration 14/25 | Loss: 0.00241369
Iteration 15/25 | Loss: 0.00241369
Iteration 16/25 | Loss: 0.00241369
Iteration 17/25 | Loss: 0.00241369
Iteration 18/25 | Loss: 0.00241369
Iteration 19/25 | Loss: 0.00241369
Iteration 20/25 | Loss: 0.00241369
Iteration 21/25 | Loss: 0.00241369
Iteration 22/25 | Loss: 0.00241369
Iteration 23/25 | Loss: 0.00241369
Iteration 24/25 | Loss: 0.00241369
Iteration 25/25 | Loss: 0.00241369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00241369
Iteration 2/1000 | Loss: 0.00252554
Iteration 3/1000 | Loss: 0.00031221
Iteration 4/1000 | Loss: 0.00066897
Iteration 5/1000 | Loss: 0.00026617
Iteration 6/1000 | Loss: 0.00024427
Iteration 7/1000 | Loss: 0.00026389
Iteration 8/1000 | Loss: 0.00035798
Iteration 9/1000 | Loss: 0.00022606
Iteration 10/1000 | Loss: 0.00044324
Iteration 11/1000 | Loss: 0.00027179
Iteration 12/1000 | Loss: 0.00154250
Iteration 13/1000 | Loss: 0.00111700
Iteration 14/1000 | Loss: 0.00082419
Iteration 15/1000 | Loss: 0.00138352
Iteration 16/1000 | Loss: 0.00221575
Iteration 17/1000 | Loss: 0.00012793
Iteration 18/1000 | Loss: 0.00011156
Iteration 19/1000 | Loss: 0.00410392
Iteration 20/1000 | Loss: 0.00053546
Iteration 21/1000 | Loss: 0.00011791
Iteration 22/1000 | Loss: 0.00160647
Iteration 23/1000 | Loss: 0.00020649
Iteration 24/1000 | Loss: 0.00067080
Iteration 25/1000 | Loss: 0.00024643
Iteration 26/1000 | Loss: 0.00017130
Iteration 27/1000 | Loss: 0.00033667
Iteration 28/1000 | Loss: 0.00024781
Iteration 29/1000 | Loss: 0.00029328
Iteration 30/1000 | Loss: 0.00007510
Iteration 31/1000 | Loss: 0.00008703
Iteration 32/1000 | Loss: 0.00051838
Iteration 33/1000 | Loss: 0.00058565
Iteration 34/1000 | Loss: 0.00006392
Iteration 35/1000 | Loss: 0.00048645
Iteration 36/1000 | Loss: 0.00005020
Iteration 37/1000 | Loss: 0.00005104
Iteration 38/1000 | Loss: 0.00004399
Iteration 39/1000 | Loss: 0.00004634
Iteration 40/1000 | Loss: 0.00004529
Iteration 41/1000 | Loss: 0.00004151
Iteration 42/1000 | Loss: 0.00004482
Iteration 43/1000 | Loss: 0.00003480
Iteration 44/1000 | Loss: 0.00012129
Iteration 45/1000 | Loss: 0.00005996
Iteration 46/1000 | Loss: 0.00005769
Iteration 47/1000 | Loss: 0.00052895
Iteration 48/1000 | Loss: 0.00006847
Iteration 49/1000 | Loss: 0.00004217
Iteration 50/1000 | Loss: 0.00004452
Iteration 51/1000 | Loss: 0.00004394
Iteration 52/1000 | Loss: 0.00012296
Iteration 53/1000 | Loss: 0.00005248
Iteration 54/1000 | Loss: 0.00004012
Iteration 55/1000 | Loss: 0.00004233
Iteration 56/1000 | Loss: 0.00005399
Iteration 57/1000 | Loss: 0.00004535
Iteration 58/1000 | Loss: 0.00004511
Iteration 59/1000 | Loss: 0.00003929
Iteration 60/1000 | Loss: 0.00005989
Iteration 61/1000 | Loss: 0.00004138
Iteration 62/1000 | Loss: 0.00007618
Iteration 63/1000 | Loss: 0.00007268
Iteration 64/1000 | Loss: 0.00004628
Iteration 65/1000 | Loss: 0.00004074
Iteration 66/1000 | Loss: 0.00003962
Iteration 67/1000 | Loss: 0.00003576
Iteration 68/1000 | Loss: 0.00013201
Iteration 69/1000 | Loss: 0.00004159
Iteration 70/1000 | Loss: 0.00006520
Iteration 71/1000 | Loss: 0.00004094
Iteration 72/1000 | Loss: 0.00004796
Iteration 73/1000 | Loss: 0.00003838
Iteration 74/1000 | Loss: 0.00003654
Iteration 75/1000 | Loss: 0.00003842
Iteration 76/1000 | Loss: 0.00004062
Iteration 77/1000 | Loss: 0.00004249
Iteration 78/1000 | Loss: 0.00003175
Iteration 79/1000 | Loss: 0.00005185
Iteration 80/1000 | Loss: 0.00004614
Iteration 81/1000 | Loss: 0.00003562
Iteration 82/1000 | Loss: 0.00005319
Iteration 83/1000 | Loss: 0.00005472
Iteration 84/1000 | Loss: 0.00004368
Iteration 85/1000 | Loss: 0.00003826
Iteration 86/1000 | Loss: 0.00003924
Iteration 87/1000 | Loss: 0.00047425
Iteration 88/1000 | Loss: 0.00009656
Iteration 89/1000 | Loss: 0.00017942
Iteration 90/1000 | Loss: 0.00012021
Iteration 91/1000 | Loss: 0.00003431
Iteration 92/1000 | Loss: 0.00004180
Iteration 93/1000 | Loss: 0.00003419
Iteration 94/1000 | Loss: 0.00003811
Iteration 95/1000 | Loss: 0.00002649
Iteration 96/1000 | Loss: 0.00045766
Iteration 97/1000 | Loss: 0.00002813
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002208
Iteration 101/1000 | Loss: 0.00002124
Iteration 102/1000 | Loss: 0.00045365
Iteration 103/1000 | Loss: 0.00002477
Iteration 104/1000 | Loss: 0.00002100
Iteration 105/1000 | Loss: 0.00001978
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001764
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001753
Iteration 114/1000 | Loss: 0.00001752
Iteration 115/1000 | Loss: 0.00001752
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001766
Iteration 118/1000 | Loss: 0.00001744
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00001742
Iteration 121/1000 | Loss: 0.00001741
Iteration 122/1000 | Loss: 0.00001735
Iteration 123/1000 | Loss: 0.00001735
Iteration 124/1000 | Loss: 0.00001735
Iteration 125/1000 | Loss: 0.00001734
Iteration 126/1000 | Loss: 0.00001734
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001734
Iteration 131/1000 | Loss: 0.00001734
Iteration 132/1000 | Loss: 0.00001734
Iteration 133/1000 | Loss: 0.00001734
Iteration 134/1000 | Loss: 0.00001734
Iteration 135/1000 | Loss: 0.00001734
Iteration 136/1000 | Loss: 0.00001734
Iteration 137/1000 | Loss: 0.00001733
Iteration 138/1000 | Loss: 0.00001733
Iteration 139/1000 | Loss: 0.00001733
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001731
Iteration 142/1000 | Loss: 0.00001731
Iteration 143/1000 | Loss: 0.00045296
Iteration 144/1000 | Loss: 0.00002446
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001767
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001581
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001538
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Iteration 158/1000 | Loss: 0.00001502
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001494
Iteration 161/1000 | Loss: 0.00001488
Iteration 162/1000 | Loss: 0.00001488
Iteration 163/1000 | Loss: 0.00001487
Iteration 164/1000 | Loss: 0.00001487
Iteration 165/1000 | Loss: 0.00001487
Iteration 166/1000 | Loss: 0.00001487
Iteration 167/1000 | Loss: 0.00001487
Iteration 168/1000 | Loss: 0.00001487
Iteration 169/1000 | Loss: 0.00001487
Iteration 170/1000 | Loss: 0.00001487
Iteration 171/1000 | Loss: 0.00001486
Iteration 172/1000 | Loss: 0.00001486
Iteration 173/1000 | Loss: 0.00001486
Iteration 174/1000 | Loss: 0.00001486
Iteration 175/1000 | Loss: 0.00001485
Iteration 176/1000 | Loss: 0.00001485
Iteration 177/1000 | Loss: 0.00001485
Iteration 178/1000 | Loss: 0.00001485
Iteration 179/1000 | Loss: 0.00001485
Iteration 180/1000 | Loss: 0.00001485
Iteration 181/1000 | Loss: 0.00001485
Iteration 182/1000 | Loss: 0.00001485
Iteration 183/1000 | Loss: 0.00001485
Iteration 184/1000 | Loss: 0.00001484
Iteration 185/1000 | Loss: 0.00001484
Iteration 186/1000 | Loss: 0.00001484
Iteration 187/1000 | Loss: 0.00001484
Iteration 188/1000 | Loss: 0.00001484
Iteration 189/1000 | Loss: 0.00001484
Iteration 190/1000 | Loss: 0.00001484
Iteration 191/1000 | Loss: 0.00001484
Iteration 192/1000 | Loss: 0.00001484
Iteration 193/1000 | Loss: 0.00001484
Iteration 194/1000 | Loss: 0.00001484
Iteration 195/1000 | Loss: 0.00001484
Iteration 196/1000 | Loss: 0.00001484
Iteration 197/1000 | Loss: 0.00001484
Iteration 198/1000 | Loss: 0.00001484
Iteration 199/1000 | Loss: 0.00001484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.4840980838926043e-05, 1.4840980838926043e-05, 1.4840980838926043e-05, 1.4840980838926043e-05, 1.4840980838926043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4840980838926043e-05

Optimization complete. Final v2v error: 2.9609391689300537 mm

Highest mean error: 12.002004623413086 mm for frame 213

Lowest mean error: 2.6561808586120605 mm for frame 221

Saving results

Total time: 246.72116589546204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445487
Iteration 2/25 | Loss: 0.00104148
Iteration 3/25 | Loss: 0.00095833
Iteration 4/25 | Loss: 0.00093891
Iteration 5/25 | Loss: 0.00093184
Iteration 6/25 | Loss: 0.00093034
Iteration 7/25 | Loss: 0.00093028
Iteration 8/25 | Loss: 0.00093028
Iteration 9/25 | Loss: 0.00093028
Iteration 10/25 | Loss: 0.00093028
Iteration 11/25 | Loss: 0.00093028
Iteration 12/25 | Loss: 0.00093028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009302764665335417, 0.0009302764665335417, 0.0009302764665335417, 0.0009302764665335417, 0.0009302764665335417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009302764665335417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48719525
Iteration 2/25 | Loss: 0.00062478
Iteration 3/25 | Loss: 0.00062478
Iteration 4/25 | Loss: 0.00062478
Iteration 5/25 | Loss: 0.00062478
Iteration 6/25 | Loss: 0.00062478
Iteration 7/25 | Loss: 0.00062478
Iteration 8/25 | Loss: 0.00062478
Iteration 9/25 | Loss: 0.00062478
Iteration 10/25 | Loss: 0.00062478
Iteration 11/25 | Loss: 0.00062478
Iteration 12/25 | Loss: 0.00062478
Iteration 13/25 | Loss: 0.00062478
Iteration 14/25 | Loss: 0.00062478
Iteration 15/25 | Loss: 0.00062478
Iteration 16/25 | Loss: 0.00062478
Iteration 17/25 | Loss: 0.00062478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006247765850275755, 0.0006247765850275755, 0.0006247765850275755, 0.0006247765850275755, 0.0006247765850275755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006247765850275755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062478
Iteration 2/1000 | Loss: 0.00002529
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001097
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001089
Iteration 14/1000 | Loss: 0.00001085
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001077
Iteration 20/1000 | Loss: 0.00001076
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001075
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001073
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001072
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001071
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001068
Iteration 37/1000 | Loss: 0.00001068
Iteration 38/1000 | Loss: 0.00001068
Iteration 39/1000 | Loss: 0.00001067
Iteration 40/1000 | Loss: 0.00001067
Iteration 41/1000 | Loss: 0.00001067
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001067
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001059
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001058
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001058
Iteration 84/1000 | Loss: 0.00001058
Iteration 85/1000 | Loss: 0.00001058
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001057
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001057
Iteration 98/1000 | Loss: 0.00001057
Iteration 99/1000 | Loss: 0.00001057
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001055
Iteration 115/1000 | Loss: 0.00001055
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001054
Iteration 135/1000 | Loss: 0.00001054
Iteration 136/1000 | Loss: 0.00001054
Iteration 137/1000 | Loss: 0.00001054
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001053
Iteration 145/1000 | Loss: 0.00001053
Iteration 146/1000 | Loss: 0.00001053
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001053
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001052
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001052
Iteration 172/1000 | Loss: 0.00001052
Iteration 173/1000 | Loss: 0.00001052
Iteration 174/1000 | Loss: 0.00001052
Iteration 175/1000 | Loss: 0.00001052
Iteration 176/1000 | Loss: 0.00001052
Iteration 177/1000 | Loss: 0.00001052
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001052
Iteration 183/1000 | Loss: 0.00001052
Iteration 184/1000 | Loss: 0.00001052
Iteration 185/1000 | Loss: 0.00001052
Iteration 186/1000 | Loss: 0.00001052
Iteration 187/1000 | Loss: 0.00001052
Iteration 188/1000 | Loss: 0.00001052
Iteration 189/1000 | Loss: 0.00001052
Iteration 190/1000 | Loss: 0.00001052
Iteration 191/1000 | Loss: 0.00001052
Iteration 192/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.052015068125911e-05, 1.052015068125911e-05, 1.052015068125911e-05, 1.052015068125911e-05, 1.052015068125911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.052015068125911e-05

Optimization complete. Final v2v error: 2.775775671005249 mm

Highest mean error: 3.135326623916626 mm for frame 113

Lowest mean error: 2.5532681941986084 mm for frame 95

Saving results

Total time: 32.829004526138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538392
Iteration 2/25 | Loss: 0.00130232
Iteration 3/25 | Loss: 0.00114552
Iteration 4/25 | Loss: 0.00113192
Iteration 5/25 | Loss: 0.00112887
Iteration 6/25 | Loss: 0.00112887
Iteration 7/25 | Loss: 0.00112887
Iteration 8/25 | Loss: 0.00112887
Iteration 9/25 | Loss: 0.00112887
Iteration 10/25 | Loss: 0.00112887
Iteration 11/25 | Loss: 0.00112887
Iteration 12/25 | Loss: 0.00112887
Iteration 13/25 | Loss: 0.00112887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011288722744211555, 0.0011288722744211555, 0.0011288722744211555, 0.0011288722744211555, 0.0011288722744211555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011288722744211555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75086993
Iteration 2/25 | Loss: 0.00061980
Iteration 3/25 | Loss: 0.00061980
Iteration 4/25 | Loss: 0.00061980
Iteration 5/25 | Loss: 0.00061979
Iteration 6/25 | Loss: 0.00061979
Iteration 7/25 | Loss: 0.00061979
Iteration 8/25 | Loss: 0.00061979
Iteration 9/25 | Loss: 0.00061979
Iteration 10/25 | Loss: 0.00061979
Iteration 11/25 | Loss: 0.00061979
Iteration 12/25 | Loss: 0.00061979
Iteration 13/25 | Loss: 0.00061979
Iteration 14/25 | Loss: 0.00061979
Iteration 15/25 | Loss: 0.00061979
Iteration 16/25 | Loss: 0.00061979
Iteration 17/25 | Loss: 0.00061979
Iteration 18/25 | Loss: 0.00061979
Iteration 19/25 | Loss: 0.00061979
Iteration 20/25 | Loss: 0.00061979
Iteration 21/25 | Loss: 0.00061979
Iteration 22/25 | Loss: 0.00061979
Iteration 23/25 | Loss: 0.00061979
Iteration 24/25 | Loss: 0.00061979
Iteration 25/25 | Loss: 0.00061979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061979
Iteration 2/1000 | Loss: 0.00004802
Iteration 3/1000 | Loss: 0.00004334
Iteration 4/1000 | Loss: 0.00004145
Iteration 5/1000 | Loss: 0.00004037
Iteration 6/1000 | Loss: 0.00003968
Iteration 7/1000 | Loss: 0.00003922
Iteration 8/1000 | Loss: 0.00003884
Iteration 9/1000 | Loss: 0.00003864
Iteration 10/1000 | Loss: 0.00003846
Iteration 11/1000 | Loss: 0.00003829
Iteration 12/1000 | Loss: 0.00003808
Iteration 13/1000 | Loss: 0.00003781
Iteration 14/1000 | Loss: 0.00003760
Iteration 15/1000 | Loss: 0.00003742
Iteration 16/1000 | Loss: 0.00003727
Iteration 17/1000 | Loss: 0.00003710
Iteration 18/1000 | Loss: 0.00003697
Iteration 19/1000 | Loss: 0.00003697
Iteration 20/1000 | Loss: 0.00003692
Iteration 21/1000 | Loss: 0.00003692
Iteration 22/1000 | Loss: 0.00003692
Iteration 23/1000 | Loss: 0.00003692
Iteration 24/1000 | Loss: 0.00003691
Iteration 25/1000 | Loss: 0.00003691
Iteration 26/1000 | Loss: 0.00003689
Iteration 27/1000 | Loss: 0.00003689
Iteration 28/1000 | Loss: 0.00003688
Iteration 29/1000 | Loss: 0.00003688
Iteration 30/1000 | Loss: 0.00003687
Iteration 31/1000 | Loss: 0.00003687
Iteration 32/1000 | Loss: 0.00003687
Iteration 33/1000 | Loss: 0.00003687
Iteration 34/1000 | Loss: 0.00003687
Iteration 35/1000 | Loss: 0.00003687
Iteration 36/1000 | Loss: 0.00003687
Iteration 37/1000 | Loss: 0.00003687
Iteration 38/1000 | Loss: 0.00003686
Iteration 39/1000 | Loss: 0.00003686
Iteration 40/1000 | Loss: 0.00003686
Iteration 41/1000 | Loss: 0.00003686
Iteration 42/1000 | Loss: 0.00003685
Iteration 43/1000 | Loss: 0.00003685
Iteration 44/1000 | Loss: 0.00003685
Iteration 45/1000 | Loss: 0.00003685
Iteration 46/1000 | Loss: 0.00003685
Iteration 47/1000 | Loss: 0.00003685
Iteration 48/1000 | Loss: 0.00003685
Iteration 49/1000 | Loss: 0.00003685
Iteration 50/1000 | Loss: 0.00003685
Iteration 51/1000 | Loss: 0.00003684
Iteration 52/1000 | Loss: 0.00003684
Iteration 53/1000 | Loss: 0.00003684
Iteration 54/1000 | Loss: 0.00003683
Iteration 55/1000 | Loss: 0.00003683
Iteration 56/1000 | Loss: 0.00003683
Iteration 57/1000 | Loss: 0.00003681
Iteration 58/1000 | Loss: 0.00003681
Iteration 59/1000 | Loss: 0.00003681
Iteration 60/1000 | Loss: 0.00003681
Iteration 61/1000 | Loss: 0.00003680
Iteration 62/1000 | Loss: 0.00003678
Iteration 63/1000 | Loss: 0.00003678
Iteration 64/1000 | Loss: 0.00003677
Iteration 65/1000 | Loss: 0.00003677
Iteration 66/1000 | Loss: 0.00003677
Iteration 67/1000 | Loss: 0.00003677
Iteration 68/1000 | Loss: 0.00003677
Iteration 69/1000 | Loss: 0.00003677
Iteration 70/1000 | Loss: 0.00003677
Iteration 71/1000 | Loss: 0.00003677
Iteration 72/1000 | Loss: 0.00003676
Iteration 73/1000 | Loss: 0.00003676
Iteration 74/1000 | Loss: 0.00003676
Iteration 75/1000 | Loss: 0.00003675
Iteration 76/1000 | Loss: 0.00003674
Iteration 77/1000 | Loss: 0.00003674
Iteration 78/1000 | Loss: 0.00003674
Iteration 79/1000 | Loss: 0.00003674
Iteration 80/1000 | Loss: 0.00003673
Iteration 81/1000 | Loss: 0.00003673
Iteration 82/1000 | Loss: 0.00003673
Iteration 83/1000 | Loss: 0.00003673
Iteration 84/1000 | Loss: 0.00003673
Iteration 85/1000 | Loss: 0.00003673
Iteration 86/1000 | Loss: 0.00003673
Iteration 87/1000 | Loss: 0.00003673
Iteration 88/1000 | Loss: 0.00003673
Iteration 89/1000 | Loss: 0.00003673
Iteration 90/1000 | Loss: 0.00003673
Iteration 91/1000 | Loss: 0.00003672
Iteration 92/1000 | Loss: 0.00003672
Iteration 93/1000 | Loss: 0.00003672
Iteration 94/1000 | Loss: 0.00003672
Iteration 95/1000 | Loss: 0.00003672
Iteration 96/1000 | Loss: 0.00003672
Iteration 97/1000 | Loss: 0.00003672
Iteration 98/1000 | Loss: 0.00003672
Iteration 99/1000 | Loss: 0.00003672
Iteration 100/1000 | Loss: 0.00003672
Iteration 101/1000 | Loss: 0.00003671
Iteration 102/1000 | Loss: 0.00003671
Iteration 103/1000 | Loss: 0.00003671
Iteration 104/1000 | Loss: 0.00003671
Iteration 105/1000 | Loss: 0.00003671
Iteration 106/1000 | Loss: 0.00003671
Iteration 107/1000 | Loss: 0.00003671
Iteration 108/1000 | Loss: 0.00003671
Iteration 109/1000 | Loss: 0.00003671
Iteration 110/1000 | Loss: 0.00003671
Iteration 111/1000 | Loss: 0.00003671
Iteration 112/1000 | Loss: 0.00003670
Iteration 113/1000 | Loss: 0.00003670
Iteration 114/1000 | Loss: 0.00003669
Iteration 115/1000 | Loss: 0.00003669
Iteration 116/1000 | Loss: 0.00003669
Iteration 117/1000 | Loss: 0.00003669
Iteration 118/1000 | Loss: 0.00003668
Iteration 119/1000 | Loss: 0.00003668
Iteration 120/1000 | Loss: 0.00003668
Iteration 121/1000 | Loss: 0.00003668
Iteration 122/1000 | Loss: 0.00003668
Iteration 123/1000 | Loss: 0.00003668
Iteration 124/1000 | Loss: 0.00003668
Iteration 125/1000 | Loss: 0.00003668
Iteration 126/1000 | Loss: 0.00003667
Iteration 127/1000 | Loss: 0.00003667
Iteration 128/1000 | Loss: 0.00003667
Iteration 129/1000 | Loss: 0.00003667
Iteration 130/1000 | Loss: 0.00003666
Iteration 131/1000 | Loss: 0.00003666
Iteration 132/1000 | Loss: 0.00003666
Iteration 133/1000 | Loss: 0.00003666
Iteration 134/1000 | Loss: 0.00003666
Iteration 135/1000 | Loss: 0.00003666
Iteration 136/1000 | Loss: 0.00003666
Iteration 137/1000 | Loss: 0.00003665
Iteration 138/1000 | Loss: 0.00003665
Iteration 139/1000 | Loss: 0.00003665
Iteration 140/1000 | Loss: 0.00003665
Iteration 141/1000 | Loss: 0.00003664
Iteration 142/1000 | Loss: 0.00003664
Iteration 143/1000 | Loss: 0.00003664
Iteration 144/1000 | Loss: 0.00003664
Iteration 145/1000 | Loss: 0.00003664
Iteration 146/1000 | Loss: 0.00003663
Iteration 147/1000 | Loss: 0.00003663
Iteration 148/1000 | Loss: 0.00003663
Iteration 149/1000 | Loss: 0.00003663
Iteration 150/1000 | Loss: 0.00003663
Iteration 151/1000 | Loss: 0.00003663
Iteration 152/1000 | Loss: 0.00003663
Iteration 153/1000 | Loss: 0.00003663
Iteration 154/1000 | Loss: 0.00003663
Iteration 155/1000 | Loss: 0.00003663
Iteration 156/1000 | Loss: 0.00003663
Iteration 157/1000 | Loss: 0.00003663
Iteration 158/1000 | Loss: 0.00003663
Iteration 159/1000 | Loss: 0.00003663
Iteration 160/1000 | Loss: 0.00003663
Iteration 161/1000 | Loss: 0.00003663
Iteration 162/1000 | Loss: 0.00003663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.662544986582361e-05, 3.662544986582361e-05, 3.662544986582361e-05, 3.662544986582361e-05, 3.662544986582361e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.662544986582361e-05

Optimization complete. Final v2v error: 4.618542194366455 mm

Highest mean error: 4.679447650909424 mm for frame 0

Lowest mean error: 4.578212738037109 mm for frame 4

Saving results

Total time: 50.90815472602844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123473
Iteration 2/25 | Loss: 0.01123473
Iteration 3/25 | Loss: 0.01123473
Iteration 4/25 | Loss: 0.01123473
Iteration 5/25 | Loss: 0.01123473
Iteration 6/25 | Loss: 0.01123473
Iteration 7/25 | Loss: 0.01123473
Iteration 8/25 | Loss: 0.01123472
Iteration 9/25 | Loss: 0.01123472
Iteration 10/25 | Loss: 0.01123472
Iteration 11/25 | Loss: 0.01123472
Iteration 12/25 | Loss: 0.01123472
Iteration 13/25 | Loss: 0.01123472
Iteration 14/25 | Loss: 0.01123472
Iteration 15/25 | Loss: 0.01123472
Iteration 16/25 | Loss: 0.01123472
Iteration 17/25 | Loss: 0.01123472
Iteration 18/25 | Loss: 0.01123472
Iteration 19/25 | Loss: 0.01123472
Iteration 20/25 | Loss: 0.01123472
Iteration 21/25 | Loss: 0.01123471
Iteration 22/25 | Loss: 0.01123471
Iteration 23/25 | Loss: 0.01123471
Iteration 24/25 | Loss: 0.01123471
Iteration 25/25 | Loss: 0.01123471

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60372841
Iteration 2/25 | Loss: 0.07874314
Iteration 3/25 | Loss: 0.07874312
Iteration 4/25 | Loss: 0.07874312
Iteration 5/25 | Loss: 0.07874311
Iteration 6/25 | Loss: 0.07874311
Iteration 7/25 | Loss: 0.07874311
Iteration 8/25 | Loss: 0.07874311
Iteration 9/25 | Loss: 0.07874311
Iteration 10/25 | Loss: 0.07874311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0787431076169014, 0.0787431076169014, 0.0787431076169014, 0.0787431076169014, 0.0787431076169014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0787431076169014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07874311
Iteration 2/1000 | Loss: 0.00049907
Iteration 3/1000 | Loss: 0.00015083
Iteration 4/1000 | Loss: 0.00005794
Iteration 5/1000 | Loss: 0.00003146
Iteration 6/1000 | Loss: 0.00002492
Iteration 7/1000 | Loss: 0.00002164
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001446
Iteration 11/1000 | Loss: 0.00001340
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001086
Iteration 16/1000 | Loss: 0.00001050
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000971
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000949
Iteration 23/1000 | Loss: 0.00000949
Iteration 24/1000 | Loss: 0.00000947
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000915
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000897
Iteration 30/1000 | Loss: 0.00000895
Iteration 31/1000 | Loss: 0.00000894
Iteration 32/1000 | Loss: 0.00000894
Iteration 33/1000 | Loss: 0.00000893
Iteration 34/1000 | Loss: 0.00000892
Iteration 35/1000 | Loss: 0.00000892
Iteration 36/1000 | Loss: 0.00000892
Iteration 37/1000 | Loss: 0.00000891
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000891
Iteration 40/1000 | Loss: 0.00000891
Iteration 41/1000 | Loss: 0.00000890
Iteration 42/1000 | Loss: 0.00000890
Iteration 43/1000 | Loss: 0.00000888
Iteration 44/1000 | Loss: 0.00000887
Iteration 45/1000 | Loss: 0.00000886
Iteration 46/1000 | Loss: 0.00000880
Iteration 47/1000 | Loss: 0.00000880
Iteration 48/1000 | Loss: 0.00000880
Iteration 49/1000 | Loss: 0.00000880
Iteration 50/1000 | Loss: 0.00000880
Iteration 51/1000 | Loss: 0.00000880
Iteration 52/1000 | Loss: 0.00000880
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000880
Iteration 55/1000 | Loss: 0.00000880
Iteration 56/1000 | Loss: 0.00000880
Iteration 57/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [8.799307579465676e-06, 8.799307579465676e-06, 8.799307579465676e-06, 8.799307579465676e-06, 8.799307579465676e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.799307579465676e-06

Optimization complete. Final v2v error: 2.5673987865448 mm

Highest mean error: 2.8103103637695312 mm for frame 159

Lowest mean error: 2.387416362762451 mm for frame 183

Saving results

Total time: 40.41307735443115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022393
Iteration 2/25 | Loss: 0.00471618
Iteration 3/25 | Loss: 0.00241580
Iteration 4/25 | Loss: 0.00193404
Iteration 5/25 | Loss: 0.00166840
Iteration 6/25 | Loss: 0.00158432
Iteration 7/25 | Loss: 0.00155287
Iteration 8/25 | Loss: 0.00151550
Iteration 9/25 | Loss: 0.00141194
Iteration 10/25 | Loss: 0.00133664
Iteration 11/25 | Loss: 0.00128531
Iteration 12/25 | Loss: 0.00124942
Iteration 13/25 | Loss: 0.00121652
Iteration 14/25 | Loss: 0.00121568
Iteration 15/25 | Loss: 0.00119699
Iteration 16/25 | Loss: 0.00119209
Iteration 17/25 | Loss: 0.00117681
Iteration 18/25 | Loss: 0.00116868
Iteration 19/25 | Loss: 0.00116188
Iteration 20/25 | Loss: 0.00115860
Iteration 21/25 | Loss: 0.00116626
Iteration 22/25 | Loss: 0.00115912
Iteration 23/25 | Loss: 0.00115247
Iteration 24/25 | Loss: 0.00115502
Iteration 25/25 | Loss: 0.00114491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33234882
Iteration 2/25 | Loss: 0.00286220
Iteration 3/25 | Loss: 0.00266950
Iteration 4/25 | Loss: 0.00266949
Iteration 5/25 | Loss: 0.00266949
Iteration 6/25 | Loss: 0.00266949
Iteration 7/25 | Loss: 0.00266949
Iteration 8/25 | Loss: 0.00266949
Iteration 9/25 | Loss: 0.00266949
Iteration 10/25 | Loss: 0.00266949
Iteration 11/25 | Loss: 0.00266949
Iteration 12/25 | Loss: 0.00266949
Iteration 13/25 | Loss: 0.00266949
Iteration 14/25 | Loss: 0.00266949
Iteration 15/25 | Loss: 0.00266949
Iteration 16/25 | Loss: 0.00266949
Iteration 17/25 | Loss: 0.00266949
Iteration 18/25 | Loss: 0.00266949
Iteration 19/25 | Loss: 0.00266949
Iteration 20/25 | Loss: 0.00266949
Iteration 21/25 | Loss: 0.00266949
Iteration 22/25 | Loss: 0.00266949
Iteration 23/25 | Loss: 0.00266949
Iteration 24/25 | Loss: 0.00266949
Iteration 25/25 | Loss: 0.00266949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266949
Iteration 2/1000 | Loss: 0.00108128
Iteration 3/1000 | Loss: 0.00187296
Iteration 4/1000 | Loss: 0.00055406
Iteration 5/1000 | Loss: 0.00259384
Iteration 6/1000 | Loss: 0.00031001
Iteration 7/1000 | Loss: 0.00031361
Iteration 8/1000 | Loss: 0.00121534
Iteration 9/1000 | Loss: 0.00022900
Iteration 10/1000 | Loss: 0.00087791
Iteration 11/1000 | Loss: 0.00096584
Iteration 12/1000 | Loss: 0.00098411
Iteration 13/1000 | Loss: 0.00050231
Iteration 14/1000 | Loss: 0.00032888
Iteration 15/1000 | Loss: 0.00036501
Iteration 16/1000 | Loss: 0.00030048
Iteration 17/1000 | Loss: 0.00035392
Iteration 18/1000 | Loss: 0.00026806
Iteration 19/1000 | Loss: 0.00026236
Iteration 20/1000 | Loss: 0.00108238
Iteration 21/1000 | Loss: 0.00041245
Iteration 22/1000 | Loss: 0.00040191
Iteration 23/1000 | Loss: 0.00041754
Iteration 24/1000 | Loss: 0.00049922
Iteration 25/1000 | Loss: 0.00066401
Iteration 26/1000 | Loss: 0.00036719
Iteration 27/1000 | Loss: 0.00006949
Iteration 28/1000 | Loss: 0.00007966
Iteration 29/1000 | Loss: 0.00025415
Iteration 30/1000 | Loss: 0.00057411
Iteration 31/1000 | Loss: 0.00007907
Iteration 32/1000 | Loss: 0.00021589
Iteration 33/1000 | Loss: 0.00034645
Iteration 34/1000 | Loss: 0.00005275
Iteration 35/1000 | Loss: 0.00004249
Iteration 36/1000 | Loss: 0.00005418
Iteration 37/1000 | Loss: 0.00003041
Iteration 38/1000 | Loss: 0.00020325
Iteration 39/1000 | Loss: 0.00030314
Iteration 40/1000 | Loss: 0.00029317
Iteration 41/1000 | Loss: 0.00031900
Iteration 42/1000 | Loss: 0.00004721
Iteration 43/1000 | Loss: 0.00006608
Iteration 44/1000 | Loss: 0.00009314
Iteration 45/1000 | Loss: 0.00003630
Iteration 46/1000 | Loss: 0.00005818
Iteration 47/1000 | Loss: 0.00004753
Iteration 48/1000 | Loss: 0.00004316
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00002815
Iteration 51/1000 | Loss: 0.00005260
Iteration 52/1000 | Loss: 0.00003130
Iteration 53/1000 | Loss: 0.00001830
Iteration 54/1000 | Loss: 0.00003660
Iteration 55/1000 | Loss: 0.00002689
Iteration 56/1000 | Loss: 0.00002961
Iteration 57/1000 | Loss: 0.00003505
Iteration 58/1000 | Loss: 0.00002838
Iteration 59/1000 | Loss: 0.00003462
Iteration 60/1000 | Loss: 0.00003332
Iteration 61/1000 | Loss: 0.00002755
Iteration 62/1000 | Loss: 0.00003802
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00004079
Iteration 65/1000 | Loss: 0.00002655
Iteration 66/1000 | Loss: 0.00020332
Iteration 67/1000 | Loss: 0.00026995
Iteration 68/1000 | Loss: 0.00017552
Iteration 69/1000 | Loss: 0.00014027
Iteration 70/1000 | Loss: 0.00003308
Iteration 71/1000 | Loss: 0.00005726
Iteration 72/1000 | Loss: 0.00005291
Iteration 73/1000 | Loss: 0.00036094
Iteration 74/1000 | Loss: 0.00034419
Iteration 75/1000 | Loss: 0.00007743
Iteration 76/1000 | Loss: 0.00010252
Iteration 77/1000 | Loss: 0.00003075
Iteration 78/1000 | Loss: 0.00002899
Iteration 79/1000 | Loss: 0.00002599
Iteration 80/1000 | Loss: 0.00001880
Iteration 81/1000 | Loss: 0.00001737
Iteration 82/1000 | Loss: 0.00016492
Iteration 83/1000 | Loss: 0.00008631
Iteration 84/1000 | Loss: 0.00015664
Iteration 85/1000 | Loss: 0.00007920
Iteration 86/1000 | Loss: 0.00014018
Iteration 87/1000 | Loss: 0.00012095
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002674
Iteration 90/1000 | Loss: 0.00002614
Iteration 91/1000 | Loss: 0.00002582
Iteration 92/1000 | Loss: 0.00001673
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00020348
Iteration 95/1000 | Loss: 0.00028864
Iteration 96/1000 | Loss: 0.00009796
Iteration 97/1000 | Loss: 0.00005658
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00038962
Iteration 100/1000 | Loss: 0.00025546
Iteration 101/1000 | Loss: 0.00015299
Iteration 102/1000 | Loss: 0.00011593
Iteration 103/1000 | Loss: 0.00007091
Iteration 104/1000 | Loss: 0.00010439
Iteration 105/1000 | Loss: 0.00007931
Iteration 106/1000 | Loss: 0.00009789
Iteration 107/1000 | Loss: 0.00007720
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00002095
Iteration 110/1000 | Loss: 0.00001687
Iteration 111/1000 | Loss: 0.00007330
Iteration 112/1000 | Loss: 0.00026553
Iteration 113/1000 | Loss: 0.00025600
Iteration 114/1000 | Loss: 0.00028276
Iteration 115/1000 | Loss: 0.00039120
Iteration 116/1000 | Loss: 0.00019542
Iteration 117/1000 | Loss: 0.00009769
Iteration 118/1000 | Loss: 0.00012794
Iteration 119/1000 | Loss: 0.00013138
Iteration 120/1000 | Loss: 0.00005711
Iteration 121/1000 | Loss: 0.00006612
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001336
Iteration 126/1000 | Loss: 0.00002389
Iteration 127/1000 | Loss: 0.00001352
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00000913
Iteration 131/1000 | Loss: 0.00001981
Iteration 132/1000 | Loss: 0.00001010
Iteration 133/1000 | Loss: 0.00000881
Iteration 134/1000 | Loss: 0.00000947
Iteration 135/1000 | Loss: 0.00000959
Iteration 136/1000 | Loss: 0.00000870
Iteration 137/1000 | Loss: 0.00000870
Iteration 138/1000 | Loss: 0.00000870
Iteration 139/1000 | Loss: 0.00000870
Iteration 140/1000 | Loss: 0.00000870
Iteration 141/1000 | Loss: 0.00000870
Iteration 142/1000 | Loss: 0.00000870
Iteration 143/1000 | Loss: 0.00000870
Iteration 144/1000 | Loss: 0.00000870
Iteration 145/1000 | Loss: 0.00000870
Iteration 146/1000 | Loss: 0.00000870
Iteration 147/1000 | Loss: 0.00000869
Iteration 148/1000 | Loss: 0.00000869
Iteration 149/1000 | Loss: 0.00000869
Iteration 150/1000 | Loss: 0.00000868
Iteration 151/1000 | Loss: 0.00000868
Iteration 152/1000 | Loss: 0.00000868
Iteration 153/1000 | Loss: 0.00000867
Iteration 154/1000 | Loss: 0.00000867
Iteration 155/1000 | Loss: 0.00000867
Iteration 156/1000 | Loss: 0.00000903
Iteration 157/1000 | Loss: 0.00000865
Iteration 158/1000 | Loss: 0.00000865
Iteration 159/1000 | Loss: 0.00000865
Iteration 160/1000 | Loss: 0.00000865
Iteration 161/1000 | Loss: 0.00000865
Iteration 162/1000 | Loss: 0.00000865
Iteration 163/1000 | Loss: 0.00001498
Iteration 164/1000 | Loss: 0.00000865
Iteration 165/1000 | Loss: 0.00000966
Iteration 166/1000 | Loss: 0.00000862
Iteration 167/1000 | Loss: 0.00000862
Iteration 168/1000 | Loss: 0.00000862
Iteration 169/1000 | Loss: 0.00000862
Iteration 170/1000 | Loss: 0.00000862
Iteration 171/1000 | Loss: 0.00000861
Iteration 172/1000 | Loss: 0.00000861
Iteration 173/1000 | Loss: 0.00000861
Iteration 174/1000 | Loss: 0.00000861
Iteration 175/1000 | Loss: 0.00000861
Iteration 176/1000 | Loss: 0.00000861
Iteration 177/1000 | Loss: 0.00000860
Iteration 178/1000 | Loss: 0.00000860
Iteration 179/1000 | Loss: 0.00000860
Iteration 180/1000 | Loss: 0.00000860
Iteration 181/1000 | Loss: 0.00000859
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001366
Iteration 184/1000 | Loss: 0.00000863
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00000858
Iteration 187/1000 | Loss: 0.00000857
Iteration 188/1000 | Loss: 0.00000857
Iteration 189/1000 | Loss: 0.00000857
Iteration 190/1000 | Loss: 0.00000857
Iteration 191/1000 | Loss: 0.00002074
Iteration 192/1000 | Loss: 0.00001938
Iteration 193/1000 | Loss: 0.00001816
Iteration 194/1000 | Loss: 0.00001466
Iteration 195/1000 | Loss: 0.00001826
Iteration 196/1000 | Loss: 0.00000975
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000963
Iteration 199/1000 | Loss: 0.00001614
Iteration 200/1000 | Loss: 0.00001182
Iteration 201/1000 | Loss: 0.00000959
Iteration 202/1000 | Loss: 0.00001021
Iteration 203/1000 | Loss: 0.00001875
Iteration 204/1000 | Loss: 0.00000905
Iteration 205/1000 | Loss: 0.00000899
Iteration 206/1000 | Loss: 0.00000897
Iteration 207/1000 | Loss: 0.00000897
Iteration 208/1000 | Loss: 0.00000896
Iteration 209/1000 | Loss: 0.00000896
Iteration 210/1000 | Loss: 0.00000896
Iteration 211/1000 | Loss: 0.00000896
Iteration 212/1000 | Loss: 0.00000896
Iteration 213/1000 | Loss: 0.00000896
Iteration 214/1000 | Loss: 0.00002165
Iteration 215/1000 | Loss: 0.00000875
Iteration 216/1000 | Loss: 0.00001669
Iteration 217/1000 | Loss: 0.00000944
Iteration 218/1000 | Loss: 0.00000851
Iteration 219/1000 | Loss: 0.00000851
Iteration 220/1000 | Loss: 0.00000851
Iteration 221/1000 | Loss: 0.00000851
Iteration 222/1000 | Loss: 0.00002450
Iteration 223/1000 | Loss: 0.00001085
Iteration 224/1000 | Loss: 0.00000837
Iteration 225/1000 | Loss: 0.00002391
Iteration 226/1000 | Loss: 0.00001162
Iteration 227/1000 | Loss: 0.00000825
Iteration 228/1000 | Loss: 0.00001158
Iteration 229/1000 | Loss: 0.00000804
Iteration 230/1000 | Loss: 0.00000803
Iteration 231/1000 | Loss: 0.00000802
Iteration 232/1000 | Loss: 0.00000820
Iteration 233/1000 | Loss: 0.00000797
Iteration 234/1000 | Loss: 0.00000797
Iteration 235/1000 | Loss: 0.00000795
Iteration 236/1000 | Loss: 0.00001244
Iteration 237/1000 | Loss: 0.00000790
Iteration 238/1000 | Loss: 0.00000789
Iteration 239/1000 | Loss: 0.00000789
Iteration 240/1000 | Loss: 0.00000789
Iteration 241/1000 | Loss: 0.00000789
Iteration 242/1000 | Loss: 0.00000788
Iteration 243/1000 | Loss: 0.00000788
Iteration 244/1000 | Loss: 0.00000788
Iteration 245/1000 | Loss: 0.00000788
Iteration 246/1000 | Loss: 0.00000783
Iteration 247/1000 | Loss: 0.00000783
Iteration 248/1000 | Loss: 0.00000783
Iteration 249/1000 | Loss: 0.00000783
Iteration 250/1000 | Loss: 0.00000782
Iteration 251/1000 | Loss: 0.00000782
Iteration 252/1000 | Loss: 0.00000782
Iteration 253/1000 | Loss: 0.00000782
Iteration 254/1000 | Loss: 0.00000782
Iteration 255/1000 | Loss: 0.00000782
Iteration 256/1000 | Loss: 0.00000782
Iteration 257/1000 | Loss: 0.00000782
Iteration 258/1000 | Loss: 0.00000782
Iteration 259/1000 | Loss: 0.00000782
Iteration 260/1000 | Loss: 0.00000781
Iteration 261/1000 | Loss: 0.00000781
Iteration 262/1000 | Loss: 0.00000781
Iteration 263/1000 | Loss: 0.00000781
Iteration 264/1000 | Loss: 0.00000781
Iteration 265/1000 | Loss: 0.00000780
Iteration 266/1000 | Loss: 0.00000780
Iteration 267/1000 | Loss: 0.00000780
Iteration 268/1000 | Loss: 0.00000780
Iteration 269/1000 | Loss: 0.00000780
Iteration 270/1000 | Loss: 0.00000780
Iteration 271/1000 | Loss: 0.00000780
Iteration 272/1000 | Loss: 0.00000780
Iteration 273/1000 | Loss: 0.00000780
Iteration 274/1000 | Loss: 0.00000780
Iteration 275/1000 | Loss: 0.00000780
Iteration 276/1000 | Loss: 0.00000780
Iteration 277/1000 | Loss: 0.00000780
Iteration 278/1000 | Loss: 0.00000780
Iteration 279/1000 | Loss: 0.00000780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [7.80050231696805e-06, 7.80050231696805e-06, 7.80050231696805e-06, 7.80050231696805e-06, 7.80050231696805e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.80050231696805e-06

Optimization complete. Final v2v error: 2.330294370651245 mm

Highest mean error: 9.066998481750488 mm for frame 84

Lowest mean error: 1.9975883960723877 mm for frame 93

Saving results

Total time: 315.4415760040283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413754
Iteration 2/25 | Loss: 0.00103395
Iteration 3/25 | Loss: 0.00093316
Iteration 4/25 | Loss: 0.00092402
Iteration 5/25 | Loss: 0.00092144
Iteration 6/25 | Loss: 0.00092067
Iteration 7/25 | Loss: 0.00092067
Iteration 8/25 | Loss: 0.00092067
Iteration 9/25 | Loss: 0.00092067
Iteration 10/25 | Loss: 0.00092067
Iteration 11/25 | Loss: 0.00092067
Iteration 12/25 | Loss: 0.00092067
Iteration 13/25 | Loss: 0.00092067
Iteration 14/25 | Loss: 0.00092067
Iteration 15/25 | Loss: 0.00092067
Iteration 16/25 | Loss: 0.00092067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009206727845594287, 0.0009206727845594287, 0.0009206727845594287, 0.0009206727845594287, 0.0009206727845594287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009206727845594287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34275019
Iteration 2/25 | Loss: 0.00057548
Iteration 3/25 | Loss: 0.00057548
Iteration 4/25 | Loss: 0.00057548
Iteration 5/25 | Loss: 0.00057548
Iteration 6/25 | Loss: 0.00057548
Iteration 7/25 | Loss: 0.00057548
Iteration 8/25 | Loss: 0.00057548
Iteration 9/25 | Loss: 0.00057548
Iteration 10/25 | Loss: 0.00057548
Iteration 11/25 | Loss: 0.00057548
Iteration 12/25 | Loss: 0.00057548
Iteration 13/25 | Loss: 0.00057548
Iteration 14/25 | Loss: 0.00057548
Iteration 15/25 | Loss: 0.00057548
Iteration 16/25 | Loss: 0.00057548
Iteration 17/25 | Loss: 0.00057548
Iteration 18/25 | Loss: 0.00057548
Iteration 19/25 | Loss: 0.00057548
Iteration 20/25 | Loss: 0.00057548
Iteration 21/25 | Loss: 0.00057548
Iteration 22/25 | Loss: 0.00057548
Iteration 23/25 | Loss: 0.00057548
Iteration 24/25 | Loss: 0.00057548
Iteration 25/25 | Loss: 0.00057548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057548
Iteration 2/1000 | Loss: 0.00002051
Iteration 3/1000 | Loss: 0.00001063
Iteration 4/1000 | Loss: 0.00000945
Iteration 5/1000 | Loss: 0.00000893
Iteration 6/1000 | Loss: 0.00000869
Iteration 7/1000 | Loss: 0.00000843
Iteration 8/1000 | Loss: 0.00000818
Iteration 9/1000 | Loss: 0.00000815
Iteration 10/1000 | Loss: 0.00000808
Iteration 11/1000 | Loss: 0.00000794
Iteration 12/1000 | Loss: 0.00000791
Iteration 13/1000 | Loss: 0.00000782
Iteration 14/1000 | Loss: 0.00000782
Iteration 15/1000 | Loss: 0.00000781
Iteration 16/1000 | Loss: 0.00000781
Iteration 17/1000 | Loss: 0.00000780
Iteration 18/1000 | Loss: 0.00000780
Iteration 19/1000 | Loss: 0.00000779
Iteration 20/1000 | Loss: 0.00000778
Iteration 21/1000 | Loss: 0.00000776
Iteration 22/1000 | Loss: 0.00000776
Iteration 23/1000 | Loss: 0.00000775
Iteration 24/1000 | Loss: 0.00000775
Iteration 25/1000 | Loss: 0.00000775
Iteration 26/1000 | Loss: 0.00000775
Iteration 27/1000 | Loss: 0.00000774
Iteration 28/1000 | Loss: 0.00000774
Iteration 29/1000 | Loss: 0.00000773
Iteration 30/1000 | Loss: 0.00000773
Iteration 31/1000 | Loss: 0.00000773
Iteration 32/1000 | Loss: 0.00000772
Iteration 33/1000 | Loss: 0.00000772
Iteration 34/1000 | Loss: 0.00000772
Iteration 35/1000 | Loss: 0.00000772
Iteration 36/1000 | Loss: 0.00000771
Iteration 37/1000 | Loss: 0.00000771
Iteration 38/1000 | Loss: 0.00000771
Iteration 39/1000 | Loss: 0.00000771
Iteration 40/1000 | Loss: 0.00000770
Iteration 41/1000 | Loss: 0.00000770
Iteration 42/1000 | Loss: 0.00000770
Iteration 43/1000 | Loss: 0.00000770
Iteration 44/1000 | Loss: 0.00000770
Iteration 45/1000 | Loss: 0.00000770
Iteration 46/1000 | Loss: 0.00000770
Iteration 47/1000 | Loss: 0.00000770
Iteration 48/1000 | Loss: 0.00000770
Iteration 49/1000 | Loss: 0.00000770
Iteration 50/1000 | Loss: 0.00000770
Iteration 51/1000 | Loss: 0.00000770
Iteration 52/1000 | Loss: 0.00000769
Iteration 53/1000 | Loss: 0.00000769
Iteration 54/1000 | Loss: 0.00000769
Iteration 55/1000 | Loss: 0.00000769
Iteration 56/1000 | Loss: 0.00000768
Iteration 57/1000 | Loss: 0.00000768
Iteration 58/1000 | Loss: 0.00000767
Iteration 59/1000 | Loss: 0.00000767
Iteration 60/1000 | Loss: 0.00000767
Iteration 61/1000 | Loss: 0.00000767
Iteration 62/1000 | Loss: 0.00000767
Iteration 63/1000 | Loss: 0.00000767
Iteration 64/1000 | Loss: 0.00000767
Iteration 65/1000 | Loss: 0.00000766
Iteration 66/1000 | Loss: 0.00000766
Iteration 67/1000 | Loss: 0.00000766
Iteration 68/1000 | Loss: 0.00000765
Iteration 69/1000 | Loss: 0.00000765
Iteration 70/1000 | Loss: 0.00000764
Iteration 71/1000 | Loss: 0.00000764
Iteration 72/1000 | Loss: 0.00000764
Iteration 73/1000 | Loss: 0.00000764
Iteration 74/1000 | Loss: 0.00000764
Iteration 75/1000 | Loss: 0.00000764
Iteration 76/1000 | Loss: 0.00000763
Iteration 77/1000 | Loss: 0.00000763
Iteration 78/1000 | Loss: 0.00000763
Iteration 79/1000 | Loss: 0.00000763
Iteration 80/1000 | Loss: 0.00000763
Iteration 81/1000 | Loss: 0.00000763
Iteration 82/1000 | Loss: 0.00000763
Iteration 83/1000 | Loss: 0.00000763
Iteration 84/1000 | Loss: 0.00000763
Iteration 85/1000 | Loss: 0.00000762
Iteration 86/1000 | Loss: 0.00000762
Iteration 87/1000 | Loss: 0.00000762
Iteration 88/1000 | Loss: 0.00000761
Iteration 89/1000 | Loss: 0.00000761
Iteration 90/1000 | Loss: 0.00000761
Iteration 91/1000 | Loss: 0.00000761
Iteration 92/1000 | Loss: 0.00000761
Iteration 93/1000 | Loss: 0.00000761
Iteration 94/1000 | Loss: 0.00000761
Iteration 95/1000 | Loss: 0.00000761
Iteration 96/1000 | Loss: 0.00000761
Iteration 97/1000 | Loss: 0.00000761
Iteration 98/1000 | Loss: 0.00000761
Iteration 99/1000 | Loss: 0.00000761
Iteration 100/1000 | Loss: 0.00000761
Iteration 101/1000 | Loss: 0.00000761
Iteration 102/1000 | Loss: 0.00000761
Iteration 103/1000 | Loss: 0.00000761
Iteration 104/1000 | Loss: 0.00000761
Iteration 105/1000 | Loss: 0.00000761
Iteration 106/1000 | Loss: 0.00000761
Iteration 107/1000 | Loss: 0.00000761
Iteration 108/1000 | Loss: 0.00000761
Iteration 109/1000 | Loss: 0.00000761
Iteration 110/1000 | Loss: 0.00000761
Iteration 111/1000 | Loss: 0.00000761
Iteration 112/1000 | Loss: 0.00000761
Iteration 113/1000 | Loss: 0.00000761
Iteration 114/1000 | Loss: 0.00000761
Iteration 115/1000 | Loss: 0.00000761
Iteration 116/1000 | Loss: 0.00000761
Iteration 117/1000 | Loss: 0.00000761
Iteration 118/1000 | Loss: 0.00000761
Iteration 119/1000 | Loss: 0.00000761
Iteration 120/1000 | Loss: 0.00000761
Iteration 121/1000 | Loss: 0.00000761
Iteration 122/1000 | Loss: 0.00000761
Iteration 123/1000 | Loss: 0.00000761
Iteration 124/1000 | Loss: 0.00000761
Iteration 125/1000 | Loss: 0.00000761
Iteration 126/1000 | Loss: 0.00000761
Iteration 127/1000 | Loss: 0.00000761
Iteration 128/1000 | Loss: 0.00000761
Iteration 129/1000 | Loss: 0.00000761
Iteration 130/1000 | Loss: 0.00000761
Iteration 131/1000 | Loss: 0.00000761
Iteration 132/1000 | Loss: 0.00000761
Iteration 133/1000 | Loss: 0.00000761
Iteration 134/1000 | Loss: 0.00000761
Iteration 135/1000 | Loss: 0.00000761
Iteration 136/1000 | Loss: 0.00000761
Iteration 137/1000 | Loss: 0.00000761
Iteration 138/1000 | Loss: 0.00000761
Iteration 139/1000 | Loss: 0.00000761
Iteration 140/1000 | Loss: 0.00000761
Iteration 141/1000 | Loss: 0.00000761
Iteration 142/1000 | Loss: 0.00000761
Iteration 143/1000 | Loss: 0.00000761
Iteration 144/1000 | Loss: 0.00000761
Iteration 145/1000 | Loss: 0.00000761
Iteration 146/1000 | Loss: 0.00000761
Iteration 147/1000 | Loss: 0.00000761
Iteration 148/1000 | Loss: 0.00000761
Iteration 149/1000 | Loss: 0.00000761
Iteration 150/1000 | Loss: 0.00000761
Iteration 151/1000 | Loss: 0.00000761
Iteration 152/1000 | Loss: 0.00000761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [7.610041393490974e-06, 7.610041393490974e-06, 7.610041393490974e-06, 7.610041393490974e-06, 7.610041393490974e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.610041393490974e-06

Optimization complete. Final v2v error: 2.39772629737854 mm

Highest mean error: 2.498098134994507 mm for frame 71

Lowest mean error: 2.2702958583831787 mm for frame 0

Saving results

Total time: 29.734526872634888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803042
Iteration 2/25 | Loss: 0.00219013
Iteration 3/25 | Loss: 0.00126592
Iteration 4/25 | Loss: 0.00108859
Iteration 5/25 | Loss: 0.00104992
Iteration 6/25 | Loss: 0.00103823
Iteration 7/25 | Loss: 0.00104112
Iteration 8/25 | Loss: 0.00105164
Iteration 9/25 | Loss: 0.00106098
Iteration 10/25 | Loss: 0.00104488
Iteration 11/25 | Loss: 0.00104079
Iteration 12/25 | Loss: 0.00104071
Iteration 13/25 | Loss: 0.00103303
Iteration 14/25 | Loss: 0.00102678
Iteration 15/25 | Loss: 0.00102801
Iteration 16/25 | Loss: 0.00102700
Iteration 17/25 | Loss: 0.00102448
Iteration 18/25 | Loss: 0.00102419
Iteration 19/25 | Loss: 0.00102267
Iteration 20/25 | Loss: 0.00101947
Iteration 21/25 | Loss: 0.00101959
Iteration 22/25 | Loss: 0.00101940
Iteration 23/25 | Loss: 0.00101266
Iteration 24/25 | Loss: 0.00100996
Iteration 25/25 | Loss: 0.00100900

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33735418
Iteration 2/25 | Loss: 0.00029986
Iteration 3/25 | Loss: 0.00029986
Iteration 4/25 | Loss: 0.00029986
Iteration 5/25 | Loss: 0.00029986
Iteration 6/25 | Loss: 0.00029986
Iteration 7/25 | Loss: 0.00029986
Iteration 8/25 | Loss: 0.00029986
Iteration 9/25 | Loss: 0.00029986
Iteration 10/25 | Loss: 0.00029986
Iteration 11/25 | Loss: 0.00029986
Iteration 12/25 | Loss: 0.00029986
Iteration 13/25 | Loss: 0.00029986
Iteration 14/25 | Loss: 0.00029986
Iteration 15/25 | Loss: 0.00029986
Iteration 16/25 | Loss: 0.00029986
Iteration 17/25 | Loss: 0.00029986
Iteration 18/25 | Loss: 0.00029986
Iteration 19/25 | Loss: 0.00029986
Iteration 20/25 | Loss: 0.00029986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0002998579293489456, 0.0002998579293489456, 0.0002998579293489456, 0.0002998579293489456, 0.0002998579293489456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002998579293489456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029986
Iteration 2/1000 | Loss: 0.00003666
Iteration 3/1000 | Loss: 0.00003174
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002749
Iteration 6/1000 | Loss: 0.00002860
Iteration 7/1000 | Loss: 0.00002543
Iteration 8/1000 | Loss: 0.00003211
Iteration 9/1000 | Loss: 0.00002733
Iteration 10/1000 | Loss: 0.00002591
Iteration 11/1000 | Loss: 0.00002324
Iteration 12/1000 | Loss: 0.00002240
Iteration 13/1000 | Loss: 0.00003143
Iteration 14/1000 | Loss: 0.00002989
Iteration 15/1000 | Loss: 0.00003024
Iteration 16/1000 | Loss: 0.00002599
Iteration 17/1000 | Loss: 0.00002463
Iteration 18/1000 | Loss: 0.00002599
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00004239
Iteration 21/1000 | Loss: 0.00002406
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002682
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002095
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002007
Iteration 30/1000 | Loss: 0.00001984
Iteration 31/1000 | Loss: 0.00001982
Iteration 32/1000 | Loss: 0.00001979
Iteration 33/1000 | Loss: 0.00001974
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001972
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001969
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001968
Iteration 46/1000 | Loss: 0.00001968
Iteration 47/1000 | Loss: 0.00001968
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001967
Iteration 53/1000 | Loss: 0.00001967
Iteration 54/1000 | Loss: 0.00001967
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001966
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001964
Iteration 77/1000 | Loss: 0.00001964
Iteration 78/1000 | Loss: 0.00001964
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001963
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001961
Iteration 87/1000 | Loss: 0.00001961
Iteration 88/1000 | Loss: 0.00001961
Iteration 89/1000 | Loss: 0.00001961
Iteration 90/1000 | Loss: 0.00001960
Iteration 91/1000 | Loss: 0.00001958
Iteration 92/1000 | Loss: 0.00001958
Iteration 93/1000 | Loss: 0.00001958
Iteration 94/1000 | Loss: 0.00001957
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001957
Iteration 97/1000 | Loss: 0.00001957
Iteration 98/1000 | Loss: 0.00001957
Iteration 99/1000 | Loss: 0.00001957
Iteration 100/1000 | Loss: 0.00001957
Iteration 101/1000 | Loss: 0.00001957
Iteration 102/1000 | Loss: 0.00001957
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001956
Iteration 106/1000 | Loss: 0.00001956
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001956
Iteration 110/1000 | Loss: 0.00001956
Iteration 111/1000 | Loss: 0.00001956
Iteration 112/1000 | Loss: 0.00001956
Iteration 113/1000 | Loss: 0.00001955
Iteration 114/1000 | Loss: 0.00001955
Iteration 115/1000 | Loss: 0.00001955
Iteration 116/1000 | Loss: 0.00001954
Iteration 117/1000 | Loss: 0.00001954
Iteration 118/1000 | Loss: 0.00001953
Iteration 119/1000 | Loss: 0.00004436
Iteration 120/1000 | Loss: 0.00003100
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00004430
Iteration 123/1000 | Loss: 0.00003100
Iteration 124/1000 | Loss: 0.00001968
Iteration 125/1000 | Loss: 0.00004430
Iteration 126/1000 | Loss: 0.00005060
Iteration 127/1000 | Loss: 0.00002743
Iteration 128/1000 | Loss: 0.00002329
Iteration 129/1000 | Loss: 0.00002163
Iteration 130/1000 | Loss: 0.00002049
Iteration 131/1000 | Loss: 0.00001997
Iteration 132/1000 | Loss: 0.00001966
Iteration 133/1000 | Loss: 0.00001954
Iteration 134/1000 | Loss: 0.00001946
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001933
Iteration 141/1000 | Loss: 0.00001933
Iteration 142/1000 | Loss: 0.00001933
Iteration 143/1000 | Loss: 0.00001932
Iteration 144/1000 | Loss: 0.00001932
Iteration 145/1000 | Loss: 0.00001931
Iteration 146/1000 | Loss: 0.00001931
Iteration 147/1000 | Loss: 0.00001931
Iteration 148/1000 | Loss: 0.00001929
Iteration 149/1000 | Loss: 0.00001929
Iteration 150/1000 | Loss: 0.00001929
Iteration 151/1000 | Loss: 0.00001929
Iteration 152/1000 | Loss: 0.00001928
Iteration 153/1000 | Loss: 0.00001928
Iteration 154/1000 | Loss: 0.00001928
Iteration 155/1000 | Loss: 0.00001927
Iteration 156/1000 | Loss: 0.00001927
Iteration 157/1000 | Loss: 0.00001927
Iteration 158/1000 | Loss: 0.00001927
Iteration 159/1000 | Loss: 0.00001926
Iteration 160/1000 | Loss: 0.00001926
Iteration 161/1000 | Loss: 0.00001925
Iteration 162/1000 | Loss: 0.00001925
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001924
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001922
Iteration 173/1000 | Loss: 0.00001922
Iteration 174/1000 | Loss: 0.00001922
Iteration 175/1000 | Loss: 0.00001922
Iteration 176/1000 | Loss: 0.00001922
Iteration 177/1000 | Loss: 0.00001922
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001922
Iteration 180/1000 | Loss: 0.00001922
Iteration 181/1000 | Loss: 0.00001922
Iteration 182/1000 | Loss: 0.00001922
Iteration 183/1000 | Loss: 0.00001922
Iteration 184/1000 | Loss: 0.00001922
Iteration 185/1000 | Loss: 0.00001922
Iteration 186/1000 | Loss: 0.00001922
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Iteration 191/1000 | Loss: 0.00001921
Iteration 192/1000 | Loss: 0.00001921
Iteration 193/1000 | Loss: 0.00001920
Iteration 194/1000 | Loss: 0.00001920
Iteration 195/1000 | Loss: 0.00001920
Iteration 196/1000 | Loss: 0.00001920
Iteration 197/1000 | Loss: 0.00001920
Iteration 198/1000 | Loss: 0.00001920
Iteration 199/1000 | Loss: 0.00001920
Iteration 200/1000 | Loss: 0.00001920
Iteration 201/1000 | Loss: 0.00001920
Iteration 202/1000 | Loss: 0.00001919
Iteration 203/1000 | Loss: 0.00001919
Iteration 204/1000 | Loss: 0.00001919
Iteration 205/1000 | Loss: 0.00001919
Iteration 206/1000 | Loss: 0.00001919
Iteration 207/1000 | Loss: 0.00001919
Iteration 208/1000 | Loss: 0.00001919
Iteration 209/1000 | Loss: 0.00001919
Iteration 210/1000 | Loss: 0.00001919
Iteration 211/1000 | Loss: 0.00001919
Iteration 212/1000 | Loss: 0.00001919
Iteration 213/1000 | Loss: 0.00001919
Iteration 214/1000 | Loss: 0.00001919
Iteration 215/1000 | Loss: 0.00001918
Iteration 216/1000 | Loss: 0.00001918
Iteration 217/1000 | Loss: 0.00001918
Iteration 218/1000 | Loss: 0.00001918
Iteration 219/1000 | Loss: 0.00001918
Iteration 220/1000 | Loss: 0.00001918
Iteration 221/1000 | Loss: 0.00001918
Iteration 222/1000 | Loss: 0.00001918
Iteration 223/1000 | Loss: 0.00001918
Iteration 224/1000 | Loss: 0.00001918
Iteration 225/1000 | Loss: 0.00001918
Iteration 226/1000 | Loss: 0.00001918
Iteration 227/1000 | Loss: 0.00001918
Iteration 228/1000 | Loss: 0.00001918
Iteration 229/1000 | Loss: 0.00001918
Iteration 230/1000 | Loss: 0.00001918
Iteration 231/1000 | Loss: 0.00001918
Iteration 232/1000 | Loss: 0.00001918
Iteration 233/1000 | Loss: 0.00001918
Iteration 234/1000 | Loss: 0.00001918
Iteration 235/1000 | Loss: 0.00001918
Iteration 236/1000 | Loss: 0.00001918
Iteration 237/1000 | Loss: 0.00001917
Iteration 238/1000 | Loss: 0.00001917
Iteration 239/1000 | Loss: 0.00001917
Iteration 240/1000 | Loss: 0.00001917
Iteration 241/1000 | Loss: 0.00001917
Iteration 242/1000 | Loss: 0.00001917
Iteration 243/1000 | Loss: 0.00001917
Iteration 244/1000 | Loss: 0.00001917
Iteration 245/1000 | Loss: 0.00001917
Iteration 246/1000 | Loss: 0.00001916
Iteration 247/1000 | Loss: 0.00001916
Iteration 248/1000 | Loss: 0.00001916
Iteration 249/1000 | Loss: 0.00001916
Iteration 250/1000 | Loss: 0.00001916
Iteration 251/1000 | Loss: 0.00001916
Iteration 252/1000 | Loss: 0.00001916
Iteration 253/1000 | Loss: 0.00001916
Iteration 254/1000 | Loss: 0.00001916
Iteration 255/1000 | Loss: 0.00001916
Iteration 256/1000 | Loss: 0.00001916
Iteration 257/1000 | Loss: 0.00001916
Iteration 258/1000 | Loss: 0.00001916
Iteration 259/1000 | Loss: 0.00001916
Iteration 260/1000 | Loss: 0.00001916
Iteration 261/1000 | Loss: 0.00001916
Iteration 262/1000 | Loss: 0.00001916
Iteration 263/1000 | Loss: 0.00001916
Iteration 264/1000 | Loss: 0.00001916
Iteration 265/1000 | Loss: 0.00001916
Iteration 266/1000 | Loss: 0.00001916
Iteration 267/1000 | Loss: 0.00001916
Iteration 268/1000 | Loss: 0.00001916
Iteration 269/1000 | Loss: 0.00001916
Iteration 270/1000 | Loss: 0.00001916
Iteration 271/1000 | Loss: 0.00001916
Iteration 272/1000 | Loss: 0.00001916
Iteration 273/1000 | Loss: 0.00001916
Iteration 274/1000 | Loss: 0.00001916
Iteration 275/1000 | Loss: 0.00001916
Iteration 276/1000 | Loss: 0.00001916
Iteration 277/1000 | Loss: 0.00001916
Iteration 278/1000 | Loss: 0.00001916
Iteration 279/1000 | Loss: 0.00001916
Iteration 280/1000 | Loss: 0.00001916
Iteration 281/1000 | Loss: 0.00001916
Iteration 282/1000 | Loss: 0.00001916
Iteration 283/1000 | Loss: 0.00001916
Iteration 284/1000 | Loss: 0.00001916
Iteration 285/1000 | Loss: 0.00001916
Iteration 286/1000 | Loss: 0.00001916
Iteration 287/1000 | Loss: 0.00001916
Iteration 288/1000 | Loss: 0.00001916
Iteration 289/1000 | Loss: 0.00001916
Iteration 290/1000 | Loss: 0.00001916
Iteration 291/1000 | Loss: 0.00001916
Iteration 292/1000 | Loss: 0.00001916
Iteration 293/1000 | Loss: 0.00001916
Iteration 294/1000 | Loss: 0.00001916
Iteration 295/1000 | Loss: 0.00001916
Iteration 296/1000 | Loss: 0.00001916
Iteration 297/1000 | Loss: 0.00001916
Iteration 298/1000 | Loss: 0.00001916
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00001916
Iteration 301/1000 | Loss: 0.00001916
Iteration 302/1000 | Loss: 0.00001916
Iteration 303/1000 | Loss: 0.00001916
Iteration 304/1000 | Loss: 0.00001916
Iteration 305/1000 | Loss: 0.00001916
Iteration 306/1000 | Loss: 0.00001916
Iteration 307/1000 | Loss: 0.00001916
Iteration 308/1000 | Loss: 0.00001916
Iteration 309/1000 | Loss: 0.00001916
Iteration 310/1000 | Loss: 0.00001916
Iteration 311/1000 | Loss: 0.00001916
Iteration 312/1000 | Loss: 0.00001916
Iteration 313/1000 | Loss: 0.00001916
Iteration 314/1000 | Loss: 0.00001916
Iteration 315/1000 | Loss: 0.00001916
Iteration 316/1000 | Loss: 0.00001916
Iteration 317/1000 | Loss: 0.00001916
Iteration 318/1000 | Loss: 0.00001916
Iteration 319/1000 | Loss: 0.00001916
Iteration 320/1000 | Loss: 0.00001916
Iteration 321/1000 | Loss: 0.00001916
Iteration 322/1000 | Loss: 0.00001916
Iteration 323/1000 | Loss: 0.00001916
Iteration 324/1000 | Loss: 0.00001916
Iteration 325/1000 | Loss: 0.00001916
Iteration 326/1000 | Loss: 0.00001916
Iteration 327/1000 | Loss: 0.00001916
Iteration 328/1000 | Loss: 0.00001916
Iteration 329/1000 | Loss: 0.00001916
Iteration 330/1000 | Loss: 0.00001916
Iteration 331/1000 | Loss: 0.00001916
Iteration 332/1000 | Loss: 0.00001916
Iteration 333/1000 | Loss: 0.00001916
Iteration 334/1000 | Loss: 0.00001916
Iteration 335/1000 | Loss: 0.00001916
Iteration 336/1000 | Loss: 0.00001916
Iteration 337/1000 | Loss: 0.00001916
Iteration 338/1000 | Loss: 0.00001916
Iteration 339/1000 | Loss: 0.00001916
Iteration 340/1000 | Loss: 0.00001916
Iteration 341/1000 | Loss: 0.00001916
Iteration 342/1000 | Loss: 0.00001916
Iteration 343/1000 | Loss: 0.00001916
Iteration 344/1000 | Loss: 0.00001916
Iteration 345/1000 | Loss: 0.00001916
Iteration 346/1000 | Loss: 0.00001916
Iteration 347/1000 | Loss: 0.00001916
Iteration 348/1000 | Loss: 0.00001916
Iteration 349/1000 | Loss: 0.00001916
Iteration 350/1000 | Loss: 0.00001916
Iteration 351/1000 | Loss: 0.00001916
Iteration 352/1000 | Loss: 0.00001916
Iteration 353/1000 | Loss: 0.00001916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.9155284462613054e-05, 1.9155284462613054e-05, 1.9155284462613054e-05, 1.9155284462613054e-05, 1.9155284462613054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9155284462613054e-05

Optimization complete. Final v2v error: 3.5844314098358154 mm

Highest mean error: 9.06594467163086 mm for frame 96

Lowest mean error: 3.0901644229888916 mm for frame 189

Saving results

Total time: 141.8002212047577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080987
Iteration 2/25 | Loss: 0.00248673
Iteration 3/25 | Loss: 0.00153765
Iteration 4/25 | Loss: 0.00133918
Iteration 5/25 | Loss: 0.00125565
Iteration 6/25 | Loss: 0.00115065
Iteration 7/25 | Loss: 0.00108321
Iteration 8/25 | Loss: 0.00097397
Iteration 9/25 | Loss: 0.00097400
Iteration 10/25 | Loss: 0.00093096
Iteration 11/25 | Loss: 0.00092909
Iteration 12/25 | Loss: 0.00090439
Iteration 13/25 | Loss: 0.00090527
Iteration 14/25 | Loss: 0.00090480
Iteration 15/25 | Loss: 0.00090626
Iteration 16/25 | Loss: 0.00090993
Iteration 17/25 | Loss: 0.00093495
Iteration 18/25 | Loss: 0.00091270
Iteration 19/25 | Loss: 0.00090144
Iteration 20/25 | Loss: 0.00089224
Iteration 21/25 | Loss: 0.00089465
Iteration 22/25 | Loss: 0.00088916
Iteration 23/25 | Loss: 0.00088904
Iteration 24/25 | Loss: 0.00088904
Iteration 25/25 | Loss: 0.00088904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42590356
Iteration 2/25 | Loss: 0.00076586
Iteration 3/25 | Loss: 0.00074671
Iteration 4/25 | Loss: 0.00074670
Iteration 5/25 | Loss: 0.00074670
Iteration 6/25 | Loss: 0.00074670
Iteration 7/25 | Loss: 0.00074670
Iteration 8/25 | Loss: 0.00074670
Iteration 9/25 | Loss: 0.00074670
Iteration 10/25 | Loss: 0.00074670
Iteration 11/25 | Loss: 0.00074670
Iteration 12/25 | Loss: 0.00074670
Iteration 13/25 | Loss: 0.00074670
Iteration 14/25 | Loss: 0.00074670
Iteration 15/25 | Loss: 0.00074670
Iteration 16/25 | Loss: 0.00074670
Iteration 17/25 | Loss: 0.00074670
Iteration 18/25 | Loss: 0.00074670
Iteration 19/25 | Loss: 0.00074670
Iteration 20/25 | Loss: 0.00074670
Iteration 21/25 | Loss: 0.00074670
Iteration 22/25 | Loss: 0.00074670
Iteration 23/25 | Loss: 0.00074670
Iteration 24/25 | Loss: 0.00074670
Iteration 25/25 | Loss: 0.00074670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074670
Iteration 2/1000 | Loss: 0.00004910
Iteration 3/1000 | Loss: 0.00005352
Iteration 4/1000 | Loss: 0.00002698
Iteration 5/1000 | Loss: 0.00013336
Iteration 6/1000 | Loss: 0.00007024
Iteration 7/1000 | Loss: 0.00003367
Iteration 8/1000 | Loss: 0.00003154
Iteration 9/1000 | Loss: 0.00003245
Iteration 10/1000 | Loss: 0.00003240
Iteration 11/1000 | Loss: 0.00032560
Iteration 12/1000 | Loss: 0.00024301
Iteration 13/1000 | Loss: 0.00121192
Iteration 14/1000 | Loss: 0.00027977
Iteration 15/1000 | Loss: 0.00025449
Iteration 16/1000 | Loss: 0.00004252
Iteration 17/1000 | Loss: 0.00002468
Iteration 18/1000 | Loss: 0.00007532
Iteration 19/1000 | Loss: 0.00001826
Iteration 20/1000 | Loss: 0.00003268
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001618
Iteration 23/1000 | Loss: 0.00001575
Iteration 24/1000 | Loss: 0.00002526
Iteration 25/1000 | Loss: 0.00001830
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00002912
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001622
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001463
Iteration 41/1000 | Loss: 0.00001462
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00002072
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001646
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001451
Iteration 54/1000 | Loss: 0.00001451
Iteration 55/1000 | Loss: 0.00001451
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Iteration 75/1000 | Loss: 0.00001448
Iteration 76/1000 | Loss: 0.00001448
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001446
Iteration 81/1000 | Loss: 0.00001446
Iteration 82/1000 | Loss: 0.00001446
Iteration 83/1000 | Loss: 0.00001446
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001445
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4407531125470996e-05, 1.4407531125470996e-05, 1.4407531125470996e-05, 1.4407531125470996e-05, 1.4407531125470996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4407531125470996e-05

Optimization complete. Final v2v error: 2.4453938007354736 mm

Highest mean error: 20.330766677856445 mm for frame 73

Lowest mean error: 1.8819493055343628 mm for frame 112

Saving results

Total time: 93.9681785106659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045884
Iteration 2/25 | Loss: 0.00294360
Iteration 3/25 | Loss: 0.00178788
Iteration 4/25 | Loss: 0.00156594
Iteration 5/25 | Loss: 0.00160827
Iteration 6/25 | Loss: 0.00165044
Iteration 7/25 | Loss: 0.00153740
Iteration 8/25 | Loss: 0.00144199
Iteration 9/25 | Loss: 0.00133210
Iteration 10/25 | Loss: 0.00128352
Iteration 11/25 | Loss: 0.00125130
Iteration 12/25 | Loss: 0.00122751
Iteration 13/25 | Loss: 0.00119955
Iteration 14/25 | Loss: 0.00119082
Iteration 15/25 | Loss: 0.00118486
Iteration 16/25 | Loss: 0.00118377
Iteration 17/25 | Loss: 0.00117449
Iteration 18/25 | Loss: 0.00116654
Iteration 19/25 | Loss: 0.00116103
Iteration 20/25 | Loss: 0.00116209
Iteration 21/25 | Loss: 0.00116232
Iteration 22/25 | Loss: 0.00115571
Iteration 23/25 | Loss: 0.00115529
Iteration 24/25 | Loss: 0.00115046
Iteration 25/25 | Loss: 0.00114729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31160855
Iteration 2/25 | Loss: 0.00320642
Iteration 3/25 | Loss: 0.00319813
Iteration 4/25 | Loss: 0.00319813
Iteration 5/25 | Loss: 0.00319813
Iteration 6/25 | Loss: 0.00319813
Iteration 7/25 | Loss: 0.00319813
Iteration 8/25 | Loss: 0.00319812
Iteration 9/25 | Loss: 0.00319812
Iteration 10/25 | Loss: 0.00319812
Iteration 11/25 | Loss: 0.00319812
Iteration 12/25 | Loss: 0.00319812
Iteration 13/25 | Loss: 0.00319812
Iteration 14/25 | Loss: 0.00319812
Iteration 15/25 | Loss: 0.00319812
Iteration 16/25 | Loss: 0.00319812
Iteration 17/25 | Loss: 0.00319812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0031981240026652813, 0.0031981240026652813, 0.0031981240026652813, 0.0031981240026652813, 0.0031981240026652813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031981240026652813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319812
Iteration 2/1000 | Loss: 0.00069877
Iteration 3/1000 | Loss: 0.00119382
Iteration 4/1000 | Loss: 0.00103478
Iteration 5/1000 | Loss: 0.00058841
Iteration 6/1000 | Loss: 0.00030760
Iteration 7/1000 | Loss: 0.00040545
Iteration 8/1000 | Loss: 0.00048658
Iteration 9/1000 | Loss: 0.00020807
Iteration 10/1000 | Loss: 0.00087743
Iteration 11/1000 | Loss: 0.00040807
Iteration 12/1000 | Loss: 0.00061423
Iteration 13/1000 | Loss: 0.00031566
Iteration 14/1000 | Loss: 0.00022447
Iteration 15/1000 | Loss: 0.00033054
Iteration 16/1000 | Loss: 0.00026625
Iteration 17/1000 | Loss: 0.00029761
Iteration 18/1000 | Loss: 0.00027125
Iteration 19/1000 | Loss: 0.00031717
Iteration 20/1000 | Loss: 0.00018164
Iteration 21/1000 | Loss: 0.00012330
Iteration 22/1000 | Loss: 0.00159231
Iteration 23/1000 | Loss: 0.00229562
Iteration 24/1000 | Loss: 0.00201625
Iteration 25/1000 | Loss: 0.00118014
Iteration 26/1000 | Loss: 0.00174626
Iteration 27/1000 | Loss: 0.00097922
Iteration 28/1000 | Loss: 0.00015989
Iteration 29/1000 | Loss: 0.00054966
Iteration 30/1000 | Loss: 0.00029761
Iteration 31/1000 | Loss: 0.00095004
Iteration 32/1000 | Loss: 0.00026486
Iteration 33/1000 | Loss: 0.00022498
Iteration 34/1000 | Loss: 0.00036644
Iteration 35/1000 | Loss: 0.00011001
Iteration 36/1000 | Loss: 0.00018492
Iteration 37/1000 | Loss: 0.00042639
Iteration 38/1000 | Loss: 0.00025654
Iteration 39/1000 | Loss: 0.00022633
Iteration 40/1000 | Loss: 0.00010585
Iteration 41/1000 | Loss: 0.00009358
Iteration 42/1000 | Loss: 0.00010109
Iteration 43/1000 | Loss: 0.00037987
Iteration 44/1000 | Loss: 0.00016208
Iteration 45/1000 | Loss: 0.00031011
Iteration 46/1000 | Loss: 0.00009180
Iteration 47/1000 | Loss: 0.00009138
Iteration 48/1000 | Loss: 0.00010174
Iteration 49/1000 | Loss: 0.00009298
Iteration 50/1000 | Loss: 0.00008775
Iteration 51/1000 | Loss: 0.00039669
Iteration 52/1000 | Loss: 0.00016887
Iteration 53/1000 | Loss: 0.00029232
Iteration 54/1000 | Loss: 0.00040437
Iteration 55/1000 | Loss: 0.00017549
Iteration 56/1000 | Loss: 0.00009331
Iteration 57/1000 | Loss: 0.00009821
Iteration 58/1000 | Loss: 0.00009450
Iteration 59/1000 | Loss: 0.00009559
Iteration 60/1000 | Loss: 0.00008425
Iteration 61/1000 | Loss: 0.00023804
Iteration 62/1000 | Loss: 0.00036342
Iteration 63/1000 | Loss: 0.00043119
Iteration 64/1000 | Loss: 0.00031335
Iteration 65/1000 | Loss: 0.00069479
Iteration 66/1000 | Loss: 0.00008893
Iteration 67/1000 | Loss: 0.00008271
Iteration 68/1000 | Loss: 0.00026551
Iteration 69/1000 | Loss: 0.00008110
Iteration 70/1000 | Loss: 0.00008045
Iteration 71/1000 | Loss: 0.00028386
Iteration 72/1000 | Loss: 0.00022846
Iteration 73/1000 | Loss: 0.00027358
Iteration 74/1000 | Loss: 0.00024113
Iteration 75/1000 | Loss: 0.00067312
Iteration 76/1000 | Loss: 0.00024452
Iteration 77/1000 | Loss: 0.00034013
Iteration 78/1000 | Loss: 0.00026611
Iteration 79/1000 | Loss: 0.00015309
Iteration 80/1000 | Loss: 0.00008815
Iteration 81/1000 | Loss: 0.00009590
Iteration 82/1000 | Loss: 0.00008316
Iteration 83/1000 | Loss: 0.00008251
Iteration 84/1000 | Loss: 0.00012391
Iteration 85/1000 | Loss: 0.00009889
Iteration 86/1000 | Loss: 0.00008713
Iteration 87/1000 | Loss: 0.00008527
Iteration 88/1000 | Loss: 0.00008192
Iteration 89/1000 | Loss: 0.00008172
Iteration 90/1000 | Loss: 0.00008169
Iteration 91/1000 | Loss: 0.00008168
Iteration 92/1000 | Loss: 0.00008154
Iteration 93/1000 | Loss: 0.00008153
Iteration 94/1000 | Loss: 0.00008153
Iteration 95/1000 | Loss: 0.00008151
Iteration 96/1000 | Loss: 0.00008145
Iteration 97/1000 | Loss: 0.00008144
Iteration 98/1000 | Loss: 0.00016899
Iteration 99/1000 | Loss: 0.00008193
Iteration 100/1000 | Loss: 0.00009004
Iteration 101/1000 | Loss: 0.00008205
Iteration 102/1000 | Loss: 0.00010606
Iteration 103/1000 | Loss: 0.00010012
Iteration 104/1000 | Loss: 0.00009666
Iteration 105/1000 | Loss: 0.00008672
Iteration 106/1000 | Loss: 0.00008136
Iteration 107/1000 | Loss: 0.00008130
Iteration 108/1000 | Loss: 0.00009307
Iteration 109/1000 | Loss: 0.00008859
Iteration 110/1000 | Loss: 0.00008502
Iteration 111/1000 | Loss: 0.00008715
Iteration 112/1000 | Loss: 0.00034173
Iteration 113/1000 | Loss: 0.00038082
Iteration 114/1000 | Loss: 0.00018740
Iteration 115/1000 | Loss: 0.00008694
Iteration 116/1000 | Loss: 0.00008439
Iteration 117/1000 | Loss: 0.00008288
Iteration 118/1000 | Loss: 0.00051199
Iteration 119/1000 | Loss: 0.00043073
Iteration 120/1000 | Loss: 0.00024929
Iteration 121/1000 | Loss: 0.00008590
Iteration 122/1000 | Loss: 0.00009985
Iteration 123/1000 | Loss: 0.00038189
Iteration 124/1000 | Loss: 0.00053331
Iteration 125/1000 | Loss: 0.00008413
Iteration 126/1000 | Loss: 0.00008291
Iteration 127/1000 | Loss: 0.00008590
Iteration 128/1000 | Loss: 0.00008183
Iteration 129/1000 | Loss: 0.00008109
Iteration 130/1000 | Loss: 0.00008056
Iteration 131/1000 | Loss: 0.00008039
Iteration 132/1000 | Loss: 0.00014278
Iteration 133/1000 | Loss: 0.00025130
Iteration 134/1000 | Loss: 0.00008624
Iteration 135/1000 | Loss: 0.00033145
Iteration 136/1000 | Loss: 0.00008463
Iteration 137/1000 | Loss: 0.00015953
Iteration 138/1000 | Loss: 0.00008104
Iteration 139/1000 | Loss: 0.00008044
Iteration 140/1000 | Loss: 0.00027286
Iteration 141/1000 | Loss: 0.00010399
Iteration 142/1000 | Loss: 0.00008035
Iteration 143/1000 | Loss: 0.00015327
Iteration 144/1000 | Loss: 0.00010285
Iteration 145/1000 | Loss: 0.00012527
Iteration 146/1000 | Loss: 0.00008988
Iteration 147/1000 | Loss: 0.00009786
Iteration 148/1000 | Loss: 0.00007961
Iteration 149/1000 | Loss: 0.00007883
Iteration 150/1000 | Loss: 0.00018486
Iteration 151/1000 | Loss: 0.00009438
Iteration 152/1000 | Loss: 0.00009239
Iteration 153/1000 | Loss: 0.00007845
Iteration 154/1000 | Loss: 0.00007793
Iteration 155/1000 | Loss: 0.00007755
Iteration 156/1000 | Loss: 0.00007745
Iteration 157/1000 | Loss: 0.00007733
Iteration 158/1000 | Loss: 0.00015231
Iteration 159/1000 | Loss: 0.00008554
Iteration 160/1000 | Loss: 0.00007732
Iteration 161/1000 | Loss: 0.00007726
Iteration 162/1000 | Loss: 0.00007726
Iteration 163/1000 | Loss: 0.00007726
Iteration 164/1000 | Loss: 0.00007725
Iteration 165/1000 | Loss: 0.00007725
Iteration 166/1000 | Loss: 0.00007725
Iteration 167/1000 | Loss: 0.00007725
Iteration 168/1000 | Loss: 0.00007725
Iteration 169/1000 | Loss: 0.00014300
Iteration 170/1000 | Loss: 0.00009049
Iteration 171/1000 | Loss: 0.00007727
Iteration 172/1000 | Loss: 0.00013437
Iteration 173/1000 | Loss: 0.00008130
Iteration 174/1000 | Loss: 0.00008030
Iteration 175/1000 | Loss: 0.00007825
Iteration 176/1000 | Loss: 0.00007911
Iteration 177/1000 | Loss: 0.00007805
Iteration 178/1000 | Loss: 0.00007822
Iteration 179/1000 | Loss: 0.00007814
Iteration 180/1000 | Loss: 0.00007829
Iteration 181/1000 | Loss: 0.00007868
Iteration 182/1000 | Loss: 0.00007966
Iteration 183/1000 | Loss: 0.00007829
Iteration 184/1000 | Loss: 0.00007872
Iteration 185/1000 | Loss: 0.00007990
Iteration 186/1000 | Loss: 0.00007827
Iteration 187/1000 | Loss: 0.00008022
Iteration 188/1000 | Loss: 0.00007729
Iteration 189/1000 | Loss: 0.00007716
Iteration 190/1000 | Loss: 0.00007716
Iteration 191/1000 | Loss: 0.00007716
Iteration 192/1000 | Loss: 0.00007715
Iteration 193/1000 | Loss: 0.00007715
Iteration 194/1000 | Loss: 0.00007715
Iteration 195/1000 | Loss: 0.00007715
Iteration 196/1000 | Loss: 0.00007715
Iteration 197/1000 | Loss: 0.00007715
Iteration 198/1000 | Loss: 0.00007715
Iteration 199/1000 | Loss: 0.00007715
Iteration 200/1000 | Loss: 0.00007715
Iteration 201/1000 | Loss: 0.00007715
Iteration 202/1000 | Loss: 0.00007715
Iteration 203/1000 | Loss: 0.00007715
Iteration 204/1000 | Loss: 0.00007714
Iteration 205/1000 | Loss: 0.00008615
Iteration 206/1000 | Loss: 0.00008470
Iteration 207/1000 | Loss: 0.00027137
Iteration 208/1000 | Loss: 0.00008708
Iteration 209/1000 | Loss: 0.00007724
Iteration 210/1000 | Loss: 0.00008239
Iteration 211/1000 | Loss: 0.00008115
Iteration 212/1000 | Loss: 0.00007964
Iteration 213/1000 | Loss: 0.00007964
Iteration 214/1000 | Loss: 0.00008664
Iteration 215/1000 | Loss: 0.00008265
Iteration 216/1000 | Loss: 0.00008406
Iteration 217/1000 | Loss: 0.00007752
Iteration 218/1000 | Loss: 0.00008387
Iteration 219/1000 | Loss: 0.00010238
Iteration 220/1000 | Loss: 0.00007909
Iteration 221/1000 | Loss: 0.00007843
Iteration 222/1000 | Loss: 0.00007724
Iteration 223/1000 | Loss: 0.00008041
Iteration 224/1000 | Loss: 0.00007791
Iteration 225/1000 | Loss: 0.00007855
Iteration 226/1000 | Loss: 0.00007791
Iteration 227/1000 | Loss: 0.00007770
Iteration 228/1000 | Loss: 0.00007780
Iteration 229/1000 | Loss: 0.00007772
Iteration 230/1000 | Loss: 0.00007780
Iteration 231/1000 | Loss: 0.00007765
Iteration 232/1000 | Loss: 0.00007711
Iteration 233/1000 | Loss: 0.00007707
Iteration 234/1000 | Loss: 0.00011664
Iteration 235/1000 | Loss: 0.00007770
Iteration 236/1000 | Loss: 0.00010411
Iteration 237/1000 | Loss: 0.00007712
Iteration 238/1000 | Loss: 0.00007712
Iteration 239/1000 | Loss: 0.00007712
Iteration 240/1000 | Loss: 0.00007712
Iteration 241/1000 | Loss: 0.00007712
Iteration 242/1000 | Loss: 0.00007712
Iteration 243/1000 | Loss: 0.00007712
Iteration 244/1000 | Loss: 0.00007712
Iteration 245/1000 | Loss: 0.00007712
Iteration 246/1000 | Loss: 0.00007760
Iteration 247/1000 | Loss: 0.00007875
Iteration 248/1000 | Loss: 0.00007756
Iteration 249/1000 | Loss: 0.00007788
Iteration 250/1000 | Loss: 0.00008139
Iteration 251/1000 | Loss: 0.00007780
Iteration 252/1000 | Loss: 0.00007942
Iteration 253/1000 | Loss: 0.00007795
Iteration 254/1000 | Loss: 0.00007795
Iteration 255/1000 | Loss: 0.00007795
Iteration 256/1000 | Loss: 0.00009139
Iteration 257/1000 | Loss: 0.00007693
Iteration 258/1000 | Loss: 0.00007812
Iteration 259/1000 | Loss: 0.00007812
Iteration 260/1000 | Loss: 0.00007779
Iteration 261/1000 | Loss: 0.00007754
Iteration 262/1000 | Loss: 0.00007738
Iteration 263/1000 | Loss: 0.00008433
Iteration 264/1000 | Loss: 0.00007784
Iteration 265/1000 | Loss: 0.00007814
Iteration 266/1000 | Loss: 0.00007776
Iteration 267/1000 | Loss: 0.00007791
Iteration 268/1000 | Loss: 0.00007778
Iteration 269/1000 | Loss: 0.00007839
Iteration 270/1000 | Loss: 0.00007778
Iteration 271/1000 | Loss: 0.00007764
Iteration 272/1000 | Loss: 0.00010834
Iteration 273/1000 | Loss: 0.00007724
Iteration 274/1000 | Loss: 0.00007708
Iteration 275/1000 | Loss: 0.00007707
Iteration 276/1000 | Loss: 0.00007707
Iteration 277/1000 | Loss: 0.00007706
Iteration 278/1000 | Loss: 0.00007706
Iteration 279/1000 | Loss: 0.00007706
Iteration 280/1000 | Loss: 0.00007706
Iteration 281/1000 | Loss: 0.00007706
Iteration 282/1000 | Loss: 0.00007706
Iteration 283/1000 | Loss: 0.00007704
Iteration 284/1000 | Loss: 0.00007704
Iteration 285/1000 | Loss: 0.00007704
Iteration 286/1000 | Loss: 0.00007704
Iteration 287/1000 | Loss: 0.00007843
Iteration 288/1000 | Loss: 0.00007837
Iteration 289/1000 | Loss: 0.00008016
Iteration 290/1000 | Loss: 0.00007789
Iteration 291/1000 | Loss: 0.00007705
Iteration 292/1000 | Loss: 0.00007705
Iteration 293/1000 | Loss: 0.00007705
Iteration 294/1000 | Loss: 0.00007705
Iteration 295/1000 | Loss: 0.00007705
Iteration 296/1000 | Loss: 0.00007705
Iteration 297/1000 | Loss: 0.00007705
Iteration 298/1000 | Loss: 0.00007705
Iteration 299/1000 | Loss: 0.00007705
Iteration 300/1000 | Loss: 0.00007705
Iteration 301/1000 | Loss: 0.00007704
Iteration 302/1000 | Loss: 0.00007704
Iteration 303/1000 | Loss: 0.00007704
Iteration 304/1000 | Loss: 0.00007704
Iteration 305/1000 | Loss: 0.00007704
Iteration 306/1000 | Loss: 0.00007704
Iteration 307/1000 | Loss: 0.00007704
Iteration 308/1000 | Loss: 0.00007704
Iteration 309/1000 | Loss: 0.00007704
Iteration 310/1000 | Loss: 0.00007704
Iteration 311/1000 | Loss: 0.00007704
Iteration 312/1000 | Loss: 0.00007704
Iteration 313/1000 | Loss: 0.00007704
Iteration 314/1000 | Loss: 0.00007704
Iteration 315/1000 | Loss: 0.00007704
Iteration 316/1000 | Loss: 0.00007704
Iteration 317/1000 | Loss: 0.00007704
Iteration 318/1000 | Loss: 0.00007704
Iteration 319/1000 | Loss: 0.00007704
Iteration 320/1000 | Loss: 0.00007704
Iteration 321/1000 | Loss: 0.00007704
Iteration 322/1000 | Loss: 0.00007704
Iteration 323/1000 | Loss: 0.00007704
Iteration 324/1000 | Loss: 0.00007704
Iteration 325/1000 | Loss: 0.00007704
Iteration 326/1000 | Loss: 0.00007704
Iteration 327/1000 | Loss: 0.00007704
Iteration 328/1000 | Loss: 0.00007704
Iteration 329/1000 | Loss: 0.00007704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [7.70361875765957e-05, 7.70361875765957e-05, 7.70361875765957e-05, 7.70361875765957e-05, 7.70361875765957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.70361875765957e-05

Optimization complete. Final v2v error: 3.9456231594085693 mm

Highest mean error: 19.5174617767334 mm for frame 208

Lowest mean error: 2.398608922958374 mm for frame 222

Saving results

Total time: 389.21680784225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824691
Iteration 2/25 | Loss: 0.00119859
Iteration 3/25 | Loss: 0.00103823
Iteration 4/25 | Loss: 0.00100761
Iteration 5/25 | Loss: 0.00100047
Iteration 6/25 | Loss: 0.00099844
Iteration 7/25 | Loss: 0.00099841
Iteration 8/25 | Loss: 0.00099841
Iteration 9/25 | Loss: 0.00099841
Iteration 10/25 | Loss: 0.00099841
Iteration 11/25 | Loss: 0.00099841
Iteration 12/25 | Loss: 0.00099841
Iteration 13/25 | Loss: 0.00099841
Iteration 14/25 | Loss: 0.00099841
Iteration 15/25 | Loss: 0.00099841
Iteration 16/25 | Loss: 0.00099841
Iteration 17/25 | Loss: 0.00099841
Iteration 18/25 | Loss: 0.00099841
Iteration 19/25 | Loss: 0.00099841
Iteration 20/25 | Loss: 0.00099841
Iteration 21/25 | Loss: 0.00099841
Iteration 22/25 | Loss: 0.00099841
Iteration 23/25 | Loss: 0.00099841
Iteration 24/25 | Loss: 0.00099841
Iteration 25/25 | Loss: 0.00099841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33401370
Iteration 2/25 | Loss: 0.00072638
Iteration 3/25 | Loss: 0.00072637
Iteration 4/25 | Loss: 0.00072637
Iteration 5/25 | Loss: 0.00072637
Iteration 6/25 | Loss: 0.00072637
Iteration 7/25 | Loss: 0.00072637
Iteration 8/25 | Loss: 0.00072637
Iteration 9/25 | Loss: 0.00072637
Iteration 10/25 | Loss: 0.00072637
Iteration 11/25 | Loss: 0.00072637
Iteration 12/25 | Loss: 0.00072637
Iteration 13/25 | Loss: 0.00072637
Iteration 14/25 | Loss: 0.00072637
Iteration 15/25 | Loss: 0.00072637
Iteration 16/25 | Loss: 0.00072637
Iteration 17/25 | Loss: 0.00072637
Iteration 18/25 | Loss: 0.00072637
Iteration 19/25 | Loss: 0.00072637
Iteration 20/25 | Loss: 0.00072637
Iteration 21/25 | Loss: 0.00072637
Iteration 22/25 | Loss: 0.00072637
Iteration 23/25 | Loss: 0.00072637
Iteration 24/25 | Loss: 0.00072637
Iteration 25/25 | Loss: 0.00072637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072637
Iteration 2/1000 | Loss: 0.00003852
Iteration 3/1000 | Loss: 0.00002600
Iteration 4/1000 | Loss: 0.00002162
Iteration 5/1000 | Loss: 0.00002047
Iteration 6/1000 | Loss: 0.00001957
Iteration 7/1000 | Loss: 0.00001896
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001807
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001791
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001783
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001778
Iteration 19/1000 | Loss: 0.00001777
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001777
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001777
Iteration 25/1000 | Loss: 0.00001777
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001776
Iteration 29/1000 | Loss: 0.00001776
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001775
Iteration 39/1000 | Loss: 0.00001775
Iteration 40/1000 | Loss: 0.00001775
Iteration 41/1000 | Loss: 0.00001775
Iteration 42/1000 | Loss: 0.00001774
Iteration 43/1000 | Loss: 0.00001774
Iteration 44/1000 | Loss: 0.00001773
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001773
Iteration 48/1000 | Loss: 0.00001773
Iteration 49/1000 | Loss: 0.00001773
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001772
Iteration 52/1000 | Loss: 0.00001772
Iteration 53/1000 | Loss: 0.00001772
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001771
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001769
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001769
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001767
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001767
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001765
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001764
Iteration 113/1000 | Loss: 0.00001764
Iteration 114/1000 | Loss: 0.00001764
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001763
Iteration 118/1000 | Loss: 0.00001763
Iteration 119/1000 | Loss: 0.00001763
Iteration 120/1000 | Loss: 0.00001763
Iteration 121/1000 | Loss: 0.00001763
Iteration 122/1000 | Loss: 0.00001762
Iteration 123/1000 | Loss: 0.00001762
Iteration 124/1000 | Loss: 0.00001762
Iteration 125/1000 | Loss: 0.00001762
Iteration 126/1000 | Loss: 0.00001762
Iteration 127/1000 | Loss: 0.00001762
Iteration 128/1000 | Loss: 0.00001762
Iteration 129/1000 | Loss: 0.00001762
Iteration 130/1000 | Loss: 0.00001762
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001762
Iteration 133/1000 | Loss: 0.00001762
Iteration 134/1000 | Loss: 0.00001762
Iteration 135/1000 | Loss: 0.00001762
Iteration 136/1000 | Loss: 0.00001762
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001761
Iteration 143/1000 | Loss: 0.00001761
Iteration 144/1000 | Loss: 0.00001761
Iteration 145/1000 | Loss: 0.00001761
Iteration 146/1000 | Loss: 0.00001761
Iteration 147/1000 | Loss: 0.00001761
Iteration 148/1000 | Loss: 0.00001761
Iteration 149/1000 | Loss: 0.00001761
Iteration 150/1000 | Loss: 0.00001761
Iteration 151/1000 | Loss: 0.00001761
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001760
Iteration 155/1000 | Loss: 0.00001760
Iteration 156/1000 | Loss: 0.00001760
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001760
Iteration 161/1000 | Loss: 0.00001760
Iteration 162/1000 | Loss: 0.00001760
Iteration 163/1000 | Loss: 0.00001760
Iteration 164/1000 | Loss: 0.00001760
Iteration 165/1000 | Loss: 0.00001760
Iteration 166/1000 | Loss: 0.00001760
Iteration 167/1000 | Loss: 0.00001760
Iteration 168/1000 | Loss: 0.00001760
Iteration 169/1000 | Loss: 0.00001760
Iteration 170/1000 | Loss: 0.00001760
Iteration 171/1000 | Loss: 0.00001760
Iteration 172/1000 | Loss: 0.00001760
Iteration 173/1000 | Loss: 0.00001760
Iteration 174/1000 | Loss: 0.00001760
Iteration 175/1000 | Loss: 0.00001760
Iteration 176/1000 | Loss: 0.00001760
Iteration 177/1000 | Loss: 0.00001760
Iteration 178/1000 | Loss: 0.00001760
Iteration 179/1000 | Loss: 0.00001760
Iteration 180/1000 | Loss: 0.00001760
Iteration 181/1000 | Loss: 0.00001760
Iteration 182/1000 | Loss: 0.00001760
Iteration 183/1000 | Loss: 0.00001760
Iteration 184/1000 | Loss: 0.00001760
Iteration 185/1000 | Loss: 0.00001760
Iteration 186/1000 | Loss: 0.00001760
Iteration 187/1000 | Loss: 0.00001760
Iteration 188/1000 | Loss: 0.00001760
Iteration 189/1000 | Loss: 0.00001760
Iteration 190/1000 | Loss: 0.00001760
Iteration 191/1000 | Loss: 0.00001760
Iteration 192/1000 | Loss: 0.00001760
Iteration 193/1000 | Loss: 0.00001760
Iteration 194/1000 | Loss: 0.00001760
Iteration 195/1000 | Loss: 0.00001760
Iteration 196/1000 | Loss: 0.00001760
Iteration 197/1000 | Loss: 0.00001760
Iteration 198/1000 | Loss: 0.00001760
Iteration 199/1000 | Loss: 0.00001760
Iteration 200/1000 | Loss: 0.00001760
Iteration 201/1000 | Loss: 0.00001760
Iteration 202/1000 | Loss: 0.00001760
Iteration 203/1000 | Loss: 0.00001760
Iteration 204/1000 | Loss: 0.00001760
Iteration 205/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.7597934856894426e-05, 1.7597934856894426e-05, 1.7597934856894426e-05, 1.7597934856894426e-05, 1.7597934856894426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7597934856894426e-05

Optimization complete. Final v2v error: 3.4023070335388184 mm

Highest mean error: 4.18315315246582 mm for frame 153

Lowest mean error: 2.847522020339966 mm for frame 11

Saving results

Total time: 36.94663166999817
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897447
Iteration 2/25 | Loss: 0.00139991
Iteration 3/25 | Loss: 0.00108492
Iteration 4/25 | Loss: 0.00104480
Iteration 5/25 | Loss: 0.00104127
Iteration 6/25 | Loss: 0.00104121
Iteration 7/25 | Loss: 0.00104121
Iteration 8/25 | Loss: 0.00104121
Iteration 9/25 | Loss: 0.00104121
Iteration 10/25 | Loss: 0.00104121
Iteration 11/25 | Loss: 0.00104121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010412093251943588, 0.0010412093251943588, 0.0010412093251943588, 0.0010412093251943588, 0.0010412093251943588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010412093251943588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89111006
Iteration 2/25 | Loss: 0.00029318
Iteration 3/25 | Loss: 0.00029317
Iteration 4/25 | Loss: 0.00029317
Iteration 5/25 | Loss: 0.00029317
Iteration 6/25 | Loss: 0.00029317
Iteration 7/25 | Loss: 0.00029317
Iteration 8/25 | Loss: 0.00029317
Iteration 9/25 | Loss: 0.00029317
Iteration 10/25 | Loss: 0.00029317
Iteration 11/25 | Loss: 0.00029317
Iteration 12/25 | Loss: 0.00029317
Iteration 13/25 | Loss: 0.00029317
Iteration 14/25 | Loss: 0.00029317
Iteration 15/25 | Loss: 0.00029317
Iteration 16/25 | Loss: 0.00029317
Iteration 17/25 | Loss: 0.00029317
Iteration 18/25 | Loss: 0.00029317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00029317024745978415, 0.00029317024745978415, 0.00029317024745978415, 0.00029317024745978415, 0.00029317024745978415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029317024745978415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029317
Iteration 2/1000 | Loss: 0.00002685
Iteration 3/1000 | Loss: 0.00002181
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001900
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001787
Iteration 8/1000 | Loss: 0.00001763
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001733
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001716
Iteration 14/1000 | Loss: 0.00001716
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001715
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001715
Iteration 20/1000 | Loss: 0.00001715
Iteration 21/1000 | Loss: 0.00001714
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001712
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001709
Iteration 35/1000 | Loss: 0.00001709
Iteration 36/1000 | Loss: 0.00001709
Iteration 37/1000 | Loss: 0.00001709
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001706
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001703
Iteration 64/1000 | Loss: 0.00001703
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001702
Iteration 72/1000 | Loss: 0.00001702
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001702
Iteration 76/1000 | Loss: 0.00001702
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001701
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001701
Iteration 88/1000 | Loss: 0.00001701
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.7007780115818605e-05, 1.7007780115818605e-05, 1.7007780115818605e-05, 1.7007780115818605e-05, 1.7007780115818605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7007780115818605e-05

Optimization complete. Final v2v error: 3.4569828510284424 mm

Highest mean error: 3.5921924114227295 mm for frame 63

Lowest mean error: 3.345818281173706 mm for frame 7

Saving results

Total time: 32.0380220413208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089414
Iteration 2/25 | Loss: 0.00243784
Iteration 3/25 | Loss: 0.00175131
Iteration 4/25 | Loss: 0.00162676
Iteration 5/25 | Loss: 0.00149085
Iteration 6/25 | Loss: 0.00130103
Iteration 7/25 | Loss: 0.00122009
Iteration 8/25 | Loss: 0.00118076
Iteration 9/25 | Loss: 0.00118169
Iteration 10/25 | Loss: 0.00115831
Iteration 11/25 | Loss: 0.00115145
Iteration 12/25 | Loss: 0.00115232
Iteration 13/25 | Loss: 0.00114922
Iteration 14/25 | Loss: 0.00115127
Iteration 15/25 | Loss: 0.00114821
Iteration 16/25 | Loss: 0.00114864
Iteration 17/25 | Loss: 0.00114815
Iteration 18/25 | Loss: 0.00115597
Iteration 19/25 | Loss: 0.00114970
Iteration 20/25 | Loss: 0.00114179
Iteration 21/25 | Loss: 0.00114135
Iteration 22/25 | Loss: 0.00114127
Iteration 23/25 | Loss: 0.00114294
Iteration 24/25 | Loss: 0.00114232
Iteration 25/25 | Loss: 0.00114124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32352674
Iteration 2/25 | Loss: 0.00230732
Iteration 3/25 | Loss: 0.00230731
Iteration 4/25 | Loss: 0.00230731
Iteration 5/25 | Loss: 0.00230731
Iteration 6/25 | Loss: 0.00230731
Iteration 7/25 | Loss: 0.00230731
Iteration 8/25 | Loss: 0.00230731
Iteration 9/25 | Loss: 0.00230731
Iteration 10/25 | Loss: 0.00230731
Iteration 11/25 | Loss: 0.00230731
Iteration 12/25 | Loss: 0.00230731
Iteration 13/25 | Loss: 0.00230731
Iteration 14/25 | Loss: 0.00230731
Iteration 15/25 | Loss: 0.00230731
Iteration 16/25 | Loss: 0.00230731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023073055781424046, 0.0023073055781424046, 0.0023073055781424046, 0.0023073055781424046, 0.0023073055781424046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023073055781424046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230731
Iteration 2/1000 | Loss: 0.00073194
Iteration 3/1000 | Loss: 0.00113642
Iteration 4/1000 | Loss: 0.00030892
Iteration 5/1000 | Loss: 0.00016188
Iteration 6/1000 | Loss: 0.00017789
Iteration 7/1000 | Loss: 0.00046282
Iteration 8/1000 | Loss: 0.00012541
Iteration 9/1000 | Loss: 0.00009935
Iteration 10/1000 | Loss: 0.00027438
Iteration 11/1000 | Loss: 0.00017974
Iteration 12/1000 | Loss: 0.00009573
Iteration 13/1000 | Loss: 0.00008390
Iteration 14/1000 | Loss: 0.00011091
Iteration 15/1000 | Loss: 0.00007810
Iteration 16/1000 | Loss: 0.00007618
Iteration 17/1000 | Loss: 0.00007477
Iteration 18/1000 | Loss: 0.00007388
Iteration 19/1000 | Loss: 0.00010613
Iteration 20/1000 | Loss: 0.00007251
Iteration 21/1000 | Loss: 0.00007191
Iteration 22/1000 | Loss: 0.00007046
Iteration 23/1000 | Loss: 0.00010848
Iteration 24/1000 | Loss: 0.00006867
Iteration 25/1000 | Loss: 0.00006779
Iteration 26/1000 | Loss: 0.00006617
Iteration 27/1000 | Loss: 0.00006502
Iteration 28/1000 | Loss: 0.00011403
Iteration 29/1000 | Loss: 0.00006355
Iteration 30/1000 | Loss: 0.00012163
Iteration 31/1000 | Loss: 0.00074539
Iteration 32/1000 | Loss: 0.00060136
Iteration 33/1000 | Loss: 0.00072969
Iteration 34/1000 | Loss: 0.00031864
Iteration 35/1000 | Loss: 0.00007324
Iteration 36/1000 | Loss: 0.00005238
Iteration 37/1000 | Loss: 0.00023468
Iteration 38/1000 | Loss: 0.00004173
Iteration 39/1000 | Loss: 0.00015303
Iteration 40/1000 | Loss: 0.00003670
Iteration 41/1000 | Loss: 0.00007771
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00005685
Iteration 44/1000 | Loss: 0.00004087
Iteration 45/1000 | Loss: 0.00003558
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001840
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001829
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001817
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001811
Iteration 65/1000 | Loss: 0.00001811
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001806
Iteration 73/1000 | Loss: 0.00001806
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001806
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001804
Iteration 92/1000 | Loss: 0.00001804
Iteration 93/1000 | Loss: 0.00001804
Iteration 94/1000 | Loss: 0.00001804
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001803
Iteration 99/1000 | Loss: 0.00001803
Iteration 100/1000 | Loss: 0.00001803
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001802
Iteration 111/1000 | Loss: 0.00001802
Iteration 112/1000 | Loss: 0.00001802
Iteration 113/1000 | Loss: 0.00001802
Iteration 114/1000 | Loss: 0.00001802
Iteration 115/1000 | Loss: 0.00001802
Iteration 116/1000 | Loss: 0.00001802
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.8020346033154055e-05, 1.8020346033154055e-05, 1.8020346033154055e-05, 1.8020346033154055e-05, 1.8020346033154055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8020346033154055e-05

Optimization complete. Final v2v error: 3.0841782093048096 mm

Highest mean error: 12.193822860717773 mm for frame 35

Lowest mean error: 2.6392900943756104 mm for frame 87

Saving results

Total time: 114.33567428588867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422795
Iteration 2/25 | Loss: 0.00108070
Iteration 3/25 | Loss: 0.00097738
Iteration 4/25 | Loss: 0.00096602
Iteration 5/25 | Loss: 0.00096194
Iteration 6/25 | Loss: 0.00096126
Iteration 7/25 | Loss: 0.00096126
Iteration 8/25 | Loss: 0.00096126
Iteration 9/25 | Loss: 0.00096126
Iteration 10/25 | Loss: 0.00096126
Iteration 11/25 | Loss: 0.00096126
Iteration 12/25 | Loss: 0.00096126
Iteration 13/25 | Loss: 0.00096126
Iteration 14/25 | Loss: 0.00096126
Iteration 15/25 | Loss: 0.00096126
Iteration 16/25 | Loss: 0.00096126
Iteration 17/25 | Loss: 0.00096126
Iteration 18/25 | Loss: 0.00096126
Iteration 19/25 | Loss: 0.00096126
Iteration 20/25 | Loss: 0.00096126
Iteration 21/25 | Loss: 0.00096126
Iteration 22/25 | Loss: 0.00096126
Iteration 23/25 | Loss: 0.00096126
Iteration 24/25 | Loss: 0.00096126
Iteration 25/25 | Loss: 0.00096126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37129807
Iteration 2/25 | Loss: 0.00058304
Iteration 3/25 | Loss: 0.00058304
Iteration 4/25 | Loss: 0.00058304
Iteration 5/25 | Loss: 0.00058304
Iteration 6/25 | Loss: 0.00058304
Iteration 7/25 | Loss: 0.00058304
Iteration 8/25 | Loss: 0.00058304
Iteration 9/25 | Loss: 0.00058304
Iteration 10/25 | Loss: 0.00058304
Iteration 11/25 | Loss: 0.00058304
Iteration 12/25 | Loss: 0.00058304
Iteration 13/25 | Loss: 0.00058304
Iteration 14/25 | Loss: 0.00058304
Iteration 15/25 | Loss: 0.00058304
Iteration 16/25 | Loss: 0.00058304
Iteration 17/25 | Loss: 0.00058304
Iteration 18/25 | Loss: 0.00058304
Iteration 19/25 | Loss: 0.00058304
Iteration 20/25 | Loss: 0.00058304
Iteration 21/25 | Loss: 0.00058304
Iteration 22/25 | Loss: 0.00058304
Iteration 23/25 | Loss: 0.00058304
Iteration 24/25 | Loss: 0.00058304
Iteration 25/25 | Loss: 0.00058304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058304
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002070
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001654
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001511
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001427
Iteration 14/1000 | Loss: 0.00001423
Iteration 15/1000 | Loss: 0.00001423
Iteration 16/1000 | Loss: 0.00001423
Iteration 17/1000 | Loss: 0.00001423
Iteration 18/1000 | Loss: 0.00001423
Iteration 19/1000 | Loss: 0.00001423
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001420
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001417
Iteration 32/1000 | Loss: 0.00001417
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001416
Iteration 36/1000 | Loss: 0.00001416
Iteration 37/1000 | Loss: 0.00001415
Iteration 38/1000 | Loss: 0.00001415
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001414
Iteration 44/1000 | Loss: 0.00001414
Iteration 45/1000 | Loss: 0.00001414
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001414
Iteration 49/1000 | Loss: 0.00001414
Iteration 50/1000 | Loss: 0.00001414
Iteration 51/1000 | Loss: 0.00001414
Iteration 52/1000 | Loss: 0.00001413
Iteration 53/1000 | Loss: 0.00001413
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001413
Iteration 58/1000 | Loss: 0.00001413
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001413
Iteration 61/1000 | Loss: 0.00001413
Iteration 62/1000 | Loss: 0.00001412
Iteration 63/1000 | Loss: 0.00001412
Iteration 64/1000 | Loss: 0.00001412
Iteration 65/1000 | Loss: 0.00001412
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001412
Iteration 69/1000 | Loss: 0.00001412
Iteration 70/1000 | Loss: 0.00001412
Iteration 71/1000 | Loss: 0.00001412
Iteration 72/1000 | Loss: 0.00001412
Iteration 73/1000 | Loss: 0.00001412
Iteration 74/1000 | Loss: 0.00001412
Iteration 75/1000 | Loss: 0.00001412
Iteration 76/1000 | Loss: 0.00001412
Iteration 77/1000 | Loss: 0.00001412
Iteration 78/1000 | Loss: 0.00001412
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.4124911103863269e-05, 1.4124911103863269e-05, 1.4124911103863269e-05, 1.4124911103863269e-05, 1.4124911103863269e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4124911103863269e-05

Optimization complete. Final v2v error: 3.1532320976257324 mm

Highest mean error: 3.642991065979004 mm for frame 24

Lowest mean error: 2.688403606414795 mm for frame 46

Saving results

Total time: 27.034104824066162
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031306
Iteration 2/25 | Loss: 0.00161439
Iteration 3/25 | Loss: 0.00114562
Iteration 4/25 | Loss: 0.00112098
Iteration 5/25 | Loss: 0.00111236
Iteration 6/25 | Loss: 0.00110903
Iteration 7/25 | Loss: 0.00110880
Iteration 8/25 | Loss: 0.00110880
Iteration 9/25 | Loss: 0.00110880
Iteration 10/25 | Loss: 0.00110880
Iteration 11/25 | Loss: 0.00110880
Iteration 12/25 | Loss: 0.00110880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011088012252002954, 0.0011088012252002954, 0.0011088012252002954, 0.0011088012252002954, 0.0011088012252002954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011088012252002954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89171869
Iteration 2/25 | Loss: 0.00069876
Iteration 3/25 | Loss: 0.00069876
Iteration 4/25 | Loss: 0.00069876
Iteration 5/25 | Loss: 0.00069876
Iteration 6/25 | Loss: 0.00069876
Iteration 7/25 | Loss: 0.00069876
Iteration 8/25 | Loss: 0.00069876
Iteration 9/25 | Loss: 0.00069876
Iteration 10/25 | Loss: 0.00069876
Iteration 11/25 | Loss: 0.00069875
Iteration 12/25 | Loss: 0.00069875
Iteration 13/25 | Loss: 0.00069875
Iteration 14/25 | Loss: 0.00069875
Iteration 15/25 | Loss: 0.00069875
Iteration 16/25 | Loss: 0.00069875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006987549131736159, 0.0006987549131736159, 0.0006987549131736159, 0.0006987549131736159, 0.0006987549131736159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006987549131736159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069875
Iteration 2/1000 | Loss: 0.00006348
Iteration 3/1000 | Loss: 0.00004394
Iteration 4/1000 | Loss: 0.00003636
Iteration 5/1000 | Loss: 0.00003472
Iteration 6/1000 | Loss: 0.00003340
Iteration 7/1000 | Loss: 0.00003260
Iteration 8/1000 | Loss: 0.00003210
Iteration 9/1000 | Loss: 0.00003168
Iteration 10/1000 | Loss: 0.00003134
Iteration 11/1000 | Loss: 0.00003110
Iteration 12/1000 | Loss: 0.00003090
Iteration 13/1000 | Loss: 0.00003066
Iteration 14/1000 | Loss: 0.00003045
Iteration 15/1000 | Loss: 0.00003024
Iteration 16/1000 | Loss: 0.00003004
Iteration 17/1000 | Loss: 0.00002990
Iteration 18/1000 | Loss: 0.00002988
Iteration 19/1000 | Loss: 0.00002975
Iteration 20/1000 | Loss: 0.00002960
Iteration 21/1000 | Loss: 0.00002948
Iteration 22/1000 | Loss: 0.00002941
Iteration 23/1000 | Loss: 0.00002941
Iteration 24/1000 | Loss: 0.00002939
Iteration 25/1000 | Loss: 0.00002939
Iteration 26/1000 | Loss: 0.00002938
Iteration 27/1000 | Loss: 0.00002937
Iteration 28/1000 | Loss: 0.00002937
Iteration 29/1000 | Loss: 0.00002936
Iteration 30/1000 | Loss: 0.00002933
Iteration 31/1000 | Loss: 0.00002931
Iteration 32/1000 | Loss: 0.00002930
Iteration 33/1000 | Loss: 0.00002927
Iteration 34/1000 | Loss: 0.00002927
Iteration 35/1000 | Loss: 0.00002925
Iteration 36/1000 | Loss: 0.00002925
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00002924
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00002924
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00002924
Iteration 43/1000 | Loss: 0.00002924
Iteration 44/1000 | Loss: 0.00002923
Iteration 45/1000 | Loss: 0.00002923
Iteration 46/1000 | Loss: 0.00002923
Iteration 47/1000 | Loss: 0.00002923
Iteration 48/1000 | Loss: 0.00002923
Iteration 49/1000 | Loss: 0.00002923
Iteration 50/1000 | Loss: 0.00002923
Iteration 51/1000 | Loss: 0.00002923
Iteration 52/1000 | Loss: 0.00002923
Iteration 53/1000 | Loss: 0.00002923
Iteration 54/1000 | Loss: 0.00002922
Iteration 55/1000 | Loss: 0.00002922
Iteration 56/1000 | Loss: 0.00002922
Iteration 57/1000 | Loss: 0.00002922
Iteration 58/1000 | Loss: 0.00002922
Iteration 59/1000 | Loss: 0.00002922
Iteration 60/1000 | Loss: 0.00002922
Iteration 61/1000 | Loss: 0.00002922
Iteration 62/1000 | Loss: 0.00002921
Iteration 63/1000 | Loss: 0.00002921
Iteration 64/1000 | Loss: 0.00002920
Iteration 65/1000 | Loss: 0.00002920
Iteration 66/1000 | Loss: 0.00002920
Iteration 67/1000 | Loss: 0.00002920
Iteration 68/1000 | Loss: 0.00002920
Iteration 69/1000 | Loss: 0.00002920
Iteration 70/1000 | Loss: 0.00002920
Iteration 71/1000 | Loss: 0.00002920
Iteration 72/1000 | Loss: 0.00002919
Iteration 73/1000 | Loss: 0.00002919
Iteration 74/1000 | Loss: 0.00002918
Iteration 75/1000 | Loss: 0.00002918
Iteration 76/1000 | Loss: 0.00002917
Iteration 77/1000 | Loss: 0.00002917
Iteration 78/1000 | Loss: 0.00002917
Iteration 79/1000 | Loss: 0.00002917
Iteration 80/1000 | Loss: 0.00002917
Iteration 81/1000 | Loss: 0.00002917
Iteration 82/1000 | Loss: 0.00002917
Iteration 83/1000 | Loss: 0.00002917
Iteration 84/1000 | Loss: 0.00002916
Iteration 85/1000 | Loss: 0.00002916
Iteration 86/1000 | Loss: 0.00002916
Iteration 87/1000 | Loss: 0.00002916
Iteration 88/1000 | Loss: 0.00002915
Iteration 89/1000 | Loss: 0.00002915
Iteration 90/1000 | Loss: 0.00002915
Iteration 91/1000 | Loss: 0.00002915
Iteration 92/1000 | Loss: 0.00002915
Iteration 93/1000 | Loss: 0.00002915
Iteration 94/1000 | Loss: 0.00002915
Iteration 95/1000 | Loss: 0.00002915
Iteration 96/1000 | Loss: 0.00002915
Iteration 97/1000 | Loss: 0.00002914
Iteration 98/1000 | Loss: 0.00002914
Iteration 99/1000 | Loss: 0.00002914
Iteration 100/1000 | Loss: 0.00002914
Iteration 101/1000 | Loss: 0.00002914
Iteration 102/1000 | Loss: 0.00002914
Iteration 103/1000 | Loss: 0.00002914
Iteration 104/1000 | Loss: 0.00002914
Iteration 105/1000 | Loss: 0.00002913
Iteration 106/1000 | Loss: 0.00002913
Iteration 107/1000 | Loss: 0.00002913
Iteration 108/1000 | Loss: 0.00002913
Iteration 109/1000 | Loss: 0.00002912
Iteration 110/1000 | Loss: 0.00002912
Iteration 111/1000 | Loss: 0.00002912
Iteration 112/1000 | Loss: 0.00002912
Iteration 113/1000 | Loss: 0.00002912
Iteration 114/1000 | Loss: 0.00002912
Iteration 115/1000 | Loss: 0.00002911
Iteration 116/1000 | Loss: 0.00002911
Iteration 117/1000 | Loss: 0.00002910
Iteration 118/1000 | Loss: 0.00002910
Iteration 119/1000 | Loss: 0.00002910
Iteration 120/1000 | Loss: 0.00002910
Iteration 121/1000 | Loss: 0.00002910
Iteration 122/1000 | Loss: 0.00002910
Iteration 123/1000 | Loss: 0.00002910
Iteration 124/1000 | Loss: 0.00002910
Iteration 125/1000 | Loss: 0.00002909
Iteration 126/1000 | Loss: 0.00002909
Iteration 127/1000 | Loss: 0.00002909
Iteration 128/1000 | Loss: 0.00002909
Iteration 129/1000 | Loss: 0.00002909
Iteration 130/1000 | Loss: 0.00002908
Iteration 131/1000 | Loss: 0.00002908
Iteration 132/1000 | Loss: 0.00002908
Iteration 133/1000 | Loss: 0.00002908
Iteration 134/1000 | Loss: 0.00002908
Iteration 135/1000 | Loss: 0.00002908
Iteration 136/1000 | Loss: 0.00002908
Iteration 137/1000 | Loss: 0.00002907
Iteration 138/1000 | Loss: 0.00002907
Iteration 139/1000 | Loss: 0.00002907
Iteration 140/1000 | Loss: 0.00002906
Iteration 141/1000 | Loss: 0.00002906
Iteration 142/1000 | Loss: 0.00002906
Iteration 143/1000 | Loss: 0.00002906
Iteration 144/1000 | Loss: 0.00002906
Iteration 145/1000 | Loss: 0.00002906
Iteration 146/1000 | Loss: 0.00002906
Iteration 147/1000 | Loss: 0.00002906
Iteration 148/1000 | Loss: 0.00002906
Iteration 149/1000 | Loss: 0.00002905
Iteration 150/1000 | Loss: 0.00002905
Iteration 151/1000 | Loss: 0.00002905
Iteration 152/1000 | Loss: 0.00002905
Iteration 153/1000 | Loss: 0.00002905
Iteration 154/1000 | Loss: 0.00002905
Iteration 155/1000 | Loss: 0.00002904
Iteration 156/1000 | Loss: 0.00002904
Iteration 157/1000 | Loss: 0.00002904
Iteration 158/1000 | Loss: 0.00002904
Iteration 159/1000 | Loss: 0.00002904
Iteration 160/1000 | Loss: 0.00002904
Iteration 161/1000 | Loss: 0.00002904
Iteration 162/1000 | Loss: 0.00002904
Iteration 163/1000 | Loss: 0.00002903
Iteration 164/1000 | Loss: 0.00002903
Iteration 165/1000 | Loss: 0.00002903
Iteration 166/1000 | Loss: 0.00002903
Iteration 167/1000 | Loss: 0.00002903
Iteration 168/1000 | Loss: 0.00002903
Iteration 169/1000 | Loss: 0.00002903
Iteration 170/1000 | Loss: 0.00002903
Iteration 171/1000 | Loss: 0.00002902
Iteration 172/1000 | Loss: 0.00002902
Iteration 173/1000 | Loss: 0.00002902
Iteration 174/1000 | Loss: 0.00002902
Iteration 175/1000 | Loss: 0.00002902
Iteration 176/1000 | Loss: 0.00002902
Iteration 177/1000 | Loss: 0.00002902
Iteration 178/1000 | Loss: 0.00002902
Iteration 179/1000 | Loss: 0.00002902
Iteration 180/1000 | Loss: 0.00002902
Iteration 181/1000 | Loss: 0.00002902
Iteration 182/1000 | Loss: 0.00002902
Iteration 183/1000 | Loss: 0.00002902
Iteration 184/1000 | Loss: 0.00002901
Iteration 185/1000 | Loss: 0.00002901
Iteration 186/1000 | Loss: 0.00002901
Iteration 187/1000 | Loss: 0.00002901
Iteration 188/1000 | Loss: 0.00002901
Iteration 189/1000 | Loss: 0.00002901
Iteration 190/1000 | Loss: 0.00002901
Iteration 191/1000 | Loss: 0.00002901
Iteration 192/1000 | Loss: 0.00002901
Iteration 193/1000 | Loss: 0.00002901
Iteration 194/1000 | Loss: 0.00002901
Iteration 195/1000 | Loss: 0.00002901
Iteration 196/1000 | Loss: 0.00002901
Iteration 197/1000 | Loss: 0.00002901
Iteration 198/1000 | Loss: 0.00002901
Iteration 199/1000 | Loss: 0.00002901
Iteration 200/1000 | Loss: 0.00002901
Iteration 201/1000 | Loss: 0.00002901
Iteration 202/1000 | Loss: 0.00002901
Iteration 203/1000 | Loss: 0.00002901
Iteration 204/1000 | Loss: 0.00002901
Iteration 205/1000 | Loss: 0.00002901
Iteration 206/1000 | Loss: 0.00002901
Iteration 207/1000 | Loss: 0.00002901
Iteration 208/1000 | Loss: 0.00002901
Iteration 209/1000 | Loss: 0.00002901
Iteration 210/1000 | Loss: 0.00002901
Iteration 211/1000 | Loss: 0.00002901
Iteration 212/1000 | Loss: 0.00002901
Iteration 213/1000 | Loss: 0.00002901
Iteration 214/1000 | Loss: 0.00002901
Iteration 215/1000 | Loss: 0.00002901
Iteration 216/1000 | Loss: 0.00002901
Iteration 217/1000 | Loss: 0.00002901
Iteration 218/1000 | Loss: 0.00002901
Iteration 219/1000 | Loss: 0.00002901
Iteration 220/1000 | Loss: 0.00002901
Iteration 221/1000 | Loss: 0.00002901
Iteration 222/1000 | Loss: 0.00002901
Iteration 223/1000 | Loss: 0.00002901
Iteration 224/1000 | Loss: 0.00002901
Iteration 225/1000 | Loss: 0.00002901
Iteration 226/1000 | Loss: 0.00002901
Iteration 227/1000 | Loss: 0.00002901
Iteration 228/1000 | Loss: 0.00002901
Iteration 229/1000 | Loss: 0.00002901
Iteration 230/1000 | Loss: 0.00002901
Iteration 231/1000 | Loss: 0.00002901
Iteration 232/1000 | Loss: 0.00002901
Iteration 233/1000 | Loss: 0.00002901
Iteration 234/1000 | Loss: 0.00002901
Iteration 235/1000 | Loss: 0.00002901
Iteration 236/1000 | Loss: 0.00002901
Iteration 237/1000 | Loss: 0.00002901
Iteration 238/1000 | Loss: 0.00002901
Iteration 239/1000 | Loss: 0.00002901
Iteration 240/1000 | Loss: 0.00002901
Iteration 241/1000 | Loss: 0.00002901
Iteration 242/1000 | Loss: 0.00002901
Iteration 243/1000 | Loss: 0.00002901
Iteration 244/1000 | Loss: 0.00002901
Iteration 245/1000 | Loss: 0.00002901
Iteration 246/1000 | Loss: 0.00002901
Iteration 247/1000 | Loss: 0.00002901
Iteration 248/1000 | Loss: 0.00002901
Iteration 249/1000 | Loss: 0.00002901
Iteration 250/1000 | Loss: 0.00002901
Iteration 251/1000 | Loss: 0.00002901
Iteration 252/1000 | Loss: 0.00002901
Iteration 253/1000 | Loss: 0.00002901
Iteration 254/1000 | Loss: 0.00002901
Iteration 255/1000 | Loss: 0.00002901
Iteration 256/1000 | Loss: 0.00002901
Iteration 257/1000 | Loss: 0.00002901
Iteration 258/1000 | Loss: 0.00002901
Iteration 259/1000 | Loss: 0.00002901
Iteration 260/1000 | Loss: 0.00002901
Iteration 261/1000 | Loss: 0.00002901
Iteration 262/1000 | Loss: 0.00002901
Iteration 263/1000 | Loss: 0.00002901
Iteration 264/1000 | Loss: 0.00002901
Iteration 265/1000 | Loss: 0.00002901
Iteration 266/1000 | Loss: 0.00002901
Iteration 267/1000 | Loss: 0.00002901
Iteration 268/1000 | Loss: 0.00002901
Iteration 269/1000 | Loss: 0.00002901
Iteration 270/1000 | Loss: 0.00002901
Iteration 271/1000 | Loss: 0.00002901
Iteration 272/1000 | Loss: 0.00002901
Iteration 273/1000 | Loss: 0.00002901
Iteration 274/1000 | Loss: 0.00002901
Iteration 275/1000 | Loss: 0.00002901
Iteration 276/1000 | Loss: 0.00002901
Iteration 277/1000 | Loss: 0.00002901
Iteration 278/1000 | Loss: 0.00002901
Iteration 279/1000 | Loss: 0.00002901
Iteration 280/1000 | Loss: 0.00002901
Iteration 281/1000 | Loss: 0.00002901
Iteration 282/1000 | Loss: 0.00002901
Iteration 283/1000 | Loss: 0.00002901
Iteration 284/1000 | Loss: 0.00002901
Iteration 285/1000 | Loss: 0.00002901
Iteration 286/1000 | Loss: 0.00002901
Iteration 287/1000 | Loss: 0.00002901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [2.9005697797401808e-05, 2.9005697797401808e-05, 2.9005697797401808e-05, 2.9005697797401808e-05, 2.9005697797401808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9005697797401808e-05

Optimization complete. Final v2v error: 4.29245138168335 mm

Highest mean error: 5.401394367218018 mm for frame 108

Lowest mean error: 3.253833055496216 mm for frame 51

Saving results

Total time: 60.059924840927124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799686
Iteration 2/25 | Loss: 0.00168735
Iteration 3/25 | Loss: 0.00116333
Iteration 4/25 | Loss: 0.00116902
Iteration 5/25 | Loss: 0.00109271
Iteration 6/25 | Loss: 0.00105497
Iteration 7/25 | Loss: 0.00104900
Iteration 8/25 | Loss: 0.00105217
Iteration 9/25 | Loss: 0.00106345
Iteration 10/25 | Loss: 0.00106119
Iteration 11/25 | Loss: 0.00105520
Iteration 12/25 | Loss: 0.00104959
Iteration 13/25 | Loss: 0.00104586
Iteration 14/25 | Loss: 0.00104511
Iteration 15/25 | Loss: 0.00104484
Iteration 16/25 | Loss: 0.00104524
Iteration 17/25 | Loss: 0.00104369
Iteration 18/25 | Loss: 0.00104351
Iteration 19/25 | Loss: 0.00104229
Iteration 20/25 | Loss: 0.00104161
Iteration 21/25 | Loss: 0.00104140
Iteration 22/25 | Loss: 0.00104133
Iteration 23/25 | Loss: 0.00104125
Iteration 24/25 | Loss: 0.00104115
Iteration 25/25 | Loss: 0.00104109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.49042845
Iteration 2/25 | Loss: 0.00045519
Iteration 3/25 | Loss: 0.00045517
Iteration 4/25 | Loss: 0.00045517
Iteration 5/25 | Loss: 0.00045517
Iteration 6/25 | Loss: 0.00045517
Iteration 7/25 | Loss: 0.00045517
Iteration 8/25 | Loss: 0.00045517
Iteration 9/25 | Loss: 0.00045517
Iteration 10/25 | Loss: 0.00045517
Iteration 11/25 | Loss: 0.00045517
Iteration 12/25 | Loss: 0.00045517
Iteration 13/25 | Loss: 0.00045517
Iteration 14/25 | Loss: 0.00045517
Iteration 15/25 | Loss: 0.00045517
Iteration 16/25 | Loss: 0.00045517
Iteration 17/25 | Loss: 0.00045517
Iteration 18/25 | Loss: 0.00045517
Iteration 19/25 | Loss: 0.00045517
Iteration 20/25 | Loss: 0.00045517
Iteration 21/25 | Loss: 0.00045517
Iteration 22/25 | Loss: 0.00045517
Iteration 23/25 | Loss: 0.00045517
Iteration 24/25 | Loss: 0.00045517
Iteration 25/25 | Loss: 0.00045517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045517
Iteration 2/1000 | Loss: 0.00002757
Iteration 3/1000 | Loss: 0.00002127
Iteration 4/1000 | Loss: 0.00001986
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001870
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001814
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001800
Iteration 13/1000 | Loss: 0.00001800
Iteration 14/1000 | Loss: 0.00001800
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001799
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001792
Iteration 21/1000 | Loss: 0.00001792
Iteration 22/1000 | Loss: 0.00001790
Iteration 23/1000 | Loss: 0.00001789
Iteration 24/1000 | Loss: 0.00001789
Iteration 25/1000 | Loss: 0.00001789
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001786
Iteration 35/1000 | Loss: 0.00001786
Iteration 36/1000 | Loss: 0.00001785
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001781
Iteration 40/1000 | Loss: 0.00001780
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001775
Iteration 52/1000 | Loss: 0.00001775
Iteration 53/1000 | Loss: 0.00001775
Iteration 54/1000 | Loss: 0.00001775
Iteration 55/1000 | Loss: 0.00001774
Iteration 56/1000 | Loss: 0.00001774
Iteration 57/1000 | Loss: 0.00001774
Iteration 58/1000 | Loss: 0.00001773
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001772
Iteration 62/1000 | Loss: 0.00001772
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001772
Iteration 67/1000 | Loss: 0.00001772
Iteration 68/1000 | Loss: 0.00001772
Iteration 69/1000 | Loss: 0.00001772
Iteration 70/1000 | Loss: 0.00001772
Iteration 71/1000 | Loss: 0.00001772
Iteration 72/1000 | Loss: 0.00001772
Iteration 73/1000 | Loss: 0.00001772
Iteration 74/1000 | Loss: 0.00001772
Iteration 75/1000 | Loss: 0.00001771
Iteration 76/1000 | Loss: 0.00001771
Iteration 77/1000 | Loss: 0.00001771
Iteration 78/1000 | Loss: 0.00001771
Iteration 79/1000 | Loss: 0.00001771
Iteration 80/1000 | Loss: 0.00001771
Iteration 81/1000 | Loss: 0.00001771
Iteration 82/1000 | Loss: 0.00001771
Iteration 83/1000 | Loss: 0.00001771
Iteration 84/1000 | Loss: 0.00001770
Iteration 85/1000 | Loss: 0.00001770
Iteration 86/1000 | Loss: 0.00001770
Iteration 87/1000 | Loss: 0.00001770
Iteration 88/1000 | Loss: 0.00001770
Iteration 89/1000 | Loss: 0.00001770
Iteration 90/1000 | Loss: 0.00001770
Iteration 91/1000 | Loss: 0.00001770
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001769
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001768
Iteration 102/1000 | Loss: 0.00001768
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001768
Iteration 107/1000 | Loss: 0.00001768
Iteration 108/1000 | Loss: 0.00001768
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.7678021322353743e-05, 1.7678021322353743e-05, 1.7678021322353743e-05, 1.7678021322353743e-05, 1.7678021322353743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7678021322353743e-05

Optimization complete. Final v2v error: 3.427593469619751 mm

Highest mean error: 3.9427480697631836 mm for frame 236

Lowest mean error: 2.9662554264068604 mm for frame 101

Saving results

Total time: 73.48129987716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00588204
Iteration 2/25 | Loss: 0.00120088
Iteration 3/25 | Loss: 0.00106705
Iteration 4/25 | Loss: 0.00104357
Iteration 5/25 | Loss: 0.00103728
Iteration 6/25 | Loss: 0.00103582
Iteration 7/25 | Loss: 0.00103582
Iteration 8/25 | Loss: 0.00103582
Iteration 9/25 | Loss: 0.00103582
Iteration 10/25 | Loss: 0.00103582
Iteration 11/25 | Loss: 0.00103582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001035816501826048, 0.001035816501826048, 0.001035816501826048, 0.001035816501826048, 0.001035816501826048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001035816501826048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31198597
Iteration 2/25 | Loss: 0.00092661
Iteration 3/25 | Loss: 0.00092661
Iteration 4/25 | Loss: 0.00092661
Iteration 5/25 | Loss: 0.00092660
Iteration 6/25 | Loss: 0.00092660
Iteration 7/25 | Loss: 0.00092660
Iteration 8/25 | Loss: 0.00092660
Iteration 9/25 | Loss: 0.00092660
Iteration 10/25 | Loss: 0.00092660
Iteration 11/25 | Loss: 0.00092660
Iteration 12/25 | Loss: 0.00092660
Iteration 13/25 | Loss: 0.00092660
Iteration 14/25 | Loss: 0.00092660
Iteration 15/25 | Loss: 0.00092660
Iteration 16/25 | Loss: 0.00092660
Iteration 17/25 | Loss: 0.00092660
Iteration 18/25 | Loss: 0.00092660
Iteration 19/25 | Loss: 0.00092660
Iteration 20/25 | Loss: 0.00092660
Iteration 21/25 | Loss: 0.00092660
Iteration 22/25 | Loss: 0.00092660
Iteration 23/25 | Loss: 0.00092660
Iteration 24/25 | Loss: 0.00092660
Iteration 25/25 | Loss: 0.00092660

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092660
Iteration 2/1000 | Loss: 0.00005548
Iteration 3/1000 | Loss: 0.00002249
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001773
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001634
Iteration 8/1000 | Loss: 0.00001604
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001567
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001545
Iteration 16/1000 | Loss: 0.00001544
Iteration 17/1000 | Loss: 0.00001543
Iteration 18/1000 | Loss: 0.00001543
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001539
Iteration 25/1000 | Loss: 0.00001539
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001538
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001536
Iteration 35/1000 | Loss: 0.00001536
Iteration 36/1000 | Loss: 0.00001536
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001534
Iteration 42/1000 | Loss: 0.00001533
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001532
Iteration 47/1000 | Loss: 0.00001532
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001532
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001526
Iteration 66/1000 | Loss: 0.00001526
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001525
Iteration 69/1000 | Loss: 0.00001525
Iteration 70/1000 | Loss: 0.00001525
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001523
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001522
Iteration 77/1000 | Loss: 0.00001522
Iteration 78/1000 | Loss: 0.00001522
Iteration 79/1000 | Loss: 0.00001522
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001520
Iteration 89/1000 | Loss: 0.00001520
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Iteration 93/1000 | Loss: 0.00001519
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001519
Iteration 101/1000 | Loss: 0.00001519
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001518
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001514
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001514
Iteration 148/1000 | Loss: 0.00001514
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001514
Iteration 153/1000 | Loss: 0.00001514
Iteration 154/1000 | Loss: 0.00001514
Iteration 155/1000 | Loss: 0.00001514
Iteration 156/1000 | Loss: 0.00001514
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001513
Iteration 159/1000 | Loss: 0.00001513
Iteration 160/1000 | Loss: 0.00001513
Iteration 161/1000 | Loss: 0.00001513
Iteration 162/1000 | Loss: 0.00001513
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001512
Iteration 167/1000 | Loss: 0.00001512
Iteration 168/1000 | Loss: 0.00001512
Iteration 169/1000 | Loss: 0.00001512
Iteration 170/1000 | Loss: 0.00001512
Iteration 171/1000 | Loss: 0.00001512
Iteration 172/1000 | Loss: 0.00001512
Iteration 173/1000 | Loss: 0.00001512
Iteration 174/1000 | Loss: 0.00001512
Iteration 175/1000 | Loss: 0.00001512
Iteration 176/1000 | Loss: 0.00001512
Iteration 177/1000 | Loss: 0.00001512
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001511
Iteration 182/1000 | Loss: 0.00001511
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001510
Iteration 186/1000 | Loss: 0.00001510
Iteration 187/1000 | Loss: 0.00001510
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001510
Iteration 190/1000 | Loss: 0.00001510
Iteration 191/1000 | Loss: 0.00001510
Iteration 192/1000 | Loss: 0.00001510
Iteration 193/1000 | Loss: 0.00001510
Iteration 194/1000 | Loss: 0.00001510
Iteration 195/1000 | Loss: 0.00001510
Iteration 196/1000 | Loss: 0.00001510
Iteration 197/1000 | Loss: 0.00001510
Iteration 198/1000 | Loss: 0.00001510
Iteration 199/1000 | Loss: 0.00001510
Iteration 200/1000 | Loss: 0.00001510
Iteration 201/1000 | Loss: 0.00001510
Iteration 202/1000 | Loss: 0.00001510
Iteration 203/1000 | Loss: 0.00001510
Iteration 204/1000 | Loss: 0.00001509
Iteration 205/1000 | Loss: 0.00001509
Iteration 206/1000 | Loss: 0.00001509
Iteration 207/1000 | Loss: 0.00001509
Iteration 208/1000 | Loss: 0.00001509
Iteration 209/1000 | Loss: 0.00001509
Iteration 210/1000 | Loss: 0.00001509
Iteration 211/1000 | Loss: 0.00001509
Iteration 212/1000 | Loss: 0.00001509
Iteration 213/1000 | Loss: 0.00001509
Iteration 214/1000 | Loss: 0.00001509
Iteration 215/1000 | Loss: 0.00001509
Iteration 216/1000 | Loss: 0.00001509
Iteration 217/1000 | Loss: 0.00001509
Iteration 218/1000 | Loss: 0.00001509
Iteration 219/1000 | Loss: 0.00001509
Iteration 220/1000 | Loss: 0.00001508
Iteration 221/1000 | Loss: 0.00001508
Iteration 222/1000 | Loss: 0.00001508
Iteration 223/1000 | Loss: 0.00001508
Iteration 224/1000 | Loss: 0.00001508
Iteration 225/1000 | Loss: 0.00001508
Iteration 226/1000 | Loss: 0.00001508
Iteration 227/1000 | Loss: 0.00001508
Iteration 228/1000 | Loss: 0.00001508
Iteration 229/1000 | Loss: 0.00001508
Iteration 230/1000 | Loss: 0.00001508
Iteration 231/1000 | Loss: 0.00001508
Iteration 232/1000 | Loss: 0.00001508
Iteration 233/1000 | Loss: 0.00001508
Iteration 234/1000 | Loss: 0.00001508
Iteration 235/1000 | Loss: 0.00001508
Iteration 236/1000 | Loss: 0.00001508
Iteration 237/1000 | Loss: 0.00001508
Iteration 238/1000 | Loss: 0.00001508
Iteration 239/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [1.5082441677805036e-05, 1.5082441677805036e-05, 1.5082441677805036e-05, 1.5082441677805036e-05, 1.5082441677805036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5082441677805036e-05

Optimization complete. Final v2v error: 3.228231191635132 mm

Highest mean error: 4.016110897064209 mm for frame 94

Lowest mean error: 2.710449695587158 mm for frame 0

Saving results

Total time: 46.313843727111816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803963
Iteration 2/25 | Loss: 0.00105779
Iteration 3/25 | Loss: 0.00097618
Iteration 4/25 | Loss: 0.00096175
Iteration 5/25 | Loss: 0.00095714
Iteration 6/25 | Loss: 0.00095601
Iteration 7/25 | Loss: 0.00095596
Iteration 8/25 | Loss: 0.00095596
Iteration 9/25 | Loss: 0.00095596
Iteration 10/25 | Loss: 0.00095596
Iteration 11/25 | Loss: 0.00095596
Iteration 12/25 | Loss: 0.00095596
Iteration 13/25 | Loss: 0.00095596
Iteration 14/25 | Loss: 0.00095596
Iteration 15/25 | Loss: 0.00095596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009559582103975117, 0.0009559582103975117, 0.0009559582103975117, 0.0009559582103975117, 0.0009559582103975117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009559582103975117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29328823
Iteration 2/25 | Loss: 0.00096541
Iteration 3/25 | Loss: 0.00096541
Iteration 4/25 | Loss: 0.00096541
Iteration 5/25 | Loss: 0.00096540
Iteration 6/25 | Loss: 0.00096540
Iteration 7/25 | Loss: 0.00096540
Iteration 8/25 | Loss: 0.00096540
Iteration 9/25 | Loss: 0.00096540
Iteration 10/25 | Loss: 0.00096540
Iteration 11/25 | Loss: 0.00096540
Iteration 12/25 | Loss: 0.00096540
Iteration 13/25 | Loss: 0.00096540
Iteration 14/25 | Loss: 0.00096540
Iteration 15/25 | Loss: 0.00096540
Iteration 16/25 | Loss: 0.00096540
Iteration 17/25 | Loss: 0.00096540
Iteration 18/25 | Loss: 0.00096540
Iteration 19/25 | Loss: 0.00096540
Iteration 20/25 | Loss: 0.00096540
Iteration 21/25 | Loss: 0.00096540
Iteration 22/25 | Loss: 0.00096540
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009654031018726528, 0.0009654031018726528, 0.0009654031018726528, 0.0009654031018726528, 0.0009654031018726528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009654031018726528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096540
Iteration 2/1000 | Loss: 0.00002884
Iteration 3/1000 | Loss: 0.00001883
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001268
Iteration 6/1000 | Loss: 0.00001218
Iteration 7/1000 | Loss: 0.00001185
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001112
Iteration 12/1000 | Loss: 0.00001106
Iteration 13/1000 | Loss: 0.00001104
Iteration 14/1000 | Loss: 0.00001102
Iteration 15/1000 | Loss: 0.00001101
Iteration 16/1000 | Loss: 0.00001100
Iteration 17/1000 | Loss: 0.00001099
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001096
Iteration 20/1000 | Loss: 0.00001095
Iteration 21/1000 | Loss: 0.00001090
Iteration 22/1000 | Loss: 0.00001089
Iteration 23/1000 | Loss: 0.00001088
Iteration 24/1000 | Loss: 0.00001087
Iteration 25/1000 | Loss: 0.00001087
Iteration 26/1000 | Loss: 0.00001087
Iteration 27/1000 | Loss: 0.00001086
Iteration 28/1000 | Loss: 0.00001086
Iteration 29/1000 | Loss: 0.00001086
Iteration 30/1000 | Loss: 0.00001085
Iteration 31/1000 | Loss: 0.00001085
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001084
Iteration 36/1000 | Loss: 0.00001084
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001083
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001083
Iteration 41/1000 | Loss: 0.00001083
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001082
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001082
Iteration 47/1000 | Loss: 0.00001082
Iteration 48/1000 | Loss: 0.00001082
Iteration 49/1000 | Loss: 0.00001082
Iteration 50/1000 | Loss: 0.00001082
Iteration 51/1000 | Loss: 0.00001081
Iteration 52/1000 | Loss: 0.00001081
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001080
Iteration 55/1000 | Loss: 0.00001080
Iteration 56/1000 | Loss: 0.00001080
Iteration 57/1000 | Loss: 0.00001080
Iteration 58/1000 | Loss: 0.00001080
Iteration 59/1000 | Loss: 0.00001080
Iteration 60/1000 | Loss: 0.00001080
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001079
Iteration 64/1000 | Loss: 0.00001079
Iteration 65/1000 | Loss: 0.00001079
Iteration 66/1000 | Loss: 0.00001079
Iteration 67/1000 | Loss: 0.00001079
Iteration 68/1000 | Loss: 0.00001079
Iteration 69/1000 | Loss: 0.00001078
Iteration 70/1000 | Loss: 0.00001078
Iteration 71/1000 | Loss: 0.00001078
Iteration 72/1000 | Loss: 0.00001078
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001077
Iteration 75/1000 | Loss: 0.00001077
Iteration 76/1000 | Loss: 0.00001077
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001076
Iteration 80/1000 | Loss: 0.00001076
Iteration 81/1000 | Loss: 0.00001076
Iteration 82/1000 | Loss: 0.00001076
Iteration 83/1000 | Loss: 0.00001076
Iteration 84/1000 | Loss: 0.00001076
Iteration 85/1000 | Loss: 0.00001076
Iteration 86/1000 | Loss: 0.00001076
Iteration 87/1000 | Loss: 0.00001076
Iteration 88/1000 | Loss: 0.00001076
Iteration 89/1000 | Loss: 0.00001076
Iteration 90/1000 | Loss: 0.00001076
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001075
Iteration 93/1000 | Loss: 0.00001075
Iteration 94/1000 | Loss: 0.00001075
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001072
Iteration 108/1000 | Loss: 0.00001072
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001071
Iteration 112/1000 | Loss: 0.00001071
Iteration 113/1000 | Loss: 0.00001071
Iteration 114/1000 | Loss: 0.00001071
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001071
Iteration 117/1000 | Loss: 0.00001070
Iteration 118/1000 | Loss: 0.00001070
Iteration 119/1000 | Loss: 0.00001070
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001070
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001069
Iteration 151/1000 | Loss: 0.00001069
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0691000170481857e-05, 1.0691000170481857e-05, 1.0691000170481857e-05, 1.0691000170481857e-05, 1.0691000170481857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0691000170481857e-05

Optimization complete. Final v2v error: 2.816715955734253 mm

Highest mean error: 3.2201144695281982 mm for frame 98

Lowest mean error: 2.594041109085083 mm for frame 68

Saving results

Total time: 33.041301250457764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041744
Iteration 2/25 | Loss: 0.01041744
Iteration 3/25 | Loss: 0.00413172
Iteration 4/25 | Loss: 0.00201179
Iteration 5/25 | Loss: 0.00176991
Iteration 6/25 | Loss: 0.00160874
Iteration 7/25 | Loss: 0.00161915
Iteration 8/25 | Loss: 0.00161191
Iteration 9/25 | Loss: 0.00147072
Iteration 10/25 | Loss: 0.00136576
Iteration 11/25 | Loss: 0.00130319
Iteration 12/25 | Loss: 0.00127190
Iteration 13/25 | Loss: 0.00126148
Iteration 14/25 | Loss: 0.00123948
Iteration 15/25 | Loss: 0.00123569
Iteration 16/25 | Loss: 0.00121984
Iteration 17/25 | Loss: 0.00120144
Iteration 18/25 | Loss: 0.00119143
Iteration 19/25 | Loss: 0.00117987
Iteration 20/25 | Loss: 0.00118361
Iteration 21/25 | Loss: 0.00118002
Iteration 22/25 | Loss: 0.00117779
Iteration 23/25 | Loss: 0.00117383
Iteration 24/25 | Loss: 0.00117010
Iteration 25/25 | Loss: 0.00117095

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34516537
Iteration 2/25 | Loss: 0.00263777
Iteration 3/25 | Loss: 0.00251582
Iteration 4/25 | Loss: 0.00251582
Iteration 5/25 | Loss: 0.00251582
Iteration 6/25 | Loss: 0.00251582
Iteration 7/25 | Loss: 0.00251582
Iteration 8/25 | Loss: 0.00251582
Iteration 9/25 | Loss: 0.00251582
Iteration 10/25 | Loss: 0.00251582
Iteration 11/25 | Loss: 0.00251582
Iteration 12/25 | Loss: 0.00251582
Iteration 13/25 | Loss: 0.00251582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0025158158969134092, 0.0025158158969134092, 0.0025158158969134092, 0.0025158158969134092, 0.0025158158969134092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025158158969134092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251582
Iteration 2/1000 | Loss: 0.00058716
Iteration 3/1000 | Loss: 0.00049847
Iteration 4/1000 | Loss: 0.00096112
Iteration 5/1000 | Loss: 0.00067513
Iteration 6/1000 | Loss: 0.00032177
Iteration 7/1000 | Loss: 0.00138236
Iteration 8/1000 | Loss: 0.00190600
Iteration 9/1000 | Loss: 0.00075047
Iteration 10/1000 | Loss: 0.00130000
Iteration 11/1000 | Loss: 0.00161737
Iteration 12/1000 | Loss: 0.00078247
Iteration 13/1000 | Loss: 0.00074715
Iteration 14/1000 | Loss: 0.00076278
Iteration 15/1000 | Loss: 0.00107408
Iteration 16/1000 | Loss: 0.00022603
Iteration 17/1000 | Loss: 0.00024995
Iteration 18/1000 | Loss: 0.00154683
Iteration 19/1000 | Loss: 0.00069143
Iteration 20/1000 | Loss: 0.00040287
Iteration 21/1000 | Loss: 0.00032906
Iteration 22/1000 | Loss: 0.00066687
Iteration 23/1000 | Loss: 0.00053976
Iteration 24/1000 | Loss: 0.00140739
Iteration 25/1000 | Loss: 0.00053742
Iteration 26/1000 | Loss: 0.00098885
Iteration 27/1000 | Loss: 0.00124856
Iteration 28/1000 | Loss: 0.00088763
Iteration 29/1000 | Loss: 0.00084708
Iteration 30/1000 | Loss: 0.00071894
Iteration 31/1000 | Loss: 0.00040214
Iteration 32/1000 | Loss: 0.00022545
Iteration 33/1000 | Loss: 0.00023738
Iteration 34/1000 | Loss: 0.00022425
Iteration 35/1000 | Loss: 0.00041434
Iteration 36/1000 | Loss: 0.00069946
Iteration 37/1000 | Loss: 0.00048205
Iteration 38/1000 | Loss: 0.00196263
Iteration 39/1000 | Loss: 0.00079622
Iteration 40/1000 | Loss: 0.00260122
Iteration 41/1000 | Loss: 0.00171973
Iteration 42/1000 | Loss: 0.00041779
Iteration 43/1000 | Loss: 0.00101504
Iteration 44/1000 | Loss: 0.00028977
Iteration 45/1000 | Loss: 0.00043020
Iteration 46/1000 | Loss: 0.00013581
Iteration 47/1000 | Loss: 0.00022477
Iteration 48/1000 | Loss: 0.00060601
Iteration 49/1000 | Loss: 0.00150104
Iteration 50/1000 | Loss: 0.00407261
Iteration 51/1000 | Loss: 0.00369261
Iteration 52/1000 | Loss: 0.00229231
Iteration 53/1000 | Loss: 0.00090508
Iteration 54/1000 | Loss: 0.00042618
Iteration 55/1000 | Loss: 0.00135695
Iteration 56/1000 | Loss: 0.00060854
Iteration 57/1000 | Loss: 0.00065992
Iteration 58/1000 | Loss: 0.00049841
Iteration 59/1000 | Loss: 0.00158488
Iteration 60/1000 | Loss: 0.00098785
Iteration 61/1000 | Loss: 0.00027138
Iteration 62/1000 | Loss: 0.00047221
Iteration 63/1000 | Loss: 0.00175062
Iteration 64/1000 | Loss: 0.00115392
Iteration 65/1000 | Loss: 0.00151118
Iteration 66/1000 | Loss: 0.00090673
Iteration 67/1000 | Loss: 0.00072530
Iteration 68/1000 | Loss: 0.00156439
Iteration 69/1000 | Loss: 0.00152175
Iteration 70/1000 | Loss: 0.00147771
Iteration 71/1000 | Loss: 0.00196482
Iteration 72/1000 | Loss: 0.00038915
Iteration 73/1000 | Loss: 0.00025483
Iteration 74/1000 | Loss: 0.00050009
Iteration 75/1000 | Loss: 0.00022693
Iteration 76/1000 | Loss: 0.00041313
Iteration 77/1000 | Loss: 0.00009325
Iteration 78/1000 | Loss: 0.00007765
Iteration 79/1000 | Loss: 0.00017339
Iteration 80/1000 | Loss: 0.00023544
Iteration 81/1000 | Loss: 0.00069781
Iteration 82/1000 | Loss: 0.00046020
Iteration 83/1000 | Loss: 0.00056423
Iteration 84/1000 | Loss: 0.00059214
Iteration 85/1000 | Loss: 0.00029759
Iteration 86/1000 | Loss: 0.00043508
Iteration 87/1000 | Loss: 0.00032573
Iteration 88/1000 | Loss: 0.00089018
Iteration 89/1000 | Loss: 0.00068985
Iteration 90/1000 | Loss: 0.00054282
Iteration 91/1000 | Loss: 0.00006347
Iteration 92/1000 | Loss: 0.00040286
Iteration 93/1000 | Loss: 0.00041463
Iteration 94/1000 | Loss: 0.00021116
Iteration 95/1000 | Loss: 0.00006551
Iteration 96/1000 | Loss: 0.00005269
Iteration 97/1000 | Loss: 0.00005770
Iteration 98/1000 | Loss: 0.00006063
Iteration 99/1000 | Loss: 0.00004661
Iteration 100/1000 | Loss: 0.00005124
Iteration 101/1000 | Loss: 0.00005368
Iteration 102/1000 | Loss: 0.00083615
Iteration 103/1000 | Loss: 0.00006960
Iteration 104/1000 | Loss: 0.00005646
Iteration 105/1000 | Loss: 0.00005349
Iteration 106/1000 | Loss: 0.00037011
Iteration 107/1000 | Loss: 0.00095732
Iteration 108/1000 | Loss: 0.00020018
Iteration 109/1000 | Loss: 0.00006746
Iteration 110/1000 | Loss: 0.00007799
Iteration 111/1000 | Loss: 0.00005965
Iteration 112/1000 | Loss: 0.00006990
Iteration 113/1000 | Loss: 0.00005557
Iteration 114/1000 | Loss: 0.00006384
Iteration 115/1000 | Loss: 0.00005010
Iteration 116/1000 | Loss: 0.00003723
Iteration 117/1000 | Loss: 0.00019777
Iteration 118/1000 | Loss: 0.00006019
Iteration 119/1000 | Loss: 0.00003478
Iteration 120/1000 | Loss: 0.00004657
Iteration 121/1000 | Loss: 0.00003741
Iteration 122/1000 | Loss: 0.00006085
Iteration 123/1000 | Loss: 0.00005167
Iteration 124/1000 | Loss: 0.00012765
Iteration 125/1000 | Loss: 0.00020610
Iteration 126/1000 | Loss: 0.00021549
Iteration 127/1000 | Loss: 0.00022115
Iteration 128/1000 | Loss: 0.00027890
Iteration 129/1000 | Loss: 0.00028250
Iteration 130/1000 | Loss: 0.00011534
Iteration 131/1000 | Loss: 0.00022681
Iteration 132/1000 | Loss: 0.00023726
Iteration 133/1000 | Loss: 0.00027382
Iteration 134/1000 | Loss: 0.00019118
Iteration 135/1000 | Loss: 0.00015486
Iteration 136/1000 | Loss: 0.00026907
Iteration 137/1000 | Loss: 0.00031262
Iteration 138/1000 | Loss: 0.00005835
Iteration 139/1000 | Loss: 0.00026700
Iteration 140/1000 | Loss: 0.00003234
Iteration 141/1000 | Loss: 0.00003095
Iteration 142/1000 | Loss: 0.00006252
Iteration 143/1000 | Loss: 0.00052432
Iteration 144/1000 | Loss: 0.00032096
Iteration 145/1000 | Loss: 0.00011009
Iteration 146/1000 | Loss: 0.00019043
Iteration 147/1000 | Loss: 0.00053450
Iteration 148/1000 | Loss: 0.00032568
Iteration 149/1000 | Loss: 0.00010486
Iteration 150/1000 | Loss: 0.00004011
Iteration 151/1000 | Loss: 0.00011259
Iteration 152/1000 | Loss: 0.00005710
Iteration 153/1000 | Loss: 0.00012045
Iteration 154/1000 | Loss: 0.00005757
Iteration 155/1000 | Loss: 0.00012981
Iteration 156/1000 | Loss: 0.00017681
Iteration 157/1000 | Loss: 0.00015927
Iteration 158/1000 | Loss: 0.00026708
Iteration 159/1000 | Loss: 0.00005295
Iteration 160/1000 | Loss: 0.00009449
Iteration 161/1000 | Loss: 0.00005141
Iteration 162/1000 | Loss: 0.00005962
Iteration 163/1000 | Loss: 0.00008760
Iteration 164/1000 | Loss: 0.00005861
Iteration 165/1000 | Loss: 0.00009041
Iteration 166/1000 | Loss: 0.00005712
Iteration 167/1000 | Loss: 0.00101929
Iteration 168/1000 | Loss: 0.00037565
Iteration 169/1000 | Loss: 0.00054689
Iteration 170/1000 | Loss: 0.00012669
Iteration 171/1000 | Loss: 0.00003151
Iteration 172/1000 | Loss: 0.00003025
Iteration 173/1000 | Loss: 0.00011244
Iteration 174/1000 | Loss: 0.00075710
Iteration 175/1000 | Loss: 0.00022294
Iteration 176/1000 | Loss: 0.00011827
Iteration 177/1000 | Loss: 0.00021265
Iteration 178/1000 | Loss: 0.00017991
Iteration 179/1000 | Loss: 0.00012063
Iteration 180/1000 | Loss: 0.00042014
Iteration 181/1000 | Loss: 0.00017736
Iteration 182/1000 | Loss: 0.00068347
Iteration 183/1000 | Loss: 0.00027504
Iteration 184/1000 | Loss: 0.00017618
Iteration 185/1000 | Loss: 0.00026449
Iteration 186/1000 | Loss: 0.00026946
Iteration 187/1000 | Loss: 0.00019104
Iteration 188/1000 | Loss: 0.00021809
Iteration 189/1000 | Loss: 0.00007081
Iteration 190/1000 | Loss: 0.00018949
Iteration 191/1000 | Loss: 0.00027893
Iteration 192/1000 | Loss: 0.00028987
Iteration 193/1000 | Loss: 0.00013509
Iteration 194/1000 | Loss: 0.00017176
Iteration 195/1000 | Loss: 0.00028087
Iteration 196/1000 | Loss: 0.00012556
Iteration 197/1000 | Loss: 0.00007049
Iteration 198/1000 | Loss: 0.00027574
Iteration 199/1000 | Loss: 0.00017005
Iteration 200/1000 | Loss: 0.00021789
Iteration 201/1000 | Loss: 0.00008006
Iteration 202/1000 | Loss: 0.00018277
Iteration 203/1000 | Loss: 0.00010706
Iteration 204/1000 | Loss: 0.00006173
Iteration 205/1000 | Loss: 0.00026462
Iteration 206/1000 | Loss: 0.00027855
Iteration 207/1000 | Loss: 0.00010648
Iteration 208/1000 | Loss: 0.00021757
Iteration 209/1000 | Loss: 0.00028687
Iteration 210/1000 | Loss: 0.00020628
Iteration 211/1000 | Loss: 0.00016770
Iteration 212/1000 | Loss: 0.00039859
Iteration 213/1000 | Loss: 0.00036462
Iteration 214/1000 | Loss: 0.00013727
Iteration 215/1000 | Loss: 0.00014420
Iteration 216/1000 | Loss: 0.00003920
Iteration 217/1000 | Loss: 0.00003235
Iteration 218/1000 | Loss: 0.00003062
Iteration 219/1000 | Loss: 0.00052927
Iteration 220/1000 | Loss: 0.00003578
Iteration 221/1000 | Loss: 0.00002925
Iteration 222/1000 | Loss: 0.00002774
Iteration 223/1000 | Loss: 0.00002646
Iteration 224/1000 | Loss: 0.00002570
Iteration 225/1000 | Loss: 0.00002496
Iteration 226/1000 | Loss: 0.00002472
Iteration 227/1000 | Loss: 0.00002450
Iteration 228/1000 | Loss: 0.00038897
Iteration 229/1000 | Loss: 0.00122469
Iteration 230/1000 | Loss: 0.00004543
Iteration 231/1000 | Loss: 0.00043070
Iteration 232/1000 | Loss: 0.00055527
Iteration 233/1000 | Loss: 0.00046776
Iteration 234/1000 | Loss: 0.00027511
Iteration 235/1000 | Loss: 0.00040328
Iteration 236/1000 | Loss: 0.00023272
Iteration 237/1000 | Loss: 0.00004228
Iteration 238/1000 | Loss: 0.00002993
Iteration 239/1000 | Loss: 0.00002552
Iteration 240/1000 | Loss: 0.00002350
Iteration 241/1000 | Loss: 0.00002214
Iteration 242/1000 | Loss: 0.00002059
Iteration 243/1000 | Loss: 0.00001941
Iteration 244/1000 | Loss: 0.00001861
Iteration 245/1000 | Loss: 0.00001834
Iteration 246/1000 | Loss: 0.00001808
Iteration 247/1000 | Loss: 0.00001785
Iteration 248/1000 | Loss: 0.00001773
Iteration 249/1000 | Loss: 0.00020565
Iteration 250/1000 | Loss: 0.00003079
Iteration 251/1000 | Loss: 0.00002224
Iteration 252/1000 | Loss: 0.00001960
Iteration 253/1000 | Loss: 0.00023636
Iteration 254/1000 | Loss: 0.00002679
Iteration 255/1000 | Loss: 0.00001912
Iteration 256/1000 | Loss: 0.00001748
Iteration 257/1000 | Loss: 0.00001612
Iteration 258/1000 | Loss: 0.00001542
Iteration 259/1000 | Loss: 0.00001486
Iteration 260/1000 | Loss: 0.00001465
Iteration 261/1000 | Loss: 0.00001465
Iteration 262/1000 | Loss: 0.00001449
Iteration 263/1000 | Loss: 0.00001445
Iteration 264/1000 | Loss: 0.00001441
Iteration 265/1000 | Loss: 0.00001434
Iteration 266/1000 | Loss: 0.00001427
Iteration 267/1000 | Loss: 0.00001426
Iteration 268/1000 | Loss: 0.00001425
Iteration 269/1000 | Loss: 0.00001424
Iteration 270/1000 | Loss: 0.00001423
Iteration 271/1000 | Loss: 0.00001423
Iteration 272/1000 | Loss: 0.00001422
Iteration 273/1000 | Loss: 0.00001421
Iteration 274/1000 | Loss: 0.00001421
Iteration 275/1000 | Loss: 0.00001420
Iteration 276/1000 | Loss: 0.00001420
Iteration 277/1000 | Loss: 0.00001419
Iteration 278/1000 | Loss: 0.00001419
Iteration 279/1000 | Loss: 0.00001418
Iteration 280/1000 | Loss: 0.00001418
Iteration 281/1000 | Loss: 0.00001418
Iteration 282/1000 | Loss: 0.00001417
Iteration 283/1000 | Loss: 0.00001417
Iteration 284/1000 | Loss: 0.00001416
Iteration 285/1000 | Loss: 0.00001416
Iteration 286/1000 | Loss: 0.00001416
Iteration 287/1000 | Loss: 0.00001415
Iteration 288/1000 | Loss: 0.00001415
Iteration 289/1000 | Loss: 0.00001415
Iteration 290/1000 | Loss: 0.00001414
Iteration 291/1000 | Loss: 0.00001414
Iteration 292/1000 | Loss: 0.00001414
Iteration 293/1000 | Loss: 0.00001414
Iteration 294/1000 | Loss: 0.00001413
Iteration 295/1000 | Loss: 0.00001413
Iteration 296/1000 | Loss: 0.00001413
Iteration 297/1000 | Loss: 0.00001413
Iteration 298/1000 | Loss: 0.00001413
Iteration 299/1000 | Loss: 0.00001412
Iteration 300/1000 | Loss: 0.00001412
Iteration 301/1000 | Loss: 0.00001412
Iteration 302/1000 | Loss: 0.00001412
Iteration 303/1000 | Loss: 0.00001412
Iteration 304/1000 | Loss: 0.00001412
Iteration 305/1000 | Loss: 0.00001412
Iteration 306/1000 | Loss: 0.00001412
Iteration 307/1000 | Loss: 0.00001411
Iteration 308/1000 | Loss: 0.00001411
Iteration 309/1000 | Loss: 0.00001411
Iteration 310/1000 | Loss: 0.00001410
Iteration 311/1000 | Loss: 0.00001410
Iteration 312/1000 | Loss: 0.00001410
Iteration 313/1000 | Loss: 0.00001410
Iteration 314/1000 | Loss: 0.00001410
Iteration 315/1000 | Loss: 0.00001409
Iteration 316/1000 | Loss: 0.00001409
Iteration 317/1000 | Loss: 0.00001409
Iteration 318/1000 | Loss: 0.00001409
Iteration 319/1000 | Loss: 0.00001409
Iteration 320/1000 | Loss: 0.00001409
Iteration 321/1000 | Loss: 0.00001409
Iteration 322/1000 | Loss: 0.00001408
Iteration 323/1000 | Loss: 0.00001408
Iteration 324/1000 | Loss: 0.00001408
Iteration 325/1000 | Loss: 0.00001408
Iteration 326/1000 | Loss: 0.00001408
Iteration 327/1000 | Loss: 0.00001408
Iteration 328/1000 | Loss: 0.00001408
Iteration 329/1000 | Loss: 0.00001408
Iteration 330/1000 | Loss: 0.00001408
Iteration 331/1000 | Loss: 0.00001408
Iteration 332/1000 | Loss: 0.00001408
Iteration 333/1000 | Loss: 0.00001408
Iteration 334/1000 | Loss: 0.00001408
Iteration 335/1000 | Loss: 0.00001408
Iteration 336/1000 | Loss: 0.00001408
Iteration 337/1000 | Loss: 0.00001408
Iteration 338/1000 | Loss: 0.00001408
Iteration 339/1000 | Loss: 0.00001407
Iteration 340/1000 | Loss: 0.00001407
Iteration 341/1000 | Loss: 0.00001407
Iteration 342/1000 | Loss: 0.00001407
Iteration 343/1000 | Loss: 0.00001407
Iteration 344/1000 | Loss: 0.00001407
Iteration 345/1000 | Loss: 0.00001407
Iteration 346/1000 | Loss: 0.00001406
Iteration 347/1000 | Loss: 0.00001406
Iteration 348/1000 | Loss: 0.00001406
Iteration 349/1000 | Loss: 0.00001406
Iteration 350/1000 | Loss: 0.00001406
Iteration 351/1000 | Loss: 0.00001406
Iteration 352/1000 | Loss: 0.00001406
Iteration 353/1000 | Loss: 0.00001405
Iteration 354/1000 | Loss: 0.00001405
Iteration 355/1000 | Loss: 0.00001405
Iteration 356/1000 | Loss: 0.00001405
Iteration 357/1000 | Loss: 0.00001405
Iteration 358/1000 | Loss: 0.00001404
Iteration 359/1000 | Loss: 0.00001404
Iteration 360/1000 | Loss: 0.00001404
Iteration 361/1000 | Loss: 0.00001404
Iteration 362/1000 | Loss: 0.00001404
Iteration 363/1000 | Loss: 0.00001404
Iteration 364/1000 | Loss: 0.00001404
Iteration 365/1000 | Loss: 0.00001403
Iteration 366/1000 | Loss: 0.00001403
Iteration 367/1000 | Loss: 0.00001403
Iteration 368/1000 | Loss: 0.00001403
Iteration 369/1000 | Loss: 0.00001403
Iteration 370/1000 | Loss: 0.00001403
Iteration 371/1000 | Loss: 0.00001403
Iteration 372/1000 | Loss: 0.00001403
Iteration 373/1000 | Loss: 0.00001403
Iteration 374/1000 | Loss: 0.00001403
Iteration 375/1000 | Loss: 0.00001402
Iteration 376/1000 | Loss: 0.00001402
Iteration 377/1000 | Loss: 0.00001402
Iteration 378/1000 | Loss: 0.00001402
Iteration 379/1000 | Loss: 0.00001402
Iteration 380/1000 | Loss: 0.00001402
Iteration 381/1000 | Loss: 0.00001402
Iteration 382/1000 | Loss: 0.00001402
Iteration 383/1000 | Loss: 0.00001402
Iteration 384/1000 | Loss: 0.00001402
Iteration 385/1000 | Loss: 0.00001401
Iteration 386/1000 | Loss: 0.00001401
Iteration 387/1000 | Loss: 0.00001401
Iteration 388/1000 | Loss: 0.00001401
Iteration 389/1000 | Loss: 0.00001401
Iteration 390/1000 | Loss: 0.00001401
Iteration 391/1000 | Loss: 0.00001401
Iteration 392/1000 | Loss: 0.00001401
Iteration 393/1000 | Loss: 0.00001401
Iteration 394/1000 | Loss: 0.00001401
Iteration 395/1000 | Loss: 0.00001401
Iteration 396/1000 | Loss: 0.00001401
Iteration 397/1000 | Loss: 0.00001401
Iteration 398/1000 | Loss: 0.00001401
Iteration 399/1000 | Loss: 0.00001401
Iteration 400/1000 | Loss: 0.00001401
Iteration 401/1000 | Loss: 0.00001401
Iteration 402/1000 | Loss: 0.00001401
Iteration 403/1000 | Loss: 0.00001400
Iteration 404/1000 | Loss: 0.00001400
Iteration 405/1000 | Loss: 0.00001400
Iteration 406/1000 | Loss: 0.00001400
Iteration 407/1000 | Loss: 0.00001400
Iteration 408/1000 | Loss: 0.00001400
Iteration 409/1000 | Loss: 0.00001400
Iteration 410/1000 | Loss: 0.00001400
Iteration 411/1000 | Loss: 0.00001400
Iteration 412/1000 | Loss: 0.00001400
Iteration 413/1000 | Loss: 0.00001400
Iteration 414/1000 | Loss: 0.00001400
Iteration 415/1000 | Loss: 0.00001400
Iteration 416/1000 | Loss: 0.00001400
Iteration 417/1000 | Loss: 0.00001400
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 417. Stopping optimization.
Last 5 losses: [1.4003378055349458e-05, 1.4003378055349458e-05, 1.4003378055349458e-05, 1.4003378055349458e-05, 1.4003378055349458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4003378055349458e-05

Optimization complete. Final v2v error: 2.553614377975464 mm

Highest mean error: 11.964110374450684 mm for frame 15

Lowest mean error: 1.8770339488983154 mm for frame 171

Saving results

Total time: 477.94893622398376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01137049
Iteration 2/25 | Loss: 0.00386495
Iteration 3/25 | Loss: 0.00234822
Iteration 4/25 | Loss: 0.00190104
Iteration 5/25 | Loss: 0.00228508
Iteration 6/25 | Loss: 0.00250060
Iteration 7/25 | Loss: 0.00232196
Iteration 8/25 | Loss: 0.00202301
Iteration 9/25 | Loss: 0.00168942
Iteration 10/25 | Loss: 0.00159506
Iteration 11/25 | Loss: 0.00156925
Iteration 12/25 | Loss: 0.00154919
Iteration 13/25 | Loss: 0.00151943
Iteration 14/25 | Loss: 0.00148012
Iteration 15/25 | Loss: 0.00144105
Iteration 16/25 | Loss: 0.00143921
Iteration 17/25 | Loss: 0.00144294
Iteration 18/25 | Loss: 0.00143757
Iteration 19/25 | Loss: 0.00143643
Iteration 20/25 | Loss: 0.00142933
Iteration 21/25 | Loss: 0.00141779
Iteration 22/25 | Loss: 0.00140798
Iteration 23/25 | Loss: 0.00141167
Iteration 24/25 | Loss: 0.00140741
Iteration 25/25 | Loss: 0.00140990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62625796
Iteration 2/25 | Loss: 0.00165649
Iteration 3/25 | Loss: 0.00146694
Iteration 4/25 | Loss: 0.00146694
Iteration 5/25 | Loss: 0.00146694
Iteration 6/25 | Loss: 0.00146694
Iteration 7/25 | Loss: 0.00146694
Iteration 8/25 | Loss: 0.00146694
Iteration 9/25 | Loss: 0.00146694
Iteration 10/25 | Loss: 0.00146694
Iteration 11/25 | Loss: 0.00146694
Iteration 12/25 | Loss: 0.00146694
Iteration 13/25 | Loss: 0.00146694
Iteration 14/25 | Loss: 0.00146694
Iteration 15/25 | Loss: 0.00146694
Iteration 16/25 | Loss: 0.00146694
Iteration 17/25 | Loss: 0.00146694
Iteration 18/25 | Loss: 0.00146694
Iteration 19/25 | Loss: 0.00146694
Iteration 20/25 | Loss: 0.00146694
Iteration 21/25 | Loss: 0.00146694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014669375959783792, 0.0014669375959783792, 0.0014669375959783792, 0.0014669375959783792, 0.0014669375959783792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014669375959783792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146694
Iteration 2/1000 | Loss: 0.00049982
Iteration 3/1000 | Loss: 0.00070684
Iteration 4/1000 | Loss: 0.00071400
Iteration 5/1000 | Loss: 0.00089257
Iteration 6/1000 | Loss: 0.00059227
Iteration 7/1000 | Loss: 0.00113627
Iteration 8/1000 | Loss: 0.00089119
Iteration 9/1000 | Loss: 0.00096413
Iteration 10/1000 | Loss: 0.00110475
Iteration 11/1000 | Loss: 0.00074964
Iteration 12/1000 | Loss: 0.00088949
Iteration 13/1000 | Loss: 0.00085875
Iteration 14/1000 | Loss: 0.00108793
Iteration 15/1000 | Loss: 0.00130668
Iteration 16/1000 | Loss: 0.00096079
Iteration 17/1000 | Loss: 0.00079562
Iteration 18/1000 | Loss: 0.00087165
Iteration 19/1000 | Loss: 0.00081986
Iteration 20/1000 | Loss: 0.00088195
Iteration 21/1000 | Loss: 0.00088949
Iteration 22/1000 | Loss: 0.00066350
Iteration 23/1000 | Loss: 0.00064381
Iteration 24/1000 | Loss: 0.00064057
Iteration 25/1000 | Loss: 0.00093949
Iteration 26/1000 | Loss: 0.00091107
Iteration 27/1000 | Loss: 0.00092337
Iteration 28/1000 | Loss: 0.00077632
Iteration 29/1000 | Loss: 0.00088109
Iteration 30/1000 | Loss: 0.00103875
Iteration 31/1000 | Loss: 0.00061309
Iteration 32/1000 | Loss: 0.00059504
Iteration 33/1000 | Loss: 0.00072538
Iteration 34/1000 | Loss: 0.00080376
Iteration 35/1000 | Loss: 0.00103726
Iteration 36/1000 | Loss: 0.00085433
Iteration 37/1000 | Loss: 0.00115299
Iteration 38/1000 | Loss: 0.00051630
Iteration 39/1000 | Loss: 0.00073823
Iteration 40/1000 | Loss: 0.00063912
Iteration 41/1000 | Loss: 0.00067647
Iteration 42/1000 | Loss: 0.00115066
Iteration 43/1000 | Loss: 0.00066372
Iteration 44/1000 | Loss: 0.00053271
Iteration 45/1000 | Loss: 0.00058262
Iteration 46/1000 | Loss: 0.00070868
Iteration 47/1000 | Loss: 0.00068888
Iteration 48/1000 | Loss: 0.00060442
Iteration 49/1000 | Loss: 0.00038316
Iteration 50/1000 | Loss: 0.00074099
Iteration 51/1000 | Loss: 0.00058167
Iteration 52/1000 | Loss: 0.00052366
Iteration 53/1000 | Loss: 0.00061875
Iteration 54/1000 | Loss: 0.00114654
Iteration 55/1000 | Loss: 0.00146781
Iteration 56/1000 | Loss: 0.00102867
Iteration 57/1000 | Loss: 0.00071947
Iteration 58/1000 | Loss: 0.00093816
Iteration 59/1000 | Loss: 0.00057303
Iteration 60/1000 | Loss: 0.00091142
Iteration 61/1000 | Loss: 0.00052753
Iteration 62/1000 | Loss: 0.00140751
Iteration 63/1000 | Loss: 0.00055291
Iteration 64/1000 | Loss: 0.00077181
Iteration 65/1000 | Loss: 0.00051696
Iteration 66/1000 | Loss: 0.00053611
Iteration 67/1000 | Loss: 0.00051710
Iteration 68/1000 | Loss: 0.00062131
Iteration 69/1000 | Loss: 0.00056415
Iteration 70/1000 | Loss: 0.00082982
Iteration 71/1000 | Loss: 0.00060353
Iteration 72/1000 | Loss: 0.00062243
Iteration 73/1000 | Loss: 0.00060969
Iteration 74/1000 | Loss: 0.00059806
Iteration 75/1000 | Loss: 0.00060785
Iteration 76/1000 | Loss: 0.00040043
Iteration 77/1000 | Loss: 0.00061376
Iteration 78/1000 | Loss: 0.00085901
Iteration 79/1000 | Loss: 0.00059411
Iteration 80/1000 | Loss: 0.00076134
Iteration 81/1000 | Loss: 0.00063113
Iteration 82/1000 | Loss: 0.00099219
Iteration 83/1000 | Loss: 0.00068020
Iteration 84/1000 | Loss: 0.00085959
Iteration 85/1000 | Loss: 0.00095707
Iteration 86/1000 | Loss: 0.00063846
Iteration 87/1000 | Loss: 0.00055402
Iteration 88/1000 | Loss: 0.00061287
Iteration 89/1000 | Loss: 0.00051994
Iteration 90/1000 | Loss: 0.00051887
Iteration 91/1000 | Loss: 0.00053834
Iteration 92/1000 | Loss: 0.00072782
Iteration 93/1000 | Loss: 0.00075362
Iteration 94/1000 | Loss: 0.00088920
Iteration 95/1000 | Loss: 0.00120489
Iteration 96/1000 | Loss: 0.00029637
Iteration 97/1000 | Loss: 0.00033186
Iteration 98/1000 | Loss: 0.00036324
Iteration 99/1000 | Loss: 0.00037132
Iteration 100/1000 | Loss: 0.00053374
Iteration 101/1000 | Loss: 0.00038129
Iteration 102/1000 | Loss: 0.00075527
Iteration 103/1000 | Loss: 0.00072467
Iteration 104/1000 | Loss: 0.00066506
Iteration 105/1000 | Loss: 0.00073274
Iteration 106/1000 | Loss: 0.00051001
Iteration 107/1000 | Loss: 0.00045991
Iteration 108/1000 | Loss: 0.00040734
Iteration 109/1000 | Loss: 0.00042766
Iteration 110/1000 | Loss: 0.00038446
Iteration 111/1000 | Loss: 0.00029356
Iteration 112/1000 | Loss: 0.00021664
Iteration 113/1000 | Loss: 0.00033889
Iteration 114/1000 | Loss: 0.00035982
Iteration 115/1000 | Loss: 0.00032418
Iteration 116/1000 | Loss: 0.00029641
Iteration 117/1000 | Loss: 0.00047568
Iteration 118/1000 | Loss: 0.00066591
Iteration 119/1000 | Loss: 0.00054880
Iteration 120/1000 | Loss: 0.00041450
Iteration 121/1000 | Loss: 0.00031218
Iteration 122/1000 | Loss: 0.00032320
Iteration 123/1000 | Loss: 0.00027488
Iteration 124/1000 | Loss: 0.00043312
Iteration 125/1000 | Loss: 0.00029137
Iteration 126/1000 | Loss: 0.00022863
Iteration 127/1000 | Loss: 0.00024539
Iteration 128/1000 | Loss: 0.00023669
Iteration 129/1000 | Loss: 0.00021331
Iteration 130/1000 | Loss: 0.00025659
Iteration 131/1000 | Loss: 0.00019794
Iteration 132/1000 | Loss: 0.00025757
Iteration 133/1000 | Loss: 0.00035126
Iteration 134/1000 | Loss: 0.00020102
Iteration 135/1000 | Loss: 0.00018171
Iteration 136/1000 | Loss: 0.00019422
Iteration 137/1000 | Loss: 0.00022085
Iteration 138/1000 | Loss: 0.00018207
Iteration 139/1000 | Loss: 0.00039078
Iteration 140/1000 | Loss: 0.00019802
Iteration 141/1000 | Loss: 0.00021793
Iteration 142/1000 | Loss: 0.00023259
Iteration 143/1000 | Loss: 0.00023559
Iteration 144/1000 | Loss: 0.00031513
Iteration 145/1000 | Loss: 0.00032070
Iteration 146/1000 | Loss: 0.00029611
Iteration 147/1000 | Loss: 0.00019788
Iteration 148/1000 | Loss: 0.00019516
Iteration 149/1000 | Loss: 0.00019378
Iteration 150/1000 | Loss: 0.00019188
Iteration 151/1000 | Loss: 0.00020443
Iteration 152/1000 | Loss: 0.00017887
Iteration 153/1000 | Loss: 0.00018534
Iteration 154/1000 | Loss: 0.00019186
Iteration 155/1000 | Loss: 0.00017965
Iteration 156/1000 | Loss: 0.00019028
Iteration 157/1000 | Loss: 0.00020593
Iteration 158/1000 | Loss: 0.00039654
Iteration 159/1000 | Loss: 0.00054189
Iteration 160/1000 | Loss: 0.00045566
Iteration 161/1000 | Loss: 0.00022709
Iteration 162/1000 | Loss: 0.00023359
Iteration 163/1000 | Loss: 0.00023397
Iteration 164/1000 | Loss: 0.00025520
Iteration 165/1000 | Loss: 0.00022732
Iteration 166/1000 | Loss: 0.00018537
Iteration 167/1000 | Loss: 0.00019811
Iteration 168/1000 | Loss: 0.00017383
Iteration 169/1000 | Loss: 0.00019888
Iteration 170/1000 | Loss: 0.00019343
Iteration 171/1000 | Loss: 0.00019547
Iteration 172/1000 | Loss: 0.00022147
Iteration 173/1000 | Loss: 0.00016309
Iteration 174/1000 | Loss: 0.00017239
Iteration 175/1000 | Loss: 0.00015057
Iteration 176/1000 | Loss: 0.00017620
Iteration 177/1000 | Loss: 0.00018038
Iteration 178/1000 | Loss: 0.00019751
Iteration 179/1000 | Loss: 0.00017462
Iteration 180/1000 | Loss: 0.00021857
Iteration 181/1000 | Loss: 0.00019982
Iteration 182/1000 | Loss: 0.00019484
Iteration 183/1000 | Loss: 0.00019527
Iteration 184/1000 | Loss: 0.00034516
Iteration 185/1000 | Loss: 0.00040207
Iteration 186/1000 | Loss: 0.00031639
Iteration 187/1000 | Loss: 0.00031419
Iteration 188/1000 | Loss: 0.00044192
Iteration 189/1000 | Loss: 0.00033829
Iteration 190/1000 | Loss: 0.00034780
Iteration 191/1000 | Loss: 0.00038169
Iteration 192/1000 | Loss: 0.00037072
Iteration 193/1000 | Loss: 0.00036552
Iteration 194/1000 | Loss: 0.00035904
Iteration 195/1000 | Loss: 0.00029955
Iteration 196/1000 | Loss: 0.00032686
Iteration 197/1000 | Loss: 0.00020370
Iteration 198/1000 | Loss: 0.00033853
Iteration 199/1000 | Loss: 0.00037812
Iteration 200/1000 | Loss: 0.00036515
Iteration 201/1000 | Loss: 0.00037807
Iteration 202/1000 | Loss: 0.00029410
Iteration 203/1000 | Loss: 0.00036247
Iteration 204/1000 | Loss: 0.00021542
Iteration 205/1000 | Loss: 0.00024788
Iteration 206/1000 | Loss: 0.00034995
Iteration 207/1000 | Loss: 0.00036341
Iteration 208/1000 | Loss: 0.00022084
Iteration 209/1000 | Loss: 0.00023365
Iteration 210/1000 | Loss: 0.00021449
Iteration 211/1000 | Loss: 0.00024668
Iteration 212/1000 | Loss: 0.00023400
Iteration 213/1000 | Loss: 0.00025702
Iteration 214/1000 | Loss: 0.00020060
Iteration 215/1000 | Loss: 0.00019298
Iteration 216/1000 | Loss: 0.00016849
Iteration 217/1000 | Loss: 0.00021898
Iteration 218/1000 | Loss: 0.00021945
Iteration 219/1000 | Loss: 0.00025712
Iteration 220/1000 | Loss: 0.00020217
Iteration 221/1000 | Loss: 0.00019744
Iteration 222/1000 | Loss: 0.00022060
Iteration 223/1000 | Loss: 0.00018307
Iteration 224/1000 | Loss: 0.00017692
Iteration 225/1000 | Loss: 0.00018378
Iteration 226/1000 | Loss: 0.00017550
Iteration 227/1000 | Loss: 0.00018569
Iteration 228/1000 | Loss: 0.00017906
Iteration 229/1000 | Loss: 0.00018199
Iteration 230/1000 | Loss: 0.00014935
Iteration 231/1000 | Loss: 0.00019555
Iteration 232/1000 | Loss: 0.00019642
Iteration 233/1000 | Loss: 0.00019842
Iteration 234/1000 | Loss: 0.00018731
Iteration 235/1000 | Loss: 0.00019333
Iteration 236/1000 | Loss: 0.00023210
Iteration 237/1000 | Loss: 0.00017782
Iteration 238/1000 | Loss: 0.00018935
Iteration 239/1000 | Loss: 0.00019633
Iteration 240/1000 | Loss: 0.00031212
Iteration 241/1000 | Loss: 0.00028216
Iteration 242/1000 | Loss: 0.00030470
Iteration 243/1000 | Loss: 0.00019618
Iteration 244/1000 | Loss: 0.00020694
Iteration 245/1000 | Loss: 0.00020419
Iteration 246/1000 | Loss: 0.00019405
Iteration 247/1000 | Loss: 0.00019804
Iteration 248/1000 | Loss: 0.00020139
Iteration 249/1000 | Loss: 0.00021030
Iteration 250/1000 | Loss: 0.00020337
Iteration 251/1000 | Loss: 0.00022511
Iteration 252/1000 | Loss: 0.00021586
Iteration 253/1000 | Loss: 0.00020047
Iteration 254/1000 | Loss: 0.00022944
Iteration 255/1000 | Loss: 0.00021472
Iteration 256/1000 | Loss: 0.00015340
Iteration 257/1000 | Loss: 0.00019999
Iteration 258/1000 | Loss: 0.00019861
Iteration 259/1000 | Loss: 0.00018001
Iteration 260/1000 | Loss: 0.00017480
Iteration 261/1000 | Loss: 0.00017241
Iteration 262/1000 | Loss: 0.00020122
Iteration 263/1000 | Loss: 0.00020414
Iteration 264/1000 | Loss: 0.00018592
Iteration 265/1000 | Loss: 0.00021218
Iteration 266/1000 | Loss: 0.00016996
Iteration 267/1000 | Loss: 0.00014935
Iteration 268/1000 | Loss: 0.00017765
Iteration 269/1000 | Loss: 0.00020925
Iteration 270/1000 | Loss: 0.00018073
Iteration 271/1000 | Loss: 0.00018953
Iteration 272/1000 | Loss: 0.00018650
Iteration 273/1000 | Loss: 0.00020547
Iteration 274/1000 | Loss: 0.00018068
Iteration 275/1000 | Loss: 0.00016377
Iteration 276/1000 | Loss: 0.00017560
Iteration 277/1000 | Loss: 0.00018891
Iteration 278/1000 | Loss: 0.00016853
Iteration 279/1000 | Loss: 0.00020359
Iteration 280/1000 | Loss: 0.00017234
Iteration 281/1000 | Loss: 0.00017276
Iteration 282/1000 | Loss: 0.00016414
Iteration 283/1000 | Loss: 0.00017852
Iteration 284/1000 | Loss: 0.00017456
Iteration 285/1000 | Loss: 0.00013811
Iteration 286/1000 | Loss: 0.00016129
Iteration 287/1000 | Loss: 0.00015050
Iteration 288/1000 | Loss: 0.00014573
Iteration 289/1000 | Loss: 0.00015079
Iteration 290/1000 | Loss: 0.00016038
Iteration 291/1000 | Loss: 0.00017340
Iteration 292/1000 | Loss: 0.00015826
Iteration 293/1000 | Loss: 0.00014510
Iteration 294/1000 | Loss: 0.00013600
Iteration 295/1000 | Loss: 0.00016248
Iteration 296/1000 | Loss: 0.00016512
Iteration 297/1000 | Loss: 0.00015941
Iteration 298/1000 | Loss: 0.00018238
Iteration 299/1000 | Loss: 0.00015352
Iteration 300/1000 | Loss: 0.00016794
Iteration 301/1000 | Loss: 0.00019840
Iteration 302/1000 | Loss: 0.00017400
Iteration 303/1000 | Loss: 0.00017768
Iteration 304/1000 | Loss: 0.00019536
Iteration 305/1000 | Loss: 0.00019262
Iteration 306/1000 | Loss: 0.00020097
Iteration 307/1000 | Loss: 0.00017131
Iteration 308/1000 | Loss: 0.00019175
Iteration 309/1000 | Loss: 0.00029069
Iteration 310/1000 | Loss: 0.00033185
Iteration 311/1000 | Loss: 0.00019044
Iteration 312/1000 | Loss: 0.00022339
Iteration 313/1000 | Loss: 0.00019207
Iteration 314/1000 | Loss: 0.00015784
Iteration 315/1000 | Loss: 0.00016063
Iteration 316/1000 | Loss: 0.00018431
Iteration 317/1000 | Loss: 0.00017733
Iteration 318/1000 | Loss: 0.00019901
Iteration 319/1000 | Loss: 0.00017192
Iteration 320/1000 | Loss: 0.00017870
Iteration 321/1000 | Loss: 0.00015014
Iteration 322/1000 | Loss: 0.00016691
Iteration 323/1000 | Loss: 0.00021792
Iteration 324/1000 | Loss: 0.00017303
Iteration 325/1000 | Loss: 0.00011264
Iteration 326/1000 | Loss: 0.00013020
Iteration 327/1000 | Loss: 0.00014947
Iteration 328/1000 | Loss: 0.00017653
Iteration 329/1000 | Loss: 0.00018169
Iteration 330/1000 | Loss: 0.00019961
Iteration 331/1000 | Loss: 0.00018371
Iteration 332/1000 | Loss: 0.00019454
Iteration 333/1000 | Loss: 0.00018426
Iteration 334/1000 | Loss: 0.00015249
Iteration 335/1000 | Loss: 0.00016886
Iteration 336/1000 | Loss: 0.00017145
Iteration 337/1000 | Loss: 0.00016997
Iteration 338/1000 | Loss: 0.00015057
Iteration 339/1000 | Loss: 0.00045797
Iteration 340/1000 | Loss: 0.00030257
Iteration 341/1000 | Loss: 0.00044078
Iteration 342/1000 | Loss: 0.00027022
Iteration 343/1000 | Loss: 0.00018454
Iteration 344/1000 | Loss: 0.00018753
Iteration 345/1000 | Loss: 0.00018688
Iteration 346/1000 | Loss: 0.00018694
Iteration 347/1000 | Loss: 0.00029824
Iteration 348/1000 | Loss: 0.00043671
Iteration 349/1000 | Loss: 0.00019935
Iteration 350/1000 | Loss: 0.00015434
Iteration 351/1000 | Loss: 0.00017154
Iteration 352/1000 | Loss: 0.00024721
Iteration 353/1000 | Loss: 0.00030968
Iteration 354/1000 | Loss: 0.00031489
Iteration 355/1000 | Loss: 0.00012056
Iteration 356/1000 | Loss: 0.00013338
Iteration 357/1000 | Loss: 0.00011989
Iteration 358/1000 | Loss: 0.00009757
Iteration 359/1000 | Loss: 0.00021321
Iteration 360/1000 | Loss: 0.00010674
Iteration 361/1000 | Loss: 0.00011183
Iteration 362/1000 | Loss: 0.00010025
Iteration 363/1000 | Loss: 0.00011790
Iteration 364/1000 | Loss: 0.00011586
Iteration 365/1000 | Loss: 0.00013089
Iteration 366/1000 | Loss: 0.00011258
Iteration 367/1000 | Loss: 0.00010797
Iteration 368/1000 | Loss: 0.00013595
Iteration 369/1000 | Loss: 0.00010808
Iteration 370/1000 | Loss: 0.00012796
Iteration 371/1000 | Loss: 0.00011494
Iteration 372/1000 | Loss: 0.00013046
Iteration 373/1000 | Loss: 0.00012678
Iteration 374/1000 | Loss: 0.00012620
Iteration 375/1000 | Loss: 0.00012387
Iteration 376/1000 | Loss: 0.00013441
Iteration 377/1000 | Loss: 0.00025016
Iteration 378/1000 | Loss: 0.00019556
Iteration 379/1000 | Loss: 0.00019735
Iteration 380/1000 | Loss: 0.00010806
Iteration 381/1000 | Loss: 0.00013003
Iteration 382/1000 | Loss: 0.00012308
Iteration 383/1000 | Loss: 0.00013100
Iteration 384/1000 | Loss: 0.00011181
Iteration 385/1000 | Loss: 0.00013508
Iteration 386/1000 | Loss: 0.00012382
Iteration 387/1000 | Loss: 0.00016222
Iteration 388/1000 | Loss: 0.00013387
Iteration 389/1000 | Loss: 0.00012822
Iteration 390/1000 | Loss: 0.00013198
Iteration 391/1000 | Loss: 0.00012809
Iteration 392/1000 | Loss: 0.00013584
Iteration 393/1000 | Loss: 0.00013186
Iteration 394/1000 | Loss: 0.00012522
Iteration 395/1000 | Loss: 0.00011060
Iteration 396/1000 | Loss: 0.00011635
Iteration 397/1000 | Loss: 0.00011318
Iteration 398/1000 | Loss: 0.00010447
Iteration 399/1000 | Loss: 0.00011673
Iteration 400/1000 | Loss: 0.00010828
Iteration 401/1000 | Loss: 0.00011147
Iteration 402/1000 | Loss: 0.00012940
Iteration 403/1000 | Loss: 0.00014356
Iteration 404/1000 | Loss: 0.00022337
Iteration 405/1000 | Loss: 0.00019640
Iteration 406/1000 | Loss: 0.00023166
Iteration 407/1000 | Loss: 0.00018660
Iteration 408/1000 | Loss: 0.00021370
Iteration 409/1000 | Loss: 0.00012465
Iteration 410/1000 | Loss: 0.00020106
Iteration 411/1000 | Loss: 0.00022190
Iteration 412/1000 | Loss: 0.00014545
Iteration 413/1000 | Loss: 0.00013529
Iteration 414/1000 | Loss: 0.00012099
Iteration 415/1000 | Loss: 0.00014227
Iteration 416/1000 | Loss: 0.00013263
Iteration 417/1000 | Loss: 0.00011088
Iteration 418/1000 | Loss: 0.00009711
Iteration 419/1000 | Loss: 0.00010794
Iteration 420/1000 | Loss: 0.00012099
Iteration 421/1000 | Loss: 0.00011283
Iteration 422/1000 | Loss: 0.00010166
Iteration 423/1000 | Loss: 0.00022702
Iteration 424/1000 | Loss: 0.00016705
Iteration 425/1000 | Loss: 0.00016581
Iteration 426/1000 | Loss: 0.00010622
Iteration 427/1000 | Loss: 0.00011653
Iteration 428/1000 | Loss: 0.00008997
Iteration 429/1000 | Loss: 0.00009147
Iteration 430/1000 | Loss: 0.00010121
Iteration 431/1000 | Loss: 0.00008720
Iteration 432/1000 | Loss: 0.00009038
Iteration 433/1000 | Loss: 0.00009142
Iteration 434/1000 | Loss: 0.00009703
Iteration 435/1000 | Loss: 0.00009803
Iteration 436/1000 | Loss: 0.00009969
Iteration 437/1000 | Loss: 0.00008748
Iteration 438/1000 | Loss: 0.00011267
Iteration 439/1000 | Loss: 0.00010533
Iteration 440/1000 | Loss: 0.00008277
Iteration 441/1000 | Loss: 0.00008147
Iteration 442/1000 | Loss: 0.00008823
Iteration 443/1000 | Loss: 0.00009655
Iteration 444/1000 | Loss: 0.00009468
Iteration 445/1000 | Loss: 0.00009173
Iteration 446/1000 | Loss: 0.00009205
Iteration 447/1000 | Loss: 0.00009713
Iteration 448/1000 | Loss: 0.00009812
Iteration 449/1000 | Loss: 0.00008844
Iteration 450/1000 | Loss: 0.00009828
Iteration 451/1000 | Loss: 0.00009940
Iteration 452/1000 | Loss: 0.00008516
Iteration 453/1000 | Loss: 0.00008832
Iteration 454/1000 | Loss: 0.00008939
Iteration 455/1000 | Loss: 0.00008199
Iteration 456/1000 | Loss: 0.00009316
Iteration 457/1000 | Loss: 0.00009960
Iteration 458/1000 | Loss: 0.00009903
Iteration 459/1000 | Loss: 0.00010754
Iteration 460/1000 | Loss: 0.00010179
Iteration 461/1000 | Loss: 0.00009007
Iteration 462/1000 | Loss: 0.00008741
Iteration 463/1000 | Loss: 0.00008431
Iteration 464/1000 | Loss: 0.00007959
Iteration 465/1000 | Loss: 0.00010079
Iteration 466/1000 | Loss: 0.00008029
Iteration 467/1000 | Loss: 0.00008237
Iteration 468/1000 | Loss: 0.00007828
Iteration 469/1000 | Loss: 0.00007162
Iteration 470/1000 | Loss: 0.00007046
Iteration 471/1000 | Loss: 0.00008675
Iteration 472/1000 | Loss: 0.00008138
Iteration 473/1000 | Loss: 0.00007527
Iteration 474/1000 | Loss: 0.00008005
Iteration 475/1000 | Loss: 0.00008149
Iteration 476/1000 | Loss: 0.00007954
Iteration 477/1000 | Loss: 0.00008126
Iteration 478/1000 | Loss: 0.00007918
Iteration 479/1000 | Loss: 0.00008313
Iteration 480/1000 | Loss: 0.00007879
Iteration 481/1000 | Loss: 0.00009333
Iteration 482/1000 | Loss: 0.00008495
Iteration 483/1000 | Loss: 0.00009707
Iteration 484/1000 | Loss: 0.00010187
Iteration 485/1000 | Loss: 0.00008020
Iteration 486/1000 | Loss: 0.00007444
Iteration 487/1000 | Loss: 0.00007166
Iteration 488/1000 | Loss: 0.00007031
Iteration 489/1000 | Loss: 0.00006944
Iteration 490/1000 | Loss: 0.00006902
Iteration 491/1000 | Loss: 0.00006865
Iteration 492/1000 | Loss: 0.00007007
Iteration 493/1000 | Loss: 0.00006863
Iteration 494/1000 | Loss: 0.00006824
Iteration 495/1000 | Loss: 0.00006809
Iteration 496/1000 | Loss: 0.00008763
Iteration 497/1000 | Loss: 0.00006917
Iteration 498/1000 | Loss: 0.00006835
Iteration 499/1000 | Loss: 0.00006786
Iteration 500/1000 | Loss: 0.00006767
Iteration 501/1000 | Loss: 0.00006753
Iteration 502/1000 | Loss: 0.00006748
Iteration 503/1000 | Loss: 0.00006747
Iteration 504/1000 | Loss: 0.00006747
Iteration 505/1000 | Loss: 0.00006747
Iteration 506/1000 | Loss: 0.00006746
Iteration 507/1000 | Loss: 0.00006746
Iteration 508/1000 | Loss: 0.00006746
Iteration 509/1000 | Loss: 0.00006746
Iteration 510/1000 | Loss: 0.00006745
Iteration 511/1000 | Loss: 0.00006742
Iteration 512/1000 | Loss: 0.00006739
Iteration 513/1000 | Loss: 0.00006738
Iteration 514/1000 | Loss: 0.00006738
Iteration 515/1000 | Loss: 0.00006737
Iteration 516/1000 | Loss: 0.00006737
Iteration 517/1000 | Loss: 0.00006737
Iteration 518/1000 | Loss: 0.00006736
Iteration 519/1000 | Loss: 0.00006736
Iteration 520/1000 | Loss: 0.00006736
Iteration 521/1000 | Loss: 0.00006736
Iteration 522/1000 | Loss: 0.00006735
Iteration 523/1000 | Loss: 0.00006735
Iteration 524/1000 | Loss: 0.00006735
Iteration 525/1000 | Loss: 0.00006735
Iteration 526/1000 | Loss: 0.00006735
Iteration 527/1000 | Loss: 0.00006735
Iteration 528/1000 | Loss: 0.00006734
Iteration 529/1000 | Loss: 0.00006734
Iteration 530/1000 | Loss: 0.00006734
Iteration 531/1000 | Loss: 0.00006733
Iteration 532/1000 | Loss: 0.00006733
Iteration 533/1000 | Loss: 0.00006733
Iteration 534/1000 | Loss: 0.00006732
Iteration 535/1000 | Loss: 0.00006732
Iteration 536/1000 | Loss: 0.00006732
Iteration 537/1000 | Loss: 0.00006732
Iteration 538/1000 | Loss: 0.00006732
Iteration 539/1000 | Loss: 0.00006732
Iteration 540/1000 | Loss: 0.00006732
Iteration 541/1000 | Loss: 0.00006732
Iteration 542/1000 | Loss: 0.00006732
Iteration 543/1000 | Loss: 0.00006732
Iteration 544/1000 | Loss: 0.00006732
Iteration 545/1000 | Loss: 0.00006732
Iteration 546/1000 | Loss: 0.00006730
Iteration 547/1000 | Loss: 0.00006730
Iteration 548/1000 | Loss: 0.00006730
Iteration 549/1000 | Loss: 0.00006730
Iteration 550/1000 | Loss: 0.00006730
Iteration 551/1000 | Loss: 0.00006730
Iteration 552/1000 | Loss: 0.00006730
Iteration 553/1000 | Loss: 0.00006730
Iteration 554/1000 | Loss: 0.00006730
Iteration 555/1000 | Loss: 0.00006730
Iteration 556/1000 | Loss: 0.00006729
Iteration 557/1000 | Loss: 0.00006729
Iteration 558/1000 | Loss: 0.00006729
Iteration 559/1000 | Loss: 0.00006729
Iteration 560/1000 | Loss: 0.00006729
Iteration 561/1000 | Loss: 0.00006729
Iteration 562/1000 | Loss: 0.00006729
Iteration 563/1000 | Loss: 0.00006729
Iteration 564/1000 | Loss: 0.00006729
Iteration 565/1000 | Loss: 0.00006729
Iteration 566/1000 | Loss: 0.00006729
Iteration 567/1000 | Loss: 0.00006729
Iteration 568/1000 | Loss: 0.00006728
Iteration 569/1000 | Loss: 0.00006728
Iteration 570/1000 | Loss: 0.00006728
Iteration 571/1000 | Loss: 0.00006728
Iteration 572/1000 | Loss: 0.00006728
Iteration 573/1000 | Loss: 0.00006728
Iteration 574/1000 | Loss: 0.00006727
Iteration 575/1000 | Loss: 0.00006727
Iteration 576/1000 | Loss: 0.00006727
Iteration 577/1000 | Loss: 0.00006727
Iteration 578/1000 | Loss: 0.00006726
Iteration 579/1000 | Loss: 0.00006726
Iteration 580/1000 | Loss: 0.00006726
Iteration 581/1000 | Loss: 0.00006726
Iteration 582/1000 | Loss: 0.00006725
Iteration 583/1000 | Loss: 0.00006724
Iteration 584/1000 | Loss: 0.00006723
Iteration 585/1000 | Loss: 0.00006721
Iteration 586/1000 | Loss: 0.00006721
Iteration 587/1000 | Loss: 0.00006715
Iteration 588/1000 | Loss: 0.00006714
Iteration 589/1000 | Loss: 0.00019781
Iteration 590/1000 | Loss: 0.00018733
Iteration 591/1000 | Loss: 0.00025428
Iteration 592/1000 | Loss: 0.00017204
Iteration 593/1000 | Loss: 0.00025835
Iteration 594/1000 | Loss: 0.00014655
Iteration 595/1000 | Loss: 0.00009070
Iteration 596/1000 | Loss: 0.00033630
Iteration 597/1000 | Loss: 0.00031750
Iteration 598/1000 | Loss: 0.00009299
Iteration 599/1000 | Loss: 0.00007728
Iteration 600/1000 | Loss: 0.00008083
Iteration 601/1000 | Loss: 0.00007309
Iteration 602/1000 | Loss: 0.00007928
Iteration 603/1000 | Loss: 0.00014411
Iteration 604/1000 | Loss: 0.00021844
Iteration 605/1000 | Loss: 0.00020884
Iteration 606/1000 | Loss: 0.00014262
Iteration 607/1000 | Loss: 0.00018755
Iteration 608/1000 | Loss: 0.00017498
Iteration 609/1000 | Loss: 0.00011072
Iteration 610/1000 | Loss: 0.00015249
Iteration 611/1000 | Loss: 0.00014742
Iteration 612/1000 | Loss: 0.00012825
Iteration 613/1000 | Loss: 0.00015431
Iteration 614/1000 | Loss: 0.00016608
Iteration 615/1000 | Loss: 0.00014596
Iteration 616/1000 | Loss: 0.00011905
Iteration 617/1000 | Loss: 0.00011935
Iteration 618/1000 | Loss: 0.00010913
Iteration 619/1000 | Loss: 0.00018100
Iteration 620/1000 | Loss: 0.00007112
Iteration 621/1000 | Loss: 0.00006872
Iteration 622/1000 | Loss: 0.00006804
Iteration 623/1000 | Loss: 0.00006737
Iteration 624/1000 | Loss: 0.00006705
Iteration 625/1000 | Loss: 0.00006684
Iteration 626/1000 | Loss: 0.00006671
Iteration 627/1000 | Loss: 0.00006664
Iteration 628/1000 | Loss: 0.00006654
Iteration 629/1000 | Loss: 0.00006654
Iteration 630/1000 | Loss: 0.00006654
Iteration 631/1000 | Loss: 0.00006653
Iteration 632/1000 | Loss: 0.00006653
Iteration 633/1000 | Loss: 0.00006653
Iteration 634/1000 | Loss: 0.00006653
Iteration 635/1000 | Loss: 0.00006653
Iteration 636/1000 | Loss: 0.00006653
Iteration 637/1000 | Loss: 0.00006652
Iteration 638/1000 | Loss: 0.00006652
Iteration 639/1000 | Loss: 0.00006652
Iteration 640/1000 | Loss: 0.00006651
Iteration 641/1000 | Loss: 0.00006651
Iteration 642/1000 | Loss: 0.00006651
Iteration 643/1000 | Loss: 0.00006650
Iteration 644/1000 | Loss: 0.00006650
Iteration 645/1000 | Loss: 0.00006650
Iteration 646/1000 | Loss: 0.00006650
Iteration 647/1000 | Loss: 0.00006650
Iteration 648/1000 | Loss: 0.00006650
Iteration 649/1000 | Loss: 0.00006650
Iteration 650/1000 | Loss: 0.00006650
Iteration 651/1000 | Loss: 0.00006650
Iteration 652/1000 | Loss: 0.00006649
Iteration 653/1000 | Loss: 0.00006649
Iteration 654/1000 | Loss: 0.00006649
Iteration 655/1000 | Loss: 0.00006649
Iteration 656/1000 | Loss: 0.00006646
Iteration 657/1000 | Loss: 0.00006646
Iteration 658/1000 | Loss: 0.00006646
Iteration 659/1000 | Loss: 0.00006645
Iteration 660/1000 | Loss: 0.00006645
Iteration 661/1000 | Loss: 0.00006645
Iteration 662/1000 | Loss: 0.00006645
Iteration 663/1000 | Loss: 0.00006645
Iteration 664/1000 | Loss: 0.00006645
Iteration 665/1000 | Loss: 0.00006645
Iteration 666/1000 | Loss: 0.00006644
Iteration 667/1000 | Loss: 0.00006644
Iteration 668/1000 | Loss: 0.00006644
Iteration 669/1000 | Loss: 0.00006644
Iteration 670/1000 | Loss: 0.00006644
Iteration 671/1000 | Loss: 0.00006644
Iteration 672/1000 | Loss: 0.00006644
Iteration 673/1000 | Loss: 0.00006643
Iteration 674/1000 | Loss: 0.00006643
Iteration 675/1000 | Loss: 0.00006643
Iteration 676/1000 | Loss: 0.00006643
Iteration 677/1000 | Loss: 0.00006643
Iteration 678/1000 | Loss: 0.00006643
Iteration 679/1000 | Loss: 0.00006642
Iteration 680/1000 | Loss: 0.00006642
Iteration 681/1000 | Loss: 0.00006642
Iteration 682/1000 | Loss: 0.00006642
Iteration 683/1000 | Loss: 0.00006642
Iteration 684/1000 | Loss: 0.00006641
Iteration 685/1000 | Loss: 0.00006641
Iteration 686/1000 | Loss: 0.00006641
Iteration 687/1000 | Loss: 0.00006641
Iteration 688/1000 | Loss: 0.00006641
Iteration 689/1000 | Loss: 0.00006641
Iteration 690/1000 | Loss: 0.00006641
Iteration 691/1000 | Loss: 0.00006641
Iteration 692/1000 | Loss: 0.00006641
Iteration 693/1000 | Loss: 0.00006640
Iteration 694/1000 | Loss: 0.00006640
Iteration 695/1000 | Loss: 0.00006640
Iteration 696/1000 | Loss: 0.00006640
Iteration 697/1000 | Loss: 0.00006640
Iteration 698/1000 | Loss: 0.00006640
Iteration 699/1000 | Loss: 0.00006640
Iteration 700/1000 | Loss: 0.00006639
Iteration 701/1000 | Loss: 0.00006639
Iteration 702/1000 | Loss: 0.00006639
Iteration 703/1000 | Loss: 0.00006639
Iteration 704/1000 | Loss: 0.00006639
Iteration 705/1000 | Loss: 0.00006639
Iteration 706/1000 | Loss: 0.00006639
Iteration 707/1000 | Loss: 0.00006639
Iteration 708/1000 | Loss: 0.00006639
Iteration 709/1000 | Loss: 0.00006639
Iteration 710/1000 | Loss: 0.00006638
Iteration 711/1000 | Loss: 0.00006638
Iteration 712/1000 | Loss: 0.00006638
Iteration 713/1000 | Loss: 0.00006638
Iteration 714/1000 | Loss: 0.00006638
Iteration 715/1000 | Loss: 0.00006638
Iteration 716/1000 | Loss: 0.00006638
Iteration 717/1000 | Loss: 0.00006638
Iteration 718/1000 | Loss: 0.00006638
Iteration 719/1000 | Loss: 0.00006638
Iteration 720/1000 | Loss: 0.00006638
Iteration 721/1000 | Loss: 0.00006638
Iteration 722/1000 | Loss: 0.00006638
Iteration 723/1000 | Loss: 0.00006638
Iteration 724/1000 | Loss: 0.00006638
Iteration 725/1000 | Loss: 0.00006638
Iteration 726/1000 | Loss: 0.00006638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 726. Stopping optimization.
Last 5 losses: [6.637854676228017e-05, 6.637854676228017e-05, 6.637854676228017e-05, 6.637854676228017e-05, 6.637854676228017e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.637854676228017e-05

Optimization complete. Final v2v error: 6.060959815979004 mm

Highest mean error: 8.41018009185791 mm for frame 80

Lowest mean error: 3.9429049491882324 mm for frame 59

Saving results

Total time: 932.893577337265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380403
Iteration 2/25 | Loss: 0.00110292
Iteration 3/25 | Loss: 0.00097847
Iteration 4/25 | Loss: 0.00096180
Iteration 5/25 | Loss: 0.00095644
Iteration 6/25 | Loss: 0.00095480
Iteration 7/25 | Loss: 0.00095453
Iteration 8/25 | Loss: 0.00095453
Iteration 9/25 | Loss: 0.00095453
Iteration 10/25 | Loss: 0.00095453
Iteration 11/25 | Loss: 0.00095453
Iteration 12/25 | Loss: 0.00095453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009545265347696841, 0.0009545265347696841, 0.0009545265347696841, 0.0009545265347696841, 0.0009545265347696841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009545265347696841

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40997040
Iteration 2/25 | Loss: 0.00080975
Iteration 3/25 | Loss: 0.00080975
Iteration 4/25 | Loss: 0.00080974
Iteration 5/25 | Loss: 0.00080974
Iteration 6/25 | Loss: 0.00080974
Iteration 7/25 | Loss: 0.00080974
Iteration 8/25 | Loss: 0.00080974
Iteration 9/25 | Loss: 0.00080974
Iteration 10/25 | Loss: 0.00080974
Iteration 11/25 | Loss: 0.00080974
Iteration 12/25 | Loss: 0.00080974
Iteration 13/25 | Loss: 0.00080974
Iteration 14/25 | Loss: 0.00080974
Iteration 15/25 | Loss: 0.00080974
Iteration 16/25 | Loss: 0.00080974
Iteration 17/25 | Loss: 0.00080974
Iteration 18/25 | Loss: 0.00080974
Iteration 19/25 | Loss: 0.00080974
Iteration 20/25 | Loss: 0.00080974
Iteration 21/25 | Loss: 0.00080974
Iteration 22/25 | Loss: 0.00080974
Iteration 23/25 | Loss: 0.00080974
Iteration 24/25 | Loss: 0.00080974
Iteration 25/25 | Loss: 0.00080974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080974
Iteration 2/1000 | Loss: 0.00003136
Iteration 3/1000 | Loss: 0.00001917
Iteration 4/1000 | Loss: 0.00001792
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00001639
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001613
Iteration 12/1000 | Loss: 0.00001610
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001609
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001601
Iteration 18/1000 | Loss: 0.00001601
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001596
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001595
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001594
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001593
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001592
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001591
Iteration 63/1000 | Loss: 0.00001591
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001588
Iteration 70/1000 | Loss: 0.00001588
Iteration 71/1000 | Loss: 0.00001588
Iteration 72/1000 | Loss: 0.00001587
Iteration 73/1000 | Loss: 0.00001587
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001585
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001584
Iteration 83/1000 | Loss: 0.00001584
Iteration 84/1000 | Loss: 0.00001584
Iteration 85/1000 | Loss: 0.00001584
Iteration 86/1000 | Loss: 0.00001583
Iteration 87/1000 | Loss: 0.00001583
Iteration 88/1000 | Loss: 0.00001583
Iteration 89/1000 | Loss: 0.00001583
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001582
Iteration 92/1000 | Loss: 0.00001582
Iteration 93/1000 | Loss: 0.00001582
Iteration 94/1000 | Loss: 0.00001582
Iteration 95/1000 | Loss: 0.00001582
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001580
Iteration 105/1000 | Loss: 0.00001580
Iteration 106/1000 | Loss: 0.00001580
Iteration 107/1000 | Loss: 0.00001580
Iteration 108/1000 | Loss: 0.00001580
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001580
Iteration 112/1000 | Loss: 0.00001580
Iteration 113/1000 | Loss: 0.00001580
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001577
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001576
Iteration 141/1000 | Loss: 0.00001576
Iteration 142/1000 | Loss: 0.00001576
Iteration 143/1000 | Loss: 0.00001576
Iteration 144/1000 | Loss: 0.00001576
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.5763907867949456e-05, 1.5763907867949456e-05, 1.5763907867949456e-05, 1.5763907867949456e-05, 1.5763907867949456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5763907867949456e-05

Optimization complete. Final v2v error: 3.1377947330474854 mm

Highest mean error: 4.119485855102539 mm for frame 143

Lowest mean error: 2.426175594329834 mm for frame 5

Saving results

Total time: 32.50203275680542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030832
Iteration 2/25 | Loss: 0.00274313
Iteration 3/25 | Loss: 0.00174805
Iteration 4/25 | Loss: 0.00146682
Iteration 5/25 | Loss: 0.00141477
Iteration 6/25 | Loss: 0.00125933
Iteration 7/25 | Loss: 0.00117054
Iteration 8/25 | Loss: 0.00112468
Iteration 9/25 | Loss: 0.00114575
Iteration 10/25 | Loss: 0.00108392
Iteration 11/25 | Loss: 0.00104957
Iteration 12/25 | Loss: 0.00103142
Iteration 13/25 | Loss: 0.00102670
Iteration 14/25 | Loss: 0.00099614
Iteration 15/25 | Loss: 0.00099681
Iteration 16/25 | Loss: 0.00098570
Iteration 17/25 | Loss: 0.00097988
Iteration 18/25 | Loss: 0.00098009
Iteration 19/25 | Loss: 0.00097405
Iteration 20/25 | Loss: 0.00096942
Iteration 21/25 | Loss: 0.00097049
Iteration 22/25 | Loss: 0.00096869
Iteration 23/25 | Loss: 0.00097161
Iteration 24/25 | Loss: 0.00097184
Iteration 25/25 | Loss: 0.00096736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32715440
Iteration 2/25 | Loss: 0.00095675
Iteration 3/25 | Loss: 0.00070672
Iteration 4/25 | Loss: 0.00070672
Iteration 5/25 | Loss: 0.00070672
Iteration 6/25 | Loss: 0.00070672
Iteration 7/25 | Loss: 0.00070672
Iteration 8/25 | Loss: 0.00070672
Iteration 9/25 | Loss: 0.00070672
Iteration 10/25 | Loss: 0.00070672
Iteration 11/25 | Loss: 0.00070672
Iteration 12/25 | Loss: 0.00070672
Iteration 13/25 | Loss: 0.00070672
Iteration 14/25 | Loss: 0.00070672
Iteration 15/25 | Loss: 0.00070672
Iteration 16/25 | Loss: 0.00070672
Iteration 17/25 | Loss: 0.00070672
Iteration 18/25 | Loss: 0.00070672
Iteration 19/25 | Loss: 0.00070672
Iteration 20/25 | Loss: 0.00070672
Iteration 21/25 | Loss: 0.00070672
Iteration 22/25 | Loss: 0.00070672
Iteration 23/25 | Loss: 0.00070672
Iteration 24/25 | Loss: 0.00070672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007067189435474575, 0.0007067189435474575, 0.0007067189435474575, 0.0007067189435474575, 0.0007067189435474575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007067189435474575

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070672
Iteration 2/1000 | Loss: 0.00040373
Iteration 3/1000 | Loss: 0.00005479
Iteration 4/1000 | Loss: 0.00008433
Iteration 5/1000 | Loss: 0.00021277
Iteration 6/1000 | Loss: 0.00004266
Iteration 7/1000 | Loss: 0.00003102
Iteration 8/1000 | Loss: 0.00005505
Iteration 9/1000 | Loss: 0.00013957
Iteration 10/1000 | Loss: 0.00004448
Iteration 11/1000 | Loss: 0.00002294
Iteration 12/1000 | Loss: 0.00002382
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001432
Iteration 15/1000 | Loss: 0.00001960
Iteration 16/1000 | Loss: 0.00003353
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001301
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001291
Iteration 23/1000 | Loss: 0.00001286
Iteration 24/1000 | Loss: 0.00001284
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00002637
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001255
Iteration 33/1000 | Loss: 0.00001255
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001248
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001248
Iteration 57/1000 | Loss: 0.00001248
Iteration 58/1000 | Loss: 0.00001248
Iteration 59/1000 | Loss: 0.00001248
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001247
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001245
Iteration 75/1000 | Loss: 0.00001245
Iteration 76/1000 | Loss: 0.00001245
Iteration 77/1000 | Loss: 0.00001245
Iteration 78/1000 | Loss: 0.00001245
Iteration 79/1000 | Loss: 0.00001382
Iteration 80/1000 | Loss: 0.00001428
Iteration 81/1000 | Loss: 0.00001334
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001242
Iteration 86/1000 | Loss: 0.00001242
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001242
Iteration 98/1000 | Loss: 0.00001242
Iteration 99/1000 | Loss: 0.00001242
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001242
Iteration 102/1000 | Loss: 0.00001242
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001242
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001242
Iteration 110/1000 | Loss: 0.00001242
Iteration 111/1000 | Loss: 0.00001242
Iteration 112/1000 | Loss: 0.00001242
Iteration 113/1000 | Loss: 0.00001242
Iteration 114/1000 | Loss: 0.00001242
Iteration 115/1000 | Loss: 0.00001242
Iteration 116/1000 | Loss: 0.00001242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.2422128747857641e-05, 1.2422128747857641e-05, 1.2422128747857641e-05, 1.2422128747857641e-05, 1.2422128747857641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2422128747857641e-05

Optimization complete. Final v2v error: 2.4535491466522217 mm

Highest mean error: 20.320945739746094 mm for frame 13

Lowest mean error: 1.9757405519485474 mm for frame 67

Saving results

Total time: 93.96494317054749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916352
Iteration 2/25 | Loss: 0.00109637
Iteration 3/25 | Loss: 0.00099754
Iteration 4/25 | Loss: 0.00098108
Iteration 5/25 | Loss: 0.00097724
Iteration 6/25 | Loss: 0.00097686
Iteration 7/25 | Loss: 0.00097686
Iteration 8/25 | Loss: 0.00097686
Iteration 9/25 | Loss: 0.00097686
Iteration 10/25 | Loss: 0.00097686
Iteration 11/25 | Loss: 0.00097686
Iteration 12/25 | Loss: 0.00097686
Iteration 13/25 | Loss: 0.00097686
Iteration 14/25 | Loss: 0.00097686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009768560994416475, 0.0009768560994416475, 0.0009768560994416475, 0.0009768560994416475, 0.0009768560994416475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009768560994416475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30548477
Iteration 2/25 | Loss: 0.00067794
Iteration 3/25 | Loss: 0.00067785
Iteration 4/25 | Loss: 0.00067785
Iteration 5/25 | Loss: 0.00067785
Iteration 6/25 | Loss: 0.00067785
Iteration 7/25 | Loss: 0.00067785
Iteration 8/25 | Loss: 0.00067785
Iteration 9/25 | Loss: 0.00067785
Iteration 10/25 | Loss: 0.00067785
Iteration 11/25 | Loss: 0.00067785
Iteration 12/25 | Loss: 0.00067785
Iteration 13/25 | Loss: 0.00067785
Iteration 14/25 | Loss: 0.00067785
Iteration 15/25 | Loss: 0.00067785
Iteration 16/25 | Loss: 0.00067785
Iteration 17/25 | Loss: 0.00067785
Iteration 18/25 | Loss: 0.00067785
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006778486422263086, 0.0006778486422263086, 0.0006778486422263086, 0.0006778486422263086, 0.0006778486422263086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006778486422263086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067785
Iteration 2/1000 | Loss: 0.00002362
Iteration 3/1000 | Loss: 0.00001392
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001127
Iteration 6/1000 | Loss: 0.00001088
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001069
Iteration 9/1000 | Loss: 0.00001059
Iteration 10/1000 | Loss: 0.00001058
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001046
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001039
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001028
Iteration 18/1000 | Loss: 0.00001026
Iteration 19/1000 | Loss: 0.00001024
Iteration 20/1000 | Loss: 0.00001024
Iteration 21/1000 | Loss: 0.00001024
Iteration 22/1000 | Loss: 0.00001023
Iteration 23/1000 | Loss: 0.00001023
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001021
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001021
Iteration 31/1000 | Loss: 0.00001021
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001020
Iteration 34/1000 | Loss: 0.00001019
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001016
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001015
Iteration 40/1000 | Loss: 0.00001015
Iteration 41/1000 | Loss: 0.00001012
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001012
Iteration 44/1000 | Loss: 0.00001012
Iteration 45/1000 | Loss: 0.00001012
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001011
Iteration 50/1000 | Loss: 0.00001011
Iteration 51/1000 | Loss: 0.00001011
Iteration 52/1000 | Loss: 0.00001011
Iteration 53/1000 | Loss: 0.00001011
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001011
Iteration 62/1000 | Loss: 0.00001011
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001010
Iteration 71/1000 | Loss: 0.00001010
Iteration 72/1000 | Loss: 0.00001010
Iteration 73/1000 | Loss: 0.00001010
Iteration 74/1000 | Loss: 0.00001010
Iteration 75/1000 | Loss: 0.00001010
Iteration 76/1000 | Loss: 0.00001009
Iteration 77/1000 | Loss: 0.00001009
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001009
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001008
Iteration 83/1000 | Loss: 0.00001008
Iteration 84/1000 | Loss: 0.00001008
Iteration 85/1000 | Loss: 0.00001008
Iteration 86/1000 | Loss: 0.00001007
Iteration 87/1000 | Loss: 0.00001007
Iteration 88/1000 | Loss: 0.00001007
Iteration 89/1000 | Loss: 0.00001007
Iteration 90/1000 | Loss: 0.00001007
Iteration 91/1000 | Loss: 0.00001007
Iteration 92/1000 | Loss: 0.00001007
Iteration 93/1000 | Loss: 0.00001007
Iteration 94/1000 | Loss: 0.00001007
Iteration 95/1000 | Loss: 0.00001007
Iteration 96/1000 | Loss: 0.00001007
Iteration 97/1000 | Loss: 0.00001006
Iteration 98/1000 | Loss: 0.00001006
Iteration 99/1000 | Loss: 0.00001006
Iteration 100/1000 | Loss: 0.00001006
Iteration 101/1000 | Loss: 0.00001006
Iteration 102/1000 | Loss: 0.00001006
Iteration 103/1000 | Loss: 0.00001006
Iteration 104/1000 | Loss: 0.00001006
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001006
Iteration 108/1000 | Loss: 0.00001006
Iteration 109/1000 | Loss: 0.00001006
Iteration 110/1000 | Loss: 0.00001005
Iteration 111/1000 | Loss: 0.00001005
Iteration 112/1000 | Loss: 0.00001005
Iteration 113/1000 | Loss: 0.00001005
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001005
Iteration 125/1000 | Loss: 0.00001005
Iteration 126/1000 | Loss: 0.00001005
Iteration 127/1000 | Loss: 0.00001004
Iteration 128/1000 | Loss: 0.00001004
Iteration 129/1000 | Loss: 0.00001004
Iteration 130/1000 | Loss: 0.00001004
Iteration 131/1000 | Loss: 0.00001004
Iteration 132/1000 | Loss: 0.00001004
Iteration 133/1000 | Loss: 0.00001004
Iteration 134/1000 | Loss: 0.00001004
Iteration 135/1000 | Loss: 0.00001004
Iteration 136/1000 | Loss: 0.00001004
Iteration 137/1000 | Loss: 0.00001004
Iteration 138/1000 | Loss: 0.00001004
Iteration 139/1000 | Loss: 0.00001004
Iteration 140/1000 | Loss: 0.00001004
Iteration 141/1000 | Loss: 0.00001004
Iteration 142/1000 | Loss: 0.00001004
Iteration 143/1000 | Loss: 0.00001004
Iteration 144/1000 | Loss: 0.00001003
Iteration 145/1000 | Loss: 0.00001003
Iteration 146/1000 | Loss: 0.00001003
Iteration 147/1000 | Loss: 0.00001003
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001003
Iteration 152/1000 | Loss: 0.00001003
Iteration 153/1000 | Loss: 0.00001003
Iteration 154/1000 | Loss: 0.00001003
Iteration 155/1000 | Loss: 0.00001003
Iteration 156/1000 | Loss: 0.00001003
Iteration 157/1000 | Loss: 0.00001003
Iteration 158/1000 | Loss: 0.00001003
Iteration 159/1000 | Loss: 0.00001003
Iteration 160/1000 | Loss: 0.00001003
Iteration 161/1000 | Loss: 0.00001003
Iteration 162/1000 | Loss: 0.00001003
Iteration 163/1000 | Loss: 0.00001003
Iteration 164/1000 | Loss: 0.00001003
Iteration 165/1000 | Loss: 0.00001003
Iteration 166/1000 | Loss: 0.00001003
Iteration 167/1000 | Loss: 0.00001003
Iteration 168/1000 | Loss: 0.00001003
Iteration 169/1000 | Loss: 0.00001003
Iteration 170/1000 | Loss: 0.00001003
Iteration 171/1000 | Loss: 0.00001003
Iteration 172/1000 | Loss: 0.00001003
Iteration 173/1000 | Loss: 0.00001003
Iteration 174/1000 | Loss: 0.00001003
Iteration 175/1000 | Loss: 0.00001003
Iteration 176/1000 | Loss: 0.00001003
Iteration 177/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.0027411917690188e-05, 1.0027411917690188e-05, 1.0027411917690188e-05, 1.0027411917690188e-05, 1.0027411917690188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0027411917690188e-05

Optimization complete. Final v2v error: 2.7226269245147705 mm

Highest mean error: 2.9859561920166016 mm for frame 124

Lowest mean error: 2.568342924118042 mm for frame 0

Saving results

Total time: 35.76324939727783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868864
Iteration 2/25 | Loss: 0.00133901
Iteration 3/25 | Loss: 0.00103593
Iteration 4/25 | Loss: 0.00099137
Iteration 5/25 | Loss: 0.00097804
Iteration 6/25 | Loss: 0.00096015
Iteration 7/25 | Loss: 0.00095474
Iteration 8/25 | Loss: 0.00095362
Iteration 9/25 | Loss: 0.00095424
Iteration 10/25 | Loss: 0.00095342
Iteration 11/25 | Loss: 0.00095232
Iteration 12/25 | Loss: 0.00095123
Iteration 13/25 | Loss: 0.00095115
Iteration 14/25 | Loss: 0.00095474
Iteration 15/25 | Loss: 0.00095092
Iteration 16/25 | Loss: 0.00095033
Iteration 17/25 | Loss: 0.00094881
Iteration 18/25 | Loss: 0.00094764
Iteration 19/25 | Loss: 0.00094733
Iteration 20/25 | Loss: 0.00094796
Iteration 21/25 | Loss: 0.00094743
Iteration 22/25 | Loss: 0.00094795
Iteration 23/25 | Loss: 0.00094781
Iteration 24/25 | Loss: 0.00094729
Iteration 25/25 | Loss: 0.00094746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90122795
Iteration 2/25 | Loss: 0.00065599
Iteration 3/25 | Loss: 0.00065598
Iteration 4/25 | Loss: 0.00065598
Iteration 5/25 | Loss: 0.00065598
Iteration 6/25 | Loss: 0.00065598
Iteration 7/25 | Loss: 0.00065598
Iteration 8/25 | Loss: 0.00065598
Iteration 9/25 | Loss: 0.00065598
Iteration 10/25 | Loss: 0.00065598
Iteration 11/25 | Loss: 0.00065598
Iteration 12/25 | Loss: 0.00065597
Iteration 13/25 | Loss: 0.00065597
Iteration 14/25 | Loss: 0.00065597
Iteration 15/25 | Loss: 0.00065597
Iteration 16/25 | Loss: 0.00065597
Iteration 17/25 | Loss: 0.00065597
Iteration 18/25 | Loss: 0.00065597
Iteration 19/25 | Loss: 0.00065597
Iteration 20/25 | Loss: 0.00065597
Iteration 21/25 | Loss: 0.00065597
Iteration 22/25 | Loss: 0.00065597
Iteration 23/25 | Loss: 0.00065597
Iteration 24/25 | Loss: 0.00065597
Iteration 25/25 | Loss: 0.00065597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065597
Iteration 2/1000 | Loss: 0.00002422
Iteration 3/1000 | Loss: 0.00001596
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001327
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001321
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001316
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001311
Iteration 16/1000 | Loss: 0.00001310
Iteration 17/1000 | Loss: 0.00001310
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001298
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001297
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001294
Iteration 33/1000 | Loss: 0.00001294
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001293
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001292
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001291
Iteration 44/1000 | Loss: 0.00001291
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001288
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001286
Iteration 60/1000 | Loss: 0.00001286
Iteration 61/1000 | Loss: 0.00001285
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001285
Iteration 64/1000 | Loss: 0.00001284
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001284
Iteration 67/1000 | Loss: 0.00001283
Iteration 68/1000 | Loss: 0.00001283
Iteration 69/1000 | Loss: 0.00001283
Iteration 70/1000 | Loss: 0.00001283
Iteration 71/1000 | Loss: 0.00001283
Iteration 72/1000 | Loss: 0.00001283
Iteration 73/1000 | Loss: 0.00001282
Iteration 74/1000 | Loss: 0.00001282
Iteration 75/1000 | Loss: 0.00001282
Iteration 76/1000 | Loss: 0.00001282
Iteration 77/1000 | Loss: 0.00001282
Iteration 78/1000 | Loss: 0.00001282
Iteration 79/1000 | Loss: 0.00001282
Iteration 80/1000 | Loss: 0.00001282
Iteration 81/1000 | Loss: 0.00001281
Iteration 82/1000 | Loss: 0.00001281
Iteration 83/1000 | Loss: 0.00001281
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001281
Iteration 89/1000 | Loss: 0.00001281
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001280
Iteration 92/1000 | Loss: 0.00001280
Iteration 93/1000 | Loss: 0.00001280
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001280
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001279
Iteration 108/1000 | Loss: 0.00001279
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Iteration 131/1000 | Loss: 0.00001279
Iteration 132/1000 | Loss: 0.00001279
Iteration 133/1000 | Loss: 0.00001279
Iteration 134/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2791227163688745e-05, 1.2791227163688745e-05, 1.2791227163688745e-05, 1.2791227163688745e-05, 1.2791227163688745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2791227163688745e-05

Optimization complete. Final v2v error: 2.930759906768799 mm

Highest mean error: 9.38514232635498 mm for frame 233

Lowest mean error: 2.3752458095550537 mm for frame 95

Saving results

Total time: 76.26033234596252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936517
Iteration 2/25 | Loss: 0.00148244
Iteration 3/25 | Loss: 0.00114801
Iteration 4/25 | Loss: 0.00109164
Iteration 5/25 | Loss: 0.00107060
Iteration 6/25 | Loss: 0.00106538
Iteration 7/25 | Loss: 0.00106367
Iteration 8/25 | Loss: 0.00106322
Iteration 9/25 | Loss: 0.00106322
Iteration 10/25 | Loss: 0.00106322
Iteration 11/25 | Loss: 0.00106322
Iteration 12/25 | Loss: 0.00106322
Iteration 13/25 | Loss: 0.00106322
Iteration 14/25 | Loss: 0.00106322
Iteration 15/25 | Loss: 0.00106322
Iteration 16/25 | Loss: 0.00106322
Iteration 17/25 | Loss: 0.00106322
Iteration 18/25 | Loss: 0.00106322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010632174089550972, 0.0010632174089550972, 0.0010632174089550972, 0.0010632174089550972, 0.0010632174089550972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010632174089550972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33142853
Iteration 2/25 | Loss: 0.00071876
Iteration 3/25 | Loss: 0.00071876
Iteration 4/25 | Loss: 0.00071875
Iteration 5/25 | Loss: 0.00071875
Iteration 6/25 | Loss: 0.00071875
Iteration 7/25 | Loss: 0.00071875
Iteration 8/25 | Loss: 0.00071875
Iteration 9/25 | Loss: 0.00071875
Iteration 10/25 | Loss: 0.00071875
Iteration 11/25 | Loss: 0.00071875
Iteration 12/25 | Loss: 0.00071875
Iteration 13/25 | Loss: 0.00071875
Iteration 14/25 | Loss: 0.00071875
Iteration 15/25 | Loss: 0.00071875
Iteration 16/25 | Loss: 0.00071875
Iteration 17/25 | Loss: 0.00071875
Iteration 18/25 | Loss: 0.00071875
Iteration 19/25 | Loss: 0.00071875
Iteration 20/25 | Loss: 0.00071875
Iteration 21/25 | Loss: 0.00071875
Iteration 22/25 | Loss: 0.00071875
Iteration 23/25 | Loss: 0.00071875
Iteration 24/25 | Loss: 0.00071875
Iteration 25/25 | Loss: 0.00071875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071875
Iteration 2/1000 | Loss: 0.00007680
Iteration 3/1000 | Loss: 0.00004773
Iteration 4/1000 | Loss: 0.00003662
Iteration 5/1000 | Loss: 0.00003198
Iteration 6/1000 | Loss: 0.00003032
Iteration 7/1000 | Loss: 0.00002933
Iteration 8/1000 | Loss: 0.00002865
Iteration 9/1000 | Loss: 0.00002815
Iteration 10/1000 | Loss: 0.00002774
Iteration 11/1000 | Loss: 0.00002724
Iteration 12/1000 | Loss: 0.00002694
Iteration 13/1000 | Loss: 0.00002675
Iteration 14/1000 | Loss: 0.00002653
Iteration 15/1000 | Loss: 0.00002638
Iteration 16/1000 | Loss: 0.00002631
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002625
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002619
Iteration 22/1000 | Loss: 0.00002619
Iteration 23/1000 | Loss: 0.00002618
Iteration 24/1000 | Loss: 0.00002618
Iteration 25/1000 | Loss: 0.00002617
Iteration 26/1000 | Loss: 0.00002617
Iteration 27/1000 | Loss: 0.00002616
Iteration 28/1000 | Loss: 0.00002616
Iteration 29/1000 | Loss: 0.00002616
Iteration 30/1000 | Loss: 0.00002615
Iteration 31/1000 | Loss: 0.00002614
Iteration 32/1000 | Loss: 0.00002614
Iteration 33/1000 | Loss: 0.00002613
Iteration 34/1000 | Loss: 0.00002612
Iteration 35/1000 | Loss: 0.00002611
Iteration 36/1000 | Loss: 0.00002610
Iteration 37/1000 | Loss: 0.00002609
Iteration 38/1000 | Loss: 0.00002609
Iteration 39/1000 | Loss: 0.00002607
Iteration 40/1000 | Loss: 0.00002606
Iteration 41/1000 | Loss: 0.00002606
Iteration 42/1000 | Loss: 0.00002606
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002606
Iteration 45/1000 | Loss: 0.00002606
Iteration 46/1000 | Loss: 0.00002604
Iteration 47/1000 | Loss: 0.00002604
Iteration 48/1000 | Loss: 0.00002604
Iteration 49/1000 | Loss: 0.00002604
Iteration 50/1000 | Loss: 0.00002604
Iteration 51/1000 | Loss: 0.00002603
Iteration 52/1000 | Loss: 0.00002603
Iteration 53/1000 | Loss: 0.00002603
Iteration 54/1000 | Loss: 0.00002603
Iteration 55/1000 | Loss: 0.00002603
Iteration 56/1000 | Loss: 0.00002603
Iteration 57/1000 | Loss: 0.00002603
Iteration 58/1000 | Loss: 0.00002603
Iteration 59/1000 | Loss: 0.00002601
Iteration 60/1000 | Loss: 0.00002601
Iteration 61/1000 | Loss: 0.00002601
Iteration 62/1000 | Loss: 0.00002601
Iteration 63/1000 | Loss: 0.00002601
Iteration 64/1000 | Loss: 0.00002601
Iteration 65/1000 | Loss: 0.00002600
Iteration 66/1000 | Loss: 0.00002600
Iteration 67/1000 | Loss: 0.00002600
Iteration 68/1000 | Loss: 0.00002600
Iteration 69/1000 | Loss: 0.00002600
Iteration 70/1000 | Loss: 0.00002599
Iteration 71/1000 | Loss: 0.00002599
Iteration 72/1000 | Loss: 0.00002598
Iteration 73/1000 | Loss: 0.00002598
Iteration 74/1000 | Loss: 0.00002598
Iteration 75/1000 | Loss: 0.00002598
Iteration 76/1000 | Loss: 0.00002598
Iteration 77/1000 | Loss: 0.00002598
Iteration 78/1000 | Loss: 0.00002598
Iteration 79/1000 | Loss: 0.00002598
Iteration 80/1000 | Loss: 0.00002597
Iteration 81/1000 | Loss: 0.00002597
Iteration 82/1000 | Loss: 0.00002596
Iteration 83/1000 | Loss: 0.00002596
Iteration 84/1000 | Loss: 0.00002596
Iteration 85/1000 | Loss: 0.00002596
Iteration 86/1000 | Loss: 0.00002596
Iteration 87/1000 | Loss: 0.00002596
Iteration 88/1000 | Loss: 0.00002596
Iteration 89/1000 | Loss: 0.00002596
Iteration 90/1000 | Loss: 0.00002596
Iteration 91/1000 | Loss: 0.00002596
Iteration 92/1000 | Loss: 0.00002596
Iteration 93/1000 | Loss: 0.00002595
Iteration 94/1000 | Loss: 0.00002595
Iteration 95/1000 | Loss: 0.00002595
Iteration 96/1000 | Loss: 0.00002595
Iteration 97/1000 | Loss: 0.00002595
Iteration 98/1000 | Loss: 0.00002594
Iteration 99/1000 | Loss: 0.00002594
Iteration 100/1000 | Loss: 0.00002594
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002593
Iteration 103/1000 | Loss: 0.00002593
Iteration 104/1000 | Loss: 0.00002593
Iteration 105/1000 | Loss: 0.00002592
Iteration 106/1000 | Loss: 0.00002592
Iteration 107/1000 | Loss: 0.00002592
Iteration 108/1000 | Loss: 0.00002592
Iteration 109/1000 | Loss: 0.00002592
Iteration 110/1000 | Loss: 0.00002592
Iteration 111/1000 | Loss: 0.00002592
Iteration 112/1000 | Loss: 0.00002591
Iteration 113/1000 | Loss: 0.00002591
Iteration 114/1000 | Loss: 0.00002591
Iteration 115/1000 | Loss: 0.00002590
Iteration 116/1000 | Loss: 0.00002590
Iteration 117/1000 | Loss: 0.00002590
Iteration 118/1000 | Loss: 0.00002589
Iteration 119/1000 | Loss: 0.00002589
Iteration 120/1000 | Loss: 0.00002589
Iteration 121/1000 | Loss: 0.00002589
Iteration 122/1000 | Loss: 0.00002588
Iteration 123/1000 | Loss: 0.00002588
Iteration 124/1000 | Loss: 0.00002588
Iteration 125/1000 | Loss: 0.00002587
Iteration 126/1000 | Loss: 0.00002587
Iteration 127/1000 | Loss: 0.00002587
Iteration 128/1000 | Loss: 0.00002587
Iteration 129/1000 | Loss: 0.00002586
Iteration 130/1000 | Loss: 0.00002586
Iteration 131/1000 | Loss: 0.00002586
Iteration 132/1000 | Loss: 0.00002585
Iteration 133/1000 | Loss: 0.00002585
Iteration 134/1000 | Loss: 0.00002585
Iteration 135/1000 | Loss: 0.00002585
Iteration 136/1000 | Loss: 0.00002585
Iteration 137/1000 | Loss: 0.00002585
Iteration 138/1000 | Loss: 0.00002584
Iteration 139/1000 | Loss: 0.00002584
Iteration 140/1000 | Loss: 0.00002584
Iteration 141/1000 | Loss: 0.00002584
Iteration 142/1000 | Loss: 0.00002584
Iteration 143/1000 | Loss: 0.00002584
Iteration 144/1000 | Loss: 0.00002583
Iteration 145/1000 | Loss: 0.00002583
Iteration 146/1000 | Loss: 0.00002583
Iteration 147/1000 | Loss: 0.00002583
Iteration 148/1000 | Loss: 0.00002582
Iteration 149/1000 | Loss: 0.00002582
Iteration 150/1000 | Loss: 0.00002582
Iteration 151/1000 | Loss: 0.00002582
Iteration 152/1000 | Loss: 0.00002582
Iteration 153/1000 | Loss: 0.00002581
Iteration 154/1000 | Loss: 0.00002581
Iteration 155/1000 | Loss: 0.00002581
Iteration 156/1000 | Loss: 0.00002581
Iteration 157/1000 | Loss: 0.00002581
Iteration 158/1000 | Loss: 0.00002581
Iteration 159/1000 | Loss: 0.00002581
Iteration 160/1000 | Loss: 0.00002581
Iteration 161/1000 | Loss: 0.00002580
Iteration 162/1000 | Loss: 0.00002580
Iteration 163/1000 | Loss: 0.00002580
Iteration 164/1000 | Loss: 0.00002580
Iteration 165/1000 | Loss: 0.00002579
Iteration 166/1000 | Loss: 0.00002579
Iteration 167/1000 | Loss: 0.00002579
Iteration 168/1000 | Loss: 0.00002579
Iteration 169/1000 | Loss: 0.00002579
Iteration 170/1000 | Loss: 0.00002579
Iteration 171/1000 | Loss: 0.00002579
Iteration 172/1000 | Loss: 0.00002579
Iteration 173/1000 | Loss: 0.00002578
Iteration 174/1000 | Loss: 0.00002578
Iteration 175/1000 | Loss: 0.00002578
Iteration 176/1000 | Loss: 0.00002578
Iteration 177/1000 | Loss: 0.00002578
Iteration 178/1000 | Loss: 0.00002578
Iteration 179/1000 | Loss: 0.00002578
Iteration 180/1000 | Loss: 0.00002578
Iteration 181/1000 | Loss: 0.00002578
Iteration 182/1000 | Loss: 0.00002577
Iteration 183/1000 | Loss: 0.00002577
Iteration 184/1000 | Loss: 0.00002577
Iteration 185/1000 | Loss: 0.00002576
Iteration 186/1000 | Loss: 0.00002576
Iteration 187/1000 | Loss: 0.00002576
Iteration 188/1000 | Loss: 0.00002576
Iteration 189/1000 | Loss: 0.00002576
Iteration 190/1000 | Loss: 0.00002575
Iteration 191/1000 | Loss: 0.00002575
Iteration 192/1000 | Loss: 0.00002575
Iteration 193/1000 | Loss: 0.00002575
Iteration 194/1000 | Loss: 0.00002575
Iteration 195/1000 | Loss: 0.00002575
Iteration 196/1000 | Loss: 0.00002575
Iteration 197/1000 | Loss: 0.00002574
Iteration 198/1000 | Loss: 0.00002574
Iteration 199/1000 | Loss: 0.00002574
Iteration 200/1000 | Loss: 0.00002574
Iteration 201/1000 | Loss: 0.00002574
Iteration 202/1000 | Loss: 0.00002574
Iteration 203/1000 | Loss: 0.00002574
Iteration 204/1000 | Loss: 0.00002574
Iteration 205/1000 | Loss: 0.00002574
Iteration 206/1000 | Loss: 0.00002573
Iteration 207/1000 | Loss: 0.00002573
Iteration 208/1000 | Loss: 0.00002573
Iteration 209/1000 | Loss: 0.00002573
Iteration 210/1000 | Loss: 0.00002573
Iteration 211/1000 | Loss: 0.00002573
Iteration 212/1000 | Loss: 0.00002573
Iteration 213/1000 | Loss: 0.00002573
Iteration 214/1000 | Loss: 0.00002573
Iteration 215/1000 | Loss: 0.00002573
Iteration 216/1000 | Loss: 0.00002573
Iteration 217/1000 | Loss: 0.00002573
Iteration 218/1000 | Loss: 0.00002573
Iteration 219/1000 | Loss: 0.00002573
Iteration 220/1000 | Loss: 0.00002573
Iteration 221/1000 | Loss: 0.00002573
Iteration 222/1000 | Loss: 0.00002573
Iteration 223/1000 | Loss: 0.00002573
Iteration 224/1000 | Loss: 0.00002573
Iteration 225/1000 | Loss: 0.00002573
Iteration 226/1000 | Loss: 0.00002573
Iteration 227/1000 | Loss: 0.00002573
Iteration 228/1000 | Loss: 0.00002573
Iteration 229/1000 | Loss: 0.00002573
Iteration 230/1000 | Loss: 0.00002573
Iteration 231/1000 | Loss: 0.00002573
Iteration 232/1000 | Loss: 0.00002573
Iteration 233/1000 | Loss: 0.00002573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.5734132577781565e-05, 2.5734132577781565e-05, 2.5734132577781565e-05, 2.5734132577781565e-05, 2.5734132577781565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5734132577781565e-05

Optimization complete. Final v2v error: 3.994323253631592 mm

Highest mean error: 7.109205722808838 mm for frame 116

Lowest mean error: 2.644766092300415 mm for frame 75

Saving results

Total time: 50.1312472820282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958475
Iteration 2/25 | Loss: 0.00392398
Iteration 3/25 | Loss: 0.00276976
Iteration 4/25 | Loss: 0.00217587
Iteration 5/25 | Loss: 0.00201671
Iteration 6/25 | Loss: 0.00202695
Iteration 7/25 | Loss: 0.00190809
Iteration 8/25 | Loss: 0.00179364
Iteration 9/25 | Loss: 0.00171472
Iteration 10/25 | Loss: 0.00168090
Iteration 11/25 | Loss: 0.00167961
Iteration 12/25 | Loss: 0.00167057
Iteration 13/25 | Loss: 0.00167400
Iteration 14/25 | Loss: 0.00167209
Iteration 15/25 | Loss: 0.00166889
Iteration 16/25 | Loss: 0.00166909
Iteration 17/25 | Loss: 0.00166852
Iteration 18/25 | Loss: 0.00166850
Iteration 19/25 | Loss: 0.00166850
Iteration 20/25 | Loss: 0.00166849
Iteration 21/25 | Loss: 0.00166849
Iteration 22/25 | Loss: 0.00166849
Iteration 23/25 | Loss: 0.00166849
Iteration 24/25 | Loss: 0.00166849
Iteration 25/25 | Loss: 0.00166849

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31196356
Iteration 2/25 | Loss: 0.01426532
Iteration 3/25 | Loss: 0.00708125
Iteration 4/25 | Loss: 0.00708125
Iteration 5/25 | Loss: 0.00708125
Iteration 6/25 | Loss: 0.00708125
Iteration 7/25 | Loss: 0.00708125
Iteration 8/25 | Loss: 0.00708125
Iteration 9/25 | Loss: 0.00708124
Iteration 10/25 | Loss: 0.00708124
Iteration 11/25 | Loss: 0.00708124
Iteration 12/25 | Loss: 0.00708124
Iteration 13/25 | Loss: 0.00708124
Iteration 14/25 | Loss: 0.00708124
Iteration 15/25 | Loss: 0.00708124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.007081243675202131, 0.007081243675202131, 0.007081243675202131, 0.007081243675202131, 0.007081243675202131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007081243675202131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00708124
Iteration 2/1000 | Loss: 0.00278222
Iteration 3/1000 | Loss: 0.00088191
Iteration 4/1000 | Loss: 0.00132051
Iteration 5/1000 | Loss: 0.01062254
Iteration 6/1000 | Loss: 0.00069665
Iteration 7/1000 | Loss: 0.00039676
Iteration 8/1000 | Loss: 0.00070014
Iteration 9/1000 | Loss: 0.00037774
Iteration 10/1000 | Loss: 0.00038240
Iteration 11/1000 | Loss: 0.00044599
Iteration 12/1000 | Loss: 0.00060868
Iteration 13/1000 | Loss: 0.00035211
Iteration 14/1000 | Loss: 0.00035322
Iteration 15/1000 | Loss: 0.00033532
Iteration 16/1000 | Loss: 0.00035595
Iteration 17/1000 | Loss: 0.00032502
Iteration 18/1000 | Loss: 0.00045192
Iteration 19/1000 | Loss: 0.00032237
Iteration 20/1000 | Loss: 0.00037399
Iteration 21/1000 | Loss: 0.00031999
Iteration 22/1000 | Loss: 0.00039894
Iteration 23/1000 | Loss: 0.00031648
Iteration 24/1000 | Loss: 0.00034804
Iteration 25/1000 | Loss: 0.00036003
Iteration 26/1000 | Loss: 0.00032956
Iteration 27/1000 | Loss: 0.00030709
Iteration 28/1000 | Loss: 0.00051299
Iteration 29/1000 | Loss: 0.00091763
Iteration 30/1000 | Loss: 0.00094907
Iteration 31/1000 | Loss: 0.00062262
Iteration 32/1000 | Loss: 0.00033704
Iteration 33/1000 | Loss: 0.00030211
Iteration 34/1000 | Loss: 0.00029503
Iteration 35/1000 | Loss: 0.00060465
Iteration 36/1000 | Loss: 0.00028697
Iteration 37/1000 | Loss: 0.00081626
Iteration 38/1000 | Loss: 0.00029744
Iteration 39/1000 | Loss: 0.00028171
Iteration 40/1000 | Loss: 0.00027773
Iteration 41/1000 | Loss: 0.00027711
Iteration 42/1000 | Loss: 0.00035029
Iteration 43/1000 | Loss: 0.00047532
Iteration 44/1000 | Loss: 0.00043353
Iteration 45/1000 | Loss: 0.00056477
Iteration 46/1000 | Loss: 0.00027576
Iteration 47/1000 | Loss: 0.00031781
Iteration 48/1000 | Loss: 0.00026989
Iteration 49/1000 | Loss: 0.00051224
Iteration 50/1000 | Loss: 0.00026862
Iteration 51/1000 | Loss: 0.00026737
Iteration 52/1000 | Loss: 0.00037934
Iteration 53/1000 | Loss: 0.00026759
Iteration 54/1000 | Loss: 0.00032330
Iteration 55/1000 | Loss: 0.00042471
Iteration 56/1000 | Loss: 0.00051265
Iteration 57/1000 | Loss: 0.00028954
Iteration 58/1000 | Loss: 0.00027821
Iteration 59/1000 | Loss: 0.00026516
Iteration 60/1000 | Loss: 0.00033135
Iteration 61/1000 | Loss: 0.00033870
Iteration 62/1000 | Loss: 0.00027429
Iteration 63/1000 | Loss: 0.00028377
Iteration 64/1000 | Loss: 0.00026451
Iteration 65/1000 | Loss: 0.00026432
Iteration 66/1000 | Loss: 0.00029709
Iteration 67/1000 | Loss: 0.00026424
Iteration 68/1000 | Loss: 0.00026417
Iteration 69/1000 | Loss: 0.00026410
Iteration 70/1000 | Loss: 0.00026403
Iteration 71/1000 | Loss: 0.00036074
Iteration 72/1000 | Loss: 0.00118805
Iteration 73/1000 | Loss: 0.00053083
Iteration 74/1000 | Loss: 0.00027233
Iteration 75/1000 | Loss: 0.00034069
Iteration 76/1000 | Loss: 0.00044549
Iteration 77/1000 | Loss: 0.00027353
Iteration 78/1000 | Loss: 0.00026402
Iteration 79/1000 | Loss: 0.00028258
Iteration 80/1000 | Loss: 0.00026400
Iteration 81/1000 | Loss: 0.00033609
Iteration 82/1000 | Loss: 0.00052038
Iteration 83/1000 | Loss: 0.00029211
Iteration 84/1000 | Loss: 0.00030450
Iteration 85/1000 | Loss: 0.00026901
Iteration 86/1000 | Loss: 0.00030769
Iteration 87/1000 | Loss: 0.00028823
Iteration 88/1000 | Loss: 0.00026395
Iteration 89/1000 | Loss: 0.00026392
Iteration 90/1000 | Loss: 0.00026391
Iteration 91/1000 | Loss: 0.00026389
Iteration 92/1000 | Loss: 0.00026619
Iteration 93/1000 | Loss: 0.00026388
Iteration 94/1000 | Loss: 0.00026385
Iteration 95/1000 | Loss: 0.00026385
Iteration 96/1000 | Loss: 0.00026385
Iteration 97/1000 | Loss: 0.00026384
Iteration 98/1000 | Loss: 0.00026384
Iteration 99/1000 | Loss: 0.00026384
Iteration 100/1000 | Loss: 0.00026383
Iteration 101/1000 | Loss: 0.00026383
Iteration 102/1000 | Loss: 0.00026383
Iteration 103/1000 | Loss: 0.00026383
Iteration 104/1000 | Loss: 0.00026382
Iteration 105/1000 | Loss: 0.00026382
Iteration 106/1000 | Loss: 0.00026382
Iteration 107/1000 | Loss: 0.00026381
Iteration 108/1000 | Loss: 0.00026381
Iteration 109/1000 | Loss: 0.00027701
Iteration 110/1000 | Loss: 0.00026385
Iteration 111/1000 | Loss: 0.00026378
Iteration 112/1000 | Loss: 0.00026378
Iteration 113/1000 | Loss: 0.00026377
Iteration 114/1000 | Loss: 0.00026377
Iteration 115/1000 | Loss: 0.00026377
Iteration 116/1000 | Loss: 0.00026377
Iteration 117/1000 | Loss: 0.00026377
Iteration 118/1000 | Loss: 0.00026377
Iteration 119/1000 | Loss: 0.00026377
Iteration 120/1000 | Loss: 0.00026377
Iteration 121/1000 | Loss: 0.00026377
Iteration 122/1000 | Loss: 0.00026377
Iteration 123/1000 | Loss: 0.00026377
Iteration 124/1000 | Loss: 0.00026377
Iteration 125/1000 | Loss: 0.00026377
Iteration 126/1000 | Loss: 0.00026376
Iteration 127/1000 | Loss: 0.00026376
Iteration 128/1000 | Loss: 0.00026376
Iteration 129/1000 | Loss: 0.00026376
Iteration 130/1000 | Loss: 0.00030839
Iteration 131/1000 | Loss: 0.00027068
Iteration 132/1000 | Loss: 0.00026429
Iteration 133/1000 | Loss: 0.00027332
Iteration 134/1000 | Loss: 0.00026466
Iteration 135/1000 | Loss: 0.00026435
Iteration 136/1000 | Loss: 0.00026429
Iteration 137/1000 | Loss: 0.00026384
Iteration 138/1000 | Loss: 0.00026384
Iteration 139/1000 | Loss: 0.00026384
Iteration 140/1000 | Loss: 0.00026384
Iteration 141/1000 | Loss: 0.00026384
Iteration 142/1000 | Loss: 0.00026384
Iteration 143/1000 | Loss: 0.00026384
Iteration 144/1000 | Loss: 0.00026384
Iteration 145/1000 | Loss: 0.00026384
Iteration 146/1000 | Loss: 0.00026384
Iteration 147/1000 | Loss: 0.00026384
Iteration 148/1000 | Loss: 0.00026382
Iteration 149/1000 | Loss: 0.00026382
Iteration 150/1000 | Loss: 0.00026382
Iteration 151/1000 | Loss: 0.00026382
Iteration 152/1000 | Loss: 0.00026382
Iteration 153/1000 | Loss: 0.00026382
Iteration 154/1000 | Loss: 0.00026382
Iteration 155/1000 | Loss: 0.00026382
Iteration 156/1000 | Loss: 0.00026382
Iteration 157/1000 | Loss: 0.00026382
Iteration 158/1000 | Loss: 0.00026381
Iteration 159/1000 | Loss: 0.00026381
Iteration 160/1000 | Loss: 0.00026381
Iteration 161/1000 | Loss: 0.00026381
Iteration 162/1000 | Loss: 0.00026381
Iteration 163/1000 | Loss: 0.00026380
Iteration 164/1000 | Loss: 0.00026380
Iteration 165/1000 | Loss: 0.00026380
Iteration 166/1000 | Loss: 0.00026380
Iteration 167/1000 | Loss: 0.00026380
Iteration 168/1000 | Loss: 0.00026380
Iteration 169/1000 | Loss: 0.00026380
Iteration 170/1000 | Loss: 0.00026380
Iteration 171/1000 | Loss: 0.00026380
Iteration 172/1000 | Loss: 0.00026379
Iteration 173/1000 | Loss: 0.00026379
Iteration 174/1000 | Loss: 0.00026379
Iteration 175/1000 | Loss: 0.00026379
Iteration 176/1000 | Loss: 0.00026379
Iteration 177/1000 | Loss: 0.00026379
Iteration 178/1000 | Loss: 0.00026379
Iteration 179/1000 | Loss: 0.00026379
Iteration 180/1000 | Loss: 0.00026379
Iteration 181/1000 | Loss: 0.00026379
Iteration 182/1000 | Loss: 0.00026379
Iteration 183/1000 | Loss: 0.00026379
Iteration 184/1000 | Loss: 0.00026379
Iteration 185/1000 | Loss: 0.00026378
Iteration 186/1000 | Loss: 0.00026378
Iteration 187/1000 | Loss: 0.00026378
Iteration 188/1000 | Loss: 0.00026378
Iteration 189/1000 | Loss: 0.00026378
Iteration 190/1000 | Loss: 0.00026378
Iteration 191/1000 | Loss: 0.00026378
Iteration 192/1000 | Loss: 0.00026377
Iteration 193/1000 | Loss: 0.00026377
Iteration 194/1000 | Loss: 0.00026377
Iteration 195/1000 | Loss: 0.00026377
Iteration 196/1000 | Loss: 0.00026376
Iteration 197/1000 | Loss: 0.00026376
Iteration 198/1000 | Loss: 0.00026376
Iteration 199/1000 | Loss: 0.00026376
Iteration 200/1000 | Loss: 0.00026376
Iteration 201/1000 | Loss: 0.00026376
Iteration 202/1000 | Loss: 0.00026376
Iteration 203/1000 | Loss: 0.00026376
Iteration 204/1000 | Loss: 0.00026376
Iteration 205/1000 | Loss: 0.00026376
Iteration 206/1000 | Loss: 0.00026376
Iteration 207/1000 | Loss: 0.00026376
Iteration 208/1000 | Loss: 0.00026376
Iteration 209/1000 | Loss: 0.00026376
Iteration 210/1000 | Loss: 0.00026376
Iteration 211/1000 | Loss: 0.00026376
Iteration 212/1000 | Loss: 0.00026376
Iteration 213/1000 | Loss: 0.00026376
Iteration 214/1000 | Loss: 0.00026376
Iteration 215/1000 | Loss: 0.00026376
Iteration 216/1000 | Loss: 0.00026376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [0.0002637602447066456, 0.0002637602447066456, 0.0002637602447066456, 0.0002637602447066456, 0.0002637602447066456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002637602447066456

Optimization complete. Final v2v error: 9.227375030517578 mm

Highest mean error: 12.684760093688965 mm for frame 197

Lowest mean error: 4.805063724517822 mm for frame 2

Saving results

Total time: 194.77559232711792
