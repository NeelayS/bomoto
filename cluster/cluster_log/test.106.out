Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=106, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5936-5991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01189269
Iteration 2/25 | Loss: 0.00234584
Iteration 3/25 | Loss: 0.00134913
Iteration 4/25 | Loss: 0.00122375
Iteration 5/25 | Loss: 0.00122507
Iteration 6/25 | Loss: 0.00113954
Iteration 7/25 | Loss: 0.00113132
Iteration 8/25 | Loss: 0.00111051
Iteration 9/25 | Loss: 0.00108542
Iteration 10/25 | Loss: 0.00108318
Iteration 11/25 | Loss: 0.00106399
Iteration 12/25 | Loss: 0.00106476
Iteration 13/25 | Loss: 0.00106258
Iteration 14/25 | Loss: 0.00106262
Iteration 15/25 | Loss: 0.00106005
Iteration 16/25 | Loss: 0.00105521
Iteration 17/25 | Loss: 0.00105483
Iteration 18/25 | Loss: 0.00105501
Iteration 19/25 | Loss: 0.00105858
Iteration 20/25 | Loss: 0.00105774
Iteration 21/25 | Loss: 0.00105518
Iteration 22/25 | Loss: 0.00105190
Iteration 23/25 | Loss: 0.00105317
Iteration 24/25 | Loss: 0.00105222
Iteration 25/25 | Loss: 0.00105016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90409285
Iteration 2/25 | Loss: 0.00150345
Iteration 3/25 | Loss: 0.00127187
Iteration 4/25 | Loss: 0.00127187
Iteration 5/25 | Loss: 0.00127187
Iteration 6/25 | Loss: 0.00127187
Iteration 7/25 | Loss: 0.00127187
Iteration 8/25 | Loss: 0.00127187
Iteration 9/25 | Loss: 0.00127187
Iteration 10/25 | Loss: 0.00127187
Iteration 11/25 | Loss: 0.00127187
Iteration 12/25 | Loss: 0.00127187
Iteration 13/25 | Loss: 0.00127187
Iteration 14/25 | Loss: 0.00127187
Iteration 15/25 | Loss: 0.00127187
Iteration 16/25 | Loss: 0.00127187
Iteration 17/25 | Loss: 0.00127187
Iteration 18/25 | Loss: 0.00127187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012718738289549947, 0.0012718738289549947, 0.0012718738289549947, 0.0012718738289549947, 0.0012718738289549947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012718738289549947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127187
Iteration 2/1000 | Loss: 0.00035935
Iteration 3/1000 | Loss: 0.00076898
Iteration 4/1000 | Loss: 0.00029066
Iteration 5/1000 | Loss: 0.00017411
Iteration 6/1000 | Loss: 0.00068261
Iteration 7/1000 | Loss: 0.00009145
Iteration 8/1000 | Loss: 0.00013070
Iteration 9/1000 | Loss: 0.00007459
Iteration 10/1000 | Loss: 0.00089121
Iteration 11/1000 | Loss: 0.00059917
Iteration 12/1000 | Loss: 0.00048514
Iteration 13/1000 | Loss: 0.00114843
Iteration 14/1000 | Loss: 0.00114981
Iteration 15/1000 | Loss: 0.00110356
Iteration 16/1000 | Loss: 0.00016298
Iteration 17/1000 | Loss: 0.00060267
Iteration 18/1000 | Loss: 0.00069886
Iteration 19/1000 | Loss: 0.00009609
Iteration 20/1000 | Loss: 0.00006290
Iteration 21/1000 | Loss: 0.00037803
Iteration 22/1000 | Loss: 0.00016655
Iteration 23/1000 | Loss: 0.00016247
Iteration 24/1000 | Loss: 0.00039168
Iteration 25/1000 | Loss: 0.00029736
Iteration 26/1000 | Loss: 0.00047100
Iteration 27/1000 | Loss: 0.00017354
Iteration 28/1000 | Loss: 0.00017627
Iteration 29/1000 | Loss: 0.00007705
Iteration 30/1000 | Loss: 0.00015415
Iteration 31/1000 | Loss: 0.00025050
Iteration 32/1000 | Loss: 0.00018802
Iteration 33/1000 | Loss: 0.00019416
Iteration 34/1000 | Loss: 0.00016220
Iteration 35/1000 | Loss: 0.00032847
Iteration 36/1000 | Loss: 0.00012411
Iteration 37/1000 | Loss: 0.00011700
Iteration 38/1000 | Loss: 0.00021639
Iteration 39/1000 | Loss: 0.00010018
Iteration 40/1000 | Loss: 0.00009244
Iteration 41/1000 | Loss: 0.00006445
Iteration 42/1000 | Loss: 0.00025203
Iteration 43/1000 | Loss: 0.00015755
Iteration 44/1000 | Loss: 0.00016421
Iteration 45/1000 | Loss: 0.00006181
Iteration 46/1000 | Loss: 0.00005887
Iteration 47/1000 | Loss: 0.00025818
Iteration 48/1000 | Loss: 0.00017726
Iteration 49/1000 | Loss: 0.00025471
Iteration 50/1000 | Loss: 0.00019229
Iteration 51/1000 | Loss: 0.00005638
Iteration 52/1000 | Loss: 0.00005513
Iteration 53/1000 | Loss: 0.00010238
Iteration 54/1000 | Loss: 0.00005375
Iteration 55/1000 | Loss: 0.00011346
Iteration 56/1000 | Loss: 0.00005303
Iteration 57/1000 | Loss: 0.00005246
Iteration 58/1000 | Loss: 0.00005208
Iteration 59/1000 | Loss: 0.00005166
Iteration 60/1000 | Loss: 0.00005131
Iteration 61/1000 | Loss: 0.00005106
Iteration 62/1000 | Loss: 0.00015645
Iteration 63/1000 | Loss: 0.00005152
Iteration 64/1000 | Loss: 0.00009627
Iteration 65/1000 | Loss: 0.00006156
Iteration 66/1000 | Loss: 0.00007618
Iteration 67/1000 | Loss: 0.00006043
Iteration 68/1000 | Loss: 0.00006260
Iteration 69/1000 | Loss: 0.00005095
Iteration 70/1000 | Loss: 0.00005081
Iteration 71/1000 | Loss: 0.00005080
Iteration 72/1000 | Loss: 0.00005078
Iteration 73/1000 | Loss: 0.00005077
Iteration 74/1000 | Loss: 0.00005077
Iteration 75/1000 | Loss: 0.00005077
Iteration 76/1000 | Loss: 0.00005076
Iteration 77/1000 | Loss: 0.00005076
Iteration 78/1000 | Loss: 0.00005076
Iteration 79/1000 | Loss: 0.00005075
Iteration 80/1000 | Loss: 0.00005075
Iteration 81/1000 | Loss: 0.00005074
Iteration 82/1000 | Loss: 0.00005074
Iteration 83/1000 | Loss: 0.00005073
Iteration 84/1000 | Loss: 0.00005073
Iteration 85/1000 | Loss: 0.00005073
Iteration 86/1000 | Loss: 0.00005072
Iteration 87/1000 | Loss: 0.00005072
Iteration 88/1000 | Loss: 0.00005070
Iteration 89/1000 | Loss: 0.00005070
Iteration 90/1000 | Loss: 0.00005070
Iteration 91/1000 | Loss: 0.00005070
Iteration 92/1000 | Loss: 0.00005070
Iteration 93/1000 | Loss: 0.00005069
Iteration 94/1000 | Loss: 0.00005069
Iteration 95/1000 | Loss: 0.00005069
Iteration 96/1000 | Loss: 0.00005069
Iteration 97/1000 | Loss: 0.00005069
Iteration 98/1000 | Loss: 0.00005069
Iteration 99/1000 | Loss: 0.00005069
Iteration 100/1000 | Loss: 0.00005069
Iteration 101/1000 | Loss: 0.00005069
Iteration 102/1000 | Loss: 0.00005068
Iteration 103/1000 | Loss: 0.00005068
Iteration 104/1000 | Loss: 0.00005068
Iteration 105/1000 | Loss: 0.00005068
Iteration 106/1000 | Loss: 0.00005068
Iteration 107/1000 | Loss: 0.00005068
Iteration 108/1000 | Loss: 0.00005068
Iteration 109/1000 | Loss: 0.00005068
Iteration 110/1000 | Loss: 0.00005068
Iteration 111/1000 | Loss: 0.00005068
Iteration 112/1000 | Loss: 0.00005068
Iteration 113/1000 | Loss: 0.00005067
Iteration 114/1000 | Loss: 0.00005067
Iteration 115/1000 | Loss: 0.00005067
Iteration 116/1000 | Loss: 0.00005067
Iteration 117/1000 | Loss: 0.00005067
Iteration 118/1000 | Loss: 0.00005067
Iteration 119/1000 | Loss: 0.00005067
Iteration 120/1000 | Loss: 0.00005067
Iteration 121/1000 | Loss: 0.00005066
Iteration 122/1000 | Loss: 0.00005066
Iteration 123/1000 | Loss: 0.00005066
Iteration 124/1000 | Loss: 0.00005066
Iteration 125/1000 | Loss: 0.00005066
Iteration 126/1000 | Loss: 0.00005065
Iteration 127/1000 | Loss: 0.00005065
Iteration 128/1000 | Loss: 0.00005065
Iteration 129/1000 | Loss: 0.00005065
Iteration 130/1000 | Loss: 0.00005065
Iteration 131/1000 | Loss: 0.00005065
Iteration 132/1000 | Loss: 0.00005065
Iteration 133/1000 | Loss: 0.00005065
Iteration 134/1000 | Loss: 0.00005064
Iteration 135/1000 | Loss: 0.00005064
Iteration 136/1000 | Loss: 0.00005064
Iteration 137/1000 | Loss: 0.00005063
Iteration 138/1000 | Loss: 0.00005063
Iteration 139/1000 | Loss: 0.00005063
Iteration 140/1000 | Loss: 0.00005063
Iteration 141/1000 | Loss: 0.00005063
Iteration 142/1000 | Loss: 0.00005063
Iteration 143/1000 | Loss: 0.00005063
Iteration 144/1000 | Loss: 0.00005063
Iteration 145/1000 | Loss: 0.00005062
Iteration 146/1000 | Loss: 0.00005062
Iteration 147/1000 | Loss: 0.00005062
Iteration 148/1000 | Loss: 0.00005062
Iteration 149/1000 | Loss: 0.00005062
Iteration 150/1000 | Loss: 0.00005062
Iteration 151/1000 | Loss: 0.00005061
Iteration 152/1000 | Loss: 0.00005061
Iteration 153/1000 | Loss: 0.00005060
Iteration 154/1000 | Loss: 0.00005060
Iteration 155/1000 | Loss: 0.00005060
Iteration 156/1000 | Loss: 0.00005060
Iteration 157/1000 | Loss: 0.00005059
Iteration 158/1000 | Loss: 0.00005059
Iteration 159/1000 | Loss: 0.00005059
Iteration 160/1000 | Loss: 0.00005059
Iteration 161/1000 | Loss: 0.00005059
Iteration 162/1000 | Loss: 0.00005059
Iteration 163/1000 | Loss: 0.00005059
Iteration 164/1000 | Loss: 0.00005059
Iteration 165/1000 | Loss: 0.00005059
Iteration 166/1000 | Loss: 0.00005058
Iteration 167/1000 | Loss: 0.00005058
Iteration 168/1000 | Loss: 0.00005058
Iteration 169/1000 | Loss: 0.00005058
Iteration 170/1000 | Loss: 0.00005058
Iteration 171/1000 | Loss: 0.00005058
Iteration 172/1000 | Loss: 0.00005058
Iteration 173/1000 | Loss: 0.00005058
Iteration 174/1000 | Loss: 0.00005057
Iteration 175/1000 | Loss: 0.00005057
Iteration 176/1000 | Loss: 0.00005057
Iteration 177/1000 | Loss: 0.00005057
Iteration 178/1000 | Loss: 0.00005057
Iteration 179/1000 | Loss: 0.00005056
Iteration 180/1000 | Loss: 0.00005056
Iteration 181/1000 | Loss: 0.00005056
Iteration 182/1000 | Loss: 0.00005056
Iteration 183/1000 | Loss: 0.00005056
Iteration 184/1000 | Loss: 0.00005056
Iteration 185/1000 | Loss: 0.00005055
Iteration 186/1000 | Loss: 0.00005055
Iteration 187/1000 | Loss: 0.00005054
Iteration 188/1000 | Loss: 0.00005054
Iteration 189/1000 | Loss: 0.00005054
Iteration 190/1000 | Loss: 0.00005054
Iteration 191/1000 | Loss: 0.00005054
Iteration 192/1000 | Loss: 0.00005054
Iteration 193/1000 | Loss: 0.00005054
Iteration 194/1000 | Loss: 0.00005054
Iteration 195/1000 | Loss: 0.00005054
Iteration 196/1000 | Loss: 0.00005054
Iteration 197/1000 | Loss: 0.00005054
Iteration 198/1000 | Loss: 0.00005054
Iteration 199/1000 | Loss: 0.00005054
Iteration 200/1000 | Loss: 0.00005054
Iteration 201/1000 | Loss: 0.00005054
Iteration 202/1000 | Loss: 0.00005054
Iteration 203/1000 | Loss: 0.00005054
Iteration 204/1000 | Loss: 0.00005054
Iteration 205/1000 | Loss: 0.00005054
Iteration 206/1000 | Loss: 0.00005054
Iteration 207/1000 | Loss: 0.00005054
Iteration 208/1000 | Loss: 0.00005054
Iteration 209/1000 | Loss: 0.00005054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [5.053836139268242e-05, 5.053836139268242e-05, 5.053836139268242e-05, 5.053836139268242e-05, 5.053836139268242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.053836139268242e-05

Optimization complete. Final v2v error: 5.991330623626709 mm

Highest mean error: 10.463888168334961 mm for frame 142

Lowest mean error: 5.4860687255859375 mm for frame 206

Saving results

Total time: 179.9611496925354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941141
Iteration 2/25 | Loss: 0.00125144
Iteration 3/25 | Loss: 0.00096257
Iteration 4/25 | Loss: 0.00093199
Iteration 5/25 | Loss: 0.00092503
Iteration 6/25 | Loss: 0.00092318
Iteration 7/25 | Loss: 0.00092318
Iteration 8/25 | Loss: 0.00092318
Iteration 9/25 | Loss: 0.00092318
Iteration 10/25 | Loss: 0.00092318
Iteration 11/25 | Loss: 0.00092318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009231790318153799, 0.0009231790318153799, 0.0009231790318153799, 0.0009231790318153799, 0.0009231790318153799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009231790318153799

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27623808
Iteration 2/25 | Loss: 0.00056506
Iteration 3/25 | Loss: 0.00056505
Iteration 4/25 | Loss: 0.00056505
Iteration 5/25 | Loss: 0.00056505
Iteration 6/25 | Loss: 0.00056505
Iteration 7/25 | Loss: 0.00056505
Iteration 8/25 | Loss: 0.00056505
Iteration 9/25 | Loss: 0.00056505
Iteration 10/25 | Loss: 0.00056505
Iteration 11/25 | Loss: 0.00056505
Iteration 12/25 | Loss: 0.00056505
Iteration 13/25 | Loss: 0.00056505
Iteration 14/25 | Loss: 0.00056505
Iteration 15/25 | Loss: 0.00056505
Iteration 16/25 | Loss: 0.00056505
Iteration 17/25 | Loss: 0.00056505
Iteration 18/25 | Loss: 0.00056505
Iteration 19/25 | Loss: 0.00056505
Iteration 20/25 | Loss: 0.00056505
Iteration 21/25 | Loss: 0.00056505
Iteration 22/25 | Loss: 0.00056505
Iteration 23/25 | Loss: 0.00056505
Iteration 24/25 | Loss: 0.00056505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005650518578477204, 0.0005650518578477204, 0.0005650518578477204, 0.0005650518578477204, 0.0005650518578477204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005650518578477204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056505
Iteration 2/1000 | Loss: 0.00004878
Iteration 3/1000 | Loss: 0.00002744
Iteration 4/1000 | Loss: 0.00002538
Iteration 5/1000 | Loss: 0.00002398
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002283
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002233
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002232
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002228
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002215
Iteration 18/1000 | Loss: 0.00002214
Iteration 19/1000 | Loss: 0.00002214
Iteration 20/1000 | Loss: 0.00002209
Iteration 21/1000 | Loss: 0.00002209
Iteration 22/1000 | Loss: 0.00002208
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002207
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002207
Iteration 28/1000 | Loss: 0.00002206
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002205
Iteration 31/1000 | Loss: 0.00002205
Iteration 32/1000 | Loss: 0.00002204
Iteration 33/1000 | Loss: 0.00002203
Iteration 34/1000 | Loss: 0.00002203
Iteration 35/1000 | Loss: 0.00002203
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002203
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002202
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002194
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002192
Iteration 46/1000 | Loss: 0.00002191
Iteration 47/1000 | Loss: 0.00002191
Iteration 48/1000 | Loss: 0.00002190
Iteration 49/1000 | Loss: 0.00002190
Iteration 50/1000 | Loss: 0.00002190
Iteration 51/1000 | Loss: 0.00002190
Iteration 52/1000 | Loss: 0.00002190
Iteration 53/1000 | Loss: 0.00002189
Iteration 54/1000 | Loss: 0.00002189
Iteration 55/1000 | Loss: 0.00002189
Iteration 56/1000 | Loss: 0.00002189
Iteration 57/1000 | Loss: 0.00002188
Iteration 58/1000 | Loss: 0.00002188
Iteration 59/1000 | Loss: 0.00002187
Iteration 60/1000 | Loss: 0.00002187
Iteration 61/1000 | Loss: 0.00002187
Iteration 62/1000 | Loss: 0.00002187
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002186
Iteration 65/1000 | Loss: 0.00002186
Iteration 66/1000 | Loss: 0.00002185
Iteration 67/1000 | Loss: 0.00002185
Iteration 68/1000 | Loss: 0.00002185
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002184
Iteration 71/1000 | Loss: 0.00002184
Iteration 72/1000 | Loss: 0.00002184
Iteration 73/1000 | Loss: 0.00002184
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002182
Iteration 78/1000 | Loss: 0.00002182
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002182
Iteration 85/1000 | Loss: 0.00002182
Iteration 86/1000 | Loss: 0.00002181
Iteration 87/1000 | Loss: 0.00002181
Iteration 88/1000 | Loss: 0.00002181
Iteration 89/1000 | Loss: 0.00002181
Iteration 90/1000 | Loss: 0.00002181
Iteration 91/1000 | Loss: 0.00002181
Iteration 92/1000 | Loss: 0.00002181
Iteration 93/1000 | Loss: 0.00002180
Iteration 94/1000 | Loss: 0.00002180
Iteration 95/1000 | Loss: 0.00002180
Iteration 96/1000 | Loss: 0.00002180
Iteration 97/1000 | Loss: 0.00002179
Iteration 98/1000 | Loss: 0.00002179
Iteration 99/1000 | Loss: 0.00002179
Iteration 100/1000 | Loss: 0.00002179
Iteration 101/1000 | Loss: 0.00002179
Iteration 102/1000 | Loss: 0.00002179
Iteration 103/1000 | Loss: 0.00002179
Iteration 104/1000 | Loss: 0.00002179
Iteration 105/1000 | Loss: 0.00002179
Iteration 106/1000 | Loss: 0.00002179
Iteration 107/1000 | Loss: 0.00002179
Iteration 108/1000 | Loss: 0.00002179
Iteration 109/1000 | Loss: 0.00002178
Iteration 110/1000 | Loss: 0.00002178
Iteration 111/1000 | Loss: 0.00002177
Iteration 112/1000 | Loss: 0.00002177
Iteration 113/1000 | Loss: 0.00002177
Iteration 114/1000 | Loss: 0.00002177
Iteration 115/1000 | Loss: 0.00002177
Iteration 116/1000 | Loss: 0.00002177
Iteration 117/1000 | Loss: 0.00002177
Iteration 118/1000 | Loss: 0.00002177
Iteration 119/1000 | Loss: 0.00002177
Iteration 120/1000 | Loss: 0.00002177
Iteration 121/1000 | Loss: 0.00002177
Iteration 122/1000 | Loss: 0.00002177
Iteration 123/1000 | Loss: 0.00002177
Iteration 124/1000 | Loss: 0.00002177
Iteration 125/1000 | Loss: 0.00002177
Iteration 126/1000 | Loss: 0.00002176
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002176
Iteration 129/1000 | Loss: 0.00002176
Iteration 130/1000 | Loss: 0.00002176
Iteration 131/1000 | Loss: 0.00002176
Iteration 132/1000 | Loss: 0.00002176
Iteration 133/1000 | Loss: 0.00002176
Iteration 134/1000 | Loss: 0.00002176
Iteration 135/1000 | Loss: 0.00002176
Iteration 136/1000 | Loss: 0.00002176
Iteration 137/1000 | Loss: 0.00002176
Iteration 138/1000 | Loss: 0.00002176
Iteration 139/1000 | Loss: 0.00002176
Iteration 140/1000 | Loss: 0.00002176
Iteration 141/1000 | Loss: 0.00002176
Iteration 142/1000 | Loss: 0.00002176
Iteration 143/1000 | Loss: 0.00002176
Iteration 144/1000 | Loss: 0.00002176
Iteration 145/1000 | Loss: 0.00002176
Iteration 146/1000 | Loss: 0.00002176
Iteration 147/1000 | Loss: 0.00002176
Iteration 148/1000 | Loss: 0.00002176
Iteration 149/1000 | Loss: 0.00002176
Iteration 150/1000 | Loss: 0.00002176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1756282876594923e-05, 2.1756282876594923e-05, 2.1756282876594923e-05, 2.1756282876594923e-05, 2.1756282876594923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1756282876594923e-05

Optimization complete. Final v2v error: 4.046726226806641 mm

Highest mean error: 4.558050155639648 mm for frame 0

Lowest mean error: 3.611982822418213 mm for frame 187

Saving results

Total time: 39.28585433959961
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862590
Iteration 2/25 | Loss: 0.00130855
Iteration 3/25 | Loss: 0.00096665
Iteration 4/25 | Loss: 0.00092033
Iteration 5/25 | Loss: 0.00091432
Iteration 6/25 | Loss: 0.00091396
Iteration 7/25 | Loss: 0.00091396
Iteration 8/25 | Loss: 0.00091396
Iteration 9/25 | Loss: 0.00091396
Iteration 10/25 | Loss: 0.00091396
Iteration 11/25 | Loss: 0.00091396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009139642352238297, 0.0009139642352238297, 0.0009139642352238297, 0.0009139642352238297, 0.0009139642352238297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009139642352238297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14344990
Iteration 2/25 | Loss: 0.00038788
Iteration 3/25 | Loss: 0.00038788
Iteration 4/25 | Loss: 0.00038788
Iteration 5/25 | Loss: 0.00038788
Iteration 6/25 | Loss: 0.00038788
Iteration 7/25 | Loss: 0.00038788
Iteration 8/25 | Loss: 0.00038788
Iteration 9/25 | Loss: 0.00038788
Iteration 10/25 | Loss: 0.00038788
Iteration 11/25 | Loss: 0.00038788
Iteration 12/25 | Loss: 0.00038788
Iteration 13/25 | Loss: 0.00038788
Iteration 14/25 | Loss: 0.00038788
Iteration 15/25 | Loss: 0.00038788
Iteration 16/25 | Loss: 0.00038788
Iteration 17/25 | Loss: 0.00038788
Iteration 18/25 | Loss: 0.00038788
Iteration 19/25 | Loss: 0.00038788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00038788007805123925, 0.00038788007805123925, 0.00038788007805123925, 0.00038788007805123925, 0.00038788007805123925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038788007805123925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038788
Iteration 2/1000 | Loss: 0.00003932
Iteration 3/1000 | Loss: 0.00002962
Iteration 4/1000 | Loss: 0.00002807
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002668
Iteration 7/1000 | Loss: 0.00002635
Iteration 8/1000 | Loss: 0.00002598
Iteration 9/1000 | Loss: 0.00002586
Iteration 10/1000 | Loss: 0.00002572
Iteration 11/1000 | Loss: 0.00002557
Iteration 12/1000 | Loss: 0.00002557
Iteration 13/1000 | Loss: 0.00002555
Iteration 14/1000 | Loss: 0.00002551
Iteration 15/1000 | Loss: 0.00002550
Iteration 16/1000 | Loss: 0.00002550
Iteration 17/1000 | Loss: 0.00002545
Iteration 18/1000 | Loss: 0.00002545
Iteration 19/1000 | Loss: 0.00002545
Iteration 20/1000 | Loss: 0.00002543
Iteration 21/1000 | Loss: 0.00002543
Iteration 22/1000 | Loss: 0.00002543
Iteration 23/1000 | Loss: 0.00002541
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002538
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002538
Iteration 30/1000 | Loss: 0.00002538
Iteration 31/1000 | Loss: 0.00002538
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00002538
Iteration 34/1000 | Loss: 0.00002537
Iteration 35/1000 | Loss: 0.00002537
Iteration 36/1000 | Loss: 0.00002534
Iteration 37/1000 | Loss: 0.00002534
Iteration 38/1000 | Loss: 0.00002534
Iteration 39/1000 | Loss: 0.00002534
Iteration 40/1000 | Loss: 0.00002534
Iteration 41/1000 | Loss: 0.00002533
Iteration 42/1000 | Loss: 0.00002532
Iteration 43/1000 | Loss: 0.00002532
Iteration 44/1000 | Loss: 0.00002532
Iteration 45/1000 | Loss: 0.00002531
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002531
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002531
Iteration 50/1000 | Loss: 0.00002530
Iteration 51/1000 | Loss: 0.00002530
Iteration 52/1000 | Loss: 0.00002530
Iteration 53/1000 | Loss: 0.00002530
Iteration 54/1000 | Loss: 0.00002529
Iteration 55/1000 | Loss: 0.00002527
Iteration 56/1000 | Loss: 0.00002526
Iteration 57/1000 | Loss: 0.00002526
Iteration 58/1000 | Loss: 0.00002526
Iteration 59/1000 | Loss: 0.00002525
Iteration 60/1000 | Loss: 0.00002524
Iteration 61/1000 | Loss: 0.00002524
Iteration 62/1000 | Loss: 0.00002524
Iteration 63/1000 | Loss: 0.00002524
Iteration 64/1000 | Loss: 0.00002524
Iteration 65/1000 | Loss: 0.00002524
Iteration 66/1000 | Loss: 0.00002524
Iteration 67/1000 | Loss: 0.00002524
Iteration 68/1000 | Loss: 0.00002523
Iteration 69/1000 | Loss: 0.00002523
Iteration 70/1000 | Loss: 0.00002523
Iteration 71/1000 | Loss: 0.00002523
Iteration 72/1000 | Loss: 0.00002523
Iteration 73/1000 | Loss: 0.00002523
Iteration 74/1000 | Loss: 0.00002523
Iteration 75/1000 | Loss: 0.00002523
Iteration 76/1000 | Loss: 0.00002523
Iteration 77/1000 | Loss: 0.00002523
Iteration 78/1000 | Loss: 0.00002523
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002523
Iteration 81/1000 | Loss: 0.00002523
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00002523
Iteration 84/1000 | Loss: 0.00002523
Iteration 85/1000 | Loss: 0.00002523
Iteration 86/1000 | Loss: 0.00002523
Iteration 87/1000 | Loss: 0.00002523
Iteration 88/1000 | Loss: 0.00002523
Iteration 89/1000 | Loss: 0.00002523
Iteration 90/1000 | Loss: 0.00002523
Iteration 91/1000 | Loss: 0.00002523
Iteration 92/1000 | Loss: 0.00002523
Iteration 93/1000 | Loss: 0.00002523
Iteration 94/1000 | Loss: 0.00002523
Iteration 95/1000 | Loss: 0.00002523
Iteration 96/1000 | Loss: 0.00002523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.5230261599062942e-05, 2.5230261599062942e-05, 2.5230261599062942e-05, 2.5230261599062942e-05, 2.5230261599062942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5230261599062942e-05

Optimization complete. Final v2v error: 4.344904899597168 mm

Highest mean error: 4.68265438079834 mm for frame 209

Lowest mean error: 4.1308441162109375 mm for frame 98

Saving results

Total time: 33.85559892654419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821514
Iteration 2/25 | Loss: 0.00146623
Iteration 3/25 | Loss: 0.00122911
Iteration 4/25 | Loss: 0.00116225
Iteration 5/25 | Loss: 0.00111244
Iteration 6/25 | Loss: 0.00110227
Iteration 7/25 | Loss: 0.00109943
Iteration 8/25 | Loss: 0.00109812
Iteration 9/25 | Loss: 0.00109603
Iteration 10/25 | Loss: 0.00109548
Iteration 11/25 | Loss: 0.00109509
Iteration 12/25 | Loss: 0.00109471
Iteration 13/25 | Loss: 0.00109429
Iteration 14/25 | Loss: 0.00109411
Iteration 15/25 | Loss: 0.00109346
Iteration 16/25 | Loss: 0.00109240
Iteration 17/25 | Loss: 0.00109176
Iteration 18/25 | Loss: 0.00109417
Iteration 19/25 | Loss: 0.00109390
Iteration 20/25 | Loss: 0.00109388
Iteration 21/25 | Loss: 0.00109455
Iteration 22/25 | Loss: 0.00109367
Iteration 23/25 | Loss: 0.00109404
Iteration 24/25 | Loss: 0.00109309
Iteration 25/25 | Loss: 0.00109321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86915731
Iteration 2/25 | Loss: 0.00119581
Iteration 3/25 | Loss: 0.00119581
Iteration 4/25 | Loss: 0.00119581
Iteration 5/25 | Loss: 0.00119581
Iteration 6/25 | Loss: 0.00119581
Iteration 7/25 | Loss: 0.00119581
Iteration 8/25 | Loss: 0.00119581
Iteration 9/25 | Loss: 0.00119580
Iteration 10/25 | Loss: 0.00119580
Iteration 11/25 | Loss: 0.00119581
Iteration 12/25 | Loss: 0.00119581
Iteration 13/25 | Loss: 0.00119581
Iteration 14/25 | Loss: 0.00119581
Iteration 15/25 | Loss: 0.00119581
Iteration 16/25 | Loss: 0.00119581
Iteration 17/25 | Loss: 0.00119581
Iteration 18/25 | Loss: 0.00119581
Iteration 19/25 | Loss: 0.00119581
Iteration 20/25 | Loss: 0.00119581
Iteration 21/25 | Loss: 0.00119581
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011958052637055516, 0.0011958052637055516, 0.0011958052637055516, 0.0011958052637055516, 0.0011958052637055516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011958052637055516

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119581
Iteration 2/1000 | Loss: 0.00053791
Iteration 3/1000 | Loss: 0.00032904
Iteration 4/1000 | Loss: 0.00022248
Iteration 5/1000 | Loss: 0.00033956
Iteration 6/1000 | Loss: 0.00015786
Iteration 7/1000 | Loss: 0.00017517
Iteration 8/1000 | Loss: 0.00088044
Iteration 9/1000 | Loss: 0.00123744
Iteration 10/1000 | Loss: 0.00056007
Iteration 11/1000 | Loss: 0.00095238
Iteration 12/1000 | Loss: 0.00096930
Iteration 13/1000 | Loss: 0.00123958
Iteration 14/1000 | Loss: 0.00053488
Iteration 15/1000 | Loss: 0.00054152
Iteration 16/1000 | Loss: 0.00008957
Iteration 17/1000 | Loss: 0.00008375
Iteration 18/1000 | Loss: 0.00006190
Iteration 19/1000 | Loss: 0.00017461
Iteration 20/1000 | Loss: 0.00020525
Iteration 21/1000 | Loss: 0.00052935
Iteration 22/1000 | Loss: 0.00019546
Iteration 23/1000 | Loss: 0.00052415
Iteration 24/1000 | Loss: 0.00015017
Iteration 25/1000 | Loss: 0.00021952
Iteration 26/1000 | Loss: 0.00009391
Iteration 27/1000 | Loss: 0.00022259
Iteration 28/1000 | Loss: 0.00020416
Iteration 29/1000 | Loss: 0.00013817
Iteration 30/1000 | Loss: 0.00016806
Iteration 31/1000 | Loss: 0.00019232
Iteration 32/1000 | Loss: 0.00006774
Iteration 33/1000 | Loss: 0.00015379
Iteration 34/1000 | Loss: 0.00019659
Iteration 35/1000 | Loss: 0.00004985
Iteration 36/1000 | Loss: 0.00004596
Iteration 37/1000 | Loss: 0.00004251
Iteration 38/1000 | Loss: 0.00004134
Iteration 39/1000 | Loss: 0.00004039
Iteration 40/1000 | Loss: 0.00003981
Iteration 41/1000 | Loss: 0.00003921
Iteration 42/1000 | Loss: 0.00003887
Iteration 43/1000 | Loss: 0.00003859
Iteration 44/1000 | Loss: 0.00003831
Iteration 45/1000 | Loss: 0.00003809
Iteration 46/1000 | Loss: 0.00003797
Iteration 47/1000 | Loss: 0.00003792
Iteration 48/1000 | Loss: 0.00003792
Iteration 49/1000 | Loss: 0.00003791
Iteration 50/1000 | Loss: 0.00003788
Iteration 51/1000 | Loss: 0.00003788
Iteration 52/1000 | Loss: 0.00003787
Iteration 53/1000 | Loss: 0.00003787
Iteration 54/1000 | Loss: 0.00003786
Iteration 55/1000 | Loss: 0.00003786
Iteration 56/1000 | Loss: 0.00003785
Iteration 57/1000 | Loss: 0.00003785
Iteration 58/1000 | Loss: 0.00003783
Iteration 59/1000 | Loss: 0.00003782
Iteration 60/1000 | Loss: 0.00003782
Iteration 61/1000 | Loss: 0.00003779
Iteration 62/1000 | Loss: 0.00003779
Iteration 63/1000 | Loss: 0.00003778
Iteration 64/1000 | Loss: 0.00003778
Iteration 65/1000 | Loss: 0.00003777
Iteration 66/1000 | Loss: 0.00003777
Iteration 67/1000 | Loss: 0.00003777
Iteration 68/1000 | Loss: 0.00003775
Iteration 69/1000 | Loss: 0.00003773
Iteration 70/1000 | Loss: 0.00003772
Iteration 71/1000 | Loss: 0.00003772
Iteration 72/1000 | Loss: 0.00003770
Iteration 73/1000 | Loss: 0.00003769
Iteration 74/1000 | Loss: 0.00003768
Iteration 75/1000 | Loss: 0.00003765
Iteration 76/1000 | Loss: 0.00003763
Iteration 77/1000 | Loss: 0.00003763
Iteration 78/1000 | Loss: 0.00003762
Iteration 79/1000 | Loss: 0.00003762
Iteration 80/1000 | Loss: 0.00003762
Iteration 81/1000 | Loss: 0.00003761
Iteration 82/1000 | Loss: 0.00003761
Iteration 83/1000 | Loss: 0.00003761
Iteration 84/1000 | Loss: 0.00003760
Iteration 85/1000 | Loss: 0.00003760
Iteration 86/1000 | Loss: 0.00003760
Iteration 87/1000 | Loss: 0.00003759
Iteration 88/1000 | Loss: 0.00003759
Iteration 89/1000 | Loss: 0.00003759
Iteration 90/1000 | Loss: 0.00003759
Iteration 91/1000 | Loss: 0.00003759
Iteration 92/1000 | Loss: 0.00003758
Iteration 93/1000 | Loss: 0.00003758
Iteration 94/1000 | Loss: 0.00003758
Iteration 95/1000 | Loss: 0.00003758
Iteration 96/1000 | Loss: 0.00003758
Iteration 97/1000 | Loss: 0.00003757
Iteration 98/1000 | Loss: 0.00003757
Iteration 99/1000 | Loss: 0.00003756
Iteration 100/1000 | Loss: 0.00003756
Iteration 101/1000 | Loss: 0.00003756
Iteration 102/1000 | Loss: 0.00003756
Iteration 103/1000 | Loss: 0.00003755
Iteration 104/1000 | Loss: 0.00003755
Iteration 105/1000 | Loss: 0.00003755
Iteration 106/1000 | Loss: 0.00003754
Iteration 107/1000 | Loss: 0.00003754
Iteration 108/1000 | Loss: 0.00003754
Iteration 109/1000 | Loss: 0.00003753
Iteration 110/1000 | Loss: 0.00003753
Iteration 111/1000 | Loss: 0.00003753
Iteration 112/1000 | Loss: 0.00003753
Iteration 113/1000 | Loss: 0.00003753
Iteration 114/1000 | Loss: 0.00003753
Iteration 115/1000 | Loss: 0.00003752
Iteration 116/1000 | Loss: 0.00003752
Iteration 117/1000 | Loss: 0.00003752
Iteration 118/1000 | Loss: 0.00003752
Iteration 119/1000 | Loss: 0.00003752
Iteration 120/1000 | Loss: 0.00003752
Iteration 121/1000 | Loss: 0.00003752
Iteration 122/1000 | Loss: 0.00003752
Iteration 123/1000 | Loss: 0.00003752
Iteration 124/1000 | Loss: 0.00003752
Iteration 125/1000 | Loss: 0.00003751
Iteration 126/1000 | Loss: 0.00003751
Iteration 127/1000 | Loss: 0.00003751
Iteration 128/1000 | Loss: 0.00003751
Iteration 129/1000 | Loss: 0.00003751
Iteration 130/1000 | Loss: 0.00003751
Iteration 131/1000 | Loss: 0.00003751
Iteration 132/1000 | Loss: 0.00003750
Iteration 133/1000 | Loss: 0.00003750
Iteration 134/1000 | Loss: 0.00003750
Iteration 135/1000 | Loss: 0.00003750
Iteration 136/1000 | Loss: 0.00003750
Iteration 137/1000 | Loss: 0.00003750
Iteration 138/1000 | Loss: 0.00003749
Iteration 139/1000 | Loss: 0.00003749
Iteration 140/1000 | Loss: 0.00003749
Iteration 141/1000 | Loss: 0.00003749
Iteration 142/1000 | Loss: 0.00003749
Iteration 143/1000 | Loss: 0.00003749
Iteration 144/1000 | Loss: 0.00003749
Iteration 145/1000 | Loss: 0.00003749
Iteration 146/1000 | Loss: 0.00003749
Iteration 147/1000 | Loss: 0.00003749
Iteration 148/1000 | Loss: 0.00003749
Iteration 149/1000 | Loss: 0.00003749
Iteration 150/1000 | Loss: 0.00003749
Iteration 151/1000 | Loss: 0.00003749
Iteration 152/1000 | Loss: 0.00003749
Iteration 153/1000 | Loss: 0.00003749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.749065945157781e-05, 3.749065945157781e-05, 3.749065945157781e-05, 3.749065945157781e-05, 3.749065945157781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.749065945157781e-05

Optimization complete. Final v2v error: 5.173511028289795 mm

Highest mean error: 5.609105110168457 mm for frame 8

Lowest mean error: 4.574165344238281 mm for frame 213

Saving results

Total time: 137.76501846313477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872994
Iteration 2/25 | Loss: 0.00129205
Iteration 3/25 | Loss: 0.00097469
Iteration 4/25 | Loss: 0.00094697
Iteration 5/25 | Loss: 0.00093746
Iteration 6/25 | Loss: 0.00093555
Iteration 7/25 | Loss: 0.00093516
Iteration 8/25 | Loss: 0.00093516
Iteration 9/25 | Loss: 0.00093516
Iteration 10/25 | Loss: 0.00093516
Iteration 11/25 | Loss: 0.00093516
Iteration 12/25 | Loss: 0.00093516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009351643384434283, 0.0009351643384434283, 0.0009351643384434283, 0.0009351643384434283, 0.0009351643384434283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009351643384434283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10751760
Iteration 2/25 | Loss: 0.00070312
Iteration 3/25 | Loss: 0.00070312
Iteration 4/25 | Loss: 0.00070311
Iteration 5/25 | Loss: 0.00070311
Iteration 6/25 | Loss: 0.00070311
Iteration 7/25 | Loss: 0.00070311
Iteration 8/25 | Loss: 0.00070311
Iteration 9/25 | Loss: 0.00070311
Iteration 10/25 | Loss: 0.00070311
Iteration 11/25 | Loss: 0.00070311
Iteration 12/25 | Loss: 0.00070311
Iteration 13/25 | Loss: 0.00070311
Iteration 14/25 | Loss: 0.00070311
Iteration 15/25 | Loss: 0.00070311
Iteration 16/25 | Loss: 0.00070311
Iteration 17/25 | Loss: 0.00070311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007031125132925808, 0.0007031125132925808, 0.0007031125132925808, 0.0007031125132925808, 0.0007031125132925808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007031125132925808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070311
Iteration 2/1000 | Loss: 0.00010508
Iteration 3/1000 | Loss: 0.00006404
Iteration 4/1000 | Loss: 0.00004242
Iteration 5/1000 | Loss: 0.00003718
Iteration 6/1000 | Loss: 0.00003489
Iteration 7/1000 | Loss: 0.00003301
Iteration 8/1000 | Loss: 0.00003198
Iteration 9/1000 | Loss: 0.00003112
Iteration 10/1000 | Loss: 0.00003067
Iteration 11/1000 | Loss: 0.00003042
Iteration 12/1000 | Loss: 0.00003041
Iteration 13/1000 | Loss: 0.00003022
Iteration 14/1000 | Loss: 0.00003004
Iteration 15/1000 | Loss: 0.00002999
Iteration 16/1000 | Loss: 0.00002983
Iteration 17/1000 | Loss: 0.00002980
Iteration 18/1000 | Loss: 0.00002967
Iteration 19/1000 | Loss: 0.00002966
Iteration 20/1000 | Loss: 0.00002965
Iteration 21/1000 | Loss: 0.00002965
Iteration 22/1000 | Loss: 0.00002965
Iteration 23/1000 | Loss: 0.00002965
Iteration 24/1000 | Loss: 0.00002965
Iteration 25/1000 | Loss: 0.00002964
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002964
Iteration 29/1000 | Loss: 0.00002964
Iteration 30/1000 | Loss: 0.00002964
Iteration 31/1000 | Loss: 0.00002964
Iteration 32/1000 | Loss: 0.00002964
Iteration 33/1000 | Loss: 0.00002964
Iteration 34/1000 | Loss: 0.00002964
Iteration 35/1000 | Loss: 0.00002964
Iteration 36/1000 | Loss: 0.00002963
Iteration 37/1000 | Loss: 0.00002963
Iteration 38/1000 | Loss: 0.00002963
Iteration 39/1000 | Loss: 0.00002963
Iteration 40/1000 | Loss: 0.00002962
Iteration 41/1000 | Loss: 0.00002962
Iteration 42/1000 | Loss: 0.00002962
Iteration 43/1000 | Loss: 0.00002961
Iteration 44/1000 | Loss: 0.00002960
Iteration 45/1000 | Loss: 0.00002960
Iteration 46/1000 | Loss: 0.00002960
Iteration 47/1000 | Loss: 0.00002959
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002959
Iteration 50/1000 | Loss: 0.00002959
Iteration 51/1000 | Loss: 0.00002958
Iteration 52/1000 | Loss: 0.00002958
Iteration 53/1000 | Loss: 0.00002958
Iteration 54/1000 | Loss: 0.00002957
Iteration 55/1000 | Loss: 0.00002957
Iteration 56/1000 | Loss: 0.00002957
Iteration 57/1000 | Loss: 0.00002957
Iteration 58/1000 | Loss: 0.00002957
Iteration 59/1000 | Loss: 0.00002957
Iteration 60/1000 | Loss: 0.00002956
Iteration 61/1000 | Loss: 0.00002956
Iteration 62/1000 | Loss: 0.00002956
Iteration 63/1000 | Loss: 0.00002956
Iteration 64/1000 | Loss: 0.00002956
Iteration 65/1000 | Loss: 0.00002955
Iteration 66/1000 | Loss: 0.00002955
Iteration 67/1000 | Loss: 0.00002954
Iteration 68/1000 | Loss: 0.00002954
Iteration 69/1000 | Loss: 0.00002953
Iteration 70/1000 | Loss: 0.00002951
Iteration 71/1000 | Loss: 0.00002951
Iteration 72/1000 | Loss: 0.00002950
Iteration 73/1000 | Loss: 0.00002949
Iteration 74/1000 | Loss: 0.00002948
Iteration 75/1000 | Loss: 0.00002948
Iteration 76/1000 | Loss: 0.00002948
Iteration 77/1000 | Loss: 0.00002948
Iteration 78/1000 | Loss: 0.00002948
Iteration 79/1000 | Loss: 0.00002947
Iteration 80/1000 | Loss: 0.00002947
Iteration 81/1000 | Loss: 0.00002947
Iteration 82/1000 | Loss: 0.00002946
Iteration 83/1000 | Loss: 0.00002946
Iteration 84/1000 | Loss: 0.00002946
Iteration 85/1000 | Loss: 0.00002946
Iteration 86/1000 | Loss: 0.00002945
Iteration 87/1000 | Loss: 0.00002945
Iteration 88/1000 | Loss: 0.00002945
Iteration 89/1000 | Loss: 0.00002945
Iteration 90/1000 | Loss: 0.00002945
Iteration 91/1000 | Loss: 0.00002945
Iteration 92/1000 | Loss: 0.00002945
Iteration 93/1000 | Loss: 0.00002945
Iteration 94/1000 | Loss: 0.00002945
Iteration 95/1000 | Loss: 0.00002945
Iteration 96/1000 | Loss: 0.00002945
Iteration 97/1000 | Loss: 0.00002945
Iteration 98/1000 | Loss: 0.00002944
Iteration 99/1000 | Loss: 0.00002944
Iteration 100/1000 | Loss: 0.00002944
Iteration 101/1000 | Loss: 0.00002944
Iteration 102/1000 | Loss: 0.00002944
Iteration 103/1000 | Loss: 0.00002943
Iteration 104/1000 | Loss: 0.00002943
Iteration 105/1000 | Loss: 0.00002943
Iteration 106/1000 | Loss: 0.00002943
Iteration 107/1000 | Loss: 0.00002943
Iteration 108/1000 | Loss: 0.00002943
Iteration 109/1000 | Loss: 0.00002942
Iteration 110/1000 | Loss: 0.00002942
Iteration 111/1000 | Loss: 0.00002942
Iteration 112/1000 | Loss: 0.00002942
Iteration 113/1000 | Loss: 0.00002942
Iteration 114/1000 | Loss: 0.00002942
Iteration 115/1000 | Loss: 0.00002941
Iteration 116/1000 | Loss: 0.00002941
Iteration 117/1000 | Loss: 0.00002941
Iteration 118/1000 | Loss: 0.00002941
Iteration 119/1000 | Loss: 0.00002941
Iteration 120/1000 | Loss: 0.00002940
Iteration 121/1000 | Loss: 0.00002940
Iteration 122/1000 | Loss: 0.00002940
Iteration 123/1000 | Loss: 0.00002940
Iteration 124/1000 | Loss: 0.00002940
Iteration 125/1000 | Loss: 0.00002940
Iteration 126/1000 | Loss: 0.00002940
Iteration 127/1000 | Loss: 0.00002940
Iteration 128/1000 | Loss: 0.00002939
Iteration 129/1000 | Loss: 0.00002939
Iteration 130/1000 | Loss: 0.00002939
Iteration 131/1000 | Loss: 0.00002939
Iteration 132/1000 | Loss: 0.00002938
Iteration 133/1000 | Loss: 0.00002938
Iteration 134/1000 | Loss: 0.00002938
Iteration 135/1000 | Loss: 0.00002938
Iteration 136/1000 | Loss: 0.00002938
Iteration 137/1000 | Loss: 0.00002937
Iteration 138/1000 | Loss: 0.00002937
Iteration 139/1000 | Loss: 0.00002937
Iteration 140/1000 | Loss: 0.00002937
Iteration 141/1000 | Loss: 0.00002937
Iteration 142/1000 | Loss: 0.00002937
Iteration 143/1000 | Loss: 0.00002937
Iteration 144/1000 | Loss: 0.00002937
Iteration 145/1000 | Loss: 0.00002937
Iteration 146/1000 | Loss: 0.00002937
Iteration 147/1000 | Loss: 0.00002937
Iteration 148/1000 | Loss: 0.00002937
Iteration 149/1000 | Loss: 0.00002937
Iteration 150/1000 | Loss: 0.00002937
Iteration 151/1000 | Loss: 0.00002937
Iteration 152/1000 | Loss: 0.00002937
Iteration 153/1000 | Loss: 0.00002936
Iteration 154/1000 | Loss: 0.00002936
Iteration 155/1000 | Loss: 0.00002936
Iteration 156/1000 | Loss: 0.00002936
Iteration 157/1000 | Loss: 0.00002936
Iteration 158/1000 | Loss: 0.00002936
Iteration 159/1000 | Loss: 0.00002936
Iteration 160/1000 | Loss: 0.00002936
Iteration 161/1000 | Loss: 0.00002936
Iteration 162/1000 | Loss: 0.00002936
Iteration 163/1000 | Loss: 0.00002936
Iteration 164/1000 | Loss: 0.00002935
Iteration 165/1000 | Loss: 0.00002935
Iteration 166/1000 | Loss: 0.00002935
Iteration 167/1000 | Loss: 0.00002935
Iteration 168/1000 | Loss: 0.00002935
Iteration 169/1000 | Loss: 0.00002935
Iteration 170/1000 | Loss: 0.00002935
Iteration 171/1000 | Loss: 0.00002935
Iteration 172/1000 | Loss: 0.00002935
Iteration 173/1000 | Loss: 0.00002935
Iteration 174/1000 | Loss: 0.00002935
Iteration 175/1000 | Loss: 0.00002935
Iteration 176/1000 | Loss: 0.00002935
Iteration 177/1000 | Loss: 0.00002934
Iteration 178/1000 | Loss: 0.00002934
Iteration 179/1000 | Loss: 0.00002934
Iteration 180/1000 | Loss: 0.00002934
Iteration 181/1000 | Loss: 0.00002934
Iteration 182/1000 | Loss: 0.00002934
Iteration 183/1000 | Loss: 0.00002934
Iteration 184/1000 | Loss: 0.00002934
Iteration 185/1000 | Loss: 0.00002934
Iteration 186/1000 | Loss: 0.00002934
Iteration 187/1000 | Loss: 0.00002934
Iteration 188/1000 | Loss: 0.00002934
Iteration 189/1000 | Loss: 0.00002934
Iteration 190/1000 | Loss: 0.00002934
Iteration 191/1000 | Loss: 0.00002934
Iteration 192/1000 | Loss: 0.00002933
Iteration 193/1000 | Loss: 0.00002933
Iteration 194/1000 | Loss: 0.00002933
Iteration 195/1000 | Loss: 0.00002933
Iteration 196/1000 | Loss: 0.00002932
Iteration 197/1000 | Loss: 0.00002932
Iteration 198/1000 | Loss: 0.00002932
Iteration 199/1000 | Loss: 0.00002932
Iteration 200/1000 | Loss: 0.00002932
Iteration 201/1000 | Loss: 0.00002932
Iteration 202/1000 | Loss: 0.00002931
Iteration 203/1000 | Loss: 0.00002931
Iteration 204/1000 | Loss: 0.00002931
Iteration 205/1000 | Loss: 0.00002931
Iteration 206/1000 | Loss: 0.00002931
Iteration 207/1000 | Loss: 0.00002931
Iteration 208/1000 | Loss: 0.00002931
Iteration 209/1000 | Loss: 0.00002931
Iteration 210/1000 | Loss: 0.00002931
Iteration 211/1000 | Loss: 0.00002931
Iteration 212/1000 | Loss: 0.00002931
Iteration 213/1000 | Loss: 0.00002931
Iteration 214/1000 | Loss: 0.00002931
Iteration 215/1000 | Loss: 0.00002931
Iteration 216/1000 | Loss: 0.00002931
Iteration 217/1000 | Loss: 0.00002931
Iteration 218/1000 | Loss: 0.00002931
Iteration 219/1000 | Loss: 0.00002931
Iteration 220/1000 | Loss: 0.00002931
Iteration 221/1000 | Loss: 0.00002931
Iteration 222/1000 | Loss: 0.00002931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [2.930706978077069e-05, 2.930706978077069e-05, 2.930706978077069e-05, 2.930706978077069e-05, 2.930706978077069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.930706978077069e-05

Optimization complete. Final v2v error: 4.65150260925293 mm

Highest mean error: 6.054624557495117 mm for frame 62

Lowest mean error: 4.053671836853027 mm for frame 90

Saving results

Total time: 47.012603759765625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512278
Iteration 2/25 | Loss: 0.00132412
Iteration 3/25 | Loss: 0.00102206
Iteration 4/25 | Loss: 0.00098102
Iteration 5/25 | Loss: 0.00096328
Iteration 6/25 | Loss: 0.00096003
Iteration 7/25 | Loss: 0.00095984
Iteration 8/25 | Loss: 0.00095984
Iteration 9/25 | Loss: 0.00095984
Iteration 10/25 | Loss: 0.00095984
Iteration 11/25 | Loss: 0.00095984
Iteration 12/25 | Loss: 0.00095984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009598378092050552, 0.0009598378092050552, 0.0009598378092050552, 0.0009598378092050552, 0.0009598378092050552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009598378092050552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68712300
Iteration 2/25 | Loss: 0.00076698
Iteration 3/25 | Loss: 0.00076698
Iteration 4/25 | Loss: 0.00076698
Iteration 5/25 | Loss: 0.00076698
Iteration 6/25 | Loss: 0.00076698
Iteration 7/25 | Loss: 0.00076698
Iteration 8/25 | Loss: 0.00076698
Iteration 9/25 | Loss: 0.00076698
Iteration 10/25 | Loss: 0.00076698
Iteration 11/25 | Loss: 0.00076698
Iteration 12/25 | Loss: 0.00076698
Iteration 13/25 | Loss: 0.00076698
Iteration 14/25 | Loss: 0.00076698
Iteration 15/25 | Loss: 0.00076698
Iteration 16/25 | Loss: 0.00076698
Iteration 17/25 | Loss: 0.00076698
Iteration 18/25 | Loss: 0.00076698
Iteration 19/25 | Loss: 0.00076698
Iteration 20/25 | Loss: 0.00076698
Iteration 21/25 | Loss: 0.00076698
Iteration 22/25 | Loss: 0.00076698
Iteration 23/25 | Loss: 0.00076698
Iteration 24/25 | Loss: 0.00076698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007669752230867743, 0.0007669752230867743, 0.0007669752230867743, 0.0007669752230867743, 0.0007669752230867743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007669752230867743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076698
Iteration 2/1000 | Loss: 0.00008542
Iteration 3/1000 | Loss: 0.00005310
Iteration 4/1000 | Loss: 0.00004338
Iteration 5/1000 | Loss: 0.00004047
Iteration 6/1000 | Loss: 0.00003900
Iteration 7/1000 | Loss: 0.00003749
Iteration 8/1000 | Loss: 0.00003655
Iteration 9/1000 | Loss: 0.00003574
Iteration 10/1000 | Loss: 0.00003509
Iteration 11/1000 | Loss: 0.00003476
Iteration 12/1000 | Loss: 0.00003447
Iteration 13/1000 | Loss: 0.00003436
Iteration 14/1000 | Loss: 0.00003416
Iteration 15/1000 | Loss: 0.00003399
Iteration 16/1000 | Loss: 0.00003382
Iteration 17/1000 | Loss: 0.00003381
Iteration 18/1000 | Loss: 0.00003380
Iteration 19/1000 | Loss: 0.00003380
Iteration 20/1000 | Loss: 0.00003379
Iteration 21/1000 | Loss: 0.00003376
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00003372
Iteration 25/1000 | Loss: 0.00003372
Iteration 26/1000 | Loss: 0.00003372
Iteration 27/1000 | Loss: 0.00003371
Iteration 28/1000 | Loss: 0.00003371
Iteration 29/1000 | Loss: 0.00003369
Iteration 30/1000 | Loss: 0.00003368
Iteration 31/1000 | Loss: 0.00003368
Iteration 32/1000 | Loss: 0.00003368
Iteration 33/1000 | Loss: 0.00003367
Iteration 34/1000 | Loss: 0.00003366
Iteration 35/1000 | Loss: 0.00003366
Iteration 36/1000 | Loss: 0.00003366
Iteration 37/1000 | Loss: 0.00003366
Iteration 38/1000 | Loss: 0.00003366
Iteration 39/1000 | Loss: 0.00003366
Iteration 40/1000 | Loss: 0.00003366
Iteration 41/1000 | Loss: 0.00003366
Iteration 42/1000 | Loss: 0.00003365
Iteration 43/1000 | Loss: 0.00003365
Iteration 44/1000 | Loss: 0.00003365
Iteration 45/1000 | Loss: 0.00003365
Iteration 46/1000 | Loss: 0.00003365
Iteration 47/1000 | Loss: 0.00003365
Iteration 48/1000 | Loss: 0.00003365
Iteration 49/1000 | Loss: 0.00003365
Iteration 50/1000 | Loss: 0.00003365
Iteration 51/1000 | Loss: 0.00003365
Iteration 52/1000 | Loss: 0.00003365
Iteration 53/1000 | Loss: 0.00003364
Iteration 54/1000 | Loss: 0.00003363
Iteration 55/1000 | Loss: 0.00003363
Iteration 56/1000 | Loss: 0.00003360
Iteration 57/1000 | Loss: 0.00003360
Iteration 58/1000 | Loss: 0.00003357
Iteration 59/1000 | Loss: 0.00003357
Iteration 60/1000 | Loss: 0.00003357
Iteration 61/1000 | Loss: 0.00003357
Iteration 62/1000 | Loss: 0.00003357
Iteration 63/1000 | Loss: 0.00003357
Iteration 64/1000 | Loss: 0.00003357
Iteration 65/1000 | Loss: 0.00003357
Iteration 66/1000 | Loss: 0.00003357
Iteration 67/1000 | Loss: 0.00003357
Iteration 68/1000 | Loss: 0.00003356
Iteration 69/1000 | Loss: 0.00003356
Iteration 70/1000 | Loss: 0.00003356
Iteration 71/1000 | Loss: 0.00003356
Iteration 72/1000 | Loss: 0.00003356
Iteration 73/1000 | Loss: 0.00003353
Iteration 74/1000 | Loss: 0.00003353
Iteration 75/1000 | Loss: 0.00003353
Iteration 76/1000 | Loss: 0.00003353
Iteration 77/1000 | Loss: 0.00003353
Iteration 78/1000 | Loss: 0.00003353
Iteration 79/1000 | Loss: 0.00003353
Iteration 80/1000 | Loss: 0.00003352
Iteration 81/1000 | Loss: 0.00003352
Iteration 82/1000 | Loss: 0.00003352
Iteration 83/1000 | Loss: 0.00003352
Iteration 84/1000 | Loss: 0.00003352
Iteration 85/1000 | Loss: 0.00003351
Iteration 86/1000 | Loss: 0.00003351
Iteration 87/1000 | Loss: 0.00003351
Iteration 88/1000 | Loss: 0.00003351
Iteration 89/1000 | Loss: 0.00003350
Iteration 90/1000 | Loss: 0.00003350
Iteration 91/1000 | Loss: 0.00003350
Iteration 92/1000 | Loss: 0.00003349
Iteration 93/1000 | Loss: 0.00003349
Iteration 94/1000 | Loss: 0.00003349
Iteration 95/1000 | Loss: 0.00003349
Iteration 96/1000 | Loss: 0.00003349
Iteration 97/1000 | Loss: 0.00003349
Iteration 98/1000 | Loss: 0.00003349
Iteration 99/1000 | Loss: 0.00003349
Iteration 100/1000 | Loss: 0.00003349
Iteration 101/1000 | Loss: 0.00003349
Iteration 102/1000 | Loss: 0.00003348
Iteration 103/1000 | Loss: 0.00003348
Iteration 104/1000 | Loss: 0.00003348
Iteration 105/1000 | Loss: 0.00003348
Iteration 106/1000 | Loss: 0.00003347
Iteration 107/1000 | Loss: 0.00003347
Iteration 108/1000 | Loss: 0.00003347
Iteration 109/1000 | Loss: 0.00003347
Iteration 110/1000 | Loss: 0.00003347
Iteration 111/1000 | Loss: 0.00003347
Iteration 112/1000 | Loss: 0.00003347
Iteration 113/1000 | Loss: 0.00003347
Iteration 114/1000 | Loss: 0.00003347
Iteration 115/1000 | Loss: 0.00003347
Iteration 116/1000 | Loss: 0.00003346
Iteration 117/1000 | Loss: 0.00003345
Iteration 118/1000 | Loss: 0.00003345
Iteration 119/1000 | Loss: 0.00003345
Iteration 120/1000 | Loss: 0.00003345
Iteration 121/1000 | Loss: 0.00003345
Iteration 122/1000 | Loss: 0.00003344
Iteration 123/1000 | Loss: 0.00003344
Iteration 124/1000 | Loss: 0.00003344
Iteration 125/1000 | Loss: 0.00003344
Iteration 126/1000 | Loss: 0.00003344
Iteration 127/1000 | Loss: 0.00003344
Iteration 128/1000 | Loss: 0.00003344
Iteration 129/1000 | Loss: 0.00003344
Iteration 130/1000 | Loss: 0.00003343
Iteration 131/1000 | Loss: 0.00003343
Iteration 132/1000 | Loss: 0.00003343
Iteration 133/1000 | Loss: 0.00003343
Iteration 134/1000 | Loss: 0.00003343
Iteration 135/1000 | Loss: 0.00003342
Iteration 136/1000 | Loss: 0.00003342
Iteration 137/1000 | Loss: 0.00003342
Iteration 138/1000 | Loss: 0.00003342
Iteration 139/1000 | Loss: 0.00003341
Iteration 140/1000 | Loss: 0.00003341
Iteration 141/1000 | Loss: 0.00003341
Iteration 142/1000 | Loss: 0.00003341
Iteration 143/1000 | Loss: 0.00003341
Iteration 144/1000 | Loss: 0.00003341
Iteration 145/1000 | Loss: 0.00003341
Iteration 146/1000 | Loss: 0.00003341
Iteration 147/1000 | Loss: 0.00003340
Iteration 148/1000 | Loss: 0.00003340
Iteration 149/1000 | Loss: 0.00003340
Iteration 150/1000 | Loss: 0.00003340
Iteration 151/1000 | Loss: 0.00003340
Iteration 152/1000 | Loss: 0.00003340
Iteration 153/1000 | Loss: 0.00003340
Iteration 154/1000 | Loss: 0.00003340
Iteration 155/1000 | Loss: 0.00003340
Iteration 156/1000 | Loss: 0.00003340
Iteration 157/1000 | Loss: 0.00003340
Iteration 158/1000 | Loss: 0.00003340
Iteration 159/1000 | Loss: 0.00003340
Iteration 160/1000 | Loss: 0.00003340
Iteration 161/1000 | Loss: 0.00003340
Iteration 162/1000 | Loss: 0.00003340
Iteration 163/1000 | Loss: 0.00003340
Iteration 164/1000 | Loss: 0.00003339
Iteration 165/1000 | Loss: 0.00003339
Iteration 166/1000 | Loss: 0.00003339
Iteration 167/1000 | Loss: 0.00003339
Iteration 168/1000 | Loss: 0.00003339
Iteration 169/1000 | Loss: 0.00003339
Iteration 170/1000 | Loss: 0.00003339
Iteration 171/1000 | Loss: 0.00003339
Iteration 172/1000 | Loss: 0.00003339
Iteration 173/1000 | Loss: 0.00003339
Iteration 174/1000 | Loss: 0.00003339
Iteration 175/1000 | Loss: 0.00003339
Iteration 176/1000 | Loss: 0.00003338
Iteration 177/1000 | Loss: 0.00003338
Iteration 178/1000 | Loss: 0.00003338
Iteration 179/1000 | Loss: 0.00003338
Iteration 180/1000 | Loss: 0.00003338
Iteration 181/1000 | Loss: 0.00003338
Iteration 182/1000 | Loss: 0.00003338
Iteration 183/1000 | Loss: 0.00003338
Iteration 184/1000 | Loss: 0.00003338
Iteration 185/1000 | Loss: 0.00003338
Iteration 186/1000 | Loss: 0.00003338
Iteration 187/1000 | Loss: 0.00003338
Iteration 188/1000 | Loss: 0.00003338
Iteration 189/1000 | Loss: 0.00003338
Iteration 190/1000 | Loss: 0.00003338
Iteration 191/1000 | Loss: 0.00003338
Iteration 192/1000 | Loss: 0.00003338
Iteration 193/1000 | Loss: 0.00003337
Iteration 194/1000 | Loss: 0.00003337
Iteration 195/1000 | Loss: 0.00003337
Iteration 196/1000 | Loss: 0.00003337
Iteration 197/1000 | Loss: 0.00003336
Iteration 198/1000 | Loss: 0.00003336
Iteration 199/1000 | Loss: 0.00003336
Iteration 200/1000 | Loss: 0.00003336
Iteration 201/1000 | Loss: 0.00003336
Iteration 202/1000 | Loss: 0.00003336
Iteration 203/1000 | Loss: 0.00003335
Iteration 204/1000 | Loss: 0.00003335
Iteration 205/1000 | Loss: 0.00003335
Iteration 206/1000 | Loss: 0.00003334
Iteration 207/1000 | Loss: 0.00003334
Iteration 208/1000 | Loss: 0.00003334
Iteration 209/1000 | Loss: 0.00003334
Iteration 210/1000 | Loss: 0.00003334
Iteration 211/1000 | Loss: 0.00003334
Iteration 212/1000 | Loss: 0.00003334
Iteration 213/1000 | Loss: 0.00003334
Iteration 214/1000 | Loss: 0.00003334
Iteration 215/1000 | Loss: 0.00003334
Iteration 216/1000 | Loss: 0.00003334
Iteration 217/1000 | Loss: 0.00003333
Iteration 218/1000 | Loss: 0.00003333
Iteration 219/1000 | Loss: 0.00003333
Iteration 220/1000 | Loss: 0.00003333
Iteration 221/1000 | Loss: 0.00003333
Iteration 222/1000 | Loss: 0.00003333
Iteration 223/1000 | Loss: 0.00003333
Iteration 224/1000 | Loss: 0.00003333
Iteration 225/1000 | Loss: 0.00003332
Iteration 226/1000 | Loss: 0.00003332
Iteration 227/1000 | Loss: 0.00003332
Iteration 228/1000 | Loss: 0.00003332
Iteration 229/1000 | Loss: 0.00003332
Iteration 230/1000 | Loss: 0.00003332
Iteration 231/1000 | Loss: 0.00003332
Iteration 232/1000 | Loss: 0.00003332
Iteration 233/1000 | Loss: 0.00003332
Iteration 234/1000 | Loss: 0.00003331
Iteration 235/1000 | Loss: 0.00003331
Iteration 236/1000 | Loss: 0.00003331
Iteration 237/1000 | Loss: 0.00003331
Iteration 238/1000 | Loss: 0.00003331
Iteration 239/1000 | Loss: 0.00003331
Iteration 240/1000 | Loss: 0.00003331
Iteration 241/1000 | Loss: 0.00003331
Iteration 242/1000 | Loss: 0.00003331
Iteration 243/1000 | Loss: 0.00003331
Iteration 244/1000 | Loss: 0.00003331
Iteration 245/1000 | Loss: 0.00003331
Iteration 246/1000 | Loss: 0.00003331
Iteration 247/1000 | Loss: 0.00003331
Iteration 248/1000 | Loss: 0.00003330
Iteration 249/1000 | Loss: 0.00003330
Iteration 250/1000 | Loss: 0.00003330
Iteration 251/1000 | Loss: 0.00003330
Iteration 252/1000 | Loss: 0.00003330
Iteration 253/1000 | Loss: 0.00003330
Iteration 254/1000 | Loss: 0.00003330
Iteration 255/1000 | Loss: 0.00003330
Iteration 256/1000 | Loss: 0.00003330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [3.330426261527464e-05, 3.330426261527464e-05, 3.330426261527464e-05, 3.330426261527464e-05, 3.330426261527464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.330426261527464e-05

Optimization complete. Final v2v error: 4.875292778015137 mm

Highest mean error: 5.503887176513672 mm for frame 250

Lowest mean error: 4.525252819061279 mm for frame 19

Saving results

Total time: 58.53213381767273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078385
Iteration 2/25 | Loss: 0.00134874
Iteration 3/25 | Loss: 0.00102053
Iteration 4/25 | Loss: 0.00097894
Iteration 5/25 | Loss: 0.00096563
Iteration 6/25 | Loss: 0.00096922
Iteration 7/25 | Loss: 0.00096495
Iteration 8/25 | Loss: 0.00096078
Iteration 9/25 | Loss: 0.00094914
Iteration 10/25 | Loss: 0.00094376
Iteration 11/25 | Loss: 0.00094195
Iteration 12/25 | Loss: 0.00094220
Iteration 13/25 | Loss: 0.00094015
Iteration 14/25 | Loss: 0.00093948
Iteration 15/25 | Loss: 0.00093923
Iteration 16/25 | Loss: 0.00093919
Iteration 17/25 | Loss: 0.00093919
Iteration 18/25 | Loss: 0.00093918
Iteration 19/25 | Loss: 0.00093918
Iteration 20/25 | Loss: 0.00093918
Iteration 21/25 | Loss: 0.00093918
Iteration 22/25 | Loss: 0.00093918
Iteration 23/25 | Loss: 0.00093918
Iteration 24/25 | Loss: 0.00093918
Iteration 25/25 | Loss: 0.00093918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29307687
Iteration 2/25 | Loss: 0.00055771
Iteration 3/25 | Loss: 0.00055771
Iteration 4/25 | Loss: 0.00055771
Iteration 5/25 | Loss: 0.00055771
Iteration 6/25 | Loss: 0.00055771
Iteration 7/25 | Loss: 0.00055771
Iteration 8/25 | Loss: 0.00055771
Iteration 9/25 | Loss: 0.00055771
Iteration 10/25 | Loss: 0.00055771
Iteration 11/25 | Loss: 0.00055771
Iteration 12/25 | Loss: 0.00055771
Iteration 13/25 | Loss: 0.00055771
Iteration 14/25 | Loss: 0.00055771
Iteration 15/25 | Loss: 0.00055771
Iteration 16/25 | Loss: 0.00055771
Iteration 17/25 | Loss: 0.00055771
Iteration 18/25 | Loss: 0.00055771
Iteration 19/25 | Loss: 0.00055771
Iteration 20/25 | Loss: 0.00055771
Iteration 21/25 | Loss: 0.00055771
Iteration 22/25 | Loss: 0.00055771
Iteration 23/25 | Loss: 0.00055771
Iteration 24/25 | Loss: 0.00055771
Iteration 25/25 | Loss: 0.00055771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055771
Iteration 2/1000 | Loss: 0.00005085
Iteration 3/1000 | Loss: 0.00003338
Iteration 4/1000 | Loss: 0.00003028
Iteration 5/1000 | Loss: 0.00002868
Iteration 6/1000 | Loss: 0.00002781
Iteration 7/1000 | Loss: 0.00002736
Iteration 8/1000 | Loss: 0.00002708
Iteration 9/1000 | Loss: 0.00002696
Iteration 10/1000 | Loss: 0.00002695
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002676
Iteration 13/1000 | Loss: 0.00002674
Iteration 14/1000 | Loss: 0.00002673
Iteration 15/1000 | Loss: 0.00002672
Iteration 16/1000 | Loss: 0.00002672
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002656
Iteration 19/1000 | Loss: 0.00002656
Iteration 20/1000 | Loss: 0.00002655
Iteration 21/1000 | Loss: 0.00002653
Iteration 22/1000 | Loss: 0.00002653
Iteration 23/1000 | Loss: 0.00002653
Iteration 24/1000 | Loss: 0.00002653
Iteration 25/1000 | Loss: 0.00002653
Iteration 26/1000 | Loss: 0.00002653
Iteration 27/1000 | Loss: 0.00002653
Iteration 28/1000 | Loss: 0.00002653
Iteration 29/1000 | Loss: 0.00002653
Iteration 30/1000 | Loss: 0.00002652
Iteration 31/1000 | Loss: 0.00002652
Iteration 32/1000 | Loss: 0.00002652
Iteration 33/1000 | Loss: 0.00002651
Iteration 34/1000 | Loss: 0.00002650
Iteration 35/1000 | Loss: 0.00002650
Iteration 36/1000 | Loss: 0.00002650
Iteration 37/1000 | Loss: 0.00002649
Iteration 38/1000 | Loss: 0.00002649
Iteration 39/1000 | Loss: 0.00002648
Iteration 40/1000 | Loss: 0.00002648
Iteration 41/1000 | Loss: 0.00002648
Iteration 42/1000 | Loss: 0.00002648
Iteration 43/1000 | Loss: 0.00002648
Iteration 44/1000 | Loss: 0.00002648
Iteration 45/1000 | Loss: 0.00002647
Iteration 46/1000 | Loss: 0.00002647
Iteration 47/1000 | Loss: 0.00002647
Iteration 48/1000 | Loss: 0.00002647
Iteration 49/1000 | Loss: 0.00002647
Iteration 50/1000 | Loss: 0.00002646
Iteration 51/1000 | Loss: 0.00002646
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00002645
Iteration 54/1000 | Loss: 0.00002645
Iteration 55/1000 | Loss: 0.00002645
Iteration 56/1000 | Loss: 0.00002644
Iteration 57/1000 | Loss: 0.00002641
Iteration 58/1000 | Loss: 0.00002640
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002639
Iteration 62/1000 | Loss: 0.00002639
Iteration 63/1000 | Loss: 0.00002639
Iteration 64/1000 | Loss: 0.00002639
Iteration 65/1000 | Loss: 0.00002639
Iteration 66/1000 | Loss: 0.00002639
Iteration 67/1000 | Loss: 0.00002639
Iteration 68/1000 | Loss: 0.00002639
Iteration 69/1000 | Loss: 0.00002638
Iteration 70/1000 | Loss: 0.00002638
Iteration 71/1000 | Loss: 0.00002638
Iteration 72/1000 | Loss: 0.00002638
Iteration 73/1000 | Loss: 0.00002637
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002637
Iteration 76/1000 | Loss: 0.00002637
Iteration 77/1000 | Loss: 0.00002637
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002637
Iteration 81/1000 | Loss: 0.00002637
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002635
Iteration 90/1000 | Loss: 0.00002635
Iteration 91/1000 | Loss: 0.00002635
Iteration 92/1000 | Loss: 0.00002635
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002634
Iteration 96/1000 | Loss: 0.00002634
Iteration 97/1000 | Loss: 0.00002634
Iteration 98/1000 | Loss: 0.00002634
Iteration 99/1000 | Loss: 0.00002634
Iteration 100/1000 | Loss: 0.00002634
Iteration 101/1000 | Loss: 0.00002634
Iteration 102/1000 | Loss: 0.00002634
Iteration 103/1000 | Loss: 0.00002633
Iteration 104/1000 | Loss: 0.00002633
Iteration 105/1000 | Loss: 0.00002633
Iteration 106/1000 | Loss: 0.00002633
Iteration 107/1000 | Loss: 0.00002633
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002633
Iteration 112/1000 | Loss: 0.00002633
Iteration 113/1000 | Loss: 0.00002632
Iteration 114/1000 | Loss: 0.00002632
Iteration 115/1000 | Loss: 0.00002632
Iteration 116/1000 | Loss: 0.00002632
Iteration 117/1000 | Loss: 0.00002632
Iteration 118/1000 | Loss: 0.00002632
Iteration 119/1000 | Loss: 0.00002632
Iteration 120/1000 | Loss: 0.00002632
Iteration 121/1000 | Loss: 0.00002632
Iteration 122/1000 | Loss: 0.00002632
Iteration 123/1000 | Loss: 0.00002632
Iteration 124/1000 | Loss: 0.00002632
Iteration 125/1000 | Loss: 0.00002631
Iteration 126/1000 | Loss: 0.00002631
Iteration 127/1000 | Loss: 0.00002631
Iteration 128/1000 | Loss: 0.00002631
Iteration 129/1000 | Loss: 0.00002631
Iteration 130/1000 | Loss: 0.00002630
Iteration 131/1000 | Loss: 0.00002630
Iteration 132/1000 | Loss: 0.00002630
Iteration 133/1000 | Loss: 0.00002630
Iteration 134/1000 | Loss: 0.00002630
Iteration 135/1000 | Loss: 0.00002630
Iteration 136/1000 | Loss: 0.00002630
Iteration 137/1000 | Loss: 0.00002630
Iteration 138/1000 | Loss: 0.00002630
Iteration 139/1000 | Loss: 0.00002630
Iteration 140/1000 | Loss: 0.00002629
Iteration 141/1000 | Loss: 0.00002629
Iteration 142/1000 | Loss: 0.00002629
Iteration 143/1000 | Loss: 0.00002629
Iteration 144/1000 | Loss: 0.00002629
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Iteration 157/1000 | Loss: 0.00002628
Iteration 158/1000 | Loss: 0.00002628
Iteration 159/1000 | Loss: 0.00002628
Iteration 160/1000 | Loss: 0.00002628
Iteration 161/1000 | Loss: 0.00002628
Iteration 162/1000 | Loss: 0.00002628
Iteration 163/1000 | Loss: 0.00002628
Iteration 164/1000 | Loss: 0.00002628
Iteration 165/1000 | Loss: 0.00002628
Iteration 166/1000 | Loss: 0.00002628
Iteration 167/1000 | Loss: 0.00002628
Iteration 168/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.627654066600371e-05, 2.627654066600371e-05, 2.627654066600371e-05, 2.627654066600371e-05, 2.627654066600371e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.627654066600371e-05

Optimization complete. Final v2v error: 4.509761333465576 mm

Highest mean error: 5.118169784545898 mm for frame 185

Lowest mean error: 4.05320930480957 mm for frame 110

Saving results

Total time: 55.283345460891724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472083
Iteration 2/25 | Loss: 0.00112860
Iteration 3/25 | Loss: 0.00104703
Iteration 4/25 | Loss: 0.00102121
Iteration 5/25 | Loss: 0.00101708
Iteration 6/25 | Loss: 0.00101639
Iteration 7/25 | Loss: 0.00101639
Iteration 8/25 | Loss: 0.00101639
Iteration 9/25 | Loss: 0.00101639
Iteration 10/25 | Loss: 0.00101639
Iteration 11/25 | Loss: 0.00101639
Iteration 12/25 | Loss: 0.00101639
Iteration 13/25 | Loss: 0.00101639
Iteration 14/25 | Loss: 0.00101639
Iteration 15/25 | Loss: 0.00101639
Iteration 16/25 | Loss: 0.00101639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010163880651816726, 0.0010163880651816726, 0.0010163880651816726, 0.0010163880651816726, 0.0010163880651816726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010163880651816726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27557600
Iteration 2/25 | Loss: 0.00079035
Iteration 3/25 | Loss: 0.00079035
Iteration 4/25 | Loss: 0.00079035
Iteration 5/25 | Loss: 0.00079035
Iteration 6/25 | Loss: 0.00079035
Iteration 7/25 | Loss: 0.00079035
Iteration 8/25 | Loss: 0.00079035
Iteration 9/25 | Loss: 0.00079035
Iteration 10/25 | Loss: 0.00079035
Iteration 11/25 | Loss: 0.00079035
Iteration 12/25 | Loss: 0.00079035
Iteration 13/25 | Loss: 0.00079035
Iteration 14/25 | Loss: 0.00079035
Iteration 15/25 | Loss: 0.00079035
Iteration 16/25 | Loss: 0.00079035
Iteration 17/25 | Loss: 0.00079035
Iteration 18/25 | Loss: 0.00079035
Iteration 19/25 | Loss: 0.00079035
Iteration 20/25 | Loss: 0.00079035
Iteration 21/25 | Loss: 0.00079035
Iteration 22/25 | Loss: 0.00079035
Iteration 23/25 | Loss: 0.00079035
Iteration 24/25 | Loss: 0.00079035
Iteration 25/25 | Loss: 0.00079035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079035
Iteration 2/1000 | Loss: 0.00010956
Iteration 3/1000 | Loss: 0.00007517
Iteration 4/1000 | Loss: 0.00005979
Iteration 5/1000 | Loss: 0.00005519
Iteration 6/1000 | Loss: 0.00005125
Iteration 7/1000 | Loss: 0.00004844
Iteration 8/1000 | Loss: 0.00004663
Iteration 9/1000 | Loss: 0.00004578
Iteration 10/1000 | Loss: 0.00004535
Iteration 11/1000 | Loss: 0.00004492
Iteration 12/1000 | Loss: 0.00004452
Iteration 13/1000 | Loss: 0.00004440
Iteration 14/1000 | Loss: 0.00004437
Iteration 15/1000 | Loss: 0.00004431
Iteration 16/1000 | Loss: 0.00004430
Iteration 17/1000 | Loss: 0.00004419
Iteration 18/1000 | Loss: 0.00004417
Iteration 19/1000 | Loss: 0.00004410
Iteration 20/1000 | Loss: 0.00004408
Iteration 21/1000 | Loss: 0.00004405
Iteration 22/1000 | Loss: 0.00004404
Iteration 23/1000 | Loss: 0.00004403
Iteration 24/1000 | Loss: 0.00004403
Iteration 25/1000 | Loss: 0.00004397
Iteration 26/1000 | Loss: 0.00004393
Iteration 27/1000 | Loss: 0.00004393
Iteration 28/1000 | Loss: 0.00004392
Iteration 29/1000 | Loss: 0.00004392
Iteration 30/1000 | Loss: 0.00004391
Iteration 31/1000 | Loss: 0.00004390
Iteration 32/1000 | Loss: 0.00004389
Iteration 33/1000 | Loss: 0.00004388
Iteration 34/1000 | Loss: 0.00004388
Iteration 35/1000 | Loss: 0.00004388
Iteration 36/1000 | Loss: 0.00004387
Iteration 37/1000 | Loss: 0.00004387
Iteration 38/1000 | Loss: 0.00004387
Iteration 39/1000 | Loss: 0.00004387
Iteration 40/1000 | Loss: 0.00004387
Iteration 41/1000 | Loss: 0.00004386
Iteration 42/1000 | Loss: 0.00004386
Iteration 43/1000 | Loss: 0.00004386
Iteration 44/1000 | Loss: 0.00004386
Iteration 45/1000 | Loss: 0.00004385
Iteration 46/1000 | Loss: 0.00004385
Iteration 47/1000 | Loss: 0.00004385
Iteration 48/1000 | Loss: 0.00004385
Iteration 49/1000 | Loss: 0.00004385
Iteration 50/1000 | Loss: 0.00004385
Iteration 51/1000 | Loss: 0.00004385
Iteration 52/1000 | Loss: 0.00004384
Iteration 53/1000 | Loss: 0.00004384
Iteration 54/1000 | Loss: 0.00004384
Iteration 55/1000 | Loss: 0.00004384
Iteration 56/1000 | Loss: 0.00004384
Iteration 57/1000 | Loss: 0.00004384
Iteration 58/1000 | Loss: 0.00004384
Iteration 59/1000 | Loss: 0.00004384
Iteration 60/1000 | Loss: 0.00004384
Iteration 61/1000 | Loss: 0.00004384
Iteration 62/1000 | Loss: 0.00004384
Iteration 63/1000 | Loss: 0.00004384
Iteration 64/1000 | Loss: 0.00004384
Iteration 65/1000 | Loss: 0.00004384
Iteration 66/1000 | Loss: 0.00004384
Iteration 67/1000 | Loss: 0.00004384
Iteration 68/1000 | Loss: 0.00004384
Iteration 69/1000 | Loss: 0.00004384
Iteration 70/1000 | Loss: 0.00004384
Iteration 71/1000 | Loss: 0.00004384
Iteration 72/1000 | Loss: 0.00004384
Iteration 73/1000 | Loss: 0.00004384
Iteration 74/1000 | Loss: 0.00004384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [4.384098429000005e-05, 4.384098429000005e-05, 4.384098429000005e-05, 4.384098429000005e-05, 4.384098429000005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.384098429000005e-05

Optimization complete. Final v2v error: 5.561906337738037 mm

Highest mean error: 5.909865379333496 mm for frame 124

Lowest mean error: 5.215916633605957 mm for frame 41

Saving results

Total time: 36.08017873764038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646369
Iteration 2/25 | Loss: 0.00140023
Iteration 3/25 | Loss: 0.00118787
Iteration 4/25 | Loss: 0.00111232
Iteration 5/25 | Loss: 0.00113780
Iteration 6/25 | Loss: 0.00108135
Iteration 7/25 | Loss: 0.00105330
Iteration 8/25 | Loss: 0.00105641
Iteration 9/25 | Loss: 0.00104943
Iteration 10/25 | Loss: 0.00104562
Iteration 11/25 | Loss: 0.00104424
Iteration 12/25 | Loss: 0.00104580
Iteration 13/25 | Loss: 0.00103925
Iteration 14/25 | Loss: 0.00103623
Iteration 15/25 | Loss: 0.00103501
Iteration 16/25 | Loss: 0.00103476
Iteration 17/25 | Loss: 0.00103471
Iteration 18/25 | Loss: 0.00103471
Iteration 19/25 | Loss: 0.00103470
Iteration 20/25 | Loss: 0.00103470
Iteration 21/25 | Loss: 0.00103470
Iteration 22/25 | Loss: 0.00103470
Iteration 23/25 | Loss: 0.00103470
Iteration 24/25 | Loss: 0.00103470
Iteration 25/25 | Loss: 0.00103470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25073671
Iteration 2/25 | Loss: 0.00071210
Iteration 3/25 | Loss: 0.00071201
Iteration 4/25 | Loss: 0.00071201
Iteration 5/25 | Loss: 0.00071201
Iteration 6/25 | Loss: 0.00071201
Iteration 7/25 | Loss: 0.00071201
Iteration 8/25 | Loss: 0.00071201
Iteration 9/25 | Loss: 0.00071201
Iteration 10/25 | Loss: 0.00071201
Iteration 11/25 | Loss: 0.00071201
Iteration 12/25 | Loss: 0.00071201
Iteration 13/25 | Loss: 0.00071201
Iteration 14/25 | Loss: 0.00071201
Iteration 15/25 | Loss: 0.00071201
Iteration 16/25 | Loss: 0.00071201
Iteration 17/25 | Loss: 0.00071201
Iteration 18/25 | Loss: 0.00071201
Iteration 19/25 | Loss: 0.00071201
Iteration 20/25 | Loss: 0.00071201
Iteration 21/25 | Loss: 0.00071201
Iteration 22/25 | Loss: 0.00071201
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007120089139789343, 0.0007120089139789343, 0.0007120089139789343, 0.0007120089139789343, 0.0007120089139789343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007120089139789343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071201
Iteration 2/1000 | Loss: 0.00011180
Iteration 3/1000 | Loss: 0.00006554
Iteration 4/1000 | Loss: 0.00005381
Iteration 5/1000 | Loss: 0.00005084
Iteration 6/1000 | Loss: 0.00004846
Iteration 7/1000 | Loss: 0.00006439
Iteration 8/1000 | Loss: 0.00004787
Iteration 9/1000 | Loss: 0.00004624
Iteration 10/1000 | Loss: 0.00004507
Iteration 11/1000 | Loss: 0.00004444
Iteration 12/1000 | Loss: 0.00004397
Iteration 13/1000 | Loss: 0.00004355
Iteration 14/1000 | Loss: 0.00004292
Iteration 15/1000 | Loss: 0.00004241
Iteration 16/1000 | Loss: 0.00004206
Iteration 17/1000 | Loss: 0.00004175
Iteration 18/1000 | Loss: 0.00004152
Iteration 19/1000 | Loss: 0.00004146
Iteration 20/1000 | Loss: 0.00004144
Iteration 21/1000 | Loss: 0.00004139
Iteration 22/1000 | Loss: 0.00004134
Iteration 23/1000 | Loss: 0.00004133
Iteration 24/1000 | Loss: 0.00004132
Iteration 25/1000 | Loss: 0.00004132
Iteration 26/1000 | Loss: 0.00004131
Iteration 27/1000 | Loss: 0.00004131
Iteration 28/1000 | Loss: 0.00004131
Iteration 29/1000 | Loss: 0.00004130
Iteration 30/1000 | Loss: 0.00004128
Iteration 31/1000 | Loss: 0.00004128
Iteration 32/1000 | Loss: 0.00004127
Iteration 33/1000 | Loss: 0.00004127
Iteration 34/1000 | Loss: 0.00004127
Iteration 35/1000 | Loss: 0.00004126
Iteration 36/1000 | Loss: 0.00004126
Iteration 37/1000 | Loss: 0.00004125
Iteration 38/1000 | Loss: 0.00004125
Iteration 39/1000 | Loss: 0.00004125
Iteration 40/1000 | Loss: 0.00004125
Iteration 41/1000 | Loss: 0.00004125
Iteration 42/1000 | Loss: 0.00004124
Iteration 43/1000 | Loss: 0.00004124
Iteration 44/1000 | Loss: 0.00004124
Iteration 45/1000 | Loss: 0.00004124
Iteration 46/1000 | Loss: 0.00004124
Iteration 47/1000 | Loss: 0.00004124
Iteration 48/1000 | Loss: 0.00004123
Iteration 49/1000 | Loss: 0.00004123
Iteration 50/1000 | Loss: 0.00004123
Iteration 51/1000 | Loss: 0.00004123
Iteration 52/1000 | Loss: 0.00004123
Iteration 53/1000 | Loss: 0.00004123
Iteration 54/1000 | Loss: 0.00004122
Iteration 55/1000 | Loss: 0.00004122
Iteration 56/1000 | Loss: 0.00004122
Iteration 57/1000 | Loss: 0.00004122
Iteration 58/1000 | Loss: 0.00004122
Iteration 59/1000 | Loss: 0.00004122
Iteration 60/1000 | Loss: 0.00004122
Iteration 61/1000 | Loss: 0.00004122
Iteration 62/1000 | Loss: 0.00004121
Iteration 63/1000 | Loss: 0.00004121
Iteration 64/1000 | Loss: 0.00004121
Iteration 65/1000 | Loss: 0.00004121
Iteration 66/1000 | Loss: 0.00004121
Iteration 67/1000 | Loss: 0.00004121
Iteration 68/1000 | Loss: 0.00004121
Iteration 69/1000 | Loss: 0.00004121
Iteration 70/1000 | Loss: 0.00004121
Iteration 71/1000 | Loss: 0.00004121
Iteration 72/1000 | Loss: 0.00004121
Iteration 73/1000 | Loss: 0.00004120
Iteration 74/1000 | Loss: 0.00004120
Iteration 75/1000 | Loss: 0.00004120
Iteration 76/1000 | Loss: 0.00004120
Iteration 77/1000 | Loss: 0.00004120
Iteration 78/1000 | Loss: 0.00004120
Iteration 79/1000 | Loss: 0.00004120
Iteration 80/1000 | Loss: 0.00004120
Iteration 81/1000 | Loss: 0.00004120
Iteration 82/1000 | Loss: 0.00004119
Iteration 83/1000 | Loss: 0.00004119
Iteration 84/1000 | Loss: 0.00004119
Iteration 85/1000 | Loss: 0.00004119
Iteration 86/1000 | Loss: 0.00004119
Iteration 87/1000 | Loss: 0.00004119
Iteration 88/1000 | Loss: 0.00004119
Iteration 89/1000 | Loss: 0.00004119
Iteration 90/1000 | Loss: 0.00004119
Iteration 91/1000 | Loss: 0.00004119
Iteration 92/1000 | Loss: 0.00004118
Iteration 93/1000 | Loss: 0.00004118
Iteration 94/1000 | Loss: 0.00004118
Iteration 95/1000 | Loss: 0.00004118
Iteration 96/1000 | Loss: 0.00004118
Iteration 97/1000 | Loss: 0.00004118
Iteration 98/1000 | Loss: 0.00004118
Iteration 99/1000 | Loss: 0.00004118
Iteration 100/1000 | Loss: 0.00004118
Iteration 101/1000 | Loss: 0.00004118
Iteration 102/1000 | Loss: 0.00004118
Iteration 103/1000 | Loss: 0.00004118
Iteration 104/1000 | Loss: 0.00004117
Iteration 105/1000 | Loss: 0.00004117
Iteration 106/1000 | Loss: 0.00004117
Iteration 107/1000 | Loss: 0.00004117
Iteration 108/1000 | Loss: 0.00004117
Iteration 109/1000 | Loss: 0.00004117
Iteration 110/1000 | Loss: 0.00004117
Iteration 111/1000 | Loss: 0.00004117
Iteration 112/1000 | Loss: 0.00004117
Iteration 113/1000 | Loss: 0.00004117
Iteration 114/1000 | Loss: 0.00004117
Iteration 115/1000 | Loss: 0.00004117
Iteration 116/1000 | Loss: 0.00004117
Iteration 117/1000 | Loss: 0.00004117
Iteration 118/1000 | Loss: 0.00004117
Iteration 119/1000 | Loss: 0.00004116
Iteration 120/1000 | Loss: 0.00004116
Iteration 121/1000 | Loss: 0.00004116
Iteration 122/1000 | Loss: 0.00004116
Iteration 123/1000 | Loss: 0.00004116
Iteration 124/1000 | Loss: 0.00004116
Iteration 125/1000 | Loss: 0.00004116
Iteration 126/1000 | Loss: 0.00004116
Iteration 127/1000 | Loss: 0.00004116
Iteration 128/1000 | Loss: 0.00004116
Iteration 129/1000 | Loss: 0.00004116
Iteration 130/1000 | Loss: 0.00004116
Iteration 131/1000 | Loss: 0.00004116
Iteration 132/1000 | Loss: 0.00004116
Iteration 133/1000 | Loss: 0.00004116
Iteration 134/1000 | Loss: 0.00004116
Iteration 135/1000 | Loss: 0.00004116
Iteration 136/1000 | Loss: 0.00004116
Iteration 137/1000 | Loss: 0.00004116
Iteration 138/1000 | Loss: 0.00004115
Iteration 139/1000 | Loss: 0.00004115
Iteration 140/1000 | Loss: 0.00004115
Iteration 141/1000 | Loss: 0.00004115
Iteration 142/1000 | Loss: 0.00004115
Iteration 143/1000 | Loss: 0.00004115
Iteration 144/1000 | Loss: 0.00004115
Iteration 145/1000 | Loss: 0.00004115
Iteration 146/1000 | Loss: 0.00004115
Iteration 147/1000 | Loss: 0.00004115
Iteration 148/1000 | Loss: 0.00004115
Iteration 149/1000 | Loss: 0.00004115
Iteration 150/1000 | Loss: 0.00004115
Iteration 151/1000 | Loss: 0.00004115
Iteration 152/1000 | Loss: 0.00004115
Iteration 153/1000 | Loss: 0.00004115
Iteration 154/1000 | Loss: 0.00004115
Iteration 155/1000 | Loss: 0.00004115
Iteration 156/1000 | Loss: 0.00004115
Iteration 157/1000 | Loss: 0.00004115
Iteration 158/1000 | Loss: 0.00004114
Iteration 159/1000 | Loss: 0.00004114
Iteration 160/1000 | Loss: 0.00004114
Iteration 161/1000 | Loss: 0.00004114
Iteration 162/1000 | Loss: 0.00004114
Iteration 163/1000 | Loss: 0.00004114
Iteration 164/1000 | Loss: 0.00004114
Iteration 165/1000 | Loss: 0.00004114
Iteration 166/1000 | Loss: 0.00004114
Iteration 167/1000 | Loss: 0.00004114
Iteration 168/1000 | Loss: 0.00004114
Iteration 169/1000 | Loss: 0.00004113
Iteration 170/1000 | Loss: 0.00004113
Iteration 171/1000 | Loss: 0.00004113
Iteration 172/1000 | Loss: 0.00004113
Iteration 173/1000 | Loss: 0.00004113
Iteration 174/1000 | Loss: 0.00004113
Iteration 175/1000 | Loss: 0.00004113
Iteration 176/1000 | Loss: 0.00004113
Iteration 177/1000 | Loss: 0.00004113
Iteration 178/1000 | Loss: 0.00004113
Iteration 179/1000 | Loss: 0.00004113
Iteration 180/1000 | Loss: 0.00004113
Iteration 181/1000 | Loss: 0.00004112
Iteration 182/1000 | Loss: 0.00004112
Iteration 183/1000 | Loss: 0.00004112
Iteration 184/1000 | Loss: 0.00004112
Iteration 185/1000 | Loss: 0.00004112
Iteration 186/1000 | Loss: 0.00004112
Iteration 187/1000 | Loss: 0.00004112
Iteration 188/1000 | Loss: 0.00004112
Iteration 189/1000 | Loss: 0.00004112
Iteration 190/1000 | Loss: 0.00004112
Iteration 191/1000 | Loss: 0.00004112
Iteration 192/1000 | Loss: 0.00004112
Iteration 193/1000 | Loss: 0.00004112
Iteration 194/1000 | Loss: 0.00004112
Iteration 195/1000 | Loss: 0.00004112
Iteration 196/1000 | Loss: 0.00004112
Iteration 197/1000 | Loss: 0.00004112
Iteration 198/1000 | Loss: 0.00004112
Iteration 199/1000 | Loss: 0.00004112
Iteration 200/1000 | Loss: 0.00004112
Iteration 201/1000 | Loss: 0.00004112
Iteration 202/1000 | Loss: 0.00004112
Iteration 203/1000 | Loss: 0.00004112
Iteration 204/1000 | Loss: 0.00004112
Iteration 205/1000 | Loss: 0.00004112
Iteration 206/1000 | Loss: 0.00004112
Iteration 207/1000 | Loss: 0.00004112
Iteration 208/1000 | Loss: 0.00004112
Iteration 209/1000 | Loss: 0.00004112
Iteration 210/1000 | Loss: 0.00004112
Iteration 211/1000 | Loss: 0.00004112
Iteration 212/1000 | Loss: 0.00004112
Iteration 213/1000 | Loss: 0.00004112
Iteration 214/1000 | Loss: 0.00004112
Iteration 215/1000 | Loss: 0.00004112
Iteration 216/1000 | Loss: 0.00004112
Iteration 217/1000 | Loss: 0.00004112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [4.112065289518796e-05, 4.112065289518796e-05, 4.112065289518796e-05, 4.112065289518796e-05, 4.112065289518796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.112065289518796e-05

Optimization complete. Final v2v error: 5.138749122619629 mm

Highest mean error: 6.5991291999816895 mm for frame 120

Lowest mean error: 4.035556793212891 mm for frame 191

Saving results

Total time: 73.27385473251343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00672804
Iteration 2/25 | Loss: 0.00148866
Iteration 3/25 | Loss: 0.00111466
Iteration 4/25 | Loss: 0.00104845
Iteration 5/25 | Loss: 0.00103450
Iteration 6/25 | Loss: 0.00103871
Iteration 7/25 | Loss: 0.00104214
Iteration 8/25 | Loss: 0.00101337
Iteration 9/25 | Loss: 0.00100677
Iteration 10/25 | Loss: 0.00100543
Iteration 11/25 | Loss: 0.00100527
Iteration 12/25 | Loss: 0.00101053
Iteration 13/25 | Loss: 0.00100963
Iteration 14/25 | Loss: 0.00100257
Iteration 15/25 | Loss: 0.00100116
Iteration 16/25 | Loss: 0.00100108
Iteration 17/25 | Loss: 0.00100108
Iteration 18/25 | Loss: 0.00100108
Iteration 19/25 | Loss: 0.00100108
Iteration 20/25 | Loss: 0.00100108
Iteration 21/25 | Loss: 0.00100108
Iteration 22/25 | Loss: 0.00100108
Iteration 23/25 | Loss: 0.00100107
Iteration 24/25 | Loss: 0.00100107
Iteration 25/25 | Loss: 0.00100107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.07618618
Iteration 2/25 | Loss: 0.00050247
Iteration 3/25 | Loss: 0.00050225
Iteration 4/25 | Loss: 0.00050225
Iteration 5/25 | Loss: 0.00050225
Iteration 6/25 | Loss: 0.00050225
Iteration 7/25 | Loss: 0.00050225
Iteration 8/25 | Loss: 0.00050225
Iteration 9/25 | Loss: 0.00050225
Iteration 10/25 | Loss: 0.00050225
Iteration 11/25 | Loss: 0.00050225
Iteration 12/25 | Loss: 0.00050225
Iteration 13/25 | Loss: 0.00050225
Iteration 14/25 | Loss: 0.00050225
Iteration 15/25 | Loss: 0.00050225
Iteration 16/25 | Loss: 0.00050225
Iteration 17/25 | Loss: 0.00050225
Iteration 18/25 | Loss: 0.00050225
Iteration 19/25 | Loss: 0.00050225
Iteration 20/25 | Loss: 0.00050225
Iteration 21/25 | Loss: 0.00050225
Iteration 22/25 | Loss: 0.00050225
Iteration 23/25 | Loss: 0.00050225
Iteration 24/25 | Loss: 0.00050225
Iteration 25/25 | Loss: 0.00050225

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050225
Iteration 2/1000 | Loss: 0.00011001
Iteration 3/1000 | Loss: 0.00007575
Iteration 4/1000 | Loss: 0.00005604
Iteration 5/1000 | Loss: 0.00005077
Iteration 6/1000 | Loss: 0.00004726
Iteration 7/1000 | Loss: 0.00004437
Iteration 8/1000 | Loss: 0.00004284
Iteration 9/1000 | Loss: 0.00004169
Iteration 10/1000 | Loss: 0.00004106
Iteration 11/1000 | Loss: 0.00004062
Iteration 12/1000 | Loss: 0.00004027
Iteration 13/1000 | Loss: 0.00003997
Iteration 14/1000 | Loss: 0.00003968
Iteration 15/1000 | Loss: 0.00003946
Iteration 16/1000 | Loss: 0.00003937
Iteration 17/1000 | Loss: 0.00003928
Iteration 18/1000 | Loss: 0.00003924
Iteration 19/1000 | Loss: 0.00003917
Iteration 20/1000 | Loss: 0.00003916
Iteration 21/1000 | Loss: 0.00003912
Iteration 22/1000 | Loss: 0.00003911
Iteration 23/1000 | Loss: 0.00003906
Iteration 24/1000 | Loss: 0.00003901
Iteration 25/1000 | Loss: 0.00003901
Iteration 26/1000 | Loss: 0.00003901
Iteration 27/1000 | Loss: 0.00003901
Iteration 28/1000 | Loss: 0.00003901
Iteration 29/1000 | Loss: 0.00003901
Iteration 30/1000 | Loss: 0.00003901
Iteration 31/1000 | Loss: 0.00003901
Iteration 32/1000 | Loss: 0.00003901
Iteration 33/1000 | Loss: 0.00003901
Iteration 34/1000 | Loss: 0.00003901
Iteration 35/1000 | Loss: 0.00003898
Iteration 36/1000 | Loss: 0.00003898
Iteration 37/1000 | Loss: 0.00003897
Iteration 38/1000 | Loss: 0.00003896
Iteration 39/1000 | Loss: 0.00003896
Iteration 40/1000 | Loss: 0.00003896
Iteration 41/1000 | Loss: 0.00003895
Iteration 42/1000 | Loss: 0.00003895
Iteration 43/1000 | Loss: 0.00003895
Iteration 44/1000 | Loss: 0.00003895
Iteration 45/1000 | Loss: 0.00003895
Iteration 46/1000 | Loss: 0.00003895
Iteration 47/1000 | Loss: 0.00003894
Iteration 48/1000 | Loss: 0.00003894
Iteration 49/1000 | Loss: 0.00003893
Iteration 50/1000 | Loss: 0.00003893
Iteration 51/1000 | Loss: 0.00003892
Iteration 52/1000 | Loss: 0.00003892
Iteration 53/1000 | Loss: 0.00003892
Iteration 54/1000 | Loss: 0.00003891
Iteration 55/1000 | Loss: 0.00003891
Iteration 56/1000 | Loss: 0.00003891
Iteration 57/1000 | Loss: 0.00003891
Iteration 58/1000 | Loss: 0.00003890
Iteration 59/1000 | Loss: 0.00003890
Iteration 60/1000 | Loss: 0.00003889
Iteration 61/1000 | Loss: 0.00003889
Iteration 62/1000 | Loss: 0.00003889
Iteration 63/1000 | Loss: 0.00003889
Iteration 64/1000 | Loss: 0.00003888
Iteration 65/1000 | Loss: 0.00003888
Iteration 66/1000 | Loss: 0.00003887
Iteration 67/1000 | Loss: 0.00003887
Iteration 68/1000 | Loss: 0.00003886
Iteration 69/1000 | Loss: 0.00003886
Iteration 70/1000 | Loss: 0.00003886
Iteration 71/1000 | Loss: 0.00003886
Iteration 72/1000 | Loss: 0.00003885
Iteration 73/1000 | Loss: 0.00003885
Iteration 74/1000 | Loss: 0.00003885
Iteration 75/1000 | Loss: 0.00003885
Iteration 76/1000 | Loss: 0.00003884
Iteration 77/1000 | Loss: 0.00003882
Iteration 78/1000 | Loss: 0.00003882
Iteration 79/1000 | Loss: 0.00003882
Iteration 80/1000 | Loss: 0.00003882
Iteration 81/1000 | Loss: 0.00003882
Iteration 82/1000 | Loss: 0.00003881
Iteration 83/1000 | Loss: 0.00003881
Iteration 84/1000 | Loss: 0.00003881
Iteration 85/1000 | Loss: 0.00003881
Iteration 86/1000 | Loss: 0.00003880
Iteration 87/1000 | Loss: 0.00003880
Iteration 88/1000 | Loss: 0.00003880
Iteration 89/1000 | Loss: 0.00003879
Iteration 90/1000 | Loss: 0.00003878
Iteration 91/1000 | Loss: 0.00003878
Iteration 92/1000 | Loss: 0.00003877
Iteration 93/1000 | Loss: 0.00003877
Iteration 94/1000 | Loss: 0.00003877
Iteration 95/1000 | Loss: 0.00003877
Iteration 96/1000 | Loss: 0.00003876
Iteration 97/1000 | Loss: 0.00003876
Iteration 98/1000 | Loss: 0.00003876
Iteration 99/1000 | Loss: 0.00003876
Iteration 100/1000 | Loss: 0.00003876
Iteration 101/1000 | Loss: 0.00003876
Iteration 102/1000 | Loss: 0.00003876
Iteration 103/1000 | Loss: 0.00003876
Iteration 104/1000 | Loss: 0.00003875
Iteration 105/1000 | Loss: 0.00003875
Iteration 106/1000 | Loss: 0.00003874
Iteration 107/1000 | Loss: 0.00003874
Iteration 108/1000 | Loss: 0.00003874
Iteration 109/1000 | Loss: 0.00003874
Iteration 110/1000 | Loss: 0.00003874
Iteration 111/1000 | Loss: 0.00003873
Iteration 112/1000 | Loss: 0.00003873
Iteration 113/1000 | Loss: 0.00003873
Iteration 114/1000 | Loss: 0.00003873
Iteration 115/1000 | Loss: 0.00003873
Iteration 116/1000 | Loss: 0.00003873
Iteration 117/1000 | Loss: 0.00003872
Iteration 118/1000 | Loss: 0.00003872
Iteration 119/1000 | Loss: 0.00003872
Iteration 120/1000 | Loss: 0.00003872
Iteration 121/1000 | Loss: 0.00003872
Iteration 122/1000 | Loss: 0.00003872
Iteration 123/1000 | Loss: 0.00003872
Iteration 124/1000 | Loss: 0.00003872
Iteration 125/1000 | Loss: 0.00003872
Iteration 126/1000 | Loss: 0.00003871
Iteration 127/1000 | Loss: 0.00003871
Iteration 128/1000 | Loss: 0.00003871
Iteration 129/1000 | Loss: 0.00003871
Iteration 130/1000 | Loss: 0.00003871
Iteration 131/1000 | Loss: 0.00003871
Iteration 132/1000 | Loss: 0.00003871
Iteration 133/1000 | Loss: 0.00003870
Iteration 134/1000 | Loss: 0.00003870
Iteration 135/1000 | Loss: 0.00003870
Iteration 136/1000 | Loss: 0.00003870
Iteration 137/1000 | Loss: 0.00003870
Iteration 138/1000 | Loss: 0.00003870
Iteration 139/1000 | Loss: 0.00003870
Iteration 140/1000 | Loss: 0.00003870
Iteration 141/1000 | Loss: 0.00003870
Iteration 142/1000 | Loss: 0.00003870
Iteration 143/1000 | Loss: 0.00003870
Iteration 144/1000 | Loss: 0.00003870
Iteration 145/1000 | Loss: 0.00003870
Iteration 146/1000 | Loss: 0.00003870
Iteration 147/1000 | Loss: 0.00003870
Iteration 148/1000 | Loss: 0.00003870
Iteration 149/1000 | Loss: 0.00003870
Iteration 150/1000 | Loss: 0.00003870
Iteration 151/1000 | Loss: 0.00003870
Iteration 152/1000 | Loss: 0.00003870
Iteration 153/1000 | Loss: 0.00003870
Iteration 154/1000 | Loss: 0.00003870
Iteration 155/1000 | Loss: 0.00003870
Iteration 156/1000 | Loss: 0.00003870
Iteration 157/1000 | Loss: 0.00003870
Iteration 158/1000 | Loss: 0.00003870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [3.8700189179508016e-05, 3.8700189179508016e-05, 3.8700189179508016e-05, 3.8700189179508016e-05, 3.8700189179508016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8700189179508016e-05

Optimization complete. Final v2v error: 5.3096699714660645 mm

Highest mean error: 7.669723987579346 mm for frame 66

Lowest mean error: 4.36103630065918 mm for frame 167

Saving results

Total time: 73.28227806091309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00347861
Iteration 2/25 | Loss: 0.00107115
Iteration 3/25 | Loss: 0.00094648
Iteration 4/25 | Loss: 0.00091615
Iteration 5/25 | Loss: 0.00090970
Iteration 6/25 | Loss: 0.00090725
Iteration 7/25 | Loss: 0.00090683
Iteration 8/25 | Loss: 0.00090683
Iteration 9/25 | Loss: 0.00090683
Iteration 10/25 | Loss: 0.00090683
Iteration 11/25 | Loss: 0.00090683
Iteration 12/25 | Loss: 0.00090683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009068349609151483, 0.0009068349609151483, 0.0009068349609151483, 0.0009068349609151483, 0.0009068349609151483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009068349609151483

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29763281
Iteration 2/25 | Loss: 0.00070540
Iteration 3/25 | Loss: 0.00070539
Iteration 4/25 | Loss: 0.00070539
Iteration 5/25 | Loss: 0.00070539
Iteration 6/25 | Loss: 0.00070539
Iteration 7/25 | Loss: 0.00070539
Iteration 8/25 | Loss: 0.00070539
Iteration 9/25 | Loss: 0.00070539
Iteration 10/25 | Loss: 0.00070539
Iteration 11/25 | Loss: 0.00070539
Iteration 12/25 | Loss: 0.00070539
Iteration 13/25 | Loss: 0.00070539
Iteration 14/25 | Loss: 0.00070539
Iteration 15/25 | Loss: 0.00070539
Iteration 16/25 | Loss: 0.00070539
Iteration 17/25 | Loss: 0.00070539
Iteration 18/25 | Loss: 0.00070539
Iteration 19/25 | Loss: 0.00070539
Iteration 20/25 | Loss: 0.00070539
Iteration 21/25 | Loss: 0.00070539
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007053922163322568, 0.0007053922163322568, 0.0007053922163322568, 0.0007053922163322568, 0.0007053922163322568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007053922163322568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070539
Iteration 2/1000 | Loss: 0.00008670
Iteration 3/1000 | Loss: 0.00004158
Iteration 4/1000 | Loss: 0.00003231
Iteration 5/1000 | Loss: 0.00002908
Iteration 6/1000 | Loss: 0.00002736
Iteration 7/1000 | Loss: 0.00002634
Iteration 8/1000 | Loss: 0.00002568
Iteration 9/1000 | Loss: 0.00002530
Iteration 10/1000 | Loss: 0.00002490
Iteration 11/1000 | Loss: 0.00002474
Iteration 12/1000 | Loss: 0.00002454
Iteration 13/1000 | Loss: 0.00002443
Iteration 14/1000 | Loss: 0.00002424
Iteration 15/1000 | Loss: 0.00002420
Iteration 16/1000 | Loss: 0.00002413
Iteration 17/1000 | Loss: 0.00002412
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002411
Iteration 20/1000 | Loss: 0.00002411
Iteration 21/1000 | Loss: 0.00002410
Iteration 22/1000 | Loss: 0.00002409
Iteration 23/1000 | Loss: 0.00002409
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002408
Iteration 27/1000 | Loss: 0.00002408
Iteration 28/1000 | Loss: 0.00002408
Iteration 29/1000 | Loss: 0.00002407
Iteration 30/1000 | Loss: 0.00002407
Iteration 31/1000 | Loss: 0.00002406
Iteration 32/1000 | Loss: 0.00002405
Iteration 33/1000 | Loss: 0.00002405
Iteration 34/1000 | Loss: 0.00002404
Iteration 35/1000 | Loss: 0.00002404
Iteration 36/1000 | Loss: 0.00002404
Iteration 37/1000 | Loss: 0.00002403
Iteration 38/1000 | Loss: 0.00002402
Iteration 39/1000 | Loss: 0.00002402
Iteration 40/1000 | Loss: 0.00002401
Iteration 41/1000 | Loss: 0.00002399
Iteration 42/1000 | Loss: 0.00002399
Iteration 43/1000 | Loss: 0.00002397
Iteration 44/1000 | Loss: 0.00002397
Iteration 45/1000 | Loss: 0.00002396
Iteration 46/1000 | Loss: 0.00002395
Iteration 47/1000 | Loss: 0.00002394
Iteration 48/1000 | Loss: 0.00002393
Iteration 49/1000 | Loss: 0.00002393
Iteration 50/1000 | Loss: 0.00002393
Iteration 51/1000 | Loss: 0.00002393
Iteration 52/1000 | Loss: 0.00002392
Iteration 53/1000 | Loss: 0.00002392
Iteration 54/1000 | Loss: 0.00002392
Iteration 55/1000 | Loss: 0.00002392
Iteration 56/1000 | Loss: 0.00002392
Iteration 57/1000 | Loss: 0.00002391
Iteration 58/1000 | Loss: 0.00002391
Iteration 59/1000 | Loss: 0.00002390
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002390
Iteration 62/1000 | Loss: 0.00002389
Iteration 63/1000 | Loss: 0.00002389
Iteration 64/1000 | Loss: 0.00002389
Iteration 65/1000 | Loss: 0.00002389
Iteration 66/1000 | Loss: 0.00002389
Iteration 67/1000 | Loss: 0.00002389
Iteration 68/1000 | Loss: 0.00002388
Iteration 69/1000 | Loss: 0.00002388
Iteration 70/1000 | Loss: 0.00002388
Iteration 71/1000 | Loss: 0.00002388
Iteration 72/1000 | Loss: 0.00002388
Iteration 73/1000 | Loss: 0.00002388
Iteration 74/1000 | Loss: 0.00002388
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002388
Iteration 78/1000 | Loss: 0.00002388
Iteration 79/1000 | Loss: 0.00002388
Iteration 80/1000 | Loss: 0.00002388
Iteration 81/1000 | Loss: 0.00002388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.3878939828136936e-05, 2.3878939828136936e-05, 2.3878939828136936e-05, 2.3878939828136936e-05, 2.3878939828136936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3878939828136936e-05

Optimization complete. Final v2v error: 4.263163089752197 mm

Highest mean error: 4.623842239379883 mm for frame 27

Lowest mean error: 3.945416212081909 mm for frame 100

Saving results

Total time: 42.73603820800781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618900
Iteration 2/25 | Loss: 0.00127024
Iteration 3/25 | Loss: 0.00106933
Iteration 4/25 | Loss: 0.00091155
Iteration 5/25 | Loss: 0.00088521
Iteration 6/25 | Loss: 0.00088523
Iteration 7/25 | Loss: 0.00087604
Iteration 8/25 | Loss: 0.00087240
Iteration 9/25 | Loss: 0.00086812
Iteration 10/25 | Loss: 0.00086634
Iteration 11/25 | Loss: 0.00086551
Iteration 12/25 | Loss: 0.00086504
Iteration 13/25 | Loss: 0.00086483
Iteration 14/25 | Loss: 0.00086473
Iteration 15/25 | Loss: 0.00086821
Iteration 16/25 | Loss: 0.00086289
Iteration 17/25 | Loss: 0.00086177
Iteration 18/25 | Loss: 0.00086133
Iteration 19/25 | Loss: 0.00086124
Iteration 20/25 | Loss: 0.00086124
Iteration 21/25 | Loss: 0.00086123
Iteration 22/25 | Loss: 0.00086123
Iteration 23/25 | Loss: 0.00086123
Iteration 24/25 | Loss: 0.00086123
Iteration 25/25 | Loss: 0.00086123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.11707926
Iteration 2/25 | Loss: 0.00055783
Iteration 3/25 | Loss: 0.00055783
Iteration 4/25 | Loss: 0.00055783
Iteration 5/25 | Loss: 0.00055783
Iteration 6/25 | Loss: 0.00055783
Iteration 7/25 | Loss: 0.00055783
Iteration 8/25 | Loss: 0.00055783
Iteration 9/25 | Loss: 0.00055783
Iteration 10/25 | Loss: 0.00055783
Iteration 11/25 | Loss: 0.00055783
Iteration 12/25 | Loss: 0.00055783
Iteration 13/25 | Loss: 0.00055783
Iteration 14/25 | Loss: 0.00055783
Iteration 15/25 | Loss: 0.00055783
Iteration 16/25 | Loss: 0.00055783
Iteration 17/25 | Loss: 0.00055783
Iteration 18/25 | Loss: 0.00055783
Iteration 19/25 | Loss: 0.00055783
Iteration 20/25 | Loss: 0.00055783
Iteration 21/25 | Loss: 0.00055783
Iteration 22/25 | Loss: 0.00055783
Iteration 23/25 | Loss: 0.00055783
Iteration 24/25 | Loss: 0.00055783
Iteration 25/25 | Loss: 0.00055783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055783
Iteration 2/1000 | Loss: 0.00005962
Iteration 3/1000 | Loss: 0.00003350
Iteration 4/1000 | Loss: 0.00002921
Iteration 5/1000 | Loss: 0.00002743
Iteration 6/1000 | Loss: 0.00002621
Iteration 7/1000 | Loss: 0.00002544
Iteration 8/1000 | Loss: 0.00002492
Iteration 9/1000 | Loss: 0.00002452
Iteration 10/1000 | Loss: 0.00002439
Iteration 11/1000 | Loss: 0.00002438
Iteration 12/1000 | Loss: 0.00002413
Iteration 13/1000 | Loss: 0.00002397
Iteration 14/1000 | Loss: 0.00002387
Iteration 15/1000 | Loss: 0.00002379
Iteration 16/1000 | Loss: 0.00002375
Iteration 17/1000 | Loss: 0.00002375
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002368
Iteration 20/1000 | Loss: 0.00002368
Iteration 21/1000 | Loss: 0.00002368
Iteration 22/1000 | Loss: 0.00002367
Iteration 23/1000 | Loss: 0.00002367
Iteration 24/1000 | Loss: 0.00002364
Iteration 25/1000 | Loss: 0.00002363
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002355
Iteration 29/1000 | Loss: 0.00002355
Iteration 30/1000 | Loss: 0.00002355
Iteration 31/1000 | Loss: 0.00002355
Iteration 32/1000 | Loss: 0.00002354
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002351
Iteration 36/1000 | Loss: 0.00002351
Iteration 37/1000 | Loss: 0.00002351
Iteration 38/1000 | Loss: 0.00002351
Iteration 39/1000 | Loss: 0.00002351
Iteration 40/1000 | Loss: 0.00002351
Iteration 41/1000 | Loss: 0.00002351
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00002350
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002350
Iteration 47/1000 | Loss: 0.00002350
Iteration 48/1000 | Loss: 0.00002350
Iteration 49/1000 | Loss: 0.00002350
Iteration 50/1000 | Loss: 0.00002349
Iteration 51/1000 | Loss: 0.00002349
Iteration 52/1000 | Loss: 0.00002349
Iteration 53/1000 | Loss: 0.00002349
Iteration 54/1000 | Loss: 0.00002349
Iteration 55/1000 | Loss: 0.00002348
Iteration 56/1000 | Loss: 0.00002348
Iteration 57/1000 | Loss: 0.00002348
Iteration 58/1000 | Loss: 0.00002348
Iteration 59/1000 | Loss: 0.00002348
Iteration 60/1000 | Loss: 0.00002348
Iteration 61/1000 | Loss: 0.00002348
Iteration 62/1000 | Loss: 0.00002348
Iteration 63/1000 | Loss: 0.00002348
Iteration 64/1000 | Loss: 0.00002348
Iteration 65/1000 | Loss: 0.00002348
Iteration 66/1000 | Loss: 0.00002348
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002348
Iteration 69/1000 | Loss: 0.00002348
Iteration 70/1000 | Loss: 0.00002348
Iteration 71/1000 | Loss: 0.00002348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [2.3482427423004992e-05, 2.3482427423004992e-05, 2.3482427423004992e-05, 2.3482427423004992e-05, 2.3482427423004992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3482427423004992e-05

Optimization complete. Final v2v error: 4.266362190246582 mm

Highest mean error: 8.57535457611084 mm for frame 194

Lowest mean error: 3.893963098526001 mm for frame 178

Saving results

Total time: 64.53050231933594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01206668
Iteration 2/25 | Loss: 0.00202145
Iteration 3/25 | Loss: 0.00132478
Iteration 4/25 | Loss: 0.00125270
Iteration 5/25 | Loss: 0.00123061
Iteration 6/25 | Loss: 0.00122382
Iteration 7/25 | Loss: 0.00122718
Iteration 8/25 | Loss: 0.00122109
Iteration 9/25 | Loss: 0.00122182
Iteration 10/25 | Loss: 0.00121716
Iteration 11/25 | Loss: 0.00121915
Iteration 12/25 | Loss: 0.00122033
Iteration 13/25 | Loss: 0.00121893
Iteration 14/25 | Loss: 0.00122387
Iteration 15/25 | Loss: 0.00122162
Iteration 16/25 | Loss: 0.00121962
Iteration 17/25 | Loss: 0.00122039
Iteration 18/25 | Loss: 0.00122159
Iteration 19/25 | Loss: 0.00122107
Iteration 20/25 | Loss: 0.00122319
Iteration 21/25 | Loss: 0.00122063
Iteration 22/25 | Loss: 0.00122094
Iteration 23/25 | Loss: 0.00122282
Iteration 24/25 | Loss: 0.00121423
Iteration 25/25 | Loss: 0.00121433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34676898
Iteration 2/25 | Loss: 0.00161716
Iteration 3/25 | Loss: 0.00161713
Iteration 4/25 | Loss: 0.00161713
Iteration 5/25 | Loss: 0.00161713
Iteration 6/25 | Loss: 0.00161713
Iteration 7/25 | Loss: 0.00161713
Iteration 8/25 | Loss: 0.00161713
Iteration 9/25 | Loss: 0.00161713
Iteration 10/25 | Loss: 0.00161713
Iteration 11/25 | Loss: 0.00161713
Iteration 12/25 | Loss: 0.00161713
Iteration 13/25 | Loss: 0.00161713
Iteration 14/25 | Loss: 0.00161713
Iteration 15/25 | Loss: 0.00161713
Iteration 16/25 | Loss: 0.00161713
Iteration 17/25 | Loss: 0.00161713
Iteration 18/25 | Loss: 0.00161713
Iteration 19/25 | Loss: 0.00161713
Iteration 20/25 | Loss: 0.00161713
Iteration 21/25 | Loss: 0.00161713
Iteration 22/25 | Loss: 0.00161713
Iteration 23/25 | Loss: 0.00161713
Iteration 24/25 | Loss: 0.00161713
Iteration 25/25 | Loss: 0.00161713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161713
Iteration 2/1000 | Loss: 0.00209423
Iteration 3/1000 | Loss: 0.00105024
Iteration 4/1000 | Loss: 0.00177448
Iteration 5/1000 | Loss: 0.00214186
Iteration 6/1000 | Loss: 0.00129208
Iteration 7/1000 | Loss: 0.00083489
Iteration 8/1000 | Loss: 0.00092721
Iteration 9/1000 | Loss: 0.00119545
Iteration 10/1000 | Loss: 0.00162640
Iteration 11/1000 | Loss: 0.00069620
Iteration 12/1000 | Loss: 0.00066633
Iteration 13/1000 | Loss: 0.00097974
Iteration 14/1000 | Loss: 0.00103609
Iteration 15/1000 | Loss: 0.00129767
Iteration 16/1000 | Loss: 0.00076248
Iteration 17/1000 | Loss: 0.00114820
Iteration 18/1000 | Loss: 0.00072054
Iteration 19/1000 | Loss: 0.00106335
Iteration 20/1000 | Loss: 0.00052355
Iteration 21/1000 | Loss: 0.00051914
Iteration 22/1000 | Loss: 0.00064219
Iteration 23/1000 | Loss: 0.00068493
Iteration 24/1000 | Loss: 0.00065782
Iteration 25/1000 | Loss: 0.00072273
Iteration 26/1000 | Loss: 0.00068076
Iteration 27/1000 | Loss: 0.00063321
Iteration 28/1000 | Loss: 0.00044660
Iteration 29/1000 | Loss: 0.00050951
Iteration 30/1000 | Loss: 0.00047160
Iteration 31/1000 | Loss: 0.00032528
Iteration 32/1000 | Loss: 0.00042254
Iteration 33/1000 | Loss: 0.00050679
Iteration 34/1000 | Loss: 0.00038004
Iteration 35/1000 | Loss: 0.00043029
Iteration 36/1000 | Loss: 0.00029461
Iteration 37/1000 | Loss: 0.00040529
Iteration 38/1000 | Loss: 0.00049615
Iteration 39/1000 | Loss: 0.00048153
Iteration 40/1000 | Loss: 0.00044230
Iteration 41/1000 | Loss: 0.00044839
Iteration 42/1000 | Loss: 0.00041430
Iteration 43/1000 | Loss: 0.00039589
Iteration 44/1000 | Loss: 0.00050995
Iteration 45/1000 | Loss: 0.00051931
Iteration 46/1000 | Loss: 0.00051501
Iteration 47/1000 | Loss: 0.00046685
Iteration 48/1000 | Loss: 0.00039239
Iteration 49/1000 | Loss: 0.00038587
Iteration 50/1000 | Loss: 0.00050212
Iteration 51/1000 | Loss: 0.00058071
Iteration 52/1000 | Loss: 0.00053888
Iteration 53/1000 | Loss: 0.00060254
Iteration 54/1000 | Loss: 0.00054741
Iteration 55/1000 | Loss: 0.00028122
Iteration 56/1000 | Loss: 0.00035189
Iteration 57/1000 | Loss: 0.00030399
Iteration 58/1000 | Loss: 0.00046433
Iteration 59/1000 | Loss: 0.00038266
Iteration 60/1000 | Loss: 0.00041712
Iteration 61/1000 | Loss: 0.00038624
Iteration 62/1000 | Loss: 0.00045490
Iteration 63/1000 | Loss: 0.00037009
Iteration 64/1000 | Loss: 0.00043084
Iteration 65/1000 | Loss: 0.00054536
Iteration 66/1000 | Loss: 0.00056629
Iteration 67/1000 | Loss: 0.00054656
Iteration 68/1000 | Loss: 0.00044496
Iteration 69/1000 | Loss: 0.00053864
Iteration 70/1000 | Loss: 0.00039136
Iteration 71/1000 | Loss: 0.00035715
Iteration 72/1000 | Loss: 0.00058295
Iteration 73/1000 | Loss: 0.00040343
Iteration 74/1000 | Loss: 0.00035813
Iteration 75/1000 | Loss: 0.00032539
Iteration 76/1000 | Loss: 0.00044659
Iteration 77/1000 | Loss: 0.00028563
Iteration 78/1000 | Loss: 0.00027302
Iteration 79/1000 | Loss: 0.00092100
Iteration 80/1000 | Loss: 0.00154602
Iteration 81/1000 | Loss: 0.00073423
Iteration 82/1000 | Loss: 0.00041093
Iteration 83/1000 | Loss: 0.00034977
Iteration 84/1000 | Loss: 0.00012354
Iteration 85/1000 | Loss: 0.00038419
Iteration 86/1000 | Loss: 0.00034225
Iteration 87/1000 | Loss: 0.00042441
Iteration 88/1000 | Loss: 0.00042379
Iteration 89/1000 | Loss: 0.00042117
Iteration 90/1000 | Loss: 0.00067026
Iteration 91/1000 | Loss: 0.00045119
Iteration 92/1000 | Loss: 0.00049733
Iteration 93/1000 | Loss: 0.00072158
Iteration 94/1000 | Loss: 0.00034701
Iteration 95/1000 | Loss: 0.00039431
Iteration 96/1000 | Loss: 0.00044458
Iteration 97/1000 | Loss: 0.00035887
Iteration 98/1000 | Loss: 0.00044784
Iteration 99/1000 | Loss: 0.00035361
Iteration 100/1000 | Loss: 0.00039601
Iteration 101/1000 | Loss: 0.00035497
Iteration 102/1000 | Loss: 0.00032710
Iteration 103/1000 | Loss: 0.00042970
Iteration 104/1000 | Loss: 0.00058481
Iteration 105/1000 | Loss: 0.00050242
Iteration 106/1000 | Loss: 0.00057148
Iteration 107/1000 | Loss: 0.00093168
Iteration 108/1000 | Loss: 0.00031100
Iteration 109/1000 | Loss: 0.00075125
Iteration 110/1000 | Loss: 0.00068392
Iteration 111/1000 | Loss: 0.00042631
Iteration 112/1000 | Loss: 0.00042470
Iteration 113/1000 | Loss: 0.00041828
Iteration 114/1000 | Loss: 0.00045187
Iteration 115/1000 | Loss: 0.00068847
Iteration 116/1000 | Loss: 0.00049624
Iteration 117/1000 | Loss: 0.00030534
Iteration 118/1000 | Loss: 0.00031143
Iteration 119/1000 | Loss: 0.00036025
Iteration 120/1000 | Loss: 0.00085670
Iteration 121/1000 | Loss: 0.00074549
Iteration 122/1000 | Loss: 0.00057562
Iteration 123/1000 | Loss: 0.00102866
Iteration 124/1000 | Loss: 0.00033737
Iteration 125/1000 | Loss: 0.00034220
Iteration 126/1000 | Loss: 0.00030084
Iteration 127/1000 | Loss: 0.00031612
Iteration 128/1000 | Loss: 0.00035435
Iteration 129/1000 | Loss: 0.00028550
Iteration 130/1000 | Loss: 0.00031146
Iteration 131/1000 | Loss: 0.00058587
Iteration 132/1000 | Loss: 0.00040504
Iteration 133/1000 | Loss: 0.00054804
Iteration 134/1000 | Loss: 0.00040533
Iteration 135/1000 | Loss: 0.00045429
Iteration 136/1000 | Loss: 0.00036195
Iteration 137/1000 | Loss: 0.00041181
Iteration 138/1000 | Loss: 0.00036611
Iteration 139/1000 | Loss: 0.00037677
Iteration 140/1000 | Loss: 0.00034841
Iteration 141/1000 | Loss: 0.00048514
Iteration 142/1000 | Loss: 0.00038933
Iteration 143/1000 | Loss: 0.00034021
Iteration 144/1000 | Loss: 0.00031300
Iteration 145/1000 | Loss: 0.00030693
Iteration 146/1000 | Loss: 0.00028607
Iteration 147/1000 | Loss: 0.00039481
Iteration 148/1000 | Loss: 0.00042408
Iteration 149/1000 | Loss: 0.00038927
Iteration 150/1000 | Loss: 0.00037573
Iteration 151/1000 | Loss: 0.00027206
Iteration 152/1000 | Loss: 0.00032557
Iteration 153/1000 | Loss: 0.00042699
Iteration 154/1000 | Loss: 0.00027302
Iteration 155/1000 | Loss: 0.00007886
Iteration 156/1000 | Loss: 0.00036171
Iteration 157/1000 | Loss: 0.00014227
Iteration 158/1000 | Loss: 0.00013803
Iteration 159/1000 | Loss: 0.00011712
Iteration 160/1000 | Loss: 0.00011026
Iteration 161/1000 | Loss: 0.00009813
Iteration 162/1000 | Loss: 0.00008389
Iteration 163/1000 | Loss: 0.00008695
Iteration 164/1000 | Loss: 0.00014509
Iteration 165/1000 | Loss: 0.00008634
Iteration 166/1000 | Loss: 0.00014618
Iteration 167/1000 | Loss: 0.00011541
Iteration 168/1000 | Loss: 0.00012725
Iteration 169/1000 | Loss: 0.00013251
Iteration 170/1000 | Loss: 0.00014787
Iteration 171/1000 | Loss: 0.00022223
Iteration 172/1000 | Loss: 0.00023588
Iteration 173/1000 | Loss: 0.00017743
Iteration 174/1000 | Loss: 0.00023891
Iteration 175/1000 | Loss: 0.00021512
Iteration 176/1000 | Loss: 0.00021583
Iteration 177/1000 | Loss: 0.00016782
Iteration 178/1000 | Loss: 0.00007760
Iteration 179/1000 | Loss: 0.00010714
Iteration 180/1000 | Loss: 0.00015639
Iteration 181/1000 | Loss: 0.00011239
Iteration 182/1000 | Loss: 0.00006049
Iteration 183/1000 | Loss: 0.00005828
Iteration 184/1000 | Loss: 0.00005735
Iteration 185/1000 | Loss: 0.00005595
Iteration 186/1000 | Loss: 0.00005500
Iteration 187/1000 | Loss: 0.00007113
Iteration 188/1000 | Loss: 0.00005775
Iteration 189/1000 | Loss: 0.00005636
Iteration 190/1000 | Loss: 0.00005487
Iteration 191/1000 | Loss: 0.00005365
Iteration 192/1000 | Loss: 0.00005263
Iteration 193/1000 | Loss: 0.00005199
Iteration 194/1000 | Loss: 0.00005169
Iteration 195/1000 | Loss: 0.00005136
Iteration 196/1000 | Loss: 0.00005103
Iteration 197/1000 | Loss: 0.00005079
Iteration 198/1000 | Loss: 0.00005072
Iteration 199/1000 | Loss: 0.00005065
Iteration 200/1000 | Loss: 0.00005062
Iteration 201/1000 | Loss: 0.00005062
Iteration 202/1000 | Loss: 0.00005062
Iteration 203/1000 | Loss: 0.00005062
Iteration 204/1000 | Loss: 0.00005061
Iteration 205/1000 | Loss: 0.00005061
Iteration 206/1000 | Loss: 0.00005060
Iteration 207/1000 | Loss: 0.00005059
Iteration 208/1000 | Loss: 0.00005059
Iteration 209/1000 | Loss: 0.00005058
Iteration 210/1000 | Loss: 0.00005058
Iteration 211/1000 | Loss: 0.00005058
Iteration 212/1000 | Loss: 0.00005057
Iteration 213/1000 | Loss: 0.00005056
Iteration 214/1000 | Loss: 0.00005047
Iteration 215/1000 | Loss: 0.00005047
Iteration 216/1000 | Loss: 0.00005046
Iteration 217/1000 | Loss: 0.00005046
Iteration 218/1000 | Loss: 0.00005046
Iteration 219/1000 | Loss: 0.00005046
Iteration 220/1000 | Loss: 0.00005046
Iteration 221/1000 | Loss: 0.00005044
Iteration 222/1000 | Loss: 0.00005044
Iteration 223/1000 | Loss: 0.00005043
Iteration 224/1000 | Loss: 0.00005043
Iteration 225/1000 | Loss: 0.00005040
Iteration 226/1000 | Loss: 0.00005039
Iteration 227/1000 | Loss: 0.00005039
Iteration 228/1000 | Loss: 0.00005039
Iteration 229/1000 | Loss: 0.00005039
Iteration 230/1000 | Loss: 0.00005038
Iteration 231/1000 | Loss: 0.00005036
Iteration 232/1000 | Loss: 0.00005036
Iteration 233/1000 | Loss: 0.00005036
Iteration 234/1000 | Loss: 0.00005035
Iteration 235/1000 | Loss: 0.00005034
Iteration 236/1000 | Loss: 0.00005030
Iteration 237/1000 | Loss: 0.00005029
Iteration 238/1000 | Loss: 0.00005029
Iteration 239/1000 | Loss: 0.00005029
Iteration 240/1000 | Loss: 0.00005029
Iteration 241/1000 | Loss: 0.00005029
Iteration 242/1000 | Loss: 0.00005028
Iteration 243/1000 | Loss: 0.00005028
Iteration 244/1000 | Loss: 0.00005028
Iteration 245/1000 | Loss: 0.00005027
Iteration 246/1000 | Loss: 0.00005027
Iteration 247/1000 | Loss: 0.00005027
Iteration 248/1000 | Loss: 0.00005027
Iteration 249/1000 | Loss: 0.00005027
Iteration 250/1000 | Loss: 0.00005027
Iteration 251/1000 | Loss: 0.00005026
Iteration 252/1000 | Loss: 0.00005026
Iteration 253/1000 | Loss: 0.00005026
Iteration 254/1000 | Loss: 0.00005026
Iteration 255/1000 | Loss: 0.00005026
Iteration 256/1000 | Loss: 0.00005026
Iteration 257/1000 | Loss: 0.00005025
Iteration 258/1000 | Loss: 0.00005025
Iteration 259/1000 | Loss: 0.00005025
Iteration 260/1000 | Loss: 0.00005025
Iteration 261/1000 | Loss: 0.00005025
Iteration 262/1000 | Loss: 0.00005025
Iteration 263/1000 | Loss: 0.00005025
Iteration 264/1000 | Loss: 0.00005024
Iteration 265/1000 | Loss: 0.00005024
Iteration 266/1000 | Loss: 0.00005024
Iteration 267/1000 | Loss: 0.00005023
Iteration 268/1000 | Loss: 0.00005023
Iteration 269/1000 | Loss: 0.00005022
Iteration 270/1000 | Loss: 0.00005022
Iteration 271/1000 | Loss: 0.00005022
Iteration 272/1000 | Loss: 0.00005021
Iteration 273/1000 | Loss: 0.00005021
Iteration 274/1000 | Loss: 0.00005021
Iteration 275/1000 | Loss: 0.00005021
Iteration 276/1000 | Loss: 0.00005021
Iteration 277/1000 | Loss: 0.00005021
Iteration 278/1000 | Loss: 0.00005021
Iteration 279/1000 | Loss: 0.00005021
Iteration 280/1000 | Loss: 0.00005020
Iteration 281/1000 | Loss: 0.00005020
Iteration 282/1000 | Loss: 0.00005020
Iteration 283/1000 | Loss: 0.00005020
Iteration 284/1000 | Loss: 0.00005020
Iteration 285/1000 | Loss: 0.00005019
Iteration 286/1000 | Loss: 0.00005019
Iteration 287/1000 | Loss: 0.00005019
Iteration 288/1000 | Loss: 0.00005019
Iteration 289/1000 | Loss: 0.00005019
Iteration 290/1000 | Loss: 0.00005019
Iteration 291/1000 | Loss: 0.00005019
Iteration 292/1000 | Loss: 0.00005019
Iteration 293/1000 | Loss: 0.00005019
Iteration 294/1000 | Loss: 0.00005018
Iteration 295/1000 | Loss: 0.00005018
Iteration 296/1000 | Loss: 0.00005018
Iteration 297/1000 | Loss: 0.00005018
Iteration 298/1000 | Loss: 0.00005018
Iteration 299/1000 | Loss: 0.00005018
Iteration 300/1000 | Loss: 0.00005018
Iteration 301/1000 | Loss: 0.00005018
Iteration 302/1000 | Loss: 0.00005018
Iteration 303/1000 | Loss: 0.00005018
Iteration 304/1000 | Loss: 0.00005018
Iteration 305/1000 | Loss: 0.00005018
Iteration 306/1000 | Loss: 0.00005018
Iteration 307/1000 | Loss: 0.00005018
Iteration 308/1000 | Loss: 0.00005018
Iteration 309/1000 | Loss: 0.00005018
Iteration 310/1000 | Loss: 0.00005018
Iteration 311/1000 | Loss: 0.00005017
Iteration 312/1000 | Loss: 0.00005017
Iteration 313/1000 | Loss: 0.00005017
Iteration 314/1000 | Loss: 0.00005017
Iteration 315/1000 | Loss: 0.00005017
Iteration 316/1000 | Loss: 0.00005017
Iteration 317/1000 | Loss: 0.00005017
Iteration 318/1000 | Loss: 0.00005016
Iteration 319/1000 | Loss: 0.00005016
Iteration 320/1000 | Loss: 0.00005016
Iteration 321/1000 | Loss: 0.00005016
Iteration 322/1000 | Loss: 0.00005016
Iteration 323/1000 | Loss: 0.00005016
Iteration 324/1000 | Loss: 0.00005016
Iteration 325/1000 | Loss: 0.00005016
Iteration 326/1000 | Loss: 0.00005016
Iteration 327/1000 | Loss: 0.00005016
Iteration 328/1000 | Loss: 0.00005016
Iteration 329/1000 | Loss: 0.00005016
Iteration 330/1000 | Loss: 0.00005016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 330. Stopping optimization.
Last 5 losses: [5.016288560000248e-05, 5.016288560000248e-05, 5.016288560000248e-05, 5.016288560000248e-05, 5.016288560000248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.016288560000248e-05

Optimization complete. Final v2v error: 5.711530685424805 mm

Highest mean error: 6.586617946624756 mm for frame 87

Lowest mean error: 4.965942859649658 mm for frame 35

Saving results

Total time: 391.1771218776703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_1332/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_1332/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513441
Iteration 2/25 | Loss: 0.00111671
Iteration 3/25 | Loss: 0.00096494
Iteration 4/25 | Loss: 0.00093890
Iteration 5/25 | Loss: 0.00092900
Iteration 6/25 | Loss: 0.00092736
Iteration 7/25 | Loss: 0.00092731
Iteration 8/25 | Loss: 0.00092731
Iteration 9/25 | Loss: 0.00092731
Iteration 10/25 | Loss: 0.00092731
Iteration 11/25 | Loss: 0.00092731
Iteration 12/25 | Loss: 0.00092731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009273107862100005, 0.0009273107862100005, 0.0009273107862100005, 0.0009273107862100005, 0.0009273107862100005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009273107862100005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31664062
Iteration 2/25 | Loss: 0.00061319
Iteration 3/25 | Loss: 0.00061317
Iteration 4/25 | Loss: 0.00061317
Iteration 5/25 | Loss: 0.00061317
Iteration 6/25 | Loss: 0.00061317
Iteration 7/25 | Loss: 0.00061317
Iteration 8/25 | Loss: 0.00061317
Iteration 9/25 | Loss: 0.00061316
Iteration 10/25 | Loss: 0.00061316
Iteration 11/25 | Loss: 0.00061316
Iteration 12/25 | Loss: 0.00061316
Iteration 13/25 | Loss: 0.00061316
Iteration 14/25 | Loss: 0.00061316
Iteration 15/25 | Loss: 0.00061316
Iteration 16/25 | Loss: 0.00061316
Iteration 17/25 | Loss: 0.00061316
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006131645641289651, 0.0006131645641289651, 0.0006131645641289651, 0.0006131645641289651, 0.0006131645641289651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006131645641289651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061316
Iteration 2/1000 | Loss: 0.00006831
Iteration 3/1000 | Loss: 0.00003834
Iteration 4/1000 | Loss: 0.00003133
Iteration 5/1000 | Loss: 0.00002954
Iteration 6/1000 | Loss: 0.00002829
Iteration 7/1000 | Loss: 0.00002763
Iteration 8/1000 | Loss: 0.00002717
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002650
Iteration 11/1000 | Loss: 0.00002636
Iteration 12/1000 | Loss: 0.00002634
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002630
Iteration 15/1000 | Loss: 0.00002629
Iteration 16/1000 | Loss: 0.00002628
Iteration 17/1000 | Loss: 0.00002614
Iteration 18/1000 | Loss: 0.00002610
Iteration 19/1000 | Loss: 0.00002609
Iteration 20/1000 | Loss: 0.00002606
Iteration 21/1000 | Loss: 0.00002605
Iteration 22/1000 | Loss: 0.00002604
Iteration 23/1000 | Loss: 0.00002604
Iteration 24/1000 | Loss: 0.00002603
Iteration 25/1000 | Loss: 0.00002603
Iteration 26/1000 | Loss: 0.00002602
Iteration 27/1000 | Loss: 0.00002602
Iteration 28/1000 | Loss: 0.00002601
Iteration 29/1000 | Loss: 0.00002600
Iteration 30/1000 | Loss: 0.00002599
Iteration 31/1000 | Loss: 0.00002596
Iteration 32/1000 | Loss: 0.00002596
Iteration 33/1000 | Loss: 0.00002596
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00002590
Iteration 36/1000 | Loss: 0.00002587
Iteration 37/1000 | Loss: 0.00002586
Iteration 38/1000 | Loss: 0.00002586
Iteration 39/1000 | Loss: 0.00002586
Iteration 40/1000 | Loss: 0.00002585
Iteration 41/1000 | Loss: 0.00002585
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002582
Iteration 46/1000 | Loss: 0.00002582
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002581
Iteration 54/1000 | Loss: 0.00002580
Iteration 55/1000 | Loss: 0.00002580
Iteration 56/1000 | Loss: 0.00002580
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002580
Iteration 59/1000 | Loss: 0.00002580
Iteration 60/1000 | Loss: 0.00002580
Iteration 61/1000 | Loss: 0.00002579
Iteration 62/1000 | Loss: 0.00002579
Iteration 63/1000 | Loss: 0.00002579
Iteration 64/1000 | Loss: 0.00002579
Iteration 65/1000 | Loss: 0.00002579
Iteration 66/1000 | Loss: 0.00002578
Iteration 67/1000 | Loss: 0.00002578
Iteration 68/1000 | Loss: 0.00002578
Iteration 69/1000 | Loss: 0.00002578
Iteration 70/1000 | Loss: 0.00002578
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002577
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002576
Iteration 79/1000 | Loss: 0.00002576
Iteration 80/1000 | Loss: 0.00002576
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002575
Iteration 83/1000 | Loss: 0.00002575
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002575
Iteration 88/1000 | Loss: 0.00002575
Iteration 89/1000 | Loss: 0.00002575
Iteration 90/1000 | Loss: 0.00002575
Iteration 91/1000 | Loss: 0.00002575
Iteration 92/1000 | Loss: 0.00002574
Iteration 93/1000 | Loss: 0.00002574
Iteration 94/1000 | Loss: 0.00002574
Iteration 95/1000 | Loss: 0.00002574
Iteration 96/1000 | Loss: 0.00002574
Iteration 97/1000 | Loss: 0.00002574
Iteration 98/1000 | Loss: 0.00002574
Iteration 99/1000 | Loss: 0.00002574
Iteration 100/1000 | Loss: 0.00002573
Iteration 101/1000 | Loss: 0.00002573
Iteration 102/1000 | Loss: 0.00002573
Iteration 103/1000 | Loss: 0.00002573
Iteration 104/1000 | Loss: 0.00002573
Iteration 105/1000 | Loss: 0.00002573
Iteration 106/1000 | Loss: 0.00002573
Iteration 107/1000 | Loss: 0.00002573
Iteration 108/1000 | Loss: 0.00002573
Iteration 109/1000 | Loss: 0.00002573
Iteration 110/1000 | Loss: 0.00002573
Iteration 111/1000 | Loss: 0.00002573
Iteration 112/1000 | Loss: 0.00002573
Iteration 113/1000 | Loss: 0.00002573
Iteration 114/1000 | Loss: 0.00002573
Iteration 115/1000 | Loss: 0.00002573
Iteration 116/1000 | Loss: 0.00002573
Iteration 117/1000 | Loss: 0.00002573
Iteration 118/1000 | Loss: 0.00002573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.572901576058939e-05, 2.572901576058939e-05, 2.572901576058939e-05, 2.572901576058939e-05, 2.572901576058939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.572901576058939e-05

Optimization complete. Final v2v error: 4.480090141296387 mm

Highest mean error: 5.217434883117676 mm for frame 32

Lowest mean error: 4.108044624328613 mm for frame 146

Saving results

Total time: 41.365710973739624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793994
Iteration 2/25 | Loss: 0.00129614
Iteration 3/25 | Loss: 0.00122235
Iteration 4/25 | Loss: 0.00120909
Iteration 5/25 | Loss: 0.00120626
Iteration 6/25 | Loss: 0.00120626
Iteration 7/25 | Loss: 0.00120626
Iteration 8/25 | Loss: 0.00120626
Iteration 9/25 | Loss: 0.00120626
Iteration 10/25 | Loss: 0.00120626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012062648311257362, 0.0012062648311257362, 0.0012062648311257362, 0.0012062648311257362, 0.0012062648311257362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012062648311257362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48366261
Iteration 2/25 | Loss: 0.00071908
Iteration 3/25 | Loss: 0.00071907
Iteration 4/25 | Loss: 0.00071907
Iteration 5/25 | Loss: 0.00071907
Iteration 6/25 | Loss: 0.00071907
Iteration 7/25 | Loss: 0.00071907
Iteration 8/25 | Loss: 0.00071907
Iteration 9/25 | Loss: 0.00071907
Iteration 10/25 | Loss: 0.00071907
Iteration 11/25 | Loss: 0.00071907
Iteration 12/25 | Loss: 0.00071907
Iteration 13/25 | Loss: 0.00071907
Iteration 14/25 | Loss: 0.00071907
Iteration 15/25 | Loss: 0.00071907
Iteration 16/25 | Loss: 0.00071907
Iteration 17/25 | Loss: 0.00071907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000719067407771945, 0.000719067407771945, 0.000719067407771945, 0.000719067407771945, 0.000719067407771945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000719067407771945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071907
Iteration 2/1000 | Loss: 0.00003037
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001692
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001588
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001566
Iteration 12/1000 | Loss: 0.00001562
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001552
Iteration 15/1000 | Loss: 0.00001552
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001529
Iteration 20/1000 | Loss: 0.00001528
Iteration 21/1000 | Loss: 0.00001528
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001523
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001523
Iteration 28/1000 | Loss: 0.00001522
Iteration 29/1000 | Loss: 0.00001521
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001512
Iteration 41/1000 | Loss: 0.00001512
Iteration 42/1000 | Loss: 0.00001511
Iteration 43/1000 | Loss: 0.00001511
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001508
Iteration 46/1000 | Loss: 0.00001508
Iteration 47/1000 | Loss: 0.00001508
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001507
Iteration 53/1000 | Loss: 0.00001507
Iteration 54/1000 | Loss: 0.00001507
Iteration 55/1000 | Loss: 0.00001507
Iteration 56/1000 | Loss: 0.00001507
Iteration 57/1000 | Loss: 0.00001507
Iteration 58/1000 | Loss: 0.00001507
Iteration 59/1000 | Loss: 0.00001507
Iteration 60/1000 | Loss: 0.00001507
Iteration 61/1000 | Loss: 0.00001507
Iteration 62/1000 | Loss: 0.00001506
Iteration 63/1000 | Loss: 0.00001505
Iteration 64/1000 | Loss: 0.00001504
Iteration 65/1000 | Loss: 0.00001504
Iteration 66/1000 | Loss: 0.00001504
Iteration 67/1000 | Loss: 0.00001503
Iteration 68/1000 | Loss: 0.00001503
Iteration 69/1000 | Loss: 0.00001503
Iteration 70/1000 | Loss: 0.00001502
Iteration 71/1000 | Loss: 0.00001502
Iteration 72/1000 | Loss: 0.00001502
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001500
Iteration 76/1000 | Loss: 0.00001500
Iteration 77/1000 | Loss: 0.00001499
Iteration 78/1000 | Loss: 0.00001499
Iteration 79/1000 | Loss: 0.00001499
Iteration 80/1000 | Loss: 0.00001499
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001497
Iteration 84/1000 | Loss: 0.00001497
Iteration 85/1000 | Loss: 0.00001497
Iteration 86/1000 | Loss: 0.00001496
Iteration 87/1000 | Loss: 0.00001496
Iteration 88/1000 | Loss: 0.00001496
Iteration 89/1000 | Loss: 0.00001496
Iteration 90/1000 | Loss: 0.00001496
Iteration 91/1000 | Loss: 0.00001495
Iteration 92/1000 | Loss: 0.00001495
Iteration 93/1000 | Loss: 0.00001495
Iteration 94/1000 | Loss: 0.00001495
Iteration 95/1000 | Loss: 0.00001495
Iteration 96/1000 | Loss: 0.00001495
Iteration 97/1000 | Loss: 0.00001494
Iteration 98/1000 | Loss: 0.00001494
Iteration 99/1000 | Loss: 0.00001494
Iteration 100/1000 | Loss: 0.00001494
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001492
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001492
Iteration 112/1000 | Loss: 0.00001492
Iteration 113/1000 | Loss: 0.00001492
Iteration 114/1000 | Loss: 0.00001492
Iteration 115/1000 | Loss: 0.00001492
Iteration 116/1000 | Loss: 0.00001491
Iteration 117/1000 | Loss: 0.00001491
Iteration 118/1000 | Loss: 0.00001491
Iteration 119/1000 | Loss: 0.00001491
Iteration 120/1000 | Loss: 0.00001491
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001489
Iteration 124/1000 | Loss: 0.00001489
Iteration 125/1000 | Loss: 0.00001489
Iteration 126/1000 | Loss: 0.00001489
Iteration 127/1000 | Loss: 0.00001488
Iteration 128/1000 | Loss: 0.00001488
Iteration 129/1000 | Loss: 0.00001488
Iteration 130/1000 | Loss: 0.00001488
Iteration 131/1000 | Loss: 0.00001488
Iteration 132/1000 | Loss: 0.00001487
Iteration 133/1000 | Loss: 0.00001487
Iteration 134/1000 | Loss: 0.00001487
Iteration 135/1000 | Loss: 0.00001486
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001484
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001484
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001483
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001482
Iteration 158/1000 | Loss: 0.00001482
Iteration 159/1000 | Loss: 0.00001482
Iteration 160/1000 | Loss: 0.00001482
Iteration 161/1000 | Loss: 0.00001481
Iteration 162/1000 | Loss: 0.00001481
Iteration 163/1000 | Loss: 0.00001481
Iteration 164/1000 | Loss: 0.00001481
Iteration 165/1000 | Loss: 0.00001480
Iteration 166/1000 | Loss: 0.00001480
Iteration 167/1000 | Loss: 0.00001480
Iteration 168/1000 | Loss: 0.00001480
Iteration 169/1000 | Loss: 0.00001480
Iteration 170/1000 | Loss: 0.00001480
Iteration 171/1000 | Loss: 0.00001480
Iteration 172/1000 | Loss: 0.00001480
Iteration 173/1000 | Loss: 0.00001480
Iteration 174/1000 | Loss: 0.00001480
Iteration 175/1000 | Loss: 0.00001480
Iteration 176/1000 | Loss: 0.00001480
Iteration 177/1000 | Loss: 0.00001480
Iteration 178/1000 | Loss: 0.00001479
Iteration 179/1000 | Loss: 0.00001479
Iteration 180/1000 | Loss: 0.00001479
Iteration 181/1000 | Loss: 0.00001479
Iteration 182/1000 | Loss: 0.00001479
Iteration 183/1000 | Loss: 0.00001479
Iteration 184/1000 | Loss: 0.00001479
Iteration 185/1000 | Loss: 0.00001479
Iteration 186/1000 | Loss: 0.00001479
Iteration 187/1000 | Loss: 0.00001479
Iteration 188/1000 | Loss: 0.00001479
Iteration 189/1000 | Loss: 0.00001479
Iteration 190/1000 | Loss: 0.00001479
Iteration 191/1000 | Loss: 0.00001478
Iteration 192/1000 | Loss: 0.00001478
Iteration 193/1000 | Loss: 0.00001478
Iteration 194/1000 | Loss: 0.00001478
Iteration 195/1000 | Loss: 0.00001478
Iteration 196/1000 | Loss: 0.00001478
Iteration 197/1000 | Loss: 0.00001478
Iteration 198/1000 | Loss: 0.00001478
Iteration 199/1000 | Loss: 0.00001478
Iteration 200/1000 | Loss: 0.00001478
Iteration 201/1000 | Loss: 0.00001478
Iteration 202/1000 | Loss: 0.00001478
Iteration 203/1000 | Loss: 0.00001478
Iteration 204/1000 | Loss: 0.00001478
Iteration 205/1000 | Loss: 0.00001478
Iteration 206/1000 | Loss: 0.00001478
Iteration 207/1000 | Loss: 0.00001478
Iteration 208/1000 | Loss: 0.00001478
Iteration 209/1000 | Loss: 0.00001478
Iteration 210/1000 | Loss: 0.00001478
Iteration 211/1000 | Loss: 0.00001478
Iteration 212/1000 | Loss: 0.00001478
Iteration 213/1000 | Loss: 0.00001477
Iteration 214/1000 | Loss: 0.00001477
Iteration 215/1000 | Loss: 0.00001477
Iteration 216/1000 | Loss: 0.00001477
Iteration 217/1000 | Loss: 0.00001477
Iteration 218/1000 | Loss: 0.00001477
Iteration 219/1000 | Loss: 0.00001477
Iteration 220/1000 | Loss: 0.00001477
Iteration 221/1000 | Loss: 0.00001477
Iteration 222/1000 | Loss: 0.00001477
Iteration 223/1000 | Loss: 0.00001477
Iteration 224/1000 | Loss: 0.00001477
Iteration 225/1000 | Loss: 0.00001477
Iteration 226/1000 | Loss: 0.00001477
Iteration 227/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.477427485951921e-05, 1.477427485951921e-05, 1.477427485951921e-05, 1.477427485951921e-05, 1.477427485951921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.477427485951921e-05

Optimization complete. Final v2v error: 3.301797389984131 mm

Highest mean error: 3.5965094566345215 mm for frame 154

Lowest mean error: 3.10392165184021 mm for frame 65

Saving results

Total time: 48.227818965911865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019660
Iteration 2/25 | Loss: 0.00205016
Iteration 3/25 | Loss: 0.00160291
Iteration 4/25 | Loss: 0.00146383
Iteration 5/25 | Loss: 0.00145807
Iteration 6/25 | Loss: 0.00138576
Iteration 7/25 | Loss: 0.00134008
Iteration 8/25 | Loss: 0.00134198
Iteration 9/25 | Loss: 0.00131899
Iteration 10/25 | Loss: 0.00132017
Iteration 11/25 | Loss: 0.00130763
Iteration 12/25 | Loss: 0.00130505
Iteration 13/25 | Loss: 0.00130715
Iteration 14/25 | Loss: 0.00129535
Iteration 15/25 | Loss: 0.00129360
Iteration 16/25 | Loss: 0.00128853
Iteration 17/25 | Loss: 0.00128907
Iteration 18/25 | Loss: 0.00129017
Iteration 19/25 | Loss: 0.00128935
Iteration 20/25 | Loss: 0.00128879
Iteration 21/25 | Loss: 0.00128825
Iteration 22/25 | Loss: 0.00128860
Iteration 23/25 | Loss: 0.00128859
Iteration 24/25 | Loss: 0.00128841
Iteration 25/25 | Loss: 0.00128932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44318676
Iteration 2/25 | Loss: 0.00135911
Iteration 3/25 | Loss: 0.00115824
Iteration 4/25 | Loss: 0.00115823
Iteration 5/25 | Loss: 0.00115823
Iteration 6/25 | Loss: 0.00115823
Iteration 7/25 | Loss: 0.00115823
Iteration 8/25 | Loss: 0.00115823
Iteration 9/25 | Loss: 0.00115823
Iteration 10/25 | Loss: 0.00115823
Iteration 11/25 | Loss: 0.00115823
Iteration 12/25 | Loss: 0.00115823
Iteration 13/25 | Loss: 0.00115823
Iteration 14/25 | Loss: 0.00115823
Iteration 15/25 | Loss: 0.00115823
Iteration 16/25 | Loss: 0.00115823
Iteration 17/25 | Loss: 0.00115823
Iteration 18/25 | Loss: 0.00115823
Iteration 19/25 | Loss: 0.00115823
Iteration 20/25 | Loss: 0.00115823
Iteration 21/25 | Loss: 0.00115823
Iteration 22/25 | Loss: 0.00115823
Iteration 23/25 | Loss: 0.00115823
Iteration 24/25 | Loss: 0.00115823
Iteration 25/25 | Loss: 0.00115823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115823
Iteration 2/1000 | Loss: 0.00027418
Iteration 3/1000 | Loss: 0.00193517
Iteration 4/1000 | Loss: 0.00033108
Iteration 5/1000 | Loss: 0.00006852
Iteration 6/1000 | Loss: 0.00015995
Iteration 7/1000 | Loss: 0.00020056
Iteration 8/1000 | Loss: 0.00013252
Iteration 9/1000 | Loss: 0.00028211
Iteration 10/1000 | Loss: 0.00009536
Iteration 11/1000 | Loss: 0.00011248
Iteration 12/1000 | Loss: 0.00021148
Iteration 13/1000 | Loss: 0.00006887
Iteration 14/1000 | Loss: 0.00006915
Iteration 15/1000 | Loss: 0.00016541
Iteration 16/1000 | Loss: 0.00011139
Iteration 17/1000 | Loss: 0.00006305
Iteration 18/1000 | Loss: 0.00005270
Iteration 19/1000 | Loss: 0.00014592
Iteration 20/1000 | Loss: 0.00011009
Iteration 21/1000 | Loss: 0.00016606
Iteration 22/1000 | Loss: 0.00013513
Iteration 23/1000 | Loss: 0.00015026
Iteration 24/1000 | Loss: 0.00047723
Iteration 25/1000 | Loss: 0.00026381
Iteration 26/1000 | Loss: 0.00045247
Iteration 27/1000 | Loss: 0.00015670
Iteration 28/1000 | Loss: 0.00005688
Iteration 29/1000 | Loss: 0.00018163
Iteration 30/1000 | Loss: 0.00112788
Iteration 31/1000 | Loss: 0.00041621
Iteration 32/1000 | Loss: 0.00018677
Iteration 33/1000 | Loss: 0.00016260
Iteration 34/1000 | Loss: 0.00006003
Iteration 35/1000 | Loss: 0.00005088
Iteration 36/1000 | Loss: 0.00007515
Iteration 37/1000 | Loss: 0.00005921
Iteration 38/1000 | Loss: 0.00005563
Iteration 39/1000 | Loss: 0.00006757
Iteration 40/1000 | Loss: 0.00004389
Iteration 41/1000 | Loss: 0.00012736
Iteration 42/1000 | Loss: 0.00009484
Iteration 43/1000 | Loss: 0.00004790
Iteration 44/1000 | Loss: 0.00004219
Iteration 45/1000 | Loss: 0.00006241
Iteration 46/1000 | Loss: 0.00008113
Iteration 47/1000 | Loss: 0.00004937
Iteration 48/1000 | Loss: 0.00008997
Iteration 49/1000 | Loss: 0.00022919
Iteration 50/1000 | Loss: 0.00005197
Iteration 51/1000 | Loss: 0.00014419
Iteration 52/1000 | Loss: 0.00006267
Iteration 53/1000 | Loss: 0.00008425
Iteration 54/1000 | Loss: 0.00021566
Iteration 55/1000 | Loss: 0.00004072
Iteration 56/1000 | Loss: 0.00007718
Iteration 57/1000 | Loss: 0.00007861
Iteration 58/1000 | Loss: 0.00004576
Iteration 59/1000 | Loss: 0.00035037
Iteration 60/1000 | Loss: 0.00024542
Iteration 61/1000 | Loss: 0.00023236
Iteration 62/1000 | Loss: 0.00030517
Iteration 63/1000 | Loss: 0.00023586
Iteration 64/1000 | Loss: 0.00068972
Iteration 65/1000 | Loss: 0.00013055
Iteration 66/1000 | Loss: 0.00004909
Iteration 67/1000 | Loss: 0.00006065
Iteration 68/1000 | Loss: 0.00014064
Iteration 69/1000 | Loss: 0.00004269
Iteration 70/1000 | Loss: 0.00051459
Iteration 71/1000 | Loss: 0.00012784
Iteration 72/1000 | Loss: 0.00006919
Iteration 73/1000 | Loss: 0.00007155
Iteration 74/1000 | Loss: 0.00004506
Iteration 75/1000 | Loss: 0.00010162
Iteration 76/1000 | Loss: 0.00013492
Iteration 77/1000 | Loss: 0.00008199
Iteration 78/1000 | Loss: 0.00008318
Iteration 79/1000 | Loss: 0.00004255
Iteration 80/1000 | Loss: 0.00003840
Iteration 81/1000 | Loss: 0.00003772
Iteration 82/1000 | Loss: 0.00016442
Iteration 83/1000 | Loss: 0.00005239
Iteration 84/1000 | Loss: 0.00003732
Iteration 85/1000 | Loss: 0.00005502
Iteration 86/1000 | Loss: 0.00003711
Iteration 87/1000 | Loss: 0.00003707
Iteration 88/1000 | Loss: 0.00010254
Iteration 89/1000 | Loss: 0.00003689
Iteration 90/1000 | Loss: 0.00013406
Iteration 91/1000 | Loss: 0.00055149
Iteration 92/1000 | Loss: 0.00017491
Iteration 93/1000 | Loss: 0.00015939
Iteration 94/1000 | Loss: 0.00015773
Iteration 95/1000 | Loss: 0.00016561
Iteration 96/1000 | Loss: 0.00015265
Iteration 97/1000 | Loss: 0.00010474
Iteration 98/1000 | Loss: 0.00004919
Iteration 99/1000 | Loss: 0.00003757
Iteration 100/1000 | Loss: 0.00005720
Iteration 101/1000 | Loss: 0.00015883
Iteration 102/1000 | Loss: 0.00018457
Iteration 103/1000 | Loss: 0.00018425
Iteration 104/1000 | Loss: 0.00024565
Iteration 105/1000 | Loss: 0.00024012
Iteration 106/1000 | Loss: 0.00015734
Iteration 107/1000 | Loss: 0.00006168
Iteration 108/1000 | Loss: 0.00007472
Iteration 109/1000 | Loss: 0.00004569
Iteration 110/1000 | Loss: 0.00005313
Iteration 111/1000 | Loss: 0.00012833
Iteration 112/1000 | Loss: 0.00023555
Iteration 113/1000 | Loss: 0.00014242
Iteration 114/1000 | Loss: 0.00005005
Iteration 115/1000 | Loss: 0.00008413
Iteration 116/1000 | Loss: 0.00009697
Iteration 117/1000 | Loss: 0.00008535
Iteration 118/1000 | Loss: 0.00004693
Iteration 119/1000 | Loss: 0.00005778
Iteration 120/1000 | Loss: 0.00011898
Iteration 121/1000 | Loss: 0.00012986
Iteration 122/1000 | Loss: 0.00013001
Iteration 123/1000 | Loss: 0.00019183
Iteration 124/1000 | Loss: 0.00005958
Iteration 125/1000 | Loss: 0.00007086
Iteration 126/1000 | Loss: 0.00012295
Iteration 127/1000 | Loss: 0.00015185
Iteration 128/1000 | Loss: 0.00006472
Iteration 129/1000 | Loss: 0.00004480
Iteration 130/1000 | Loss: 0.00003673
Iteration 131/1000 | Loss: 0.00005256
Iteration 132/1000 | Loss: 0.00003646
Iteration 133/1000 | Loss: 0.00004216
Iteration 134/1000 | Loss: 0.00003640
Iteration 135/1000 | Loss: 0.00003634
Iteration 136/1000 | Loss: 0.00003634
Iteration 137/1000 | Loss: 0.00003641
Iteration 138/1000 | Loss: 0.00003632
Iteration 139/1000 | Loss: 0.00003631
Iteration 140/1000 | Loss: 0.00003631
Iteration 141/1000 | Loss: 0.00003630
Iteration 142/1000 | Loss: 0.00003630
Iteration 143/1000 | Loss: 0.00003629
Iteration 144/1000 | Loss: 0.00003627
Iteration 145/1000 | Loss: 0.00003626
Iteration 146/1000 | Loss: 0.00003800
Iteration 147/1000 | Loss: 0.00003623
Iteration 148/1000 | Loss: 0.00003623
Iteration 149/1000 | Loss: 0.00003623
Iteration 150/1000 | Loss: 0.00003622
Iteration 151/1000 | Loss: 0.00003622
Iteration 152/1000 | Loss: 0.00003622
Iteration 153/1000 | Loss: 0.00003622
Iteration 154/1000 | Loss: 0.00003622
Iteration 155/1000 | Loss: 0.00003622
Iteration 156/1000 | Loss: 0.00003622
Iteration 157/1000 | Loss: 0.00003621
Iteration 158/1000 | Loss: 0.00003621
Iteration 159/1000 | Loss: 0.00003621
Iteration 160/1000 | Loss: 0.00003621
Iteration 161/1000 | Loss: 0.00003621
Iteration 162/1000 | Loss: 0.00003621
Iteration 163/1000 | Loss: 0.00003621
Iteration 164/1000 | Loss: 0.00003621
Iteration 165/1000 | Loss: 0.00003621
Iteration 166/1000 | Loss: 0.00003620
Iteration 167/1000 | Loss: 0.00003620
Iteration 168/1000 | Loss: 0.00003620
Iteration 169/1000 | Loss: 0.00003619
Iteration 170/1000 | Loss: 0.00003619
Iteration 171/1000 | Loss: 0.00003619
Iteration 172/1000 | Loss: 0.00003619
Iteration 173/1000 | Loss: 0.00003619
Iteration 174/1000 | Loss: 0.00003619
Iteration 175/1000 | Loss: 0.00003619
Iteration 176/1000 | Loss: 0.00003619
Iteration 177/1000 | Loss: 0.00003619
Iteration 178/1000 | Loss: 0.00003619
Iteration 179/1000 | Loss: 0.00003619
Iteration 180/1000 | Loss: 0.00005985
Iteration 181/1000 | Loss: 0.00014828
Iteration 182/1000 | Loss: 0.00004549
Iteration 183/1000 | Loss: 0.00003625
Iteration 184/1000 | Loss: 0.00003622
Iteration 185/1000 | Loss: 0.00003621
Iteration 186/1000 | Loss: 0.00003621
Iteration 187/1000 | Loss: 0.00004949
Iteration 188/1000 | Loss: 0.00004949
Iteration 189/1000 | Loss: 0.00053411
Iteration 190/1000 | Loss: 0.00029649
Iteration 191/1000 | Loss: 0.00040946
Iteration 192/1000 | Loss: 0.00051110
Iteration 193/1000 | Loss: 0.00019660
Iteration 194/1000 | Loss: 0.00011754
Iteration 195/1000 | Loss: 0.00008722
Iteration 196/1000 | Loss: 0.00004284
Iteration 197/1000 | Loss: 0.00003804
Iteration 198/1000 | Loss: 0.00006079
Iteration 199/1000 | Loss: 0.00003665
Iteration 200/1000 | Loss: 0.00003982
Iteration 201/1000 | Loss: 0.00003581
Iteration 202/1000 | Loss: 0.00003541
Iteration 203/1000 | Loss: 0.00004647
Iteration 204/1000 | Loss: 0.00004082
Iteration 205/1000 | Loss: 0.00003506
Iteration 206/1000 | Loss: 0.00003505
Iteration 207/1000 | Loss: 0.00003505
Iteration 208/1000 | Loss: 0.00003505
Iteration 209/1000 | Loss: 0.00003505
Iteration 210/1000 | Loss: 0.00003505
Iteration 211/1000 | Loss: 0.00003505
Iteration 212/1000 | Loss: 0.00003505
Iteration 213/1000 | Loss: 0.00003505
Iteration 214/1000 | Loss: 0.00003505
Iteration 215/1000 | Loss: 0.00003503
Iteration 216/1000 | Loss: 0.00003502
Iteration 217/1000 | Loss: 0.00004797
Iteration 218/1000 | Loss: 0.00008542
Iteration 219/1000 | Loss: 0.00004626
Iteration 220/1000 | Loss: 0.00003519
Iteration 221/1000 | Loss: 0.00003489
Iteration 222/1000 | Loss: 0.00003488
Iteration 223/1000 | Loss: 0.00003488
Iteration 224/1000 | Loss: 0.00003487
Iteration 225/1000 | Loss: 0.00003487
Iteration 226/1000 | Loss: 0.00003487
Iteration 227/1000 | Loss: 0.00003487
Iteration 228/1000 | Loss: 0.00003486
Iteration 229/1000 | Loss: 0.00003486
Iteration 230/1000 | Loss: 0.00003486
Iteration 231/1000 | Loss: 0.00003486
Iteration 232/1000 | Loss: 0.00003486
Iteration 233/1000 | Loss: 0.00003561
Iteration 234/1000 | Loss: 0.00003484
Iteration 235/1000 | Loss: 0.00003484
Iteration 236/1000 | Loss: 0.00003484
Iteration 237/1000 | Loss: 0.00003483
Iteration 238/1000 | Loss: 0.00003483
Iteration 239/1000 | Loss: 0.00003483
Iteration 240/1000 | Loss: 0.00003483
Iteration 241/1000 | Loss: 0.00003483
Iteration 242/1000 | Loss: 0.00003483
Iteration 243/1000 | Loss: 0.00003483
Iteration 244/1000 | Loss: 0.00003483
Iteration 245/1000 | Loss: 0.00003482
Iteration 246/1000 | Loss: 0.00003482
Iteration 247/1000 | Loss: 0.00003482
Iteration 248/1000 | Loss: 0.00003481
Iteration 249/1000 | Loss: 0.00003481
Iteration 250/1000 | Loss: 0.00003481
Iteration 251/1000 | Loss: 0.00003480
Iteration 252/1000 | Loss: 0.00003480
Iteration 253/1000 | Loss: 0.00003490
Iteration 254/1000 | Loss: 0.00003489
Iteration 255/1000 | Loss: 0.00007007
Iteration 256/1000 | Loss: 0.00004803
Iteration 257/1000 | Loss: 0.00003593
Iteration 258/1000 | Loss: 0.00003751
Iteration 259/1000 | Loss: 0.00008519
Iteration 260/1000 | Loss: 0.00005500
Iteration 261/1000 | Loss: 0.00003516
Iteration 262/1000 | Loss: 0.00003782
Iteration 263/1000 | Loss: 0.00004747
Iteration 264/1000 | Loss: 0.00003479
Iteration 265/1000 | Loss: 0.00003476
Iteration 266/1000 | Loss: 0.00003476
Iteration 267/1000 | Loss: 0.00003476
Iteration 268/1000 | Loss: 0.00003475
Iteration 269/1000 | Loss: 0.00003475
Iteration 270/1000 | Loss: 0.00003475
Iteration 271/1000 | Loss: 0.00003475
Iteration 272/1000 | Loss: 0.00003475
Iteration 273/1000 | Loss: 0.00003475
Iteration 274/1000 | Loss: 0.00003475
Iteration 275/1000 | Loss: 0.00003475
Iteration 276/1000 | Loss: 0.00003475
Iteration 277/1000 | Loss: 0.00003475
Iteration 278/1000 | Loss: 0.00003475
Iteration 279/1000 | Loss: 0.00003475
Iteration 280/1000 | Loss: 0.00003475
Iteration 281/1000 | Loss: 0.00003474
Iteration 282/1000 | Loss: 0.00003474
Iteration 283/1000 | Loss: 0.00003474
Iteration 284/1000 | Loss: 0.00003474
Iteration 285/1000 | Loss: 0.00003654
Iteration 286/1000 | Loss: 0.00003475
Iteration 287/1000 | Loss: 0.00003474
Iteration 288/1000 | Loss: 0.00003474
Iteration 289/1000 | Loss: 0.00003473
Iteration 290/1000 | Loss: 0.00003473
Iteration 291/1000 | Loss: 0.00003473
Iteration 292/1000 | Loss: 0.00003473
Iteration 293/1000 | Loss: 0.00003473
Iteration 294/1000 | Loss: 0.00003473
Iteration 295/1000 | Loss: 0.00003473
Iteration 296/1000 | Loss: 0.00003473
Iteration 297/1000 | Loss: 0.00003473
Iteration 298/1000 | Loss: 0.00003473
Iteration 299/1000 | Loss: 0.00003473
Iteration 300/1000 | Loss: 0.00003473
Iteration 301/1000 | Loss: 0.00003473
Iteration 302/1000 | Loss: 0.00003473
Iteration 303/1000 | Loss: 0.00003510
Iteration 304/1000 | Loss: 0.00003482
Iteration 305/1000 | Loss: 0.00003474
Iteration 306/1000 | Loss: 0.00003473
Iteration 307/1000 | Loss: 0.00003473
Iteration 308/1000 | Loss: 0.00003473
Iteration 309/1000 | Loss: 0.00003473
Iteration 310/1000 | Loss: 0.00003473
Iteration 311/1000 | Loss: 0.00003473
Iteration 312/1000 | Loss: 0.00003473
Iteration 313/1000 | Loss: 0.00003473
Iteration 314/1000 | Loss: 0.00003473
Iteration 315/1000 | Loss: 0.00003473
Iteration 316/1000 | Loss: 0.00003473
Iteration 317/1000 | Loss: 0.00003473
Iteration 318/1000 | Loss: 0.00003473
Iteration 319/1000 | Loss: 0.00003473
Iteration 320/1000 | Loss: 0.00003473
Iteration 321/1000 | Loss: 0.00003473
Iteration 322/1000 | Loss: 0.00003473
Iteration 323/1000 | Loss: 0.00003473
Iteration 324/1000 | Loss: 0.00003473
Iteration 325/1000 | Loss: 0.00003473
Iteration 326/1000 | Loss: 0.00003473
Iteration 327/1000 | Loss: 0.00003473
Iteration 328/1000 | Loss: 0.00003473
Iteration 329/1000 | Loss: 0.00003473
Iteration 330/1000 | Loss: 0.00003473
Iteration 331/1000 | Loss: 0.00003473
Iteration 332/1000 | Loss: 0.00003473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [3.473010292509571e-05, 3.473010292509571e-05, 3.473010292509571e-05, 3.473010292509571e-05, 3.473010292509571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.473010292509571e-05

Optimization complete. Final v2v error: 3.5948212146759033 mm

Highest mean error: 11.146963119506836 mm for frame 207

Lowest mean error: 2.9173736572265625 mm for frame 33

Saving results

Total time: 333.9082918167114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767625
Iteration 2/25 | Loss: 0.00154822
Iteration 3/25 | Loss: 0.00130749
Iteration 4/25 | Loss: 0.00126595
Iteration 5/25 | Loss: 0.00125478
Iteration 6/25 | Loss: 0.00125237
Iteration 7/25 | Loss: 0.00124875
Iteration 8/25 | Loss: 0.00124711
Iteration 9/25 | Loss: 0.00124740
Iteration 10/25 | Loss: 0.00124463
Iteration 11/25 | Loss: 0.00124367
Iteration 12/25 | Loss: 0.00124355
Iteration 13/25 | Loss: 0.00124355
Iteration 14/25 | Loss: 0.00124354
Iteration 15/25 | Loss: 0.00124354
Iteration 16/25 | Loss: 0.00124354
Iteration 17/25 | Loss: 0.00124354
Iteration 18/25 | Loss: 0.00124354
Iteration 19/25 | Loss: 0.00124354
Iteration 20/25 | Loss: 0.00124354
Iteration 21/25 | Loss: 0.00124354
Iteration 22/25 | Loss: 0.00124354
Iteration 23/25 | Loss: 0.00124354
Iteration 24/25 | Loss: 0.00124354
Iteration 25/25 | Loss: 0.00124354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41079640
Iteration 2/25 | Loss: 0.00067350
Iteration 3/25 | Loss: 0.00067350
Iteration 4/25 | Loss: 0.00067350
Iteration 5/25 | Loss: 0.00067350
Iteration 6/25 | Loss: 0.00067350
Iteration 7/25 | Loss: 0.00067350
Iteration 8/25 | Loss: 0.00067350
Iteration 9/25 | Loss: 0.00067350
Iteration 10/25 | Loss: 0.00067350
Iteration 11/25 | Loss: 0.00067350
Iteration 12/25 | Loss: 0.00067349
Iteration 13/25 | Loss: 0.00067349
Iteration 14/25 | Loss: 0.00067349
Iteration 15/25 | Loss: 0.00067349
Iteration 16/25 | Loss: 0.00067349
Iteration 17/25 | Loss: 0.00067349
Iteration 18/25 | Loss: 0.00067349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006734947091899812, 0.0006734947091899812, 0.0006734947091899812, 0.0006734947091899812, 0.0006734947091899812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006734947091899812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067349
Iteration 2/1000 | Loss: 0.00006049
Iteration 3/1000 | Loss: 0.00004271
Iteration 4/1000 | Loss: 0.00003903
Iteration 5/1000 | Loss: 0.00003606
Iteration 6/1000 | Loss: 0.00003397
Iteration 7/1000 | Loss: 0.00003261
Iteration 8/1000 | Loss: 0.00003125
Iteration 9/1000 | Loss: 0.00003045
Iteration 10/1000 | Loss: 0.00002988
Iteration 11/1000 | Loss: 0.00002923
Iteration 12/1000 | Loss: 0.00002890
Iteration 13/1000 | Loss: 0.00002851
Iteration 14/1000 | Loss: 0.00002820
Iteration 15/1000 | Loss: 0.00002798
Iteration 16/1000 | Loss: 0.00002789
Iteration 17/1000 | Loss: 0.00002786
Iteration 18/1000 | Loss: 0.00002772
Iteration 19/1000 | Loss: 0.00002766
Iteration 20/1000 | Loss: 0.00002761
Iteration 21/1000 | Loss: 0.00002759
Iteration 22/1000 | Loss: 0.00002759
Iteration 23/1000 | Loss: 0.00002758
Iteration 24/1000 | Loss: 0.00002755
Iteration 25/1000 | Loss: 0.00002750
Iteration 26/1000 | Loss: 0.00002749
Iteration 27/1000 | Loss: 0.00002749
Iteration 28/1000 | Loss: 0.00002747
Iteration 29/1000 | Loss: 0.00002747
Iteration 30/1000 | Loss: 0.00002747
Iteration 31/1000 | Loss: 0.00002747
Iteration 32/1000 | Loss: 0.00002746
Iteration 33/1000 | Loss: 0.00002746
Iteration 34/1000 | Loss: 0.00002746
Iteration 35/1000 | Loss: 0.00002745
Iteration 36/1000 | Loss: 0.00002744
Iteration 37/1000 | Loss: 0.00002744
Iteration 38/1000 | Loss: 0.00002744
Iteration 39/1000 | Loss: 0.00002744
Iteration 40/1000 | Loss: 0.00002741
Iteration 41/1000 | Loss: 0.00002740
Iteration 42/1000 | Loss: 0.00002738
Iteration 43/1000 | Loss: 0.00002738
Iteration 44/1000 | Loss: 0.00002737
Iteration 45/1000 | Loss: 0.00002737
Iteration 46/1000 | Loss: 0.00002736
Iteration 47/1000 | Loss: 0.00002736
Iteration 48/1000 | Loss: 0.00002735
Iteration 49/1000 | Loss: 0.00002735
Iteration 50/1000 | Loss: 0.00002734
Iteration 51/1000 | Loss: 0.00002733
Iteration 52/1000 | Loss: 0.00002732
Iteration 53/1000 | Loss: 0.00002732
Iteration 54/1000 | Loss: 0.00002732
Iteration 55/1000 | Loss: 0.00002731
Iteration 56/1000 | Loss: 0.00002731
Iteration 57/1000 | Loss: 0.00002730
Iteration 58/1000 | Loss: 0.00002730
Iteration 59/1000 | Loss: 0.00002730
Iteration 60/1000 | Loss: 0.00002730
Iteration 61/1000 | Loss: 0.00002729
Iteration 62/1000 | Loss: 0.00002729
Iteration 63/1000 | Loss: 0.00002729
Iteration 64/1000 | Loss: 0.00002729
Iteration 65/1000 | Loss: 0.00002729
Iteration 66/1000 | Loss: 0.00002728
Iteration 67/1000 | Loss: 0.00002728
Iteration 68/1000 | Loss: 0.00002728
Iteration 69/1000 | Loss: 0.00002728
Iteration 70/1000 | Loss: 0.00002728
Iteration 71/1000 | Loss: 0.00002728
Iteration 72/1000 | Loss: 0.00002728
Iteration 73/1000 | Loss: 0.00002728
Iteration 74/1000 | Loss: 0.00002727
Iteration 75/1000 | Loss: 0.00002727
Iteration 76/1000 | Loss: 0.00002727
Iteration 77/1000 | Loss: 0.00002726
Iteration 78/1000 | Loss: 0.00002726
Iteration 79/1000 | Loss: 0.00002726
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00002725
Iteration 82/1000 | Loss: 0.00002725
Iteration 83/1000 | Loss: 0.00002725
Iteration 84/1000 | Loss: 0.00002725
Iteration 85/1000 | Loss: 0.00002725
Iteration 86/1000 | Loss: 0.00002725
Iteration 87/1000 | Loss: 0.00002725
Iteration 88/1000 | Loss: 0.00002725
Iteration 89/1000 | Loss: 0.00002725
Iteration 90/1000 | Loss: 0.00002725
Iteration 91/1000 | Loss: 0.00002725
Iteration 92/1000 | Loss: 0.00002724
Iteration 93/1000 | Loss: 0.00002724
Iteration 94/1000 | Loss: 0.00002724
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002723
Iteration 97/1000 | Loss: 0.00002723
Iteration 98/1000 | Loss: 0.00002722
Iteration 99/1000 | Loss: 0.00002722
Iteration 100/1000 | Loss: 0.00002722
Iteration 101/1000 | Loss: 0.00002722
Iteration 102/1000 | Loss: 0.00002722
Iteration 103/1000 | Loss: 0.00002722
Iteration 104/1000 | Loss: 0.00002721
Iteration 105/1000 | Loss: 0.00002721
Iteration 106/1000 | Loss: 0.00002721
Iteration 107/1000 | Loss: 0.00002721
Iteration 108/1000 | Loss: 0.00002720
Iteration 109/1000 | Loss: 0.00002720
Iteration 110/1000 | Loss: 0.00002720
Iteration 111/1000 | Loss: 0.00002719
Iteration 112/1000 | Loss: 0.00002719
Iteration 113/1000 | Loss: 0.00002719
Iteration 114/1000 | Loss: 0.00002719
Iteration 115/1000 | Loss: 0.00002718
Iteration 116/1000 | Loss: 0.00002718
Iteration 117/1000 | Loss: 0.00002718
Iteration 118/1000 | Loss: 0.00002717
Iteration 119/1000 | Loss: 0.00002717
Iteration 120/1000 | Loss: 0.00002717
Iteration 121/1000 | Loss: 0.00002717
Iteration 122/1000 | Loss: 0.00002717
Iteration 123/1000 | Loss: 0.00002717
Iteration 124/1000 | Loss: 0.00002717
Iteration 125/1000 | Loss: 0.00002717
Iteration 126/1000 | Loss: 0.00002717
Iteration 127/1000 | Loss: 0.00002716
Iteration 128/1000 | Loss: 0.00002716
Iteration 129/1000 | Loss: 0.00002716
Iteration 130/1000 | Loss: 0.00002716
Iteration 131/1000 | Loss: 0.00002716
Iteration 132/1000 | Loss: 0.00002716
Iteration 133/1000 | Loss: 0.00002716
Iteration 134/1000 | Loss: 0.00002716
Iteration 135/1000 | Loss: 0.00002716
Iteration 136/1000 | Loss: 0.00002716
Iteration 137/1000 | Loss: 0.00002716
Iteration 138/1000 | Loss: 0.00002715
Iteration 139/1000 | Loss: 0.00002715
Iteration 140/1000 | Loss: 0.00002715
Iteration 141/1000 | Loss: 0.00002715
Iteration 142/1000 | Loss: 0.00002715
Iteration 143/1000 | Loss: 0.00002715
Iteration 144/1000 | Loss: 0.00002715
Iteration 145/1000 | Loss: 0.00002715
Iteration 146/1000 | Loss: 0.00002715
Iteration 147/1000 | Loss: 0.00002715
Iteration 148/1000 | Loss: 0.00002715
Iteration 149/1000 | Loss: 0.00002715
Iteration 150/1000 | Loss: 0.00002715
Iteration 151/1000 | Loss: 0.00002715
Iteration 152/1000 | Loss: 0.00002715
Iteration 153/1000 | Loss: 0.00002715
Iteration 154/1000 | Loss: 0.00002715
Iteration 155/1000 | Loss: 0.00002715
Iteration 156/1000 | Loss: 0.00002715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.71466687991051e-05, 2.71466687991051e-05, 2.71466687991051e-05, 2.71466687991051e-05, 2.71466687991051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.71466687991051e-05

Optimization complete. Final v2v error: 4.329902648925781 mm

Highest mean error: 5.723324298858643 mm for frame 13

Lowest mean error: 3.387456178665161 mm for frame 4

Saving results

Total time: 64.39623785018921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042904
Iteration 2/25 | Loss: 0.00233407
Iteration 3/25 | Loss: 0.00176981
Iteration 4/25 | Loss: 0.00163531
Iteration 5/25 | Loss: 0.00153027
Iteration 6/25 | Loss: 0.00138601
Iteration 7/25 | Loss: 0.00131114
Iteration 8/25 | Loss: 0.00127649
Iteration 9/25 | Loss: 0.00126142
Iteration 10/25 | Loss: 0.00125682
Iteration 11/25 | Loss: 0.00124759
Iteration 12/25 | Loss: 0.00125245
Iteration 13/25 | Loss: 0.00124322
Iteration 14/25 | Loss: 0.00123942
Iteration 15/25 | Loss: 0.00123830
Iteration 16/25 | Loss: 0.00124284
Iteration 17/25 | Loss: 0.00123776
Iteration 18/25 | Loss: 0.00123757
Iteration 19/25 | Loss: 0.00123741
Iteration 20/25 | Loss: 0.00123732
Iteration 21/25 | Loss: 0.00123731
Iteration 22/25 | Loss: 0.00123731
Iteration 23/25 | Loss: 0.00123731
Iteration 24/25 | Loss: 0.00123730
Iteration 25/25 | Loss: 0.00123730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38014948
Iteration 2/25 | Loss: 0.00095230
Iteration 3/25 | Loss: 0.00095230
Iteration 4/25 | Loss: 0.00095230
Iteration 5/25 | Loss: 0.00095230
Iteration 6/25 | Loss: 0.00095230
Iteration 7/25 | Loss: 0.00095230
Iteration 8/25 | Loss: 0.00095230
Iteration 9/25 | Loss: 0.00095230
Iteration 10/25 | Loss: 0.00095230
Iteration 11/25 | Loss: 0.00095230
Iteration 12/25 | Loss: 0.00095230
Iteration 13/25 | Loss: 0.00095230
Iteration 14/25 | Loss: 0.00095230
Iteration 15/25 | Loss: 0.00095230
Iteration 16/25 | Loss: 0.00095230
Iteration 17/25 | Loss: 0.00095230
Iteration 18/25 | Loss: 0.00095230
Iteration 19/25 | Loss: 0.00095230
Iteration 20/25 | Loss: 0.00095230
Iteration 21/25 | Loss: 0.00095230
Iteration 22/25 | Loss: 0.00095230
Iteration 23/25 | Loss: 0.00095230
Iteration 24/25 | Loss: 0.00095230
Iteration 25/25 | Loss: 0.00095230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095230
Iteration 2/1000 | Loss: 0.00005960
Iteration 3/1000 | Loss: 0.00004623
Iteration 4/1000 | Loss: 0.00003971
Iteration 5/1000 | Loss: 0.00003716
Iteration 6/1000 | Loss: 0.00003550
Iteration 7/1000 | Loss: 0.00003401
Iteration 8/1000 | Loss: 0.00003300
Iteration 9/1000 | Loss: 0.00003204
Iteration 10/1000 | Loss: 0.00003122
Iteration 11/1000 | Loss: 0.00003058
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00064742
Iteration 14/1000 | Loss: 0.00412747
Iteration 15/1000 | Loss: 0.00006337
Iteration 16/1000 | Loss: 0.00010393
Iteration 17/1000 | Loss: 0.00003513
Iteration 18/1000 | Loss: 0.00027379
Iteration 19/1000 | Loss: 0.00003356
Iteration 20/1000 | Loss: 0.00012481
Iteration 21/1000 | Loss: 0.00004623
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001671
Iteration 24/1000 | Loss: 0.00005228
Iteration 25/1000 | Loss: 0.00028931
Iteration 26/1000 | Loss: 0.00001671
Iteration 27/1000 | Loss: 0.00002462
Iteration 28/1000 | Loss: 0.00004273
Iteration 29/1000 | Loss: 0.00033581
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00003709
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001185
Iteration 35/1000 | Loss: 0.00001155
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001141
Iteration 38/1000 | Loss: 0.00001139
Iteration 39/1000 | Loss: 0.00001133
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001117
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001108
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001107
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001106
Iteration 52/1000 | Loss: 0.00001106
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001105
Iteration 57/1000 | Loss: 0.00001105
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001104
Iteration 60/1000 | Loss: 0.00001104
Iteration 61/1000 | Loss: 0.00001104
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001104
Iteration 64/1000 | Loss: 0.00001104
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001101
Iteration 91/1000 | Loss: 0.00001101
Iteration 92/1000 | Loss: 0.00001101
Iteration 93/1000 | Loss: 0.00001101
Iteration 94/1000 | Loss: 0.00001101
Iteration 95/1000 | Loss: 0.00001101
Iteration 96/1000 | Loss: 0.00001101
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001100
Iteration 102/1000 | Loss: 0.00001100
Iteration 103/1000 | Loss: 0.00001100
Iteration 104/1000 | Loss: 0.00001100
Iteration 105/1000 | Loss: 0.00001100
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001099
Iteration 108/1000 | Loss: 0.00001099
Iteration 109/1000 | Loss: 0.00001099
Iteration 110/1000 | Loss: 0.00001099
Iteration 111/1000 | Loss: 0.00001099
Iteration 112/1000 | Loss: 0.00001099
Iteration 113/1000 | Loss: 0.00001099
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001099
Iteration 116/1000 | Loss: 0.00001099
Iteration 117/1000 | Loss: 0.00001099
Iteration 118/1000 | Loss: 0.00001099
Iteration 119/1000 | Loss: 0.00001099
Iteration 120/1000 | Loss: 0.00001099
Iteration 121/1000 | Loss: 0.00001099
Iteration 122/1000 | Loss: 0.00001099
Iteration 123/1000 | Loss: 0.00001099
Iteration 124/1000 | Loss: 0.00001099
Iteration 125/1000 | Loss: 0.00001099
Iteration 126/1000 | Loss: 0.00001099
Iteration 127/1000 | Loss: 0.00001098
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001098
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001097
Iteration 138/1000 | Loss: 0.00001097
Iteration 139/1000 | Loss: 0.00001097
Iteration 140/1000 | Loss: 0.00001097
Iteration 141/1000 | Loss: 0.00001097
Iteration 142/1000 | Loss: 0.00001097
Iteration 143/1000 | Loss: 0.00001097
Iteration 144/1000 | Loss: 0.00001097
Iteration 145/1000 | Loss: 0.00001097
Iteration 146/1000 | Loss: 0.00001097
Iteration 147/1000 | Loss: 0.00001097
Iteration 148/1000 | Loss: 0.00001097
Iteration 149/1000 | Loss: 0.00001097
Iteration 150/1000 | Loss: 0.00001097
Iteration 151/1000 | Loss: 0.00001097
Iteration 152/1000 | Loss: 0.00001097
Iteration 153/1000 | Loss: 0.00001097
Iteration 154/1000 | Loss: 0.00001097
Iteration 155/1000 | Loss: 0.00001097
Iteration 156/1000 | Loss: 0.00001097
Iteration 157/1000 | Loss: 0.00001097
Iteration 158/1000 | Loss: 0.00001097
Iteration 159/1000 | Loss: 0.00001097
Iteration 160/1000 | Loss: 0.00001097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.0971138181048445e-05, 1.0971138181048445e-05, 1.0971138181048445e-05, 1.0971138181048445e-05, 1.0971138181048445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0971138181048445e-05

Optimization complete. Final v2v error: 2.814488649368286 mm

Highest mean error: 3.0469348430633545 mm for frame 81

Lowest mean error: 2.7300941944122314 mm for frame 84

Saving results

Total time: 94.74117588996887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401356
Iteration 2/25 | Loss: 0.00129878
Iteration 3/25 | Loss: 0.00122586
Iteration 4/25 | Loss: 0.00121979
Iteration 5/25 | Loss: 0.00121678
Iteration 6/25 | Loss: 0.00121620
Iteration 7/25 | Loss: 0.00121620
Iteration 8/25 | Loss: 0.00121620
Iteration 9/25 | Loss: 0.00121620
Iteration 10/25 | Loss: 0.00121620
Iteration 11/25 | Loss: 0.00121620
Iteration 12/25 | Loss: 0.00121620
Iteration 13/25 | Loss: 0.00121620
Iteration 14/25 | Loss: 0.00121620
Iteration 15/25 | Loss: 0.00121620
Iteration 16/25 | Loss: 0.00121620
Iteration 17/25 | Loss: 0.00121620
Iteration 18/25 | Loss: 0.00121620
Iteration 19/25 | Loss: 0.00121620
Iteration 20/25 | Loss: 0.00121620
Iteration 21/25 | Loss: 0.00121620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012162047205492854, 0.0012162047205492854, 0.0012162047205492854, 0.0012162047205492854, 0.0012162047205492854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012162047205492854

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68641019
Iteration 2/25 | Loss: 0.00061892
Iteration 3/25 | Loss: 0.00061892
Iteration 4/25 | Loss: 0.00061892
Iteration 5/25 | Loss: 0.00061892
Iteration 6/25 | Loss: 0.00061892
Iteration 7/25 | Loss: 0.00061892
Iteration 8/25 | Loss: 0.00061892
Iteration 9/25 | Loss: 0.00061892
Iteration 10/25 | Loss: 0.00061892
Iteration 11/25 | Loss: 0.00061892
Iteration 12/25 | Loss: 0.00061892
Iteration 13/25 | Loss: 0.00061892
Iteration 14/25 | Loss: 0.00061892
Iteration 15/25 | Loss: 0.00061892
Iteration 16/25 | Loss: 0.00061892
Iteration 17/25 | Loss: 0.00061892
Iteration 18/25 | Loss: 0.00061892
Iteration 19/25 | Loss: 0.00061892
Iteration 20/25 | Loss: 0.00061892
Iteration 21/25 | Loss: 0.00061892
Iteration 22/25 | Loss: 0.00061892
Iteration 23/25 | Loss: 0.00061892
Iteration 24/25 | Loss: 0.00061892
Iteration 25/25 | Loss: 0.00061892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061892
Iteration 2/1000 | Loss: 0.00003052
Iteration 3/1000 | Loss: 0.00002097
Iteration 4/1000 | Loss: 0.00001683
Iteration 5/1000 | Loss: 0.00001599
Iteration 6/1000 | Loss: 0.00001516
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001371
Iteration 10/1000 | Loss: 0.00001333
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001251
Iteration 18/1000 | Loss: 0.00001250
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001246
Iteration 29/1000 | Loss: 0.00001246
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001243
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001237
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001236
Iteration 70/1000 | Loss: 0.00001236
Iteration 71/1000 | Loss: 0.00001236
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001235
Iteration 74/1000 | Loss: 0.00001235
Iteration 75/1000 | Loss: 0.00001235
Iteration 76/1000 | Loss: 0.00001235
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001233
Iteration 84/1000 | Loss: 0.00001233
Iteration 85/1000 | Loss: 0.00001233
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001233
Iteration 88/1000 | Loss: 0.00001233
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001231
Iteration 100/1000 | Loss: 0.00001231
Iteration 101/1000 | Loss: 0.00001231
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001230
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001230
Iteration 118/1000 | Loss: 0.00001230
Iteration 119/1000 | Loss: 0.00001230
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001230
Iteration 130/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2303826224524528e-05, 1.2303826224524528e-05, 1.2303826224524528e-05, 1.2303826224524528e-05, 1.2303826224524528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2303826224524528e-05

Optimization complete. Final v2v error: 2.9909448623657227 mm

Highest mean error: 3.326420783996582 mm for frame 102

Lowest mean error: 2.740204095840454 mm for frame 244

Saving results

Total time: 41.432605504989624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469284
Iteration 2/25 | Loss: 0.00138510
Iteration 3/25 | Loss: 0.00129172
Iteration 4/25 | Loss: 0.00127006
Iteration 5/25 | Loss: 0.00126194
Iteration 6/25 | Loss: 0.00126070
Iteration 7/25 | Loss: 0.00126070
Iteration 8/25 | Loss: 0.00126070
Iteration 9/25 | Loss: 0.00126070
Iteration 10/25 | Loss: 0.00126070
Iteration 11/25 | Loss: 0.00126070
Iteration 12/25 | Loss: 0.00126070
Iteration 13/25 | Loss: 0.00126070
Iteration 14/25 | Loss: 0.00126070
Iteration 15/25 | Loss: 0.00126070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012606971431523561, 0.0012606971431523561, 0.0012606971431523561, 0.0012606971431523561, 0.0012606971431523561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012606971431523561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.86303234
Iteration 2/25 | Loss: 0.00092864
Iteration 3/25 | Loss: 0.00092863
Iteration 4/25 | Loss: 0.00092863
Iteration 5/25 | Loss: 0.00092863
Iteration 6/25 | Loss: 0.00092863
Iteration 7/25 | Loss: 0.00092863
Iteration 8/25 | Loss: 0.00092863
Iteration 9/25 | Loss: 0.00092863
Iteration 10/25 | Loss: 0.00092863
Iteration 11/25 | Loss: 0.00092863
Iteration 12/25 | Loss: 0.00092863
Iteration 13/25 | Loss: 0.00092863
Iteration 14/25 | Loss: 0.00092863
Iteration 15/25 | Loss: 0.00092863
Iteration 16/25 | Loss: 0.00092863
Iteration 17/25 | Loss: 0.00092863
Iteration 18/25 | Loss: 0.00092863
Iteration 19/25 | Loss: 0.00092863
Iteration 20/25 | Loss: 0.00092863
Iteration 21/25 | Loss: 0.00092863
Iteration 22/25 | Loss: 0.00092863
Iteration 23/25 | Loss: 0.00092863
Iteration 24/25 | Loss: 0.00092863
Iteration 25/25 | Loss: 0.00092863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092863
Iteration 2/1000 | Loss: 0.00003470
Iteration 3/1000 | Loss: 0.00002464
Iteration 4/1000 | Loss: 0.00002298
Iteration 5/1000 | Loss: 0.00002214
Iteration 6/1000 | Loss: 0.00002151
Iteration 7/1000 | Loss: 0.00002110
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002038
Iteration 10/1000 | Loss: 0.00002021
Iteration 11/1000 | Loss: 0.00002003
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001990
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001976
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001974
Iteration 26/1000 | Loss: 0.00001974
Iteration 27/1000 | Loss: 0.00001973
Iteration 28/1000 | Loss: 0.00001973
Iteration 29/1000 | Loss: 0.00001973
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00001963
Iteration 37/1000 | Loss: 0.00001963
Iteration 38/1000 | Loss: 0.00001962
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001961
Iteration 41/1000 | Loss: 0.00001961
Iteration 42/1000 | Loss: 0.00001961
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001959
Iteration 46/1000 | Loss: 0.00001959
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001958
Iteration 49/1000 | Loss: 0.00001958
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001957
Iteration 53/1000 | Loss: 0.00001957
Iteration 54/1000 | Loss: 0.00001957
Iteration 55/1000 | Loss: 0.00001957
Iteration 56/1000 | Loss: 0.00001957
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001956
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001954
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001954
Iteration 71/1000 | Loss: 0.00001954
Iteration 72/1000 | Loss: 0.00001953
Iteration 73/1000 | Loss: 0.00001953
Iteration 74/1000 | Loss: 0.00001952
Iteration 75/1000 | Loss: 0.00001952
Iteration 76/1000 | Loss: 0.00001952
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001950
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001949
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001947
Iteration 91/1000 | Loss: 0.00001947
Iteration 92/1000 | Loss: 0.00001947
Iteration 93/1000 | Loss: 0.00001947
Iteration 94/1000 | Loss: 0.00001947
Iteration 95/1000 | Loss: 0.00001947
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001946
Iteration 98/1000 | Loss: 0.00001946
Iteration 99/1000 | Loss: 0.00001946
Iteration 100/1000 | Loss: 0.00001945
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001945
Iteration 103/1000 | Loss: 0.00001945
Iteration 104/1000 | Loss: 0.00001945
Iteration 105/1000 | Loss: 0.00001945
Iteration 106/1000 | Loss: 0.00001944
Iteration 107/1000 | Loss: 0.00001944
Iteration 108/1000 | Loss: 0.00001944
Iteration 109/1000 | Loss: 0.00001944
Iteration 110/1000 | Loss: 0.00001944
Iteration 111/1000 | Loss: 0.00001944
Iteration 112/1000 | Loss: 0.00001943
Iteration 113/1000 | Loss: 0.00001943
Iteration 114/1000 | Loss: 0.00001943
Iteration 115/1000 | Loss: 0.00001942
Iteration 116/1000 | Loss: 0.00001942
Iteration 117/1000 | Loss: 0.00001941
Iteration 118/1000 | Loss: 0.00001941
Iteration 119/1000 | Loss: 0.00001941
Iteration 120/1000 | Loss: 0.00001941
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00001941
Iteration 123/1000 | Loss: 0.00001940
Iteration 124/1000 | Loss: 0.00001940
Iteration 125/1000 | Loss: 0.00001940
Iteration 126/1000 | Loss: 0.00001939
Iteration 127/1000 | Loss: 0.00001939
Iteration 128/1000 | Loss: 0.00001939
Iteration 129/1000 | Loss: 0.00001939
Iteration 130/1000 | Loss: 0.00001938
Iteration 131/1000 | Loss: 0.00001938
Iteration 132/1000 | Loss: 0.00001938
Iteration 133/1000 | Loss: 0.00001937
Iteration 134/1000 | Loss: 0.00001937
Iteration 135/1000 | Loss: 0.00001937
Iteration 136/1000 | Loss: 0.00001936
Iteration 137/1000 | Loss: 0.00001936
Iteration 138/1000 | Loss: 0.00001936
Iteration 139/1000 | Loss: 0.00001936
Iteration 140/1000 | Loss: 0.00001935
Iteration 141/1000 | Loss: 0.00001935
Iteration 142/1000 | Loss: 0.00001935
Iteration 143/1000 | Loss: 0.00001935
Iteration 144/1000 | Loss: 0.00001935
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001933
Iteration 150/1000 | Loss: 0.00001933
Iteration 151/1000 | Loss: 0.00001932
Iteration 152/1000 | Loss: 0.00001932
Iteration 153/1000 | Loss: 0.00001932
Iteration 154/1000 | Loss: 0.00001932
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Iteration 160/1000 | Loss: 0.00001931
Iteration 161/1000 | Loss: 0.00001931
Iteration 162/1000 | Loss: 0.00001931
Iteration 163/1000 | Loss: 0.00001931
Iteration 164/1000 | Loss: 0.00001931
Iteration 165/1000 | Loss: 0.00001931
Iteration 166/1000 | Loss: 0.00001931
Iteration 167/1000 | Loss: 0.00001931
Iteration 168/1000 | Loss: 0.00001931
Iteration 169/1000 | Loss: 0.00001931
Iteration 170/1000 | Loss: 0.00001931
Iteration 171/1000 | Loss: 0.00001931
Iteration 172/1000 | Loss: 0.00001931
Iteration 173/1000 | Loss: 0.00001931
Iteration 174/1000 | Loss: 0.00001931
Iteration 175/1000 | Loss: 0.00001931
Iteration 176/1000 | Loss: 0.00001931
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Iteration 180/1000 | Loss: 0.00001931
Iteration 181/1000 | Loss: 0.00001931
Iteration 182/1000 | Loss: 0.00001931
Iteration 183/1000 | Loss: 0.00001931
Iteration 184/1000 | Loss: 0.00001931
Iteration 185/1000 | Loss: 0.00001931
Iteration 186/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.9309181880089454e-05, 1.9309181880089454e-05, 1.9309181880089454e-05, 1.9309181880089454e-05, 1.9309181880089454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9309181880089454e-05

Optimization complete. Final v2v error: 3.7205660343170166 mm

Highest mean error: 4.249561309814453 mm for frame 230

Lowest mean error: 3.3507494926452637 mm for frame 251

Saving results

Total time: 45.732670068740845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053389
Iteration 2/25 | Loss: 0.00335781
Iteration 3/25 | Loss: 0.00239924
Iteration 4/25 | Loss: 0.00205613
Iteration 5/25 | Loss: 0.00194389
Iteration 6/25 | Loss: 0.00178934
Iteration 7/25 | Loss: 0.00151710
Iteration 8/25 | Loss: 0.00134374
Iteration 9/25 | Loss: 0.00130447
Iteration 10/25 | Loss: 0.00127439
Iteration 11/25 | Loss: 0.00127545
Iteration 12/25 | Loss: 0.00126423
Iteration 13/25 | Loss: 0.00126209
Iteration 14/25 | Loss: 0.00126290
Iteration 15/25 | Loss: 0.00126199
Iteration 16/25 | Loss: 0.00126042
Iteration 17/25 | Loss: 0.00125752
Iteration 18/25 | Loss: 0.00125698
Iteration 19/25 | Loss: 0.00125701
Iteration 20/25 | Loss: 0.00125680
Iteration 21/25 | Loss: 0.00125671
Iteration 22/25 | Loss: 0.00125631
Iteration 23/25 | Loss: 0.00125715
Iteration 24/25 | Loss: 0.00125687
Iteration 25/25 | Loss: 0.00125636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52786779
Iteration 2/25 | Loss: 0.00150893
Iteration 3/25 | Loss: 0.00091851
Iteration 4/25 | Loss: 0.00091851
Iteration 5/25 | Loss: 0.00091851
Iteration 6/25 | Loss: 0.00091851
Iteration 7/25 | Loss: 0.00091851
Iteration 8/25 | Loss: 0.00091851
Iteration 9/25 | Loss: 0.00091851
Iteration 10/25 | Loss: 0.00091851
Iteration 11/25 | Loss: 0.00091851
Iteration 12/25 | Loss: 0.00091851
Iteration 13/25 | Loss: 0.00091851
Iteration 14/25 | Loss: 0.00091851
Iteration 15/25 | Loss: 0.00091851
Iteration 16/25 | Loss: 0.00091851
Iteration 17/25 | Loss: 0.00091851
Iteration 18/25 | Loss: 0.00091851
Iteration 19/25 | Loss: 0.00091851
Iteration 20/25 | Loss: 0.00091851
Iteration 21/25 | Loss: 0.00091851
Iteration 22/25 | Loss: 0.00091851
Iteration 23/25 | Loss: 0.00091851
Iteration 24/25 | Loss: 0.00091851
Iteration 25/25 | Loss: 0.00091851
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009185058297589421, 0.0009185058297589421, 0.0009185058297589421, 0.0009185058297589421, 0.0009185058297589421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009185058297589421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091851
Iteration 2/1000 | Loss: 0.00086384
Iteration 3/1000 | Loss: 0.00006597
Iteration 4/1000 | Loss: 0.00004847
Iteration 5/1000 | Loss: 0.00007329
Iteration 6/1000 | Loss: 0.00005527
Iteration 7/1000 | Loss: 0.00007458
Iteration 8/1000 | Loss: 0.00005113
Iteration 9/1000 | Loss: 0.00005264
Iteration 10/1000 | Loss: 0.00005524
Iteration 11/1000 | Loss: 0.00005764
Iteration 12/1000 | Loss: 0.00005079
Iteration 13/1000 | Loss: 0.00005385
Iteration 14/1000 | Loss: 0.00005229
Iteration 15/1000 | Loss: 0.00006127
Iteration 16/1000 | Loss: 0.00004815
Iteration 17/1000 | Loss: 0.00005413
Iteration 18/1000 | Loss: 0.00006312
Iteration 19/1000 | Loss: 0.00005854
Iteration 20/1000 | Loss: 0.00005010
Iteration 21/1000 | Loss: 0.00005888
Iteration 22/1000 | Loss: 0.00005020
Iteration 23/1000 | Loss: 0.00005534
Iteration 24/1000 | Loss: 0.00004925
Iteration 25/1000 | Loss: 0.00005956
Iteration 26/1000 | Loss: 0.00004797
Iteration 27/1000 | Loss: 0.00004819
Iteration 28/1000 | Loss: 0.00004682
Iteration 29/1000 | Loss: 0.00005242
Iteration 30/1000 | Loss: 0.00005207
Iteration 31/1000 | Loss: 0.00004943
Iteration 32/1000 | Loss: 0.00005633
Iteration 33/1000 | Loss: 0.00005343
Iteration 34/1000 | Loss: 0.00007430
Iteration 35/1000 | Loss: 0.00005246
Iteration 36/1000 | Loss: 0.00005329
Iteration 37/1000 | Loss: 0.00375626
Iteration 38/1000 | Loss: 0.00012654
Iteration 39/1000 | Loss: 0.00004488
Iteration 40/1000 | Loss: 0.00002999
Iteration 41/1000 | Loss: 0.00011124
Iteration 42/1000 | Loss: 0.00004504
Iteration 43/1000 | Loss: 0.00002483
Iteration 44/1000 | Loss: 0.00004411
Iteration 45/1000 | Loss: 0.00004852
Iteration 46/1000 | Loss: 0.00003466
Iteration 47/1000 | Loss: 0.00004585
Iteration 48/1000 | Loss: 0.00003374
Iteration 49/1000 | Loss: 0.00003899
Iteration 50/1000 | Loss: 0.00003163
Iteration 51/1000 | Loss: 0.00006541
Iteration 52/1000 | Loss: 0.00003356
Iteration 53/1000 | Loss: 0.00003395
Iteration 54/1000 | Loss: 0.00005133
Iteration 55/1000 | Loss: 0.00006178
Iteration 56/1000 | Loss: 0.00003911
Iteration 57/1000 | Loss: 0.00003303
Iteration 58/1000 | Loss: 0.00003839
Iteration 59/1000 | Loss: 0.00002860
Iteration 60/1000 | Loss: 0.00005315
Iteration 61/1000 | Loss: 0.00003602
Iteration 62/1000 | Loss: 0.00005813
Iteration 63/1000 | Loss: 0.00003522
Iteration 64/1000 | Loss: 0.00003120
Iteration 65/1000 | Loss: 0.00003647
Iteration 66/1000 | Loss: 0.00003313
Iteration 67/1000 | Loss: 0.00003485
Iteration 68/1000 | Loss: 0.00002817
Iteration 69/1000 | Loss: 0.00003517
Iteration 70/1000 | Loss: 0.00003067
Iteration 71/1000 | Loss: 0.00002991
Iteration 72/1000 | Loss: 0.00005022
Iteration 73/1000 | Loss: 0.00003746
Iteration 74/1000 | Loss: 0.00004265
Iteration 75/1000 | Loss: 0.00002828
Iteration 76/1000 | Loss: 0.00003177
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001636
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001573
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001558
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00002872
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001549
Iteration 95/1000 | Loss: 0.00001549
Iteration 96/1000 | Loss: 0.00001549
Iteration 97/1000 | Loss: 0.00001549
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001548
Iteration 101/1000 | Loss: 0.00001548
Iteration 102/1000 | Loss: 0.00001548
Iteration 103/1000 | Loss: 0.00001548
Iteration 104/1000 | Loss: 0.00001548
Iteration 105/1000 | Loss: 0.00001548
Iteration 106/1000 | Loss: 0.00001548
Iteration 107/1000 | Loss: 0.00001548
Iteration 108/1000 | Loss: 0.00001548
Iteration 109/1000 | Loss: 0.00001548
Iteration 110/1000 | Loss: 0.00001548
Iteration 111/1000 | Loss: 0.00001548
Iteration 112/1000 | Loss: 0.00001547
Iteration 113/1000 | Loss: 0.00001547
Iteration 114/1000 | Loss: 0.00001547
Iteration 115/1000 | Loss: 0.00001798
Iteration 116/1000 | Loss: 0.00001543
Iteration 117/1000 | Loss: 0.00001542
Iteration 118/1000 | Loss: 0.00001542
Iteration 119/1000 | Loss: 0.00001542
Iteration 120/1000 | Loss: 0.00001542
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001542
Iteration 123/1000 | Loss: 0.00001542
Iteration 124/1000 | Loss: 0.00001542
Iteration 125/1000 | Loss: 0.00001542
Iteration 126/1000 | Loss: 0.00001542
Iteration 127/1000 | Loss: 0.00001542
Iteration 128/1000 | Loss: 0.00001542
Iteration 129/1000 | Loss: 0.00001541
Iteration 130/1000 | Loss: 0.00001541
Iteration 131/1000 | Loss: 0.00001541
Iteration 132/1000 | Loss: 0.00001541
Iteration 133/1000 | Loss: 0.00001541
Iteration 134/1000 | Loss: 0.00001541
Iteration 135/1000 | Loss: 0.00001541
Iteration 136/1000 | Loss: 0.00001540
Iteration 137/1000 | Loss: 0.00001540
Iteration 138/1000 | Loss: 0.00001540
Iteration 139/1000 | Loss: 0.00001706
Iteration 140/1000 | Loss: 0.00001538
Iteration 141/1000 | Loss: 0.00001537
Iteration 142/1000 | Loss: 0.00001537
Iteration 143/1000 | Loss: 0.00001537
Iteration 144/1000 | Loss: 0.00001537
Iteration 145/1000 | Loss: 0.00001537
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001537
Iteration 148/1000 | Loss: 0.00001537
Iteration 149/1000 | Loss: 0.00001537
Iteration 150/1000 | Loss: 0.00001536
Iteration 151/1000 | Loss: 0.00001536
Iteration 152/1000 | Loss: 0.00001536
Iteration 153/1000 | Loss: 0.00001536
Iteration 154/1000 | Loss: 0.00001536
Iteration 155/1000 | Loss: 0.00001535
Iteration 156/1000 | Loss: 0.00001535
Iteration 157/1000 | Loss: 0.00001535
Iteration 158/1000 | Loss: 0.00001535
Iteration 159/1000 | Loss: 0.00001535
Iteration 160/1000 | Loss: 0.00001535
Iteration 161/1000 | Loss: 0.00001535
Iteration 162/1000 | Loss: 0.00001535
Iteration 163/1000 | Loss: 0.00001535
Iteration 164/1000 | Loss: 0.00001535
Iteration 165/1000 | Loss: 0.00001535
Iteration 166/1000 | Loss: 0.00001535
Iteration 167/1000 | Loss: 0.00001535
Iteration 168/1000 | Loss: 0.00001535
Iteration 169/1000 | Loss: 0.00001534
Iteration 170/1000 | Loss: 0.00001534
Iteration 171/1000 | Loss: 0.00001534
Iteration 172/1000 | Loss: 0.00001534
Iteration 173/1000 | Loss: 0.00001534
Iteration 174/1000 | Loss: 0.00001534
Iteration 175/1000 | Loss: 0.00001533
Iteration 176/1000 | Loss: 0.00001533
Iteration 177/1000 | Loss: 0.00001533
Iteration 178/1000 | Loss: 0.00001533
Iteration 179/1000 | Loss: 0.00001533
Iteration 180/1000 | Loss: 0.00001533
Iteration 181/1000 | Loss: 0.00001533
Iteration 182/1000 | Loss: 0.00001532
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00001532
Iteration 185/1000 | Loss: 0.00001532
Iteration 186/1000 | Loss: 0.00001532
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001532
Iteration 189/1000 | Loss: 0.00001532
Iteration 190/1000 | Loss: 0.00001532
Iteration 191/1000 | Loss: 0.00001532
Iteration 192/1000 | Loss: 0.00001532
Iteration 193/1000 | Loss: 0.00001532
Iteration 194/1000 | Loss: 0.00002343
Iteration 195/1000 | Loss: 0.00001534
Iteration 196/1000 | Loss: 0.00001534
Iteration 197/1000 | Loss: 0.00001534
Iteration 198/1000 | Loss: 0.00001534
Iteration 199/1000 | Loss: 0.00001534
Iteration 200/1000 | Loss: 0.00001534
Iteration 201/1000 | Loss: 0.00001534
Iteration 202/1000 | Loss: 0.00001533
Iteration 203/1000 | Loss: 0.00001533
Iteration 204/1000 | Loss: 0.00001533
Iteration 205/1000 | Loss: 0.00001533
Iteration 206/1000 | Loss: 0.00001533
Iteration 207/1000 | Loss: 0.00001532
Iteration 208/1000 | Loss: 0.00001532
Iteration 209/1000 | Loss: 0.00001532
Iteration 210/1000 | Loss: 0.00001532
Iteration 211/1000 | Loss: 0.00001532
Iteration 212/1000 | Loss: 0.00001531
Iteration 213/1000 | Loss: 0.00001531
Iteration 214/1000 | Loss: 0.00001531
Iteration 215/1000 | Loss: 0.00001531
Iteration 216/1000 | Loss: 0.00001531
Iteration 217/1000 | Loss: 0.00001531
Iteration 218/1000 | Loss: 0.00001531
Iteration 219/1000 | Loss: 0.00001531
Iteration 220/1000 | Loss: 0.00001531
Iteration 221/1000 | Loss: 0.00001531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.530814370198641e-05, 1.530814370198641e-05, 1.530814370198641e-05, 1.530814370198641e-05, 1.530814370198641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.530814370198641e-05

Optimization complete. Final v2v error: 3.309675931930542 mm

Highest mean error: 3.877793312072754 mm for frame 89

Lowest mean error: 3.1033036708831787 mm for frame 65

Saving results

Total time: 176.56708407402039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783686
Iteration 2/25 | Loss: 0.00134842
Iteration 3/25 | Loss: 0.00124319
Iteration 4/25 | Loss: 0.00123247
Iteration 5/25 | Loss: 0.00123031
Iteration 6/25 | Loss: 0.00123029
Iteration 7/25 | Loss: 0.00123029
Iteration 8/25 | Loss: 0.00123029
Iteration 9/25 | Loss: 0.00123029
Iteration 10/25 | Loss: 0.00123029
Iteration 11/25 | Loss: 0.00123029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001230287947691977, 0.001230287947691977, 0.001230287947691977, 0.001230287947691977, 0.001230287947691977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001230287947691977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43585539
Iteration 2/25 | Loss: 0.00079641
Iteration 3/25 | Loss: 0.00079641
Iteration 4/25 | Loss: 0.00079641
Iteration 5/25 | Loss: 0.00079641
Iteration 6/25 | Loss: 0.00079641
Iteration 7/25 | Loss: 0.00079641
Iteration 8/25 | Loss: 0.00079641
Iteration 9/25 | Loss: 0.00079641
Iteration 10/25 | Loss: 0.00079641
Iteration 11/25 | Loss: 0.00079641
Iteration 12/25 | Loss: 0.00079641
Iteration 13/25 | Loss: 0.00079641
Iteration 14/25 | Loss: 0.00079640
Iteration 15/25 | Loss: 0.00079640
Iteration 16/25 | Loss: 0.00079640
Iteration 17/25 | Loss: 0.00079640
Iteration 18/25 | Loss: 0.00079640
Iteration 19/25 | Loss: 0.00079640
Iteration 20/25 | Loss: 0.00079640
Iteration 21/25 | Loss: 0.00079640
Iteration 22/25 | Loss: 0.00079640
Iteration 23/25 | Loss: 0.00079640
Iteration 24/25 | Loss: 0.00079640
Iteration 25/25 | Loss: 0.00079640

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079640
Iteration 2/1000 | Loss: 0.00002932
Iteration 3/1000 | Loss: 0.00002045
Iteration 4/1000 | Loss: 0.00001663
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001429
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001319
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001292
Iteration 11/1000 | Loss: 0.00001269
Iteration 12/1000 | Loss: 0.00001250
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001246
Iteration 15/1000 | Loss: 0.00001243
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001239
Iteration 23/1000 | Loss: 0.00001239
Iteration 24/1000 | Loss: 0.00001239
Iteration 25/1000 | Loss: 0.00001238
Iteration 26/1000 | Loss: 0.00001238
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001230
Iteration 29/1000 | Loss: 0.00001229
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001226
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001226
Iteration 35/1000 | Loss: 0.00001226
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001225
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001225
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001222
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001214
Iteration 70/1000 | Loss: 0.00001214
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001213
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001207
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001206
Iteration 107/1000 | Loss: 0.00001206
Iteration 108/1000 | Loss: 0.00001206
Iteration 109/1000 | Loss: 0.00001206
Iteration 110/1000 | Loss: 0.00001205
Iteration 111/1000 | Loss: 0.00001205
Iteration 112/1000 | Loss: 0.00001205
Iteration 113/1000 | Loss: 0.00001205
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001201
Iteration 133/1000 | Loss: 0.00001201
Iteration 134/1000 | Loss: 0.00001201
Iteration 135/1000 | Loss: 0.00001201
Iteration 136/1000 | Loss: 0.00001201
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001200
Iteration 142/1000 | Loss: 0.00001200
Iteration 143/1000 | Loss: 0.00001200
Iteration 144/1000 | Loss: 0.00001200
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001196
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001195
Iteration 195/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.1946511222049594e-05, 1.1946511222049594e-05, 1.1946511222049594e-05, 1.1946511222049594e-05, 1.1946511222049594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1946511222049594e-05

Optimization complete. Final v2v error: 2.9541025161743164 mm

Highest mean error: 3.3595504760742188 mm for frame 101

Lowest mean error: 2.798309087753296 mm for frame 140

Saving results

Total time: 42.823633909225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014614
Iteration 2/25 | Loss: 0.00234211
Iteration 3/25 | Loss: 0.00182784
Iteration 4/25 | Loss: 0.00172269
Iteration 5/25 | Loss: 0.00168474
Iteration 6/25 | Loss: 0.00152295
Iteration 7/25 | Loss: 0.00145300
Iteration 8/25 | Loss: 0.00143893
Iteration 9/25 | Loss: 0.00141545
Iteration 10/25 | Loss: 0.00140528
Iteration 11/25 | Loss: 0.00138445
Iteration 12/25 | Loss: 0.00137969
Iteration 13/25 | Loss: 0.00137793
Iteration 14/25 | Loss: 0.00138050
Iteration 15/25 | Loss: 0.00138028
Iteration 16/25 | Loss: 0.00138345
Iteration 17/25 | Loss: 0.00138108
Iteration 18/25 | Loss: 0.00138119
Iteration 19/25 | Loss: 0.00138116
Iteration 20/25 | Loss: 0.00138163
Iteration 21/25 | Loss: 0.00137697
Iteration 22/25 | Loss: 0.00137699
Iteration 23/25 | Loss: 0.00137760
Iteration 24/25 | Loss: 0.00137644
Iteration 25/25 | Loss: 0.00137731

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43423963
Iteration 2/25 | Loss: 0.00217238
Iteration 3/25 | Loss: 0.00196277
Iteration 4/25 | Loss: 0.00195270
Iteration 5/25 | Loss: 0.00195270
Iteration 6/25 | Loss: 0.00195270
Iteration 7/25 | Loss: 0.00195270
Iteration 8/25 | Loss: 0.00195270
Iteration 9/25 | Loss: 0.00195269
Iteration 10/25 | Loss: 0.00195269
Iteration 11/25 | Loss: 0.00195269
Iteration 12/25 | Loss: 0.00195269
Iteration 13/25 | Loss: 0.00195269
Iteration 14/25 | Loss: 0.00195269
Iteration 15/25 | Loss: 0.00195269
Iteration 16/25 | Loss: 0.00195269
Iteration 17/25 | Loss: 0.00195269
Iteration 18/25 | Loss: 0.00195269
Iteration 19/25 | Loss: 0.00195269
Iteration 20/25 | Loss: 0.00195269
Iteration 21/25 | Loss: 0.00195269
Iteration 22/25 | Loss: 0.00195269
Iteration 23/25 | Loss: 0.00195269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019526926334947348, 0.0019526926334947348, 0.0019526926334947348, 0.0019526926334947348, 0.0019526926334947348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019526926334947348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195269
Iteration 2/1000 | Loss: 0.00085322
Iteration 3/1000 | Loss: 0.00275691
Iteration 4/1000 | Loss: 0.00056965
Iteration 5/1000 | Loss: 0.00026395
Iteration 6/1000 | Loss: 0.00139181
Iteration 7/1000 | Loss: 0.00048378
Iteration 8/1000 | Loss: 0.00046337
Iteration 9/1000 | Loss: 0.00095436
Iteration 10/1000 | Loss: 0.00023055
Iteration 11/1000 | Loss: 0.00026034
Iteration 12/1000 | Loss: 0.00027222
Iteration 13/1000 | Loss: 0.00017535
Iteration 14/1000 | Loss: 0.00013377
Iteration 15/1000 | Loss: 0.00031397
Iteration 16/1000 | Loss: 0.00014684
Iteration 17/1000 | Loss: 0.00051644
Iteration 18/1000 | Loss: 0.00033601
Iteration 19/1000 | Loss: 0.00028632
Iteration 20/1000 | Loss: 0.00023540
Iteration 21/1000 | Loss: 0.00022022
Iteration 22/1000 | Loss: 0.00017954
Iteration 23/1000 | Loss: 0.00028796
Iteration 24/1000 | Loss: 0.00016694
Iteration 25/1000 | Loss: 0.00026251
Iteration 26/1000 | Loss: 0.00018126
Iteration 27/1000 | Loss: 0.00032558
Iteration 28/1000 | Loss: 0.00039579
Iteration 29/1000 | Loss: 0.00017501
Iteration 30/1000 | Loss: 0.00019403
Iteration 31/1000 | Loss: 0.00035144
Iteration 32/1000 | Loss: 0.00071229
Iteration 33/1000 | Loss: 0.00083269
Iteration 34/1000 | Loss: 0.00056263
Iteration 35/1000 | Loss: 0.00054194
Iteration 36/1000 | Loss: 0.00023663
Iteration 37/1000 | Loss: 0.00012495
Iteration 38/1000 | Loss: 0.00025896
Iteration 39/1000 | Loss: 0.00021577
Iteration 40/1000 | Loss: 0.00010458
Iteration 41/1000 | Loss: 0.00030779
Iteration 42/1000 | Loss: 0.00010983
Iteration 43/1000 | Loss: 0.00009484
Iteration 44/1000 | Loss: 0.00015908
Iteration 45/1000 | Loss: 0.00008826
Iteration 46/1000 | Loss: 0.00008824
Iteration 47/1000 | Loss: 0.00010422
Iteration 48/1000 | Loss: 0.00016453
Iteration 49/1000 | Loss: 0.00008905
Iteration 50/1000 | Loss: 0.00009508
Iteration 51/1000 | Loss: 0.00009812
Iteration 52/1000 | Loss: 0.00011639
Iteration 53/1000 | Loss: 0.00010606
Iteration 54/1000 | Loss: 0.00025357
Iteration 55/1000 | Loss: 0.00021099
Iteration 56/1000 | Loss: 0.00012806
Iteration 57/1000 | Loss: 0.00009242
Iteration 58/1000 | Loss: 0.00008426
Iteration 59/1000 | Loss: 0.00008829
Iteration 60/1000 | Loss: 0.00008977
Iteration 61/1000 | Loss: 0.00029575
Iteration 62/1000 | Loss: 0.00009417
Iteration 63/1000 | Loss: 0.00009277
Iteration 64/1000 | Loss: 0.00009552
Iteration 65/1000 | Loss: 0.00024170
Iteration 66/1000 | Loss: 0.00025518
Iteration 67/1000 | Loss: 0.00032126
Iteration 68/1000 | Loss: 0.00034277
Iteration 69/1000 | Loss: 0.00063283
Iteration 70/1000 | Loss: 0.00058330
Iteration 71/1000 | Loss: 0.00040975
Iteration 72/1000 | Loss: 0.00023023
Iteration 73/1000 | Loss: 0.00009711
Iteration 74/1000 | Loss: 0.00014561
Iteration 75/1000 | Loss: 0.00020699
Iteration 76/1000 | Loss: 0.00033734
Iteration 77/1000 | Loss: 0.00009748
Iteration 78/1000 | Loss: 0.00007598
Iteration 79/1000 | Loss: 0.00009561
Iteration 80/1000 | Loss: 0.00009823
Iteration 81/1000 | Loss: 0.00008481
Iteration 82/1000 | Loss: 0.00008491
Iteration 83/1000 | Loss: 0.00011199
Iteration 84/1000 | Loss: 0.00008273
Iteration 85/1000 | Loss: 0.00010241
Iteration 86/1000 | Loss: 0.00007102
Iteration 87/1000 | Loss: 0.00026272
Iteration 88/1000 | Loss: 0.00043469
Iteration 89/1000 | Loss: 0.00013951
Iteration 90/1000 | Loss: 0.00009856
Iteration 91/1000 | Loss: 0.00011405
Iteration 92/1000 | Loss: 0.00006610
Iteration 93/1000 | Loss: 0.00006401
Iteration 94/1000 | Loss: 0.00017417
Iteration 95/1000 | Loss: 0.00052965
Iteration 96/1000 | Loss: 0.00051795
Iteration 97/1000 | Loss: 0.00012676
Iteration 98/1000 | Loss: 0.00006616
Iteration 99/1000 | Loss: 0.00030711
Iteration 100/1000 | Loss: 0.00006635
Iteration 101/1000 | Loss: 0.00008622
Iteration 102/1000 | Loss: 0.00020956
Iteration 103/1000 | Loss: 0.00017156
Iteration 104/1000 | Loss: 0.00017583
Iteration 105/1000 | Loss: 0.00006662
Iteration 106/1000 | Loss: 0.00007835
Iteration 107/1000 | Loss: 0.00033945
Iteration 108/1000 | Loss: 0.00128265
Iteration 109/1000 | Loss: 0.00035943
Iteration 110/1000 | Loss: 0.00036232
Iteration 111/1000 | Loss: 0.00009572
Iteration 112/1000 | Loss: 0.00006486
Iteration 113/1000 | Loss: 0.00005981
Iteration 114/1000 | Loss: 0.00007643
Iteration 115/1000 | Loss: 0.00005789
Iteration 116/1000 | Loss: 0.00010028
Iteration 117/1000 | Loss: 0.00005687
Iteration 118/1000 | Loss: 0.00007871
Iteration 119/1000 | Loss: 0.00005631
Iteration 120/1000 | Loss: 0.00028015
Iteration 121/1000 | Loss: 0.00045393
Iteration 122/1000 | Loss: 0.00091036
Iteration 123/1000 | Loss: 0.00072152
Iteration 124/1000 | Loss: 0.00039926
Iteration 125/1000 | Loss: 0.00025848
Iteration 126/1000 | Loss: 0.00019344
Iteration 127/1000 | Loss: 0.00008318
Iteration 128/1000 | Loss: 0.00008868
Iteration 129/1000 | Loss: 0.00005678
Iteration 130/1000 | Loss: 0.00008478
Iteration 131/1000 | Loss: 0.00005536
Iteration 132/1000 | Loss: 0.00005413
Iteration 133/1000 | Loss: 0.00015344
Iteration 134/1000 | Loss: 0.00005863
Iteration 135/1000 | Loss: 0.00005548
Iteration 136/1000 | Loss: 0.00005386
Iteration 137/1000 | Loss: 0.00005256
Iteration 138/1000 | Loss: 0.00005181
Iteration 139/1000 | Loss: 0.00007232
Iteration 140/1000 | Loss: 0.00005158
Iteration 141/1000 | Loss: 0.00007554
Iteration 142/1000 | Loss: 0.00026677
Iteration 143/1000 | Loss: 0.00011990
Iteration 144/1000 | Loss: 0.00013075
Iteration 145/1000 | Loss: 0.00005456
Iteration 146/1000 | Loss: 0.00005264
Iteration 147/1000 | Loss: 0.00005134
Iteration 148/1000 | Loss: 0.00005038
Iteration 149/1000 | Loss: 0.00004993
Iteration 150/1000 | Loss: 0.00004960
Iteration 151/1000 | Loss: 0.00004934
Iteration 152/1000 | Loss: 0.00004922
Iteration 153/1000 | Loss: 0.00004921
Iteration 154/1000 | Loss: 0.00004919
Iteration 155/1000 | Loss: 0.00004918
Iteration 156/1000 | Loss: 0.00004916
Iteration 157/1000 | Loss: 0.00025138
Iteration 158/1000 | Loss: 0.00070946
Iteration 159/1000 | Loss: 0.00053699
Iteration 160/1000 | Loss: 0.00062780
Iteration 161/1000 | Loss: 0.00037892
Iteration 162/1000 | Loss: 0.00020802
Iteration 163/1000 | Loss: 0.00039998
Iteration 164/1000 | Loss: 0.00023882
Iteration 165/1000 | Loss: 0.00006067
Iteration 166/1000 | Loss: 0.00006482
Iteration 167/1000 | Loss: 0.00005417
Iteration 168/1000 | Loss: 0.00005031
Iteration 169/1000 | Loss: 0.00008842
Iteration 170/1000 | Loss: 0.00005626
Iteration 171/1000 | Loss: 0.00006140
Iteration 172/1000 | Loss: 0.00004734
Iteration 173/1000 | Loss: 0.00004720
Iteration 174/1000 | Loss: 0.00004695
Iteration 175/1000 | Loss: 0.00007042
Iteration 176/1000 | Loss: 0.00004667
Iteration 177/1000 | Loss: 0.00004650
Iteration 178/1000 | Loss: 0.00004647
Iteration 179/1000 | Loss: 0.00004644
Iteration 180/1000 | Loss: 0.00004639
Iteration 181/1000 | Loss: 0.00004638
Iteration 182/1000 | Loss: 0.00006663
Iteration 183/1000 | Loss: 0.00004678
Iteration 184/1000 | Loss: 0.00004754
Iteration 185/1000 | Loss: 0.00004630
Iteration 186/1000 | Loss: 0.00004630
Iteration 187/1000 | Loss: 0.00004630
Iteration 188/1000 | Loss: 0.00004630
Iteration 189/1000 | Loss: 0.00004630
Iteration 190/1000 | Loss: 0.00004630
Iteration 191/1000 | Loss: 0.00004630
Iteration 192/1000 | Loss: 0.00004630
Iteration 193/1000 | Loss: 0.00004630
Iteration 194/1000 | Loss: 0.00004629
Iteration 195/1000 | Loss: 0.00004629
Iteration 196/1000 | Loss: 0.00004629
Iteration 197/1000 | Loss: 0.00004628
Iteration 198/1000 | Loss: 0.00004628
Iteration 199/1000 | Loss: 0.00004627
Iteration 200/1000 | Loss: 0.00004627
Iteration 201/1000 | Loss: 0.00004627
Iteration 202/1000 | Loss: 0.00004627
Iteration 203/1000 | Loss: 0.00004627
Iteration 204/1000 | Loss: 0.00004626
Iteration 205/1000 | Loss: 0.00004626
Iteration 206/1000 | Loss: 0.00004625
Iteration 207/1000 | Loss: 0.00004625
Iteration 208/1000 | Loss: 0.00004622
Iteration 209/1000 | Loss: 0.00004619
Iteration 210/1000 | Loss: 0.00004619
Iteration 211/1000 | Loss: 0.00004617
Iteration 212/1000 | Loss: 0.00004617
Iteration 213/1000 | Loss: 0.00004616
Iteration 214/1000 | Loss: 0.00004616
Iteration 215/1000 | Loss: 0.00004616
Iteration 216/1000 | Loss: 0.00004615
Iteration 217/1000 | Loss: 0.00004615
Iteration 218/1000 | Loss: 0.00004615
Iteration 219/1000 | Loss: 0.00004615
Iteration 220/1000 | Loss: 0.00004614
Iteration 221/1000 | Loss: 0.00004614
Iteration 222/1000 | Loss: 0.00004614
Iteration 223/1000 | Loss: 0.00004614
Iteration 224/1000 | Loss: 0.00004614
Iteration 225/1000 | Loss: 0.00004614
Iteration 226/1000 | Loss: 0.00004614
Iteration 227/1000 | Loss: 0.00004614
Iteration 228/1000 | Loss: 0.00004614
Iteration 229/1000 | Loss: 0.00004613
Iteration 230/1000 | Loss: 0.00004613
Iteration 231/1000 | Loss: 0.00004612
Iteration 232/1000 | Loss: 0.00004612
Iteration 233/1000 | Loss: 0.00004611
Iteration 234/1000 | Loss: 0.00004611
Iteration 235/1000 | Loss: 0.00004611
Iteration 236/1000 | Loss: 0.00004611
Iteration 237/1000 | Loss: 0.00004611
Iteration 238/1000 | Loss: 0.00004610
Iteration 239/1000 | Loss: 0.00004610
Iteration 240/1000 | Loss: 0.00004610
Iteration 241/1000 | Loss: 0.00004610
Iteration 242/1000 | Loss: 0.00004609
Iteration 243/1000 | Loss: 0.00004609
Iteration 244/1000 | Loss: 0.00004609
Iteration 245/1000 | Loss: 0.00004609
Iteration 246/1000 | Loss: 0.00004609
Iteration 247/1000 | Loss: 0.00004609
Iteration 248/1000 | Loss: 0.00004609
Iteration 249/1000 | Loss: 0.00004609
Iteration 250/1000 | Loss: 0.00004609
Iteration 251/1000 | Loss: 0.00004609
Iteration 252/1000 | Loss: 0.00004608
Iteration 253/1000 | Loss: 0.00004608
Iteration 254/1000 | Loss: 0.00004608
Iteration 255/1000 | Loss: 0.00004608
Iteration 256/1000 | Loss: 0.00004608
Iteration 257/1000 | Loss: 0.00004608
Iteration 258/1000 | Loss: 0.00004607
Iteration 259/1000 | Loss: 0.00004607
Iteration 260/1000 | Loss: 0.00004607
Iteration 261/1000 | Loss: 0.00004607
Iteration 262/1000 | Loss: 0.00004607
Iteration 263/1000 | Loss: 0.00004607
Iteration 264/1000 | Loss: 0.00004607
Iteration 265/1000 | Loss: 0.00004607
Iteration 266/1000 | Loss: 0.00004607
Iteration 267/1000 | Loss: 0.00004607
Iteration 268/1000 | Loss: 0.00004607
Iteration 269/1000 | Loss: 0.00004606
Iteration 270/1000 | Loss: 0.00004606
Iteration 271/1000 | Loss: 0.00004606
Iteration 272/1000 | Loss: 0.00004606
Iteration 273/1000 | Loss: 0.00004606
Iteration 274/1000 | Loss: 0.00004606
Iteration 275/1000 | Loss: 0.00004606
Iteration 276/1000 | Loss: 0.00004606
Iteration 277/1000 | Loss: 0.00004606
Iteration 278/1000 | Loss: 0.00004606
Iteration 279/1000 | Loss: 0.00004606
Iteration 280/1000 | Loss: 0.00004606
Iteration 281/1000 | Loss: 0.00004606
Iteration 282/1000 | Loss: 0.00004606
Iteration 283/1000 | Loss: 0.00004606
Iteration 284/1000 | Loss: 0.00004605
Iteration 285/1000 | Loss: 0.00004605
Iteration 286/1000 | Loss: 0.00004605
Iteration 287/1000 | Loss: 0.00004605
Iteration 288/1000 | Loss: 0.00004605
Iteration 289/1000 | Loss: 0.00004605
Iteration 290/1000 | Loss: 0.00004604
Iteration 291/1000 | Loss: 0.00004604
Iteration 292/1000 | Loss: 0.00004604
Iteration 293/1000 | Loss: 0.00004604
Iteration 294/1000 | Loss: 0.00004604
Iteration 295/1000 | Loss: 0.00004604
Iteration 296/1000 | Loss: 0.00004604
Iteration 297/1000 | Loss: 0.00004604
Iteration 298/1000 | Loss: 0.00004604
Iteration 299/1000 | Loss: 0.00004604
Iteration 300/1000 | Loss: 0.00004604
Iteration 301/1000 | Loss: 0.00004604
Iteration 302/1000 | Loss: 0.00004604
Iteration 303/1000 | Loss: 0.00004603
Iteration 304/1000 | Loss: 0.00004603
Iteration 305/1000 | Loss: 0.00004603
Iteration 306/1000 | Loss: 0.00004603
Iteration 307/1000 | Loss: 0.00004603
Iteration 308/1000 | Loss: 0.00004603
Iteration 309/1000 | Loss: 0.00004603
Iteration 310/1000 | Loss: 0.00004603
Iteration 311/1000 | Loss: 0.00004603
Iteration 312/1000 | Loss: 0.00004603
Iteration 313/1000 | Loss: 0.00004603
Iteration 314/1000 | Loss: 0.00004603
Iteration 315/1000 | Loss: 0.00004603
Iteration 316/1000 | Loss: 0.00004603
Iteration 317/1000 | Loss: 0.00004603
Iteration 318/1000 | Loss: 0.00004603
Iteration 319/1000 | Loss: 0.00004603
Iteration 320/1000 | Loss: 0.00004603
Iteration 321/1000 | Loss: 0.00004603
Iteration 322/1000 | Loss: 0.00004603
Iteration 323/1000 | Loss: 0.00004603
Iteration 324/1000 | Loss: 0.00004603
Iteration 325/1000 | Loss: 0.00004603
Iteration 326/1000 | Loss: 0.00004603
Iteration 327/1000 | Loss: 0.00004603
Iteration 328/1000 | Loss: 0.00004603
Iteration 329/1000 | Loss: 0.00004603
Iteration 330/1000 | Loss: 0.00004603
Iteration 331/1000 | Loss: 0.00004603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [4.6032007958274335e-05, 4.6032007958274335e-05, 4.6032007958274335e-05, 4.6032007958274335e-05, 4.6032007958274335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.6032007958274335e-05

Optimization complete. Final v2v error: 3.9233217239379883 mm

Highest mean error: 11.132611274719238 mm for frame 144

Lowest mean error: 3.0051984786987305 mm for frame 167

Saving results

Total time: 345.1248986721039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813174
Iteration 2/25 | Loss: 0.00131149
Iteration 3/25 | Loss: 0.00121306
Iteration 4/25 | Loss: 0.00120336
Iteration 5/25 | Loss: 0.00120101
Iteration 6/25 | Loss: 0.00120101
Iteration 7/25 | Loss: 0.00120101
Iteration 8/25 | Loss: 0.00120101
Iteration 9/25 | Loss: 0.00120101
Iteration 10/25 | Loss: 0.00120101
Iteration 11/25 | Loss: 0.00120101
Iteration 12/25 | Loss: 0.00120101
Iteration 13/25 | Loss: 0.00120101
Iteration 14/25 | Loss: 0.00120101
Iteration 15/25 | Loss: 0.00120101
Iteration 16/25 | Loss: 0.00120101
Iteration 17/25 | Loss: 0.00120101
Iteration 18/25 | Loss: 0.00120101
Iteration 19/25 | Loss: 0.00120101
Iteration 20/25 | Loss: 0.00120101
Iteration 21/25 | Loss: 0.00120101
Iteration 22/25 | Loss: 0.00120101
Iteration 23/25 | Loss: 0.00120101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012010071659460664, 0.0012010071659460664, 0.0012010071659460664, 0.0012010071659460664, 0.0012010071659460664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012010071659460664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43450654
Iteration 2/25 | Loss: 0.00072787
Iteration 3/25 | Loss: 0.00072787
Iteration 4/25 | Loss: 0.00072787
Iteration 5/25 | Loss: 0.00072787
Iteration 6/25 | Loss: 0.00072787
Iteration 7/25 | Loss: 0.00072787
Iteration 8/25 | Loss: 0.00072787
Iteration 9/25 | Loss: 0.00072787
Iteration 10/25 | Loss: 0.00072787
Iteration 11/25 | Loss: 0.00072787
Iteration 12/25 | Loss: 0.00072787
Iteration 13/25 | Loss: 0.00072787
Iteration 14/25 | Loss: 0.00072787
Iteration 15/25 | Loss: 0.00072787
Iteration 16/25 | Loss: 0.00072787
Iteration 17/25 | Loss: 0.00072787
Iteration 18/25 | Loss: 0.00072787
Iteration 19/25 | Loss: 0.00072787
Iteration 20/25 | Loss: 0.00072787
Iteration 21/25 | Loss: 0.00072787
Iteration 22/25 | Loss: 0.00072787
Iteration 23/25 | Loss: 0.00072787
Iteration 24/25 | Loss: 0.00072787
Iteration 25/25 | Loss: 0.00072787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072787
Iteration 2/1000 | Loss: 0.00002199
Iteration 3/1000 | Loss: 0.00001615
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001269
Iteration 7/1000 | Loss: 0.00001233
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001171
Iteration 13/1000 | Loss: 0.00001169
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001155
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001153
Iteration 24/1000 | Loss: 0.00001149
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001147
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001146
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00001144
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001143
Iteration 34/1000 | Loss: 0.00001143
Iteration 35/1000 | Loss: 0.00001142
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001141
Iteration 43/1000 | Loss: 0.00001141
Iteration 44/1000 | Loss: 0.00001141
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001139
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001138
Iteration 51/1000 | Loss: 0.00001138
Iteration 52/1000 | Loss: 0.00001138
Iteration 53/1000 | Loss: 0.00001137
Iteration 54/1000 | Loss: 0.00001136
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001135
Iteration 57/1000 | Loss: 0.00001135
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001134
Iteration 60/1000 | Loss: 0.00001133
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001129
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001128
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001122
Iteration 70/1000 | Loss: 0.00001119
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001116
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001113
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001110
Iteration 110/1000 | Loss: 0.00001110
Iteration 111/1000 | Loss: 0.00001110
Iteration 112/1000 | Loss: 0.00001110
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001107
Iteration 128/1000 | Loss: 0.00001107
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001106
Iteration 138/1000 | Loss: 0.00001106
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001103
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001099
Iteration 171/1000 | Loss: 0.00001099
Iteration 172/1000 | Loss: 0.00001099
Iteration 173/1000 | Loss: 0.00001099
Iteration 174/1000 | Loss: 0.00001099
Iteration 175/1000 | Loss: 0.00001099
Iteration 176/1000 | Loss: 0.00001099
Iteration 177/1000 | Loss: 0.00001099
Iteration 178/1000 | Loss: 0.00001099
Iteration 179/1000 | Loss: 0.00001098
Iteration 180/1000 | Loss: 0.00001098
Iteration 181/1000 | Loss: 0.00001098
Iteration 182/1000 | Loss: 0.00001097
Iteration 183/1000 | Loss: 0.00001097
Iteration 184/1000 | Loss: 0.00001097
Iteration 185/1000 | Loss: 0.00001097
Iteration 186/1000 | Loss: 0.00001097
Iteration 187/1000 | Loss: 0.00001097
Iteration 188/1000 | Loss: 0.00001097
Iteration 189/1000 | Loss: 0.00001097
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001095
Iteration 196/1000 | Loss: 0.00001095
Iteration 197/1000 | Loss: 0.00001095
Iteration 198/1000 | Loss: 0.00001095
Iteration 199/1000 | Loss: 0.00001095
Iteration 200/1000 | Loss: 0.00001095
Iteration 201/1000 | Loss: 0.00001094
Iteration 202/1000 | Loss: 0.00001094
Iteration 203/1000 | Loss: 0.00001094
Iteration 204/1000 | Loss: 0.00001094
Iteration 205/1000 | Loss: 0.00001094
Iteration 206/1000 | Loss: 0.00001094
Iteration 207/1000 | Loss: 0.00001094
Iteration 208/1000 | Loss: 0.00001093
Iteration 209/1000 | Loss: 0.00001093
Iteration 210/1000 | Loss: 0.00001093
Iteration 211/1000 | Loss: 0.00001093
Iteration 212/1000 | Loss: 0.00001093
Iteration 213/1000 | Loss: 0.00001093
Iteration 214/1000 | Loss: 0.00001093
Iteration 215/1000 | Loss: 0.00001093
Iteration 216/1000 | Loss: 0.00001093
Iteration 217/1000 | Loss: 0.00001093
Iteration 218/1000 | Loss: 0.00001093
Iteration 219/1000 | Loss: 0.00001093
Iteration 220/1000 | Loss: 0.00001093
Iteration 221/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0934378224192187e-05, 1.0934378224192187e-05, 1.0934378224192187e-05, 1.0934378224192187e-05, 1.0934378224192187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0934378224192187e-05

Optimization complete. Final v2v error: 2.824657440185547 mm

Highest mean error: 2.993882417678833 mm for frame 109

Lowest mean error: 2.6884286403656006 mm for frame 208

Saving results

Total time: 44.129671573638916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804754
Iteration 2/25 | Loss: 0.00133675
Iteration 3/25 | Loss: 0.00121745
Iteration 4/25 | Loss: 0.00120384
Iteration 5/25 | Loss: 0.00120135
Iteration 6/25 | Loss: 0.00120124
Iteration 7/25 | Loss: 0.00120124
Iteration 8/25 | Loss: 0.00120124
Iteration 9/25 | Loss: 0.00120124
Iteration 10/25 | Loss: 0.00120124
Iteration 11/25 | Loss: 0.00120124
Iteration 12/25 | Loss: 0.00120124
Iteration 13/25 | Loss: 0.00120124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001201236736960709, 0.001201236736960709, 0.001201236736960709, 0.001201236736960709, 0.001201236736960709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001201236736960709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43586290
Iteration 2/25 | Loss: 0.00070848
Iteration 3/25 | Loss: 0.00070848
Iteration 4/25 | Loss: 0.00070848
Iteration 5/25 | Loss: 0.00070848
Iteration 6/25 | Loss: 0.00070848
Iteration 7/25 | Loss: 0.00070848
Iteration 8/25 | Loss: 0.00070848
Iteration 9/25 | Loss: 0.00070848
Iteration 10/25 | Loss: 0.00070848
Iteration 11/25 | Loss: 0.00070848
Iteration 12/25 | Loss: 0.00070848
Iteration 13/25 | Loss: 0.00070848
Iteration 14/25 | Loss: 0.00070848
Iteration 15/25 | Loss: 0.00070848
Iteration 16/25 | Loss: 0.00070848
Iteration 17/25 | Loss: 0.00070848
Iteration 18/25 | Loss: 0.00070848
Iteration 19/25 | Loss: 0.00070848
Iteration 20/25 | Loss: 0.00070848
Iteration 21/25 | Loss: 0.00070848
Iteration 22/25 | Loss: 0.00070848
Iteration 23/25 | Loss: 0.00070848
Iteration 24/25 | Loss: 0.00070848
Iteration 25/25 | Loss: 0.00070848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070848
Iteration 2/1000 | Loss: 0.00002393
Iteration 3/1000 | Loss: 0.00001720
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001318
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001265
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001238
Iteration 14/1000 | Loss: 0.00001237
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001229
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001226
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001213
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001211
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00001207
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001203
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001203
Iteration 63/1000 | Loss: 0.00001202
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00001202
Iteration 66/1000 | Loss: 0.00001202
Iteration 67/1000 | Loss: 0.00001202
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001200
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001199
Iteration 79/1000 | Loss: 0.00001199
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001198
Iteration 84/1000 | Loss: 0.00001197
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001197
Iteration 88/1000 | Loss: 0.00001197
Iteration 89/1000 | Loss: 0.00001197
Iteration 90/1000 | Loss: 0.00001197
Iteration 91/1000 | Loss: 0.00001196
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001195
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001194
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001186
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001179
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001179
Iteration 153/1000 | Loss: 0.00001179
Iteration 154/1000 | Loss: 0.00001179
Iteration 155/1000 | Loss: 0.00001179
Iteration 156/1000 | Loss: 0.00001179
Iteration 157/1000 | Loss: 0.00001179
Iteration 158/1000 | Loss: 0.00001179
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001178
Iteration 161/1000 | Loss: 0.00001178
Iteration 162/1000 | Loss: 0.00001178
Iteration 163/1000 | Loss: 0.00001178
Iteration 164/1000 | Loss: 0.00001178
Iteration 165/1000 | Loss: 0.00001178
Iteration 166/1000 | Loss: 0.00001178
Iteration 167/1000 | Loss: 0.00001178
Iteration 168/1000 | Loss: 0.00001178
Iteration 169/1000 | Loss: 0.00001177
Iteration 170/1000 | Loss: 0.00001177
Iteration 171/1000 | Loss: 0.00001177
Iteration 172/1000 | Loss: 0.00001177
Iteration 173/1000 | Loss: 0.00001177
Iteration 174/1000 | Loss: 0.00001177
Iteration 175/1000 | Loss: 0.00001177
Iteration 176/1000 | Loss: 0.00001177
Iteration 177/1000 | Loss: 0.00001177
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Iteration 184/1000 | Loss: 0.00001176
Iteration 185/1000 | Loss: 0.00001176
Iteration 186/1000 | Loss: 0.00001176
Iteration 187/1000 | Loss: 0.00001176
Iteration 188/1000 | Loss: 0.00001176
Iteration 189/1000 | Loss: 0.00001176
Iteration 190/1000 | Loss: 0.00001175
Iteration 191/1000 | Loss: 0.00001175
Iteration 192/1000 | Loss: 0.00001175
Iteration 193/1000 | Loss: 0.00001175
Iteration 194/1000 | Loss: 0.00001175
Iteration 195/1000 | Loss: 0.00001175
Iteration 196/1000 | Loss: 0.00001175
Iteration 197/1000 | Loss: 0.00001175
Iteration 198/1000 | Loss: 0.00001175
Iteration 199/1000 | Loss: 0.00001175
Iteration 200/1000 | Loss: 0.00001174
Iteration 201/1000 | Loss: 0.00001174
Iteration 202/1000 | Loss: 0.00001174
Iteration 203/1000 | Loss: 0.00001174
Iteration 204/1000 | Loss: 0.00001174
Iteration 205/1000 | Loss: 0.00001174
Iteration 206/1000 | Loss: 0.00001174
Iteration 207/1000 | Loss: 0.00001174
Iteration 208/1000 | Loss: 0.00001174
Iteration 209/1000 | Loss: 0.00001174
Iteration 210/1000 | Loss: 0.00001174
Iteration 211/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.173635246232152e-05, 1.173635246232152e-05, 1.173635246232152e-05, 1.173635246232152e-05, 1.173635246232152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.173635246232152e-05

Optimization complete. Final v2v error: 2.9265596866607666 mm

Highest mean error: 3.230363607406616 mm for frame 46

Lowest mean error: 2.767409324645996 mm for frame 160

Saving results

Total time: 40.280279874801636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402656
Iteration 2/25 | Loss: 0.00129927
Iteration 3/25 | Loss: 0.00123229
Iteration 4/25 | Loss: 0.00122194
Iteration 5/25 | Loss: 0.00121769
Iteration 6/25 | Loss: 0.00121717
Iteration 7/25 | Loss: 0.00121717
Iteration 8/25 | Loss: 0.00121717
Iteration 9/25 | Loss: 0.00121717
Iteration 10/25 | Loss: 0.00121717
Iteration 11/25 | Loss: 0.00121717
Iteration 12/25 | Loss: 0.00121717
Iteration 13/25 | Loss: 0.00121717
Iteration 14/25 | Loss: 0.00121717
Iteration 15/25 | Loss: 0.00121717
Iteration 16/25 | Loss: 0.00121717
Iteration 17/25 | Loss: 0.00121717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012171692214906216, 0.0012171692214906216, 0.0012171692214906216, 0.0012171692214906216, 0.0012171692214906216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171692214906216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43980014
Iteration 2/25 | Loss: 0.00080349
Iteration 3/25 | Loss: 0.00080349
Iteration 4/25 | Loss: 0.00080348
Iteration 5/25 | Loss: 0.00080348
Iteration 6/25 | Loss: 0.00080348
Iteration 7/25 | Loss: 0.00080348
Iteration 8/25 | Loss: 0.00080348
Iteration 9/25 | Loss: 0.00080348
Iteration 10/25 | Loss: 0.00080348
Iteration 11/25 | Loss: 0.00080348
Iteration 12/25 | Loss: 0.00080348
Iteration 13/25 | Loss: 0.00080348
Iteration 14/25 | Loss: 0.00080348
Iteration 15/25 | Loss: 0.00080348
Iteration 16/25 | Loss: 0.00080348
Iteration 17/25 | Loss: 0.00080348
Iteration 18/25 | Loss: 0.00080348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008034817292355001, 0.0008034817292355001, 0.0008034817292355001, 0.0008034817292355001, 0.0008034817292355001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008034817292355001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080348
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001571
Iteration 4/1000 | Loss: 0.00001432
Iteration 5/1000 | Loss: 0.00001382
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001294
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001277
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001247
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001228
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001213
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001210
Iteration 35/1000 | Loss: 0.00001210
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001205
Iteration 41/1000 | Loss: 0.00001205
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001204
Iteration 46/1000 | Loss: 0.00001204
Iteration 47/1000 | Loss: 0.00001204
Iteration 48/1000 | Loss: 0.00001204
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001203
Iteration 52/1000 | Loss: 0.00001203
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001191
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001188
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001187
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001185
Iteration 88/1000 | Loss: 0.00001183
Iteration 89/1000 | Loss: 0.00001183
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001177
Iteration 105/1000 | Loss: 0.00001176
Iteration 106/1000 | Loss: 0.00001176
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001175
Iteration 113/1000 | Loss: 0.00001175
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001171
Iteration 123/1000 | Loss: 0.00001171
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001171
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001170
Iteration 132/1000 | Loss: 0.00001170
Iteration 133/1000 | Loss: 0.00001170
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001168
Iteration 139/1000 | Loss: 0.00001168
Iteration 140/1000 | Loss: 0.00001168
Iteration 141/1000 | Loss: 0.00001168
Iteration 142/1000 | Loss: 0.00001168
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001167
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Iteration 163/1000 | Loss: 0.00001166
Iteration 164/1000 | Loss: 0.00001166
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001166
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001166
Iteration 186/1000 | Loss: 0.00001166
Iteration 187/1000 | Loss: 0.00001166
Iteration 188/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1655947673716582e-05, 1.1655947673716582e-05, 1.1655947673716582e-05, 1.1655947673716582e-05, 1.1655947673716582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1655947673716582e-05

Optimization complete. Final v2v error: 2.965764045715332 mm

Highest mean error: 3.0467264652252197 mm for frame 62

Lowest mean error: 2.910372495651245 mm for frame 0

Saving results

Total time: 38.487316370010376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390369
Iteration 2/25 | Loss: 0.00127658
Iteration 3/25 | Loss: 0.00119807
Iteration 4/25 | Loss: 0.00118557
Iteration 5/25 | Loss: 0.00118135
Iteration 6/25 | Loss: 0.00118079
Iteration 7/25 | Loss: 0.00118079
Iteration 8/25 | Loss: 0.00118079
Iteration 9/25 | Loss: 0.00118079
Iteration 10/25 | Loss: 0.00118079
Iteration 11/25 | Loss: 0.00118079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011807894334197044, 0.0011807894334197044, 0.0011807894334197044, 0.0011807894334197044, 0.0011807894334197044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011807894334197044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52507102
Iteration 2/25 | Loss: 0.00074404
Iteration 3/25 | Loss: 0.00074404
Iteration 4/25 | Loss: 0.00074403
Iteration 5/25 | Loss: 0.00074403
Iteration 6/25 | Loss: 0.00074403
Iteration 7/25 | Loss: 0.00074403
Iteration 8/25 | Loss: 0.00074403
Iteration 9/25 | Loss: 0.00074403
Iteration 10/25 | Loss: 0.00074403
Iteration 11/25 | Loss: 0.00074403
Iteration 12/25 | Loss: 0.00074403
Iteration 13/25 | Loss: 0.00074403
Iteration 14/25 | Loss: 0.00074403
Iteration 15/25 | Loss: 0.00074403
Iteration 16/25 | Loss: 0.00074403
Iteration 17/25 | Loss: 0.00074403
Iteration 18/25 | Loss: 0.00074403
Iteration 19/25 | Loss: 0.00074403
Iteration 20/25 | Loss: 0.00074403
Iteration 21/25 | Loss: 0.00074403
Iteration 22/25 | Loss: 0.00074403
Iteration 23/25 | Loss: 0.00074403
Iteration 24/25 | Loss: 0.00074403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000744032149668783, 0.000744032149668783, 0.000744032149668783, 0.000744032149668783, 0.000744032149668783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000744032149668783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074403
Iteration 2/1000 | Loss: 0.00002058
Iteration 3/1000 | Loss: 0.00001335
Iteration 4/1000 | Loss: 0.00001223
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001111
Iteration 7/1000 | Loss: 0.00001094
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001091
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001042
Iteration 14/1000 | Loss: 0.00001038
Iteration 15/1000 | Loss: 0.00001037
Iteration 16/1000 | Loss: 0.00001037
Iteration 17/1000 | Loss: 0.00001036
Iteration 18/1000 | Loss: 0.00001029
Iteration 19/1000 | Loss: 0.00001029
Iteration 20/1000 | Loss: 0.00001028
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001028
Iteration 23/1000 | Loss: 0.00001028
Iteration 24/1000 | Loss: 0.00001023
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001022
Iteration 27/1000 | Loss: 0.00001022
Iteration 28/1000 | Loss: 0.00001021
Iteration 29/1000 | Loss: 0.00001020
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001019
Iteration 32/1000 | Loss: 0.00001018
Iteration 33/1000 | Loss: 0.00001018
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001017
Iteration 36/1000 | Loss: 0.00001017
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001015
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001015
Iteration 43/1000 | Loss: 0.00001014
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001012
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001007
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001006
Iteration 65/1000 | Loss: 0.00001006
Iteration 66/1000 | Loss: 0.00001006
Iteration 67/1000 | Loss: 0.00001006
Iteration 68/1000 | Loss: 0.00001006
Iteration 69/1000 | Loss: 0.00001006
Iteration 70/1000 | Loss: 0.00001006
Iteration 71/1000 | Loss: 0.00001006
Iteration 72/1000 | Loss: 0.00001006
Iteration 73/1000 | Loss: 0.00001006
Iteration 74/1000 | Loss: 0.00001005
Iteration 75/1000 | Loss: 0.00001005
Iteration 76/1000 | Loss: 0.00001005
Iteration 77/1000 | Loss: 0.00001005
Iteration 78/1000 | Loss: 0.00001005
Iteration 79/1000 | Loss: 0.00001005
Iteration 80/1000 | Loss: 0.00001005
Iteration 81/1000 | Loss: 0.00001005
Iteration 82/1000 | Loss: 0.00001004
Iteration 83/1000 | Loss: 0.00001004
Iteration 84/1000 | Loss: 0.00001004
Iteration 85/1000 | Loss: 0.00001003
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001001
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000993
Iteration 98/1000 | Loss: 0.00000993
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000991
Iteration 101/1000 | Loss: 0.00000990
Iteration 102/1000 | Loss: 0.00000990
Iteration 103/1000 | Loss: 0.00000990
Iteration 104/1000 | Loss: 0.00000989
Iteration 105/1000 | Loss: 0.00000988
Iteration 106/1000 | Loss: 0.00000988
Iteration 107/1000 | Loss: 0.00000988
Iteration 108/1000 | Loss: 0.00000987
Iteration 109/1000 | Loss: 0.00000987
Iteration 110/1000 | Loss: 0.00000986
Iteration 111/1000 | Loss: 0.00000986
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000984
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000983
Iteration 121/1000 | Loss: 0.00000983
Iteration 122/1000 | Loss: 0.00000982
Iteration 123/1000 | Loss: 0.00000982
Iteration 124/1000 | Loss: 0.00000982
Iteration 125/1000 | Loss: 0.00000982
Iteration 126/1000 | Loss: 0.00000981
Iteration 127/1000 | Loss: 0.00000981
Iteration 128/1000 | Loss: 0.00000981
Iteration 129/1000 | Loss: 0.00000980
Iteration 130/1000 | Loss: 0.00000980
Iteration 131/1000 | Loss: 0.00000979
Iteration 132/1000 | Loss: 0.00000979
Iteration 133/1000 | Loss: 0.00000978
Iteration 134/1000 | Loss: 0.00000978
Iteration 135/1000 | Loss: 0.00000978
Iteration 136/1000 | Loss: 0.00000978
Iteration 137/1000 | Loss: 0.00000977
Iteration 138/1000 | Loss: 0.00000977
Iteration 139/1000 | Loss: 0.00000977
Iteration 140/1000 | Loss: 0.00000977
Iteration 141/1000 | Loss: 0.00000977
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000976
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000975
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000975
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000975
Iteration 158/1000 | Loss: 0.00000975
Iteration 159/1000 | Loss: 0.00000975
Iteration 160/1000 | Loss: 0.00000975
Iteration 161/1000 | Loss: 0.00000975
Iteration 162/1000 | Loss: 0.00000975
Iteration 163/1000 | Loss: 0.00000975
Iteration 164/1000 | Loss: 0.00000975
Iteration 165/1000 | Loss: 0.00000975
Iteration 166/1000 | Loss: 0.00000975
Iteration 167/1000 | Loss: 0.00000975
Iteration 168/1000 | Loss: 0.00000975
Iteration 169/1000 | Loss: 0.00000975
Iteration 170/1000 | Loss: 0.00000975
Iteration 171/1000 | Loss: 0.00000975
Iteration 172/1000 | Loss: 0.00000975
Iteration 173/1000 | Loss: 0.00000975
Iteration 174/1000 | Loss: 0.00000974
Iteration 175/1000 | Loss: 0.00000974
Iteration 176/1000 | Loss: 0.00000974
Iteration 177/1000 | Loss: 0.00000974
Iteration 178/1000 | Loss: 0.00000974
Iteration 179/1000 | Loss: 0.00000974
Iteration 180/1000 | Loss: 0.00000974
Iteration 181/1000 | Loss: 0.00000974
Iteration 182/1000 | Loss: 0.00000974
Iteration 183/1000 | Loss: 0.00000974
Iteration 184/1000 | Loss: 0.00000974
Iteration 185/1000 | Loss: 0.00000974
Iteration 186/1000 | Loss: 0.00000974
Iteration 187/1000 | Loss: 0.00000974
Iteration 188/1000 | Loss: 0.00000974
Iteration 189/1000 | Loss: 0.00000974
Iteration 190/1000 | Loss: 0.00000974
Iteration 191/1000 | Loss: 0.00000974
Iteration 192/1000 | Loss: 0.00000973
Iteration 193/1000 | Loss: 0.00000973
Iteration 194/1000 | Loss: 0.00000973
Iteration 195/1000 | Loss: 0.00000973
Iteration 196/1000 | Loss: 0.00000973
Iteration 197/1000 | Loss: 0.00000973
Iteration 198/1000 | Loss: 0.00000973
Iteration 199/1000 | Loss: 0.00000973
Iteration 200/1000 | Loss: 0.00000973
Iteration 201/1000 | Loss: 0.00000973
Iteration 202/1000 | Loss: 0.00000973
Iteration 203/1000 | Loss: 0.00000973
Iteration 204/1000 | Loss: 0.00000973
Iteration 205/1000 | Loss: 0.00000972
Iteration 206/1000 | Loss: 0.00000972
Iteration 207/1000 | Loss: 0.00000972
Iteration 208/1000 | Loss: 0.00000972
Iteration 209/1000 | Loss: 0.00000972
Iteration 210/1000 | Loss: 0.00000972
Iteration 211/1000 | Loss: 0.00000972
Iteration 212/1000 | Loss: 0.00000972
Iteration 213/1000 | Loss: 0.00000972
Iteration 214/1000 | Loss: 0.00000972
Iteration 215/1000 | Loss: 0.00000972
Iteration 216/1000 | Loss: 0.00000972
Iteration 217/1000 | Loss: 0.00000972
Iteration 218/1000 | Loss: 0.00000972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [9.72228644968709e-06, 9.72228644968709e-06, 9.72228644968709e-06, 9.72228644968709e-06, 9.72228644968709e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.72228644968709e-06

Optimization complete. Final v2v error: 2.700854778289795 mm

Highest mean error: 2.795865297317505 mm for frame 134

Lowest mean error: 2.650498151779175 mm for frame 58

Saving results

Total time: 39.98687434196472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027760
Iteration 2/25 | Loss: 0.01027760
Iteration 3/25 | Loss: 0.01027760
Iteration 4/25 | Loss: 0.01027760
Iteration 5/25 | Loss: 0.01027760
Iteration 6/25 | Loss: 0.01027760
Iteration 7/25 | Loss: 0.01027760
Iteration 8/25 | Loss: 0.01027760
Iteration 9/25 | Loss: 0.01027759
Iteration 10/25 | Loss: 0.01027759
Iteration 11/25 | Loss: 0.01027759
Iteration 12/25 | Loss: 0.01027759
Iteration 13/25 | Loss: 0.01027759
Iteration 14/25 | Loss: 0.01027759
Iteration 15/25 | Loss: 0.01027759
Iteration 16/25 | Loss: 0.01027759
Iteration 17/25 | Loss: 0.01027759
Iteration 18/25 | Loss: 0.01027758
Iteration 19/25 | Loss: 0.01027758
Iteration 20/25 | Loss: 0.01027758
Iteration 21/25 | Loss: 0.01027758
Iteration 22/25 | Loss: 0.01027758
Iteration 23/25 | Loss: 0.01027758
Iteration 24/25 | Loss: 0.01027758
Iteration 25/25 | Loss: 0.01027758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92714286
Iteration 2/25 | Loss: 0.09108005
Iteration 3/25 | Loss: 0.08835240
Iteration 4/25 | Loss: 0.08803080
Iteration 5/25 | Loss: 0.08803079
Iteration 6/25 | Loss: 0.08803079
Iteration 7/25 | Loss: 0.08803077
Iteration 8/25 | Loss: 0.08803077
Iteration 9/25 | Loss: 0.08803077
Iteration 10/25 | Loss: 0.08803077
Iteration 11/25 | Loss: 0.08803077
Iteration 12/25 | Loss: 0.08803077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.08803077042102814, 0.08803077042102814, 0.08803077042102814, 0.08803077042102814, 0.08803077042102814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08803077042102814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08803077
Iteration 2/1000 | Loss: 0.00328813
Iteration 3/1000 | Loss: 0.00237415
Iteration 4/1000 | Loss: 0.00040793
Iteration 5/1000 | Loss: 0.00072311
Iteration 6/1000 | Loss: 0.00074377
Iteration 7/1000 | Loss: 0.00017470
Iteration 8/1000 | Loss: 0.00010388
Iteration 9/1000 | Loss: 0.00012108
Iteration 10/1000 | Loss: 0.00032234
Iteration 11/1000 | Loss: 0.00004239
Iteration 12/1000 | Loss: 0.00020103
Iteration 13/1000 | Loss: 0.00003505
Iteration 14/1000 | Loss: 0.00003161
Iteration 15/1000 | Loss: 0.00002967
Iteration 16/1000 | Loss: 0.00002820
Iteration 17/1000 | Loss: 0.00006353
Iteration 18/1000 | Loss: 0.00002723
Iteration 19/1000 | Loss: 0.00003997
Iteration 20/1000 | Loss: 0.00002522
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00011588
Iteration 23/1000 | Loss: 0.00010477
Iteration 24/1000 | Loss: 0.00002327
Iteration 25/1000 | Loss: 0.00002275
Iteration 26/1000 | Loss: 0.00004204
Iteration 27/1000 | Loss: 0.00002186
Iteration 28/1000 | Loss: 0.00002113
Iteration 29/1000 | Loss: 0.00013209
Iteration 30/1000 | Loss: 0.00006169
Iteration 31/1000 | Loss: 0.00004036
Iteration 32/1000 | Loss: 0.00004068
Iteration 33/1000 | Loss: 0.00005113
Iteration 34/1000 | Loss: 0.00002555
Iteration 35/1000 | Loss: 0.00003780
Iteration 36/1000 | Loss: 0.00001986
Iteration 37/1000 | Loss: 0.00004588
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001895
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001847
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001830
Iteration 48/1000 | Loss: 0.00001830
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001823
Iteration 51/1000 | Loss: 0.00001821
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00001818
Iteration 56/1000 | Loss: 0.00001818
Iteration 57/1000 | Loss: 0.00001817
Iteration 58/1000 | Loss: 0.00001816
Iteration 59/1000 | Loss: 0.00001816
Iteration 60/1000 | Loss: 0.00001816
Iteration 61/1000 | Loss: 0.00001816
Iteration 62/1000 | Loss: 0.00001816
Iteration 63/1000 | Loss: 0.00001816
Iteration 64/1000 | Loss: 0.00001815
Iteration 65/1000 | Loss: 0.00001815
Iteration 66/1000 | Loss: 0.00001813
Iteration 67/1000 | Loss: 0.00001813
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00004660
Iteration 72/1000 | Loss: 0.00004660
Iteration 73/1000 | Loss: 0.00002872
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001811
Iteration 76/1000 | Loss: 0.00002211
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00002202
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001802
Iteration 87/1000 | Loss: 0.00001802
Iteration 88/1000 | Loss: 0.00001802
Iteration 89/1000 | Loss: 0.00001802
Iteration 90/1000 | Loss: 0.00001802
Iteration 91/1000 | Loss: 0.00001802
Iteration 92/1000 | Loss: 0.00001802
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001802
Iteration 96/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.8019592971540987e-05, 1.8019592971540987e-05, 1.8019592971540987e-05, 1.8019592971540987e-05, 1.8019592971540987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8019592971540987e-05

Optimization complete. Final v2v error: 3.6458933353424072 mm

Highest mean error: 4.282896041870117 mm for frame 121

Lowest mean error: 3.256819009780884 mm for frame 105

Saving results

Total time: 88.80190181732178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396185
Iteration 2/25 | Loss: 0.00133801
Iteration 3/25 | Loss: 0.00125940
Iteration 4/25 | Loss: 0.00125000
Iteration 5/25 | Loss: 0.00124680
Iteration 6/25 | Loss: 0.00124599
Iteration 7/25 | Loss: 0.00124589
Iteration 8/25 | Loss: 0.00124589
Iteration 9/25 | Loss: 0.00124589
Iteration 10/25 | Loss: 0.00124589
Iteration 11/25 | Loss: 0.00124589
Iteration 12/25 | Loss: 0.00124589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012458859710022807, 0.0012458859710022807, 0.0012458859710022807, 0.0012458859710022807, 0.0012458859710022807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012458859710022807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54057705
Iteration 2/25 | Loss: 0.00094423
Iteration 3/25 | Loss: 0.00094423
Iteration 4/25 | Loss: 0.00094423
Iteration 5/25 | Loss: 0.00094423
Iteration 6/25 | Loss: 0.00094423
Iteration 7/25 | Loss: 0.00094423
Iteration 8/25 | Loss: 0.00094423
Iteration 9/25 | Loss: 0.00094423
Iteration 10/25 | Loss: 0.00094423
Iteration 11/25 | Loss: 0.00094423
Iteration 12/25 | Loss: 0.00094423
Iteration 13/25 | Loss: 0.00094423
Iteration 14/25 | Loss: 0.00094423
Iteration 15/25 | Loss: 0.00094423
Iteration 16/25 | Loss: 0.00094423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000944229366723448, 0.000944229366723448, 0.000944229366723448, 0.000944229366723448, 0.000944229366723448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000944229366723448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094423
Iteration 2/1000 | Loss: 0.00003953
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001771
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001488
Iteration 10/1000 | Loss: 0.00001469
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001447
Iteration 13/1000 | Loss: 0.00001434
Iteration 14/1000 | Loss: 0.00001433
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001429
Iteration 17/1000 | Loss: 0.00001428
Iteration 18/1000 | Loss: 0.00001427
Iteration 19/1000 | Loss: 0.00001426
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001423
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001422
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001419
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001419
Iteration 30/1000 | Loss: 0.00001417
Iteration 31/1000 | Loss: 0.00001416
Iteration 32/1000 | Loss: 0.00001416
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001414
Iteration 35/1000 | Loss: 0.00001414
Iteration 36/1000 | Loss: 0.00001414
Iteration 37/1000 | Loss: 0.00001414
Iteration 38/1000 | Loss: 0.00001413
Iteration 39/1000 | Loss: 0.00001413
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001413
Iteration 43/1000 | Loss: 0.00001413
Iteration 44/1000 | Loss: 0.00001413
Iteration 45/1000 | Loss: 0.00001412
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001410
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001408
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001407
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001404
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001400
Iteration 72/1000 | Loss: 0.00001400
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001400
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001398
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001398
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001396
Iteration 85/1000 | Loss: 0.00001396
Iteration 86/1000 | Loss: 0.00001396
Iteration 87/1000 | Loss: 0.00001396
Iteration 88/1000 | Loss: 0.00001395
Iteration 89/1000 | Loss: 0.00001395
Iteration 90/1000 | Loss: 0.00001395
Iteration 91/1000 | Loss: 0.00001395
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001393
Iteration 99/1000 | Loss: 0.00001393
Iteration 100/1000 | Loss: 0.00001393
Iteration 101/1000 | Loss: 0.00001393
Iteration 102/1000 | Loss: 0.00001393
Iteration 103/1000 | Loss: 0.00001393
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001392
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001391
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001390
Iteration 131/1000 | Loss: 0.00001390
Iteration 132/1000 | Loss: 0.00001390
Iteration 133/1000 | Loss: 0.00001390
Iteration 134/1000 | Loss: 0.00001390
Iteration 135/1000 | Loss: 0.00001390
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001389
Iteration 138/1000 | Loss: 0.00001389
Iteration 139/1000 | Loss: 0.00001389
Iteration 140/1000 | Loss: 0.00001389
Iteration 141/1000 | Loss: 0.00001389
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001388
Iteration 147/1000 | Loss: 0.00001388
Iteration 148/1000 | Loss: 0.00001388
Iteration 149/1000 | Loss: 0.00001388
Iteration 150/1000 | Loss: 0.00001388
Iteration 151/1000 | Loss: 0.00001388
Iteration 152/1000 | Loss: 0.00001388
Iteration 153/1000 | Loss: 0.00001388
Iteration 154/1000 | Loss: 0.00001388
Iteration 155/1000 | Loss: 0.00001388
Iteration 156/1000 | Loss: 0.00001388
Iteration 157/1000 | Loss: 0.00001388
Iteration 158/1000 | Loss: 0.00001388
Iteration 159/1000 | Loss: 0.00001388
Iteration 160/1000 | Loss: 0.00001388
Iteration 161/1000 | Loss: 0.00001388
Iteration 162/1000 | Loss: 0.00001387
Iteration 163/1000 | Loss: 0.00001387
Iteration 164/1000 | Loss: 0.00001387
Iteration 165/1000 | Loss: 0.00001387
Iteration 166/1000 | Loss: 0.00001387
Iteration 167/1000 | Loss: 0.00001387
Iteration 168/1000 | Loss: 0.00001387
Iteration 169/1000 | Loss: 0.00001387
Iteration 170/1000 | Loss: 0.00001387
Iteration 171/1000 | Loss: 0.00001387
Iteration 172/1000 | Loss: 0.00001387
Iteration 173/1000 | Loss: 0.00001387
Iteration 174/1000 | Loss: 0.00001387
Iteration 175/1000 | Loss: 0.00001387
Iteration 176/1000 | Loss: 0.00001387
Iteration 177/1000 | Loss: 0.00001386
Iteration 178/1000 | Loss: 0.00001386
Iteration 179/1000 | Loss: 0.00001386
Iteration 180/1000 | Loss: 0.00001386
Iteration 181/1000 | Loss: 0.00001386
Iteration 182/1000 | Loss: 0.00001386
Iteration 183/1000 | Loss: 0.00001386
Iteration 184/1000 | Loss: 0.00001386
Iteration 185/1000 | Loss: 0.00001386
Iteration 186/1000 | Loss: 0.00001386
Iteration 187/1000 | Loss: 0.00001386
Iteration 188/1000 | Loss: 0.00001386
Iteration 189/1000 | Loss: 0.00001386
Iteration 190/1000 | Loss: 0.00001386
Iteration 191/1000 | Loss: 0.00001386
Iteration 192/1000 | Loss: 0.00001386
Iteration 193/1000 | Loss: 0.00001386
Iteration 194/1000 | Loss: 0.00001386
Iteration 195/1000 | Loss: 0.00001386
Iteration 196/1000 | Loss: 0.00001386
Iteration 197/1000 | Loss: 0.00001386
Iteration 198/1000 | Loss: 0.00001385
Iteration 199/1000 | Loss: 0.00001385
Iteration 200/1000 | Loss: 0.00001385
Iteration 201/1000 | Loss: 0.00001385
Iteration 202/1000 | Loss: 0.00001385
Iteration 203/1000 | Loss: 0.00001385
Iteration 204/1000 | Loss: 0.00001385
Iteration 205/1000 | Loss: 0.00001385
Iteration 206/1000 | Loss: 0.00001385
Iteration 207/1000 | Loss: 0.00001385
Iteration 208/1000 | Loss: 0.00001385
Iteration 209/1000 | Loss: 0.00001385
Iteration 210/1000 | Loss: 0.00001385
Iteration 211/1000 | Loss: 0.00001384
Iteration 212/1000 | Loss: 0.00001384
Iteration 213/1000 | Loss: 0.00001384
Iteration 214/1000 | Loss: 0.00001384
Iteration 215/1000 | Loss: 0.00001384
Iteration 216/1000 | Loss: 0.00001384
Iteration 217/1000 | Loss: 0.00001384
Iteration 218/1000 | Loss: 0.00001384
Iteration 219/1000 | Loss: 0.00001384
Iteration 220/1000 | Loss: 0.00001384
Iteration 221/1000 | Loss: 0.00001384
Iteration 222/1000 | Loss: 0.00001384
Iteration 223/1000 | Loss: 0.00001384
Iteration 224/1000 | Loss: 0.00001383
Iteration 225/1000 | Loss: 0.00001383
Iteration 226/1000 | Loss: 0.00001383
Iteration 227/1000 | Loss: 0.00001383
Iteration 228/1000 | Loss: 0.00001383
Iteration 229/1000 | Loss: 0.00001383
Iteration 230/1000 | Loss: 0.00001383
Iteration 231/1000 | Loss: 0.00001383
Iteration 232/1000 | Loss: 0.00001383
Iteration 233/1000 | Loss: 0.00001383
Iteration 234/1000 | Loss: 0.00001383
Iteration 235/1000 | Loss: 0.00001383
Iteration 236/1000 | Loss: 0.00001383
Iteration 237/1000 | Loss: 0.00001383
Iteration 238/1000 | Loss: 0.00001383
Iteration 239/1000 | Loss: 0.00001383
Iteration 240/1000 | Loss: 0.00001383
Iteration 241/1000 | Loss: 0.00001383
Iteration 242/1000 | Loss: 0.00001383
Iteration 243/1000 | Loss: 0.00001383
Iteration 244/1000 | Loss: 0.00001383
Iteration 245/1000 | Loss: 0.00001383
Iteration 246/1000 | Loss: 0.00001383
Iteration 247/1000 | Loss: 0.00001383
Iteration 248/1000 | Loss: 0.00001383
Iteration 249/1000 | Loss: 0.00001383
Iteration 250/1000 | Loss: 0.00001383
Iteration 251/1000 | Loss: 0.00001383
Iteration 252/1000 | Loss: 0.00001383
Iteration 253/1000 | Loss: 0.00001383
Iteration 254/1000 | Loss: 0.00001383
Iteration 255/1000 | Loss: 0.00001383
Iteration 256/1000 | Loss: 0.00001383
Iteration 257/1000 | Loss: 0.00001383
Iteration 258/1000 | Loss: 0.00001383
Iteration 259/1000 | Loss: 0.00001383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.3833998309564777e-05, 1.3833998309564777e-05, 1.3833998309564777e-05, 1.3833998309564777e-05, 1.3833998309564777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3833998309564777e-05

Optimization complete. Final v2v error: 3.115762710571289 mm

Highest mean error: 4.515049934387207 mm for frame 55

Lowest mean error: 2.7457752227783203 mm for frame 24

Saving results

Total time: 41.49664807319641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381192
Iteration 2/25 | Loss: 0.00137644
Iteration 3/25 | Loss: 0.00124661
Iteration 4/25 | Loss: 0.00121654
Iteration 5/25 | Loss: 0.00120962
Iteration 6/25 | Loss: 0.00120781
Iteration 7/25 | Loss: 0.00120781
Iteration 8/25 | Loss: 0.00120781
Iteration 9/25 | Loss: 0.00120781
Iteration 10/25 | Loss: 0.00120781
Iteration 11/25 | Loss: 0.00120781
Iteration 12/25 | Loss: 0.00120781
Iteration 13/25 | Loss: 0.00120781
Iteration 14/25 | Loss: 0.00120781
Iteration 15/25 | Loss: 0.00120781
Iteration 16/25 | Loss: 0.00120781
Iteration 17/25 | Loss: 0.00120781
Iteration 18/25 | Loss: 0.00120781
Iteration 19/25 | Loss: 0.00120781
Iteration 20/25 | Loss: 0.00120781
Iteration 21/25 | Loss: 0.00120781
Iteration 22/25 | Loss: 0.00120781
Iteration 23/25 | Loss: 0.00120781
Iteration 24/25 | Loss: 0.00120781
Iteration 25/25 | Loss: 0.00120781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41489649
Iteration 2/25 | Loss: 0.00062056
Iteration 3/25 | Loss: 0.00062055
Iteration 4/25 | Loss: 0.00062055
Iteration 5/25 | Loss: 0.00062055
Iteration 6/25 | Loss: 0.00062055
Iteration 7/25 | Loss: 0.00062055
Iteration 8/25 | Loss: 0.00062055
Iteration 9/25 | Loss: 0.00062055
Iteration 10/25 | Loss: 0.00062055
Iteration 11/25 | Loss: 0.00062055
Iteration 12/25 | Loss: 0.00062055
Iteration 13/25 | Loss: 0.00062055
Iteration 14/25 | Loss: 0.00062055
Iteration 15/25 | Loss: 0.00062055
Iteration 16/25 | Loss: 0.00062055
Iteration 17/25 | Loss: 0.00062055
Iteration 18/25 | Loss: 0.00062055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006205461104400456, 0.0006205461104400456, 0.0006205461104400456, 0.0006205461104400456, 0.0006205461104400456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006205461104400456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062055
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002331
Iteration 4/1000 | Loss: 0.00002105
Iteration 5/1000 | Loss: 0.00001996
Iteration 6/1000 | Loss: 0.00001872
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001716
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001707
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001694
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001689
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001688
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001683
Iteration 35/1000 | Loss: 0.00001683
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001682
Iteration 38/1000 | Loss: 0.00001682
Iteration 39/1000 | Loss: 0.00001681
Iteration 40/1000 | Loss: 0.00001676
Iteration 41/1000 | Loss: 0.00001675
Iteration 42/1000 | Loss: 0.00001674
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001670
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001665
Iteration 48/1000 | Loss: 0.00001664
Iteration 49/1000 | Loss: 0.00001664
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001663
Iteration 52/1000 | Loss: 0.00001662
Iteration 53/1000 | Loss: 0.00001662
Iteration 54/1000 | Loss: 0.00001662
Iteration 55/1000 | Loss: 0.00001660
Iteration 56/1000 | Loss: 0.00001660
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001660
Iteration 59/1000 | Loss: 0.00001660
Iteration 60/1000 | Loss: 0.00001660
Iteration 61/1000 | Loss: 0.00001660
Iteration 62/1000 | Loss: 0.00001660
Iteration 63/1000 | Loss: 0.00001659
Iteration 64/1000 | Loss: 0.00001659
Iteration 65/1000 | Loss: 0.00001659
Iteration 66/1000 | Loss: 0.00001659
Iteration 67/1000 | Loss: 0.00001659
Iteration 68/1000 | Loss: 0.00001657
Iteration 69/1000 | Loss: 0.00001656
Iteration 70/1000 | Loss: 0.00001655
Iteration 71/1000 | Loss: 0.00001655
Iteration 72/1000 | Loss: 0.00001654
Iteration 73/1000 | Loss: 0.00001654
Iteration 74/1000 | Loss: 0.00001653
Iteration 75/1000 | Loss: 0.00001653
Iteration 76/1000 | Loss: 0.00001652
Iteration 77/1000 | Loss: 0.00001652
Iteration 78/1000 | Loss: 0.00001651
Iteration 79/1000 | Loss: 0.00001651
Iteration 80/1000 | Loss: 0.00001651
Iteration 81/1000 | Loss: 0.00001651
Iteration 82/1000 | Loss: 0.00001651
Iteration 83/1000 | Loss: 0.00001651
Iteration 84/1000 | Loss: 0.00001651
Iteration 85/1000 | Loss: 0.00001651
Iteration 86/1000 | Loss: 0.00001650
Iteration 87/1000 | Loss: 0.00001650
Iteration 88/1000 | Loss: 0.00001650
Iteration 89/1000 | Loss: 0.00001650
Iteration 90/1000 | Loss: 0.00001649
Iteration 91/1000 | Loss: 0.00001649
Iteration 92/1000 | Loss: 0.00001649
Iteration 93/1000 | Loss: 0.00001648
Iteration 94/1000 | Loss: 0.00001648
Iteration 95/1000 | Loss: 0.00001648
Iteration 96/1000 | Loss: 0.00001647
Iteration 97/1000 | Loss: 0.00001647
Iteration 98/1000 | Loss: 0.00001647
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Iteration 101/1000 | Loss: 0.00001646
Iteration 102/1000 | Loss: 0.00001645
Iteration 103/1000 | Loss: 0.00001645
Iteration 104/1000 | Loss: 0.00001645
Iteration 105/1000 | Loss: 0.00001645
Iteration 106/1000 | Loss: 0.00001645
Iteration 107/1000 | Loss: 0.00001645
Iteration 108/1000 | Loss: 0.00001645
Iteration 109/1000 | Loss: 0.00001645
Iteration 110/1000 | Loss: 0.00001645
Iteration 111/1000 | Loss: 0.00001645
Iteration 112/1000 | Loss: 0.00001645
Iteration 113/1000 | Loss: 0.00001644
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001644
Iteration 116/1000 | Loss: 0.00001644
Iteration 117/1000 | Loss: 0.00001644
Iteration 118/1000 | Loss: 0.00001644
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001643
Iteration 121/1000 | Loss: 0.00001643
Iteration 122/1000 | Loss: 0.00001643
Iteration 123/1000 | Loss: 0.00001642
Iteration 124/1000 | Loss: 0.00001642
Iteration 125/1000 | Loss: 0.00001642
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001641
Iteration 129/1000 | Loss: 0.00001641
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001639
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001638
Iteration 150/1000 | Loss: 0.00001638
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001638
Iteration 159/1000 | Loss: 0.00001638
Iteration 160/1000 | Loss: 0.00001638
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001636
Iteration 169/1000 | Loss: 0.00001636
Iteration 170/1000 | Loss: 0.00001636
Iteration 171/1000 | Loss: 0.00001636
Iteration 172/1000 | Loss: 0.00001636
Iteration 173/1000 | Loss: 0.00001636
Iteration 174/1000 | Loss: 0.00001636
Iteration 175/1000 | Loss: 0.00001636
Iteration 176/1000 | Loss: 0.00001636
Iteration 177/1000 | Loss: 0.00001636
Iteration 178/1000 | Loss: 0.00001635
Iteration 179/1000 | Loss: 0.00001635
Iteration 180/1000 | Loss: 0.00001635
Iteration 181/1000 | Loss: 0.00001635
Iteration 182/1000 | Loss: 0.00001635
Iteration 183/1000 | Loss: 0.00001635
Iteration 184/1000 | Loss: 0.00001635
Iteration 185/1000 | Loss: 0.00001635
Iteration 186/1000 | Loss: 0.00001635
Iteration 187/1000 | Loss: 0.00001635
Iteration 188/1000 | Loss: 0.00001635
Iteration 189/1000 | Loss: 0.00001635
Iteration 190/1000 | Loss: 0.00001635
Iteration 191/1000 | Loss: 0.00001635
Iteration 192/1000 | Loss: 0.00001635
Iteration 193/1000 | Loss: 0.00001635
Iteration 194/1000 | Loss: 0.00001635
Iteration 195/1000 | Loss: 0.00001635
Iteration 196/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.6354604667867534e-05, 1.6354604667867534e-05, 1.6354604667867534e-05, 1.6354604667867534e-05, 1.6354604667867534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6354604667867534e-05

Optimization complete. Final v2v error: 3.441864490509033 mm

Highest mean error: 3.672598361968994 mm for frame 145

Lowest mean error: 2.9284541606903076 mm for frame 2

Saving results

Total time: 48.72928547859192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040502
Iteration 2/25 | Loss: 0.00141930
Iteration 3/25 | Loss: 0.00125873
Iteration 4/25 | Loss: 0.00123029
Iteration 5/25 | Loss: 0.00123569
Iteration 6/25 | Loss: 0.00123201
Iteration 7/25 | Loss: 0.00122917
Iteration 8/25 | Loss: 0.00122629
Iteration 9/25 | Loss: 0.00122446
Iteration 10/25 | Loss: 0.00122829
Iteration 11/25 | Loss: 0.00122804
Iteration 12/25 | Loss: 0.00122574
Iteration 13/25 | Loss: 0.00122156
Iteration 14/25 | Loss: 0.00121945
Iteration 15/25 | Loss: 0.00121881
Iteration 16/25 | Loss: 0.00121861
Iteration 17/25 | Loss: 0.00121858
Iteration 18/25 | Loss: 0.00121857
Iteration 19/25 | Loss: 0.00121857
Iteration 20/25 | Loss: 0.00121857
Iteration 21/25 | Loss: 0.00121857
Iteration 22/25 | Loss: 0.00121857
Iteration 23/25 | Loss: 0.00121857
Iteration 24/25 | Loss: 0.00121856
Iteration 25/25 | Loss: 0.00121856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36719918
Iteration 2/25 | Loss: 0.00069151
Iteration 3/25 | Loss: 0.00069151
Iteration 4/25 | Loss: 0.00069150
Iteration 5/25 | Loss: 0.00069150
Iteration 6/25 | Loss: 0.00069150
Iteration 7/25 | Loss: 0.00069150
Iteration 8/25 | Loss: 0.00069150
Iteration 9/25 | Loss: 0.00069150
Iteration 10/25 | Loss: 0.00069150
Iteration 11/25 | Loss: 0.00069150
Iteration 12/25 | Loss: 0.00069150
Iteration 13/25 | Loss: 0.00069150
Iteration 14/25 | Loss: 0.00069150
Iteration 15/25 | Loss: 0.00069150
Iteration 16/25 | Loss: 0.00069150
Iteration 17/25 | Loss: 0.00069150
Iteration 18/25 | Loss: 0.00069150
Iteration 19/25 | Loss: 0.00069150
Iteration 20/25 | Loss: 0.00069150
Iteration 21/25 | Loss: 0.00069150
Iteration 22/25 | Loss: 0.00069150
Iteration 23/25 | Loss: 0.00069150
Iteration 24/25 | Loss: 0.00069150
Iteration 25/25 | Loss: 0.00069150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006915015401318669, 0.0006915015401318669, 0.0006915015401318669, 0.0006915015401318669, 0.0006915015401318669]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006915015401318669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069150
Iteration 2/1000 | Loss: 0.00003170
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001717
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001267
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001239
Iteration 17/1000 | Loss: 0.00001238
Iteration 18/1000 | Loss: 0.00001233
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001231
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001230
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001228
Iteration 30/1000 | Loss: 0.00001228
Iteration 31/1000 | Loss: 0.00001228
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001227
Iteration 34/1000 | Loss: 0.00001227
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001218
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001217
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001213
Iteration 52/1000 | Loss: 0.00001213
Iteration 53/1000 | Loss: 0.00001213
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001213
Iteration 58/1000 | Loss: 0.00001213
Iteration 59/1000 | Loss: 0.00001213
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001212
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001209
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001209
Iteration 92/1000 | Loss: 0.00001209
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001209
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001208
Iteration 97/1000 | Loss: 0.00001208
Iteration 98/1000 | Loss: 0.00001208
Iteration 99/1000 | Loss: 0.00001207
Iteration 100/1000 | Loss: 0.00001207
Iteration 101/1000 | Loss: 0.00001207
Iteration 102/1000 | Loss: 0.00001207
Iteration 103/1000 | Loss: 0.00001207
Iteration 104/1000 | Loss: 0.00001207
Iteration 105/1000 | Loss: 0.00001207
Iteration 106/1000 | Loss: 0.00001207
Iteration 107/1000 | Loss: 0.00001207
Iteration 108/1000 | Loss: 0.00001207
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001206
Iteration 112/1000 | Loss: 0.00001206
Iteration 113/1000 | Loss: 0.00001206
Iteration 114/1000 | Loss: 0.00001206
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001206
Iteration 120/1000 | Loss: 0.00001206
Iteration 121/1000 | Loss: 0.00001206
Iteration 122/1000 | Loss: 0.00001206
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001205
Iteration 126/1000 | Loss: 0.00001205
Iteration 127/1000 | Loss: 0.00001205
Iteration 128/1000 | Loss: 0.00001205
Iteration 129/1000 | Loss: 0.00001205
Iteration 130/1000 | Loss: 0.00001205
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001205
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001205
Iteration 139/1000 | Loss: 0.00001205
Iteration 140/1000 | Loss: 0.00001205
Iteration 141/1000 | Loss: 0.00001205
Iteration 142/1000 | Loss: 0.00001205
Iteration 143/1000 | Loss: 0.00001205
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001205
Iteration 147/1000 | Loss: 0.00001205
Iteration 148/1000 | Loss: 0.00001205
Iteration 149/1000 | Loss: 0.00001205
Iteration 150/1000 | Loss: 0.00001205
Iteration 151/1000 | Loss: 0.00001205
Iteration 152/1000 | Loss: 0.00001205
Iteration 153/1000 | Loss: 0.00001205
Iteration 154/1000 | Loss: 0.00001205
Iteration 155/1000 | Loss: 0.00001205
Iteration 156/1000 | Loss: 0.00001205
Iteration 157/1000 | Loss: 0.00001205
Iteration 158/1000 | Loss: 0.00001205
Iteration 159/1000 | Loss: 0.00001205
Iteration 160/1000 | Loss: 0.00001205
Iteration 161/1000 | Loss: 0.00001205
Iteration 162/1000 | Loss: 0.00001205
Iteration 163/1000 | Loss: 0.00001205
Iteration 164/1000 | Loss: 0.00001205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.2053330465278123e-05, 1.2053330465278123e-05, 1.2053330465278123e-05, 1.2053330465278123e-05, 1.2053330465278123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2053330465278123e-05

Optimization complete. Final v2v error: 2.9679784774780273 mm

Highest mean error: 3.204571485519409 mm for frame 0

Lowest mean error: 2.805821657180786 mm for frame 205

Saving results

Total time: 67.17071890830994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394469
Iteration 2/25 | Loss: 0.00133418
Iteration 3/25 | Loss: 0.00124493
Iteration 4/25 | Loss: 0.00122319
Iteration 5/25 | Loss: 0.00121477
Iteration 6/25 | Loss: 0.00121366
Iteration 7/25 | Loss: 0.00121366
Iteration 8/25 | Loss: 0.00121366
Iteration 9/25 | Loss: 0.00121366
Iteration 10/25 | Loss: 0.00121366
Iteration 11/25 | Loss: 0.00121366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001213656971231103, 0.001213656971231103, 0.001213656971231103, 0.001213656971231103, 0.001213656971231103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001213656971231103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42607164
Iteration 2/25 | Loss: 0.00070604
Iteration 3/25 | Loss: 0.00070604
Iteration 4/25 | Loss: 0.00070604
Iteration 5/25 | Loss: 0.00070604
Iteration 6/25 | Loss: 0.00070604
Iteration 7/25 | Loss: 0.00070604
Iteration 8/25 | Loss: 0.00070604
Iteration 9/25 | Loss: 0.00070604
Iteration 10/25 | Loss: 0.00070604
Iteration 11/25 | Loss: 0.00070604
Iteration 12/25 | Loss: 0.00070604
Iteration 13/25 | Loss: 0.00070604
Iteration 14/25 | Loss: 0.00070604
Iteration 15/25 | Loss: 0.00070604
Iteration 16/25 | Loss: 0.00070604
Iteration 17/25 | Loss: 0.00070604
Iteration 18/25 | Loss: 0.00070604
Iteration 19/25 | Loss: 0.00070604
Iteration 20/25 | Loss: 0.00070604
Iteration 21/25 | Loss: 0.00070604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007060357602313161, 0.0007060357602313161, 0.0007060357602313161, 0.0007060357602313161, 0.0007060357602313161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007060357602313161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070604
Iteration 2/1000 | Loss: 0.00004666
Iteration 3/1000 | Loss: 0.00003193
Iteration 4/1000 | Loss: 0.00002808
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002482
Iteration 7/1000 | Loss: 0.00002398
Iteration 8/1000 | Loss: 0.00002352
Iteration 9/1000 | Loss: 0.00002313
Iteration 10/1000 | Loss: 0.00002276
Iteration 11/1000 | Loss: 0.00002251
Iteration 12/1000 | Loss: 0.00002240
Iteration 13/1000 | Loss: 0.00002222
Iteration 14/1000 | Loss: 0.00002216
Iteration 15/1000 | Loss: 0.00002216
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002210
Iteration 18/1000 | Loss: 0.00002209
Iteration 19/1000 | Loss: 0.00002209
Iteration 20/1000 | Loss: 0.00002208
Iteration 21/1000 | Loss: 0.00002207
Iteration 22/1000 | Loss: 0.00002206
Iteration 23/1000 | Loss: 0.00002206
Iteration 24/1000 | Loss: 0.00002206
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002204
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002203
Iteration 32/1000 | Loss: 0.00002202
Iteration 33/1000 | Loss: 0.00002202
Iteration 34/1000 | Loss: 0.00002201
Iteration 35/1000 | Loss: 0.00002201
Iteration 36/1000 | Loss: 0.00002201
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002198
Iteration 42/1000 | Loss: 0.00002198
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002196
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002191
Iteration 56/1000 | Loss: 0.00002191
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002190
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002189
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002188
Iteration 68/1000 | Loss: 0.00002188
Iteration 69/1000 | Loss: 0.00002188
Iteration 70/1000 | Loss: 0.00002187
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002186
Iteration 74/1000 | Loss: 0.00002186
Iteration 75/1000 | Loss: 0.00002186
Iteration 76/1000 | Loss: 0.00002186
Iteration 77/1000 | Loss: 0.00002186
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002185
Iteration 80/1000 | Loss: 0.00002185
Iteration 81/1000 | Loss: 0.00002185
Iteration 82/1000 | Loss: 0.00002185
Iteration 83/1000 | Loss: 0.00002184
Iteration 84/1000 | Loss: 0.00002184
Iteration 85/1000 | Loss: 0.00002184
Iteration 86/1000 | Loss: 0.00002184
Iteration 87/1000 | Loss: 0.00002183
Iteration 88/1000 | Loss: 0.00002183
Iteration 89/1000 | Loss: 0.00002183
Iteration 90/1000 | Loss: 0.00002183
Iteration 91/1000 | Loss: 0.00002182
Iteration 92/1000 | Loss: 0.00002182
Iteration 93/1000 | Loss: 0.00002182
Iteration 94/1000 | Loss: 0.00002181
Iteration 95/1000 | Loss: 0.00002181
Iteration 96/1000 | Loss: 0.00002181
Iteration 97/1000 | Loss: 0.00002181
Iteration 98/1000 | Loss: 0.00002180
Iteration 99/1000 | Loss: 0.00002180
Iteration 100/1000 | Loss: 0.00002180
Iteration 101/1000 | Loss: 0.00002180
Iteration 102/1000 | Loss: 0.00002180
Iteration 103/1000 | Loss: 0.00002180
Iteration 104/1000 | Loss: 0.00002180
Iteration 105/1000 | Loss: 0.00002180
Iteration 106/1000 | Loss: 0.00002180
Iteration 107/1000 | Loss: 0.00002179
Iteration 108/1000 | Loss: 0.00002179
Iteration 109/1000 | Loss: 0.00002179
Iteration 110/1000 | Loss: 0.00002179
Iteration 111/1000 | Loss: 0.00002179
Iteration 112/1000 | Loss: 0.00002179
Iteration 113/1000 | Loss: 0.00002179
Iteration 114/1000 | Loss: 0.00002179
Iteration 115/1000 | Loss: 0.00002178
Iteration 116/1000 | Loss: 0.00002178
Iteration 117/1000 | Loss: 0.00002178
Iteration 118/1000 | Loss: 0.00002178
Iteration 119/1000 | Loss: 0.00002177
Iteration 120/1000 | Loss: 0.00002177
Iteration 121/1000 | Loss: 0.00002177
Iteration 122/1000 | Loss: 0.00002176
Iteration 123/1000 | Loss: 0.00002176
Iteration 124/1000 | Loss: 0.00002176
Iteration 125/1000 | Loss: 0.00002176
Iteration 126/1000 | Loss: 0.00002176
Iteration 127/1000 | Loss: 0.00002176
Iteration 128/1000 | Loss: 0.00002175
Iteration 129/1000 | Loss: 0.00002175
Iteration 130/1000 | Loss: 0.00002175
Iteration 131/1000 | Loss: 0.00002175
Iteration 132/1000 | Loss: 0.00002175
Iteration 133/1000 | Loss: 0.00002175
Iteration 134/1000 | Loss: 0.00002175
Iteration 135/1000 | Loss: 0.00002175
Iteration 136/1000 | Loss: 0.00002175
Iteration 137/1000 | Loss: 0.00002175
Iteration 138/1000 | Loss: 0.00002175
Iteration 139/1000 | Loss: 0.00002175
Iteration 140/1000 | Loss: 0.00002175
Iteration 141/1000 | Loss: 0.00002175
Iteration 142/1000 | Loss: 0.00002175
Iteration 143/1000 | Loss: 0.00002175
Iteration 144/1000 | Loss: 0.00002175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [2.1747393475379795e-05, 2.1747393475379795e-05, 2.1747393475379795e-05, 2.1747393475379795e-05, 2.1747393475379795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1747393475379795e-05

Optimization complete. Final v2v error: 3.817115545272827 mm

Highest mean error: 4.796574592590332 mm for frame 103

Lowest mean error: 2.8589749336242676 mm for frame 0

Saving results

Total time: 42.53407168388367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391480
Iteration 2/25 | Loss: 0.00129625
Iteration 3/25 | Loss: 0.00123595
Iteration 4/25 | Loss: 0.00122730
Iteration 5/25 | Loss: 0.00122440
Iteration 6/25 | Loss: 0.00122440
Iteration 7/25 | Loss: 0.00122440
Iteration 8/25 | Loss: 0.00122440
Iteration 9/25 | Loss: 0.00122440
Iteration 10/25 | Loss: 0.00122440
Iteration 11/25 | Loss: 0.00122440
Iteration 12/25 | Loss: 0.00122440
Iteration 13/25 | Loss: 0.00122440
Iteration 14/25 | Loss: 0.00122440
Iteration 15/25 | Loss: 0.00122440
Iteration 16/25 | Loss: 0.00122440
Iteration 17/25 | Loss: 0.00122440
Iteration 18/25 | Loss: 0.00122440
Iteration 19/25 | Loss: 0.00122440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012244004756212234, 0.0012244004756212234, 0.0012244004756212234, 0.0012244004756212234, 0.0012244004756212234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012244004756212234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45082545
Iteration 2/25 | Loss: 0.00087117
Iteration 3/25 | Loss: 0.00087117
Iteration 4/25 | Loss: 0.00087117
Iteration 5/25 | Loss: 0.00087117
Iteration 6/25 | Loss: 0.00087117
Iteration 7/25 | Loss: 0.00087117
Iteration 8/25 | Loss: 0.00087117
Iteration 9/25 | Loss: 0.00087117
Iteration 10/25 | Loss: 0.00087117
Iteration 11/25 | Loss: 0.00087117
Iteration 12/25 | Loss: 0.00087117
Iteration 13/25 | Loss: 0.00087117
Iteration 14/25 | Loss: 0.00087117
Iteration 15/25 | Loss: 0.00087117
Iteration 16/25 | Loss: 0.00087117
Iteration 17/25 | Loss: 0.00087117
Iteration 18/25 | Loss: 0.00087117
Iteration 19/25 | Loss: 0.00087117
Iteration 20/25 | Loss: 0.00087117
Iteration 21/25 | Loss: 0.00087117
Iteration 22/25 | Loss: 0.00087117
Iteration 23/25 | Loss: 0.00087117
Iteration 24/25 | Loss: 0.00087117
Iteration 25/25 | Loss: 0.00087117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087117
Iteration 2/1000 | Loss: 0.00002727
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001466
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001366
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001321
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001320
Iteration 13/1000 | Loss: 0.00001319
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001313
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001311
Iteration 20/1000 | Loss: 0.00001307
Iteration 21/1000 | Loss: 0.00001305
Iteration 22/1000 | Loss: 0.00001305
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001301
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001298
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001297
Iteration 30/1000 | Loss: 0.00001297
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001296
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001293
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001293
Iteration 41/1000 | Loss: 0.00001292
Iteration 42/1000 | Loss: 0.00001292
Iteration 43/1000 | Loss: 0.00001292
Iteration 44/1000 | Loss: 0.00001292
Iteration 45/1000 | Loss: 0.00001291
Iteration 46/1000 | Loss: 0.00001291
Iteration 47/1000 | Loss: 0.00001291
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001290
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001289
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001288
Iteration 69/1000 | Loss: 0.00001288
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001285
Iteration 103/1000 | Loss: 0.00001285
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001283
Iteration 115/1000 | Loss: 0.00001283
Iteration 116/1000 | Loss: 0.00001283
Iteration 117/1000 | Loss: 0.00001283
Iteration 118/1000 | Loss: 0.00001283
Iteration 119/1000 | Loss: 0.00001283
Iteration 120/1000 | Loss: 0.00001283
Iteration 121/1000 | Loss: 0.00001283
Iteration 122/1000 | Loss: 0.00001283
Iteration 123/1000 | Loss: 0.00001283
Iteration 124/1000 | Loss: 0.00001283
Iteration 125/1000 | Loss: 0.00001283
Iteration 126/1000 | Loss: 0.00001283
Iteration 127/1000 | Loss: 0.00001283
Iteration 128/1000 | Loss: 0.00001283
Iteration 129/1000 | Loss: 0.00001283
Iteration 130/1000 | Loss: 0.00001283
Iteration 131/1000 | Loss: 0.00001282
Iteration 132/1000 | Loss: 0.00001282
Iteration 133/1000 | Loss: 0.00001281
Iteration 134/1000 | Loss: 0.00001281
Iteration 135/1000 | Loss: 0.00001280
Iteration 136/1000 | Loss: 0.00001280
Iteration 137/1000 | Loss: 0.00001280
Iteration 138/1000 | Loss: 0.00001280
Iteration 139/1000 | Loss: 0.00001280
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001279
Iteration 142/1000 | Loss: 0.00001279
Iteration 143/1000 | Loss: 0.00001279
Iteration 144/1000 | Loss: 0.00001279
Iteration 145/1000 | Loss: 0.00001279
Iteration 146/1000 | Loss: 0.00001278
Iteration 147/1000 | Loss: 0.00001278
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001277
Iteration 154/1000 | Loss: 0.00001277
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001276
Iteration 160/1000 | Loss: 0.00001276
Iteration 161/1000 | Loss: 0.00001276
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001275
Iteration 164/1000 | Loss: 0.00001275
Iteration 165/1000 | Loss: 0.00001275
Iteration 166/1000 | Loss: 0.00001274
Iteration 167/1000 | Loss: 0.00001274
Iteration 168/1000 | Loss: 0.00001274
Iteration 169/1000 | Loss: 0.00001274
Iteration 170/1000 | Loss: 0.00001273
Iteration 171/1000 | Loss: 0.00001273
Iteration 172/1000 | Loss: 0.00001273
Iteration 173/1000 | Loss: 0.00001273
Iteration 174/1000 | Loss: 0.00001273
Iteration 175/1000 | Loss: 0.00001273
Iteration 176/1000 | Loss: 0.00001273
Iteration 177/1000 | Loss: 0.00001273
Iteration 178/1000 | Loss: 0.00001272
Iteration 179/1000 | Loss: 0.00001272
Iteration 180/1000 | Loss: 0.00001272
Iteration 181/1000 | Loss: 0.00001272
Iteration 182/1000 | Loss: 0.00001272
Iteration 183/1000 | Loss: 0.00001272
Iteration 184/1000 | Loss: 0.00001272
Iteration 185/1000 | Loss: 0.00001272
Iteration 186/1000 | Loss: 0.00001272
Iteration 187/1000 | Loss: 0.00001272
Iteration 188/1000 | Loss: 0.00001272
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.2706441339105368e-05, 1.2706441339105368e-05, 1.2706441339105368e-05, 1.2706441339105368e-05, 1.2706441339105368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2706441339105368e-05

Optimization complete. Final v2v error: 3.0308918952941895 mm

Highest mean error: 3.238926410675049 mm for frame 175

Lowest mean error: 2.8599274158477783 mm for frame 98

Saving results

Total time: 36.90347862243652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921899
Iteration 2/25 | Loss: 0.00220813
Iteration 3/25 | Loss: 0.00158933
Iteration 4/25 | Loss: 0.00153045
Iteration 5/25 | Loss: 0.00153719
Iteration 6/25 | Loss: 0.00153025
Iteration 7/25 | Loss: 0.00151479
Iteration 8/25 | Loss: 0.00149253
Iteration 9/25 | Loss: 0.00147904
Iteration 10/25 | Loss: 0.00146591
Iteration 11/25 | Loss: 0.00146121
Iteration 12/25 | Loss: 0.00145427
Iteration 13/25 | Loss: 0.00144711
Iteration 14/25 | Loss: 0.00144566
Iteration 15/25 | Loss: 0.00144488
Iteration 16/25 | Loss: 0.00145503
Iteration 17/25 | Loss: 0.00145130
Iteration 18/25 | Loss: 0.00144917
Iteration 19/25 | Loss: 0.00145012
Iteration 20/25 | Loss: 0.00145181
Iteration 21/25 | Loss: 0.00145126
Iteration 22/25 | Loss: 0.00144599
Iteration 23/25 | Loss: 0.00144435
Iteration 24/25 | Loss: 0.00144289
Iteration 25/25 | Loss: 0.00144239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41523933
Iteration 2/25 | Loss: 0.00264471
Iteration 3/25 | Loss: 0.00264471
Iteration 4/25 | Loss: 0.00264471
Iteration 5/25 | Loss: 0.00264471
Iteration 6/25 | Loss: 0.00264470
Iteration 7/25 | Loss: 0.00264470
Iteration 8/25 | Loss: 0.00264470
Iteration 9/25 | Loss: 0.00264470
Iteration 10/25 | Loss: 0.00264470
Iteration 11/25 | Loss: 0.00264470
Iteration 12/25 | Loss: 0.00264470
Iteration 13/25 | Loss: 0.00264470
Iteration 14/25 | Loss: 0.00264470
Iteration 15/25 | Loss: 0.00264470
Iteration 16/25 | Loss: 0.00264470
Iteration 17/25 | Loss: 0.00264470
Iteration 18/25 | Loss: 0.00264470
Iteration 19/25 | Loss: 0.00264470
Iteration 20/25 | Loss: 0.00264470
Iteration 21/25 | Loss: 0.00264470
Iteration 22/25 | Loss: 0.00264470
Iteration 23/25 | Loss: 0.00264470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00264470255933702, 0.00264470255933702, 0.00264470255933702, 0.00264470255933702, 0.00264470255933702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00264470255933702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264470
Iteration 2/1000 | Loss: 0.00033121
Iteration 3/1000 | Loss: 0.00119660
Iteration 4/1000 | Loss: 0.00319099
Iteration 5/1000 | Loss: 0.00222602
Iteration 6/1000 | Loss: 0.00210649
Iteration 7/1000 | Loss: 0.00156677
Iteration 8/1000 | Loss: 0.00094412
Iteration 9/1000 | Loss: 0.00150476
Iteration 10/1000 | Loss: 0.00070851
Iteration 11/1000 | Loss: 0.00072153
Iteration 12/1000 | Loss: 0.00031398
Iteration 13/1000 | Loss: 0.00010358
Iteration 14/1000 | Loss: 0.00079753
Iteration 15/1000 | Loss: 0.00066380
Iteration 16/1000 | Loss: 0.00064397
Iteration 17/1000 | Loss: 0.00327762
Iteration 18/1000 | Loss: 0.00110478
Iteration 19/1000 | Loss: 0.00007528
Iteration 20/1000 | Loss: 0.00189570
Iteration 21/1000 | Loss: 0.00109763
Iteration 22/1000 | Loss: 0.00026415
Iteration 23/1000 | Loss: 0.00005622
Iteration 24/1000 | Loss: 0.00004687
Iteration 25/1000 | Loss: 0.00004144
Iteration 26/1000 | Loss: 0.00088122
Iteration 27/1000 | Loss: 0.00016390
Iteration 28/1000 | Loss: 0.00044747
Iteration 29/1000 | Loss: 0.00005456
Iteration 30/1000 | Loss: 0.00003802
Iteration 31/1000 | Loss: 0.00048289
Iteration 32/1000 | Loss: 0.00081899
Iteration 33/1000 | Loss: 0.00007352
Iteration 34/1000 | Loss: 0.00003289
Iteration 35/1000 | Loss: 0.00002923
Iteration 36/1000 | Loss: 0.00002720
Iteration 37/1000 | Loss: 0.00002543
Iteration 38/1000 | Loss: 0.00002391
Iteration 39/1000 | Loss: 0.00002308
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002192
Iteration 42/1000 | Loss: 0.00002150
Iteration 43/1000 | Loss: 0.00002119
Iteration 44/1000 | Loss: 0.00002097
Iteration 45/1000 | Loss: 0.00002073
Iteration 46/1000 | Loss: 0.00002053
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002028
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002025
Iteration 52/1000 | Loss: 0.00002024
Iteration 53/1000 | Loss: 0.00002020
Iteration 54/1000 | Loss: 0.00002008
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00001999
Iteration 58/1000 | Loss: 0.00001999
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001997
Iteration 62/1000 | Loss: 0.00001997
Iteration 63/1000 | Loss: 0.00001997
Iteration 64/1000 | Loss: 0.00001996
Iteration 65/1000 | Loss: 0.00001996
Iteration 66/1000 | Loss: 0.00001996
Iteration 67/1000 | Loss: 0.00001995
Iteration 68/1000 | Loss: 0.00001995
Iteration 69/1000 | Loss: 0.00001994
Iteration 70/1000 | Loss: 0.00001994
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001984
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001981
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001977
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001976
Iteration 87/1000 | Loss: 0.00001976
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001971
Iteration 90/1000 | Loss: 0.00001971
Iteration 91/1000 | Loss: 0.00001971
Iteration 92/1000 | Loss: 0.00001969
Iteration 93/1000 | Loss: 0.00001969
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001967
Iteration 96/1000 | Loss: 0.00001967
Iteration 97/1000 | Loss: 0.00001966
Iteration 98/1000 | Loss: 0.00001966
Iteration 99/1000 | Loss: 0.00001966
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00044541
Iteration 104/1000 | Loss: 0.00002114
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001787
Iteration 108/1000 | Loss: 0.00001747
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001693
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001692
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001684
Iteration 137/1000 | Loss: 0.00001684
Iteration 138/1000 | Loss: 0.00001683
Iteration 139/1000 | Loss: 0.00001682
Iteration 140/1000 | Loss: 0.00001682
Iteration 141/1000 | Loss: 0.00001682
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001678
Iteration 152/1000 | Loss: 0.00001678
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001677
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001676
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001674
Iteration 164/1000 | Loss: 0.00001674
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001673
Iteration 171/1000 | Loss: 0.00001673
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001672
Iteration 174/1000 | Loss: 0.00001672
Iteration 175/1000 | Loss: 0.00001672
Iteration 176/1000 | Loss: 0.00001672
Iteration 177/1000 | Loss: 0.00001672
Iteration 178/1000 | Loss: 0.00001672
Iteration 179/1000 | Loss: 0.00001672
Iteration 180/1000 | Loss: 0.00001672
Iteration 181/1000 | Loss: 0.00001672
Iteration 182/1000 | Loss: 0.00001672
Iteration 183/1000 | Loss: 0.00001672
Iteration 184/1000 | Loss: 0.00001672
Iteration 185/1000 | Loss: 0.00001671
Iteration 186/1000 | Loss: 0.00001671
Iteration 187/1000 | Loss: 0.00001671
Iteration 188/1000 | Loss: 0.00001671
Iteration 189/1000 | Loss: 0.00001671
Iteration 190/1000 | Loss: 0.00001671
Iteration 191/1000 | Loss: 0.00001671
Iteration 192/1000 | Loss: 0.00001671
Iteration 193/1000 | Loss: 0.00001670
Iteration 194/1000 | Loss: 0.00001670
Iteration 195/1000 | Loss: 0.00001670
Iteration 196/1000 | Loss: 0.00001670
Iteration 197/1000 | Loss: 0.00001670
Iteration 198/1000 | Loss: 0.00001670
Iteration 199/1000 | Loss: 0.00001669
Iteration 200/1000 | Loss: 0.00001669
Iteration 201/1000 | Loss: 0.00001669
Iteration 202/1000 | Loss: 0.00001669
Iteration 203/1000 | Loss: 0.00001669
Iteration 204/1000 | Loss: 0.00001669
Iteration 205/1000 | Loss: 0.00001669
Iteration 206/1000 | Loss: 0.00001669
Iteration 207/1000 | Loss: 0.00001669
Iteration 208/1000 | Loss: 0.00001669
Iteration 209/1000 | Loss: 0.00001669
Iteration 210/1000 | Loss: 0.00001669
Iteration 211/1000 | Loss: 0.00001668
Iteration 212/1000 | Loss: 0.00001668
Iteration 213/1000 | Loss: 0.00001668
Iteration 214/1000 | Loss: 0.00001668
Iteration 215/1000 | Loss: 0.00001668
Iteration 216/1000 | Loss: 0.00001668
Iteration 217/1000 | Loss: 0.00001668
Iteration 218/1000 | Loss: 0.00001668
Iteration 219/1000 | Loss: 0.00001668
Iteration 220/1000 | Loss: 0.00001668
Iteration 221/1000 | Loss: 0.00001668
Iteration 222/1000 | Loss: 0.00001668
Iteration 223/1000 | Loss: 0.00001668
Iteration 224/1000 | Loss: 0.00001668
Iteration 225/1000 | Loss: 0.00001668
Iteration 226/1000 | Loss: 0.00001668
Iteration 227/1000 | Loss: 0.00001668
Iteration 228/1000 | Loss: 0.00001668
Iteration 229/1000 | Loss: 0.00001668
Iteration 230/1000 | Loss: 0.00001668
Iteration 231/1000 | Loss: 0.00001668
Iteration 232/1000 | Loss: 0.00001667
Iteration 233/1000 | Loss: 0.00001667
Iteration 234/1000 | Loss: 0.00001667
Iteration 235/1000 | Loss: 0.00001667
Iteration 236/1000 | Loss: 0.00001667
Iteration 237/1000 | Loss: 0.00001667
Iteration 238/1000 | Loss: 0.00001667
Iteration 239/1000 | Loss: 0.00001667
Iteration 240/1000 | Loss: 0.00001667
Iteration 241/1000 | Loss: 0.00001667
Iteration 242/1000 | Loss: 0.00001667
Iteration 243/1000 | Loss: 0.00001667
Iteration 244/1000 | Loss: 0.00001667
Iteration 245/1000 | Loss: 0.00001667
Iteration 246/1000 | Loss: 0.00001667
Iteration 247/1000 | Loss: 0.00001667
Iteration 248/1000 | Loss: 0.00001667
Iteration 249/1000 | Loss: 0.00001667
Iteration 250/1000 | Loss: 0.00001666
Iteration 251/1000 | Loss: 0.00001666
Iteration 252/1000 | Loss: 0.00001666
Iteration 253/1000 | Loss: 0.00001666
Iteration 254/1000 | Loss: 0.00001666
Iteration 255/1000 | Loss: 0.00001666
Iteration 256/1000 | Loss: 0.00001666
Iteration 257/1000 | Loss: 0.00001666
Iteration 258/1000 | Loss: 0.00001666
Iteration 259/1000 | Loss: 0.00001666
Iteration 260/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.6663485439494252e-05, 1.6663485439494252e-05, 1.6663485439494252e-05, 1.6663485439494252e-05, 1.6663485439494252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6663485439494252e-05

Optimization complete. Final v2v error: 3.3772246837615967 mm

Highest mean error: 5.079647064208984 mm for frame 63

Lowest mean error: 2.7891271114349365 mm for frame 107

Saving results

Total time: 144.46268105506897
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00394330
Iteration 2/25 | Loss: 0.00129046
Iteration 3/25 | Loss: 0.00123318
Iteration 4/25 | Loss: 0.00122804
Iteration 5/25 | Loss: 0.00122671
Iteration 6/25 | Loss: 0.00122671
Iteration 7/25 | Loss: 0.00122671
Iteration 8/25 | Loss: 0.00122671
Iteration 9/25 | Loss: 0.00122671
Iteration 10/25 | Loss: 0.00122671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001226713415235281, 0.001226713415235281, 0.001226713415235281, 0.001226713415235281, 0.001226713415235281]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001226713415235281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57420254
Iteration 2/25 | Loss: 0.00071128
Iteration 3/25 | Loss: 0.00071128
Iteration 4/25 | Loss: 0.00071128
Iteration 5/25 | Loss: 0.00071128
Iteration 6/25 | Loss: 0.00071128
Iteration 7/25 | Loss: 0.00071128
Iteration 8/25 | Loss: 0.00071128
Iteration 9/25 | Loss: 0.00071128
Iteration 10/25 | Loss: 0.00071128
Iteration 11/25 | Loss: 0.00071128
Iteration 12/25 | Loss: 0.00071128
Iteration 13/25 | Loss: 0.00071128
Iteration 14/25 | Loss: 0.00071128
Iteration 15/25 | Loss: 0.00071128
Iteration 16/25 | Loss: 0.00071128
Iteration 17/25 | Loss: 0.00071128
Iteration 18/25 | Loss: 0.00071128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007112757302820683, 0.0007112757302820683, 0.0007112757302820683, 0.0007112757302820683, 0.0007112757302820683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007112757302820683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071128
Iteration 2/1000 | Loss: 0.00003017
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001512
Iteration 6/1000 | Loss: 0.00001436
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001251
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001243
Iteration 21/1000 | Loss: 0.00001243
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001242
Iteration 25/1000 | Loss: 0.00001242
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001241
Iteration 28/1000 | Loss: 0.00001238
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001236
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001233
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001231
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001224
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001221
Iteration 63/1000 | Loss: 0.00001221
Iteration 64/1000 | Loss: 0.00001221
Iteration 65/1000 | Loss: 0.00001221
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001218
Iteration 70/1000 | Loss: 0.00001217
Iteration 71/1000 | Loss: 0.00001217
Iteration 72/1000 | Loss: 0.00001217
Iteration 73/1000 | Loss: 0.00001217
Iteration 74/1000 | Loss: 0.00001216
Iteration 75/1000 | Loss: 0.00001216
Iteration 76/1000 | Loss: 0.00001216
Iteration 77/1000 | Loss: 0.00001216
Iteration 78/1000 | Loss: 0.00001216
Iteration 79/1000 | Loss: 0.00001216
Iteration 80/1000 | Loss: 0.00001215
Iteration 81/1000 | Loss: 0.00001215
Iteration 82/1000 | Loss: 0.00001214
Iteration 83/1000 | Loss: 0.00001214
Iteration 84/1000 | Loss: 0.00001214
Iteration 85/1000 | Loss: 0.00001213
Iteration 86/1000 | Loss: 0.00001213
Iteration 87/1000 | Loss: 0.00001213
Iteration 88/1000 | Loss: 0.00001213
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001212
Iteration 94/1000 | Loss: 0.00001212
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001211
Iteration 98/1000 | Loss: 0.00001211
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001209
Iteration 112/1000 | Loss: 0.00001208
Iteration 113/1000 | Loss: 0.00001208
Iteration 114/1000 | Loss: 0.00001208
Iteration 115/1000 | Loss: 0.00001208
Iteration 116/1000 | Loss: 0.00001207
Iteration 117/1000 | Loss: 0.00001207
Iteration 118/1000 | Loss: 0.00001207
Iteration 119/1000 | Loss: 0.00001207
Iteration 120/1000 | Loss: 0.00001207
Iteration 121/1000 | Loss: 0.00001207
Iteration 122/1000 | Loss: 0.00001207
Iteration 123/1000 | Loss: 0.00001206
Iteration 124/1000 | Loss: 0.00001206
Iteration 125/1000 | Loss: 0.00001206
Iteration 126/1000 | Loss: 0.00001206
Iteration 127/1000 | Loss: 0.00001206
Iteration 128/1000 | Loss: 0.00001206
Iteration 129/1000 | Loss: 0.00001206
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001205
Iteration 132/1000 | Loss: 0.00001205
Iteration 133/1000 | Loss: 0.00001204
Iteration 134/1000 | Loss: 0.00001204
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001203
Iteration 137/1000 | Loss: 0.00001203
Iteration 138/1000 | Loss: 0.00001203
Iteration 139/1000 | Loss: 0.00001203
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001202
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001199
Iteration 158/1000 | Loss: 0.00001199
Iteration 159/1000 | Loss: 0.00001199
Iteration 160/1000 | Loss: 0.00001199
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001196
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001195
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001194
Iteration 192/1000 | Loss: 0.00001194
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Iteration 201/1000 | Loss: 0.00001193
Iteration 202/1000 | Loss: 0.00001193
Iteration 203/1000 | Loss: 0.00001193
Iteration 204/1000 | Loss: 0.00001193
Iteration 205/1000 | Loss: 0.00001193
Iteration 206/1000 | Loss: 0.00001193
Iteration 207/1000 | Loss: 0.00001193
Iteration 208/1000 | Loss: 0.00001193
Iteration 209/1000 | Loss: 0.00001193
Iteration 210/1000 | Loss: 0.00001193
Iteration 211/1000 | Loss: 0.00001193
Iteration 212/1000 | Loss: 0.00001193
Iteration 213/1000 | Loss: 0.00001193
Iteration 214/1000 | Loss: 0.00001192
Iteration 215/1000 | Loss: 0.00001192
Iteration 216/1000 | Loss: 0.00001192
Iteration 217/1000 | Loss: 0.00001192
Iteration 218/1000 | Loss: 0.00001192
Iteration 219/1000 | Loss: 0.00001192
Iteration 220/1000 | Loss: 0.00001191
Iteration 221/1000 | Loss: 0.00001191
Iteration 222/1000 | Loss: 0.00001191
Iteration 223/1000 | Loss: 0.00001191
Iteration 224/1000 | Loss: 0.00001191
Iteration 225/1000 | Loss: 0.00001191
Iteration 226/1000 | Loss: 0.00001191
Iteration 227/1000 | Loss: 0.00001191
Iteration 228/1000 | Loss: 0.00001191
Iteration 229/1000 | Loss: 0.00001191
Iteration 230/1000 | Loss: 0.00001191
Iteration 231/1000 | Loss: 0.00001191
Iteration 232/1000 | Loss: 0.00001191
Iteration 233/1000 | Loss: 0.00001191
Iteration 234/1000 | Loss: 0.00001191
Iteration 235/1000 | Loss: 0.00001191
Iteration 236/1000 | Loss: 0.00001191
Iteration 237/1000 | Loss: 0.00001191
Iteration 238/1000 | Loss: 0.00001191
Iteration 239/1000 | Loss: 0.00001191
Iteration 240/1000 | Loss: 0.00001191
Iteration 241/1000 | Loss: 0.00001191
Iteration 242/1000 | Loss: 0.00001191
Iteration 243/1000 | Loss: 0.00001191
Iteration 244/1000 | Loss: 0.00001191
Iteration 245/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.1907070074812509e-05, 1.1907070074812509e-05, 1.1907070074812509e-05, 1.1907070074812509e-05, 1.1907070074812509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1907070074812509e-05

Optimization complete. Final v2v error: 2.9492347240448 mm

Highest mean error: 3.1119513511657715 mm for frame 82

Lowest mean error: 2.8133349418640137 mm for frame 59

Saving results

Total time: 48.34772562980652
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991891
Iteration 2/25 | Loss: 0.00991891
Iteration 3/25 | Loss: 0.00317485
Iteration 4/25 | Loss: 0.00225021
Iteration 5/25 | Loss: 0.00212157
Iteration 6/25 | Loss: 0.00189745
Iteration 7/25 | Loss: 0.00185006
Iteration 8/25 | Loss: 0.00182639
Iteration 9/25 | Loss: 0.00177682
Iteration 10/25 | Loss: 0.00168035
Iteration 11/25 | Loss: 0.00161348
Iteration 12/25 | Loss: 0.00162131
Iteration 13/25 | Loss: 0.00160232
Iteration 14/25 | Loss: 0.00156073
Iteration 15/25 | Loss: 0.00154650
Iteration 16/25 | Loss: 0.00152003
Iteration 17/25 | Loss: 0.00151368
Iteration 18/25 | Loss: 0.00151572
Iteration 19/25 | Loss: 0.00151201
Iteration 20/25 | Loss: 0.00150927
Iteration 21/25 | Loss: 0.00150391
Iteration 22/25 | Loss: 0.00150577
Iteration 23/25 | Loss: 0.00149645
Iteration 24/25 | Loss: 0.00148492
Iteration 25/25 | Loss: 0.00148358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43292749
Iteration 2/25 | Loss: 0.00388383
Iteration 3/25 | Loss: 0.00388383
Iteration 4/25 | Loss: 0.00328488
Iteration 5/25 | Loss: 0.00308545
Iteration 6/25 | Loss: 0.00294350
Iteration 7/25 | Loss: 0.00294350
Iteration 8/25 | Loss: 0.00294350
Iteration 9/25 | Loss: 0.00294350
Iteration 10/25 | Loss: 0.00294350
Iteration 11/25 | Loss: 0.00294350
Iteration 12/25 | Loss: 0.00294350
Iteration 13/25 | Loss: 0.00294350
Iteration 14/25 | Loss: 0.00294350
Iteration 15/25 | Loss: 0.00294350
Iteration 16/25 | Loss: 0.00294350
Iteration 17/25 | Loss: 0.00294350
Iteration 18/25 | Loss: 0.00294350
Iteration 19/25 | Loss: 0.00294350
Iteration 20/25 | Loss: 0.00294350
Iteration 21/25 | Loss: 0.00294350
Iteration 22/25 | Loss: 0.00294350
Iteration 23/25 | Loss: 0.00294350
Iteration 24/25 | Loss: 0.00294350
Iteration 25/25 | Loss: 0.00294350

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00294350
Iteration 2/1000 | Loss: 0.00094052
Iteration 3/1000 | Loss: 0.00085403
Iteration 4/1000 | Loss: 0.00095424
Iteration 5/1000 | Loss: 0.00049981
Iteration 6/1000 | Loss: 0.00047613
Iteration 7/1000 | Loss: 0.00024181
Iteration 8/1000 | Loss: 0.00018829
Iteration 9/1000 | Loss: 0.00040798
Iteration 10/1000 | Loss: 0.00043813
Iteration 11/1000 | Loss: 0.00029765
Iteration 12/1000 | Loss: 0.00043762
Iteration 13/1000 | Loss: 0.00033131
Iteration 14/1000 | Loss: 0.00016495
Iteration 15/1000 | Loss: 0.00015073
Iteration 16/1000 | Loss: 0.00015172
Iteration 17/1000 | Loss: 0.00014944
Iteration 18/1000 | Loss: 0.00015189
Iteration 19/1000 | Loss: 0.00014766
Iteration 20/1000 | Loss: 0.00041827
Iteration 21/1000 | Loss: 0.00016578
Iteration 22/1000 | Loss: 0.00012904
Iteration 23/1000 | Loss: 0.00072108
Iteration 24/1000 | Loss: 0.00053817
Iteration 25/1000 | Loss: 0.00053824
Iteration 26/1000 | Loss: 0.00017173
Iteration 27/1000 | Loss: 0.00015103
Iteration 28/1000 | Loss: 0.00014100
Iteration 29/1000 | Loss: 0.00060461
Iteration 30/1000 | Loss: 0.00013796
Iteration 31/1000 | Loss: 0.00012670
Iteration 32/1000 | Loss: 0.00012742
Iteration 33/1000 | Loss: 0.00014234
Iteration 34/1000 | Loss: 0.00033964
Iteration 35/1000 | Loss: 0.00014824
Iteration 36/1000 | Loss: 0.00013561
Iteration 37/1000 | Loss: 0.00025694
Iteration 38/1000 | Loss: 0.00026664
Iteration 39/1000 | Loss: 0.00033134
Iteration 40/1000 | Loss: 0.00028678
Iteration 41/1000 | Loss: 0.00012633
Iteration 42/1000 | Loss: 0.00016503
Iteration 43/1000 | Loss: 0.00011495
Iteration 44/1000 | Loss: 0.00012598
Iteration 45/1000 | Loss: 0.00012599
Iteration 46/1000 | Loss: 0.00011303
Iteration 47/1000 | Loss: 0.00011936
Iteration 48/1000 | Loss: 0.00012154
Iteration 49/1000 | Loss: 0.00013420
Iteration 50/1000 | Loss: 0.00016020
Iteration 51/1000 | Loss: 0.00012851
Iteration 52/1000 | Loss: 0.00011856
Iteration 53/1000 | Loss: 0.00011978
Iteration 54/1000 | Loss: 0.00012468
Iteration 55/1000 | Loss: 0.00011849
Iteration 56/1000 | Loss: 0.00011381
Iteration 57/1000 | Loss: 0.00011489
Iteration 58/1000 | Loss: 0.00034261
Iteration 59/1000 | Loss: 0.00324126
Iteration 60/1000 | Loss: 0.00040247
Iteration 61/1000 | Loss: 0.00024251
Iteration 62/1000 | Loss: 0.00012286
Iteration 63/1000 | Loss: 0.00011586
Iteration 64/1000 | Loss: 0.00012953
Iteration 65/1000 | Loss: 0.00010649
Iteration 66/1000 | Loss: 0.00011625
Iteration 67/1000 | Loss: 0.00012855
Iteration 68/1000 | Loss: 0.00014787
Iteration 69/1000 | Loss: 0.00010314
Iteration 70/1000 | Loss: 0.00011065
Iteration 71/1000 | Loss: 0.00010990
Iteration 72/1000 | Loss: 0.00011243
Iteration 73/1000 | Loss: 0.00013292
Iteration 74/1000 | Loss: 0.00014543
Iteration 75/1000 | Loss: 0.00011221
Iteration 76/1000 | Loss: 0.00015856
Iteration 77/1000 | Loss: 0.00011131
Iteration 78/1000 | Loss: 0.00022990
Iteration 79/1000 | Loss: 0.00011262
Iteration 80/1000 | Loss: 0.00011067
Iteration 81/1000 | Loss: 0.00010864
Iteration 82/1000 | Loss: 0.00010743
Iteration 83/1000 | Loss: 0.00011792
Iteration 84/1000 | Loss: 0.00047247
Iteration 85/1000 | Loss: 0.00025043
Iteration 86/1000 | Loss: 0.00025966
Iteration 87/1000 | Loss: 0.00023427
Iteration 88/1000 | Loss: 0.00010497
Iteration 89/1000 | Loss: 0.00010332
Iteration 90/1000 | Loss: 0.00009691
Iteration 91/1000 | Loss: 0.00009875
Iteration 92/1000 | Loss: 0.00010663
Iteration 93/1000 | Loss: 0.00010823
Iteration 94/1000 | Loss: 0.00010742
Iteration 95/1000 | Loss: 0.00013992
Iteration 96/1000 | Loss: 0.00011176
Iteration 97/1000 | Loss: 0.00009438
Iteration 98/1000 | Loss: 0.00012387
Iteration 99/1000 | Loss: 0.00011234
Iteration 100/1000 | Loss: 0.00011816
Iteration 101/1000 | Loss: 0.00061982
Iteration 102/1000 | Loss: 0.00050428
Iteration 103/1000 | Loss: 0.00013494
Iteration 104/1000 | Loss: 0.00009387
Iteration 105/1000 | Loss: 0.00009146
Iteration 106/1000 | Loss: 0.00009973
Iteration 107/1000 | Loss: 0.00009449
Iteration 108/1000 | Loss: 0.00017111
Iteration 109/1000 | Loss: 0.00034469
Iteration 110/1000 | Loss: 0.00013450
Iteration 111/1000 | Loss: 0.00013913
Iteration 112/1000 | Loss: 0.00013154
Iteration 113/1000 | Loss: 0.00009954
Iteration 114/1000 | Loss: 0.00008711
Iteration 115/1000 | Loss: 0.00012173
Iteration 116/1000 | Loss: 0.00008695
Iteration 117/1000 | Loss: 0.00012456
Iteration 118/1000 | Loss: 0.00008504
Iteration 119/1000 | Loss: 0.00016601
Iteration 120/1000 | Loss: 0.00009753
Iteration 121/1000 | Loss: 0.00008711
Iteration 122/1000 | Loss: 0.00021809
Iteration 123/1000 | Loss: 0.00017501
Iteration 124/1000 | Loss: 0.00010501
Iteration 125/1000 | Loss: 0.00011194
Iteration 126/1000 | Loss: 0.00024251
Iteration 127/1000 | Loss: 0.00009159
Iteration 128/1000 | Loss: 0.00018704
Iteration 129/1000 | Loss: 0.00008530
Iteration 130/1000 | Loss: 0.00008743
Iteration 131/1000 | Loss: 0.00008659
Iteration 132/1000 | Loss: 0.00008326
Iteration 133/1000 | Loss: 0.00008221
Iteration 134/1000 | Loss: 0.00008279
Iteration 135/1000 | Loss: 0.00009096
Iteration 136/1000 | Loss: 0.00008264
Iteration 137/1000 | Loss: 0.00008200
Iteration 138/1000 | Loss: 0.00020350
Iteration 139/1000 | Loss: 0.00009959
Iteration 140/1000 | Loss: 0.00022842
Iteration 141/1000 | Loss: 0.00012241
Iteration 142/1000 | Loss: 0.00022101
Iteration 143/1000 | Loss: 0.00044448
Iteration 144/1000 | Loss: 0.00028555
Iteration 145/1000 | Loss: 0.00028268
Iteration 146/1000 | Loss: 0.00027034
Iteration 147/1000 | Loss: 0.00051472
Iteration 148/1000 | Loss: 0.00011117
Iteration 149/1000 | Loss: 0.00008789
Iteration 150/1000 | Loss: 0.00020571
Iteration 151/1000 | Loss: 0.00035427
Iteration 152/1000 | Loss: 0.00036015
Iteration 153/1000 | Loss: 0.00113234
Iteration 154/1000 | Loss: 0.00326109
Iteration 155/1000 | Loss: 0.00114346
Iteration 156/1000 | Loss: 0.00029526
Iteration 157/1000 | Loss: 0.00029545
Iteration 158/1000 | Loss: 0.00029709
Iteration 159/1000 | Loss: 0.00024143
Iteration 160/1000 | Loss: 0.00028316
Iteration 161/1000 | Loss: 0.00008409
Iteration 162/1000 | Loss: 0.00008407
Iteration 163/1000 | Loss: 0.00028894
Iteration 164/1000 | Loss: 0.00008467
Iteration 165/1000 | Loss: 0.00010667
Iteration 166/1000 | Loss: 0.00010884
Iteration 167/1000 | Loss: 0.00029611
Iteration 168/1000 | Loss: 0.00009605
Iteration 169/1000 | Loss: 0.00008747
Iteration 170/1000 | Loss: 0.00008733
Iteration 171/1000 | Loss: 0.00008749
Iteration 172/1000 | Loss: 0.00009340
Iteration 173/1000 | Loss: 0.00008243
Iteration 174/1000 | Loss: 0.00018110
Iteration 175/1000 | Loss: 0.00008726
Iteration 176/1000 | Loss: 0.00008098
Iteration 177/1000 | Loss: 0.00008749
Iteration 178/1000 | Loss: 0.00029650
Iteration 179/1000 | Loss: 0.00035486
Iteration 180/1000 | Loss: 0.00009592
Iteration 181/1000 | Loss: 0.00012175
Iteration 182/1000 | Loss: 0.00008171
Iteration 183/1000 | Loss: 0.00023141
Iteration 184/1000 | Loss: 0.00013837
Iteration 185/1000 | Loss: 0.00013594
Iteration 186/1000 | Loss: 0.00008235
Iteration 187/1000 | Loss: 0.00008172
Iteration 188/1000 | Loss: 0.00008010
Iteration 189/1000 | Loss: 0.00007973
Iteration 190/1000 | Loss: 0.00008349
Iteration 191/1000 | Loss: 0.00030828
Iteration 192/1000 | Loss: 0.00021016
Iteration 193/1000 | Loss: 0.00013945
Iteration 194/1000 | Loss: 0.00010326
Iteration 195/1000 | Loss: 0.00024718
Iteration 196/1000 | Loss: 0.00021433
Iteration 197/1000 | Loss: 0.00011747
Iteration 198/1000 | Loss: 0.00007953
Iteration 199/1000 | Loss: 0.00031622
Iteration 200/1000 | Loss: 0.00009612
Iteration 201/1000 | Loss: 0.00021006
Iteration 202/1000 | Loss: 0.00010489
Iteration 203/1000 | Loss: 0.00018201
Iteration 204/1000 | Loss: 0.00007861
Iteration 205/1000 | Loss: 0.00007701
Iteration 206/1000 | Loss: 0.00009607
Iteration 207/1000 | Loss: 0.00007604
Iteration 208/1000 | Loss: 0.00007575
Iteration 209/1000 | Loss: 0.00007554
Iteration 210/1000 | Loss: 0.00007545
Iteration 211/1000 | Loss: 0.00007545
Iteration 212/1000 | Loss: 0.00007545
Iteration 213/1000 | Loss: 0.00007544
Iteration 214/1000 | Loss: 0.00007544
Iteration 215/1000 | Loss: 0.00007543
Iteration 216/1000 | Loss: 0.00007543
Iteration 217/1000 | Loss: 0.00007543
Iteration 218/1000 | Loss: 0.00007543
Iteration 219/1000 | Loss: 0.00007543
Iteration 220/1000 | Loss: 0.00007543
Iteration 221/1000 | Loss: 0.00007542
Iteration 222/1000 | Loss: 0.00007541
Iteration 223/1000 | Loss: 0.00007541
Iteration 224/1000 | Loss: 0.00007541
Iteration 225/1000 | Loss: 0.00007540
Iteration 226/1000 | Loss: 0.00007539
Iteration 227/1000 | Loss: 0.00008437
Iteration 228/1000 | Loss: 0.00007533
Iteration 229/1000 | Loss: 0.00007532
Iteration 230/1000 | Loss: 0.00007530
Iteration 231/1000 | Loss: 0.00007530
Iteration 232/1000 | Loss: 0.00007530
Iteration 233/1000 | Loss: 0.00007530
Iteration 234/1000 | Loss: 0.00007529
Iteration 235/1000 | Loss: 0.00007529
Iteration 236/1000 | Loss: 0.00007528
Iteration 237/1000 | Loss: 0.00007528
Iteration 238/1000 | Loss: 0.00007527
Iteration 239/1000 | Loss: 0.00007527
Iteration 240/1000 | Loss: 0.00007527
Iteration 241/1000 | Loss: 0.00007526
Iteration 242/1000 | Loss: 0.00007526
Iteration 243/1000 | Loss: 0.00007525
Iteration 244/1000 | Loss: 0.00007525
Iteration 245/1000 | Loss: 0.00008174
Iteration 246/1000 | Loss: 0.00007526
Iteration 247/1000 | Loss: 0.00008805
Iteration 248/1000 | Loss: 0.00007526
Iteration 249/1000 | Loss: 0.00007522
Iteration 250/1000 | Loss: 0.00007522
Iteration 251/1000 | Loss: 0.00007521
Iteration 252/1000 | Loss: 0.00007521
Iteration 253/1000 | Loss: 0.00007521
Iteration 254/1000 | Loss: 0.00007520
Iteration 255/1000 | Loss: 0.00007520
Iteration 256/1000 | Loss: 0.00007520
Iteration 257/1000 | Loss: 0.00008147
Iteration 258/1000 | Loss: 0.00007521
Iteration 259/1000 | Loss: 0.00007518
Iteration 260/1000 | Loss: 0.00007518
Iteration 261/1000 | Loss: 0.00007518
Iteration 262/1000 | Loss: 0.00007518
Iteration 263/1000 | Loss: 0.00007518
Iteration 264/1000 | Loss: 0.00007518
Iteration 265/1000 | Loss: 0.00007518
Iteration 266/1000 | Loss: 0.00007518
Iteration 267/1000 | Loss: 0.00007518
Iteration 268/1000 | Loss: 0.00007518
Iteration 269/1000 | Loss: 0.00007517
Iteration 270/1000 | Loss: 0.00007517
Iteration 271/1000 | Loss: 0.00007517
Iteration 272/1000 | Loss: 0.00007517
Iteration 273/1000 | Loss: 0.00007517
Iteration 274/1000 | Loss: 0.00007517
Iteration 275/1000 | Loss: 0.00007517
Iteration 276/1000 | Loss: 0.00007517
Iteration 277/1000 | Loss: 0.00007517
Iteration 278/1000 | Loss: 0.00007517
Iteration 279/1000 | Loss: 0.00007517
Iteration 280/1000 | Loss: 0.00007516
Iteration 281/1000 | Loss: 0.00007516
Iteration 282/1000 | Loss: 0.00007516
Iteration 283/1000 | Loss: 0.00007516
Iteration 284/1000 | Loss: 0.00007516
Iteration 285/1000 | Loss: 0.00007516
Iteration 286/1000 | Loss: 0.00007516
Iteration 287/1000 | Loss: 0.00007516
Iteration 288/1000 | Loss: 0.00007516
Iteration 289/1000 | Loss: 0.00007516
Iteration 290/1000 | Loss: 0.00007516
Iteration 291/1000 | Loss: 0.00007516
Iteration 292/1000 | Loss: 0.00007516
Iteration 293/1000 | Loss: 0.00007516
Iteration 294/1000 | Loss: 0.00007516
Iteration 295/1000 | Loss: 0.00007516
Iteration 296/1000 | Loss: 0.00007516
Iteration 297/1000 | Loss: 0.00007516
Iteration 298/1000 | Loss: 0.00007516
Iteration 299/1000 | Loss: 0.00007516
Iteration 300/1000 | Loss: 0.00007516
Iteration 301/1000 | Loss: 0.00007516
Iteration 302/1000 | Loss: 0.00007516
Iteration 303/1000 | Loss: 0.00007516
Iteration 304/1000 | Loss: 0.00007516
Iteration 305/1000 | Loss: 0.00007516
Iteration 306/1000 | Loss: 0.00007516
Iteration 307/1000 | Loss: 0.00007516
Iteration 308/1000 | Loss: 0.00007516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [7.516416371800005e-05, 7.516416371800005e-05, 7.516416371800005e-05, 7.516416371800005e-05, 7.516416371800005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.516416371800005e-05

Optimization complete. Final v2v error: 4.621164798736572 mm

Highest mean error: 11.690101623535156 mm for frame 84

Lowest mean error: 2.8491296768188477 mm for frame 168

Saving results

Total time: 406.2641053199768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00732737
Iteration 2/25 | Loss: 0.00180244
Iteration 3/25 | Loss: 0.00143206
Iteration 4/25 | Loss: 0.00140277
Iteration 5/25 | Loss: 0.00139994
Iteration 6/25 | Loss: 0.00142506
Iteration 7/25 | Loss: 0.00139403
Iteration 8/25 | Loss: 0.00141461
Iteration 9/25 | Loss: 0.00138745
Iteration 10/25 | Loss: 0.00138654
Iteration 11/25 | Loss: 0.00137993
Iteration 12/25 | Loss: 0.00137669
Iteration 13/25 | Loss: 0.00137341
Iteration 14/25 | Loss: 0.00137284
Iteration 15/25 | Loss: 0.00137276
Iteration 16/25 | Loss: 0.00137276
Iteration 17/25 | Loss: 0.00137276
Iteration 18/25 | Loss: 0.00137276
Iteration 19/25 | Loss: 0.00137276
Iteration 20/25 | Loss: 0.00137276
Iteration 21/25 | Loss: 0.00137276
Iteration 22/25 | Loss: 0.00137276
Iteration 23/25 | Loss: 0.00137276
Iteration 24/25 | Loss: 0.00137275
Iteration 25/25 | Loss: 0.00137275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72823083
Iteration 2/25 | Loss: 0.00095715
Iteration 3/25 | Loss: 0.00095715
Iteration 4/25 | Loss: 0.00095715
Iteration 5/25 | Loss: 0.00095715
Iteration 6/25 | Loss: 0.00095715
Iteration 7/25 | Loss: 0.00095715
Iteration 8/25 | Loss: 0.00095715
Iteration 9/25 | Loss: 0.00095715
Iteration 10/25 | Loss: 0.00095715
Iteration 11/25 | Loss: 0.00095715
Iteration 12/25 | Loss: 0.00095715
Iteration 13/25 | Loss: 0.00095715
Iteration 14/25 | Loss: 0.00095715
Iteration 15/25 | Loss: 0.00095715
Iteration 16/25 | Loss: 0.00095715
Iteration 17/25 | Loss: 0.00095715
Iteration 18/25 | Loss: 0.00095715
Iteration 19/25 | Loss: 0.00095715
Iteration 20/25 | Loss: 0.00095715
Iteration 21/25 | Loss: 0.00095715
Iteration 22/25 | Loss: 0.00095715
Iteration 23/25 | Loss: 0.00095715
Iteration 24/25 | Loss: 0.00095715
Iteration 25/25 | Loss: 0.00095715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095715
Iteration 2/1000 | Loss: 0.00005450
Iteration 3/1000 | Loss: 0.00003637
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00003037
Iteration 6/1000 | Loss: 0.00002891
Iteration 7/1000 | Loss: 0.00002788
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002674
Iteration 10/1000 | Loss: 0.00002636
Iteration 11/1000 | Loss: 0.00002603
Iteration 12/1000 | Loss: 0.00002579
Iteration 13/1000 | Loss: 0.00002556
Iteration 14/1000 | Loss: 0.00002554
Iteration 15/1000 | Loss: 0.00002546
Iteration 16/1000 | Loss: 0.00002544
Iteration 17/1000 | Loss: 0.00002543
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00002543
Iteration 20/1000 | Loss: 0.00002542
Iteration 21/1000 | Loss: 0.00002542
Iteration 22/1000 | Loss: 0.00002541
Iteration 23/1000 | Loss: 0.00002540
Iteration 24/1000 | Loss: 0.00002540
Iteration 25/1000 | Loss: 0.00002539
Iteration 26/1000 | Loss: 0.00002539
Iteration 27/1000 | Loss: 0.00002538
Iteration 28/1000 | Loss: 0.00002538
Iteration 29/1000 | Loss: 0.00002538
Iteration 30/1000 | Loss: 0.00002537
Iteration 31/1000 | Loss: 0.00002537
Iteration 32/1000 | Loss: 0.00002536
Iteration 33/1000 | Loss: 0.00002535
Iteration 34/1000 | Loss: 0.00002535
Iteration 35/1000 | Loss: 0.00002534
Iteration 36/1000 | Loss: 0.00002533
Iteration 37/1000 | Loss: 0.00002533
Iteration 38/1000 | Loss: 0.00002532
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002530
Iteration 41/1000 | Loss: 0.00002530
Iteration 42/1000 | Loss: 0.00002529
Iteration 43/1000 | Loss: 0.00002526
Iteration 44/1000 | Loss: 0.00002523
Iteration 45/1000 | Loss: 0.00002522
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002519
Iteration 48/1000 | Loss: 0.00002517
Iteration 49/1000 | Loss: 0.00002516
Iteration 50/1000 | Loss: 0.00002516
Iteration 51/1000 | Loss: 0.00002516
Iteration 52/1000 | Loss: 0.00002516
Iteration 53/1000 | Loss: 0.00002515
Iteration 54/1000 | Loss: 0.00002515
Iteration 55/1000 | Loss: 0.00002514
Iteration 56/1000 | Loss: 0.00002514
Iteration 57/1000 | Loss: 0.00002514
Iteration 58/1000 | Loss: 0.00002514
Iteration 59/1000 | Loss: 0.00002513
Iteration 60/1000 | Loss: 0.00002513
Iteration 61/1000 | Loss: 0.00002512
Iteration 62/1000 | Loss: 0.00002512
Iteration 63/1000 | Loss: 0.00002512
Iteration 64/1000 | Loss: 0.00002512
Iteration 65/1000 | Loss: 0.00002512
Iteration 66/1000 | Loss: 0.00002511
Iteration 67/1000 | Loss: 0.00002511
Iteration 68/1000 | Loss: 0.00002510
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002508
Iteration 72/1000 | Loss: 0.00002508
Iteration 73/1000 | Loss: 0.00002508
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002506
Iteration 78/1000 | Loss: 0.00002506
Iteration 79/1000 | Loss: 0.00002505
Iteration 80/1000 | Loss: 0.00002505
Iteration 81/1000 | Loss: 0.00002505
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002504
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002503
Iteration 86/1000 | Loss: 0.00002503
Iteration 87/1000 | Loss: 0.00002503
Iteration 88/1000 | Loss: 0.00002502
Iteration 89/1000 | Loss: 0.00002502
Iteration 90/1000 | Loss: 0.00002502
Iteration 91/1000 | Loss: 0.00002501
Iteration 92/1000 | Loss: 0.00002501
Iteration 93/1000 | Loss: 0.00002501
Iteration 94/1000 | Loss: 0.00002501
Iteration 95/1000 | Loss: 0.00002500
Iteration 96/1000 | Loss: 0.00002500
Iteration 97/1000 | Loss: 0.00002500
Iteration 98/1000 | Loss: 0.00002500
Iteration 99/1000 | Loss: 0.00002500
Iteration 100/1000 | Loss: 0.00002500
Iteration 101/1000 | Loss: 0.00002500
Iteration 102/1000 | Loss: 0.00002500
Iteration 103/1000 | Loss: 0.00002500
Iteration 104/1000 | Loss: 0.00002499
Iteration 105/1000 | Loss: 0.00002499
Iteration 106/1000 | Loss: 0.00002499
Iteration 107/1000 | Loss: 0.00002499
Iteration 108/1000 | Loss: 0.00002499
Iteration 109/1000 | Loss: 0.00002498
Iteration 110/1000 | Loss: 0.00002498
Iteration 111/1000 | Loss: 0.00002498
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002497
Iteration 114/1000 | Loss: 0.00002497
Iteration 115/1000 | Loss: 0.00002497
Iteration 116/1000 | Loss: 0.00002497
Iteration 117/1000 | Loss: 0.00002496
Iteration 118/1000 | Loss: 0.00002496
Iteration 119/1000 | Loss: 0.00002496
Iteration 120/1000 | Loss: 0.00002496
Iteration 121/1000 | Loss: 0.00002496
Iteration 122/1000 | Loss: 0.00002495
Iteration 123/1000 | Loss: 0.00002495
Iteration 124/1000 | Loss: 0.00002495
Iteration 125/1000 | Loss: 0.00002495
Iteration 126/1000 | Loss: 0.00002495
Iteration 127/1000 | Loss: 0.00002495
Iteration 128/1000 | Loss: 0.00002494
Iteration 129/1000 | Loss: 0.00002494
Iteration 130/1000 | Loss: 0.00002494
Iteration 131/1000 | Loss: 0.00002494
Iteration 132/1000 | Loss: 0.00002494
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Iteration 135/1000 | Loss: 0.00002493
Iteration 136/1000 | Loss: 0.00002493
Iteration 137/1000 | Loss: 0.00002493
Iteration 138/1000 | Loss: 0.00002492
Iteration 139/1000 | Loss: 0.00002492
Iteration 140/1000 | Loss: 0.00002492
Iteration 141/1000 | Loss: 0.00002492
Iteration 142/1000 | Loss: 0.00002491
Iteration 143/1000 | Loss: 0.00002491
Iteration 144/1000 | Loss: 0.00002491
Iteration 145/1000 | Loss: 0.00002491
Iteration 146/1000 | Loss: 0.00002491
Iteration 147/1000 | Loss: 0.00002491
Iteration 148/1000 | Loss: 0.00002490
Iteration 149/1000 | Loss: 0.00002490
Iteration 150/1000 | Loss: 0.00002490
Iteration 151/1000 | Loss: 0.00002490
Iteration 152/1000 | Loss: 0.00002490
Iteration 153/1000 | Loss: 0.00002490
Iteration 154/1000 | Loss: 0.00002490
Iteration 155/1000 | Loss: 0.00002489
Iteration 156/1000 | Loss: 0.00002489
Iteration 157/1000 | Loss: 0.00002489
Iteration 158/1000 | Loss: 0.00002489
Iteration 159/1000 | Loss: 0.00002489
Iteration 160/1000 | Loss: 0.00002489
Iteration 161/1000 | Loss: 0.00002489
Iteration 162/1000 | Loss: 0.00002489
Iteration 163/1000 | Loss: 0.00002489
Iteration 164/1000 | Loss: 0.00002489
Iteration 165/1000 | Loss: 0.00002489
Iteration 166/1000 | Loss: 0.00002490
Iteration 167/1000 | Loss: 0.00002489
Iteration 168/1000 | Loss: 0.00002489
Iteration 169/1000 | Loss: 0.00002489
Iteration 170/1000 | Loss: 0.00002489
Iteration 171/1000 | Loss: 0.00002489
Iteration 172/1000 | Loss: 0.00002489
Iteration 173/1000 | Loss: 0.00002490
Iteration 174/1000 | Loss: 0.00002490
Iteration 175/1000 | Loss: 0.00002489
Iteration 176/1000 | Loss: 0.00002489
Iteration 177/1000 | Loss: 0.00002489
Iteration 178/1000 | Loss: 0.00002489
Iteration 179/1000 | Loss: 0.00002489
Iteration 180/1000 | Loss: 0.00002490
Iteration 181/1000 | Loss: 0.00002489
Iteration 182/1000 | Loss: 0.00002489
Iteration 183/1000 | Loss: 0.00002489
Iteration 184/1000 | Loss: 0.00002489
Iteration 185/1000 | Loss: 0.00002489
Iteration 186/1000 | Loss: 0.00002489
Iteration 187/1000 | Loss: 0.00002489
Iteration 188/1000 | Loss: 0.00002489
Iteration 189/1000 | Loss: 0.00002489
Iteration 190/1000 | Loss: 0.00002489
Iteration 191/1000 | Loss: 0.00002489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.4894998205127195e-05, 2.4894998205127195e-05, 2.4894998205127195e-05, 2.4894998205127195e-05, 2.4894998205127195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4894998205127195e-05

Optimization complete. Final v2v error: 4.004316329956055 mm

Highest mean error: 5.159062385559082 mm for frame 77

Lowest mean error: 3.2066690921783447 mm for frame 110

Saving results

Total time: 66.945880651474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049885
Iteration 2/25 | Loss: 0.00506039
Iteration 3/25 | Loss: 0.00385869
Iteration 4/25 | Loss: 0.00285530
Iteration 5/25 | Loss: 0.00249105
Iteration 6/25 | Loss: 0.00235705
Iteration 7/25 | Loss: 0.00246990
Iteration 8/25 | Loss: 0.00220845
Iteration 9/25 | Loss: 0.00212506
Iteration 10/25 | Loss: 0.00188125
Iteration 11/25 | Loss: 0.00174737
Iteration 12/25 | Loss: 0.00168474
Iteration 13/25 | Loss: 0.00162367
Iteration 14/25 | Loss: 0.00161274
Iteration 15/25 | Loss: 0.00160651
Iteration 16/25 | Loss: 0.00159888
Iteration 17/25 | Loss: 0.00161172
Iteration 18/25 | Loss: 0.00159568
Iteration 19/25 | Loss: 0.00159457
Iteration 20/25 | Loss: 0.00159135
Iteration 21/25 | Loss: 0.00158941
Iteration 22/25 | Loss: 0.00158909
Iteration 23/25 | Loss: 0.00158845
Iteration 24/25 | Loss: 0.00158698
Iteration 25/25 | Loss: 0.00158825

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70494181
Iteration 2/25 | Loss: 0.00170582
Iteration 3/25 | Loss: 0.00170582
Iteration 4/25 | Loss: 0.00170582
Iteration 5/25 | Loss: 0.00170582
Iteration 6/25 | Loss: 0.00170582
Iteration 7/25 | Loss: 0.00170582
Iteration 8/25 | Loss: 0.00170582
Iteration 9/25 | Loss: 0.00170582
Iteration 10/25 | Loss: 0.00170582
Iteration 11/25 | Loss: 0.00170582
Iteration 12/25 | Loss: 0.00170582
Iteration 13/25 | Loss: 0.00170582
Iteration 14/25 | Loss: 0.00170582
Iteration 15/25 | Loss: 0.00170582
Iteration 16/25 | Loss: 0.00170582
Iteration 17/25 | Loss: 0.00170582
Iteration 18/25 | Loss: 0.00170582
Iteration 19/25 | Loss: 0.00170582
Iteration 20/25 | Loss: 0.00170582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017058172961696982, 0.0017058172961696982, 0.0017058172961696982, 0.0017058172961696982, 0.0017058172961696982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017058172961696982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170582
Iteration 2/1000 | Loss: 0.00217905
Iteration 3/1000 | Loss: 0.00148689
Iteration 4/1000 | Loss: 0.00048854
Iteration 5/1000 | Loss: 0.00022274
Iteration 6/1000 | Loss: 0.00027663
Iteration 7/1000 | Loss: 0.00031583
Iteration 8/1000 | Loss: 0.00013985
Iteration 9/1000 | Loss: 0.00008856
Iteration 10/1000 | Loss: 0.00006711
Iteration 11/1000 | Loss: 0.00005504
Iteration 12/1000 | Loss: 0.00004973
Iteration 13/1000 | Loss: 0.00004633
Iteration 14/1000 | Loss: 0.00004256
Iteration 15/1000 | Loss: 0.00015235
Iteration 16/1000 | Loss: 0.00004538
Iteration 17/1000 | Loss: 0.00004129
Iteration 18/1000 | Loss: 0.00009710
Iteration 19/1000 | Loss: 0.00023298
Iteration 20/1000 | Loss: 0.00014994
Iteration 21/1000 | Loss: 0.00005651
Iteration 22/1000 | Loss: 0.00017524
Iteration 23/1000 | Loss: 0.00011482
Iteration 24/1000 | Loss: 0.00012835
Iteration 25/1000 | Loss: 0.00004087
Iteration 26/1000 | Loss: 0.00003663
Iteration 27/1000 | Loss: 0.00003456
Iteration 28/1000 | Loss: 0.00020583
Iteration 29/1000 | Loss: 0.00005427
Iteration 30/1000 | Loss: 0.00003592
Iteration 31/1000 | Loss: 0.00022251
Iteration 32/1000 | Loss: 0.00003512
Iteration 33/1000 | Loss: 0.00003162
Iteration 34/1000 | Loss: 0.00003048
Iteration 35/1000 | Loss: 0.00022862
Iteration 36/1000 | Loss: 0.00003545
Iteration 37/1000 | Loss: 0.00003075
Iteration 38/1000 | Loss: 0.00002842
Iteration 39/1000 | Loss: 0.00002675
Iteration 40/1000 | Loss: 0.00002586
Iteration 41/1000 | Loss: 0.00002520
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002463
Iteration 45/1000 | Loss: 0.00002462
Iteration 46/1000 | Loss: 0.00002456
Iteration 47/1000 | Loss: 0.00002453
Iteration 48/1000 | Loss: 0.00002453
Iteration 49/1000 | Loss: 0.00002453
Iteration 50/1000 | Loss: 0.00002444
Iteration 51/1000 | Loss: 0.00002443
Iteration 52/1000 | Loss: 0.00002443
Iteration 53/1000 | Loss: 0.00002441
Iteration 54/1000 | Loss: 0.00002434
Iteration 55/1000 | Loss: 0.00002433
Iteration 56/1000 | Loss: 0.00002432
Iteration 57/1000 | Loss: 0.00002432
Iteration 58/1000 | Loss: 0.00002431
Iteration 59/1000 | Loss: 0.00002430
Iteration 60/1000 | Loss: 0.00002430
Iteration 61/1000 | Loss: 0.00002430
Iteration 62/1000 | Loss: 0.00002430
Iteration 63/1000 | Loss: 0.00002430
Iteration 64/1000 | Loss: 0.00002430
Iteration 65/1000 | Loss: 0.00002430
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002429
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002428
Iteration 72/1000 | Loss: 0.00002428
Iteration 73/1000 | Loss: 0.00002428
Iteration 74/1000 | Loss: 0.00002428
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002426
Iteration 77/1000 | Loss: 0.00002426
Iteration 78/1000 | Loss: 0.00002425
Iteration 79/1000 | Loss: 0.00002425
Iteration 80/1000 | Loss: 0.00002425
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002424
Iteration 83/1000 | Loss: 0.00002424
Iteration 84/1000 | Loss: 0.00002423
Iteration 85/1000 | Loss: 0.00002423
Iteration 86/1000 | Loss: 0.00002423
Iteration 87/1000 | Loss: 0.00002423
Iteration 88/1000 | Loss: 0.00002423
Iteration 89/1000 | Loss: 0.00002423
Iteration 90/1000 | Loss: 0.00002423
Iteration 91/1000 | Loss: 0.00002423
Iteration 92/1000 | Loss: 0.00002423
Iteration 93/1000 | Loss: 0.00002423
Iteration 94/1000 | Loss: 0.00002423
Iteration 95/1000 | Loss: 0.00002423
Iteration 96/1000 | Loss: 0.00002423
Iteration 97/1000 | Loss: 0.00002422
Iteration 98/1000 | Loss: 0.00002422
Iteration 99/1000 | Loss: 0.00002422
Iteration 100/1000 | Loss: 0.00002422
Iteration 101/1000 | Loss: 0.00002422
Iteration 102/1000 | Loss: 0.00002422
Iteration 103/1000 | Loss: 0.00002422
Iteration 104/1000 | Loss: 0.00002422
Iteration 105/1000 | Loss: 0.00002422
Iteration 106/1000 | Loss: 0.00002422
Iteration 107/1000 | Loss: 0.00002422
Iteration 108/1000 | Loss: 0.00002422
Iteration 109/1000 | Loss: 0.00002422
Iteration 110/1000 | Loss: 0.00002422
Iteration 111/1000 | Loss: 0.00002422
Iteration 112/1000 | Loss: 0.00002422
Iteration 113/1000 | Loss: 0.00002422
Iteration 114/1000 | Loss: 0.00002422
Iteration 115/1000 | Loss: 0.00002422
Iteration 116/1000 | Loss: 0.00002422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.4215358280343935e-05, 2.4215358280343935e-05, 2.4215358280343935e-05, 2.4215358280343935e-05, 2.4215358280343935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4215358280343935e-05

Optimization complete. Final v2v error: 4.036496162414551 mm

Highest mean error: 4.615512847900391 mm for frame 239

Lowest mean error: 3.7968368530273438 mm for frame 32

Saving results

Total time: 134.05077266693115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044783
Iteration 2/25 | Loss: 0.00225748
Iteration 3/25 | Loss: 0.00213322
Iteration 4/25 | Loss: 0.00208397
Iteration 5/25 | Loss: 0.00163807
Iteration 6/25 | Loss: 0.00153956
Iteration 7/25 | Loss: 0.00138234
Iteration 8/25 | Loss: 0.00132550
Iteration 9/25 | Loss: 0.00124377
Iteration 10/25 | Loss: 0.00121899
Iteration 11/25 | Loss: 0.00121608
Iteration 12/25 | Loss: 0.00121150
Iteration 13/25 | Loss: 0.00121113
Iteration 14/25 | Loss: 0.00121107
Iteration 15/25 | Loss: 0.00121106
Iteration 16/25 | Loss: 0.00121106
Iteration 17/25 | Loss: 0.00121106
Iteration 18/25 | Loss: 0.00121106
Iteration 19/25 | Loss: 0.00121106
Iteration 20/25 | Loss: 0.00121106
Iteration 21/25 | Loss: 0.00121106
Iteration 22/25 | Loss: 0.00121105
Iteration 23/25 | Loss: 0.00121105
Iteration 24/25 | Loss: 0.00121105
Iteration 25/25 | Loss: 0.00121104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39800143
Iteration 2/25 | Loss: 0.00075311
Iteration 3/25 | Loss: 0.00075311
Iteration 4/25 | Loss: 0.00075311
Iteration 5/25 | Loss: 0.00075311
Iteration 6/25 | Loss: 0.00075311
Iteration 7/25 | Loss: 0.00075311
Iteration 8/25 | Loss: 0.00075311
Iteration 9/25 | Loss: 0.00075311
Iteration 10/25 | Loss: 0.00075311
Iteration 11/25 | Loss: 0.00075311
Iteration 12/25 | Loss: 0.00075311
Iteration 13/25 | Loss: 0.00075311
Iteration 14/25 | Loss: 0.00075311
Iteration 15/25 | Loss: 0.00075311
Iteration 16/25 | Loss: 0.00075311
Iteration 17/25 | Loss: 0.00075311
Iteration 18/25 | Loss: 0.00075311
Iteration 19/25 | Loss: 0.00075311
Iteration 20/25 | Loss: 0.00075311
Iteration 21/25 | Loss: 0.00075311
Iteration 22/25 | Loss: 0.00075311
Iteration 23/25 | Loss: 0.00075311
Iteration 24/25 | Loss: 0.00075311
Iteration 25/25 | Loss: 0.00075311

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075311
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00001994
Iteration 4/1000 | Loss: 0.00001842
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00001616
Iteration 9/1000 | Loss: 0.00001584
Iteration 10/1000 | Loss: 0.00001564
Iteration 11/1000 | Loss: 0.00001547
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001525
Iteration 14/1000 | Loss: 0.00001525
Iteration 15/1000 | Loss: 0.00001523
Iteration 16/1000 | Loss: 0.00001522
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001517
Iteration 20/1000 | Loss: 0.00001517
Iteration 21/1000 | Loss: 0.00001516
Iteration 22/1000 | Loss: 0.00001516
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001512
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001511
Iteration 27/1000 | Loss: 0.00001511
Iteration 28/1000 | Loss: 0.00001511
Iteration 29/1000 | Loss: 0.00001510
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001502
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001495
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001490
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001473
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001471
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001457
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001456
Iteration 93/1000 | Loss: 0.00001456
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001455
Iteration 107/1000 | Loss: 0.00001455
Iteration 108/1000 | Loss: 0.00001455
Iteration 109/1000 | Loss: 0.00001455
Iteration 110/1000 | Loss: 0.00001455
Iteration 111/1000 | Loss: 0.00001455
Iteration 112/1000 | Loss: 0.00001455
Iteration 113/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.4554128028976265e-05, 1.4554128028976265e-05, 1.4554128028976265e-05, 1.4554128028976265e-05, 1.4554128028976265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4554128028976265e-05

Optimization complete. Final v2v error: 3.2449259757995605 mm

Highest mean error: 3.4041125774383545 mm for frame 65

Lowest mean error: 3.1219286918640137 mm for frame 107

Saving results

Total time: 53.387935161590576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982369
Iteration 2/25 | Loss: 0.00182177
Iteration 3/25 | Loss: 0.00150495
Iteration 4/25 | Loss: 0.00147754
Iteration 5/25 | Loss: 0.00146558
Iteration 6/25 | Loss: 0.00144781
Iteration 7/25 | Loss: 0.00144572
Iteration 8/25 | Loss: 0.00142945
Iteration 9/25 | Loss: 0.00142303
Iteration 10/25 | Loss: 0.00141154
Iteration 11/25 | Loss: 0.00142409
Iteration 12/25 | Loss: 0.00141345
Iteration 13/25 | Loss: 0.00139926
Iteration 14/25 | Loss: 0.00139295
Iteration 15/25 | Loss: 0.00139394
Iteration 16/25 | Loss: 0.00138556
Iteration 17/25 | Loss: 0.00138039
Iteration 18/25 | Loss: 0.00137688
Iteration 19/25 | Loss: 0.00137482
Iteration 20/25 | Loss: 0.00137245
Iteration 21/25 | Loss: 0.00137125
Iteration 22/25 | Loss: 0.00137128
Iteration 23/25 | Loss: 0.00137284
Iteration 24/25 | Loss: 0.00137216
Iteration 25/25 | Loss: 0.00136769

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84963262
Iteration 2/25 | Loss: 0.00193279
Iteration 3/25 | Loss: 0.00182492
Iteration 4/25 | Loss: 0.00182492
Iteration 5/25 | Loss: 0.00182492
Iteration 6/25 | Loss: 0.00182492
Iteration 7/25 | Loss: 0.00182492
Iteration 8/25 | Loss: 0.00182492
Iteration 9/25 | Loss: 0.00182492
Iteration 10/25 | Loss: 0.00182492
Iteration 11/25 | Loss: 0.00182492
Iteration 12/25 | Loss: 0.00182492
Iteration 13/25 | Loss: 0.00182492
Iteration 14/25 | Loss: 0.00182492
Iteration 15/25 | Loss: 0.00182492
Iteration 16/25 | Loss: 0.00182492
Iteration 17/25 | Loss: 0.00182492
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018249183194711804, 0.0018249183194711804, 0.0018249183194711804, 0.0018249183194711804, 0.0018249183194711804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018249183194711804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182492
Iteration 2/1000 | Loss: 0.00034018
Iteration 3/1000 | Loss: 0.00018440
Iteration 4/1000 | Loss: 0.00033631
Iteration 5/1000 | Loss: 0.00030125
Iteration 6/1000 | Loss: 0.00043558
Iteration 7/1000 | Loss: 0.00053873
Iteration 8/1000 | Loss: 0.00011209
Iteration 9/1000 | Loss: 0.00011138
Iteration 10/1000 | Loss: 0.00011877
Iteration 11/1000 | Loss: 0.00009846
Iteration 12/1000 | Loss: 0.00018121
Iteration 13/1000 | Loss: 0.00089024
Iteration 14/1000 | Loss: 0.00054417
Iteration 15/1000 | Loss: 0.00048063
Iteration 16/1000 | Loss: 0.00009835
Iteration 17/1000 | Loss: 0.00011218
Iteration 18/1000 | Loss: 0.00039097
Iteration 19/1000 | Loss: 0.00011812
Iteration 20/1000 | Loss: 0.00010118
Iteration 21/1000 | Loss: 0.00043666
Iteration 22/1000 | Loss: 0.00110323
Iteration 23/1000 | Loss: 0.00090473
Iteration 24/1000 | Loss: 0.00077529
Iteration 25/1000 | Loss: 0.00044728
Iteration 26/1000 | Loss: 0.00068579
Iteration 27/1000 | Loss: 0.00042376
Iteration 28/1000 | Loss: 0.00008860
Iteration 29/1000 | Loss: 0.00011239
Iteration 30/1000 | Loss: 0.00007168
Iteration 31/1000 | Loss: 0.00008704
Iteration 32/1000 | Loss: 0.00007360
Iteration 33/1000 | Loss: 0.00021216
Iteration 34/1000 | Loss: 0.00023915
Iteration 35/1000 | Loss: 0.00010094
Iteration 36/1000 | Loss: 0.00008061
Iteration 37/1000 | Loss: 0.00018551
Iteration 38/1000 | Loss: 0.00007523
Iteration 39/1000 | Loss: 0.00007116
Iteration 40/1000 | Loss: 0.00007550
Iteration 41/1000 | Loss: 0.00007442
Iteration 42/1000 | Loss: 0.00007248
Iteration 43/1000 | Loss: 0.00006352
Iteration 44/1000 | Loss: 0.00007435
Iteration 45/1000 | Loss: 0.00009408
Iteration 46/1000 | Loss: 0.00014831
Iteration 47/1000 | Loss: 0.00010347
Iteration 48/1000 | Loss: 0.00007921
Iteration 49/1000 | Loss: 0.00018087
Iteration 50/1000 | Loss: 0.00010249
Iteration 51/1000 | Loss: 0.00006353
Iteration 52/1000 | Loss: 0.00009141
Iteration 53/1000 | Loss: 0.00005477
Iteration 54/1000 | Loss: 0.00005124
Iteration 55/1000 | Loss: 0.00004951
Iteration 56/1000 | Loss: 0.00004823
Iteration 57/1000 | Loss: 0.00004734
Iteration 58/1000 | Loss: 0.00004650
Iteration 59/1000 | Loss: 0.00004588
Iteration 60/1000 | Loss: 0.00040703
Iteration 61/1000 | Loss: 0.00110340
Iteration 62/1000 | Loss: 0.00152598
Iteration 63/1000 | Loss: 0.00005387
Iteration 64/1000 | Loss: 0.00004414
Iteration 65/1000 | Loss: 0.00003982
Iteration 66/1000 | Loss: 0.00003665
Iteration 67/1000 | Loss: 0.00003430
Iteration 68/1000 | Loss: 0.00003234
Iteration 69/1000 | Loss: 0.00003121
Iteration 70/1000 | Loss: 0.00003029
Iteration 71/1000 | Loss: 0.00041145
Iteration 72/1000 | Loss: 0.00003170
Iteration 73/1000 | Loss: 0.00002954
Iteration 74/1000 | Loss: 0.00002839
Iteration 75/1000 | Loss: 0.00002753
Iteration 76/1000 | Loss: 0.00042294
Iteration 77/1000 | Loss: 0.00048287
Iteration 78/1000 | Loss: 0.00006665
Iteration 79/1000 | Loss: 0.00003511
Iteration 80/1000 | Loss: 0.00003009
Iteration 81/1000 | Loss: 0.00002755
Iteration 82/1000 | Loss: 0.00002585
Iteration 83/1000 | Loss: 0.00002487
Iteration 84/1000 | Loss: 0.00002389
Iteration 85/1000 | Loss: 0.00002320
Iteration 86/1000 | Loss: 0.00002283
Iteration 87/1000 | Loss: 0.00002263
Iteration 88/1000 | Loss: 0.00002246
Iteration 89/1000 | Loss: 0.00002244
Iteration 90/1000 | Loss: 0.00002231
Iteration 91/1000 | Loss: 0.00002228
Iteration 92/1000 | Loss: 0.00002211
Iteration 93/1000 | Loss: 0.00002209
Iteration 94/1000 | Loss: 0.00002209
Iteration 95/1000 | Loss: 0.00002208
Iteration 96/1000 | Loss: 0.00002208
Iteration 97/1000 | Loss: 0.00002206
Iteration 98/1000 | Loss: 0.00002206
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002203
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00002203
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002202
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002201
Iteration 115/1000 | Loss: 0.00002201
Iteration 116/1000 | Loss: 0.00002201
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002201
Iteration 119/1000 | Loss: 0.00002201
Iteration 120/1000 | Loss: 0.00002201
Iteration 121/1000 | Loss: 0.00002201
Iteration 122/1000 | Loss: 0.00002201
Iteration 123/1000 | Loss: 0.00002201
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002200
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002200
Iteration 128/1000 | Loss: 0.00002200
Iteration 129/1000 | Loss: 0.00002200
Iteration 130/1000 | Loss: 0.00002200
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002199
Iteration 133/1000 | Loss: 0.00002199
Iteration 134/1000 | Loss: 0.00002198
Iteration 135/1000 | Loss: 0.00002198
Iteration 136/1000 | Loss: 0.00002198
Iteration 137/1000 | Loss: 0.00002198
Iteration 138/1000 | Loss: 0.00002198
Iteration 139/1000 | Loss: 0.00002197
Iteration 140/1000 | Loss: 0.00002197
Iteration 141/1000 | Loss: 0.00002197
Iteration 142/1000 | Loss: 0.00002197
Iteration 143/1000 | Loss: 0.00002196
Iteration 144/1000 | Loss: 0.00002196
Iteration 145/1000 | Loss: 0.00002196
Iteration 146/1000 | Loss: 0.00002196
Iteration 147/1000 | Loss: 0.00002196
Iteration 148/1000 | Loss: 0.00002196
Iteration 149/1000 | Loss: 0.00002196
Iteration 150/1000 | Loss: 0.00002196
Iteration 151/1000 | Loss: 0.00002196
Iteration 152/1000 | Loss: 0.00002196
Iteration 153/1000 | Loss: 0.00002195
Iteration 154/1000 | Loss: 0.00002195
Iteration 155/1000 | Loss: 0.00002195
Iteration 156/1000 | Loss: 0.00002195
Iteration 157/1000 | Loss: 0.00002195
Iteration 158/1000 | Loss: 0.00002195
Iteration 159/1000 | Loss: 0.00002195
Iteration 160/1000 | Loss: 0.00002195
Iteration 161/1000 | Loss: 0.00002194
Iteration 162/1000 | Loss: 0.00002194
Iteration 163/1000 | Loss: 0.00002194
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002194
Iteration 168/1000 | Loss: 0.00002194
Iteration 169/1000 | Loss: 0.00002194
Iteration 170/1000 | Loss: 0.00002194
Iteration 171/1000 | Loss: 0.00002193
Iteration 172/1000 | Loss: 0.00002193
Iteration 173/1000 | Loss: 0.00002193
Iteration 174/1000 | Loss: 0.00002193
Iteration 175/1000 | Loss: 0.00002193
Iteration 176/1000 | Loss: 0.00002192
Iteration 177/1000 | Loss: 0.00002192
Iteration 178/1000 | Loss: 0.00002192
Iteration 179/1000 | Loss: 0.00002192
Iteration 180/1000 | Loss: 0.00002192
Iteration 181/1000 | Loss: 0.00002192
Iteration 182/1000 | Loss: 0.00002192
Iteration 183/1000 | Loss: 0.00002191
Iteration 184/1000 | Loss: 0.00002191
Iteration 185/1000 | Loss: 0.00002191
Iteration 186/1000 | Loss: 0.00002191
Iteration 187/1000 | Loss: 0.00002191
Iteration 188/1000 | Loss: 0.00002191
Iteration 189/1000 | Loss: 0.00002191
Iteration 190/1000 | Loss: 0.00002191
Iteration 191/1000 | Loss: 0.00002191
Iteration 192/1000 | Loss: 0.00002191
Iteration 193/1000 | Loss: 0.00002191
Iteration 194/1000 | Loss: 0.00002191
Iteration 195/1000 | Loss: 0.00002190
Iteration 196/1000 | Loss: 0.00002190
Iteration 197/1000 | Loss: 0.00002190
Iteration 198/1000 | Loss: 0.00002190
Iteration 199/1000 | Loss: 0.00002190
Iteration 200/1000 | Loss: 0.00002190
Iteration 201/1000 | Loss: 0.00002190
Iteration 202/1000 | Loss: 0.00002190
Iteration 203/1000 | Loss: 0.00002190
Iteration 204/1000 | Loss: 0.00002190
Iteration 205/1000 | Loss: 0.00002190
Iteration 206/1000 | Loss: 0.00002190
Iteration 207/1000 | Loss: 0.00002190
Iteration 208/1000 | Loss: 0.00002190
Iteration 209/1000 | Loss: 0.00002190
Iteration 210/1000 | Loss: 0.00002190
Iteration 211/1000 | Loss: 0.00002190
Iteration 212/1000 | Loss: 0.00002190
Iteration 213/1000 | Loss: 0.00002190
Iteration 214/1000 | Loss: 0.00002190
Iteration 215/1000 | Loss: 0.00002190
Iteration 216/1000 | Loss: 0.00002190
Iteration 217/1000 | Loss: 0.00002190
Iteration 218/1000 | Loss: 0.00002190
Iteration 219/1000 | Loss: 0.00002190
Iteration 220/1000 | Loss: 0.00002190
Iteration 221/1000 | Loss: 0.00002190
Iteration 222/1000 | Loss: 0.00002190
Iteration 223/1000 | Loss: 0.00002190
Iteration 224/1000 | Loss: 0.00002190
Iteration 225/1000 | Loss: 0.00002190
Iteration 226/1000 | Loss: 0.00002190
Iteration 227/1000 | Loss: 0.00002190
Iteration 228/1000 | Loss: 0.00002190
Iteration 229/1000 | Loss: 0.00002190
Iteration 230/1000 | Loss: 0.00002190
Iteration 231/1000 | Loss: 0.00002190
Iteration 232/1000 | Loss: 0.00002190
Iteration 233/1000 | Loss: 0.00002190
Iteration 234/1000 | Loss: 0.00002190
Iteration 235/1000 | Loss: 0.00002190
Iteration 236/1000 | Loss: 0.00002190
Iteration 237/1000 | Loss: 0.00002190
Iteration 238/1000 | Loss: 0.00002190
Iteration 239/1000 | Loss: 0.00002190
Iteration 240/1000 | Loss: 0.00002190
Iteration 241/1000 | Loss: 0.00002190
Iteration 242/1000 | Loss: 0.00002190
Iteration 243/1000 | Loss: 0.00002190
Iteration 244/1000 | Loss: 0.00002190
Iteration 245/1000 | Loss: 0.00002190
Iteration 246/1000 | Loss: 0.00002190
Iteration 247/1000 | Loss: 0.00002190
Iteration 248/1000 | Loss: 0.00002190
Iteration 249/1000 | Loss: 0.00002190
Iteration 250/1000 | Loss: 0.00002190
Iteration 251/1000 | Loss: 0.00002190
Iteration 252/1000 | Loss: 0.00002190
Iteration 253/1000 | Loss: 0.00002190
Iteration 254/1000 | Loss: 0.00002190
Iteration 255/1000 | Loss: 0.00002190
Iteration 256/1000 | Loss: 0.00002190
Iteration 257/1000 | Loss: 0.00002190
Iteration 258/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [2.190229315601755e-05, 2.190229315601755e-05, 2.190229315601755e-05, 2.190229315601755e-05, 2.190229315601755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.190229315601755e-05

Optimization complete. Final v2v error: 3.9247307777404785 mm

Highest mean error: 6.342024803161621 mm for frame 29

Lowest mean error: 3.357365131378174 mm for frame 185

Saving results

Total time: 210.58110570907593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549951
Iteration 2/25 | Loss: 0.00149243
Iteration 3/25 | Loss: 0.00138877
Iteration 4/25 | Loss: 0.00138031
Iteration 5/25 | Loss: 0.00137804
Iteration 6/25 | Loss: 0.00137804
Iteration 7/25 | Loss: 0.00137804
Iteration 8/25 | Loss: 0.00137804
Iteration 9/25 | Loss: 0.00137804
Iteration 10/25 | Loss: 0.00137804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001378037966787815, 0.001378037966787815, 0.001378037966787815, 0.001378037966787815, 0.001378037966787815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001378037966787815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94466615
Iteration 2/25 | Loss: 0.00094652
Iteration 3/25 | Loss: 0.00094649
Iteration 4/25 | Loss: 0.00094649
Iteration 5/25 | Loss: 0.00094649
Iteration 6/25 | Loss: 0.00094649
Iteration 7/25 | Loss: 0.00094649
Iteration 8/25 | Loss: 0.00094649
Iteration 9/25 | Loss: 0.00094649
Iteration 10/25 | Loss: 0.00094649
Iteration 11/25 | Loss: 0.00094649
Iteration 12/25 | Loss: 0.00094649
Iteration 13/25 | Loss: 0.00094649
Iteration 14/25 | Loss: 0.00094649
Iteration 15/25 | Loss: 0.00094649
Iteration 16/25 | Loss: 0.00094649
Iteration 17/25 | Loss: 0.00094649
Iteration 18/25 | Loss: 0.00094649
Iteration 19/25 | Loss: 0.00094649
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009464912000112236, 0.0009464912000112236, 0.0009464912000112236, 0.0009464912000112236, 0.0009464912000112236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009464912000112236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094649
Iteration 2/1000 | Loss: 0.00004300
Iteration 3/1000 | Loss: 0.00003219
Iteration 4/1000 | Loss: 0.00002934
Iteration 5/1000 | Loss: 0.00002798
Iteration 6/1000 | Loss: 0.00002691
Iteration 7/1000 | Loss: 0.00002623
Iteration 8/1000 | Loss: 0.00002583
Iteration 9/1000 | Loss: 0.00002545
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002491
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002474
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002461
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002458
Iteration 19/1000 | Loss: 0.00002458
Iteration 20/1000 | Loss: 0.00002457
Iteration 21/1000 | Loss: 0.00002453
Iteration 22/1000 | Loss: 0.00002450
Iteration 23/1000 | Loss: 0.00002450
Iteration 24/1000 | Loss: 0.00002443
Iteration 25/1000 | Loss: 0.00002443
Iteration 26/1000 | Loss: 0.00002441
Iteration 27/1000 | Loss: 0.00002438
Iteration 28/1000 | Loss: 0.00002437
Iteration 29/1000 | Loss: 0.00002436
Iteration 30/1000 | Loss: 0.00002436
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002432
Iteration 34/1000 | Loss: 0.00002431
Iteration 35/1000 | Loss: 0.00002431
Iteration 36/1000 | Loss: 0.00002431
Iteration 37/1000 | Loss: 0.00002423
Iteration 38/1000 | Loss: 0.00002423
Iteration 39/1000 | Loss: 0.00002422
Iteration 40/1000 | Loss: 0.00002421
Iteration 41/1000 | Loss: 0.00002421
Iteration 42/1000 | Loss: 0.00002416
Iteration 43/1000 | Loss: 0.00002416
Iteration 44/1000 | Loss: 0.00002415
Iteration 45/1000 | Loss: 0.00002414
Iteration 46/1000 | Loss: 0.00002413
Iteration 47/1000 | Loss: 0.00002412
Iteration 48/1000 | Loss: 0.00002412
Iteration 49/1000 | Loss: 0.00002411
Iteration 50/1000 | Loss: 0.00002411
Iteration 51/1000 | Loss: 0.00002410
Iteration 52/1000 | Loss: 0.00002409
Iteration 53/1000 | Loss: 0.00002409
Iteration 54/1000 | Loss: 0.00002409
Iteration 55/1000 | Loss: 0.00002409
Iteration 56/1000 | Loss: 0.00002409
Iteration 57/1000 | Loss: 0.00002409
Iteration 58/1000 | Loss: 0.00002408
Iteration 59/1000 | Loss: 0.00002408
Iteration 60/1000 | Loss: 0.00002408
Iteration 61/1000 | Loss: 0.00002408
Iteration 62/1000 | Loss: 0.00002407
Iteration 63/1000 | Loss: 0.00002407
Iteration 64/1000 | Loss: 0.00002407
Iteration 65/1000 | Loss: 0.00002407
Iteration 66/1000 | Loss: 0.00002406
Iteration 67/1000 | Loss: 0.00002406
Iteration 68/1000 | Loss: 0.00002406
Iteration 69/1000 | Loss: 0.00002406
Iteration 70/1000 | Loss: 0.00002405
Iteration 71/1000 | Loss: 0.00002405
Iteration 72/1000 | Loss: 0.00002405
Iteration 73/1000 | Loss: 0.00002405
Iteration 74/1000 | Loss: 0.00002405
Iteration 75/1000 | Loss: 0.00002404
Iteration 76/1000 | Loss: 0.00002404
Iteration 77/1000 | Loss: 0.00002404
Iteration 78/1000 | Loss: 0.00002403
Iteration 79/1000 | Loss: 0.00002403
Iteration 80/1000 | Loss: 0.00002403
Iteration 81/1000 | Loss: 0.00002402
Iteration 82/1000 | Loss: 0.00002402
Iteration 83/1000 | Loss: 0.00002402
Iteration 84/1000 | Loss: 0.00002402
Iteration 85/1000 | Loss: 0.00002401
Iteration 86/1000 | Loss: 0.00002401
Iteration 87/1000 | Loss: 0.00002401
Iteration 88/1000 | Loss: 0.00002400
Iteration 89/1000 | Loss: 0.00002400
Iteration 90/1000 | Loss: 0.00002400
Iteration 91/1000 | Loss: 0.00002399
Iteration 92/1000 | Loss: 0.00002399
Iteration 93/1000 | Loss: 0.00002399
Iteration 94/1000 | Loss: 0.00002398
Iteration 95/1000 | Loss: 0.00002398
Iteration 96/1000 | Loss: 0.00002398
Iteration 97/1000 | Loss: 0.00002398
Iteration 98/1000 | Loss: 0.00002398
Iteration 99/1000 | Loss: 0.00002397
Iteration 100/1000 | Loss: 0.00002397
Iteration 101/1000 | Loss: 0.00002397
Iteration 102/1000 | Loss: 0.00002397
Iteration 103/1000 | Loss: 0.00002397
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00002397
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002396
Iteration 109/1000 | Loss: 0.00002396
Iteration 110/1000 | Loss: 0.00002395
Iteration 111/1000 | Loss: 0.00002395
Iteration 112/1000 | Loss: 0.00002395
Iteration 113/1000 | Loss: 0.00002395
Iteration 114/1000 | Loss: 0.00002395
Iteration 115/1000 | Loss: 0.00002395
Iteration 116/1000 | Loss: 0.00002395
Iteration 117/1000 | Loss: 0.00002395
Iteration 118/1000 | Loss: 0.00002395
Iteration 119/1000 | Loss: 0.00002395
Iteration 120/1000 | Loss: 0.00002394
Iteration 121/1000 | Loss: 0.00002394
Iteration 122/1000 | Loss: 0.00002394
Iteration 123/1000 | Loss: 0.00002394
Iteration 124/1000 | Loss: 0.00002394
Iteration 125/1000 | Loss: 0.00002394
Iteration 126/1000 | Loss: 0.00002394
Iteration 127/1000 | Loss: 0.00002394
Iteration 128/1000 | Loss: 0.00002394
Iteration 129/1000 | Loss: 0.00002394
Iteration 130/1000 | Loss: 0.00002394
Iteration 131/1000 | Loss: 0.00002394
Iteration 132/1000 | Loss: 0.00002394
Iteration 133/1000 | Loss: 0.00002394
Iteration 134/1000 | Loss: 0.00002394
Iteration 135/1000 | Loss: 0.00002394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.3942182451719418e-05, 2.3942182451719418e-05, 2.3942182451719418e-05, 2.3942182451719418e-05, 2.3942182451719418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3942182451719418e-05

Optimization complete. Final v2v error: 4.00307035446167 mm

Highest mean error: 4.46197509765625 mm for frame 163

Lowest mean error: 3.6593427658081055 mm for frame 220

Saving results

Total time: 44.07058835029602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991131
Iteration 2/25 | Loss: 0.00238510
Iteration 3/25 | Loss: 0.00184423
Iteration 4/25 | Loss: 0.00176737
Iteration 5/25 | Loss: 0.00159639
Iteration 6/25 | Loss: 0.00146175
Iteration 7/25 | Loss: 0.00141292
Iteration 8/25 | Loss: 0.00138280
Iteration 9/25 | Loss: 0.00135902
Iteration 10/25 | Loss: 0.00133954
Iteration 11/25 | Loss: 0.00132606
Iteration 12/25 | Loss: 0.00131932
Iteration 13/25 | Loss: 0.00131932
Iteration 14/25 | Loss: 0.00131777
Iteration 15/25 | Loss: 0.00131294
Iteration 16/25 | Loss: 0.00131338
Iteration 17/25 | Loss: 0.00131060
Iteration 18/25 | Loss: 0.00130942
Iteration 19/25 | Loss: 0.00130858
Iteration 20/25 | Loss: 0.00130835
Iteration 21/25 | Loss: 0.00130834
Iteration 22/25 | Loss: 0.00130834
Iteration 23/25 | Loss: 0.00130834
Iteration 24/25 | Loss: 0.00130834
Iteration 25/25 | Loss: 0.00130834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44283950
Iteration 2/25 | Loss: 0.00147883
Iteration 3/25 | Loss: 0.00145927
Iteration 4/25 | Loss: 0.00145927
Iteration 5/25 | Loss: 0.00145927
Iteration 6/25 | Loss: 0.00145927
Iteration 7/25 | Loss: 0.00145927
Iteration 8/25 | Loss: 0.00145927
Iteration 9/25 | Loss: 0.00145927
Iteration 10/25 | Loss: 0.00145927
Iteration 11/25 | Loss: 0.00145927
Iteration 12/25 | Loss: 0.00145927
Iteration 13/25 | Loss: 0.00145927
Iteration 14/25 | Loss: 0.00145927
Iteration 15/25 | Loss: 0.00145927
Iteration 16/25 | Loss: 0.00145927
Iteration 17/25 | Loss: 0.00145927
Iteration 18/25 | Loss: 0.00145927
Iteration 19/25 | Loss: 0.00145927
Iteration 20/25 | Loss: 0.00145927
Iteration 21/25 | Loss: 0.00145927
Iteration 22/25 | Loss: 0.00145927
Iteration 23/25 | Loss: 0.00145927
Iteration 24/25 | Loss: 0.00145927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014592669904232025, 0.0014592669904232025, 0.0014592669904232025, 0.0014592669904232025, 0.0014592669904232025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014592669904232025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145927
Iteration 2/1000 | Loss: 0.00037234
Iteration 3/1000 | Loss: 0.00012324
Iteration 4/1000 | Loss: 0.00027510
Iteration 5/1000 | Loss: 0.00025259
Iteration 6/1000 | Loss: 0.00018243
Iteration 7/1000 | Loss: 0.00016446
Iteration 8/1000 | Loss: 0.00019224
Iteration 9/1000 | Loss: 0.00020240
Iteration 10/1000 | Loss: 0.00032323
Iteration 11/1000 | Loss: 0.00011471
Iteration 12/1000 | Loss: 0.00006081
Iteration 13/1000 | Loss: 0.00007799
Iteration 14/1000 | Loss: 0.00004922
Iteration 15/1000 | Loss: 0.00005966
Iteration 16/1000 | Loss: 0.00005779
Iteration 17/1000 | Loss: 0.00005790
Iteration 18/1000 | Loss: 0.00006186
Iteration 19/1000 | Loss: 0.00005557
Iteration 20/1000 | Loss: 0.00005132
Iteration 21/1000 | Loss: 0.00004996
Iteration 22/1000 | Loss: 0.00004232
Iteration 23/1000 | Loss: 0.00007175
Iteration 24/1000 | Loss: 0.00005814
Iteration 25/1000 | Loss: 0.00005451
Iteration 26/1000 | Loss: 0.00006809
Iteration 27/1000 | Loss: 0.00006025
Iteration 28/1000 | Loss: 0.00006903
Iteration 29/1000 | Loss: 0.00005582
Iteration 30/1000 | Loss: 0.00005012
Iteration 31/1000 | Loss: 0.00005748
Iteration 32/1000 | Loss: 0.00004940
Iteration 33/1000 | Loss: 0.00006193
Iteration 34/1000 | Loss: 0.00004364
Iteration 35/1000 | Loss: 0.00005192
Iteration 36/1000 | Loss: 0.00005386
Iteration 37/1000 | Loss: 0.00004986
Iteration 38/1000 | Loss: 0.00005184
Iteration 39/1000 | Loss: 0.00004287
Iteration 40/1000 | Loss: 0.00007119
Iteration 41/1000 | Loss: 0.00004656
Iteration 42/1000 | Loss: 0.00004844
Iteration 43/1000 | Loss: 0.00004825
Iteration 44/1000 | Loss: 0.00005084
Iteration 45/1000 | Loss: 0.00005050
Iteration 46/1000 | Loss: 0.00005033
Iteration 47/1000 | Loss: 0.00005239
Iteration 48/1000 | Loss: 0.00004949
Iteration 49/1000 | Loss: 0.00005082
Iteration 50/1000 | Loss: 0.00005433
Iteration 51/1000 | Loss: 0.00004851
Iteration 52/1000 | Loss: 0.00005970
Iteration 53/1000 | Loss: 0.00005115
Iteration 54/1000 | Loss: 0.00004748
Iteration 55/1000 | Loss: 0.00004803
Iteration 56/1000 | Loss: 0.00004865
Iteration 57/1000 | Loss: 0.00004738
Iteration 58/1000 | Loss: 0.00004786
Iteration 59/1000 | Loss: 0.00004754
Iteration 60/1000 | Loss: 0.00004882
Iteration 61/1000 | Loss: 0.00004751
Iteration 62/1000 | Loss: 0.00004941
Iteration 63/1000 | Loss: 0.00005014
Iteration 64/1000 | Loss: 0.00004871
Iteration 65/1000 | Loss: 0.00004141
Iteration 66/1000 | Loss: 0.00004036
Iteration 67/1000 | Loss: 0.00004470
Iteration 68/1000 | Loss: 0.00003816
Iteration 69/1000 | Loss: 0.00004376
Iteration 70/1000 | Loss: 0.00003756
Iteration 71/1000 | Loss: 0.00003726
Iteration 72/1000 | Loss: 0.00003689
Iteration 73/1000 | Loss: 0.00003688
Iteration 74/1000 | Loss: 0.00003660
Iteration 75/1000 | Loss: 0.00004845
Iteration 76/1000 | Loss: 0.00004025
Iteration 77/1000 | Loss: 0.00003622
Iteration 78/1000 | Loss: 0.00004027
Iteration 79/1000 | Loss: 0.00003598
Iteration 80/1000 | Loss: 0.00003598
Iteration 81/1000 | Loss: 0.00003597
Iteration 82/1000 | Loss: 0.00004326
Iteration 83/1000 | Loss: 0.00003594
Iteration 84/1000 | Loss: 0.00003586
Iteration 85/1000 | Loss: 0.00003580
Iteration 86/1000 | Loss: 0.00003580
Iteration 87/1000 | Loss: 0.00003829
Iteration 88/1000 | Loss: 0.00005233
Iteration 89/1000 | Loss: 0.00003857
Iteration 90/1000 | Loss: 0.00003561
Iteration 91/1000 | Loss: 0.00003561
Iteration 92/1000 | Loss: 0.00003637
Iteration 93/1000 | Loss: 0.00003555
Iteration 94/1000 | Loss: 0.00003555
Iteration 95/1000 | Loss: 0.00003555
Iteration 96/1000 | Loss: 0.00003555
Iteration 97/1000 | Loss: 0.00003555
Iteration 98/1000 | Loss: 0.00003555
Iteration 99/1000 | Loss: 0.00003555
Iteration 100/1000 | Loss: 0.00003555
Iteration 101/1000 | Loss: 0.00003555
Iteration 102/1000 | Loss: 0.00003555
Iteration 103/1000 | Loss: 0.00003554
Iteration 104/1000 | Loss: 0.00003554
Iteration 105/1000 | Loss: 0.00003554
Iteration 106/1000 | Loss: 0.00003554
Iteration 107/1000 | Loss: 0.00003554
Iteration 108/1000 | Loss: 0.00003554
Iteration 109/1000 | Loss: 0.00003554
Iteration 110/1000 | Loss: 0.00003554
Iteration 111/1000 | Loss: 0.00003554
Iteration 112/1000 | Loss: 0.00003554
Iteration 113/1000 | Loss: 0.00003553
Iteration 114/1000 | Loss: 0.00003552
Iteration 115/1000 | Loss: 0.00003552
Iteration 116/1000 | Loss: 0.00003556
Iteration 117/1000 | Loss: 0.00003550
Iteration 118/1000 | Loss: 0.00003550
Iteration 119/1000 | Loss: 0.00003550
Iteration 120/1000 | Loss: 0.00003550
Iteration 121/1000 | Loss: 0.00003550
Iteration 122/1000 | Loss: 0.00003550
Iteration 123/1000 | Loss: 0.00003550
Iteration 124/1000 | Loss: 0.00003550
Iteration 125/1000 | Loss: 0.00003550
Iteration 126/1000 | Loss: 0.00003549
Iteration 127/1000 | Loss: 0.00003549
Iteration 128/1000 | Loss: 0.00003549
Iteration 129/1000 | Loss: 0.00003548
Iteration 130/1000 | Loss: 0.00003548
Iteration 131/1000 | Loss: 0.00003548
Iteration 132/1000 | Loss: 0.00003547
Iteration 133/1000 | Loss: 0.00003547
Iteration 134/1000 | Loss: 0.00003546
Iteration 135/1000 | Loss: 0.00003546
Iteration 136/1000 | Loss: 0.00003546
Iteration 137/1000 | Loss: 0.00003545
Iteration 138/1000 | Loss: 0.00003545
Iteration 139/1000 | Loss: 0.00003601
Iteration 140/1000 | Loss: 0.00003542
Iteration 141/1000 | Loss: 0.00003542
Iteration 142/1000 | Loss: 0.00003542
Iteration 143/1000 | Loss: 0.00003542
Iteration 144/1000 | Loss: 0.00003542
Iteration 145/1000 | Loss: 0.00003542
Iteration 146/1000 | Loss: 0.00003542
Iteration 147/1000 | Loss: 0.00003542
Iteration 148/1000 | Loss: 0.00003542
Iteration 149/1000 | Loss: 0.00003541
Iteration 150/1000 | Loss: 0.00003541
Iteration 151/1000 | Loss: 0.00003541
Iteration 152/1000 | Loss: 0.00003541
Iteration 153/1000 | Loss: 0.00003541
Iteration 154/1000 | Loss: 0.00003541
Iteration 155/1000 | Loss: 0.00003541
Iteration 156/1000 | Loss: 0.00003541
Iteration 157/1000 | Loss: 0.00003541
Iteration 158/1000 | Loss: 0.00003541
Iteration 159/1000 | Loss: 0.00003541
Iteration 160/1000 | Loss: 0.00003541
Iteration 161/1000 | Loss: 0.00003541
Iteration 162/1000 | Loss: 0.00003541
Iteration 163/1000 | Loss: 0.00003541
Iteration 164/1000 | Loss: 0.00003541
Iteration 165/1000 | Loss: 0.00003541
Iteration 166/1000 | Loss: 0.00003541
Iteration 167/1000 | Loss: 0.00003541
Iteration 168/1000 | Loss: 0.00003541
Iteration 169/1000 | Loss: 0.00003541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.541465412126854e-05, 3.541465412126854e-05, 3.541465412126854e-05, 3.541465412126854e-05, 3.541465412126854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.541465412126854e-05

Optimization complete. Final v2v error: 4.165943145751953 mm

Highest mean error: 12.495104789733887 mm for frame 40

Lowest mean error: 3.271212339401245 mm for frame 63

Saving results

Total time: 182.41630697250366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786925
Iteration 2/25 | Loss: 0.00175439
Iteration 3/25 | Loss: 0.00134571
Iteration 4/25 | Loss: 0.00127595
Iteration 5/25 | Loss: 0.00126422
Iteration 6/25 | Loss: 0.00126157
Iteration 7/25 | Loss: 0.00126094
Iteration 8/25 | Loss: 0.00126094
Iteration 9/25 | Loss: 0.00126094
Iteration 10/25 | Loss: 0.00126094
Iteration 11/25 | Loss: 0.00126094
Iteration 12/25 | Loss: 0.00126094
Iteration 13/25 | Loss: 0.00126094
Iteration 14/25 | Loss: 0.00126094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012609435943886638, 0.0012609435943886638, 0.0012609435943886638, 0.0012609435943886638, 0.0012609435943886638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012609435943886638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41330123
Iteration 2/25 | Loss: 0.00067343
Iteration 3/25 | Loss: 0.00067343
Iteration 4/25 | Loss: 0.00067343
Iteration 5/25 | Loss: 0.00067343
Iteration 6/25 | Loss: 0.00067343
Iteration 7/25 | Loss: 0.00067343
Iteration 8/25 | Loss: 0.00067343
Iteration 9/25 | Loss: 0.00067343
Iteration 10/25 | Loss: 0.00067343
Iteration 11/25 | Loss: 0.00067343
Iteration 12/25 | Loss: 0.00067343
Iteration 13/25 | Loss: 0.00067343
Iteration 14/25 | Loss: 0.00067343
Iteration 15/25 | Loss: 0.00067343
Iteration 16/25 | Loss: 0.00067343
Iteration 17/25 | Loss: 0.00067343
Iteration 18/25 | Loss: 0.00067343
Iteration 19/25 | Loss: 0.00067343
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006734257913194597, 0.0006734257913194597, 0.0006734257913194597, 0.0006734257913194597, 0.0006734257913194597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006734257913194597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067343
Iteration 2/1000 | Loss: 0.00003556
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00002226
Iteration 5/1000 | Loss: 0.00002933
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00002011
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00001877
Iteration 10/1000 | Loss: 0.00001839
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001747
Iteration 15/1000 | Loss: 0.00001732
Iteration 16/1000 | Loss: 0.00001725
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001716
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001647
Iteration 23/1000 | Loss: 0.00001645
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001640
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001639
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001622
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001620
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001619
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001618
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001617
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001616
Iteration 78/1000 | Loss: 0.00001616
Iteration 79/1000 | Loss: 0.00001616
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001614
Iteration 92/1000 | Loss: 0.00001614
Iteration 93/1000 | Loss: 0.00001614
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001614
Iteration 96/1000 | Loss: 0.00001614
Iteration 97/1000 | Loss: 0.00001614
Iteration 98/1000 | Loss: 0.00001614
Iteration 99/1000 | Loss: 0.00001614
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001613
Iteration 111/1000 | Loss: 0.00001613
Iteration 112/1000 | Loss: 0.00001613
Iteration 113/1000 | Loss: 0.00001613
Iteration 114/1000 | Loss: 0.00001613
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Iteration 120/1000 | Loss: 0.00001612
Iteration 121/1000 | Loss: 0.00001612
Iteration 122/1000 | Loss: 0.00001612
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001612
Iteration 125/1000 | Loss: 0.00001612
Iteration 126/1000 | Loss: 0.00001612
Iteration 127/1000 | Loss: 0.00001612
Iteration 128/1000 | Loss: 0.00001612
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001612
Iteration 132/1000 | Loss: 0.00001612
Iteration 133/1000 | Loss: 0.00001612
Iteration 134/1000 | Loss: 0.00001612
Iteration 135/1000 | Loss: 0.00001612
Iteration 136/1000 | Loss: 0.00001612
Iteration 137/1000 | Loss: 0.00001612
Iteration 138/1000 | Loss: 0.00001612
Iteration 139/1000 | Loss: 0.00001612
Iteration 140/1000 | Loss: 0.00001612
Iteration 141/1000 | Loss: 0.00001612
Iteration 142/1000 | Loss: 0.00001612
Iteration 143/1000 | Loss: 0.00001612
Iteration 144/1000 | Loss: 0.00001612
Iteration 145/1000 | Loss: 0.00001612
Iteration 146/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.6116839105961844e-05, 1.6116839105961844e-05, 1.6116839105961844e-05, 1.6116839105961844e-05, 1.6116839105961844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6116839105961844e-05

Optimization complete. Final v2v error: 3.4188947677612305 mm

Highest mean error: 4.1920166015625 mm for frame 56

Lowest mean error: 2.9497294425964355 mm for frame 3

Saving results

Total time: 53.92416524887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703143
Iteration 2/25 | Loss: 0.00137025
Iteration 3/25 | Loss: 0.00127145
Iteration 4/25 | Loss: 0.00123949
Iteration 5/25 | Loss: 0.00123066
Iteration 6/25 | Loss: 0.00122760
Iteration 7/25 | Loss: 0.00122914
Iteration 8/25 | Loss: 0.00122681
Iteration 9/25 | Loss: 0.00122647
Iteration 10/25 | Loss: 0.00122626
Iteration 11/25 | Loss: 0.00122600
Iteration 12/25 | Loss: 0.00122586
Iteration 13/25 | Loss: 0.00122582
Iteration 14/25 | Loss: 0.00122581
Iteration 15/25 | Loss: 0.00122581
Iteration 16/25 | Loss: 0.00122581
Iteration 17/25 | Loss: 0.00122581
Iteration 18/25 | Loss: 0.00122581
Iteration 19/25 | Loss: 0.00122581
Iteration 20/25 | Loss: 0.00122580
Iteration 21/25 | Loss: 0.00122580
Iteration 22/25 | Loss: 0.00122580
Iteration 23/25 | Loss: 0.00122580
Iteration 24/25 | Loss: 0.00122580
Iteration 25/25 | Loss: 0.00122580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64478731
Iteration 2/25 | Loss: 0.00079538
Iteration 3/25 | Loss: 0.00079538
Iteration 4/25 | Loss: 0.00077499
Iteration 5/25 | Loss: 0.00077499
Iteration 6/25 | Loss: 0.00077499
Iteration 7/25 | Loss: 0.00077499
Iteration 8/25 | Loss: 0.00077499
Iteration 9/25 | Loss: 0.00077499
Iteration 10/25 | Loss: 0.00077499
Iteration 11/25 | Loss: 0.00077499
Iteration 12/25 | Loss: 0.00077499
Iteration 13/25 | Loss: 0.00077499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007749855285510421, 0.0007749855285510421, 0.0007749855285510421, 0.0007749855285510421, 0.0007749855285510421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007749855285510421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077499
Iteration 2/1000 | Loss: 0.00002311
Iteration 3/1000 | Loss: 0.00004418
Iteration 4/1000 | Loss: 0.00001825
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001620
Iteration 8/1000 | Loss: 0.00029325
Iteration 9/1000 | Loss: 0.00005268
Iteration 10/1000 | Loss: 0.00005571
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001483
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00003615
Iteration 16/1000 | Loss: 0.00003316
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00003668
Iteration 21/1000 | Loss: 0.00001648
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001408
Iteration 24/1000 | Loss: 0.00003121
Iteration 25/1000 | Loss: 0.00008080
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00003077
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001488
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001389
Iteration 36/1000 | Loss: 0.00001389
Iteration 37/1000 | Loss: 0.00001389
Iteration 38/1000 | Loss: 0.00001388
Iteration 39/1000 | Loss: 0.00001388
Iteration 40/1000 | Loss: 0.00001388
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001386
Iteration 44/1000 | Loss: 0.00001386
Iteration 45/1000 | Loss: 0.00001385
Iteration 46/1000 | Loss: 0.00001385
Iteration 47/1000 | Loss: 0.00001384
Iteration 48/1000 | Loss: 0.00001639
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001377
Iteration 58/1000 | Loss: 0.00001377
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001369
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001366
Iteration 78/1000 | Loss: 0.00001366
Iteration 79/1000 | Loss: 0.00001365
Iteration 80/1000 | Loss: 0.00001365
Iteration 81/1000 | Loss: 0.00001365
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001364
Iteration 87/1000 | Loss: 0.00001364
Iteration 88/1000 | Loss: 0.00001364
Iteration 89/1000 | Loss: 0.00001364
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001364
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001360
Iteration 116/1000 | Loss: 0.00001360
Iteration 117/1000 | Loss: 0.00001359
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001358
Iteration 122/1000 | Loss: 0.00001358
Iteration 123/1000 | Loss: 0.00001358
Iteration 124/1000 | Loss: 0.00001358
Iteration 125/1000 | Loss: 0.00001357
Iteration 126/1000 | Loss: 0.00001357
Iteration 127/1000 | Loss: 0.00001357
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001356
Iteration 131/1000 | Loss: 0.00001356
Iteration 132/1000 | Loss: 0.00001356
Iteration 133/1000 | Loss: 0.00001356
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001355
Iteration 139/1000 | Loss: 0.00001355
Iteration 140/1000 | Loss: 0.00001355
Iteration 141/1000 | Loss: 0.00001355
Iteration 142/1000 | Loss: 0.00001355
Iteration 143/1000 | Loss: 0.00001355
Iteration 144/1000 | Loss: 0.00001355
Iteration 145/1000 | Loss: 0.00001355
Iteration 146/1000 | Loss: 0.00001355
Iteration 147/1000 | Loss: 0.00001355
Iteration 148/1000 | Loss: 0.00001355
Iteration 149/1000 | Loss: 0.00001354
Iteration 150/1000 | Loss: 0.00001354
Iteration 151/1000 | Loss: 0.00001354
Iteration 152/1000 | Loss: 0.00001354
Iteration 153/1000 | Loss: 0.00001354
Iteration 154/1000 | Loss: 0.00001354
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001354
Iteration 157/1000 | Loss: 0.00001354
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001353
Iteration 163/1000 | Loss: 0.00001353
Iteration 164/1000 | Loss: 0.00001353
Iteration 165/1000 | Loss: 0.00001353
Iteration 166/1000 | Loss: 0.00001353
Iteration 167/1000 | Loss: 0.00001353
Iteration 168/1000 | Loss: 0.00001353
Iteration 169/1000 | Loss: 0.00001353
Iteration 170/1000 | Loss: 0.00001353
Iteration 171/1000 | Loss: 0.00001353
Iteration 172/1000 | Loss: 0.00001353
Iteration 173/1000 | Loss: 0.00001353
Iteration 174/1000 | Loss: 0.00001353
Iteration 175/1000 | Loss: 0.00001353
Iteration 176/1000 | Loss: 0.00001353
Iteration 177/1000 | Loss: 0.00001353
Iteration 178/1000 | Loss: 0.00001353
Iteration 179/1000 | Loss: 0.00001353
Iteration 180/1000 | Loss: 0.00001353
Iteration 181/1000 | Loss: 0.00001352
Iteration 182/1000 | Loss: 0.00001352
Iteration 183/1000 | Loss: 0.00001352
Iteration 184/1000 | Loss: 0.00001352
Iteration 185/1000 | Loss: 0.00001352
Iteration 186/1000 | Loss: 0.00001352
Iteration 187/1000 | Loss: 0.00001352
Iteration 188/1000 | Loss: 0.00001352
Iteration 189/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3523796951631084e-05, 1.3523796951631084e-05, 1.3523796951631084e-05, 1.3523796951631084e-05, 1.3523796951631084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3523796951631084e-05

Optimization complete. Final v2v error: 3.137824058532715 mm

Highest mean error: 3.9506871700286865 mm for frame 140

Lowest mean error: 2.9045448303222656 mm for frame 19

Saving results

Total time: 84.03289771080017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431642
Iteration 2/25 | Loss: 0.00133924
Iteration 3/25 | Loss: 0.00127667
Iteration 4/25 | Loss: 0.00127033
Iteration 5/25 | Loss: 0.00126815
Iteration 6/25 | Loss: 0.00126796
Iteration 7/25 | Loss: 0.00126796
Iteration 8/25 | Loss: 0.00126796
Iteration 9/25 | Loss: 0.00126796
Iteration 10/25 | Loss: 0.00126796
Iteration 11/25 | Loss: 0.00126796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012679587816819549, 0.0012679587816819549, 0.0012679587816819549, 0.0012679587816819549, 0.0012679587816819549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012679587816819549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43604743
Iteration 2/25 | Loss: 0.00095174
Iteration 3/25 | Loss: 0.00095174
Iteration 4/25 | Loss: 0.00095174
Iteration 5/25 | Loss: 0.00095174
Iteration 6/25 | Loss: 0.00095174
Iteration 7/25 | Loss: 0.00095174
Iteration 8/25 | Loss: 0.00095174
Iteration 9/25 | Loss: 0.00095174
Iteration 10/25 | Loss: 0.00095174
Iteration 11/25 | Loss: 0.00095174
Iteration 12/25 | Loss: 0.00095174
Iteration 13/25 | Loss: 0.00095174
Iteration 14/25 | Loss: 0.00095174
Iteration 15/25 | Loss: 0.00095174
Iteration 16/25 | Loss: 0.00095174
Iteration 17/25 | Loss: 0.00095174
Iteration 18/25 | Loss: 0.00095174
Iteration 19/25 | Loss: 0.00095174
Iteration 20/25 | Loss: 0.00095174
Iteration 21/25 | Loss: 0.00095174
Iteration 22/25 | Loss: 0.00095174
Iteration 23/25 | Loss: 0.00095174
Iteration 24/25 | Loss: 0.00095174
Iteration 25/25 | Loss: 0.00095174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095174
Iteration 2/1000 | Loss: 0.00004242
Iteration 3/1000 | Loss: 0.00002181
Iteration 4/1000 | Loss: 0.00001859
Iteration 5/1000 | Loss: 0.00001754
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001662
Iteration 8/1000 | Loss: 0.00001637
Iteration 9/1000 | Loss: 0.00001617
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001602
Iteration 13/1000 | Loss: 0.00001591
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001575
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001568
Iteration 19/1000 | Loss: 0.00001568
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001564
Iteration 26/1000 | Loss: 0.00001564
Iteration 27/1000 | Loss: 0.00001564
Iteration 28/1000 | Loss: 0.00001563
Iteration 29/1000 | Loss: 0.00001563
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001558
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001556
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001555
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001554
Iteration 39/1000 | Loss: 0.00001553
Iteration 40/1000 | Loss: 0.00001553
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001552
Iteration 54/1000 | Loss: 0.00001552
Iteration 55/1000 | Loss: 0.00001552
Iteration 56/1000 | Loss: 0.00001552
Iteration 57/1000 | Loss: 0.00001552
Iteration 58/1000 | Loss: 0.00001552
Iteration 59/1000 | Loss: 0.00001552
Iteration 60/1000 | Loss: 0.00001552
Iteration 61/1000 | Loss: 0.00001552
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001552
Iteration 64/1000 | Loss: 0.00001552
Iteration 65/1000 | Loss: 0.00001552
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.55175930558471e-05, 1.55175930558471e-05, 1.55175930558471e-05, 1.55175930558471e-05, 1.55175930558471e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.55175930558471e-05

Optimization complete. Final v2v error: 3.4052906036376953 mm

Highest mean error: 3.6340229511260986 mm for frame 29

Lowest mean error: 3.245835542678833 mm for frame 57

Saving results

Total time: 28.776612997055054
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499844
Iteration 2/25 | Loss: 0.00133314
Iteration 3/25 | Loss: 0.00124671
Iteration 4/25 | Loss: 0.00123162
Iteration 5/25 | Loss: 0.00122644
Iteration 6/25 | Loss: 0.00122502
Iteration 7/25 | Loss: 0.00122486
Iteration 8/25 | Loss: 0.00122486
Iteration 9/25 | Loss: 0.00122486
Iteration 10/25 | Loss: 0.00122486
Iteration 11/25 | Loss: 0.00122486
Iteration 12/25 | Loss: 0.00122486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012248599668964744, 0.0012248599668964744, 0.0012248599668964744, 0.0012248599668964744, 0.0012248599668964744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012248599668964744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.56217003
Iteration 2/25 | Loss: 0.00076733
Iteration 3/25 | Loss: 0.00076730
Iteration 4/25 | Loss: 0.00076729
Iteration 5/25 | Loss: 0.00076729
Iteration 6/25 | Loss: 0.00076729
Iteration 7/25 | Loss: 0.00076729
Iteration 8/25 | Loss: 0.00076729
Iteration 9/25 | Loss: 0.00076729
Iteration 10/25 | Loss: 0.00076729
Iteration 11/25 | Loss: 0.00076729
Iteration 12/25 | Loss: 0.00076729
Iteration 13/25 | Loss: 0.00076729
Iteration 14/25 | Loss: 0.00076729
Iteration 15/25 | Loss: 0.00076729
Iteration 16/25 | Loss: 0.00076729
Iteration 17/25 | Loss: 0.00076729
Iteration 18/25 | Loss: 0.00076729
Iteration 19/25 | Loss: 0.00076729
Iteration 20/25 | Loss: 0.00076729
Iteration 21/25 | Loss: 0.00076729
Iteration 22/25 | Loss: 0.00076729
Iteration 23/25 | Loss: 0.00076729
Iteration 24/25 | Loss: 0.00076729
Iteration 25/25 | Loss: 0.00076729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076729
Iteration 2/1000 | Loss: 0.00003326
Iteration 3/1000 | Loss: 0.00002116
Iteration 4/1000 | Loss: 0.00001830
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001655
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001540
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001454
Iteration 12/1000 | Loss: 0.00001449
Iteration 13/1000 | Loss: 0.00001431
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001402
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001400
Iteration 26/1000 | Loss: 0.00001400
Iteration 27/1000 | Loss: 0.00001400
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001393
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001387
Iteration 42/1000 | Loss: 0.00001387
Iteration 43/1000 | Loss: 0.00001385
Iteration 44/1000 | Loss: 0.00001385
Iteration 45/1000 | Loss: 0.00001384
Iteration 46/1000 | Loss: 0.00001384
Iteration 47/1000 | Loss: 0.00001383
Iteration 48/1000 | Loss: 0.00001382
Iteration 49/1000 | Loss: 0.00001382
Iteration 50/1000 | Loss: 0.00001382
Iteration 51/1000 | Loss: 0.00001382
Iteration 52/1000 | Loss: 0.00001381
Iteration 53/1000 | Loss: 0.00001381
Iteration 54/1000 | Loss: 0.00001381
Iteration 55/1000 | Loss: 0.00001380
Iteration 56/1000 | Loss: 0.00001380
Iteration 57/1000 | Loss: 0.00001380
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001379
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001378
Iteration 62/1000 | Loss: 0.00001378
Iteration 63/1000 | Loss: 0.00001378
Iteration 64/1000 | Loss: 0.00001378
Iteration 65/1000 | Loss: 0.00001378
Iteration 66/1000 | Loss: 0.00001378
Iteration 67/1000 | Loss: 0.00001377
Iteration 68/1000 | Loss: 0.00001377
Iteration 69/1000 | Loss: 0.00001377
Iteration 70/1000 | Loss: 0.00001376
Iteration 71/1000 | Loss: 0.00001376
Iteration 72/1000 | Loss: 0.00001376
Iteration 73/1000 | Loss: 0.00001376
Iteration 74/1000 | Loss: 0.00001376
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001374
Iteration 82/1000 | Loss: 0.00001374
Iteration 83/1000 | Loss: 0.00001374
Iteration 84/1000 | Loss: 0.00001374
Iteration 85/1000 | Loss: 0.00001374
Iteration 86/1000 | Loss: 0.00001374
Iteration 87/1000 | Loss: 0.00001373
Iteration 88/1000 | Loss: 0.00001373
Iteration 89/1000 | Loss: 0.00001373
Iteration 90/1000 | Loss: 0.00001373
Iteration 91/1000 | Loss: 0.00001372
Iteration 92/1000 | Loss: 0.00001372
Iteration 93/1000 | Loss: 0.00001372
Iteration 94/1000 | Loss: 0.00001372
Iteration 95/1000 | Loss: 0.00001372
Iteration 96/1000 | Loss: 0.00001372
Iteration 97/1000 | Loss: 0.00001372
Iteration 98/1000 | Loss: 0.00001372
Iteration 99/1000 | Loss: 0.00001372
Iteration 100/1000 | Loss: 0.00001372
Iteration 101/1000 | Loss: 0.00001372
Iteration 102/1000 | Loss: 0.00001372
Iteration 103/1000 | Loss: 0.00001372
Iteration 104/1000 | Loss: 0.00001372
Iteration 105/1000 | Loss: 0.00001372
Iteration 106/1000 | Loss: 0.00001372
Iteration 107/1000 | Loss: 0.00001372
Iteration 108/1000 | Loss: 0.00001372
Iteration 109/1000 | Loss: 0.00001372
Iteration 110/1000 | Loss: 0.00001372
Iteration 111/1000 | Loss: 0.00001372
Iteration 112/1000 | Loss: 0.00001372
Iteration 113/1000 | Loss: 0.00001372
Iteration 114/1000 | Loss: 0.00001372
Iteration 115/1000 | Loss: 0.00001372
Iteration 116/1000 | Loss: 0.00001372
Iteration 117/1000 | Loss: 0.00001372
Iteration 118/1000 | Loss: 0.00001372
Iteration 119/1000 | Loss: 0.00001372
Iteration 120/1000 | Loss: 0.00001372
Iteration 121/1000 | Loss: 0.00001372
Iteration 122/1000 | Loss: 0.00001372
Iteration 123/1000 | Loss: 0.00001372
Iteration 124/1000 | Loss: 0.00001372
Iteration 125/1000 | Loss: 0.00001372
Iteration 126/1000 | Loss: 0.00001372
Iteration 127/1000 | Loss: 0.00001372
Iteration 128/1000 | Loss: 0.00001372
Iteration 129/1000 | Loss: 0.00001372
Iteration 130/1000 | Loss: 0.00001372
Iteration 131/1000 | Loss: 0.00001372
Iteration 132/1000 | Loss: 0.00001372
Iteration 133/1000 | Loss: 0.00001372
Iteration 134/1000 | Loss: 0.00001372
Iteration 135/1000 | Loss: 0.00001372
Iteration 136/1000 | Loss: 0.00001372
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001372
Iteration 141/1000 | Loss: 0.00001372
Iteration 142/1000 | Loss: 0.00001372
Iteration 143/1000 | Loss: 0.00001372
Iteration 144/1000 | Loss: 0.00001372
Iteration 145/1000 | Loss: 0.00001372
Iteration 146/1000 | Loss: 0.00001372
Iteration 147/1000 | Loss: 0.00001372
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3715120985580143e-05, 1.3715120985580143e-05, 1.3715120985580143e-05, 1.3715120985580143e-05, 1.3715120985580143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3715120985580143e-05

Optimization complete. Final v2v error: 3.157693862915039 mm

Highest mean error: 3.540698289871216 mm for frame 111

Lowest mean error: 2.730726957321167 mm for frame 147

Saving results

Total time: 39.54471802711487
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852101
Iteration 2/25 | Loss: 0.00164256
Iteration 3/25 | Loss: 0.00139504
Iteration 4/25 | Loss: 0.00131464
Iteration 5/25 | Loss: 0.00131427
Iteration 6/25 | Loss: 0.00128769
Iteration 7/25 | Loss: 0.00126425
Iteration 8/25 | Loss: 0.00125462
Iteration 9/25 | Loss: 0.00124223
Iteration 10/25 | Loss: 0.00124112
Iteration 11/25 | Loss: 0.00124075
Iteration 12/25 | Loss: 0.00124056
Iteration 13/25 | Loss: 0.00124055
Iteration 14/25 | Loss: 0.00124055
Iteration 15/25 | Loss: 0.00124055
Iteration 16/25 | Loss: 0.00124055
Iteration 17/25 | Loss: 0.00124055
Iteration 18/25 | Loss: 0.00124055
Iteration 19/25 | Loss: 0.00124055
Iteration 20/25 | Loss: 0.00124055
Iteration 21/25 | Loss: 0.00124055
Iteration 22/25 | Loss: 0.00124055
Iteration 23/25 | Loss: 0.00124055
Iteration 24/25 | Loss: 0.00124055
Iteration 25/25 | Loss: 0.00124055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.93933678
Iteration 2/25 | Loss: 0.00092430
Iteration 3/25 | Loss: 0.00092196
Iteration 4/25 | Loss: 0.00092196
Iteration 5/25 | Loss: 0.00092196
Iteration 6/25 | Loss: 0.00092196
Iteration 7/25 | Loss: 0.00092196
Iteration 8/25 | Loss: 0.00092196
Iteration 9/25 | Loss: 0.00092196
Iteration 10/25 | Loss: 0.00092196
Iteration 11/25 | Loss: 0.00092196
Iteration 12/25 | Loss: 0.00092196
Iteration 13/25 | Loss: 0.00092196
Iteration 14/25 | Loss: 0.00092196
Iteration 15/25 | Loss: 0.00092196
Iteration 16/25 | Loss: 0.00092196
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009219557978212833, 0.0009219557978212833, 0.0009219557978212833, 0.0009219557978212833, 0.0009219557978212833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009219557978212833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092196
Iteration 2/1000 | Loss: 0.00003026
Iteration 3/1000 | Loss: 0.00001842
Iteration 4/1000 | Loss: 0.00003571
Iteration 5/1000 | Loss: 0.00004143
Iteration 6/1000 | Loss: 0.00001622
Iteration 7/1000 | Loss: 0.00003167
Iteration 8/1000 | Loss: 0.00001566
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001486
Iteration 21/1000 | Loss: 0.00001485
Iteration 22/1000 | Loss: 0.00002661
Iteration 23/1000 | Loss: 0.00001727
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001473
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001472
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001931
Iteration 46/1000 | Loss: 0.00001798
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001460
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001460
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001460
Iteration 59/1000 | Loss: 0.00001460
Iteration 60/1000 | Loss: 0.00001460
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001462
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00002655
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001469
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001444
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001444
Iteration 120/1000 | Loss: 0.00001444
Iteration 121/1000 | Loss: 0.00001444
Iteration 122/1000 | Loss: 0.00001444
Iteration 123/1000 | Loss: 0.00001444
Iteration 124/1000 | Loss: 0.00001444
Iteration 125/1000 | Loss: 0.00001444
Iteration 126/1000 | Loss: 0.00001444
Iteration 127/1000 | Loss: 0.00001444
Iteration 128/1000 | Loss: 0.00001444
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001444
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4437315257964656e-05, 1.4437315257964656e-05, 1.4437315257964656e-05, 1.4437315257964656e-05, 1.4437315257964656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4437315257964656e-05

Optimization complete. Final v2v error: 3.2040157318115234 mm

Highest mean error: 3.5879199504852295 mm for frame 189

Lowest mean error: 2.9172236919403076 mm for frame 132

Saving results

Total time: 63.67113971710205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993039
Iteration 2/25 | Loss: 0.00287408
Iteration 3/25 | Loss: 0.00200483
Iteration 4/25 | Loss: 0.00192755
Iteration 5/25 | Loss: 0.00186804
Iteration 6/25 | Loss: 0.00187939
Iteration 7/25 | Loss: 0.00163558
Iteration 8/25 | Loss: 0.00151594
Iteration 9/25 | Loss: 0.00146798
Iteration 10/25 | Loss: 0.00143646
Iteration 11/25 | Loss: 0.00142709
Iteration 12/25 | Loss: 0.00141365
Iteration 13/25 | Loss: 0.00140177
Iteration 14/25 | Loss: 0.00139233
Iteration 15/25 | Loss: 0.00139158
Iteration 16/25 | Loss: 0.00139097
Iteration 17/25 | Loss: 0.00138882
Iteration 18/25 | Loss: 0.00139127
Iteration 19/25 | Loss: 0.00138916
Iteration 20/25 | Loss: 0.00138749
Iteration 21/25 | Loss: 0.00138839
Iteration 22/25 | Loss: 0.00138730
Iteration 23/25 | Loss: 0.00138663
Iteration 24/25 | Loss: 0.00138457
Iteration 25/25 | Loss: 0.00138568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43114686
Iteration 2/25 | Loss: 0.00260656
Iteration 3/25 | Loss: 0.00257415
Iteration 4/25 | Loss: 0.00257415
Iteration 5/25 | Loss: 0.00257415
Iteration 6/25 | Loss: 0.00257415
Iteration 7/25 | Loss: 0.00257415
Iteration 8/25 | Loss: 0.00257415
Iteration 9/25 | Loss: 0.00257415
Iteration 10/25 | Loss: 0.00257415
Iteration 11/25 | Loss: 0.00257415
Iteration 12/25 | Loss: 0.00257415
Iteration 13/25 | Loss: 0.00257415
Iteration 14/25 | Loss: 0.00257415
Iteration 15/25 | Loss: 0.00257415
Iteration 16/25 | Loss: 0.00257415
Iteration 17/25 | Loss: 0.00257415
Iteration 18/25 | Loss: 0.00257415
Iteration 19/25 | Loss: 0.00257415
Iteration 20/25 | Loss: 0.00257415
Iteration 21/25 | Loss: 0.00257415
Iteration 22/25 | Loss: 0.00257415
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0025741460267454386, 0.0025741460267454386, 0.0025741460267454386, 0.0025741460267454386, 0.0025741460267454386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025741460267454386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00257415
Iteration 2/1000 | Loss: 0.00042880
Iteration 3/1000 | Loss: 0.00030041
Iteration 4/1000 | Loss: 0.00017469
Iteration 5/1000 | Loss: 0.00022040
Iteration 6/1000 | Loss: 0.00020771
Iteration 7/1000 | Loss: 0.00012566
Iteration 8/1000 | Loss: 0.00018970
Iteration 9/1000 | Loss: 0.00010579
Iteration 10/1000 | Loss: 0.00009185
Iteration 11/1000 | Loss: 0.00024973
Iteration 12/1000 | Loss: 0.00010494
Iteration 13/1000 | Loss: 0.00008590
Iteration 14/1000 | Loss: 0.00009130
Iteration 15/1000 | Loss: 0.00026502
Iteration 16/1000 | Loss: 0.00121121
Iteration 17/1000 | Loss: 0.00319345
Iteration 18/1000 | Loss: 0.00123643
Iteration 19/1000 | Loss: 0.00042385
Iteration 20/1000 | Loss: 0.00053850
Iteration 21/1000 | Loss: 0.00016855
Iteration 22/1000 | Loss: 0.00027391
Iteration 23/1000 | Loss: 0.00016623
Iteration 24/1000 | Loss: 0.00022492
Iteration 25/1000 | Loss: 0.00021171
Iteration 26/1000 | Loss: 0.00017709
Iteration 27/1000 | Loss: 0.00014971
Iteration 28/1000 | Loss: 0.00013392
Iteration 29/1000 | Loss: 0.00014455
Iteration 30/1000 | Loss: 0.00011812
Iteration 31/1000 | Loss: 0.00009979
Iteration 32/1000 | Loss: 0.00011900
Iteration 33/1000 | Loss: 0.00014713
Iteration 34/1000 | Loss: 0.00013673
Iteration 35/1000 | Loss: 0.00005023
Iteration 36/1000 | Loss: 0.00008983
Iteration 37/1000 | Loss: 0.00003405
Iteration 38/1000 | Loss: 0.00005132
Iteration 39/1000 | Loss: 0.00021038
Iteration 40/1000 | Loss: 0.00005325
Iteration 41/1000 | Loss: 0.00004490
Iteration 42/1000 | Loss: 0.00004536
Iteration 43/1000 | Loss: 0.00003581
Iteration 44/1000 | Loss: 0.00004186
Iteration 45/1000 | Loss: 0.00005010
Iteration 46/1000 | Loss: 0.00006876
Iteration 47/1000 | Loss: 0.00004069
Iteration 48/1000 | Loss: 0.00005826
Iteration 49/1000 | Loss: 0.00006756
Iteration 50/1000 | Loss: 0.00004872
Iteration 51/1000 | Loss: 0.00004361
Iteration 52/1000 | Loss: 0.00004065
Iteration 53/1000 | Loss: 0.00004317
Iteration 54/1000 | Loss: 0.00005283
Iteration 55/1000 | Loss: 0.00006925
Iteration 56/1000 | Loss: 0.00004622
Iteration 57/1000 | Loss: 0.00006163
Iteration 58/1000 | Loss: 0.00015085
Iteration 59/1000 | Loss: 0.00029116
Iteration 60/1000 | Loss: 0.00019749
Iteration 61/1000 | Loss: 0.00004030
Iteration 62/1000 | Loss: 0.00002557
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002395
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001892
Iteration 71/1000 | Loss: 0.00003495
Iteration 72/1000 | Loss: 0.00001978
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001845
Iteration 85/1000 | Loss: 0.00001845
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001844
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001842
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00001862
Iteration 93/1000 | Loss: 0.00001830
Iteration 94/1000 | Loss: 0.00001830
Iteration 95/1000 | Loss: 0.00001830
Iteration 96/1000 | Loss: 0.00001830
Iteration 97/1000 | Loss: 0.00001830
Iteration 98/1000 | Loss: 0.00001830
Iteration 99/1000 | Loss: 0.00001830
Iteration 100/1000 | Loss: 0.00001830
Iteration 101/1000 | Loss: 0.00001830
Iteration 102/1000 | Loss: 0.00001830
Iteration 103/1000 | Loss: 0.00001830
Iteration 104/1000 | Loss: 0.00001830
Iteration 105/1000 | Loss: 0.00001830
Iteration 106/1000 | Loss: 0.00001830
Iteration 107/1000 | Loss: 0.00001830
Iteration 108/1000 | Loss: 0.00001830
Iteration 109/1000 | Loss: 0.00001830
Iteration 110/1000 | Loss: 0.00001830
Iteration 111/1000 | Loss: 0.00001830
Iteration 112/1000 | Loss: 0.00001830
Iteration 113/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.8302946045878343e-05, 1.8302946045878343e-05, 1.8302946045878343e-05, 1.8302946045878343e-05, 1.8302946045878343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8302946045878343e-05

Optimization complete. Final v2v error: 3.0761022567749023 mm

Highest mean error: 10.559099197387695 mm for frame 73

Lowest mean error: 2.6898722648620605 mm for frame 232

Saving results

Total time: 173.63605284690857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481851
Iteration 2/25 | Loss: 0.00135551
Iteration 3/25 | Loss: 0.00125836
Iteration 4/25 | Loss: 0.00124549
Iteration 5/25 | Loss: 0.00124280
Iteration 6/25 | Loss: 0.00124280
Iteration 7/25 | Loss: 0.00124280
Iteration 8/25 | Loss: 0.00124280
Iteration 9/25 | Loss: 0.00124280
Iteration 10/25 | Loss: 0.00124280
Iteration 11/25 | Loss: 0.00124280
Iteration 12/25 | Loss: 0.00124280
Iteration 13/25 | Loss: 0.00124280
Iteration 14/25 | Loss: 0.00124280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012428027112036943, 0.0012428027112036943, 0.0012428027112036943, 0.0012428027112036943, 0.0012428027112036943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012428027112036943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09139013
Iteration 2/25 | Loss: 0.00082360
Iteration 3/25 | Loss: 0.00082359
Iteration 4/25 | Loss: 0.00082359
Iteration 5/25 | Loss: 0.00082359
Iteration 6/25 | Loss: 0.00082359
Iteration 7/25 | Loss: 0.00082359
Iteration 8/25 | Loss: 0.00082359
Iteration 9/25 | Loss: 0.00082359
Iteration 10/25 | Loss: 0.00082359
Iteration 11/25 | Loss: 0.00082359
Iteration 12/25 | Loss: 0.00082359
Iteration 13/25 | Loss: 0.00082359
Iteration 14/25 | Loss: 0.00082359
Iteration 15/25 | Loss: 0.00082359
Iteration 16/25 | Loss: 0.00082359
Iteration 17/25 | Loss: 0.00082359
Iteration 18/25 | Loss: 0.00082359
Iteration 19/25 | Loss: 0.00082359
Iteration 20/25 | Loss: 0.00082359
Iteration 21/25 | Loss: 0.00082359
Iteration 22/25 | Loss: 0.00082359
Iteration 23/25 | Loss: 0.00082359
Iteration 24/25 | Loss: 0.00082359
Iteration 25/25 | Loss: 0.00082359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082359
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001949
Iteration 4/1000 | Loss: 0.00001838
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001602
Iteration 9/1000 | Loss: 0.00001573
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001535
Iteration 12/1000 | Loss: 0.00001528
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001517
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001516
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001506
Iteration 22/1000 | Loss: 0.00001493
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001475
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001474
Iteration 37/1000 | Loss: 0.00001474
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001473
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001472
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001471
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001458
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001452
Iteration 82/1000 | Loss: 0.00001452
Iteration 83/1000 | Loss: 0.00001452
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001451
Iteration 87/1000 | Loss: 0.00001451
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001450
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001446
Iteration 104/1000 | Loss: 0.00001446
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001445
Iteration 108/1000 | Loss: 0.00001445
Iteration 109/1000 | Loss: 0.00001445
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001444
Iteration 118/1000 | Loss: 0.00001444
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001443
Iteration 121/1000 | Loss: 0.00001443
Iteration 122/1000 | Loss: 0.00001443
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001442
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001442
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Iteration 135/1000 | Loss: 0.00001442
Iteration 136/1000 | Loss: 0.00001442
Iteration 137/1000 | Loss: 0.00001442
Iteration 138/1000 | Loss: 0.00001442
Iteration 139/1000 | Loss: 0.00001442
Iteration 140/1000 | Loss: 0.00001442
Iteration 141/1000 | Loss: 0.00001442
Iteration 142/1000 | Loss: 0.00001442
Iteration 143/1000 | Loss: 0.00001442
Iteration 144/1000 | Loss: 0.00001442
Iteration 145/1000 | Loss: 0.00001442
Iteration 146/1000 | Loss: 0.00001442
Iteration 147/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.4419510080188047e-05, 1.4419510080188047e-05, 1.4419510080188047e-05, 1.4419510080188047e-05, 1.4419510080188047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4419510080188047e-05

Optimization complete. Final v2v error: 3.2364635467529297 mm

Highest mean error: 3.5293662548065186 mm for frame 212

Lowest mean error: 3.089552879333496 mm for frame 47

Saving results

Total time: 40.75242853164673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541871
Iteration 2/25 | Loss: 0.00147175
Iteration 3/25 | Loss: 0.00132738
Iteration 4/25 | Loss: 0.00131443
Iteration 5/25 | Loss: 0.00131242
Iteration 6/25 | Loss: 0.00131242
Iteration 7/25 | Loss: 0.00131242
Iteration 8/25 | Loss: 0.00131242
Iteration 9/25 | Loss: 0.00131242
Iteration 10/25 | Loss: 0.00131242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013124202378094196, 0.0013124202378094196, 0.0013124202378094196, 0.0013124202378094196, 0.0013124202378094196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013124202378094196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88911682
Iteration 2/25 | Loss: 0.00083300
Iteration 3/25 | Loss: 0.00083299
Iteration 4/25 | Loss: 0.00083299
Iteration 5/25 | Loss: 0.00083299
Iteration 6/25 | Loss: 0.00083299
Iteration 7/25 | Loss: 0.00083299
Iteration 8/25 | Loss: 0.00083299
Iteration 9/25 | Loss: 0.00083299
Iteration 10/25 | Loss: 0.00083299
Iteration 11/25 | Loss: 0.00083299
Iteration 12/25 | Loss: 0.00083299
Iteration 13/25 | Loss: 0.00083299
Iteration 14/25 | Loss: 0.00083299
Iteration 15/25 | Loss: 0.00083299
Iteration 16/25 | Loss: 0.00083299
Iteration 17/25 | Loss: 0.00083299
Iteration 18/25 | Loss: 0.00083299
Iteration 19/25 | Loss: 0.00083299
Iteration 20/25 | Loss: 0.00083299
Iteration 21/25 | Loss: 0.00083299
Iteration 22/25 | Loss: 0.00083299
Iteration 23/25 | Loss: 0.00083299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008329864940606058, 0.0008329864940606058, 0.0008329864940606058, 0.0008329864940606058, 0.0008329864940606058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008329864940606058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083299
Iteration 2/1000 | Loss: 0.00003380
Iteration 3/1000 | Loss: 0.00002526
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002100
Iteration 7/1000 | Loss: 0.00002042
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001974
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001919
Iteration 12/1000 | Loss: 0.00001895
Iteration 13/1000 | Loss: 0.00001869
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001840
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001806
Iteration 18/1000 | Loss: 0.00001793
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001765
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001761
Iteration 24/1000 | Loss: 0.00001761
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001760
Iteration 30/1000 | Loss: 0.00001760
Iteration 31/1000 | Loss: 0.00001760
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001757
Iteration 37/1000 | Loss: 0.00001757
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001757
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001756
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001754
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001752
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001751
Iteration 61/1000 | Loss: 0.00001751
Iteration 62/1000 | Loss: 0.00001751
Iteration 63/1000 | Loss: 0.00001750
Iteration 64/1000 | Loss: 0.00001750
Iteration 65/1000 | Loss: 0.00001750
Iteration 66/1000 | Loss: 0.00001749
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001749
Iteration 71/1000 | Loss: 0.00001749
Iteration 72/1000 | Loss: 0.00001749
Iteration 73/1000 | Loss: 0.00001749
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001748
Iteration 80/1000 | Loss: 0.00001747
Iteration 81/1000 | Loss: 0.00001746
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00001746
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001745
Iteration 88/1000 | Loss: 0.00001745
Iteration 89/1000 | Loss: 0.00001745
Iteration 90/1000 | Loss: 0.00001745
Iteration 91/1000 | Loss: 0.00001745
Iteration 92/1000 | Loss: 0.00001745
Iteration 93/1000 | Loss: 0.00001744
Iteration 94/1000 | Loss: 0.00001744
Iteration 95/1000 | Loss: 0.00001744
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001743
Iteration 98/1000 | Loss: 0.00001743
Iteration 99/1000 | Loss: 0.00001743
Iteration 100/1000 | Loss: 0.00001743
Iteration 101/1000 | Loss: 0.00001743
Iteration 102/1000 | Loss: 0.00001742
Iteration 103/1000 | Loss: 0.00001742
Iteration 104/1000 | Loss: 0.00001742
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001742
Iteration 107/1000 | Loss: 0.00001742
Iteration 108/1000 | Loss: 0.00001742
Iteration 109/1000 | Loss: 0.00001742
Iteration 110/1000 | Loss: 0.00001742
Iteration 111/1000 | Loss: 0.00001741
Iteration 112/1000 | Loss: 0.00001741
Iteration 113/1000 | Loss: 0.00001741
Iteration 114/1000 | Loss: 0.00001741
Iteration 115/1000 | Loss: 0.00001741
Iteration 116/1000 | Loss: 0.00001741
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001739
Iteration 125/1000 | Loss: 0.00001739
Iteration 126/1000 | Loss: 0.00001739
Iteration 127/1000 | Loss: 0.00001739
Iteration 128/1000 | Loss: 0.00001739
Iteration 129/1000 | Loss: 0.00001739
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001738
Iteration 132/1000 | Loss: 0.00001738
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001738
Iteration 135/1000 | Loss: 0.00001737
Iteration 136/1000 | Loss: 0.00001737
Iteration 137/1000 | Loss: 0.00001737
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001737
Iteration 140/1000 | Loss: 0.00001737
Iteration 141/1000 | Loss: 0.00001737
Iteration 142/1000 | Loss: 0.00001737
Iteration 143/1000 | Loss: 0.00001737
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001736
Iteration 147/1000 | Loss: 0.00001736
Iteration 148/1000 | Loss: 0.00001736
Iteration 149/1000 | Loss: 0.00001736
Iteration 150/1000 | Loss: 0.00001736
Iteration 151/1000 | Loss: 0.00001735
Iteration 152/1000 | Loss: 0.00001735
Iteration 153/1000 | Loss: 0.00001735
Iteration 154/1000 | Loss: 0.00001735
Iteration 155/1000 | Loss: 0.00001735
Iteration 156/1000 | Loss: 0.00001735
Iteration 157/1000 | Loss: 0.00001735
Iteration 158/1000 | Loss: 0.00001735
Iteration 159/1000 | Loss: 0.00001735
Iteration 160/1000 | Loss: 0.00001735
Iteration 161/1000 | Loss: 0.00001734
Iteration 162/1000 | Loss: 0.00001734
Iteration 163/1000 | Loss: 0.00001734
Iteration 164/1000 | Loss: 0.00001734
Iteration 165/1000 | Loss: 0.00001734
Iteration 166/1000 | Loss: 0.00001734
Iteration 167/1000 | Loss: 0.00001734
Iteration 168/1000 | Loss: 0.00001734
Iteration 169/1000 | Loss: 0.00001734
Iteration 170/1000 | Loss: 0.00001734
Iteration 171/1000 | Loss: 0.00001734
Iteration 172/1000 | Loss: 0.00001734
Iteration 173/1000 | Loss: 0.00001734
Iteration 174/1000 | Loss: 0.00001733
Iteration 175/1000 | Loss: 0.00001733
Iteration 176/1000 | Loss: 0.00001733
Iteration 177/1000 | Loss: 0.00001733
Iteration 178/1000 | Loss: 0.00001733
Iteration 179/1000 | Loss: 0.00001733
Iteration 180/1000 | Loss: 0.00001733
Iteration 181/1000 | Loss: 0.00001733
Iteration 182/1000 | Loss: 0.00001733
Iteration 183/1000 | Loss: 0.00001733
Iteration 184/1000 | Loss: 0.00001733
Iteration 185/1000 | Loss: 0.00001732
Iteration 186/1000 | Loss: 0.00001732
Iteration 187/1000 | Loss: 0.00001732
Iteration 188/1000 | Loss: 0.00001732
Iteration 189/1000 | Loss: 0.00001732
Iteration 190/1000 | Loss: 0.00001732
Iteration 191/1000 | Loss: 0.00001732
Iteration 192/1000 | Loss: 0.00001732
Iteration 193/1000 | Loss: 0.00001732
Iteration 194/1000 | Loss: 0.00001732
Iteration 195/1000 | Loss: 0.00001732
Iteration 196/1000 | Loss: 0.00001732
Iteration 197/1000 | Loss: 0.00001731
Iteration 198/1000 | Loss: 0.00001731
Iteration 199/1000 | Loss: 0.00001731
Iteration 200/1000 | Loss: 0.00001731
Iteration 201/1000 | Loss: 0.00001731
Iteration 202/1000 | Loss: 0.00001731
Iteration 203/1000 | Loss: 0.00001731
Iteration 204/1000 | Loss: 0.00001731
Iteration 205/1000 | Loss: 0.00001731
Iteration 206/1000 | Loss: 0.00001731
Iteration 207/1000 | Loss: 0.00001731
Iteration 208/1000 | Loss: 0.00001731
Iteration 209/1000 | Loss: 0.00001731
Iteration 210/1000 | Loss: 0.00001731
Iteration 211/1000 | Loss: 0.00001731
Iteration 212/1000 | Loss: 0.00001731
Iteration 213/1000 | Loss: 0.00001731
Iteration 214/1000 | Loss: 0.00001731
Iteration 215/1000 | Loss: 0.00001731
Iteration 216/1000 | Loss: 0.00001731
Iteration 217/1000 | Loss: 0.00001731
Iteration 218/1000 | Loss: 0.00001731
Iteration 219/1000 | Loss: 0.00001731
Iteration 220/1000 | Loss: 0.00001731
Iteration 221/1000 | Loss: 0.00001731
Iteration 222/1000 | Loss: 0.00001731
Iteration 223/1000 | Loss: 0.00001731
Iteration 224/1000 | Loss: 0.00001731
Iteration 225/1000 | Loss: 0.00001731
Iteration 226/1000 | Loss: 0.00001731
Iteration 227/1000 | Loss: 0.00001731
Iteration 228/1000 | Loss: 0.00001731
Iteration 229/1000 | Loss: 0.00001731
Iteration 230/1000 | Loss: 0.00001731
Iteration 231/1000 | Loss: 0.00001731
Iteration 232/1000 | Loss: 0.00001731
Iteration 233/1000 | Loss: 0.00001731
Iteration 234/1000 | Loss: 0.00001731
Iteration 235/1000 | Loss: 0.00001731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.7307904272456653e-05, 1.7307904272456653e-05, 1.7307904272456653e-05, 1.7307904272456653e-05, 1.7307904272456653e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7307904272456653e-05

Optimization complete. Final v2v error: 3.482682466506958 mm

Highest mean error: 3.7833330631256104 mm for frame 57

Lowest mean error: 3.095050096511841 mm for frame 218

Saving results

Total time: 55.25139832496643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713636
Iteration 2/25 | Loss: 0.00238307
Iteration 3/25 | Loss: 0.00157379
Iteration 4/25 | Loss: 0.00137923
Iteration 5/25 | Loss: 0.00130655
Iteration 6/25 | Loss: 0.00131373
Iteration 7/25 | Loss: 0.00125548
Iteration 8/25 | Loss: 0.00122327
Iteration 9/25 | Loss: 0.00120566
Iteration 10/25 | Loss: 0.00120048
Iteration 11/25 | Loss: 0.00119476
Iteration 12/25 | Loss: 0.00119375
Iteration 13/25 | Loss: 0.00119258
Iteration 14/25 | Loss: 0.00119203
Iteration 15/25 | Loss: 0.00119176
Iteration 16/25 | Loss: 0.00119166
Iteration 17/25 | Loss: 0.00119164
Iteration 18/25 | Loss: 0.00119164
Iteration 19/25 | Loss: 0.00119164
Iteration 20/25 | Loss: 0.00119163
Iteration 21/25 | Loss: 0.00119163
Iteration 22/25 | Loss: 0.00119163
Iteration 23/25 | Loss: 0.00119163
Iteration 24/25 | Loss: 0.00119163
Iteration 25/25 | Loss: 0.00119162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75732505
Iteration 2/25 | Loss: 0.00081816
Iteration 3/25 | Loss: 0.00081816
Iteration 4/25 | Loss: 0.00081816
Iteration 5/25 | Loss: 0.00081815
Iteration 6/25 | Loss: 0.00081815
Iteration 7/25 | Loss: 0.00081815
Iteration 8/25 | Loss: 0.00081815
Iteration 9/25 | Loss: 0.00081815
Iteration 10/25 | Loss: 0.00081815
Iteration 11/25 | Loss: 0.00081815
Iteration 12/25 | Loss: 0.00081815
Iteration 13/25 | Loss: 0.00081815
Iteration 14/25 | Loss: 0.00081815
Iteration 15/25 | Loss: 0.00081815
Iteration 16/25 | Loss: 0.00081815
Iteration 17/25 | Loss: 0.00081815
Iteration 18/25 | Loss: 0.00081815
Iteration 19/25 | Loss: 0.00081815
Iteration 20/25 | Loss: 0.00081815
Iteration 21/25 | Loss: 0.00081815
Iteration 22/25 | Loss: 0.00081815
Iteration 23/25 | Loss: 0.00081815
Iteration 24/25 | Loss: 0.00081815
Iteration 25/25 | Loss: 0.00081815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081815
Iteration 2/1000 | Loss: 0.00002249
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00003248
Iteration 6/1000 | Loss: 0.00002756
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00001310
Iteration 10/1000 | Loss: 0.00008441
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001210
Iteration 18/1000 | Loss: 0.00001199
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001191
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001179
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001177
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00002842
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001169
Iteration 37/1000 | Loss: 0.00001165
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001163
Iteration 44/1000 | Loss: 0.00001163
Iteration 45/1000 | Loss: 0.00001163
Iteration 46/1000 | Loss: 0.00001163
Iteration 47/1000 | Loss: 0.00001163
Iteration 48/1000 | Loss: 0.00001163
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001161
Iteration 55/1000 | Loss: 0.00001161
Iteration 56/1000 | Loss: 0.00001161
Iteration 57/1000 | Loss: 0.00001161
Iteration 58/1000 | Loss: 0.00001161
Iteration 59/1000 | Loss: 0.00001161
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001157
Iteration 72/1000 | Loss: 0.00001157
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001156
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001151
Iteration 95/1000 | Loss: 0.00001151
Iteration 96/1000 | Loss: 0.00001150
Iteration 97/1000 | Loss: 0.00001150
Iteration 98/1000 | Loss: 0.00001150
Iteration 99/1000 | Loss: 0.00001149
Iteration 100/1000 | Loss: 0.00001149
Iteration 101/1000 | Loss: 0.00001149
Iteration 102/1000 | Loss: 0.00001149
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001148
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001147
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001144
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001141
Iteration 149/1000 | Loss: 0.00001141
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001141
Iteration 158/1000 | Loss: 0.00001141
Iteration 159/1000 | Loss: 0.00001141
Iteration 160/1000 | Loss: 0.00001141
Iteration 161/1000 | Loss: 0.00001141
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001140
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001140
Iteration 166/1000 | Loss: 0.00001140
Iteration 167/1000 | Loss: 0.00001140
Iteration 168/1000 | Loss: 0.00001139
Iteration 169/1000 | Loss: 0.00001139
Iteration 170/1000 | Loss: 0.00001139
Iteration 171/1000 | Loss: 0.00001139
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001138
Iteration 178/1000 | Loss: 0.00001138
Iteration 179/1000 | Loss: 0.00001138
Iteration 180/1000 | Loss: 0.00001138
Iteration 181/1000 | Loss: 0.00001138
Iteration 182/1000 | Loss: 0.00001138
Iteration 183/1000 | Loss: 0.00001138
Iteration 184/1000 | Loss: 0.00001138
Iteration 185/1000 | Loss: 0.00001138
Iteration 186/1000 | Loss: 0.00001138
Iteration 187/1000 | Loss: 0.00001138
Iteration 188/1000 | Loss: 0.00001138
Iteration 189/1000 | Loss: 0.00001138
Iteration 190/1000 | Loss: 0.00001138
Iteration 191/1000 | Loss: 0.00001138
Iteration 192/1000 | Loss: 0.00001138
Iteration 193/1000 | Loss: 0.00001138
Iteration 194/1000 | Loss: 0.00001138
Iteration 195/1000 | Loss: 0.00001138
Iteration 196/1000 | Loss: 0.00001138
Iteration 197/1000 | Loss: 0.00001138
Iteration 198/1000 | Loss: 0.00001138
Iteration 199/1000 | Loss: 0.00001138
Iteration 200/1000 | Loss: 0.00001138
Iteration 201/1000 | Loss: 0.00001138
Iteration 202/1000 | Loss: 0.00001138
Iteration 203/1000 | Loss: 0.00001138
Iteration 204/1000 | Loss: 0.00001138
Iteration 205/1000 | Loss: 0.00001138
Iteration 206/1000 | Loss: 0.00001138
Iteration 207/1000 | Loss: 0.00001138
Iteration 208/1000 | Loss: 0.00001138
Iteration 209/1000 | Loss: 0.00001138
Iteration 210/1000 | Loss: 0.00001138
Iteration 211/1000 | Loss: 0.00001138
Iteration 212/1000 | Loss: 0.00001138
Iteration 213/1000 | Loss: 0.00001138
Iteration 214/1000 | Loss: 0.00001138
Iteration 215/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1377020200598054e-05, 1.1377020200598054e-05, 1.1377020200598054e-05, 1.1377020200598054e-05, 1.1377020200598054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1377020200598054e-05

Optimization complete. Final v2v error: 2.9008941650390625 mm

Highest mean error: 3.368273973464966 mm for frame 104

Lowest mean error: 2.7395107746124268 mm for frame 13

Saving results

Total time: 70.13314700126648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828944
Iteration 2/25 | Loss: 0.00140029
Iteration 3/25 | Loss: 0.00124151
Iteration 4/25 | Loss: 0.00121106
Iteration 5/25 | Loss: 0.00121105
Iteration 6/25 | Loss: 0.00120407
Iteration 7/25 | Loss: 0.00120478
Iteration 8/25 | Loss: 0.00120426
Iteration 9/25 | Loss: 0.00120315
Iteration 10/25 | Loss: 0.00120311
Iteration 11/25 | Loss: 0.00120311
Iteration 12/25 | Loss: 0.00120311
Iteration 13/25 | Loss: 0.00120311
Iteration 14/25 | Loss: 0.00120311
Iteration 15/25 | Loss: 0.00120311
Iteration 16/25 | Loss: 0.00120311
Iteration 17/25 | Loss: 0.00120311
Iteration 18/25 | Loss: 0.00120311
Iteration 19/25 | Loss: 0.00120311
Iteration 20/25 | Loss: 0.00120311
Iteration 21/25 | Loss: 0.00120311
Iteration 22/25 | Loss: 0.00120311
Iteration 23/25 | Loss: 0.00120310
Iteration 24/25 | Loss: 0.00120310
Iteration 25/25 | Loss: 0.00120310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80156231
Iteration 2/25 | Loss: 0.00083609
Iteration 3/25 | Loss: 0.00083584
Iteration 4/25 | Loss: 0.00083584
Iteration 5/25 | Loss: 0.00083583
Iteration 6/25 | Loss: 0.00083583
Iteration 7/25 | Loss: 0.00083583
Iteration 8/25 | Loss: 0.00083583
Iteration 9/25 | Loss: 0.00083583
Iteration 10/25 | Loss: 0.00083583
Iteration 11/25 | Loss: 0.00083583
Iteration 12/25 | Loss: 0.00083583
Iteration 13/25 | Loss: 0.00083583
Iteration 14/25 | Loss: 0.00083583
Iteration 15/25 | Loss: 0.00083583
Iteration 16/25 | Loss: 0.00083583
Iteration 17/25 | Loss: 0.00083583
Iteration 18/25 | Loss: 0.00083583
Iteration 19/25 | Loss: 0.00083583
Iteration 20/25 | Loss: 0.00083583
Iteration 21/25 | Loss: 0.00083583
Iteration 22/25 | Loss: 0.00083583
Iteration 23/25 | Loss: 0.00083583
Iteration 24/25 | Loss: 0.00083583
Iteration 25/25 | Loss: 0.00083583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083583
Iteration 2/1000 | Loss: 0.00002992
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00002142
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001370
Iteration 8/1000 | Loss: 0.00001330
Iteration 9/1000 | Loss: 0.00001308
Iteration 10/1000 | Loss: 0.00002288
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001261
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001221
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001220
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001220
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001219
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001219
Iteration 36/1000 | Loss: 0.00001219
Iteration 37/1000 | Loss: 0.00001218
Iteration 38/1000 | Loss: 0.00001218
Iteration 39/1000 | Loss: 0.00001218
Iteration 40/1000 | Loss: 0.00001217
Iteration 41/1000 | Loss: 0.00001217
Iteration 42/1000 | Loss: 0.00001217
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001197
Iteration 58/1000 | Loss: 0.00001197
Iteration 59/1000 | Loss: 0.00001197
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001191
Iteration 78/1000 | Loss: 0.00001191
Iteration 79/1000 | Loss: 0.00001191
Iteration 80/1000 | Loss: 0.00001191
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001190
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001190
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001189
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001188
Iteration 90/1000 | Loss: 0.00001188
Iteration 91/1000 | Loss: 0.00001424
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001355
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001355
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001184
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001179
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001179
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001178
Iteration 136/1000 | Loss: 0.00001178
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001176
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001177
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001174
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001173
Iteration 172/1000 | Loss: 0.00001173
Iteration 173/1000 | Loss: 0.00001173
Iteration 174/1000 | Loss: 0.00001173
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001173
Iteration 181/1000 | Loss: 0.00001173
Iteration 182/1000 | Loss: 0.00001172
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001172
Iteration 186/1000 | Loss: 0.00001172
Iteration 187/1000 | Loss: 0.00001172
Iteration 188/1000 | Loss: 0.00001172
Iteration 189/1000 | Loss: 0.00001172
Iteration 190/1000 | Loss: 0.00001172
Iteration 191/1000 | Loss: 0.00001172
Iteration 192/1000 | Loss: 0.00001172
Iteration 193/1000 | Loss: 0.00001171
Iteration 194/1000 | Loss: 0.00001171
Iteration 195/1000 | Loss: 0.00001171
Iteration 196/1000 | Loss: 0.00001171
Iteration 197/1000 | Loss: 0.00001171
Iteration 198/1000 | Loss: 0.00001171
Iteration 199/1000 | Loss: 0.00001171
Iteration 200/1000 | Loss: 0.00001171
Iteration 201/1000 | Loss: 0.00001171
Iteration 202/1000 | Loss: 0.00001171
Iteration 203/1000 | Loss: 0.00001171
Iteration 204/1000 | Loss: 0.00001171
Iteration 205/1000 | Loss: 0.00001171
Iteration 206/1000 | Loss: 0.00001171
Iteration 207/1000 | Loss: 0.00001171
Iteration 208/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.1711003025993705e-05, 1.1711003025993705e-05, 1.1711003025993705e-05, 1.1711003025993705e-05, 1.1711003025993705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1711003025993705e-05

Optimization complete. Final v2v error: 2.9464879035949707 mm

Highest mean error: 3.2493999004364014 mm for frame 40

Lowest mean error: 2.6917319297790527 mm for frame 101

Saving results

Total time: 70.99494457244873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00645189
Iteration 2/25 | Loss: 0.00169421
Iteration 3/25 | Loss: 0.00137784
Iteration 4/25 | Loss: 0.00130388
Iteration 5/25 | Loss: 0.00129931
Iteration 6/25 | Loss: 0.00129512
Iteration 7/25 | Loss: 0.00129878
Iteration 8/25 | Loss: 0.00129477
Iteration 9/25 | Loss: 0.00129282
Iteration 10/25 | Loss: 0.00129135
Iteration 11/25 | Loss: 0.00129525
Iteration 12/25 | Loss: 0.00129095
Iteration 13/25 | Loss: 0.00129088
Iteration 14/25 | Loss: 0.00129088
Iteration 15/25 | Loss: 0.00129088
Iteration 16/25 | Loss: 0.00129088
Iteration 17/25 | Loss: 0.00129088
Iteration 18/25 | Loss: 0.00129088
Iteration 19/25 | Loss: 0.00129088
Iteration 20/25 | Loss: 0.00129088
Iteration 21/25 | Loss: 0.00129087
Iteration 22/25 | Loss: 0.00129087
Iteration 23/25 | Loss: 0.00129087
Iteration 24/25 | Loss: 0.00129087
Iteration 25/25 | Loss: 0.00129087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96688318
Iteration 2/25 | Loss: 0.00085073
Iteration 3/25 | Loss: 0.00085068
Iteration 4/25 | Loss: 0.00085068
Iteration 5/25 | Loss: 0.00085068
Iteration 6/25 | Loss: 0.00085068
Iteration 7/25 | Loss: 0.00085068
Iteration 8/25 | Loss: 0.00085068
Iteration 9/25 | Loss: 0.00085068
Iteration 10/25 | Loss: 0.00085067
Iteration 11/25 | Loss: 0.00085067
Iteration 12/25 | Loss: 0.00085067
Iteration 13/25 | Loss: 0.00085067
Iteration 14/25 | Loss: 0.00085067
Iteration 15/25 | Loss: 0.00085067
Iteration 16/25 | Loss: 0.00085067
Iteration 17/25 | Loss: 0.00085067
Iteration 18/25 | Loss: 0.00085067
Iteration 19/25 | Loss: 0.00085067
Iteration 20/25 | Loss: 0.00085067
Iteration 21/25 | Loss: 0.00085067
Iteration 22/25 | Loss: 0.00085067
Iteration 23/25 | Loss: 0.00085067
Iteration 24/25 | Loss: 0.00085067
Iteration 25/25 | Loss: 0.00085067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085067
Iteration 2/1000 | Loss: 0.00003408
Iteration 3/1000 | Loss: 0.00002408
Iteration 4/1000 | Loss: 0.00002038
Iteration 5/1000 | Loss: 0.00008842
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001760
Iteration 9/1000 | Loss: 0.00001737
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001687
Iteration 12/1000 | Loss: 0.00001670
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001650
Iteration 15/1000 | Loss: 0.00001647
Iteration 16/1000 | Loss: 0.00001640
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001621
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001611
Iteration 40/1000 | Loss: 0.00001610
Iteration 41/1000 | Loss: 0.00001610
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001609
Iteration 44/1000 | Loss: 0.00001609
Iteration 45/1000 | Loss: 0.00001609
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001601
Iteration 50/1000 | Loss: 0.00001601
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001600
Iteration 54/1000 | Loss: 0.00001599
Iteration 55/1000 | Loss: 0.00001597
Iteration 56/1000 | Loss: 0.00001597
Iteration 57/1000 | Loss: 0.00001594
Iteration 58/1000 | Loss: 0.00001594
Iteration 59/1000 | Loss: 0.00001593
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001591
Iteration 63/1000 | Loss: 0.00001591
Iteration 64/1000 | Loss: 0.00001591
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001590
Iteration 72/1000 | Loss: 0.00001590
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001586
Iteration 86/1000 | Loss: 0.00001586
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001584
Iteration 96/1000 | Loss: 0.00001584
Iteration 97/1000 | Loss: 0.00001584
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001583
Iteration 102/1000 | Loss: 0.00001583
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001580
Iteration 119/1000 | Loss: 0.00001580
Iteration 120/1000 | Loss: 0.00001580
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001579
Iteration 125/1000 | Loss: 0.00001579
Iteration 126/1000 | Loss: 0.00001579
Iteration 127/1000 | Loss: 0.00001579
Iteration 128/1000 | Loss: 0.00001579
Iteration 129/1000 | Loss: 0.00001579
Iteration 130/1000 | Loss: 0.00001578
Iteration 131/1000 | Loss: 0.00001578
Iteration 132/1000 | Loss: 0.00001578
Iteration 133/1000 | Loss: 0.00001578
Iteration 134/1000 | Loss: 0.00001578
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001577
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001576
Iteration 146/1000 | Loss: 0.00001576
Iteration 147/1000 | Loss: 0.00001576
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001575
Iteration 151/1000 | Loss: 0.00001575
Iteration 152/1000 | Loss: 0.00001575
Iteration 153/1000 | Loss: 0.00001575
Iteration 154/1000 | Loss: 0.00001575
Iteration 155/1000 | Loss: 0.00001575
Iteration 156/1000 | Loss: 0.00001575
Iteration 157/1000 | Loss: 0.00001575
Iteration 158/1000 | Loss: 0.00001575
Iteration 159/1000 | Loss: 0.00001575
Iteration 160/1000 | Loss: 0.00001575
Iteration 161/1000 | Loss: 0.00001575
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001575
Iteration 165/1000 | Loss: 0.00001575
Iteration 166/1000 | Loss: 0.00001575
Iteration 167/1000 | Loss: 0.00001574
Iteration 168/1000 | Loss: 0.00001574
Iteration 169/1000 | Loss: 0.00001574
Iteration 170/1000 | Loss: 0.00001574
Iteration 171/1000 | Loss: 0.00001574
Iteration 172/1000 | Loss: 0.00001573
Iteration 173/1000 | Loss: 0.00001573
Iteration 174/1000 | Loss: 0.00001573
Iteration 175/1000 | Loss: 0.00001573
Iteration 176/1000 | Loss: 0.00001573
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001572
Iteration 185/1000 | Loss: 0.00001572
Iteration 186/1000 | Loss: 0.00001572
Iteration 187/1000 | Loss: 0.00001572
Iteration 188/1000 | Loss: 0.00001572
Iteration 189/1000 | Loss: 0.00001572
Iteration 190/1000 | Loss: 0.00001572
Iteration 191/1000 | Loss: 0.00001572
Iteration 192/1000 | Loss: 0.00001572
Iteration 193/1000 | Loss: 0.00001571
Iteration 194/1000 | Loss: 0.00001571
Iteration 195/1000 | Loss: 0.00001571
Iteration 196/1000 | Loss: 0.00001571
Iteration 197/1000 | Loss: 0.00001571
Iteration 198/1000 | Loss: 0.00001571
Iteration 199/1000 | Loss: 0.00001571
Iteration 200/1000 | Loss: 0.00001571
Iteration 201/1000 | Loss: 0.00001571
Iteration 202/1000 | Loss: 0.00001571
Iteration 203/1000 | Loss: 0.00001571
Iteration 204/1000 | Loss: 0.00001571
Iteration 205/1000 | Loss: 0.00001571
Iteration 206/1000 | Loss: 0.00001571
Iteration 207/1000 | Loss: 0.00001571
Iteration 208/1000 | Loss: 0.00001571
Iteration 209/1000 | Loss: 0.00001571
Iteration 210/1000 | Loss: 0.00001571
Iteration 211/1000 | Loss: 0.00001571
Iteration 212/1000 | Loss: 0.00001571
Iteration 213/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.570909626025241e-05, 1.570909626025241e-05, 1.570909626025241e-05, 1.570909626025241e-05, 1.570909626025241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.570909626025241e-05

Optimization complete. Final v2v error: 3.2954418659210205 mm

Highest mean error: 4.15898323059082 mm for frame 92

Lowest mean error: 2.8328821659088135 mm for frame 44

Saving results

Total time: 70.50005388259888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00604817
Iteration 2/25 | Loss: 0.00162114
Iteration 3/25 | Loss: 0.00140438
Iteration 4/25 | Loss: 0.00138239
Iteration 5/25 | Loss: 0.00137792
Iteration 6/25 | Loss: 0.00137792
Iteration 7/25 | Loss: 0.00137792
Iteration 8/25 | Loss: 0.00137792
Iteration 9/25 | Loss: 0.00137792
Iteration 10/25 | Loss: 0.00137792
Iteration 11/25 | Loss: 0.00137792
Iteration 12/25 | Loss: 0.00137785
Iteration 13/25 | Loss: 0.00137785
Iteration 14/25 | Loss: 0.00137785
Iteration 15/25 | Loss: 0.00137785
Iteration 16/25 | Loss: 0.00137785
Iteration 17/25 | Loss: 0.00137785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001377846347168088, 0.001377846347168088, 0.001377846347168088, 0.001377846347168088, 0.001377846347168088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001377846347168088

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04972267
Iteration 2/25 | Loss: 0.00069723
Iteration 3/25 | Loss: 0.00069723
Iteration 4/25 | Loss: 0.00069723
Iteration 5/25 | Loss: 0.00069723
Iteration 6/25 | Loss: 0.00069723
Iteration 7/25 | Loss: 0.00069723
Iteration 8/25 | Loss: 0.00069723
Iteration 9/25 | Loss: 0.00069723
Iteration 10/25 | Loss: 0.00069723
Iteration 11/25 | Loss: 0.00069723
Iteration 12/25 | Loss: 0.00069723
Iteration 13/25 | Loss: 0.00069723
Iteration 14/25 | Loss: 0.00069723
Iteration 15/25 | Loss: 0.00069723
Iteration 16/25 | Loss: 0.00069723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006972250994294882, 0.0006972250994294882, 0.0006972250994294882, 0.0006972250994294882, 0.0006972250994294882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006972250994294882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069723
Iteration 2/1000 | Loss: 0.00004134
Iteration 3/1000 | Loss: 0.00003144
Iteration 4/1000 | Loss: 0.00002939
Iteration 5/1000 | Loss: 0.00002776
Iteration 6/1000 | Loss: 0.00002701
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002608
Iteration 9/1000 | Loss: 0.00002578
Iteration 10/1000 | Loss: 0.00002552
Iteration 11/1000 | Loss: 0.00002520
Iteration 12/1000 | Loss: 0.00002503
Iteration 13/1000 | Loss: 0.00002486
Iteration 14/1000 | Loss: 0.00002484
Iteration 15/1000 | Loss: 0.00002468
Iteration 16/1000 | Loss: 0.00002468
Iteration 17/1000 | Loss: 0.00002467
Iteration 18/1000 | Loss: 0.00002467
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002467
Iteration 21/1000 | Loss: 0.00002467
Iteration 22/1000 | Loss: 0.00002467
Iteration 23/1000 | Loss: 0.00002467
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002466
Iteration 26/1000 | Loss: 0.00002466
Iteration 27/1000 | Loss: 0.00002466
Iteration 28/1000 | Loss: 0.00002466
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002464
Iteration 31/1000 | Loss: 0.00002463
Iteration 32/1000 | Loss: 0.00002463
Iteration 33/1000 | Loss: 0.00002462
Iteration 34/1000 | Loss: 0.00002462
Iteration 35/1000 | Loss: 0.00002462
Iteration 36/1000 | Loss: 0.00002461
Iteration 37/1000 | Loss: 0.00002461
Iteration 38/1000 | Loss: 0.00002461
Iteration 39/1000 | Loss: 0.00002461
Iteration 40/1000 | Loss: 0.00002461
Iteration 41/1000 | Loss: 0.00002461
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002461
Iteration 47/1000 | Loss: 0.00002461
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002460
Iteration 52/1000 | Loss: 0.00002460
Iteration 53/1000 | Loss: 0.00002460
Iteration 54/1000 | Loss: 0.00002460
Iteration 55/1000 | Loss: 0.00002460
Iteration 56/1000 | Loss: 0.00002460
Iteration 57/1000 | Loss: 0.00002459
Iteration 58/1000 | Loss: 0.00002458
Iteration 59/1000 | Loss: 0.00002458
Iteration 60/1000 | Loss: 0.00002458
Iteration 61/1000 | Loss: 0.00002458
Iteration 62/1000 | Loss: 0.00002458
Iteration 63/1000 | Loss: 0.00002458
Iteration 64/1000 | Loss: 0.00002458
Iteration 65/1000 | Loss: 0.00002458
Iteration 66/1000 | Loss: 0.00002458
Iteration 67/1000 | Loss: 0.00002458
Iteration 68/1000 | Loss: 0.00002458
Iteration 69/1000 | Loss: 0.00002458
Iteration 70/1000 | Loss: 0.00002458
Iteration 71/1000 | Loss: 0.00002458
Iteration 72/1000 | Loss: 0.00002458
Iteration 73/1000 | Loss: 0.00002458
Iteration 74/1000 | Loss: 0.00002458
Iteration 75/1000 | Loss: 0.00002458
Iteration 76/1000 | Loss: 0.00002458
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [2.4576653231633827e-05, 2.4576653231633827e-05, 2.4576653231633827e-05, 2.4576653231633827e-05, 2.4576653231633827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4576653231633827e-05

Optimization complete. Final v2v error: 4.185088634490967 mm

Highest mean error: 4.238714218139648 mm for frame 40

Lowest mean error: 4.140059947967529 mm for frame 229

Saving results

Total time: 35.29596567153931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802476
Iteration 2/25 | Loss: 0.00181886
Iteration 3/25 | Loss: 0.00153246
Iteration 4/25 | Loss: 0.00148100
Iteration 5/25 | Loss: 0.00146493
Iteration 6/25 | Loss: 0.00144006
Iteration 7/25 | Loss: 0.00141797
Iteration 8/25 | Loss: 0.00140459
Iteration 9/25 | Loss: 0.00139595
Iteration 10/25 | Loss: 0.00139199
Iteration 11/25 | Loss: 0.00138527
Iteration 12/25 | Loss: 0.00138486
Iteration 13/25 | Loss: 0.00138478
Iteration 14/25 | Loss: 0.00138478
Iteration 15/25 | Loss: 0.00138478
Iteration 16/25 | Loss: 0.00138477
Iteration 17/25 | Loss: 0.00138477
Iteration 18/25 | Loss: 0.00138477
Iteration 19/25 | Loss: 0.00138477
Iteration 20/25 | Loss: 0.00138477
Iteration 21/25 | Loss: 0.00138477
Iteration 22/25 | Loss: 0.00138477
Iteration 23/25 | Loss: 0.00138476
Iteration 24/25 | Loss: 0.00138476
Iteration 25/25 | Loss: 0.00138475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64886391
Iteration 2/25 | Loss: 0.00082435
Iteration 3/25 | Loss: 0.00082433
Iteration 4/25 | Loss: 0.00082433
Iteration 5/25 | Loss: 0.00082433
Iteration 6/25 | Loss: 0.00082432
Iteration 7/25 | Loss: 0.00082432
Iteration 8/25 | Loss: 0.00082432
Iteration 9/25 | Loss: 0.00082432
Iteration 10/25 | Loss: 0.00082432
Iteration 11/25 | Loss: 0.00082432
Iteration 12/25 | Loss: 0.00082432
Iteration 13/25 | Loss: 0.00082432
Iteration 14/25 | Loss: 0.00082432
Iteration 15/25 | Loss: 0.00082432
Iteration 16/25 | Loss: 0.00082432
Iteration 17/25 | Loss: 0.00082432
Iteration 18/25 | Loss: 0.00082432
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008243231568485498, 0.0008243231568485498, 0.0008243231568485498, 0.0008243231568485498, 0.0008243231568485498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008243231568485498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082432
Iteration 2/1000 | Loss: 0.00005652
Iteration 3/1000 | Loss: 0.00003956
Iteration 4/1000 | Loss: 0.00003576
Iteration 5/1000 | Loss: 0.00003371
Iteration 6/1000 | Loss: 0.00003243
Iteration 7/1000 | Loss: 0.00003151
Iteration 8/1000 | Loss: 0.00003095
Iteration 9/1000 | Loss: 0.00003044
Iteration 10/1000 | Loss: 0.00003008
Iteration 11/1000 | Loss: 0.00002979
Iteration 12/1000 | Loss: 0.00002973
Iteration 13/1000 | Loss: 0.00002953
Iteration 14/1000 | Loss: 0.00002939
Iteration 15/1000 | Loss: 0.00002934
Iteration 16/1000 | Loss: 0.00002928
Iteration 17/1000 | Loss: 0.00002928
Iteration 18/1000 | Loss: 0.00002922
Iteration 19/1000 | Loss: 0.00002922
Iteration 20/1000 | Loss: 0.00002917
Iteration 21/1000 | Loss: 0.00002917
Iteration 22/1000 | Loss: 0.00002917
Iteration 23/1000 | Loss: 0.00002917
Iteration 24/1000 | Loss: 0.00002917
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002916
Iteration 27/1000 | Loss: 0.00002916
Iteration 28/1000 | Loss: 0.00002914
Iteration 29/1000 | Loss: 0.00002914
Iteration 30/1000 | Loss: 0.00002914
Iteration 31/1000 | Loss: 0.00002914
Iteration 32/1000 | Loss: 0.00002914
Iteration 33/1000 | Loss: 0.00002914
Iteration 34/1000 | Loss: 0.00002914
Iteration 35/1000 | Loss: 0.00002913
Iteration 36/1000 | Loss: 0.00002913
Iteration 37/1000 | Loss: 0.00002913
Iteration 38/1000 | Loss: 0.00002913
Iteration 39/1000 | Loss: 0.00002912
Iteration 40/1000 | Loss: 0.00002912
Iteration 41/1000 | Loss: 0.00002911
Iteration 42/1000 | Loss: 0.00002911
Iteration 43/1000 | Loss: 0.00002911
Iteration 44/1000 | Loss: 0.00002911
Iteration 45/1000 | Loss: 0.00002911
Iteration 46/1000 | Loss: 0.00002911
Iteration 47/1000 | Loss: 0.00002910
Iteration 48/1000 | Loss: 0.00002910
Iteration 49/1000 | Loss: 0.00002910
Iteration 50/1000 | Loss: 0.00002910
Iteration 51/1000 | Loss: 0.00002910
Iteration 52/1000 | Loss: 0.00002910
Iteration 53/1000 | Loss: 0.00002910
Iteration 54/1000 | Loss: 0.00002910
Iteration 55/1000 | Loss: 0.00002909
Iteration 56/1000 | Loss: 0.00002909
Iteration 57/1000 | Loss: 0.00002909
Iteration 58/1000 | Loss: 0.00002909
Iteration 59/1000 | Loss: 0.00002908
Iteration 60/1000 | Loss: 0.00002908
Iteration 61/1000 | Loss: 0.00002907
Iteration 62/1000 | Loss: 0.00002907
Iteration 63/1000 | Loss: 0.00002907
Iteration 64/1000 | Loss: 0.00002907
Iteration 65/1000 | Loss: 0.00002906
Iteration 66/1000 | Loss: 0.00002906
Iteration 67/1000 | Loss: 0.00002906
Iteration 68/1000 | Loss: 0.00002905
Iteration 69/1000 | Loss: 0.00002905
Iteration 70/1000 | Loss: 0.00002905
Iteration 71/1000 | Loss: 0.00002904
Iteration 72/1000 | Loss: 0.00002904
Iteration 73/1000 | Loss: 0.00002903
Iteration 74/1000 | Loss: 0.00002903
Iteration 75/1000 | Loss: 0.00002903
Iteration 76/1000 | Loss: 0.00002903
Iteration 77/1000 | Loss: 0.00002902
Iteration 78/1000 | Loss: 0.00002902
Iteration 79/1000 | Loss: 0.00002902
Iteration 80/1000 | Loss: 0.00002901
Iteration 81/1000 | Loss: 0.00002901
Iteration 82/1000 | Loss: 0.00002901
Iteration 83/1000 | Loss: 0.00002901
Iteration 84/1000 | Loss: 0.00002900
Iteration 85/1000 | Loss: 0.00002900
Iteration 86/1000 | Loss: 0.00002900
Iteration 87/1000 | Loss: 0.00002899
Iteration 88/1000 | Loss: 0.00002898
Iteration 89/1000 | Loss: 0.00002898
Iteration 90/1000 | Loss: 0.00002898
Iteration 91/1000 | Loss: 0.00002897
Iteration 92/1000 | Loss: 0.00002897
Iteration 93/1000 | Loss: 0.00002897
Iteration 94/1000 | Loss: 0.00002896
Iteration 95/1000 | Loss: 0.00002896
Iteration 96/1000 | Loss: 0.00002896
Iteration 97/1000 | Loss: 0.00002896
Iteration 98/1000 | Loss: 0.00002896
Iteration 99/1000 | Loss: 0.00002895
Iteration 100/1000 | Loss: 0.00002895
Iteration 101/1000 | Loss: 0.00002895
Iteration 102/1000 | Loss: 0.00002895
Iteration 103/1000 | Loss: 0.00002894
Iteration 104/1000 | Loss: 0.00002894
Iteration 105/1000 | Loss: 0.00002894
Iteration 106/1000 | Loss: 0.00002894
Iteration 107/1000 | Loss: 0.00002894
Iteration 108/1000 | Loss: 0.00002894
Iteration 109/1000 | Loss: 0.00002894
Iteration 110/1000 | Loss: 0.00002894
Iteration 111/1000 | Loss: 0.00002893
Iteration 112/1000 | Loss: 0.00002893
Iteration 113/1000 | Loss: 0.00002893
Iteration 114/1000 | Loss: 0.00002893
Iteration 115/1000 | Loss: 0.00002893
Iteration 116/1000 | Loss: 0.00002893
Iteration 117/1000 | Loss: 0.00002893
Iteration 118/1000 | Loss: 0.00002893
Iteration 119/1000 | Loss: 0.00002893
Iteration 120/1000 | Loss: 0.00002893
Iteration 121/1000 | Loss: 0.00002893
Iteration 122/1000 | Loss: 0.00002893
Iteration 123/1000 | Loss: 0.00002893
Iteration 124/1000 | Loss: 0.00002893
Iteration 125/1000 | Loss: 0.00002893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.8928128813277e-05, 2.8928128813277e-05, 2.8928128813277e-05, 2.8928128813277e-05, 2.8928128813277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8928128813277e-05

Optimization complete. Final v2v error: 4.3593430519104 mm

Highest mean error: 6.164990425109863 mm for frame 188

Lowest mean error: 3.24672532081604 mm for frame 110

Saving results

Total time: 60.13525700569153
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483700
Iteration 2/25 | Loss: 0.00149175
Iteration 3/25 | Loss: 0.00128061
Iteration 4/25 | Loss: 0.00126020
Iteration 5/25 | Loss: 0.00125688
Iteration 6/25 | Loss: 0.00125585
Iteration 7/25 | Loss: 0.00125584
Iteration 8/25 | Loss: 0.00125584
Iteration 9/25 | Loss: 0.00125584
Iteration 10/25 | Loss: 0.00125584
Iteration 11/25 | Loss: 0.00125584
Iteration 12/25 | Loss: 0.00125584
Iteration 13/25 | Loss: 0.00125584
Iteration 14/25 | Loss: 0.00125584
Iteration 15/25 | Loss: 0.00125584
Iteration 16/25 | Loss: 0.00125584
Iteration 17/25 | Loss: 0.00125584
Iteration 18/25 | Loss: 0.00125584
Iteration 19/25 | Loss: 0.00125584
Iteration 20/25 | Loss: 0.00125584
Iteration 21/25 | Loss: 0.00125584
Iteration 22/25 | Loss: 0.00125584
Iteration 23/25 | Loss: 0.00125584
Iteration 24/25 | Loss: 0.00125584
Iteration 25/25 | Loss: 0.00125584

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45592844
Iteration 2/25 | Loss: 0.00075647
Iteration 3/25 | Loss: 0.00075647
Iteration 4/25 | Loss: 0.00075647
Iteration 5/25 | Loss: 0.00075647
Iteration 6/25 | Loss: 0.00075647
Iteration 7/25 | Loss: 0.00075647
Iteration 8/25 | Loss: 0.00075647
Iteration 9/25 | Loss: 0.00075647
Iteration 10/25 | Loss: 0.00075647
Iteration 11/25 | Loss: 0.00075647
Iteration 12/25 | Loss: 0.00075647
Iteration 13/25 | Loss: 0.00075647
Iteration 14/25 | Loss: 0.00075647
Iteration 15/25 | Loss: 0.00075647
Iteration 16/25 | Loss: 0.00075647
Iteration 17/25 | Loss: 0.00075647
Iteration 18/25 | Loss: 0.00075647
Iteration 19/25 | Loss: 0.00075647
Iteration 20/25 | Loss: 0.00075647
Iteration 21/25 | Loss: 0.00075647
Iteration 22/25 | Loss: 0.00075647
Iteration 23/25 | Loss: 0.00075647
Iteration 24/25 | Loss: 0.00075647
Iteration 25/25 | Loss: 0.00075647

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075647
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002179
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001927
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001792
Iteration 9/1000 | Loss: 0.00001766
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001724
Iteration 12/1000 | Loss: 0.00001713
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001705
Iteration 15/1000 | Loss: 0.00001705
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001700
Iteration 19/1000 | Loss: 0.00001700
Iteration 20/1000 | Loss: 0.00001699
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001696
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001689
Iteration 26/1000 | Loss: 0.00001689
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001687
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001686
Iteration 31/1000 | Loss: 0.00001686
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001682
Iteration 43/1000 | Loss: 0.00001682
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001681
Iteration 47/1000 | Loss: 0.00001681
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001680
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001679
Iteration 54/1000 | Loss: 0.00001679
Iteration 55/1000 | Loss: 0.00001678
Iteration 56/1000 | Loss: 0.00001678
Iteration 57/1000 | Loss: 0.00001678
Iteration 58/1000 | Loss: 0.00001677
Iteration 59/1000 | Loss: 0.00001677
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001674
Iteration 67/1000 | Loss: 0.00001674
Iteration 68/1000 | Loss: 0.00001674
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001673
Iteration 71/1000 | Loss: 0.00001673
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001670
Iteration 82/1000 | Loss: 0.00001670
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001669
Iteration 85/1000 | Loss: 0.00001669
Iteration 86/1000 | Loss: 0.00001669
Iteration 87/1000 | Loss: 0.00001668
Iteration 88/1000 | Loss: 0.00001668
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001667
Iteration 92/1000 | Loss: 0.00001667
Iteration 93/1000 | Loss: 0.00001667
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001665
Iteration 99/1000 | Loss: 0.00001665
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001664
Iteration 102/1000 | Loss: 0.00001664
Iteration 103/1000 | Loss: 0.00001664
Iteration 104/1000 | Loss: 0.00001664
Iteration 105/1000 | Loss: 0.00001663
Iteration 106/1000 | Loss: 0.00001663
Iteration 107/1000 | Loss: 0.00001663
Iteration 108/1000 | Loss: 0.00001663
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001663
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001662
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001657
Iteration 134/1000 | Loss: 0.00001657
Iteration 135/1000 | Loss: 0.00001657
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001656
Iteration 138/1000 | Loss: 0.00001656
Iteration 139/1000 | Loss: 0.00001656
Iteration 140/1000 | Loss: 0.00001656
Iteration 141/1000 | Loss: 0.00001656
Iteration 142/1000 | Loss: 0.00001656
Iteration 143/1000 | Loss: 0.00001656
Iteration 144/1000 | Loss: 0.00001656
Iteration 145/1000 | Loss: 0.00001656
Iteration 146/1000 | Loss: 0.00001655
Iteration 147/1000 | Loss: 0.00001655
Iteration 148/1000 | Loss: 0.00001655
Iteration 149/1000 | Loss: 0.00001655
Iteration 150/1000 | Loss: 0.00001654
Iteration 151/1000 | Loss: 0.00001654
Iteration 152/1000 | Loss: 0.00001654
Iteration 153/1000 | Loss: 0.00001654
Iteration 154/1000 | Loss: 0.00001654
Iteration 155/1000 | Loss: 0.00001653
Iteration 156/1000 | Loss: 0.00001653
Iteration 157/1000 | Loss: 0.00001653
Iteration 158/1000 | Loss: 0.00001653
Iteration 159/1000 | Loss: 0.00001652
Iteration 160/1000 | Loss: 0.00001652
Iteration 161/1000 | Loss: 0.00001652
Iteration 162/1000 | Loss: 0.00001652
Iteration 163/1000 | Loss: 0.00001652
Iteration 164/1000 | Loss: 0.00001652
Iteration 165/1000 | Loss: 0.00001652
Iteration 166/1000 | Loss: 0.00001652
Iteration 167/1000 | Loss: 0.00001652
Iteration 168/1000 | Loss: 0.00001652
Iteration 169/1000 | Loss: 0.00001652
Iteration 170/1000 | Loss: 0.00001652
Iteration 171/1000 | Loss: 0.00001652
Iteration 172/1000 | Loss: 0.00001652
Iteration 173/1000 | Loss: 0.00001651
Iteration 174/1000 | Loss: 0.00001651
Iteration 175/1000 | Loss: 0.00001651
Iteration 176/1000 | Loss: 0.00001651
Iteration 177/1000 | Loss: 0.00001651
Iteration 178/1000 | Loss: 0.00001651
Iteration 179/1000 | Loss: 0.00001651
Iteration 180/1000 | Loss: 0.00001651
Iteration 181/1000 | Loss: 0.00001651
Iteration 182/1000 | Loss: 0.00001651
Iteration 183/1000 | Loss: 0.00001651
Iteration 184/1000 | Loss: 0.00001651
Iteration 185/1000 | Loss: 0.00001651
Iteration 186/1000 | Loss: 0.00001650
Iteration 187/1000 | Loss: 0.00001650
Iteration 188/1000 | Loss: 0.00001650
Iteration 189/1000 | Loss: 0.00001650
Iteration 190/1000 | Loss: 0.00001650
Iteration 191/1000 | Loss: 0.00001650
Iteration 192/1000 | Loss: 0.00001650
Iteration 193/1000 | Loss: 0.00001650
Iteration 194/1000 | Loss: 0.00001650
Iteration 195/1000 | Loss: 0.00001650
Iteration 196/1000 | Loss: 0.00001650
Iteration 197/1000 | Loss: 0.00001650
Iteration 198/1000 | Loss: 0.00001650
Iteration 199/1000 | Loss: 0.00001650
Iteration 200/1000 | Loss: 0.00001650
Iteration 201/1000 | Loss: 0.00001650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.649816840654239e-05, 1.649816840654239e-05, 1.649816840654239e-05, 1.649816840654239e-05, 1.649816840654239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.649816840654239e-05

Optimization complete. Final v2v error: 3.30389666557312 mm

Highest mean error: 4.738961219787598 mm for frame 73

Lowest mean error: 2.8550665378570557 mm for frame 165

Saving results

Total time: 43.36587071418762
