Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=243, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13608-13663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030001
Iteration 2/25 | Loss: 0.01030001
Iteration 3/25 | Loss: 0.01030001
Iteration 4/25 | Loss: 0.01030001
Iteration 5/25 | Loss: 0.01030000
Iteration 6/25 | Loss: 0.01030000
Iteration 7/25 | Loss: 0.01030000
Iteration 8/25 | Loss: 0.01030000
Iteration 9/25 | Loss: 0.01030000
Iteration 10/25 | Loss: 0.01030000
Iteration 11/25 | Loss: 0.01030000
Iteration 12/25 | Loss: 0.01029999
Iteration 13/25 | Loss: 0.01029999
Iteration 14/25 | Loss: 0.01029999
Iteration 15/25 | Loss: 0.01029999
Iteration 16/25 | Loss: 0.01029999
Iteration 17/25 | Loss: 0.01029999
Iteration 18/25 | Loss: 0.01029999
Iteration 19/25 | Loss: 0.01029998
Iteration 20/25 | Loss: 0.01029998
Iteration 21/25 | Loss: 0.01029998
Iteration 22/25 | Loss: 0.01029998
Iteration 23/25 | Loss: 0.01029998
Iteration 24/25 | Loss: 0.01029998
Iteration 25/25 | Loss: 0.01029998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56099832
Iteration 2/25 | Loss: 0.08383577
Iteration 3/25 | Loss: 0.08374614
Iteration 4/25 | Loss: 0.08374469
Iteration 5/25 | Loss: 0.08374469
Iteration 6/25 | Loss: 0.08374468
Iteration 7/25 | Loss: 0.08374468
Iteration 8/25 | Loss: 0.08374468
Iteration 9/25 | Loss: 0.08374468
Iteration 10/25 | Loss: 0.08374468
Iteration 11/25 | Loss: 0.08374468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08374468237161636, 0.08374468237161636, 0.08374468237161636, 0.08374468237161636, 0.08374468237161636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08374468237161636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08374468
Iteration 2/1000 | Loss: 0.00060370
Iteration 3/1000 | Loss: 0.00021392
Iteration 4/1000 | Loss: 0.00015156
Iteration 5/1000 | Loss: 0.00007059
Iteration 6/1000 | Loss: 0.00034790
Iteration 7/1000 | Loss: 0.00011506
Iteration 8/1000 | Loss: 0.00004799
Iteration 9/1000 | Loss: 0.00004688
Iteration 10/1000 | Loss: 0.00002987
Iteration 11/1000 | Loss: 0.00012717
Iteration 12/1000 | Loss: 0.00002423
Iteration 13/1000 | Loss: 0.00003666
Iteration 14/1000 | Loss: 0.00003109
Iteration 15/1000 | Loss: 0.00009473
Iteration 16/1000 | Loss: 0.00003741
Iteration 17/1000 | Loss: 0.00003868
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00003116
Iteration 20/1000 | Loss: 0.00004328
Iteration 21/1000 | Loss: 0.00002059
Iteration 22/1000 | Loss: 0.00001533
Iteration 23/1000 | Loss: 0.00003385
Iteration 24/1000 | Loss: 0.00001464
Iteration 25/1000 | Loss: 0.00002914
Iteration 26/1000 | Loss: 0.00001402
Iteration 27/1000 | Loss: 0.00005068
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00005129
Iteration 32/1000 | Loss: 0.00002342
Iteration 33/1000 | Loss: 0.00002864
Iteration 34/1000 | Loss: 0.00009145
Iteration 35/1000 | Loss: 0.00009616
Iteration 36/1000 | Loss: 0.00003115
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00003693
Iteration 40/1000 | Loss: 0.00003995
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001545
Iteration 43/1000 | Loss: 0.00003178
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001157
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001156
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001153
Iteration 62/1000 | Loss: 0.00004966
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001502
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001123
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001121
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001117
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001115
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001105
Iteration 99/1000 | Loss: 0.00001105
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001103
Iteration 104/1000 | Loss: 0.00001103
Iteration 105/1000 | Loss: 0.00001130
Iteration 106/1000 | Loss: 0.00001099
Iteration 107/1000 | Loss: 0.00001098
Iteration 108/1000 | Loss: 0.00001098
Iteration 109/1000 | Loss: 0.00001098
Iteration 110/1000 | Loss: 0.00001098
Iteration 111/1000 | Loss: 0.00001098
Iteration 112/1000 | Loss: 0.00001098
Iteration 113/1000 | Loss: 0.00001098
Iteration 114/1000 | Loss: 0.00001098
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001098
Iteration 120/1000 | Loss: 0.00001098
Iteration 121/1000 | Loss: 0.00001098
Iteration 122/1000 | Loss: 0.00001098
Iteration 123/1000 | Loss: 0.00001098
Iteration 124/1000 | Loss: 0.00001098
Iteration 125/1000 | Loss: 0.00001098
Iteration 126/1000 | Loss: 0.00001098
Iteration 127/1000 | Loss: 0.00001098
Iteration 128/1000 | Loss: 0.00001098
Iteration 129/1000 | Loss: 0.00001098
Iteration 130/1000 | Loss: 0.00001098
Iteration 131/1000 | Loss: 0.00001098
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001098
Iteration 136/1000 | Loss: 0.00001098
Iteration 137/1000 | Loss: 0.00001098
Iteration 138/1000 | Loss: 0.00001098
Iteration 139/1000 | Loss: 0.00001098
Iteration 140/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.09804514067946e-05, 1.09804514067946e-05, 1.09804514067946e-05, 1.09804514067946e-05, 1.09804514067946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.09804514067946e-05

Optimization complete. Final v2v error: 2.9016571044921875 mm

Highest mean error: 3.419081449508667 mm for frame 20

Lowest mean error: 2.6155309677124023 mm for frame 22

Saving results

Total time: 87.62675023078918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00638931
Iteration 2/25 | Loss: 0.00143292
Iteration 3/25 | Loss: 0.00129921
Iteration 4/25 | Loss: 0.00127492
Iteration 5/25 | Loss: 0.00126796
Iteration 6/25 | Loss: 0.00126563
Iteration 7/25 | Loss: 0.00125532
Iteration 8/25 | Loss: 0.00124861
Iteration 9/25 | Loss: 0.00124584
Iteration 10/25 | Loss: 0.00124357
Iteration 11/25 | Loss: 0.00124303
Iteration 12/25 | Loss: 0.00124286
Iteration 13/25 | Loss: 0.00124282
Iteration 14/25 | Loss: 0.00124282
Iteration 15/25 | Loss: 0.00124282
Iteration 16/25 | Loss: 0.00124282
Iteration 17/25 | Loss: 0.00124282
Iteration 18/25 | Loss: 0.00124281
Iteration 19/25 | Loss: 0.00124281
Iteration 20/25 | Loss: 0.00124281
Iteration 21/25 | Loss: 0.00124281
Iteration 22/25 | Loss: 0.00124281
Iteration 23/25 | Loss: 0.00124281
Iteration 24/25 | Loss: 0.00124281
Iteration 25/25 | Loss: 0.00124281

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32683647
Iteration 2/25 | Loss: 0.00174279
Iteration 3/25 | Loss: 0.00174279
Iteration 4/25 | Loss: 0.00174279
Iteration 5/25 | Loss: 0.00174279
Iteration 6/25 | Loss: 0.00174279
Iteration 7/25 | Loss: 0.00174279
Iteration 8/25 | Loss: 0.00174279
Iteration 9/25 | Loss: 0.00174279
Iteration 10/25 | Loss: 0.00174279
Iteration 11/25 | Loss: 0.00174279
Iteration 12/25 | Loss: 0.00174279
Iteration 13/25 | Loss: 0.00174279
Iteration 14/25 | Loss: 0.00174279
Iteration 15/25 | Loss: 0.00174279
Iteration 16/25 | Loss: 0.00174279
Iteration 17/25 | Loss: 0.00174279
Iteration 18/25 | Loss: 0.00174279
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017427869606763124, 0.0017427869606763124, 0.0017427869606763124, 0.0017427869606763124, 0.0017427869606763124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017427869606763124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174279
Iteration 2/1000 | Loss: 0.00006157
Iteration 3/1000 | Loss: 0.00004275
Iteration 4/1000 | Loss: 0.00003481
Iteration 5/1000 | Loss: 0.00003187
Iteration 6/1000 | Loss: 0.00002974
Iteration 7/1000 | Loss: 0.00002840
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002649
Iteration 10/1000 | Loss: 0.00008476
Iteration 11/1000 | Loss: 0.00002543
Iteration 12/1000 | Loss: 0.00002371
Iteration 13/1000 | Loss: 0.00002302
Iteration 14/1000 | Loss: 0.00002243
Iteration 15/1000 | Loss: 0.00002201
Iteration 16/1000 | Loss: 0.00002162
Iteration 17/1000 | Loss: 0.00002135
Iteration 18/1000 | Loss: 0.00002125
Iteration 19/1000 | Loss: 0.00002106
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002098
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002096
Iteration 24/1000 | Loss: 0.00002094
Iteration 25/1000 | Loss: 0.00002094
Iteration 26/1000 | Loss: 0.00002093
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002076
Iteration 31/1000 | Loss: 0.00002075
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002073
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002065
Iteration 36/1000 | Loss: 0.00002064
Iteration 37/1000 | Loss: 0.00002063
Iteration 38/1000 | Loss: 0.00039640
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002033
Iteration 41/1000 | Loss: 0.00001967
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001913
Iteration 44/1000 | Loss: 0.00001902
Iteration 45/1000 | Loss: 0.00001893
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001879
Iteration 48/1000 | Loss: 0.00001878
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001873
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001871
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001866
Iteration 62/1000 | Loss: 0.00001866
Iteration 63/1000 | Loss: 0.00001866
Iteration 64/1000 | Loss: 0.00001865
Iteration 65/1000 | Loss: 0.00001865
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001861
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001860
Iteration 74/1000 | Loss: 0.00001860
Iteration 75/1000 | Loss: 0.00001859
Iteration 76/1000 | Loss: 0.00001859
Iteration 77/1000 | Loss: 0.00001859
Iteration 78/1000 | Loss: 0.00001858
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001857
Iteration 81/1000 | Loss: 0.00001857
Iteration 82/1000 | Loss: 0.00001857
Iteration 83/1000 | Loss: 0.00001857
Iteration 84/1000 | Loss: 0.00001857
Iteration 85/1000 | Loss: 0.00001857
Iteration 86/1000 | Loss: 0.00001857
Iteration 87/1000 | Loss: 0.00001857
Iteration 88/1000 | Loss: 0.00001857
Iteration 89/1000 | Loss: 0.00001857
Iteration 90/1000 | Loss: 0.00001857
Iteration 91/1000 | Loss: 0.00001857
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001855
Iteration 101/1000 | Loss: 0.00001855
Iteration 102/1000 | Loss: 0.00001855
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001854
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001852
Iteration 110/1000 | Loss: 0.00001852
Iteration 111/1000 | Loss: 0.00001852
Iteration 112/1000 | Loss: 0.00001852
Iteration 113/1000 | Loss: 0.00001852
Iteration 114/1000 | Loss: 0.00001852
Iteration 115/1000 | Loss: 0.00001852
Iteration 116/1000 | Loss: 0.00001852
Iteration 117/1000 | Loss: 0.00001852
Iteration 118/1000 | Loss: 0.00001851
Iteration 119/1000 | Loss: 0.00001851
Iteration 120/1000 | Loss: 0.00001851
Iteration 121/1000 | Loss: 0.00001851
Iteration 122/1000 | Loss: 0.00001851
Iteration 123/1000 | Loss: 0.00001851
Iteration 124/1000 | Loss: 0.00001851
Iteration 125/1000 | Loss: 0.00001851
Iteration 126/1000 | Loss: 0.00001851
Iteration 127/1000 | Loss: 0.00001851
Iteration 128/1000 | Loss: 0.00001851
Iteration 129/1000 | Loss: 0.00001851
Iteration 130/1000 | Loss: 0.00001851
Iteration 131/1000 | Loss: 0.00001851
Iteration 132/1000 | Loss: 0.00001850
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001849
Iteration 145/1000 | Loss: 0.00001849
Iteration 146/1000 | Loss: 0.00001849
Iteration 147/1000 | Loss: 0.00001849
Iteration 148/1000 | Loss: 0.00001849
Iteration 149/1000 | Loss: 0.00001849
Iteration 150/1000 | Loss: 0.00001849
Iteration 151/1000 | Loss: 0.00001849
Iteration 152/1000 | Loss: 0.00001849
Iteration 153/1000 | Loss: 0.00001849
Iteration 154/1000 | Loss: 0.00001849
Iteration 155/1000 | Loss: 0.00001849
Iteration 156/1000 | Loss: 0.00001849
Iteration 157/1000 | Loss: 0.00001849
Iteration 158/1000 | Loss: 0.00001849
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001849
Iteration 167/1000 | Loss: 0.00001849
Iteration 168/1000 | Loss: 0.00001849
Iteration 169/1000 | Loss: 0.00001849
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001849
Iteration 172/1000 | Loss: 0.00001849
Iteration 173/1000 | Loss: 0.00001849
Iteration 174/1000 | Loss: 0.00001849
Iteration 175/1000 | Loss: 0.00001849
Iteration 176/1000 | Loss: 0.00001849
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001849
Iteration 179/1000 | Loss: 0.00001849
Iteration 180/1000 | Loss: 0.00001849
Iteration 181/1000 | Loss: 0.00001849
Iteration 182/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.848855936259497e-05, 1.848855936259497e-05, 1.848855936259497e-05, 1.848855936259497e-05, 1.848855936259497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.848855936259497e-05

Optimization complete. Final v2v error: 3.660017728805542 mm

Highest mean error: 5.262262344360352 mm for frame 63

Lowest mean error: 3.065136194229126 mm for frame 133

Saving results

Total time: 83.97767162322998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823989
Iteration 2/25 | Loss: 0.00127600
Iteration 3/25 | Loss: 0.00119842
Iteration 4/25 | Loss: 0.00118866
Iteration 5/25 | Loss: 0.00118559
Iteration 6/25 | Loss: 0.00118488
Iteration 7/25 | Loss: 0.00118488
Iteration 8/25 | Loss: 0.00118488
Iteration 9/25 | Loss: 0.00118488
Iteration 10/25 | Loss: 0.00118488
Iteration 11/25 | Loss: 0.00118488
Iteration 12/25 | Loss: 0.00118488
Iteration 13/25 | Loss: 0.00118488
Iteration 14/25 | Loss: 0.00118488
Iteration 15/25 | Loss: 0.00118488
Iteration 16/25 | Loss: 0.00118488
Iteration 17/25 | Loss: 0.00118488
Iteration 18/25 | Loss: 0.00118488
Iteration 19/25 | Loss: 0.00118488
Iteration 20/25 | Loss: 0.00118488
Iteration 21/25 | Loss: 0.00118488
Iteration 22/25 | Loss: 0.00118488
Iteration 23/25 | Loss: 0.00118488
Iteration 24/25 | Loss: 0.00118488
Iteration 25/25 | Loss: 0.00118488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49147701
Iteration 2/25 | Loss: 0.00148163
Iteration 3/25 | Loss: 0.00148162
Iteration 4/25 | Loss: 0.00148162
Iteration 5/25 | Loss: 0.00148162
Iteration 6/25 | Loss: 0.00148162
Iteration 7/25 | Loss: 0.00148162
Iteration 8/25 | Loss: 0.00148162
Iteration 9/25 | Loss: 0.00148162
Iteration 10/25 | Loss: 0.00148162
Iteration 11/25 | Loss: 0.00148162
Iteration 12/25 | Loss: 0.00148162
Iteration 13/25 | Loss: 0.00148162
Iteration 14/25 | Loss: 0.00148162
Iteration 15/25 | Loss: 0.00148162
Iteration 16/25 | Loss: 0.00148162
Iteration 17/25 | Loss: 0.00148162
Iteration 18/25 | Loss: 0.00148162
Iteration 19/25 | Loss: 0.00148162
Iteration 20/25 | Loss: 0.00148162
Iteration 21/25 | Loss: 0.00148162
Iteration 22/25 | Loss: 0.00148162
Iteration 23/25 | Loss: 0.00148162
Iteration 24/25 | Loss: 0.00148162
Iteration 25/25 | Loss: 0.00148162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148162
Iteration 2/1000 | Loss: 0.00001997
Iteration 3/1000 | Loss: 0.00001365
Iteration 4/1000 | Loss: 0.00001207
Iteration 5/1000 | Loss: 0.00001134
Iteration 6/1000 | Loss: 0.00001092
Iteration 7/1000 | Loss: 0.00001036
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000994
Iteration 10/1000 | Loss: 0.00000972
Iteration 11/1000 | Loss: 0.00000946
Iteration 12/1000 | Loss: 0.00000944
Iteration 13/1000 | Loss: 0.00000937
Iteration 14/1000 | Loss: 0.00000935
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000924
Iteration 17/1000 | Loss: 0.00000923
Iteration 18/1000 | Loss: 0.00000922
Iteration 19/1000 | Loss: 0.00000909
Iteration 20/1000 | Loss: 0.00000909
Iteration 21/1000 | Loss: 0.00000909
Iteration 22/1000 | Loss: 0.00000907
Iteration 23/1000 | Loss: 0.00000905
Iteration 24/1000 | Loss: 0.00000902
Iteration 25/1000 | Loss: 0.00000902
Iteration 26/1000 | Loss: 0.00000902
Iteration 27/1000 | Loss: 0.00000901
Iteration 28/1000 | Loss: 0.00000901
Iteration 29/1000 | Loss: 0.00000899
Iteration 30/1000 | Loss: 0.00000898
Iteration 31/1000 | Loss: 0.00000896
Iteration 32/1000 | Loss: 0.00000896
Iteration 33/1000 | Loss: 0.00000894
Iteration 34/1000 | Loss: 0.00000894
Iteration 35/1000 | Loss: 0.00000893
Iteration 36/1000 | Loss: 0.00000892
Iteration 37/1000 | Loss: 0.00000892
Iteration 38/1000 | Loss: 0.00000891
Iteration 39/1000 | Loss: 0.00000890
Iteration 40/1000 | Loss: 0.00000890
Iteration 41/1000 | Loss: 0.00000889
Iteration 42/1000 | Loss: 0.00000889
Iteration 43/1000 | Loss: 0.00000889
Iteration 44/1000 | Loss: 0.00000888
Iteration 45/1000 | Loss: 0.00000888
Iteration 46/1000 | Loss: 0.00000887
Iteration 47/1000 | Loss: 0.00000887
Iteration 48/1000 | Loss: 0.00000887
Iteration 49/1000 | Loss: 0.00000886
Iteration 50/1000 | Loss: 0.00000886
Iteration 51/1000 | Loss: 0.00000886
Iteration 52/1000 | Loss: 0.00000886
Iteration 53/1000 | Loss: 0.00000885
Iteration 54/1000 | Loss: 0.00000885
Iteration 55/1000 | Loss: 0.00000885
Iteration 56/1000 | Loss: 0.00000885
Iteration 57/1000 | Loss: 0.00000885
Iteration 58/1000 | Loss: 0.00000885
Iteration 59/1000 | Loss: 0.00000884
Iteration 60/1000 | Loss: 0.00000884
Iteration 61/1000 | Loss: 0.00000884
Iteration 62/1000 | Loss: 0.00000884
Iteration 63/1000 | Loss: 0.00000883
Iteration 64/1000 | Loss: 0.00000883
Iteration 65/1000 | Loss: 0.00000883
Iteration 66/1000 | Loss: 0.00000883
Iteration 67/1000 | Loss: 0.00000883
Iteration 68/1000 | Loss: 0.00000882
Iteration 69/1000 | Loss: 0.00000882
Iteration 70/1000 | Loss: 0.00000882
Iteration 71/1000 | Loss: 0.00000882
Iteration 72/1000 | Loss: 0.00000882
Iteration 73/1000 | Loss: 0.00000882
Iteration 74/1000 | Loss: 0.00000882
Iteration 75/1000 | Loss: 0.00000882
Iteration 76/1000 | Loss: 0.00000881
Iteration 77/1000 | Loss: 0.00000881
Iteration 78/1000 | Loss: 0.00000881
Iteration 79/1000 | Loss: 0.00000881
Iteration 80/1000 | Loss: 0.00000881
Iteration 81/1000 | Loss: 0.00000881
Iteration 82/1000 | Loss: 0.00000880
Iteration 83/1000 | Loss: 0.00000880
Iteration 84/1000 | Loss: 0.00000879
Iteration 85/1000 | Loss: 0.00000879
Iteration 86/1000 | Loss: 0.00000879
Iteration 87/1000 | Loss: 0.00000879
Iteration 88/1000 | Loss: 0.00000879
Iteration 89/1000 | Loss: 0.00000878
Iteration 90/1000 | Loss: 0.00000877
Iteration 91/1000 | Loss: 0.00000877
Iteration 92/1000 | Loss: 0.00000876
Iteration 93/1000 | Loss: 0.00000876
Iteration 94/1000 | Loss: 0.00000876
Iteration 95/1000 | Loss: 0.00000876
Iteration 96/1000 | Loss: 0.00000875
Iteration 97/1000 | Loss: 0.00000875
Iteration 98/1000 | Loss: 0.00000874
Iteration 99/1000 | Loss: 0.00000874
Iteration 100/1000 | Loss: 0.00000873
Iteration 101/1000 | Loss: 0.00000873
Iteration 102/1000 | Loss: 0.00000873
Iteration 103/1000 | Loss: 0.00000872
Iteration 104/1000 | Loss: 0.00000872
Iteration 105/1000 | Loss: 0.00000872
Iteration 106/1000 | Loss: 0.00000872
Iteration 107/1000 | Loss: 0.00000872
Iteration 108/1000 | Loss: 0.00000871
Iteration 109/1000 | Loss: 0.00000871
Iteration 110/1000 | Loss: 0.00000871
Iteration 111/1000 | Loss: 0.00000871
Iteration 112/1000 | Loss: 0.00000870
Iteration 113/1000 | Loss: 0.00000870
Iteration 114/1000 | Loss: 0.00000870
Iteration 115/1000 | Loss: 0.00000870
Iteration 116/1000 | Loss: 0.00000869
Iteration 117/1000 | Loss: 0.00000869
Iteration 118/1000 | Loss: 0.00000869
Iteration 119/1000 | Loss: 0.00000869
Iteration 120/1000 | Loss: 0.00000869
Iteration 121/1000 | Loss: 0.00000869
Iteration 122/1000 | Loss: 0.00000869
Iteration 123/1000 | Loss: 0.00000869
Iteration 124/1000 | Loss: 0.00000869
Iteration 125/1000 | Loss: 0.00000869
Iteration 126/1000 | Loss: 0.00000869
Iteration 127/1000 | Loss: 0.00000869
Iteration 128/1000 | Loss: 0.00000869
Iteration 129/1000 | Loss: 0.00000869
Iteration 130/1000 | Loss: 0.00000868
Iteration 131/1000 | Loss: 0.00000868
Iteration 132/1000 | Loss: 0.00000868
Iteration 133/1000 | Loss: 0.00000868
Iteration 134/1000 | Loss: 0.00000867
Iteration 135/1000 | Loss: 0.00000867
Iteration 136/1000 | Loss: 0.00000867
Iteration 137/1000 | Loss: 0.00000867
Iteration 138/1000 | Loss: 0.00000867
Iteration 139/1000 | Loss: 0.00000867
Iteration 140/1000 | Loss: 0.00000867
Iteration 141/1000 | Loss: 0.00000867
Iteration 142/1000 | Loss: 0.00000867
Iteration 143/1000 | Loss: 0.00000867
Iteration 144/1000 | Loss: 0.00000867
Iteration 145/1000 | Loss: 0.00000867
Iteration 146/1000 | Loss: 0.00000866
Iteration 147/1000 | Loss: 0.00000866
Iteration 148/1000 | Loss: 0.00000866
Iteration 149/1000 | Loss: 0.00000866
Iteration 150/1000 | Loss: 0.00000866
Iteration 151/1000 | Loss: 0.00000866
Iteration 152/1000 | Loss: 0.00000866
Iteration 153/1000 | Loss: 0.00000866
Iteration 154/1000 | Loss: 0.00000866
Iteration 155/1000 | Loss: 0.00000866
Iteration 156/1000 | Loss: 0.00000866
Iteration 157/1000 | Loss: 0.00000866
Iteration 158/1000 | Loss: 0.00000866
Iteration 159/1000 | Loss: 0.00000866
Iteration 160/1000 | Loss: 0.00000866
Iteration 161/1000 | Loss: 0.00000866
Iteration 162/1000 | Loss: 0.00000866
Iteration 163/1000 | Loss: 0.00000866
Iteration 164/1000 | Loss: 0.00000866
Iteration 165/1000 | Loss: 0.00000866
Iteration 166/1000 | Loss: 0.00000866
Iteration 167/1000 | Loss: 0.00000866
Iteration 168/1000 | Loss: 0.00000866
Iteration 169/1000 | Loss: 0.00000866
Iteration 170/1000 | Loss: 0.00000866
Iteration 171/1000 | Loss: 0.00000866
Iteration 172/1000 | Loss: 0.00000866
Iteration 173/1000 | Loss: 0.00000866
Iteration 174/1000 | Loss: 0.00000866
Iteration 175/1000 | Loss: 0.00000866
Iteration 176/1000 | Loss: 0.00000866
Iteration 177/1000 | Loss: 0.00000866
Iteration 178/1000 | Loss: 0.00000866
Iteration 179/1000 | Loss: 0.00000866
Iteration 180/1000 | Loss: 0.00000866
Iteration 181/1000 | Loss: 0.00000866
Iteration 182/1000 | Loss: 0.00000866
Iteration 183/1000 | Loss: 0.00000866
Iteration 184/1000 | Loss: 0.00000866
Iteration 185/1000 | Loss: 0.00000866
Iteration 186/1000 | Loss: 0.00000866
Iteration 187/1000 | Loss: 0.00000866
Iteration 188/1000 | Loss: 0.00000866
Iteration 189/1000 | Loss: 0.00000866
Iteration 190/1000 | Loss: 0.00000866
Iteration 191/1000 | Loss: 0.00000866
Iteration 192/1000 | Loss: 0.00000866
Iteration 193/1000 | Loss: 0.00000866
Iteration 194/1000 | Loss: 0.00000866
Iteration 195/1000 | Loss: 0.00000866
Iteration 196/1000 | Loss: 0.00000866
Iteration 197/1000 | Loss: 0.00000866
Iteration 198/1000 | Loss: 0.00000866
Iteration 199/1000 | Loss: 0.00000866
Iteration 200/1000 | Loss: 0.00000866
Iteration 201/1000 | Loss: 0.00000866
Iteration 202/1000 | Loss: 0.00000866
Iteration 203/1000 | Loss: 0.00000866
Iteration 204/1000 | Loss: 0.00000866
Iteration 205/1000 | Loss: 0.00000866
Iteration 206/1000 | Loss: 0.00000866
Iteration 207/1000 | Loss: 0.00000866
Iteration 208/1000 | Loss: 0.00000866
Iteration 209/1000 | Loss: 0.00000866
Iteration 210/1000 | Loss: 0.00000866
Iteration 211/1000 | Loss: 0.00000866
Iteration 212/1000 | Loss: 0.00000866
Iteration 213/1000 | Loss: 0.00000866
Iteration 214/1000 | Loss: 0.00000866
Iteration 215/1000 | Loss: 0.00000866
Iteration 216/1000 | Loss: 0.00000866
Iteration 217/1000 | Loss: 0.00000866
Iteration 218/1000 | Loss: 0.00000866
Iteration 219/1000 | Loss: 0.00000866
Iteration 220/1000 | Loss: 0.00000866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [8.656007594254334e-06, 8.656007594254334e-06, 8.656007594254334e-06, 8.656007594254334e-06, 8.656007594254334e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.656007594254334e-06

Optimization complete. Final v2v error: 2.560913324356079 mm

Highest mean error: 2.929255485534668 mm for frame 52

Lowest mean error: 2.426239252090454 mm for frame 104

Saving results

Total time: 40.797160148620605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679243
Iteration 2/25 | Loss: 0.00173499
Iteration 3/25 | Loss: 0.00146667
Iteration 4/25 | Loss: 0.00141572
Iteration 5/25 | Loss: 0.00139470
Iteration 6/25 | Loss: 0.00134770
Iteration 7/25 | Loss: 0.00133332
Iteration 8/25 | Loss: 0.00133869
Iteration 9/25 | Loss: 0.00133678
Iteration 10/25 | Loss: 0.00132961
Iteration 11/25 | Loss: 0.00132668
Iteration 12/25 | Loss: 0.00132401
Iteration 13/25 | Loss: 0.00132089
Iteration 14/25 | Loss: 0.00131751
Iteration 15/25 | Loss: 0.00132252
Iteration 16/25 | Loss: 0.00132164
Iteration 17/25 | Loss: 0.00131910
Iteration 18/25 | Loss: 0.00131865
Iteration 19/25 | Loss: 0.00132235
Iteration 20/25 | Loss: 0.00131787
Iteration 21/25 | Loss: 0.00131563
Iteration 22/25 | Loss: 0.00131519
Iteration 23/25 | Loss: 0.00131503
Iteration 24/25 | Loss: 0.00131497
Iteration 25/25 | Loss: 0.00131496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18236208
Iteration 2/25 | Loss: 0.00136181
Iteration 3/25 | Loss: 0.00136177
Iteration 4/25 | Loss: 0.00136176
Iteration 5/25 | Loss: 0.00136176
Iteration 6/25 | Loss: 0.00136176
Iteration 7/25 | Loss: 0.00136176
Iteration 8/25 | Loss: 0.00136176
Iteration 9/25 | Loss: 0.00136176
Iteration 10/25 | Loss: 0.00136176
Iteration 11/25 | Loss: 0.00136176
Iteration 12/25 | Loss: 0.00136176
Iteration 13/25 | Loss: 0.00136176
Iteration 14/25 | Loss: 0.00136176
Iteration 15/25 | Loss: 0.00136176
Iteration 16/25 | Loss: 0.00136176
Iteration 17/25 | Loss: 0.00136176
Iteration 18/25 | Loss: 0.00136176
Iteration 19/25 | Loss: 0.00136176
Iteration 20/25 | Loss: 0.00136176
Iteration 21/25 | Loss: 0.00136176
Iteration 22/25 | Loss: 0.00136176
Iteration 23/25 | Loss: 0.00136176
Iteration 24/25 | Loss: 0.00136176
Iteration 25/25 | Loss: 0.00136176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013617612421512604, 0.0013617612421512604, 0.0013617612421512604, 0.0013617612421512604, 0.0013617612421512604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013617612421512604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136176
Iteration 2/1000 | Loss: 0.00003572
Iteration 3/1000 | Loss: 0.00002528
Iteration 4/1000 | Loss: 0.00002245
Iteration 5/1000 | Loss: 0.00002096
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00001970
Iteration 8/1000 | Loss: 0.00001917
Iteration 9/1000 | Loss: 0.00001870
Iteration 10/1000 | Loss: 0.00001828
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00008852
Iteration 13/1000 | Loss: 0.00001949
Iteration 14/1000 | Loss: 0.00001789
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001614
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001580
Iteration 21/1000 | Loss: 0.00001571
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001563
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001562
Iteration 30/1000 | Loss: 0.00001561
Iteration 31/1000 | Loss: 0.00001553
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001550
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001541
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001540
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001540
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001540
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001539
Iteration 50/1000 | Loss: 0.00001539
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001537
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001536
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001533
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001531
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001530
Iteration 72/1000 | Loss: 0.00001530
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001529
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001527
Iteration 101/1000 | Loss: 0.00001527
Iteration 102/1000 | Loss: 0.00001527
Iteration 103/1000 | Loss: 0.00001527
Iteration 104/1000 | Loss: 0.00001527
Iteration 105/1000 | Loss: 0.00001527
Iteration 106/1000 | Loss: 0.00001527
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001526
Iteration 109/1000 | Loss: 0.00001526
Iteration 110/1000 | Loss: 0.00001526
Iteration 111/1000 | Loss: 0.00001526
Iteration 112/1000 | Loss: 0.00001526
Iteration 113/1000 | Loss: 0.00001526
Iteration 114/1000 | Loss: 0.00001526
Iteration 115/1000 | Loss: 0.00001526
Iteration 116/1000 | Loss: 0.00001526
Iteration 117/1000 | Loss: 0.00001526
Iteration 118/1000 | Loss: 0.00001526
Iteration 119/1000 | Loss: 0.00001526
Iteration 120/1000 | Loss: 0.00001525
Iteration 121/1000 | Loss: 0.00001525
Iteration 122/1000 | Loss: 0.00001525
Iteration 123/1000 | Loss: 0.00001525
Iteration 124/1000 | Loss: 0.00001525
Iteration 125/1000 | Loss: 0.00001525
Iteration 126/1000 | Loss: 0.00001525
Iteration 127/1000 | Loss: 0.00001525
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001525
Iteration 138/1000 | Loss: 0.00001525
Iteration 139/1000 | Loss: 0.00001525
Iteration 140/1000 | Loss: 0.00001525
Iteration 141/1000 | Loss: 0.00001525
Iteration 142/1000 | Loss: 0.00001525
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001525
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.525435163785005e-05, 1.525435163785005e-05, 1.525435163785005e-05, 1.525435163785005e-05, 1.525435163785005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.525435163785005e-05

Optimization complete. Final v2v error: 3.2704856395721436 mm

Highest mean error: 3.964141368865967 mm for frame 18

Lowest mean error: 2.8322055339813232 mm for frame 134

Saving results

Total time: 83.13534021377563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389700
Iteration 2/25 | Loss: 0.00133876
Iteration 3/25 | Loss: 0.00124809
Iteration 4/25 | Loss: 0.00123831
Iteration 5/25 | Loss: 0.00123619
Iteration 6/25 | Loss: 0.00123619
Iteration 7/25 | Loss: 0.00123619
Iteration 8/25 | Loss: 0.00123619
Iteration 9/25 | Loss: 0.00123619
Iteration 10/25 | Loss: 0.00123619
Iteration 11/25 | Loss: 0.00123619
Iteration 12/25 | Loss: 0.00123619
Iteration 13/25 | Loss: 0.00123619
Iteration 14/25 | Loss: 0.00123619
Iteration 15/25 | Loss: 0.00123619
Iteration 16/25 | Loss: 0.00123619
Iteration 17/25 | Loss: 0.00123619
Iteration 18/25 | Loss: 0.00123619
Iteration 19/25 | Loss: 0.00123619
Iteration 20/25 | Loss: 0.00123619
Iteration 21/25 | Loss: 0.00123619
Iteration 22/25 | Loss: 0.00123619
Iteration 23/25 | Loss: 0.00123619
Iteration 24/25 | Loss: 0.00123619
Iteration 25/25 | Loss: 0.00123619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29021883
Iteration 2/25 | Loss: 0.00156110
Iteration 3/25 | Loss: 0.00156110
Iteration 4/25 | Loss: 0.00156110
Iteration 5/25 | Loss: 0.00156110
Iteration 6/25 | Loss: 0.00156110
Iteration 7/25 | Loss: 0.00156110
Iteration 8/25 | Loss: 0.00156110
Iteration 9/25 | Loss: 0.00156110
Iteration 10/25 | Loss: 0.00156110
Iteration 11/25 | Loss: 0.00156110
Iteration 12/25 | Loss: 0.00156110
Iteration 13/25 | Loss: 0.00156110
Iteration 14/25 | Loss: 0.00156110
Iteration 15/25 | Loss: 0.00156110
Iteration 16/25 | Loss: 0.00156110
Iteration 17/25 | Loss: 0.00156110
Iteration 18/25 | Loss: 0.00156110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015610953560099006, 0.0015610953560099006, 0.0015610953560099006, 0.0015610953560099006, 0.0015610953560099006]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015610953560099006

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156110
Iteration 2/1000 | Loss: 0.00003497
Iteration 3/1000 | Loss: 0.00002328
Iteration 4/1000 | Loss: 0.00001895
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001472
Iteration 9/1000 | Loss: 0.00001444
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001404
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001347
Iteration 18/1000 | Loss: 0.00001347
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001336
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001330
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001328
Iteration 30/1000 | Loss: 0.00001327
Iteration 31/1000 | Loss: 0.00001321
Iteration 32/1000 | Loss: 0.00001319
Iteration 33/1000 | Loss: 0.00001318
Iteration 34/1000 | Loss: 0.00001318
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00001313
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001310
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001310
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001308
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001307
Iteration 71/1000 | Loss: 0.00001307
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001303
Iteration 84/1000 | Loss: 0.00001303
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001302
Iteration 88/1000 | Loss: 0.00001302
Iteration 89/1000 | Loss: 0.00001302
Iteration 90/1000 | Loss: 0.00001301
Iteration 91/1000 | Loss: 0.00001301
Iteration 92/1000 | Loss: 0.00001301
Iteration 93/1000 | Loss: 0.00001301
Iteration 94/1000 | Loss: 0.00001300
Iteration 95/1000 | Loss: 0.00001300
Iteration 96/1000 | Loss: 0.00001300
Iteration 97/1000 | Loss: 0.00001300
Iteration 98/1000 | Loss: 0.00001300
Iteration 99/1000 | Loss: 0.00001300
Iteration 100/1000 | Loss: 0.00001299
Iteration 101/1000 | Loss: 0.00001299
Iteration 102/1000 | Loss: 0.00001299
Iteration 103/1000 | Loss: 0.00001299
Iteration 104/1000 | Loss: 0.00001299
Iteration 105/1000 | Loss: 0.00001299
Iteration 106/1000 | Loss: 0.00001299
Iteration 107/1000 | Loss: 0.00001299
Iteration 108/1000 | Loss: 0.00001299
Iteration 109/1000 | Loss: 0.00001299
Iteration 110/1000 | Loss: 0.00001299
Iteration 111/1000 | Loss: 0.00001298
Iteration 112/1000 | Loss: 0.00001298
Iteration 113/1000 | Loss: 0.00001298
Iteration 114/1000 | Loss: 0.00001298
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001297
Iteration 118/1000 | Loss: 0.00001297
Iteration 119/1000 | Loss: 0.00001297
Iteration 120/1000 | Loss: 0.00001297
Iteration 121/1000 | Loss: 0.00001297
Iteration 122/1000 | Loss: 0.00001297
Iteration 123/1000 | Loss: 0.00001297
Iteration 124/1000 | Loss: 0.00001297
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001296
Iteration 127/1000 | Loss: 0.00001296
Iteration 128/1000 | Loss: 0.00001295
Iteration 129/1000 | Loss: 0.00001295
Iteration 130/1000 | Loss: 0.00001295
Iteration 131/1000 | Loss: 0.00001295
Iteration 132/1000 | Loss: 0.00001295
Iteration 133/1000 | Loss: 0.00001295
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Iteration 137/1000 | Loss: 0.00001294
Iteration 138/1000 | Loss: 0.00001294
Iteration 139/1000 | Loss: 0.00001294
Iteration 140/1000 | Loss: 0.00001294
Iteration 141/1000 | Loss: 0.00001294
Iteration 142/1000 | Loss: 0.00001294
Iteration 143/1000 | Loss: 0.00001293
Iteration 144/1000 | Loss: 0.00001293
Iteration 145/1000 | Loss: 0.00001293
Iteration 146/1000 | Loss: 0.00001293
Iteration 147/1000 | Loss: 0.00001293
Iteration 148/1000 | Loss: 0.00001293
Iteration 149/1000 | Loss: 0.00001293
Iteration 150/1000 | Loss: 0.00001293
Iteration 151/1000 | Loss: 0.00001292
Iteration 152/1000 | Loss: 0.00001292
Iteration 153/1000 | Loss: 0.00001292
Iteration 154/1000 | Loss: 0.00001292
Iteration 155/1000 | Loss: 0.00001292
Iteration 156/1000 | Loss: 0.00001292
Iteration 157/1000 | Loss: 0.00001292
Iteration 158/1000 | Loss: 0.00001292
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001291
Iteration 165/1000 | Loss: 0.00001291
Iteration 166/1000 | Loss: 0.00001291
Iteration 167/1000 | Loss: 0.00001291
Iteration 168/1000 | Loss: 0.00001291
Iteration 169/1000 | Loss: 0.00001291
Iteration 170/1000 | Loss: 0.00001291
Iteration 171/1000 | Loss: 0.00001290
Iteration 172/1000 | Loss: 0.00001290
Iteration 173/1000 | Loss: 0.00001290
Iteration 174/1000 | Loss: 0.00001290
Iteration 175/1000 | Loss: 0.00001290
Iteration 176/1000 | Loss: 0.00001290
Iteration 177/1000 | Loss: 0.00001290
Iteration 178/1000 | Loss: 0.00001290
Iteration 179/1000 | Loss: 0.00001290
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001289
Iteration 183/1000 | Loss: 0.00001289
Iteration 184/1000 | Loss: 0.00001289
Iteration 185/1000 | Loss: 0.00001289
Iteration 186/1000 | Loss: 0.00001289
Iteration 187/1000 | Loss: 0.00001289
Iteration 188/1000 | Loss: 0.00001289
Iteration 189/1000 | Loss: 0.00001289
Iteration 190/1000 | Loss: 0.00001289
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001289
Iteration 193/1000 | Loss: 0.00001289
Iteration 194/1000 | Loss: 0.00001289
Iteration 195/1000 | Loss: 0.00001289
Iteration 196/1000 | Loss: 0.00001288
Iteration 197/1000 | Loss: 0.00001288
Iteration 198/1000 | Loss: 0.00001288
Iteration 199/1000 | Loss: 0.00001288
Iteration 200/1000 | Loss: 0.00001288
Iteration 201/1000 | Loss: 0.00001288
Iteration 202/1000 | Loss: 0.00001288
Iteration 203/1000 | Loss: 0.00001288
Iteration 204/1000 | Loss: 0.00001287
Iteration 205/1000 | Loss: 0.00001287
Iteration 206/1000 | Loss: 0.00001287
Iteration 207/1000 | Loss: 0.00001287
Iteration 208/1000 | Loss: 0.00001287
Iteration 209/1000 | Loss: 0.00001287
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001287
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Iteration 222/1000 | Loss: 0.00001286
Iteration 223/1000 | Loss: 0.00001285
Iteration 224/1000 | Loss: 0.00001285
Iteration 225/1000 | Loss: 0.00001285
Iteration 226/1000 | Loss: 0.00001285
Iteration 227/1000 | Loss: 0.00001285
Iteration 228/1000 | Loss: 0.00001285
Iteration 229/1000 | Loss: 0.00001285
Iteration 230/1000 | Loss: 0.00001285
Iteration 231/1000 | Loss: 0.00001285
Iteration 232/1000 | Loss: 0.00001285
Iteration 233/1000 | Loss: 0.00001285
Iteration 234/1000 | Loss: 0.00001285
Iteration 235/1000 | Loss: 0.00001284
Iteration 236/1000 | Loss: 0.00001284
Iteration 237/1000 | Loss: 0.00001284
Iteration 238/1000 | Loss: 0.00001284
Iteration 239/1000 | Loss: 0.00001284
Iteration 240/1000 | Loss: 0.00001284
Iteration 241/1000 | Loss: 0.00001284
Iteration 242/1000 | Loss: 0.00001284
Iteration 243/1000 | Loss: 0.00001284
Iteration 244/1000 | Loss: 0.00001284
Iteration 245/1000 | Loss: 0.00001284
Iteration 246/1000 | Loss: 0.00001283
Iteration 247/1000 | Loss: 0.00001283
Iteration 248/1000 | Loss: 0.00001283
Iteration 249/1000 | Loss: 0.00001283
Iteration 250/1000 | Loss: 0.00001283
Iteration 251/1000 | Loss: 0.00001283
Iteration 252/1000 | Loss: 0.00001283
Iteration 253/1000 | Loss: 0.00001283
Iteration 254/1000 | Loss: 0.00001283
Iteration 255/1000 | Loss: 0.00001283
Iteration 256/1000 | Loss: 0.00001283
Iteration 257/1000 | Loss: 0.00001283
Iteration 258/1000 | Loss: 0.00001283
Iteration 259/1000 | Loss: 0.00001283
Iteration 260/1000 | Loss: 0.00001283
Iteration 261/1000 | Loss: 0.00001282
Iteration 262/1000 | Loss: 0.00001282
Iteration 263/1000 | Loss: 0.00001282
Iteration 264/1000 | Loss: 0.00001282
Iteration 265/1000 | Loss: 0.00001282
Iteration 266/1000 | Loss: 0.00001282
Iteration 267/1000 | Loss: 0.00001282
Iteration 268/1000 | Loss: 0.00001281
Iteration 269/1000 | Loss: 0.00001281
Iteration 270/1000 | Loss: 0.00001281
Iteration 271/1000 | Loss: 0.00001281
Iteration 272/1000 | Loss: 0.00001281
Iteration 273/1000 | Loss: 0.00001281
Iteration 274/1000 | Loss: 0.00001281
Iteration 275/1000 | Loss: 0.00001281
Iteration 276/1000 | Loss: 0.00001281
Iteration 277/1000 | Loss: 0.00001281
Iteration 278/1000 | Loss: 0.00001281
Iteration 279/1000 | Loss: 0.00001281
Iteration 280/1000 | Loss: 0.00001281
Iteration 281/1000 | Loss: 0.00001281
Iteration 282/1000 | Loss: 0.00001281
Iteration 283/1000 | Loss: 0.00001281
Iteration 284/1000 | Loss: 0.00001281
Iteration 285/1000 | Loss: 0.00001280
Iteration 286/1000 | Loss: 0.00001280
Iteration 287/1000 | Loss: 0.00001280
Iteration 288/1000 | Loss: 0.00001280
Iteration 289/1000 | Loss: 0.00001280
Iteration 290/1000 | Loss: 0.00001280
Iteration 291/1000 | Loss: 0.00001280
Iteration 292/1000 | Loss: 0.00001279
Iteration 293/1000 | Loss: 0.00001279
Iteration 294/1000 | Loss: 0.00001279
Iteration 295/1000 | Loss: 0.00001279
Iteration 296/1000 | Loss: 0.00001279
Iteration 297/1000 | Loss: 0.00001279
Iteration 298/1000 | Loss: 0.00001279
Iteration 299/1000 | Loss: 0.00001279
Iteration 300/1000 | Loss: 0.00001279
Iteration 301/1000 | Loss: 0.00001279
Iteration 302/1000 | Loss: 0.00001279
Iteration 303/1000 | Loss: 0.00001279
Iteration 304/1000 | Loss: 0.00001279
Iteration 305/1000 | Loss: 0.00001279
Iteration 306/1000 | Loss: 0.00001279
Iteration 307/1000 | Loss: 0.00001279
Iteration 308/1000 | Loss: 0.00001279
Iteration 309/1000 | Loss: 0.00001279
Iteration 310/1000 | Loss: 0.00001279
Iteration 311/1000 | Loss: 0.00001279
Iteration 312/1000 | Loss: 0.00001279
Iteration 313/1000 | Loss: 0.00001279
Iteration 314/1000 | Loss: 0.00001279
Iteration 315/1000 | Loss: 0.00001279
Iteration 316/1000 | Loss: 0.00001279
Iteration 317/1000 | Loss: 0.00001279
Iteration 318/1000 | Loss: 0.00001279
Iteration 319/1000 | Loss: 0.00001279
Iteration 320/1000 | Loss: 0.00001279
Iteration 321/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [1.2789312677341513e-05, 1.2789312677341513e-05, 1.2789312677341513e-05, 1.2789312677341513e-05, 1.2789312677341513e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2789312677341513e-05

Optimization complete. Final v2v error: 2.971349000930786 mm

Highest mean error: 3.4099202156066895 mm for frame 87

Lowest mean error: 2.4625680446624756 mm for frame 16

Saving results

Total time: 51.49311280250549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036485
Iteration 2/25 | Loss: 0.00191990
Iteration 3/25 | Loss: 0.00164591
Iteration 4/25 | Loss: 0.00146933
Iteration 5/25 | Loss: 0.00137986
Iteration 6/25 | Loss: 0.00134343
Iteration 7/25 | Loss: 0.00134752
Iteration 8/25 | Loss: 0.00129658
Iteration 9/25 | Loss: 0.00127048
Iteration 10/25 | Loss: 0.00127032
Iteration 11/25 | Loss: 0.00127412
Iteration 12/25 | Loss: 0.00127148
Iteration 13/25 | Loss: 0.00127128
Iteration 14/25 | Loss: 0.00126462
Iteration 15/25 | Loss: 0.00126360
Iteration 16/25 | Loss: 0.00126074
Iteration 17/25 | Loss: 0.00126061
Iteration 18/25 | Loss: 0.00126022
Iteration 19/25 | Loss: 0.00126060
Iteration 20/25 | Loss: 0.00127181
Iteration 21/25 | Loss: 0.00126635
Iteration 22/25 | Loss: 0.00126082
Iteration 23/25 | Loss: 0.00126905
Iteration 24/25 | Loss: 0.00126086
Iteration 25/25 | Loss: 0.00126073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26852703
Iteration 2/25 | Loss: 0.00252773
Iteration 3/25 | Loss: 0.00151205
Iteration 4/25 | Loss: 0.00151205
Iteration 5/25 | Loss: 0.00151205
Iteration 6/25 | Loss: 0.00151205
Iteration 7/25 | Loss: 0.00151205
Iteration 8/25 | Loss: 0.00151205
Iteration 9/25 | Loss: 0.00151205
Iteration 10/25 | Loss: 0.00151205
Iteration 11/25 | Loss: 0.00151205
Iteration 12/25 | Loss: 0.00151205
Iteration 13/25 | Loss: 0.00151205
Iteration 14/25 | Loss: 0.00151205
Iteration 15/25 | Loss: 0.00151205
Iteration 16/25 | Loss: 0.00151205
Iteration 17/25 | Loss: 0.00151205
Iteration 18/25 | Loss: 0.00151205
Iteration 19/25 | Loss: 0.00151205
Iteration 20/25 | Loss: 0.00151205
Iteration 21/25 | Loss: 0.00151205
Iteration 22/25 | Loss: 0.00151205
Iteration 23/25 | Loss: 0.00151205
Iteration 24/25 | Loss: 0.00151205
Iteration 25/25 | Loss: 0.00151205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151205
Iteration 2/1000 | Loss: 0.00003452
Iteration 3/1000 | Loss: 0.00002773
Iteration 4/1000 | Loss: 0.00002550
Iteration 5/1000 | Loss: 0.00002139
Iteration 6/1000 | Loss: 0.00002019
Iteration 7/1000 | Loss: 0.00001972
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00001879
Iteration 10/1000 | Loss: 0.00001843
Iteration 11/1000 | Loss: 0.00001820
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001785
Iteration 14/1000 | Loss: 0.00001780
Iteration 15/1000 | Loss: 0.00001763
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001756
Iteration 24/1000 | Loss: 0.00001756
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001755
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001754
Iteration 29/1000 | Loss: 0.00001754
Iteration 30/1000 | Loss: 0.00001754
Iteration 31/1000 | Loss: 0.00001753
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001748
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001747
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001743
Iteration 44/1000 | Loss: 0.00001743
Iteration 45/1000 | Loss: 0.00001743
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001741
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001740
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001739
Iteration 60/1000 | Loss: 0.00001739
Iteration 61/1000 | Loss: 0.00001739
Iteration 62/1000 | Loss: 0.00001739
Iteration 63/1000 | Loss: 0.00001739
Iteration 64/1000 | Loss: 0.00001739
Iteration 65/1000 | Loss: 0.00001739
Iteration 66/1000 | Loss: 0.00001739
Iteration 67/1000 | Loss: 0.00001739
Iteration 68/1000 | Loss: 0.00001738
Iteration 69/1000 | Loss: 0.00001737
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001736
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001736
Iteration 74/1000 | Loss: 0.00001736
Iteration 75/1000 | Loss: 0.00001736
Iteration 76/1000 | Loss: 0.00001736
Iteration 77/1000 | Loss: 0.00001736
Iteration 78/1000 | Loss: 0.00001736
Iteration 79/1000 | Loss: 0.00001736
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Iteration 82/1000 | Loss: 0.00001734
Iteration 83/1000 | Loss: 0.00001734
Iteration 84/1000 | Loss: 0.00001733
Iteration 85/1000 | Loss: 0.00001733
Iteration 86/1000 | Loss: 0.00001733
Iteration 87/1000 | Loss: 0.00001733
Iteration 88/1000 | Loss: 0.00001733
Iteration 89/1000 | Loss: 0.00001733
Iteration 90/1000 | Loss: 0.00001733
Iteration 91/1000 | Loss: 0.00001733
Iteration 92/1000 | Loss: 0.00001733
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001732
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001732
Iteration 97/1000 | Loss: 0.00001732
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001732
Iteration 101/1000 | Loss: 0.00001732
Iteration 102/1000 | Loss: 0.00001732
Iteration 103/1000 | Loss: 0.00001732
Iteration 104/1000 | Loss: 0.00001732
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.731857810227666e-05, 1.731857810227666e-05, 1.731857810227666e-05, 1.731857810227666e-05, 1.731857810227666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.731857810227666e-05

Optimization complete. Final v2v error: 3.5701141357421875 mm

Highest mean error: 3.966965675354004 mm for frame 55

Lowest mean error: 3.4631187915802 mm for frame 171

Saving results

Total time: 84.32520747184753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018270
Iteration 2/25 | Loss: 0.00181261
Iteration 3/25 | Loss: 0.00196621
Iteration 4/25 | Loss: 0.00202194
Iteration 5/25 | Loss: 0.00198758
Iteration 6/25 | Loss: 0.00150202
Iteration 7/25 | Loss: 0.00139103
Iteration 8/25 | Loss: 0.00127463
Iteration 9/25 | Loss: 0.00124608
Iteration 10/25 | Loss: 0.00124012
Iteration 11/25 | Loss: 0.00123600
Iteration 12/25 | Loss: 0.00122991
Iteration 13/25 | Loss: 0.00122586
Iteration 14/25 | Loss: 0.00122576
Iteration 15/25 | Loss: 0.00122491
Iteration 16/25 | Loss: 0.00122481
Iteration 17/25 | Loss: 0.00122548
Iteration 18/25 | Loss: 0.00122463
Iteration 19/25 | Loss: 0.00122461
Iteration 20/25 | Loss: 0.00122460
Iteration 21/25 | Loss: 0.00122460
Iteration 22/25 | Loss: 0.00122460
Iteration 23/25 | Loss: 0.00122459
Iteration 24/25 | Loss: 0.00122459
Iteration 25/25 | Loss: 0.00122459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23057294
Iteration 2/25 | Loss: 0.00190206
Iteration 3/25 | Loss: 0.00187144
Iteration 4/25 | Loss: 0.00187144
Iteration 5/25 | Loss: 0.00187144
Iteration 6/25 | Loss: 0.00187144
Iteration 7/25 | Loss: 0.00187144
Iteration 8/25 | Loss: 0.00187144
Iteration 9/25 | Loss: 0.00187144
Iteration 10/25 | Loss: 0.00187144
Iteration 11/25 | Loss: 0.00187144
Iteration 12/25 | Loss: 0.00187144
Iteration 13/25 | Loss: 0.00187144
Iteration 14/25 | Loss: 0.00187144
Iteration 15/25 | Loss: 0.00187144
Iteration 16/25 | Loss: 0.00187144
Iteration 17/25 | Loss: 0.00187144
Iteration 18/25 | Loss: 0.00187144
Iteration 19/25 | Loss: 0.00187144
Iteration 20/25 | Loss: 0.00187144
Iteration 21/25 | Loss: 0.00187144
Iteration 22/25 | Loss: 0.00187144
Iteration 23/25 | Loss: 0.00187144
Iteration 24/25 | Loss: 0.00187144
Iteration 25/25 | Loss: 0.00187144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187144
Iteration 2/1000 | Loss: 0.00006192
Iteration 3/1000 | Loss: 0.00020269
Iteration 4/1000 | Loss: 0.00004405
Iteration 5/1000 | Loss: 0.00005220
Iteration 6/1000 | Loss: 0.00007466
Iteration 7/1000 | Loss: 0.00011542
Iteration 8/1000 | Loss: 0.00042654
Iteration 9/1000 | Loss: 0.00003674
Iteration 10/1000 | Loss: 0.00016534
Iteration 11/1000 | Loss: 0.00002974
Iteration 12/1000 | Loss: 0.00008550
Iteration 13/1000 | Loss: 0.00001826
Iteration 14/1000 | Loss: 0.00012141
Iteration 15/1000 | Loss: 0.00006280
Iteration 16/1000 | Loss: 0.00029135
Iteration 17/1000 | Loss: 0.00005865
Iteration 18/1000 | Loss: 0.00003115
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00007896
Iteration 21/1000 | Loss: 0.00001615
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00005368
Iteration 24/1000 | Loss: 0.00008446
Iteration 25/1000 | Loss: 0.00003245
Iteration 26/1000 | Loss: 0.00001557
Iteration 27/1000 | Loss: 0.00001807
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001533
Iteration 32/1000 | Loss: 0.00001533
Iteration 33/1000 | Loss: 0.00001533
Iteration 34/1000 | Loss: 0.00001533
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001533
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001532
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001519
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001519
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001518
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001517
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001515
Iteration 67/1000 | Loss: 0.00001515
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00002115
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001513
Iteration 83/1000 | Loss: 0.00001513
Iteration 84/1000 | Loss: 0.00001513
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001512
Iteration 90/1000 | Loss: 0.00001512
Iteration 91/1000 | Loss: 0.00001512
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001510
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001510
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001510
Iteration 110/1000 | Loss: 0.00001510
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001507
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001744
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00002214
Iteration 127/1000 | Loss: 0.00002214
Iteration 128/1000 | Loss: 0.00002719
Iteration 129/1000 | Loss: 0.00001502
Iteration 130/1000 | Loss: 0.00001502
Iteration 131/1000 | Loss: 0.00001502
Iteration 132/1000 | Loss: 0.00001502
Iteration 133/1000 | Loss: 0.00001502
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001502
Iteration 136/1000 | Loss: 0.00001502
Iteration 137/1000 | Loss: 0.00001502
Iteration 138/1000 | Loss: 0.00001502
Iteration 139/1000 | Loss: 0.00001502
Iteration 140/1000 | Loss: 0.00001502
Iteration 141/1000 | Loss: 0.00001502
Iteration 142/1000 | Loss: 0.00001502
Iteration 143/1000 | Loss: 0.00001502
Iteration 144/1000 | Loss: 0.00001502
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.5017711120890453e-05, 1.5017711120890453e-05, 1.5017711120890453e-05, 1.5017711120890453e-05, 1.5017711120890453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5017711120890453e-05

Optimization complete. Final v2v error: 3.317209482192993 mm

Highest mean error: 3.716641902923584 mm for frame 58

Lowest mean error: 3.009209156036377 mm for frame 175

Saving results

Total time: 90.22622036933899
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455661
Iteration 2/25 | Loss: 0.00144549
Iteration 3/25 | Loss: 0.00126783
Iteration 4/25 | Loss: 0.00125158
Iteration 5/25 | Loss: 0.00124846
Iteration 6/25 | Loss: 0.00124770
Iteration 7/25 | Loss: 0.00124770
Iteration 8/25 | Loss: 0.00124758
Iteration 9/25 | Loss: 0.00124758
Iteration 10/25 | Loss: 0.00124758
Iteration 11/25 | Loss: 0.00124758
Iteration 12/25 | Loss: 0.00124758
Iteration 13/25 | Loss: 0.00124758
Iteration 14/25 | Loss: 0.00124758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012475810945034027, 0.0012475810945034027, 0.0012475810945034027, 0.0012475810945034027, 0.0012475810945034027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012475810945034027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42243671
Iteration 2/25 | Loss: 0.00118951
Iteration 3/25 | Loss: 0.00118951
Iteration 4/25 | Loss: 0.00118951
Iteration 5/25 | Loss: 0.00118951
Iteration 6/25 | Loss: 0.00118951
Iteration 7/25 | Loss: 0.00118951
Iteration 8/25 | Loss: 0.00118951
Iteration 9/25 | Loss: 0.00118950
Iteration 10/25 | Loss: 0.00118950
Iteration 11/25 | Loss: 0.00118950
Iteration 12/25 | Loss: 0.00118950
Iteration 13/25 | Loss: 0.00118950
Iteration 14/25 | Loss: 0.00118950
Iteration 15/25 | Loss: 0.00118950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001189504866488278, 0.001189504866488278, 0.001189504866488278, 0.001189504866488278, 0.001189504866488278]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001189504866488278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118950
Iteration 2/1000 | Loss: 0.00003428
Iteration 3/1000 | Loss: 0.00002175
Iteration 4/1000 | Loss: 0.00001847
Iteration 5/1000 | Loss: 0.00001725
Iteration 6/1000 | Loss: 0.00001646
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001512
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001463
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001448
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001415
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001403
Iteration 23/1000 | Loss: 0.00001403
Iteration 24/1000 | Loss: 0.00001402
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001397
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001396
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001392
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001383
Iteration 37/1000 | Loss: 0.00001382
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001379
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001379
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001378
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001374
Iteration 54/1000 | Loss: 0.00001374
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001372
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001369
Iteration 61/1000 | Loss: 0.00001368
Iteration 62/1000 | Loss: 0.00001368
Iteration 63/1000 | Loss: 0.00001368
Iteration 64/1000 | Loss: 0.00001367
Iteration 65/1000 | Loss: 0.00001367
Iteration 66/1000 | Loss: 0.00001367
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001365
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001364
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001361
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001360
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001359
Iteration 85/1000 | Loss: 0.00001359
Iteration 86/1000 | Loss: 0.00001359
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001356
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Iteration 104/1000 | Loss: 0.00001356
Iteration 105/1000 | Loss: 0.00001355
Iteration 106/1000 | Loss: 0.00001355
Iteration 107/1000 | Loss: 0.00001355
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001354
Iteration 112/1000 | Loss: 0.00001354
Iteration 113/1000 | Loss: 0.00001354
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001353
Iteration 117/1000 | Loss: 0.00001353
Iteration 118/1000 | Loss: 0.00001353
Iteration 119/1000 | Loss: 0.00001353
Iteration 120/1000 | Loss: 0.00001353
Iteration 121/1000 | Loss: 0.00001353
Iteration 122/1000 | Loss: 0.00001353
Iteration 123/1000 | Loss: 0.00001353
Iteration 124/1000 | Loss: 0.00001353
Iteration 125/1000 | Loss: 0.00001353
Iteration 126/1000 | Loss: 0.00001353
Iteration 127/1000 | Loss: 0.00001353
Iteration 128/1000 | Loss: 0.00001353
Iteration 129/1000 | Loss: 0.00001353
Iteration 130/1000 | Loss: 0.00001353
Iteration 131/1000 | Loss: 0.00001353
Iteration 132/1000 | Loss: 0.00001353
Iteration 133/1000 | Loss: 0.00001353
Iteration 134/1000 | Loss: 0.00001353
Iteration 135/1000 | Loss: 0.00001353
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001353
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001353
Iteration 146/1000 | Loss: 0.00001353
Iteration 147/1000 | Loss: 0.00001353
Iteration 148/1000 | Loss: 0.00001353
Iteration 149/1000 | Loss: 0.00001353
Iteration 150/1000 | Loss: 0.00001353
Iteration 151/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3527830560633447e-05, 1.3527830560633447e-05, 1.3527830560633447e-05, 1.3527830560633447e-05, 1.3527830560633447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3527830560633447e-05

Optimization complete. Final v2v error: 3.087995767593384 mm

Highest mean error: 4.428420066833496 mm for frame 67

Lowest mean error: 2.579988718032837 mm for frame 27

Saving results

Total time: 39.94507932662964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414952
Iteration 2/25 | Loss: 0.00128718
Iteration 3/25 | Loss: 0.00122068
Iteration 4/25 | Loss: 0.00121254
Iteration 5/25 | Loss: 0.00121038
Iteration 6/25 | Loss: 0.00121032
Iteration 7/25 | Loss: 0.00121032
Iteration 8/25 | Loss: 0.00121032
Iteration 9/25 | Loss: 0.00121032
Iteration 10/25 | Loss: 0.00121032
Iteration 11/25 | Loss: 0.00121032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001210320508107543, 0.001210320508107543, 0.001210320508107543, 0.001210320508107543, 0.001210320508107543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210320508107543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30249476
Iteration 2/25 | Loss: 0.00133049
Iteration 3/25 | Loss: 0.00133049
Iteration 4/25 | Loss: 0.00133049
Iteration 5/25 | Loss: 0.00133049
Iteration 6/25 | Loss: 0.00133049
Iteration 7/25 | Loss: 0.00133048
Iteration 8/25 | Loss: 0.00133048
Iteration 9/25 | Loss: 0.00133048
Iteration 10/25 | Loss: 0.00133048
Iteration 11/25 | Loss: 0.00133048
Iteration 12/25 | Loss: 0.00133048
Iteration 13/25 | Loss: 0.00133048
Iteration 14/25 | Loss: 0.00133048
Iteration 15/25 | Loss: 0.00133048
Iteration 16/25 | Loss: 0.00133048
Iteration 17/25 | Loss: 0.00133048
Iteration 18/25 | Loss: 0.00133048
Iteration 19/25 | Loss: 0.00133048
Iteration 20/25 | Loss: 0.00133048
Iteration 21/25 | Loss: 0.00133048
Iteration 22/25 | Loss: 0.00133048
Iteration 23/25 | Loss: 0.00133048
Iteration 24/25 | Loss: 0.00133048
Iteration 25/25 | Loss: 0.00133048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133048
Iteration 2/1000 | Loss: 0.00002152
Iteration 3/1000 | Loss: 0.00001441
Iteration 4/1000 | Loss: 0.00001317
Iteration 5/1000 | Loss: 0.00001266
Iteration 6/1000 | Loss: 0.00001216
Iteration 7/1000 | Loss: 0.00001173
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001102
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001055
Iteration 13/1000 | Loss: 0.00001045
Iteration 14/1000 | Loss: 0.00001044
Iteration 15/1000 | Loss: 0.00001042
Iteration 16/1000 | Loss: 0.00001042
Iteration 17/1000 | Loss: 0.00001042
Iteration 18/1000 | Loss: 0.00001042
Iteration 19/1000 | Loss: 0.00001040
Iteration 20/1000 | Loss: 0.00001040
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001037
Iteration 24/1000 | Loss: 0.00001037
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001025
Iteration 27/1000 | Loss: 0.00001024
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001023
Iteration 31/1000 | Loss: 0.00001023
Iteration 32/1000 | Loss: 0.00001023
Iteration 33/1000 | Loss: 0.00001023
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001022
Iteration 37/1000 | Loss: 0.00001021
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001020
Iteration 40/1000 | Loss: 0.00001020
Iteration 41/1000 | Loss: 0.00001020
Iteration 42/1000 | Loss: 0.00001020
Iteration 43/1000 | Loss: 0.00001020
Iteration 44/1000 | Loss: 0.00001020
Iteration 45/1000 | Loss: 0.00001019
Iteration 46/1000 | Loss: 0.00001019
Iteration 47/1000 | Loss: 0.00001019
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001019
Iteration 50/1000 | Loss: 0.00001019
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001018
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001014
Iteration 63/1000 | Loss: 0.00001014
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001013
Iteration 66/1000 | Loss: 0.00001013
Iteration 67/1000 | Loss: 0.00001012
Iteration 68/1000 | Loss: 0.00001012
Iteration 69/1000 | Loss: 0.00001012
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001008
Iteration 75/1000 | Loss: 0.00001008
Iteration 76/1000 | Loss: 0.00001008
Iteration 77/1000 | Loss: 0.00001008
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001006
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001005
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001005
Iteration 87/1000 | Loss: 0.00001005
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001004
Iteration 90/1000 | Loss: 0.00001004
Iteration 91/1000 | Loss: 0.00001004
Iteration 92/1000 | Loss: 0.00001004
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001003
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001002
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00000999
Iteration 108/1000 | Loss: 0.00000999
Iteration 109/1000 | Loss: 0.00000999
Iteration 110/1000 | Loss: 0.00000999
Iteration 111/1000 | Loss: 0.00000999
Iteration 112/1000 | Loss: 0.00000998
Iteration 113/1000 | Loss: 0.00000998
Iteration 114/1000 | Loss: 0.00000998
Iteration 115/1000 | Loss: 0.00000998
Iteration 116/1000 | Loss: 0.00000998
Iteration 117/1000 | Loss: 0.00000997
Iteration 118/1000 | Loss: 0.00000997
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000997
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.964091077563353e-06, 9.964091077563353e-06, 9.964091077563353e-06, 9.964091077563353e-06, 9.964091077563353e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.964091077563353e-06

Optimization complete. Final v2v error: 2.737381935119629 mm

Highest mean error: 2.8237366676330566 mm for frame 35

Lowest mean error: 2.632537841796875 mm for frame 125

Saving results

Total time: 35.541882038116455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434079
Iteration 2/25 | Loss: 0.00135662
Iteration 3/25 | Loss: 0.00126296
Iteration 4/25 | Loss: 0.00125152
Iteration 5/25 | Loss: 0.00125035
Iteration 6/25 | Loss: 0.00125035
Iteration 7/25 | Loss: 0.00125035
Iteration 8/25 | Loss: 0.00125035
Iteration 9/25 | Loss: 0.00125035
Iteration 10/25 | Loss: 0.00125035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012503535253927112, 0.0012503535253927112, 0.0012503535253927112, 0.0012503535253927112, 0.0012503535253927112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012503535253927112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29899609
Iteration 2/25 | Loss: 0.00128800
Iteration 3/25 | Loss: 0.00128800
Iteration 4/25 | Loss: 0.00128799
Iteration 5/25 | Loss: 0.00128799
Iteration 6/25 | Loss: 0.00128799
Iteration 7/25 | Loss: 0.00128799
Iteration 8/25 | Loss: 0.00128799
Iteration 9/25 | Loss: 0.00128799
Iteration 10/25 | Loss: 0.00128799
Iteration 11/25 | Loss: 0.00128799
Iteration 12/25 | Loss: 0.00128799
Iteration 13/25 | Loss: 0.00128799
Iteration 14/25 | Loss: 0.00128799
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012879931600764394, 0.0012879931600764394, 0.0012879931600764394, 0.0012879931600764394, 0.0012879931600764394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012879931600764394

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128799
Iteration 2/1000 | Loss: 0.00002202
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001420
Iteration 7/1000 | Loss: 0.00001388
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001255
Iteration 18/1000 | Loss: 0.00001252
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001234
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001224
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001219
Iteration 53/1000 | Loss: 0.00001219
Iteration 54/1000 | Loss: 0.00001219
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001218
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001208
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001207
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001205
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001204
Iteration 78/1000 | Loss: 0.00001204
Iteration 79/1000 | Loss: 0.00001203
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001199
Iteration 89/1000 | Loss: 0.00001199
Iteration 90/1000 | Loss: 0.00001199
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001197
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001190
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001189
Iteration 129/1000 | Loss: 0.00001189
Iteration 130/1000 | Loss: 0.00001189
Iteration 131/1000 | Loss: 0.00001189
Iteration 132/1000 | Loss: 0.00001189
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001185
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001185
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001184
Iteration 161/1000 | Loss: 0.00001184
Iteration 162/1000 | Loss: 0.00001184
Iteration 163/1000 | Loss: 0.00001184
Iteration 164/1000 | Loss: 0.00001184
Iteration 165/1000 | Loss: 0.00001184
Iteration 166/1000 | Loss: 0.00001184
Iteration 167/1000 | Loss: 0.00001183
Iteration 168/1000 | Loss: 0.00001183
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001182
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001182
Iteration 174/1000 | Loss: 0.00001182
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Iteration 177/1000 | Loss: 0.00001181
Iteration 178/1000 | Loss: 0.00001181
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Iteration 183/1000 | Loss: 0.00001180
Iteration 184/1000 | Loss: 0.00001180
Iteration 185/1000 | Loss: 0.00001180
Iteration 186/1000 | Loss: 0.00001180
Iteration 187/1000 | Loss: 0.00001180
Iteration 188/1000 | Loss: 0.00001180
Iteration 189/1000 | Loss: 0.00001180
Iteration 190/1000 | Loss: 0.00001180
Iteration 191/1000 | Loss: 0.00001180
Iteration 192/1000 | Loss: 0.00001180
Iteration 193/1000 | Loss: 0.00001180
Iteration 194/1000 | Loss: 0.00001180
Iteration 195/1000 | Loss: 0.00001180
Iteration 196/1000 | Loss: 0.00001180
Iteration 197/1000 | Loss: 0.00001180
Iteration 198/1000 | Loss: 0.00001180
Iteration 199/1000 | Loss: 0.00001180
Iteration 200/1000 | Loss: 0.00001180
Iteration 201/1000 | Loss: 0.00001180
Iteration 202/1000 | Loss: 0.00001180
Iteration 203/1000 | Loss: 0.00001180
Iteration 204/1000 | Loss: 0.00001180
Iteration 205/1000 | Loss: 0.00001180
Iteration 206/1000 | Loss: 0.00001180
Iteration 207/1000 | Loss: 0.00001180
Iteration 208/1000 | Loss: 0.00001180
Iteration 209/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.1803525922005065e-05, 1.1803525922005065e-05, 1.1803525922005065e-05, 1.1803525922005065e-05, 1.1803525922005065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1803525922005065e-05

Optimization complete. Final v2v error: 2.921365261077881 mm

Highest mean error: 3.0897867679595947 mm for frame 178

Lowest mean error: 2.7418882846832275 mm for frame 239

Saving results

Total time: 47.7340943813324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824272
Iteration 2/25 | Loss: 0.00146505
Iteration 3/25 | Loss: 0.00124792
Iteration 4/25 | Loss: 0.00121687
Iteration 5/25 | Loss: 0.00121031
Iteration 6/25 | Loss: 0.00120921
Iteration 7/25 | Loss: 0.00120921
Iteration 8/25 | Loss: 0.00120921
Iteration 9/25 | Loss: 0.00120912
Iteration 10/25 | Loss: 0.00120912
Iteration 11/25 | Loss: 0.00120912
Iteration 12/25 | Loss: 0.00120912
Iteration 13/25 | Loss: 0.00120912
Iteration 14/25 | Loss: 0.00120912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012091243406757712, 0.0012091243406757712, 0.0012091243406757712, 0.0012091243406757712, 0.0012091243406757712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012091243406757712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91281068
Iteration 2/25 | Loss: 0.00094088
Iteration 3/25 | Loss: 0.00094088
Iteration 4/25 | Loss: 0.00094087
Iteration 5/25 | Loss: 0.00094087
Iteration 6/25 | Loss: 0.00094087
Iteration 7/25 | Loss: 0.00094087
Iteration 8/25 | Loss: 0.00094087
Iteration 9/25 | Loss: 0.00094087
Iteration 10/25 | Loss: 0.00094087
Iteration 11/25 | Loss: 0.00094087
Iteration 12/25 | Loss: 0.00094087
Iteration 13/25 | Loss: 0.00094087
Iteration 14/25 | Loss: 0.00094087
Iteration 15/25 | Loss: 0.00094087
Iteration 16/25 | Loss: 0.00094087
Iteration 17/25 | Loss: 0.00094087
Iteration 18/25 | Loss: 0.00094087
Iteration 19/25 | Loss: 0.00094087
Iteration 20/25 | Loss: 0.00094087
Iteration 21/25 | Loss: 0.00094087
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009408725309185684, 0.0009408725309185684, 0.0009408725309185684, 0.0009408725309185684, 0.0009408725309185684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009408725309185684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094087
Iteration 2/1000 | Loss: 0.00003993
Iteration 3/1000 | Loss: 0.00003006
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002560
Iteration 6/1000 | Loss: 0.00002422
Iteration 7/1000 | Loss: 0.00002334
Iteration 8/1000 | Loss: 0.00002271
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002201
Iteration 11/1000 | Loss: 0.00002172
Iteration 12/1000 | Loss: 0.00002147
Iteration 13/1000 | Loss: 0.00002126
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002103
Iteration 16/1000 | Loss: 0.00002093
Iteration 17/1000 | Loss: 0.00002093
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002091
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002089
Iteration 22/1000 | Loss: 0.00002082
Iteration 23/1000 | Loss: 0.00002081
Iteration 24/1000 | Loss: 0.00002079
Iteration 25/1000 | Loss: 0.00002076
Iteration 26/1000 | Loss: 0.00002076
Iteration 27/1000 | Loss: 0.00002075
Iteration 28/1000 | Loss: 0.00002075
Iteration 29/1000 | Loss: 0.00002073
Iteration 30/1000 | Loss: 0.00002072
Iteration 31/1000 | Loss: 0.00002072
Iteration 32/1000 | Loss: 0.00002072
Iteration 33/1000 | Loss: 0.00002072
Iteration 34/1000 | Loss: 0.00002071
Iteration 35/1000 | Loss: 0.00002071
Iteration 36/1000 | Loss: 0.00002071
Iteration 37/1000 | Loss: 0.00002071
Iteration 38/1000 | Loss: 0.00002070
Iteration 39/1000 | Loss: 0.00002069
Iteration 40/1000 | Loss: 0.00002069
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002069
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002068
Iteration 48/1000 | Loss: 0.00002068
Iteration 49/1000 | Loss: 0.00002068
Iteration 50/1000 | Loss: 0.00002068
Iteration 51/1000 | Loss: 0.00002067
Iteration 52/1000 | Loss: 0.00002067
Iteration 53/1000 | Loss: 0.00002066
Iteration 54/1000 | Loss: 0.00002066
Iteration 55/1000 | Loss: 0.00002065
Iteration 56/1000 | Loss: 0.00002065
Iteration 57/1000 | Loss: 0.00002065
Iteration 58/1000 | Loss: 0.00002064
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002049
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002048
Iteration 68/1000 | Loss: 0.00002048
Iteration 69/1000 | Loss: 0.00002048
Iteration 70/1000 | Loss: 0.00002048
Iteration 71/1000 | Loss: 0.00002048
Iteration 72/1000 | Loss: 0.00002048
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002046
Iteration 78/1000 | Loss: 0.00002046
Iteration 79/1000 | Loss: 0.00002046
Iteration 80/1000 | Loss: 0.00002045
Iteration 81/1000 | Loss: 0.00002045
Iteration 82/1000 | Loss: 0.00002044
Iteration 83/1000 | Loss: 0.00002044
Iteration 84/1000 | Loss: 0.00002043
Iteration 85/1000 | Loss: 0.00002043
Iteration 86/1000 | Loss: 0.00002043
Iteration 87/1000 | Loss: 0.00002043
Iteration 88/1000 | Loss: 0.00002043
Iteration 89/1000 | Loss: 0.00002042
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002041
Iteration 96/1000 | Loss: 0.00002041
Iteration 97/1000 | Loss: 0.00002041
Iteration 98/1000 | Loss: 0.00002041
Iteration 99/1000 | Loss: 0.00002041
Iteration 100/1000 | Loss: 0.00002041
Iteration 101/1000 | Loss: 0.00002041
Iteration 102/1000 | Loss: 0.00002040
Iteration 103/1000 | Loss: 0.00002040
Iteration 104/1000 | Loss: 0.00002040
Iteration 105/1000 | Loss: 0.00002039
Iteration 106/1000 | Loss: 0.00002039
Iteration 107/1000 | Loss: 0.00002039
Iteration 108/1000 | Loss: 0.00002039
Iteration 109/1000 | Loss: 0.00002039
Iteration 110/1000 | Loss: 0.00002039
Iteration 111/1000 | Loss: 0.00002039
Iteration 112/1000 | Loss: 0.00002039
Iteration 113/1000 | Loss: 0.00002039
Iteration 114/1000 | Loss: 0.00002039
Iteration 115/1000 | Loss: 0.00002038
Iteration 116/1000 | Loss: 0.00002038
Iteration 117/1000 | Loss: 0.00002038
Iteration 118/1000 | Loss: 0.00002038
Iteration 119/1000 | Loss: 0.00002038
Iteration 120/1000 | Loss: 0.00002037
Iteration 121/1000 | Loss: 0.00002037
Iteration 122/1000 | Loss: 0.00002037
Iteration 123/1000 | Loss: 0.00002037
Iteration 124/1000 | Loss: 0.00002037
Iteration 125/1000 | Loss: 0.00002037
Iteration 126/1000 | Loss: 0.00002036
Iteration 127/1000 | Loss: 0.00002036
Iteration 128/1000 | Loss: 0.00002036
Iteration 129/1000 | Loss: 0.00002036
Iteration 130/1000 | Loss: 0.00002036
Iteration 131/1000 | Loss: 0.00002036
Iteration 132/1000 | Loss: 0.00002036
Iteration 133/1000 | Loss: 0.00002036
Iteration 134/1000 | Loss: 0.00002036
Iteration 135/1000 | Loss: 0.00002036
Iteration 136/1000 | Loss: 0.00002035
Iteration 137/1000 | Loss: 0.00002035
Iteration 138/1000 | Loss: 0.00002035
Iteration 139/1000 | Loss: 0.00002035
Iteration 140/1000 | Loss: 0.00002035
Iteration 141/1000 | Loss: 0.00002035
Iteration 142/1000 | Loss: 0.00002035
Iteration 143/1000 | Loss: 0.00002035
Iteration 144/1000 | Loss: 0.00002035
Iteration 145/1000 | Loss: 0.00002035
Iteration 146/1000 | Loss: 0.00002035
Iteration 147/1000 | Loss: 0.00002035
Iteration 148/1000 | Loss: 0.00002035
Iteration 149/1000 | Loss: 0.00002035
Iteration 150/1000 | Loss: 0.00002035
Iteration 151/1000 | Loss: 0.00002035
Iteration 152/1000 | Loss: 0.00002035
Iteration 153/1000 | Loss: 0.00002035
Iteration 154/1000 | Loss: 0.00002035
Iteration 155/1000 | Loss: 0.00002035
Iteration 156/1000 | Loss: 0.00002035
Iteration 157/1000 | Loss: 0.00002035
Iteration 158/1000 | Loss: 0.00002035
Iteration 159/1000 | Loss: 0.00002035
Iteration 160/1000 | Loss: 0.00002035
Iteration 161/1000 | Loss: 0.00002035
Iteration 162/1000 | Loss: 0.00002035
Iteration 163/1000 | Loss: 0.00002035
Iteration 164/1000 | Loss: 0.00002035
Iteration 165/1000 | Loss: 0.00002035
Iteration 166/1000 | Loss: 0.00002035
Iteration 167/1000 | Loss: 0.00002035
Iteration 168/1000 | Loss: 0.00002035
Iteration 169/1000 | Loss: 0.00002035
Iteration 170/1000 | Loss: 0.00002035
Iteration 171/1000 | Loss: 0.00002035
Iteration 172/1000 | Loss: 0.00002035
Iteration 173/1000 | Loss: 0.00002035
Iteration 174/1000 | Loss: 0.00002035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.0346878955024295e-05, 2.0346878955024295e-05, 2.0346878955024295e-05, 2.0346878955024295e-05, 2.0346878955024295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0346878955024295e-05

Optimization complete. Final v2v error: 3.937924861907959 mm

Highest mean error: 4.147849082946777 mm for frame 42

Lowest mean error: 3.8320040702819824 mm for frame 119

Saving results

Total time: 43.91205811500549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045976
Iteration 2/25 | Loss: 0.01045976
Iteration 3/25 | Loss: 0.01045976
Iteration 4/25 | Loss: 0.01045976
Iteration 5/25 | Loss: 0.01045976
Iteration 6/25 | Loss: 0.01045976
Iteration 7/25 | Loss: 0.01045976
Iteration 8/25 | Loss: 0.01045976
Iteration 9/25 | Loss: 0.01045976
Iteration 10/25 | Loss: 0.01045976
Iteration 11/25 | Loss: 0.01045976
Iteration 12/25 | Loss: 0.01045976
Iteration 13/25 | Loss: 0.01045976
Iteration 14/25 | Loss: 0.01045976
Iteration 15/25 | Loss: 0.01045976
Iteration 16/25 | Loss: 0.01045975
Iteration 17/25 | Loss: 0.01045975
Iteration 18/25 | Loss: 0.01045975
Iteration 19/25 | Loss: 0.01045975
Iteration 20/25 | Loss: 0.01045975
Iteration 21/25 | Loss: 0.01045975
Iteration 22/25 | Loss: 0.01045975
Iteration 23/25 | Loss: 0.01045975
Iteration 24/25 | Loss: 0.01045975
Iteration 25/25 | Loss: 0.01045975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44450498
Iteration 2/25 | Loss: 0.11724149
Iteration 3/25 | Loss: 0.09327786
Iteration 4/25 | Loss: 0.09287145
Iteration 5/25 | Loss: 0.09287206
Iteration 6/25 | Loss: 0.09287140
Iteration 7/25 | Loss: 0.09287138
Iteration 8/25 | Loss: 0.09287138
Iteration 9/25 | Loss: 0.09287136
Iteration 10/25 | Loss: 0.09287131
Iteration 11/25 | Loss: 0.09286690
Iteration 12/25 | Loss: 0.09286688
Iteration 13/25 | Loss: 0.09286688
Iteration 14/25 | Loss: 0.09286687
Iteration 15/25 | Loss: 0.09286687
Iteration 16/25 | Loss: 0.09286687
Iteration 17/25 | Loss: 0.09286687
Iteration 18/25 | Loss: 0.09286687
Iteration 19/25 | Loss: 0.09286687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.09286686778068542, 0.09286686778068542, 0.09286686778068542, 0.09286686778068542, 0.09286686778068542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09286686778068542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09286687
Iteration 2/1000 | Loss: 0.00173399
Iteration 3/1000 | Loss: 0.00302954
Iteration 4/1000 | Loss: 0.00028876
Iteration 5/1000 | Loss: 0.00048038
Iteration 6/1000 | Loss: 0.00157479
Iteration 7/1000 | Loss: 0.00029138
Iteration 8/1000 | Loss: 0.00016287
Iteration 9/1000 | Loss: 0.00032213
Iteration 10/1000 | Loss: 0.00036778
Iteration 11/1000 | Loss: 0.00013068
Iteration 12/1000 | Loss: 0.00005822
Iteration 13/1000 | Loss: 0.00002950
Iteration 14/1000 | Loss: 0.00092528
Iteration 15/1000 | Loss: 0.00019627
Iteration 16/1000 | Loss: 0.00007410
Iteration 17/1000 | Loss: 0.00006254
Iteration 18/1000 | Loss: 0.00005706
Iteration 19/1000 | Loss: 0.00028875
Iteration 20/1000 | Loss: 0.00003600
Iteration 21/1000 | Loss: 0.00006324
Iteration 22/1000 | Loss: 0.00001864
Iteration 23/1000 | Loss: 0.00005482
Iteration 24/1000 | Loss: 0.00003397
Iteration 25/1000 | Loss: 0.00065924
Iteration 26/1000 | Loss: 0.00003312
Iteration 27/1000 | Loss: 0.00001568
Iteration 28/1000 | Loss: 0.00005731
Iteration 29/1000 | Loss: 0.00001502
Iteration 30/1000 | Loss: 0.00007336
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00003980
Iteration 35/1000 | Loss: 0.00036364
Iteration 36/1000 | Loss: 0.00141501
Iteration 37/1000 | Loss: 0.00013466
Iteration 38/1000 | Loss: 0.00003904
Iteration 39/1000 | Loss: 0.00007093
Iteration 40/1000 | Loss: 0.00042831
Iteration 41/1000 | Loss: 0.00012260
Iteration 42/1000 | Loss: 0.00003701
Iteration 43/1000 | Loss: 0.00003407
Iteration 44/1000 | Loss: 0.00001804
Iteration 45/1000 | Loss: 0.00013426
Iteration 46/1000 | Loss: 0.00003618
Iteration 47/1000 | Loss: 0.00001308
Iteration 48/1000 | Loss: 0.00001287
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001260
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001252
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00023641
Iteration 59/1000 | Loss: 0.00042232
Iteration 60/1000 | Loss: 0.00084406
Iteration 61/1000 | Loss: 0.00030393
Iteration 62/1000 | Loss: 0.00007354
Iteration 63/1000 | Loss: 0.00007247
Iteration 64/1000 | Loss: 0.00002942
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00007790
Iteration 69/1000 | Loss: 0.00002149
Iteration 70/1000 | Loss: 0.00003816
Iteration 71/1000 | Loss: 0.00006398
Iteration 72/1000 | Loss: 0.00008290
Iteration 73/1000 | Loss: 0.00035830
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001577
Iteration 76/1000 | Loss: 0.00002457
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00012182
Iteration 79/1000 | Loss: 0.00040077
Iteration 80/1000 | Loss: 0.00016389
Iteration 81/1000 | Loss: 0.00002258
Iteration 82/1000 | Loss: 0.00007556
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00003769
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001190
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001185
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001184
Iteration 91/1000 | Loss: 0.00001183
Iteration 92/1000 | Loss: 0.00001183
Iteration 93/1000 | Loss: 0.00001183
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001178
Iteration 100/1000 | Loss: 0.00001178
Iteration 101/1000 | Loss: 0.00001178
Iteration 102/1000 | Loss: 0.00001178
Iteration 103/1000 | Loss: 0.00001178
Iteration 104/1000 | Loss: 0.00001178
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001178
Iteration 107/1000 | Loss: 0.00001178
Iteration 108/1000 | Loss: 0.00001178
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.1781273315136787e-05, 1.1781273315136787e-05, 1.1781273315136787e-05, 1.1781273315136787e-05, 1.1781273315136787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1781273315136787e-05

Optimization complete. Final v2v error: 2.942523956298828 mm

Highest mean error: 3.9349021911621094 mm for frame 28

Lowest mean error: 2.638756513595581 mm for frame 234

Saving results

Total time: 140.8572907447815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035631
Iteration 2/25 | Loss: 0.01035631
Iteration 3/25 | Loss: 0.01035630
Iteration 4/25 | Loss: 0.01035630
Iteration 5/25 | Loss: 0.01035630
Iteration 6/25 | Loss: 0.01035630
Iteration 7/25 | Loss: 0.01035630
Iteration 8/25 | Loss: 0.01035630
Iteration 9/25 | Loss: 0.01035630
Iteration 10/25 | Loss: 0.01035630
Iteration 11/25 | Loss: 0.01035630
Iteration 12/25 | Loss: 0.01035630
Iteration 13/25 | Loss: 0.01035630
Iteration 14/25 | Loss: 0.01035629
Iteration 15/25 | Loss: 0.01035629
Iteration 16/25 | Loss: 0.01035629
Iteration 17/25 | Loss: 0.01035629
Iteration 18/25 | Loss: 0.01035629
Iteration 19/25 | Loss: 0.01035629
Iteration 20/25 | Loss: 0.01035629
Iteration 21/25 | Loss: 0.01035629
Iteration 22/25 | Loss: 0.01035629
Iteration 23/25 | Loss: 0.01035629
Iteration 24/25 | Loss: 0.01035629
Iteration 25/25 | Loss: 0.01035629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62581730
Iteration 2/25 | Loss: 0.09720994
Iteration 3/25 | Loss: 0.09608460
Iteration 4/25 | Loss: 0.09467214
Iteration 5/25 | Loss: 0.09232409
Iteration 6/25 | Loss: 0.09232409
Iteration 7/25 | Loss: 0.09232409
Iteration 8/25 | Loss: 0.09232408
Iteration 9/25 | Loss: 0.09232408
Iteration 10/25 | Loss: 0.09232408
Iteration 11/25 | Loss: 0.09232408
Iteration 12/25 | Loss: 0.09232408
Iteration 13/25 | Loss: 0.09232408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.09232407808303833, 0.09232407808303833, 0.09232407808303833, 0.09232407808303833, 0.09232407808303833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09232407808303833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09232408
Iteration 2/1000 | Loss: 0.00414352
Iteration 3/1000 | Loss: 0.00258046
Iteration 4/1000 | Loss: 0.00044201
Iteration 5/1000 | Loss: 0.00049049
Iteration 6/1000 | Loss: 0.00042067
Iteration 7/1000 | Loss: 0.00014023
Iteration 8/1000 | Loss: 0.00033734
Iteration 9/1000 | Loss: 0.00185068
Iteration 10/1000 | Loss: 0.00326177
Iteration 11/1000 | Loss: 0.00603083
Iteration 12/1000 | Loss: 0.00321178
Iteration 13/1000 | Loss: 0.00037933
Iteration 14/1000 | Loss: 0.00027635
Iteration 15/1000 | Loss: 0.00019886
Iteration 16/1000 | Loss: 0.00005617
Iteration 17/1000 | Loss: 0.00102453
Iteration 18/1000 | Loss: 0.00032565
Iteration 19/1000 | Loss: 0.00017363
Iteration 20/1000 | Loss: 0.00014383
Iteration 21/1000 | Loss: 0.00010008
Iteration 22/1000 | Loss: 0.00032637
Iteration 23/1000 | Loss: 0.00015362
Iteration 24/1000 | Loss: 0.00003329
Iteration 25/1000 | Loss: 0.00024910
Iteration 26/1000 | Loss: 0.00014286
Iteration 27/1000 | Loss: 0.00016838
Iteration 28/1000 | Loss: 0.00013491
Iteration 29/1000 | Loss: 0.00016610
Iteration 30/1000 | Loss: 0.00012574
Iteration 31/1000 | Loss: 0.00004010
Iteration 32/1000 | Loss: 0.00005238
Iteration 33/1000 | Loss: 0.00003570
Iteration 34/1000 | Loss: 0.00003065
Iteration 35/1000 | Loss: 0.00008155
Iteration 36/1000 | Loss: 0.00005763
Iteration 37/1000 | Loss: 0.00002170
Iteration 38/1000 | Loss: 0.00009804
Iteration 39/1000 | Loss: 0.00002064
Iteration 40/1000 | Loss: 0.00001906
Iteration 41/1000 | Loss: 0.00005248
Iteration 42/1000 | Loss: 0.00001799
Iteration 43/1000 | Loss: 0.00002294
Iteration 44/1000 | Loss: 0.00001715
Iteration 45/1000 | Loss: 0.00010711
Iteration 46/1000 | Loss: 0.00006145
Iteration 47/1000 | Loss: 0.00026164
Iteration 48/1000 | Loss: 0.00001680
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00005721
Iteration 53/1000 | Loss: 0.00001438
Iteration 54/1000 | Loss: 0.00010641
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00004542
Iteration 59/1000 | Loss: 0.00011436
Iteration 60/1000 | Loss: 0.00003612
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00007965
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001682
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00002190
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001241
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001237
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001273
Iteration 98/1000 | Loss: 0.00001463
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001948
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001211
Iteration 112/1000 | Loss: 0.00001211
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001210
Iteration 118/1000 | Loss: 0.00001209
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001199
Iteration 126/1000 | Loss: 0.00001199
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001195
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00002106
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001193
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001192
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001192
Iteration 165/1000 | Loss: 0.00001192
Iteration 166/1000 | Loss: 0.00001192
Iteration 167/1000 | Loss: 0.00001192
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.192303989228094e-05, 1.192303989228094e-05, 1.192303989228094e-05, 1.192303989228094e-05, 1.192303989228094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.192303989228094e-05

Optimization complete. Final v2v error: 2.9581055641174316 mm

Highest mean error: 3.504513740539551 mm for frame 103

Lowest mean error: 2.4173026084899902 mm for frame 150

Saving results

Total time: 134.26173615455627
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015811
Iteration 2/25 | Loss: 0.00257873
Iteration 3/25 | Loss: 0.00229440
Iteration 4/25 | Loss: 0.00203140
Iteration 5/25 | Loss: 0.00203137
Iteration 6/25 | Loss: 0.00182185
Iteration 7/25 | Loss: 0.00170510
Iteration 8/25 | Loss: 0.00177648
Iteration 9/25 | Loss: 0.00194669
Iteration 10/25 | Loss: 0.00177690
Iteration 11/25 | Loss: 0.00153392
Iteration 12/25 | Loss: 0.00141597
Iteration 13/25 | Loss: 0.00136330
Iteration 14/25 | Loss: 0.00135011
Iteration 15/25 | Loss: 0.00133455
Iteration 16/25 | Loss: 0.00131716
Iteration 17/25 | Loss: 0.00131348
Iteration 18/25 | Loss: 0.00127253
Iteration 19/25 | Loss: 0.00125508
Iteration 20/25 | Loss: 0.00124094
Iteration 21/25 | Loss: 0.00123479
Iteration 22/25 | Loss: 0.00123222
Iteration 23/25 | Loss: 0.00123154
Iteration 24/25 | Loss: 0.00122993
Iteration 25/25 | Loss: 0.00123125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48926854
Iteration 2/25 | Loss: 0.00125178
Iteration 3/25 | Loss: 0.00125177
Iteration 4/25 | Loss: 0.00125177
Iteration 5/25 | Loss: 0.00125177
Iteration 6/25 | Loss: 0.00125177
Iteration 7/25 | Loss: 0.00125177
Iteration 8/25 | Loss: 0.00125177
Iteration 9/25 | Loss: 0.00125177
Iteration 10/25 | Loss: 0.00125177
Iteration 11/25 | Loss: 0.00125177
Iteration 12/25 | Loss: 0.00125177
Iteration 13/25 | Loss: 0.00125177
Iteration 14/25 | Loss: 0.00125177
Iteration 15/25 | Loss: 0.00125177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012517732102423906, 0.0012517732102423906, 0.0012517732102423906, 0.0012517732102423906, 0.0012517732102423906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012517732102423906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125177
Iteration 2/1000 | Loss: 0.00016114
Iteration 3/1000 | Loss: 0.00004587
Iteration 4/1000 | Loss: 0.00020037
Iteration 5/1000 | Loss: 0.00030713
Iteration 6/1000 | Loss: 0.00014528
Iteration 7/1000 | Loss: 0.00005584
Iteration 8/1000 | Loss: 0.00003764
Iteration 9/1000 | Loss: 0.00003801
Iteration 10/1000 | Loss: 0.00003105
Iteration 11/1000 | Loss: 0.00017009
Iteration 12/1000 | Loss: 0.00003103
Iteration 13/1000 | Loss: 0.00002861
Iteration 14/1000 | Loss: 0.00003493
Iteration 15/1000 | Loss: 0.00006785
Iteration 16/1000 | Loss: 0.00003834
Iteration 17/1000 | Loss: 0.00024668
Iteration 18/1000 | Loss: 0.00006079
Iteration 19/1000 | Loss: 0.00004154
Iteration 20/1000 | Loss: 0.00002866
Iteration 21/1000 | Loss: 0.00003866
Iteration 22/1000 | Loss: 0.00003667
Iteration 23/1000 | Loss: 0.00002701
Iteration 24/1000 | Loss: 0.00003816
Iteration 25/1000 | Loss: 0.00002711
Iteration 26/1000 | Loss: 0.00003426
Iteration 27/1000 | Loss: 0.00002562
Iteration 28/1000 | Loss: 0.00002674
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002600
Iteration 31/1000 | Loss: 0.00002507
Iteration 32/1000 | Loss: 0.00002236
Iteration 33/1000 | Loss: 0.00002826
Iteration 34/1000 | Loss: 0.00002901
Iteration 35/1000 | Loss: 0.00002869
Iteration 36/1000 | Loss: 0.00004005
Iteration 37/1000 | Loss: 0.00002872
Iteration 38/1000 | Loss: 0.00002769
Iteration 39/1000 | Loss: 0.00025963
Iteration 40/1000 | Loss: 0.00013658
Iteration 41/1000 | Loss: 0.00018818
Iteration 42/1000 | Loss: 0.00002237
Iteration 43/1000 | Loss: 0.00002377
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002109
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00002028
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00003426
Iteration 51/1000 | Loss: 0.00002668
Iteration 52/1000 | Loss: 0.00002865
Iteration 53/1000 | Loss: 0.00002609
Iteration 54/1000 | Loss: 0.00002710
Iteration 55/1000 | Loss: 0.00003873
Iteration 56/1000 | Loss: 0.00002740
Iteration 57/1000 | Loss: 0.00003611
Iteration 58/1000 | Loss: 0.00003358
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00002554
Iteration 61/1000 | Loss: 0.00003230
Iteration 62/1000 | Loss: 0.00002564
Iteration 63/1000 | Loss: 0.00003066
Iteration 64/1000 | Loss: 0.00002313
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00002613
Iteration 67/1000 | Loss: 0.00002885
Iteration 68/1000 | Loss: 0.00002453
Iteration 69/1000 | Loss: 0.00002373
Iteration 70/1000 | Loss: 0.00002293
Iteration 71/1000 | Loss: 0.00002758
Iteration 72/1000 | Loss: 0.00002541
Iteration 73/1000 | Loss: 0.00002639
Iteration 74/1000 | Loss: 0.00003125
Iteration 75/1000 | Loss: 0.00002747
Iteration 76/1000 | Loss: 0.00003079
Iteration 77/1000 | Loss: 0.00002602
Iteration 78/1000 | Loss: 0.00002709
Iteration 79/1000 | Loss: 0.00002622
Iteration 80/1000 | Loss: 0.00002401
Iteration 81/1000 | Loss: 0.00002508
Iteration 82/1000 | Loss: 0.00002566
Iteration 83/1000 | Loss: 0.00002949
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002490
Iteration 87/1000 | Loss: 0.00002490
Iteration 88/1000 | Loss: 0.00003979
Iteration 89/1000 | Loss: 0.00002434
Iteration 90/1000 | Loss: 0.00002507
Iteration 91/1000 | Loss: 0.00003194
Iteration 92/1000 | Loss: 0.00002576
Iteration 93/1000 | Loss: 0.00002292
Iteration 94/1000 | Loss: 0.00002366
Iteration 95/1000 | Loss: 0.00002721
Iteration 96/1000 | Loss: 0.00002408
Iteration 97/1000 | Loss: 0.00002257
Iteration 98/1000 | Loss: 0.00002502
Iteration 99/1000 | Loss: 0.00002502
Iteration 100/1000 | Loss: 0.00003167
Iteration 101/1000 | Loss: 0.00002803
Iteration 102/1000 | Loss: 0.00003462
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001380
Iteration 109/1000 | Loss: 0.00001380
Iteration 110/1000 | Loss: 0.00001379
Iteration 111/1000 | Loss: 0.00001379
Iteration 112/1000 | Loss: 0.00001375
Iteration 113/1000 | Loss: 0.00001370
Iteration 114/1000 | Loss: 0.00001367
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001359
Iteration 120/1000 | Loss: 0.00001359
Iteration 121/1000 | Loss: 0.00001359
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001337
Iteration 134/1000 | Loss: 0.00001337
Iteration 135/1000 | Loss: 0.00001337
Iteration 136/1000 | Loss: 0.00001337
Iteration 137/1000 | Loss: 0.00001337
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001336
Iteration 141/1000 | Loss: 0.00001335
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001334
Iteration 144/1000 | Loss: 0.00001334
Iteration 145/1000 | Loss: 0.00001334
Iteration 146/1000 | Loss: 0.00001334
Iteration 147/1000 | Loss: 0.00001334
Iteration 148/1000 | Loss: 0.00001333
Iteration 149/1000 | Loss: 0.00001333
Iteration 150/1000 | Loss: 0.00001333
Iteration 151/1000 | Loss: 0.00001332
Iteration 152/1000 | Loss: 0.00001332
Iteration 153/1000 | Loss: 0.00001331
Iteration 154/1000 | Loss: 0.00001331
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001330
Iteration 158/1000 | Loss: 0.00001330
Iteration 159/1000 | Loss: 0.00001330
Iteration 160/1000 | Loss: 0.00001330
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001329
Iteration 163/1000 | Loss: 0.00001329
Iteration 164/1000 | Loss: 0.00001329
Iteration 165/1000 | Loss: 0.00001329
Iteration 166/1000 | Loss: 0.00001329
Iteration 167/1000 | Loss: 0.00001329
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001328
Iteration 171/1000 | Loss: 0.00001328
Iteration 172/1000 | Loss: 0.00001328
Iteration 173/1000 | Loss: 0.00001328
Iteration 174/1000 | Loss: 0.00001328
Iteration 175/1000 | Loss: 0.00001328
Iteration 176/1000 | Loss: 0.00001327
Iteration 177/1000 | Loss: 0.00001327
Iteration 178/1000 | Loss: 0.00001327
Iteration 179/1000 | Loss: 0.00001327
Iteration 180/1000 | Loss: 0.00001327
Iteration 181/1000 | Loss: 0.00001326
Iteration 182/1000 | Loss: 0.00001326
Iteration 183/1000 | Loss: 0.00001326
Iteration 184/1000 | Loss: 0.00001856
Iteration 185/1000 | Loss: 0.00001516
Iteration 186/1000 | Loss: 0.00001324
Iteration 187/1000 | Loss: 0.00001324
Iteration 188/1000 | Loss: 0.00001323
Iteration 189/1000 | Loss: 0.00001323
Iteration 190/1000 | Loss: 0.00001323
Iteration 191/1000 | Loss: 0.00001323
Iteration 192/1000 | Loss: 0.00001323
Iteration 193/1000 | Loss: 0.00001323
Iteration 194/1000 | Loss: 0.00001323
Iteration 195/1000 | Loss: 0.00001323
Iteration 196/1000 | Loss: 0.00001323
Iteration 197/1000 | Loss: 0.00001323
Iteration 198/1000 | Loss: 0.00001323
Iteration 199/1000 | Loss: 0.00001322
Iteration 200/1000 | Loss: 0.00001321
Iteration 201/1000 | Loss: 0.00001321
Iteration 202/1000 | Loss: 0.00001321
Iteration 203/1000 | Loss: 0.00001321
Iteration 204/1000 | Loss: 0.00001320
Iteration 205/1000 | Loss: 0.00001320
Iteration 206/1000 | Loss: 0.00001320
Iteration 207/1000 | Loss: 0.00001320
Iteration 208/1000 | Loss: 0.00001320
Iteration 209/1000 | Loss: 0.00001319
Iteration 210/1000 | Loss: 0.00001319
Iteration 211/1000 | Loss: 0.00001318
Iteration 212/1000 | Loss: 0.00001318
Iteration 213/1000 | Loss: 0.00001318
Iteration 214/1000 | Loss: 0.00001318
Iteration 215/1000 | Loss: 0.00001318
Iteration 216/1000 | Loss: 0.00001318
Iteration 217/1000 | Loss: 0.00001318
Iteration 218/1000 | Loss: 0.00001318
Iteration 219/1000 | Loss: 0.00001318
Iteration 220/1000 | Loss: 0.00001318
Iteration 221/1000 | Loss: 0.00001318
Iteration 222/1000 | Loss: 0.00001318
Iteration 223/1000 | Loss: 0.00001318
Iteration 224/1000 | Loss: 0.00001318
Iteration 225/1000 | Loss: 0.00001318
Iteration 226/1000 | Loss: 0.00001318
Iteration 227/1000 | Loss: 0.00001318
Iteration 228/1000 | Loss: 0.00001318
Iteration 229/1000 | Loss: 0.00001318
Iteration 230/1000 | Loss: 0.00001318
Iteration 231/1000 | Loss: 0.00001318
Iteration 232/1000 | Loss: 0.00001318
Iteration 233/1000 | Loss: 0.00001318
Iteration 234/1000 | Loss: 0.00001318
Iteration 235/1000 | Loss: 0.00001318
Iteration 236/1000 | Loss: 0.00001318
Iteration 237/1000 | Loss: 0.00001318
Iteration 238/1000 | Loss: 0.00001318
Iteration 239/1000 | Loss: 0.00001318
Iteration 240/1000 | Loss: 0.00001318
Iteration 241/1000 | Loss: 0.00001318
Iteration 242/1000 | Loss: 0.00001318
Iteration 243/1000 | Loss: 0.00001318
Iteration 244/1000 | Loss: 0.00001318
Iteration 245/1000 | Loss: 0.00001318
Iteration 246/1000 | Loss: 0.00001318
Iteration 247/1000 | Loss: 0.00001318
Iteration 248/1000 | Loss: 0.00001318
Iteration 249/1000 | Loss: 0.00001318
Iteration 250/1000 | Loss: 0.00001318
Iteration 251/1000 | Loss: 0.00001318
Iteration 252/1000 | Loss: 0.00001318
Iteration 253/1000 | Loss: 0.00001318
Iteration 254/1000 | Loss: 0.00001318
Iteration 255/1000 | Loss: 0.00001318
Iteration 256/1000 | Loss: 0.00001318
Iteration 257/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.317684927926166e-05, 1.317684927926166e-05, 1.317684927926166e-05, 1.317684927926166e-05, 1.317684927926166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.317684927926166e-05

Optimization complete. Final v2v error: 3.163158893585205 mm

Highest mean error: 4.010637283325195 mm for frame 41

Lowest mean error: 3.0235700607299805 mm for frame 218

Saving results

Total time: 234.72744369506836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798004
Iteration 2/25 | Loss: 0.00171557
Iteration 3/25 | Loss: 0.00153565
Iteration 4/25 | Loss: 0.00150374
Iteration 5/25 | Loss: 0.00148545
Iteration 6/25 | Loss: 0.00148718
Iteration 7/25 | Loss: 0.00146904
Iteration 8/25 | Loss: 0.00147080
Iteration 9/25 | Loss: 0.00144406
Iteration 10/25 | Loss: 0.00143793
Iteration 11/25 | Loss: 0.00143466
Iteration 12/25 | Loss: 0.00143538
Iteration 13/25 | Loss: 0.00142300
Iteration 14/25 | Loss: 0.00141331
Iteration 15/25 | Loss: 0.00140965
Iteration 16/25 | Loss: 0.00140813
Iteration 17/25 | Loss: 0.00140778
Iteration 18/25 | Loss: 0.00140753
Iteration 19/25 | Loss: 0.00140720
Iteration 20/25 | Loss: 0.00140895
Iteration 21/25 | Loss: 0.00140453
Iteration 22/25 | Loss: 0.00139765
Iteration 23/25 | Loss: 0.00139416
Iteration 24/25 | Loss: 0.00139739
Iteration 25/25 | Loss: 0.00138717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22371304
Iteration 2/25 | Loss: 0.00378819
Iteration 3/25 | Loss: 0.00378817
Iteration 4/25 | Loss: 0.00378817
Iteration 5/25 | Loss: 0.00378817
Iteration 6/25 | Loss: 0.00378817
Iteration 7/25 | Loss: 0.00378817
Iteration 8/25 | Loss: 0.00378817
Iteration 9/25 | Loss: 0.00378817
Iteration 10/25 | Loss: 0.00378817
Iteration 11/25 | Loss: 0.00378817
Iteration 12/25 | Loss: 0.00378817
Iteration 13/25 | Loss: 0.00378817
Iteration 14/25 | Loss: 0.00378817
Iteration 15/25 | Loss: 0.00378817
Iteration 16/25 | Loss: 0.00378817
Iteration 17/25 | Loss: 0.00378817
Iteration 18/25 | Loss: 0.00378817
Iteration 19/25 | Loss: 0.00378817
Iteration 20/25 | Loss: 0.00378817
Iteration 21/25 | Loss: 0.00378817
Iteration 22/25 | Loss: 0.00378817
Iteration 23/25 | Loss: 0.00378817
Iteration 24/25 | Loss: 0.00378817
Iteration 25/25 | Loss: 0.00378817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378817
Iteration 2/1000 | Loss: 0.00023388
Iteration 3/1000 | Loss: 0.00017485
Iteration 4/1000 | Loss: 0.00036110
Iteration 5/1000 | Loss: 0.00030049
Iteration 6/1000 | Loss: 0.00024249
Iteration 7/1000 | Loss: 0.00019191
Iteration 8/1000 | Loss: 0.00014308
Iteration 9/1000 | Loss: 0.00018558
Iteration 10/1000 | Loss: 0.00012592
Iteration 11/1000 | Loss: 0.00011856
Iteration 12/1000 | Loss: 0.00041177
Iteration 13/1000 | Loss: 0.00015485
Iteration 14/1000 | Loss: 0.00015199
Iteration 15/1000 | Loss: 0.00026340
Iteration 16/1000 | Loss: 0.00022889
Iteration 17/1000 | Loss: 0.00021191
Iteration 18/1000 | Loss: 0.00014218
Iteration 19/1000 | Loss: 0.00012005
Iteration 20/1000 | Loss: 0.00011167
Iteration 21/1000 | Loss: 0.00010710
Iteration 22/1000 | Loss: 0.00010419
Iteration 23/1000 | Loss: 0.00010231
Iteration 24/1000 | Loss: 0.00039042
Iteration 25/1000 | Loss: 0.00342903
Iteration 26/1000 | Loss: 0.00012735
Iteration 27/1000 | Loss: 0.00009849
Iteration 28/1000 | Loss: 0.00009072
Iteration 29/1000 | Loss: 0.00033704
Iteration 30/1000 | Loss: 0.00009998
Iteration 31/1000 | Loss: 0.00008712
Iteration 32/1000 | Loss: 0.00075665
Iteration 33/1000 | Loss: 0.00008162
Iteration 34/1000 | Loss: 0.00007352
Iteration 35/1000 | Loss: 0.00059363
Iteration 36/1000 | Loss: 0.00007083
Iteration 37/1000 | Loss: 0.00006686
Iteration 38/1000 | Loss: 0.00061192
Iteration 39/1000 | Loss: 0.00006440
Iteration 40/1000 | Loss: 0.00006078
Iteration 41/1000 | Loss: 0.00063073
Iteration 42/1000 | Loss: 0.00006006
Iteration 43/1000 | Loss: 0.00057198
Iteration 44/1000 | Loss: 0.00167649
Iteration 45/1000 | Loss: 0.00062450
Iteration 46/1000 | Loss: 0.00010437
Iteration 47/1000 | Loss: 0.00006403
Iteration 48/1000 | Loss: 0.00005691
Iteration 49/1000 | Loss: 0.00005223
Iteration 50/1000 | Loss: 0.00004705
Iteration 51/1000 | Loss: 0.00123052
Iteration 52/1000 | Loss: 0.00005313
Iteration 53/1000 | Loss: 0.00004234
Iteration 54/1000 | Loss: 0.00003859
Iteration 55/1000 | Loss: 0.00065694
Iteration 56/1000 | Loss: 0.00004054
Iteration 57/1000 | Loss: 0.00003389
Iteration 58/1000 | Loss: 0.00003135
Iteration 59/1000 | Loss: 0.00062968
Iteration 60/1000 | Loss: 0.00003496
Iteration 61/1000 | Loss: 0.00113828
Iteration 62/1000 | Loss: 0.00093991
Iteration 63/1000 | Loss: 0.00053772
Iteration 64/1000 | Loss: 0.00004530
Iteration 65/1000 | Loss: 0.00003069
Iteration 66/1000 | Loss: 0.00002602
Iteration 67/1000 | Loss: 0.00002293
Iteration 68/1000 | Loss: 0.00002121
Iteration 69/1000 | Loss: 0.00001946
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001748
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001632
Iteration 74/1000 | Loss: 0.00001606
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001551
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001548
Iteration 79/1000 | Loss: 0.00001545
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001542
Iteration 84/1000 | Loss: 0.00001541
Iteration 85/1000 | Loss: 0.00001534
Iteration 86/1000 | Loss: 0.00001533
Iteration 87/1000 | Loss: 0.00001533
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001502
Iteration 93/1000 | Loss: 0.00001502
Iteration 94/1000 | Loss: 0.00001501
Iteration 95/1000 | Loss: 0.00001501
Iteration 96/1000 | Loss: 0.00001501
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001499
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001497
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001496
Iteration 108/1000 | Loss: 0.00001496
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001494
Iteration 116/1000 | Loss: 0.00001494
Iteration 117/1000 | Loss: 0.00001493
Iteration 118/1000 | Loss: 0.00001493
Iteration 119/1000 | Loss: 0.00001493
Iteration 120/1000 | Loss: 0.00001493
Iteration 121/1000 | Loss: 0.00001493
Iteration 122/1000 | Loss: 0.00001493
Iteration 123/1000 | Loss: 0.00001493
Iteration 124/1000 | Loss: 0.00001493
Iteration 125/1000 | Loss: 0.00001493
Iteration 126/1000 | Loss: 0.00001493
Iteration 127/1000 | Loss: 0.00001492
Iteration 128/1000 | Loss: 0.00001492
Iteration 129/1000 | Loss: 0.00001492
Iteration 130/1000 | Loss: 0.00001492
Iteration 131/1000 | Loss: 0.00001492
Iteration 132/1000 | Loss: 0.00001491
Iteration 133/1000 | Loss: 0.00001491
Iteration 134/1000 | Loss: 0.00001491
Iteration 135/1000 | Loss: 0.00001491
Iteration 136/1000 | Loss: 0.00001491
Iteration 137/1000 | Loss: 0.00001491
Iteration 138/1000 | Loss: 0.00001491
Iteration 139/1000 | Loss: 0.00001490
Iteration 140/1000 | Loss: 0.00001490
Iteration 141/1000 | Loss: 0.00001490
Iteration 142/1000 | Loss: 0.00001490
Iteration 143/1000 | Loss: 0.00001490
Iteration 144/1000 | Loss: 0.00001490
Iteration 145/1000 | Loss: 0.00001490
Iteration 146/1000 | Loss: 0.00001489
Iteration 147/1000 | Loss: 0.00001489
Iteration 148/1000 | Loss: 0.00001489
Iteration 149/1000 | Loss: 0.00001489
Iteration 150/1000 | Loss: 0.00001489
Iteration 151/1000 | Loss: 0.00001489
Iteration 152/1000 | Loss: 0.00001489
Iteration 153/1000 | Loss: 0.00001489
Iteration 154/1000 | Loss: 0.00001489
Iteration 155/1000 | Loss: 0.00001488
Iteration 156/1000 | Loss: 0.00001488
Iteration 157/1000 | Loss: 0.00001488
Iteration 158/1000 | Loss: 0.00001488
Iteration 159/1000 | Loss: 0.00001488
Iteration 160/1000 | Loss: 0.00001488
Iteration 161/1000 | Loss: 0.00001488
Iteration 162/1000 | Loss: 0.00001488
Iteration 163/1000 | Loss: 0.00001488
Iteration 164/1000 | Loss: 0.00001488
Iteration 165/1000 | Loss: 0.00001488
Iteration 166/1000 | Loss: 0.00001488
Iteration 167/1000 | Loss: 0.00001488
Iteration 168/1000 | Loss: 0.00001488
Iteration 169/1000 | Loss: 0.00001488
Iteration 170/1000 | Loss: 0.00001488
Iteration 171/1000 | Loss: 0.00001488
Iteration 172/1000 | Loss: 0.00001488
Iteration 173/1000 | Loss: 0.00001488
Iteration 174/1000 | Loss: 0.00001488
Iteration 175/1000 | Loss: 0.00001488
Iteration 176/1000 | Loss: 0.00001488
Iteration 177/1000 | Loss: 0.00001488
Iteration 178/1000 | Loss: 0.00001488
Iteration 179/1000 | Loss: 0.00001488
Iteration 180/1000 | Loss: 0.00001488
Iteration 181/1000 | Loss: 0.00001488
Iteration 182/1000 | Loss: 0.00001488
Iteration 183/1000 | Loss: 0.00001488
Iteration 184/1000 | Loss: 0.00001488
Iteration 185/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.4878767615300603e-05, 1.4878767615300603e-05, 1.4878767615300603e-05, 1.4878767615300603e-05, 1.4878767615300603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4878767615300603e-05

Optimization complete. Final v2v error: 3.227721929550171 mm

Highest mean error: 3.8735055923461914 mm for frame 82

Lowest mean error: 2.642637014389038 mm for frame 3

Saving results

Total time: 162.2232735157013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998792
Iteration 2/25 | Loss: 0.00237753
Iteration 3/25 | Loss: 0.00200609
Iteration 4/25 | Loss: 0.00192011
Iteration 5/25 | Loss: 0.00186479
Iteration 6/25 | Loss: 0.00179084
Iteration 7/25 | Loss: 0.00174368
Iteration 8/25 | Loss: 0.00169855
Iteration 9/25 | Loss: 0.00168848
Iteration 10/25 | Loss: 0.00166568
Iteration 11/25 | Loss: 0.00165840
Iteration 12/25 | Loss: 0.00163651
Iteration 13/25 | Loss: 0.00162571
Iteration 14/25 | Loss: 0.00161495
Iteration 15/25 | Loss: 0.00161266
Iteration 16/25 | Loss: 0.00161199
Iteration 17/25 | Loss: 0.00161174
Iteration 18/25 | Loss: 0.00161160
Iteration 19/25 | Loss: 0.00161155
Iteration 20/25 | Loss: 0.00161155
Iteration 21/25 | Loss: 0.00161154
Iteration 22/25 | Loss: 0.00161154
Iteration 23/25 | Loss: 0.00161154
Iteration 24/25 | Loss: 0.00161154
Iteration 25/25 | Loss: 0.00161154

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25641501
Iteration 2/25 | Loss: 0.00393975
Iteration 3/25 | Loss: 0.00393974
Iteration 4/25 | Loss: 0.00393974
Iteration 5/25 | Loss: 0.00393974
Iteration 6/25 | Loss: 0.00393974
Iteration 7/25 | Loss: 0.00393974
Iteration 8/25 | Loss: 0.00393974
Iteration 9/25 | Loss: 0.00393974
Iteration 10/25 | Loss: 0.00393974
Iteration 11/25 | Loss: 0.00393974
Iteration 12/25 | Loss: 0.00393974
Iteration 13/25 | Loss: 0.00393974
Iteration 14/25 | Loss: 0.00393974
Iteration 15/25 | Loss: 0.00393974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003939738031476736, 0.003939738031476736, 0.003939738031476736, 0.003939738031476736, 0.003939738031476736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003939738031476736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393974
Iteration 2/1000 | Loss: 0.00048770
Iteration 3/1000 | Loss: 0.00037638
Iteration 4/1000 | Loss: 0.00034497
Iteration 5/1000 | Loss: 0.00022419
Iteration 6/1000 | Loss: 0.00019748
Iteration 7/1000 | Loss: 0.00018250
Iteration 8/1000 | Loss: 0.00016461
Iteration 9/1000 | Loss: 0.00015175
Iteration 10/1000 | Loss: 0.00014379
Iteration 11/1000 | Loss: 0.00013900
Iteration 12/1000 | Loss: 0.00013458
Iteration 13/1000 | Loss: 0.00013158
Iteration 14/1000 | Loss: 0.00012946
Iteration 15/1000 | Loss: 0.00012747
Iteration 16/1000 | Loss: 0.00012580
Iteration 17/1000 | Loss: 0.00012473
Iteration 18/1000 | Loss: 0.00012343
Iteration 19/1000 | Loss: 0.00012263
Iteration 20/1000 | Loss: 0.00012204
Iteration 21/1000 | Loss: 0.00012148
Iteration 22/1000 | Loss: 0.00012117
Iteration 23/1000 | Loss: 0.00023888
Iteration 24/1000 | Loss: 0.00080458
Iteration 25/1000 | Loss: 0.00208091
Iteration 26/1000 | Loss: 0.00089439
Iteration 27/1000 | Loss: 0.00055318
Iteration 28/1000 | Loss: 0.00038902
Iteration 29/1000 | Loss: 0.00028094
Iteration 30/1000 | Loss: 0.00017269
Iteration 31/1000 | Loss: 0.00020675
Iteration 32/1000 | Loss: 0.00012882
Iteration 33/1000 | Loss: 0.00008509
Iteration 34/1000 | Loss: 0.00006447
Iteration 35/1000 | Loss: 0.00005414
Iteration 36/1000 | Loss: 0.00004748
Iteration 37/1000 | Loss: 0.00004238
Iteration 38/1000 | Loss: 0.00003928
Iteration 39/1000 | Loss: 0.00003642
Iteration 40/1000 | Loss: 0.00003499
Iteration 41/1000 | Loss: 0.00003304
Iteration 42/1000 | Loss: 0.00003207
Iteration 43/1000 | Loss: 0.00003113
Iteration 44/1000 | Loss: 0.00003046
Iteration 45/1000 | Loss: 0.00002999
Iteration 46/1000 | Loss: 0.00002963
Iteration 47/1000 | Loss: 0.00002943
Iteration 48/1000 | Loss: 0.00002940
Iteration 49/1000 | Loss: 0.00002939
Iteration 50/1000 | Loss: 0.00002927
Iteration 51/1000 | Loss: 0.00002922
Iteration 52/1000 | Loss: 0.00002922
Iteration 53/1000 | Loss: 0.00002922
Iteration 54/1000 | Loss: 0.00002920
Iteration 55/1000 | Loss: 0.00002920
Iteration 56/1000 | Loss: 0.00002919
Iteration 57/1000 | Loss: 0.00002918
Iteration 58/1000 | Loss: 0.00002917
Iteration 59/1000 | Loss: 0.00002917
Iteration 60/1000 | Loss: 0.00002916
Iteration 61/1000 | Loss: 0.00002916
Iteration 62/1000 | Loss: 0.00002915
Iteration 63/1000 | Loss: 0.00002914
Iteration 64/1000 | Loss: 0.00002914
Iteration 65/1000 | Loss: 0.00002914
Iteration 66/1000 | Loss: 0.00002914
Iteration 67/1000 | Loss: 0.00002913
Iteration 68/1000 | Loss: 0.00002913
Iteration 69/1000 | Loss: 0.00002913
Iteration 70/1000 | Loss: 0.00002912
Iteration 71/1000 | Loss: 0.00002911
Iteration 72/1000 | Loss: 0.00002911
Iteration 73/1000 | Loss: 0.00002911
Iteration 74/1000 | Loss: 0.00002911
Iteration 75/1000 | Loss: 0.00002911
Iteration 76/1000 | Loss: 0.00002910
Iteration 77/1000 | Loss: 0.00002910
Iteration 78/1000 | Loss: 0.00002910
Iteration 79/1000 | Loss: 0.00002910
Iteration 80/1000 | Loss: 0.00002910
Iteration 81/1000 | Loss: 0.00002909
Iteration 82/1000 | Loss: 0.00002909
Iteration 83/1000 | Loss: 0.00002909
Iteration 84/1000 | Loss: 0.00002909
Iteration 85/1000 | Loss: 0.00002909
Iteration 86/1000 | Loss: 0.00002909
Iteration 87/1000 | Loss: 0.00002909
Iteration 88/1000 | Loss: 0.00002908
Iteration 89/1000 | Loss: 0.00002908
Iteration 90/1000 | Loss: 0.00002908
Iteration 91/1000 | Loss: 0.00002908
Iteration 92/1000 | Loss: 0.00002908
Iteration 93/1000 | Loss: 0.00002908
Iteration 94/1000 | Loss: 0.00002908
Iteration 95/1000 | Loss: 0.00002908
Iteration 96/1000 | Loss: 0.00002908
Iteration 97/1000 | Loss: 0.00002907
Iteration 98/1000 | Loss: 0.00002907
Iteration 99/1000 | Loss: 0.00002907
Iteration 100/1000 | Loss: 0.00002907
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002907
Iteration 103/1000 | Loss: 0.00002906
Iteration 104/1000 | Loss: 0.00002906
Iteration 105/1000 | Loss: 0.00002906
Iteration 106/1000 | Loss: 0.00002906
Iteration 107/1000 | Loss: 0.00002906
Iteration 108/1000 | Loss: 0.00002905
Iteration 109/1000 | Loss: 0.00002905
Iteration 110/1000 | Loss: 0.00002905
Iteration 111/1000 | Loss: 0.00002905
Iteration 112/1000 | Loss: 0.00002904
Iteration 113/1000 | Loss: 0.00002904
Iteration 114/1000 | Loss: 0.00002904
Iteration 115/1000 | Loss: 0.00002904
Iteration 116/1000 | Loss: 0.00002903
Iteration 117/1000 | Loss: 0.00002903
Iteration 118/1000 | Loss: 0.00002903
Iteration 119/1000 | Loss: 0.00002903
Iteration 120/1000 | Loss: 0.00002902
Iteration 121/1000 | Loss: 0.00002902
Iteration 122/1000 | Loss: 0.00002902
Iteration 123/1000 | Loss: 0.00002902
Iteration 124/1000 | Loss: 0.00002902
Iteration 125/1000 | Loss: 0.00002902
Iteration 126/1000 | Loss: 0.00002902
Iteration 127/1000 | Loss: 0.00002902
Iteration 128/1000 | Loss: 0.00002901
Iteration 129/1000 | Loss: 0.00002901
Iteration 130/1000 | Loss: 0.00002901
Iteration 131/1000 | Loss: 0.00002900
Iteration 132/1000 | Loss: 0.00002900
Iteration 133/1000 | Loss: 0.00002900
Iteration 134/1000 | Loss: 0.00002900
Iteration 135/1000 | Loss: 0.00002900
Iteration 136/1000 | Loss: 0.00002900
Iteration 137/1000 | Loss: 0.00002900
Iteration 138/1000 | Loss: 0.00002900
Iteration 139/1000 | Loss: 0.00002900
Iteration 140/1000 | Loss: 0.00002900
Iteration 141/1000 | Loss: 0.00002900
Iteration 142/1000 | Loss: 0.00002900
Iteration 143/1000 | Loss: 0.00002900
Iteration 144/1000 | Loss: 0.00002900
Iteration 145/1000 | Loss: 0.00002900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.8996299079153687e-05, 2.8996299079153687e-05, 2.8996299079153687e-05, 2.8996299079153687e-05, 2.8996299079153687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8996299079153687e-05

Optimization complete. Final v2v error: 3.4335830211639404 mm

Highest mean error: 10.872117042541504 mm for frame 197

Lowest mean error: 2.8580148220062256 mm for frame 5

Saving results

Total time: 122.88623929023743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410632
Iteration 2/25 | Loss: 0.00127225
Iteration 3/25 | Loss: 0.00120261
Iteration 4/25 | Loss: 0.00119155
Iteration 5/25 | Loss: 0.00118838
Iteration 6/25 | Loss: 0.00118766
Iteration 7/25 | Loss: 0.00118766
Iteration 8/25 | Loss: 0.00118766
Iteration 9/25 | Loss: 0.00118766
Iteration 10/25 | Loss: 0.00118766
Iteration 11/25 | Loss: 0.00118766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011876595672219992, 0.0011876595672219992, 0.0011876595672219992, 0.0011876595672219992, 0.0011876595672219992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011876595672219992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.38412905
Iteration 2/25 | Loss: 0.00141328
Iteration 3/25 | Loss: 0.00141328
Iteration 4/25 | Loss: 0.00141328
Iteration 5/25 | Loss: 0.00141328
Iteration 6/25 | Loss: 0.00141328
Iteration 7/25 | Loss: 0.00141328
Iteration 8/25 | Loss: 0.00141328
Iteration 9/25 | Loss: 0.00141328
Iteration 10/25 | Loss: 0.00141328
Iteration 11/25 | Loss: 0.00141328
Iteration 12/25 | Loss: 0.00141328
Iteration 13/25 | Loss: 0.00141328
Iteration 14/25 | Loss: 0.00141328
Iteration 15/25 | Loss: 0.00141328
Iteration 16/25 | Loss: 0.00141328
Iteration 17/25 | Loss: 0.00141328
Iteration 18/25 | Loss: 0.00141328
Iteration 19/25 | Loss: 0.00141328
Iteration 20/25 | Loss: 0.00141328
Iteration 21/25 | Loss: 0.00141328
Iteration 22/25 | Loss: 0.00141328
Iteration 23/25 | Loss: 0.00141328
Iteration 24/25 | Loss: 0.00141328
Iteration 25/25 | Loss: 0.00141328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141328
Iteration 2/1000 | Loss: 0.00002456
Iteration 3/1000 | Loss: 0.00001601
Iteration 4/1000 | Loss: 0.00001357
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001177
Iteration 8/1000 | Loss: 0.00001144
Iteration 9/1000 | Loss: 0.00001119
Iteration 10/1000 | Loss: 0.00001092
Iteration 11/1000 | Loss: 0.00001076
Iteration 12/1000 | Loss: 0.00001073
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001046
Iteration 17/1000 | Loss: 0.00001042
Iteration 18/1000 | Loss: 0.00001036
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001036
Iteration 21/1000 | Loss: 0.00001032
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001028
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001026
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001024
Iteration 32/1000 | Loss: 0.00001022
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001018
Iteration 36/1000 | Loss: 0.00001018
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001015
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001012
Iteration 44/1000 | Loss: 0.00001012
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001008
Iteration 49/1000 | Loss: 0.00001007
Iteration 50/1000 | Loss: 0.00001007
Iteration 51/1000 | Loss: 0.00001007
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00000999
Iteration 72/1000 | Loss: 0.00000999
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000995
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00000994
Iteration 94/1000 | Loss: 0.00000993
Iteration 95/1000 | Loss: 0.00000993
Iteration 96/1000 | Loss: 0.00000992
Iteration 97/1000 | Loss: 0.00000992
Iteration 98/1000 | Loss: 0.00000992
Iteration 99/1000 | Loss: 0.00000992
Iteration 100/1000 | Loss: 0.00000992
Iteration 101/1000 | Loss: 0.00000992
Iteration 102/1000 | Loss: 0.00000991
Iteration 103/1000 | Loss: 0.00000991
Iteration 104/1000 | Loss: 0.00000991
Iteration 105/1000 | Loss: 0.00000991
Iteration 106/1000 | Loss: 0.00000991
Iteration 107/1000 | Loss: 0.00000991
Iteration 108/1000 | Loss: 0.00000991
Iteration 109/1000 | Loss: 0.00000991
Iteration 110/1000 | Loss: 0.00000991
Iteration 111/1000 | Loss: 0.00000991
Iteration 112/1000 | Loss: 0.00000991
Iteration 113/1000 | Loss: 0.00000991
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000991
Iteration 116/1000 | Loss: 0.00000991
Iteration 117/1000 | Loss: 0.00000991
Iteration 118/1000 | Loss: 0.00000990
Iteration 119/1000 | Loss: 0.00000990
Iteration 120/1000 | Loss: 0.00000990
Iteration 121/1000 | Loss: 0.00000990
Iteration 122/1000 | Loss: 0.00000990
Iteration 123/1000 | Loss: 0.00000990
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00000990
Iteration 129/1000 | Loss: 0.00000989
Iteration 130/1000 | Loss: 0.00000989
Iteration 131/1000 | Loss: 0.00000989
Iteration 132/1000 | Loss: 0.00000989
Iteration 133/1000 | Loss: 0.00000989
Iteration 134/1000 | Loss: 0.00000989
Iteration 135/1000 | Loss: 0.00000989
Iteration 136/1000 | Loss: 0.00000989
Iteration 137/1000 | Loss: 0.00000989
Iteration 138/1000 | Loss: 0.00000989
Iteration 139/1000 | Loss: 0.00000989
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000989
Iteration 142/1000 | Loss: 0.00000989
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000989
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000989
Iteration 149/1000 | Loss: 0.00000989
Iteration 150/1000 | Loss: 0.00000989
Iteration 151/1000 | Loss: 0.00000989
Iteration 152/1000 | Loss: 0.00000989
Iteration 153/1000 | Loss: 0.00000989
Iteration 154/1000 | Loss: 0.00000989
Iteration 155/1000 | Loss: 0.00000989
Iteration 156/1000 | Loss: 0.00000989
Iteration 157/1000 | Loss: 0.00000989
Iteration 158/1000 | Loss: 0.00000989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [9.892412890621927e-06, 9.892412890621927e-06, 9.892412890621927e-06, 9.892412890621927e-06, 9.892412890621927e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.892412890621927e-06

Optimization complete. Final v2v error: 2.7281789779663086 mm

Highest mean error: 3.111548662185669 mm for frame 87

Lowest mean error: 2.4852006435394287 mm for frame 45

Saving results

Total time: 37.72574543952942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444709
Iteration 2/25 | Loss: 0.00133073
Iteration 3/25 | Loss: 0.00120195
Iteration 4/25 | Loss: 0.00118775
Iteration 5/25 | Loss: 0.00118438
Iteration 6/25 | Loss: 0.00118413
Iteration 7/25 | Loss: 0.00118413
Iteration 8/25 | Loss: 0.00118413
Iteration 9/25 | Loss: 0.00118413
Iteration 10/25 | Loss: 0.00118413
Iteration 11/25 | Loss: 0.00118413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001184133579954505, 0.001184133579954505, 0.001184133579954505, 0.001184133579954505, 0.001184133579954505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184133579954505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22133934
Iteration 2/25 | Loss: 0.00148519
Iteration 3/25 | Loss: 0.00148519
Iteration 4/25 | Loss: 0.00148519
Iteration 5/25 | Loss: 0.00148519
Iteration 6/25 | Loss: 0.00148518
Iteration 7/25 | Loss: 0.00148518
Iteration 8/25 | Loss: 0.00148518
Iteration 9/25 | Loss: 0.00148518
Iteration 10/25 | Loss: 0.00148518
Iteration 11/25 | Loss: 0.00148518
Iteration 12/25 | Loss: 0.00148518
Iteration 13/25 | Loss: 0.00148518
Iteration 14/25 | Loss: 0.00148518
Iteration 15/25 | Loss: 0.00148518
Iteration 16/25 | Loss: 0.00148518
Iteration 17/25 | Loss: 0.00148518
Iteration 18/25 | Loss: 0.00148518
Iteration 19/25 | Loss: 0.00148518
Iteration 20/25 | Loss: 0.00148518
Iteration 21/25 | Loss: 0.00148518
Iteration 22/25 | Loss: 0.00148518
Iteration 23/25 | Loss: 0.00148518
Iteration 24/25 | Loss: 0.00148518
Iteration 25/25 | Loss: 0.00148518

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148518
Iteration 2/1000 | Loss: 0.00004603
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002273
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002020
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001812
Iteration 12/1000 | Loss: 0.00001797
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001789
Iteration 15/1000 | Loss: 0.00001783
Iteration 16/1000 | Loss: 0.00001782
Iteration 17/1000 | Loss: 0.00001782
Iteration 18/1000 | Loss: 0.00001782
Iteration 19/1000 | Loss: 0.00001782
Iteration 20/1000 | Loss: 0.00001781
Iteration 21/1000 | Loss: 0.00001777
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001775
Iteration 25/1000 | Loss: 0.00001775
Iteration 26/1000 | Loss: 0.00001775
Iteration 27/1000 | Loss: 0.00001774
Iteration 28/1000 | Loss: 0.00001773
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001773
Iteration 31/1000 | Loss: 0.00001772
Iteration 32/1000 | Loss: 0.00001772
Iteration 33/1000 | Loss: 0.00001772
Iteration 34/1000 | Loss: 0.00001772
Iteration 35/1000 | Loss: 0.00001772
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001771
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001771
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001771
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001770
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001766
Iteration 50/1000 | Loss: 0.00001766
Iteration 51/1000 | Loss: 0.00001766
Iteration 52/1000 | Loss: 0.00001765
Iteration 53/1000 | Loss: 0.00001765
Iteration 54/1000 | Loss: 0.00001764
Iteration 55/1000 | Loss: 0.00001764
Iteration 56/1000 | Loss: 0.00001764
Iteration 57/1000 | Loss: 0.00001764
Iteration 58/1000 | Loss: 0.00001764
Iteration 59/1000 | Loss: 0.00001764
Iteration 60/1000 | Loss: 0.00001763
Iteration 61/1000 | Loss: 0.00001763
Iteration 62/1000 | Loss: 0.00001762
Iteration 63/1000 | Loss: 0.00001762
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001761
Iteration 69/1000 | Loss: 0.00001761
Iteration 70/1000 | Loss: 0.00001761
Iteration 71/1000 | Loss: 0.00001761
Iteration 72/1000 | Loss: 0.00001760
Iteration 73/1000 | Loss: 0.00001760
Iteration 74/1000 | Loss: 0.00001760
Iteration 75/1000 | Loss: 0.00001760
Iteration 76/1000 | Loss: 0.00001760
Iteration 77/1000 | Loss: 0.00001760
Iteration 78/1000 | Loss: 0.00001759
Iteration 79/1000 | Loss: 0.00001759
Iteration 80/1000 | Loss: 0.00001759
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001759
Iteration 85/1000 | Loss: 0.00001759
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001758
Iteration 94/1000 | Loss: 0.00001758
Iteration 95/1000 | Loss: 0.00001758
Iteration 96/1000 | Loss: 0.00001758
Iteration 97/1000 | Loss: 0.00001758
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001757
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001756
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001754
Iteration 123/1000 | Loss: 0.00001754
Iteration 124/1000 | Loss: 0.00001754
Iteration 125/1000 | Loss: 0.00001754
Iteration 126/1000 | Loss: 0.00001754
Iteration 127/1000 | Loss: 0.00001754
Iteration 128/1000 | Loss: 0.00001754
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7541235138196498e-05, 1.7541235138196498e-05, 1.7541235138196498e-05, 1.7541235138196498e-05, 1.7541235138196498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7541235138196498e-05

Optimization complete. Final v2v error: 3.625656843185425 mm

Highest mean error: 4.1354756355285645 mm for frame 175

Lowest mean error: 3.2988274097442627 mm for frame 9

Saving results

Total time: 39.40010452270508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462324
Iteration 2/25 | Loss: 0.00124698
Iteration 3/25 | Loss: 0.00118416
Iteration 4/25 | Loss: 0.00117868
Iteration 5/25 | Loss: 0.00117670
Iteration 6/25 | Loss: 0.00117630
Iteration 7/25 | Loss: 0.00117630
Iteration 8/25 | Loss: 0.00117630
Iteration 9/25 | Loss: 0.00117630
Iteration 10/25 | Loss: 0.00117630
Iteration 11/25 | Loss: 0.00117630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011763009242713451, 0.0011763009242713451, 0.0011763009242713451, 0.0011763009242713451, 0.0011763009242713451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011763009242713451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.32745361
Iteration 2/25 | Loss: 0.00147947
Iteration 3/25 | Loss: 0.00147947
Iteration 4/25 | Loss: 0.00147947
Iteration 5/25 | Loss: 0.00147947
Iteration 6/25 | Loss: 0.00147947
Iteration 7/25 | Loss: 0.00147947
Iteration 8/25 | Loss: 0.00147947
Iteration 9/25 | Loss: 0.00147947
Iteration 10/25 | Loss: 0.00147947
Iteration 11/25 | Loss: 0.00147947
Iteration 12/25 | Loss: 0.00147947
Iteration 13/25 | Loss: 0.00147947
Iteration 14/25 | Loss: 0.00147947
Iteration 15/25 | Loss: 0.00147947
Iteration 16/25 | Loss: 0.00147947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014794663293287158, 0.0014794663293287158, 0.0014794663293287158, 0.0014794663293287158, 0.0014794663293287158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014794663293287158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147947
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00002703
Iteration 4/1000 | Loss: 0.00002265
Iteration 5/1000 | Loss: 0.00002065
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001832
Iteration 10/1000 | Loss: 0.00001812
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001800
Iteration 13/1000 | Loss: 0.00001797
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001791
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00001786
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001782
Iteration 21/1000 | Loss: 0.00001782
Iteration 22/1000 | Loss: 0.00001781
Iteration 23/1000 | Loss: 0.00001781
Iteration 24/1000 | Loss: 0.00001781
Iteration 25/1000 | Loss: 0.00001780
Iteration 26/1000 | Loss: 0.00001780
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001778
Iteration 32/1000 | Loss: 0.00001778
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001778
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001777
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001776
Iteration 44/1000 | Loss: 0.00001776
Iteration 45/1000 | Loss: 0.00001776
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001774
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001774
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001773
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001772
Iteration 57/1000 | Loss: 0.00001772
Iteration 58/1000 | Loss: 0.00001772
Iteration 59/1000 | Loss: 0.00001772
Iteration 60/1000 | Loss: 0.00001772
Iteration 61/1000 | Loss: 0.00001772
Iteration 62/1000 | Loss: 0.00001771
Iteration 63/1000 | Loss: 0.00001771
Iteration 64/1000 | Loss: 0.00001771
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001770
Iteration 67/1000 | Loss: 0.00001769
Iteration 68/1000 | Loss: 0.00001769
Iteration 69/1000 | Loss: 0.00001769
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001766
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00001766
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001765
Iteration 84/1000 | Loss: 0.00001764
Iteration 85/1000 | Loss: 0.00001764
Iteration 86/1000 | Loss: 0.00001764
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001763
Iteration 94/1000 | Loss: 0.00001763
Iteration 95/1000 | Loss: 0.00001763
Iteration 96/1000 | Loss: 0.00001763
Iteration 97/1000 | Loss: 0.00001763
Iteration 98/1000 | Loss: 0.00001762
Iteration 99/1000 | Loss: 0.00001762
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001761
Iteration 104/1000 | Loss: 0.00001761
Iteration 105/1000 | Loss: 0.00001761
Iteration 106/1000 | Loss: 0.00001761
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001760
Iteration 109/1000 | Loss: 0.00001760
Iteration 110/1000 | Loss: 0.00001760
Iteration 111/1000 | Loss: 0.00001760
Iteration 112/1000 | Loss: 0.00001760
Iteration 113/1000 | Loss: 0.00001760
Iteration 114/1000 | Loss: 0.00001759
Iteration 115/1000 | Loss: 0.00001759
Iteration 116/1000 | Loss: 0.00001759
Iteration 117/1000 | Loss: 0.00001759
Iteration 118/1000 | Loss: 0.00001759
Iteration 119/1000 | Loss: 0.00001759
Iteration 120/1000 | Loss: 0.00001759
Iteration 121/1000 | Loss: 0.00001759
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.7587470210855827e-05, 1.7587470210855827e-05, 1.7587470210855827e-05, 1.7587470210855827e-05, 1.7587470210855827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7587470210855827e-05

Optimization complete. Final v2v error: 3.6169588565826416 mm

Highest mean error: 4.4951491355896 mm for frame 96

Lowest mean error: 3.053626298904419 mm for frame 38

Saving results

Total time: 34.16408157348633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827556
Iteration 2/25 | Loss: 0.00157498
Iteration 3/25 | Loss: 0.00131095
Iteration 4/25 | Loss: 0.00126003
Iteration 5/25 | Loss: 0.00124498
Iteration 6/25 | Loss: 0.00123976
Iteration 7/25 | Loss: 0.00123848
Iteration 8/25 | Loss: 0.00123848
Iteration 9/25 | Loss: 0.00123848
Iteration 10/25 | Loss: 0.00123848
Iteration 11/25 | Loss: 0.00123848
Iteration 12/25 | Loss: 0.00123848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012384775327518582, 0.0012384775327518582, 0.0012384775327518582, 0.0012384775327518582, 0.0012384775327518582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012384775327518582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22428644
Iteration 2/25 | Loss: 0.00182479
Iteration 3/25 | Loss: 0.00182478
Iteration 4/25 | Loss: 0.00182478
Iteration 5/25 | Loss: 0.00182478
Iteration 6/25 | Loss: 0.00182478
Iteration 7/25 | Loss: 0.00182478
Iteration 8/25 | Loss: 0.00182478
Iteration 9/25 | Loss: 0.00182478
Iteration 10/25 | Loss: 0.00182478
Iteration 11/25 | Loss: 0.00182478
Iteration 12/25 | Loss: 0.00182478
Iteration 13/25 | Loss: 0.00182478
Iteration 14/25 | Loss: 0.00182478
Iteration 15/25 | Loss: 0.00182478
Iteration 16/25 | Loss: 0.00182478
Iteration 17/25 | Loss: 0.00182478
Iteration 18/25 | Loss: 0.00182478
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018247789703309536, 0.0018247789703309536, 0.0018247789703309536, 0.0018247789703309536, 0.0018247789703309536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018247789703309536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182478
Iteration 2/1000 | Loss: 0.00008610
Iteration 3/1000 | Loss: 0.00004877
Iteration 4/1000 | Loss: 0.00003781
Iteration 5/1000 | Loss: 0.00003374
Iteration 6/1000 | Loss: 0.00003102
Iteration 7/1000 | Loss: 0.00002936
Iteration 8/1000 | Loss: 0.00002824
Iteration 9/1000 | Loss: 0.00002742
Iteration 10/1000 | Loss: 0.00002682
Iteration 11/1000 | Loss: 0.00002629
Iteration 12/1000 | Loss: 0.00002589
Iteration 13/1000 | Loss: 0.00002554
Iteration 14/1000 | Loss: 0.00002529
Iteration 15/1000 | Loss: 0.00002511
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002502
Iteration 18/1000 | Loss: 0.00002495
Iteration 19/1000 | Loss: 0.00002494
Iteration 20/1000 | Loss: 0.00002493
Iteration 21/1000 | Loss: 0.00002490
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00002489
Iteration 25/1000 | Loss: 0.00002488
Iteration 26/1000 | Loss: 0.00002487
Iteration 27/1000 | Loss: 0.00002487
Iteration 28/1000 | Loss: 0.00002482
Iteration 29/1000 | Loss: 0.00002480
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002478
Iteration 32/1000 | Loss: 0.00002476
Iteration 33/1000 | Loss: 0.00002475
Iteration 34/1000 | Loss: 0.00002474
Iteration 35/1000 | Loss: 0.00002473
Iteration 36/1000 | Loss: 0.00002472
Iteration 37/1000 | Loss: 0.00002471
Iteration 38/1000 | Loss: 0.00002471
Iteration 39/1000 | Loss: 0.00002470
Iteration 40/1000 | Loss: 0.00002470
Iteration 41/1000 | Loss: 0.00002470
Iteration 42/1000 | Loss: 0.00002469
Iteration 43/1000 | Loss: 0.00002469
Iteration 44/1000 | Loss: 0.00002469
Iteration 45/1000 | Loss: 0.00002468
Iteration 46/1000 | Loss: 0.00002468
Iteration 47/1000 | Loss: 0.00002467
Iteration 48/1000 | Loss: 0.00002467
Iteration 49/1000 | Loss: 0.00002466
Iteration 50/1000 | Loss: 0.00002466
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002466
Iteration 54/1000 | Loss: 0.00002465
Iteration 55/1000 | Loss: 0.00002465
Iteration 56/1000 | Loss: 0.00002465
Iteration 57/1000 | Loss: 0.00002464
Iteration 58/1000 | Loss: 0.00002464
Iteration 59/1000 | Loss: 0.00002464
Iteration 60/1000 | Loss: 0.00002464
Iteration 61/1000 | Loss: 0.00002463
Iteration 62/1000 | Loss: 0.00002463
Iteration 63/1000 | Loss: 0.00002463
Iteration 64/1000 | Loss: 0.00002463
Iteration 65/1000 | Loss: 0.00002462
Iteration 66/1000 | Loss: 0.00002462
Iteration 67/1000 | Loss: 0.00002462
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002460
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002460
Iteration 74/1000 | Loss: 0.00002460
Iteration 75/1000 | Loss: 0.00002460
Iteration 76/1000 | Loss: 0.00002459
Iteration 77/1000 | Loss: 0.00002459
Iteration 78/1000 | Loss: 0.00002459
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002458
Iteration 84/1000 | Loss: 0.00002458
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002457
Iteration 88/1000 | Loss: 0.00002457
Iteration 89/1000 | Loss: 0.00002457
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002456
Iteration 95/1000 | Loss: 0.00002456
Iteration 96/1000 | Loss: 0.00002456
Iteration 97/1000 | Loss: 0.00002456
Iteration 98/1000 | Loss: 0.00002456
Iteration 99/1000 | Loss: 0.00002456
Iteration 100/1000 | Loss: 0.00002456
Iteration 101/1000 | Loss: 0.00002456
Iteration 102/1000 | Loss: 0.00002455
Iteration 103/1000 | Loss: 0.00002455
Iteration 104/1000 | Loss: 0.00002455
Iteration 105/1000 | Loss: 0.00002454
Iteration 106/1000 | Loss: 0.00002454
Iteration 107/1000 | Loss: 0.00002454
Iteration 108/1000 | Loss: 0.00002454
Iteration 109/1000 | Loss: 0.00002454
Iteration 110/1000 | Loss: 0.00002453
Iteration 111/1000 | Loss: 0.00002453
Iteration 112/1000 | Loss: 0.00002453
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002452
Iteration 116/1000 | Loss: 0.00002452
Iteration 117/1000 | Loss: 0.00002452
Iteration 118/1000 | Loss: 0.00002452
Iteration 119/1000 | Loss: 0.00002452
Iteration 120/1000 | Loss: 0.00002452
Iteration 121/1000 | Loss: 0.00002452
Iteration 122/1000 | Loss: 0.00002452
Iteration 123/1000 | Loss: 0.00002452
Iteration 124/1000 | Loss: 0.00002452
Iteration 125/1000 | Loss: 0.00002452
Iteration 126/1000 | Loss: 0.00002452
Iteration 127/1000 | Loss: 0.00002452
Iteration 128/1000 | Loss: 0.00002452
Iteration 129/1000 | Loss: 0.00002452
Iteration 130/1000 | Loss: 0.00002452
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002452
Iteration 141/1000 | Loss: 0.00002452
Iteration 142/1000 | Loss: 0.00002452
Iteration 143/1000 | Loss: 0.00002452
Iteration 144/1000 | Loss: 0.00002452
Iteration 145/1000 | Loss: 0.00002452
Iteration 146/1000 | Loss: 0.00002452
Iteration 147/1000 | Loss: 0.00002452
Iteration 148/1000 | Loss: 0.00002452
Iteration 149/1000 | Loss: 0.00002452
Iteration 150/1000 | Loss: 0.00002452
Iteration 151/1000 | Loss: 0.00002452
Iteration 152/1000 | Loss: 0.00002452
Iteration 153/1000 | Loss: 0.00002452
Iteration 154/1000 | Loss: 0.00002452
Iteration 155/1000 | Loss: 0.00002452
Iteration 156/1000 | Loss: 0.00002452
Iteration 157/1000 | Loss: 0.00002452
Iteration 158/1000 | Loss: 0.00002452
Iteration 159/1000 | Loss: 0.00002452
Iteration 160/1000 | Loss: 0.00002452
Iteration 161/1000 | Loss: 0.00002452
Iteration 162/1000 | Loss: 0.00002452
Iteration 163/1000 | Loss: 0.00002452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.45234423346119e-05, 2.45234423346119e-05, 2.45234423346119e-05, 2.45234423346119e-05, 2.45234423346119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.45234423346119e-05

Optimization complete. Final v2v error: 4.124093532562256 mm

Highest mean error: 5.50620174407959 mm for frame 76

Lowest mean error: 3.33914852142334 mm for frame 124

Saving results

Total time: 49.90266251564026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889843
Iteration 2/25 | Loss: 0.00130932
Iteration 3/25 | Loss: 0.00119847
Iteration 4/25 | Loss: 0.00118075
Iteration 5/25 | Loss: 0.00117667
Iteration 6/25 | Loss: 0.00117549
Iteration 7/25 | Loss: 0.00117544
Iteration 8/25 | Loss: 0.00117544
Iteration 9/25 | Loss: 0.00117544
Iteration 10/25 | Loss: 0.00117544
Iteration 11/25 | Loss: 0.00117544
Iteration 12/25 | Loss: 0.00117544
Iteration 13/25 | Loss: 0.00117544
Iteration 14/25 | Loss: 0.00117544
Iteration 15/25 | Loss: 0.00117544
Iteration 16/25 | Loss: 0.00117544
Iteration 17/25 | Loss: 0.00117544
Iteration 18/25 | Loss: 0.00117544
Iteration 19/25 | Loss: 0.00117544
Iteration 20/25 | Loss: 0.00117544
Iteration 21/25 | Loss: 0.00117544
Iteration 22/25 | Loss: 0.00117544
Iteration 23/25 | Loss: 0.00117544
Iteration 24/25 | Loss: 0.00117544
Iteration 25/25 | Loss: 0.00117544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23568726
Iteration 2/25 | Loss: 0.00129315
Iteration 3/25 | Loss: 0.00129309
Iteration 4/25 | Loss: 0.00129309
Iteration 5/25 | Loss: 0.00129309
Iteration 6/25 | Loss: 0.00129309
Iteration 7/25 | Loss: 0.00129309
Iteration 8/25 | Loss: 0.00129309
Iteration 9/25 | Loss: 0.00129309
Iteration 10/25 | Loss: 0.00129309
Iteration 11/25 | Loss: 0.00129309
Iteration 12/25 | Loss: 0.00129309
Iteration 13/25 | Loss: 0.00129309
Iteration 14/25 | Loss: 0.00129309
Iteration 15/25 | Loss: 0.00129309
Iteration 16/25 | Loss: 0.00129309
Iteration 17/25 | Loss: 0.00129309
Iteration 18/25 | Loss: 0.00129309
Iteration 19/25 | Loss: 0.00129309
Iteration 20/25 | Loss: 0.00129309
Iteration 21/25 | Loss: 0.00129309
Iteration 22/25 | Loss: 0.00129309
Iteration 23/25 | Loss: 0.00129309
Iteration 24/25 | Loss: 0.00129309
Iteration 25/25 | Loss: 0.00129309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129309
Iteration 2/1000 | Loss: 0.00006818
Iteration 3/1000 | Loss: 0.00004167
Iteration 4/1000 | Loss: 0.00002756
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00001765
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001543
Iteration 9/1000 | Loss: 0.00001470
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001385
Iteration 12/1000 | Loss: 0.00001358
Iteration 13/1000 | Loss: 0.00001339
Iteration 14/1000 | Loss: 0.00001324
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001314
Iteration 21/1000 | Loss: 0.00001313
Iteration 22/1000 | Loss: 0.00001313
Iteration 23/1000 | Loss: 0.00001312
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001311
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001310
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001309
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001309
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001307
Iteration 42/1000 | Loss: 0.00001307
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001307
Iteration 45/1000 | Loss: 0.00001307
Iteration 46/1000 | Loss: 0.00001307
Iteration 47/1000 | Loss: 0.00001307
Iteration 48/1000 | Loss: 0.00001307
Iteration 49/1000 | Loss: 0.00001307
Iteration 50/1000 | Loss: 0.00001307
Iteration 51/1000 | Loss: 0.00001307
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001306
Iteration 58/1000 | Loss: 0.00001306
Iteration 59/1000 | Loss: 0.00001306
Iteration 60/1000 | Loss: 0.00001305
Iteration 61/1000 | Loss: 0.00001305
Iteration 62/1000 | Loss: 0.00001305
Iteration 63/1000 | Loss: 0.00001305
Iteration 64/1000 | Loss: 0.00001305
Iteration 65/1000 | Loss: 0.00001305
Iteration 66/1000 | Loss: 0.00001305
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001305
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001305
Iteration 72/1000 | Loss: 0.00001305
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001304
Iteration 85/1000 | Loss: 0.00001304
Iteration 86/1000 | Loss: 0.00001304
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001304
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001303
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001303
Iteration 102/1000 | Loss: 0.00001303
Iteration 103/1000 | Loss: 0.00001303
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001302
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Iteration 111/1000 | Loss: 0.00001302
Iteration 112/1000 | Loss: 0.00001302
Iteration 113/1000 | Loss: 0.00001302
Iteration 114/1000 | Loss: 0.00001302
Iteration 115/1000 | Loss: 0.00001302
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001301
Iteration 120/1000 | Loss: 0.00001301
Iteration 121/1000 | Loss: 0.00001301
Iteration 122/1000 | Loss: 0.00001301
Iteration 123/1000 | Loss: 0.00001301
Iteration 124/1000 | Loss: 0.00001301
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001300
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001299
Iteration 139/1000 | Loss: 0.00001299
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001298
Iteration 147/1000 | Loss: 0.00001298
Iteration 148/1000 | Loss: 0.00001298
Iteration 149/1000 | Loss: 0.00001298
Iteration 150/1000 | Loss: 0.00001297
Iteration 151/1000 | Loss: 0.00001297
Iteration 152/1000 | Loss: 0.00001297
Iteration 153/1000 | Loss: 0.00001297
Iteration 154/1000 | Loss: 0.00001297
Iteration 155/1000 | Loss: 0.00001297
Iteration 156/1000 | Loss: 0.00001297
Iteration 157/1000 | Loss: 0.00001297
Iteration 158/1000 | Loss: 0.00001297
Iteration 159/1000 | Loss: 0.00001297
Iteration 160/1000 | Loss: 0.00001296
Iteration 161/1000 | Loss: 0.00001296
Iteration 162/1000 | Loss: 0.00001296
Iteration 163/1000 | Loss: 0.00001296
Iteration 164/1000 | Loss: 0.00001296
Iteration 165/1000 | Loss: 0.00001296
Iteration 166/1000 | Loss: 0.00001296
Iteration 167/1000 | Loss: 0.00001296
Iteration 168/1000 | Loss: 0.00001296
Iteration 169/1000 | Loss: 0.00001296
Iteration 170/1000 | Loss: 0.00001296
Iteration 171/1000 | Loss: 0.00001295
Iteration 172/1000 | Loss: 0.00001295
Iteration 173/1000 | Loss: 0.00001295
Iteration 174/1000 | Loss: 0.00001295
Iteration 175/1000 | Loss: 0.00001295
Iteration 176/1000 | Loss: 0.00001295
Iteration 177/1000 | Loss: 0.00001295
Iteration 178/1000 | Loss: 0.00001295
Iteration 179/1000 | Loss: 0.00001295
Iteration 180/1000 | Loss: 0.00001295
Iteration 181/1000 | Loss: 0.00001295
Iteration 182/1000 | Loss: 0.00001295
Iteration 183/1000 | Loss: 0.00001295
Iteration 184/1000 | Loss: 0.00001295
Iteration 185/1000 | Loss: 0.00001295
Iteration 186/1000 | Loss: 0.00001295
Iteration 187/1000 | Loss: 0.00001295
Iteration 188/1000 | Loss: 0.00001295
Iteration 189/1000 | Loss: 0.00001295
Iteration 190/1000 | Loss: 0.00001294
Iteration 191/1000 | Loss: 0.00001294
Iteration 192/1000 | Loss: 0.00001294
Iteration 193/1000 | Loss: 0.00001294
Iteration 194/1000 | Loss: 0.00001294
Iteration 195/1000 | Loss: 0.00001294
Iteration 196/1000 | Loss: 0.00001294
Iteration 197/1000 | Loss: 0.00001294
Iteration 198/1000 | Loss: 0.00001294
Iteration 199/1000 | Loss: 0.00001294
Iteration 200/1000 | Loss: 0.00001294
Iteration 201/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.2944832633365877e-05, 1.2944832633365877e-05, 1.2944832633365877e-05, 1.2944832633365877e-05, 1.2944832633365877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2944832633365877e-05

Optimization complete. Final v2v error: 3.1354856491088867 mm

Highest mean error: 3.639120101928711 mm for frame 0

Lowest mean error: 2.92191743850708 mm for frame 121

Saving results

Total time: 40.75760102272034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453851
Iteration 2/25 | Loss: 0.00130139
Iteration 3/25 | Loss: 0.00116633
Iteration 4/25 | Loss: 0.00115633
Iteration 5/25 | Loss: 0.00115324
Iteration 6/25 | Loss: 0.00115274
Iteration 7/25 | Loss: 0.00115274
Iteration 8/25 | Loss: 0.00115274
Iteration 9/25 | Loss: 0.00115274
Iteration 10/25 | Loss: 0.00115274
Iteration 11/25 | Loss: 0.00115274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011527371825650334, 0.0011527371825650334, 0.0011527371825650334, 0.0011527371825650334, 0.0011527371825650334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011527371825650334

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23444271
Iteration 2/25 | Loss: 0.00156589
Iteration 3/25 | Loss: 0.00156589
Iteration 4/25 | Loss: 0.00156589
Iteration 5/25 | Loss: 0.00156589
Iteration 6/25 | Loss: 0.00156589
Iteration 7/25 | Loss: 0.00156589
Iteration 8/25 | Loss: 0.00156589
Iteration 9/25 | Loss: 0.00156589
Iteration 10/25 | Loss: 0.00156589
Iteration 11/25 | Loss: 0.00156589
Iteration 12/25 | Loss: 0.00156589
Iteration 13/25 | Loss: 0.00156589
Iteration 14/25 | Loss: 0.00156589
Iteration 15/25 | Loss: 0.00156589
Iteration 16/25 | Loss: 0.00156589
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015658903867006302, 0.0015658903867006302, 0.0015658903867006302, 0.0015658903867006302, 0.0015658903867006302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015658903867006302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156589
Iteration 2/1000 | Loss: 0.00004189
Iteration 3/1000 | Loss: 0.00002272
Iteration 4/1000 | Loss: 0.00001904
Iteration 5/1000 | Loss: 0.00001715
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001588
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001477
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001459
Iteration 16/1000 | Loss: 0.00001459
Iteration 17/1000 | Loss: 0.00001459
Iteration 18/1000 | Loss: 0.00001458
Iteration 19/1000 | Loss: 0.00001458
Iteration 20/1000 | Loss: 0.00001457
Iteration 21/1000 | Loss: 0.00001456
Iteration 22/1000 | Loss: 0.00001455
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001455
Iteration 25/1000 | Loss: 0.00001455
Iteration 26/1000 | Loss: 0.00001455
Iteration 27/1000 | Loss: 0.00001454
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001452
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001451
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001449
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001449
Iteration 72/1000 | Loss: 0.00001449
Iteration 73/1000 | Loss: 0.00001449
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.4487358384940308e-05, 1.4487358384940308e-05, 1.4487358384940308e-05, 1.4487358384940308e-05, 1.4487358384940308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4487358384940308e-05

Optimization complete. Final v2v error: 3.281919002532959 mm

Highest mean error: 3.6526312828063965 mm for frame 76

Lowest mean error: 3.0897722244262695 mm for frame 161

Saving results

Total time: 30.475801706314087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948607
Iteration 2/25 | Loss: 0.00186858
Iteration 3/25 | Loss: 0.00143645
Iteration 4/25 | Loss: 0.00135806
Iteration 5/25 | Loss: 0.00130159
Iteration 6/25 | Loss: 0.00126240
Iteration 7/25 | Loss: 0.00125361
Iteration 8/25 | Loss: 0.00124805
Iteration 9/25 | Loss: 0.00124633
Iteration 10/25 | Loss: 0.00124553
Iteration 11/25 | Loss: 0.00124588
Iteration 12/25 | Loss: 0.00124329
Iteration 13/25 | Loss: 0.00124238
Iteration 14/25 | Loss: 0.00124204
Iteration 15/25 | Loss: 0.00124194
Iteration 16/25 | Loss: 0.00124192
Iteration 17/25 | Loss: 0.00124192
Iteration 18/25 | Loss: 0.00124191
Iteration 19/25 | Loss: 0.00124190
Iteration 20/25 | Loss: 0.00124189
Iteration 21/25 | Loss: 0.00124189
Iteration 22/25 | Loss: 0.00124189
Iteration 23/25 | Loss: 0.00124189
Iteration 24/25 | Loss: 0.00124189
Iteration 25/25 | Loss: 0.00124189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43385422
Iteration 2/25 | Loss: 0.00128657
Iteration 3/25 | Loss: 0.00128657
Iteration 4/25 | Loss: 0.00128657
Iteration 5/25 | Loss: 0.00128657
Iteration 6/25 | Loss: 0.00128657
Iteration 7/25 | Loss: 0.00128657
Iteration 8/25 | Loss: 0.00128657
Iteration 9/25 | Loss: 0.00128657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.001286570681259036, 0.001286570681259036, 0.001286570681259036, 0.001286570681259036, 0.001286570681259036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001286570681259036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128657
Iteration 2/1000 | Loss: 0.00006814
Iteration 3/1000 | Loss: 0.00004524
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00002750
Iteration 6/1000 | Loss: 0.00002554
Iteration 7/1000 | Loss: 0.00002448
Iteration 8/1000 | Loss: 0.00002378
Iteration 9/1000 | Loss: 0.00002322
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002259
Iteration 12/1000 | Loss: 0.00002230
Iteration 13/1000 | Loss: 0.00002216
Iteration 14/1000 | Loss: 0.00002207
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002200
Iteration 17/1000 | Loss: 0.00002198
Iteration 18/1000 | Loss: 0.00002197
Iteration 19/1000 | Loss: 0.00002197
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002196
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002194
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002193
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002191
Iteration 28/1000 | Loss: 0.00002191
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002190
Iteration 31/1000 | Loss: 0.00002190
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002189
Iteration 35/1000 | Loss: 0.00002189
Iteration 36/1000 | Loss: 0.00002189
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002188
Iteration 39/1000 | Loss: 0.00002188
Iteration 40/1000 | Loss: 0.00002188
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002187
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002187
Iteration 45/1000 | Loss: 0.00002187
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002187
Iteration 49/1000 | Loss: 0.00002187
Iteration 50/1000 | Loss: 0.00002187
Iteration 51/1000 | Loss: 0.00002187
Iteration 52/1000 | Loss: 0.00002186
Iteration 53/1000 | Loss: 0.00002186
Iteration 54/1000 | Loss: 0.00002186
Iteration 55/1000 | Loss: 0.00002186
Iteration 56/1000 | Loss: 0.00002186
Iteration 57/1000 | Loss: 0.00002186
Iteration 58/1000 | Loss: 0.00002186
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00002186
Iteration 61/1000 | Loss: 0.00002185
Iteration 62/1000 | Loss: 0.00002185
Iteration 63/1000 | Loss: 0.00002184
Iteration 64/1000 | Loss: 0.00002184
Iteration 65/1000 | Loss: 0.00002184
Iteration 66/1000 | Loss: 0.00002183
Iteration 67/1000 | Loss: 0.00002182
Iteration 68/1000 | Loss: 0.00002182
Iteration 69/1000 | Loss: 0.00002182
Iteration 70/1000 | Loss: 0.00002182
Iteration 71/1000 | Loss: 0.00002182
Iteration 72/1000 | Loss: 0.00002182
Iteration 73/1000 | Loss: 0.00002181
Iteration 74/1000 | Loss: 0.00002181
Iteration 75/1000 | Loss: 0.00002181
Iteration 76/1000 | Loss: 0.00002181
Iteration 77/1000 | Loss: 0.00002181
Iteration 78/1000 | Loss: 0.00002181
Iteration 79/1000 | Loss: 0.00002180
Iteration 80/1000 | Loss: 0.00002180
Iteration 81/1000 | Loss: 0.00002179
Iteration 82/1000 | Loss: 0.00002179
Iteration 83/1000 | Loss: 0.00002179
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002178
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002177
Iteration 89/1000 | Loss: 0.00002177
Iteration 90/1000 | Loss: 0.00002177
Iteration 91/1000 | Loss: 0.00002177
Iteration 92/1000 | Loss: 0.00002176
Iteration 93/1000 | Loss: 0.00002176
Iteration 94/1000 | Loss: 0.00002176
Iteration 95/1000 | Loss: 0.00002176
Iteration 96/1000 | Loss: 0.00002176
Iteration 97/1000 | Loss: 0.00002176
Iteration 98/1000 | Loss: 0.00002176
Iteration 99/1000 | Loss: 0.00002176
Iteration 100/1000 | Loss: 0.00002175
Iteration 101/1000 | Loss: 0.00002175
Iteration 102/1000 | Loss: 0.00002175
Iteration 103/1000 | Loss: 0.00002175
Iteration 104/1000 | Loss: 0.00002174
Iteration 105/1000 | Loss: 0.00002174
Iteration 106/1000 | Loss: 0.00002174
Iteration 107/1000 | Loss: 0.00002174
Iteration 108/1000 | Loss: 0.00002174
Iteration 109/1000 | Loss: 0.00002174
Iteration 110/1000 | Loss: 0.00002174
Iteration 111/1000 | Loss: 0.00002174
Iteration 112/1000 | Loss: 0.00002173
Iteration 113/1000 | Loss: 0.00002173
Iteration 114/1000 | Loss: 0.00002173
Iteration 115/1000 | Loss: 0.00002173
Iteration 116/1000 | Loss: 0.00002173
Iteration 117/1000 | Loss: 0.00002173
Iteration 118/1000 | Loss: 0.00002172
Iteration 119/1000 | Loss: 0.00002172
Iteration 120/1000 | Loss: 0.00002172
Iteration 121/1000 | Loss: 0.00002172
Iteration 122/1000 | Loss: 0.00002172
Iteration 123/1000 | Loss: 0.00002172
Iteration 124/1000 | Loss: 0.00002172
Iteration 125/1000 | Loss: 0.00002172
Iteration 126/1000 | Loss: 0.00002171
Iteration 127/1000 | Loss: 0.00002171
Iteration 128/1000 | Loss: 0.00002171
Iteration 129/1000 | Loss: 0.00002171
Iteration 130/1000 | Loss: 0.00002171
Iteration 131/1000 | Loss: 0.00002171
Iteration 132/1000 | Loss: 0.00002171
Iteration 133/1000 | Loss: 0.00002170
Iteration 134/1000 | Loss: 0.00002170
Iteration 135/1000 | Loss: 0.00002170
Iteration 136/1000 | Loss: 0.00002170
Iteration 137/1000 | Loss: 0.00002170
Iteration 138/1000 | Loss: 0.00002170
Iteration 139/1000 | Loss: 0.00002170
Iteration 140/1000 | Loss: 0.00002170
Iteration 141/1000 | Loss: 0.00002170
Iteration 142/1000 | Loss: 0.00002170
Iteration 143/1000 | Loss: 0.00002170
Iteration 144/1000 | Loss: 0.00002170
Iteration 145/1000 | Loss: 0.00002170
Iteration 146/1000 | Loss: 0.00002169
Iteration 147/1000 | Loss: 0.00002169
Iteration 148/1000 | Loss: 0.00002169
Iteration 149/1000 | Loss: 0.00002169
Iteration 150/1000 | Loss: 0.00002169
Iteration 151/1000 | Loss: 0.00002169
Iteration 152/1000 | Loss: 0.00002169
Iteration 153/1000 | Loss: 0.00002169
Iteration 154/1000 | Loss: 0.00002169
Iteration 155/1000 | Loss: 0.00002169
Iteration 156/1000 | Loss: 0.00002169
Iteration 157/1000 | Loss: 0.00002168
Iteration 158/1000 | Loss: 0.00002168
Iteration 159/1000 | Loss: 0.00002168
Iteration 160/1000 | Loss: 0.00002168
Iteration 161/1000 | Loss: 0.00002168
Iteration 162/1000 | Loss: 0.00002168
Iteration 163/1000 | Loss: 0.00002168
Iteration 164/1000 | Loss: 0.00002168
Iteration 165/1000 | Loss: 0.00002168
Iteration 166/1000 | Loss: 0.00002168
Iteration 167/1000 | Loss: 0.00002168
Iteration 168/1000 | Loss: 0.00002168
Iteration 169/1000 | Loss: 0.00002168
Iteration 170/1000 | Loss: 0.00002168
Iteration 171/1000 | Loss: 0.00002168
Iteration 172/1000 | Loss: 0.00002168
Iteration 173/1000 | Loss: 0.00002168
Iteration 174/1000 | Loss: 0.00002168
Iteration 175/1000 | Loss: 0.00002168
Iteration 176/1000 | Loss: 0.00002168
Iteration 177/1000 | Loss: 0.00002168
Iteration 178/1000 | Loss: 0.00002168
Iteration 179/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.1678371922462247e-05, 2.1678371922462247e-05, 2.1678371922462247e-05, 2.1678371922462247e-05, 2.1678371922462247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1678371922462247e-05

Optimization complete. Final v2v error: 3.8387162685394287 mm

Highest mean error: 5.210115909576416 mm for frame 91

Lowest mean error: 2.9121477603912354 mm for frame 3

Saving results

Total time: 55.099940061569214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094453
Iteration 2/25 | Loss: 0.01094453
Iteration 3/25 | Loss: 0.01094452
Iteration 4/25 | Loss: 0.01094452
Iteration 5/25 | Loss: 0.01094452
Iteration 6/25 | Loss: 0.01094452
Iteration 7/25 | Loss: 0.01094452
Iteration 8/25 | Loss: 0.01094451
Iteration 9/25 | Loss: 0.01094451
Iteration 10/25 | Loss: 0.01094451
Iteration 11/25 | Loss: 0.01094450
Iteration 12/25 | Loss: 0.01094450
Iteration 13/25 | Loss: 0.01094450
Iteration 14/25 | Loss: 0.01094450
Iteration 15/25 | Loss: 0.01094449
Iteration 16/25 | Loss: 0.01094449
Iteration 17/25 | Loss: 0.01094449
Iteration 18/25 | Loss: 0.01094448
Iteration 19/25 | Loss: 0.01094448
Iteration 20/25 | Loss: 0.01094448
Iteration 21/25 | Loss: 0.01094448
Iteration 22/25 | Loss: 0.01094448
Iteration 23/25 | Loss: 0.01094448
Iteration 24/25 | Loss: 0.01094448
Iteration 25/25 | Loss: 0.01094447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01589394
Iteration 2/25 | Loss: 0.07279804
Iteration 3/25 | Loss: 0.07237783
Iteration 4/25 | Loss: 0.07220668
Iteration 5/25 | Loss: 0.07220665
Iteration 6/25 | Loss: 0.07220664
Iteration 7/25 | Loss: 0.07220664
Iteration 8/25 | Loss: 0.07220664
Iteration 9/25 | Loss: 0.07220662
Iteration 10/25 | Loss: 0.07220662
Iteration 11/25 | Loss: 0.07220662
Iteration 12/25 | Loss: 0.07220662
Iteration 13/25 | Loss: 0.07220662
Iteration 14/25 | Loss: 0.07220662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.07220662385225296, 0.07220662385225296, 0.07220662385225296, 0.07220662385225296, 0.07220662385225296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07220662385225296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07220662
Iteration 2/1000 | Loss: 0.00332894
Iteration 3/1000 | Loss: 0.00140631
Iteration 4/1000 | Loss: 0.00085523
Iteration 5/1000 | Loss: 0.00050949
Iteration 6/1000 | Loss: 0.00054825
Iteration 7/1000 | Loss: 0.00032539
Iteration 8/1000 | Loss: 0.00064812
Iteration 9/1000 | Loss: 0.00125541
Iteration 10/1000 | Loss: 0.00085987
Iteration 11/1000 | Loss: 0.00105051
Iteration 12/1000 | Loss: 0.00016881
Iteration 13/1000 | Loss: 0.00014878
Iteration 14/1000 | Loss: 0.00053668
Iteration 15/1000 | Loss: 0.00015694
Iteration 16/1000 | Loss: 0.00013305
Iteration 17/1000 | Loss: 0.00011264
Iteration 18/1000 | Loss: 0.00010455
Iteration 19/1000 | Loss: 0.00009830
Iteration 20/1000 | Loss: 0.00009141
Iteration 21/1000 | Loss: 0.00008612
Iteration 22/1000 | Loss: 0.00008255
Iteration 23/1000 | Loss: 0.00009755
Iteration 24/1000 | Loss: 0.00008472
Iteration 25/1000 | Loss: 0.00008294
Iteration 26/1000 | Loss: 0.00025403
Iteration 27/1000 | Loss: 0.00027114
Iteration 28/1000 | Loss: 0.00007704
Iteration 29/1000 | Loss: 0.00007444
Iteration 30/1000 | Loss: 0.00007293
Iteration 31/1000 | Loss: 0.00007174
Iteration 32/1000 | Loss: 0.00021538
Iteration 33/1000 | Loss: 0.00008274
Iteration 34/1000 | Loss: 0.00006928
Iteration 35/1000 | Loss: 0.00006789
Iteration 36/1000 | Loss: 0.00007566
Iteration 37/1000 | Loss: 0.00006780
Iteration 38/1000 | Loss: 0.00006541
Iteration 39/1000 | Loss: 0.00006410
Iteration 40/1000 | Loss: 0.00010632
Iteration 41/1000 | Loss: 0.00011527
Iteration 42/1000 | Loss: 0.00012626
Iteration 43/1000 | Loss: 0.00015224
Iteration 44/1000 | Loss: 0.00014042
Iteration 45/1000 | Loss: 0.00014661
Iteration 46/1000 | Loss: 0.00009409
Iteration 47/1000 | Loss: 0.00012495
Iteration 48/1000 | Loss: 0.00008318
Iteration 49/1000 | Loss: 0.00009154
Iteration 50/1000 | Loss: 0.00016074
Iteration 51/1000 | Loss: 0.00008801
Iteration 52/1000 | Loss: 0.00008454
Iteration 53/1000 | Loss: 0.00006720
Iteration 54/1000 | Loss: 0.00006400
Iteration 55/1000 | Loss: 0.00006217
Iteration 56/1000 | Loss: 0.00006840
Iteration 57/1000 | Loss: 0.00006952
Iteration 58/1000 | Loss: 0.00006765
Iteration 59/1000 | Loss: 0.00005934
Iteration 60/1000 | Loss: 0.00006497
Iteration 61/1000 | Loss: 0.00006594
Iteration 62/1000 | Loss: 0.00006474
Iteration 63/1000 | Loss: 0.00006770
Iteration 64/1000 | Loss: 0.00005817
Iteration 65/1000 | Loss: 0.00009817
Iteration 66/1000 | Loss: 0.00008146
Iteration 67/1000 | Loss: 0.00008671
Iteration 68/1000 | Loss: 0.00009161
Iteration 69/1000 | Loss: 0.00009685
Iteration 70/1000 | Loss: 0.00053863
Iteration 71/1000 | Loss: 0.00036630
Iteration 72/1000 | Loss: 0.00017253
Iteration 73/1000 | Loss: 0.00079509
Iteration 74/1000 | Loss: 0.00007508
Iteration 75/1000 | Loss: 0.00008161
Iteration 76/1000 | Loss: 0.00005895
Iteration 77/1000 | Loss: 0.00006582
Iteration 78/1000 | Loss: 0.00005503
Iteration 79/1000 | Loss: 0.00005469
Iteration 80/1000 | Loss: 0.00005426
Iteration 81/1000 | Loss: 0.00006738
Iteration 82/1000 | Loss: 0.00005268
Iteration 83/1000 | Loss: 0.00005330
Iteration 84/1000 | Loss: 0.00005485
Iteration 85/1000 | Loss: 0.00005256
Iteration 86/1000 | Loss: 0.00005156
Iteration 87/1000 | Loss: 0.00005150
Iteration 88/1000 | Loss: 0.00005580
Iteration 89/1000 | Loss: 0.00005478
Iteration 90/1000 | Loss: 0.00005116
Iteration 91/1000 | Loss: 0.00005111
Iteration 92/1000 | Loss: 0.00005186
Iteration 93/1000 | Loss: 0.00005381
Iteration 94/1000 | Loss: 0.00005095
Iteration 95/1000 | Loss: 0.00005095
Iteration 96/1000 | Loss: 0.00005095
Iteration 97/1000 | Loss: 0.00005095
Iteration 98/1000 | Loss: 0.00005095
Iteration 99/1000 | Loss: 0.00005095
Iteration 100/1000 | Loss: 0.00005095
Iteration 101/1000 | Loss: 0.00005095
Iteration 102/1000 | Loss: 0.00005095
Iteration 103/1000 | Loss: 0.00005095
Iteration 104/1000 | Loss: 0.00005095
Iteration 105/1000 | Loss: 0.00005095
Iteration 106/1000 | Loss: 0.00005094
Iteration 107/1000 | Loss: 0.00005094
Iteration 108/1000 | Loss: 0.00005094
Iteration 109/1000 | Loss: 0.00005093
Iteration 110/1000 | Loss: 0.00005092
Iteration 111/1000 | Loss: 0.00005091
Iteration 112/1000 | Loss: 0.00005091
Iteration 113/1000 | Loss: 0.00005090
Iteration 114/1000 | Loss: 0.00005089
Iteration 115/1000 | Loss: 0.00005082
Iteration 116/1000 | Loss: 0.00005082
Iteration 117/1000 | Loss: 0.00005280
Iteration 118/1000 | Loss: 0.00005079
Iteration 119/1000 | Loss: 0.00005079
Iteration 120/1000 | Loss: 0.00005079
Iteration 121/1000 | Loss: 0.00005079
Iteration 122/1000 | Loss: 0.00005079
Iteration 123/1000 | Loss: 0.00005079
Iteration 124/1000 | Loss: 0.00005079
Iteration 125/1000 | Loss: 0.00005079
Iteration 126/1000 | Loss: 0.00005079
Iteration 127/1000 | Loss: 0.00005079
Iteration 128/1000 | Loss: 0.00005079
Iteration 129/1000 | Loss: 0.00005079
Iteration 130/1000 | Loss: 0.00005079
Iteration 131/1000 | Loss: 0.00005078
Iteration 132/1000 | Loss: 0.00005078
Iteration 133/1000 | Loss: 0.00005434
Iteration 134/1000 | Loss: 0.00005073
Iteration 135/1000 | Loss: 0.00005071
Iteration 136/1000 | Loss: 0.00005252
Iteration 137/1000 | Loss: 0.00005071
Iteration 138/1000 | Loss: 0.00005071
Iteration 139/1000 | Loss: 0.00005070
Iteration 140/1000 | Loss: 0.00005070
Iteration 141/1000 | Loss: 0.00005100
Iteration 142/1000 | Loss: 0.00005117
Iteration 143/1000 | Loss: 0.00005117
Iteration 144/1000 | Loss: 0.00005062
Iteration 145/1000 | Loss: 0.00005061
Iteration 146/1000 | Loss: 0.00005059
Iteration 147/1000 | Loss: 0.00005059
Iteration 148/1000 | Loss: 0.00005059
Iteration 149/1000 | Loss: 0.00005059
Iteration 150/1000 | Loss: 0.00005059
Iteration 151/1000 | Loss: 0.00005059
Iteration 152/1000 | Loss: 0.00005059
Iteration 153/1000 | Loss: 0.00005059
Iteration 154/1000 | Loss: 0.00005059
Iteration 155/1000 | Loss: 0.00005059
Iteration 156/1000 | Loss: 0.00005059
Iteration 157/1000 | Loss: 0.00005059
Iteration 158/1000 | Loss: 0.00005058
Iteration 159/1000 | Loss: 0.00005058
Iteration 160/1000 | Loss: 0.00005058
Iteration 161/1000 | Loss: 0.00005058
Iteration 162/1000 | Loss: 0.00005058
Iteration 163/1000 | Loss: 0.00005058
Iteration 164/1000 | Loss: 0.00005058
Iteration 165/1000 | Loss: 0.00005058
Iteration 166/1000 | Loss: 0.00005058
Iteration 167/1000 | Loss: 0.00005057
Iteration 168/1000 | Loss: 0.00005057
Iteration 169/1000 | Loss: 0.00005057
Iteration 170/1000 | Loss: 0.00005057
Iteration 171/1000 | Loss: 0.00005057
Iteration 172/1000 | Loss: 0.00005057
Iteration 173/1000 | Loss: 0.00005175
Iteration 174/1000 | Loss: 0.00005051
Iteration 175/1000 | Loss: 0.00005050
Iteration 176/1000 | Loss: 0.00005049
Iteration 177/1000 | Loss: 0.00005049
Iteration 178/1000 | Loss: 0.00005049
Iteration 179/1000 | Loss: 0.00005049
Iteration 180/1000 | Loss: 0.00005048
Iteration 181/1000 | Loss: 0.00005048
Iteration 182/1000 | Loss: 0.00005048
Iteration 183/1000 | Loss: 0.00005048
Iteration 184/1000 | Loss: 0.00005048
Iteration 185/1000 | Loss: 0.00005048
Iteration 186/1000 | Loss: 0.00005048
Iteration 187/1000 | Loss: 0.00005048
Iteration 188/1000 | Loss: 0.00005048
Iteration 189/1000 | Loss: 0.00005048
Iteration 190/1000 | Loss: 0.00005048
Iteration 191/1000 | Loss: 0.00005048
Iteration 192/1000 | Loss: 0.00005048
Iteration 193/1000 | Loss: 0.00005048
Iteration 194/1000 | Loss: 0.00005048
Iteration 195/1000 | Loss: 0.00005048
Iteration 196/1000 | Loss: 0.00005047
Iteration 197/1000 | Loss: 0.00008141
Iteration 198/1000 | Loss: 0.00005301
Iteration 199/1000 | Loss: 0.00009840
Iteration 200/1000 | Loss: 0.00005159
Iteration 201/1000 | Loss: 0.00005088
Iteration 202/1000 | Loss: 0.00006151
Iteration 203/1000 | Loss: 0.00005053
Iteration 204/1000 | Loss: 0.00005842
Iteration 205/1000 | Loss: 0.00005022
Iteration 206/1000 | Loss: 0.00005019
Iteration 207/1000 | Loss: 0.00005018
Iteration 208/1000 | Loss: 0.00005092
Iteration 209/1000 | Loss: 0.00005016
Iteration 210/1000 | Loss: 0.00005016
Iteration 211/1000 | Loss: 0.00005016
Iteration 212/1000 | Loss: 0.00005016
Iteration 213/1000 | Loss: 0.00005016
Iteration 214/1000 | Loss: 0.00005015
Iteration 215/1000 | Loss: 0.00005015
Iteration 216/1000 | Loss: 0.00005015
Iteration 217/1000 | Loss: 0.00005015
Iteration 218/1000 | Loss: 0.00005015
Iteration 219/1000 | Loss: 0.00005025
Iteration 220/1000 | Loss: 0.00005065
Iteration 221/1000 | Loss: 0.00005358
Iteration 222/1000 | Loss: 0.00005199
Iteration 223/1000 | Loss: 0.00005007
Iteration 224/1000 | Loss: 0.00005002
Iteration 225/1000 | Loss: 0.00005001
Iteration 226/1000 | Loss: 0.00005001
Iteration 227/1000 | Loss: 0.00005000
Iteration 228/1000 | Loss: 0.00004998
Iteration 229/1000 | Loss: 0.00004998
Iteration 230/1000 | Loss: 0.00004998
Iteration 231/1000 | Loss: 0.00005487
Iteration 232/1000 | Loss: 0.00005000
Iteration 233/1000 | Loss: 0.00004996
Iteration 234/1000 | Loss: 0.00004996
Iteration 235/1000 | Loss: 0.00004996
Iteration 236/1000 | Loss: 0.00004995
Iteration 237/1000 | Loss: 0.00004995
Iteration 238/1000 | Loss: 0.00004995
Iteration 239/1000 | Loss: 0.00004995
Iteration 240/1000 | Loss: 0.00004995
Iteration 241/1000 | Loss: 0.00004995
Iteration 242/1000 | Loss: 0.00005287
Iteration 243/1000 | Loss: 0.00004994
Iteration 244/1000 | Loss: 0.00004992
Iteration 245/1000 | Loss: 0.00004992
Iteration 246/1000 | Loss: 0.00005033
Iteration 247/1000 | Loss: 0.00004990
Iteration 248/1000 | Loss: 0.00004990
Iteration 249/1000 | Loss: 0.00004990
Iteration 250/1000 | Loss: 0.00004990
Iteration 251/1000 | Loss: 0.00004990
Iteration 252/1000 | Loss: 0.00004990
Iteration 253/1000 | Loss: 0.00004990
Iteration 254/1000 | Loss: 0.00004990
Iteration 255/1000 | Loss: 0.00004990
Iteration 256/1000 | Loss: 0.00004990
Iteration 257/1000 | Loss: 0.00004989
Iteration 258/1000 | Loss: 0.00004989
Iteration 259/1000 | Loss: 0.00004989
Iteration 260/1000 | Loss: 0.00005355
Iteration 261/1000 | Loss: 0.00005593
Iteration 262/1000 | Loss: 0.00004962
Iteration 263/1000 | Loss: 0.00004962
Iteration 264/1000 | Loss: 0.00004960
Iteration 265/1000 | Loss: 0.00004959
Iteration 266/1000 | Loss: 0.00004959
Iteration 267/1000 | Loss: 0.00004959
Iteration 268/1000 | Loss: 0.00004958
Iteration 269/1000 | Loss: 0.00004958
Iteration 270/1000 | Loss: 0.00004958
Iteration 271/1000 | Loss: 0.00006747
Iteration 272/1000 | Loss: 0.00004953
Iteration 273/1000 | Loss: 0.00004952
Iteration 274/1000 | Loss: 0.00004951
Iteration 275/1000 | Loss: 0.00004951
Iteration 276/1000 | Loss: 0.00004951
Iteration 277/1000 | Loss: 0.00004951
Iteration 278/1000 | Loss: 0.00004951
Iteration 279/1000 | Loss: 0.00004951
Iteration 280/1000 | Loss: 0.00004951
Iteration 281/1000 | Loss: 0.00004951
Iteration 282/1000 | Loss: 0.00004951
Iteration 283/1000 | Loss: 0.00004951
Iteration 284/1000 | Loss: 0.00004951
Iteration 285/1000 | Loss: 0.00004950
Iteration 286/1000 | Loss: 0.00004950
Iteration 287/1000 | Loss: 0.00004950
Iteration 288/1000 | Loss: 0.00004950
Iteration 289/1000 | Loss: 0.00004950
Iteration 290/1000 | Loss: 0.00004950
Iteration 291/1000 | Loss: 0.00004950
Iteration 292/1000 | Loss: 0.00004950
Iteration 293/1000 | Loss: 0.00004949
Iteration 294/1000 | Loss: 0.00004948
Iteration 295/1000 | Loss: 0.00004947
Iteration 296/1000 | Loss: 0.00004946
Iteration 297/1000 | Loss: 0.00004946
Iteration 298/1000 | Loss: 0.00004946
Iteration 299/1000 | Loss: 0.00004945
Iteration 300/1000 | Loss: 0.00004945
Iteration 301/1000 | Loss: 0.00004945
Iteration 302/1000 | Loss: 0.00004944
Iteration 303/1000 | Loss: 0.00004944
Iteration 304/1000 | Loss: 0.00004943
Iteration 305/1000 | Loss: 0.00004943
Iteration 306/1000 | Loss: 0.00004943
Iteration 307/1000 | Loss: 0.00004943
Iteration 308/1000 | Loss: 0.00004942
Iteration 309/1000 | Loss: 0.00004942
Iteration 310/1000 | Loss: 0.00004942
Iteration 311/1000 | Loss: 0.00004941
Iteration 312/1000 | Loss: 0.00004941
Iteration 313/1000 | Loss: 0.00004941
Iteration 314/1000 | Loss: 0.00004941
Iteration 315/1000 | Loss: 0.00004941
Iteration 316/1000 | Loss: 0.00004941
Iteration 317/1000 | Loss: 0.00004941
Iteration 318/1000 | Loss: 0.00005386
Iteration 319/1000 | Loss: 0.00004953
Iteration 320/1000 | Loss: 0.00004939
Iteration 321/1000 | Loss: 0.00004938
Iteration 322/1000 | Loss: 0.00004938
Iteration 323/1000 | Loss: 0.00004938
Iteration 324/1000 | Loss: 0.00004938
Iteration 325/1000 | Loss: 0.00004938
Iteration 326/1000 | Loss: 0.00004938
Iteration 327/1000 | Loss: 0.00004938
Iteration 328/1000 | Loss: 0.00004938
Iteration 329/1000 | Loss: 0.00004938
Iteration 330/1000 | Loss: 0.00004938
Iteration 331/1000 | Loss: 0.00004938
Iteration 332/1000 | Loss: 0.00004938
Iteration 333/1000 | Loss: 0.00004938
Iteration 334/1000 | Loss: 0.00004938
Iteration 335/1000 | Loss: 0.00004938
Iteration 336/1000 | Loss: 0.00004938
Iteration 337/1000 | Loss: 0.00004938
Iteration 338/1000 | Loss: 0.00004938
Iteration 339/1000 | Loss: 0.00004938
Iteration 340/1000 | Loss: 0.00004938
Iteration 341/1000 | Loss: 0.00004938
Iteration 342/1000 | Loss: 0.00004938
Iteration 343/1000 | Loss: 0.00004938
Iteration 344/1000 | Loss: 0.00004938
Iteration 345/1000 | Loss: 0.00004938
Iteration 346/1000 | Loss: 0.00004938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 346. Stopping optimization.
Last 5 losses: [4.9378017138224095e-05, 4.9378017138224095e-05, 4.9378017138224095e-05, 4.9378017138224095e-05, 4.9378017138224095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9378017138224095e-05

Optimization complete. Final v2v error: 4.861194133758545 mm

Highest mean error: 18.42017936706543 mm for frame 208

Lowest mean error: 3.496473550796509 mm for frame 0

Saving results

Total time: 199.10459518432617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909679
Iteration 2/25 | Loss: 0.00224075
Iteration 3/25 | Loss: 0.00172219
Iteration 4/25 | Loss: 0.00155963
Iteration 5/25 | Loss: 0.00134633
Iteration 6/25 | Loss: 0.00126904
Iteration 7/25 | Loss: 0.00124729
Iteration 8/25 | Loss: 0.00123996
Iteration 9/25 | Loss: 0.00123746
Iteration 10/25 | Loss: 0.00123475
Iteration 11/25 | Loss: 0.00123405
Iteration 12/25 | Loss: 0.00123383
Iteration 13/25 | Loss: 0.00123247
Iteration 14/25 | Loss: 0.00123227
Iteration 15/25 | Loss: 0.00123215
Iteration 16/25 | Loss: 0.00123212
Iteration 17/25 | Loss: 0.00123211
Iteration 18/25 | Loss: 0.00123211
Iteration 19/25 | Loss: 0.00123211
Iteration 20/25 | Loss: 0.00123211
Iteration 21/25 | Loss: 0.00123211
Iteration 22/25 | Loss: 0.00123211
Iteration 23/25 | Loss: 0.00123211
Iteration 24/25 | Loss: 0.00123210
Iteration 25/25 | Loss: 0.00123210

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92999500
Iteration 2/25 | Loss: 0.00073185
Iteration 3/25 | Loss: 0.00073185
Iteration 4/25 | Loss: 0.00073185
Iteration 5/25 | Loss: 0.00073185
Iteration 6/25 | Loss: 0.00073185
Iteration 7/25 | Loss: 0.00073185
Iteration 8/25 | Loss: 0.00073185
Iteration 9/25 | Loss: 0.00073185
Iteration 10/25 | Loss: 0.00073185
Iteration 11/25 | Loss: 0.00073185
Iteration 12/25 | Loss: 0.00073185
Iteration 13/25 | Loss: 0.00073185
Iteration 14/25 | Loss: 0.00073185
Iteration 15/25 | Loss: 0.00073185
Iteration 16/25 | Loss: 0.00073185
Iteration 17/25 | Loss: 0.00073185
Iteration 18/25 | Loss: 0.00073185
Iteration 19/25 | Loss: 0.00073185
Iteration 20/25 | Loss: 0.00073185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007318473653867841, 0.0007318473653867841, 0.0007318473653867841, 0.0007318473653867841, 0.0007318473653867841]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007318473653867841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073185
Iteration 2/1000 | Loss: 0.00006495
Iteration 3/1000 | Loss: 0.00004127
Iteration 4/1000 | Loss: 0.00003061
Iteration 5/1000 | Loss: 0.00002855
Iteration 6/1000 | Loss: 0.00002712
Iteration 7/1000 | Loss: 0.00002624
Iteration 8/1000 | Loss: 0.00002565
Iteration 9/1000 | Loss: 0.00002524
Iteration 10/1000 | Loss: 0.00002489
Iteration 11/1000 | Loss: 0.00002450
Iteration 12/1000 | Loss: 0.00002413
Iteration 13/1000 | Loss: 0.00002393
Iteration 14/1000 | Loss: 0.00002381
Iteration 15/1000 | Loss: 0.00002380
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002358
Iteration 19/1000 | Loss: 0.00002357
Iteration 20/1000 | Loss: 0.00002357
Iteration 21/1000 | Loss: 0.00002355
Iteration 22/1000 | Loss: 0.00002354
Iteration 23/1000 | Loss: 0.00002354
Iteration 24/1000 | Loss: 0.00002354
Iteration 25/1000 | Loss: 0.00002353
Iteration 26/1000 | Loss: 0.00002353
Iteration 27/1000 | Loss: 0.00002353
Iteration 28/1000 | Loss: 0.00002353
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002352
Iteration 32/1000 | Loss: 0.00002352
Iteration 33/1000 | Loss: 0.00002351
Iteration 34/1000 | Loss: 0.00002351
Iteration 35/1000 | Loss: 0.00002350
Iteration 36/1000 | Loss: 0.00002350
Iteration 37/1000 | Loss: 0.00002349
Iteration 38/1000 | Loss: 0.00002349
Iteration 39/1000 | Loss: 0.00002348
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002347
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [2.3462755052605644e-05, 2.3462755052605644e-05, 2.3462755052605644e-05, 2.3462755052605644e-05, 2.3462755052605644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3462755052605644e-05

Optimization complete. Final v2v error: 4.045980930328369 mm

Highest mean error: 4.710934162139893 mm for frame 141

Lowest mean error: 3.6921794414520264 mm for frame 62

Saving results

Total time: 58.535701274871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399478
Iteration 2/25 | Loss: 0.00122769
Iteration 3/25 | Loss: 0.00114321
Iteration 4/25 | Loss: 0.00113572
Iteration 5/25 | Loss: 0.00113273
Iteration 6/25 | Loss: 0.00113241
Iteration 7/25 | Loss: 0.00113241
Iteration 8/25 | Loss: 0.00113241
Iteration 9/25 | Loss: 0.00113241
Iteration 10/25 | Loss: 0.00113241
Iteration 11/25 | Loss: 0.00113241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001132411533035338, 0.001132411533035338, 0.001132411533035338, 0.001132411533035338, 0.001132411533035338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001132411533035338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.87256026
Iteration 2/25 | Loss: 0.00154689
Iteration 3/25 | Loss: 0.00154689
Iteration 4/25 | Loss: 0.00154689
Iteration 5/25 | Loss: 0.00154688
Iteration 6/25 | Loss: 0.00154688
Iteration 7/25 | Loss: 0.00154688
Iteration 8/25 | Loss: 0.00154688
Iteration 9/25 | Loss: 0.00154688
Iteration 10/25 | Loss: 0.00154688
Iteration 11/25 | Loss: 0.00154688
Iteration 12/25 | Loss: 0.00154688
Iteration 13/25 | Loss: 0.00154688
Iteration 14/25 | Loss: 0.00154688
Iteration 15/25 | Loss: 0.00154688
Iteration 16/25 | Loss: 0.00154688
Iteration 17/25 | Loss: 0.00154688
Iteration 18/25 | Loss: 0.00154688
Iteration 19/25 | Loss: 0.00154688
Iteration 20/25 | Loss: 0.00154688
Iteration 21/25 | Loss: 0.00154688
Iteration 22/25 | Loss: 0.00154688
Iteration 23/25 | Loss: 0.00154688
Iteration 24/25 | Loss: 0.00154688
Iteration 25/25 | Loss: 0.00154688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154688
Iteration 2/1000 | Loss: 0.00004474
Iteration 3/1000 | Loss: 0.00002505
Iteration 4/1000 | Loss: 0.00001991
Iteration 5/1000 | Loss: 0.00001782
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001585
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001499
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001486
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001483
Iteration 22/1000 | Loss: 0.00001482
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001481
Iteration 25/1000 | Loss: 0.00001480
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001479
Iteration 34/1000 | Loss: 0.00001479
Iteration 35/1000 | Loss: 0.00001478
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001478
Iteration 39/1000 | Loss: 0.00001478
Iteration 40/1000 | Loss: 0.00001478
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001474
Iteration 50/1000 | Loss: 0.00001474
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001472
Iteration 58/1000 | Loss: 0.00001472
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001472
Iteration 64/1000 | Loss: 0.00001472
Iteration 65/1000 | Loss: 0.00001472
Iteration 66/1000 | Loss: 0.00001472
Iteration 67/1000 | Loss: 0.00001472
Iteration 68/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.471875384595478e-05, 1.471875384595478e-05, 1.471875384595478e-05, 1.471875384595478e-05, 1.471875384595478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.471875384595478e-05

Optimization complete. Final v2v error: 3.323000431060791 mm

Highest mean error: 3.7478866577148438 mm for frame 79

Lowest mean error: 3.0809483528137207 mm for frame 201

Saving results

Total time: 27.829777002334595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496938
Iteration 2/25 | Loss: 0.00135118
Iteration 3/25 | Loss: 0.00116388
Iteration 4/25 | Loss: 0.00115697
Iteration 5/25 | Loss: 0.00115565
Iteration 6/25 | Loss: 0.00115565
Iteration 7/25 | Loss: 0.00115565
Iteration 8/25 | Loss: 0.00115565
Iteration 9/25 | Loss: 0.00115565
Iteration 10/25 | Loss: 0.00115565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011556540848687291, 0.0011556540848687291, 0.0011556540848687291, 0.0011556540848687291, 0.0011556540848687291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011556540848687291

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24569237
Iteration 2/25 | Loss: 0.00146075
Iteration 3/25 | Loss: 0.00146075
Iteration 4/25 | Loss: 0.00146075
Iteration 5/25 | Loss: 0.00146075
Iteration 6/25 | Loss: 0.00146075
Iteration 7/25 | Loss: 0.00146075
Iteration 8/25 | Loss: 0.00146075
Iteration 9/25 | Loss: 0.00146075
Iteration 10/25 | Loss: 0.00146075
Iteration 11/25 | Loss: 0.00146075
Iteration 12/25 | Loss: 0.00146075
Iteration 13/25 | Loss: 0.00146075
Iteration 14/25 | Loss: 0.00146075
Iteration 15/25 | Loss: 0.00146075
Iteration 16/25 | Loss: 0.00146075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014607466291636229, 0.0014607466291636229, 0.0014607466291636229, 0.0014607466291636229, 0.0014607466291636229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014607466291636229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146075
Iteration 2/1000 | Loss: 0.00004582
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002733
Iteration 5/1000 | Loss: 0.00002544
Iteration 6/1000 | Loss: 0.00002428
Iteration 7/1000 | Loss: 0.00002374
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002315
Iteration 10/1000 | Loss: 0.00002311
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00002296
Iteration 13/1000 | Loss: 0.00002291
Iteration 14/1000 | Loss: 0.00002291
Iteration 15/1000 | Loss: 0.00002290
Iteration 16/1000 | Loss: 0.00002290
Iteration 17/1000 | Loss: 0.00002290
Iteration 18/1000 | Loss: 0.00002287
Iteration 19/1000 | Loss: 0.00002287
Iteration 20/1000 | Loss: 0.00002287
Iteration 21/1000 | Loss: 0.00002287
Iteration 22/1000 | Loss: 0.00002287
Iteration 23/1000 | Loss: 0.00002287
Iteration 24/1000 | Loss: 0.00002286
Iteration 25/1000 | Loss: 0.00002286
Iteration 26/1000 | Loss: 0.00002286
Iteration 27/1000 | Loss: 0.00002285
Iteration 28/1000 | Loss: 0.00002285
Iteration 29/1000 | Loss: 0.00002284
Iteration 30/1000 | Loss: 0.00002284
Iteration 31/1000 | Loss: 0.00002284
Iteration 32/1000 | Loss: 0.00002283
Iteration 33/1000 | Loss: 0.00002283
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002281
Iteration 37/1000 | Loss: 0.00002277
Iteration 38/1000 | Loss: 0.00002277
Iteration 39/1000 | Loss: 0.00002276
Iteration 40/1000 | Loss: 0.00002274
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002273
Iteration 47/1000 | Loss: 0.00002273
Iteration 48/1000 | Loss: 0.00002273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [2.2726722818333656e-05, 2.2726722818333656e-05, 2.2726722818333656e-05, 2.2726722818333656e-05, 2.2726722818333656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2726722818333656e-05

Optimization complete. Final v2v error: 3.803309202194214 mm

Highest mean error: 4.372964382171631 mm for frame 157

Lowest mean error: 3.4997811317443848 mm for frame 201

Saving results

Total time: 29.236599683761597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869298
Iteration 2/25 | Loss: 0.00149040
Iteration 3/25 | Loss: 0.00128766
Iteration 4/25 | Loss: 0.00127237
Iteration 5/25 | Loss: 0.00127035
Iteration 6/25 | Loss: 0.00127027
Iteration 7/25 | Loss: 0.00127027
Iteration 8/25 | Loss: 0.00127027
Iteration 9/25 | Loss: 0.00127027
Iteration 10/25 | Loss: 0.00127027
Iteration 11/25 | Loss: 0.00127027
Iteration 12/25 | Loss: 0.00127027
Iteration 13/25 | Loss: 0.00127027
Iteration 14/25 | Loss: 0.00127027
Iteration 15/25 | Loss: 0.00127027
Iteration 16/25 | Loss: 0.00127027
Iteration 17/25 | Loss: 0.00127027
Iteration 18/25 | Loss: 0.00127027
Iteration 19/25 | Loss: 0.00127027
Iteration 20/25 | Loss: 0.00127027
Iteration 21/25 | Loss: 0.00127027
Iteration 22/25 | Loss: 0.00127027
Iteration 23/25 | Loss: 0.00127027
Iteration 24/25 | Loss: 0.00127027
Iteration 25/25 | Loss: 0.00127027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24775612
Iteration 2/25 | Loss: 0.00088439
Iteration 3/25 | Loss: 0.00088439
Iteration 4/25 | Loss: 0.00088439
Iteration 5/25 | Loss: 0.00088439
Iteration 6/25 | Loss: 0.00088439
Iteration 7/25 | Loss: 0.00088439
Iteration 8/25 | Loss: 0.00088439
Iteration 9/25 | Loss: 0.00088439
Iteration 10/25 | Loss: 0.00088439
Iteration 11/25 | Loss: 0.00088439
Iteration 12/25 | Loss: 0.00088439
Iteration 13/25 | Loss: 0.00088439
Iteration 14/25 | Loss: 0.00088439
Iteration 15/25 | Loss: 0.00088439
Iteration 16/25 | Loss: 0.00088439
Iteration 17/25 | Loss: 0.00088439
Iteration 18/25 | Loss: 0.00088439
Iteration 19/25 | Loss: 0.00088439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008843907853588462, 0.0008843907853588462, 0.0008843907853588462, 0.0008843907853588462, 0.0008843907853588462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008843907853588462

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088439
Iteration 2/1000 | Loss: 0.00008067
Iteration 3/1000 | Loss: 0.00005382
Iteration 4/1000 | Loss: 0.00004689
Iteration 5/1000 | Loss: 0.00004329
Iteration 6/1000 | Loss: 0.00004184
Iteration 7/1000 | Loss: 0.00004110
Iteration 8/1000 | Loss: 0.00004060
Iteration 9/1000 | Loss: 0.00004033
Iteration 10/1000 | Loss: 0.00004004
Iteration 11/1000 | Loss: 0.00003977
Iteration 12/1000 | Loss: 0.00003970
Iteration 13/1000 | Loss: 0.00003967
Iteration 14/1000 | Loss: 0.00003967
Iteration 15/1000 | Loss: 0.00003967
Iteration 16/1000 | Loss: 0.00003967
Iteration 17/1000 | Loss: 0.00003967
Iteration 18/1000 | Loss: 0.00003967
Iteration 19/1000 | Loss: 0.00003967
Iteration 20/1000 | Loss: 0.00003967
Iteration 21/1000 | Loss: 0.00003967
Iteration 22/1000 | Loss: 0.00003966
Iteration 23/1000 | Loss: 0.00003966
Iteration 24/1000 | Loss: 0.00003966
Iteration 25/1000 | Loss: 0.00003966
Iteration 26/1000 | Loss: 0.00003966
Iteration 27/1000 | Loss: 0.00003963
Iteration 28/1000 | Loss: 0.00003963
Iteration 29/1000 | Loss: 0.00003963
Iteration 30/1000 | Loss: 0.00003963
Iteration 31/1000 | Loss: 0.00003962
Iteration 32/1000 | Loss: 0.00003962
Iteration 33/1000 | Loss: 0.00003962
Iteration 34/1000 | Loss: 0.00003962
Iteration 35/1000 | Loss: 0.00003962
Iteration 36/1000 | Loss: 0.00003962
Iteration 37/1000 | Loss: 0.00003961
Iteration 38/1000 | Loss: 0.00003961
Iteration 39/1000 | Loss: 0.00003958
Iteration 40/1000 | Loss: 0.00003958
Iteration 41/1000 | Loss: 0.00003957
Iteration 42/1000 | Loss: 0.00003956
Iteration 43/1000 | Loss: 0.00003956
Iteration 44/1000 | Loss: 0.00003955
Iteration 45/1000 | Loss: 0.00003955
Iteration 46/1000 | Loss: 0.00003954
Iteration 47/1000 | Loss: 0.00003954
Iteration 48/1000 | Loss: 0.00003954
Iteration 49/1000 | Loss: 0.00003953
Iteration 50/1000 | Loss: 0.00003953
Iteration 51/1000 | Loss: 0.00003952
Iteration 52/1000 | Loss: 0.00003952
Iteration 53/1000 | Loss: 0.00003952
Iteration 54/1000 | Loss: 0.00003952
Iteration 55/1000 | Loss: 0.00003952
Iteration 56/1000 | Loss: 0.00003951
Iteration 57/1000 | Loss: 0.00003951
Iteration 58/1000 | Loss: 0.00003951
Iteration 59/1000 | Loss: 0.00003950
Iteration 60/1000 | Loss: 0.00003950
Iteration 61/1000 | Loss: 0.00003950
Iteration 62/1000 | Loss: 0.00003950
Iteration 63/1000 | Loss: 0.00003950
Iteration 64/1000 | Loss: 0.00003950
Iteration 65/1000 | Loss: 0.00003950
Iteration 66/1000 | Loss: 0.00003950
Iteration 67/1000 | Loss: 0.00003949
Iteration 68/1000 | Loss: 0.00003949
Iteration 69/1000 | Loss: 0.00003949
Iteration 70/1000 | Loss: 0.00003948
Iteration 71/1000 | Loss: 0.00003948
Iteration 72/1000 | Loss: 0.00003947
Iteration 73/1000 | Loss: 0.00003947
Iteration 74/1000 | Loss: 0.00003947
Iteration 75/1000 | Loss: 0.00003947
Iteration 76/1000 | Loss: 0.00003947
Iteration 77/1000 | Loss: 0.00003947
Iteration 78/1000 | Loss: 0.00003946
Iteration 79/1000 | Loss: 0.00003946
Iteration 80/1000 | Loss: 0.00003946
Iteration 81/1000 | Loss: 0.00003946
Iteration 82/1000 | Loss: 0.00003946
Iteration 83/1000 | Loss: 0.00003946
Iteration 84/1000 | Loss: 0.00003946
Iteration 85/1000 | Loss: 0.00003946
Iteration 86/1000 | Loss: 0.00003946
Iteration 87/1000 | Loss: 0.00003946
Iteration 88/1000 | Loss: 0.00003945
Iteration 89/1000 | Loss: 0.00003945
Iteration 90/1000 | Loss: 0.00003945
Iteration 91/1000 | Loss: 0.00003945
Iteration 92/1000 | Loss: 0.00003945
Iteration 93/1000 | Loss: 0.00003944
Iteration 94/1000 | Loss: 0.00003944
Iteration 95/1000 | Loss: 0.00003944
Iteration 96/1000 | Loss: 0.00003944
Iteration 97/1000 | Loss: 0.00003944
Iteration 98/1000 | Loss: 0.00003944
Iteration 99/1000 | Loss: 0.00003944
Iteration 100/1000 | Loss: 0.00003943
Iteration 101/1000 | Loss: 0.00003943
Iteration 102/1000 | Loss: 0.00003943
Iteration 103/1000 | Loss: 0.00003943
Iteration 104/1000 | Loss: 0.00003943
Iteration 105/1000 | Loss: 0.00003943
Iteration 106/1000 | Loss: 0.00003943
Iteration 107/1000 | Loss: 0.00003943
Iteration 108/1000 | Loss: 0.00003943
Iteration 109/1000 | Loss: 0.00003943
Iteration 110/1000 | Loss: 0.00003943
Iteration 111/1000 | Loss: 0.00003943
Iteration 112/1000 | Loss: 0.00003942
Iteration 113/1000 | Loss: 0.00003942
Iteration 114/1000 | Loss: 0.00003942
Iteration 115/1000 | Loss: 0.00003942
Iteration 116/1000 | Loss: 0.00003942
Iteration 117/1000 | Loss: 0.00003942
Iteration 118/1000 | Loss: 0.00003942
Iteration 119/1000 | Loss: 0.00003942
Iteration 120/1000 | Loss: 0.00003942
Iteration 121/1000 | Loss: 0.00003942
Iteration 122/1000 | Loss: 0.00003942
Iteration 123/1000 | Loss: 0.00003942
Iteration 124/1000 | Loss: 0.00003942
Iteration 125/1000 | Loss: 0.00003942
Iteration 126/1000 | Loss: 0.00003942
Iteration 127/1000 | Loss: 0.00003942
Iteration 128/1000 | Loss: 0.00003942
Iteration 129/1000 | Loss: 0.00003942
Iteration 130/1000 | Loss: 0.00003942
Iteration 131/1000 | Loss: 0.00003942
Iteration 132/1000 | Loss: 0.00003942
Iteration 133/1000 | Loss: 0.00003942
Iteration 134/1000 | Loss: 0.00003942
Iteration 135/1000 | Loss: 0.00003942
Iteration 136/1000 | Loss: 0.00003942
Iteration 137/1000 | Loss: 0.00003942
Iteration 138/1000 | Loss: 0.00003942
Iteration 139/1000 | Loss: 0.00003942
Iteration 140/1000 | Loss: 0.00003942
Iteration 141/1000 | Loss: 0.00003942
Iteration 142/1000 | Loss: 0.00003942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.9418042433680966e-05, 3.9418042433680966e-05, 3.9418042433680966e-05, 3.9418042433680966e-05, 3.9418042433680966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9418042433680966e-05

Optimization complete. Final v2v error: 5.2481584548950195 mm

Highest mean error: 5.601775646209717 mm for frame 47

Lowest mean error: 4.711720943450928 mm for frame 132

Saving results

Total time: 31.18927574157715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076185
Iteration 2/25 | Loss: 0.00168228
Iteration 3/25 | Loss: 0.00135355
Iteration 4/25 | Loss: 0.00132812
Iteration 5/25 | Loss: 0.00132020
Iteration 6/25 | Loss: 0.00131746
Iteration 7/25 | Loss: 0.00131680
Iteration 8/25 | Loss: 0.00131680
Iteration 9/25 | Loss: 0.00131680
Iteration 10/25 | Loss: 0.00131680
Iteration 11/25 | Loss: 0.00131680
Iteration 12/25 | Loss: 0.00131680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013168021105229855, 0.0013168021105229855, 0.0013168021105229855, 0.0013168021105229855, 0.0013168021105229855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013168021105229855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95407248
Iteration 2/25 | Loss: 0.00121645
Iteration 3/25 | Loss: 0.00121643
Iteration 4/25 | Loss: 0.00121643
Iteration 5/25 | Loss: 0.00121643
Iteration 6/25 | Loss: 0.00121643
Iteration 7/25 | Loss: 0.00121643
Iteration 8/25 | Loss: 0.00121643
Iteration 9/25 | Loss: 0.00121643
Iteration 10/25 | Loss: 0.00121643
Iteration 11/25 | Loss: 0.00121643
Iteration 12/25 | Loss: 0.00121643
Iteration 13/25 | Loss: 0.00121643
Iteration 14/25 | Loss: 0.00121643
Iteration 15/25 | Loss: 0.00121643
Iteration 16/25 | Loss: 0.00121643
Iteration 17/25 | Loss: 0.00121643
Iteration 18/25 | Loss: 0.00121643
Iteration 19/25 | Loss: 0.00121643
Iteration 20/25 | Loss: 0.00121643
Iteration 21/25 | Loss: 0.00121643
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012164302170276642, 0.0012164302170276642, 0.0012164302170276642, 0.0012164302170276642, 0.0012164302170276642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012164302170276642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121643
Iteration 2/1000 | Loss: 0.00009080
Iteration 3/1000 | Loss: 0.00005590
Iteration 4/1000 | Loss: 0.00004700
Iteration 5/1000 | Loss: 0.00004283
Iteration 6/1000 | Loss: 0.00004070
Iteration 7/1000 | Loss: 0.00003927
Iteration 8/1000 | Loss: 0.00003815
Iteration 9/1000 | Loss: 0.00003723
Iteration 10/1000 | Loss: 0.00003648
Iteration 11/1000 | Loss: 0.00003601
Iteration 12/1000 | Loss: 0.00003576
Iteration 13/1000 | Loss: 0.00003554
Iteration 14/1000 | Loss: 0.00003535
Iteration 15/1000 | Loss: 0.00003524
Iteration 16/1000 | Loss: 0.00003517
Iteration 17/1000 | Loss: 0.00003512
Iteration 18/1000 | Loss: 0.00003512
Iteration 19/1000 | Loss: 0.00003510
Iteration 20/1000 | Loss: 0.00003505
Iteration 21/1000 | Loss: 0.00003497
Iteration 22/1000 | Loss: 0.00003497
Iteration 23/1000 | Loss: 0.00003492
Iteration 24/1000 | Loss: 0.00003491
Iteration 25/1000 | Loss: 0.00003491
Iteration 26/1000 | Loss: 0.00003486
Iteration 27/1000 | Loss: 0.00003486
Iteration 28/1000 | Loss: 0.00003484
Iteration 29/1000 | Loss: 0.00003484
Iteration 30/1000 | Loss: 0.00003483
Iteration 31/1000 | Loss: 0.00003482
Iteration 32/1000 | Loss: 0.00003482
Iteration 33/1000 | Loss: 0.00003481
Iteration 34/1000 | Loss: 0.00003481
Iteration 35/1000 | Loss: 0.00003480
Iteration 36/1000 | Loss: 0.00003480
Iteration 37/1000 | Loss: 0.00003479
Iteration 38/1000 | Loss: 0.00003479
Iteration 39/1000 | Loss: 0.00003479
Iteration 40/1000 | Loss: 0.00003479
Iteration 41/1000 | Loss: 0.00003479
Iteration 42/1000 | Loss: 0.00003479
Iteration 43/1000 | Loss: 0.00003479
Iteration 44/1000 | Loss: 0.00003479
Iteration 45/1000 | Loss: 0.00003479
Iteration 46/1000 | Loss: 0.00003478
Iteration 47/1000 | Loss: 0.00003478
Iteration 48/1000 | Loss: 0.00003478
Iteration 49/1000 | Loss: 0.00003477
Iteration 50/1000 | Loss: 0.00003477
Iteration 51/1000 | Loss: 0.00003477
Iteration 52/1000 | Loss: 0.00003477
Iteration 53/1000 | Loss: 0.00003476
Iteration 54/1000 | Loss: 0.00003476
Iteration 55/1000 | Loss: 0.00003476
Iteration 56/1000 | Loss: 0.00003476
Iteration 57/1000 | Loss: 0.00003476
Iteration 58/1000 | Loss: 0.00003476
Iteration 59/1000 | Loss: 0.00003476
Iteration 60/1000 | Loss: 0.00003476
Iteration 61/1000 | Loss: 0.00003475
Iteration 62/1000 | Loss: 0.00003475
Iteration 63/1000 | Loss: 0.00003475
Iteration 64/1000 | Loss: 0.00003475
Iteration 65/1000 | Loss: 0.00003475
Iteration 66/1000 | Loss: 0.00003475
Iteration 67/1000 | Loss: 0.00003475
Iteration 68/1000 | Loss: 0.00003475
Iteration 69/1000 | Loss: 0.00003474
Iteration 70/1000 | Loss: 0.00003474
Iteration 71/1000 | Loss: 0.00003474
Iteration 72/1000 | Loss: 0.00003474
Iteration 73/1000 | Loss: 0.00003474
Iteration 74/1000 | Loss: 0.00003474
Iteration 75/1000 | Loss: 0.00003474
Iteration 76/1000 | Loss: 0.00003473
Iteration 77/1000 | Loss: 0.00003473
Iteration 78/1000 | Loss: 0.00003473
Iteration 79/1000 | Loss: 0.00003473
Iteration 80/1000 | Loss: 0.00003473
Iteration 81/1000 | Loss: 0.00003473
Iteration 82/1000 | Loss: 0.00003472
Iteration 83/1000 | Loss: 0.00003472
Iteration 84/1000 | Loss: 0.00003472
Iteration 85/1000 | Loss: 0.00003472
Iteration 86/1000 | Loss: 0.00003472
Iteration 87/1000 | Loss: 0.00003472
Iteration 88/1000 | Loss: 0.00003472
Iteration 89/1000 | Loss: 0.00003471
Iteration 90/1000 | Loss: 0.00003471
Iteration 91/1000 | Loss: 0.00003471
Iteration 92/1000 | Loss: 0.00003470
Iteration 93/1000 | Loss: 0.00003470
Iteration 94/1000 | Loss: 0.00003470
Iteration 95/1000 | Loss: 0.00003468
Iteration 96/1000 | Loss: 0.00003468
Iteration 97/1000 | Loss: 0.00003468
Iteration 98/1000 | Loss: 0.00003468
Iteration 99/1000 | Loss: 0.00003468
Iteration 100/1000 | Loss: 0.00003468
Iteration 101/1000 | Loss: 0.00003468
Iteration 102/1000 | Loss: 0.00003468
Iteration 103/1000 | Loss: 0.00003468
Iteration 104/1000 | Loss: 0.00003468
Iteration 105/1000 | Loss: 0.00003468
Iteration 106/1000 | Loss: 0.00003468
Iteration 107/1000 | Loss: 0.00003467
Iteration 108/1000 | Loss: 0.00003467
Iteration 109/1000 | Loss: 0.00003467
Iteration 110/1000 | Loss: 0.00003467
Iteration 111/1000 | Loss: 0.00003466
Iteration 112/1000 | Loss: 0.00003466
Iteration 113/1000 | Loss: 0.00003466
Iteration 114/1000 | Loss: 0.00003466
Iteration 115/1000 | Loss: 0.00003465
Iteration 116/1000 | Loss: 0.00003465
Iteration 117/1000 | Loss: 0.00003465
Iteration 118/1000 | Loss: 0.00003465
Iteration 119/1000 | Loss: 0.00003465
Iteration 120/1000 | Loss: 0.00003465
Iteration 121/1000 | Loss: 0.00003465
Iteration 122/1000 | Loss: 0.00003465
Iteration 123/1000 | Loss: 0.00003465
Iteration 124/1000 | Loss: 0.00003464
Iteration 125/1000 | Loss: 0.00003464
Iteration 126/1000 | Loss: 0.00003464
Iteration 127/1000 | Loss: 0.00003464
Iteration 128/1000 | Loss: 0.00003464
Iteration 129/1000 | Loss: 0.00003464
Iteration 130/1000 | Loss: 0.00003464
Iteration 131/1000 | Loss: 0.00003463
Iteration 132/1000 | Loss: 0.00003463
Iteration 133/1000 | Loss: 0.00003463
Iteration 134/1000 | Loss: 0.00003463
Iteration 135/1000 | Loss: 0.00003463
Iteration 136/1000 | Loss: 0.00003463
Iteration 137/1000 | Loss: 0.00003463
Iteration 138/1000 | Loss: 0.00003463
Iteration 139/1000 | Loss: 0.00003463
Iteration 140/1000 | Loss: 0.00003463
Iteration 141/1000 | Loss: 0.00003463
Iteration 142/1000 | Loss: 0.00003463
Iteration 143/1000 | Loss: 0.00003463
Iteration 144/1000 | Loss: 0.00003463
Iteration 145/1000 | Loss: 0.00003463
Iteration 146/1000 | Loss: 0.00003463
Iteration 147/1000 | Loss: 0.00003463
Iteration 148/1000 | Loss: 0.00003463
Iteration 149/1000 | Loss: 0.00003463
Iteration 150/1000 | Loss: 0.00003463
Iteration 151/1000 | Loss: 0.00003463
Iteration 152/1000 | Loss: 0.00003463
Iteration 153/1000 | Loss: 0.00003463
Iteration 154/1000 | Loss: 0.00003463
Iteration 155/1000 | Loss: 0.00003462
Iteration 156/1000 | Loss: 0.00003462
Iteration 157/1000 | Loss: 0.00003462
Iteration 158/1000 | Loss: 0.00003462
Iteration 159/1000 | Loss: 0.00003462
Iteration 160/1000 | Loss: 0.00003462
Iteration 161/1000 | Loss: 0.00003462
Iteration 162/1000 | Loss: 0.00003462
Iteration 163/1000 | Loss: 0.00003462
Iteration 164/1000 | Loss: 0.00003461
Iteration 165/1000 | Loss: 0.00003461
Iteration 166/1000 | Loss: 0.00003461
Iteration 167/1000 | Loss: 0.00003461
Iteration 168/1000 | Loss: 0.00003461
Iteration 169/1000 | Loss: 0.00003461
Iteration 170/1000 | Loss: 0.00003461
Iteration 171/1000 | Loss: 0.00003461
Iteration 172/1000 | Loss: 0.00003461
Iteration 173/1000 | Loss: 0.00003460
Iteration 174/1000 | Loss: 0.00003460
Iteration 175/1000 | Loss: 0.00003460
Iteration 176/1000 | Loss: 0.00003459
Iteration 177/1000 | Loss: 0.00003459
Iteration 178/1000 | Loss: 0.00003459
Iteration 179/1000 | Loss: 0.00003459
Iteration 180/1000 | Loss: 0.00003459
Iteration 181/1000 | Loss: 0.00003459
Iteration 182/1000 | Loss: 0.00003459
Iteration 183/1000 | Loss: 0.00003459
Iteration 184/1000 | Loss: 0.00003458
Iteration 185/1000 | Loss: 0.00003458
Iteration 186/1000 | Loss: 0.00003458
Iteration 187/1000 | Loss: 0.00003458
Iteration 188/1000 | Loss: 0.00003458
Iteration 189/1000 | Loss: 0.00003458
Iteration 190/1000 | Loss: 0.00003457
Iteration 191/1000 | Loss: 0.00003457
Iteration 192/1000 | Loss: 0.00003457
Iteration 193/1000 | Loss: 0.00003457
Iteration 194/1000 | Loss: 0.00003456
Iteration 195/1000 | Loss: 0.00003456
Iteration 196/1000 | Loss: 0.00003456
Iteration 197/1000 | Loss: 0.00003456
Iteration 198/1000 | Loss: 0.00003456
Iteration 199/1000 | Loss: 0.00003456
Iteration 200/1000 | Loss: 0.00003456
Iteration 201/1000 | Loss: 0.00003456
Iteration 202/1000 | Loss: 0.00003455
Iteration 203/1000 | Loss: 0.00003455
Iteration 204/1000 | Loss: 0.00003455
Iteration 205/1000 | Loss: 0.00003455
Iteration 206/1000 | Loss: 0.00003455
Iteration 207/1000 | Loss: 0.00003455
Iteration 208/1000 | Loss: 0.00003455
Iteration 209/1000 | Loss: 0.00003454
Iteration 210/1000 | Loss: 0.00003454
Iteration 211/1000 | Loss: 0.00003454
Iteration 212/1000 | Loss: 0.00003454
Iteration 213/1000 | Loss: 0.00003454
Iteration 214/1000 | Loss: 0.00003454
Iteration 215/1000 | Loss: 0.00003454
Iteration 216/1000 | Loss: 0.00003454
Iteration 217/1000 | Loss: 0.00003453
Iteration 218/1000 | Loss: 0.00003453
Iteration 219/1000 | Loss: 0.00003453
Iteration 220/1000 | Loss: 0.00003453
Iteration 221/1000 | Loss: 0.00003453
Iteration 222/1000 | Loss: 0.00003453
Iteration 223/1000 | Loss: 0.00003453
Iteration 224/1000 | Loss: 0.00003453
Iteration 225/1000 | Loss: 0.00003453
Iteration 226/1000 | Loss: 0.00003452
Iteration 227/1000 | Loss: 0.00003452
Iteration 228/1000 | Loss: 0.00003452
Iteration 229/1000 | Loss: 0.00003452
Iteration 230/1000 | Loss: 0.00003452
Iteration 231/1000 | Loss: 0.00003452
Iteration 232/1000 | Loss: 0.00003452
Iteration 233/1000 | Loss: 0.00003452
Iteration 234/1000 | Loss: 0.00003452
Iteration 235/1000 | Loss: 0.00003452
Iteration 236/1000 | Loss: 0.00003452
Iteration 237/1000 | Loss: 0.00003451
Iteration 238/1000 | Loss: 0.00003451
Iteration 239/1000 | Loss: 0.00003451
Iteration 240/1000 | Loss: 0.00003451
Iteration 241/1000 | Loss: 0.00003451
Iteration 242/1000 | Loss: 0.00003451
Iteration 243/1000 | Loss: 0.00003451
Iteration 244/1000 | Loss: 0.00003451
Iteration 245/1000 | Loss: 0.00003450
Iteration 246/1000 | Loss: 0.00003450
Iteration 247/1000 | Loss: 0.00003450
Iteration 248/1000 | Loss: 0.00003450
Iteration 249/1000 | Loss: 0.00003450
Iteration 250/1000 | Loss: 0.00003450
Iteration 251/1000 | Loss: 0.00003450
Iteration 252/1000 | Loss: 0.00003450
Iteration 253/1000 | Loss: 0.00003450
Iteration 254/1000 | Loss: 0.00003449
Iteration 255/1000 | Loss: 0.00003449
Iteration 256/1000 | Loss: 0.00003449
Iteration 257/1000 | Loss: 0.00003449
Iteration 258/1000 | Loss: 0.00003449
Iteration 259/1000 | Loss: 0.00003449
Iteration 260/1000 | Loss: 0.00003449
Iteration 261/1000 | Loss: 0.00003449
Iteration 262/1000 | Loss: 0.00003449
Iteration 263/1000 | Loss: 0.00003449
Iteration 264/1000 | Loss: 0.00003449
Iteration 265/1000 | Loss: 0.00003448
Iteration 266/1000 | Loss: 0.00003448
Iteration 267/1000 | Loss: 0.00003448
Iteration 268/1000 | Loss: 0.00003448
Iteration 269/1000 | Loss: 0.00003448
Iteration 270/1000 | Loss: 0.00003448
Iteration 271/1000 | Loss: 0.00003448
Iteration 272/1000 | Loss: 0.00003448
Iteration 273/1000 | Loss: 0.00003448
Iteration 274/1000 | Loss: 0.00003448
Iteration 275/1000 | Loss: 0.00003447
Iteration 276/1000 | Loss: 0.00003447
Iteration 277/1000 | Loss: 0.00003447
Iteration 278/1000 | Loss: 0.00003447
Iteration 279/1000 | Loss: 0.00003447
Iteration 280/1000 | Loss: 0.00003447
Iteration 281/1000 | Loss: 0.00003447
Iteration 282/1000 | Loss: 0.00003447
Iteration 283/1000 | Loss: 0.00003447
Iteration 284/1000 | Loss: 0.00003447
Iteration 285/1000 | Loss: 0.00003447
Iteration 286/1000 | Loss: 0.00003447
Iteration 287/1000 | Loss: 0.00003447
Iteration 288/1000 | Loss: 0.00003447
Iteration 289/1000 | Loss: 0.00003447
Iteration 290/1000 | Loss: 0.00003447
Iteration 291/1000 | Loss: 0.00003446
Iteration 292/1000 | Loss: 0.00003446
Iteration 293/1000 | Loss: 0.00003446
Iteration 294/1000 | Loss: 0.00003446
Iteration 295/1000 | Loss: 0.00003446
Iteration 296/1000 | Loss: 0.00003446
Iteration 297/1000 | Loss: 0.00003446
Iteration 298/1000 | Loss: 0.00003446
Iteration 299/1000 | Loss: 0.00003446
Iteration 300/1000 | Loss: 0.00003445
Iteration 301/1000 | Loss: 0.00003445
Iteration 302/1000 | Loss: 0.00003445
Iteration 303/1000 | Loss: 0.00003445
Iteration 304/1000 | Loss: 0.00003445
Iteration 305/1000 | Loss: 0.00003445
Iteration 306/1000 | Loss: 0.00003445
Iteration 307/1000 | Loss: 0.00003445
Iteration 308/1000 | Loss: 0.00003445
Iteration 309/1000 | Loss: 0.00003445
Iteration 310/1000 | Loss: 0.00003445
Iteration 311/1000 | Loss: 0.00003444
Iteration 312/1000 | Loss: 0.00003444
Iteration 313/1000 | Loss: 0.00003444
Iteration 314/1000 | Loss: 0.00003444
Iteration 315/1000 | Loss: 0.00003444
Iteration 316/1000 | Loss: 0.00003444
Iteration 317/1000 | Loss: 0.00003444
Iteration 318/1000 | Loss: 0.00003444
Iteration 319/1000 | Loss: 0.00003444
Iteration 320/1000 | Loss: 0.00003444
Iteration 321/1000 | Loss: 0.00003444
Iteration 322/1000 | Loss: 0.00003444
Iteration 323/1000 | Loss: 0.00003444
Iteration 324/1000 | Loss: 0.00003444
Iteration 325/1000 | Loss: 0.00003443
Iteration 326/1000 | Loss: 0.00003443
Iteration 327/1000 | Loss: 0.00003443
Iteration 328/1000 | Loss: 0.00003443
Iteration 329/1000 | Loss: 0.00003443
Iteration 330/1000 | Loss: 0.00003443
Iteration 331/1000 | Loss: 0.00003443
Iteration 332/1000 | Loss: 0.00003442
Iteration 333/1000 | Loss: 0.00003442
Iteration 334/1000 | Loss: 0.00003442
Iteration 335/1000 | Loss: 0.00003442
Iteration 336/1000 | Loss: 0.00003442
Iteration 337/1000 | Loss: 0.00003442
Iteration 338/1000 | Loss: 0.00003442
Iteration 339/1000 | Loss: 0.00003442
Iteration 340/1000 | Loss: 0.00003442
Iteration 341/1000 | Loss: 0.00003442
Iteration 342/1000 | Loss: 0.00003442
Iteration 343/1000 | Loss: 0.00003442
Iteration 344/1000 | Loss: 0.00003442
Iteration 345/1000 | Loss: 0.00003442
Iteration 346/1000 | Loss: 0.00003441
Iteration 347/1000 | Loss: 0.00003441
Iteration 348/1000 | Loss: 0.00003441
Iteration 349/1000 | Loss: 0.00003441
Iteration 350/1000 | Loss: 0.00003441
Iteration 351/1000 | Loss: 0.00003441
Iteration 352/1000 | Loss: 0.00003441
Iteration 353/1000 | Loss: 0.00003441
Iteration 354/1000 | Loss: 0.00003441
Iteration 355/1000 | Loss: 0.00003441
Iteration 356/1000 | Loss: 0.00003441
Iteration 357/1000 | Loss: 0.00003441
Iteration 358/1000 | Loss: 0.00003441
Iteration 359/1000 | Loss: 0.00003441
Iteration 360/1000 | Loss: 0.00003441
Iteration 361/1000 | Loss: 0.00003441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 361. Stopping optimization.
Last 5 losses: [3.441087028477341e-05, 3.441087028477341e-05, 3.441087028477341e-05, 3.441087028477341e-05, 3.441087028477341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.441087028477341e-05

Optimization complete. Final v2v error: 4.754566669464111 mm

Highest mean error: 5.959662437438965 mm for frame 75

Lowest mean error: 3.900059938430786 mm for frame 29

Saving results

Total time: 58.61152529716492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00458562
Iteration 2/25 | Loss: 0.00146933
Iteration 3/25 | Loss: 0.00123972
Iteration 4/25 | Loss: 0.00121683
Iteration 5/25 | Loss: 0.00121430
Iteration 6/25 | Loss: 0.00121365
Iteration 7/25 | Loss: 0.00121365
Iteration 8/25 | Loss: 0.00121365
Iteration 9/25 | Loss: 0.00121365
Iteration 10/25 | Loss: 0.00121365
Iteration 11/25 | Loss: 0.00121365
Iteration 12/25 | Loss: 0.00121365
Iteration 13/25 | Loss: 0.00121365
Iteration 14/25 | Loss: 0.00121365
Iteration 15/25 | Loss: 0.00121365
Iteration 16/25 | Loss: 0.00121365
Iteration 17/25 | Loss: 0.00121365
Iteration 18/25 | Loss: 0.00121365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012136523146182299, 0.0012136523146182299, 0.0012136523146182299, 0.0012136523146182299, 0.0012136523146182299]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012136523146182299

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.97243500
Iteration 2/25 | Loss: 0.00122875
Iteration 3/25 | Loss: 0.00122872
Iteration 4/25 | Loss: 0.00122872
Iteration 5/25 | Loss: 0.00122872
Iteration 6/25 | Loss: 0.00122872
Iteration 7/25 | Loss: 0.00122872
Iteration 8/25 | Loss: 0.00122872
Iteration 9/25 | Loss: 0.00122872
Iteration 10/25 | Loss: 0.00122871
Iteration 11/25 | Loss: 0.00122871
Iteration 12/25 | Loss: 0.00122871
Iteration 13/25 | Loss: 0.00122871
Iteration 14/25 | Loss: 0.00122871
Iteration 15/25 | Loss: 0.00122871
Iteration 16/25 | Loss: 0.00122871
Iteration 17/25 | Loss: 0.00122871
Iteration 18/25 | Loss: 0.00122871
Iteration 19/25 | Loss: 0.00122871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012287144782021642, 0.0012287144782021642, 0.0012287144782021642, 0.0012287144782021642, 0.0012287144782021642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012287144782021642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122871
Iteration 2/1000 | Loss: 0.00006662
Iteration 3/1000 | Loss: 0.00003628
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00002289
Iteration 7/1000 | Loss: 0.00002194
Iteration 8/1000 | Loss: 0.00002151
Iteration 9/1000 | Loss: 0.00002103
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002055
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002030
Iteration 14/1000 | Loss: 0.00002021
Iteration 15/1000 | Loss: 0.00002020
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002013
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002011
Iteration 20/1000 | Loss: 0.00002010
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002006
Iteration 23/1000 | Loss: 0.00002006
Iteration 24/1000 | Loss: 0.00002006
Iteration 25/1000 | Loss: 0.00002006
Iteration 26/1000 | Loss: 0.00002006
Iteration 27/1000 | Loss: 0.00002003
Iteration 28/1000 | Loss: 0.00002003
Iteration 29/1000 | Loss: 0.00002002
Iteration 30/1000 | Loss: 0.00002002
Iteration 31/1000 | Loss: 0.00002002
Iteration 32/1000 | Loss: 0.00002001
Iteration 33/1000 | Loss: 0.00002001
Iteration 34/1000 | Loss: 0.00002001
Iteration 35/1000 | Loss: 0.00002000
Iteration 36/1000 | Loss: 0.00002000
Iteration 37/1000 | Loss: 0.00002000
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00002000
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00002000
Iteration 42/1000 | Loss: 0.00002000
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00001999
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001997
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001996
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001986
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001985
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001981
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Iteration 91/1000 | Loss: 0.00001977
Iteration 92/1000 | Loss: 0.00001977
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001976
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001975
Iteration 100/1000 | Loss: 0.00001975
Iteration 101/1000 | Loss: 0.00001975
Iteration 102/1000 | Loss: 0.00001975
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001973
Iteration 110/1000 | Loss: 0.00001973
Iteration 111/1000 | Loss: 0.00001973
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001972
Iteration 116/1000 | Loss: 0.00001972
Iteration 117/1000 | Loss: 0.00001972
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001972
Iteration 120/1000 | Loss: 0.00001972
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001972
Iteration 131/1000 | Loss: 0.00001972
Iteration 132/1000 | Loss: 0.00001972
Iteration 133/1000 | Loss: 0.00001972
Iteration 134/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.9719405827345327e-05, 1.9719405827345327e-05, 1.9719405827345327e-05, 1.9719405827345327e-05, 1.9719405827345327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9719405827345327e-05

Optimization complete. Final v2v error: 3.8802297115325928 mm

Highest mean error: 4.369305610656738 mm for frame 46

Lowest mean error: 3.4495489597320557 mm for frame 30

Saving results

Total time: 37.009973764419556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819247
Iteration 2/25 | Loss: 0.00183255
Iteration 3/25 | Loss: 0.00137911
Iteration 4/25 | Loss: 0.00127715
Iteration 5/25 | Loss: 0.00126462
Iteration 6/25 | Loss: 0.00124955
Iteration 7/25 | Loss: 0.00124563
Iteration 8/25 | Loss: 0.00123874
Iteration 9/25 | Loss: 0.00123723
Iteration 10/25 | Loss: 0.00123118
Iteration 11/25 | Loss: 0.00122505
Iteration 12/25 | Loss: 0.00122280
Iteration 13/25 | Loss: 0.00122293
Iteration 14/25 | Loss: 0.00122193
Iteration 15/25 | Loss: 0.00121889
Iteration 16/25 | Loss: 0.00121769
Iteration 17/25 | Loss: 0.00121747
Iteration 18/25 | Loss: 0.00121738
Iteration 19/25 | Loss: 0.00121734
Iteration 20/25 | Loss: 0.00121734
Iteration 21/25 | Loss: 0.00121734
Iteration 22/25 | Loss: 0.00121734
Iteration 23/25 | Loss: 0.00121734
Iteration 24/25 | Loss: 0.00121734
Iteration 25/25 | Loss: 0.00121734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67077160
Iteration 2/25 | Loss: 0.00154082
Iteration 3/25 | Loss: 0.00149366
Iteration 4/25 | Loss: 0.00149366
Iteration 5/25 | Loss: 0.00149366
Iteration 6/25 | Loss: 0.00149366
Iteration 7/25 | Loss: 0.00149366
Iteration 8/25 | Loss: 0.00149366
Iteration 9/25 | Loss: 0.00149366
Iteration 10/25 | Loss: 0.00149366
Iteration 11/25 | Loss: 0.00149366
Iteration 12/25 | Loss: 0.00149366
Iteration 13/25 | Loss: 0.00149366
Iteration 14/25 | Loss: 0.00149366
Iteration 15/25 | Loss: 0.00149366
Iteration 16/25 | Loss: 0.00149366
Iteration 17/25 | Loss: 0.00149366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014936580555513501, 0.0014936580555513501, 0.0014936580555513501, 0.0014936580555513501, 0.0014936580555513501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014936580555513501

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149366
Iteration 2/1000 | Loss: 0.00005894
Iteration 3/1000 | Loss: 0.00004280
Iteration 4/1000 | Loss: 0.00006869
Iteration 5/1000 | Loss: 0.00005122
Iteration 6/1000 | Loss: 0.00003372
Iteration 7/1000 | Loss: 0.00007513
Iteration 8/1000 | Loss: 0.00003176
Iteration 9/1000 | Loss: 0.00007177
Iteration 10/1000 | Loss: 0.00003124
Iteration 11/1000 | Loss: 0.00003031
Iteration 12/1000 | Loss: 0.00024284
Iteration 13/1000 | Loss: 0.00022511
Iteration 14/1000 | Loss: 0.00003180
Iteration 15/1000 | Loss: 0.00005624
Iteration 16/1000 | Loss: 0.00002870
Iteration 17/1000 | Loss: 0.00002774
Iteration 18/1000 | Loss: 0.00002689
Iteration 19/1000 | Loss: 0.00002621
Iteration 20/1000 | Loss: 0.00002593
Iteration 21/1000 | Loss: 0.00002581
Iteration 22/1000 | Loss: 0.00002560
Iteration 23/1000 | Loss: 0.00002559
Iteration 24/1000 | Loss: 0.00002559
Iteration 25/1000 | Loss: 0.00002559
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00002551
Iteration 28/1000 | Loss: 0.00002550
Iteration 29/1000 | Loss: 0.00002550
Iteration 30/1000 | Loss: 0.00002549
Iteration 31/1000 | Loss: 0.00004943
Iteration 32/1000 | Loss: 0.00002545
Iteration 33/1000 | Loss: 0.00002541
Iteration 34/1000 | Loss: 0.00002540
Iteration 35/1000 | Loss: 0.00004379
Iteration 36/1000 | Loss: 0.00002545
Iteration 37/1000 | Loss: 0.00002531
Iteration 38/1000 | Loss: 0.00002531
Iteration 39/1000 | Loss: 0.00002528
Iteration 40/1000 | Loss: 0.00002528
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002528
Iteration 43/1000 | Loss: 0.00002528
Iteration 44/1000 | Loss: 0.00002528
Iteration 45/1000 | Loss: 0.00002528
Iteration 46/1000 | Loss: 0.00002524
Iteration 47/1000 | Loss: 0.00002521
Iteration 48/1000 | Loss: 0.00002520
Iteration 49/1000 | Loss: 0.00002520
Iteration 50/1000 | Loss: 0.00002519
Iteration 51/1000 | Loss: 0.00002519
Iteration 52/1000 | Loss: 0.00002518
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002518
Iteration 55/1000 | Loss: 0.00002518
Iteration 56/1000 | Loss: 0.00002518
Iteration 57/1000 | Loss: 0.00002518
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002518
Iteration 61/1000 | Loss: 0.00002518
Iteration 62/1000 | Loss: 0.00002518
Iteration 63/1000 | Loss: 0.00002517
Iteration 64/1000 | Loss: 0.00002517
Iteration 65/1000 | Loss: 0.00002517
Iteration 66/1000 | Loss: 0.00002517
Iteration 67/1000 | Loss: 0.00002517
Iteration 68/1000 | Loss: 0.00002517
Iteration 69/1000 | Loss: 0.00002517
Iteration 70/1000 | Loss: 0.00002517
Iteration 71/1000 | Loss: 0.00002517
Iteration 72/1000 | Loss: 0.00002516
Iteration 73/1000 | Loss: 0.00002516
Iteration 74/1000 | Loss: 0.00002516
Iteration 75/1000 | Loss: 0.00002516
Iteration 76/1000 | Loss: 0.00002516
Iteration 77/1000 | Loss: 0.00002515
Iteration 78/1000 | Loss: 0.00002515
Iteration 79/1000 | Loss: 0.00002515
Iteration 80/1000 | Loss: 0.00002515
Iteration 81/1000 | Loss: 0.00002515
Iteration 82/1000 | Loss: 0.00002514
Iteration 83/1000 | Loss: 0.00002514
Iteration 84/1000 | Loss: 0.00002514
Iteration 85/1000 | Loss: 0.00002514
Iteration 86/1000 | Loss: 0.00002514
Iteration 87/1000 | Loss: 0.00002513
Iteration 88/1000 | Loss: 0.00002513
Iteration 89/1000 | Loss: 0.00002513
Iteration 90/1000 | Loss: 0.00002513
Iteration 91/1000 | Loss: 0.00002512
Iteration 92/1000 | Loss: 0.00002512
Iteration 93/1000 | Loss: 0.00002512
Iteration 94/1000 | Loss: 0.00002512
Iteration 95/1000 | Loss: 0.00002512
Iteration 96/1000 | Loss: 0.00002511
Iteration 97/1000 | Loss: 0.00002511
Iteration 98/1000 | Loss: 0.00002511
Iteration 99/1000 | Loss: 0.00002511
Iteration 100/1000 | Loss: 0.00002511
Iteration 101/1000 | Loss: 0.00002511
Iteration 102/1000 | Loss: 0.00002511
Iteration 103/1000 | Loss: 0.00002510
Iteration 104/1000 | Loss: 0.00002510
Iteration 105/1000 | Loss: 0.00002510
Iteration 106/1000 | Loss: 0.00002510
Iteration 107/1000 | Loss: 0.00002510
Iteration 108/1000 | Loss: 0.00002510
Iteration 109/1000 | Loss: 0.00002510
Iteration 110/1000 | Loss: 0.00002510
Iteration 111/1000 | Loss: 0.00002510
Iteration 112/1000 | Loss: 0.00002510
Iteration 113/1000 | Loss: 0.00002510
Iteration 114/1000 | Loss: 0.00002510
Iteration 115/1000 | Loss: 0.00002509
Iteration 116/1000 | Loss: 0.00002509
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002509
Iteration 119/1000 | Loss: 0.00002509
Iteration 120/1000 | Loss: 0.00002509
Iteration 121/1000 | Loss: 0.00002509
Iteration 122/1000 | Loss: 0.00002509
Iteration 123/1000 | Loss: 0.00002509
Iteration 124/1000 | Loss: 0.00002509
Iteration 125/1000 | Loss: 0.00002509
Iteration 126/1000 | Loss: 0.00002509
Iteration 127/1000 | Loss: 0.00002509
Iteration 128/1000 | Loss: 0.00002509
Iteration 129/1000 | Loss: 0.00002509
Iteration 130/1000 | Loss: 0.00002509
Iteration 131/1000 | Loss: 0.00002509
Iteration 132/1000 | Loss: 0.00002509
Iteration 133/1000 | Loss: 0.00002508
Iteration 134/1000 | Loss: 0.00002508
Iteration 135/1000 | Loss: 0.00002508
Iteration 136/1000 | Loss: 0.00002508
Iteration 137/1000 | Loss: 0.00002508
Iteration 138/1000 | Loss: 0.00002508
Iteration 139/1000 | Loss: 0.00002508
Iteration 140/1000 | Loss: 0.00002507
Iteration 141/1000 | Loss: 0.00002507
Iteration 142/1000 | Loss: 0.00002507
Iteration 143/1000 | Loss: 0.00002507
Iteration 144/1000 | Loss: 0.00002507
Iteration 145/1000 | Loss: 0.00002507
Iteration 146/1000 | Loss: 0.00002507
Iteration 147/1000 | Loss: 0.00002507
Iteration 148/1000 | Loss: 0.00002507
Iteration 149/1000 | Loss: 0.00002507
Iteration 150/1000 | Loss: 0.00002507
Iteration 151/1000 | Loss: 0.00002507
Iteration 152/1000 | Loss: 0.00002507
Iteration 153/1000 | Loss: 0.00002507
Iteration 154/1000 | Loss: 0.00002507
Iteration 155/1000 | Loss: 0.00002507
Iteration 156/1000 | Loss: 0.00002507
Iteration 157/1000 | Loss: 0.00002507
Iteration 158/1000 | Loss: 0.00002507
Iteration 159/1000 | Loss: 0.00002507
Iteration 160/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.5069686671486124e-05, 2.5069686671486124e-05, 2.5069686671486124e-05, 2.5069686671486124e-05, 2.5069686671486124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5069686671486124e-05

Optimization complete. Final v2v error: 4.148094654083252 mm

Highest mean error: 11.112098693847656 mm for frame 0

Lowest mean error: 3.651937484741211 mm for frame 215

Saving results

Total time: 90.13791823387146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881790
Iteration 2/25 | Loss: 0.00123184
Iteration 3/25 | Loss: 0.00114661
Iteration 4/25 | Loss: 0.00112938
Iteration 5/25 | Loss: 0.00112240
Iteration 6/25 | Loss: 0.00112085
Iteration 7/25 | Loss: 0.00112085
Iteration 8/25 | Loss: 0.00112085
Iteration 9/25 | Loss: 0.00112085
Iteration 10/25 | Loss: 0.00112085
Iteration 11/25 | Loss: 0.00112085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011208488140255213, 0.0011208488140255213, 0.0011208488140255213, 0.0011208488140255213, 0.0011208488140255213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011208488140255213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28468263
Iteration 2/25 | Loss: 0.00150902
Iteration 3/25 | Loss: 0.00150902
Iteration 4/25 | Loss: 0.00150902
Iteration 5/25 | Loss: 0.00150902
Iteration 6/25 | Loss: 0.00150902
Iteration 7/25 | Loss: 0.00150901
Iteration 8/25 | Loss: 0.00150901
Iteration 9/25 | Loss: 0.00150901
Iteration 10/25 | Loss: 0.00150901
Iteration 11/25 | Loss: 0.00150901
Iteration 12/25 | Loss: 0.00150901
Iteration 13/25 | Loss: 0.00150901
Iteration 14/25 | Loss: 0.00150901
Iteration 15/25 | Loss: 0.00150901
Iteration 16/25 | Loss: 0.00150901
Iteration 17/25 | Loss: 0.00150901
Iteration 18/25 | Loss: 0.00150901
Iteration 19/25 | Loss: 0.00150901
Iteration 20/25 | Loss: 0.00150901
Iteration 21/25 | Loss: 0.00150901
Iteration 22/25 | Loss: 0.00150901
Iteration 23/25 | Loss: 0.00150901
Iteration 24/25 | Loss: 0.00150901
Iteration 25/25 | Loss: 0.00150901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150901
Iteration 2/1000 | Loss: 0.00004351
Iteration 3/1000 | Loss: 0.00002385
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001899
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001724
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001609
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001598
Iteration 14/1000 | Loss: 0.00001597
Iteration 15/1000 | Loss: 0.00001595
Iteration 16/1000 | Loss: 0.00001591
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001589
Iteration 19/1000 | Loss: 0.00001587
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001580
Iteration 23/1000 | Loss: 0.00001580
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001579
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001570
Iteration 43/1000 | Loss: 0.00001570
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001570
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001570
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001570
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001570
Iteration 71/1000 | Loss: 0.00001570
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Iteration 78/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.5700072253821418e-05, 1.5700072253821418e-05, 1.5700072253821418e-05, 1.5700072253821418e-05, 1.5700072253821418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5700072253821418e-05

Optimization complete. Final v2v error: 3.4102439880371094 mm

Highest mean error: 3.800294876098633 mm for frame 26

Lowest mean error: 3.1627721786499023 mm for frame 59

Saving results

Total time: 27.943504810333252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434199
Iteration 2/25 | Loss: 0.00132882
Iteration 3/25 | Loss: 0.00122328
Iteration 4/25 | Loss: 0.00120605
Iteration 5/25 | Loss: 0.00119991
Iteration 6/25 | Loss: 0.00119842
Iteration 7/25 | Loss: 0.00119836
Iteration 8/25 | Loss: 0.00119836
Iteration 9/25 | Loss: 0.00119836
Iteration 10/25 | Loss: 0.00119836
Iteration 11/25 | Loss: 0.00119836
Iteration 12/25 | Loss: 0.00119836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011983594158664346, 0.0011983594158664346, 0.0011983594158664346, 0.0011983594158664346, 0.0011983594158664346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011983594158664346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31026316
Iteration 2/25 | Loss: 0.00151231
Iteration 3/25 | Loss: 0.00151231
Iteration 4/25 | Loss: 0.00151230
Iteration 5/25 | Loss: 0.00151230
Iteration 6/25 | Loss: 0.00151230
Iteration 7/25 | Loss: 0.00151230
Iteration 8/25 | Loss: 0.00151230
Iteration 9/25 | Loss: 0.00151230
Iteration 10/25 | Loss: 0.00151230
Iteration 11/25 | Loss: 0.00151230
Iteration 12/25 | Loss: 0.00151230
Iteration 13/25 | Loss: 0.00151230
Iteration 14/25 | Loss: 0.00151230
Iteration 15/25 | Loss: 0.00151230
Iteration 16/25 | Loss: 0.00151230
Iteration 17/25 | Loss: 0.00151230
Iteration 18/25 | Loss: 0.00151230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015123020857572556, 0.0015123020857572556, 0.0015123020857572556, 0.0015123020857572556, 0.0015123020857572556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015123020857572556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151230
Iteration 2/1000 | Loss: 0.00005228
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00003040
Iteration 5/1000 | Loss: 0.00002798
Iteration 6/1000 | Loss: 0.00002675
Iteration 7/1000 | Loss: 0.00002576
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002481
Iteration 10/1000 | Loss: 0.00002446
Iteration 11/1000 | Loss: 0.00002442
Iteration 12/1000 | Loss: 0.00002437
Iteration 13/1000 | Loss: 0.00002433
Iteration 14/1000 | Loss: 0.00002433
Iteration 15/1000 | Loss: 0.00002426
Iteration 16/1000 | Loss: 0.00002425
Iteration 17/1000 | Loss: 0.00002413
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002410
Iteration 20/1000 | Loss: 0.00002410
Iteration 21/1000 | Loss: 0.00002409
Iteration 22/1000 | Loss: 0.00002409
Iteration 23/1000 | Loss: 0.00002408
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002408
Iteration 26/1000 | Loss: 0.00002408
Iteration 27/1000 | Loss: 0.00002408
Iteration 28/1000 | Loss: 0.00002407
Iteration 29/1000 | Loss: 0.00002407
Iteration 30/1000 | Loss: 0.00002407
Iteration 31/1000 | Loss: 0.00002407
Iteration 32/1000 | Loss: 0.00002407
Iteration 33/1000 | Loss: 0.00002407
Iteration 34/1000 | Loss: 0.00002407
Iteration 35/1000 | Loss: 0.00002407
Iteration 36/1000 | Loss: 0.00002406
Iteration 37/1000 | Loss: 0.00002406
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002405
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002405
Iteration 42/1000 | Loss: 0.00002405
Iteration 43/1000 | Loss: 0.00002405
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002405
Iteration 46/1000 | Loss: 0.00002404
Iteration 47/1000 | Loss: 0.00002404
Iteration 48/1000 | Loss: 0.00002404
Iteration 49/1000 | Loss: 0.00002404
Iteration 50/1000 | Loss: 0.00002404
Iteration 51/1000 | Loss: 0.00002404
Iteration 52/1000 | Loss: 0.00002404
Iteration 53/1000 | Loss: 0.00002403
Iteration 54/1000 | Loss: 0.00002403
Iteration 55/1000 | Loss: 0.00002403
Iteration 56/1000 | Loss: 0.00002403
Iteration 57/1000 | Loss: 0.00002402
Iteration 58/1000 | Loss: 0.00002402
Iteration 59/1000 | Loss: 0.00002402
Iteration 60/1000 | Loss: 0.00002402
Iteration 61/1000 | Loss: 0.00002402
Iteration 62/1000 | Loss: 0.00002402
Iteration 63/1000 | Loss: 0.00002402
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00002402
Iteration 66/1000 | Loss: 0.00002402
Iteration 67/1000 | Loss: 0.00002402
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002402
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002402
Iteration 72/1000 | Loss: 0.00002402
Iteration 73/1000 | Loss: 0.00002402
Iteration 74/1000 | Loss: 0.00002402
Iteration 75/1000 | Loss: 0.00002402
Iteration 76/1000 | Loss: 0.00002402
Iteration 77/1000 | Loss: 0.00002402
Iteration 78/1000 | Loss: 0.00002402
Iteration 79/1000 | Loss: 0.00002402
Iteration 80/1000 | Loss: 0.00002402
Iteration 81/1000 | Loss: 0.00002402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.4021797798923217e-05, 2.4021797798923217e-05, 2.4021797798923217e-05, 2.4021797798923217e-05, 2.4021797798923217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4021797798923217e-05

Optimization complete. Final v2v error: 4.210236072540283 mm

Highest mean error: 4.556731700897217 mm for frame 25

Lowest mean error: 3.7999448776245117 mm for frame 45

Saving results

Total time: 29.736857414245605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867393
Iteration 2/25 | Loss: 0.00159309
Iteration 3/25 | Loss: 0.00118915
Iteration 4/25 | Loss: 0.00114547
Iteration 5/25 | Loss: 0.00113990
Iteration 6/25 | Loss: 0.00113868
Iteration 7/25 | Loss: 0.00113836
Iteration 8/25 | Loss: 0.00113836
Iteration 9/25 | Loss: 0.00113836
Iteration 10/25 | Loss: 0.00113836
Iteration 11/25 | Loss: 0.00113836
Iteration 12/25 | Loss: 0.00113836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011383587261661887, 0.0011383587261661887, 0.0011383587261661887, 0.0011383587261661887, 0.0011383587261661887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011383587261661887

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24663222
Iteration 2/25 | Loss: 0.00142599
Iteration 3/25 | Loss: 0.00142599
Iteration 4/25 | Loss: 0.00142599
Iteration 5/25 | Loss: 0.00142599
Iteration 6/25 | Loss: 0.00142599
Iteration 7/25 | Loss: 0.00142599
Iteration 8/25 | Loss: 0.00142599
Iteration 9/25 | Loss: 0.00142599
Iteration 10/25 | Loss: 0.00142599
Iteration 11/25 | Loss: 0.00142599
Iteration 12/25 | Loss: 0.00142599
Iteration 13/25 | Loss: 0.00142599
Iteration 14/25 | Loss: 0.00142599
Iteration 15/25 | Loss: 0.00142599
Iteration 16/25 | Loss: 0.00142599
Iteration 17/25 | Loss: 0.00142599
Iteration 18/25 | Loss: 0.00142599
Iteration 19/25 | Loss: 0.00142599
Iteration 20/25 | Loss: 0.00142599
Iteration 21/25 | Loss: 0.00142599
Iteration 22/25 | Loss: 0.00142599
Iteration 23/25 | Loss: 0.00142599
Iteration 24/25 | Loss: 0.00142599
Iteration 25/25 | Loss: 0.00142599

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142599
Iteration 2/1000 | Loss: 0.00004875
Iteration 3/1000 | Loss: 0.00002754
Iteration 4/1000 | Loss: 0.00002087
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001654
Iteration 7/1000 | Loss: 0.00001566
Iteration 8/1000 | Loss: 0.00001500
Iteration 9/1000 | Loss: 0.00001468
Iteration 10/1000 | Loss: 0.00001432
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001394
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001391
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001379
Iteration 19/1000 | Loss: 0.00001376
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001375
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001374
Iteration 24/1000 | Loss: 0.00001374
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001371
Iteration 34/1000 | Loss: 0.00001371
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001368
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001368
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001367
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001367
Iteration 53/1000 | Loss: 0.00001367
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001367
Iteration 56/1000 | Loss: 0.00001367
Iteration 57/1000 | Loss: 0.00001366
Iteration 58/1000 | Loss: 0.00001366
Iteration 59/1000 | Loss: 0.00001366
Iteration 60/1000 | Loss: 0.00001366
Iteration 61/1000 | Loss: 0.00001366
Iteration 62/1000 | Loss: 0.00001365
Iteration 63/1000 | Loss: 0.00001365
Iteration 64/1000 | Loss: 0.00001365
Iteration 65/1000 | Loss: 0.00001365
Iteration 66/1000 | Loss: 0.00001365
Iteration 67/1000 | Loss: 0.00001365
Iteration 68/1000 | Loss: 0.00001364
Iteration 69/1000 | Loss: 0.00001364
Iteration 70/1000 | Loss: 0.00001364
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001363
Iteration 73/1000 | Loss: 0.00001363
Iteration 74/1000 | Loss: 0.00001363
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001362
Iteration 80/1000 | Loss: 0.00001362
Iteration 81/1000 | Loss: 0.00001362
Iteration 82/1000 | Loss: 0.00001362
Iteration 83/1000 | Loss: 0.00001361
Iteration 84/1000 | Loss: 0.00001361
Iteration 85/1000 | Loss: 0.00001361
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001361
Iteration 90/1000 | Loss: 0.00001360
Iteration 91/1000 | Loss: 0.00001360
Iteration 92/1000 | Loss: 0.00001360
Iteration 93/1000 | Loss: 0.00001360
Iteration 94/1000 | Loss: 0.00001360
Iteration 95/1000 | Loss: 0.00001360
Iteration 96/1000 | Loss: 0.00001360
Iteration 97/1000 | Loss: 0.00001360
Iteration 98/1000 | Loss: 0.00001360
Iteration 99/1000 | Loss: 0.00001360
Iteration 100/1000 | Loss: 0.00001359
Iteration 101/1000 | Loss: 0.00001359
Iteration 102/1000 | Loss: 0.00001359
Iteration 103/1000 | Loss: 0.00001359
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001359
Iteration 106/1000 | Loss: 0.00001359
Iteration 107/1000 | Loss: 0.00001359
Iteration 108/1000 | Loss: 0.00001359
Iteration 109/1000 | Loss: 0.00001359
Iteration 110/1000 | Loss: 0.00001359
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001357
Iteration 115/1000 | Loss: 0.00001357
Iteration 116/1000 | Loss: 0.00001357
Iteration 117/1000 | Loss: 0.00001357
Iteration 118/1000 | Loss: 0.00001357
Iteration 119/1000 | Loss: 0.00001356
Iteration 120/1000 | Loss: 0.00001356
Iteration 121/1000 | Loss: 0.00001356
Iteration 122/1000 | Loss: 0.00001356
Iteration 123/1000 | Loss: 0.00001356
Iteration 124/1000 | Loss: 0.00001356
Iteration 125/1000 | Loss: 0.00001356
Iteration 126/1000 | Loss: 0.00001356
Iteration 127/1000 | Loss: 0.00001356
Iteration 128/1000 | Loss: 0.00001356
Iteration 129/1000 | Loss: 0.00001356
Iteration 130/1000 | Loss: 0.00001355
Iteration 131/1000 | Loss: 0.00001355
Iteration 132/1000 | Loss: 0.00001355
Iteration 133/1000 | Loss: 0.00001355
Iteration 134/1000 | Loss: 0.00001355
Iteration 135/1000 | Loss: 0.00001355
Iteration 136/1000 | Loss: 0.00001355
Iteration 137/1000 | Loss: 0.00001355
Iteration 138/1000 | Loss: 0.00001355
Iteration 139/1000 | Loss: 0.00001354
Iteration 140/1000 | Loss: 0.00001354
Iteration 141/1000 | Loss: 0.00001354
Iteration 142/1000 | Loss: 0.00001354
Iteration 143/1000 | Loss: 0.00001354
Iteration 144/1000 | Loss: 0.00001354
Iteration 145/1000 | Loss: 0.00001354
Iteration 146/1000 | Loss: 0.00001354
Iteration 147/1000 | Loss: 0.00001354
Iteration 148/1000 | Loss: 0.00001354
Iteration 149/1000 | Loss: 0.00001354
Iteration 150/1000 | Loss: 0.00001354
Iteration 151/1000 | Loss: 0.00001354
Iteration 152/1000 | Loss: 0.00001354
Iteration 153/1000 | Loss: 0.00001354
Iteration 154/1000 | Loss: 0.00001354
Iteration 155/1000 | Loss: 0.00001354
Iteration 156/1000 | Loss: 0.00001354
Iteration 157/1000 | Loss: 0.00001354
Iteration 158/1000 | Loss: 0.00001354
Iteration 159/1000 | Loss: 0.00001354
Iteration 160/1000 | Loss: 0.00001354
Iteration 161/1000 | Loss: 0.00001354
Iteration 162/1000 | Loss: 0.00001354
Iteration 163/1000 | Loss: 0.00001354
Iteration 164/1000 | Loss: 0.00001354
Iteration 165/1000 | Loss: 0.00001354
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001354
Iteration 180/1000 | Loss: 0.00001354
Iteration 181/1000 | Loss: 0.00001354
Iteration 182/1000 | Loss: 0.00001354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.3540181498683523e-05, 1.3540181498683523e-05, 1.3540181498683523e-05, 1.3540181498683523e-05, 1.3540181498683523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3540181498683523e-05

Optimization complete. Final v2v error: 3.204317331314087 mm

Highest mean error: 3.4453253746032715 mm for frame 146

Lowest mean error: 3.10396146774292 mm for frame 115

Saving results

Total time: 39.478328704833984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880262
Iteration 2/25 | Loss: 0.00153424
Iteration 3/25 | Loss: 0.00122772
Iteration 4/25 | Loss: 0.00119816
Iteration 5/25 | Loss: 0.00119356
Iteration 6/25 | Loss: 0.00119326
Iteration 7/25 | Loss: 0.00119326
Iteration 8/25 | Loss: 0.00119326
Iteration 9/25 | Loss: 0.00119326
Iteration 10/25 | Loss: 0.00119326
Iteration 11/25 | Loss: 0.00119326
Iteration 12/25 | Loss: 0.00119326
Iteration 13/25 | Loss: 0.00119326
Iteration 14/25 | Loss: 0.00119326
Iteration 15/25 | Loss: 0.00119326
Iteration 16/25 | Loss: 0.00119326
Iteration 17/25 | Loss: 0.00119326
Iteration 18/25 | Loss: 0.00119326
Iteration 19/25 | Loss: 0.00119326
Iteration 20/25 | Loss: 0.00119326
Iteration 21/25 | Loss: 0.00119326
Iteration 22/25 | Loss: 0.00119326
Iteration 23/25 | Loss: 0.00119326
Iteration 24/25 | Loss: 0.00119326
Iteration 25/25 | Loss: 0.00119326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22706461
Iteration 2/25 | Loss: 0.00140251
Iteration 3/25 | Loss: 0.00140251
Iteration 4/25 | Loss: 0.00140251
Iteration 5/25 | Loss: 0.00140251
Iteration 6/25 | Loss: 0.00140251
Iteration 7/25 | Loss: 0.00140251
Iteration 8/25 | Loss: 0.00140251
Iteration 9/25 | Loss: 0.00140251
Iteration 10/25 | Loss: 0.00140251
Iteration 11/25 | Loss: 0.00140251
Iteration 12/25 | Loss: 0.00140251
Iteration 13/25 | Loss: 0.00140251
Iteration 14/25 | Loss: 0.00140251
Iteration 15/25 | Loss: 0.00140251
Iteration 16/25 | Loss: 0.00140251
Iteration 17/25 | Loss: 0.00140251
Iteration 18/25 | Loss: 0.00140251
Iteration 19/25 | Loss: 0.00140251
Iteration 20/25 | Loss: 0.00140251
Iteration 21/25 | Loss: 0.00140251
Iteration 22/25 | Loss: 0.00140251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014025107957422733, 0.0014025107957422733, 0.0014025107957422733, 0.0014025107957422733, 0.0014025107957422733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014025107957422733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140251
Iteration 2/1000 | Loss: 0.00006295
Iteration 3/1000 | Loss: 0.00003314
Iteration 4/1000 | Loss: 0.00002644
Iteration 5/1000 | Loss: 0.00002308
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002057
Iteration 8/1000 | Loss: 0.00002009
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001942
Iteration 11/1000 | Loss: 0.00001924
Iteration 12/1000 | Loss: 0.00001905
Iteration 13/1000 | Loss: 0.00001885
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001876
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001872
Iteration 18/1000 | Loss: 0.00001872
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001870
Iteration 22/1000 | Loss: 0.00001870
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001869
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001869
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001867
Iteration 32/1000 | Loss: 0.00001866
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001866
Iteration 35/1000 | Loss: 0.00001866
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001865
Iteration 41/1000 | Loss: 0.00001865
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001864
Iteration 45/1000 | Loss: 0.00001864
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001861
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001854
Iteration 62/1000 | Loss: 0.00001854
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00001853
Iteration 67/1000 | Loss: 0.00001853
Iteration 68/1000 | Loss: 0.00001852
Iteration 69/1000 | Loss: 0.00001852
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001850
Iteration 72/1000 | Loss: 0.00001850
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001850
Iteration 75/1000 | Loss: 0.00001850
Iteration 76/1000 | Loss: 0.00001850
Iteration 77/1000 | Loss: 0.00001850
Iteration 78/1000 | Loss: 0.00001850
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001849
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001849
Iteration 89/1000 | Loss: 0.00001848
Iteration 90/1000 | Loss: 0.00001848
Iteration 91/1000 | Loss: 0.00001848
Iteration 92/1000 | Loss: 0.00001848
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001847
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001847
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001846
Iteration 102/1000 | Loss: 0.00001846
Iteration 103/1000 | Loss: 0.00001846
Iteration 104/1000 | Loss: 0.00001846
Iteration 105/1000 | Loss: 0.00001846
Iteration 106/1000 | Loss: 0.00001846
Iteration 107/1000 | Loss: 0.00001845
Iteration 108/1000 | Loss: 0.00001845
Iteration 109/1000 | Loss: 0.00001844
Iteration 110/1000 | Loss: 0.00001844
Iteration 111/1000 | Loss: 0.00001844
Iteration 112/1000 | Loss: 0.00001844
Iteration 113/1000 | Loss: 0.00001844
Iteration 114/1000 | Loss: 0.00001844
Iteration 115/1000 | Loss: 0.00001844
Iteration 116/1000 | Loss: 0.00001844
Iteration 117/1000 | Loss: 0.00001844
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001843
Iteration 120/1000 | Loss: 0.00001843
Iteration 121/1000 | Loss: 0.00001843
Iteration 122/1000 | Loss: 0.00001843
Iteration 123/1000 | Loss: 0.00001843
Iteration 124/1000 | Loss: 0.00001843
Iteration 125/1000 | Loss: 0.00001843
Iteration 126/1000 | Loss: 0.00001843
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001842
Iteration 132/1000 | Loss: 0.00001842
Iteration 133/1000 | Loss: 0.00001842
Iteration 134/1000 | Loss: 0.00001842
Iteration 135/1000 | Loss: 0.00001842
Iteration 136/1000 | Loss: 0.00001842
Iteration 137/1000 | Loss: 0.00001842
Iteration 138/1000 | Loss: 0.00001842
Iteration 139/1000 | Loss: 0.00001841
Iteration 140/1000 | Loss: 0.00001841
Iteration 141/1000 | Loss: 0.00001841
Iteration 142/1000 | Loss: 0.00001841
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001841
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001841
Iteration 149/1000 | Loss: 0.00001841
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.8407334209769033e-05, 1.8407334209769033e-05, 1.8407334209769033e-05, 1.8407334209769033e-05, 1.8407334209769033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8407334209769033e-05

Optimization complete. Final v2v error: 3.735248327255249 mm

Highest mean error: 4.119143009185791 mm for frame 55

Lowest mean error: 3.381669759750366 mm for frame 12

Saving results

Total time: 39.65972185134888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444588
Iteration 2/25 | Loss: 0.00126157
Iteration 3/25 | Loss: 0.00118422
Iteration 4/25 | Loss: 0.00117337
Iteration 5/25 | Loss: 0.00116988
Iteration 6/25 | Loss: 0.00116934
Iteration 7/25 | Loss: 0.00116934
Iteration 8/25 | Loss: 0.00116934
Iteration 9/25 | Loss: 0.00116934
Iteration 10/25 | Loss: 0.00116934
Iteration 11/25 | Loss: 0.00116934
Iteration 12/25 | Loss: 0.00116934
Iteration 13/25 | Loss: 0.00116934
Iteration 14/25 | Loss: 0.00116934
Iteration 15/25 | Loss: 0.00116934
Iteration 16/25 | Loss: 0.00116934
Iteration 17/25 | Loss: 0.00116934
Iteration 18/25 | Loss: 0.00116934
Iteration 19/25 | Loss: 0.00116934
Iteration 20/25 | Loss: 0.00116934
Iteration 21/25 | Loss: 0.00116934
Iteration 22/25 | Loss: 0.00116934
Iteration 23/25 | Loss: 0.00116934
Iteration 24/25 | Loss: 0.00116934
Iteration 25/25 | Loss: 0.00116934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25493515
Iteration 2/25 | Loss: 0.00158972
Iteration 3/25 | Loss: 0.00158972
Iteration 4/25 | Loss: 0.00158972
Iteration 5/25 | Loss: 0.00158972
Iteration 6/25 | Loss: 0.00158972
Iteration 7/25 | Loss: 0.00158972
Iteration 8/25 | Loss: 0.00158972
Iteration 9/25 | Loss: 0.00158972
Iteration 10/25 | Loss: 0.00158972
Iteration 11/25 | Loss: 0.00158971
Iteration 12/25 | Loss: 0.00158971
Iteration 13/25 | Loss: 0.00158971
Iteration 14/25 | Loss: 0.00158971
Iteration 15/25 | Loss: 0.00158971
Iteration 16/25 | Loss: 0.00158971
Iteration 17/25 | Loss: 0.00158971
Iteration 18/25 | Loss: 0.00158971
Iteration 19/25 | Loss: 0.00158971
Iteration 20/25 | Loss: 0.00158971
Iteration 21/25 | Loss: 0.00158971
Iteration 22/25 | Loss: 0.00158971
Iteration 23/25 | Loss: 0.00158971
Iteration 24/25 | Loss: 0.00158971
Iteration 25/25 | Loss: 0.00158971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158971
Iteration 2/1000 | Loss: 0.00004319
Iteration 3/1000 | Loss: 0.00002898
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00002331
Iteration 6/1000 | Loss: 0.00002192
Iteration 7/1000 | Loss: 0.00002141
Iteration 8/1000 | Loss: 0.00002077
Iteration 9/1000 | Loss: 0.00002035
Iteration 10/1000 | Loss: 0.00002009
Iteration 11/1000 | Loss: 0.00002006
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001994
Iteration 15/1000 | Loss: 0.00001987
Iteration 16/1000 | Loss: 0.00001986
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001982
Iteration 19/1000 | Loss: 0.00001982
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001981
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001979
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001972
Iteration 26/1000 | Loss: 0.00001971
Iteration 27/1000 | Loss: 0.00001970
Iteration 28/1000 | Loss: 0.00001970
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001970
Iteration 31/1000 | Loss: 0.00001969
Iteration 32/1000 | Loss: 0.00001969
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001965
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001961
Iteration 50/1000 | Loss: 0.00001961
Iteration 51/1000 | Loss: 0.00001961
Iteration 52/1000 | Loss: 0.00001961
Iteration 53/1000 | Loss: 0.00001960
Iteration 54/1000 | Loss: 0.00001960
Iteration 55/1000 | Loss: 0.00001960
Iteration 56/1000 | Loss: 0.00001959
Iteration 57/1000 | Loss: 0.00001959
Iteration 58/1000 | Loss: 0.00001959
Iteration 59/1000 | Loss: 0.00001958
Iteration 60/1000 | Loss: 0.00001958
Iteration 61/1000 | Loss: 0.00001958
Iteration 62/1000 | Loss: 0.00001957
Iteration 63/1000 | Loss: 0.00001957
Iteration 64/1000 | Loss: 0.00001956
Iteration 65/1000 | Loss: 0.00001956
Iteration 66/1000 | Loss: 0.00001956
Iteration 67/1000 | Loss: 0.00001956
Iteration 68/1000 | Loss: 0.00001956
Iteration 69/1000 | Loss: 0.00001956
Iteration 70/1000 | Loss: 0.00001956
Iteration 71/1000 | Loss: 0.00001956
Iteration 72/1000 | Loss: 0.00001956
Iteration 73/1000 | Loss: 0.00001956
Iteration 74/1000 | Loss: 0.00001956
Iteration 75/1000 | Loss: 0.00001955
Iteration 76/1000 | Loss: 0.00001955
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001955
Iteration 80/1000 | Loss: 0.00001955
Iteration 81/1000 | Loss: 0.00001954
Iteration 82/1000 | Loss: 0.00001954
Iteration 83/1000 | Loss: 0.00001954
Iteration 84/1000 | Loss: 0.00001954
Iteration 85/1000 | Loss: 0.00001953
Iteration 86/1000 | Loss: 0.00001953
Iteration 87/1000 | Loss: 0.00001953
Iteration 88/1000 | Loss: 0.00001953
Iteration 89/1000 | Loss: 0.00001953
Iteration 90/1000 | Loss: 0.00001953
Iteration 91/1000 | Loss: 0.00001953
Iteration 92/1000 | Loss: 0.00001953
Iteration 93/1000 | Loss: 0.00001953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.952823186002206e-05, 1.952823186002206e-05, 1.952823186002206e-05, 1.952823186002206e-05, 1.952823186002206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.952823186002206e-05

Optimization complete. Final v2v error: 3.813981294631958 mm

Highest mean error: 4.032710552215576 mm for frame 6

Lowest mean error: 3.5684456825256348 mm for frame 154

Saving results

Total time: 32.32368755340576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054041
Iteration 2/25 | Loss: 0.00254707
Iteration 3/25 | Loss: 0.00214781
Iteration 4/25 | Loss: 0.00202353
Iteration 5/25 | Loss: 0.00194498
Iteration 6/25 | Loss: 0.00164355
Iteration 7/25 | Loss: 0.00148406
Iteration 8/25 | Loss: 0.00143344
Iteration 9/25 | Loss: 0.00140487
Iteration 10/25 | Loss: 0.00140003
Iteration 11/25 | Loss: 0.00139786
Iteration 12/25 | Loss: 0.00139703
Iteration 13/25 | Loss: 0.00139613
Iteration 14/25 | Loss: 0.00140782
Iteration 15/25 | Loss: 0.00141028
Iteration 16/25 | Loss: 0.00139792
Iteration 17/25 | Loss: 0.00139270
Iteration 18/25 | Loss: 0.00139093
Iteration 19/25 | Loss: 0.00139060
Iteration 20/25 | Loss: 0.00139032
Iteration 21/25 | Loss: 0.00139027
Iteration 22/25 | Loss: 0.00139027
Iteration 23/25 | Loss: 0.00139027
Iteration 24/25 | Loss: 0.00139027
Iteration 25/25 | Loss: 0.00139027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22354662
Iteration 2/25 | Loss: 0.00184309
Iteration 3/25 | Loss: 0.00172177
Iteration 4/25 | Loss: 0.00172177
Iteration 5/25 | Loss: 0.00172177
Iteration 6/25 | Loss: 0.00172177
Iteration 7/25 | Loss: 0.00172177
Iteration 8/25 | Loss: 0.00172177
Iteration 9/25 | Loss: 0.00172177
Iteration 10/25 | Loss: 0.00172176
Iteration 11/25 | Loss: 0.00172176
Iteration 12/25 | Loss: 0.00172176
Iteration 13/25 | Loss: 0.00172176
Iteration 14/25 | Loss: 0.00172176
Iteration 15/25 | Loss: 0.00172176
Iteration 16/25 | Loss: 0.00172176
Iteration 17/25 | Loss: 0.00172176
Iteration 18/25 | Loss: 0.00172176
Iteration 19/25 | Loss: 0.00172176
Iteration 20/25 | Loss: 0.00172176
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0017217636341229081, 0.0017217636341229081, 0.0017217636341229081, 0.0017217636341229081, 0.0017217636341229081]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017217636341229081

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172176
Iteration 2/1000 | Loss: 0.00171634
Iteration 3/1000 | Loss: 0.00073502
Iteration 4/1000 | Loss: 0.00014802
Iteration 5/1000 | Loss: 0.00049079
Iteration 6/1000 | Loss: 0.00186288
Iteration 7/1000 | Loss: 0.00088876
Iteration 8/1000 | Loss: 0.00048237
Iteration 9/1000 | Loss: 0.00067090
Iteration 10/1000 | Loss: 0.00033992
Iteration 11/1000 | Loss: 0.00046388
Iteration 12/1000 | Loss: 0.00015246
Iteration 13/1000 | Loss: 0.00013164
Iteration 14/1000 | Loss: 0.00173704
Iteration 15/1000 | Loss: 0.00008704
Iteration 16/1000 | Loss: 0.00007565
Iteration 17/1000 | Loss: 0.00006849
Iteration 18/1000 | Loss: 0.00006317
Iteration 19/1000 | Loss: 0.00005894
Iteration 20/1000 | Loss: 0.00005604
Iteration 21/1000 | Loss: 0.00005357
Iteration 22/1000 | Loss: 0.00005157
Iteration 23/1000 | Loss: 0.00018162
Iteration 24/1000 | Loss: 0.00017719
Iteration 25/1000 | Loss: 0.00006549
Iteration 26/1000 | Loss: 0.00005397
Iteration 27/1000 | Loss: 0.00005034
Iteration 28/1000 | Loss: 0.00004777
Iteration 29/1000 | Loss: 0.00004678
Iteration 30/1000 | Loss: 0.00004610
Iteration 31/1000 | Loss: 0.00004509
Iteration 32/1000 | Loss: 0.00015264
Iteration 33/1000 | Loss: 0.00005403
Iteration 34/1000 | Loss: 0.00004780
Iteration 35/1000 | Loss: 0.00004538
Iteration 36/1000 | Loss: 0.00004422
Iteration 37/1000 | Loss: 0.00004334
Iteration 38/1000 | Loss: 0.00004274
Iteration 39/1000 | Loss: 0.00004238
Iteration 40/1000 | Loss: 0.00004220
Iteration 41/1000 | Loss: 0.00004208
Iteration 42/1000 | Loss: 0.00004207
Iteration 43/1000 | Loss: 0.00004200
Iteration 44/1000 | Loss: 0.00004194
Iteration 45/1000 | Loss: 0.00004190
Iteration 46/1000 | Loss: 0.00004190
Iteration 47/1000 | Loss: 0.00004190
Iteration 48/1000 | Loss: 0.00004190
Iteration 49/1000 | Loss: 0.00004189
Iteration 50/1000 | Loss: 0.00004189
Iteration 51/1000 | Loss: 0.00004189
Iteration 52/1000 | Loss: 0.00004189
Iteration 53/1000 | Loss: 0.00004188
Iteration 54/1000 | Loss: 0.00004188
Iteration 55/1000 | Loss: 0.00004188
Iteration 56/1000 | Loss: 0.00004187
Iteration 57/1000 | Loss: 0.00004187
Iteration 58/1000 | Loss: 0.00004187
Iteration 59/1000 | Loss: 0.00004187
Iteration 60/1000 | Loss: 0.00004187
Iteration 61/1000 | Loss: 0.00004187
Iteration 62/1000 | Loss: 0.00004186
Iteration 63/1000 | Loss: 0.00004186
Iteration 64/1000 | Loss: 0.00004185
Iteration 65/1000 | Loss: 0.00004185
Iteration 66/1000 | Loss: 0.00004184
Iteration 67/1000 | Loss: 0.00004184
Iteration 68/1000 | Loss: 0.00004183
Iteration 69/1000 | Loss: 0.00004180
Iteration 70/1000 | Loss: 0.00004179
Iteration 71/1000 | Loss: 0.00004177
Iteration 72/1000 | Loss: 0.00004177
Iteration 73/1000 | Loss: 0.00004177
Iteration 74/1000 | Loss: 0.00004177
Iteration 75/1000 | Loss: 0.00004177
Iteration 76/1000 | Loss: 0.00004177
Iteration 77/1000 | Loss: 0.00004177
Iteration 78/1000 | Loss: 0.00004177
Iteration 79/1000 | Loss: 0.00004176
Iteration 80/1000 | Loss: 0.00004176
Iteration 81/1000 | Loss: 0.00004176
Iteration 82/1000 | Loss: 0.00004176
Iteration 83/1000 | Loss: 0.00004175
Iteration 84/1000 | Loss: 0.00004175
Iteration 85/1000 | Loss: 0.00004175
Iteration 86/1000 | Loss: 0.00004174
Iteration 87/1000 | Loss: 0.00004174
Iteration 88/1000 | Loss: 0.00004174
Iteration 89/1000 | Loss: 0.00004174
Iteration 90/1000 | Loss: 0.00004174
Iteration 91/1000 | Loss: 0.00004173
Iteration 92/1000 | Loss: 0.00004173
Iteration 93/1000 | Loss: 0.00004173
Iteration 94/1000 | Loss: 0.00004173
Iteration 95/1000 | Loss: 0.00004173
Iteration 96/1000 | Loss: 0.00004173
Iteration 97/1000 | Loss: 0.00004173
Iteration 98/1000 | Loss: 0.00004173
Iteration 99/1000 | Loss: 0.00004173
Iteration 100/1000 | Loss: 0.00004173
Iteration 101/1000 | Loss: 0.00004173
Iteration 102/1000 | Loss: 0.00004173
Iteration 103/1000 | Loss: 0.00004173
Iteration 104/1000 | Loss: 0.00004172
Iteration 105/1000 | Loss: 0.00004172
Iteration 106/1000 | Loss: 0.00004172
Iteration 107/1000 | Loss: 0.00004171
Iteration 108/1000 | Loss: 0.00004171
Iteration 109/1000 | Loss: 0.00004171
Iteration 110/1000 | Loss: 0.00004171
Iteration 111/1000 | Loss: 0.00004171
Iteration 112/1000 | Loss: 0.00004171
Iteration 113/1000 | Loss: 0.00004171
Iteration 114/1000 | Loss: 0.00004171
Iteration 115/1000 | Loss: 0.00004171
Iteration 116/1000 | Loss: 0.00004170
Iteration 117/1000 | Loss: 0.00004170
Iteration 118/1000 | Loss: 0.00004170
Iteration 119/1000 | Loss: 0.00004170
Iteration 120/1000 | Loss: 0.00004169
Iteration 121/1000 | Loss: 0.00004169
Iteration 122/1000 | Loss: 0.00004169
Iteration 123/1000 | Loss: 0.00004169
Iteration 124/1000 | Loss: 0.00004169
Iteration 125/1000 | Loss: 0.00004168
Iteration 126/1000 | Loss: 0.00004168
Iteration 127/1000 | Loss: 0.00004168
Iteration 128/1000 | Loss: 0.00004168
Iteration 129/1000 | Loss: 0.00004168
Iteration 130/1000 | Loss: 0.00004168
Iteration 131/1000 | Loss: 0.00004168
Iteration 132/1000 | Loss: 0.00004168
Iteration 133/1000 | Loss: 0.00004168
Iteration 134/1000 | Loss: 0.00004168
Iteration 135/1000 | Loss: 0.00004167
Iteration 136/1000 | Loss: 0.00004167
Iteration 137/1000 | Loss: 0.00004167
Iteration 138/1000 | Loss: 0.00004167
Iteration 139/1000 | Loss: 0.00004167
Iteration 140/1000 | Loss: 0.00004167
Iteration 141/1000 | Loss: 0.00004167
Iteration 142/1000 | Loss: 0.00004167
Iteration 143/1000 | Loss: 0.00004167
Iteration 144/1000 | Loss: 0.00004167
Iteration 145/1000 | Loss: 0.00004166
Iteration 146/1000 | Loss: 0.00004166
Iteration 147/1000 | Loss: 0.00004166
Iteration 148/1000 | Loss: 0.00004166
Iteration 149/1000 | Loss: 0.00004166
Iteration 150/1000 | Loss: 0.00004166
Iteration 151/1000 | Loss: 0.00004166
Iteration 152/1000 | Loss: 0.00004166
Iteration 153/1000 | Loss: 0.00004166
Iteration 154/1000 | Loss: 0.00004166
Iteration 155/1000 | Loss: 0.00004166
Iteration 156/1000 | Loss: 0.00004166
Iteration 157/1000 | Loss: 0.00004166
Iteration 158/1000 | Loss: 0.00004166
Iteration 159/1000 | Loss: 0.00004166
Iteration 160/1000 | Loss: 0.00004166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [4.166077633271925e-05, 4.166077633271925e-05, 4.166077633271925e-05, 4.166077633271925e-05, 4.166077633271925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.166077633271925e-05

Optimization complete. Final v2v error: 5.008666038513184 mm

Highest mean error: 12.0678129196167 mm for frame 219

Lowest mean error: 4.52131986618042 mm for frame 112

Saving results

Total time: 119.1399519443512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100911
Iteration 2/25 | Loss: 0.00171765
Iteration 3/25 | Loss: 0.00136004
Iteration 4/25 | Loss: 0.00130238
Iteration 5/25 | Loss: 0.00133696
Iteration 6/25 | Loss: 0.00130715
Iteration 7/25 | Loss: 0.00126777
Iteration 8/25 | Loss: 0.00123850
Iteration 9/25 | Loss: 0.00122850
Iteration 10/25 | Loss: 0.00122058
Iteration 11/25 | Loss: 0.00121483
Iteration 12/25 | Loss: 0.00121707
Iteration 13/25 | Loss: 0.00121775
Iteration 14/25 | Loss: 0.00121671
Iteration 15/25 | Loss: 0.00121541
Iteration 16/25 | Loss: 0.00121664
Iteration 17/25 | Loss: 0.00121592
Iteration 18/25 | Loss: 0.00121602
Iteration 19/25 | Loss: 0.00121857
Iteration 20/25 | Loss: 0.00121504
Iteration 21/25 | Loss: 0.00121868
Iteration 22/25 | Loss: 0.00121740
Iteration 23/25 | Loss: 0.00121407
Iteration 24/25 | Loss: 0.00121693
Iteration 25/25 | Loss: 0.00122112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48268294
Iteration 2/25 | Loss: 0.00148907
Iteration 3/25 | Loss: 0.00148905
Iteration 4/25 | Loss: 0.00148905
Iteration 5/25 | Loss: 0.00148905
Iteration 6/25 | Loss: 0.00148905
Iteration 7/25 | Loss: 0.00148905
Iteration 8/25 | Loss: 0.00148905
Iteration 9/25 | Loss: 0.00148905
Iteration 10/25 | Loss: 0.00148905
Iteration 11/25 | Loss: 0.00148905
Iteration 12/25 | Loss: 0.00148905
Iteration 13/25 | Loss: 0.00148905
Iteration 14/25 | Loss: 0.00148905
Iteration 15/25 | Loss: 0.00148905
Iteration 16/25 | Loss: 0.00148905
Iteration 17/25 | Loss: 0.00148905
Iteration 18/25 | Loss: 0.00148905
Iteration 19/25 | Loss: 0.00148905
Iteration 20/25 | Loss: 0.00148905
Iteration 21/25 | Loss: 0.00148905
Iteration 22/25 | Loss: 0.00148905
Iteration 23/25 | Loss: 0.00148905
Iteration 24/25 | Loss: 0.00148905
Iteration 25/25 | Loss: 0.00148905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148905
Iteration 2/1000 | Loss: 0.00006326
Iteration 3/1000 | Loss: 0.00003794
Iteration 4/1000 | Loss: 0.00003152
Iteration 5/1000 | Loss: 0.00002862
Iteration 6/1000 | Loss: 0.00002647
Iteration 7/1000 | Loss: 0.00002534
Iteration 8/1000 | Loss: 0.00002380
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002110
Iteration 13/1000 | Loss: 0.00002104
Iteration 14/1000 | Loss: 0.00002085
Iteration 15/1000 | Loss: 0.00002076
Iteration 16/1000 | Loss: 0.00002075
Iteration 17/1000 | Loss: 0.00002074
Iteration 18/1000 | Loss: 0.00002070
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002054
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002054
Iteration 27/1000 | Loss: 0.00002053
Iteration 28/1000 | Loss: 0.00002053
Iteration 29/1000 | Loss: 0.00002052
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002050
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002049
Iteration 40/1000 | Loss: 0.00002049
Iteration 41/1000 | Loss: 0.00002049
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002048
Iteration 44/1000 | Loss: 0.00002048
Iteration 45/1000 | Loss: 0.00002048
Iteration 46/1000 | Loss: 0.00002048
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002047
Iteration 49/1000 | Loss: 0.00002047
Iteration 50/1000 | Loss: 0.00002047
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002047
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002046
Iteration 57/1000 | Loss: 0.00002045
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002044
Iteration 61/1000 | Loss: 0.00002044
Iteration 62/1000 | Loss: 0.00002043
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00002043
Iteration 66/1000 | Loss: 0.00002042
Iteration 67/1000 | Loss: 0.00002042
Iteration 68/1000 | Loss: 0.00002042
Iteration 69/1000 | Loss: 0.00002042
Iteration 70/1000 | Loss: 0.00002042
Iteration 71/1000 | Loss: 0.00002042
Iteration 72/1000 | Loss: 0.00002041
Iteration 73/1000 | Loss: 0.00002041
Iteration 74/1000 | Loss: 0.00002041
Iteration 75/1000 | Loss: 0.00002041
Iteration 76/1000 | Loss: 0.00002041
Iteration 77/1000 | Loss: 0.00002041
Iteration 78/1000 | Loss: 0.00002040
Iteration 79/1000 | Loss: 0.00002040
Iteration 80/1000 | Loss: 0.00002040
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002040
Iteration 85/1000 | Loss: 0.00002039
Iteration 86/1000 | Loss: 0.00002039
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002039
Iteration 89/1000 | Loss: 0.00002039
Iteration 90/1000 | Loss: 0.00002039
Iteration 91/1000 | Loss: 0.00002039
Iteration 92/1000 | Loss: 0.00002039
Iteration 93/1000 | Loss: 0.00002038
Iteration 94/1000 | Loss: 0.00002038
Iteration 95/1000 | Loss: 0.00002038
Iteration 96/1000 | Loss: 0.00002038
Iteration 97/1000 | Loss: 0.00002038
Iteration 98/1000 | Loss: 0.00002038
Iteration 99/1000 | Loss: 0.00002038
Iteration 100/1000 | Loss: 0.00002038
Iteration 101/1000 | Loss: 0.00002038
Iteration 102/1000 | Loss: 0.00002038
Iteration 103/1000 | Loss: 0.00002037
Iteration 104/1000 | Loss: 0.00002037
Iteration 105/1000 | Loss: 0.00002037
Iteration 106/1000 | Loss: 0.00002037
Iteration 107/1000 | Loss: 0.00002037
Iteration 108/1000 | Loss: 0.00002037
Iteration 109/1000 | Loss: 0.00002037
Iteration 110/1000 | Loss: 0.00002036
Iteration 111/1000 | Loss: 0.00002036
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002036
Iteration 114/1000 | Loss: 0.00002036
Iteration 115/1000 | Loss: 0.00002036
Iteration 116/1000 | Loss: 0.00002036
Iteration 117/1000 | Loss: 0.00002035
Iteration 118/1000 | Loss: 0.00002035
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002034
Iteration 126/1000 | Loss: 0.00002034
Iteration 127/1000 | Loss: 0.00002034
Iteration 128/1000 | Loss: 0.00002034
Iteration 129/1000 | Loss: 0.00002034
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002034
Iteration 141/1000 | Loss: 0.00002034
Iteration 142/1000 | Loss: 0.00002034
Iteration 143/1000 | Loss: 0.00002034
Iteration 144/1000 | Loss: 0.00002034
Iteration 145/1000 | Loss: 0.00002034
Iteration 146/1000 | Loss: 0.00002034
Iteration 147/1000 | Loss: 0.00002034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.0337716705398634e-05, 2.0337716705398634e-05, 2.0337716705398634e-05, 2.0337716705398634e-05, 2.0337716705398634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0337716705398634e-05

Optimization complete. Final v2v error: 3.652557611465454 mm

Highest mean error: 9.692520141601562 mm for frame 18

Lowest mean error: 3.275747060775757 mm for frame 55

Saving results

Total time: 75.63294577598572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00618002
Iteration 2/25 | Loss: 0.00151942
Iteration 3/25 | Loss: 0.00115890
Iteration 4/25 | Loss: 0.00108512
Iteration 5/25 | Loss: 0.00106004
Iteration 6/25 | Loss: 0.00105370
Iteration 7/25 | Loss: 0.00104629
Iteration 8/25 | Loss: 0.00103921
Iteration 9/25 | Loss: 0.00102666
Iteration 10/25 | Loss: 0.00102883
Iteration 11/25 | Loss: 0.00102006
Iteration 12/25 | Loss: 0.00102160
Iteration 13/25 | Loss: 0.00102793
Iteration 14/25 | Loss: 0.00102062
Iteration 15/25 | Loss: 0.00102017
Iteration 16/25 | Loss: 0.00101924
Iteration 17/25 | Loss: 0.00101904
Iteration 18/25 | Loss: 0.00101904
Iteration 19/25 | Loss: 0.00101903
Iteration 20/25 | Loss: 0.00101903
Iteration 21/25 | Loss: 0.00101903
Iteration 22/25 | Loss: 0.00101903
Iteration 23/25 | Loss: 0.00101903
Iteration 24/25 | Loss: 0.00101903
Iteration 25/25 | Loss: 0.00101903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.47995853
Iteration 2/25 | Loss: 0.00152381
Iteration 3/25 | Loss: 0.00150820
Iteration 4/25 | Loss: 0.00150820
Iteration 5/25 | Loss: 0.00150820
Iteration 6/25 | Loss: 0.00150820
Iteration 7/25 | Loss: 0.00150820
Iteration 8/25 | Loss: 0.00150820
Iteration 9/25 | Loss: 0.00150819
Iteration 10/25 | Loss: 0.00150819
Iteration 11/25 | Loss: 0.00150819
Iteration 12/25 | Loss: 0.00150819
Iteration 13/25 | Loss: 0.00150819
Iteration 14/25 | Loss: 0.00150819
Iteration 15/25 | Loss: 0.00150819
Iteration 16/25 | Loss: 0.00150819
Iteration 17/25 | Loss: 0.00150819
Iteration 18/25 | Loss: 0.00150819
Iteration 19/25 | Loss: 0.00150819
Iteration 20/25 | Loss: 0.00150819
Iteration 21/25 | Loss: 0.00150819
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015081941382959485, 0.0015081941382959485, 0.0015081941382959485, 0.0015081941382959485, 0.0015081941382959485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015081941382959485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150819
Iteration 2/1000 | Loss: 0.00005899
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002060
Iteration 5/1000 | Loss: 0.00003405
Iteration 6/1000 | Loss: 0.00002421
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00003092
Iteration 11/1000 | Loss: 0.00002345
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001536
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001505
Iteration 19/1000 | Loss: 0.00001499
Iteration 20/1000 | Loss: 0.00001499
Iteration 21/1000 | Loss: 0.00001498
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001496
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001496
Iteration 30/1000 | Loss: 0.00001496
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00002695
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001498
Iteration 43/1000 | Loss: 0.00001490
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001490
Iteration 46/1000 | Loss: 0.00001490
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001487
Iteration 52/1000 | Loss: 0.00001487
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00002104
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001640
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001478
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001478
Iteration 72/1000 | Loss: 0.00001478
Iteration 73/1000 | Loss: 0.00001478
Iteration 74/1000 | Loss: 0.00001478
Iteration 75/1000 | Loss: 0.00001478
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001478
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.4777038813917898e-05, 1.4777038813917898e-05, 1.4777038813917898e-05, 1.4777038813917898e-05, 1.4777038813917898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4777038813917898e-05

Optimization complete. Final v2v error: 3.106267213821411 mm

Highest mean error: 9.575787544250488 mm for frame 183

Lowest mean error: 2.694065809249878 mm for frame 34

Saving results

Total time: 73.52408146858215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641799
Iteration 2/25 | Loss: 0.00122154
Iteration 3/25 | Loss: 0.00114338
Iteration 4/25 | Loss: 0.00113177
Iteration 5/25 | Loss: 0.00112753
Iteration 6/25 | Loss: 0.00112651
Iteration 7/25 | Loss: 0.00112651
Iteration 8/25 | Loss: 0.00112651
Iteration 9/25 | Loss: 0.00112651
Iteration 10/25 | Loss: 0.00112651
Iteration 11/25 | Loss: 0.00112651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011265070643275976, 0.0011265070643275976, 0.0011265070643275976, 0.0011265070643275976, 0.0011265070643275976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265070643275976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54624557
Iteration 2/25 | Loss: 0.00153846
Iteration 3/25 | Loss: 0.00153846
Iteration 4/25 | Loss: 0.00153846
Iteration 5/25 | Loss: 0.00153846
Iteration 6/25 | Loss: 0.00153846
Iteration 7/25 | Loss: 0.00153846
Iteration 8/25 | Loss: 0.00153846
Iteration 9/25 | Loss: 0.00153846
Iteration 10/25 | Loss: 0.00153846
Iteration 11/25 | Loss: 0.00153846
Iteration 12/25 | Loss: 0.00153846
Iteration 13/25 | Loss: 0.00153846
Iteration 14/25 | Loss: 0.00153846
Iteration 15/25 | Loss: 0.00153846
Iteration 16/25 | Loss: 0.00153846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015384550206363201, 0.0015384550206363201, 0.0015384550206363201, 0.0015384550206363201, 0.0015384550206363201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015384550206363201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153846
Iteration 2/1000 | Loss: 0.00003366
Iteration 3/1000 | Loss: 0.00002364
Iteration 4/1000 | Loss: 0.00002030
Iteration 5/1000 | Loss: 0.00001855
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001599
Iteration 12/1000 | Loss: 0.00001598
Iteration 13/1000 | Loss: 0.00001588
Iteration 14/1000 | Loss: 0.00001587
Iteration 15/1000 | Loss: 0.00001587
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001583
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001583
Iteration 21/1000 | Loss: 0.00001582
Iteration 22/1000 | Loss: 0.00001582
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001579
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001578
Iteration 30/1000 | Loss: 0.00001578
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001578
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001575
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001575
Iteration 39/1000 | Loss: 0.00001575
Iteration 40/1000 | Loss: 0.00001575
Iteration 41/1000 | Loss: 0.00001575
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001575
Iteration 47/1000 | Loss: 0.00001574
Iteration 48/1000 | Loss: 0.00001574
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001571
Iteration 54/1000 | Loss: 0.00001571
Iteration 55/1000 | Loss: 0.00001571
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001570
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001569
Iteration 61/1000 | Loss: 0.00001569
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001567
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001564
Iteration 107/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.5639803677913733e-05, 1.5639803677913733e-05, 1.5639803677913733e-05, 1.5639803677913733e-05, 1.5639803677913733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5639803677913733e-05

Optimization complete. Final v2v error: 3.423966884613037 mm

Highest mean error: 3.9371793270111084 mm for frame 55

Lowest mean error: 3.0359151363372803 mm for frame 121

Saving results

Total time: 31.7798912525177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393417
Iteration 2/25 | Loss: 0.00121115
Iteration 3/25 | Loss: 0.00110959
Iteration 4/25 | Loss: 0.00110409
Iteration 5/25 | Loss: 0.00110212
Iteration 6/25 | Loss: 0.00110199
Iteration 7/25 | Loss: 0.00110199
Iteration 8/25 | Loss: 0.00110199
Iteration 9/25 | Loss: 0.00110199
Iteration 10/25 | Loss: 0.00110199
Iteration 11/25 | Loss: 0.00110199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011019899975508451, 0.0011019899975508451, 0.0011019899975508451, 0.0011019899975508451, 0.0011019899975508451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011019899975508451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26729190
Iteration 2/25 | Loss: 0.00154941
Iteration 3/25 | Loss: 0.00154941
Iteration 4/25 | Loss: 0.00154941
Iteration 5/25 | Loss: 0.00154941
Iteration 6/25 | Loss: 0.00154941
Iteration 7/25 | Loss: 0.00154941
Iteration 8/25 | Loss: 0.00154941
Iteration 9/25 | Loss: 0.00154941
Iteration 10/25 | Loss: 0.00154941
Iteration 11/25 | Loss: 0.00154941
Iteration 12/25 | Loss: 0.00154941
Iteration 13/25 | Loss: 0.00154941
Iteration 14/25 | Loss: 0.00154941
Iteration 15/25 | Loss: 0.00154941
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001549411448650062, 0.001549411448650062, 0.001549411448650062, 0.001549411448650062, 0.001549411448650062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001549411448650062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154941
Iteration 2/1000 | Loss: 0.00003160
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001464
Iteration 5/1000 | Loss: 0.00001351
Iteration 6/1000 | Loss: 0.00001296
Iteration 7/1000 | Loss: 0.00001268
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001229
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001220
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001217
Iteration 16/1000 | Loss: 0.00001217
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001206
Iteration 25/1000 | Loss: 0.00001205
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001203
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001200
Iteration 42/1000 | Loss: 0.00001200
Iteration 43/1000 | Loss: 0.00001200
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001199
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001198
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001198
Iteration 72/1000 | Loss: 0.00001198
Iteration 73/1000 | Loss: 0.00001198
Iteration 74/1000 | Loss: 0.00001198
Iteration 75/1000 | Loss: 0.00001197
Iteration 76/1000 | Loss: 0.00001197
Iteration 77/1000 | Loss: 0.00001197
Iteration 78/1000 | Loss: 0.00001197
Iteration 79/1000 | Loss: 0.00001196
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001195
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001192
Iteration 107/1000 | Loss: 0.00001192
Iteration 108/1000 | Loss: 0.00001192
Iteration 109/1000 | Loss: 0.00001192
Iteration 110/1000 | Loss: 0.00001192
Iteration 111/1000 | Loss: 0.00001192
Iteration 112/1000 | Loss: 0.00001192
Iteration 113/1000 | Loss: 0.00001192
Iteration 114/1000 | Loss: 0.00001192
Iteration 115/1000 | Loss: 0.00001192
Iteration 116/1000 | Loss: 0.00001192
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001192
Iteration 123/1000 | Loss: 0.00001192
Iteration 124/1000 | Loss: 0.00001192
Iteration 125/1000 | Loss: 0.00001192
Iteration 126/1000 | Loss: 0.00001192
Iteration 127/1000 | Loss: 0.00001192
Iteration 128/1000 | Loss: 0.00001192
Iteration 129/1000 | Loss: 0.00001192
Iteration 130/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.192222407553345e-05, 1.192222407553345e-05, 1.192222407553345e-05, 1.192222407553345e-05, 1.192222407553345e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.192222407553345e-05

Optimization complete. Final v2v error: 2.980745315551758 mm

Highest mean error: 3.180608034133911 mm for frame 34

Lowest mean error: 2.8162715435028076 mm for frame 1

Saving results

Total time: 27.932461261749268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_35_us_2652/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_35_us_2652/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00510552
Iteration 2/25 | Loss: 0.00150873
Iteration 3/25 | Loss: 0.00121281
Iteration 4/25 | Loss: 0.00117730
Iteration 5/25 | Loss: 0.00117245
Iteration 6/25 | Loss: 0.00117168
Iteration 7/25 | Loss: 0.00117168
Iteration 8/25 | Loss: 0.00117168
Iteration 9/25 | Loss: 0.00117168
Iteration 10/25 | Loss: 0.00117168
Iteration 11/25 | Loss: 0.00117168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001171683776192367, 0.001171683776192367, 0.001171683776192367, 0.001171683776192367, 0.001171683776192367]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171683776192367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28234959
Iteration 2/25 | Loss: 0.00131863
Iteration 3/25 | Loss: 0.00131863
Iteration 4/25 | Loss: 0.00131863
Iteration 5/25 | Loss: 0.00131863
Iteration 6/25 | Loss: 0.00131863
Iteration 7/25 | Loss: 0.00131863
Iteration 8/25 | Loss: 0.00131863
Iteration 9/25 | Loss: 0.00131863
Iteration 10/25 | Loss: 0.00131863
Iteration 11/25 | Loss: 0.00131863
Iteration 12/25 | Loss: 0.00131863
Iteration 13/25 | Loss: 0.00131863
Iteration 14/25 | Loss: 0.00131863
Iteration 15/25 | Loss: 0.00131863
Iteration 16/25 | Loss: 0.00131863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001318626687861979, 0.001318626687861979, 0.001318626687861979, 0.001318626687861979, 0.001318626687861979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001318626687861979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131863
Iteration 2/1000 | Loss: 0.00005838
Iteration 3/1000 | Loss: 0.00002848
Iteration 4/1000 | Loss: 0.00002262
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001869
Iteration 7/1000 | Loss: 0.00001797
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001702
Iteration 10/1000 | Loss: 0.00001659
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001590
Iteration 14/1000 | Loss: 0.00001589
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001577
Iteration 21/1000 | Loss: 0.00001577
Iteration 22/1000 | Loss: 0.00001577
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001576
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001576
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001573
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001572
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001569
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001565
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001563
Iteration 78/1000 | Loss: 0.00001563
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001562
Iteration 84/1000 | Loss: 0.00001562
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001559
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001557
Iteration 114/1000 | Loss: 0.00001557
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001557
Iteration 127/1000 | Loss: 0.00001557
Iteration 128/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5566523870802484e-05, 1.5566523870802484e-05, 1.5566523870802484e-05, 1.5566523870802484e-05, 1.5566523870802484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5566523870802484e-05

Optimization complete. Final v2v error: 3.308807849884033 mm

Highest mean error: 4.678526878356934 mm for frame 92

Lowest mean error: 2.9350197315216064 mm for frame 195

Saving results

Total time: 37.63633990287781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629097
Iteration 2/25 | Loss: 0.00175479
Iteration 3/25 | Loss: 0.00149368
Iteration 4/25 | Loss: 0.00146352
Iteration 5/25 | Loss: 0.00146026
Iteration 6/25 | Loss: 0.00145974
Iteration 7/25 | Loss: 0.00145974
Iteration 8/25 | Loss: 0.00145974
Iteration 9/25 | Loss: 0.00145974
Iteration 10/25 | Loss: 0.00145974
Iteration 11/25 | Loss: 0.00145974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001459736842662096, 0.001459736842662096, 0.001459736842662096, 0.001459736842662096, 0.001459736842662096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001459736842662096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07510519
Iteration 2/25 | Loss: 0.00122038
Iteration 3/25 | Loss: 0.00122038
Iteration 4/25 | Loss: 0.00122038
Iteration 5/25 | Loss: 0.00122038
Iteration 6/25 | Loss: 0.00122038
Iteration 7/25 | Loss: 0.00122038
Iteration 8/25 | Loss: 0.00122038
Iteration 9/25 | Loss: 0.00122038
Iteration 10/25 | Loss: 0.00122038
Iteration 11/25 | Loss: 0.00122038
Iteration 12/25 | Loss: 0.00122038
Iteration 13/25 | Loss: 0.00122038
Iteration 14/25 | Loss: 0.00122038
Iteration 15/25 | Loss: 0.00122038
Iteration 16/25 | Loss: 0.00122038
Iteration 17/25 | Loss: 0.00122038
Iteration 18/25 | Loss: 0.00122038
Iteration 19/25 | Loss: 0.00122038
Iteration 20/25 | Loss: 0.00122038
Iteration 21/25 | Loss: 0.00122038
Iteration 22/25 | Loss: 0.00122038
Iteration 23/25 | Loss: 0.00122038
Iteration 24/25 | Loss: 0.00122038
Iteration 25/25 | Loss: 0.00122038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122038
Iteration 2/1000 | Loss: 0.00007563
Iteration 3/1000 | Loss: 0.00003793
Iteration 4/1000 | Loss: 0.00002926
Iteration 5/1000 | Loss: 0.00002685
Iteration 6/1000 | Loss: 0.00002528
Iteration 7/1000 | Loss: 0.00002460
Iteration 8/1000 | Loss: 0.00002416
Iteration 9/1000 | Loss: 0.00002377
Iteration 10/1000 | Loss: 0.00002352
Iteration 11/1000 | Loss: 0.00002332
Iteration 12/1000 | Loss: 0.00002315
Iteration 13/1000 | Loss: 0.00002315
Iteration 14/1000 | Loss: 0.00002310
Iteration 15/1000 | Loss: 0.00002297
Iteration 16/1000 | Loss: 0.00002295
Iteration 17/1000 | Loss: 0.00002289
Iteration 18/1000 | Loss: 0.00002282
Iteration 19/1000 | Loss: 0.00002281
Iteration 20/1000 | Loss: 0.00002280
Iteration 21/1000 | Loss: 0.00002278
Iteration 22/1000 | Loss: 0.00002278
Iteration 23/1000 | Loss: 0.00002274
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002266
Iteration 27/1000 | Loss: 0.00002265
Iteration 28/1000 | Loss: 0.00002263
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002263
Iteration 31/1000 | Loss: 0.00002263
Iteration 32/1000 | Loss: 0.00002263
Iteration 33/1000 | Loss: 0.00002263
Iteration 34/1000 | Loss: 0.00002262
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002261
Iteration 37/1000 | Loss: 0.00002261
Iteration 38/1000 | Loss: 0.00002260
Iteration 39/1000 | Loss: 0.00002259
Iteration 40/1000 | Loss: 0.00002259
Iteration 41/1000 | Loss: 0.00002258
Iteration 42/1000 | Loss: 0.00002258
Iteration 43/1000 | Loss: 0.00002258
Iteration 44/1000 | Loss: 0.00002258
Iteration 45/1000 | Loss: 0.00002258
Iteration 46/1000 | Loss: 0.00002258
Iteration 47/1000 | Loss: 0.00002258
Iteration 48/1000 | Loss: 0.00002258
Iteration 49/1000 | Loss: 0.00002258
Iteration 50/1000 | Loss: 0.00002257
Iteration 51/1000 | Loss: 0.00002257
Iteration 52/1000 | Loss: 0.00002257
Iteration 53/1000 | Loss: 0.00002257
Iteration 54/1000 | Loss: 0.00002257
Iteration 55/1000 | Loss: 0.00002256
Iteration 56/1000 | Loss: 0.00002256
Iteration 57/1000 | Loss: 0.00002256
Iteration 58/1000 | Loss: 0.00002256
Iteration 59/1000 | Loss: 0.00002256
Iteration 60/1000 | Loss: 0.00002256
Iteration 61/1000 | Loss: 0.00002256
Iteration 62/1000 | Loss: 0.00002256
Iteration 63/1000 | Loss: 0.00002255
Iteration 64/1000 | Loss: 0.00002255
Iteration 65/1000 | Loss: 0.00002255
Iteration 66/1000 | Loss: 0.00002255
Iteration 67/1000 | Loss: 0.00002255
Iteration 68/1000 | Loss: 0.00002255
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002255
Iteration 71/1000 | Loss: 0.00002254
Iteration 72/1000 | Loss: 0.00002254
Iteration 73/1000 | Loss: 0.00002254
Iteration 74/1000 | Loss: 0.00002253
Iteration 75/1000 | Loss: 0.00002253
Iteration 76/1000 | Loss: 0.00002252
Iteration 77/1000 | Loss: 0.00002252
Iteration 78/1000 | Loss: 0.00002251
Iteration 79/1000 | Loss: 0.00002251
Iteration 80/1000 | Loss: 0.00002251
Iteration 81/1000 | Loss: 0.00002251
Iteration 82/1000 | Loss: 0.00002251
Iteration 83/1000 | Loss: 0.00002251
Iteration 84/1000 | Loss: 0.00002251
Iteration 85/1000 | Loss: 0.00002251
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00002250
Iteration 88/1000 | Loss: 0.00002250
Iteration 89/1000 | Loss: 0.00002250
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002248
Iteration 95/1000 | Loss: 0.00002248
Iteration 96/1000 | Loss: 0.00002247
Iteration 97/1000 | Loss: 0.00002247
Iteration 98/1000 | Loss: 0.00002247
Iteration 99/1000 | Loss: 0.00002247
Iteration 100/1000 | Loss: 0.00002247
Iteration 101/1000 | Loss: 0.00002246
Iteration 102/1000 | Loss: 0.00002246
Iteration 103/1000 | Loss: 0.00002246
Iteration 104/1000 | Loss: 0.00002246
Iteration 105/1000 | Loss: 0.00002246
Iteration 106/1000 | Loss: 0.00002246
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002246
Iteration 109/1000 | Loss: 0.00002246
Iteration 110/1000 | Loss: 0.00002246
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002245
Iteration 113/1000 | Loss: 0.00002245
Iteration 114/1000 | Loss: 0.00002245
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002244
Iteration 117/1000 | Loss: 0.00002244
Iteration 118/1000 | Loss: 0.00002243
Iteration 119/1000 | Loss: 0.00002243
Iteration 120/1000 | Loss: 0.00002243
Iteration 121/1000 | Loss: 0.00002243
Iteration 122/1000 | Loss: 0.00002243
Iteration 123/1000 | Loss: 0.00002243
Iteration 124/1000 | Loss: 0.00002242
Iteration 125/1000 | Loss: 0.00002242
Iteration 126/1000 | Loss: 0.00002242
Iteration 127/1000 | Loss: 0.00002242
Iteration 128/1000 | Loss: 0.00002242
Iteration 129/1000 | Loss: 0.00002242
Iteration 130/1000 | Loss: 0.00002242
Iteration 131/1000 | Loss: 0.00002242
Iteration 132/1000 | Loss: 0.00002242
Iteration 133/1000 | Loss: 0.00002242
Iteration 134/1000 | Loss: 0.00002242
Iteration 135/1000 | Loss: 0.00002242
Iteration 136/1000 | Loss: 0.00002242
Iteration 137/1000 | Loss: 0.00002242
Iteration 138/1000 | Loss: 0.00002242
Iteration 139/1000 | Loss: 0.00002242
Iteration 140/1000 | Loss: 0.00002242
Iteration 141/1000 | Loss: 0.00002241
Iteration 142/1000 | Loss: 0.00002241
Iteration 143/1000 | Loss: 0.00002241
Iteration 144/1000 | Loss: 0.00002241
Iteration 145/1000 | Loss: 0.00002241
Iteration 146/1000 | Loss: 0.00002241
Iteration 147/1000 | Loss: 0.00002241
Iteration 148/1000 | Loss: 0.00002241
Iteration 149/1000 | Loss: 0.00002240
Iteration 150/1000 | Loss: 0.00002240
Iteration 151/1000 | Loss: 0.00002240
Iteration 152/1000 | Loss: 0.00002240
Iteration 153/1000 | Loss: 0.00002240
Iteration 154/1000 | Loss: 0.00002240
Iteration 155/1000 | Loss: 0.00002240
Iteration 156/1000 | Loss: 0.00002240
Iteration 157/1000 | Loss: 0.00002240
Iteration 158/1000 | Loss: 0.00002240
Iteration 159/1000 | Loss: 0.00002240
Iteration 160/1000 | Loss: 0.00002239
Iteration 161/1000 | Loss: 0.00002239
Iteration 162/1000 | Loss: 0.00002239
Iteration 163/1000 | Loss: 0.00002239
Iteration 164/1000 | Loss: 0.00002239
Iteration 165/1000 | Loss: 0.00002239
Iteration 166/1000 | Loss: 0.00002239
Iteration 167/1000 | Loss: 0.00002239
Iteration 168/1000 | Loss: 0.00002239
Iteration 169/1000 | Loss: 0.00002239
Iteration 170/1000 | Loss: 0.00002239
Iteration 171/1000 | Loss: 0.00002239
Iteration 172/1000 | Loss: 0.00002239
Iteration 173/1000 | Loss: 0.00002239
Iteration 174/1000 | Loss: 0.00002239
Iteration 175/1000 | Loss: 0.00002239
Iteration 176/1000 | Loss: 0.00002239
Iteration 177/1000 | Loss: 0.00002239
Iteration 178/1000 | Loss: 0.00002238
Iteration 179/1000 | Loss: 0.00002238
Iteration 180/1000 | Loss: 0.00002238
Iteration 181/1000 | Loss: 0.00002238
Iteration 182/1000 | Loss: 0.00002238
Iteration 183/1000 | Loss: 0.00002238
Iteration 184/1000 | Loss: 0.00002238
Iteration 185/1000 | Loss: 0.00002237
Iteration 186/1000 | Loss: 0.00002237
Iteration 187/1000 | Loss: 0.00002237
Iteration 188/1000 | Loss: 0.00002237
Iteration 189/1000 | Loss: 0.00002237
Iteration 190/1000 | Loss: 0.00002237
Iteration 191/1000 | Loss: 0.00002237
Iteration 192/1000 | Loss: 0.00002237
Iteration 193/1000 | Loss: 0.00002237
Iteration 194/1000 | Loss: 0.00002237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.2369962607626803e-05, 2.2369962607626803e-05, 2.2369962607626803e-05, 2.2369962607626803e-05, 2.2369962607626803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2369962607626803e-05

Optimization complete. Final v2v error: 3.8076791763305664 mm

Highest mean error: 6.225667953491211 mm for frame 55

Lowest mean error: 3.3249077796936035 mm for frame 115

Saving results

Total time: 42.53553748130798
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391610
Iteration 2/25 | Loss: 0.00165792
Iteration 3/25 | Loss: 0.00146954
Iteration 4/25 | Loss: 0.00145500
Iteration 5/25 | Loss: 0.00145185
Iteration 6/25 | Loss: 0.00145121
Iteration 7/25 | Loss: 0.00145121
Iteration 8/25 | Loss: 0.00145121
Iteration 9/25 | Loss: 0.00145121
Iteration 10/25 | Loss: 0.00145121
Iteration 11/25 | Loss: 0.00145121
Iteration 12/25 | Loss: 0.00145121
Iteration 13/25 | Loss: 0.00145121
Iteration 14/25 | Loss: 0.00145121
Iteration 15/25 | Loss: 0.00145121
Iteration 16/25 | Loss: 0.00145121
Iteration 17/25 | Loss: 0.00145121
Iteration 18/25 | Loss: 0.00145121
Iteration 19/25 | Loss: 0.00145121
Iteration 20/25 | Loss: 0.00145121
Iteration 21/25 | Loss: 0.00145121
Iteration 22/25 | Loss: 0.00145121
Iteration 23/25 | Loss: 0.00145121
Iteration 24/25 | Loss: 0.00145121
Iteration 25/25 | Loss: 0.00145121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014512093039229512, 0.0014512093039229512, 0.0014512093039229512, 0.0014512093039229512, 0.0014512093039229512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014512093039229512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20570147
Iteration 2/25 | Loss: 0.00236642
Iteration 3/25 | Loss: 0.00236642
Iteration 4/25 | Loss: 0.00236642
Iteration 5/25 | Loss: 0.00236642
Iteration 6/25 | Loss: 0.00236642
Iteration 7/25 | Loss: 0.00236642
Iteration 8/25 | Loss: 0.00236642
Iteration 9/25 | Loss: 0.00236642
Iteration 10/25 | Loss: 0.00236642
Iteration 11/25 | Loss: 0.00236642
Iteration 12/25 | Loss: 0.00236642
Iteration 13/25 | Loss: 0.00236642
Iteration 14/25 | Loss: 0.00236642
Iteration 15/25 | Loss: 0.00236642
Iteration 16/25 | Loss: 0.00236642
Iteration 17/25 | Loss: 0.00236642
Iteration 18/25 | Loss: 0.00236642
Iteration 19/25 | Loss: 0.00236642
Iteration 20/25 | Loss: 0.00236642
Iteration 21/25 | Loss: 0.00236642
Iteration 22/25 | Loss: 0.00236642
Iteration 23/25 | Loss: 0.00236642
Iteration 24/25 | Loss: 0.00236642
Iteration 25/25 | Loss: 0.00236642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236642
Iteration 2/1000 | Loss: 0.00008901
Iteration 3/1000 | Loss: 0.00003405
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001806
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001767
Iteration 12/1000 | Loss: 0.00001749
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001741
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001729
Iteration 18/1000 | Loss: 0.00001728
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001726
Iteration 23/1000 | Loss: 0.00001726
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001721
Iteration 28/1000 | Loss: 0.00001721
Iteration 29/1000 | Loss: 0.00001721
Iteration 30/1000 | Loss: 0.00001721
Iteration 31/1000 | Loss: 0.00001721
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001720
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001715
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001710
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001709
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001708
Iteration 68/1000 | Loss: 0.00001708
Iteration 69/1000 | Loss: 0.00001707
Iteration 70/1000 | Loss: 0.00001707
Iteration 71/1000 | Loss: 0.00001707
Iteration 72/1000 | Loss: 0.00001707
Iteration 73/1000 | Loss: 0.00001707
Iteration 74/1000 | Loss: 0.00001707
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001706
Iteration 77/1000 | Loss: 0.00001706
Iteration 78/1000 | Loss: 0.00001706
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001702
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001700
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001696
Iteration 112/1000 | Loss: 0.00001696
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001695
Iteration 115/1000 | Loss: 0.00001695
Iteration 116/1000 | Loss: 0.00001695
Iteration 117/1000 | Loss: 0.00001695
Iteration 118/1000 | Loss: 0.00001695
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001693
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001693
Iteration 127/1000 | Loss: 0.00001693
Iteration 128/1000 | Loss: 0.00001693
Iteration 129/1000 | Loss: 0.00001692
Iteration 130/1000 | Loss: 0.00001692
Iteration 131/1000 | Loss: 0.00001692
Iteration 132/1000 | Loss: 0.00001692
Iteration 133/1000 | Loss: 0.00001692
Iteration 134/1000 | Loss: 0.00001691
Iteration 135/1000 | Loss: 0.00001691
Iteration 136/1000 | Loss: 0.00001691
Iteration 137/1000 | Loss: 0.00001690
Iteration 138/1000 | Loss: 0.00001690
Iteration 139/1000 | Loss: 0.00001690
Iteration 140/1000 | Loss: 0.00001690
Iteration 141/1000 | Loss: 0.00001690
Iteration 142/1000 | Loss: 0.00001690
Iteration 143/1000 | Loss: 0.00001690
Iteration 144/1000 | Loss: 0.00001690
Iteration 145/1000 | Loss: 0.00001690
Iteration 146/1000 | Loss: 0.00001689
Iteration 147/1000 | Loss: 0.00001689
Iteration 148/1000 | Loss: 0.00001689
Iteration 149/1000 | Loss: 0.00001689
Iteration 150/1000 | Loss: 0.00001689
Iteration 151/1000 | Loss: 0.00001689
Iteration 152/1000 | Loss: 0.00001689
Iteration 153/1000 | Loss: 0.00001689
Iteration 154/1000 | Loss: 0.00001689
Iteration 155/1000 | Loss: 0.00001688
Iteration 156/1000 | Loss: 0.00001688
Iteration 157/1000 | Loss: 0.00001688
Iteration 158/1000 | Loss: 0.00001688
Iteration 159/1000 | Loss: 0.00001688
Iteration 160/1000 | Loss: 0.00001688
Iteration 161/1000 | Loss: 0.00001688
Iteration 162/1000 | Loss: 0.00001688
Iteration 163/1000 | Loss: 0.00001688
Iteration 164/1000 | Loss: 0.00001688
Iteration 165/1000 | Loss: 0.00001688
Iteration 166/1000 | Loss: 0.00001688
Iteration 167/1000 | Loss: 0.00001688
Iteration 168/1000 | Loss: 0.00001688
Iteration 169/1000 | Loss: 0.00001688
Iteration 170/1000 | Loss: 0.00001688
Iteration 171/1000 | Loss: 0.00001688
Iteration 172/1000 | Loss: 0.00001687
Iteration 173/1000 | Loss: 0.00001687
Iteration 174/1000 | Loss: 0.00001687
Iteration 175/1000 | Loss: 0.00001687
Iteration 176/1000 | Loss: 0.00001687
Iteration 177/1000 | Loss: 0.00001687
Iteration 178/1000 | Loss: 0.00001687
Iteration 179/1000 | Loss: 0.00001687
Iteration 180/1000 | Loss: 0.00001687
Iteration 181/1000 | Loss: 0.00001687
Iteration 182/1000 | Loss: 0.00001687
Iteration 183/1000 | Loss: 0.00001687
Iteration 184/1000 | Loss: 0.00001686
Iteration 185/1000 | Loss: 0.00001686
Iteration 186/1000 | Loss: 0.00001686
Iteration 187/1000 | Loss: 0.00001686
Iteration 188/1000 | Loss: 0.00001686
Iteration 189/1000 | Loss: 0.00001686
Iteration 190/1000 | Loss: 0.00001686
Iteration 191/1000 | Loss: 0.00001686
Iteration 192/1000 | Loss: 0.00001686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.6863830751390196e-05, 1.6863830751390196e-05, 1.6863830751390196e-05, 1.6863830751390196e-05, 1.6863830751390196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6863830751390196e-05

Optimization complete. Final v2v error: 3.486264228820801 mm

Highest mean error: 3.743911027908325 mm for frame 119

Lowest mean error: 3.329430103302002 mm for frame 57

Saving results

Total time: 38.4168062210083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500604
Iteration 2/25 | Loss: 0.00146958
Iteration 3/25 | Loss: 0.00138511
Iteration 4/25 | Loss: 0.00137315
Iteration 5/25 | Loss: 0.00136960
Iteration 6/25 | Loss: 0.00136907
Iteration 7/25 | Loss: 0.00136907
Iteration 8/25 | Loss: 0.00136907
Iteration 9/25 | Loss: 0.00136907
Iteration 10/25 | Loss: 0.00136907
Iteration 11/25 | Loss: 0.00136907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001369068049825728, 0.001369068049825728, 0.001369068049825728, 0.001369068049825728, 0.001369068049825728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001369068049825728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.60579443
Iteration 2/25 | Loss: 0.00216427
Iteration 3/25 | Loss: 0.00216427
Iteration 4/25 | Loss: 0.00216426
Iteration 5/25 | Loss: 0.00216426
Iteration 6/25 | Loss: 0.00216426
Iteration 7/25 | Loss: 0.00216426
Iteration 8/25 | Loss: 0.00216426
Iteration 9/25 | Loss: 0.00216426
Iteration 10/25 | Loss: 0.00216426
Iteration 11/25 | Loss: 0.00216426
Iteration 12/25 | Loss: 0.00216426
Iteration 13/25 | Loss: 0.00216426
Iteration 14/25 | Loss: 0.00216426
Iteration 15/25 | Loss: 0.00216426
Iteration 16/25 | Loss: 0.00216426
Iteration 17/25 | Loss: 0.00216426
Iteration 18/25 | Loss: 0.00216426
Iteration 19/25 | Loss: 0.00216426
Iteration 20/25 | Loss: 0.00216426
Iteration 21/25 | Loss: 0.00216426
Iteration 22/25 | Loss: 0.00216426
Iteration 23/25 | Loss: 0.00216426
Iteration 24/25 | Loss: 0.00216426
Iteration 25/25 | Loss: 0.00216426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0021642616484314203, 0.0021642616484314203, 0.0021642616484314203, 0.0021642616484314203, 0.0021642616484314203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021642616484314203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216426
Iteration 2/1000 | Loss: 0.00003241
Iteration 3/1000 | Loss: 0.00002305
Iteration 4/1000 | Loss: 0.00002101
Iteration 5/1000 | Loss: 0.00002000
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001860
Iteration 8/1000 | Loss: 0.00001828
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001776
Iteration 13/1000 | Loss: 0.00001774
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001751
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001747
Iteration 19/1000 | Loss: 0.00001746
Iteration 20/1000 | Loss: 0.00001746
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001731
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001730
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001726
Iteration 32/1000 | Loss: 0.00001726
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001725
Iteration 36/1000 | Loss: 0.00001725
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001725
Iteration 41/1000 | Loss: 0.00001725
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001723
Iteration 44/1000 | Loss: 0.00001722
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001721
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001720
Iteration 50/1000 | Loss: 0.00001719
Iteration 51/1000 | Loss: 0.00001719
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001714
Iteration 68/1000 | Loss: 0.00001714
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001714
Iteration 73/1000 | Loss: 0.00001714
Iteration 74/1000 | Loss: 0.00001714
Iteration 75/1000 | Loss: 0.00001714
Iteration 76/1000 | Loss: 0.00001714
Iteration 77/1000 | Loss: 0.00001713
Iteration 78/1000 | Loss: 0.00001713
Iteration 79/1000 | Loss: 0.00001713
Iteration 80/1000 | Loss: 0.00001713
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001712
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001711
Iteration 98/1000 | Loss: 0.00001711
Iteration 99/1000 | Loss: 0.00001711
Iteration 100/1000 | Loss: 0.00001711
Iteration 101/1000 | Loss: 0.00001711
Iteration 102/1000 | Loss: 0.00001711
Iteration 103/1000 | Loss: 0.00001711
Iteration 104/1000 | Loss: 0.00001711
Iteration 105/1000 | Loss: 0.00001711
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001711
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001711
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001710
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001710
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001709
Iteration 129/1000 | Loss: 0.00001709
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001709
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001709
Iteration 138/1000 | Loss: 0.00001709
Iteration 139/1000 | Loss: 0.00001709
Iteration 140/1000 | Loss: 0.00001708
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001708
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Iteration 149/1000 | Loss: 0.00001708
Iteration 150/1000 | Loss: 0.00001707
Iteration 151/1000 | Loss: 0.00001707
Iteration 152/1000 | Loss: 0.00001707
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001706
Iteration 159/1000 | Loss: 0.00001706
Iteration 160/1000 | Loss: 0.00001706
Iteration 161/1000 | Loss: 0.00001706
Iteration 162/1000 | Loss: 0.00001706
Iteration 163/1000 | Loss: 0.00001706
Iteration 164/1000 | Loss: 0.00001706
Iteration 165/1000 | Loss: 0.00001706
Iteration 166/1000 | Loss: 0.00001706
Iteration 167/1000 | Loss: 0.00001706
Iteration 168/1000 | Loss: 0.00001706
Iteration 169/1000 | Loss: 0.00001706
Iteration 170/1000 | Loss: 0.00001706
Iteration 171/1000 | Loss: 0.00001706
Iteration 172/1000 | Loss: 0.00001706
Iteration 173/1000 | Loss: 0.00001706
Iteration 174/1000 | Loss: 0.00001706
Iteration 175/1000 | Loss: 0.00001706
Iteration 176/1000 | Loss: 0.00001706
Iteration 177/1000 | Loss: 0.00001706
Iteration 178/1000 | Loss: 0.00001706
Iteration 179/1000 | Loss: 0.00001706
Iteration 180/1000 | Loss: 0.00001706
Iteration 181/1000 | Loss: 0.00001706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.7058786397683434e-05, 1.7058786397683434e-05, 1.7058786397683434e-05, 1.7058786397683434e-05, 1.7058786397683434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7058786397683434e-05

Optimization complete. Final v2v error: 3.4996719360351562 mm

Highest mean error: 3.7275028228759766 mm for frame 57

Lowest mean error: 3.2624876499176025 mm for frame 68

Saving results

Total time: 39.044435024261475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00452206
Iteration 2/25 | Loss: 0.00160752
Iteration 3/25 | Loss: 0.00146294
Iteration 4/25 | Loss: 0.00145045
Iteration 5/25 | Loss: 0.00144726
Iteration 6/25 | Loss: 0.00144634
Iteration 7/25 | Loss: 0.00144634
Iteration 8/25 | Loss: 0.00144634
Iteration 9/25 | Loss: 0.00144634
Iteration 10/25 | Loss: 0.00144634
Iteration 11/25 | Loss: 0.00144634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014463418629020452, 0.0014463418629020452, 0.0014463418629020452, 0.0014463418629020452, 0.0014463418629020452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014463418629020452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.82623243
Iteration 2/25 | Loss: 0.00172488
Iteration 3/25 | Loss: 0.00172485
Iteration 4/25 | Loss: 0.00172485
Iteration 5/25 | Loss: 0.00172485
Iteration 6/25 | Loss: 0.00172485
Iteration 7/25 | Loss: 0.00172485
Iteration 8/25 | Loss: 0.00172485
Iteration 9/25 | Loss: 0.00172485
Iteration 10/25 | Loss: 0.00172485
Iteration 11/25 | Loss: 0.00172485
Iteration 12/25 | Loss: 0.00172485
Iteration 13/25 | Loss: 0.00172485
Iteration 14/25 | Loss: 0.00172485
Iteration 15/25 | Loss: 0.00172485
Iteration 16/25 | Loss: 0.00172485
Iteration 17/25 | Loss: 0.00172485
Iteration 18/25 | Loss: 0.00172485
Iteration 19/25 | Loss: 0.00172485
Iteration 20/25 | Loss: 0.00172485
Iteration 21/25 | Loss: 0.00172485
Iteration 22/25 | Loss: 0.00172485
Iteration 23/25 | Loss: 0.00172485
Iteration 24/25 | Loss: 0.00172485
Iteration 25/25 | Loss: 0.00172485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172485
Iteration 2/1000 | Loss: 0.00004853
Iteration 3/1000 | Loss: 0.00003394
Iteration 4/1000 | Loss: 0.00002998
Iteration 5/1000 | Loss: 0.00002801
Iteration 6/1000 | Loss: 0.00002700
Iteration 7/1000 | Loss: 0.00002618
Iteration 8/1000 | Loss: 0.00002550
Iteration 9/1000 | Loss: 0.00002512
Iteration 10/1000 | Loss: 0.00002482
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002447
Iteration 13/1000 | Loss: 0.00002433
Iteration 14/1000 | Loss: 0.00002425
Iteration 15/1000 | Loss: 0.00002421
Iteration 16/1000 | Loss: 0.00002420
Iteration 17/1000 | Loss: 0.00002420
Iteration 18/1000 | Loss: 0.00002419
Iteration 19/1000 | Loss: 0.00002419
Iteration 20/1000 | Loss: 0.00002418
Iteration 21/1000 | Loss: 0.00002418
Iteration 22/1000 | Loss: 0.00002417
Iteration 23/1000 | Loss: 0.00002417
Iteration 24/1000 | Loss: 0.00002416
Iteration 25/1000 | Loss: 0.00002416
Iteration 26/1000 | Loss: 0.00002416
Iteration 27/1000 | Loss: 0.00002416
Iteration 28/1000 | Loss: 0.00002416
Iteration 29/1000 | Loss: 0.00002416
Iteration 30/1000 | Loss: 0.00002416
Iteration 31/1000 | Loss: 0.00002416
Iteration 32/1000 | Loss: 0.00002415
Iteration 33/1000 | Loss: 0.00002415
Iteration 34/1000 | Loss: 0.00002414
Iteration 35/1000 | Loss: 0.00002414
Iteration 36/1000 | Loss: 0.00002414
Iteration 37/1000 | Loss: 0.00002414
Iteration 38/1000 | Loss: 0.00002413
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00002413
Iteration 41/1000 | Loss: 0.00002413
Iteration 42/1000 | Loss: 0.00002413
Iteration 43/1000 | Loss: 0.00002413
Iteration 44/1000 | Loss: 0.00002413
Iteration 45/1000 | Loss: 0.00002413
Iteration 46/1000 | Loss: 0.00002413
Iteration 47/1000 | Loss: 0.00002413
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002413
Iteration 50/1000 | Loss: 0.00002412
Iteration 51/1000 | Loss: 0.00002412
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002412
Iteration 54/1000 | Loss: 0.00002412
Iteration 55/1000 | Loss: 0.00002412
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002411
Iteration 58/1000 | Loss: 0.00002411
Iteration 59/1000 | Loss: 0.00002411
Iteration 60/1000 | Loss: 0.00002411
Iteration 61/1000 | Loss: 0.00002411
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002411
Iteration 64/1000 | Loss: 0.00002411
Iteration 65/1000 | Loss: 0.00002410
Iteration 66/1000 | Loss: 0.00002410
Iteration 67/1000 | Loss: 0.00002410
Iteration 68/1000 | Loss: 0.00002410
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002409
Iteration 72/1000 | Loss: 0.00002409
Iteration 73/1000 | Loss: 0.00002409
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002408
Iteration 78/1000 | Loss: 0.00002408
Iteration 79/1000 | Loss: 0.00002408
Iteration 80/1000 | Loss: 0.00002407
Iteration 81/1000 | Loss: 0.00002407
Iteration 82/1000 | Loss: 0.00002407
Iteration 83/1000 | Loss: 0.00002406
Iteration 84/1000 | Loss: 0.00002406
Iteration 85/1000 | Loss: 0.00002406
Iteration 86/1000 | Loss: 0.00002406
Iteration 87/1000 | Loss: 0.00002406
Iteration 88/1000 | Loss: 0.00002406
Iteration 89/1000 | Loss: 0.00002405
Iteration 90/1000 | Loss: 0.00002405
Iteration 91/1000 | Loss: 0.00002405
Iteration 92/1000 | Loss: 0.00002405
Iteration 93/1000 | Loss: 0.00002405
Iteration 94/1000 | Loss: 0.00002404
Iteration 95/1000 | Loss: 0.00002404
Iteration 96/1000 | Loss: 0.00002404
Iteration 97/1000 | Loss: 0.00002404
Iteration 98/1000 | Loss: 0.00002404
Iteration 99/1000 | Loss: 0.00002403
Iteration 100/1000 | Loss: 0.00002403
Iteration 101/1000 | Loss: 0.00002403
Iteration 102/1000 | Loss: 0.00002403
Iteration 103/1000 | Loss: 0.00002403
Iteration 104/1000 | Loss: 0.00002403
Iteration 105/1000 | Loss: 0.00002403
Iteration 106/1000 | Loss: 0.00002403
Iteration 107/1000 | Loss: 0.00002403
Iteration 108/1000 | Loss: 0.00002403
Iteration 109/1000 | Loss: 0.00002402
Iteration 110/1000 | Loss: 0.00002402
Iteration 111/1000 | Loss: 0.00002402
Iteration 112/1000 | Loss: 0.00002401
Iteration 113/1000 | Loss: 0.00002401
Iteration 114/1000 | Loss: 0.00002401
Iteration 115/1000 | Loss: 0.00002400
Iteration 116/1000 | Loss: 0.00002400
Iteration 117/1000 | Loss: 0.00002400
Iteration 118/1000 | Loss: 0.00002400
Iteration 119/1000 | Loss: 0.00002400
Iteration 120/1000 | Loss: 0.00002400
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002399
Iteration 123/1000 | Loss: 0.00002399
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002398
Iteration 130/1000 | Loss: 0.00002398
Iteration 131/1000 | Loss: 0.00002398
Iteration 132/1000 | Loss: 0.00002398
Iteration 133/1000 | Loss: 0.00002397
Iteration 134/1000 | Loss: 0.00002397
Iteration 135/1000 | Loss: 0.00002397
Iteration 136/1000 | Loss: 0.00002397
Iteration 137/1000 | Loss: 0.00002397
Iteration 138/1000 | Loss: 0.00002397
Iteration 139/1000 | Loss: 0.00002396
Iteration 140/1000 | Loss: 0.00002396
Iteration 141/1000 | Loss: 0.00002396
Iteration 142/1000 | Loss: 0.00002396
Iteration 143/1000 | Loss: 0.00002396
Iteration 144/1000 | Loss: 0.00002395
Iteration 145/1000 | Loss: 0.00002395
Iteration 146/1000 | Loss: 0.00002395
Iteration 147/1000 | Loss: 0.00002395
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00002395
Iteration 150/1000 | Loss: 0.00002395
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002395
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002394
Iteration 155/1000 | Loss: 0.00002394
Iteration 156/1000 | Loss: 0.00002394
Iteration 157/1000 | Loss: 0.00002394
Iteration 158/1000 | Loss: 0.00002394
Iteration 159/1000 | Loss: 0.00002394
Iteration 160/1000 | Loss: 0.00002394
Iteration 161/1000 | Loss: 0.00002394
Iteration 162/1000 | Loss: 0.00002394
Iteration 163/1000 | Loss: 0.00002394
Iteration 164/1000 | Loss: 0.00002394
Iteration 165/1000 | Loss: 0.00002394
Iteration 166/1000 | Loss: 0.00002393
Iteration 167/1000 | Loss: 0.00002393
Iteration 168/1000 | Loss: 0.00002393
Iteration 169/1000 | Loss: 0.00002393
Iteration 170/1000 | Loss: 0.00002393
Iteration 171/1000 | Loss: 0.00002393
Iteration 172/1000 | Loss: 0.00002393
Iteration 173/1000 | Loss: 0.00002393
Iteration 174/1000 | Loss: 0.00002393
Iteration 175/1000 | Loss: 0.00002393
Iteration 176/1000 | Loss: 0.00002392
Iteration 177/1000 | Loss: 0.00002392
Iteration 178/1000 | Loss: 0.00002392
Iteration 179/1000 | Loss: 0.00002392
Iteration 180/1000 | Loss: 0.00002392
Iteration 181/1000 | Loss: 0.00002392
Iteration 182/1000 | Loss: 0.00002392
Iteration 183/1000 | Loss: 0.00002392
Iteration 184/1000 | Loss: 0.00002392
Iteration 185/1000 | Loss: 0.00002392
Iteration 186/1000 | Loss: 0.00002392
Iteration 187/1000 | Loss: 0.00002392
Iteration 188/1000 | Loss: 0.00002392
Iteration 189/1000 | Loss: 0.00002392
Iteration 190/1000 | Loss: 0.00002392
Iteration 191/1000 | Loss: 0.00002392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.3921318643260747e-05, 2.3921318643260747e-05, 2.3921318643260747e-05, 2.3921318643260747e-05, 2.3921318643260747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3921318643260747e-05

Optimization complete. Final v2v error: 4.14390754699707 mm

Highest mean error: 4.406703948974609 mm for frame 31

Lowest mean error: 3.9190590381622314 mm for frame 103

Saving results

Total time: 40.06673836708069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106644
Iteration 2/25 | Loss: 0.00285529
Iteration 3/25 | Loss: 0.00225654
Iteration 4/25 | Loss: 0.00209705
Iteration 5/25 | Loss: 0.00202755
Iteration 6/25 | Loss: 0.00195558
Iteration 7/25 | Loss: 0.00192890
Iteration 8/25 | Loss: 0.00189452
Iteration 9/25 | Loss: 0.00187202
Iteration 10/25 | Loss: 0.00184959
Iteration 11/25 | Loss: 0.00184105
Iteration 12/25 | Loss: 0.00184346
Iteration 13/25 | Loss: 0.00183336
Iteration 14/25 | Loss: 0.00182346
Iteration 15/25 | Loss: 0.00182092
Iteration 16/25 | Loss: 0.00182032
Iteration 17/25 | Loss: 0.00182012
Iteration 18/25 | Loss: 0.00182004
Iteration 19/25 | Loss: 0.00182000
Iteration 20/25 | Loss: 0.00182000
Iteration 21/25 | Loss: 0.00181998
Iteration 22/25 | Loss: 0.00181998
Iteration 23/25 | Loss: 0.00181998
Iteration 24/25 | Loss: 0.00181998
Iteration 25/25 | Loss: 0.00181998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16539431
Iteration 2/25 | Loss: 0.00744776
Iteration 3/25 | Loss: 0.00539568
Iteration 4/25 | Loss: 0.00539568
Iteration 5/25 | Loss: 0.00539568
Iteration 6/25 | Loss: 0.00539567
Iteration 7/25 | Loss: 0.00539567
Iteration 8/25 | Loss: 0.00539567
Iteration 9/25 | Loss: 0.00539567
Iteration 10/25 | Loss: 0.00539567
Iteration 11/25 | Loss: 0.00539567
Iteration 12/25 | Loss: 0.00539567
Iteration 13/25 | Loss: 0.00539567
Iteration 14/25 | Loss: 0.00539567
Iteration 15/25 | Loss: 0.00539567
Iteration 16/25 | Loss: 0.00539567
Iteration 17/25 | Loss: 0.00539567
Iteration 18/25 | Loss: 0.00539567
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005395670887082815, 0.005395670887082815, 0.005395670887082815, 0.005395670887082815, 0.005395670887082815]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005395670887082815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00539567
Iteration 2/1000 | Loss: 0.00154925
Iteration 3/1000 | Loss: 0.00072012
Iteration 4/1000 | Loss: 0.00606711
Iteration 5/1000 | Loss: 0.00088191
Iteration 6/1000 | Loss: 0.00055032
Iteration 7/1000 | Loss: 0.00068384
Iteration 8/1000 | Loss: 0.00091255
Iteration 9/1000 | Loss: 0.00134424
Iteration 10/1000 | Loss: 0.00202934
Iteration 11/1000 | Loss: 0.00413939
Iteration 12/1000 | Loss: 0.00503074
Iteration 13/1000 | Loss: 0.00236434
Iteration 14/1000 | Loss: 0.00215094
Iteration 15/1000 | Loss: 0.00408877
Iteration 16/1000 | Loss: 0.00210485
Iteration 17/1000 | Loss: 0.00177942
Iteration 18/1000 | Loss: 0.00225675
Iteration 19/1000 | Loss: 0.00172151
Iteration 20/1000 | Loss: 0.00112153
Iteration 21/1000 | Loss: 0.00144577
Iteration 22/1000 | Loss: 0.00122550
Iteration 23/1000 | Loss: 0.00127553
Iteration 24/1000 | Loss: 0.00139819
Iteration 25/1000 | Loss: 0.00128294
Iteration 26/1000 | Loss: 0.00112035
Iteration 27/1000 | Loss: 0.00090866
Iteration 28/1000 | Loss: 0.00106013
Iteration 29/1000 | Loss: 0.00157852
Iteration 30/1000 | Loss: 0.00051017
Iteration 31/1000 | Loss: 0.00082421
Iteration 32/1000 | Loss: 0.00050945
Iteration 33/1000 | Loss: 0.00155107
Iteration 34/1000 | Loss: 0.00094957
Iteration 35/1000 | Loss: 0.00087072
Iteration 36/1000 | Loss: 0.00090290
Iteration 37/1000 | Loss: 0.00102428
Iteration 38/1000 | Loss: 0.00115346
Iteration 39/1000 | Loss: 0.00111548
Iteration 40/1000 | Loss: 0.00091837
Iteration 41/1000 | Loss: 0.00168277
Iteration 42/1000 | Loss: 0.00080752
Iteration 43/1000 | Loss: 0.00152309
Iteration 44/1000 | Loss: 0.00162558
Iteration 45/1000 | Loss: 0.00118055
Iteration 46/1000 | Loss: 0.00109431
Iteration 47/1000 | Loss: 0.00120869
Iteration 48/1000 | Loss: 0.00128319
Iteration 49/1000 | Loss: 0.00156458
Iteration 50/1000 | Loss: 0.00130870
Iteration 51/1000 | Loss: 0.00124070
Iteration 52/1000 | Loss: 0.00133644
Iteration 53/1000 | Loss: 0.00087266
Iteration 54/1000 | Loss: 0.00075227
Iteration 55/1000 | Loss: 0.00103017
Iteration 56/1000 | Loss: 0.00180648
Iteration 57/1000 | Loss: 0.00089967
Iteration 58/1000 | Loss: 0.00111415
Iteration 59/1000 | Loss: 0.00134671
Iteration 60/1000 | Loss: 0.00122554
Iteration 61/1000 | Loss: 0.00072934
Iteration 62/1000 | Loss: 0.00171918
Iteration 63/1000 | Loss: 0.00093048
Iteration 64/1000 | Loss: 0.00254828
Iteration 65/1000 | Loss: 0.00139761
Iteration 66/1000 | Loss: 0.00132099
Iteration 67/1000 | Loss: 0.00137033
Iteration 68/1000 | Loss: 0.00041501
Iteration 69/1000 | Loss: 0.00104412
Iteration 70/1000 | Loss: 0.00068546
Iteration 71/1000 | Loss: 0.00081986
Iteration 72/1000 | Loss: 0.00076420
Iteration 73/1000 | Loss: 0.00050340
Iteration 74/1000 | Loss: 0.00133762
Iteration 75/1000 | Loss: 0.00071764
Iteration 76/1000 | Loss: 0.00081115
Iteration 77/1000 | Loss: 0.00123272
Iteration 78/1000 | Loss: 0.00072903
Iteration 79/1000 | Loss: 0.00111181
Iteration 80/1000 | Loss: 0.00079283
Iteration 81/1000 | Loss: 0.00087752
Iteration 82/1000 | Loss: 0.00154597
Iteration 83/1000 | Loss: 0.00096830
Iteration 84/1000 | Loss: 0.00039149
Iteration 85/1000 | Loss: 0.00021699
Iteration 86/1000 | Loss: 0.00029849
Iteration 87/1000 | Loss: 0.00062461
Iteration 88/1000 | Loss: 0.00037145
Iteration 89/1000 | Loss: 0.00062206
Iteration 90/1000 | Loss: 0.00027702
Iteration 91/1000 | Loss: 0.00032633
Iteration 92/1000 | Loss: 0.00048105
Iteration 93/1000 | Loss: 0.00030491
Iteration 94/1000 | Loss: 0.00031107
Iteration 95/1000 | Loss: 0.00032874
Iteration 96/1000 | Loss: 0.00030210
Iteration 97/1000 | Loss: 0.00034172
Iteration 98/1000 | Loss: 0.00051554
Iteration 99/1000 | Loss: 0.00048063
Iteration 100/1000 | Loss: 0.00072710
Iteration 101/1000 | Loss: 0.00029969
Iteration 102/1000 | Loss: 0.00017259
Iteration 103/1000 | Loss: 0.00015705
Iteration 104/1000 | Loss: 0.00014815
Iteration 105/1000 | Loss: 0.00028137
Iteration 106/1000 | Loss: 0.00014717
Iteration 107/1000 | Loss: 0.00029586
Iteration 108/1000 | Loss: 0.00017075
Iteration 109/1000 | Loss: 0.00014087
Iteration 110/1000 | Loss: 0.00021673
Iteration 111/1000 | Loss: 0.00033797
Iteration 112/1000 | Loss: 0.00030736
Iteration 113/1000 | Loss: 0.00059990
Iteration 114/1000 | Loss: 0.00014081
Iteration 115/1000 | Loss: 0.00013867
Iteration 116/1000 | Loss: 0.00013812
Iteration 117/1000 | Loss: 0.00042572
Iteration 118/1000 | Loss: 0.00020908
Iteration 119/1000 | Loss: 0.00014281
Iteration 120/1000 | Loss: 0.00019533
Iteration 121/1000 | Loss: 0.00023260
Iteration 122/1000 | Loss: 0.00013702
Iteration 123/1000 | Loss: 0.00019728
Iteration 124/1000 | Loss: 0.00014784
Iteration 125/1000 | Loss: 0.00018659
Iteration 126/1000 | Loss: 0.00014803
Iteration 127/1000 | Loss: 0.00016780
Iteration 128/1000 | Loss: 0.00014148
Iteration 129/1000 | Loss: 0.00013962
Iteration 130/1000 | Loss: 0.00013677
Iteration 131/1000 | Loss: 0.00013647
Iteration 132/1000 | Loss: 0.00013626
Iteration 133/1000 | Loss: 0.00013626
Iteration 134/1000 | Loss: 0.00013612
Iteration 135/1000 | Loss: 0.00013608
Iteration 136/1000 | Loss: 0.00013607
Iteration 137/1000 | Loss: 0.00013607
Iteration 138/1000 | Loss: 0.00013603
Iteration 139/1000 | Loss: 0.00013602
Iteration 140/1000 | Loss: 0.00013602
Iteration 141/1000 | Loss: 0.00013601
Iteration 142/1000 | Loss: 0.00013600
Iteration 143/1000 | Loss: 0.00013599
Iteration 144/1000 | Loss: 0.00013599
Iteration 145/1000 | Loss: 0.00013599
Iteration 146/1000 | Loss: 0.00013598
Iteration 147/1000 | Loss: 0.00013598
Iteration 148/1000 | Loss: 0.00013598
Iteration 149/1000 | Loss: 0.00013597
Iteration 150/1000 | Loss: 0.00013597
Iteration 151/1000 | Loss: 0.00013597
Iteration 152/1000 | Loss: 0.00013597
Iteration 153/1000 | Loss: 0.00013597
Iteration 154/1000 | Loss: 0.00013596
Iteration 155/1000 | Loss: 0.00013596
Iteration 156/1000 | Loss: 0.00013595
Iteration 157/1000 | Loss: 0.00013595
Iteration 158/1000 | Loss: 0.00013595
Iteration 159/1000 | Loss: 0.00013594
Iteration 160/1000 | Loss: 0.00013594
Iteration 161/1000 | Loss: 0.00013594
Iteration 162/1000 | Loss: 0.00013593
Iteration 163/1000 | Loss: 0.00013593
Iteration 164/1000 | Loss: 0.00013593
Iteration 165/1000 | Loss: 0.00013593
Iteration 166/1000 | Loss: 0.00013592
Iteration 167/1000 | Loss: 0.00013592
Iteration 168/1000 | Loss: 0.00013591
Iteration 169/1000 | Loss: 0.00013591
Iteration 170/1000 | Loss: 0.00013591
Iteration 171/1000 | Loss: 0.00013590
Iteration 172/1000 | Loss: 0.00013590
Iteration 173/1000 | Loss: 0.00013590
Iteration 174/1000 | Loss: 0.00013590
Iteration 175/1000 | Loss: 0.00013590
Iteration 176/1000 | Loss: 0.00013590
Iteration 177/1000 | Loss: 0.00013590
Iteration 178/1000 | Loss: 0.00013589
Iteration 179/1000 | Loss: 0.00013589
Iteration 180/1000 | Loss: 0.00013589
Iteration 181/1000 | Loss: 0.00013589
Iteration 182/1000 | Loss: 0.00013588
Iteration 183/1000 | Loss: 0.00013588
Iteration 184/1000 | Loss: 0.00013588
Iteration 185/1000 | Loss: 0.00013588
Iteration 186/1000 | Loss: 0.00013588
Iteration 187/1000 | Loss: 0.00013588
Iteration 188/1000 | Loss: 0.00013587
Iteration 189/1000 | Loss: 0.00013587
Iteration 190/1000 | Loss: 0.00013587
Iteration 191/1000 | Loss: 0.00013587
Iteration 192/1000 | Loss: 0.00013587
Iteration 193/1000 | Loss: 0.00013587
Iteration 194/1000 | Loss: 0.00013587
Iteration 195/1000 | Loss: 0.00013587
Iteration 196/1000 | Loss: 0.00013587
Iteration 197/1000 | Loss: 0.00013587
Iteration 198/1000 | Loss: 0.00013587
Iteration 199/1000 | Loss: 0.00013586
Iteration 200/1000 | Loss: 0.00013586
Iteration 201/1000 | Loss: 0.00013586
Iteration 202/1000 | Loss: 0.00013586
Iteration 203/1000 | Loss: 0.00013586
Iteration 204/1000 | Loss: 0.00013586
Iteration 205/1000 | Loss: 0.00013586
Iteration 206/1000 | Loss: 0.00013586
Iteration 207/1000 | Loss: 0.00013585
Iteration 208/1000 | Loss: 0.00013585
Iteration 209/1000 | Loss: 0.00013585
Iteration 210/1000 | Loss: 0.00013585
Iteration 211/1000 | Loss: 0.00013585
Iteration 212/1000 | Loss: 0.00013585
Iteration 213/1000 | Loss: 0.00013585
Iteration 214/1000 | Loss: 0.00013585
Iteration 215/1000 | Loss: 0.00013585
Iteration 216/1000 | Loss: 0.00013585
Iteration 217/1000 | Loss: 0.00013585
Iteration 218/1000 | Loss: 0.00013585
Iteration 219/1000 | Loss: 0.00013585
Iteration 220/1000 | Loss: 0.00013584
Iteration 221/1000 | Loss: 0.00013584
Iteration 222/1000 | Loss: 0.00013584
Iteration 223/1000 | Loss: 0.00013584
Iteration 224/1000 | Loss: 0.00013583
Iteration 225/1000 | Loss: 0.00013583
Iteration 226/1000 | Loss: 0.00013583
Iteration 227/1000 | Loss: 0.00013583
Iteration 228/1000 | Loss: 0.00013583
Iteration 229/1000 | Loss: 0.00013583
Iteration 230/1000 | Loss: 0.00013583
Iteration 231/1000 | Loss: 0.00013583
Iteration 232/1000 | Loss: 0.00013582
Iteration 233/1000 | Loss: 0.00013582
Iteration 234/1000 | Loss: 0.00013582
Iteration 235/1000 | Loss: 0.00013582
Iteration 236/1000 | Loss: 0.00013582
Iteration 237/1000 | Loss: 0.00013582
Iteration 238/1000 | Loss: 0.00013582
Iteration 239/1000 | Loss: 0.00013581
Iteration 240/1000 | Loss: 0.00013581
Iteration 241/1000 | Loss: 0.00013581
Iteration 242/1000 | Loss: 0.00013581
Iteration 243/1000 | Loss: 0.00013581
Iteration 244/1000 | Loss: 0.00013581
Iteration 245/1000 | Loss: 0.00013581
Iteration 246/1000 | Loss: 0.00013581
Iteration 247/1000 | Loss: 0.00013581
Iteration 248/1000 | Loss: 0.00013581
Iteration 249/1000 | Loss: 0.00013581
Iteration 250/1000 | Loss: 0.00013581
Iteration 251/1000 | Loss: 0.00013580
Iteration 252/1000 | Loss: 0.00013580
Iteration 253/1000 | Loss: 0.00013580
Iteration 254/1000 | Loss: 0.00013580
Iteration 255/1000 | Loss: 0.00013580
Iteration 256/1000 | Loss: 0.00013580
Iteration 257/1000 | Loss: 0.00013580
Iteration 258/1000 | Loss: 0.00013580
Iteration 259/1000 | Loss: 0.00013580
Iteration 260/1000 | Loss: 0.00013580
Iteration 261/1000 | Loss: 0.00013580
Iteration 262/1000 | Loss: 0.00013580
Iteration 263/1000 | Loss: 0.00013580
Iteration 264/1000 | Loss: 0.00013580
Iteration 265/1000 | Loss: 0.00013580
Iteration 266/1000 | Loss: 0.00013580
Iteration 267/1000 | Loss: 0.00013580
Iteration 268/1000 | Loss: 0.00013580
Iteration 269/1000 | Loss: 0.00013579
Iteration 270/1000 | Loss: 0.00013579
Iteration 271/1000 | Loss: 0.00013579
Iteration 272/1000 | Loss: 0.00013579
Iteration 273/1000 | Loss: 0.00013579
Iteration 274/1000 | Loss: 0.00013579
Iteration 275/1000 | Loss: 0.00013579
Iteration 276/1000 | Loss: 0.00013579
Iteration 277/1000 | Loss: 0.00013579
Iteration 278/1000 | Loss: 0.00013579
Iteration 279/1000 | Loss: 0.00013579
Iteration 280/1000 | Loss: 0.00013579
Iteration 281/1000 | Loss: 0.00013579
Iteration 282/1000 | Loss: 0.00013579
Iteration 283/1000 | Loss: 0.00013579
Iteration 284/1000 | Loss: 0.00013579
Iteration 285/1000 | Loss: 0.00013579
Iteration 286/1000 | Loss: 0.00013578
Iteration 287/1000 | Loss: 0.00013578
Iteration 288/1000 | Loss: 0.00013578
Iteration 289/1000 | Loss: 0.00013578
Iteration 290/1000 | Loss: 0.00013578
Iteration 291/1000 | Loss: 0.00013578
Iteration 292/1000 | Loss: 0.00013578
Iteration 293/1000 | Loss: 0.00013578
Iteration 294/1000 | Loss: 0.00013578
Iteration 295/1000 | Loss: 0.00013578
Iteration 296/1000 | Loss: 0.00013578
Iteration 297/1000 | Loss: 0.00013578
Iteration 298/1000 | Loss: 0.00013578
Iteration 299/1000 | Loss: 0.00013578
Iteration 300/1000 | Loss: 0.00013578
Iteration 301/1000 | Loss: 0.00013578
Iteration 302/1000 | Loss: 0.00013578
Iteration 303/1000 | Loss: 0.00013577
Iteration 304/1000 | Loss: 0.00013577
Iteration 305/1000 | Loss: 0.00013577
Iteration 306/1000 | Loss: 0.00013577
Iteration 307/1000 | Loss: 0.00013577
Iteration 308/1000 | Loss: 0.00013577
Iteration 309/1000 | Loss: 0.00013577
Iteration 310/1000 | Loss: 0.00013577
Iteration 311/1000 | Loss: 0.00013577
Iteration 312/1000 | Loss: 0.00013577
Iteration 313/1000 | Loss: 0.00013577
Iteration 314/1000 | Loss: 0.00013577
Iteration 315/1000 | Loss: 0.00013577
Iteration 316/1000 | Loss: 0.00013577
Iteration 317/1000 | Loss: 0.00013577
Iteration 318/1000 | Loss: 0.00013577
Iteration 319/1000 | Loss: 0.00013577
Iteration 320/1000 | Loss: 0.00013577
Iteration 321/1000 | Loss: 0.00013577
Iteration 322/1000 | Loss: 0.00013577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [0.00013576752098742872, 0.00013576752098742872, 0.00013576752098742872, 0.00013576752098742872, 0.00013576752098742872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013576752098742872

Optimization complete. Final v2v error: 5.796536445617676 mm

Highest mean error: 13.293194770812988 mm for frame 165

Lowest mean error: 3.714562177658081 mm for frame 64

Saving results

Total time: 265.01609921455383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432440
Iteration 2/25 | Loss: 0.00151092
Iteration 3/25 | Loss: 0.00142915
Iteration 4/25 | Loss: 0.00141912
Iteration 5/25 | Loss: 0.00141674
Iteration 6/25 | Loss: 0.00141674
Iteration 7/25 | Loss: 0.00141674
Iteration 8/25 | Loss: 0.00141674
Iteration 9/25 | Loss: 0.00141674
Iteration 10/25 | Loss: 0.00141674
Iteration 11/25 | Loss: 0.00141674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014167410554364324, 0.0014167410554364324, 0.0014167410554364324, 0.0014167410554364324, 0.0014167410554364324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014167410554364324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25071096
Iteration 2/25 | Loss: 0.00232903
Iteration 3/25 | Loss: 0.00232903
Iteration 4/25 | Loss: 0.00232903
Iteration 5/25 | Loss: 0.00232903
Iteration 6/25 | Loss: 0.00232902
Iteration 7/25 | Loss: 0.00232902
Iteration 8/25 | Loss: 0.00232902
Iteration 9/25 | Loss: 0.00232902
Iteration 10/25 | Loss: 0.00232902
Iteration 11/25 | Loss: 0.00232902
Iteration 12/25 | Loss: 0.00232902
Iteration 13/25 | Loss: 0.00232902
Iteration 14/25 | Loss: 0.00232902
Iteration 15/25 | Loss: 0.00232902
Iteration 16/25 | Loss: 0.00232902
Iteration 17/25 | Loss: 0.00232902
Iteration 18/25 | Loss: 0.00232902
Iteration 19/25 | Loss: 0.00232902
Iteration 20/25 | Loss: 0.00232902
Iteration 21/25 | Loss: 0.00232902
Iteration 22/25 | Loss: 0.00232902
Iteration 23/25 | Loss: 0.00232902
Iteration 24/25 | Loss: 0.00232902
Iteration 25/25 | Loss: 0.00232902

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232902
Iteration 2/1000 | Loss: 0.00004476
Iteration 3/1000 | Loss: 0.00002927
Iteration 4/1000 | Loss: 0.00002615
Iteration 5/1000 | Loss: 0.00002487
Iteration 6/1000 | Loss: 0.00002408
Iteration 7/1000 | Loss: 0.00002350
Iteration 8/1000 | Loss: 0.00002308
Iteration 9/1000 | Loss: 0.00002286
Iteration 10/1000 | Loss: 0.00002267
Iteration 11/1000 | Loss: 0.00002264
Iteration 12/1000 | Loss: 0.00002264
Iteration 13/1000 | Loss: 0.00002263
Iteration 14/1000 | Loss: 0.00002263
Iteration 15/1000 | Loss: 0.00002263
Iteration 16/1000 | Loss: 0.00002258
Iteration 17/1000 | Loss: 0.00002257
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002252
Iteration 22/1000 | Loss: 0.00002252
Iteration 23/1000 | Loss: 0.00002251
Iteration 24/1000 | Loss: 0.00002236
Iteration 25/1000 | Loss: 0.00002236
Iteration 26/1000 | Loss: 0.00002230
Iteration 27/1000 | Loss: 0.00002228
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002219
Iteration 30/1000 | Loss: 0.00002219
Iteration 31/1000 | Loss: 0.00002218
Iteration 32/1000 | Loss: 0.00002218
Iteration 33/1000 | Loss: 0.00002218
Iteration 34/1000 | Loss: 0.00002218
Iteration 35/1000 | Loss: 0.00002217
Iteration 36/1000 | Loss: 0.00002217
Iteration 37/1000 | Loss: 0.00002217
Iteration 38/1000 | Loss: 0.00002216
Iteration 39/1000 | Loss: 0.00002216
Iteration 40/1000 | Loss: 0.00002216
Iteration 41/1000 | Loss: 0.00002216
Iteration 42/1000 | Loss: 0.00002216
Iteration 43/1000 | Loss: 0.00002216
Iteration 44/1000 | Loss: 0.00002215
Iteration 45/1000 | Loss: 0.00002215
Iteration 46/1000 | Loss: 0.00002215
Iteration 47/1000 | Loss: 0.00002215
Iteration 48/1000 | Loss: 0.00002215
Iteration 49/1000 | Loss: 0.00002215
Iteration 50/1000 | Loss: 0.00002215
Iteration 51/1000 | Loss: 0.00002215
Iteration 52/1000 | Loss: 0.00002215
Iteration 53/1000 | Loss: 0.00002215
Iteration 54/1000 | Loss: 0.00002214
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002214
Iteration 57/1000 | Loss: 0.00002213
Iteration 58/1000 | Loss: 0.00002212
Iteration 59/1000 | Loss: 0.00002212
Iteration 60/1000 | Loss: 0.00002212
Iteration 61/1000 | Loss: 0.00002212
Iteration 62/1000 | Loss: 0.00002211
Iteration 63/1000 | Loss: 0.00002211
Iteration 64/1000 | Loss: 0.00002211
Iteration 65/1000 | Loss: 0.00002211
Iteration 66/1000 | Loss: 0.00002210
Iteration 67/1000 | Loss: 0.00002210
Iteration 68/1000 | Loss: 0.00002209
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002209
Iteration 71/1000 | Loss: 0.00002208
Iteration 72/1000 | Loss: 0.00002208
Iteration 73/1000 | Loss: 0.00002208
Iteration 74/1000 | Loss: 0.00002208
Iteration 75/1000 | Loss: 0.00002208
Iteration 76/1000 | Loss: 0.00002207
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002205
Iteration 80/1000 | Loss: 0.00002205
Iteration 81/1000 | Loss: 0.00002205
Iteration 82/1000 | Loss: 0.00002205
Iteration 83/1000 | Loss: 0.00002205
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002205
Iteration 86/1000 | Loss: 0.00002205
Iteration 87/1000 | Loss: 0.00002205
Iteration 88/1000 | Loss: 0.00002205
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002203
Iteration 96/1000 | Loss: 0.00002203
Iteration 97/1000 | Loss: 0.00002202
Iteration 98/1000 | Loss: 0.00002202
Iteration 99/1000 | Loss: 0.00002202
Iteration 100/1000 | Loss: 0.00002202
Iteration 101/1000 | Loss: 0.00002202
Iteration 102/1000 | Loss: 0.00002202
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002201
Iteration 107/1000 | Loss: 0.00002201
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002200
Iteration 111/1000 | Loss: 0.00002200
Iteration 112/1000 | Loss: 0.00002200
Iteration 113/1000 | Loss: 0.00002200
Iteration 114/1000 | Loss: 0.00002199
Iteration 115/1000 | Loss: 0.00002199
Iteration 116/1000 | Loss: 0.00002199
Iteration 117/1000 | Loss: 0.00002199
Iteration 118/1000 | Loss: 0.00002199
Iteration 119/1000 | Loss: 0.00002198
Iteration 120/1000 | Loss: 0.00002198
Iteration 121/1000 | Loss: 0.00002198
Iteration 122/1000 | Loss: 0.00002198
Iteration 123/1000 | Loss: 0.00002198
Iteration 124/1000 | Loss: 0.00002198
Iteration 125/1000 | Loss: 0.00002198
Iteration 126/1000 | Loss: 0.00002198
Iteration 127/1000 | Loss: 0.00002198
Iteration 128/1000 | Loss: 0.00002198
Iteration 129/1000 | Loss: 0.00002197
Iteration 130/1000 | Loss: 0.00002197
Iteration 131/1000 | Loss: 0.00002197
Iteration 132/1000 | Loss: 0.00002197
Iteration 133/1000 | Loss: 0.00002196
Iteration 134/1000 | Loss: 0.00002196
Iteration 135/1000 | Loss: 0.00002196
Iteration 136/1000 | Loss: 0.00002196
Iteration 137/1000 | Loss: 0.00002196
Iteration 138/1000 | Loss: 0.00002196
Iteration 139/1000 | Loss: 0.00002196
Iteration 140/1000 | Loss: 0.00002196
Iteration 141/1000 | Loss: 0.00002196
Iteration 142/1000 | Loss: 0.00002196
Iteration 143/1000 | Loss: 0.00002196
Iteration 144/1000 | Loss: 0.00002196
Iteration 145/1000 | Loss: 0.00002196
Iteration 146/1000 | Loss: 0.00002196
Iteration 147/1000 | Loss: 0.00002196
Iteration 148/1000 | Loss: 0.00002196
Iteration 149/1000 | Loss: 0.00002196
Iteration 150/1000 | Loss: 0.00002196
Iteration 151/1000 | Loss: 0.00002196
Iteration 152/1000 | Loss: 0.00002196
Iteration 153/1000 | Loss: 0.00002196
Iteration 154/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.1960207959637046e-05, 2.1960207959637046e-05, 2.1960207959637046e-05, 2.1960207959637046e-05, 2.1960207959637046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1960207959637046e-05

Optimization complete. Final v2v error: 3.8786849975585938 mm

Highest mean error: 4.157207012176514 mm for frame 27

Lowest mean error: 3.6930596828460693 mm for frame 128

Saving results

Total time: 36.743364334106445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449132
Iteration 2/25 | Loss: 0.00149580
Iteration 3/25 | Loss: 0.00140209
Iteration 4/25 | Loss: 0.00139284
Iteration 5/25 | Loss: 0.00139040
Iteration 6/25 | Loss: 0.00138975
Iteration 7/25 | Loss: 0.00138975
Iteration 8/25 | Loss: 0.00138975
Iteration 9/25 | Loss: 0.00138975
Iteration 10/25 | Loss: 0.00138975
Iteration 11/25 | Loss: 0.00138975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013897452736273408, 0.0013897452736273408, 0.0013897452736273408, 0.0013897452736273408, 0.0013897452736273408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013897452736273408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34007788
Iteration 2/25 | Loss: 0.00224072
Iteration 3/25 | Loss: 0.00224072
Iteration 4/25 | Loss: 0.00224072
Iteration 5/25 | Loss: 0.00224072
Iteration 6/25 | Loss: 0.00224072
Iteration 7/25 | Loss: 0.00224072
Iteration 8/25 | Loss: 0.00224072
Iteration 9/25 | Loss: 0.00224072
Iteration 10/25 | Loss: 0.00224072
Iteration 11/25 | Loss: 0.00224072
Iteration 12/25 | Loss: 0.00224072
Iteration 13/25 | Loss: 0.00224072
Iteration 14/25 | Loss: 0.00224072
Iteration 15/25 | Loss: 0.00224072
Iteration 16/25 | Loss: 0.00224072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0022407202050089836, 0.0022407202050089836, 0.0022407202050089836, 0.0022407202050089836, 0.0022407202050089836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022407202050089836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224072
Iteration 2/1000 | Loss: 0.00006675
Iteration 3/1000 | Loss: 0.00003294
Iteration 4/1000 | Loss: 0.00002498
Iteration 5/1000 | Loss: 0.00002212
Iteration 6/1000 | Loss: 0.00002107
Iteration 7/1000 | Loss: 0.00002049
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001952
Iteration 10/1000 | Loss: 0.00001926
Iteration 11/1000 | Loss: 0.00001904
Iteration 12/1000 | Loss: 0.00001897
Iteration 13/1000 | Loss: 0.00001896
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001894
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001891
Iteration 22/1000 | Loss: 0.00001886
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001875
Iteration 30/1000 | Loss: 0.00001875
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001873
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001863
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001862
Iteration 46/1000 | Loss: 0.00001862
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001861
Iteration 49/1000 | Loss: 0.00001861
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001852
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001851
Iteration 79/1000 | Loss: 0.00001851
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001850
Iteration 85/1000 | Loss: 0.00001850
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001849
Iteration 89/1000 | Loss: 0.00001849
Iteration 90/1000 | Loss: 0.00001849
Iteration 91/1000 | Loss: 0.00001849
Iteration 92/1000 | Loss: 0.00001849
Iteration 93/1000 | Loss: 0.00001849
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00001848
Iteration 96/1000 | Loss: 0.00001848
Iteration 97/1000 | Loss: 0.00001848
Iteration 98/1000 | Loss: 0.00001848
Iteration 99/1000 | Loss: 0.00001848
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001848
Iteration 104/1000 | Loss: 0.00001848
Iteration 105/1000 | Loss: 0.00001848
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001847
Iteration 110/1000 | Loss: 0.00001847
Iteration 111/1000 | Loss: 0.00001847
Iteration 112/1000 | Loss: 0.00001847
Iteration 113/1000 | Loss: 0.00001847
Iteration 114/1000 | Loss: 0.00001847
Iteration 115/1000 | Loss: 0.00001847
Iteration 116/1000 | Loss: 0.00001847
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001846
Iteration 125/1000 | Loss: 0.00001846
Iteration 126/1000 | Loss: 0.00001846
Iteration 127/1000 | Loss: 0.00001846
Iteration 128/1000 | Loss: 0.00001846
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001845
Iteration 134/1000 | Loss: 0.00001845
Iteration 135/1000 | Loss: 0.00001845
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001845
Iteration 140/1000 | Loss: 0.00001845
Iteration 141/1000 | Loss: 0.00001844
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001844
Iteration 145/1000 | Loss: 0.00001844
Iteration 146/1000 | Loss: 0.00001844
Iteration 147/1000 | Loss: 0.00001844
Iteration 148/1000 | Loss: 0.00001844
Iteration 149/1000 | Loss: 0.00001844
Iteration 150/1000 | Loss: 0.00001843
Iteration 151/1000 | Loss: 0.00001843
Iteration 152/1000 | Loss: 0.00001843
Iteration 153/1000 | Loss: 0.00001843
Iteration 154/1000 | Loss: 0.00001843
Iteration 155/1000 | Loss: 0.00001843
Iteration 156/1000 | Loss: 0.00001843
Iteration 157/1000 | Loss: 0.00001843
Iteration 158/1000 | Loss: 0.00001843
Iteration 159/1000 | Loss: 0.00001843
Iteration 160/1000 | Loss: 0.00001842
Iteration 161/1000 | Loss: 0.00001842
Iteration 162/1000 | Loss: 0.00001842
Iteration 163/1000 | Loss: 0.00001842
Iteration 164/1000 | Loss: 0.00001842
Iteration 165/1000 | Loss: 0.00001842
Iteration 166/1000 | Loss: 0.00001842
Iteration 167/1000 | Loss: 0.00001842
Iteration 168/1000 | Loss: 0.00001842
Iteration 169/1000 | Loss: 0.00001841
Iteration 170/1000 | Loss: 0.00001841
Iteration 171/1000 | Loss: 0.00001841
Iteration 172/1000 | Loss: 0.00001841
Iteration 173/1000 | Loss: 0.00001841
Iteration 174/1000 | Loss: 0.00001841
Iteration 175/1000 | Loss: 0.00001841
Iteration 176/1000 | Loss: 0.00001841
Iteration 177/1000 | Loss: 0.00001841
Iteration 178/1000 | Loss: 0.00001841
Iteration 179/1000 | Loss: 0.00001840
Iteration 180/1000 | Loss: 0.00001840
Iteration 181/1000 | Loss: 0.00001840
Iteration 182/1000 | Loss: 0.00001840
Iteration 183/1000 | Loss: 0.00001840
Iteration 184/1000 | Loss: 0.00001840
Iteration 185/1000 | Loss: 0.00001840
Iteration 186/1000 | Loss: 0.00001839
Iteration 187/1000 | Loss: 0.00001839
Iteration 188/1000 | Loss: 0.00001839
Iteration 189/1000 | Loss: 0.00001839
Iteration 190/1000 | Loss: 0.00001839
Iteration 191/1000 | Loss: 0.00001839
Iteration 192/1000 | Loss: 0.00001839
Iteration 193/1000 | Loss: 0.00001839
Iteration 194/1000 | Loss: 0.00001839
Iteration 195/1000 | Loss: 0.00001839
Iteration 196/1000 | Loss: 0.00001839
Iteration 197/1000 | Loss: 0.00001838
Iteration 198/1000 | Loss: 0.00001838
Iteration 199/1000 | Loss: 0.00001838
Iteration 200/1000 | Loss: 0.00001838
Iteration 201/1000 | Loss: 0.00001838
Iteration 202/1000 | Loss: 0.00001838
Iteration 203/1000 | Loss: 0.00001838
Iteration 204/1000 | Loss: 0.00001838
Iteration 205/1000 | Loss: 0.00001838
Iteration 206/1000 | Loss: 0.00001838
Iteration 207/1000 | Loss: 0.00001838
Iteration 208/1000 | Loss: 0.00001838
Iteration 209/1000 | Loss: 0.00001837
Iteration 210/1000 | Loss: 0.00001837
Iteration 211/1000 | Loss: 0.00001837
Iteration 212/1000 | Loss: 0.00001837
Iteration 213/1000 | Loss: 0.00001837
Iteration 214/1000 | Loss: 0.00001837
Iteration 215/1000 | Loss: 0.00001837
Iteration 216/1000 | Loss: 0.00001837
Iteration 217/1000 | Loss: 0.00001837
Iteration 218/1000 | Loss: 0.00001837
Iteration 219/1000 | Loss: 0.00001837
Iteration 220/1000 | Loss: 0.00001837
Iteration 221/1000 | Loss: 0.00001837
Iteration 222/1000 | Loss: 0.00001837
Iteration 223/1000 | Loss: 0.00001837
Iteration 224/1000 | Loss: 0.00001837
Iteration 225/1000 | Loss: 0.00001837
Iteration 226/1000 | Loss: 0.00001837
Iteration 227/1000 | Loss: 0.00001837
Iteration 228/1000 | Loss: 0.00001837
Iteration 229/1000 | Loss: 0.00001837
Iteration 230/1000 | Loss: 0.00001837
Iteration 231/1000 | Loss: 0.00001837
Iteration 232/1000 | Loss: 0.00001837
Iteration 233/1000 | Loss: 0.00001837
Iteration 234/1000 | Loss: 0.00001837
Iteration 235/1000 | Loss: 0.00001837
Iteration 236/1000 | Loss: 0.00001837
Iteration 237/1000 | Loss: 0.00001837
Iteration 238/1000 | Loss: 0.00001837
Iteration 239/1000 | Loss: 0.00001837
Iteration 240/1000 | Loss: 0.00001837
Iteration 241/1000 | Loss: 0.00001837
Iteration 242/1000 | Loss: 0.00001837
Iteration 243/1000 | Loss: 0.00001837
Iteration 244/1000 | Loss: 0.00001837
Iteration 245/1000 | Loss: 0.00001837
Iteration 246/1000 | Loss: 0.00001837
Iteration 247/1000 | Loss: 0.00001837
Iteration 248/1000 | Loss: 0.00001837
Iteration 249/1000 | Loss: 0.00001837
Iteration 250/1000 | Loss: 0.00001837
Iteration 251/1000 | Loss: 0.00001837
Iteration 252/1000 | Loss: 0.00001837
Iteration 253/1000 | Loss: 0.00001837
Iteration 254/1000 | Loss: 0.00001837
Iteration 255/1000 | Loss: 0.00001837
Iteration 256/1000 | Loss: 0.00001837
Iteration 257/1000 | Loss: 0.00001837
Iteration 258/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.836570845625829e-05, 1.836570845625829e-05, 1.836570845625829e-05, 1.836570845625829e-05, 1.836570845625829e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.836570845625829e-05

Optimization complete. Final v2v error: 3.5872738361358643 mm

Highest mean error: 4.677204132080078 mm for frame 47

Lowest mean error: 3.2448930740356445 mm for frame 63

Saving results

Total time: 42.86862230300903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846454
Iteration 2/25 | Loss: 0.00156755
Iteration 3/25 | Loss: 0.00144809
Iteration 4/25 | Loss: 0.00144100
Iteration 5/25 | Loss: 0.00144050
Iteration 6/25 | Loss: 0.00144050
Iteration 7/25 | Loss: 0.00144050
Iteration 8/25 | Loss: 0.00144050
Iteration 9/25 | Loss: 0.00144050
Iteration 10/25 | Loss: 0.00144050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014404956018552184, 0.0014404956018552184, 0.0014404956018552184, 0.0014404956018552184, 0.0014404956018552184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014404956018552184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19256675
Iteration 2/25 | Loss: 0.00170454
Iteration 3/25 | Loss: 0.00170454
Iteration 4/25 | Loss: 0.00170454
Iteration 5/25 | Loss: 0.00170454
Iteration 6/25 | Loss: 0.00170454
Iteration 7/25 | Loss: 0.00170454
Iteration 8/25 | Loss: 0.00170454
Iteration 9/25 | Loss: 0.00170454
Iteration 10/25 | Loss: 0.00170454
Iteration 11/25 | Loss: 0.00170454
Iteration 12/25 | Loss: 0.00170454
Iteration 13/25 | Loss: 0.00170454
Iteration 14/25 | Loss: 0.00170454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017045361455529928, 0.0017045361455529928, 0.0017045361455529928, 0.0017045361455529928, 0.0017045361455529928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017045361455529928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170454
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00002308
Iteration 4/1000 | Loss: 0.00002067
Iteration 5/1000 | Loss: 0.00001905
Iteration 6/1000 | Loss: 0.00001835
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001736
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001691
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001683
Iteration 15/1000 | Loss: 0.00001668
Iteration 16/1000 | Loss: 0.00001661
Iteration 17/1000 | Loss: 0.00001660
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001651
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001648
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001646
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001645
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00001644
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001643
Iteration 43/1000 | Loss: 0.00001643
Iteration 44/1000 | Loss: 0.00001643
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001642
Iteration 47/1000 | Loss: 0.00001642
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001639
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001636
Iteration 87/1000 | Loss: 0.00001636
Iteration 88/1000 | Loss: 0.00001636
Iteration 89/1000 | Loss: 0.00001636
Iteration 90/1000 | Loss: 0.00001636
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001635
Iteration 94/1000 | Loss: 0.00001635
Iteration 95/1000 | Loss: 0.00001635
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001634
Iteration 106/1000 | Loss: 0.00001634
Iteration 107/1000 | Loss: 0.00001633
Iteration 108/1000 | Loss: 0.00001632
Iteration 109/1000 | Loss: 0.00001632
Iteration 110/1000 | Loss: 0.00001632
Iteration 111/1000 | Loss: 0.00001632
Iteration 112/1000 | Loss: 0.00001632
Iteration 113/1000 | Loss: 0.00001632
Iteration 114/1000 | Loss: 0.00001632
Iteration 115/1000 | Loss: 0.00001631
Iteration 116/1000 | Loss: 0.00001631
Iteration 117/1000 | Loss: 0.00001630
Iteration 118/1000 | Loss: 0.00001630
Iteration 119/1000 | Loss: 0.00001630
Iteration 120/1000 | Loss: 0.00001630
Iteration 121/1000 | Loss: 0.00001630
Iteration 122/1000 | Loss: 0.00001630
Iteration 123/1000 | Loss: 0.00001630
Iteration 124/1000 | Loss: 0.00001630
Iteration 125/1000 | Loss: 0.00001630
Iteration 126/1000 | Loss: 0.00001630
Iteration 127/1000 | Loss: 0.00001630
Iteration 128/1000 | Loss: 0.00001629
Iteration 129/1000 | Loss: 0.00001629
Iteration 130/1000 | Loss: 0.00001629
Iteration 131/1000 | Loss: 0.00001629
Iteration 132/1000 | Loss: 0.00001629
Iteration 133/1000 | Loss: 0.00001629
Iteration 134/1000 | Loss: 0.00001629
Iteration 135/1000 | Loss: 0.00001629
Iteration 136/1000 | Loss: 0.00001629
Iteration 137/1000 | Loss: 0.00001629
Iteration 138/1000 | Loss: 0.00001628
Iteration 139/1000 | Loss: 0.00001628
Iteration 140/1000 | Loss: 0.00001628
Iteration 141/1000 | Loss: 0.00001628
Iteration 142/1000 | Loss: 0.00001628
Iteration 143/1000 | Loss: 0.00001628
Iteration 144/1000 | Loss: 0.00001628
Iteration 145/1000 | Loss: 0.00001628
Iteration 146/1000 | Loss: 0.00001628
Iteration 147/1000 | Loss: 0.00001628
Iteration 148/1000 | Loss: 0.00001628
Iteration 149/1000 | Loss: 0.00001628
Iteration 150/1000 | Loss: 0.00001628
Iteration 151/1000 | Loss: 0.00001628
Iteration 152/1000 | Loss: 0.00001628
Iteration 153/1000 | Loss: 0.00001628
Iteration 154/1000 | Loss: 0.00001628
Iteration 155/1000 | Loss: 0.00001628
Iteration 156/1000 | Loss: 0.00001628
Iteration 157/1000 | Loss: 0.00001628
Iteration 158/1000 | Loss: 0.00001628
Iteration 159/1000 | Loss: 0.00001628
Iteration 160/1000 | Loss: 0.00001628
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.627652636670973e-05, 1.627652636670973e-05, 1.627652636670973e-05, 1.627652636670973e-05, 1.627652636670973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.627652636670973e-05

Optimization complete. Final v2v error: 3.4695169925689697 mm

Highest mean error: 3.72482967376709 mm for frame 216

Lowest mean error: 3.1952340602874756 mm for frame 84

Saving results

Total time: 38.73556661605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826746
Iteration 2/25 | Loss: 0.00180970
Iteration 3/25 | Loss: 0.00165222
Iteration 4/25 | Loss: 0.00153461
Iteration 5/25 | Loss: 0.00152126
Iteration 6/25 | Loss: 0.00152577
Iteration 7/25 | Loss: 0.00150669
Iteration 8/25 | Loss: 0.00150622
Iteration 9/25 | Loss: 0.00150611
Iteration 10/25 | Loss: 0.00150610
Iteration 11/25 | Loss: 0.00150606
Iteration 12/25 | Loss: 0.00150606
Iteration 13/25 | Loss: 0.00150606
Iteration 14/25 | Loss: 0.00150606
Iteration 15/25 | Loss: 0.00150606
Iteration 16/25 | Loss: 0.00150606
Iteration 17/25 | Loss: 0.00150606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001506059430539608, 0.001506059430539608, 0.001506059430539608, 0.001506059430539608, 0.001506059430539608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001506059430539608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.95006585
Iteration 2/25 | Loss: 0.00259789
Iteration 3/25 | Loss: 0.00259789
Iteration 4/25 | Loss: 0.00259789
Iteration 5/25 | Loss: 0.00259789
Iteration 6/25 | Loss: 0.00259789
Iteration 7/25 | Loss: 0.00259789
Iteration 8/25 | Loss: 0.00259789
Iteration 9/25 | Loss: 0.00259789
Iteration 10/25 | Loss: 0.00259789
Iteration 11/25 | Loss: 0.00259789
Iteration 12/25 | Loss: 0.00259789
Iteration 13/25 | Loss: 0.00259789
Iteration 14/25 | Loss: 0.00259789
Iteration 15/25 | Loss: 0.00259789
Iteration 16/25 | Loss: 0.00259789
Iteration 17/25 | Loss: 0.00259789
Iteration 18/25 | Loss: 0.00259789
Iteration 19/25 | Loss: 0.00259789
Iteration 20/25 | Loss: 0.00259789
Iteration 21/25 | Loss: 0.00259789
Iteration 22/25 | Loss: 0.00259789
Iteration 23/25 | Loss: 0.00259789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0025978905614465475, 0.0025978905614465475, 0.0025978905614465475, 0.0025978905614465475, 0.0025978905614465475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025978905614465475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259789
Iteration 2/1000 | Loss: 0.00004641
Iteration 3/1000 | Loss: 0.00005067
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002913
Iteration 6/1000 | Loss: 0.00004674
Iteration 7/1000 | Loss: 0.00003452
Iteration 8/1000 | Loss: 0.00002770
Iteration 9/1000 | Loss: 0.00002724
Iteration 10/1000 | Loss: 0.00002686
Iteration 11/1000 | Loss: 0.00004225
Iteration 12/1000 | Loss: 0.00003408
Iteration 13/1000 | Loss: 0.00002662
Iteration 14/1000 | Loss: 0.00002648
Iteration 15/1000 | Loss: 0.00002635
Iteration 16/1000 | Loss: 0.00002635
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002623
Iteration 19/1000 | Loss: 0.00002621
Iteration 20/1000 | Loss: 0.00002618
Iteration 21/1000 | Loss: 0.00002618
Iteration 22/1000 | Loss: 0.00002618
Iteration 23/1000 | Loss: 0.00002616
Iteration 24/1000 | Loss: 0.00002615
Iteration 25/1000 | Loss: 0.00002615
Iteration 26/1000 | Loss: 0.00002615
Iteration 27/1000 | Loss: 0.00002615
Iteration 28/1000 | Loss: 0.00002615
Iteration 29/1000 | Loss: 0.00002614
Iteration 30/1000 | Loss: 0.00002614
Iteration 31/1000 | Loss: 0.00002614
Iteration 32/1000 | Loss: 0.00002613
Iteration 33/1000 | Loss: 0.00002613
Iteration 34/1000 | Loss: 0.00002613
Iteration 35/1000 | Loss: 0.00002612
Iteration 36/1000 | Loss: 0.00002612
Iteration 37/1000 | Loss: 0.00002612
Iteration 38/1000 | Loss: 0.00002611
Iteration 39/1000 | Loss: 0.00002611
Iteration 40/1000 | Loss: 0.00002611
Iteration 41/1000 | Loss: 0.00002611
Iteration 42/1000 | Loss: 0.00002611
Iteration 43/1000 | Loss: 0.00002611
Iteration 44/1000 | Loss: 0.00002610
Iteration 45/1000 | Loss: 0.00002610
Iteration 46/1000 | Loss: 0.00002610
Iteration 47/1000 | Loss: 0.00002610
Iteration 48/1000 | Loss: 0.00002609
Iteration 49/1000 | Loss: 0.00002609
Iteration 50/1000 | Loss: 0.00002608
Iteration 51/1000 | Loss: 0.00002608
Iteration 52/1000 | Loss: 0.00002608
Iteration 53/1000 | Loss: 0.00002608
Iteration 54/1000 | Loss: 0.00002608
Iteration 55/1000 | Loss: 0.00002608
Iteration 56/1000 | Loss: 0.00002607
Iteration 57/1000 | Loss: 0.00002607
Iteration 58/1000 | Loss: 0.00002607
Iteration 59/1000 | Loss: 0.00002607
Iteration 60/1000 | Loss: 0.00002607
Iteration 61/1000 | Loss: 0.00002607
Iteration 62/1000 | Loss: 0.00002606
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002605
Iteration 65/1000 | Loss: 0.00002605
Iteration 66/1000 | Loss: 0.00002605
Iteration 67/1000 | Loss: 0.00002605
Iteration 68/1000 | Loss: 0.00002605
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002604
Iteration 71/1000 | Loss: 0.00002604
Iteration 72/1000 | Loss: 0.00002604
Iteration 73/1000 | Loss: 0.00002604
Iteration 74/1000 | Loss: 0.00002603
Iteration 75/1000 | Loss: 0.00002603
Iteration 76/1000 | Loss: 0.00002603
Iteration 77/1000 | Loss: 0.00002603
Iteration 78/1000 | Loss: 0.00002603
Iteration 79/1000 | Loss: 0.00002603
Iteration 80/1000 | Loss: 0.00002602
Iteration 81/1000 | Loss: 0.00002602
Iteration 82/1000 | Loss: 0.00002602
Iteration 83/1000 | Loss: 0.00002602
Iteration 84/1000 | Loss: 0.00002602
Iteration 85/1000 | Loss: 0.00002602
Iteration 86/1000 | Loss: 0.00002602
Iteration 87/1000 | Loss: 0.00002602
Iteration 88/1000 | Loss: 0.00002602
Iteration 89/1000 | Loss: 0.00002602
Iteration 90/1000 | Loss: 0.00002602
Iteration 91/1000 | Loss: 0.00002602
Iteration 92/1000 | Loss: 0.00002602
Iteration 93/1000 | Loss: 0.00002602
Iteration 94/1000 | Loss: 0.00002602
Iteration 95/1000 | Loss: 0.00002602
Iteration 96/1000 | Loss: 0.00002602
Iteration 97/1000 | Loss: 0.00002602
Iteration 98/1000 | Loss: 0.00002602
Iteration 99/1000 | Loss: 0.00002602
Iteration 100/1000 | Loss: 0.00002602
Iteration 101/1000 | Loss: 0.00002602
Iteration 102/1000 | Loss: 0.00002602
Iteration 103/1000 | Loss: 0.00002602
Iteration 104/1000 | Loss: 0.00002602
Iteration 105/1000 | Loss: 0.00002602
Iteration 106/1000 | Loss: 0.00002602
Iteration 107/1000 | Loss: 0.00002602
Iteration 108/1000 | Loss: 0.00002602
Iteration 109/1000 | Loss: 0.00002602
Iteration 110/1000 | Loss: 0.00002602
Iteration 111/1000 | Loss: 0.00002602
Iteration 112/1000 | Loss: 0.00002602
Iteration 113/1000 | Loss: 0.00002602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.6021531084552407e-05, 2.6021531084552407e-05, 2.6021531084552407e-05, 2.6021531084552407e-05, 2.6021531084552407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6021531084552407e-05

Optimization complete. Final v2v error: 4.32932710647583 mm

Highest mean error: 4.910439491271973 mm for frame 77

Lowest mean error: 3.520871639251709 mm for frame 226

Saving results

Total time: 49.762383460998535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01236173
Iteration 2/25 | Loss: 0.00306926
Iteration 3/25 | Loss: 0.00200907
Iteration 4/25 | Loss: 0.00191529
Iteration 5/25 | Loss: 0.00203792
Iteration 6/25 | Loss: 0.00187592
Iteration 7/25 | Loss: 0.00172359
Iteration 8/25 | Loss: 0.00173005
Iteration 9/25 | Loss: 0.00175421
Iteration 10/25 | Loss: 0.00166373
Iteration 11/25 | Loss: 0.00165384
Iteration 12/25 | Loss: 0.00162282
Iteration 13/25 | Loss: 0.00166337
Iteration 14/25 | Loss: 0.00161107
Iteration 15/25 | Loss: 0.00160895
Iteration 16/25 | Loss: 0.00160791
Iteration 17/25 | Loss: 0.00160558
Iteration 18/25 | Loss: 0.00160325
Iteration 19/25 | Loss: 0.00159979
Iteration 20/25 | Loss: 0.00161821
Iteration 21/25 | Loss: 0.00159159
Iteration 22/25 | Loss: 0.00158709
Iteration 23/25 | Loss: 0.00158657
Iteration 24/25 | Loss: 0.00158645
Iteration 25/25 | Loss: 0.00158635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.75025672
Iteration 2/25 | Loss: 0.00402148
Iteration 3/25 | Loss: 0.00402148
Iteration 4/25 | Loss: 0.00402147
Iteration 5/25 | Loss: 0.00402147
Iteration 6/25 | Loss: 0.00402147
Iteration 7/25 | Loss: 0.00402147
Iteration 8/25 | Loss: 0.00402147
Iteration 9/25 | Loss: 0.00402147
Iteration 10/25 | Loss: 0.00402147
Iteration 11/25 | Loss: 0.00402147
Iteration 12/25 | Loss: 0.00402147
Iteration 13/25 | Loss: 0.00402147
Iteration 14/25 | Loss: 0.00402147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.004021471831947565, 0.004021471831947565, 0.004021471831947565, 0.004021471831947565, 0.004021471831947565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004021471831947565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00402147
Iteration 2/1000 | Loss: 0.00097930
Iteration 3/1000 | Loss: 0.00140866
Iteration 4/1000 | Loss: 0.00027047
Iteration 5/1000 | Loss: 0.00024064
Iteration 6/1000 | Loss: 0.00021542
Iteration 7/1000 | Loss: 0.00019883
Iteration 8/1000 | Loss: 0.00018675
Iteration 9/1000 | Loss: 0.00083442
Iteration 10/1000 | Loss: 0.00115087
Iteration 11/1000 | Loss: 0.00093118
Iteration 12/1000 | Loss: 0.00071770
Iteration 13/1000 | Loss: 0.00019269
Iteration 14/1000 | Loss: 0.00065201
Iteration 15/1000 | Loss: 0.00034223
Iteration 16/1000 | Loss: 0.00017199
Iteration 17/1000 | Loss: 0.00060742
Iteration 18/1000 | Loss: 0.00099117
Iteration 19/1000 | Loss: 0.00091773
Iteration 20/1000 | Loss: 0.00090497
Iteration 21/1000 | Loss: 0.00077454
Iteration 22/1000 | Loss: 0.00069835
Iteration 23/1000 | Loss: 0.00017603
Iteration 24/1000 | Loss: 0.00052248
Iteration 25/1000 | Loss: 0.00126925
Iteration 26/1000 | Loss: 0.00043783
Iteration 27/1000 | Loss: 0.00049936
Iteration 28/1000 | Loss: 0.00134665
Iteration 29/1000 | Loss: 0.00065958
Iteration 30/1000 | Loss: 0.00033176
Iteration 31/1000 | Loss: 0.00044422
Iteration 32/1000 | Loss: 0.00016607
Iteration 33/1000 | Loss: 0.00015783
Iteration 34/1000 | Loss: 0.00015408
Iteration 35/1000 | Loss: 0.00112933
Iteration 36/1000 | Loss: 0.00068733
Iteration 37/1000 | Loss: 0.00075157
Iteration 38/1000 | Loss: 0.00118059
Iteration 39/1000 | Loss: 0.00064203
Iteration 40/1000 | Loss: 0.00069446
Iteration 41/1000 | Loss: 0.00015490
Iteration 42/1000 | Loss: 0.00015138
Iteration 43/1000 | Loss: 0.00014973
Iteration 44/1000 | Loss: 0.00014776
Iteration 45/1000 | Loss: 0.00159674
Iteration 46/1000 | Loss: 0.00081213
Iteration 47/1000 | Loss: 0.00084929
Iteration 48/1000 | Loss: 0.00127847
Iteration 49/1000 | Loss: 0.00067059
Iteration 50/1000 | Loss: 0.00063141
Iteration 51/1000 | Loss: 0.00018372
Iteration 52/1000 | Loss: 0.00016433
Iteration 53/1000 | Loss: 0.00014867
Iteration 54/1000 | Loss: 0.00099957
Iteration 55/1000 | Loss: 0.00070603
Iteration 56/1000 | Loss: 0.00068695
Iteration 57/1000 | Loss: 0.00017645
Iteration 58/1000 | Loss: 0.00016144
Iteration 59/1000 | Loss: 0.00073113
Iteration 60/1000 | Loss: 0.00016568
Iteration 61/1000 | Loss: 0.00015436
Iteration 62/1000 | Loss: 0.00161408
Iteration 63/1000 | Loss: 0.00157982
Iteration 64/1000 | Loss: 0.00054009
Iteration 65/1000 | Loss: 0.00014553
Iteration 66/1000 | Loss: 0.00087946
Iteration 67/1000 | Loss: 0.00028829
Iteration 68/1000 | Loss: 0.00013842
Iteration 69/1000 | Loss: 0.00163646
Iteration 70/1000 | Loss: 0.00038153
Iteration 71/1000 | Loss: 0.00140948
Iteration 72/1000 | Loss: 0.00026249
Iteration 73/1000 | Loss: 0.00013450
Iteration 74/1000 | Loss: 0.00013304
Iteration 75/1000 | Loss: 0.00013191
Iteration 76/1000 | Loss: 0.00106639
Iteration 77/1000 | Loss: 0.00052836
Iteration 78/1000 | Loss: 0.00014174
Iteration 79/1000 | Loss: 0.00013350
Iteration 80/1000 | Loss: 0.00012922
Iteration 81/1000 | Loss: 0.00012770
Iteration 82/1000 | Loss: 0.00012697
Iteration 83/1000 | Loss: 0.00087810
Iteration 84/1000 | Loss: 0.00030756
Iteration 85/1000 | Loss: 0.00014969
Iteration 86/1000 | Loss: 0.00073952
Iteration 87/1000 | Loss: 0.00027939
Iteration 88/1000 | Loss: 0.00013388
Iteration 89/1000 | Loss: 0.00057372
Iteration 90/1000 | Loss: 0.00084856
Iteration 91/1000 | Loss: 0.00013687
Iteration 92/1000 | Loss: 0.00095921
Iteration 93/1000 | Loss: 0.00073277
Iteration 94/1000 | Loss: 0.00012624
Iteration 95/1000 | Loss: 0.00046818
Iteration 96/1000 | Loss: 0.00044586
Iteration 97/1000 | Loss: 0.00013198
Iteration 98/1000 | Loss: 0.00012519
Iteration 99/1000 | Loss: 0.00012360
Iteration 100/1000 | Loss: 0.00012254
Iteration 101/1000 | Loss: 0.00012167
Iteration 102/1000 | Loss: 0.00012099
Iteration 103/1000 | Loss: 0.00012041
Iteration 104/1000 | Loss: 0.00011998
Iteration 105/1000 | Loss: 0.00073709
Iteration 106/1000 | Loss: 0.00078175
Iteration 107/1000 | Loss: 0.00063735
Iteration 108/1000 | Loss: 0.00012818
Iteration 109/1000 | Loss: 0.00012058
Iteration 110/1000 | Loss: 0.00011767
Iteration 111/1000 | Loss: 0.00011681
Iteration 112/1000 | Loss: 0.00011612
Iteration 113/1000 | Loss: 0.00011575
Iteration 114/1000 | Loss: 0.00011550
Iteration 115/1000 | Loss: 0.00011528
Iteration 116/1000 | Loss: 0.00011501
Iteration 117/1000 | Loss: 0.00011488
Iteration 118/1000 | Loss: 0.00011471
Iteration 119/1000 | Loss: 0.00011466
Iteration 120/1000 | Loss: 0.00011464
Iteration 121/1000 | Loss: 0.00011462
Iteration 122/1000 | Loss: 0.00011461
Iteration 123/1000 | Loss: 0.00011460
Iteration 124/1000 | Loss: 0.00011458
Iteration 125/1000 | Loss: 0.00011458
Iteration 126/1000 | Loss: 0.00011458
Iteration 127/1000 | Loss: 0.00011457
Iteration 128/1000 | Loss: 0.00011456
Iteration 129/1000 | Loss: 0.00011456
Iteration 130/1000 | Loss: 0.00011453
Iteration 131/1000 | Loss: 0.00011448
Iteration 132/1000 | Loss: 0.00011448
Iteration 133/1000 | Loss: 0.00011446
Iteration 134/1000 | Loss: 0.00011445
Iteration 135/1000 | Loss: 0.00011444
Iteration 136/1000 | Loss: 0.00011444
Iteration 137/1000 | Loss: 0.00011444
Iteration 138/1000 | Loss: 0.00011444
Iteration 139/1000 | Loss: 0.00011443
Iteration 140/1000 | Loss: 0.00011443
Iteration 141/1000 | Loss: 0.00011443
Iteration 142/1000 | Loss: 0.00011443
Iteration 143/1000 | Loss: 0.00011442
Iteration 144/1000 | Loss: 0.00011442
Iteration 145/1000 | Loss: 0.00011442
Iteration 146/1000 | Loss: 0.00011441
Iteration 147/1000 | Loss: 0.00011441
Iteration 148/1000 | Loss: 0.00011441
Iteration 149/1000 | Loss: 0.00011441
Iteration 150/1000 | Loss: 0.00011441
Iteration 151/1000 | Loss: 0.00011441
Iteration 152/1000 | Loss: 0.00011441
Iteration 153/1000 | Loss: 0.00011440
Iteration 154/1000 | Loss: 0.00011440
Iteration 155/1000 | Loss: 0.00011440
Iteration 156/1000 | Loss: 0.00011440
Iteration 157/1000 | Loss: 0.00011440
Iteration 158/1000 | Loss: 0.00011440
Iteration 159/1000 | Loss: 0.00011440
Iteration 160/1000 | Loss: 0.00011440
Iteration 161/1000 | Loss: 0.00011440
Iteration 162/1000 | Loss: 0.00011440
Iteration 163/1000 | Loss: 0.00011440
Iteration 164/1000 | Loss: 0.00011440
Iteration 165/1000 | Loss: 0.00011440
Iteration 166/1000 | Loss: 0.00011439
Iteration 167/1000 | Loss: 0.00011439
Iteration 168/1000 | Loss: 0.00011439
Iteration 169/1000 | Loss: 0.00011439
Iteration 170/1000 | Loss: 0.00011439
Iteration 171/1000 | Loss: 0.00011439
Iteration 172/1000 | Loss: 0.00011439
Iteration 173/1000 | Loss: 0.00011439
Iteration 174/1000 | Loss: 0.00011438
Iteration 175/1000 | Loss: 0.00011438
Iteration 176/1000 | Loss: 0.00011438
Iteration 177/1000 | Loss: 0.00011438
Iteration 178/1000 | Loss: 0.00011438
Iteration 179/1000 | Loss: 0.00011438
Iteration 180/1000 | Loss: 0.00011438
Iteration 181/1000 | Loss: 0.00011437
Iteration 182/1000 | Loss: 0.00011437
Iteration 183/1000 | Loss: 0.00011437
Iteration 184/1000 | Loss: 0.00011437
Iteration 185/1000 | Loss: 0.00011437
Iteration 186/1000 | Loss: 0.00011437
Iteration 187/1000 | Loss: 0.00011437
Iteration 188/1000 | Loss: 0.00011437
Iteration 189/1000 | Loss: 0.00011437
Iteration 190/1000 | Loss: 0.00011437
Iteration 191/1000 | Loss: 0.00011437
Iteration 192/1000 | Loss: 0.00011437
Iteration 193/1000 | Loss: 0.00011437
Iteration 194/1000 | Loss: 0.00011437
Iteration 195/1000 | Loss: 0.00011437
Iteration 196/1000 | Loss: 0.00011437
Iteration 197/1000 | Loss: 0.00011437
Iteration 198/1000 | Loss: 0.00011437
Iteration 199/1000 | Loss: 0.00011437
Iteration 200/1000 | Loss: 0.00011437
Iteration 201/1000 | Loss: 0.00011437
Iteration 202/1000 | Loss: 0.00011437
Iteration 203/1000 | Loss: 0.00011437
Iteration 204/1000 | Loss: 0.00011437
Iteration 205/1000 | Loss: 0.00011437
Iteration 206/1000 | Loss: 0.00011437
Iteration 207/1000 | Loss: 0.00011437
Iteration 208/1000 | Loss: 0.00011437
Iteration 209/1000 | Loss: 0.00011437
Iteration 210/1000 | Loss: 0.00011437
Iteration 211/1000 | Loss: 0.00011437
Iteration 212/1000 | Loss: 0.00011437
Iteration 213/1000 | Loss: 0.00011437
Iteration 214/1000 | Loss: 0.00011437
Iteration 215/1000 | Loss: 0.00011437
Iteration 216/1000 | Loss: 0.00011437
Iteration 217/1000 | Loss: 0.00011437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [0.0001143702247645706, 0.0001143702247645706, 0.0001143702247645706, 0.0001143702247645706, 0.0001143702247645706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001143702247645706

Optimization complete. Final v2v error: 6.849702835083008 mm

Highest mean error: 12.397216796875 mm for frame 13

Lowest mean error: 5.121550559997559 mm for frame 2

Saving results

Total time: 224.72270274162292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059493
Iteration 2/25 | Loss: 0.00227230
Iteration 3/25 | Loss: 0.00199670
Iteration 4/25 | Loss: 0.00173427
Iteration 5/25 | Loss: 0.00186400
Iteration 6/25 | Loss: 0.00173793
Iteration 7/25 | Loss: 0.00164223
Iteration 8/25 | Loss: 0.00153159
Iteration 9/25 | Loss: 0.00147496
Iteration 10/25 | Loss: 0.00146279
Iteration 11/25 | Loss: 0.00152564
Iteration 12/25 | Loss: 0.00145211
Iteration 13/25 | Loss: 0.00144550
Iteration 14/25 | Loss: 0.00144467
Iteration 15/25 | Loss: 0.00144407
Iteration 16/25 | Loss: 0.00144392
Iteration 17/25 | Loss: 0.00144388
Iteration 18/25 | Loss: 0.00144388
Iteration 19/25 | Loss: 0.00144387
Iteration 20/25 | Loss: 0.00144387
Iteration 21/25 | Loss: 0.00144387
Iteration 22/25 | Loss: 0.00144387
Iteration 23/25 | Loss: 0.00144387
Iteration 24/25 | Loss: 0.00144386
Iteration 25/25 | Loss: 0.00144386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65371871
Iteration 2/25 | Loss: 0.00219372
Iteration 3/25 | Loss: 0.00219372
Iteration 4/25 | Loss: 0.00219371
Iteration 5/25 | Loss: 0.00219371
Iteration 6/25 | Loss: 0.00219371
Iteration 7/25 | Loss: 0.00219371
Iteration 8/25 | Loss: 0.00219371
Iteration 9/25 | Loss: 0.00219371
Iteration 10/25 | Loss: 0.00219371
Iteration 11/25 | Loss: 0.00219371
Iteration 12/25 | Loss: 0.00219371
Iteration 13/25 | Loss: 0.00219371
Iteration 14/25 | Loss: 0.00219371
Iteration 15/25 | Loss: 0.00219371
Iteration 16/25 | Loss: 0.00219371
Iteration 17/25 | Loss: 0.00219371
Iteration 18/25 | Loss: 0.00219371
Iteration 19/25 | Loss: 0.00219371
Iteration 20/25 | Loss: 0.00219371
Iteration 21/25 | Loss: 0.00219371
Iteration 22/25 | Loss: 0.00219371
Iteration 23/25 | Loss: 0.00219371
Iteration 24/25 | Loss: 0.00219371
Iteration 25/25 | Loss: 0.00219371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219371
Iteration 2/1000 | Loss: 0.00384306
Iteration 3/1000 | Loss: 0.00007568
Iteration 4/1000 | Loss: 0.00006290
Iteration 5/1000 | Loss: 0.00005530
Iteration 6/1000 | Loss: 0.00023007
Iteration 7/1000 | Loss: 0.00017538
Iteration 8/1000 | Loss: 0.00005085
Iteration 9/1000 | Loss: 0.00004196
Iteration 10/1000 | Loss: 0.00003626
Iteration 11/1000 | Loss: 0.00027232
Iteration 12/1000 | Loss: 0.00004257
Iteration 13/1000 | Loss: 0.00003071
Iteration 14/1000 | Loss: 0.00002701
Iteration 15/1000 | Loss: 0.00002482
Iteration 16/1000 | Loss: 0.00002370
Iteration 17/1000 | Loss: 0.00002306
Iteration 18/1000 | Loss: 0.00002259
Iteration 19/1000 | Loss: 0.00002226
Iteration 20/1000 | Loss: 0.00002203
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002169
Iteration 23/1000 | Loss: 0.00002156
Iteration 24/1000 | Loss: 0.00002145
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002131
Iteration 29/1000 | Loss: 0.00002130
Iteration 30/1000 | Loss: 0.00002129
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002127
Iteration 34/1000 | Loss: 0.00002126
Iteration 35/1000 | Loss: 0.00002126
Iteration 36/1000 | Loss: 0.00002126
Iteration 37/1000 | Loss: 0.00002125
Iteration 38/1000 | Loss: 0.00002125
Iteration 39/1000 | Loss: 0.00002125
Iteration 40/1000 | Loss: 0.00002121
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002119
Iteration 46/1000 | Loss: 0.00002119
Iteration 47/1000 | Loss: 0.00002118
Iteration 48/1000 | Loss: 0.00002118
Iteration 49/1000 | Loss: 0.00002118
Iteration 50/1000 | Loss: 0.00002118
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002116
Iteration 55/1000 | Loss: 0.00002116
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002115
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002114
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002113
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002112
Iteration 72/1000 | Loss: 0.00002112
Iteration 73/1000 | Loss: 0.00002112
Iteration 74/1000 | Loss: 0.00002112
Iteration 75/1000 | Loss: 0.00002112
Iteration 76/1000 | Loss: 0.00002112
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002111
Iteration 80/1000 | Loss: 0.00002111
Iteration 81/1000 | Loss: 0.00002111
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002110
Iteration 84/1000 | Loss: 0.00002110
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002110
Iteration 87/1000 | Loss: 0.00002110
Iteration 88/1000 | Loss: 0.00002110
Iteration 89/1000 | Loss: 0.00002110
Iteration 90/1000 | Loss: 0.00002109
Iteration 91/1000 | Loss: 0.00002109
Iteration 92/1000 | Loss: 0.00002109
Iteration 93/1000 | Loss: 0.00002109
Iteration 94/1000 | Loss: 0.00002109
Iteration 95/1000 | Loss: 0.00002109
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002109
Iteration 99/1000 | Loss: 0.00002109
Iteration 100/1000 | Loss: 0.00002109
Iteration 101/1000 | Loss: 0.00002108
Iteration 102/1000 | Loss: 0.00002108
Iteration 103/1000 | Loss: 0.00002108
Iteration 104/1000 | Loss: 0.00002108
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002108
Iteration 107/1000 | Loss: 0.00002107
Iteration 108/1000 | Loss: 0.00002107
Iteration 109/1000 | Loss: 0.00002107
Iteration 110/1000 | Loss: 0.00002107
Iteration 111/1000 | Loss: 0.00002107
Iteration 112/1000 | Loss: 0.00002107
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002107
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002106
Iteration 124/1000 | Loss: 0.00002106
Iteration 125/1000 | Loss: 0.00002106
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002106
Iteration 130/1000 | Loss: 0.00002106
Iteration 131/1000 | Loss: 0.00002106
Iteration 132/1000 | Loss: 0.00002106
Iteration 133/1000 | Loss: 0.00002106
Iteration 134/1000 | Loss: 0.00002106
Iteration 135/1000 | Loss: 0.00002106
Iteration 136/1000 | Loss: 0.00002106
Iteration 137/1000 | Loss: 0.00002106
Iteration 138/1000 | Loss: 0.00002106
Iteration 139/1000 | Loss: 0.00002106
Iteration 140/1000 | Loss: 0.00002106
Iteration 141/1000 | Loss: 0.00002106
Iteration 142/1000 | Loss: 0.00002106
Iteration 143/1000 | Loss: 0.00002106
Iteration 144/1000 | Loss: 0.00002106
Iteration 145/1000 | Loss: 0.00002106
Iteration 146/1000 | Loss: 0.00002106
Iteration 147/1000 | Loss: 0.00002106
Iteration 148/1000 | Loss: 0.00002106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1063486201455817e-05, 2.1063486201455817e-05, 2.1063486201455817e-05, 2.1063486201455817e-05, 2.1063486201455817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1063486201455817e-05

Optimization complete. Final v2v error: 3.8170387744903564 mm

Highest mean error: 5.298091411590576 mm for frame 64

Lowest mean error: 3.245849370956421 mm for frame 102

Saving results

Total time: 71.20435881614685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01089259
Iteration 2/25 | Loss: 0.00306121
Iteration 3/25 | Loss: 0.00233700
Iteration 4/25 | Loss: 0.00230769
Iteration 5/25 | Loss: 0.00188605
Iteration 6/25 | Loss: 0.00162220
Iteration 7/25 | Loss: 0.00154172
Iteration 8/25 | Loss: 0.00151569
Iteration 9/25 | Loss: 0.00149867
Iteration 10/25 | Loss: 0.00150105
Iteration 11/25 | Loss: 0.00149367
Iteration 12/25 | Loss: 0.00149380
Iteration 13/25 | Loss: 0.00149197
Iteration 14/25 | Loss: 0.00149167
Iteration 15/25 | Loss: 0.00148792
Iteration 16/25 | Loss: 0.00149102
Iteration 17/25 | Loss: 0.00149054
Iteration 18/25 | Loss: 0.00148893
Iteration 19/25 | Loss: 0.00149125
Iteration 20/25 | Loss: 0.00149065
Iteration 21/25 | Loss: 0.00148908
Iteration 22/25 | Loss: 0.00149081
Iteration 23/25 | Loss: 0.00149062
Iteration 24/25 | Loss: 0.00148992
Iteration 25/25 | Loss: 0.00149075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20998979
Iteration 2/25 | Loss: 0.00188247
Iteration 3/25 | Loss: 0.00188246
Iteration 4/25 | Loss: 0.00188246
Iteration 5/25 | Loss: 0.00188246
Iteration 6/25 | Loss: 0.00188246
Iteration 7/25 | Loss: 0.00188246
Iteration 8/25 | Loss: 0.00188246
Iteration 9/25 | Loss: 0.00188246
Iteration 10/25 | Loss: 0.00188246
Iteration 11/25 | Loss: 0.00188246
Iteration 12/25 | Loss: 0.00188246
Iteration 13/25 | Loss: 0.00188246
Iteration 14/25 | Loss: 0.00188246
Iteration 15/25 | Loss: 0.00188246
Iteration 16/25 | Loss: 0.00188246
Iteration 17/25 | Loss: 0.00188246
Iteration 18/25 | Loss: 0.00188246
Iteration 19/25 | Loss: 0.00188246
Iteration 20/25 | Loss: 0.00188246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018824638100340962, 0.0018824638100340962, 0.0018824638100340962, 0.0018824638100340962, 0.0018824638100340962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018824638100340962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188246
Iteration 2/1000 | Loss: 0.00007451
Iteration 3/1000 | Loss: 0.00007363
Iteration 4/1000 | Loss: 0.00006699
Iteration 5/1000 | Loss: 0.00006908
Iteration 6/1000 | Loss: 0.00006757
Iteration 7/1000 | Loss: 0.00005501
Iteration 8/1000 | Loss: 0.00006267
Iteration 9/1000 | Loss: 0.00008041
Iteration 10/1000 | Loss: 0.00007639
Iteration 11/1000 | Loss: 0.00006922
Iteration 12/1000 | Loss: 0.00006593
Iteration 13/1000 | Loss: 0.00006575
Iteration 14/1000 | Loss: 0.00007541
Iteration 15/1000 | Loss: 0.00006564
Iteration 16/1000 | Loss: 0.00008091
Iteration 17/1000 | Loss: 0.00006820
Iteration 18/1000 | Loss: 0.00005792
Iteration 19/1000 | Loss: 0.00004811
Iteration 20/1000 | Loss: 0.00005736
Iteration 21/1000 | Loss: 0.00007665
Iteration 22/1000 | Loss: 0.00009275
Iteration 23/1000 | Loss: 0.00007737
Iteration 24/1000 | Loss: 0.00007364
Iteration 25/1000 | Loss: 0.00004146
Iteration 26/1000 | Loss: 0.00005254
Iteration 27/1000 | Loss: 0.00006566
Iteration 28/1000 | Loss: 0.00006267
Iteration 29/1000 | Loss: 0.00006601
Iteration 30/1000 | Loss: 0.00006889
Iteration 31/1000 | Loss: 0.00007058
Iteration 32/1000 | Loss: 0.00006582
Iteration 33/1000 | Loss: 0.00005722
Iteration 34/1000 | Loss: 0.00006701
Iteration 35/1000 | Loss: 0.00006279
Iteration 36/1000 | Loss: 0.00003765
Iteration 37/1000 | Loss: 0.00005744
Iteration 38/1000 | Loss: 0.00007699
Iteration 39/1000 | Loss: 0.00006527
Iteration 40/1000 | Loss: 0.00006837
Iteration 41/1000 | Loss: 0.00006964
Iteration 42/1000 | Loss: 0.00006903
Iteration 43/1000 | Loss: 0.00007094
Iteration 44/1000 | Loss: 0.00006480
Iteration 45/1000 | Loss: 0.00008034
Iteration 46/1000 | Loss: 0.00006727
Iteration 47/1000 | Loss: 0.00007021
Iteration 48/1000 | Loss: 0.00006671
Iteration 49/1000 | Loss: 0.00007592
Iteration 50/1000 | Loss: 0.00007135
Iteration 51/1000 | Loss: 0.00007315
Iteration 52/1000 | Loss: 0.00007053
Iteration 53/1000 | Loss: 0.00005751
Iteration 54/1000 | Loss: 0.00004634
Iteration 55/1000 | Loss: 0.00005858
Iteration 56/1000 | Loss: 0.00005063
Iteration 57/1000 | Loss: 0.00004357
Iteration 58/1000 | Loss: 0.00003689
Iteration 59/1000 | Loss: 0.00005333
Iteration 60/1000 | Loss: 0.00006027
Iteration 61/1000 | Loss: 0.00006480
Iteration 62/1000 | Loss: 0.00004464
Iteration 63/1000 | Loss: 0.00004565
Iteration 64/1000 | Loss: 0.00004128
Iteration 65/1000 | Loss: 0.00004282
Iteration 66/1000 | Loss: 0.00004009
Iteration 67/1000 | Loss: 0.00003277
Iteration 68/1000 | Loss: 0.00004077
Iteration 69/1000 | Loss: 0.00004185
Iteration 70/1000 | Loss: 0.00003353
Iteration 71/1000 | Loss: 0.00004588
Iteration 72/1000 | Loss: 0.00004429
Iteration 73/1000 | Loss: 0.00004058
Iteration 74/1000 | Loss: 0.00004440
Iteration 75/1000 | Loss: 0.00003899
Iteration 76/1000 | Loss: 0.00005081
Iteration 77/1000 | Loss: 0.00003694
Iteration 78/1000 | Loss: 0.00003731
Iteration 79/1000 | Loss: 0.00003705
Iteration 80/1000 | Loss: 0.00004484
Iteration 81/1000 | Loss: 0.00004436
Iteration 82/1000 | Loss: 0.00003848
Iteration 83/1000 | Loss: 0.00004119
Iteration 84/1000 | Loss: 0.00004313
Iteration 85/1000 | Loss: 0.00003524
Iteration 86/1000 | Loss: 0.00004106
Iteration 87/1000 | Loss: 0.00004356
Iteration 88/1000 | Loss: 0.00057061
Iteration 89/1000 | Loss: 0.00019393
Iteration 90/1000 | Loss: 0.00006601
Iteration 91/1000 | Loss: 0.00004571
Iteration 92/1000 | Loss: 0.00003969
Iteration 93/1000 | Loss: 0.00003411
Iteration 94/1000 | Loss: 0.00003118
Iteration 95/1000 | Loss: 0.00021713
Iteration 96/1000 | Loss: 0.00025367
Iteration 97/1000 | Loss: 0.00005395
Iteration 98/1000 | Loss: 0.00003599
Iteration 99/1000 | Loss: 0.00011680
Iteration 100/1000 | Loss: 0.00003841
Iteration 101/1000 | Loss: 0.00002946
Iteration 102/1000 | Loss: 0.00002802
Iteration 103/1000 | Loss: 0.00002699
Iteration 104/1000 | Loss: 0.00002621
Iteration 105/1000 | Loss: 0.00002598
Iteration 106/1000 | Loss: 0.00002567
Iteration 107/1000 | Loss: 0.00002541
Iteration 108/1000 | Loss: 0.00002518
Iteration 109/1000 | Loss: 0.00002494
Iteration 110/1000 | Loss: 0.00002481
Iteration 111/1000 | Loss: 0.00002469
Iteration 112/1000 | Loss: 0.00002467
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002461
Iteration 115/1000 | Loss: 0.00002461
Iteration 116/1000 | Loss: 0.00002460
Iteration 117/1000 | Loss: 0.00002459
Iteration 118/1000 | Loss: 0.00002459
Iteration 119/1000 | Loss: 0.00002458
Iteration 120/1000 | Loss: 0.00002457
Iteration 121/1000 | Loss: 0.00002456
Iteration 122/1000 | Loss: 0.00002455
Iteration 123/1000 | Loss: 0.00002455
Iteration 124/1000 | Loss: 0.00002455
Iteration 125/1000 | Loss: 0.00002455
Iteration 126/1000 | Loss: 0.00002455
Iteration 127/1000 | Loss: 0.00002454
Iteration 128/1000 | Loss: 0.00002454
Iteration 129/1000 | Loss: 0.00002453
Iteration 130/1000 | Loss: 0.00002453
Iteration 131/1000 | Loss: 0.00002453
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002451
Iteration 141/1000 | Loss: 0.00002451
Iteration 142/1000 | Loss: 0.00002451
Iteration 143/1000 | Loss: 0.00002451
Iteration 144/1000 | Loss: 0.00002451
Iteration 145/1000 | Loss: 0.00002450
Iteration 146/1000 | Loss: 0.00002450
Iteration 147/1000 | Loss: 0.00002450
Iteration 148/1000 | Loss: 0.00002450
Iteration 149/1000 | Loss: 0.00002450
Iteration 150/1000 | Loss: 0.00002450
Iteration 151/1000 | Loss: 0.00002450
Iteration 152/1000 | Loss: 0.00002450
Iteration 153/1000 | Loss: 0.00002450
Iteration 154/1000 | Loss: 0.00002450
Iteration 155/1000 | Loss: 0.00002449
Iteration 156/1000 | Loss: 0.00002449
Iteration 157/1000 | Loss: 0.00002449
Iteration 158/1000 | Loss: 0.00002449
Iteration 159/1000 | Loss: 0.00002449
Iteration 160/1000 | Loss: 0.00002449
Iteration 161/1000 | Loss: 0.00002449
Iteration 162/1000 | Loss: 0.00002449
Iteration 163/1000 | Loss: 0.00002449
Iteration 164/1000 | Loss: 0.00002449
Iteration 165/1000 | Loss: 0.00002449
Iteration 166/1000 | Loss: 0.00002449
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002449
Iteration 171/1000 | Loss: 0.00002449
Iteration 172/1000 | Loss: 0.00002449
Iteration 173/1000 | Loss: 0.00002449
Iteration 174/1000 | Loss: 0.00002449
Iteration 175/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.4485145331709646e-05, 2.4485145331709646e-05, 2.4485145331709646e-05, 2.4485145331709646e-05, 2.4485145331709646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4485145331709646e-05

Optimization complete. Final v2v error: 3.878892421722412 mm

Highest mean error: 12.46352767944336 mm for frame 125

Lowest mean error: 3.508403778076172 mm for frame 5

Saving results

Total time: 209.66983699798584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427430
Iteration 2/25 | Loss: 0.00146156
Iteration 3/25 | Loss: 0.00139002
Iteration 4/25 | Loss: 0.00138144
Iteration 5/25 | Loss: 0.00137901
Iteration 6/25 | Loss: 0.00137885
Iteration 7/25 | Loss: 0.00137885
Iteration 8/25 | Loss: 0.00137885
Iteration 9/25 | Loss: 0.00137885
Iteration 10/25 | Loss: 0.00137885
Iteration 11/25 | Loss: 0.00137885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013788539217785, 0.0013788539217785, 0.0013788539217785, 0.0013788539217785, 0.0013788539217785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013788539217785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.25575686
Iteration 2/25 | Loss: 0.00207562
Iteration 3/25 | Loss: 0.00207562
Iteration 4/25 | Loss: 0.00207562
Iteration 5/25 | Loss: 0.00207562
Iteration 6/25 | Loss: 0.00207562
Iteration 7/25 | Loss: 0.00207562
Iteration 8/25 | Loss: 0.00207562
Iteration 9/25 | Loss: 0.00207562
Iteration 10/25 | Loss: 0.00207562
Iteration 11/25 | Loss: 0.00207562
Iteration 12/25 | Loss: 0.00207562
Iteration 13/25 | Loss: 0.00207562
Iteration 14/25 | Loss: 0.00207562
Iteration 15/25 | Loss: 0.00207562
Iteration 16/25 | Loss: 0.00207562
Iteration 17/25 | Loss: 0.00207562
Iteration 18/25 | Loss: 0.00207562
Iteration 19/25 | Loss: 0.00207562
Iteration 20/25 | Loss: 0.00207562
Iteration 21/25 | Loss: 0.00207562
Iteration 22/25 | Loss: 0.00207562
Iteration 23/25 | Loss: 0.00207562
Iteration 24/25 | Loss: 0.00207562
Iteration 25/25 | Loss: 0.00207562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00207562
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00002283
Iteration 4/1000 | Loss: 0.00002003
Iteration 5/1000 | Loss: 0.00001874
Iteration 6/1000 | Loss: 0.00001797
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001717
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001700
Iteration 12/1000 | Loss: 0.00001700
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001698
Iteration 16/1000 | Loss: 0.00001698
Iteration 17/1000 | Loss: 0.00001697
Iteration 18/1000 | Loss: 0.00001696
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001694
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001690
Iteration 33/1000 | Loss: 0.00001690
Iteration 34/1000 | Loss: 0.00001690
Iteration 35/1000 | Loss: 0.00001690
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001687
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001686
Iteration 43/1000 | Loss: 0.00001686
Iteration 44/1000 | Loss: 0.00001686
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001684
Iteration 53/1000 | Loss: 0.00001684
Iteration 54/1000 | Loss: 0.00001684
Iteration 55/1000 | Loss: 0.00001683
Iteration 56/1000 | Loss: 0.00001683
Iteration 57/1000 | Loss: 0.00001683
Iteration 58/1000 | Loss: 0.00001682
Iteration 59/1000 | Loss: 0.00001682
Iteration 60/1000 | Loss: 0.00001682
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001681
Iteration 66/1000 | Loss: 0.00001681
Iteration 67/1000 | Loss: 0.00001681
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001680
Iteration 73/1000 | Loss: 0.00001680
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001679
Iteration 79/1000 | Loss: 0.00001679
Iteration 80/1000 | Loss: 0.00001679
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001678
Iteration 97/1000 | Loss: 0.00001678
Iteration 98/1000 | Loss: 0.00001678
Iteration 99/1000 | Loss: 0.00001678
Iteration 100/1000 | Loss: 0.00001678
Iteration 101/1000 | Loss: 0.00001678
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.6776401025708765e-05, 1.6776401025708765e-05, 1.6776401025708765e-05, 1.6776401025708765e-05, 1.6776401025708765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6776401025708765e-05

Optimization complete. Final v2v error: 3.426004409790039 mm

Highest mean error: 3.8313560485839844 mm for frame 21

Lowest mean error: 3.1845993995666504 mm for frame 113

Saving results

Total time: 27.47412872314453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024910
Iteration 2/25 | Loss: 0.00192930
Iteration 3/25 | Loss: 0.00153018
Iteration 4/25 | Loss: 0.00151323
Iteration 5/25 | Loss: 0.00150746
Iteration 6/25 | Loss: 0.00150605
Iteration 7/25 | Loss: 0.00150605
Iteration 8/25 | Loss: 0.00150605
Iteration 9/25 | Loss: 0.00150605
Iteration 10/25 | Loss: 0.00150605
Iteration 11/25 | Loss: 0.00150605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015060534933581948, 0.0015060534933581948, 0.0015060534933581948, 0.0015060534933581948, 0.0015060534933581948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015060534933581948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.72113764
Iteration 2/25 | Loss: 0.00190772
Iteration 3/25 | Loss: 0.00190772
Iteration 4/25 | Loss: 0.00190772
Iteration 5/25 | Loss: 0.00190772
Iteration 6/25 | Loss: 0.00190772
Iteration 7/25 | Loss: 0.00190772
Iteration 8/25 | Loss: 0.00190772
Iteration 9/25 | Loss: 0.00190772
Iteration 10/25 | Loss: 0.00190772
Iteration 11/25 | Loss: 0.00190772
Iteration 12/25 | Loss: 0.00190772
Iteration 13/25 | Loss: 0.00190772
Iteration 14/25 | Loss: 0.00190772
Iteration 15/25 | Loss: 0.00190772
Iteration 16/25 | Loss: 0.00190772
Iteration 17/25 | Loss: 0.00190772
Iteration 18/25 | Loss: 0.00190772
Iteration 19/25 | Loss: 0.00190772
Iteration 20/25 | Loss: 0.00190772
Iteration 21/25 | Loss: 0.00190772
Iteration 22/25 | Loss: 0.00190772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019077167380601168, 0.0019077167380601168, 0.0019077167380601168, 0.0019077167380601168, 0.0019077167380601168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019077167380601168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190772
Iteration 2/1000 | Loss: 0.00008808
Iteration 3/1000 | Loss: 0.00006218
Iteration 4/1000 | Loss: 0.00005409
Iteration 5/1000 | Loss: 0.00005177
Iteration 6/1000 | Loss: 0.00004934
Iteration 7/1000 | Loss: 0.00004760
Iteration 8/1000 | Loss: 0.00004655
Iteration 9/1000 | Loss: 0.00004581
Iteration 10/1000 | Loss: 0.00004531
Iteration 11/1000 | Loss: 0.00004492
Iteration 12/1000 | Loss: 0.00004454
Iteration 13/1000 | Loss: 0.00004425
Iteration 14/1000 | Loss: 0.00004401
Iteration 15/1000 | Loss: 0.00004369
Iteration 16/1000 | Loss: 0.00004341
Iteration 17/1000 | Loss: 0.00004319
Iteration 18/1000 | Loss: 0.00004293
Iteration 19/1000 | Loss: 0.00004272
Iteration 20/1000 | Loss: 0.00004246
Iteration 21/1000 | Loss: 0.00004227
Iteration 22/1000 | Loss: 0.00004212
Iteration 23/1000 | Loss: 0.00004201
Iteration 24/1000 | Loss: 0.00004197
Iteration 25/1000 | Loss: 0.00004197
Iteration 26/1000 | Loss: 0.00004196
Iteration 27/1000 | Loss: 0.00004196
Iteration 28/1000 | Loss: 0.00004195
Iteration 29/1000 | Loss: 0.00004194
Iteration 30/1000 | Loss: 0.00004193
Iteration 31/1000 | Loss: 0.00004193
Iteration 32/1000 | Loss: 0.00004192
Iteration 33/1000 | Loss: 0.00004191
Iteration 34/1000 | Loss: 0.00004191
Iteration 35/1000 | Loss: 0.00004189
Iteration 36/1000 | Loss: 0.00004189
Iteration 37/1000 | Loss: 0.00004188
Iteration 38/1000 | Loss: 0.00004188
Iteration 39/1000 | Loss: 0.00004188
Iteration 40/1000 | Loss: 0.00004187
Iteration 41/1000 | Loss: 0.00004186
Iteration 42/1000 | Loss: 0.00004186
Iteration 43/1000 | Loss: 0.00004186
Iteration 44/1000 | Loss: 0.00004186
Iteration 45/1000 | Loss: 0.00004186
Iteration 46/1000 | Loss: 0.00004186
Iteration 47/1000 | Loss: 0.00004186
Iteration 48/1000 | Loss: 0.00004186
Iteration 49/1000 | Loss: 0.00004186
Iteration 50/1000 | Loss: 0.00004186
Iteration 51/1000 | Loss: 0.00004186
Iteration 52/1000 | Loss: 0.00004186
Iteration 53/1000 | Loss: 0.00004186
Iteration 54/1000 | Loss: 0.00004185
Iteration 55/1000 | Loss: 0.00004185
Iteration 56/1000 | Loss: 0.00004185
Iteration 57/1000 | Loss: 0.00004185
Iteration 58/1000 | Loss: 0.00004185
Iteration 59/1000 | Loss: 0.00004185
Iteration 60/1000 | Loss: 0.00004185
Iteration 61/1000 | Loss: 0.00004184
Iteration 62/1000 | Loss: 0.00004184
Iteration 63/1000 | Loss: 0.00004184
Iteration 64/1000 | Loss: 0.00004183
Iteration 65/1000 | Loss: 0.00004183
Iteration 66/1000 | Loss: 0.00004183
Iteration 67/1000 | Loss: 0.00004183
Iteration 68/1000 | Loss: 0.00004183
Iteration 69/1000 | Loss: 0.00004182
Iteration 70/1000 | Loss: 0.00004182
Iteration 71/1000 | Loss: 0.00004182
Iteration 72/1000 | Loss: 0.00004182
Iteration 73/1000 | Loss: 0.00004182
Iteration 74/1000 | Loss: 0.00004182
Iteration 75/1000 | Loss: 0.00004182
Iteration 76/1000 | Loss: 0.00004181
Iteration 77/1000 | Loss: 0.00004181
Iteration 78/1000 | Loss: 0.00004181
Iteration 79/1000 | Loss: 0.00004181
Iteration 80/1000 | Loss: 0.00004181
Iteration 81/1000 | Loss: 0.00004181
Iteration 82/1000 | Loss: 0.00004181
Iteration 83/1000 | Loss: 0.00004181
Iteration 84/1000 | Loss: 0.00004181
Iteration 85/1000 | Loss: 0.00004181
Iteration 86/1000 | Loss: 0.00004181
Iteration 87/1000 | Loss: 0.00004180
Iteration 88/1000 | Loss: 0.00004179
Iteration 89/1000 | Loss: 0.00004179
Iteration 90/1000 | Loss: 0.00004179
Iteration 91/1000 | Loss: 0.00004179
Iteration 92/1000 | Loss: 0.00004179
Iteration 93/1000 | Loss: 0.00004179
Iteration 94/1000 | Loss: 0.00004179
Iteration 95/1000 | Loss: 0.00004179
Iteration 96/1000 | Loss: 0.00004179
Iteration 97/1000 | Loss: 0.00004178
Iteration 98/1000 | Loss: 0.00004178
Iteration 99/1000 | Loss: 0.00004178
Iteration 100/1000 | Loss: 0.00004178
Iteration 101/1000 | Loss: 0.00004178
Iteration 102/1000 | Loss: 0.00004178
Iteration 103/1000 | Loss: 0.00004178
Iteration 104/1000 | Loss: 0.00004177
Iteration 105/1000 | Loss: 0.00004177
Iteration 106/1000 | Loss: 0.00004177
Iteration 107/1000 | Loss: 0.00004177
Iteration 108/1000 | Loss: 0.00004177
Iteration 109/1000 | Loss: 0.00004177
Iteration 110/1000 | Loss: 0.00004177
Iteration 111/1000 | Loss: 0.00004177
Iteration 112/1000 | Loss: 0.00004176
Iteration 113/1000 | Loss: 0.00004176
Iteration 114/1000 | Loss: 0.00004176
Iteration 115/1000 | Loss: 0.00004176
Iteration 116/1000 | Loss: 0.00004175
Iteration 117/1000 | Loss: 0.00004175
Iteration 118/1000 | Loss: 0.00004175
Iteration 119/1000 | Loss: 0.00004175
Iteration 120/1000 | Loss: 0.00004175
Iteration 121/1000 | Loss: 0.00004175
Iteration 122/1000 | Loss: 0.00004174
Iteration 123/1000 | Loss: 0.00004174
Iteration 124/1000 | Loss: 0.00004174
Iteration 125/1000 | Loss: 0.00004174
Iteration 126/1000 | Loss: 0.00004174
Iteration 127/1000 | Loss: 0.00004174
Iteration 128/1000 | Loss: 0.00004174
Iteration 129/1000 | Loss: 0.00004174
Iteration 130/1000 | Loss: 0.00004174
Iteration 131/1000 | Loss: 0.00004174
Iteration 132/1000 | Loss: 0.00004174
Iteration 133/1000 | Loss: 0.00004174
Iteration 134/1000 | Loss: 0.00004174
Iteration 135/1000 | Loss: 0.00004173
Iteration 136/1000 | Loss: 0.00004173
Iteration 137/1000 | Loss: 0.00004173
Iteration 138/1000 | Loss: 0.00004173
Iteration 139/1000 | Loss: 0.00004173
Iteration 140/1000 | Loss: 0.00004173
Iteration 141/1000 | Loss: 0.00004173
Iteration 142/1000 | Loss: 0.00004173
Iteration 143/1000 | Loss: 0.00004173
Iteration 144/1000 | Loss: 0.00004173
Iteration 145/1000 | Loss: 0.00004173
Iteration 146/1000 | Loss: 0.00004172
Iteration 147/1000 | Loss: 0.00004172
Iteration 148/1000 | Loss: 0.00004172
Iteration 149/1000 | Loss: 0.00004172
Iteration 150/1000 | Loss: 0.00004171
Iteration 151/1000 | Loss: 0.00004171
Iteration 152/1000 | Loss: 0.00004171
Iteration 153/1000 | Loss: 0.00004171
Iteration 154/1000 | Loss: 0.00004171
Iteration 155/1000 | Loss: 0.00004171
Iteration 156/1000 | Loss: 0.00004171
Iteration 157/1000 | Loss: 0.00004170
Iteration 158/1000 | Loss: 0.00004170
Iteration 159/1000 | Loss: 0.00004170
Iteration 160/1000 | Loss: 0.00004170
Iteration 161/1000 | Loss: 0.00004170
Iteration 162/1000 | Loss: 0.00004169
Iteration 163/1000 | Loss: 0.00004169
Iteration 164/1000 | Loss: 0.00004169
Iteration 165/1000 | Loss: 0.00004169
Iteration 166/1000 | Loss: 0.00004169
Iteration 167/1000 | Loss: 0.00004168
Iteration 168/1000 | Loss: 0.00004168
Iteration 169/1000 | Loss: 0.00004168
Iteration 170/1000 | Loss: 0.00004168
Iteration 171/1000 | Loss: 0.00004168
Iteration 172/1000 | Loss: 0.00004168
Iteration 173/1000 | Loss: 0.00004168
Iteration 174/1000 | Loss: 0.00004168
Iteration 175/1000 | Loss: 0.00004168
Iteration 176/1000 | Loss: 0.00004168
Iteration 177/1000 | Loss: 0.00004168
Iteration 178/1000 | Loss: 0.00004168
Iteration 179/1000 | Loss: 0.00004168
Iteration 180/1000 | Loss: 0.00004167
Iteration 181/1000 | Loss: 0.00004167
Iteration 182/1000 | Loss: 0.00004167
Iteration 183/1000 | Loss: 0.00004167
Iteration 184/1000 | Loss: 0.00004167
Iteration 185/1000 | Loss: 0.00004167
Iteration 186/1000 | Loss: 0.00004167
Iteration 187/1000 | Loss: 0.00004167
Iteration 188/1000 | Loss: 0.00004167
Iteration 189/1000 | Loss: 0.00004167
Iteration 190/1000 | Loss: 0.00004167
Iteration 191/1000 | Loss: 0.00004167
Iteration 192/1000 | Loss: 0.00004167
Iteration 193/1000 | Loss: 0.00004167
Iteration 194/1000 | Loss: 0.00004167
Iteration 195/1000 | Loss: 0.00004167
Iteration 196/1000 | Loss: 0.00004167
Iteration 197/1000 | Loss: 0.00004167
Iteration 198/1000 | Loss: 0.00004167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [4.167156293988228e-05, 4.167156293988228e-05, 4.167156293988228e-05, 4.167156293988228e-05, 4.167156293988228e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.167156293988228e-05

Optimization complete. Final v2v error: 5.234050273895264 mm

Highest mean error: 5.840363502502441 mm for frame 98

Lowest mean error: 4.332193374633789 mm for frame 3

Saving results

Total time: 57.55540704727173
