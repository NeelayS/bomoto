Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=39, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2184-2239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00683206
Iteration 2/25 | Loss: 0.00178107
Iteration 3/25 | Loss: 0.00164798
Iteration 4/25 | Loss: 0.00162755
Iteration 5/25 | Loss: 0.00162089
Iteration 6/25 | Loss: 0.00161942
Iteration 7/25 | Loss: 0.00161942
Iteration 8/25 | Loss: 0.00161942
Iteration 9/25 | Loss: 0.00161942
Iteration 10/25 | Loss: 0.00161942
Iteration 11/25 | Loss: 0.00161942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016194236231967807, 0.0016194236231967807, 0.0016194236231967807, 0.0016194236231967807, 0.0016194236231967807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016194236231967807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63534915
Iteration 2/25 | Loss: 0.00237349
Iteration 3/25 | Loss: 0.00237345
Iteration 4/25 | Loss: 0.00237345
Iteration 5/25 | Loss: 0.00237345
Iteration 6/25 | Loss: 0.00237344
Iteration 7/25 | Loss: 0.00237344
Iteration 8/25 | Loss: 0.00237344
Iteration 9/25 | Loss: 0.00237344
Iteration 10/25 | Loss: 0.00237344
Iteration 11/25 | Loss: 0.00237344
Iteration 12/25 | Loss: 0.00237344
Iteration 13/25 | Loss: 0.00237344
Iteration 14/25 | Loss: 0.00237344
Iteration 15/25 | Loss: 0.00237344
Iteration 16/25 | Loss: 0.00237344
Iteration 17/25 | Loss: 0.00237344
Iteration 18/25 | Loss: 0.00237344
Iteration 19/25 | Loss: 0.00237344
Iteration 20/25 | Loss: 0.00237344
Iteration 21/25 | Loss: 0.00237344
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023734443821012974, 0.0023734443821012974, 0.0023734443821012974, 0.0023734443821012974, 0.0023734443821012974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023734443821012974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237344
Iteration 2/1000 | Loss: 0.00007903
Iteration 3/1000 | Loss: 0.00005497
Iteration 4/1000 | Loss: 0.00004693
Iteration 5/1000 | Loss: 0.00004270
Iteration 6/1000 | Loss: 0.00004001
Iteration 7/1000 | Loss: 0.00003822
Iteration 8/1000 | Loss: 0.00003706
Iteration 9/1000 | Loss: 0.00003619
Iteration 10/1000 | Loss: 0.00003565
Iteration 11/1000 | Loss: 0.00003519
Iteration 12/1000 | Loss: 0.00003480
Iteration 13/1000 | Loss: 0.00003450
Iteration 14/1000 | Loss: 0.00003428
Iteration 15/1000 | Loss: 0.00003418
Iteration 16/1000 | Loss: 0.00003418
Iteration 17/1000 | Loss: 0.00003413
Iteration 18/1000 | Loss: 0.00003412
Iteration 19/1000 | Loss: 0.00003408
Iteration 20/1000 | Loss: 0.00003406
Iteration 21/1000 | Loss: 0.00003405
Iteration 22/1000 | Loss: 0.00003404
Iteration 23/1000 | Loss: 0.00003402
Iteration 24/1000 | Loss: 0.00003402
Iteration 25/1000 | Loss: 0.00003402
Iteration 26/1000 | Loss: 0.00003401
Iteration 27/1000 | Loss: 0.00003401
Iteration 28/1000 | Loss: 0.00003400
Iteration 29/1000 | Loss: 0.00003400
Iteration 30/1000 | Loss: 0.00003399
Iteration 31/1000 | Loss: 0.00003399
Iteration 32/1000 | Loss: 0.00003398
Iteration 33/1000 | Loss: 0.00003398
Iteration 34/1000 | Loss: 0.00003397
Iteration 35/1000 | Loss: 0.00003396
Iteration 36/1000 | Loss: 0.00003396
Iteration 37/1000 | Loss: 0.00003395
Iteration 38/1000 | Loss: 0.00003395
Iteration 39/1000 | Loss: 0.00003394
Iteration 40/1000 | Loss: 0.00003391
Iteration 41/1000 | Loss: 0.00003391
Iteration 42/1000 | Loss: 0.00003389
Iteration 43/1000 | Loss: 0.00003389
Iteration 44/1000 | Loss: 0.00003389
Iteration 45/1000 | Loss: 0.00003389
Iteration 46/1000 | Loss: 0.00003389
Iteration 47/1000 | Loss: 0.00003389
Iteration 48/1000 | Loss: 0.00003388
Iteration 49/1000 | Loss: 0.00003388
Iteration 50/1000 | Loss: 0.00003388
Iteration 51/1000 | Loss: 0.00003388
Iteration 52/1000 | Loss: 0.00003388
Iteration 53/1000 | Loss: 0.00003387
Iteration 54/1000 | Loss: 0.00003387
Iteration 55/1000 | Loss: 0.00003387
Iteration 56/1000 | Loss: 0.00003387
Iteration 57/1000 | Loss: 0.00003387
Iteration 58/1000 | Loss: 0.00003386
Iteration 59/1000 | Loss: 0.00003386
Iteration 60/1000 | Loss: 0.00003385
Iteration 61/1000 | Loss: 0.00003385
Iteration 62/1000 | Loss: 0.00003385
Iteration 63/1000 | Loss: 0.00003384
Iteration 64/1000 | Loss: 0.00003384
Iteration 65/1000 | Loss: 0.00003384
Iteration 66/1000 | Loss: 0.00003384
Iteration 67/1000 | Loss: 0.00003384
Iteration 68/1000 | Loss: 0.00003383
Iteration 69/1000 | Loss: 0.00003383
Iteration 70/1000 | Loss: 0.00003383
Iteration 71/1000 | Loss: 0.00003382
Iteration 72/1000 | Loss: 0.00003382
Iteration 73/1000 | Loss: 0.00003382
Iteration 74/1000 | Loss: 0.00003382
Iteration 75/1000 | Loss: 0.00003382
Iteration 76/1000 | Loss: 0.00003381
Iteration 77/1000 | Loss: 0.00003381
Iteration 78/1000 | Loss: 0.00003381
Iteration 79/1000 | Loss: 0.00003381
Iteration 80/1000 | Loss: 0.00003381
Iteration 81/1000 | Loss: 0.00003381
Iteration 82/1000 | Loss: 0.00003380
Iteration 83/1000 | Loss: 0.00003380
Iteration 84/1000 | Loss: 0.00003380
Iteration 85/1000 | Loss: 0.00003380
Iteration 86/1000 | Loss: 0.00003380
Iteration 87/1000 | Loss: 0.00003380
Iteration 88/1000 | Loss: 0.00003379
Iteration 89/1000 | Loss: 0.00003379
Iteration 90/1000 | Loss: 0.00003379
Iteration 91/1000 | Loss: 0.00003378
Iteration 92/1000 | Loss: 0.00003378
Iteration 93/1000 | Loss: 0.00003378
Iteration 94/1000 | Loss: 0.00003378
Iteration 95/1000 | Loss: 0.00003378
Iteration 96/1000 | Loss: 0.00003378
Iteration 97/1000 | Loss: 0.00003377
Iteration 98/1000 | Loss: 0.00003377
Iteration 99/1000 | Loss: 0.00003377
Iteration 100/1000 | Loss: 0.00003377
Iteration 101/1000 | Loss: 0.00003377
Iteration 102/1000 | Loss: 0.00003377
Iteration 103/1000 | Loss: 0.00003377
Iteration 104/1000 | Loss: 0.00003376
Iteration 105/1000 | Loss: 0.00003376
Iteration 106/1000 | Loss: 0.00003376
Iteration 107/1000 | Loss: 0.00003376
Iteration 108/1000 | Loss: 0.00003376
Iteration 109/1000 | Loss: 0.00003376
Iteration 110/1000 | Loss: 0.00003375
Iteration 111/1000 | Loss: 0.00003375
Iteration 112/1000 | Loss: 0.00003375
Iteration 113/1000 | Loss: 0.00003375
Iteration 114/1000 | Loss: 0.00003375
Iteration 115/1000 | Loss: 0.00003375
Iteration 116/1000 | Loss: 0.00003375
Iteration 117/1000 | Loss: 0.00003375
Iteration 118/1000 | Loss: 0.00003375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.375317101017572e-05, 3.375317101017572e-05, 3.375317101017572e-05, 3.375317101017572e-05, 3.375317101017572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.375317101017572e-05

Optimization complete. Final v2v error: 5.001516342163086 mm

Highest mean error: 5.751823425292969 mm for frame 36

Lowest mean error: 4.204776763916016 mm for frame 222

Saving results

Total time: 46.53409934043884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996858
Iteration 2/25 | Loss: 0.00303788
Iteration 3/25 | Loss: 0.00226322
Iteration 4/25 | Loss: 0.00194484
Iteration 5/25 | Loss: 0.00183917
Iteration 6/25 | Loss: 0.00171516
Iteration 7/25 | Loss: 0.00167350
Iteration 8/25 | Loss: 0.00159174
Iteration 9/25 | Loss: 0.00155142
Iteration 10/25 | Loss: 0.00153056
Iteration 11/25 | Loss: 0.00153307
Iteration 12/25 | Loss: 0.00152231
Iteration 13/25 | Loss: 0.00152163
Iteration 14/25 | Loss: 0.00151663
Iteration 15/25 | Loss: 0.00150484
Iteration 16/25 | Loss: 0.00150018
Iteration 17/25 | Loss: 0.00149583
Iteration 18/25 | Loss: 0.00149470
Iteration 19/25 | Loss: 0.00149426
Iteration 20/25 | Loss: 0.00149407
Iteration 21/25 | Loss: 0.00149403
Iteration 22/25 | Loss: 0.00149402
Iteration 23/25 | Loss: 0.00149401
Iteration 24/25 | Loss: 0.00149401
Iteration 25/25 | Loss: 0.00149401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59590495
Iteration 2/25 | Loss: 0.00208378
Iteration 3/25 | Loss: 0.00208378
Iteration 4/25 | Loss: 0.00208378
Iteration 5/25 | Loss: 0.00208378
Iteration 6/25 | Loss: 0.00208378
Iteration 7/25 | Loss: 0.00208378
Iteration 8/25 | Loss: 0.00208378
Iteration 9/25 | Loss: 0.00208378
Iteration 10/25 | Loss: 0.00208378
Iteration 11/25 | Loss: 0.00208378
Iteration 12/25 | Loss: 0.00208378
Iteration 13/25 | Loss: 0.00208378
Iteration 14/25 | Loss: 0.00208378
Iteration 15/25 | Loss: 0.00208378
Iteration 16/25 | Loss: 0.00208378
Iteration 17/25 | Loss: 0.00208378
Iteration 18/25 | Loss: 0.00208378
Iteration 19/25 | Loss: 0.00208378
Iteration 20/25 | Loss: 0.00208378
Iteration 21/25 | Loss: 0.00208378
Iteration 22/25 | Loss: 0.00208378
Iteration 23/25 | Loss: 0.00208378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0020837783813476562, 0.0020837783813476562, 0.0020837783813476562, 0.0020837783813476562, 0.0020837783813476562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020837783813476562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208378
Iteration 2/1000 | Loss: 0.00012468
Iteration 3/1000 | Loss: 0.00008824
Iteration 4/1000 | Loss: 0.00007493
Iteration 5/1000 | Loss: 0.00006848
Iteration 6/1000 | Loss: 0.00006408
Iteration 7/1000 | Loss: 0.00019424
Iteration 8/1000 | Loss: 0.00018590
Iteration 9/1000 | Loss: 0.00006667
Iteration 10/1000 | Loss: 0.00005811
Iteration 11/1000 | Loss: 0.00005369
Iteration 12/1000 | Loss: 0.00005071
Iteration 13/1000 | Loss: 0.00019301
Iteration 14/1000 | Loss: 0.00030069
Iteration 15/1000 | Loss: 0.00031995
Iteration 16/1000 | Loss: 0.00009366
Iteration 17/1000 | Loss: 0.00006564
Iteration 18/1000 | Loss: 0.00014031
Iteration 19/1000 | Loss: 0.00013352
Iteration 20/1000 | Loss: 0.00016285
Iteration 21/1000 | Loss: 0.00013452
Iteration 22/1000 | Loss: 0.00012637
Iteration 23/1000 | Loss: 0.00007423
Iteration 24/1000 | Loss: 0.00004330
Iteration 25/1000 | Loss: 0.00004156
Iteration 26/1000 | Loss: 0.00020531
Iteration 27/1000 | Loss: 0.00005862
Iteration 28/1000 | Loss: 0.00004484
Iteration 29/1000 | Loss: 0.00004060
Iteration 30/1000 | Loss: 0.00003897
Iteration 31/1000 | Loss: 0.00003770
Iteration 32/1000 | Loss: 0.00003718
Iteration 33/1000 | Loss: 0.00003676
Iteration 34/1000 | Loss: 0.00003657
Iteration 35/1000 | Loss: 0.00003644
Iteration 36/1000 | Loss: 0.00003638
Iteration 37/1000 | Loss: 0.00003637
Iteration 38/1000 | Loss: 0.00003637
Iteration 39/1000 | Loss: 0.00003636
Iteration 40/1000 | Loss: 0.00003631
Iteration 41/1000 | Loss: 0.00003630
Iteration 42/1000 | Loss: 0.00003630
Iteration 43/1000 | Loss: 0.00003629
Iteration 44/1000 | Loss: 0.00003628
Iteration 45/1000 | Loss: 0.00003627
Iteration 46/1000 | Loss: 0.00003626
Iteration 47/1000 | Loss: 0.00003625
Iteration 48/1000 | Loss: 0.00003625
Iteration 49/1000 | Loss: 0.00003624
Iteration 50/1000 | Loss: 0.00003624
Iteration 51/1000 | Loss: 0.00003623
Iteration 52/1000 | Loss: 0.00003623
Iteration 53/1000 | Loss: 0.00003622
Iteration 54/1000 | Loss: 0.00003622
Iteration 55/1000 | Loss: 0.00003621
Iteration 56/1000 | Loss: 0.00003621
Iteration 57/1000 | Loss: 0.00003620
Iteration 58/1000 | Loss: 0.00003620
Iteration 59/1000 | Loss: 0.00003619
Iteration 60/1000 | Loss: 0.00003619
Iteration 61/1000 | Loss: 0.00003618
Iteration 62/1000 | Loss: 0.00003618
Iteration 63/1000 | Loss: 0.00003618
Iteration 64/1000 | Loss: 0.00003617
Iteration 65/1000 | Loss: 0.00003617
Iteration 66/1000 | Loss: 0.00003616
Iteration 67/1000 | Loss: 0.00003616
Iteration 68/1000 | Loss: 0.00003616
Iteration 69/1000 | Loss: 0.00003616
Iteration 70/1000 | Loss: 0.00003615
Iteration 71/1000 | Loss: 0.00003615
Iteration 72/1000 | Loss: 0.00003615
Iteration 73/1000 | Loss: 0.00003615
Iteration 74/1000 | Loss: 0.00003615
Iteration 75/1000 | Loss: 0.00003614
Iteration 76/1000 | Loss: 0.00003614
Iteration 77/1000 | Loss: 0.00003614
Iteration 78/1000 | Loss: 0.00003614
Iteration 79/1000 | Loss: 0.00003614
Iteration 80/1000 | Loss: 0.00003614
Iteration 81/1000 | Loss: 0.00003614
Iteration 82/1000 | Loss: 0.00003614
Iteration 83/1000 | Loss: 0.00003614
Iteration 84/1000 | Loss: 0.00003613
Iteration 85/1000 | Loss: 0.00003613
Iteration 86/1000 | Loss: 0.00003613
Iteration 87/1000 | Loss: 0.00003613
Iteration 88/1000 | Loss: 0.00003613
Iteration 89/1000 | Loss: 0.00003613
Iteration 90/1000 | Loss: 0.00003613
Iteration 91/1000 | Loss: 0.00003613
Iteration 92/1000 | Loss: 0.00003613
Iteration 93/1000 | Loss: 0.00003613
Iteration 94/1000 | Loss: 0.00003613
Iteration 95/1000 | Loss: 0.00003613
Iteration 96/1000 | Loss: 0.00003612
Iteration 97/1000 | Loss: 0.00003612
Iteration 98/1000 | Loss: 0.00003612
Iteration 99/1000 | Loss: 0.00003612
Iteration 100/1000 | Loss: 0.00003612
Iteration 101/1000 | Loss: 0.00003612
Iteration 102/1000 | Loss: 0.00003612
Iteration 103/1000 | Loss: 0.00003611
Iteration 104/1000 | Loss: 0.00003611
Iteration 105/1000 | Loss: 0.00003611
Iteration 106/1000 | Loss: 0.00003611
Iteration 107/1000 | Loss: 0.00003611
Iteration 108/1000 | Loss: 0.00003611
Iteration 109/1000 | Loss: 0.00003611
Iteration 110/1000 | Loss: 0.00003611
Iteration 111/1000 | Loss: 0.00003611
Iteration 112/1000 | Loss: 0.00003611
Iteration 113/1000 | Loss: 0.00003611
Iteration 114/1000 | Loss: 0.00003611
Iteration 115/1000 | Loss: 0.00003611
Iteration 116/1000 | Loss: 0.00003611
Iteration 117/1000 | Loss: 0.00003611
Iteration 118/1000 | Loss: 0.00003611
Iteration 119/1000 | Loss: 0.00003611
Iteration 120/1000 | Loss: 0.00003611
Iteration 121/1000 | Loss: 0.00003611
Iteration 122/1000 | Loss: 0.00003611
Iteration 123/1000 | Loss: 0.00003611
Iteration 124/1000 | Loss: 0.00003610
Iteration 125/1000 | Loss: 0.00003610
Iteration 126/1000 | Loss: 0.00003610
Iteration 127/1000 | Loss: 0.00003610
Iteration 128/1000 | Loss: 0.00003610
Iteration 129/1000 | Loss: 0.00003610
Iteration 130/1000 | Loss: 0.00003610
Iteration 131/1000 | Loss: 0.00003610
Iteration 132/1000 | Loss: 0.00003610
Iteration 133/1000 | Loss: 0.00003610
Iteration 134/1000 | Loss: 0.00003610
Iteration 135/1000 | Loss: 0.00003610
Iteration 136/1000 | Loss: 0.00003610
Iteration 137/1000 | Loss: 0.00003610
Iteration 138/1000 | Loss: 0.00003610
Iteration 139/1000 | Loss: 0.00003610
Iteration 140/1000 | Loss: 0.00003610
Iteration 141/1000 | Loss: 0.00003610
Iteration 142/1000 | Loss: 0.00003610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [3.610305066104047e-05, 3.610305066104047e-05, 3.610305066104047e-05, 3.610305066104047e-05, 3.610305066104047e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.610305066104047e-05

Optimization complete. Final v2v error: 4.8019490242004395 mm

Highest mean error: 21.276500701904297 mm for frame 143

Lowest mean error: 4.320598125457764 mm for frame 14

Saving results

Total time: 106.20885586738586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918712
Iteration 2/25 | Loss: 0.00178709
Iteration 3/25 | Loss: 0.00157728
Iteration 4/25 | Loss: 0.00154491
Iteration 5/25 | Loss: 0.00153722
Iteration 6/25 | Loss: 0.00153531
Iteration 7/25 | Loss: 0.00153479
Iteration 8/25 | Loss: 0.00153479
Iteration 9/25 | Loss: 0.00153479
Iteration 10/25 | Loss: 0.00153479
Iteration 11/25 | Loss: 0.00153479
Iteration 12/25 | Loss: 0.00153479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015347945736721158, 0.0015347945736721158, 0.0015347945736721158, 0.0015347945736721158, 0.0015347945736721158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015347945736721158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63672185
Iteration 2/25 | Loss: 0.00193926
Iteration 3/25 | Loss: 0.00193926
Iteration 4/25 | Loss: 0.00193926
Iteration 5/25 | Loss: 0.00193926
Iteration 6/25 | Loss: 0.00193926
Iteration 7/25 | Loss: 0.00193926
Iteration 8/25 | Loss: 0.00193926
Iteration 9/25 | Loss: 0.00193926
Iteration 10/25 | Loss: 0.00193926
Iteration 11/25 | Loss: 0.00193926
Iteration 12/25 | Loss: 0.00193926
Iteration 13/25 | Loss: 0.00193926
Iteration 14/25 | Loss: 0.00193926
Iteration 15/25 | Loss: 0.00193926
Iteration 16/25 | Loss: 0.00193926
Iteration 17/25 | Loss: 0.00193926
Iteration 18/25 | Loss: 0.00193926
Iteration 19/25 | Loss: 0.00193926
Iteration 20/25 | Loss: 0.00193926
Iteration 21/25 | Loss: 0.00193926
Iteration 22/25 | Loss: 0.00193926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019392595859244466, 0.0019392595859244466, 0.0019392595859244466, 0.0019392595859244466, 0.0019392595859244466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019392595859244466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193926
Iteration 2/1000 | Loss: 0.00006213
Iteration 3/1000 | Loss: 0.00004743
Iteration 4/1000 | Loss: 0.00003994
Iteration 5/1000 | Loss: 0.00003664
Iteration 6/1000 | Loss: 0.00003535
Iteration 7/1000 | Loss: 0.00003455
Iteration 8/1000 | Loss: 0.00003411
Iteration 9/1000 | Loss: 0.00003368
Iteration 10/1000 | Loss: 0.00003352
Iteration 11/1000 | Loss: 0.00003342
Iteration 12/1000 | Loss: 0.00003336
Iteration 13/1000 | Loss: 0.00003336
Iteration 14/1000 | Loss: 0.00003335
Iteration 15/1000 | Loss: 0.00003332
Iteration 16/1000 | Loss: 0.00003324
Iteration 17/1000 | Loss: 0.00003323
Iteration 18/1000 | Loss: 0.00003322
Iteration 19/1000 | Loss: 0.00003322
Iteration 20/1000 | Loss: 0.00003321
Iteration 21/1000 | Loss: 0.00003321
Iteration 22/1000 | Loss: 0.00003320
Iteration 23/1000 | Loss: 0.00003319
Iteration 24/1000 | Loss: 0.00003318
Iteration 25/1000 | Loss: 0.00003318
Iteration 26/1000 | Loss: 0.00003318
Iteration 27/1000 | Loss: 0.00003317
Iteration 28/1000 | Loss: 0.00003317
Iteration 29/1000 | Loss: 0.00003317
Iteration 30/1000 | Loss: 0.00003317
Iteration 31/1000 | Loss: 0.00003317
Iteration 32/1000 | Loss: 0.00003315
Iteration 33/1000 | Loss: 0.00003313
Iteration 34/1000 | Loss: 0.00003312
Iteration 35/1000 | Loss: 0.00003311
Iteration 36/1000 | Loss: 0.00003311
Iteration 37/1000 | Loss: 0.00003311
Iteration 38/1000 | Loss: 0.00003310
Iteration 39/1000 | Loss: 0.00003310
Iteration 40/1000 | Loss: 0.00003310
Iteration 41/1000 | Loss: 0.00003309
Iteration 42/1000 | Loss: 0.00003309
Iteration 43/1000 | Loss: 0.00003309
Iteration 44/1000 | Loss: 0.00003308
Iteration 45/1000 | Loss: 0.00003308
Iteration 46/1000 | Loss: 0.00003308
Iteration 47/1000 | Loss: 0.00003307
Iteration 48/1000 | Loss: 0.00003307
Iteration 49/1000 | Loss: 0.00003307
Iteration 50/1000 | Loss: 0.00003306
Iteration 51/1000 | Loss: 0.00003306
Iteration 52/1000 | Loss: 0.00003305
Iteration 53/1000 | Loss: 0.00003305
Iteration 54/1000 | Loss: 0.00003305
Iteration 55/1000 | Loss: 0.00003304
Iteration 56/1000 | Loss: 0.00003304
Iteration 57/1000 | Loss: 0.00003303
Iteration 58/1000 | Loss: 0.00003303
Iteration 59/1000 | Loss: 0.00003303
Iteration 60/1000 | Loss: 0.00003303
Iteration 61/1000 | Loss: 0.00003302
Iteration 62/1000 | Loss: 0.00003302
Iteration 63/1000 | Loss: 0.00003302
Iteration 64/1000 | Loss: 0.00003302
Iteration 65/1000 | Loss: 0.00003302
Iteration 66/1000 | Loss: 0.00003302
Iteration 67/1000 | Loss: 0.00003301
Iteration 68/1000 | Loss: 0.00003301
Iteration 69/1000 | Loss: 0.00003301
Iteration 70/1000 | Loss: 0.00003301
Iteration 71/1000 | Loss: 0.00003301
Iteration 72/1000 | Loss: 0.00003301
Iteration 73/1000 | Loss: 0.00003300
Iteration 74/1000 | Loss: 0.00003300
Iteration 75/1000 | Loss: 0.00003300
Iteration 76/1000 | Loss: 0.00003300
Iteration 77/1000 | Loss: 0.00003300
Iteration 78/1000 | Loss: 0.00003300
Iteration 79/1000 | Loss: 0.00003300
Iteration 80/1000 | Loss: 0.00003300
Iteration 81/1000 | Loss: 0.00003300
Iteration 82/1000 | Loss: 0.00003300
Iteration 83/1000 | Loss: 0.00003300
Iteration 84/1000 | Loss: 0.00003300
Iteration 85/1000 | Loss: 0.00003300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [3.2995594665408134e-05, 3.2995594665408134e-05, 3.2995594665408134e-05, 3.2995594665408134e-05, 3.2995594665408134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2995594665408134e-05

Optimization complete. Final v2v error: 5.045750141143799 mm

Highest mean error: 5.402876853942871 mm for frame 125

Lowest mean error: 4.753617286682129 mm for frame 12

Saving results

Total time: 36.746636629104614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122990
Iteration 2/25 | Loss: 0.01122990
Iteration 3/25 | Loss: 0.01122990
Iteration 4/25 | Loss: 0.00307039
Iteration 5/25 | Loss: 0.00252465
Iteration 6/25 | Loss: 0.00243887
Iteration 7/25 | Loss: 0.00241423
Iteration 8/25 | Loss: 0.00240614
Iteration 9/25 | Loss: 0.00240358
Iteration 10/25 | Loss: 0.00240164
Iteration 11/25 | Loss: 0.00240043
Iteration 12/25 | Loss: 0.00240042
Iteration 13/25 | Loss: 0.00239992
Iteration 14/25 | Loss: 0.00239960
Iteration 15/25 | Loss: 0.00239948
Iteration 16/25 | Loss: 0.00239946
Iteration 17/25 | Loss: 0.00239946
Iteration 18/25 | Loss: 0.00239946
Iteration 19/25 | Loss: 0.00239946
Iteration 20/25 | Loss: 0.00239946
Iteration 21/25 | Loss: 0.00239946
Iteration 22/25 | Loss: 0.00239946
Iteration 23/25 | Loss: 0.00239945
Iteration 24/25 | Loss: 0.00239945
Iteration 25/25 | Loss: 0.00239945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52767801
Iteration 2/25 | Loss: 0.01408356
Iteration 3/25 | Loss: 0.01213724
Iteration 4/25 | Loss: 0.01213345
Iteration 5/25 | Loss: 0.01171635
Iteration 6/25 | Loss: 0.01171634
Iteration 7/25 | Loss: 0.01171634
Iteration 8/25 | Loss: 0.01171634
Iteration 9/25 | Loss: 0.01171634
Iteration 10/25 | Loss: 0.01171634
Iteration 11/25 | Loss: 0.01171634
Iteration 12/25 | Loss: 0.01171634
Iteration 13/25 | Loss: 0.01171634
Iteration 14/25 | Loss: 0.01171634
Iteration 15/25 | Loss: 0.01171634
Iteration 16/25 | Loss: 0.01171634
Iteration 17/25 | Loss: 0.01171634
Iteration 18/25 | Loss: 0.01171634
Iteration 19/25 | Loss: 0.01171634
Iteration 20/25 | Loss: 0.01171634
Iteration 21/25 | Loss: 0.01171634
Iteration 22/25 | Loss: 0.01171634
Iteration 23/25 | Loss: 0.01171634
Iteration 24/25 | Loss: 0.01171634
Iteration 25/25 | Loss: 0.01171634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01171634
Iteration 2/1000 | Loss: 0.00407116
Iteration 3/1000 | Loss: 0.00156849
Iteration 4/1000 | Loss: 0.00131045
Iteration 5/1000 | Loss: 0.01986324
Iteration 6/1000 | Loss: 0.00455998
Iteration 7/1000 | Loss: 0.01770409
Iteration 8/1000 | Loss: 0.01659161
Iteration 9/1000 | Loss: 0.01594978
Iteration 10/1000 | Loss: 0.00326971
Iteration 11/1000 | Loss: 0.00184451
Iteration 12/1000 | Loss: 0.00105399
Iteration 13/1000 | Loss: 0.00280252
Iteration 14/1000 | Loss: 0.00407805
Iteration 15/1000 | Loss: 0.00382740
Iteration 16/1000 | Loss: 0.00558440
Iteration 17/1000 | Loss: 0.00367954
Iteration 18/1000 | Loss: 0.00296312
Iteration 19/1000 | Loss: 0.00129977
Iteration 20/1000 | Loss: 0.00158833
Iteration 21/1000 | Loss: 0.00325658
Iteration 22/1000 | Loss: 0.00299597
Iteration 23/1000 | Loss: 0.00440211
Iteration 24/1000 | Loss: 0.00177147
Iteration 25/1000 | Loss: 0.00201642
Iteration 26/1000 | Loss: 0.00098785
Iteration 27/1000 | Loss: 0.00180207
Iteration 28/1000 | Loss: 0.00206740
Iteration 29/1000 | Loss: 0.00156071
Iteration 30/1000 | Loss: 0.00239409
Iteration 31/1000 | Loss: 0.00188429
Iteration 32/1000 | Loss: 0.00117309
Iteration 33/1000 | Loss: 0.00217685
Iteration 34/1000 | Loss: 0.00407425
Iteration 35/1000 | Loss: 0.00191930
Iteration 36/1000 | Loss: 0.00473778
Iteration 37/1000 | Loss: 0.00826340
Iteration 38/1000 | Loss: 0.00516958
Iteration 39/1000 | Loss: 0.00270293
Iteration 40/1000 | Loss: 0.00475991
Iteration 41/1000 | Loss: 0.00234643
Iteration 42/1000 | Loss: 0.00532565
Iteration 43/1000 | Loss: 0.00407860
Iteration 44/1000 | Loss: 0.00216431
Iteration 45/1000 | Loss: 0.00231559
Iteration 46/1000 | Loss: 0.00503240
Iteration 47/1000 | Loss: 0.00330785
Iteration 48/1000 | Loss: 0.00760530
Iteration 49/1000 | Loss: 0.00386615
Iteration 50/1000 | Loss: 0.00682059
Iteration 51/1000 | Loss: 0.00615450
Iteration 52/1000 | Loss: 0.00359005
Iteration 53/1000 | Loss: 0.00365218
Iteration 54/1000 | Loss: 0.00496879
Iteration 55/1000 | Loss: 0.00861278
Iteration 56/1000 | Loss: 0.00736299
Iteration 57/1000 | Loss: 0.00740801
Iteration 58/1000 | Loss: 0.01101752
Iteration 59/1000 | Loss: 0.00426330
Iteration 60/1000 | Loss: 0.00891277
Iteration 61/1000 | Loss: 0.00529144
Iteration 62/1000 | Loss: 0.00709840
Iteration 63/1000 | Loss: 0.00464931
Iteration 64/1000 | Loss: 0.00740198
Iteration 65/1000 | Loss: 0.00682699
Iteration 66/1000 | Loss: 0.00557011
Iteration 67/1000 | Loss: 0.00706226
Iteration 68/1000 | Loss: 0.00987306
Iteration 69/1000 | Loss: 0.00617899
Iteration 70/1000 | Loss: 0.00971018
Iteration 71/1000 | Loss: 0.00399876
Iteration 72/1000 | Loss: 0.00921603
Iteration 73/1000 | Loss: 0.00629042
Iteration 74/1000 | Loss: 0.00498051
Iteration 75/1000 | Loss: 0.00497239
Iteration 76/1000 | Loss: 0.00546688
Iteration 77/1000 | Loss: 0.01630512
Iteration 78/1000 | Loss: 0.03449310
Iteration 79/1000 | Loss: 0.00533987
Iteration 80/1000 | Loss: 0.00724643
Iteration 81/1000 | Loss: 0.00449937
Iteration 82/1000 | Loss: 0.00530206
Iteration 83/1000 | Loss: 0.00539111
Iteration 84/1000 | Loss: 0.00610564
Iteration 85/1000 | Loss: 0.00634697
Iteration 86/1000 | Loss: 0.00685237
Iteration 87/1000 | Loss: 0.00905079
Iteration 88/1000 | Loss: 0.00524109
Iteration 89/1000 | Loss: 0.00539706
Iteration 90/1000 | Loss: 0.00528245
Iteration 91/1000 | Loss: 0.00797294
Iteration 92/1000 | Loss: 0.01201498
Iteration 93/1000 | Loss: 0.00887635
Iteration 94/1000 | Loss: 0.00544420
Iteration 95/1000 | Loss: 0.00726178
Iteration 96/1000 | Loss: 0.00601146
Iteration 97/1000 | Loss: 0.00785883
Iteration 98/1000 | Loss: 0.00639752
Iteration 99/1000 | Loss: 0.00682689
Iteration 100/1000 | Loss: 0.01097345
Iteration 101/1000 | Loss: 0.01621936
Iteration 102/1000 | Loss: 0.01612632
Iteration 103/1000 | Loss: 0.01799806
Iteration 104/1000 | Loss: 0.00972837
Iteration 105/1000 | Loss: 0.01170649
Iteration 106/1000 | Loss: 0.00675338
Iteration 107/1000 | Loss: 0.01146708
Iteration 108/1000 | Loss: 0.01054604
Iteration 109/1000 | Loss: 0.01085107
Iteration 110/1000 | Loss: 0.00849361
Iteration 111/1000 | Loss: 0.00810166
Iteration 112/1000 | Loss: 0.01158428
Iteration 113/1000 | Loss: 0.01089877
Iteration 114/1000 | Loss: 0.01014495
Iteration 115/1000 | Loss: 0.00679395
Iteration 116/1000 | Loss: 0.01075202
Iteration 117/1000 | Loss: 0.01145668
Iteration 118/1000 | Loss: 0.00748699
Iteration 119/1000 | Loss: 0.00771744
Iteration 120/1000 | Loss: 0.00932484
Iteration 121/1000 | Loss: 0.01016023
Iteration 122/1000 | Loss: 0.00768176
Iteration 123/1000 | Loss: 0.01141623
Iteration 124/1000 | Loss: 0.01110733
Iteration 125/1000 | Loss: 0.00859316
Iteration 126/1000 | Loss: 0.00728611
Iteration 127/1000 | Loss: 0.00588976
Iteration 128/1000 | Loss: 0.00723506
Iteration 129/1000 | Loss: 0.01189760
Iteration 130/1000 | Loss: 0.00479313
Iteration 131/1000 | Loss: 0.00679760
Iteration 132/1000 | Loss: 0.00904483
Iteration 133/1000 | Loss: 0.01215620
Iteration 134/1000 | Loss: 0.01435412
Iteration 135/1000 | Loss: 0.00808398
Iteration 136/1000 | Loss: 0.00633413
Iteration 137/1000 | Loss: 0.00891869
Iteration 138/1000 | Loss: 0.01013687
Iteration 139/1000 | Loss: 0.01229781
Iteration 140/1000 | Loss: 0.00980725
Iteration 141/1000 | Loss: 0.00587402
Iteration 142/1000 | Loss: 0.00600886
Iteration 143/1000 | Loss: 0.00914308
Iteration 144/1000 | Loss: 0.00480805
Iteration 145/1000 | Loss: 0.00850262
Iteration 146/1000 | Loss: 0.00484676
Iteration 147/1000 | Loss: 0.00707090
Iteration 148/1000 | Loss: 0.00457928
Iteration 149/1000 | Loss: 0.00692246
Iteration 150/1000 | Loss: 0.01032475
Iteration 151/1000 | Loss: 0.01113147
Iteration 152/1000 | Loss: 0.00494843
Iteration 153/1000 | Loss: 0.00539782
Iteration 154/1000 | Loss: 0.01148214
Iteration 155/1000 | Loss: 0.01001628
Iteration 156/1000 | Loss: 0.00418486
Iteration 157/1000 | Loss: 0.00521119
Iteration 158/1000 | Loss: 0.00292086
Iteration 159/1000 | Loss: 0.00392448
Iteration 160/1000 | Loss: 0.00404707
Iteration 161/1000 | Loss: 0.00746298
Iteration 162/1000 | Loss: 0.00370355
Iteration 163/1000 | Loss: 0.00734266
Iteration 164/1000 | Loss: 0.00783794
Iteration 165/1000 | Loss: 0.00414780
Iteration 166/1000 | Loss: 0.00503805
Iteration 167/1000 | Loss: 0.00706603
Iteration 168/1000 | Loss: 0.00432251
Iteration 169/1000 | Loss: 0.00303654
Iteration 170/1000 | Loss: 0.00527179
Iteration 171/1000 | Loss: 0.00756042
Iteration 172/1000 | Loss: 0.00882048
Iteration 173/1000 | Loss: 0.00555071
Iteration 174/1000 | Loss: 0.00308754
Iteration 175/1000 | Loss: 0.00367012
Iteration 176/1000 | Loss: 0.00746007
Iteration 177/1000 | Loss: 0.00404876
Iteration 178/1000 | Loss: 0.00282238
Iteration 179/1000 | Loss: 0.00276119
Iteration 180/1000 | Loss: 0.00480754
Iteration 181/1000 | Loss: 0.00346068
Iteration 182/1000 | Loss: 0.00344873
Iteration 183/1000 | Loss: 0.00259754
Iteration 184/1000 | Loss: 0.00332853
Iteration 185/1000 | Loss: 0.00519782
Iteration 186/1000 | Loss: 0.00185969
Iteration 187/1000 | Loss: 0.00482296
Iteration 188/1000 | Loss: 0.00470101
Iteration 189/1000 | Loss: 0.00358855
Iteration 190/1000 | Loss: 0.00389025
Iteration 191/1000 | Loss: 0.00499523
Iteration 192/1000 | Loss: 0.00211898
Iteration 193/1000 | Loss: 0.00275571
Iteration 194/1000 | Loss: 0.00205566
Iteration 195/1000 | Loss: 0.00310230
Iteration 196/1000 | Loss: 0.00733081
Iteration 197/1000 | Loss: 0.00479367
Iteration 198/1000 | Loss: 0.00313958
Iteration 199/1000 | Loss: 0.00209564
Iteration 200/1000 | Loss: 0.00271784
Iteration 201/1000 | Loss: 0.00485470
Iteration 202/1000 | Loss: 0.00586828
Iteration 203/1000 | Loss: 0.00263267
Iteration 204/1000 | Loss: 0.00438163
Iteration 205/1000 | Loss: 0.00229479
Iteration 206/1000 | Loss: 0.00238276
Iteration 207/1000 | Loss: 0.00338005
Iteration 208/1000 | Loss: 0.00303747
Iteration 209/1000 | Loss: 0.00157928
Iteration 210/1000 | Loss: 0.00279105
Iteration 211/1000 | Loss: 0.00170638
Iteration 212/1000 | Loss: 0.00440038
Iteration 213/1000 | Loss: 0.00601018
Iteration 214/1000 | Loss: 0.00702121
Iteration 215/1000 | Loss: 0.00639748
Iteration 216/1000 | Loss: 0.00688029
Iteration 217/1000 | Loss: 0.00171019
Iteration 218/1000 | Loss: 0.00208323
Iteration 219/1000 | Loss: 0.00255185
Iteration 220/1000 | Loss: 0.00191500
Iteration 221/1000 | Loss: 0.00159567
Iteration 222/1000 | Loss: 0.00192886
Iteration 223/1000 | Loss: 0.00492738
Iteration 224/1000 | Loss: 0.00215885
Iteration 225/1000 | Loss: 0.00302762
Iteration 226/1000 | Loss: 0.00415907
Iteration 227/1000 | Loss: 0.00278429
Iteration 228/1000 | Loss: 0.00464622
Iteration 229/1000 | Loss: 0.00220423
Iteration 230/1000 | Loss: 0.00485898
Iteration 231/1000 | Loss: 0.00260546
Iteration 232/1000 | Loss: 0.00500965
Iteration 233/1000 | Loss: 0.00619450
Iteration 234/1000 | Loss: 0.00746236
Iteration 235/1000 | Loss: 0.00807968
Iteration 236/1000 | Loss: 0.00345702
Iteration 237/1000 | Loss: 0.00404108
Iteration 238/1000 | Loss: 0.00298436
Iteration 239/1000 | Loss: 0.00472235
Iteration 240/1000 | Loss: 0.00278136
Iteration 241/1000 | Loss: 0.00280560
Iteration 242/1000 | Loss: 0.00190959
Iteration 243/1000 | Loss: 0.00423550
Iteration 244/1000 | Loss: 0.00220285
Iteration 245/1000 | Loss: 0.00204228
Iteration 246/1000 | Loss: 0.00164748
Iteration 247/1000 | Loss: 0.00633648
Iteration 248/1000 | Loss: 0.00565172
Iteration 249/1000 | Loss: 0.00333308
Iteration 250/1000 | Loss: 0.00395192
Iteration 251/1000 | Loss: 0.00250522
Iteration 252/1000 | Loss: 0.00318346
Iteration 253/1000 | Loss: 0.00391228
Iteration 254/1000 | Loss: 0.00358277
Iteration 255/1000 | Loss: 0.00499643
Iteration 256/1000 | Loss: 0.00352399
Iteration 257/1000 | Loss: 0.00179319
Iteration 258/1000 | Loss: 0.00273463
Iteration 259/1000 | Loss: 0.00227454
Iteration 260/1000 | Loss: 0.00305384
Iteration 261/1000 | Loss: 0.00644513
Iteration 262/1000 | Loss: 0.00168010
Iteration 263/1000 | Loss: 0.00168805
Iteration 264/1000 | Loss: 0.00225229
Iteration 265/1000 | Loss: 0.00239118
Iteration 266/1000 | Loss: 0.00134576
Iteration 267/1000 | Loss: 0.00104147
Iteration 268/1000 | Loss: 0.00119503
Iteration 269/1000 | Loss: 0.00088479
Iteration 270/1000 | Loss: 0.00216083
Iteration 271/1000 | Loss: 0.00147119
Iteration 272/1000 | Loss: 0.00096656
Iteration 273/1000 | Loss: 0.00106274
Iteration 274/1000 | Loss: 0.00196109
Iteration 275/1000 | Loss: 0.00083779
Iteration 276/1000 | Loss: 0.00101897
Iteration 277/1000 | Loss: 0.00162218
Iteration 278/1000 | Loss: 0.00172033
Iteration 279/1000 | Loss: 0.00053715
Iteration 280/1000 | Loss: 0.00095003
Iteration 281/1000 | Loss: 0.00075829
Iteration 282/1000 | Loss: 0.00161354
Iteration 283/1000 | Loss: 0.00050316
Iteration 284/1000 | Loss: 0.00084973
Iteration 285/1000 | Loss: 0.00127508
Iteration 286/1000 | Loss: 0.00127183
Iteration 287/1000 | Loss: 0.00094652
Iteration 288/1000 | Loss: 0.00060633
Iteration 289/1000 | Loss: 0.00063208
Iteration 290/1000 | Loss: 0.00059837
Iteration 291/1000 | Loss: 0.00171239
Iteration 292/1000 | Loss: 0.00221907
Iteration 293/1000 | Loss: 0.00131324
Iteration 294/1000 | Loss: 0.00078358
Iteration 295/1000 | Loss: 0.00062402
Iteration 296/1000 | Loss: 0.00161308
Iteration 297/1000 | Loss: 0.00162132
Iteration 298/1000 | Loss: 0.00046556
Iteration 299/1000 | Loss: 0.00063001
Iteration 300/1000 | Loss: 0.00102983
Iteration 301/1000 | Loss: 0.00083131
Iteration 302/1000 | Loss: 0.00093951
Iteration 303/1000 | Loss: 0.00098162
Iteration 304/1000 | Loss: 0.00062149
Iteration 305/1000 | Loss: 0.00051721
Iteration 306/1000 | Loss: 0.00079750
Iteration 307/1000 | Loss: 0.00150502
Iteration 308/1000 | Loss: 0.00093965
Iteration 309/1000 | Loss: 0.00077328
Iteration 310/1000 | Loss: 0.00139527
Iteration 311/1000 | Loss: 0.00062471
Iteration 312/1000 | Loss: 0.00105292
Iteration 313/1000 | Loss: 0.00037511
Iteration 314/1000 | Loss: 0.00057275
Iteration 315/1000 | Loss: 0.00114969
Iteration 316/1000 | Loss: 0.00177058
Iteration 317/1000 | Loss: 0.00410750
Iteration 318/1000 | Loss: 0.00080936
Iteration 319/1000 | Loss: 0.00053646
Iteration 320/1000 | Loss: 0.00072798
Iteration 321/1000 | Loss: 0.00037794
Iteration 322/1000 | Loss: 0.00052957
Iteration 323/1000 | Loss: 0.00059803
Iteration 324/1000 | Loss: 0.00080364
Iteration 325/1000 | Loss: 0.00051406
Iteration 326/1000 | Loss: 0.00060962
Iteration 327/1000 | Loss: 0.00035446
Iteration 328/1000 | Loss: 0.00084733
Iteration 329/1000 | Loss: 0.00042093
Iteration 330/1000 | Loss: 0.00082505
Iteration 331/1000 | Loss: 0.00072943
Iteration 332/1000 | Loss: 0.00059003
Iteration 333/1000 | Loss: 0.00123804
Iteration 334/1000 | Loss: 0.00213004
Iteration 335/1000 | Loss: 0.00116274
Iteration 336/1000 | Loss: 0.00088401
Iteration 337/1000 | Loss: 0.00076995
Iteration 338/1000 | Loss: 0.00085325
Iteration 339/1000 | Loss: 0.00067539
Iteration 340/1000 | Loss: 0.00059186
Iteration 341/1000 | Loss: 0.00062133
Iteration 342/1000 | Loss: 0.00070951
Iteration 343/1000 | Loss: 0.00096398
Iteration 344/1000 | Loss: 0.00179331
Iteration 345/1000 | Loss: 0.00068250
Iteration 346/1000 | Loss: 0.00131720
Iteration 347/1000 | Loss: 0.00044557
Iteration 348/1000 | Loss: 0.00039837
Iteration 349/1000 | Loss: 0.00071126
Iteration 350/1000 | Loss: 0.00068473
Iteration 351/1000 | Loss: 0.00036906
Iteration 352/1000 | Loss: 0.00048935
Iteration 353/1000 | Loss: 0.00034384
Iteration 354/1000 | Loss: 0.00046604
Iteration 355/1000 | Loss: 0.00033131
Iteration 356/1000 | Loss: 0.00079976
Iteration 357/1000 | Loss: 0.00042045
Iteration 358/1000 | Loss: 0.00047974
Iteration 359/1000 | Loss: 0.00042874
Iteration 360/1000 | Loss: 0.00047324
Iteration 361/1000 | Loss: 0.00047821
Iteration 362/1000 | Loss: 0.00050198
Iteration 363/1000 | Loss: 0.00045993
Iteration 364/1000 | Loss: 0.00049923
Iteration 365/1000 | Loss: 0.00080222
Iteration 366/1000 | Loss: 0.00049232
Iteration 367/1000 | Loss: 0.00032277
Iteration 368/1000 | Loss: 0.00033012
Iteration 369/1000 | Loss: 0.00033498
Iteration 370/1000 | Loss: 0.00054589
Iteration 371/1000 | Loss: 0.00118288
Iteration 372/1000 | Loss: 0.00088127
Iteration 373/1000 | Loss: 0.00095292
Iteration 374/1000 | Loss: 0.00064493
Iteration 375/1000 | Loss: 0.00035532
Iteration 376/1000 | Loss: 0.00084487
Iteration 377/1000 | Loss: 0.00045390
Iteration 378/1000 | Loss: 0.00088156
Iteration 379/1000 | Loss: 0.00034349
Iteration 380/1000 | Loss: 0.00032053
Iteration 381/1000 | Loss: 0.00039469
Iteration 382/1000 | Loss: 0.00051767
Iteration 383/1000 | Loss: 0.00041915
Iteration 384/1000 | Loss: 0.00031071
Iteration 385/1000 | Loss: 0.00050182
Iteration 386/1000 | Loss: 0.00037391
Iteration 387/1000 | Loss: 0.00034023
Iteration 388/1000 | Loss: 0.00031726
Iteration 389/1000 | Loss: 0.00033647
Iteration 390/1000 | Loss: 0.00086630
Iteration 391/1000 | Loss: 0.00061853
Iteration 392/1000 | Loss: 0.00054911
Iteration 393/1000 | Loss: 0.00034439
Iteration 394/1000 | Loss: 0.00042805
Iteration 395/1000 | Loss: 0.00033825
Iteration 396/1000 | Loss: 0.00033169
Iteration 397/1000 | Loss: 0.00040035
Iteration 398/1000 | Loss: 0.00030961
Iteration 399/1000 | Loss: 0.00031078
Iteration 400/1000 | Loss: 0.00050645
Iteration 401/1000 | Loss: 0.00115089
Iteration 402/1000 | Loss: 0.00034985
Iteration 403/1000 | Loss: 0.00032785
Iteration 404/1000 | Loss: 0.00031906
Iteration 405/1000 | Loss: 0.00058587
Iteration 406/1000 | Loss: 0.00031146
Iteration 407/1000 | Loss: 0.00029814
Iteration 408/1000 | Loss: 0.00029696
Iteration 409/1000 | Loss: 0.00029634
Iteration 410/1000 | Loss: 0.00029587
Iteration 411/1000 | Loss: 0.00029557
Iteration 412/1000 | Loss: 0.00029530
Iteration 413/1000 | Loss: 0.00100127
Iteration 414/1000 | Loss: 0.00081659
Iteration 415/1000 | Loss: 0.00054102
Iteration 416/1000 | Loss: 0.00061712
Iteration 417/1000 | Loss: 0.00122190
Iteration 418/1000 | Loss: 0.00146514
Iteration 419/1000 | Loss: 0.00121848
Iteration 420/1000 | Loss: 0.00035647
Iteration 421/1000 | Loss: 0.00037555
Iteration 422/1000 | Loss: 0.00048497
Iteration 423/1000 | Loss: 0.00031530
Iteration 424/1000 | Loss: 0.00031493
Iteration 425/1000 | Loss: 0.00031477
Iteration 426/1000 | Loss: 0.00048999
Iteration 427/1000 | Loss: 0.00034123
Iteration 428/1000 | Loss: 0.00031797
Iteration 429/1000 | Loss: 0.00033760
Iteration 430/1000 | Loss: 0.00031186
Iteration 431/1000 | Loss: 0.00030853
Iteration 432/1000 | Loss: 0.00031034
Iteration 433/1000 | Loss: 0.00030567
Iteration 434/1000 | Loss: 0.00030117
Iteration 435/1000 | Loss: 0.00031063
Iteration 436/1000 | Loss: 0.00030589
Iteration 437/1000 | Loss: 0.00030282
Iteration 438/1000 | Loss: 0.00049724
Iteration 439/1000 | Loss: 0.00054238
Iteration 440/1000 | Loss: 0.00033232
Iteration 441/1000 | Loss: 0.00030878
Iteration 442/1000 | Loss: 0.00066366
Iteration 443/1000 | Loss: 0.00058100
Iteration 444/1000 | Loss: 0.00068686
Iteration 445/1000 | Loss: 0.00060424
Iteration 446/1000 | Loss: 0.00065974
Iteration 447/1000 | Loss: 0.00054563
Iteration 448/1000 | Loss: 0.00068860
Iteration 449/1000 | Loss: 0.00059569
Iteration 450/1000 | Loss: 0.00039165
Iteration 451/1000 | Loss: 0.00061264
Iteration 452/1000 | Loss: 0.00085510
Iteration 453/1000 | Loss: 0.00031482
Iteration 454/1000 | Loss: 0.00030330
Iteration 455/1000 | Loss: 0.00054989
Iteration 456/1000 | Loss: 0.00029654
Iteration 457/1000 | Loss: 0.00029423
Iteration 458/1000 | Loss: 0.00029273
Iteration 459/1000 | Loss: 0.00048964
Iteration 460/1000 | Loss: 0.00029969
Iteration 461/1000 | Loss: 0.00029384
Iteration 462/1000 | Loss: 0.00029060
Iteration 463/1000 | Loss: 0.00028982
Iteration 464/1000 | Loss: 0.00028936
Iteration 465/1000 | Loss: 0.00028893
Iteration 466/1000 | Loss: 0.00028863
Iteration 467/1000 | Loss: 0.00064605
Iteration 468/1000 | Loss: 0.00044810
Iteration 469/1000 | Loss: 0.00034599
Iteration 470/1000 | Loss: 0.00029123
Iteration 471/1000 | Loss: 0.00028861
Iteration 472/1000 | Loss: 0.00028723
Iteration 473/1000 | Loss: 0.00028631
Iteration 474/1000 | Loss: 0.00028585
Iteration 475/1000 | Loss: 0.00028566
Iteration 476/1000 | Loss: 0.00028558
Iteration 477/1000 | Loss: 0.00028553
Iteration 478/1000 | Loss: 0.00028553
Iteration 479/1000 | Loss: 0.00028553
Iteration 480/1000 | Loss: 0.00028553
Iteration 481/1000 | Loss: 0.00028553
Iteration 482/1000 | Loss: 0.00028553
Iteration 483/1000 | Loss: 0.00028552
Iteration 484/1000 | Loss: 0.00028552
Iteration 485/1000 | Loss: 0.00028552
Iteration 486/1000 | Loss: 0.00028552
Iteration 487/1000 | Loss: 0.00028552
Iteration 488/1000 | Loss: 0.00028550
Iteration 489/1000 | Loss: 0.00028550
Iteration 490/1000 | Loss: 0.00028550
Iteration 491/1000 | Loss: 0.00028550
Iteration 492/1000 | Loss: 0.00028549
Iteration 493/1000 | Loss: 0.00028549
Iteration 494/1000 | Loss: 0.00028549
Iteration 495/1000 | Loss: 0.00028549
Iteration 496/1000 | Loss: 0.00028549
Iteration 497/1000 | Loss: 0.00028549
Iteration 498/1000 | Loss: 0.00028549
Iteration 499/1000 | Loss: 0.00028549
Iteration 500/1000 | Loss: 0.00028549
Iteration 501/1000 | Loss: 0.00028548
Iteration 502/1000 | Loss: 0.00028548
Iteration 503/1000 | Loss: 0.00028547
Iteration 504/1000 | Loss: 0.00028547
Iteration 505/1000 | Loss: 0.00028547
Iteration 506/1000 | Loss: 0.00028547
Iteration 507/1000 | Loss: 0.00028547
Iteration 508/1000 | Loss: 0.00028547
Iteration 509/1000 | Loss: 0.00028546
Iteration 510/1000 | Loss: 0.00028546
Iteration 511/1000 | Loss: 0.00028546
Iteration 512/1000 | Loss: 0.00028546
Iteration 513/1000 | Loss: 0.00028546
Iteration 514/1000 | Loss: 0.00028546
Iteration 515/1000 | Loss: 0.00028546
Iteration 516/1000 | Loss: 0.00028546
Iteration 517/1000 | Loss: 0.00028546
Iteration 518/1000 | Loss: 0.00028546
Iteration 519/1000 | Loss: 0.00028546
Iteration 520/1000 | Loss: 0.00028546
Iteration 521/1000 | Loss: 0.00028546
Iteration 522/1000 | Loss: 0.00028545
Iteration 523/1000 | Loss: 0.00028545
Iteration 524/1000 | Loss: 0.00028545
Iteration 525/1000 | Loss: 0.00028545
Iteration 526/1000 | Loss: 0.00028545
Iteration 527/1000 | Loss: 0.00028545
Iteration 528/1000 | Loss: 0.00028545
Iteration 529/1000 | Loss: 0.00028544
Iteration 530/1000 | Loss: 0.00028544
Iteration 531/1000 | Loss: 0.00028544
Iteration 532/1000 | Loss: 0.00028544
Iteration 533/1000 | Loss: 0.00028544
Iteration 534/1000 | Loss: 0.00028544
Iteration 535/1000 | Loss: 0.00028543
Iteration 536/1000 | Loss: 0.00028543
Iteration 537/1000 | Loss: 0.00028543
Iteration 538/1000 | Loss: 0.00028543
Iteration 539/1000 | Loss: 0.00028543
Iteration 540/1000 | Loss: 0.00028543
Iteration 541/1000 | Loss: 0.00028543
Iteration 542/1000 | Loss: 0.00028543
Iteration 543/1000 | Loss: 0.00028543
Iteration 544/1000 | Loss: 0.00028542
Iteration 545/1000 | Loss: 0.00028542
Iteration 546/1000 | Loss: 0.00028542
Iteration 547/1000 | Loss: 0.00028542
Iteration 548/1000 | Loss: 0.00028542
Iteration 549/1000 | Loss: 0.00028542
Iteration 550/1000 | Loss: 0.00028542
Iteration 551/1000 | Loss: 0.00028542
Iteration 552/1000 | Loss: 0.00028542
Iteration 553/1000 | Loss: 0.00028542
Iteration 554/1000 | Loss: 0.00028542
Iteration 555/1000 | Loss: 0.00028542
Iteration 556/1000 | Loss: 0.00028542
Iteration 557/1000 | Loss: 0.00028542
Iteration 558/1000 | Loss: 0.00028542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 558. Stopping optimization.
Last 5 losses: [0.00028541748179122806, 0.00028541748179122806, 0.00028541748179122806, 0.00028541748179122806, 0.00028541748179122806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028541748179122806

Optimization complete. Final v2v error: 9.293196678161621 mm

Highest mean error: 26.07388687133789 mm for frame 202

Lowest mean error: 5.8315935134887695 mm for frame 1

Saving results

Total time: 794.4953410625458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552890
Iteration 2/25 | Loss: 0.00175502
Iteration 3/25 | Loss: 0.00167935
Iteration 4/25 | Loss: 0.00165007
Iteration 5/25 | Loss: 0.00164298
Iteration 6/25 | Loss: 0.00164051
Iteration 7/25 | Loss: 0.00163987
Iteration 8/25 | Loss: 0.00163987
Iteration 9/25 | Loss: 0.00163987
Iteration 10/25 | Loss: 0.00163987
Iteration 11/25 | Loss: 0.00163987
Iteration 12/25 | Loss: 0.00163987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016398694133386016, 0.0016398694133386016, 0.0016398694133386016, 0.0016398694133386016, 0.0016398694133386016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016398694133386016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45648861
Iteration 2/25 | Loss: 0.00225553
Iteration 3/25 | Loss: 0.00225552
Iteration 4/25 | Loss: 0.00225552
Iteration 5/25 | Loss: 0.00225551
Iteration 6/25 | Loss: 0.00225551
Iteration 7/25 | Loss: 0.00225551
Iteration 8/25 | Loss: 0.00225551
Iteration 9/25 | Loss: 0.00225551
Iteration 10/25 | Loss: 0.00225551
Iteration 11/25 | Loss: 0.00225551
Iteration 12/25 | Loss: 0.00225551
Iteration 13/25 | Loss: 0.00225551
Iteration 14/25 | Loss: 0.00225551
Iteration 15/25 | Loss: 0.00225551
Iteration 16/25 | Loss: 0.00225551
Iteration 17/25 | Loss: 0.00225551
Iteration 18/25 | Loss: 0.00225551
Iteration 19/25 | Loss: 0.00225551
Iteration 20/25 | Loss: 0.00225551
Iteration 21/25 | Loss: 0.00225551
Iteration 22/25 | Loss: 0.00225551
Iteration 23/25 | Loss: 0.00225551
Iteration 24/25 | Loss: 0.00225551
Iteration 25/25 | Loss: 0.00225551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225551
Iteration 2/1000 | Loss: 0.00007620
Iteration 3/1000 | Loss: 0.00005443
Iteration 4/1000 | Loss: 0.00004883
Iteration 5/1000 | Loss: 0.00004638
Iteration 6/1000 | Loss: 0.00004472
Iteration 7/1000 | Loss: 0.00004367
Iteration 8/1000 | Loss: 0.00004285
Iteration 9/1000 | Loss: 0.00004237
Iteration 10/1000 | Loss: 0.00004202
Iteration 11/1000 | Loss: 0.00004174
Iteration 12/1000 | Loss: 0.00004174
Iteration 13/1000 | Loss: 0.00004169
Iteration 14/1000 | Loss: 0.00004164
Iteration 15/1000 | Loss: 0.00004159
Iteration 16/1000 | Loss: 0.00004158
Iteration 17/1000 | Loss: 0.00004158
Iteration 18/1000 | Loss: 0.00004157
Iteration 19/1000 | Loss: 0.00004153
Iteration 20/1000 | Loss: 0.00004152
Iteration 21/1000 | Loss: 0.00004152
Iteration 22/1000 | Loss: 0.00004151
Iteration 23/1000 | Loss: 0.00004148
Iteration 24/1000 | Loss: 0.00004148
Iteration 25/1000 | Loss: 0.00004147
Iteration 26/1000 | Loss: 0.00004147
Iteration 27/1000 | Loss: 0.00004147
Iteration 28/1000 | Loss: 0.00004146
Iteration 29/1000 | Loss: 0.00004146
Iteration 30/1000 | Loss: 0.00004146
Iteration 31/1000 | Loss: 0.00004145
Iteration 32/1000 | Loss: 0.00004144
Iteration 33/1000 | Loss: 0.00004144
Iteration 34/1000 | Loss: 0.00004144
Iteration 35/1000 | Loss: 0.00004143
Iteration 36/1000 | Loss: 0.00004143
Iteration 37/1000 | Loss: 0.00004143
Iteration 38/1000 | Loss: 0.00004142
Iteration 39/1000 | Loss: 0.00004142
Iteration 40/1000 | Loss: 0.00004141
Iteration 41/1000 | Loss: 0.00004141
Iteration 42/1000 | Loss: 0.00004141
Iteration 43/1000 | Loss: 0.00004140
Iteration 44/1000 | Loss: 0.00004140
Iteration 45/1000 | Loss: 0.00004140
Iteration 46/1000 | Loss: 0.00004140
Iteration 47/1000 | Loss: 0.00004140
Iteration 48/1000 | Loss: 0.00004139
Iteration 49/1000 | Loss: 0.00004139
Iteration 50/1000 | Loss: 0.00004139
Iteration 51/1000 | Loss: 0.00004139
Iteration 52/1000 | Loss: 0.00004139
Iteration 53/1000 | Loss: 0.00004139
Iteration 54/1000 | Loss: 0.00004139
Iteration 55/1000 | Loss: 0.00004139
Iteration 56/1000 | Loss: 0.00004138
Iteration 57/1000 | Loss: 0.00004138
Iteration 58/1000 | Loss: 0.00004138
Iteration 59/1000 | Loss: 0.00004138
Iteration 60/1000 | Loss: 0.00004137
Iteration 61/1000 | Loss: 0.00004137
Iteration 62/1000 | Loss: 0.00004137
Iteration 63/1000 | Loss: 0.00004137
Iteration 64/1000 | Loss: 0.00004137
Iteration 65/1000 | Loss: 0.00004137
Iteration 66/1000 | Loss: 0.00004137
Iteration 67/1000 | Loss: 0.00004137
Iteration 68/1000 | Loss: 0.00004137
Iteration 69/1000 | Loss: 0.00004137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [4.1373441490577534e-05, 4.1373441490577534e-05, 4.1373441490577534e-05, 4.1373441490577534e-05, 4.1373441490577534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1373441490577534e-05

Optimization complete. Final v2v error: 5.539376735687256 mm

Highest mean error: 6.280883312225342 mm for frame 18

Lowest mean error: 4.852344512939453 mm for frame 5

Saving results

Total time: 31.887428522109985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412680
Iteration 2/25 | Loss: 0.00186534
Iteration 3/25 | Loss: 0.00159790
Iteration 4/25 | Loss: 0.00153282
Iteration 5/25 | Loss: 0.00152273
Iteration 6/25 | Loss: 0.00152043
Iteration 7/25 | Loss: 0.00151970
Iteration 8/25 | Loss: 0.00151969
Iteration 9/25 | Loss: 0.00151969
Iteration 10/25 | Loss: 0.00151969
Iteration 11/25 | Loss: 0.00151969
Iteration 12/25 | Loss: 0.00151969
Iteration 13/25 | Loss: 0.00151969
Iteration 14/25 | Loss: 0.00151969
Iteration 15/25 | Loss: 0.00151969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001519686309620738, 0.001519686309620738, 0.001519686309620738, 0.001519686309620738, 0.001519686309620738]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001519686309620738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56596732
Iteration 2/25 | Loss: 0.00211868
Iteration 3/25 | Loss: 0.00211868
Iteration 4/25 | Loss: 0.00211868
Iteration 5/25 | Loss: 0.00211868
Iteration 6/25 | Loss: 0.00211868
Iteration 7/25 | Loss: 0.00211868
Iteration 8/25 | Loss: 0.00211868
Iteration 9/25 | Loss: 0.00211868
Iteration 10/25 | Loss: 0.00211868
Iteration 11/25 | Loss: 0.00211868
Iteration 12/25 | Loss: 0.00211868
Iteration 13/25 | Loss: 0.00211868
Iteration 14/25 | Loss: 0.00211868
Iteration 15/25 | Loss: 0.00211868
Iteration 16/25 | Loss: 0.00211868
Iteration 17/25 | Loss: 0.00211868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0021186794620007277, 0.0021186794620007277, 0.0021186794620007277, 0.0021186794620007277, 0.0021186794620007277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021186794620007277

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211868
Iteration 2/1000 | Loss: 0.00005644
Iteration 3/1000 | Loss: 0.00003990
Iteration 4/1000 | Loss: 0.00003479
Iteration 5/1000 | Loss: 0.00003079
Iteration 6/1000 | Loss: 0.00002854
Iteration 7/1000 | Loss: 0.00002719
Iteration 8/1000 | Loss: 0.00002635
Iteration 9/1000 | Loss: 0.00002579
Iteration 10/1000 | Loss: 0.00002552
Iteration 11/1000 | Loss: 0.00002525
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002476
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002466
Iteration 16/1000 | Loss: 0.00002465
Iteration 17/1000 | Loss: 0.00002465
Iteration 18/1000 | Loss: 0.00002464
Iteration 19/1000 | Loss: 0.00002463
Iteration 20/1000 | Loss: 0.00002462
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002461
Iteration 23/1000 | Loss: 0.00002460
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002458
Iteration 27/1000 | Loss: 0.00002457
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002456
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002452
Iteration 36/1000 | Loss: 0.00002452
Iteration 37/1000 | Loss: 0.00002452
Iteration 38/1000 | Loss: 0.00002452
Iteration 39/1000 | Loss: 0.00002452
Iteration 40/1000 | Loss: 0.00002452
Iteration 41/1000 | Loss: 0.00002452
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002452
Iteration 44/1000 | Loss: 0.00002451
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002450
Iteration 51/1000 | Loss: 0.00002450
Iteration 52/1000 | Loss: 0.00002450
Iteration 53/1000 | Loss: 0.00002450
Iteration 54/1000 | Loss: 0.00002450
Iteration 55/1000 | Loss: 0.00002450
Iteration 56/1000 | Loss: 0.00002450
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002449
Iteration 59/1000 | Loss: 0.00002449
Iteration 60/1000 | Loss: 0.00002449
Iteration 61/1000 | Loss: 0.00002449
Iteration 62/1000 | Loss: 0.00002449
Iteration 63/1000 | Loss: 0.00002449
Iteration 64/1000 | Loss: 0.00002449
Iteration 65/1000 | Loss: 0.00002449
Iteration 66/1000 | Loss: 0.00002449
Iteration 67/1000 | Loss: 0.00002449
Iteration 68/1000 | Loss: 0.00002449
Iteration 69/1000 | Loss: 0.00002449
Iteration 70/1000 | Loss: 0.00002449
Iteration 71/1000 | Loss: 0.00002449
Iteration 72/1000 | Loss: 0.00002448
Iteration 73/1000 | Loss: 0.00002448
Iteration 74/1000 | Loss: 0.00002448
Iteration 75/1000 | Loss: 0.00002448
Iteration 76/1000 | Loss: 0.00002448
Iteration 77/1000 | Loss: 0.00002448
Iteration 78/1000 | Loss: 0.00002448
Iteration 79/1000 | Loss: 0.00002447
Iteration 80/1000 | Loss: 0.00002447
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002447
Iteration 85/1000 | Loss: 0.00002446
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00002446
Iteration 88/1000 | Loss: 0.00002446
Iteration 89/1000 | Loss: 0.00002446
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002446
Iteration 92/1000 | Loss: 0.00002446
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00002445
Iteration 99/1000 | Loss: 0.00002445
Iteration 100/1000 | Loss: 0.00002445
Iteration 101/1000 | Loss: 0.00002445
Iteration 102/1000 | Loss: 0.00002445
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002444
Iteration 105/1000 | Loss: 0.00002444
Iteration 106/1000 | Loss: 0.00002444
Iteration 107/1000 | Loss: 0.00002443
Iteration 108/1000 | Loss: 0.00002443
Iteration 109/1000 | Loss: 0.00002443
Iteration 110/1000 | Loss: 0.00002442
Iteration 111/1000 | Loss: 0.00002442
Iteration 112/1000 | Loss: 0.00002442
Iteration 113/1000 | Loss: 0.00002442
Iteration 114/1000 | Loss: 0.00002442
Iteration 115/1000 | Loss: 0.00002442
Iteration 116/1000 | Loss: 0.00002442
Iteration 117/1000 | Loss: 0.00002441
Iteration 118/1000 | Loss: 0.00002441
Iteration 119/1000 | Loss: 0.00002441
Iteration 120/1000 | Loss: 0.00002441
Iteration 121/1000 | Loss: 0.00002441
Iteration 122/1000 | Loss: 0.00002440
Iteration 123/1000 | Loss: 0.00002440
Iteration 124/1000 | Loss: 0.00002440
Iteration 125/1000 | Loss: 0.00002440
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002439
Iteration 128/1000 | Loss: 0.00002439
Iteration 129/1000 | Loss: 0.00002439
Iteration 130/1000 | Loss: 0.00002439
Iteration 131/1000 | Loss: 0.00002439
Iteration 132/1000 | Loss: 0.00002439
Iteration 133/1000 | Loss: 0.00002439
Iteration 134/1000 | Loss: 0.00002438
Iteration 135/1000 | Loss: 0.00002438
Iteration 136/1000 | Loss: 0.00002438
Iteration 137/1000 | Loss: 0.00002438
Iteration 138/1000 | Loss: 0.00002438
Iteration 139/1000 | Loss: 0.00002438
Iteration 140/1000 | Loss: 0.00002438
Iteration 141/1000 | Loss: 0.00002437
Iteration 142/1000 | Loss: 0.00002437
Iteration 143/1000 | Loss: 0.00002437
Iteration 144/1000 | Loss: 0.00002437
Iteration 145/1000 | Loss: 0.00002437
Iteration 146/1000 | Loss: 0.00002437
Iteration 147/1000 | Loss: 0.00002437
Iteration 148/1000 | Loss: 0.00002437
Iteration 149/1000 | Loss: 0.00002437
Iteration 150/1000 | Loss: 0.00002437
Iteration 151/1000 | Loss: 0.00002437
Iteration 152/1000 | Loss: 0.00002437
Iteration 153/1000 | Loss: 0.00002436
Iteration 154/1000 | Loss: 0.00002436
Iteration 155/1000 | Loss: 0.00002436
Iteration 156/1000 | Loss: 0.00002436
Iteration 157/1000 | Loss: 0.00002436
Iteration 158/1000 | Loss: 0.00002436
Iteration 159/1000 | Loss: 0.00002436
Iteration 160/1000 | Loss: 0.00002436
Iteration 161/1000 | Loss: 0.00002436
Iteration 162/1000 | Loss: 0.00002436
Iteration 163/1000 | Loss: 0.00002436
Iteration 164/1000 | Loss: 0.00002436
Iteration 165/1000 | Loss: 0.00002435
Iteration 166/1000 | Loss: 0.00002435
Iteration 167/1000 | Loss: 0.00002435
Iteration 168/1000 | Loss: 0.00002435
Iteration 169/1000 | Loss: 0.00002435
Iteration 170/1000 | Loss: 0.00002435
Iteration 171/1000 | Loss: 0.00002435
Iteration 172/1000 | Loss: 0.00002435
Iteration 173/1000 | Loss: 0.00002435
Iteration 174/1000 | Loss: 0.00002435
Iteration 175/1000 | Loss: 0.00002435
Iteration 176/1000 | Loss: 0.00002435
Iteration 177/1000 | Loss: 0.00002435
Iteration 178/1000 | Loss: 0.00002435
Iteration 179/1000 | Loss: 0.00002435
Iteration 180/1000 | Loss: 0.00002435
Iteration 181/1000 | Loss: 0.00002435
Iteration 182/1000 | Loss: 0.00002435
Iteration 183/1000 | Loss: 0.00002435
Iteration 184/1000 | Loss: 0.00002435
Iteration 185/1000 | Loss: 0.00002435
Iteration 186/1000 | Loss: 0.00002435
Iteration 187/1000 | Loss: 0.00002435
Iteration 188/1000 | Loss: 0.00002435
Iteration 189/1000 | Loss: 0.00002435
Iteration 190/1000 | Loss: 0.00002435
Iteration 191/1000 | Loss: 0.00002435
Iteration 192/1000 | Loss: 0.00002435
Iteration 193/1000 | Loss: 0.00002435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.434962152619846e-05, 2.434962152619846e-05, 2.434962152619846e-05, 2.434962152619846e-05, 2.434962152619846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.434962152619846e-05

Optimization complete. Final v2v error: 4.347064971923828 mm

Highest mean error: 4.872891426086426 mm for frame 114

Lowest mean error: 3.830350160598755 mm for frame 61

Saving results

Total time: 39.91592860221863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872196
Iteration 2/25 | Loss: 0.00267368
Iteration 3/25 | Loss: 0.00162568
Iteration 4/25 | Loss: 0.00159192
Iteration 5/25 | Loss: 0.00151562
Iteration 6/25 | Loss: 0.00148324
Iteration 7/25 | Loss: 0.00142395
Iteration 8/25 | Loss: 0.00141721
Iteration 9/25 | Loss: 0.00141513
Iteration 10/25 | Loss: 0.00138883
Iteration 11/25 | Loss: 0.00138578
Iteration 12/25 | Loss: 0.00138640
Iteration 13/25 | Loss: 0.00136763
Iteration 14/25 | Loss: 0.00136853
Iteration 15/25 | Loss: 0.00136747
Iteration 16/25 | Loss: 0.00136634
Iteration 17/25 | Loss: 0.00136734
Iteration 18/25 | Loss: 0.00136637
Iteration 19/25 | Loss: 0.00136750
Iteration 20/25 | Loss: 0.00136619
Iteration 21/25 | Loss: 0.00136731
Iteration 22/25 | Loss: 0.00136612
Iteration 23/25 | Loss: 0.00136739
Iteration 24/25 | Loss: 0.00136588
Iteration 25/25 | Loss: 0.00136569

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52403533
Iteration 2/25 | Loss: 0.00138453
Iteration 3/25 | Loss: 0.00138453
Iteration 4/25 | Loss: 0.00138453
Iteration 5/25 | Loss: 0.00138453
Iteration 6/25 | Loss: 0.00138453
Iteration 7/25 | Loss: 0.00138453
Iteration 8/25 | Loss: 0.00138453
Iteration 9/25 | Loss: 0.00138453
Iteration 10/25 | Loss: 0.00138453
Iteration 11/25 | Loss: 0.00138453
Iteration 12/25 | Loss: 0.00138453
Iteration 13/25 | Loss: 0.00138453
Iteration 14/25 | Loss: 0.00138453
Iteration 15/25 | Loss: 0.00138453
Iteration 16/25 | Loss: 0.00138453
Iteration 17/25 | Loss: 0.00138453
Iteration 18/25 | Loss: 0.00138453
Iteration 19/25 | Loss: 0.00138453
Iteration 20/25 | Loss: 0.00138453
Iteration 21/25 | Loss: 0.00138453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013845318462699652, 0.0013845318462699652, 0.0013845318462699652, 0.0013845318462699652, 0.0013845318462699652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013845318462699652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138453
Iteration 2/1000 | Loss: 0.00005417
Iteration 3/1000 | Loss: 0.00003862
Iteration 4/1000 | Loss: 0.00012663
Iteration 5/1000 | Loss: 0.00003355
Iteration 6/1000 | Loss: 0.00002997
Iteration 7/1000 | Loss: 0.00014270
Iteration 8/1000 | Loss: 0.00002886
Iteration 9/1000 | Loss: 0.00002851
Iteration 10/1000 | Loss: 0.00012686
Iteration 11/1000 | Loss: 0.00002877
Iteration 12/1000 | Loss: 0.00002778
Iteration 13/1000 | Loss: 0.00002766
Iteration 14/1000 | Loss: 0.00002765
Iteration 15/1000 | Loss: 0.00002763
Iteration 16/1000 | Loss: 0.00002761
Iteration 17/1000 | Loss: 0.00002759
Iteration 18/1000 | Loss: 0.00002753
Iteration 19/1000 | Loss: 0.00002750
Iteration 20/1000 | Loss: 0.00002750
Iteration 21/1000 | Loss: 0.00002745
Iteration 22/1000 | Loss: 0.00002745
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002744
Iteration 26/1000 | Loss: 0.00002744
Iteration 27/1000 | Loss: 0.00002744
Iteration 28/1000 | Loss: 0.00002744
Iteration 29/1000 | Loss: 0.00002744
Iteration 30/1000 | Loss: 0.00002744
Iteration 31/1000 | Loss: 0.00002744
Iteration 32/1000 | Loss: 0.00002743
Iteration 33/1000 | Loss: 0.00002743
Iteration 34/1000 | Loss: 0.00002743
Iteration 35/1000 | Loss: 0.00002743
Iteration 36/1000 | Loss: 0.00002743
Iteration 37/1000 | Loss: 0.00002741
Iteration 38/1000 | Loss: 0.00002741
Iteration 39/1000 | Loss: 0.00002741
Iteration 40/1000 | Loss: 0.00002741
Iteration 41/1000 | Loss: 0.00002741
Iteration 42/1000 | Loss: 0.00002741
Iteration 43/1000 | Loss: 0.00002741
Iteration 44/1000 | Loss: 0.00002741
Iteration 45/1000 | Loss: 0.00002741
Iteration 46/1000 | Loss: 0.00002741
Iteration 47/1000 | Loss: 0.00002741
Iteration 48/1000 | Loss: 0.00002741
Iteration 49/1000 | Loss: 0.00002741
Iteration 50/1000 | Loss: 0.00002741
Iteration 51/1000 | Loss: 0.00002741
Iteration 52/1000 | Loss: 0.00002741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [2.7408887035562657e-05, 2.7408887035562657e-05, 2.7408887035562657e-05, 2.7408887035562657e-05, 2.7408887035562657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7408887035562657e-05

Optimization complete. Final v2v error: 4.325233459472656 mm

Highest mean error: 11.04015064239502 mm for frame 133

Lowest mean error: 3.841862916946411 mm for frame 156

Saving results

Total time: 68.19835710525513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463265
Iteration 2/25 | Loss: 0.00160981
Iteration 3/25 | Loss: 0.00153497
Iteration 4/25 | Loss: 0.00152605
Iteration 5/25 | Loss: 0.00152203
Iteration 6/25 | Loss: 0.00152173
Iteration 7/25 | Loss: 0.00152173
Iteration 8/25 | Loss: 0.00152173
Iteration 9/25 | Loss: 0.00152173
Iteration 10/25 | Loss: 0.00152173
Iteration 11/25 | Loss: 0.00152173
Iteration 12/25 | Loss: 0.00152173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015217347536236048, 0.0015217347536236048, 0.0015217347536236048, 0.0015217347536236048, 0.0015217347536236048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015217347536236048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90623498
Iteration 2/25 | Loss: 0.00192629
Iteration 3/25 | Loss: 0.00192629
Iteration 4/25 | Loss: 0.00192629
Iteration 5/25 | Loss: 0.00192629
Iteration 6/25 | Loss: 0.00192629
Iteration 7/25 | Loss: 0.00192629
Iteration 8/25 | Loss: 0.00192629
Iteration 9/25 | Loss: 0.00192629
Iteration 10/25 | Loss: 0.00192629
Iteration 11/25 | Loss: 0.00192629
Iteration 12/25 | Loss: 0.00192629
Iteration 13/25 | Loss: 0.00192629
Iteration 14/25 | Loss: 0.00192629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019262908026576042, 0.0019262908026576042, 0.0019262908026576042, 0.0019262908026576042, 0.0019262908026576042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019262908026576042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192629
Iteration 2/1000 | Loss: 0.00005997
Iteration 3/1000 | Loss: 0.00003829
Iteration 4/1000 | Loss: 0.00003327
Iteration 5/1000 | Loss: 0.00003139
Iteration 6/1000 | Loss: 0.00003038
Iteration 7/1000 | Loss: 0.00002974
Iteration 8/1000 | Loss: 0.00002932
Iteration 9/1000 | Loss: 0.00002892
Iteration 10/1000 | Loss: 0.00002878
Iteration 11/1000 | Loss: 0.00002871
Iteration 12/1000 | Loss: 0.00002870
Iteration 13/1000 | Loss: 0.00002869
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002866
Iteration 16/1000 | Loss: 0.00002866
Iteration 17/1000 | Loss: 0.00002864
Iteration 18/1000 | Loss: 0.00002864
Iteration 19/1000 | Loss: 0.00002861
Iteration 20/1000 | Loss: 0.00002859
Iteration 21/1000 | Loss: 0.00002858
Iteration 22/1000 | Loss: 0.00002857
Iteration 23/1000 | Loss: 0.00002855
Iteration 24/1000 | Loss: 0.00002855
Iteration 25/1000 | Loss: 0.00002855
Iteration 26/1000 | Loss: 0.00002855
Iteration 27/1000 | Loss: 0.00002855
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00002854
Iteration 30/1000 | Loss: 0.00002854
Iteration 31/1000 | Loss: 0.00002854
Iteration 32/1000 | Loss: 0.00002854
Iteration 33/1000 | Loss: 0.00002854
Iteration 34/1000 | Loss: 0.00002851
Iteration 35/1000 | Loss: 0.00002851
Iteration 36/1000 | Loss: 0.00002851
Iteration 37/1000 | Loss: 0.00002850
Iteration 38/1000 | Loss: 0.00002850
Iteration 39/1000 | Loss: 0.00002850
Iteration 40/1000 | Loss: 0.00002850
Iteration 41/1000 | Loss: 0.00002850
Iteration 42/1000 | Loss: 0.00002848
Iteration 43/1000 | Loss: 0.00002848
Iteration 44/1000 | Loss: 0.00002847
Iteration 45/1000 | Loss: 0.00002847
Iteration 46/1000 | Loss: 0.00002847
Iteration 47/1000 | Loss: 0.00002846
Iteration 48/1000 | Loss: 0.00002846
Iteration 49/1000 | Loss: 0.00002846
Iteration 50/1000 | Loss: 0.00002846
Iteration 51/1000 | Loss: 0.00002845
Iteration 52/1000 | Loss: 0.00002845
Iteration 53/1000 | Loss: 0.00002845
Iteration 54/1000 | Loss: 0.00002844
Iteration 55/1000 | Loss: 0.00002844
Iteration 56/1000 | Loss: 0.00002843
Iteration 57/1000 | Loss: 0.00002843
Iteration 58/1000 | Loss: 0.00002843
Iteration 59/1000 | Loss: 0.00002843
Iteration 60/1000 | Loss: 0.00002842
Iteration 61/1000 | Loss: 0.00002842
Iteration 62/1000 | Loss: 0.00002841
Iteration 63/1000 | Loss: 0.00002841
Iteration 64/1000 | Loss: 0.00002841
Iteration 65/1000 | Loss: 0.00002841
Iteration 66/1000 | Loss: 0.00002841
Iteration 67/1000 | Loss: 0.00002840
Iteration 68/1000 | Loss: 0.00002840
Iteration 69/1000 | Loss: 0.00002840
Iteration 70/1000 | Loss: 0.00002839
Iteration 71/1000 | Loss: 0.00002838
Iteration 72/1000 | Loss: 0.00002838
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002838
Iteration 75/1000 | Loss: 0.00002838
Iteration 76/1000 | Loss: 0.00002838
Iteration 77/1000 | Loss: 0.00002838
Iteration 78/1000 | Loss: 0.00002838
Iteration 79/1000 | Loss: 0.00002838
Iteration 80/1000 | Loss: 0.00002837
Iteration 81/1000 | Loss: 0.00002837
Iteration 82/1000 | Loss: 0.00002837
Iteration 83/1000 | Loss: 0.00002837
Iteration 84/1000 | Loss: 0.00002837
Iteration 85/1000 | Loss: 0.00002837
Iteration 86/1000 | Loss: 0.00002837
Iteration 87/1000 | Loss: 0.00002837
Iteration 88/1000 | Loss: 0.00002837
Iteration 89/1000 | Loss: 0.00002836
Iteration 90/1000 | Loss: 0.00002836
Iteration 91/1000 | Loss: 0.00002836
Iteration 92/1000 | Loss: 0.00002836
Iteration 93/1000 | Loss: 0.00002836
Iteration 94/1000 | Loss: 0.00002836
Iteration 95/1000 | Loss: 0.00002836
Iteration 96/1000 | Loss: 0.00002836
Iteration 97/1000 | Loss: 0.00002836
Iteration 98/1000 | Loss: 0.00002836
Iteration 99/1000 | Loss: 0.00002836
Iteration 100/1000 | Loss: 0.00002836
Iteration 101/1000 | Loss: 0.00002836
Iteration 102/1000 | Loss: 0.00002836
Iteration 103/1000 | Loss: 0.00002836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.8358794224914163e-05, 2.8358794224914163e-05, 2.8358794224914163e-05, 2.8358794224914163e-05, 2.8358794224914163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8358794224914163e-05

Optimization complete. Final v2v error: 4.579569339752197 mm

Highest mean error: 4.922542095184326 mm for frame 96

Lowest mean error: 4.360526084899902 mm for frame 0

Saving results

Total time: 32.721009254455566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464597
Iteration 2/25 | Loss: 0.00162750
Iteration 3/25 | Loss: 0.00154082
Iteration 4/25 | Loss: 0.00152390
Iteration 5/25 | Loss: 0.00151697
Iteration 6/25 | Loss: 0.00151555
Iteration 7/25 | Loss: 0.00151555
Iteration 8/25 | Loss: 0.00151555
Iteration 9/25 | Loss: 0.00151555
Iteration 10/25 | Loss: 0.00151555
Iteration 11/25 | Loss: 0.00151555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001515552750788629, 0.001515552750788629, 0.001515552750788629, 0.001515552750788629, 0.001515552750788629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001515552750788629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.47791529
Iteration 2/25 | Loss: 0.00182264
Iteration 3/25 | Loss: 0.00182264
Iteration 4/25 | Loss: 0.00182264
Iteration 5/25 | Loss: 0.00182264
Iteration 6/25 | Loss: 0.00182264
Iteration 7/25 | Loss: 0.00182264
Iteration 8/25 | Loss: 0.00182264
Iteration 9/25 | Loss: 0.00182264
Iteration 10/25 | Loss: 0.00182264
Iteration 11/25 | Loss: 0.00182264
Iteration 12/25 | Loss: 0.00182264
Iteration 13/25 | Loss: 0.00182264
Iteration 14/25 | Loss: 0.00182264
Iteration 15/25 | Loss: 0.00182264
Iteration 16/25 | Loss: 0.00182264
Iteration 17/25 | Loss: 0.00182264
Iteration 18/25 | Loss: 0.00182264
Iteration 19/25 | Loss: 0.00182264
Iteration 20/25 | Loss: 0.00182264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0018226414686068892, 0.0018226414686068892, 0.0018226414686068892, 0.0018226414686068892, 0.0018226414686068892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018226414686068892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182264
Iteration 2/1000 | Loss: 0.00004787
Iteration 3/1000 | Loss: 0.00003859
Iteration 4/1000 | Loss: 0.00003540
Iteration 5/1000 | Loss: 0.00003390
Iteration 6/1000 | Loss: 0.00003312
Iteration 7/1000 | Loss: 0.00003266
Iteration 8/1000 | Loss: 0.00003225
Iteration 9/1000 | Loss: 0.00003193
Iteration 10/1000 | Loss: 0.00003178
Iteration 11/1000 | Loss: 0.00003177
Iteration 12/1000 | Loss: 0.00003176
Iteration 13/1000 | Loss: 0.00003173
Iteration 14/1000 | Loss: 0.00003172
Iteration 15/1000 | Loss: 0.00003172
Iteration 16/1000 | Loss: 0.00003171
Iteration 17/1000 | Loss: 0.00003171
Iteration 18/1000 | Loss: 0.00003171
Iteration 19/1000 | Loss: 0.00003171
Iteration 20/1000 | Loss: 0.00003171
Iteration 21/1000 | Loss: 0.00003171
Iteration 22/1000 | Loss: 0.00003171
Iteration 23/1000 | Loss: 0.00003171
Iteration 24/1000 | Loss: 0.00003169
Iteration 25/1000 | Loss: 0.00003167
Iteration 26/1000 | Loss: 0.00003162
Iteration 27/1000 | Loss: 0.00003160
Iteration 28/1000 | Loss: 0.00003156
Iteration 29/1000 | Loss: 0.00003153
Iteration 30/1000 | Loss: 0.00003151
Iteration 31/1000 | Loss: 0.00003151
Iteration 32/1000 | Loss: 0.00003150
Iteration 33/1000 | Loss: 0.00003150
Iteration 34/1000 | Loss: 0.00003150
Iteration 35/1000 | Loss: 0.00003150
Iteration 36/1000 | Loss: 0.00003149
Iteration 37/1000 | Loss: 0.00003149
Iteration 38/1000 | Loss: 0.00003149
Iteration 39/1000 | Loss: 0.00003148
Iteration 40/1000 | Loss: 0.00003147
Iteration 41/1000 | Loss: 0.00003147
Iteration 42/1000 | Loss: 0.00003147
Iteration 43/1000 | Loss: 0.00003146
Iteration 44/1000 | Loss: 0.00003146
Iteration 45/1000 | Loss: 0.00003146
Iteration 46/1000 | Loss: 0.00003145
Iteration 47/1000 | Loss: 0.00003145
Iteration 48/1000 | Loss: 0.00003145
Iteration 49/1000 | Loss: 0.00003145
Iteration 50/1000 | Loss: 0.00003144
Iteration 51/1000 | Loss: 0.00003144
Iteration 52/1000 | Loss: 0.00003144
Iteration 53/1000 | Loss: 0.00003143
Iteration 54/1000 | Loss: 0.00003142
Iteration 55/1000 | Loss: 0.00003142
Iteration 56/1000 | Loss: 0.00003142
Iteration 57/1000 | Loss: 0.00003142
Iteration 58/1000 | Loss: 0.00003142
Iteration 59/1000 | Loss: 0.00003142
Iteration 60/1000 | Loss: 0.00003142
Iteration 61/1000 | Loss: 0.00003142
Iteration 62/1000 | Loss: 0.00003142
Iteration 63/1000 | Loss: 0.00003141
Iteration 64/1000 | Loss: 0.00003141
Iteration 65/1000 | Loss: 0.00003141
Iteration 66/1000 | Loss: 0.00003141
Iteration 67/1000 | Loss: 0.00003141
Iteration 68/1000 | Loss: 0.00003141
Iteration 69/1000 | Loss: 0.00003140
Iteration 70/1000 | Loss: 0.00003140
Iteration 71/1000 | Loss: 0.00003139
Iteration 72/1000 | Loss: 0.00003139
Iteration 73/1000 | Loss: 0.00003138
Iteration 74/1000 | Loss: 0.00003138
Iteration 75/1000 | Loss: 0.00003138
Iteration 76/1000 | Loss: 0.00003138
Iteration 77/1000 | Loss: 0.00003138
Iteration 78/1000 | Loss: 0.00003138
Iteration 79/1000 | Loss: 0.00003138
Iteration 80/1000 | Loss: 0.00003138
Iteration 81/1000 | Loss: 0.00003138
Iteration 82/1000 | Loss: 0.00003138
Iteration 83/1000 | Loss: 0.00003138
Iteration 84/1000 | Loss: 0.00003137
Iteration 85/1000 | Loss: 0.00003137
Iteration 86/1000 | Loss: 0.00003137
Iteration 87/1000 | Loss: 0.00003137
Iteration 88/1000 | Loss: 0.00003137
Iteration 89/1000 | Loss: 0.00003137
Iteration 90/1000 | Loss: 0.00003137
Iteration 91/1000 | Loss: 0.00003136
Iteration 92/1000 | Loss: 0.00003136
Iteration 93/1000 | Loss: 0.00003136
Iteration 94/1000 | Loss: 0.00003136
Iteration 95/1000 | Loss: 0.00003136
Iteration 96/1000 | Loss: 0.00003136
Iteration 97/1000 | Loss: 0.00003136
Iteration 98/1000 | Loss: 0.00003136
Iteration 99/1000 | Loss: 0.00003136
Iteration 100/1000 | Loss: 0.00003136
Iteration 101/1000 | Loss: 0.00003136
Iteration 102/1000 | Loss: 0.00003136
Iteration 103/1000 | Loss: 0.00003136
Iteration 104/1000 | Loss: 0.00003136
Iteration 105/1000 | Loss: 0.00003136
Iteration 106/1000 | Loss: 0.00003136
Iteration 107/1000 | Loss: 0.00003136
Iteration 108/1000 | Loss: 0.00003136
Iteration 109/1000 | Loss: 0.00003136
Iteration 110/1000 | Loss: 0.00003136
Iteration 111/1000 | Loss: 0.00003135
Iteration 112/1000 | Loss: 0.00003135
Iteration 113/1000 | Loss: 0.00003135
Iteration 114/1000 | Loss: 0.00003135
Iteration 115/1000 | Loss: 0.00003135
Iteration 116/1000 | Loss: 0.00003135
Iteration 117/1000 | Loss: 0.00003135
Iteration 118/1000 | Loss: 0.00003135
Iteration 119/1000 | Loss: 0.00003135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [3.135314545943402e-05, 3.135314545943402e-05, 3.135314545943402e-05, 3.135314545943402e-05, 3.135314545943402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.135314545943402e-05

Optimization complete. Final v2v error: 4.8603515625 mm

Highest mean error: 5.149590492248535 mm for frame 163

Lowest mean error: 4.502718448638916 mm for frame 69

Saving results

Total time: 34.820306062698364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976606
Iteration 2/25 | Loss: 0.00169264
Iteration 3/25 | Loss: 0.00160521
Iteration 4/25 | Loss: 0.00158576
Iteration 5/25 | Loss: 0.00158343
Iteration 6/25 | Loss: 0.00158343
Iteration 7/25 | Loss: 0.00158343
Iteration 8/25 | Loss: 0.00158343
Iteration 9/25 | Loss: 0.00158343
Iteration 10/25 | Loss: 0.00158343
Iteration 11/25 | Loss: 0.00158343
Iteration 12/25 | Loss: 0.00158343
Iteration 13/25 | Loss: 0.00158343
Iteration 14/25 | Loss: 0.00158343
Iteration 15/25 | Loss: 0.00158343
Iteration 16/25 | Loss: 0.00158343
Iteration 17/25 | Loss: 0.00158343
Iteration 18/25 | Loss: 0.00158343
Iteration 19/25 | Loss: 0.00158343
Iteration 20/25 | Loss: 0.00158343
Iteration 21/25 | Loss: 0.00158343
Iteration 22/25 | Loss: 0.00158343
Iteration 23/25 | Loss: 0.00158343
Iteration 24/25 | Loss: 0.00158343
Iteration 25/25 | Loss: 0.00158343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52283204
Iteration 2/25 | Loss: 0.00202796
Iteration 3/25 | Loss: 0.00202795
Iteration 4/25 | Loss: 0.00202795
Iteration 5/25 | Loss: 0.00202795
Iteration 6/25 | Loss: 0.00202795
Iteration 7/25 | Loss: 0.00202795
Iteration 8/25 | Loss: 0.00202795
Iteration 9/25 | Loss: 0.00202795
Iteration 10/25 | Loss: 0.00202795
Iteration 11/25 | Loss: 0.00202795
Iteration 12/25 | Loss: 0.00202795
Iteration 13/25 | Loss: 0.00202795
Iteration 14/25 | Loss: 0.00202795
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002027947688475251, 0.002027947688475251, 0.002027947688475251, 0.002027947688475251, 0.002027947688475251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002027947688475251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202795
Iteration 2/1000 | Loss: 0.00006222
Iteration 3/1000 | Loss: 0.00004692
Iteration 4/1000 | Loss: 0.00004262
Iteration 5/1000 | Loss: 0.00003984
Iteration 6/1000 | Loss: 0.00003833
Iteration 7/1000 | Loss: 0.00003727
Iteration 8/1000 | Loss: 0.00003676
Iteration 9/1000 | Loss: 0.00003614
Iteration 10/1000 | Loss: 0.00003593
Iteration 11/1000 | Loss: 0.00003575
Iteration 12/1000 | Loss: 0.00003567
Iteration 13/1000 | Loss: 0.00003566
Iteration 14/1000 | Loss: 0.00003565
Iteration 15/1000 | Loss: 0.00003565
Iteration 16/1000 | Loss: 0.00003565
Iteration 17/1000 | Loss: 0.00003564
Iteration 18/1000 | Loss: 0.00003564
Iteration 19/1000 | Loss: 0.00003563
Iteration 20/1000 | Loss: 0.00003563
Iteration 21/1000 | Loss: 0.00003562
Iteration 22/1000 | Loss: 0.00003560
Iteration 23/1000 | Loss: 0.00003559
Iteration 24/1000 | Loss: 0.00003559
Iteration 25/1000 | Loss: 0.00003558
Iteration 26/1000 | Loss: 0.00003558
Iteration 27/1000 | Loss: 0.00003557
Iteration 28/1000 | Loss: 0.00003556
Iteration 29/1000 | Loss: 0.00003555
Iteration 30/1000 | Loss: 0.00003554
Iteration 31/1000 | Loss: 0.00003554
Iteration 32/1000 | Loss: 0.00003551
Iteration 33/1000 | Loss: 0.00003549
Iteration 34/1000 | Loss: 0.00003547
Iteration 35/1000 | Loss: 0.00003547
Iteration 36/1000 | Loss: 0.00003546
Iteration 37/1000 | Loss: 0.00003546
Iteration 38/1000 | Loss: 0.00003545
Iteration 39/1000 | Loss: 0.00003544
Iteration 40/1000 | Loss: 0.00003544
Iteration 41/1000 | Loss: 0.00003543
Iteration 42/1000 | Loss: 0.00003543
Iteration 43/1000 | Loss: 0.00003543
Iteration 44/1000 | Loss: 0.00003542
Iteration 45/1000 | Loss: 0.00003542
Iteration 46/1000 | Loss: 0.00003541
Iteration 47/1000 | Loss: 0.00003541
Iteration 48/1000 | Loss: 0.00003540
Iteration 49/1000 | Loss: 0.00003540
Iteration 50/1000 | Loss: 0.00003540
Iteration 51/1000 | Loss: 0.00003539
Iteration 52/1000 | Loss: 0.00003539
Iteration 53/1000 | Loss: 0.00003539
Iteration 54/1000 | Loss: 0.00003538
Iteration 55/1000 | Loss: 0.00003538
Iteration 56/1000 | Loss: 0.00003538
Iteration 57/1000 | Loss: 0.00003538
Iteration 58/1000 | Loss: 0.00003535
Iteration 59/1000 | Loss: 0.00003535
Iteration 60/1000 | Loss: 0.00003534
Iteration 61/1000 | Loss: 0.00003534
Iteration 62/1000 | Loss: 0.00003534
Iteration 63/1000 | Loss: 0.00003533
Iteration 64/1000 | Loss: 0.00003532
Iteration 65/1000 | Loss: 0.00003531
Iteration 66/1000 | Loss: 0.00003531
Iteration 67/1000 | Loss: 0.00003531
Iteration 68/1000 | Loss: 0.00003530
Iteration 69/1000 | Loss: 0.00003530
Iteration 70/1000 | Loss: 0.00003530
Iteration 71/1000 | Loss: 0.00003529
Iteration 72/1000 | Loss: 0.00003529
Iteration 73/1000 | Loss: 0.00003528
Iteration 74/1000 | Loss: 0.00003528
Iteration 75/1000 | Loss: 0.00003528
Iteration 76/1000 | Loss: 0.00003528
Iteration 77/1000 | Loss: 0.00003528
Iteration 78/1000 | Loss: 0.00003528
Iteration 79/1000 | Loss: 0.00003528
Iteration 80/1000 | Loss: 0.00003528
Iteration 81/1000 | Loss: 0.00003528
Iteration 82/1000 | Loss: 0.00003528
Iteration 83/1000 | Loss: 0.00003528
Iteration 84/1000 | Loss: 0.00003528
Iteration 85/1000 | Loss: 0.00003528
Iteration 86/1000 | Loss: 0.00003527
Iteration 87/1000 | Loss: 0.00003527
Iteration 88/1000 | Loss: 0.00003527
Iteration 89/1000 | Loss: 0.00003527
Iteration 90/1000 | Loss: 0.00003527
Iteration 91/1000 | Loss: 0.00003527
Iteration 92/1000 | Loss: 0.00003527
Iteration 93/1000 | Loss: 0.00003527
Iteration 94/1000 | Loss: 0.00003527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.527452645357698e-05, 3.527452645357698e-05, 3.527452645357698e-05, 3.527452645357698e-05, 3.527452645357698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.527452645357698e-05

Optimization complete. Final v2v error: 5.160825729370117 mm

Highest mean error: 5.373195171356201 mm for frame 31

Lowest mean error: 4.923383712768555 mm for frame 0

Saving results

Total time: 35.433647871017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111494
Iteration 2/25 | Loss: 0.00380090
Iteration 3/25 | Loss: 0.00258486
Iteration 4/25 | Loss: 0.00235629
Iteration 5/25 | Loss: 0.00228863
Iteration 6/25 | Loss: 0.00228347
Iteration 7/25 | Loss: 0.00217917
Iteration 8/25 | Loss: 0.00210065
Iteration 9/25 | Loss: 0.00206423
Iteration 10/25 | Loss: 0.00199902
Iteration 11/25 | Loss: 0.00195589
Iteration 12/25 | Loss: 0.00193863
Iteration 13/25 | Loss: 0.00191724
Iteration 14/25 | Loss: 0.00191301
Iteration 15/25 | Loss: 0.00190220
Iteration 16/25 | Loss: 0.00189576
Iteration 17/25 | Loss: 0.00190048
Iteration 18/25 | Loss: 0.00189194
Iteration 19/25 | Loss: 0.00189791
Iteration 20/25 | Loss: 0.00188859
Iteration 21/25 | Loss: 0.00188204
Iteration 22/25 | Loss: 0.00188420
Iteration 23/25 | Loss: 0.00188021
Iteration 24/25 | Loss: 0.00188326
Iteration 25/25 | Loss: 0.00188045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51154876
Iteration 2/25 | Loss: 0.00586110
Iteration 3/25 | Loss: 0.00570003
Iteration 4/25 | Loss: 0.00570003
Iteration 5/25 | Loss: 0.00570003
Iteration 6/25 | Loss: 0.00570003
Iteration 7/25 | Loss: 0.00570003
Iteration 8/25 | Loss: 0.00570003
Iteration 9/25 | Loss: 0.00570003
Iteration 10/25 | Loss: 0.00570003
Iteration 11/25 | Loss: 0.00570003
Iteration 12/25 | Loss: 0.00570003
Iteration 13/25 | Loss: 0.00570003
Iteration 14/25 | Loss: 0.00570003
Iteration 15/25 | Loss: 0.00570003
Iteration 16/25 | Loss: 0.00570003
Iteration 17/25 | Loss: 0.00570003
Iteration 18/25 | Loss: 0.00570003
Iteration 19/25 | Loss: 0.00570003
Iteration 20/25 | Loss: 0.00570003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005700028035789728, 0.005700028035789728, 0.005700028035789728, 0.005700028035789728, 0.005700028035789728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005700028035789728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00570003
Iteration 2/1000 | Loss: 0.00127695
Iteration 3/1000 | Loss: 0.00209774
Iteration 4/1000 | Loss: 0.00084061
Iteration 5/1000 | Loss: 0.00207923
Iteration 6/1000 | Loss: 0.00093389
Iteration 7/1000 | Loss: 0.00196967
Iteration 8/1000 | Loss: 0.00045009
Iteration 9/1000 | Loss: 0.00209251
Iteration 10/1000 | Loss: 0.00421280
Iteration 11/1000 | Loss: 0.00187345
Iteration 12/1000 | Loss: 0.00060091
Iteration 13/1000 | Loss: 0.00081695
Iteration 14/1000 | Loss: 0.00072958
Iteration 15/1000 | Loss: 0.00156401
Iteration 16/1000 | Loss: 0.00286555
Iteration 17/1000 | Loss: 0.00055970
Iteration 18/1000 | Loss: 0.00160511
Iteration 19/1000 | Loss: 0.00256869
Iteration 20/1000 | Loss: 0.00108648
Iteration 21/1000 | Loss: 0.00101979
Iteration 22/1000 | Loss: 0.00083586
Iteration 23/1000 | Loss: 0.00051954
Iteration 24/1000 | Loss: 0.00207019
Iteration 25/1000 | Loss: 0.00425729
Iteration 26/1000 | Loss: 0.00618262
Iteration 27/1000 | Loss: 0.00463886
Iteration 28/1000 | Loss: 0.00355673
Iteration 29/1000 | Loss: 0.00400659
Iteration 30/1000 | Loss: 0.00093714
Iteration 31/1000 | Loss: 0.00170516
Iteration 32/1000 | Loss: 0.00247617
Iteration 33/1000 | Loss: 0.00107664
Iteration 34/1000 | Loss: 0.00113317
Iteration 35/1000 | Loss: 0.00179824
Iteration 36/1000 | Loss: 0.00618418
Iteration 37/1000 | Loss: 0.00118828
Iteration 38/1000 | Loss: 0.00125111
Iteration 39/1000 | Loss: 0.00032442
Iteration 40/1000 | Loss: 0.00036221
Iteration 41/1000 | Loss: 0.00047626
Iteration 42/1000 | Loss: 0.00153223
Iteration 43/1000 | Loss: 0.00357294
Iteration 44/1000 | Loss: 0.00061567
Iteration 45/1000 | Loss: 0.00025327
Iteration 46/1000 | Loss: 0.00035711
Iteration 47/1000 | Loss: 0.00088152
Iteration 48/1000 | Loss: 0.00281752
Iteration 49/1000 | Loss: 0.00036279
Iteration 50/1000 | Loss: 0.00150497
Iteration 51/1000 | Loss: 0.00107493
Iteration 52/1000 | Loss: 0.00185641
Iteration 53/1000 | Loss: 0.00045751
Iteration 54/1000 | Loss: 0.00097388
Iteration 55/1000 | Loss: 0.00060853
Iteration 56/1000 | Loss: 0.00213577
Iteration 57/1000 | Loss: 0.00122178
Iteration 58/1000 | Loss: 0.00092391
Iteration 59/1000 | Loss: 0.00096695
Iteration 60/1000 | Loss: 0.00079956
Iteration 61/1000 | Loss: 0.00146064
Iteration 62/1000 | Loss: 0.00128064
Iteration 63/1000 | Loss: 0.00117665
Iteration 64/1000 | Loss: 0.00160650
Iteration 65/1000 | Loss: 0.00032964
Iteration 66/1000 | Loss: 0.00200064
Iteration 67/1000 | Loss: 0.00099577
Iteration 68/1000 | Loss: 0.00085820
Iteration 69/1000 | Loss: 0.00130054
Iteration 70/1000 | Loss: 0.00104682
Iteration 71/1000 | Loss: 0.00062371
Iteration 72/1000 | Loss: 0.00087597
Iteration 73/1000 | Loss: 0.00044072
Iteration 74/1000 | Loss: 0.00150010
Iteration 75/1000 | Loss: 0.00157747
Iteration 76/1000 | Loss: 0.00148278
Iteration 77/1000 | Loss: 0.00071494
Iteration 78/1000 | Loss: 0.00099565
Iteration 79/1000 | Loss: 0.00045996
Iteration 80/1000 | Loss: 0.00031215
Iteration 81/1000 | Loss: 0.00123471
Iteration 82/1000 | Loss: 0.00042458
Iteration 83/1000 | Loss: 0.00152044
Iteration 84/1000 | Loss: 0.00184711
Iteration 85/1000 | Loss: 0.00127408
Iteration 86/1000 | Loss: 0.00170377
Iteration 87/1000 | Loss: 0.00172971
Iteration 88/1000 | Loss: 0.00084765
Iteration 89/1000 | Loss: 0.00064640
Iteration 90/1000 | Loss: 0.00100177
Iteration 91/1000 | Loss: 0.00079313
Iteration 92/1000 | Loss: 0.00133086
Iteration 93/1000 | Loss: 0.00114035
Iteration 94/1000 | Loss: 0.00119372
Iteration 95/1000 | Loss: 0.00201902
Iteration 96/1000 | Loss: 0.00170592
Iteration 97/1000 | Loss: 0.00099458
Iteration 98/1000 | Loss: 0.00092168
Iteration 99/1000 | Loss: 0.00105141
Iteration 100/1000 | Loss: 0.00103188
Iteration 101/1000 | Loss: 0.00083324
Iteration 102/1000 | Loss: 0.00113623
Iteration 103/1000 | Loss: 0.00081493
Iteration 104/1000 | Loss: 0.00074294
Iteration 105/1000 | Loss: 0.00061494
Iteration 106/1000 | Loss: 0.00081473
Iteration 107/1000 | Loss: 0.00031876
Iteration 108/1000 | Loss: 0.00088384
Iteration 109/1000 | Loss: 0.00037334
Iteration 110/1000 | Loss: 0.00044303
Iteration 111/1000 | Loss: 0.00039388
Iteration 112/1000 | Loss: 0.00081322
Iteration 113/1000 | Loss: 0.00050622
Iteration 114/1000 | Loss: 0.00039193
Iteration 115/1000 | Loss: 0.00039221
Iteration 116/1000 | Loss: 0.00026843
Iteration 117/1000 | Loss: 0.00020180
Iteration 118/1000 | Loss: 0.00025552
Iteration 119/1000 | Loss: 0.00024644
Iteration 120/1000 | Loss: 0.00020227
Iteration 121/1000 | Loss: 0.00023314
Iteration 122/1000 | Loss: 0.00024779
Iteration 123/1000 | Loss: 0.00193255
Iteration 124/1000 | Loss: 0.00173580
Iteration 125/1000 | Loss: 0.00047641
Iteration 126/1000 | Loss: 0.00054584
Iteration 127/1000 | Loss: 0.00055616
Iteration 128/1000 | Loss: 0.00070566
Iteration 129/1000 | Loss: 0.00060360
Iteration 130/1000 | Loss: 0.00057470
Iteration 131/1000 | Loss: 0.00031815
Iteration 132/1000 | Loss: 0.00028481
Iteration 133/1000 | Loss: 0.00031301
Iteration 134/1000 | Loss: 0.00025796
Iteration 135/1000 | Loss: 0.00024845
Iteration 136/1000 | Loss: 0.00024181
Iteration 137/1000 | Loss: 0.00022066
Iteration 138/1000 | Loss: 0.00077432
Iteration 139/1000 | Loss: 0.00132460
Iteration 140/1000 | Loss: 0.00034498
Iteration 141/1000 | Loss: 0.00024574
Iteration 142/1000 | Loss: 0.00021392
Iteration 143/1000 | Loss: 0.00065275
Iteration 144/1000 | Loss: 0.00089945
Iteration 145/1000 | Loss: 0.00051119
Iteration 146/1000 | Loss: 0.00051847
Iteration 147/1000 | Loss: 0.00046140
Iteration 148/1000 | Loss: 0.00060083
Iteration 149/1000 | Loss: 0.00032733
Iteration 150/1000 | Loss: 0.00029855
Iteration 151/1000 | Loss: 0.00033790
Iteration 152/1000 | Loss: 0.00076518
Iteration 153/1000 | Loss: 0.00056254
Iteration 154/1000 | Loss: 0.00063640
Iteration 155/1000 | Loss: 0.00013884
Iteration 156/1000 | Loss: 0.00015265
Iteration 157/1000 | Loss: 0.00013350
Iteration 158/1000 | Loss: 0.00017341
Iteration 159/1000 | Loss: 0.00072325
Iteration 160/1000 | Loss: 0.00016729
Iteration 161/1000 | Loss: 0.00017759
Iteration 162/1000 | Loss: 0.00016395
Iteration 163/1000 | Loss: 0.00026845
Iteration 164/1000 | Loss: 0.00144711
Iteration 165/1000 | Loss: 0.00026666
Iteration 166/1000 | Loss: 0.00103102
Iteration 167/1000 | Loss: 0.00014875
Iteration 168/1000 | Loss: 0.00013348
Iteration 169/1000 | Loss: 0.00011713
Iteration 170/1000 | Loss: 0.00097639
Iteration 171/1000 | Loss: 0.00045776
Iteration 172/1000 | Loss: 0.00014315
Iteration 173/1000 | Loss: 0.00011167
Iteration 174/1000 | Loss: 0.00092600
Iteration 175/1000 | Loss: 0.00028796
Iteration 176/1000 | Loss: 0.00012051
Iteration 177/1000 | Loss: 0.00010902
Iteration 178/1000 | Loss: 0.00012272
Iteration 179/1000 | Loss: 0.00096226
Iteration 180/1000 | Loss: 0.00020511
Iteration 181/1000 | Loss: 0.00011790
Iteration 182/1000 | Loss: 0.00012205
Iteration 183/1000 | Loss: 0.00011458
Iteration 184/1000 | Loss: 0.00012339
Iteration 185/1000 | Loss: 0.00011916
Iteration 186/1000 | Loss: 0.00011440
Iteration 187/1000 | Loss: 0.00012048
Iteration 188/1000 | Loss: 0.00084882
Iteration 189/1000 | Loss: 0.00022705
Iteration 190/1000 | Loss: 0.00013347
Iteration 191/1000 | Loss: 0.00012617
Iteration 192/1000 | Loss: 0.00011677
Iteration 193/1000 | Loss: 0.00014927
Iteration 194/1000 | Loss: 0.00011665
Iteration 195/1000 | Loss: 0.00011586
Iteration 196/1000 | Loss: 0.00011477
Iteration 197/1000 | Loss: 0.00011601
Iteration 198/1000 | Loss: 0.00011618
Iteration 199/1000 | Loss: 0.00011998
Iteration 200/1000 | Loss: 0.00011599
Iteration 201/1000 | Loss: 0.00078423
Iteration 202/1000 | Loss: 0.00097394
Iteration 203/1000 | Loss: 0.00020555
Iteration 204/1000 | Loss: 0.00075237
Iteration 205/1000 | Loss: 0.00047033
Iteration 206/1000 | Loss: 0.00016000
Iteration 207/1000 | Loss: 0.00065540
Iteration 208/1000 | Loss: 0.00026555
Iteration 209/1000 | Loss: 0.00012563
Iteration 210/1000 | Loss: 0.00011897
Iteration 211/1000 | Loss: 0.00011756
Iteration 212/1000 | Loss: 0.00011651
Iteration 213/1000 | Loss: 0.00011595
Iteration 214/1000 | Loss: 0.00011598
Iteration 215/1000 | Loss: 0.00011522
Iteration 216/1000 | Loss: 0.00011458
Iteration 217/1000 | Loss: 0.00011466
Iteration 218/1000 | Loss: 0.00011383
Iteration 219/1000 | Loss: 0.00011009
Iteration 220/1000 | Loss: 0.00011314
Iteration 221/1000 | Loss: 0.00074778
Iteration 222/1000 | Loss: 0.00023057
Iteration 223/1000 | Loss: 0.00045404
Iteration 224/1000 | Loss: 0.00073343
Iteration 225/1000 | Loss: 0.00020764
Iteration 226/1000 | Loss: 0.00039580
Iteration 227/1000 | Loss: 0.00063762
Iteration 228/1000 | Loss: 0.00033751
Iteration 229/1000 | Loss: 0.00011116
Iteration 230/1000 | Loss: 0.00010997
Iteration 231/1000 | Loss: 0.00010784
Iteration 232/1000 | Loss: 0.00010724
Iteration 233/1000 | Loss: 0.00010370
Iteration 234/1000 | Loss: 0.00010514
Iteration 235/1000 | Loss: 0.00010284
Iteration 236/1000 | Loss: 0.00010246
Iteration 237/1000 | Loss: 0.00010216
Iteration 238/1000 | Loss: 0.00010186
Iteration 239/1000 | Loss: 0.00010177
Iteration 240/1000 | Loss: 0.00010166
Iteration 241/1000 | Loss: 0.00010164
Iteration 242/1000 | Loss: 0.00010164
Iteration 243/1000 | Loss: 0.00010163
Iteration 244/1000 | Loss: 0.00010160
Iteration 245/1000 | Loss: 0.00010160
Iteration 246/1000 | Loss: 0.00010160
Iteration 247/1000 | Loss: 0.00010160
Iteration 248/1000 | Loss: 0.00010160
Iteration 249/1000 | Loss: 0.00010160
Iteration 250/1000 | Loss: 0.00010160
Iteration 251/1000 | Loss: 0.00010160
Iteration 252/1000 | Loss: 0.00010160
Iteration 253/1000 | Loss: 0.00010160
Iteration 254/1000 | Loss: 0.00010159
Iteration 255/1000 | Loss: 0.00010159
Iteration 256/1000 | Loss: 0.00010159
Iteration 257/1000 | Loss: 0.00010159
Iteration 258/1000 | Loss: 0.00010158
Iteration 259/1000 | Loss: 0.00010158
Iteration 260/1000 | Loss: 0.00010157
Iteration 261/1000 | Loss: 0.00010157
Iteration 262/1000 | Loss: 0.00010157
Iteration 263/1000 | Loss: 0.00010157
Iteration 264/1000 | Loss: 0.00010157
Iteration 265/1000 | Loss: 0.00010157
Iteration 266/1000 | Loss: 0.00010157
Iteration 267/1000 | Loss: 0.00010156
Iteration 268/1000 | Loss: 0.00010156
Iteration 269/1000 | Loss: 0.00010156
Iteration 270/1000 | Loss: 0.00010156
Iteration 271/1000 | Loss: 0.00010156
Iteration 272/1000 | Loss: 0.00010156
Iteration 273/1000 | Loss: 0.00010156
Iteration 274/1000 | Loss: 0.00010156
Iteration 275/1000 | Loss: 0.00010155
Iteration 276/1000 | Loss: 0.00010155
Iteration 277/1000 | Loss: 0.00010155
Iteration 278/1000 | Loss: 0.00010154
Iteration 279/1000 | Loss: 0.00010154
Iteration 280/1000 | Loss: 0.00010154
Iteration 281/1000 | Loss: 0.00010154
Iteration 282/1000 | Loss: 0.00010154
Iteration 283/1000 | Loss: 0.00010154
Iteration 284/1000 | Loss: 0.00010154
Iteration 285/1000 | Loss: 0.00010154
Iteration 286/1000 | Loss: 0.00010154
Iteration 287/1000 | Loss: 0.00010154
Iteration 288/1000 | Loss: 0.00010154
Iteration 289/1000 | Loss: 0.00010154
Iteration 290/1000 | Loss: 0.00010154
Iteration 291/1000 | Loss: 0.00010154
Iteration 292/1000 | Loss: 0.00010154
Iteration 293/1000 | Loss: 0.00010154
Iteration 294/1000 | Loss: 0.00010154
Iteration 295/1000 | Loss: 0.00010154
Iteration 296/1000 | Loss: 0.00010154
Iteration 297/1000 | Loss: 0.00010154
Iteration 298/1000 | Loss: 0.00010154
Iteration 299/1000 | Loss: 0.00010154
Iteration 300/1000 | Loss: 0.00010154
Iteration 301/1000 | Loss: 0.00010154
Iteration 302/1000 | Loss: 0.00010154
Iteration 303/1000 | Loss: 0.00010154
Iteration 304/1000 | Loss: 0.00010153
Iteration 305/1000 | Loss: 0.00010153
Iteration 306/1000 | Loss: 0.00010153
Iteration 307/1000 | Loss: 0.00010153
Iteration 308/1000 | Loss: 0.00010153
Iteration 309/1000 | Loss: 0.00010153
Iteration 310/1000 | Loss: 0.00010153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 310. Stopping optimization.
Last 5 losses: [0.00010153483890462667, 0.00010153483890462667, 0.00010153483890462667, 0.00010153483890462667, 0.00010153483890462667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010153483890462667

Optimization complete. Final v2v error: 5.843351364135742 mm

Highest mean error: 15.606958389282227 mm for frame 126

Lowest mean error: 4.414270401000977 mm for frame 113

Saving results

Total time: 436.59020352363586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595446
Iteration 2/25 | Loss: 0.00180005
Iteration 3/25 | Loss: 0.00155804
Iteration 4/25 | Loss: 0.00153467
Iteration 5/25 | Loss: 0.00152980
Iteration 6/25 | Loss: 0.00152870
Iteration 7/25 | Loss: 0.00152870
Iteration 8/25 | Loss: 0.00152870
Iteration 9/25 | Loss: 0.00152870
Iteration 10/25 | Loss: 0.00152870
Iteration 11/25 | Loss: 0.00152870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001528700697235763, 0.001528700697235763, 0.001528700697235763, 0.001528700697235763, 0.001528700697235763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001528700697235763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83417344
Iteration 2/25 | Loss: 0.00189086
Iteration 3/25 | Loss: 0.00189086
Iteration 4/25 | Loss: 0.00189086
Iteration 5/25 | Loss: 0.00189086
Iteration 6/25 | Loss: 0.00189086
Iteration 7/25 | Loss: 0.00189086
Iteration 8/25 | Loss: 0.00189086
Iteration 9/25 | Loss: 0.00189086
Iteration 10/25 | Loss: 0.00189086
Iteration 11/25 | Loss: 0.00189085
Iteration 12/25 | Loss: 0.00189085
Iteration 13/25 | Loss: 0.00189086
Iteration 14/25 | Loss: 0.00189086
Iteration 15/25 | Loss: 0.00189086
Iteration 16/25 | Loss: 0.00189086
Iteration 17/25 | Loss: 0.00189086
Iteration 18/25 | Loss: 0.00189086
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018908552592620254, 0.0018908552592620254, 0.0018908552592620254, 0.0018908552592620254, 0.0018908552592620254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018908552592620254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189086
Iteration 2/1000 | Loss: 0.00005236
Iteration 3/1000 | Loss: 0.00003389
Iteration 4/1000 | Loss: 0.00002986
Iteration 5/1000 | Loss: 0.00002761
Iteration 6/1000 | Loss: 0.00002669
Iteration 7/1000 | Loss: 0.00002607
Iteration 8/1000 | Loss: 0.00002572
Iteration 9/1000 | Loss: 0.00002537
Iteration 10/1000 | Loss: 0.00002500
Iteration 11/1000 | Loss: 0.00002482
Iteration 12/1000 | Loss: 0.00002479
Iteration 13/1000 | Loss: 0.00002466
Iteration 14/1000 | Loss: 0.00002466
Iteration 15/1000 | Loss: 0.00002466
Iteration 16/1000 | Loss: 0.00002465
Iteration 17/1000 | Loss: 0.00002460
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002460
Iteration 20/1000 | Loss: 0.00002459
Iteration 21/1000 | Loss: 0.00002459
Iteration 22/1000 | Loss: 0.00002459
Iteration 23/1000 | Loss: 0.00002459
Iteration 24/1000 | Loss: 0.00002459
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002458
Iteration 27/1000 | Loss: 0.00002458
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002454
Iteration 34/1000 | Loss: 0.00002454
Iteration 35/1000 | Loss: 0.00002454
Iteration 36/1000 | Loss: 0.00002454
Iteration 37/1000 | Loss: 0.00002454
Iteration 38/1000 | Loss: 0.00002452
Iteration 39/1000 | Loss: 0.00002448
Iteration 40/1000 | Loss: 0.00002445
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002444
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002442
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002439
Iteration 47/1000 | Loss: 0.00002439
Iteration 48/1000 | Loss: 0.00002439
Iteration 49/1000 | Loss: 0.00002439
Iteration 50/1000 | Loss: 0.00002439
Iteration 51/1000 | Loss: 0.00002438
Iteration 52/1000 | Loss: 0.00002438
Iteration 53/1000 | Loss: 0.00002438
Iteration 54/1000 | Loss: 0.00002438
Iteration 55/1000 | Loss: 0.00002437
Iteration 56/1000 | Loss: 0.00002437
Iteration 57/1000 | Loss: 0.00002435
Iteration 58/1000 | Loss: 0.00002434
Iteration 59/1000 | Loss: 0.00002434
Iteration 60/1000 | Loss: 0.00002433
Iteration 61/1000 | Loss: 0.00002433
Iteration 62/1000 | Loss: 0.00002433
Iteration 63/1000 | Loss: 0.00002431
Iteration 64/1000 | Loss: 0.00002431
Iteration 65/1000 | Loss: 0.00002431
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002429
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002429
Iteration 72/1000 | Loss: 0.00002429
Iteration 73/1000 | Loss: 0.00002429
Iteration 74/1000 | Loss: 0.00002429
Iteration 75/1000 | Loss: 0.00002429
Iteration 76/1000 | Loss: 0.00002428
Iteration 77/1000 | Loss: 0.00002428
Iteration 78/1000 | Loss: 0.00002428
Iteration 79/1000 | Loss: 0.00002428
Iteration 80/1000 | Loss: 0.00002428
Iteration 81/1000 | Loss: 0.00002428
Iteration 82/1000 | Loss: 0.00002428
Iteration 83/1000 | Loss: 0.00002428
Iteration 84/1000 | Loss: 0.00002427
Iteration 85/1000 | Loss: 0.00002427
Iteration 86/1000 | Loss: 0.00002427
Iteration 87/1000 | Loss: 0.00002427
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002427
Iteration 91/1000 | Loss: 0.00002427
Iteration 92/1000 | Loss: 0.00002427
Iteration 93/1000 | Loss: 0.00002427
Iteration 94/1000 | Loss: 0.00002427
Iteration 95/1000 | Loss: 0.00002426
Iteration 96/1000 | Loss: 0.00002426
Iteration 97/1000 | Loss: 0.00002426
Iteration 98/1000 | Loss: 0.00002426
Iteration 99/1000 | Loss: 0.00002426
Iteration 100/1000 | Loss: 0.00002426
Iteration 101/1000 | Loss: 0.00002426
Iteration 102/1000 | Loss: 0.00002426
Iteration 103/1000 | Loss: 0.00002426
Iteration 104/1000 | Loss: 0.00002426
Iteration 105/1000 | Loss: 0.00002426
Iteration 106/1000 | Loss: 0.00002425
Iteration 107/1000 | Loss: 0.00002425
Iteration 108/1000 | Loss: 0.00002425
Iteration 109/1000 | Loss: 0.00002425
Iteration 110/1000 | Loss: 0.00002425
Iteration 111/1000 | Loss: 0.00002425
Iteration 112/1000 | Loss: 0.00002425
Iteration 113/1000 | Loss: 0.00002425
Iteration 114/1000 | Loss: 0.00002425
Iteration 115/1000 | Loss: 0.00002425
Iteration 116/1000 | Loss: 0.00002425
Iteration 117/1000 | Loss: 0.00002425
Iteration 118/1000 | Loss: 0.00002425
Iteration 119/1000 | Loss: 0.00002424
Iteration 120/1000 | Loss: 0.00002424
Iteration 121/1000 | Loss: 0.00002424
Iteration 122/1000 | Loss: 0.00002424
Iteration 123/1000 | Loss: 0.00002424
Iteration 124/1000 | Loss: 0.00002424
Iteration 125/1000 | Loss: 0.00002424
Iteration 126/1000 | Loss: 0.00002424
Iteration 127/1000 | Loss: 0.00002424
Iteration 128/1000 | Loss: 0.00002424
Iteration 129/1000 | Loss: 0.00002424
Iteration 130/1000 | Loss: 0.00002424
Iteration 131/1000 | Loss: 0.00002424
Iteration 132/1000 | Loss: 0.00002424
Iteration 133/1000 | Loss: 0.00002424
Iteration 134/1000 | Loss: 0.00002424
Iteration 135/1000 | Loss: 0.00002423
Iteration 136/1000 | Loss: 0.00002423
Iteration 137/1000 | Loss: 0.00002423
Iteration 138/1000 | Loss: 0.00002423
Iteration 139/1000 | Loss: 0.00002423
Iteration 140/1000 | Loss: 0.00002423
Iteration 141/1000 | Loss: 0.00002423
Iteration 142/1000 | Loss: 0.00002423
Iteration 143/1000 | Loss: 0.00002423
Iteration 144/1000 | Loss: 0.00002423
Iteration 145/1000 | Loss: 0.00002423
Iteration 146/1000 | Loss: 0.00002422
Iteration 147/1000 | Loss: 0.00002422
Iteration 148/1000 | Loss: 0.00002422
Iteration 149/1000 | Loss: 0.00002422
Iteration 150/1000 | Loss: 0.00002422
Iteration 151/1000 | Loss: 0.00002422
Iteration 152/1000 | Loss: 0.00002422
Iteration 153/1000 | Loss: 0.00002422
Iteration 154/1000 | Loss: 0.00002422
Iteration 155/1000 | Loss: 0.00002422
Iteration 156/1000 | Loss: 0.00002422
Iteration 157/1000 | Loss: 0.00002422
Iteration 158/1000 | Loss: 0.00002422
Iteration 159/1000 | Loss: 0.00002422
Iteration 160/1000 | Loss: 0.00002422
Iteration 161/1000 | Loss: 0.00002422
Iteration 162/1000 | Loss: 0.00002422
Iteration 163/1000 | Loss: 0.00002422
Iteration 164/1000 | Loss: 0.00002422
Iteration 165/1000 | Loss: 0.00002422
Iteration 166/1000 | Loss: 0.00002422
Iteration 167/1000 | Loss: 0.00002422
Iteration 168/1000 | Loss: 0.00002422
Iteration 169/1000 | Loss: 0.00002422
Iteration 170/1000 | Loss: 0.00002422
Iteration 171/1000 | Loss: 0.00002422
Iteration 172/1000 | Loss: 0.00002422
Iteration 173/1000 | Loss: 0.00002422
Iteration 174/1000 | Loss: 0.00002422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.4221702915383503e-05, 2.4221702915383503e-05, 2.4221702915383503e-05, 2.4221702915383503e-05, 2.4221702915383503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4221702915383503e-05

Optimization complete. Final v2v error: 4.312664031982422 mm

Highest mean error: 4.807065010070801 mm for frame 223

Lowest mean error: 4.196159362792969 mm for frame 60

Saving results

Total time: 42.972132444381714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460401
Iteration 2/25 | Loss: 0.00166897
Iteration 3/25 | Loss: 0.00156804
Iteration 4/25 | Loss: 0.00155771
Iteration 5/25 | Loss: 0.00155447
Iteration 6/25 | Loss: 0.00155414
Iteration 7/25 | Loss: 0.00155414
Iteration 8/25 | Loss: 0.00155414
Iteration 9/25 | Loss: 0.00155414
Iteration 10/25 | Loss: 0.00155414
Iteration 11/25 | Loss: 0.00155414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015541387256234884, 0.0015541387256234884, 0.0015541387256234884, 0.0015541387256234884, 0.0015541387256234884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015541387256234884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48549592
Iteration 2/25 | Loss: 0.00205282
Iteration 3/25 | Loss: 0.00205282
Iteration 4/25 | Loss: 0.00205282
Iteration 5/25 | Loss: 0.00205282
Iteration 6/25 | Loss: 0.00205282
Iteration 7/25 | Loss: 0.00205282
Iteration 8/25 | Loss: 0.00205282
Iteration 9/25 | Loss: 0.00205282
Iteration 10/25 | Loss: 0.00205282
Iteration 11/25 | Loss: 0.00205282
Iteration 12/25 | Loss: 0.00205282
Iteration 13/25 | Loss: 0.00205282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0020528172608464956, 0.0020528172608464956, 0.0020528172608464956, 0.0020528172608464956, 0.0020528172608464956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020528172608464956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205282
Iteration 2/1000 | Loss: 0.00005603
Iteration 3/1000 | Loss: 0.00004044
Iteration 4/1000 | Loss: 0.00003636
Iteration 5/1000 | Loss: 0.00003442
Iteration 6/1000 | Loss: 0.00003312
Iteration 7/1000 | Loss: 0.00003242
Iteration 8/1000 | Loss: 0.00003203
Iteration 9/1000 | Loss: 0.00003176
Iteration 10/1000 | Loss: 0.00003160
Iteration 11/1000 | Loss: 0.00003153
Iteration 12/1000 | Loss: 0.00003152
Iteration 13/1000 | Loss: 0.00003152
Iteration 14/1000 | Loss: 0.00003150
Iteration 15/1000 | Loss: 0.00003148
Iteration 16/1000 | Loss: 0.00003147
Iteration 17/1000 | Loss: 0.00003142
Iteration 18/1000 | Loss: 0.00003141
Iteration 19/1000 | Loss: 0.00003141
Iteration 20/1000 | Loss: 0.00003141
Iteration 21/1000 | Loss: 0.00003140
Iteration 22/1000 | Loss: 0.00003137
Iteration 23/1000 | Loss: 0.00003137
Iteration 24/1000 | Loss: 0.00003137
Iteration 25/1000 | Loss: 0.00003137
Iteration 26/1000 | Loss: 0.00003137
Iteration 27/1000 | Loss: 0.00003137
Iteration 28/1000 | Loss: 0.00003137
Iteration 29/1000 | Loss: 0.00003137
Iteration 30/1000 | Loss: 0.00003137
Iteration 31/1000 | Loss: 0.00003136
Iteration 32/1000 | Loss: 0.00003136
Iteration 33/1000 | Loss: 0.00003133
Iteration 34/1000 | Loss: 0.00003133
Iteration 35/1000 | Loss: 0.00003133
Iteration 36/1000 | Loss: 0.00003133
Iteration 37/1000 | Loss: 0.00003133
Iteration 38/1000 | Loss: 0.00003133
Iteration 39/1000 | Loss: 0.00003133
Iteration 40/1000 | Loss: 0.00003133
Iteration 41/1000 | Loss: 0.00003133
Iteration 42/1000 | Loss: 0.00003132
Iteration 43/1000 | Loss: 0.00003132
Iteration 44/1000 | Loss: 0.00003132
Iteration 45/1000 | Loss: 0.00003132
Iteration 46/1000 | Loss: 0.00003132
Iteration 47/1000 | Loss: 0.00003131
Iteration 48/1000 | Loss: 0.00003131
Iteration 49/1000 | Loss: 0.00003131
Iteration 50/1000 | Loss: 0.00003130
Iteration 51/1000 | Loss: 0.00003130
Iteration 52/1000 | Loss: 0.00003130
Iteration 53/1000 | Loss: 0.00003130
Iteration 54/1000 | Loss: 0.00003129
Iteration 55/1000 | Loss: 0.00003129
Iteration 56/1000 | Loss: 0.00003129
Iteration 57/1000 | Loss: 0.00003129
Iteration 58/1000 | Loss: 0.00003128
Iteration 59/1000 | Loss: 0.00003128
Iteration 60/1000 | Loss: 0.00003128
Iteration 61/1000 | Loss: 0.00003128
Iteration 62/1000 | Loss: 0.00003127
Iteration 63/1000 | Loss: 0.00003127
Iteration 64/1000 | Loss: 0.00003127
Iteration 65/1000 | Loss: 0.00003127
Iteration 66/1000 | Loss: 0.00003126
Iteration 67/1000 | Loss: 0.00003126
Iteration 68/1000 | Loss: 0.00003126
Iteration 69/1000 | Loss: 0.00003125
Iteration 70/1000 | Loss: 0.00003125
Iteration 71/1000 | Loss: 0.00003125
Iteration 72/1000 | Loss: 0.00003125
Iteration 73/1000 | Loss: 0.00003125
Iteration 74/1000 | Loss: 0.00003125
Iteration 75/1000 | Loss: 0.00003125
Iteration 76/1000 | Loss: 0.00003125
Iteration 77/1000 | Loss: 0.00003124
Iteration 78/1000 | Loss: 0.00003124
Iteration 79/1000 | Loss: 0.00003124
Iteration 80/1000 | Loss: 0.00003124
Iteration 81/1000 | Loss: 0.00003124
Iteration 82/1000 | Loss: 0.00003123
Iteration 83/1000 | Loss: 0.00003123
Iteration 84/1000 | Loss: 0.00003123
Iteration 85/1000 | Loss: 0.00003122
Iteration 86/1000 | Loss: 0.00003122
Iteration 87/1000 | Loss: 0.00003122
Iteration 88/1000 | Loss: 0.00003122
Iteration 89/1000 | Loss: 0.00003122
Iteration 90/1000 | Loss: 0.00003122
Iteration 91/1000 | Loss: 0.00003122
Iteration 92/1000 | Loss: 0.00003122
Iteration 93/1000 | Loss: 0.00003122
Iteration 94/1000 | Loss: 0.00003122
Iteration 95/1000 | Loss: 0.00003122
Iteration 96/1000 | Loss: 0.00003122
Iteration 97/1000 | Loss: 0.00003122
Iteration 98/1000 | Loss: 0.00003122
Iteration 99/1000 | Loss: 0.00003122
Iteration 100/1000 | Loss: 0.00003122
Iteration 101/1000 | Loss: 0.00003121
Iteration 102/1000 | Loss: 0.00003121
Iteration 103/1000 | Loss: 0.00003121
Iteration 104/1000 | Loss: 0.00003121
Iteration 105/1000 | Loss: 0.00003121
Iteration 106/1000 | Loss: 0.00003121
Iteration 107/1000 | Loss: 0.00003121
Iteration 108/1000 | Loss: 0.00003121
Iteration 109/1000 | Loss: 0.00003121
Iteration 110/1000 | Loss: 0.00003121
Iteration 111/1000 | Loss: 0.00003121
Iteration 112/1000 | Loss: 0.00003121
Iteration 113/1000 | Loss: 0.00003121
Iteration 114/1000 | Loss: 0.00003121
Iteration 115/1000 | Loss: 0.00003121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.121022746199742e-05, 3.121022746199742e-05, 3.121022746199742e-05, 3.121022746199742e-05, 3.121022746199742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.121022746199742e-05

Optimization complete. Final v2v error: 4.877449989318848 mm

Highest mean error: 5.256776809692383 mm for frame 201

Lowest mean error: 4.520440578460693 mm for frame 247

Saving results

Total time: 35.70808529853821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466486
Iteration 2/25 | Loss: 0.00172570
Iteration 3/25 | Loss: 0.00162157
Iteration 4/25 | Loss: 0.00160496
Iteration 5/25 | Loss: 0.00159520
Iteration 6/25 | Loss: 0.00159254
Iteration 7/25 | Loss: 0.00159254
Iteration 8/25 | Loss: 0.00159254
Iteration 9/25 | Loss: 0.00159254
Iteration 10/25 | Loss: 0.00159254
Iteration 11/25 | Loss: 0.00159254
Iteration 12/25 | Loss: 0.00159254
Iteration 13/25 | Loss: 0.00159254
Iteration 14/25 | Loss: 0.00159254
Iteration 15/25 | Loss: 0.00159254
Iteration 16/25 | Loss: 0.00159254
Iteration 17/25 | Loss: 0.00159254
Iteration 18/25 | Loss: 0.00159254
Iteration 19/25 | Loss: 0.00159254
Iteration 20/25 | Loss: 0.00159254
Iteration 21/25 | Loss: 0.00159254
Iteration 22/25 | Loss: 0.00159254
Iteration 23/25 | Loss: 0.00159254
Iteration 24/25 | Loss: 0.00159254
Iteration 25/25 | Loss: 0.00159254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79032981
Iteration 2/25 | Loss: 0.00225969
Iteration 3/25 | Loss: 0.00225969
Iteration 4/25 | Loss: 0.00225969
Iteration 5/25 | Loss: 0.00225969
Iteration 6/25 | Loss: 0.00225969
Iteration 7/25 | Loss: 0.00225969
Iteration 8/25 | Loss: 0.00225969
Iteration 9/25 | Loss: 0.00225969
Iteration 10/25 | Loss: 0.00225969
Iteration 11/25 | Loss: 0.00225969
Iteration 12/25 | Loss: 0.00225969
Iteration 13/25 | Loss: 0.00225969
Iteration 14/25 | Loss: 0.00225969
Iteration 15/25 | Loss: 0.00225969
Iteration 16/25 | Loss: 0.00225969
Iteration 17/25 | Loss: 0.00225969
Iteration 18/25 | Loss: 0.00225969
Iteration 19/25 | Loss: 0.00225969
Iteration 20/25 | Loss: 0.00225969
Iteration 21/25 | Loss: 0.00225969
Iteration 22/25 | Loss: 0.00225969
Iteration 23/25 | Loss: 0.00225969
Iteration 24/25 | Loss: 0.00225969
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0022596882190555334, 0.0022596882190555334, 0.0022596882190555334, 0.0022596882190555334, 0.0022596882190555334]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022596882190555334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225969
Iteration 2/1000 | Loss: 0.00005569
Iteration 3/1000 | Loss: 0.00004190
Iteration 4/1000 | Loss: 0.00003674
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00003264
Iteration 7/1000 | Loss: 0.00003160
Iteration 8/1000 | Loss: 0.00003094
Iteration 9/1000 | Loss: 0.00003053
Iteration 10/1000 | Loss: 0.00003017
Iteration 11/1000 | Loss: 0.00002984
Iteration 12/1000 | Loss: 0.00002969
Iteration 13/1000 | Loss: 0.00002964
Iteration 14/1000 | Loss: 0.00002945
Iteration 15/1000 | Loss: 0.00002943
Iteration 16/1000 | Loss: 0.00002933
Iteration 17/1000 | Loss: 0.00002929
Iteration 18/1000 | Loss: 0.00002929
Iteration 19/1000 | Loss: 0.00002928
Iteration 20/1000 | Loss: 0.00002928
Iteration 21/1000 | Loss: 0.00002925
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002917
Iteration 27/1000 | Loss: 0.00002917
Iteration 28/1000 | Loss: 0.00002917
Iteration 29/1000 | Loss: 0.00002916
Iteration 30/1000 | Loss: 0.00002916
Iteration 31/1000 | Loss: 0.00002915
Iteration 32/1000 | Loss: 0.00002914
Iteration 33/1000 | Loss: 0.00002913
Iteration 34/1000 | Loss: 0.00002913
Iteration 35/1000 | Loss: 0.00002913
Iteration 36/1000 | Loss: 0.00002912
Iteration 37/1000 | Loss: 0.00002912
Iteration 38/1000 | Loss: 0.00002912
Iteration 39/1000 | Loss: 0.00002912
Iteration 40/1000 | Loss: 0.00002912
Iteration 41/1000 | Loss: 0.00002911
Iteration 42/1000 | Loss: 0.00002909
Iteration 43/1000 | Loss: 0.00002909
Iteration 44/1000 | Loss: 0.00002909
Iteration 45/1000 | Loss: 0.00002908
Iteration 46/1000 | Loss: 0.00002908
Iteration 47/1000 | Loss: 0.00002907
Iteration 48/1000 | Loss: 0.00002907
Iteration 49/1000 | Loss: 0.00002907
Iteration 50/1000 | Loss: 0.00002906
Iteration 51/1000 | Loss: 0.00002906
Iteration 52/1000 | Loss: 0.00002906
Iteration 53/1000 | Loss: 0.00002906
Iteration 54/1000 | Loss: 0.00002906
Iteration 55/1000 | Loss: 0.00002906
Iteration 56/1000 | Loss: 0.00002906
Iteration 57/1000 | Loss: 0.00002906
Iteration 58/1000 | Loss: 0.00002906
Iteration 59/1000 | Loss: 0.00002905
Iteration 60/1000 | Loss: 0.00002905
Iteration 61/1000 | Loss: 0.00002905
Iteration 62/1000 | Loss: 0.00002905
Iteration 63/1000 | Loss: 0.00002905
Iteration 64/1000 | Loss: 0.00002905
Iteration 65/1000 | Loss: 0.00002905
Iteration 66/1000 | Loss: 0.00002905
Iteration 67/1000 | Loss: 0.00002904
Iteration 68/1000 | Loss: 0.00002904
Iteration 69/1000 | Loss: 0.00002904
Iteration 70/1000 | Loss: 0.00002903
Iteration 71/1000 | Loss: 0.00002903
Iteration 72/1000 | Loss: 0.00002903
Iteration 73/1000 | Loss: 0.00002902
Iteration 74/1000 | Loss: 0.00002902
Iteration 75/1000 | Loss: 0.00002902
Iteration 76/1000 | Loss: 0.00002902
Iteration 77/1000 | Loss: 0.00002902
Iteration 78/1000 | Loss: 0.00002902
Iteration 79/1000 | Loss: 0.00002901
Iteration 80/1000 | Loss: 0.00002901
Iteration 81/1000 | Loss: 0.00002901
Iteration 82/1000 | Loss: 0.00002901
Iteration 83/1000 | Loss: 0.00002901
Iteration 84/1000 | Loss: 0.00002901
Iteration 85/1000 | Loss: 0.00002900
Iteration 86/1000 | Loss: 0.00002900
Iteration 87/1000 | Loss: 0.00002900
Iteration 88/1000 | Loss: 0.00002900
Iteration 89/1000 | Loss: 0.00002900
Iteration 90/1000 | Loss: 0.00002900
Iteration 91/1000 | Loss: 0.00002900
Iteration 92/1000 | Loss: 0.00002900
Iteration 93/1000 | Loss: 0.00002900
Iteration 94/1000 | Loss: 0.00002900
Iteration 95/1000 | Loss: 0.00002900
Iteration 96/1000 | Loss: 0.00002900
Iteration 97/1000 | Loss: 0.00002900
Iteration 98/1000 | Loss: 0.00002899
Iteration 99/1000 | Loss: 0.00002899
Iteration 100/1000 | Loss: 0.00002899
Iteration 101/1000 | Loss: 0.00002899
Iteration 102/1000 | Loss: 0.00002899
Iteration 103/1000 | Loss: 0.00002899
Iteration 104/1000 | Loss: 0.00002899
Iteration 105/1000 | Loss: 0.00002899
Iteration 106/1000 | Loss: 0.00002899
Iteration 107/1000 | Loss: 0.00002899
Iteration 108/1000 | Loss: 0.00002899
Iteration 109/1000 | Loss: 0.00002899
Iteration 110/1000 | Loss: 0.00002899
Iteration 111/1000 | Loss: 0.00002898
Iteration 112/1000 | Loss: 0.00002898
Iteration 113/1000 | Loss: 0.00002898
Iteration 114/1000 | Loss: 0.00002898
Iteration 115/1000 | Loss: 0.00002898
Iteration 116/1000 | Loss: 0.00002898
Iteration 117/1000 | Loss: 0.00002898
Iteration 118/1000 | Loss: 0.00002897
Iteration 119/1000 | Loss: 0.00002897
Iteration 120/1000 | Loss: 0.00002897
Iteration 121/1000 | Loss: 0.00002897
Iteration 122/1000 | Loss: 0.00002897
Iteration 123/1000 | Loss: 0.00002897
Iteration 124/1000 | Loss: 0.00002897
Iteration 125/1000 | Loss: 0.00002897
Iteration 126/1000 | Loss: 0.00002897
Iteration 127/1000 | Loss: 0.00002897
Iteration 128/1000 | Loss: 0.00002897
Iteration 129/1000 | Loss: 0.00002897
Iteration 130/1000 | Loss: 0.00002897
Iteration 131/1000 | Loss: 0.00002897
Iteration 132/1000 | Loss: 0.00002897
Iteration 133/1000 | Loss: 0.00002897
Iteration 134/1000 | Loss: 0.00002897
Iteration 135/1000 | Loss: 0.00002897
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002897
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002897
Iteration 142/1000 | Loss: 0.00002897
Iteration 143/1000 | Loss: 0.00002897
Iteration 144/1000 | Loss: 0.00002897
Iteration 145/1000 | Loss: 0.00002897
Iteration 146/1000 | Loss: 0.00002897
Iteration 147/1000 | Loss: 0.00002897
Iteration 148/1000 | Loss: 0.00002897
Iteration 149/1000 | Loss: 0.00002897
Iteration 150/1000 | Loss: 0.00002897
Iteration 151/1000 | Loss: 0.00002897
Iteration 152/1000 | Loss: 0.00002897
Iteration 153/1000 | Loss: 0.00002897
Iteration 154/1000 | Loss: 0.00002897
Iteration 155/1000 | Loss: 0.00002897
Iteration 156/1000 | Loss: 0.00002897
Iteration 157/1000 | Loss: 0.00002897
Iteration 158/1000 | Loss: 0.00002897
Iteration 159/1000 | Loss: 0.00002897
Iteration 160/1000 | Loss: 0.00002897
Iteration 161/1000 | Loss: 0.00002897
Iteration 162/1000 | Loss: 0.00002897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.8968899641768076e-05, 2.8968899641768076e-05, 2.8968899641768076e-05, 2.8968899641768076e-05, 2.8968899641768076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8968899641768076e-05

Optimization complete. Final v2v error: 4.708616733551025 mm

Highest mean error: 5.011420249938965 mm for frame 59

Lowest mean error: 4.251495361328125 mm for frame 9

Saving results

Total time: 44.89873266220093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926412
Iteration 2/25 | Loss: 0.00182268
Iteration 3/25 | Loss: 0.00166786
Iteration 4/25 | Loss: 0.00163519
Iteration 5/25 | Loss: 0.00162367
Iteration 6/25 | Loss: 0.00162059
Iteration 7/25 | Loss: 0.00161880
Iteration 8/25 | Loss: 0.00161856
Iteration 9/25 | Loss: 0.00161856
Iteration 10/25 | Loss: 0.00161856
Iteration 11/25 | Loss: 0.00161856
Iteration 12/25 | Loss: 0.00161856
Iteration 13/25 | Loss: 0.00161856
Iteration 14/25 | Loss: 0.00161856
Iteration 15/25 | Loss: 0.00161856
Iteration 16/25 | Loss: 0.00161856
Iteration 17/25 | Loss: 0.00161856
Iteration 18/25 | Loss: 0.00161856
Iteration 19/25 | Loss: 0.00161856
Iteration 20/25 | Loss: 0.00161856
Iteration 21/25 | Loss: 0.00161856
Iteration 22/25 | Loss: 0.00161856
Iteration 23/25 | Loss: 0.00161856
Iteration 24/25 | Loss: 0.00161856
Iteration 25/25 | Loss: 0.00161856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45451307
Iteration 2/25 | Loss: 0.00266545
Iteration 3/25 | Loss: 0.00266544
Iteration 4/25 | Loss: 0.00266544
Iteration 5/25 | Loss: 0.00266544
Iteration 6/25 | Loss: 0.00266544
Iteration 7/25 | Loss: 0.00266544
Iteration 8/25 | Loss: 0.00266544
Iteration 9/25 | Loss: 0.00266544
Iteration 10/25 | Loss: 0.00266544
Iteration 11/25 | Loss: 0.00266544
Iteration 12/25 | Loss: 0.00266544
Iteration 13/25 | Loss: 0.00266544
Iteration 14/25 | Loss: 0.00266544
Iteration 15/25 | Loss: 0.00266544
Iteration 16/25 | Loss: 0.00266544
Iteration 17/25 | Loss: 0.00266544
Iteration 18/25 | Loss: 0.00266544
Iteration 19/25 | Loss: 0.00266544
Iteration 20/25 | Loss: 0.00266544
Iteration 21/25 | Loss: 0.00266544
Iteration 22/25 | Loss: 0.00266544
Iteration 23/25 | Loss: 0.00266544
Iteration 24/25 | Loss: 0.00266544
Iteration 25/25 | Loss: 0.00266544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266544
Iteration 2/1000 | Loss: 0.00006506
Iteration 3/1000 | Loss: 0.00004680
Iteration 4/1000 | Loss: 0.00004115
Iteration 5/1000 | Loss: 0.00003760
Iteration 6/1000 | Loss: 0.00003565
Iteration 7/1000 | Loss: 0.00003412
Iteration 8/1000 | Loss: 0.00003323
Iteration 9/1000 | Loss: 0.00003270
Iteration 10/1000 | Loss: 0.00003235
Iteration 11/1000 | Loss: 0.00003193
Iteration 12/1000 | Loss: 0.00003156
Iteration 13/1000 | Loss: 0.00003134
Iteration 14/1000 | Loss: 0.00003131
Iteration 15/1000 | Loss: 0.00003129
Iteration 16/1000 | Loss: 0.00003128
Iteration 17/1000 | Loss: 0.00003126
Iteration 18/1000 | Loss: 0.00003123
Iteration 19/1000 | Loss: 0.00003109
Iteration 20/1000 | Loss: 0.00003105
Iteration 21/1000 | Loss: 0.00003104
Iteration 22/1000 | Loss: 0.00003103
Iteration 23/1000 | Loss: 0.00003102
Iteration 24/1000 | Loss: 0.00003101
Iteration 25/1000 | Loss: 0.00003100
Iteration 26/1000 | Loss: 0.00003099
Iteration 27/1000 | Loss: 0.00003099
Iteration 28/1000 | Loss: 0.00003098
Iteration 29/1000 | Loss: 0.00003097
Iteration 30/1000 | Loss: 0.00003096
Iteration 31/1000 | Loss: 0.00003095
Iteration 32/1000 | Loss: 0.00003094
Iteration 33/1000 | Loss: 0.00003094
Iteration 34/1000 | Loss: 0.00003088
Iteration 35/1000 | Loss: 0.00003088
Iteration 36/1000 | Loss: 0.00003085
Iteration 37/1000 | Loss: 0.00003084
Iteration 38/1000 | Loss: 0.00003081
Iteration 39/1000 | Loss: 0.00003081
Iteration 40/1000 | Loss: 0.00003080
Iteration 41/1000 | Loss: 0.00003080
Iteration 42/1000 | Loss: 0.00003077
Iteration 43/1000 | Loss: 0.00003076
Iteration 44/1000 | Loss: 0.00003076
Iteration 45/1000 | Loss: 0.00003076
Iteration 46/1000 | Loss: 0.00003075
Iteration 47/1000 | Loss: 0.00003075
Iteration 48/1000 | Loss: 0.00003074
Iteration 49/1000 | Loss: 0.00003073
Iteration 50/1000 | Loss: 0.00003073
Iteration 51/1000 | Loss: 0.00003073
Iteration 52/1000 | Loss: 0.00003072
Iteration 53/1000 | Loss: 0.00003072
Iteration 54/1000 | Loss: 0.00003071
Iteration 55/1000 | Loss: 0.00003071
Iteration 56/1000 | Loss: 0.00003071
Iteration 57/1000 | Loss: 0.00003070
Iteration 58/1000 | Loss: 0.00003070
Iteration 59/1000 | Loss: 0.00003070
Iteration 60/1000 | Loss: 0.00003070
Iteration 61/1000 | Loss: 0.00003069
Iteration 62/1000 | Loss: 0.00003069
Iteration 63/1000 | Loss: 0.00003069
Iteration 64/1000 | Loss: 0.00003068
Iteration 65/1000 | Loss: 0.00003068
Iteration 66/1000 | Loss: 0.00003068
Iteration 67/1000 | Loss: 0.00003067
Iteration 68/1000 | Loss: 0.00003067
Iteration 69/1000 | Loss: 0.00003067
Iteration 70/1000 | Loss: 0.00003066
Iteration 71/1000 | Loss: 0.00003066
Iteration 72/1000 | Loss: 0.00003066
Iteration 73/1000 | Loss: 0.00003066
Iteration 74/1000 | Loss: 0.00003065
Iteration 75/1000 | Loss: 0.00003065
Iteration 76/1000 | Loss: 0.00003065
Iteration 77/1000 | Loss: 0.00003065
Iteration 78/1000 | Loss: 0.00003065
Iteration 79/1000 | Loss: 0.00003064
Iteration 80/1000 | Loss: 0.00003064
Iteration 81/1000 | Loss: 0.00003063
Iteration 82/1000 | Loss: 0.00003063
Iteration 83/1000 | Loss: 0.00003063
Iteration 84/1000 | Loss: 0.00003062
Iteration 85/1000 | Loss: 0.00003062
Iteration 86/1000 | Loss: 0.00003062
Iteration 87/1000 | Loss: 0.00003062
Iteration 88/1000 | Loss: 0.00003062
Iteration 89/1000 | Loss: 0.00003062
Iteration 90/1000 | Loss: 0.00003062
Iteration 91/1000 | Loss: 0.00003062
Iteration 92/1000 | Loss: 0.00003062
Iteration 93/1000 | Loss: 0.00003061
Iteration 94/1000 | Loss: 0.00003061
Iteration 95/1000 | Loss: 0.00003061
Iteration 96/1000 | Loss: 0.00003061
Iteration 97/1000 | Loss: 0.00003061
Iteration 98/1000 | Loss: 0.00003061
Iteration 99/1000 | Loss: 0.00003060
Iteration 100/1000 | Loss: 0.00003060
Iteration 101/1000 | Loss: 0.00003060
Iteration 102/1000 | Loss: 0.00003060
Iteration 103/1000 | Loss: 0.00003059
Iteration 104/1000 | Loss: 0.00003059
Iteration 105/1000 | Loss: 0.00003059
Iteration 106/1000 | Loss: 0.00003059
Iteration 107/1000 | Loss: 0.00003059
Iteration 108/1000 | Loss: 0.00003059
Iteration 109/1000 | Loss: 0.00003059
Iteration 110/1000 | Loss: 0.00003058
Iteration 111/1000 | Loss: 0.00003058
Iteration 112/1000 | Loss: 0.00003058
Iteration 113/1000 | Loss: 0.00003058
Iteration 114/1000 | Loss: 0.00003058
Iteration 115/1000 | Loss: 0.00003058
Iteration 116/1000 | Loss: 0.00003057
Iteration 117/1000 | Loss: 0.00003057
Iteration 118/1000 | Loss: 0.00003057
Iteration 119/1000 | Loss: 0.00003057
Iteration 120/1000 | Loss: 0.00003057
Iteration 121/1000 | Loss: 0.00003057
Iteration 122/1000 | Loss: 0.00003056
Iteration 123/1000 | Loss: 0.00003056
Iteration 124/1000 | Loss: 0.00003056
Iteration 125/1000 | Loss: 0.00003056
Iteration 126/1000 | Loss: 0.00003056
Iteration 127/1000 | Loss: 0.00003056
Iteration 128/1000 | Loss: 0.00003055
Iteration 129/1000 | Loss: 0.00003055
Iteration 130/1000 | Loss: 0.00003055
Iteration 131/1000 | Loss: 0.00003055
Iteration 132/1000 | Loss: 0.00003055
Iteration 133/1000 | Loss: 0.00003055
Iteration 134/1000 | Loss: 0.00003055
Iteration 135/1000 | Loss: 0.00003055
Iteration 136/1000 | Loss: 0.00003054
Iteration 137/1000 | Loss: 0.00003054
Iteration 138/1000 | Loss: 0.00003054
Iteration 139/1000 | Loss: 0.00003054
Iteration 140/1000 | Loss: 0.00003054
Iteration 141/1000 | Loss: 0.00003054
Iteration 142/1000 | Loss: 0.00003054
Iteration 143/1000 | Loss: 0.00003054
Iteration 144/1000 | Loss: 0.00003053
Iteration 145/1000 | Loss: 0.00003053
Iteration 146/1000 | Loss: 0.00003053
Iteration 147/1000 | Loss: 0.00003053
Iteration 148/1000 | Loss: 0.00003052
Iteration 149/1000 | Loss: 0.00003052
Iteration 150/1000 | Loss: 0.00003052
Iteration 151/1000 | Loss: 0.00003052
Iteration 152/1000 | Loss: 0.00003052
Iteration 153/1000 | Loss: 0.00003052
Iteration 154/1000 | Loss: 0.00003052
Iteration 155/1000 | Loss: 0.00003051
Iteration 156/1000 | Loss: 0.00003051
Iteration 157/1000 | Loss: 0.00003051
Iteration 158/1000 | Loss: 0.00003051
Iteration 159/1000 | Loss: 0.00003051
Iteration 160/1000 | Loss: 0.00003051
Iteration 161/1000 | Loss: 0.00003051
Iteration 162/1000 | Loss: 0.00003051
Iteration 163/1000 | Loss: 0.00003051
Iteration 164/1000 | Loss: 0.00003051
Iteration 165/1000 | Loss: 0.00003051
Iteration 166/1000 | Loss: 0.00003050
Iteration 167/1000 | Loss: 0.00003050
Iteration 168/1000 | Loss: 0.00003050
Iteration 169/1000 | Loss: 0.00003050
Iteration 170/1000 | Loss: 0.00003050
Iteration 171/1000 | Loss: 0.00003050
Iteration 172/1000 | Loss: 0.00003050
Iteration 173/1000 | Loss: 0.00003050
Iteration 174/1000 | Loss: 0.00003050
Iteration 175/1000 | Loss: 0.00003050
Iteration 176/1000 | Loss: 0.00003050
Iteration 177/1000 | Loss: 0.00003050
Iteration 178/1000 | Loss: 0.00003050
Iteration 179/1000 | Loss: 0.00003050
Iteration 180/1000 | Loss: 0.00003050
Iteration 181/1000 | Loss: 0.00003050
Iteration 182/1000 | Loss: 0.00003049
Iteration 183/1000 | Loss: 0.00003049
Iteration 184/1000 | Loss: 0.00003049
Iteration 185/1000 | Loss: 0.00003049
Iteration 186/1000 | Loss: 0.00003049
Iteration 187/1000 | Loss: 0.00003049
Iteration 188/1000 | Loss: 0.00003049
Iteration 189/1000 | Loss: 0.00003049
Iteration 190/1000 | Loss: 0.00003049
Iteration 191/1000 | Loss: 0.00003049
Iteration 192/1000 | Loss: 0.00003049
Iteration 193/1000 | Loss: 0.00003049
Iteration 194/1000 | Loss: 0.00003049
Iteration 195/1000 | Loss: 0.00003049
Iteration 196/1000 | Loss: 0.00003049
Iteration 197/1000 | Loss: 0.00003048
Iteration 198/1000 | Loss: 0.00003048
Iteration 199/1000 | Loss: 0.00003048
Iteration 200/1000 | Loss: 0.00003048
Iteration 201/1000 | Loss: 0.00003048
Iteration 202/1000 | Loss: 0.00003048
Iteration 203/1000 | Loss: 0.00003048
Iteration 204/1000 | Loss: 0.00003048
Iteration 205/1000 | Loss: 0.00003048
Iteration 206/1000 | Loss: 0.00003048
Iteration 207/1000 | Loss: 0.00003048
Iteration 208/1000 | Loss: 0.00003048
Iteration 209/1000 | Loss: 0.00003048
Iteration 210/1000 | Loss: 0.00003048
Iteration 211/1000 | Loss: 0.00003048
Iteration 212/1000 | Loss: 0.00003048
Iteration 213/1000 | Loss: 0.00003048
Iteration 214/1000 | Loss: 0.00003048
Iteration 215/1000 | Loss: 0.00003048
Iteration 216/1000 | Loss: 0.00003048
Iteration 217/1000 | Loss: 0.00003048
Iteration 218/1000 | Loss: 0.00003048
Iteration 219/1000 | Loss: 0.00003048
Iteration 220/1000 | Loss: 0.00003048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [3.0476394385914318e-05, 3.0476394385914318e-05, 3.0476394385914318e-05, 3.0476394385914318e-05, 3.0476394385914318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0476394385914318e-05

Optimization complete. Final v2v error: 4.770328044891357 mm

Highest mean error: 5.562992572784424 mm for frame 66

Lowest mean error: 4.525186538696289 mm for frame 32

Saving results

Total time: 47.89177131652832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_43_nl_6500/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_43_nl_6500/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940348
Iteration 2/25 | Loss: 0.00191874
Iteration 3/25 | Loss: 0.00173825
Iteration 4/25 | Loss: 0.00169649
Iteration 5/25 | Loss: 0.00168897
Iteration 6/25 | Loss: 0.00168755
Iteration 7/25 | Loss: 0.00168725
Iteration 8/25 | Loss: 0.00168725
Iteration 9/25 | Loss: 0.00168725
Iteration 10/25 | Loss: 0.00168725
Iteration 11/25 | Loss: 0.00168725
Iteration 12/25 | Loss: 0.00168725
Iteration 13/25 | Loss: 0.00168725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0016872490523383021, 0.0016872490523383021, 0.0016872490523383021, 0.0016872490523383021, 0.0016872490523383021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016872490523383021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41574061
Iteration 2/25 | Loss: 0.00239474
Iteration 3/25 | Loss: 0.00239470
Iteration 4/25 | Loss: 0.00239470
Iteration 5/25 | Loss: 0.00239470
Iteration 6/25 | Loss: 0.00239470
Iteration 7/25 | Loss: 0.00239470
Iteration 8/25 | Loss: 0.00239469
Iteration 9/25 | Loss: 0.00239469
Iteration 10/25 | Loss: 0.00239469
Iteration 11/25 | Loss: 0.00239469
Iteration 12/25 | Loss: 0.00239469
Iteration 13/25 | Loss: 0.00239469
Iteration 14/25 | Loss: 0.00239469
Iteration 15/25 | Loss: 0.00239469
Iteration 16/25 | Loss: 0.00239469
Iteration 17/25 | Loss: 0.00239469
Iteration 18/25 | Loss: 0.00239469
Iteration 19/25 | Loss: 0.00239469
Iteration 20/25 | Loss: 0.00239469
Iteration 21/25 | Loss: 0.00239469
Iteration 22/25 | Loss: 0.00239469
Iteration 23/25 | Loss: 0.00239469
Iteration 24/25 | Loss: 0.00239469
Iteration 25/25 | Loss: 0.00239469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239469
Iteration 2/1000 | Loss: 0.00006385
Iteration 3/1000 | Loss: 0.00004737
Iteration 4/1000 | Loss: 0.00004055
Iteration 5/1000 | Loss: 0.00003851
Iteration 6/1000 | Loss: 0.00003690
Iteration 7/1000 | Loss: 0.00003576
Iteration 8/1000 | Loss: 0.00003501
Iteration 9/1000 | Loss: 0.00003450
Iteration 10/1000 | Loss: 0.00003402
Iteration 11/1000 | Loss: 0.00003373
Iteration 12/1000 | Loss: 0.00003346
Iteration 13/1000 | Loss: 0.00003332
Iteration 14/1000 | Loss: 0.00003330
Iteration 15/1000 | Loss: 0.00003328
Iteration 16/1000 | Loss: 0.00003327
Iteration 17/1000 | Loss: 0.00003327
Iteration 18/1000 | Loss: 0.00003326
Iteration 19/1000 | Loss: 0.00003326
Iteration 20/1000 | Loss: 0.00003325
Iteration 21/1000 | Loss: 0.00003325
Iteration 22/1000 | Loss: 0.00003324
Iteration 23/1000 | Loss: 0.00003324
Iteration 24/1000 | Loss: 0.00003323
Iteration 25/1000 | Loss: 0.00003323
Iteration 26/1000 | Loss: 0.00003322
Iteration 27/1000 | Loss: 0.00003322
Iteration 28/1000 | Loss: 0.00003316
Iteration 29/1000 | Loss: 0.00003315
Iteration 30/1000 | Loss: 0.00003315
Iteration 31/1000 | Loss: 0.00003313
Iteration 32/1000 | Loss: 0.00003313
Iteration 33/1000 | Loss: 0.00003313
Iteration 34/1000 | Loss: 0.00003313
Iteration 35/1000 | Loss: 0.00003313
Iteration 36/1000 | Loss: 0.00003312
Iteration 37/1000 | Loss: 0.00003312
Iteration 38/1000 | Loss: 0.00003312
Iteration 39/1000 | Loss: 0.00003312
Iteration 40/1000 | Loss: 0.00003312
Iteration 41/1000 | Loss: 0.00003312
Iteration 42/1000 | Loss: 0.00003312
Iteration 43/1000 | Loss: 0.00003311
Iteration 44/1000 | Loss: 0.00003311
Iteration 45/1000 | Loss: 0.00003311
Iteration 46/1000 | Loss: 0.00003310
Iteration 47/1000 | Loss: 0.00003310
Iteration 48/1000 | Loss: 0.00003310
Iteration 49/1000 | Loss: 0.00003310
Iteration 50/1000 | Loss: 0.00003310
Iteration 51/1000 | Loss: 0.00003309
Iteration 52/1000 | Loss: 0.00003309
Iteration 53/1000 | Loss: 0.00003308
Iteration 54/1000 | Loss: 0.00003308
Iteration 55/1000 | Loss: 0.00003308
Iteration 56/1000 | Loss: 0.00003308
Iteration 57/1000 | Loss: 0.00003308
Iteration 58/1000 | Loss: 0.00003308
Iteration 59/1000 | Loss: 0.00003307
Iteration 60/1000 | Loss: 0.00003307
Iteration 61/1000 | Loss: 0.00003307
Iteration 62/1000 | Loss: 0.00003307
Iteration 63/1000 | Loss: 0.00003306
Iteration 64/1000 | Loss: 0.00003306
Iteration 65/1000 | Loss: 0.00003306
Iteration 66/1000 | Loss: 0.00003306
Iteration 67/1000 | Loss: 0.00003305
Iteration 68/1000 | Loss: 0.00003305
Iteration 69/1000 | Loss: 0.00003305
Iteration 70/1000 | Loss: 0.00003304
Iteration 71/1000 | Loss: 0.00003304
Iteration 72/1000 | Loss: 0.00003304
Iteration 73/1000 | Loss: 0.00003304
Iteration 74/1000 | Loss: 0.00003303
Iteration 75/1000 | Loss: 0.00003303
Iteration 76/1000 | Loss: 0.00003303
Iteration 77/1000 | Loss: 0.00003303
Iteration 78/1000 | Loss: 0.00003303
Iteration 79/1000 | Loss: 0.00003303
Iteration 80/1000 | Loss: 0.00003303
Iteration 81/1000 | Loss: 0.00003303
Iteration 82/1000 | Loss: 0.00003303
Iteration 83/1000 | Loss: 0.00003303
Iteration 84/1000 | Loss: 0.00003303
Iteration 85/1000 | Loss: 0.00003303
Iteration 86/1000 | Loss: 0.00003303
Iteration 87/1000 | Loss: 0.00003303
Iteration 88/1000 | Loss: 0.00003303
Iteration 89/1000 | Loss: 0.00003303
Iteration 90/1000 | Loss: 0.00003302
Iteration 91/1000 | Loss: 0.00003302
Iteration 92/1000 | Loss: 0.00003302
Iteration 93/1000 | Loss: 0.00003302
Iteration 94/1000 | Loss: 0.00003302
Iteration 95/1000 | Loss: 0.00003302
Iteration 96/1000 | Loss: 0.00003302
Iteration 97/1000 | Loss: 0.00003302
Iteration 98/1000 | Loss: 0.00003302
Iteration 99/1000 | Loss: 0.00003302
Iteration 100/1000 | Loss: 0.00003301
Iteration 101/1000 | Loss: 0.00003301
Iteration 102/1000 | Loss: 0.00003301
Iteration 103/1000 | Loss: 0.00003301
Iteration 104/1000 | Loss: 0.00003301
Iteration 105/1000 | Loss: 0.00003301
Iteration 106/1000 | Loss: 0.00003301
Iteration 107/1000 | Loss: 0.00003301
Iteration 108/1000 | Loss: 0.00003301
Iteration 109/1000 | Loss: 0.00003301
Iteration 110/1000 | Loss: 0.00003301
Iteration 111/1000 | Loss: 0.00003301
Iteration 112/1000 | Loss: 0.00003301
Iteration 113/1000 | Loss: 0.00003301
Iteration 114/1000 | Loss: 0.00003301
Iteration 115/1000 | Loss: 0.00003301
Iteration 116/1000 | Loss: 0.00003301
Iteration 117/1000 | Loss: 0.00003301
Iteration 118/1000 | Loss: 0.00003301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.301123433629982e-05, 3.301123433629982e-05, 3.301123433629982e-05, 3.301123433629982e-05, 3.301123433629982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.301123433629982e-05

Optimization complete. Final v2v error: 4.950181484222412 mm

Highest mean error: 5.2321648597717285 mm for frame 160

Lowest mean error: 4.595555305480957 mm for frame 74

Saving results

Total time: 36.61547613143921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388466
Iteration 2/25 | Loss: 0.00114513
Iteration 3/25 | Loss: 0.00107730
Iteration 4/25 | Loss: 0.00107115
Iteration 5/25 | Loss: 0.00106902
Iteration 6/25 | Loss: 0.00106902
Iteration 7/25 | Loss: 0.00106902
Iteration 8/25 | Loss: 0.00106902
Iteration 9/25 | Loss: 0.00106898
Iteration 10/25 | Loss: 0.00106898
Iteration 11/25 | Loss: 0.00106898
Iteration 12/25 | Loss: 0.00106898
Iteration 13/25 | Loss: 0.00106898
Iteration 14/25 | Loss: 0.00106898
Iteration 15/25 | Loss: 0.00106898
Iteration 16/25 | Loss: 0.00106898
Iteration 17/25 | Loss: 0.00106898
Iteration 18/25 | Loss: 0.00106898
Iteration 19/25 | Loss: 0.00106898
Iteration 20/25 | Loss: 0.00106898
Iteration 21/25 | Loss: 0.00106898
Iteration 22/25 | Loss: 0.00106898
Iteration 23/25 | Loss: 0.00106898
Iteration 24/25 | Loss: 0.00106898
Iteration 25/25 | Loss: 0.00106898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23435855
Iteration 2/25 | Loss: 0.00148457
Iteration 3/25 | Loss: 0.00148457
Iteration 4/25 | Loss: 0.00148457
Iteration 5/25 | Loss: 0.00148457
Iteration 6/25 | Loss: 0.00148457
Iteration 7/25 | Loss: 0.00148457
Iteration 8/25 | Loss: 0.00148457
Iteration 9/25 | Loss: 0.00148457
Iteration 10/25 | Loss: 0.00148457
Iteration 11/25 | Loss: 0.00148457
Iteration 12/25 | Loss: 0.00148457
Iteration 13/25 | Loss: 0.00148457
Iteration 14/25 | Loss: 0.00148457
Iteration 15/25 | Loss: 0.00148457
Iteration 16/25 | Loss: 0.00148457
Iteration 17/25 | Loss: 0.00148457
Iteration 18/25 | Loss: 0.00148457
Iteration 19/25 | Loss: 0.00148457
Iteration 20/25 | Loss: 0.00148457
Iteration 21/25 | Loss: 0.00148457
Iteration 22/25 | Loss: 0.00148457
Iteration 23/25 | Loss: 0.00148457
Iteration 24/25 | Loss: 0.00148457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014845700934529305, 0.0014845700934529305, 0.0014845700934529305, 0.0014845700934529305, 0.0014845700934529305]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014845700934529305

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148457
Iteration 2/1000 | Loss: 0.00001836
Iteration 3/1000 | Loss: 0.00001400
Iteration 4/1000 | Loss: 0.00001291
Iteration 5/1000 | Loss: 0.00001220
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001126
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001111
Iteration 15/1000 | Loss: 0.00001110
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001109
Iteration 18/1000 | Loss: 0.00001109
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001108
Iteration 22/1000 | Loss: 0.00001107
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001095
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001082
Iteration 49/1000 | Loss: 0.00001080
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001078
Iteration 59/1000 | Loss: 0.00001078
Iteration 60/1000 | Loss: 0.00001078
Iteration 61/1000 | Loss: 0.00001078
Iteration 62/1000 | Loss: 0.00001078
Iteration 63/1000 | Loss: 0.00001077
Iteration 64/1000 | Loss: 0.00001077
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001075
Iteration 69/1000 | Loss: 0.00001075
Iteration 70/1000 | Loss: 0.00001075
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001074
Iteration 85/1000 | Loss: 0.00001074
Iteration 86/1000 | Loss: 0.00001074
Iteration 87/1000 | Loss: 0.00001074
Iteration 88/1000 | Loss: 0.00001074
Iteration 89/1000 | Loss: 0.00001074
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001073
Iteration 96/1000 | Loss: 0.00001073
Iteration 97/1000 | Loss: 0.00001073
Iteration 98/1000 | Loss: 0.00001073
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001073
Iteration 101/1000 | Loss: 0.00001073
Iteration 102/1000 | Loss: 0.00001073
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.0713138181017712e-05, 1.0713138181017712e-05, 1.0713138181017712e-05, 1.0713138181017712e-05, 1.0713138181017712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0713138181017712e-05

Optimization complete. Final v2v error: 2.900726079940796 mm

Highest mean error: 3.089073657989502 mm for frame 8

Lowest mean error: 2.7736446857452393 mm for frame 200

Saving results

Total time: 36.843743085861206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930102
Iteration 2/25 | Loss: 0.00139346
Iteration 3/25 | Loss: 0.00120720
Iteration 4/25 | Loss: 0.00116380
Iteration 5/25 | Loss: 0.00115083
Iteration 6/25 | Loss: 0.00114779
Iteration 7/25 | Loss: 0.00114727
Iteration 8/25 | Loss: 0.00114727
Iteration 9/25 | Loss: 0.00114727
Iteration 10/25 | Loss: 0.00114727
Iteration 11/25 | Loss: 0.00114727
Iteration 12/25 | Loss: 0.00114727
Iteration 13/25 | Loss: 0.00114727
Iteration 14/25 | Loss: 0.00114727
Iteration 15/25 | Loss: 0.00114727
Iteration 16/25 | Loss: 0.00114727
Iteration 17/25 | Loss: 0.00114727
Iteration 18/25 | Loss: 0.00114727
Iteration 19/25 | Loss: 0.00114727
Iteration 20/25 | Loss: 0.00114727
Iteration 21/25 | Loss: 0.00114727
Iteration 22/25 | Loss: 0.00114727
Iteration 23/25 | Loss: 0.00114727
Iteration 24/25 | Loss: 0.00114727
Iteration 25/25 | Loss: 0.00114727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27384973
Iteration 2/25 | Loss: 0.00148379
Iteration 3/25 | Loss: 0.00148376
Iteration 4/25 | Loss: 0.00148375
Iteration 5/25 | Loss: 0.00148375
Iteration 6/25 | Loss: 0.00148375
Iteration 7/25 | Loss: 0.00148375
Iteration 8/25 | Loss: 0.00148375
Iteration 9/25 | Loss: 0.00148375
Iteration 10/25 | Loss: 0.00148375
Iteration 11/25 | Loss: 0.00148375
Iteration 12/25 | Loss: 0.00148375
Iteration 13/25 | Loss: 0.00148375
Iteration 14/25 | Loss: 0.00148375
Iteration 15/25 | Loss: 0.00148375
Iteration 16/25 | Loss: 0.00148375
Iteration 17/25 | Loss: 0.00148375
Iteration 18/25 | Loss: 0.00148375
Iteration 19/25 | Loss: 0.00148375
Iteration 20/25 | Loss: 0.00148375
Iteration 21/25 | Loss: 0.00148375
Iteration 22/25 | Loss: 0.00148375
Iteration 23/25 | Loss: 0.00148375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001483751810155809, 0.001483751810155809, 0.001483751810155809, 0.001483751810155809, 0.001483751810155809]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001483751810155809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148375
Iteration 2/1000 | Loss: 0.00005947
Iteration 3/1000 | Loss: 0.00003345
Iteration 4/1000 | Loss: 0.00002639
Iteration 5/1000 | Loss: 0.00002442
Iteration 6/1000 | Loss: 0.00002296
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002124
Iteration 9/1000 | Loss: 0.00002054
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001977
Iteration 12/1000 | Loss: 0.00001969
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00001937
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001936
Iteration 18/1000 | Loss: 0.00001935
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001932
Iteration 21/1000 | Loss: 0.00001932
Iteration 22/1000 | Loss: 0.00001931
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001930
Iteration 26/1000 | Loss: 0.00001930
Iteration 27/1000 | Loss: 0.00001929
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001928
Iteration 30/1000 | Loss: 0.00001928
Iteration 31/1000 | Loss: 0.00001927
Iteration 32/1000 | Loss: 0.00001927
Iteration 33/1000 | Loss: 0.00001927
Iteration 34/1000 | Loss: 0.00001926
Iteration 35/1000 | Loss: 0.00001926
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001925
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001925
Iteration 43/1000 | Loss: 0.00001924
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001924
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001923
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001922
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001921
Iteration 62/1000 | Loss: 0.00001921
Iteration 63/1000 | Loss: 0.00001921
Iteration 64/1000 | Loss: 0.00001921
Iteration 65/1000 | Loss: 0.00001921
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001920
Iteration 71/1000 | Loss: 0.00001920
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001920
Iteration 75/1000 | Loss: 0.00001920
Iteration 76/1000 | Loss: 0.00001920
Iteration 77/1000 | Loss: 0.00001920
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00001919
Iteration 80/1000 | Loss: 0.00001919
Iteration 81/1000 | Loss: 0.00001919
Iteration 82/1000 | Loss: 0.00001919
Iteration 83/1000 | Loss: 0.00001919
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001919
Iteration 90/1000 | Loss: 0.00001919
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001918
Iteration 94/1000 | Loss: 0.00001918
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001918
Iteration 97/1000 | Loss: 0.00001918
Iteration 98/1000 | Loss: 0.00001918
Iteration 99/1000 | Loss: 0.00001918
Iteration 100/1000 | Loss: 0.00001917
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001917
Iteration 103/1000 | Loss: 0.00001917
Iteration 104/1000 | Loss: 0.00001917
Iteration 105/1000 | Loss: 0.00001916
Iteration 106/1000 | Loss: 0.00001916
Iteration 107/1000 | Loss: 0.00001916
Iteration 108/1000 | Loss: 0.00001916
Iteration 109/1000 | Loss: 0.00001916
Iteration 110/1000 | Loss: 0.00001915
Iteration 111/1000 | Loss: 0.00001915
Iteration 112/1000 | Loss: 0.00001915
Iteration 113/1000 | Loss: 0.00001915
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001915
Iteration 116/1000 | Loss: 0.00001915
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001915
Iteration 119/1000 | Loss: 0.00001915
Iteration 120/1000 | Loss: 0.00001915
Iteration 121/1000 | Loss: 0.00001915
Iteration 122/1000 | Loss: 0.00001915
Iteration 123/1000 | Loss: 0.00001914
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001914
Iteration 126/1000 | Loss: 0.00001914
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001914
Iteration 130/1000 | Loss: 0.00001914
Iteration 131/1000 | Loss: 0.00001913
Iteration 132/1000 | Loss: 0.00001913
Iteration 133/1000 | Loss: 0.00001913
Iteration 134/1000 | Loss: 0.00001913
Iteration 135/1000 | Loss: 0.00001913
Iteration 136/1000 | Loss: 0.00001913
Iteration 137/1000 | Loss: 0.00001913
Iteration 138/1000 | Loss: 0.00001913
Iteration 139/1000 | Loss: 0.00001913
Iteration 140/1000 | Loss: 0.00001913
Iteration 141/1000 | Loss: 0.00001913
Iteration 142/1000 | Loss: 0.00001913
Iteration 143/1000 | Loss: 0.00001913
Iteration 144/1000 | Loss: 0.00001913
Iteration 145/1000 | Loss: 0.00001913
Iteration 146/1000 | Loss: 0.00001913
Iteration 147/1000 | Loss: 0.00001913
Iteration 148/1000 | Loss: 0.00001913
Iteration 149/1000 | Loss: 0.00001913
Iteration 150/1000 | Loss: 0.00001913
Iteration 151/1000 | Loss: 0.00001913
Iteration 152/1000 | Loss: 0.00001913
Iteration 153/1000 | Loss: 0.00001913
Iteration 154/1000 | Loss: 0.00001913
Iteration 155/1000 | Loss: 0.00001913
Iteration 156/1000 | Loss: 0.00001913
Iteration 157/1000 | Loss: 0.00001913
Iteration 158/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.9131091903545894e-05, 1.9131091903545894e-05, 1.9131091903545894e-05, 1.9131091903545894e-05, 1.9131091903545894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9131091903545894e-05

Optimization complete. Final v2v error: 3.701768159866333 mm

Highest mean error: 5.417794704437256 mm for frame 69

Lowest mean error: 3.011836290359497 mm for frame 1

Saving results

Total time: 37.238924980163574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077528
Iteration 2/25 | Loss: 0.00182598
Iteration 3/25 | Loss: 0.00146006
Iteration 4/25 | Loss: 0.00131749
Iteration 5/25 | Loss: 0.00129087
Iteration 6/25 | Loss: 0.00127522
Iteration 7/25 | Loss: 0.00124347
Iteration 8/25 | Loss: 0.00126394
Iteration 9/25 | Loss: 0.00121147
Iteration 10/25 | Loss: 0.00120819
Iteration 11/25 | Loss: 0.00119066
Iteration 12/25 | Loss: 0.00118106
Iteration 13/25 | Loss: 0.00117245
Iteration 14/25 | Loss: 0.00117048
Iteration 15/25 | Loss: 0.00117285
Iteration 16/25 | Loss: 0.00117162
Iteration 17/25 | Loss: 0.00117078
Iteration 18/25 | Loss: 0.00116976
Iteration 19/25 | Loss: 0.00116854
Iteration 20/25 | Loss: 0.00116779
Iteration 21/25 | Loss: 0.00116890
Iteration 22/25 | Loss: 0.00116522
Iteration 23/25 | Loss: 0.00116652
Iteration 24/25 | Loss: 0.00116724
Iteration 25/25 | Loss: 0.00117071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39266789
Iteration 2/25 | Loss: 0.00191530
Iteration 3/25 | Loss: 0.00191530
Iteration 4/25 | Loss: 0.00191530
Iteration 5/25 | Loss: 0.00191530
Iteration 6/25 | Loss: 0.00191530
Iteration 7/25 | Loss: 0.00191530
Iteration 8/25 | Loss: 0.00191530
Iteration 9/25 | Loss: 0.00191529
Iteration 10/25 | Loss: 0.00191529
Iteration 11/25 | Loss: 0.00191529
Iteration 12/25 | Loss: 0.00191529
Iteration 13/25 | Loss: 0.00191529
Iteration 14/25 | Loss: 0.00191529
Iteration 15/25 | Loss: 0.00191529
Iteration 16/25 | Loss: 0.00191529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001915294211357832, 0.001915294211357832, 0.001915294211357832, 0.001915294211357832, 0.001915294211357832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001915294211357832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191529
Iteration 2/1000 | Loss: 0.00216025
Iteration 3/1000 | Loss: 0.00008846
Iteration 4/1000 | Loss: 0.00082294
Iteration 5/1000 | Loss: 0.00016842
Iteration 6/1000 | Loss: 0.00005883
Iteration 7/1000 | Loss: 0.00042240
Iteration 8/1000 | Loss: 0.00006162
Iteration 9/1000 | Loss: 0.00004734
Iteration 10/1000 | Loss: 0.00003959
Iteration 11/1000 | Loss: 0.00003705
Iteration 12/1000 | Loss: 0.00003517
Iteration 13/1000 | Loss: 0.00003404
Iteration 14/1000 | Loss: 0.00003319
Iteration 15/1000 | Loss: 0.00003246
Iteration 16/1000 | Loss: 0.00003183
Iteration 17/1000 | Loss: 0.00003151
Iteration 18/1000 | Loss: 0.00003109
Iteration 19/1000 | Loss: 0.00003076
Iteration 20/1000 | Loss: 0.00190465
Iteration 21/1000 | Loss: 0.00032826
Iteration 22/1000 | Loss: 0.00023462
Iteration 23/1000 | Loss: 0.00020074
Iteration 24/1000 | Loss: 0.00021801
Iteration 25/1000 | Loss: 0.00019373
Iteration 26/1000 | Loss: 0.00024596
Iteration 27/1000 | Loss: 0.00017074
Iteration 28/1000 | Loss: 0.00019609
Iteration 29/1000 | Loss: 0.00016594
Iteration 30/1000 | Loss: 0.00004602
Iteration 31/1000 | Loss: 0.00023030
Iteration 32/1000 | Loss: 0.00011838
Iteration 33/1000 | Loss: 0.00018976
Iteration 34/1000 | Loss: 0.00024412
Iteration 35/1000 | Loss: 0.00004455
Iteration 36/1000 | Loss: 0.00002859
Iteration 37/1000 | Loss: 0.00002302
Iteration 38/1000 | Loss: 0.00002147
Iteration 39/1000 | Loss: 0.00002044
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001843
Iteration 43/1000 | Loss: 0.00001785
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001666
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001598
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001588
Iteration 50/1000 | Loss: 0.00001587
Iteration 51/1000 | Loss: 0.00001586
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001564
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001561
Iteration 66/1000 | Loss: 0.00001561
Iteration 67/1000 | Loss: 0.00001560
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001558
Iteration 71/1000 | Loss: 0.00001557
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001555
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001552
Iteration 83/1000 | Loss: 0.00001552
Iteration 84/1000 | Loss: 0.00001552
Iteration 85/1000 | Loss: 0.00001551
Iteration 86/1000 | Loss: 0.00001551
Iteration 87/1000 | Loss: 0.00001551
Iteration 88/1000 | Loss: 0.00001550
Iteration 89/1000 | Loss: 0.00001550
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001549
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001548
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001547
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001544
Iteration 120/1000 | Loss: 0.00001544
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001543
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001542
Iteration 134/1000 | Loss: 0.00001542
Iteration 135/1000 | Loss: 0.00001542
Iteration 136/1000 | Loss: 0.00001542
Iteration 137/1000 | Loss: 0.00001542
Iteration 138/1000 | Loss: 0.00001542
Iteration 139/1000 | Loss: 0.00001542
Iteration 140/1000 | Loss: 0.00001542
Iteration 141/1000 | Loss: 0.00001542
Iteration 142/1000 | Loss: 0.00001542
Iteration 143/1000 | Loss: 0.00001542
Iteration 144/1000 | Loss: 0.00001542
Iteration 145/1000 | Loss: 0.00001541
Iteration 146/1000 | Loss: 0.00001541
Iteration 147/1000 | Loss: 0.00001541
Iteration 148/1000 | Loss: 0.00001541
Iteration 149/1000 | Loss: 0.00001541
Iteration 150/1000 | Loss: 0.00001541
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001541
Iteration 156/1000 | Loss: 0.00001541
Iteration 157/1000 | Loss: 0.00001541
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001540
Iteration 160/1000 | Loss: 0.00001540
Iteration 161/1000 | Loss: 0.00001540
Iteration 162/1000 | Loss: 0.00001540
Iteration 163/1000 | Loss: 0.00001540
Iteration 164/1000 | Loss: 0.00001540
Iteration 165/1000 | Loss: 0.00001540
Iteration 166/1000 | Loss: 0.00001540
Iteration 167/1000 | Loss: 0.00001540
Iteration 168/1000 | Loss: 0.00001540
Iteration 169/1000 | Loss: 0.00001540
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001540
Iteration 183/1000 | Loss: 0.00001540
Iteration 184/1000 | Loss: 0.00001540
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Iteration 187/1000 | Loss: 0.00001540
Iteration 188/1000 | Loss: 0.00001540
Iteration 189/1000 | Loss: 0.00001540
Iteration 190/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.5404582882183604e-05, 1.5404582882183604e-05, 1.5404582882183604e-05, 1.5404582882183604e-05, 1.5404582882183604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5404582882183604e-05

Optimization complete. Final v2v error: 3.1902806758880615 mm

Highest mean error: 6.160501480102539 mm for frame 50

Lowest mean error: 2.5381157398223877 mm for frame 2

Saving results

Total time: 121.69288682937622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01112801
Iteration 2/25 | Loss: 0.01112801
Iteration 3/25 | Loss: 0.01112801
Iteration 4/25 | Loss: 0.01112801
Iteration 5/25 | Loss: 0.01112801
Iteration 6/25 | Loss: 0.01112801
Iteration 7/25 | Loss: 0.01112800
Iteration 8/25 | Loss: 0.01112800
Iteration 9/25 | Loss: 0.01112800
Iteration 10/25 | Loss: 0.01112800
Iteration 11/25 | Loss: 0.01112800
Iteration 12/25 | Loss: 0.01112800
Iteration 13/25 | Loss: 0.01112799
Iteration 14/25 | Loss: 0.01112799
Iteration 15/25 | Loss: 0.01112799
Iteration 16/25 | Loss: 0.01112799
Iteration 17/25 | Loss: 0.01112799
Iteration 18/25 | Loss: 0.01112799
Iteration 19/25 | Loss: 0.01112798
Iteration 20/25 | Loss: 0.01112798
Iteration 21/25 | Loss: 0.01112798
Iteration 22/25 | Loss: 0.01112798
Iteration 23/25 | Loss: 0.01112798
Iteration 24/25 | Loss: 0.01112797
Iteration 25/25 | Loss: 0.01112797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66748703
Iteration 2/25 | Loss: 0.17551823
Iteration 3/25 | Loss: 0.08296670
Iteration 4/25 | Loss: 0.08296540
Iteration 5/25 | Loss: 0.08296540
Iteration 6/25 | Loss: 0.08296540
Iteration 7/25 | Loss: 0.08296538
Iteration 8/25 | Loss: 0.08296538
Iteration 9/25 | Loss: 0.08296538
Iteration 10/25 | Loss: 0.08296537
Iteration 11/25 | Loss: 0.08296537
Iteration 12/25 | Loss: 0.08296537
Iteration 13/25 | Loss: 0.08296537
Iteration 14/25 | Loss: 0.08296537
Iteration 15/25 | Loss: 0.08296537
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08296537399291992, 0.08296537399291992, 0.08296537399291992, 0.08296537399291992, 0.08296537399291992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08296537399291992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08296537
Iteration 2/1000 | Loss: 0.00702869
Iteration 3/1000 | Loss: 0.00387521
Iteration 4/1000 | Loss: 0.00152156
Iteration 5/1000 | Loss: 0.00143045
Iteration 6/1000 | Loss: 0.00166714
Iteration 7/1000 | Loss: 0.00076984
Iteration 8/1000 | Loss: 0.00482723
Iteration 9/1000 | Loss: 0.00053823
Iteration 10/1000 | Loss: 0.00100088
Iteration 11/1000 | Loss: 0.00037375
Iteration 12/1000 | Loss: 0.00090965
Iteration 13/1000 | Loss: 0.00058760
Iteration 14/1000 | Loss: 0.00031559
Iteration 15/1000 | Loss: 0.00105437
Iteration 16/1000 | Loss: 0.00338778
Iteration 17/1000 | Loss: 0.00260914
Iteration 18/1000 | Loss: 0.00490851
Iteration 19/1000 | Loss: 0.00048755
Iteration 20/1000 | Loss: 0.00119099
Iteration 21/1000 | Loss: 0.00243133
Iteration 22/1000 | Loss: 0.00238163
Iteration 23/1000 | Loss: 0.00406940
Iteration 24/1000 | Loss: 0.00200866
Iteration 25/1000 | Loss: 0.00242318
Iteration 26/1000 | Loss: 0.00127911
Iteration 27/1000 | Loss: 0.00147693
Iteration 28/1000 | Loss: 0.00216464
Iteration 29/1000 | Loss: 0.00150388
Iteration 30/1000 | Loss: 0.00181949
Iteration 31/1000 | Loss: 0.00180149
Iteration 32/1000 | Loss: 0.00268885
Iteration 33/1000 | Loss: 0.00245794
Iteration 34/1000 | Loss: 0.00185028
Iteration 35/1000 | Loss: 0.00195945
Iteration 36/1000 | Loss: 0.00129965
Iteration 37/1000 | Loss: 0.00015232
Iteration 38/1000 | Loss: 0.00014324
Iteration 39/1000 | Loss: 0.00014762
Iteration 40/1000 | Loss: 0.00025601
Iteration 41/1000 | Loss: 0.00015863
Iteration 42/1000 | Loss: 0.00018521
Iteration 43/1000 | Loss: 0.00014676
Iteration 44/1000 | Loss: 0.00010827
Iteration 45/1000 | Loss: 0.00009313
Iteration 46/1000 | Loss: 0.00007623
Iteration 47/1000 | Loss: 0.00006910
Iteration 48/1000 | Loss: 0.00006298
Iteration 49/1000 | Loss: 0.00005850
Iteration 50/1000 | Loss: 0.00019013
Iteration 51/1000 | Loss: 0.00022732
Iteration 52/1000 | Loss: 0.00008980
Iteration 53/1000 | Loss: 0.00007975
Iteration 54/1000 | Loss: 0.00008328
Iteration 55/1000 | Loss: 0.00006467
Iteration 56/1000 | Loss: 0.00005421
Iteration 57/1000 | Loss: 0.00006955
Iteration 58/1000 | Loss: 0.00005803
Iteration 59/1000 | Loss: 0.00006582
Iteration 60/1000 | Loss: 0.00005599
Iteration 61/1000 | Loss: 0.00005200
Iteration 62/1000 | Loss: 0.00006654
Iteration 63/1000 | Loss: 0.00005605
Iteration 64/1000 | Loss: 0.00006025
Iteration 65/1000 | Loss: 0.00006129
Iteration 66/1000 | Loss: 0.00009975
Iteration 67/1000 | Loss: 0.00005600
Iteration 68/1000 | Loss: 0.00007265
Iteration 69/1000 | Loss: 0.00006315
Iteration 70/1000 | Loss: 0.00006854
Iteration 71/1000 | Loss: 0.00006329
Iteration 72/1000 | Loss: 0.00006835
Iteration 73/1000 | Loss: 0.00005972
Iteration 74/1000 | Loss: 0.00007246
Iteration 75/1000 | Loss: 0.00006960
Iteration 76/1000 | Loss: 0.00006163
Iteration 77/1000 | Loss: 0.00005420
Iteration 78/1000 | Loss: 0.00005429
Iteration 79/1000 | Loss: 0.00007978
Iteration 80/1000 | Loss: 0.00005752
Iteration 81/1000 | Loss: 0.00005602
Iteration 82/1000 | Loss: 0.00005041
Iteration 83/1000 | Loss: 0.00004888
Iteration 84/1000 | Loss: 0.00007631
Iteration 85/1000 | Loss: 0.00005660
Iteration 86/1000 | Loss: 0.00005002
Iteration 87/1000 | Loss: 0.00008237
Iteration 88/1000 | Loss: 0.00005481
Iteration 89/1000 | Loss: 0.00005134
Iteration 90/1000 | Loss: 0.00005986
Iteration 91/1000 | Loss: 0.00004958
Iteration 92/1000 | Loss: 0.00006445
Iteration 93/1000 | Loss: 0.00004972
Iteration 94/1000 | Loss: 0.00007396
Iteration 95/1000 | Loss: 0.00005656
Iteration 96/1000 | Loss: 0.00005095
Iteration 97/1000 | Loss: 0.00005790
Iteration 98/1000 | Loss: 0.00005859
Iteration 99/1000 | Loss: 0.00005686
Iteration 100/1000 | Loss: 0.00005441
Iteration 101/1000 | Loss: 0.00005847
Iteration 102/1000 | Loss: 0.00005310
Iteration 103/1000 | Loss: 0.00004520
Iteration 104/1000 | Loss: 0.00005188
Iteration 105/1000 | Loss: 0.00007837
Iteration 106/1000 | Loss: 0.00007366
Iteration 107/1000 | Loss: 0.00006050
Iteration 108/1000 | Loss: 0.00005490
Iteration 109/1000 | Loss: 0.00005493
Iteration 110/1000 | Loss: 0.00004339
Iteration 111/1000 | Loss: 0.00004773
Iteration 112/1000 | Loss: 0.00004948
Iteration 113/1000 | Loss: 0.00005029
Iteration 114/1000 | Loss: 0.00005703
Iteration 115/1000 | Loss: 0.00005452
Iteration 116/1000 | Loss: 0.00005508
Iteration 117/1000 | Loss: 0.00004978
Iteration 118/1000 | Loss: 0.00005638
Iteration 119/1000 | Loss: 0.00005158
Iteration 120/1000 | Loss: 0.00005524
Iteration 121/1000 | Loss: 0.00005271
Iteration 122/1000 | Loss: 0.00005309
Iteration 123/1000 | Loss: 0.00005129
Iteration 124/1000 | Loss: 0.00005820
Iteration 125/1000 | Loss: 0.00005468
Iteration 126/1000 | Loss: 0.00005720
Iteration 127/1000 | Loss: 0.00005100
Iteration 128/1000 | Loss: 0.00005628
Iteration 129/1000 | Loss: 0.00005449
Iteration 130/1000 | Loss: 0.00005848
Iteration 131/1000 | Loss: 0.00005423
Iteration 132/1000 | Loss: 0.00005761
Iteration 133/1000 | Loss: 0.00005707
Iteration 134/1000 | Loss: 0.00005691
Iteration 135/1000 | Loss: 0.00005895
Iteration 136/1000 | Loss: 0.00005568
Iteration 137/1000 | Loss: 0.00005591
Iteration 138/1000 | Loss: 0.00005802
Iteration 139/1000 | Loss: 0.00005456
Iteration 140/1000 | Loss: 0.00004836
Iteration 141/1000 | Loss: 0.00006119
Iteration 142/1000 | Loss: 0.00005734
Iteration 143/1000 | Loss: 0.00005114
Iteration 144/1000 | Loss: 0.00006780
Iteration 145/1000 | Loss: 0.00004584
Iteration 146/1000 | Loss: 0.00004004
Iteration 147/1000 | Loss: 0.00003625
Iteration 148/1000 | Loss: 0.00003482
Iteration 149/1000 | Loss: 0.00003392
Iteration 150/1000 | Loss: 0.00003331
Iteration 151/1000 | Loss: 0.00003278
Iteration 152/1000 | Loss: 0.00003242
Iteration 153/1000 | Loss: 0.00003220
Iteration 154/1000 | Loss: 0.00003203
Iteration 155/1000 | Loss: 0.00003195
Iteration 156/1000 | Loss: 0.00003194
Iteration 157/1000 | Loss: 0.00003194
Iteration 158/1000 | Loss: 0.00003194
Iteration 159/1000 | Loss: 0.00003193
Iteration 160/1000 | Loss: 0.00003177
Iteration 161/1000 | Loss: 0.00003159
Iteration 162/1000 | Loss: 0.00003147
Iteration 163/1000 | Loss: 0.00003144
Iteration 164/1000 | Loss: 0.00003141
Iteration 165/1000 | Loss: 0.00003141
Iteration 166/1000 | Loss: 0.00003139
Iteration 167/1000 | Loss: 0.00003139
Iteration 168/1000 | Loss: 0.00003139
Iteration 169/1000 | Loss: 0.00003139
Iteration 170/1000 | Loss: 0.00003139
Iteration 171/1000 | Loss: 0.00003139
Iteration 172/1000 | Loss: 0.00003139
Iteration 173/1000 | Loss: 0.00003138
Iteration 174/1000 | Loss: 0.00003138
Iteration 175/1000 | Loss: 0.00003137
Iteration 176/1000 | Loss: 0.00003137
Iteration 177/1000 | Loss: 0.00003137
Iteration 178/1000 | Loss: 0.00003137
Iteration 179/1000 | Loss: 0.00003137
Iteration 180/1000 | Loss: 0.00003137
Iteration 181/1000 | Loss: 0.00003136
Iteration 182/1000 | Loss: 0.00003136
Iteration 183/1000 | Loss: 0.00003136
Iteration 184/1000 | Loss: 0.00003136
Iteration 185/1000 | Loss: 0.00003135
Iteration 186/1000 | Loss: 0.00003135
Iteration 187/1000 | Loss: 0.00003135
Iteration 188/1000 | Loss: 0.00003135
Iteration 189/1000 | Loss: 0.00003135
Iteration 190/1000 | Loss: 0.00003135
Iteration 191/1000 | Loss: 0.00003134
Iteration 192/1000 | Loss: 0.00003134
Iteration 193/1000 | Loss: 0.00003134
Iteration 194/1000 | Loss: 0.00003133
Iteration 195/1000 | Loss: 0.00003133
Iteration 196/1000 | Loss: 0.00003133
Iteration 197/1000 | Loss: 0.00003133
Iteration 198/1000 | Loss: 0.00003133
Iteration 199/1000 | Loss: 0.00003133
Iteration 200/1000 | Loss: 0.00003133
Iteration 201/1000 | Loss: 0.00003133
Iteration 202/1000 | Loss: 0.00003133
Iteration 203/1000 | Loss: 0.00003132
Iteration 204/1000 | Loss: 0.00003132
Iteration 205/1000 | Loss: 0.00003132
Iteration 206/1000 | Loss: 0.00003132
Iteration 207/1000 | Loss: 0.00003132
Iteration 208/1000 | Loss: 0.00003132
Iteration 209/1000 | Loss: 0.00003132
Iteration 210/1000 | Loss: 0.00003132
Iteration 211/1000 | Loss: 0.00003132
Iteration 212/1000 | Loss: 0.00003132
Iteration 213/1000 | Loss: 0.00003132
Iteration 214/1000 | Loss: 0.00003132
Iteration 215/1000 | Loss: 0.00003131
Iteration 216/1000 | Loss: 0.00003131
Iteration 217/1000 | Loss: 0.00003131
Iteration 218/1000 | Loss: 0.00003131
Iteration 219/1000 | Loss: 0.00003130
Iteration 220/1000 | Loss: 0.00003130
Iteration 221/1000 | Loss: 0.00003130
Iteration 222/1000 | Loss: 0.00003130
Iteration 223/1000 | Loss: 0.00003130
Iteration 224/1000 | Loss: 0.00003130
Iteration 225/1000 | Loss: 0.00003130
Iteration 226/1000 | Loss: 0.00003129
Iteration 227/1000 | Loss: 0.00003129
Iteration 228/1000 | Loss: 0.00003129
Iteration 229/1000 | Loss: 0.00003129
Iteration 230/1000 | Loss: 0.00003129
Iteration 231/1000 | Loss: 0.00003129
Iteration 232/1000 | Loss: 0.00003129
Iteration 233/1000 | Loss: 0.00003129
Iteration 234/1000 | Loss: 0.00003129
Iteration 235/1000 | Loss: 0.00003129
Iteration 236/1000 | Loss: 0.00003129
Iteration 237/1000 | Loss: 0.00003128
Iteration 238/1000 | Loss: 0.00003128
Iteration 239/1000 | Loss: 0.00003128
Iteration 240/1000 | Loss: 0.00003128
Iteration 241/1000 | Loss: 0.00003128
Iteration 242/1000 | Loss: 0.00003128
Iteration 243/1000 | Loss: 0.00003128
Iteration 244/1000 | Loss: 0.00003128
Iteration 245/1000 | Loss: 0.00003128
Iteration 246/1000 | Loss: 0.00003128
Iteration 247/1000 | Loss: 0.00003127
Iteration 248/1000 | Loss: 0.00003127
Iteration 249/1000 | Loss: 0.00003127
Iteration 250/1000 | Loss: 0.00003127
Iteration 251/1000 | Loss: 0.00003127
Iteration 252/1000 | Loss: 0.00003127
Iteration 253/1000 | Loss: 0.00003127
Iteration 254/1000 | Loss: 0.00003127
Iteration 255/1000 | Loss: 0.00003127
Iteration 256/1000 | Loss: 0.00003127
Iteration 257/1000 | Loss: 0.00003127
Iteration 258/1000 | Loss: 0.00003126
Iteration 259/1000 | Loss: 0.00003126
Iteration 260/1000 | Loss: 0.00003126
Iteration 261/1000 | Loss: 0.00003126
Iteration 262/1000 | Loss: 0.00003126
Iteration 263/1000 | Loss: 0.00003126
Iteration 264/1000 | Loss: 0.00003126
Iteration 265/1000 | Loss: 0.00003126
Iteration 266/1000 | Loss: 0.00003126
Iteration 267/1000 | Loss: 0.00003125
Iteration 268/1000 | Loss: 0.00003125
Iteration 269/1000 | Loss: 0.00003125
Iteration 270/1000 | Loss: 0.00003125
Iteration 271/1000 | Loss: 0.00003125
Iteration 272/1000 | Loss: 0.00003125
Iteration 273/1000 | Loss: 0.00003125
Iteration 274/1000 | Loss: 0.00003125
Iteration 275/1000 | Loss: 0.00003125
Iteration 276/1000 | Loss: 0.00003125
Iteration 277/1000 | Loss: 0.00003125
Iteration 278/1000 | Loss: 0.00003125
Iteration 279/1000 | Loss: 0.00003125
Iteration 280/1000 | Loss: 0.00003125
Iteration 281/1000 | Loss: 0.00003125
Iteration 282/1000 | Loss: 0.00003125
Iteration 283/1000 | Loss: 0.00003125
Iteration 284/1000 | Loss: 0.00003125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [3.124525028397329e-05, 3.124525028397329e-05, 3.124525028397329e-05, 3.124525028397329e-05, 3.124525028397329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.124525028397329e-05

Optimization complete. Final v2v error: 4.157502174377441 mm

Highest mean error: 6.721354007720947 mm for frame 7

Lowest mean error: 3.4079959392547607 mm for frame 82

Saving results

Total time: 268.8505291938782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891085
Iteration 2/25 | Loss: 0.00144690
Iteration 3/25 | Loss: 0.00128165
Iteration 4/25 | Loss: 0.00125282
Iteration 5/25 | Loss: 0.00124739
Iteration 6/25 | Loss: 0.00124639
Iteration 7/25 | Loss: 0.00124639
Iteration 8/25 | Loss: 0.00124639
Iteration 9/25 | Loss: 0.00124639
Iteration 10/25 | Loss: 0.00124639
Iteration 11/25 | Loss: 0.00124639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012463935418054461, 0.0012463935418054461, 0.0012463935418054461, 0.0012463935418054461, 0.0012463935418054461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012463935418054461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17957258
Iteration 2/25 | Loss: 0.00133281
Iteration 3/25 | Loss: 0.00133273
Iteration 4/25 | Loss: 0.00133273
Iteration 5/25 | Loss: 0.00133273
Iteration 6/25 | Loss: 0.00133273
Iteration 7/25 | Loss: 0.00133273
Iteration 8/25 | Loss: 0.00133273
Iteration 9/25 | Loss: 0.00133273
Iteration 10/25 | Loss: 0.00133273
Iteration 11/25 | Loss: 0.00133273
Iteration 12/25 | Loss: 0.00133273
Iteration 13/25 | Loss: 0.00133273
Iteration 14/25 | Loss: 0.00133273
Iteration 15/25 | Loss: 0.00133273
Iteration 16/25 | Loss: 0.00133273
Iteration 17/25 | Loss: 0.00133273
Iteration 18/25 | Loss: 0.00133273
Iteration 19/25 | Loss: 0.00133273
Iteration 20/25 | Loss: 0.00133273
Iteration 21/25 | Loss: 0.00133273
Iteration 22/25 | Loss: 0.00133273
Iteration 23/25 | Loss: 0.00133273
Iteration 24/25 | Loss: 0.00133273
Iteration 25/25 | Loss: 0.00133273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133273
Iteration 2/1000 | Loss: 0.00006044
Iteration 3/1000 | Loss: 0.00003906
Iteration 4/1000 | Loss: 0.00003372
Iteration 5/1000 | Loss: 0.00003226
Iteration 6/1000 | Loss: 0.00003058
Iteration 7/1000 | Loss: 0.00002952
Iteration 8/1000 | Loss: 0.00002829
Iteration 9/1000 | Loss: 0.00002763
Iteration 10/1000 | Loss: 0.00002720
Iteration 11/1000 | Loss: 0.00002696
Iteration 12/1000 | Loss: 0.00002681
Iteration 13/1000 | Loss: 0.00002675
Iteration 14/1000 | Loss: 0.00002675
Iteration 15/1000 | Loss: 0.00002675
Iteration 16/1000 | Loss: 0.00002675
Iteration 17/1000 | Loss: 0.00002674
Iteration 18/1000 | Loss: 0.00002674
Iteration 19/1000 | Loss: 0.00002672
Iteration 20/1000 | Loss: 0.00002671
Iteration 21/1000 | Loss: 0.00002671
Iteration 22/1000 | Loss: 0.00002671
Iteration 23/1000 | Loss: 0.00002670
Iteration 24/1000 | Loss: 0.00002667
Iteration 25/1000 | Loss: 0.00002666
Iteration 26/1000 | Loss: 0.00002665
Iteration 27/1000 | Loss: 0.00002664
Iteration 28/1000 | Loss: 0.00002664
Iteration 29/1000 | Loss: 0.00002663
Iteration 30/1000 | Loss: 0.00002660
Iteration 31/1000 | Loss: 0.00002656
Iteration 32/1000 | Loss: 0.00002656
Iteration 33/1000 | Loss: 0.00002656
Iteration 34/1000 | Loss: 0.00002654
Iteration 35/1000 | Loss: 0.00002654
Iteration 36/1000 | Loss: 0.00002652
Iteration 37/1000 | Loss: 0.00002652
Iteration 38/1000 | Loss: 0.00002652
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002651
Iteration 41/1000 | Loss: 0.00002651
Iteration 42/1000 | Loss: 0.00002650
Iteration 43/1000 | Loss: 0.00002650
Iteration 44/1000 | Loss: 0.00002650
Iteration 45/1000 | Loss: 0.00002649
Iteration 46/1000 | Loss: 0.00002649
Iteration 47/1000 | Loss: 0.00002649
Iteration 48/1000 | Loss: 0.00002648
Iteration 49/1000 | Loss: 0.00002648
Iteration 50/1000 | Loss: 0.00002648
Iteration 51/1000 | Loss: 0.00002648
Iteration 52/1000 | Loss: 0.00002647
Iteration 53/1000 | Loss: 0.00002647
Iteration 54/1000 | Loss: 0.00002647
Iteration 55/1000 | Loss: 0.00002647
Iteration 56/1000 | Loss: 0.00002647
Iteration 57/1000 | Loss: 0.00002647
Iteration 58/1000 | Loss: 0.00002647
Iteration 59/1000 | Loss: 0.00002646
Iteration 60/1000 | Loss: 0.00002646
Iteration 61/1000 | Loss: 0.00002645
Iteration 62/1000 | Loss: 0.00002645
Iteration 63/1000 | Loss: 0.00002645
Iteration 64/1000 | Loss: 0.00002645
Iteration 65/1000 | Loss: 0.00002645
Iteration 66/1000 | Loss: 0.00002644
Iteration 67/1000 | Loss: 0.00002644
Iteration 68/1000 | Loss: 0.00002644
Iteration 69/1000 | Loss: 0.00002644
Iteration 70/1000 | Loss: 0.00002644
Iteration 71/1000 | Loss: 0.00002643
Iteration 72/1000 | Loss: 0.00002643
Iteration 73/1000 | Loss: 0.00002643
Iteration 74/1000 | Loss: 0.00002643
Iteration 75/1000 | Loss: 0.00002643
Iteration 76/1000 | Loss: 0.00002643
Iteration 77/1000 | Loss: 0.00002642
Iteration 78/1000 | Loss: 0.00002642
Iteration 79/1000 | Loss: 0.00002642
Iteration 80/1000 | Loss: 0.00002642
Iteration 81/1000 | Loss: 0.00002642
Iteration 82/1000 | Loss: 0.00002642
Iteration 83/1000 | Loss: 0.00002641
Iteration 84/1000 | Loss: 0.00002641
Iteration 85/1000 | Loss: 0.00002641
Iteration 86/1000 | Loss: 0.00002641
Iteration 87/1000 | Loss: 0.00002641
Iteration 88/1000 | Loss: 0.00002640
Iteration 89/1000 | Loss: 0.00002640
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00002640
Iteration 92/1000 | Loss: 0.00002640
Iteration 93/1000 | Loss: 0.00002640
Iteration 94/1000 | Loss: 0.00002639
Iteration 95/1000 | Loss: 0.00002639
Iteration 96/1000 | Loss: 0.00002639
Iteration 97/1000 | Loss: 0.00002638
Iteration 98/1000 | Loss: 0.00002638
Iteration 99/1000 | Loss: 0.00002638
Iteration 100/1000 | Loss: 0.00002638
Iteration 101/1000 | Loss: 0.00002637
Iteration 102/1000 | Loss: 0.00002637
Iteration 103/1000 | Loss: 0.00002637
Iteration 104/1000 | Loss: 0.00002637
Iteration 105/1000 | Loss: 0.00002637
Iteration 106/1000 | Loss: 0.00002637
Iteration 107/1000 | Loss: 0.00002637
Iteration 108/1000 | Loss: 0.00002637
Iteration 109/1000 | Loss: 0.00002636
Iteration 110/1000 | Loss: 0.00002636
Iteration 111/1000 | Loss: 0.00002636
Iteration 112/1000 | Loss: 0.00002636
Iteration 113/1000 | Loss: 0.00002636
Iteration 114/1000 | Loss: 0.00002636
Iteration 115/1000 | Loss: 0.00002636
Iteration 116/1000 | Loss: 0.00002636
Iteration 117/1000 | Loss: 0.00002635
Iteration 118/1000 | Loss: 0.00002635
Iteration 119/1000 | Loss: 0.00002635
Iteration 120/1000 | Loss: 0.00002635
Iteration 121/1000 | Loss: 0.00002635
Iteration 122/1000 | Loss: 0.00002635
Iteration 123/1000 | Loss: 0.00002635
Iteration 124/1000 | Loss: 0.00002635
Iteration 125/1000 | Loss: 0.00002635
Iteration 126/1000 | Loss: 0.00002635
Iteration 127/1000 | Loss: 0.00002635
Iteration 128/1000 | Loss: 0.00002635
Iteration 129/1000 | Loss: 0.00002635
Iteration 130/1000 | Loss: 0.00002635
Iteration 131/1000 | Loss: 0.00002635
Iteration 132/1000 | Loss: 0.00002635
Iteration 133/1000 | Loss: 0.00002635
Iteration 134/1000 | Loss: 0.00002635
Iteration 135/1000 | Loss: 0.00002635
Iteration 136/1000 | Loss: 0.00002635
Iteration 137/1000 | Loss: 0.00002635
Iteration 138/1000 | Loss: 0.00002635
Iteration 139/1000 | Loss: 0.00002635
Iteration 140/1000 | Loss: 0.00002635
Iteration 141/1000 | Loss: 0.00002635
Iteration 142/1000 | Loss: 0.00002635
Iteration 143/1000 | Loss: 0.00002635
Iteration 144/1000 | Loss: 0.00002635
Iteration 145/1000 | Loss: 0.00002635
Iteration 146/1000 | Loss: 0.00002635
Iteration 147/1000 | Loss: 0.00002635
Iteration 148/1000 | Loss: 0.00002635
Iteration 149/1000 | Loss: 0.00002635
Iteration 150/1000 | Loss: 0.00002635
Iteration 151/1000 | Loss: 0.00002635
Iteration 152/1000 | Loss: 0.00002635
Iteration 153/1000 | Loss: 0.00002635
Iteration 154/1000 | Loss: 0.00002635
Iteration 155/1000 | Loss: 0.00002635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.635193959577009e-05, 2.635193959577009e-05, 2.635193959577009e-05, 2.635193959577009e-05, 2.635193959577009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.635193959577009e-05

Optimization complete. Final v2v error: 4.376779079437256 mm

Highest mean error: 4.6958465576171875 mm for frame 19

Lowest mean error: 3.948320150375366 mm for frame 119

Saving results

Total time: 36.433826208114624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837437
Iteration 2/25 | Loss: 0.00133717
Iteration 3/25 | Loss: 0.00115209
Iteration 4/25 | Loss: 0.00112397
Iteration 5/25 | Loss: 0.00112015
Iteration 6/25 | Loss: 0.00111982
Iteration 7/25 | Loss: 0.00111982
Iteration 8/25 | Loss: 0.00111982
Iteration 9/25 | Loss: 0.00111982
Iteration 10/25 | Loss: 0.00111982
Iteration 11/25 | Loss: 0.00111982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00111981644295156, 0.00111981644295156, 0.00111981644295156, 0.00111981644295156, 0.00111981644295156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00111981644295156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21580052
Iteration 2/25 | Loss: 0.00157509
Iteration 3/25 | Loss: 0.00157509
Iteration 4/25 | Loss: 0.00157509
Iteration 5/25 | Loss: 0.00157508
Iteration 6/25 | Loss: 0.00157508
Iteration 7/25 | Loss: 0.00157508
Iteration 8/25 | Loss: 0.00157508
Iteration 9/25 | Loss: 0.00157508
Iteration 10/25 | Loss: 0.00157508
Iteration 11/25 | Loss: 0.00157508
Iteration 12/25 | Loss: 0.00157508
Iteration 13/25 | Loss: 0.00157508
Iteration 14/25 | Loss: 0.00157508
Iteration 15/25 | Loss: 0.00157508
Iteration 16/25 | Loss: 0.00157508
Iteration 17/25 | Loss: 0.00157508
Iteration 18/25 | Loss: 0.00157508
Iteration 19/25 | Loss: 0.00157508
Iteration 20/25 | Loss: 0.00157508
Iteration 21/25 | Loss: 0.00157508
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015750839374959469, 0.0015750839374959469, 0.0015750839374959469, 0.0015750839374959469, 0.0015750839374959469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015750839374959469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157508
Iteration 2/1000 | Loss: 0.00003010
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001456
Iteration 6/1000 | Loss: 0.00001393
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001297
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001271
Iteration 13/1000 | Loss: 0.00001270
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001263
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001258
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001246
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001241
Iteration 38/1000 | Loss: 0.00001240
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001237
Iteration 47/1000 | Loss: 0.00001237
Iteration 48/1000 | Loss: 0.00001237
Iteration 49/1000 | Loss: 0.00001236
Iteration 50/1000 | Loss: 0.00001236
Iteration 51/1000 | Loss: 0.00001236
Iteration 52/1000 | Loss: 0.00001235
Iteration 53/1000 | Loss: 0.00001235
Iteration 54/1000 | Loss: 0.00001234
Iteration 55/1000 | Loss: 0.00001234
Iteration 56/1000 | Loss: 0.00001234
Iteration 57/1000 | Loss: 0.00001234
Iteration 58/1000 | Loss: 0.00001234
Iteration 59/1000 | Loss: 0.00001234
Iteration 60/1000 | Loss: 0.00001234
Iteration 61/1000 | Loss: 0.00001233
Iteration 62/1000 | Loss: 0.00001233
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001232
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.2309988960623741e-05, 1.2309988960623741e-05, 1.2309988960623741e-05, 1.2309988960623741e-05, 1.2309988960623741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2309988960623741e-05

Optimization complete. Final v2v error: 3.0111241340637207 mm

Highest mean error: 3.3289949893951416 mm for frame 132

Lowest mean error: 2.8300461769104004 mm for frame 2

Saving results

Total time: 34.494678020477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811577
Iteration 2/25 | Loss: 0.00137047
Iteration 3/25 | Loss: 0.00118427
Iteration 4/25 | Loss: 0.00115651
Iteration 5/25 | Loss: 0.00114938
Iteration 6/25 | Loss: 0.00114732
Iteration 7/25 | Loss: 0.00114731
Iteration 8/25 | Loss: 0.00114731
Iteration 9/25 | Loss: 0.00114731
Iteration 10/25 | Loss: 0.00114731
Iteration 11/25 | Loss: 0.00114731
Iteration 12/25 | Loss: 0.00114731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001147305709309876, 0.001147305709309876, 0.001147305709309876, 0.001147305709309876, 0.001147305709309876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001147305709309876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26464319
Iteration 2/25 | Loss: 0.00186920
Iteration 3/25 | Loss: 0.00186920
Iteration 4/25 | Loss: 0.00186920
Iteration 5/25 | Loss: 0.00186920
Iteration 6/25 | Loss: 0.00186920
Iteration 7/25 | Loss: 0.00186920
Iteration 8/25 | Loss: 0.00186920
Iteration 9/25 | Loss: 0.00186920
Iteration 10/25 | Loss: 0.00186920
Iteration 11/25 | Loss: 0.00186920
Iteration 12/25 | Loss: 0.00186920
Iteration 13/25 | Loss: 0.00186920
Iteration 14/25 | Loss: 0.00186920
Iteration 15/25 | Loss: 0.00186920
Iteration 16/25 | Loss: 0.00186920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0018692006124183536, 0.0018692006124183536, 0.0018692006124183536, 0.0018692006124183536, 0.0018692006124183536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018692006124183536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186920
Iteration 2/1000 | Loss: 0.00004615
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001849
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001717
Iteration 10/1000 | Loss: 0.00001680
Iteration 11/1000 | Loss: 0.00001640
Iteration 12/1000 | Loss: 0.00001616
Iteration 13/1000 | Loss: 0.00001606
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001601
Iteration 16/1000 | Loss: 0.00001596
Iteration 17/1000 | Loss: 0.00001589
Iteration 18/1000 | Loss: 0.00001589
Iteration 19/1000 | Loss: 0.00001585
Iteration 20/1000 | Loss: 0.00001585
Iteration 21/1000 | Loss: 0.00001584
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001580
Iteration 25/1000 | Loss: 0.00001580
Iteration 26/1000 | Loss: 0.00001579
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001577
Iteration 31/1000 | Loss: 0.00001577
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001575
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001572
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001571
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001569
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001565
Iteration 84/1000 | Loss: 0.00001565
Iteration 85/1000 | Loss: 0.00001565
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001562
Iteration 95/1000 | Loss: 0.00001562
Iteration 96/1000 | Loss: 0.00001562
Iteration 97/1000 | Loss: 0.00001562
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001562
Iteration 102/1000 | Loss: 0.00001562
Iteration 103/1000 | Loss: 0.00001561
Iteration 104/1000 | Loss: 0.00001561
Iteration 105/1000 | Loss: 0.00001561
Iteration 106/1000 | Loss: 0.00001561
Iteration 107/1000 | Loss: 0.00001561
Iteration 108/1000 | Loss: 0.00001560
Iteration 109/1000 | Loss: 0.00001560
Iteration 110/1000 | Loss: 0.00001560
Iteration 111/1000 | Loss: 0.00001560
Iteration 112/1000 | Loss: 0.00001560
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001559
Iteration 117/1000 | Loss: 0.00001559
Iteration 118/1000 | Loss: 0.00001559
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001559
Iteration 128/1000 | Loss: 0.00001559
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001556
Iteration 139/1000 | Loss: 0.00001556
Iteration 140/1000 | Loss: 0.00001556
Iteration 141/1000 | Loss: 0.00001556
Iteration 142/1000 | Loss: 0.00001556
Iteration 143/1000 | Loss: 0.00001556
Iteration 144/1000 | Loss: 0.00001555
Iteration 145/1000 | Loss: 0.00001555
Iteration 146/1000 | Loss: 0.00001555
Iteration 147/1000 | Loss: 0.00001555
Iteration 148/1000 | Loss: 0.00001555
Iteration 149/1000 | Loss: 0.00001555
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001555
Iteration 156/1000 | Loss: 0.00001555
Iteration 157/1000 | Loss: 0.00001555
Iteration 158/1000 | Loss: 0.00001555
Iteration 159/1000 | Loss: 0.00001555
Iteration 160/1000 | Loss: 0.00001555
Iteration 161/1000 | Loss: 0.00001555
Iteration 162/1000 | Loss: 0.00001555
Iteration 163/1000 | Loss: 0.00001555
Iteration 164/1000 | Loss: 0.00001555
Iteration 165/1000 | Loss: 0.00001555
Iteration 166/1000 | Loss: 0.00001555
Iteration 167/1000 | Loss: 0.00001555
Iteration 168/1000 | Loss: 0.00001555
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Iteration 172/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.554745358589571e-05, 1.554745358589571e-05, 1.554745358589571e-05, 1.554745358589571e-05, 1.554745358589571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.554745358589571e-05

Optimization complete. Final v2v error: 3.468639612197876 mm

Highest mean error: 3.867316246032715 mm for frame 132

Lowest mean error: 2.9283154010772705 mm for frame 169

Saving results

Total time: 43.90075635910034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492295
Iteration 2/25 | Loss: 0.00122952
Iteration 3/25 | Loss: 0.00112231
Iteration 4/25 | Loss: 0.00110753
Iteration 5/25 | Loss: 0.00110264
Iteration 6/25 | Loss: 0.00110164
Iteration 7/25 | Loss: 0.00110164
Iteration 8/25 | Loss: 0.00110164
Iteration 9/25 | Loss: 0.00110164
Iteration 10/25 | Loss: 0.00110164
Iteration 11/25 | Loss: 0.00110164
Iteration 12/25 | Loss: 0.00110164
Iteration 13/25 | Loss: 0.00110164
Iteration 14/25 | Loss: 0.00110164
Iteration 15/25 | Loss: 0.00110164
Iteration 16/25 | Loss: 0.00110164
Iteration 17/25 | Loss: 0.00110164
Iteration 18/25 | Loss: 0.00110164
Iteration 19/25 | Loss: 0.00110164
Iteration 20/25 | Loss: 0.00110164
Iteration 21/25 | Loss: 0.00110164
Iteration 22/25 | Loss: 0.00110164
Iteration 23/25 | Loss: 0.00110164
Iteration 24/25 | Loss: 0.00110164
Iteration 25/25 | Loss: 0.00110164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29947436
Iteration 2/25 | Loss: 0.00139927
Iteration 3/25 | Loss: 0.00139926
Iteration 4/25 | Loss: 0.00139926
Iteration 5/25 | Loss: 0.00139926
Iteration 6/25 | Loss: 0.00139926
Iteration 7/25 | Loss: 0.00139926
Iteration 8/25 | Loss: 0.00139926
Iteration 9/25 | Loss: 0.00139926
Iteration 10/25 | Loss: 0.00139926
Iteration 11/25 | Loss: 0.00139926
Iteration 12/25 | Loss: 0.00139926
Iteration 13/25 | Loss: 0.00139926
Iteration 14/25 | Loss: 0.00139926
Iteration 15/25 | Loss: 0.00139926
Iteration 16/25 | Loss: 0.00139926
Iteration 17/25 | Loss: 0.00139926
Iteration 18/25 | Loss: 0.00139926
Iteration 19/25 | Loss: 0.00139926
Iteration 20/25 | Loss: 0.00139926
Iteration 21/25 | Loss: 0.00139926
Iteration 22/25 | Loss: 0.00139926
Iteration 23/25 | Loss: 0.00139926
Iteration 24/25 | Loss: 0.00139926
Iteration 25/25 | Loss: 0.00139926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013992582680657506, 0.0013992582680657506, 0.0013992582680657506, 0.0013992582680657506, 0.0013992582680657506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013992582680657506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139926
Iteration 2/1000 | Loss: 0.00002704
Iteration 3/1000 | Loss: 0.00001721
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001427
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001339
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001286
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001278
Iteration 14/1000 | Loss: 0.00001278
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001266
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001264
Iteration 30/1000 | Loss: 0.00001260
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001255
Iteration 40/1000 | Loss: 0.00001255
Iteration 41/1000 | Loss: 0.00001255
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001254
Iteration 46/1000 | Loss: 0.00001254
Iteration 47/1000 | Loss: 0.00001254
Iteration 48/1000 | Loss: 0.00001254
Iteration 49/1000 | Loss: 0.00001254
Iteration 50/1000 | Loss: 0.00001254
Iteration 51/1000 | Loss: 0.00001254
Iteration 52/1000 | Loss: 0.00001254
Iteration 53/1000 | Loss: 0.00001254
Iteration 54/1000 | Loss: 0.00001253
Iteration 55/1000 | Loss: 0.00001252
Iteration 56/1000 | Loss: 0.00001251
Iteration 57/1000 | Loss: 0.00001251
Iteration 58/1000 | Loss: 0.00001251
Iteration 59/1000 | Loss: 0.00001251
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001250
Iteration 62/1000 | Loss: 0.00001250
Iteration 63/1000 | Loss: 0.00001250
Iteration 64/1000 | Loss: 0.00001250
Iteration 65/1000 | Loss: 0.00001250
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001249
Iteration 69/1000 | Loss: 0.00001249
Iteration 70/1000 | Loss: 0.00001248
Iteration 71/1000 | Loss: 0.00001248
Iteration 72/1000 | Loss: 0.00001248
Iteration 73/1000 | Loss: 0.00001247
Iteration 74/1000 | Loss: 0.00001247
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001247
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001246
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001246
Iteration 85/1000 | Loss: 0.00001245
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001245
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001244
Iteration 93/1000 | Loss: 0.00001244
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001243
Iteration 98/1000 | Loss: 0.00001243
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001242
Iteration 101/1000 | Loss: 0.00001242
Iteration 102/1000 | Loss: 0.00001242
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001242
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001240
Iteration 118/1000 | Loss: 0.00001240
Iteration 119/1000 | Loss: 0.00001240
Iteration 120/1000 | Loss: 0.00001240
Iteration 121/1000 | Loss: 0.00001240
Iteration 122/1000 | Loss: 0.00001240
Iteration 123/1000 | Loss: 0.00001239
Iteration 124/1000 | Loss: 0.00001239
Iteration 125/1000 | Loss: 0.00001239
Iteration 126/1000 | Loss: 0.00001239
Iteration 127/1000 | Loss: 0.00001239
Iteration 128/1000 | Loss: 0.00001239
Iteration 129/1000 | Loss: 0.00001239
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001239
Iteration 147/1000 | Loss: 0.00001239
Iteration 148/1000 | Loss: 0.00001239
Iteration 149/1000 | Loss: 0.00001239
Iteration 150/1000 | Loss: 0.00001239
Iteration 151/1000 | Loss: 0.00001239
Iteration 152/1000 | Loss: 0.00001239
Iteration 153/1000 | Loss: 0.00001239
Iteration 154/1000 | Loss: 0.00001239
Iteration 155/1000 | Loss: 0.00001239
Iteration 156/1000 | Loss: 0.00001239
Iteration 157/1000 | Loss: 0.00001239
Iteration 158/1000 | Loss: 0.00001239
Iteration 159/1000 | Loss: 0.00001239
Iteration 160/1000 | Loss: 0.00001239
Iteration 161/1000 | Loss: 0.00001239
Iteration 162/1000 | Loss: 0.00001239
Iteration 163/1000 | Loss: 0.00001239
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001239
Iteration 170/1000 | Loss: 0.00001239
Iteration 171/1000 | Loss: 0.00001239
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001239
Iteration 182/1000 | Loss: 0.00001239
Iteration 183/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.2388682080199942e-05, 1.2388682080199942e-05, 1.2388682080199942e-05, 1.2388682080199942e-05, 1.2388682080199942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2388682080199942e-05

Optimization complete. Final v2v error: 3.10969877243042 mm

Highest mean error: 3.4265081882476807 mm for frame 97

Lowest mean error: 2.643399238586426 mm for frame 219

Saving results

Total time: 38.62199854850769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121849
Iteration 2/25 | Loss: 0.01121849
Iteration 3/25 | Loss: 0.01121849
Iteration 4/25 | Loss: 0.01121849
Iteration 5/25 | Loss: 0.01121849
Iteration 6/25 | Loss: 0.01121849
Iteration 7/25 | Loss: 0.01121849
Iteration 8/25 | Loss: 0.01121849
Iteration 9/25 | Loss: 0.01121849
Iteration 10/25 | Loss: 0.01121849
Iteration 11/25 | Loss: 0.01121848
Iteration 12/25 | Loss: 0.01121848
Iteration 13/25 | Loss: 0.01121848
Iteration 14/25 | Loss: 0.01121848
Iteration 15/25 | Loss: 0.01121848
Iteration 16/25 | Loss: 0.01121848
Iteration 17/25 | Loss: 0.01121848
Iteration 18/25 | Loss: 0.01121848
Iteration 19/25 | Loss: 0.01121848
Iteration 20/25 | Loss: 0.01121848
Iteration 21/25 | Loss: 0.01121848
Iteration 22/25 | Loss: 0.01121847
Iteration 23/25 | Loss: 0.01121847
Iteration 24/25 | Loss: 0.01121847
Iteration 25/25 | Loss: 0.01121847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.51218414
Iteration 2/25 | Loss: 0.19941330
Iteration 3/25 | Loss: 0.19370928
Iteration 4/25 | Loss: 0.19253764
Iteration 5/25 | Loss: 0.19225122
Iteration 6/25 | Loss: 0.19225094
Iteration 7/25 | Loss: 0.19225092
Iteration 8/25 | Loss: 0.19225092
Iteration 9/25 | Loss: 0.19225092
Iteration 10/25 | Loss: 0.19225092
Iteration 11/25 | Loss: 0.19225091
Iteration 12/25 | Loss: 0.19225088
Iteration 13/25 | Loss: 0.19225091
Iteration 14/25 | Loss: 0.19225091
Iteration 15/25 | Loss: 0.19225088
Iteration 16/25 | Loss: 0.19225088
Iteration 17/25 | Loss: 0.19225088
Iteration 18/25 | Loss: 0.19225088
Iteration 19/25 | Loss: 0.19225088
Iteration 20/25 | Loss: 0.19225088
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.19225087761878967, 0.19225087761878967, 0.19225087761878967, 0.19225087761878967, 0.19225087761878967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.19225087761878967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.19225088
Iteration 2/1000 | Loss: 0.00265719
Iteration 3/1000 | Loss: 0.00101896
Iteration 4/1000 | Loss: 0.00042232
Iteration 5/1000 | Loss: 0.00022637
Iteration 6/1000 | Loss: 0.00011304
Iteration 7/1000 | Loss: 0.00016799
Iteration 8/1000 | Loss: 0.00007807
Iteration 9/1000 | Loss: 0.00008051
Iteration 10/1000 | Loss: 0.00008672
Iteration 11/1000 | Loss: 0.00006012
Iteration 12/1000 | Loss: 0.00006838
Iteration 13/1000 | Loss: 0.00004050
Iteration 14/1000 | Loss: 0.00003617
Iteration 15/1000 | Loss: 0.00005218
Iteration 16/1000 | Loss: 0.00007419
Iteration 17/1000 | Loss: 0.00004150
Iteration 18/1000 | Loss: 0.00003796
Iteration 19/1000 | Loss: 0.00005287
Iteration 20/1000 | Loss: 0.00005497
Iteration 21/1000 | Loss: 0.00009398
Iteration 22/1000 | Loss: 0.00005919
Iteration 23/1000 | Loss: 0.00004881
Iteration 24/1000 | Loss: 0.00003962
Iteration 25/1000 | Loss: 0.00003270
Iteration 26/1000 | Loss: 0.00005434
Iteration 27/1000 | Loss: 0.00004689
Iteration 28/1000 | Loss: 0.00004020
Iteration 29/1000 | Loss: 0.00002868
Iteration 30/1000 | Loss: 0.00002911
Iteration 31/1000 | Loss: 0.00002887
Iteration 32/1000 | Loss: 0.00002884
Iteration 33/1000 | Loss: 0.00002665
Iteration 34/1000 | Loss: 0.00002602
Iteration 35/1000 | Loss: 0.00006028
Iteration 36/1000 | Loss: 0.00003068
Iteration 37/1000 | Loss: 0.00005391
Iteration 38/1000 | Loss: 0.00002803
Iteration 39/1000 | Loss: 0.00002953
Iteration 40/1000 | Loss: 0.00003051
Iteration 41/1000 | Loss: 0.00003931
Iteration 42/1000 | Loss: 0.00003120
Iteration 43/1000 | Loss: 0.00006269
Iteration 44/1000 | Loss: 0.00008443
Iteration 45/1000 | Loss: 0.00005235
Iteration 46/1000 | Loss: 0.00003939
Iteration 47/1000 | Loss: 0.00004604
Iteration 48/1000 | Loss: 0.00003195
Iteration 49/1000 | Loss: 0.00003274
Iteration 50/1000 | Loss: 0.00003174
Iteration 51/1000 | Loss: 0.00003246
Iteration 52/1000 | Loss: 0.00003769
Iteration 53/1000 | Loss: 0.00003533
Iteration 54/1000 | Loss: 0.00007642
Iteration 55/1000 | Loss: 0.00029454
Iteration 56/1000 | Loss: 0.00004243
Iteration 57/1000 | Loss: 0.00003246
Iteration 58/1000 | Loss: 0.00002712
Iteration 59/1000 | Loss: 0.00002588
Iteration 60/1000 | Loss: 0.00002541
Iteration 61/1000 | Loss: 0.00004150
Iteration 62/1000 | Loss: 0.00003976
Iteration 63/1000 | Loss: 0.00002672
Iteration 64/1000 | Loss: 0.00003571
Iteration 65/1000 | Loss: 0.00002420
Iteration 66/1000 | Loss: 0.00002417
Iteration 67/1000 | Loss: 0.00002413
Iteration 68/1000 | Loss: 0.00002684
Iteration 69/1000 | Loss: 0.00003315
Iteration 70/1000 | Loss: 0.00002557
Iteration 71/1000 | Loss: 0.00005013
Iteration 72/1000 | Loss: 0.00002543
Iteration 73/1000 | Loss: 0.00003904
Iteration 74/1000 | Loss: 0.00003601
Iteration 75/1000 | Loss: 0.00003875
Iteration 76/1000 | Loss: 0.00003808
Iteration 77/1000 | Loss: 0.00003794
Iteration 78/1000 | Loss: 0.00004068
Iteration 79/1000 | Loss: 0.00002771
Iteration 80/1000 | Loss: 0.00002753
Iteration 81/1000 | Loss: 0.00003182
Iteration 82/1000 | Loss: 0.00002821
Iteration 83/1000 | Loss: 0.00002881
Iteration 84/1000 | Loss: 0.00002547
Iteration 85/1000 | Loss: 0.00002761
Iteration 86/1000 | Loss: 0.00002887
Iteration 87/1000 | Loss: 0.00002882
Iteration 88/1000 | Loss: 0.00004964
Iteration 89/1000 | Loss: 0.00003004
Iteration 90/1000 | Loss: 0.00003748
Iteration 91/1000 | Loss: 0.00002954
Iteration 92/1000 | Loss: 0.00003636
Iteration 93/1000 | Loss: 0.00002783
Iteration 94/1000 | Loss: 0.00002831
Iteration 95/1000 | Loss: 0.00002761
Iteration 96/1000 | Loss: 0.00003335
Iteration 97/1000 | Loss: 0.00003335
Iteration 98/1000 | Loss: 0.00009982
Iteration 99/1000 | Loss: 0.00002647
Iteration 100/1000 | Loss: 0.00005610
Iteration 101/1000 | Loss: 0.00003388
Iteration 102/1000 | Loss: 0.00003787
Iteration 103/1000 | Loss: 0.00002401
Iteration 104/1000 | Loss: 0.00002400
Iteration 105/1000 | Loss: 0.00002399
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00003400
Iteration 108/1000 | Loss: 0.00002653
Iteration 109/1000 | Loss: 0.00002389
Iteration 110/1000 | Loss: 0.00002387
Iteration 111/1000 | Loss: 0.00002386
Iteration 112/1000 | Loss: 0.00002386
Iteration 113/1000 | Loss: 0.00002386
Iteration 114/1000 | Loss: 0.00002385
Iteration 115/1000 | Loss: 0.00002385
Iteration 116/1000 | Loss: 0.00002385
Iteration 117/1000 | Loss: 0.00002385
Iteration 118/1000 | Loss: 0.00002384
Iteration 119/1000 | Loss: 0.00002383
Iteration 120/1000 | Loss: 0.00002383
Iteration 121/1000 | Loss: 0.00002382
Iteration 122/1000 | Loss: 0.00002381
Iteration 123/1000 | Loss: 0.00002381
Iteration 124/1000 | Loss: 0.00002381
Iteration 125/1000 | Loss: 0.00002381
Iteration 126/1000 | Loss: 0.00002381
Iteration 127/1000 | Loss: 0.00002381
Iteration 128/1000 | Loss: 0.00002381
Iteration 129/1000 | Loss: 0.00002381
Iteration 130/1000 | Loss: 0.00002381
Iteration 131/1000 | Loss: 0.00002381
Iteration 132/1000 | Loss: 0.00002381
Iteration 133/1000 | Loss: 0.00002381
Iteration 134/1000 | Loss: 0.00002381
Iteration 135/1000 | Loss: 0.00002381
Iteration 136/1000 | Loss: 0.00002380
Iteration 137/1000 | Loss: 0.00002380
Iteration 138/1000 | Loss: 0.00002380
Iteration 139/1000 | Loss: 0.00002380
Iteration 140/1000 | Loss: 0.00002380
Iteration 141/1000 | Loss: 0.00002380
Iteration 142/1000 | Loss: 0.00002380
Iteration 143/1000 | Loss: 0.00002380
Iteration 144/1000 | Loss: 0.00002380
Iteration 145/1000 | Loss: 0.00002379
Iteration 146/1000 | Loss: 0.00002379
Iteration 147/1000 | Loss: 0.00002379
Iteration 148/1000 | Loss: 0.00002379
Iteration 149/1000 | Loss: 0.00002379
Iteration 150/1000 | Loss: 0.00002379
Iteration 151/1000 | Loss: 0.00002379
Iteration 152/1000 | Loss: 0.00002379
Iteration 153/1000 | Loss: 0.00002379
Iteration 154/1000 | Loss: 0.00002379
Iteration 155/1000 | Loss: 0.00002379
Iteration 156/1000 | Loss: 0.00002379
Iteration 157/1000 | Loss: 0.00002379
Iteration 158/1000 | Loss: 0.00002378
Iteration 159/1000 | Loss: 0.00002378
Iteration 160/1000 | Loss: 0.00002378
Iteration 161/1000 | Loss: 0.00002378
Iteration 162/1000 | Loss: 0.00002378
Iteration 163/1000 | Loss: 0.00002378
Iteration 164/1000 | Loss: 0.00002378
Iteration 165/1000 | Loss: 0.00002378
Iteration 166/1000 | Loss: 0.00002378
Iteration 167/1000 | Loss: 0.00002378
Iteration 168/1000 | Loss: 0.00002378
Iteration 169/1000 | Loss: 0.00002377
Iteration 170/1000 | Loss: 0.00002377
Iteration 171/1000 | Loss: 0.00002377
Iteration 172/1000 | Loss: 0.00002377
Iteration 173/1000 | Loss: 0.00002377
Iteration 174/1000 | Loss: 0.00002377
Iteration 175/1000 | Loss: 0.00002377
Iteration 176/1000 | Loss: 0.00002377
Iteration 177/1000 | Loss: 0.00002377
Iteration 178/1000 | Loss: 0.00002377
Iteration 179/1000 | Loss: 0.00002377
Iteration 180/1000 | Loss: 0.00002377
Iteration 181/1000 | Loss: 0.00002377
Iteration 182/1000 | Loss: 0.00002376
Iteration 183/1000 | Loss: 0.00003717
Iteration 184/1000 | Loss: 0.00003813
Iteration 185/1000 | Loss: 0.00002373
Iteration 186/1000 | Loss: 0.00002373
Iteration 187/1000 | Loss: 0.00002373
Iteration 188/1000 | Loss: 0.00002373
Iteration 189/1000 | Loss: 0.00002373
Iteration 190/1000 | Loss: 0.00002372
Iteration 191/1000 | Loss: 0.00002506
Iteration 192/1000 | Loss: 0.00002372
Iteration 193/1000 | Loss: 0.00002372
Iteration 194/1000 | Loss: 0.00002372
Iteration 195/1000 | Loss: 0.00002372
Iteration 196/1000 | Loss: 0.00002372
Iteration 197/1000 | Loss: 0.00002372
Iteration 198/1000 | Loss: 0.00002372
Iteration 199/1000 | Loss: 0.00002372
Iteration 200/1000 | Loss: 0.00002372
Iteration 201/1000 | Loss: 0.00002372
Iteration 202/1000 | Loss: 0.00002372
Iteration 203/1000 | Loss: 0.00002372
Iteration 204/1000 | Loss: 0.00002372
Iteration 205/1000 | Loss: 0.00002372
Iteration 206/1000 | Loss: 0.00002372
Iteration 207/1000 | Loss: 0.00002372
Iteration 208/1000 | Loss: 0.00002372
Iteration 209/1000 | Loss: 0.00002372
Iteration 210/1000 | Loss: 0.00002371
Iteration 211/1000 | Loss: 0.00002371
Iteration 212/1000 | Loss: 0.00002371
Iteration 213/1000 | Loss: 0.00002371
Iteration 214/1000 | Loss: 0.00002371
Iteration 215/1000 | Loss: 0.00002371
Iteration 216/1000 | Loss: 0.00002371
Iteration 217/1000 | Loss: 0.00002371
Iteration 218/1000 | Loss: 0.00002371
Iteration 219/1000 | Loss: 0.00002371
Iteration 220/1000 | Loss: 0.00002371
Iteration 221/1000 | Loss: 0.00002371
Iteration 222/1000 | Loss: 0.00002371
Iteration 223/1000 | Loss: 0.00002371
Iteration 224/1000 | Loss: 0.00002371
Iteration 225/1000 | Loss: 0.00002371
Iteration 226/1000 | Loss: 0.00002371
Iteration 227/1000 | Loss: 0.00002371
Iteration 228/1000 | Loss: 0.00002371
Iteration 229/1000 | Loss: 0.00002371
Iteration 230/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [2.371417031099554e-05, 2.371417031099554e-05, 2.371417031099554e-05, 2.371417031099554e-05, 2.371417031099554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.371417031099554e-05

Optimization complete. Final v2v error: 3.738537073135376 mm

Highest mean error: 9.537432670593262 mm for frame 195

Lowest mean error: 3.0365467071533203 mm for frame 235

Saving results

Total time: 181.9688582420349
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854436
Iteration 2/25 | Loss: 0.00149357
Iteration 3/25 | Loss: 0.00127260
Iteration 4/25 | Loss: 0.00123691
Iteration 5/25 | Loss: 0.00121201
Iteration 6/25 | Loss: 0.00119759
Iteration 7/25 | Loss: 0.00120279
Iteration 8/25 | Loss: 0.00119376
Iteration 9/25 | Loss: 0.00118987
Iteration 10/25 | Loss: 0.00119090
Iteration 11/25 | Loss: 0.00119223
Iteration 12/25 | Loss: 0.00118845
Iteration 13/25 | Loss: 0.00118470
Iteration 14/25 | Loss: 0.00118187
Iteration 15/25 | Loss: 0.00118069
Iteration 16/25 | Loss: 0.00118040
Iteration 17/25 | Loss: 0.00118021
Iteration 18/25 | Loss: 0.00118010
Iteration 19/25 | Loss: 0.00118006
Iteration 20/25 | Loss: 0.00118005
Iteration 21/25 | Loss: 0.00118005
Iteration 22/25 | Loss: 0.00118005
Iteration 23/25 | Loss: 0.00118005
Iteration 24/25 | Loss: 0.00118005
Iteration 25/25 | Loss: 0.00118004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67935383
Iteration 2/25 | Loss: 0.00187616
Iteration 3/25 | Loss: 0.00187614
Iteration 4/25 | Loss: 0.00187614
Iteration 5/25 | Loss: 0.00187613
Iteration 6/25 | Loss: 0.00187613
Iteration 7/25 | Loss: 0.00187613
Iteration 8/25 | Loss: 0.00187613
Iteration 9/25 | Loss: 0.00187613
Iteration 10/25 | Loss: 0.00187613
Iteration 11/25 | Loss: 0.00187613
Iteration 12/25 | Loss: 0.00187613
Iteration 13/25 | Loss: 0.00187613
Iteration 14/25 | Loss: 0.00187613
Iteration 15/25 | Loss: 0.00187613
Iteration 16/25 | Loss: 0.00187613
Iteration 17/25 | Loss: 0.00187613
Iteration 18/25 | Loss: 0.00187613
Iteration 19/25 | Loss: 0.00187613
Iteration 20/25 | Loss: 0.00187613
Iteration 21/25 | Loss: 0.00187613
Iteration 22/25 | Loss: 0.00187613
Iteration 23/25 | Loss: 0.00187613
Iteration 24/25 | Loss: 0.00187613
Iteration 25/25 | Loss: 0.00187613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187613
Iteration 2/1000 | Loss: 0.00006711
Iteration 3/1000 | Loss: 0.00006063
Iteration 4/1000 | Loss: 0.00003093
Iteration 5/1000 | Loss: 0.00009645
Iteration 6/1000 | Loss: 0.00002706
Iteration 7/1000 | Loss: 0.00002533
Iteration 8/1000 | Loss: 0.00002395
Iteration 9/1000 | Loss: 0.00002273
Iteration 10/1000 | Loss: 0.00004045
Iteration 11/1000 | Loss: 0.00002124
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00001995
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001949
Iteration 16/1000 | Loss: 0.00001936
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001927
Iteration 23/1000 | Loss: 0.00001927
Iteration 24/1000 | Loss: 0.00001926
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001924
Iteration 27/1000 | Loss: 0.00001924
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00003631
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001921
Iteration 36/1000 | Loss: 0.00001920
Iteration 37/1000 | Loss: 0.00001920
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001918
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001916
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001916
Iteration 55/1000 | Loss: 0.00001916
Iteration 56/1000 | Loss: 0.00001916
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001915
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001915
Iteration 68/1000 | Loss: 0.00001915
Iteration 69/1000 | Loss: 0.00001915
Iteration 70/1000 | Loss: 0.00001915
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001914
Iteration 80/1000 | Loss: 0.00001914
Iteration 81/1000 | Loss: 0.00001914
Iteration 82/1000 | Loss: 0.00001914
Iteration 83/1000 | Loss: 0.00001914
Iteration 84/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.9144126781611703e-05, 1.9144126781611703e-05, 1.9144126781611703e-05, 1.9144126781611703e-05, 1.9144126781611703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9144126781611703e-05

Optimization complete. Final v2v error: 3.4692530632019043 mm

Highest mean error: 13.520466804504395 mm for frame 48

Lowest mean error: 3.179504156112671 mm for frame 36

Saving results

Total time: 58.91221904754639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008090
Iteration 2/25 | Loss: 0.00300921
Iteration 3/25 | Loss: 0.00235246
Iteration 4/25 | Loss: 0.00221558
Iteration 5/25 | Loss: 0.00213075
Iteration 6/25 | Loss: 0.00179640
Iteration 7/25 | Loss: 0.00157207
Iteration 8/25 | Loss: 0.00143502
Iteration 9/25 | Loss: 0.00137113
Iteration 10/25 | Loss: 0.00134394
Iteration 11/25 | Loss: 0.00130317
Iteration 12/25 | Loss: 0.00126592
Iteration 13/25 | Loss: 0.00123410
Iteration 14/25 | Loss: 0.00121380
Iteration 15/25 | Loss: 0.00120832
Iteration 16/25 | Loss: 0.00119614
Iteration 17/25 | Loss: 0.00118666
Iteration 18/25 | Loss: 0.00118118
Iteration 19/25 | Loss: 0.00117937
Iteration 20/25 | Loss: 0.00117733
Iteration 21/25 | Loss: 0.00117409
Iteration 22/25 | Loss: 0.00117042
Iteration 23/25 | Loss: 0.00117210
Iteration 24/25 | Loss: 0.00117021
Iteration 25/25 | Loss: 0.00117005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20332503
Iteration 2/25 | Loss: 0.00292994
Iteration 3/25 | Loss: 0.00253383
Iteration 4/25 | Loss: 0.00253382
Iteration 5/25 | Loss: 0.00253382
Iteration 6/25 | Loss: 0.00253382
Iteration 7/25 | Loss: 0.00253382
Iteration 8/25 | Loss: 0.00253382
Iteration 9/25 | Loss: 0.00253382
Iteration 10/25 | Loss: 0.00253382
Iteration 11/25 | Loss: 0.00253382
Iteration 12/25 | Loss: 0.00253382
Iteration 13/25 | Loss: 0.00253382
Iteration 14/25 | Loss: 0.00253382
Iteration 15/25 | Loss: 0.00253382
Iteration 16/25 | Loss: 0.00253382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0025338188279420137, 0.0025338188279420137, 0.0025338188279420137, 0.0025338188279420137, 0.0025338188279420137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025338188279420137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00253382
Iteration 2/1000 | Loss: 0.00047810
Iteration 3/1000 | Loss: 0.00012760
Iteration 4/1000 | Loss: 0.00012452
Iteration 5/1000 | Loss: 0.00011873
Iteration 6/1000 | Loss: 0.00011860
Iteration 7/1000 | Loss: 0.00009136
Iteration 8/1000 | Loss: 0.00011108
Iteration 9/1000 | Loss: 0.00009166
Iteration 10/1000 | Loss: 0.00010052
Iteration 11/1000 | Loss: 0.00009416
Iteration 12/1000 | Loss: 0.00008477
Iteration 13/1000 | Loss: 0.00009420
Iteration 14/1000 | Loss: 0.00009560
Iteration 15/1000 | Loss: 0.00008848
Iteration 16/1000 | Loss: 0.00010251
Iteration 17/1000 | Loss: 0.00009378
Iteration 18/1000 | Loss: 0.00008879
Iteration 19/1000 | Loss: 0.00009177
Iteration 20/1000 | Loss: 0.00008467
Iteration 21/1000 | Loss: 0.00008437
Iteration 22/1000 | Loss: 0.00024505
Iteration 23/1000 | Loss: 0.00030148
Iteration 24/1000 | Loss: 0.00074921
Iteration 25/1000 | Loss: 0.00224050
Iteration 26/1000 | Loss: 0.00465208
Iteration 27/1000 | Loss: 0.00264431
Iteration 28/1000 | Loss: 0.00271036
Iteration 29/1000 | Loss: 0.00037693
Iteration 30/1000 | Loss: 0.00096750
Iteration 31/1000 | Loss: 0.00199040
Iteration 32/1000 | Loss: 0.00008654
Iteration 33/1000 | Loss: 0.00025275
Iteration 34/1000 | Loss: 0.00018026
Iteration 35/1000 | Loss: 0.00017988
Iteration 36/1000 | Loss: 0.00005109
Iteration 37/1000 | Loss: 0.00018029
Iteration 38/1000 | Loss: 0.00037925
Iteration 39/1000 | Loss: 0.00035359
Iteration 40/1000 | Loss: 0.00016376
Iteration 41/1000 | Loss: 0.00009586
Iteration 42/1000 | Loss: 0.00004649
Iteration 43/1000 | Loss: 0.00016289
Iteration 44/1000 | Loss: 0.00006213
Iteration 45/1000 | Loss: 0.00008661
Iteration 46/1000 | Loss: 0.00005614
Iteration 47/1000 | Loss: 0.00005598
Iteration 48/1000 | Loss: 0.00003402
Iteration 49/1000 | Loss: 0.00006913
Iteration 50/1000 | Loss: 0.00005803
Iteration 51/1000 | Loss: 0.00006658
Iteration 52/1000 | Loss: 0.00006657
Iteration 53/1000 | Loss: 0.00007002
Iteration 54/1000 | Loss: 0.00018536
Iteration 55/1000 | Loss: 0.00007728
Iteration 56/1000 | Loss: 0.00006950
Iteration 57/1000 | Loss: 0.00006733
Iteration 58/1000 | Loss: 0.00007158
Iteration 59/1000 | Loss: 0.00005508
Iteration 60/1000 | Loss: 0.00004498
Iteration 61/1000 | Loss: 0.00002978
Iteration 62/1000 | Loss: 0.00003625
Iteration 63/1000 | Loss: 0.00006063
Iteration 64/1000 | Loss: 0.00004105
Iteration 65/1000 | Loss: 0.00002829
Iteration 66/1000 | Loss: 0.00004286
Iteration 67/1000 | Loss: 0.00004791
Iteration 68/1000 | Loss: 0.00003026
Iteration 69/1000 | Loss: 0.00003498
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002592
Iteration 72/1000 | Loss: 0.00002243
Iteration 73/1000 | Loss: 0.00002897
Iteration 74/1000 | Loss: 0.00003446
Iteration 75/1000 | Loss: 0.00002971
Iteration 76/1000 | Loss: 0.00003281
Iteration 77/1000 | Loss: 0.00003742
Iteration 78/1000 | Loss: 0.00002918
Iteration 79/1000 | Loss: 0.00002679
Iteration 80/1000 | Loss: 0.00005096
Iteration 81/1000 | Loss: 0.00004507
Iteration 82/1000 | Loss: 0.00003853
Iteration 83/1000 | Loss: 0.00005064
Iteration 84/1000 | Loss: 0.00004264
Iteration 85/1000 | Loss: 0.00004027
Iteration 86/1000 | Loss: 0.00002936
Iteration 87/1000 | Loss: 0.00003841
Iteration 88/1000 | Loss: 0.00003954
Iteration 89/1000 | Loss: 0.00004103
Iteration 90/1000 | Loss: 0.00004069
Iteration 91/1000 | Loss: 0.00004274
Iteration 92/1000 | Loss: 0.00003663
Iteration 93/1000 | Loss: 0.00003984
Iteration 94/1000 | Loss: 0.00003733
Iteration 95/1000 | Loss: 0.00003262
Iteration 96/1000 | Loss: 0.00004905
Iteration 97/1000 | Loss: 0.00004079
Iteration 98/1000 | Loss: 0.00004500
Iteration 99/1000 | Loss: 0.00003574
Iteration 100/1000 | Loss: 0.00003941
Iteration 101/1000 | Loss: 0.00004404
Iteration 102/1000 | Loss: 0.00005031
Iteration 103/1000 | Loss: 0.00002761
Iteration 104/1000 | Loss: 0.00003700
Iteration 105/1000 | Loss: 0.00004451
Iteration 106/1000 | Loss: 0.00004714
Iteration 107/1000 | Loss: 0.00003153
Iteration 108/1000 | Loss: 0.00004263
Iteration 109/1000 | Loss: 0.00005016
Iteration 110/1000 | Loss: 0.00004920
Iteration 111/1000 | Loss: 0.00004986
Iteration 112/1000 | Loss: 0.00004596
Iteration 113/1000 | Loss: 0.00004726
Iteration 114/1000 | Loss: 0.00004906
Iteration 115/1000 | Loss: 0.00004912
Iteration 116/1000 | Loss: 0.00004417
Iteration 117/1000 | Loss: 0.00004951
Iteration 118/1000 | Loss: 0.00006955
Iteration 119/1000 | Loss: 0.00004026
Iteration 120/1000 | Loss: 0.00004147
Iteration 121/1000 | Loss: 0.00004694
Iteration 122/1000 | Loss: 0.00004931
Iteration 123/1000 | Loss: 0.00004321
Iteration 124/1000 | Loss: 0.00004041
Iteration 125/1000 | Loss: 0.00006101
Iteration 126/1000 | Loss: 0.00005508
Iteration 127/1000 | Loss: 0.00003986
Iteration 128/1000 | Loss: 0.00005644
Iteration 129/1000 | Loss: 0.00004346
Iteration 130/1000 | Loss: 0.00006941
Iteration 131/1000 | Loss: 0.00005502
Iteration 132/1000 | Loss: 0.00003558
Iteration 133/1000 | Loss: 0.00003700
Iteration 134/1000 | Loss: 0.00004599
Iteration 135/1000 | Loss: 0.00004167
Iteration 136/1000 | Loss: 0.00003508
Iteration 137/1000 | Loss: 0.00004846
Iteration 138/1000 | Loss: 0.00003027
Iteration 139/1000 | Loss: 0.00004444
Iteration 140/1000 | Loss: 0.00003216
Iteration 141/1000 | Loss: 0.00003377
Iteration 142/1000 | Loss: 0.00003209
Iteration 143/1000 | Loss: 0.00003302
Iteration 144/1000 | Loss: 0.00003058
Iteration 145/1000 | Loss: 0.00004803
Iteration 146/1000 | Loss: 0.00004965
Iteration 147/1000 | Loss: 0.00005154
Iteration 148/1000 | Loss: 0.00005030
Iteration 149/1000 | Loss: 0.00005872
Iteration 150/1000 | Loss: 0.00004470
Iteration 151/1000 | Loss: 0.00006085
Iteration 152/1000 | Loss: 0.00004912
Iteration 153/1000 | Loss: 0.00004572
Iteration 154/1000 | Loss: 0.00004828
Iteration 155/1000 | Loss: 0.00002868
Iteration 156/1000 | Loss: 0.00004331
Iteration 157/1000 | Loss: 0.00004813
Iteration 158/1000 | Loss: 0.00003765
Iteration 159/1000 | Loss: 0.00004985
Iteration 160/1000 | Loss: 0.00002869
Iteration 161/1000 | Loss: 0.00004776
Iteration 162/1000 | Loss: 0.00004311
Iteration 163/1000 | Loss: 0.00005253
Iteration 164/1000 | Loss: 0.00004192
Iteration 165/1000 | Loss: 0.00003473
Iteration 166/1000 | Loss: 0.00005564
Iteration 167/1000 | Loss: 0.00003163
Iteration 168/1000 | Loss: 0.00003415
Iteration 169/1000 | Loss: 0.00002386
Iteration 170/1000 | Loss: 0.00003157
Iteration 171/1000 | Loss: 0.00003504
Iteration 172/1000 | Loss: 0.00003815
Iteration 173/1000 | Loss: 0.00003201
Iteration 174/1000 | Loss: 0.00003326
Iteration 175/1000 | Loss: 0.00002543
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00003528
Iteration 178/1000 | Loss: 0.00003448
Iteration 179/1000 | Loss: 0.00003589
Iteration 180/1000 | Loss: 0.00003519
Iteration 181/1000 | Loss: 0.00003310
Iteration 182/1000 | Loss: 0.00003314
Iteration 183/1000 | Loss: 0.00002517
Iteration 184/1000 | Loss: 0.00003914
Iteration 185/1000 | Loss: 0.00002351
Iteration 186/1000 | Loss: 0.00002146
Iteration 187/1000 | Loss: 0.00002071
Iteration 188/1000 | Loss: 0.00002046
Iteration 189/1000 | Loss: 0.00002040
Iteration 190/1000 | Loss: 0.00002040
Iteration 191/1000 | Loss: 0.00002039
Iteration 192/1000 | Loss: 0.00002038
Iteration 193/1000 | Loss: 0.00002038
Iteration 194/1000 | Loss: 0.00002037
Iteration 195/1000 | Loss: 0.00002037
Iteration 196/1000 | Loss: 0.00002032
Iteration 197/1000 | Loss: 0.00002028
Iteration 198/1000 | Loss: 0.00002027
Iteration 199/1000 | Loss: 0.00002025
Iteration 200/1000 | Loss: 0.00002023
Iteration 201/1000 | Loss: 0.00002022
Iteration 202/1000 | Loss: 0.00002021
Iteration 203/1000 | Loss: 0.00002021
Iteration 204/1000 | Loss: 0.00002013
Iteration 205/1000 | Loss: 0.00002013
Iteration 206/1000 | Loss: 0.00002012
Iteration 207/1000 | Loss: 0.00002012
Iteration 208/1000 | Loss: 0.00002012
Iteration 209/1000 | Loss: 0.00002012
Iteration 210/1000 | Loss: 0.00002011
Iteration 211/1000 | Loss: 0.00002011
Iteration 212/1000 | Loss: 0.00002011
Iteration 213/1000 | Loss: 0.00002011
Iteration 214/1000 | Loss: 0.00002011
Iteration 215/1000 | Loss: 0.00002011
Iteration 216/1000 | Loss: 0.00002011
Iteration 217/1000 | Loss: 0.00002011
Iteration 218/1000 | Loss: 0.00002011
Iteration 219/1000 | Loss: 0.00002010
Iteration 220/1000 | Loss: 0.00002010
Iteration 221/1000 | Loss: 0.00002010
Iteration 222/1000 | Loss: 0.00002009
Iteration 223/1000 | Loss: 0.00002009
Iteration 224/1000 | Loss: 0.00002008
Iteration 225/1000 | Loss: 0.00002008
Iteration 226/1000 | Loss: 0.00002008
Iteration 227/1000 | Loss: 0.00002007
Iteration 228/1000 | Loss: 0.00002007
Iteration 229/1000 | Loss: 0.00002007
Iteration 230/1000 | Loss: 0.00002006
Iteration 231/1000 | Loss: 0.00002006
Iteration 232/1000 | Loss: 0.00002006
Iteration 233/1000 | Loss: 0.00002006
Iteration 234/1000 | Loss: 0.00002006
Iteration 235/1000 | Loss: 0.00002006
Iteration 236/1000 | Loss: 0.00002006
Iteration 237/1000 | Loss: 0.00002006
Iteration 238/1000 | Loss: 0.00002006
Iteration 239/1000 | Loss: 0.00002006
Iteration 240/1000 | Loss: 0.00002006
Iteration 241/1000 | Loss: 0.00002006
Iteration 242/1000 | Loss: 0.00002005
Iteration 243/1000 | Loss: 0.00002005
Iteration 244/1000 | Loss: 0.00002005
Iteration 245/1000 | Loss: 0.00002005
Iteration 246/1000 | Loss: 0.00002005
Iteration 247/1000 | Loss: 0.00002005
Iteration 248/1000 | Loss: 0.00002005
Iteration 249/1000 | Loss: 0.00002005
Iteration 250/1000 | Loss: 0.00002005
Iteration 251/1000 | Loss: 0.00002005
Iteration 252/1000 | Loss: 0.00002005
Iteration 253/1000 | Loss: 0.00002005
Iteration 254/1000 | Loss: 0.00002005
Iteration 255/1000 | Loss: 0.00002005
Iteration 256/1000 | Loss: 0.00002005
Iteration 257/1000 | Loss: 0.00002005
Iteration 258/1000 | Loss: 0.00002004
Iteration 259/1000 | Loss: 0.00002004
Iteration 260/1000 | Loss: 0.00002004
Iteration 261/1000 | Loss: 0.00002004
Iteration 262/1000 | Loss: 0.00002004
Iteration 263/1000 | Loss: 0.00002004
Iteration 264/1000 | Loss: 0.00002004
Iteration 265/1000 | Loss: 0.00002004
Iteration 266/1000 | Loss: 0.00002004
Iteration 267/1000 | Loss: 0.00002004
Iteration 268/1000 | Loss: 0.00002004
Iteration 269/1000 | Loss: 0.00002003
Iteration 270/1000 | Loss: 0.00002003
Iteration 271/1000 | Loss: 0.00002003
Iteration 272/1000 | Loss: 0.00002003
Iteration 273/1000 | Loss: 0.00002003
Iteration 274/1000 | Loss: 0.00002003
Iteration 275/1000 | Loss: 0.00002002
Iteration 276/1000 | Loss: 0.00002002
Iteration 277/1000 | Loss: 0.00002002
Iteration 278/1000 | Loss: 0.00002001
Iteration 279/1000 | Loss: 0.00002001
Iteration 280/1000 | Loss: 0.00002001
Iteration 281/1000 | Loss: 0.00002000
Iteration 282/1000 | Loss: 0.00002000
Iteration 283/1000 | Loss: 0.00002000
Iteration 284/1000 | Loss: 0.00002000
Iteration 285/1000 | Loss: 0.00001999
Iteration 286/1000 | Loss: 0.00001999
Iteration 287/1000 | Loss: 0.00001998
Iteration 288/1000 | Loss: 0.00001998
Iteration 289/1000 | Loss: 0.00001998
Iteration 290/1000 | Loss: 0.00001998
Iteration 291/1000 | Loss: 0.00001998
Iteration 292/1000 | Loss: 0.00001998
Iteration 293/1000 | Loss: 0.00001997
Iteration 294/1000 | Loss: 0.00001997
Iteration 295/1000 | Loss: 0.00001997
Iteration 296/1000 | Loss: 0.00001996
Iteration 297/1000 | Loss: 0.00001996
Iteration 298/1000 | Loss: 0.00001996
Iteration 299/1000 | Loss: 0.00001995
Iteration 300/1000 | Loss: 0.00001995
Iteration 301/1000 | Loss: 0.00001994
Iteration 302/1000 | Loss: 0.00001994
Iteration 303/1000 | Loss: 0.00001994
Iteration 304/1000 | Loss: 0.00001993
Iteration 305/1000 | Loss: 0.00001993
Iteration 306/1000 | Loss: 0.00001993
Iteration 307/1000 | Loss: 0.00001993
Iteration 308/1000 | Loss: 0.00001993
Iteration 309/1000 | Loss: 0.00001993
Iteration 310/1000 | Loss: 0.00001993
Iteration 311/1000 | Loss: 0.00001993
Iteration 312/1000 | Loss: 0.00001993
Iteration 313/1000 | Loss: 0.00001993
Iteration 314/1000 | Loss: 0.00001993
Iteration 315/1000 | Loss: 0.00001993
Iteration 316/1000 | Loss: 0.00001993
Iteration 317/1000 | Loss: 0.00001993
Iteration 318/1000 | Loss: 0.00001993
Iteration 319/1000 | Loss: 0.00001993
Iteration 320/1000 | Loss: 0.00001993
Iteration 321/1000 | Loss: 0.00001993
Iteration 322/1000 | Loss: 0.00001993
Iteration 323/1000 | Loss: 0.00001993
Iteration 324/1000 | Loss: 0.00001993
Iteration 325/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.9932671420974657e-05, 1.9932671420974657e-05, 1.9932671420974657e-05, 1.9932671420974657e-05, 1.9932671420974657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9932671420974657e-05

Optimization complete. Final v2v error: 3.586176633834839 mm

Highest mean error: 10.040690422058105 mm for frame 185

Lowest mean error: 2.9907374382019043 mm for frame 11

Saving results

Total time: 364.84272265434265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101984
Iteration 2/25 | Loss: 0.00138827
Iteration 3/25 | Loss: 0.00109342
Iteration 4/25 | Loss: 0.00107736
Iteration 5/25 | Loss: 0.00106424
Iteration 6/25 | Loss: 0.00106301
Iteration 7/25 | Loss: 0.00106287
Iteration 8/25 | Loss: 0.00106286
Iteration 9/25 | Loss: 0.00106286
Iteration 10/25 | Loss: 0.00106286
Iteration 11/25 | Loss: 0.00106286
Iteration 12/25 | Loss: 0.00106285
Iteration 13/25 | Loss: 0.00106285
Iteration 14/25 | Loss: 0.00106285
Iteration 15/25 | Loss: 0.00106285
Iteration 16/25 | Loss: 0.00106285
Iteration 17/25 | Loss: 0.00106285
Iteration 18/25 | Loss: 0.00106285
Iteration 19/25 | Loss: 0.00106285
Iteration 20/25 | Loss: 0.00106285
Iteration 21/25 | Loss: 0.00106284
Iteration 22/25 | Loss: 0.00106284
Iteration 23/25 | Loss: 0.00106284
Iteration 24/25 | Loss: 0.00106284
Iteration 25/25 | Loss: 0.00106284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22147834
Iteration 2/25 | Loss: 0.00161240
Iteration 3/25 | Loss: 0.00161240
Iteration 4/25 | Loss: 0.00161240
Iteration 5/25 | Loss: 0.00161240
Iteration 6/25 | Loss: 0.00161240
Iteration 7/25 | Loss: 0.00161240
Iteration 8/25 | Loss: 0.00161240
Iteration 9/25 | Loss: 0.00161240
Iteration 10/25 | Loss: 0.00161240
Iteration 11/25 | Loss: 0.00161240
Iteration 12/25 | Loss: 0.00161240
Iteration 13/25 | Loss: 0.00161240
Iteration 14/25 | Loss: 0.00161240
Iteration 15/25 | Loss: 0.00161240
Iteration 16/25 | Loss: 0.00161240
Iteration 17/25 | Loss: 0.00161240
Iteration 18/25 | Loss: 0.00161240
Iteration 19/25 | Loss: 0.00161240
Iteration 20/25 | Loss: 0.00161240
Iteration 21/25 | Loss: 0.00161240
Iteration 22/25 | Loss: 0.00161240
Iteration 23/25 | Loss: 0.00161240
Iteration 24/25 | Loss: 0.00161240
Iteration 25/25 | Loss: 0.00161240

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161240
Iteration 2/1000 | Loss: 0.00002727
Iteration 3/1000 | Loss: 0.00001658
Iteration 4/1000 | Loss: 0.00001502
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00002830
Iteration 7/1000 | Loss: 0.00001343
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001288
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001249
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001246
Iteration 16/1000 | Loss: 0.00001244
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001238
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001228
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001221
Iteration 26/1000 | Loss: 0.00001221
Iteration 27/1000 | Loss: 0.00001221
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001221
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001220
Iteration 34/1000 | Loss: 0.00001219
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00003191
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00002109
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001212
Iteration 46/1000 | Loss: 0.00001212
Iteration 47/1000 | Loss: 0.00001211
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001208
Iteration 51/1000 | Loss: 0.00002931
Iteration 52/1000 | Loss: 0.00001691
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001203
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001200
Iteration 64/1000 | Loss: 0.00001200
Iteration 65/1000 | Loss: 0.00001200
Iteration 66/1000 | Loss: 0.00001200
Iteration 67/1000 | Loss: 0.00001200
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001199
Iteration 70/1000 | Loss: 0.00001199
Iteration 71/1000 | Loss: 0.00001199
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001199
Iteration 76/1000 | Loss: 0.00001198
Iteration 77/1000 | Loss: 0.00001198
Iteration 78/1000 | Loss: 0.00001198
Iteration 79/1000 | Loss: 0.00001198
Iteration 80/1000 | Loss: 0.00001198
Iteration 81/1000 | Loss: 0.00001198
Iteration 82/1000 | Loss: 0.00001198
Iteration 83/1000 | Loss: 0.00001198
Iteration 84/1000 | Loss: 0.00001198
Iteration 85/1000 | Loss: 0.00001198
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001197
Iteration 88/1000 | Loss: 0.00001197
Iteration 89/1000 | Loss: 0.00001197
Iteration 90/1000 | Loss: 0.00001197
Iteration 91/1000 | Loss: 0.00001197
Iteration 92/1000 | Loss: 0.00001197
Iteration 93/1000 | Loss: 0.00001197
Iteration 94/1000 | Loss: 0.00001197
Iteration 95/1000 | Loss: 0.00001197
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001194
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001193
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.1926588740607258e-05, 1.1926588740607258e-05, 1.1926588740607258e-05, 1.1926588740607258e-05, 1.1926588740607258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1926588740607258e-05

Optimization complete. Final v2v error: 3.010896682739258 mm

Highest mean error: 3.23313307762146 mm for frame 29

Lowest mean error: 2.39316725730896 mm for frame 2

Saving results

Total time: 41.90889859199524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125836
Iteration 2/25 | Loss: 0.00235238
Iteration 3/25 | Loss: 0.00181825
Iteration 4/25 | Loss: 0.00240339
Iteration 5/25 | Loss: 0.00223200
Iteration 6/25 | Loss: 0.00182449
Iteration 7/25 | Loss: 0.00170970
Iteration 8/25 | Loss: 0.00161732
Iteration 9/25 | Loss: 0.00148856
Iteration 10/25 | Loss: 0.00141734
Iteration 11/25 | Loss: 0.00140411
Iteration 12/25 | Loss: 0.00133567
Iteration 13/25 | Loss: 0.00132300
Iteration 14/25 | Loss: 0.00129199
Iteration 15/25 | Loss: 0.00128305
Iteration 16/25 | Loss: 0.00128293
Iteration 17/25 | Loss: 0.00127779
Iteration 18/25 | Loss: 0.00127520
Iteration 19/25 | Loss: 0.00126519
Iteration 20/25 | Loss: 0.00125880
Iteration 21/25 | Loss: 0.00126928
Iteration 22/25 | Loss: 0.00125921
Iteration 23/25 | Loss: 0.00125665
Iteration 24/25 | Loss: 0.00125578
Iteration 25/25 | Loss: 0.00125110

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40488863
Iteration 2/25 | Loss: 0.00187603
Iteration 3/25 | Loss: 0.00187603
Iteration 4/25 | Loss: 0.00187602
Iteration 5/25 | Loss: 0.00187602
Iteration 6/25 | Loss: 0.00187602
Iteration 7/25 | Loss: 0.00187602
Iteration 8/25 | Loss: 0.00187602
Iteration 9/25 | Loss: 0.00187602
Iteration 10/25 | Loss: 0.00187602
Iteration 11/25 | Loss: 0.00187602
Iteration 12/25 | Loss: 0.00187602
Iteration 13/25 | Loss: 0.00187602
Iteration 14/25 | Loss: 0.00187602
Iteration 15/25 | Loss: 0.00187602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018760230159386992, 0.0018760230159386992, 0.0018760230159386992, 0.0018760230159386992, 0.0018760230159386992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018760230159386992

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187602
Iteration 2/1000 | Loss: 0.00046612
Iteration 3/1000 | Loss: 0.00051782
Iteration 4/1000 | Loss: 0.00048289
Iteration 5/1000 | Loss: 0.00060011
Iteration 6/1000 | Loss: 0.00035023
Iteration 7/1000 | Loss: 0.00044632
Iteration 8/1000 | Loss: 0.00049365
Iteration 9/1000 | Loss: 0.00090250
Iteration 10/1000 | Loss: 0.00069119
Iteration 11/1000 | Loss: 0.00033957
Iteration 12/1000 | Loss: 0.00093268
Iteration 13/1000 | Loss: 0.00042086
Iteration 14/1000 | Loss: 0.00055077
Iteration 15/1000 | Loss: 0.00044416
Iteration 16/1000 | Loss: 0.00028573
Iteration 17/1000 | Loss: 0.00044460
Iteration 18/1000 | Loss: 0.00033510
Iteration 19/1000 | Loss: 0.00043896
Iteration 20/1000 | Loss: 0.00047776
Iteration 21/1000 | Loss: 0.00050836
Iteration 22/1000 | Loss: 0.00048916
Iteration 23/1000 | Loss: 0.00049824
Iteration 24/1000 | Loss: 0.00047752
Iteration 25/1000 | Loss: 0.00066023
Iteration 26/1000 | Loss: 0.00049201
Iteration 27/1000 | Loss: 0.00056373
Iteration 28/1000 | Loss: 0.00043689
Iteration 29/1000 | Loss: 0.00078601
Iteration 30/1000 | Loss: 0.00068056
Iteration 31/1000 | Loss: 0.00064585
Iteration 32/1000 | Loss: 0.00046432
Iteration 33/1000 | Loss: 0.00058100
Iteration 34/1000 | Loss: 0.00041288
Iteration 35/1000 | Loss: 0.00059136
Iteration 36/1000 | Loss: 0.00063466
Iteration 37/1000 | Loss: 0.00064581
Iteration 38/1000 | Loss: 0.00067545
Iteration 39/1000 | Loss: 0.00074450
Iteration 40/1000 | Loss: 0.00075385
Iteration 41/1000 | Loss: 0.00071818
Iteration 42/1000 | Loss: 0.00075041
Iteration 43/1000 | Loss: 0.00073978
Iteration 44/1000 | Loss: 0.00077245
Iteration 45/1000 | Loss: 0.00062583
Iteration 46/1000 | Loss: 0.00081206
Iteration 47/1000 | Loss: 0.00042177
Iteration 48/1000 | Loss: 0.00055504
Iteration 49/1000 | Loss: 0.00069832
Iteration 50/1000 | Loss: 0.00063906
Iteration 51/1000 | Loss: 0.00032976
Iteration 52/1000 | Loss: 0.00032169
Iteration 53/1000 | Loss: 0.00037193
Iteration 54/1000 | Loss: 0.00049442
Iteration 55/1000 | Loss: 0.00060538
Iteration 56/1000 | Loss: 0.00062383
Iteration 57/1000 | Loss: 0.00059202
Iteration 58/1000 | Loss: 0.00062139
Iteration 59/1000 | Loss: 0.00057377
Iteration 60/1000 | Loss: 0.00060925
Iteration 61/1000 | Loss: 0.00076817
Iteration 62/1000 | Loss: 0.00079551
Iteration 63/1000 | Loss: 0.00076645
Iteration 64/1000 | Loss: 0.00077144
Iteration 65/1000 | Loss: 0.00101492
Iteration 66/1000 | Loss: 0.00079807
Iteration 67/1000 | Loss: 0.00058969
Iteration 68/1000 | Loss: 0.00067133
Iteration 69/1000 | Loss: 0.00067305
Iteration 70/1000 | Loss: 0.00067984
Iteration 71/1000 | Loss: 0.00046603
Iteration 72/1000 | Loss: 0.00122031
Iteration 73/1000 | Loss: 0.00052733
Iteration 74/1000 | Loss: 0.00024165
Iteration 75/1000 | Loss: 0.00022743
Iteration 76/1000 | Loss: 0.00046769
Iteration 77/1000 | Loss: 0.00045118
Iteration 78/1000 | Loss: 0.00058100
Iteration 79/1000 | Loss: 0.00051085
Iteration 80/1000 | Loss: 0.00040191
Iteration 81/1000 | Loss: 0.00035393
Iteration 82/1000 | Loss: 0.00045191
Iteration 83/1000 | Loss: 0.00049973
Iteration 84/1000 | Loss: 0.00026144
Iteration 85/1000 | Loss: 0.00029853
Iteration 86/1000 | Loss: 0.00038161
Iteration 87/1000 | Loss: 0.00043305
Iteration 88/1000 | Loss: 0.00045301
Iteration 89/1000 | Loss: 0.00042524
Iteration 90/1000 | Loss: 0.00052951
Iteration 91/1000 | Loss: 0.00037365
Iteration 92/1000 | Loss: 0.00037026
Iteration 93/1000 | Loss: 0.00034536
Iteration 94/1000 | Loss: 0.00041179
Iteration 95/1000 | Loss: 0.00030492
Iteration 96/1000 | Loss: 0.00038620
Iteration 97/1000 | Loss: 0.00039275
Iteration 98/1000 | Loss: 0.00052067
Iteration 99/1000 | Loss: 0.00043877
Iteration 100/1000 | Loss: 0.00034544
Iteration 101/1000 | Loss: 0.00039856
Iteration 102/1000 | Loss: 0.00042872
Iteration 103/1000 | Loss: 0.00043984
Iteration 104/1000 | Loss: 0.00040473
Iteration 105/1000 | Loss: 0.00045843
Iteration 106/1000 | Loss: 0.00043455
Iteration 107/1000 | Loss: 0.00050059
Iteration 108/1000 | Loss: 0.00042970
Iteration 109/1000 | Loss: 0.00069816
Iteration 110/1000 | Loss: 0.00065947
Iteration 111/1000 | Loss: 0.00068920
Iteration 112/1000 | Loss: 0.00064496
Iteration 113/1000 | Loss: 0.00037529
Iteration 114/1000 | Loss: 0.00029851
Iteration 115/1000 | Loss: 0.00040117
Iteration 116/1000 | Loss: 0.00046157
Iteration 117/1000 | Loss: 0.00040531
Iteration 118/1000 | Loss: 0.00043590
Iteration 119/1000 | Loss: 0.00031482
Iteration 120/1000 | Loss: 0.00029679
Iteration 121/1000 | Loss: 0.00028374
Iteration 122/1000 | Loss: 0.00037834
Iteration 123/1000 | Loss: 0.00046853
Iteration 124/1000 | Loss: 0.00041519
Iteration 125/1000 | Loss: 0.00045421
Iteration 126/1000 | Loss: 0.00044668
Iteration 127/1000 | Loss: 0.00039701
Iteration 128/1000 | Loss: 0.00049035
Iteration 129/1000 | Loss: 0.00037214
Iteration 130/1000 | Loss: 0.00041343
Iteration 131/1000 | Loss: 0.00040658
Iteration 132/1000 | Loss: 0.00040681
Iteration 133/1000 | Loss: 0.00074292
Iteration 134/1000 | Loss: 0.00041255
Iteration 135/1000 | Loss: 0.00042776
Iteration 136/1000 | Loss: 0.00041887
Iteration 137/1000 | Loss: 0.00047351
Iteration 138/1000 | Loss: 0.00044348
Iteration 139/1000 | Loss: 0.00049433
Iteration 140/1000 | Loss: 0.00043985
Iteration 141/1000 | Loss: 0.00042730
Iteration 142/1000 | Loss: 0.00037728
Iteration 143/1000 | Loss: 0.00042381
Iteration 144/1000 | Loss: 0.00045101
Iteration 145/1000 | Loss: 0.00031521
Iteration 146/1000 | Loss: 0.00031742
Iteration 147/1000 | Loss: 0.00035064
Iteration 148/1000 | Loss: 0.00035934
Iteration 149/1000 | Loss: 0.00029862
Iteration 150/1000 | Loss: 0.00035777
Iteration 151/1000 | Loss: 0.00052683
Iteration 152/1000 | Loss: 0.00044171
Iteration 153/1000 | Loss: 0.00044044
Iteration 154/1000 | Loss: 0.00047526
Iteration 155/1000 | Loss: 0.00027484
Iteration 156/1000 | Loss: 0.00036650
Iteration 157/1000 | Loss: 0.00032676
Iteration 158/1000 | Loss: 0.00036484
Iteration 159/1000 | Loss: 0.00045554
Iteration 160/1000 | Loss: 0.00044680
Iteration 161/1000 | Loss: 0.00052300
Iteration 162/1000 | Loss: 0.00042414
Iteration 163/1000 | Loss: 0.00045773
Iteration 164/1000 | Loss: 0.00042854
Iteration 165/1000 | Loss: 0.00050565
Iteration 166/1000 | Loss: 0.00043268
Iteration 167/1000 | Loss: 0.00041241
Iteration 168/1000 | Loss: 0.00041751
Iteration 169/1000 | Loss: 0.00040092
Iteration 170/1000 | Loss: 0.00059851
Iteration 171/1000 | Loss: 0.00057065
Iteration 172/1000 | Loss: 0.00045990
Iteration 173/1000 | Loss: 0.00045586
Iteration 174/1000 | Loss: 0.00035914
Iteration 175/1000 | Loss: 0.00031087
Iteration 176/1000 | Loss: 0.00029533
Iteration 177/1000 | Loss: 0.00037710
Iteration 178/1000 | Loss: 0.00034230
Iteration 179/1000 | Loss: 0.00032182
Iteration 180/1000 | Loss: 0.00036959
Iteration 181/1000 | Loss: 0.00042250
Iteration 182/1000 | Loss: 0.00044738
Iteration 183/1000 | Loss: 0.00040965
Iteration 184/1000 | Loss: 0.00046555
Iteration 185/1000 | Loss: 0.00043263
Iteration 186/1000 | Loss: 0.00045836
Iteration 187/1000 | Loss: 0.00038655
Iteration 188/1000 | Loss: 0.00037067
Iteration 189/1000 | Loss: 0.00038342
Iteration 190/1000 | Loss: 0.00039301
Iteration 191/1000 | Loss: 0.00031259
Iteration 192/1000 | Loss: 0.00031738
Iteration 193/1000 | Loss: 0.00045934
Iteration 194/1000 | Loss: 0.00043665
Iteration 195/1000 | Loss: 0.00043112
Iteration 196/1000 | Loss: 0.00046404
Iteration 197/1000 | Loss: 0.00044418
Iteration 198/1000 | Loss: 0.00048258
Iteration 199/1000 | Loss: 0.00044832
Iteration 200/1000 | Loss: 0.00049550
Iteration 201/1000 | Loss: 0.00043310
Iteration 202/1000 | Loss: 0.00050822
Iteration 203/1000 | Loss: 0.00093644
Iteration 204/1000 | Loss: 0.00045891
Iteration 205/1000 | Loss: 0.00050398
Iteration 206/1000 | Loss: 0.00048545
Iteration 207/1000 | Loss: 0.00044403
Iteration 208/1000 | Loss: 0.00051876
Iteration 209/1000 | Loss: 0.00050404
Iteration 210/1000 | Loss: 0.00057354
Iteration 211/1000 | Loss: 0.00050524
Iteration 212/1000 | Loss: 0.00072234
Iteration 213/1000 | Loss: 0.00118158
Iteration 214/1000 | Loss: 0.00111613
Iteration 215/1000 | Loss: 0.00039172
Iteration 216/1000 | Loss: 0.00037700
Iteration 217/1000 | Loss: 0.00036914
Iteration 218/1000 | Loss: 0.00032685
Iteration 219/1000 | Loss: 0.00028688
Iteration 220/1000 | Loss: 0.00041835
Iteration 221/1000 | Loss: 0.00076741
Iteration 222/1000 | Loss: 0.00032294
Iteration 223/1000 | Loss: 0.00041513
Iteration 224/1000 | Loss: 0.00049002
Iteration 225/1000 | Loss: 0.00045169
Iteration 226/1000 | Loss: 0.00052101
Iteration 227/1000 | Loss: 0.00052731
Iteration 228/1000 | Loss: 0.00045834
Iteration 229/1000 | Loss: 0.00043280
Iteration 230/1000 | Loss: 0.00040837
Iteration 231/1000 | Loss: 0.00043958
Iteration 232/1000 | Loss: 0.00044656
Iteration 233/1000 | Loss: 0.00024803
Iteration 234/1000 | Loss: 0.00024821
Iteration 235/1000 | Loss: 0.00020671
Iteration 236/1000 | Loss: 0.00019756
Iteration 237/1000 | Loss: 0.00019263
Iteration 238/1000 | Loss: 0.00012213
Iteration 239/1000 | Loss: 0.00013809
Iteration 240/1000 | Loss: 0.00014477
Iteration 241/1000 | Loss: 0.00013055
Iteration 242/1000 | Loss: 0.00009165
Iteration 243/1000 | Loss: 0.00008616
Iteration 244/1000 | Loss: 0.00008801
Iteration 245/1000 | Loss: 0.00011017
Iteration 246/1000 | Loss: 0.00010428
Iteration 247/1000 | Loss: 0.00010503
Iteration 248/1000 | Loss: 0.00010493
Iteration 249/1000 | Loss: 0.00011575
Iteration 250/1000 | Loss: 0.00010506
Iteration 251/1000 | Loss: 0.00007946
Iteration 252/1000 | Loss: 0.00009860
Iteration 253/1000 | Loss: 0.00008598
Iteration 254/1000 | Loss: 0.00009729
Iteration 255/1000 | Loss: 0.00010164
Iteration 256/1000 | Loss: 0.00008728
Iteration 257/1000 | Loss: 0.00011829
Iteration 258/1000 | Loss: 0.00010179
Iteration 259/1000 | Loss: 0.00010716
Iteration 260/1000 | Loss: 0.00009660
Iteration 261/1000 | Loss: 0.00010256
Iteration 262/1000 | Loss: 0.00010164
Iteration 263/1000 | Loss: 0.00009796
Iteration 264/1000 | Loss: 0.00008870
Iteration 265/1000 | Loss: 0.00009354
Iteration 266/1000 | Loss: 0.00009279
Iteration 267/1000 | Loss: 0.00005946
Iteration 268/1000 | Loss: 0.00009776
Iteration 269/1000 | Loss: 0.00008335
Iteration 270/1000 | Loss: 0.00009165
Iteration 271/1000 | Loss: 0.00009420
Iteration 272/1000 | Loss: 0.00008767
Iteration 273/1000 | Loss: 0.00009420
Iteration 274/1000 | Loss: 0.00009848
Iteration 275/1000 | Loss: 0.00010451
Iteration 276/1000 | Loss: 0.00011423
Iteration 277/1000 | Loss: 0.00011191
Iteration 278/1000 | Loss: 0.00009274
Iteration 279/1000 | Loss: 0.00010334
Iteration 280/1000 | Loss: 0.00012525
Iteration 281/1000 | Loss: 0.00010415
Iteration 282/1000 | Loss: 0.00006253
Iteration 283/1000 | Loss: 0.00009899
Iteration 284/1000 | Loss: 0.00009196
Iteration 285/1000 | Loss: 0.00009785
Iteration 286/1000 | Loss: 0.00009514
Iteration 287/1000 | Loss: 0.00009909
Iteration 288/1000 | Loss: 0.00009834
Iteration 289/1000 | Loss: 0.00010485
Iteration 290/1000 | Loss: 0.00007812
Iteration 291/1000 | Loss: 0.00006970
Iteration 292/1000 | Loss: 0.00010519
Iteration 293/1000 | Loss: 0.00011241
Iteration 294/1000 | Loss: 0.00009920
Iteration 295/1000 | Loss: 0.00010396
Iteration 296/1000 | Loss: 0.00008483
Iteration 297/1000 | Loss: 0.00008102
Iteration 298/1000 | Loss: 0.00009880
Iteration 299/1000 | Loss: 0.00010428
Iteration 300/1000 | Loss: 0.00010007
Iteration 301/1000 | Loss: 0.00010352
Iteration 302/1000 | Loss: 0.00010338
Iteration 303/1000 | Loss: 0.00010666
Iteration 304/1000 | Loss: 0.00009993
Iteration 305/1000 | Loss: 0.00011150
Iteration 306/1000 | Loss: 0.00009521
Iteration 307/1000 | Loss: 0.00010803
Iteration 308/1000 | Loss: 0.00009359
Iteration 309/1000 | Loss: 0.00010468
Iteration 310/1000 | Loss: 0.00009707
Iteration 311/1000 | Loss: 0.00010232
Iteration 312/1000 | Loss: 0.00009501
Iteration 313/1000 | Loss: 0.00010733
Iteration 314/1000 | Loss: 0.00010274
Iteration 315/1000 | Loss: 0.00010454
Iteration 316/1000 | Loss: 0.00009441
Iteration 317/1000 | Loss: 0.00008109
Iteration 318/1000 | Loss: 0.00009944
Iteration 319/1000 | Loss: 0.00009800
Iteration 320/1000 | Loss: 0.00009240
Iteration 321/1000 | Loss: 0.00010650
Iteration 322/1000 | Loss: 0.00009828
Iteration 323/1000 | Loss: 0.00009538
Iteration 324/1000 | Loss: 0.00010170
Iteration 325/1000 | Loss: 0.00010129
Iteration 326/1000 | Loss: 0.00010665
Iteration 327/1000 | Loss: 0.00008496
Iteration 328/1000 | Loss: 0.00008583
Iteration 329/1000 | Loss: 0.00010059
Iteration 330/1000 | Loss: 0.00009371
Iteration 331/1000 | Loss: 0.00009458
Iteration 332/1000 | Loss: 0.00009733
Iteration 333/1000 | Loss: 0.00010037
Iteration 334/1000 | Loss: 0.00009706
Iteration 335/1000 | Loss: 0.00010058
Iteration 336/1000 | Loss: 0.00009594
Iteration 337/1000 | Loss: 0.00009960
Iteration 338/1000 | Loss: 0.00010426
Iteration 339/1000 | Loss: 0.00010451
Iteration 340/1000 | Loss: 0.00011037
Iteration 341/1000 | Loss: 0.00009374
Iteration 342/1000 | Loss: 0.00010797
Iteration 343/1000 | Loss: 0.00010134
Iteration 344/1000 | Loss: 0.00010331
Iteration 345/1000 | Loss: 0.00010556
Iteration 346/1000 | Loss: 0.00009381
Iteration 347/1000 | Loss: 0.00009056
Iteration 348/1000 | Loss: 0.00008773
Iteration 349/1000 | Loss: 0.00009821
Iteration 350/1000 | Loss: 0.00010044
Iteration 351/1000 | Loss: 0.00006704
Iteration 352/1000 | Loss: 0.00010726
Iteration 353/1000 | Loss: 0.00010041
Iteration 354/1000 | Loss: 0.00008651
Iteration 355/1000 | Loss: 0.00006456
Iteration 356/1000 | Loss: 0.00007835
Iteration 357/1000 | Loss: 0.00007448
Iteration 358/1000 | Loss: 0.00008465
Iteration 359/1000 | Loss: 0.00010014
Iteration 360/1000 | Loss: 0.00007849
Iteration 361/1000 | Loss: 0.00009205
Iteration 362/1000 | Loss: 0.00011300
Iteration 363/1000 | Loss: 0.00010288
Iteration 364/1000 | Loss: 0.00009303
Iteration 365/1000 | Loss: 0.00009525
Iteration 366/1000 | Loss: 0.00009975
Iteration 367/1000 | Loss: 0.00010298
Iteration 368/1000 | Loss: 0.00009151
Iteration 369/1000 | Loss: 0.00008892
Iteration 370/1000 | Loss: 0.00009528
Iteration 371/1000 | Loss: 0.00009711
Iteration 372/1000 | Loss: 0.00011022
Iteration 373/1000 | Loss: 0.00009755
Iteration 374/1000 | Loss: 0.00009358
Iteration 375/1000 | Loss: 0.00009625
Iteration 376/1000 | Loss: 0.00008410
Iteration 377/1000 | Loss: 0.00007762
Iteration 378/1000 | Loss: 0.00008665
Iteration 379/1000 | Loss: 0.00009582
Iteration 380/1000 | Loss: 0.00009492
Iteration 381/1000 | Loss: 0.00009195
Iteration 382/1000 | Loss: 0.00006867
Iteration 383/1000 | Loss: 0.00007922
Iteration 384/1000 | Loss: 0.00008345
Iteration 385/1000 | Loss: 0.00010009
Iteration 386/1000 | Loss: 0.00008705
Iteration 387/1000 | Loss: 0.00010994
Iteration 388/1000 | Loss: 0.00009110
Iteration 389/1000 | Loss: 0.00007718
Iteration 390/1000 | Loss: 0.00010453
Iteration 391/1000 | Loss: 0.00009295
Iteration 392/1000 | Loss: 0.00009747
Iteration 393/1000 | Loss: 0.00007869
Iteration 394/1000 | Loss: 0.00006999
Iteration 395/1000 | Loss: 0.00005022
Iteration 396/1000 | Loss: 0.00004445
Iteration 397/1000 | Loss: 0.00004232
Iteration 398/1000 | Loss: 0.00005301
Iteration 399/1000 | Loss: 0.00006373
Iteration 400/1000 | Loss: 0.00006792
Iteration 401/1000 | Loss: 0.00005865
Iteration 402/1000 | Loss: 0.00006522
Iteration 403/1000 | Loss: 0.00007536
Iteration 404/1000 | Loss: 0.00006357
Iteration 405/1000 | Loss: 0.00006162
Iteration 406/1000 | Loss: 0.00004397
Iteration 407/1000 | Loss: 0.00003060
Iteration 408/1000 | Loss: 0.00002773
Iteration 409/1000 | Loss: 0.00003482
Iteration 410/1000 | Loss: 0.00003633
Iteration 411/1000 | Loss: 0.00004946
Iteration 412/1000 | Loss: 0.00004014
Iteration 413/1000 | Loss: 0.00004675
Iteration 414/1000 | Loss: 0.00003722
Iteration 415/1000 | Loss: 0.00002842
Iteration 416/1000 | Loss: 0.00004088
Iteration 417/1000 | Loss: 0.00004354
Iteration 418/1000 | Loss: 0.00002685
Iteration 419/1000 | Loss: 0.00002418
Iteration 420/1000 | Loss: 0.00002285
Iteration 421/1000 | Loss: 0.00002206
Iteration 422/1000 | Loss: 0.00002171
Iteration 423/1000 | Loss: 0.00002144
Iteration 424/1000 | Loss: 0.00002132
Iteration 425/1000 | Loss: 0.00002127
Iteration 426/1000 | Loss: 0.00002125
Iteration 427/1000 | Loss: 0.00002116
Iteration 428/1000 | Loss: 0.00002112
Iteration 429/1000 | Loss: 0.00002110
Iteration 430/1000 | Loss: 0.00002104
Iteration 431/1000 | Loss: 0.00002095
Iteration 432/1000 | Loss: 0.00002095
Iteration 433/1000 | Loss: 0.00002093
Iteration 434/1000 | Loss: 0.00002087
Iteration 435/1000 | Loss: 0.00002087
Iteration 436/1000 | Loss: 0.00002086
Iteration 437/1000 | Loss: 0.00002086
Iteration 438/1000 | Loss: 0.00002085
Iteration 439/1000 | Loss: 0.00002084
Iteration 440/1000 | Loss: 0.00002083
Iteration 441/1000 | Loss: 0.00002082
Iteration 442/1000 | Loss: 0.00002082
Iteration 443/1000 | Loss: 0.00002082
Iteration 444/1000 | Loss: 0.00002078
Iteration 445/1000 | Loss: 0.00002078
Iteration 446/1000 | Loss: 0.00002078
Iteration 447/1000 | Loss: 0.00002078
Iteration 448/1000 | Loss: 0.00002078
Iteration 449/1000 | Loss: 0.00002078
Iteration 450/1000 | Loss: 0.00002078
Iteration 451/1000 | Loss: 0.00002078
Iteration 452/1000 | Loss: 0.00002078
Iteration 453/1000 | Loss: 0.00002078
Iteration 454/1000 | Loss: 0.00002078
Iteration 455/1000 | Loss: 0.00002078
Iteration 456/1000 | Loss: 0.00002078
Iteration 457/1000 | Loss: 0.00002078
Iteration 458/1000 | Loss: 0.00002078
Iteration 459/1000 | Loss: 0.00002077
Iteration 460/1000 | Loss: 0.00002077
Iteration 461/1000 | Loss: 0.00002077
Iteration 462/1000 | Loss: 0.00002077
Iteration 463/1000 | Loss: 0.00002077
Iteration 464/1000 | Loss: 0.00002077
Iteration 465/1000 | Loss: 0.00002077
Iteration 466/1000 | Loss: 0.00002077
Iteration 467/1000 | Loss: 0.00002077
Iteration 468/1000 | Loss: 0.00002077
Iteration 469/1000 | Loss: 0.00002077
Iteration 470/1000 | Loss: 0.00002077
Iteration 471/1000 | Loss: 0.00002076
Iteration 472/1000 | Loss: 0.00002076
Iteration 473/1000 | Loss: 0.00002076
Iteration 474/1000 | Loss: 0.00002076
Iteration 475/1000 | Loss: 0.00002076
Iteration 476/1000 | Loss: 0.00002076
Iteration 477/1000 | Loss: 0.00002076
Iteration 478/1000 | Loss: 0.00002076
Iteration 479/1000 | Loss: 0.00002076
Iteration 480/1000 | Loss: 0.00002076
Iteration 481/1000 | Loss: 0.00002075
Iteration 482/1000 | Loss: 0.00002075
Iteration 483/1000 | Loss: 0.00002075
Iteration 484/1000 | Loss: 0.00002075
Iteration 485/1000 | Loss: 0.00002075
Iteration 486/1000 | Loss: 0.00002075
Iteration 487/1000 | Loss: 0.00002075
Iteration 488/1000 | Loss: 0.00002075
Iteration 489/1000 | Loss: 0.00002075
Iteration 490/1000 | Loss: 0.00002075
Iteration 491/1000 | Loss: 0.00002075
Iteration 492/1000 | Loss: 0.00002075
Iteration 493/1000 | Loss: 0.00002075
Iteration 494/1000 | Loss: 0.00002075
Iteration 495/1000 | Loss: 0.00002075
Iteration 496/1000 | Loss: 0.00002074
Iteration 497/1000 | Loss: 0.00002074
Iteration 498/1000 | Loss: 0.00002074
Iteration 499/1000 | Loss: 0.00002074
Iteration 500/1000 | Loss: 0.00002074
Iteration 501/1000 | Loss: 0.00002074
Iteration 502/1000 | Loss: 0.00002074
Iteration 503/1000 | Loss: 0.00002074
Iteration 504/1000 | Loss: 0.00002074
Iteration 505/1000 | Loss: 0.00002074
Iteration 506/1000 | Loss: 0.00002074
Iteration 507/1000 | Loss: 0.00002074
Iteration 508/1000 | Loss: 0.00002074
Iteration 509/1000 | Loss: 0.00002074
Iteration 510/1000 | Loss: 0.00002074
Iteration 511/1000 | Loss: 0.00002074
Iteration 512/1000 | Loss: 0.00002074
Iteration 513/1000 | Loss: 0.00002074
Iteration 514/1000 | Loss: 0.00002074
Iteration 515/1000 | Loss: 0.00002074
Iteration 516/1000 | Loss: 0.00002074
Iteration 517/1000 | Loss: 0.00002074
Iteration 518/1000 | Loss: 0.00002074
Iteration 519/1000 | Loss: 0.00002074
Iteration 520/1000 | Loss: 0.00002074
Iteration 521/1000 | Loss: 0.00002074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 521. Stopping optimization.
Last 5 losses: [2.0738089006044902e-05, 2.0738089006044902e-05, 2.0738089006044902e-05, 2.0738089006044902e-05, 2.0738089006044902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0738089006044902e-05

Optimization complete. Final v2v error: 3.9061672687530518 mm

Highest mean error: 9.008103370666504 mm for frame 153

Lowest mean error: 3.448108196258545 mm for frame 146

Saving results

Total time: 647.7757186889648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695034
Iteration 2/25 | Loss: 0.00141330
Iteration 3/25 | Loss: 0.00117676
Iteration 4/25 | Loss: 0.00115038
Iteration 5/25 | Loss: 0.00114450
Iteration 6/25 | Loss: 0.00114274
Iteration 7/25 | Loss: 0.00114274
Iteration 8/25 | Loss: 0.00114274
Iteration 9/25 | Loss: 0.00114274
Iteration 10/25 | Loss: 0.00114274
Iteration 11/25 | Loss: 0.00114274
Iteration 12/25 | Loss: 0.00114274
Iteration 13/25 | Loss: 0.00114274
Iteration 14/25 | Loss: 0.00114274
Iteration 15/25 | Loss: 0.00114274
Iteration 16/25 | Loss: 0.00114274
Iteration 17/25 | Loss: 0.00114274
Iteration 18/25 | Loss: 0.00114274
Iteration 19/25 | Loss: 0.00114274
Iteration 20/25 | Loss: 0.00114274
Iteration 21/25 | Loss: 0.00114274
Iteration 22/25 | Loss: 0.00114274
Iteration 23/25 | Loss: 0.00114274
Iteration 24/25 | Loss: 0.00114274
Iteration 25/25 | Loss: 0.00114274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80348277
Iteration 2/25 | Loss: 0.00086389
Iteration 3/25 | Loss: 0.00086387
Iteration 4/25 | Loss: 0.00086387
Iteration 5/25 | Loss: 0.00086387
Iteration 6/25 | Loss: 0.00086387
Iteration 7/25 | Loss: 0.00086387
Iteration 8/25 | Loss: 0.00086387
Iteration 9/25 | Loss: 0.00086387
Iteration 10/25 | Loss: 0.00086387
Iteration 11/25 | Loss: 0.00086387
Iteration 12/25 | Loss: 0.00086387
Iteration 13/25 | Loss: 0.00086387
Iteration 14/25 | Loss: 0.00086387
Iteration 15/25 | Loss: 0.00086387
Iteration 16/25 | Loss: 0.00086387
Iteration 17/25 | Loss: 0.00086387
Iteration 18/25 | Loss: 0.00086387
Iteration 19/25 | Loss: 0.00086387
Iteration 20/25 | Loss: 0.00086387
Iteration 21/25 | Loss: 0.00086387
Iteration 22/25 | Loss: 0.00086387
Iteration 23/25 | Loss: 0.00086387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008638681611046195, 0.0008638681611046195, 0.0008638681611046195, 0.0008638681611046195, 0.0008638681611046195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008638681611046195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086387
Iteration 2/1000 | Loss: 0.00003706
Iteration 3/1000 | Loss: 0.00002508
Iteration 4/1000 | Loss: 0.00002332
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002164
Iteration 7/1000 | Loss: 0.00002107
Iteration 8/1000 | Loss: 0.00002064
Iteration 9/1000 | Loss: 0.00002034
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001992
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001978
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001977
Iteration 27/1000 | Loss: 0.00001977
Iteration 28/1000 | Loss: 0.00001977
Iteration 29/1000 | Loss: 0.00001977
Iteration 30/1000 | Loss: 0.00001974
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001973
Iteration 33/1000 | Loss: 0.00001973
Iteration 34/1000 | Loss: 0.00001973
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001972
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001971
Iteration 39/1000 | Loss: 0.00001971
Iteration 40/1000 | Loss: 0.00001971
Iteration 41/1000 | Loss: 0.00001970
Iteration 42/1000 | Loss: 0.00001970
Iteration 43/1000 | Loss: 0.00001970
Iteration 44/1000 | Loss: 0.00001969
Iteration 45/1000 | Loss: 0.00001969
Iteration 46/1000 | Loss: 0.00001969
Iteration 47/1000 | Loss: 0.00001969
Iteration 48/1000 | Loss: 0.00001968
Iteration 49/1000 | Loss: 0.00001968
Iteration 50/1000 | Loss: 0.00001968
Iteration 51/1000 | Loss: 0.00001968
Iteration 52/1000 | Loss: 0.00001968
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001968
Iteration 56/1000 | Loss: 0.00001968
Iteration 57/1000 | Loss: 0.00001967
Iteration 58/1000 | Loss: 0.00001967
Iteration 59/1000 | Loss: 0.00001967
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001966
Iteration 62/1000 | Loss: 0.00001966
Iteration 63/1000 | Loss: 0.00001966
Iteration 64/1000 | Loss: 0.00001966
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001964
Iteration 71/1000 | Loss: 0.00001964
Iteration 72/1000 | Loss: 0.00001964
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00001964
Iteration 75/1000 | Loss: 0.00001964
Iteration 76/1000 | Loss: 0.00001963
Iteration 77/1000 | Loss: 0.00001963
Iteration 78/1000 | Loss: 0.00001963
Iteration 79/1000 | Loss: 0.00001963
Iteration 80/1000 | Loss: 0.00001963
Iteration 81/1000 | Loss: 0.00001962
Iteration 82/1000 | Loss: 0.00001962
Iteration 83/1000 | Loss: 0.00001962
Iteration 84/1000 | Loss: 0.00001962
Iteration 85/1000 | Loss: 0.00001962
Iteration 86/1000 | Loss: 0.00001962
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001962
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001961
Iteration 92/1000 | Loss: 0.00001961
Iteration 93/1000 | Loss: 0.00001961
Iteration 94/1000 | Loss: 0.00001961
Iteration 95/1000 | Loss: 0.00001961
Iteration 96/1000 | Loss: 0.00001961
Iteration 97/1000 | Loss: 0.00001961
Iteration 98/1000 | Loss: 0.00001961
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001960
Iteration 102/1000 | Loss: 0.00001960
Iteration 103/1000 | Loss: 0.00001960
Iteration 104/1000 | Loss: 0.00001960
Iteration 105/1000 | Loss: 0.00001960
Iteration 106/1000 | Loss: 0.00001960
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001960
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001959
Iteration 113/1000 | Loss: 0.00001959
Iteration 114/1000 | Loss: 0.00001959
Iteration 115/1000 | Loss: 0.00001959
Iteration 116/1000 | Loss: 0.00001959
Iteration 117/1000 | Loss: 0.00001959
Iteration 118/1000 | Loss: 0.00001959
Iteration 119/1000 | Loss: 0.00001959
Iteration 120/1000 | Loss: 0.00001959
Iteration 121/1000 | Loss: 0.00001959
Iteration 122/1000 | Loss: 0.00001959
Iteration 123/1000 | Loss: 0.00001959
Iteration 124/1000 | Loss: 0.00001959
Iteration 125/1000 | Loss: 0.00001958
Iteration 126/1000 | Loss: 0.00001958
Iteration 127/1000 | Loss: 0.00001958
Iteration 128/1000 | Loss: 0.00001958
Iteration 129/1000 | Loss: 0.00001958
Iteration 130/1000 | Loss: 0.00001958
Iteration 131/1000 | Loss: 0.00001958
Iteration 132/1000 | Loss: 0.00001958
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001958
Iteration 137/1000 | Loss: 0.00001958
Iteration 138/1000 | Loss: 0.00001958
Iteration 139/1000 | Loss: 0.00001958
Iteration 140/1000 | Loss: 0.00001958
Iteration 141/1000 | Loss: 0.00001958
Iteration 142/1000 | Loss: 0.00001957
Iteration 143/1000 | Loss: 0.00001957
Iteration 144/1000 | Loss: 0.00001957
Iteration 145/1000 | Loss: 0.00001957
Iteration 146/1000 | Loss: 0.00001957
Iteration 147/1000 | Loss: 0.00001957
Iteration 148/1000 | Loss: 0.00001956
Iteration 149/1000 | Loss: 0.00001956
Iteration 150/1000 | Loss: 0.00001956
Iteration 151/1000 | Loss: 0.00001956
Iteration 152/1000 | Loss: 0.00001956
Iteration 153/1000 | Loss: 0.00001956
Iteration 154/1000 | Loss: 0.00001956
Iteration 155/1000 | Loss: 0.00001956
Iteration 156/1000 | Loss: 0.00001956
Iteration 157/1000 | Loss: 0.00001956
Iteration 158/1000 | Loss: 0.00001956
Iteration 159/1000 | Loss: 0.00001956
Iteration 160/1000 | Loss: 0.00001955
Iteration 161/1000 | Loss: 0.00001955
Iteration 162/1000 | Loss: 0.00001955
Iteration 163/1000 | Loss: 0.00001955
Iteration 164/1000 | Loss: 0.00001955
Iteration 165/1000 | Loss: 0.00001955
Iteration 166/1000 | Loss: 0.00001955
Iteration 167/1000 | Loss: 0.00001955
Iteration 168/1000 | Loss: 0.00001955
Iteration 169/1000 | Loss: 0.00001955
Iteration 170/1000 | Loss: 0.00001955
Iteration 171/1000 | Loss: 0.00001955
Iteration 172/1000 | Loss: 0.00001955
Iteration 173/1000 | Loss: 0.00001955
Iteration 174/1000 | Loss: 0.00001955
Iteration 175/1000 | Loss: 0.00001955
Iteration 176/1000 | Loss: 0.00001955
Iteration 177/1000 | Loss: 0.00001955
Iteration 178/1000 | Loss: 0.00001955
Iteration 179/1000 | Loss: 0.00001955
Iteration 180/1000 | Loss: 0.00001954
Iteration 181/1000 | Loss: 0.00001954
Iteration 182/1000 | Loss: 0.00001954
Iteration 183/1000 | Loss: 0.00001954
Iteration 184/1000 | Loss: 0.00001954
Iteration 185/1000 | Loss: 0.00001954
Iteration 186/1000 | Loss: 0.00001954
Iteration 187/1000 | Loss: 0.00001954
Iteration 188/1000 | Loss: 0.00001954
Iteration 189/1000 | Loss: 0.00001954
Iteration 190/1000 | Loss: 0.00001954
Iteration 191/1000 | Loss: 0.00001954
Iteration 192/1000 | Loss: 0.00001954
Iteration 193/1000 | Loss: 0.00001954
Iteration 194/1000 | Loss: 0.00001954
Iteration 195/1000 | Loss: 0.00001954
Iteration 196/1000 | Loss: 0.00001954
Iteration 197/1000 | Loss: 0.00001953
Iteration 198/1000 | Loss: 0.00001953
Iteration 199/1000 | Loss: 0.00001953
Iteration 200/1000 | Loss: 0.00001953
Iteration 201/1000 | Loss: 0.00001953
Iteration 202/1000 | Loss: 0.00001953
Iteration 203/1000 | Loss: 0.00001953
Iteration 204/1000 | Loss: 0.00001953
Iteration 205/1000 | Loss: 0.00001953
Iteration 206/1000 | Loss: 0.00001953
Iteration 207/1000 | Loss: 0.00001953
Iteration 208/1000 | Loss: 0.00001953
Iteration 209/1000 | Loss: 0.00001953
Iteration 210/1000 | Loss: 0.00001953
Iteration 211/1000 | Loss: 0.00001953
Iteration 212/1000 | Loss: 0.00001953
Iteration 213/1000 | Loss: 0.00001952
Iteration 214/1000 | Loss: 0.00001952
Iteration 215/1000 | Loss: 0.00001952
Iteration 216/1000 | Loss: 0.00001952
Iteration 217/1000 | Loss: 0.00001952
Iteration 218/1000 | Loss: 0.00001952
Iteration 219/1000 | Loss: 0.00001952
Iteration 220/1000 | Loss: 0.00001952
Iteration 221/1000 | Loss: 0.00001952
Iteration 222/1000 | Loss: 0.00001952
Iteration 223/1000 | Loss: 0.00001952
Iteration 224/1000 | Loss: 0.00001952
Iteration 225/1000 | Loss: 0.00001952
Iteration 226/1000 | Loss: 0.00001952
Iteration 227/1000 | Loss: 0.00001952
Iteration 228/1000 | Loss: 0.00001952
Iteration 229/1000 | Loss: 0.00001952
Iteration 230/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.9524997696862556e-05, 1.9524997696862556e-05, 1.9524997696862556e-05, 1.9524997696862556e-05, 1.9524997696862556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9524997696862556e-05

Optimization complete. Final v2v error: 3.6653287410736084 mm

Highest mean error: 4.686681747436523 mm for frame 93

Lowest mean error: 3.166367769241333 mm for frame 21

Saving results

Total time: 45.97714352607727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013582
Iteration 2/25 | Loss: 0.00183957
Iteration 3/25 | Loss: 0.00154218
Iteration 4/25 | Loss: 0.00150169
Iteration 5/25 | Loss: 0.00147447
Iteration 6/25 | Loss: 0.00144691
Iteration 7/25 | Loss: 0.00141505
Iteration 8/25 | Loss: 0.00139731
Iteration 9/25 | Loss: 0.00139149
Iteration 10/25 | Loss: 0.00138525
Iteration 11/25 | Loss: 0.00138678
Iteration 12/25 | Loss: 0.00138347
Iteration 13/25 | Loss: 0.00137565
Iteration 14/25 | Loss: 0.00137009
Iteration 15/25 | Loss: 0.00136621
Iteration 16/25 | Loss: 0.00136540
Iteration 17/25 | Loss: 0.00137062
Iteration 18/25 | Loss: 0.00136234
Iteration 19/25 | Loss: 0.00136044
Iteration 20/25 | Loss: 0.00136000
Iteration 21/25 | Loss: 0.00135991
Iteration 22/25 | Loss: 0.00135980
Iteration 23/25 | Loss: 0.00135976
Iteration 24/25 | Loss: 0.00135976
Iteration 25/25 | Loss: 0.00135976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.20442374
Iteration 2/25 | Loss: 0.00184777
Iteration 3/25 | Loss: 0.00184777
Iteration 4/25 | Loss: 0.00184777
Iteration 5/25 | Loss: 0.00184777
Iteration 6/25 | Loss: 0.00184777
Iteration 7/25 | Loss: 0.00184777
Iteration 8/25 | Loss: 0.00184777
Iteration 9/25 | Loss: 0.00184777
Iteration 10/25 | Loss: 0.00184777
Iteration 11/25 | Loss: 0.00184777
Iteration 12/25 | Loss: 0.00184777
Iteration 13/25 | Loss: 0.00184777
Iteration 14/25 | Loss: 0.00184777
Iteration 15/25 | Loss: 0.00184777
Iteration 16/25 | Loss: 0.00184777
Iteration 17/25 | Loss: 0.00184777
Iteration 18/25 | Loss: 0.00184777
Iteration 19/25 | Loss: 0.00184777
Iteration 20/25 | Loss: 0.00184777
Iteration 21/25 | Loss: 0.00184777
Iteration 22/25 | Loss: 0.00184777
Iteration 23/25 | Loss: 0.00184777
Iteration 24/25 | Loss: 0.00184777
Iteration 25/25 | Loss: 0.00184777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184777
Iteration 2/1000 | Loss: 0.00014066
Iteration 3/1000 | Loss: 0.00046830
Iteration 4/1000 | Loss: 0.00025855
Iteration 5/1000 | Loss: 0.00008533
Iteration 6/1000 | Loss: 0.00030882
Iteration 7/1000 | Loss: 0.00009997
Iteration 8/1000 | Loss: 0.00006500
Iteration 9/1000 | Loss: 0.00005662
Iteration 10/1000 | Loss: 0.00005057
Iteration 11/1000 | Loss: 0.00004668
Iteration 12/1000 | Loss: 0.00004491
Iteration 13/1000 | Loss: 0.00004322
Iteration 14/1000 | Loss: 0.00004227
Iteration 15/1000 | Loss: 0.00004138
Iteration 16/1000 | Loss: 0.00026645
Iteration 17/1000 | Loss: 0.00004570
Iteration 18/1000 | Loss: 0.00004057
Iteration 19/1000 | Loss: 0.00003866
Iteration 20/1000 | Loss: 0.00003764
Iteration 21/1000 | Loss: 0.00003706
Iteration 22/1000 | Loss: 0.00003667
Iteration 23/1000 | Loss: 0.00003644
Iteration 24/1000 | Loss: 0.00003620
Iteration 25/1000 | Loss: 0.00003599
Iteration 26/1000 | Loss: 0.00003591
Iteration 27/1000 | Loss: 0.00003590
Iteration 28/1000 | Loss: 0.00003587
Iteration 29/1000 | Loss: 0.00003587
Iteration 30/1000 | Loss: 0.00003586
Iteration 31/1000 | Loss: 0.00003586
Iteration 32/1000 | Loss: 0.00003585
Iteration 33/1000 | Loss: 0.00003584
Iteration 34/1000 | Loss: 0.00003580
Iteration 35/1000 | Loss: 0.00003577
Iteration 36/1000 | Loss: 0.00003577
Iteration 37/1000 | Loss: 0.00003574
Iteration 38/1000 | Loss: 0.00003574
Iteration 39/1000 | Loss: 0.00003574
Iteration 40/1000 | Loss: 0.00003574
Iteration 41/1000 | Loss: 0.00003574
Iteration 42/1000 | Loss: 0.00003574
Iteration 43/1000 | Loss: 0.00003574
Iteration 44/1000 | Loss: 0.00003574
Iteration 45/1000 | Loss: 0.00003574
Iteration 46/1000 | Loss: 0.00003574
Iteration 47/1000 | Loss: 0.00003573
Iteration 48/1000 | Loss: 0.00003573
Iteration 49/1000 | Loss: 0.00003571
Iteration 50/1000 | Loss: 0.00003570
Iteration 51/1000 | Loss: 0.00003570
Iteration 52/1000 | Loss: 0.00003570
Iteration 53/1000 | Loss: 0.00003570
Iteration 54/1000 | Loss: 0.00003570
Iteration 55/1000 | Loss: 0.00003570
Iteration 56/1000 | Loss: 0.00003569
Iteration 57/1000 | Loss: 0.00003569
Iteration 58/1000 | Loss: 0.00003569
Iteration 59/1000 | Loss: 0.00003568
Iteration 60/1000 | Loss: 0.00003568
Iteration 61/1000 | Loss: 0.00003567
Iteration 62/1000 | Loss: 0.00003567
Iteration 63/1000 | Loss: 0.00003567
Iteration 64/1000 | Loss: 0.00003566
Iteration 65/1000 | Loss: 0.00003566
Iteration 66/1000 | Loss: 0.00003566
Iteration 67/1000 | Loss: 0.00003566
Iteration 68/1000 | Loss: 0.00003566
Iteration 69/1000 | Loss: 0.00003566
Iteration 70/1000 | Loss: 0.00003566
Iteration 71/1000 | Loss: 0.00003566
Iteration 72/1000 | Loss: 0.00003566
Iteration 73/1000 | Loss: 0.00003565
Iteration 74/1000 | Loss: 0.00003565
Iteration 75/1000 | Loss: 0.00003565
Iteration 76/1000 | Loss: 0.00003565
Iteration 77/1000 | Loss: 0.00003565
Iteration 78/1000 | Loss: 0.00003565
Iteration 79/1000 | Loss: 0.00003565
Iteration 80/1000 | Loss: 0.00003565
Iteration 81/1000 | Loss: 0.00003565
Iteration 82/1000 | Loss: 0.00003565
Iteration 83/1000 | Loss: 0.00003565
Iteration 84/1000 | Loss: 0.00003565
Iteration 85/1000 | Loss: 0.00003565
Iteration 86/1000 | Loss: 0.00003564
Iteration 87/1000 | Loss: 0.00003564
Iteration 88/1000 | Loss: 0.00003564
Iteration 89/1000 | Loss: 0.00003564
Iteration 90/1000 | Loss: 0.00003564
Iteration 91/1000 | Loss: 0.00003564
Iteration 92/1000 | Loss: 0.00003563
Iteration 93/1000 | Loss: 0.00003563
Iteration 94/1000 | Loss: 0.00003563
Iteration 95/1000 | Loss: 0.00003563
Iteration 96/1000 | Loss: 0.00003563
Iteration 97/1000 | Loss: 0.00003563
Iteration 98/1000 | Loss: 0.00003563
Iteration 99/1000 | Loss: 0.00003563
Iteration 100/1000 | Loss: 0.00003563
Iteration 101/1000 | Loss: 0.00003563
Iteration 102/1000 | Loss: 0.00003563
Iteration 103/1000 | Loss: 0.00003563
Iteration 104/1000 | Loss: 0.00003563
Iteration 105/1000 | Loss: 0.00003563
Iteration 106/1000 | Loss: 0.00003563
Iteration 107/1000 | Loss: 0.00003563
Iteration 108/1000 | Loss: 0.00003563
Iteration 109/1000 | Loss: 0.00003563
Iteration 110/1000 | Loss: 0.00003563
Iteration 111/1000 | Loss: 0.00003563
Iteration 112/1000 | Loss: 0.00003563
Iteration 113/1000 | Loss: 0.00003563
Iteration 114/1000 | Loss: 0.00003563
Iteration 115/1000 | Loss: 0.00003563
Iteration 116/1000 | Loss: 0.00003563
Iteration 117/1000 | Loss: 0.00003563
Iteration 118/1000 | Loss: 0.00003563
Iteration 119/1000 | Loss: 0.00003563
Iteration 120/1000 | Loss: 0.00003563
Iteration 121/1000 | Loss: 0.00003563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.562659549061209e-05, 3.562659549061209e-05, 3.562659549061209e-05, 3.562659549061209e-05, 3.562659549061209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.562659549061209e-05

Optimization complete. Final v2v error: 4.810054302215576 mm

Highest mean error: 6.152320384979248 mm for frame 0

Lowest mean error: 3.975659132003784 mm for frame 137

Saving results

Total time: 81.10257768630981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442405
Iteration 2/25 | Loss: 0.00122167
Iteration 3/25 | Loss: 0.00111742
Iteration 4/25 | Loss: 0.00110275
Iteration 5/25 | Loss: 0.00109720
Iteration 6/25 | Loss: 0.00109577
Iteration 7/25 | Loss: 0.00109577
Iteration 8/25 | Loss: 0.00109577
Iteration 9/25 | Loss: 0.00109577
Iteration 10/25 | Loss: 0.00109577
Iteration 11/25 | Loss: 0.00109577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001095765270292759, 0.001095765270292759, 0.001095765270292759, 0.001095765270292759, 0.001095765270292759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095765270292759

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45254230
Iteration 2/25 | Loss: 0.00153697
Iteration 3/25 | Loss: 0.00153696
Iteration 4/25 | Loss: 0.00153696
Iteration 5/25 | Loss: 0.00153696
Iteration 6/25 | Loss: 0.00153696
Iteration 7/25 | Loss: 0.00153696
Iteration 8/25 | Loss: 0.00153696
Iteration 9/25 | Loss: 0.00153696
Iteration 10/25 | Loss: 0.00153696
Iteration 11/25 | Loss: 0.00153696
Iteration 12/25 | Loss: 0.00153696
Iteration 13/25 | Loss: 0.00153696
Iteration 14/25 | Loss: 0.00153696
Iteration 15/25 | Loss: 0.00153696
Iteration 16/25 | Loss: 0.00153696
Iteration 17/25 | Loss: 0.00153696
Iteration 18/25 | Loss: 0.00153696
Iteration 19/25 | Loss: 0.00153696
Iteration 20/25 | Loss: 0.00153696
Iteration 21/25 | Loss: 0.00153696
Iteration 22/25 | Loss: 0.00153696
Iteration 23/25 | Loss: 0.00153696
Iteration 24/25 | Loss: 0.00153696
Iteration 25/25 | Loss: 0.00153696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153696
Iteration 2/1000 | Loss: 0.00003073
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001700
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001491
Iteration 8/1000 | Loss: 0.00001446
Iteration 9/1000 | Loss: 0.00001413
Iteration 10/1000 | Loss: 0.00001389
Iteration 11/1000 | Loss: 0.00001380
Iteration 12/1000 | Loss: 0.00001380
Iteration 13/1000 | Loss: 0.00001378
Iteration 14/1000 | Loss: 0.00001378
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001373
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001366
Iteration 24/1000 | Loss: 0.00001365
Iteration 25/1000 | Loss: 0.00001364
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001363
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001356
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001350
Iteration 38/1000 | Loss: 0.00001349
Iteration 39/1000 | Loss: 0.00001349
Iteration 40/1000 | Loss: 0.00001348
Iteration 41/1000 | Loss: 0.00001348
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001345
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001343
Iteration 66/1000 | Loss: 0.00001343
Iteration 67/1000 | Loss: 0.00001343
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001342
Iteration 72/1000 | Loss: 0.00001342
Iteration 73/1000 | Loss: 0.00001342
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001341
Iteration 76/1000 | Loss: 0.00001341
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001339
Iteration 80/1000 | Loss: 0.00001339
Iteration 81/1000 | Loss: 0.00001339
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00001337
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001333
Iteration 130/1000 | Loss: 0.00001333
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001333
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001333
Iteration 137/1000 | Loss: 0.00001333
Iteration 138/1000 | Loss: 0.00001333
Iteration 139/1000 | Loss: 0.00001333
Iteration 140/1000 | Loss: 0.00001333
Iteration 141/1000 | Loss: 0.00001333
Iteration 142/1000 | Loss: 0.00001333
Iteration 143/1000 | Loss: 0.00001333
Iteration 144/1000 | Loss: 0.00001333
Iteration 145/1000 | Loss: 0.00001333
Iteration 146/1000 | Loss: 0.00001333
Iteration 147/1000 | Loss: 0.00001333
Iteration 148/1000 | Loss: 0.00001333
Iteration 149/1000 | Loss: 0.00001333
Iteration 150/1000 | Loss: 0.00001333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3331434274732601e-05, 1.3331434274732601e-05, 1.3331434274732601e-05, 1.3331434274732601e-05, 1.3331434274732601e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3331434274732601e-05

Optimization complete. Final v2v error: 3.2066233158111572 mm

Highest mean error: 3.6713531017303467 mm for frame 92

Lowest mean error: 2.6124179363250732 mm for frame 200

Saving results

Total time: 39.24515223503113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052910
Iteration 2/25 | Loss: 0.00227236
Iteration 3/25 | Loss: 0.00181935
Iteration 4/25 | Loss: 0.00156998
Iteration 5/25 | Loss: 0.00143222
Iteration 6/25 | Loss: 0.00149884
Iteration 7/25 | Loss: 0.00139421
Iteration 8/25 | Loss: 0.00125080
Iteration 9/25 | Loss: 0.00121298
Iteration 10/25 | Loss: 0.00119014
Iteration 11/25 | Loss: 0.00117044
Iteration 12/25 | Loss: 0.00115872
Iteration 13/25 | Loss: 0.00114620
Iteration 14/25 | Loss: 0.00114050
Iteration 15/25 | Loss: 0.00113649
Iteration 16/25 | Loss: 0.00113601
Iteration 17/25 | Loss: 0.00113399
Iteration 18/25 | Loss: 0.00113306
Iteration 19/25 | Loss: 0.00113042
Iteration 20/25 | Loss: 0.00113037
Iteration 21/25 | Loss: 0.00113024
Iteration 22/25 | Loss: 0.00112844
Iteration 23/25 | Loss: 0.00113734
Iteration 24/25 | Loss: 0.00113080
Iteration 25/25 | Loss: 0.00113206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17388690
Iteration 2/25 | Loss: 0.00184137
Iteration 3/25 | Loss: 0.00184137
Iteration 4/25 | Loss: 0.00184137
Iteration 5/25 | Loss: 0.00184136
Iteration 6/25 | Loss: 0.00184136
Iteration 7/25 | Loss: 0.00184136
Iteration 8/25 | Loss: 0.00184136
Iteration 9/25 | Loss: 0.00184136
Iteration 10/25 | Loss: 0.00184136
Iteration 11/25 | Loss: 0.00184136
Iteration 12/25 | Loss: 0.00184136
Iteration 13/25 | Loss: 0.00184136
Iteration 14/25 | Loss: 0.00184136
Iteration 15/25 | Loss: 0.00184136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001841363264247775, 0.001841363264247775, 0.001841363264247775, 0.001841363264247775, 0.001841363264247775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001841363264247775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184136
Iteration 2/1000 | Loss: 0.00010786
Iteration 3/1000 | Loss: 0.00008403
Iteration 4/1000 | Loss: 0.00009004
Iteration 5/1000 | Loss: 0.00008174
Iteration 6/1000 | Loss: 0.00008315
Iteration 7/1000 | Loss: 0.00008334
Iteration 8/1000 | Loss: 0.00006488
Iteration 9/1000 | Loss: 0.00008157
Iteration 10/1000 | Loss: 0.00008101
Iteration 11/1000 | Loss: 0.00009191
Iteration 12/1000 | Loss: 0.00005813
Iteration 13/1000 | Loss: 0.00007589
Iteration 14/1000 | Loss: 0.00007746
Iteration 15/1000 | Loss: 0.00008129
Iteration 16/1000 | Loss: 0.00007362
Iteration 17/1000 | Loss: 0.00006326
Iteration 18/1000 | Loss: 0.00007172
Iteration 19/1000 | Loss: 0.00006158
Iteration 20/1000 | Loss: 0.00007000
Iteration 21/1000 | Loss: 0.00008579
Iteration 22/1000 | Loss: 0.00007839
Iteration 23/1000 | Loss: 0.00006263
Iteration 24/1000 | Loss: 0.00006666
Iteration 25/1000 | Loss: 0.00007244
Iteration 26/1000 | Loss: 0.00007501
Iteration 27/1000 | Loss: 0.00009246
Iteration 28/1000 | Loss: 0.00009419
Iteration 29/1000 | Loss: 0.00006672
Iteration 30/1000 | Loss: 0.00006799
Iteration 31/1000 | Loss: 0.00006384
Iteration 32/1000 | Loss: 0.00007588
Iteration 33/1000 | Loss: 0.00007613
Iteration 34/1000 | Loss: 0.00007016
Iteration 35/1000 | Loss: 0.00006363
Iteration 36/1000 | Loss: 0.00008216
Iteration 37/1000 | Loss: 0.00006308
Iteration 38/1000 | Loss: 0.00008413
Iteration 39/1000 | Loss: 0.00007573
Iteration 40/1000 | Loss: 0.00009521
Iteration 41/1000 | Loss: 0.00007504
Iteration 42/1000 | Loss: 0.00009860
Iteration 43/1000 | Loss: 0.00007219
Iteration 44/1000 | Loss: 0.00009134
Iteration 45/1000 | Loss: 0.00005960
Iteration 46/1000 | Loss: 0.00006786
Iteration 47/1000 | Loss: 0.00006640
Iteration 48/1000 | Loss: 0.00006689
Iteration 49/1000 | Loss: 0.00007107
Iteration 50/1000 | Loss: 0.00006313
Iteration 51/1000 | Loss: 0.00005718
Iteration 52/1000 | Loss: 0.00007024
Iteration 53/1000 | Loss: 0.00006074
Iteration 54/1000 | Loss: 0.00007252
Iteration 55/1000 | Loss: 0.00006221
Iteration 56/1000 | Loss: 0.00007291
Iteration 57/1000 | Loss: 0.00006246
Iteration 58/1000 | Loss: 0.00007188
Iteration 59/1000 | Loss: 0.00006396
Iteration 60/1000 | Loss: 0.00007008
Iteration 61/1000 | Loss: 0.00005786
Iteration 62/1000 | Loss: 0.00005777
Iteration 63/1000 | Loss: 0.00005855
Iteration 64/1000 | Loss: 0.00006648
Iteration 65/1000 | Loss: 0.00006268
Iteration 66/1000 | Loss: 0.00005716
Iteration 67/1000 | Loss: 0.00008911
Iteration 68/1000 | Loss: 0.00007348
Iteration 69/1000 | Loss: 0.00008747
Iteration 70/1000 | Loss: 0.00005893
Iteration 71/1000 | Loss: 0.00005459
Iteration 72/1000 | Loss: 0.00004830
Iteration 73/1000 | Loss: 0.00004654
Iteration 74/1000 | Loss: 0.00004417
Iteration 75/1000 | Loss: 0.00004329
Iteration 76/1000 | Loss: 0.00004931
Iteration 77/1000 | Loss: 0.00004237
Iteration 78/1000 | Loss: 0.00004058
Iteration 79/1000 | Loss: 0.00005940
Iteration 80/1000 | Loss: 0.00005276
Iteration 81/1000 | Loss: 0.00004647
Iteration 82/1000 | Loss: 0.00005989
Iteration 83/1000 | Loss: 0.00004239
Iteration 84/1000 | Loss: 0.00003906
Iteration 85/1000 | Loss: 0.00003811
Iteration 86/1000 | Loss: 0.00003715
Iteration 87/1000 | Loss: 0.00003644
Iteration 88/1000 | Loss: 0.00003590
Iteration 89/1000 | Loss: 0.00003555
Iteration 90/1000 | Loss: 0.00003508
Iteration 91/1000 | Loss: 0.00003472
Iteration 92/1000 | Loss: 0.00003459
Iteration 93/1000 | Loss: 0.00003446
Iteration 94/1000 | Loss: 0.00003444
Iteration 95/1000 | Loss: 0.00003442
Iteration 96/1000 | Loss: 0.00003441
Iteration 97/1000 | Loss: 0.00003438
Iteration 98/1000 | Loss: 0.00003436
Iteration 99/1000 | Loss: 0.00003436
Iteration 100/1000 | Loss: 0.00003435
Iteration 101/1000 | Loss: 0.00003435
Iteration 102/1000 | Loss: 0.00003435
Iteration 103/1000 | Loss: 0.00003434
Iteration 104/1000 | Loss: 0.00003433
Iteration 105/1000 | Loss: 0.00003433
Iteration 106/1000 | Loss: 0.00003432
Iteration 107/1000 | Loss: 0.00003431
Iteration 108/1000 | Loss: 0.00003428
Iteration 109/1000 | Loss: 0.00003427
Iteration 110/1000 | Loss: 0.00003427
Iteration 111/1000 | Loss: 0.00003425
Iteration 112/1000 | Loss: 0.00003425
Iteration 113/1000 | Loss: 0.00003568
Iteration 114/1000 | Loss: 0.00003567
Iteration 115/1000 | Loss: 0.00003567
Iteration 116/1000 | Loss: 0.00003566
Iteration 117/1000 | Loss: 0.00003566
Iteration 118/1000 | Loss: 0.00003565
Iteration 119/1000 | Loss: 0.00003545
Iteration 120/1000 | Loss: 0.00003520
Iteration 121/1000 | Loss: 0.00003469
Iteration 122/1000 | Loss: 0.00003436
Iteration 123/1000 | Loss: 0.00003464
Iteration 124/1000 | Loss: 0.00003447
Iteration 125/1000 | Loss: 0.00003469
Iteration 126/1000 | Loss: 0.00003451
Iteration 127/1000 | Loss: 0.00003464
Iteration 128/1000 | Loss: 0.00003463
Iteration 129/1000 | Loss: 0.00003459
Iteration 130/1000 | Loss: 0.00003454
Iteration 131/1000 | Loss: 0.00003454
Iteration 132/1000 | Loss: 0.00003450
Iteration 133/1000 | Loss: 0.00003451
Iteration 134/1000 | Loss: 0.00003443
Iteration 135/1000 | Loss: 0.00003456
Iteration 136/1000 | Loss: 0.00003428
Iteration 137/1000 | Loss: 0.00003441
Iteration 138/1000 | Loss: 0.00003462
Iteration 139/1000 | Loss: 0.00003451
Iteration 140/1000 | Loss: 0.00003462
Iteration 141/1000 | Loss: 0.00003457
Iteration 142/1000 | Loss: 0.00003456
Iteration 143/1000 | Loss: 0.00003449
Iteration 144/1000 | Loss: 0.00003452
Iteration 145/1000 | Loss: 0.00003449
Iteration 146/1000 | Loss: 0.00003463
Iteration 147/1000 | Loss: 0.00003463
Iteration 148/1000 | Loss: 0.00003450
Iteration 149/1000 | Loss: 0.00003451
Iteration 150/1000 | Loss: 0.00003450
Iteration 151/1000 | Loss: 0.00003456
Iteration 152/1000 | Loss: 0.00003451
Iteration 153/1000 | Loss: 0.00003449
Iteration 154/1000 | Loss: 0.00003447
Iteration 155/1000 | Loss: 0.00003447
Iteration 156/1000 | Loss: 0.00003447
Iteration 157/1000 | Loss: 0.00003447
Iteration 158/1000 | Loss: 0.00003446
Iteration 159/1000 | Loss: 0.00003446
Iteration 160/1000 | Loss: 0.00003446
Iteration 161/1000 | Loss: 0.00003446
Iteration 162/1000 | Loss: 0.00003446
Iteration 163/1000 | Loss: 0.00003446
Iteration 164/1000 | Loss: 0.00003446
Iteration 165/1000 | Loss: 0.00003445
Iteration 166/1000 | Loss: 0.00003445
Iteration 167/1000 | Loss: 0.00003445
Iteration 168/1000 | Loss: 0.00003445
Iteration 169/1000 | Loss: 0.00003445
Iteration 170/1000 | Loss: 0.00003445
Iteration 171/1000 | Loss: 0.00003445
Iteration 172/1000 | Loss: 0.00003445
Iteration 173/1000 | Loss: 0.00003445
Iteration 174/1000 | Loss: 0.00003444
Iteration 175/1000 | Loss: 0.00003443
Iteration 176/1000 | Loss: 0.00003442
Iteration 177/1000 | Loss: 0.00003441
Iteration 178/1000 | Loss: 0.00003441
Iteration 179/1000 | Loss: 0.00003440
Iteration 180/1000 | Loss: 0.00003419
Iteration 181/1000 | Loss: 0.00003452
Iteration 182/1000 | Loss: 0.00003444
Iteration 183/1000 | Loss: 0.00003446
Iteration 184/1000 | Loss: 0.00003446
Iteration 185/1000 | Loss: 0.00003446
Iteration 186/1000 | Loss: 0.00003445
Iteration 187/1000 | Loss: 0.00003439
Iteration 188/1000 | Loss: 0.00003448
Iteration 189/1000 | Loss: 0.00003440
Iteration 190/1000 | Loss: 0.00003405
Iteration 191/1000 | Loss: 0.00003449
Iteration 192/1000 | Loss: 0.00003442
Iteration 193/1000 | Loss: 0.00003451
Iteration 194/1000 | Loss: 0.00003445
Iteration 195/1000 | Loss: 0.00003449
Iteration 196/1000 | Loss: 0.00003446
Iteration 197/1000 | Loss: 0.00003454
Iteration 198/1000 | Loss: 0.00003400
Iteration 199/1000 | Loss: 0.00003400
Iteration 200/1000 | Loss: 0.00003454
Iteration 201/1000 | Loss: 0.00003454
Iteration 202/1000 | Loss: 0.00003453
Iteration 203/1000 | Loss: 0.00003453
Iteration 204/1000 | Loss: 0.00003452
Iteration 205/1000 | Loss: 0.00003408
Iteration 206/1000 | Loss: 0.00003399
Iteration 207/1000 | Loss: 0.00003399
Iteration 208/1000 | Loss: 0.00003399
Iteration 209/1000 | Loss: 0.00003399
Iteration 210/1000 | Loss: 0.00003399
Iteration 211/1000 | Loss: 0.00003399
Iteration 212/1000 | Loss: 0.00003399
Iteration 213/1000 | Loss: 0.00003399
Iteration 214/1000 | Loss: 0.00003399
Iteration 215/1000 | Loss: 0.00003399
Iteration 216/1000 | Loss: 0.00003399
Iteration 217/1000 | Loss: 0.00003399
Iteration 218/1000 | Loss: 0.00003399
Iteration 219/1000 | Loss: 0.00003399
Iteration 220/1000 | Loss: 0.00003399
Iteration 221/1000 | Loss: 0.00003399
Iteration 222/1000 | Loss: 0.00003399
Iteration 223/1000 | Loss: 0.00003399
Iteration 224/1000 | Loss: 0.00003399
Iteration 225/1000 | Loss: 0.00003399
Iteration 226/1000 | Loss: 0.00003399
Iteration 227/1000 | Loss: 0.00003399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [3.398940680199303e-05, 3.398940680199303e-05, 3.398940680199303e-05, 3.398940680199303e-05, 3.398940680199303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.398940680199303e-05

Optimization complete. Final v2v error: 4.14454984664917 mm

Highest mean error: 14.322457313537598 mm for frame 36

Lowest mean error: 3.4424190521240234 mm for frame 132

Saving results

Total time: 232.0877766609192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853570
Iteration 2/25 | Loss: 0.00148882
Iteration 3/25 | Loss: 0.00120981
Iteration 4/25 | Loss: 0.00118229
Iteration 5/25 | Loss: 0.00117414
Iteration 6/25 | Loss: 0.00117189
Iteration 7/25 | Loss: 0.00117189
Iteration 8/25 | Loss: 0.00117189
Iteration 9/25 | Loss: 0.00117189
Iteration 10/25 | Loss: 0.00117189
Iteration 11/25 | Loss: 0.00117189
Iteration 12/25 | Loss: 0.00117189
Iteration 13/25 | Loss: 0.00117189
Iteration 14/25 | Loss: 0.00117189
Iteration 15/25 | Loss: 0.00117189
Iteration 16/25 | Loss: 0.00117189
Iteration 17/25 | Loss: 0.00117189
Iteration 18/25 | Loss: 0.00117189
Iteration 19/25 | Loss: 0.00117189
Iteration 20/25 | Loss: 0.00117189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001171894371509552, 0.001171894371509552, 0.001171894371509552, 0.001171894371509552, 0.001171894371509552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001171894371509552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.00861549
Iteration 2/25 | Loss: 0.00162497
Iteration 3/25 | Loss: 0.00162497
Iteration 4/25 | Loss: 0.00162497
Iteration 5/25 | Loss: 0.00162497
Iteration 6/25 | Loss: 0.00162497
Iteration 7/25 | Loss: 0.00162497
Iteration 8/25 | Loss: 0.00162497
Iteration 9/25 | Loss: 0.00162497
Iteration 10/25 | Loss: 0.00162497
Iteration 11/25 | Loss: 0.00162497
Iteration 12/25 | Loss: 0.00162497
Iteration 13/25 | Loss: 0.00162497
Iteration 14/25 | Loss: 0.00162497
Iteration 15/25 | Loss: 0.00162497
Iteration 16/25 | Loss: 0.00162497
Iteration 17/25 | Loss: 0.00162497
Iteration 18/25 | Loss: 0.00162497
Iteration 19/25 | Loss: 0.00162497
Iteration 20/25 | Loss: 0.00162497
Iteration 21/25 | Loss: 0.00162497
Iteration 22/25 | Loss: 0.00162497
Iteration 23/25 | Loss: 0.00162497
Iteration 24/25 | Loss: 0.00162497
Iteration 25/25 | Loss: 0.00162497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162497
Iteration 2/1000 | Loss: 0.00005496
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00002354
Iteration 5/1000 | Loss: 0.00002181
Iteration 6/1000 | Loss: 0.00002073
Iteration 7/1000 | Loss: 0.00002006
Iteration 8/1000 | Loss: 0.00001956
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001853
Iteration 13/1000 | Loss: 0.00001848
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001827
Iteration 21/1000 | Loss: 0.00001826
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001825
Iteration 24/1000 | Loss: 0.00001825
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001824
Iteration 28/1000 | Loss: 0.00001823
Iteration 29/1000 | Loss: 0.00001823
Iteration 30/1000 | Loss: 0.00001822
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001822
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001817
Iteration 47/1000 | Loss: 0.00001816
Iteration 48/1000 | Loss: 0.00001816
Iteration 49/1000 | Loss: 0.00001816
Iteration 50/1000 | Loss: 0.00001816
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001815
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001814
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001813
Iteration 58/1000 | Loss: 0.00001813
Iteration 59/1000 | Loss: 0.00001813
Iteration 60/1000 | Loss: 0.00001812
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001812
Iteration 66/1000 | Loss: 0.00001811
Iteration 67/1000 | Loss: 0.00001811
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001809
Iteration 80/1000 | Loss: 0.00001809
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001808
Iteration 86/1000 | Loss: 0.00001808
Iteration 87/1000 | Loss: 0.00001808
Iteration 88/1000 | Loss: 0.00001808
Iteration 89/1000 | Loss: 0.00001808
Iteration 90/1000 | Loss: 0.00001808
Iteration 91/1000 | Loss: 0.00001808
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001807
Iteration 94/1000 | Loss: 0.00001807
Iteration 95/1000 | Loss: 0.00001807
Iteration 96/1000 | Loss: 0.00001807
Iteration 97/1000 | Loss: 0.00001807
Iteration 98/1000 | Loss: 0.00001807
Iteration 99/1000 | Loss: 0.00001807
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001807
Iteration 105/1000 | Loss: 0.00001807
Iteration 106/1000 | Loss: 0.00001807
Iteration 107/1000 | Loss: 0.00001807
Iteration 108/1000 | Loss: 0.00001807
Iteration 109/1000 | Loss: 0.00001807
Iteration 110/1000 | Loss: 0.00001807
Iteration 111/1000 | Loss: 0.00001807
Iteration 112/1000 | Loss: 0.00001807
Iteration 113/1000 | Loss: 0.00001807
Iteration 114/1000 | Loss: 0.00001807
Iteration 115/1000 | Loss: 0.00001807
Iteration 116/1000 | Loss: 0.00001807
Iteration 117/1000 | Loss: 0.00001807
Iteration 118/1000 | Loss: 0.00001807
Iteration 119/1000 | Loss: 0.00001807
Iteration 120/1000 | Loss: 0.00001807
Iteration 121/1000 | Loss: 0.00001807
Iteration 122/1000 | Loss: 0.00001807
Iteration 123/1000 | Loss: 0.00001807
Iteration 124/1000 | Loss: 0.00001807
Iteration 125/1000 | Loss: 0.00001807
Iteration 126/1000 | Loss: 0.00001807
Iteration 127/1000 | Loss: 0.00001807
Iteration 128/1000 | Loss: 0.00001807
Iteration 129/1000 | Loss: 0.00001807
Iteration 130/1000 | Loss: 0.00001807
Iteration 131/1000 | Loss: 0.00001807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.8067503333440982e-05, 1.8067503333440982e-05, 1.8067503333440982e-05, 1.8067503333440982e-05, 1.8067503333440982e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8067503333440982e-05

Optimization complete. Final v2v error: 3.615037441253662 mm

Highest mean error: 4.432826519012451 mm for frame 141

Lowest mean error: 2.9024250507354736 mm for frame 127

Saving results

Total time: 40.268444299697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092062
Iteration 2/25 | Loss: 0.00227051
Iteration 3/25 | Loss: 0.00205432
Iteration 4/25 | Loss: 0.00219051
Iteration 5/25 | Loss: 0.00146240
Iteration 6/25 | Loss: 0.00135734
Iteration 7/25 | Loss: 0.00133336
Iteration 8/25 | Loss: 0.00126048
Iteration 9/25 | Loss: 0.00124906
Iteration 10/25 | Loss: 0.00124569
Iteration 11/25 | Loss: 0.00123912
Iteration 12/25 | Loss: 0.00123505
Iteration 13/25 | Loss: 0.00123425
Iteration 14/25 | Loss: 0.00123407
Iteration 15/25 | Loss: 0.00123400
Iteration 16/25 | Loss: 0.00123400
Iteration 17/25 | Loss: 0.00123400
Iteration 18/25 | Loss: 0.00123400
Iteration 19/25 | Loss: 0.00123400
Iteration 20/25 | Loss: 0.00123400
Iteration 21/25 | Loss: 0.00123400
Iteration 22/25 | Loss: 0.00123400
Iteration 23/25 | Loss: 0.00123400
Iteration 24/25 | Loss: 0.00123399
Iteration 25/25 | Loss: 0.00123399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18267751
Iteration 2/25 | Loss: 0.00119127
Iteration 3/25 | Loss: 0.00119126
Iteration 4/25 | Loss: 0.00119126
Iteration 5/25 | Loss: 0.00119126
Iteration 6/25 | Loss: 0.00119126
Iteration 7/25 | Loss: 0.00119126
Iteration 8/25 | Loss: 0.00119126
Iteration 9/25 | Loss: 0.00119126
Iteration 10/25 | Loss: 0.00119126
Iteration 11/25 | Loss: 0.00119126
Iteration 12/25 | Loss: 0.00119126
Iteration 13/25 | Loss: 0.00119126
Iteration 14/25 | Loss: 0.00119126
Iteration 15/25 | Loss: 0.00119126
Iteration 16/25 | Loss: 0.00119126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011912562185898423, 0.0011912562185898423, 0.0011912562185898423, 0.0011912562185898423, 0.0011912562185898423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011912562185898423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119126
Iteration 2/1000 | Loss: 0.00005536
Iteration 3/1000 | Loss: 0.00003537
Iteration 4/1000 | Loss: 0.00003265
Iteration 5/1000 | Loss: 0.00003091
Iteration 6/1000 | Loss: 0.00002984
Iteration 7/1000 | Loss: 0.00002847
Iteration 8/1000 | Loss: 0.00002742
Iteration 9/1000 | Loss: 0.00002689
Iteration 10/1000 | Loss: 0.00002643
Iteration 11/1000 | Loss: 0.00002628
Iteration 12/1000 | Loss: 0.00002607
Iteration 13/1000 | Loss: 0.00002590
Iteration 14/1000 | Loss: 0.00002577
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002571
Iteration 17/1000 | Loss: 0.00002571
Iteration 18/1000 | Loss: 0.00002569
Iteration 19/1000 | Loss: 0.00002563
Iteration 20/1000 | Loss: 0.00002562
Iteration 21/1000 | Loss: 0.00002562
Iteration 22/1000 | Loss: 0.00002561
Iteration 23/1000 | Loss: 0.00002561
Iteration 24/1000 | Loss: 0.00002560
Iteration 25/1000 | Loss: 0.00002560
Iteration 26/1000 | Loss: 0.00002558
Iteration 27/1000 | Loss: 0.00002558
Iteration 28/1000 | Loss: 0.00002558
Iteration 29/1000 | Loss: 0.00002558
Iteration 30/1000 | Loss: 0.00002558
Iteration 31/1000 | Loss: 0.00002557
Iteration 32/1000 | Loss: 0.00002557
Iteration 33/1000 | Loss: 0.00002557
Iteration 34/1000 | Loss: 0.00002556
Iteration 35/1000 | Loss: 0.00002556
Iteration 36/1000 | Loss: 0.00002556
Iteration 37/1000 | Loss: 0.00002556
Iteration 38/1000 | Loss: 0.00002556
Iteration 39/1000 | Loss: 0.00002556
Iteration 40/1000 | Loss: 0.00002556
Iteration 41/1000 | Loss: 0.00002556
Iteration 42/1000 | Loss: 0.00002556
Iteration 43/1000 | Loss: 0.00002556
Iteration 44/1000 | Loss: 0.00002556
Iteration 45/1000 | Loss: 0.00002555
Iteration 46/1000 | Loss: 0.00002555
Iteration 47/1000 | Loss: 0.00002555
Iteration 48/1000 | Loss: 0.00002555
Iteration 49/1000 | Loss: 0.00002555
Iteration 50/1000 | Loss: 0.00002555
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002554
Iteration 53/1000 | Loss: 0.00002554
Iteration 54/1000 | Loss: 0.00002554
Iteration 55/1000 | Loss: 0.00002554
Iteration 56/1000 | Loss: 0.00002554
Iteration 57/1000 | Loss: 0.00002554
Iteration 58/1000 | Loss: 0.00002554
Iteration 59/1000 | Loss: 0.00002553
Iteration 60/1000 | Loss: 0.00002553
Iteration 61/1000 | Loss: 0.00002553
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002553
Iteration 64/1000 | Loss: 0.00002553
Iteration 65/1000 | Loss: 0.00002552
Iteration 66/1000 | Loss: 0.00002552
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002551
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002549
Iteration 74/1000 | Loss: 0.00002549
Iteration 75/1000 | Loss: 0.00002549
Iteration 76/1000 | Loss: 0.00002549
Iteration 77/1000 | Loss: 0.00002548
Iteration 78/1000 | Loss: 0.00002547
Iteration 79/1000 | Loss: 0.00002547
Iteration 80/1000 | Loss: 0.00002547
Iteration 81/1000 | Loss: 0.00002547
Iteration 82/1000 | Loss: 0.00002547
Iteration 83/1000 | Loss: 0.00002547
Iteration 84/1000 | Loss: 0.00002546
Iteration 85/1000 | Loss: 0.00002546
Iteration 86/1000 | Loss: 0.00002546
Iteration 87/1000 | Loss: 0.00002546
Iteration 88/1000 | Loss: 0.00002546
Iteration 89/1000 | Loss: 0.00002546
Iteration 90/1000 | Loss: 0.00002546
Iteration 91/1000 | Loss: 0.00002546
Iteration 92/1000 | Loss: 0.00002546
Iteration 93/1000 | Loss: 0.00002546
Iteration 94/1000 | Loss: 0.00002546
Iteration 95/1000 | Loss: 0.00002546
Iteration 96/1000 | Loss: 0.00002546
Iteration 97/1000 | Loss: 0.00002545
Iteration 98/1000 | Loss: 0.00002546
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002546
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002546
Iteration 104/1000 | Loss: 0.00002545
Iteration 105/1000 | Loss: 0.00002545
Iteration 106/1000 | Loss: 0.00002546
Iteration 107/1000 | Loss: 0.00002546
Iteration 108/1000 | Loss: 0.00002546
Iteration 109/1000 | Loss: 0.00002546
Iteration 110/1000 | Loss: 0.00002546
Iteration 111/1000 | Loss: 0.00002546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.545500137784984e-05, 2.545500137784984e-05, 2.545500137784984e-05, 2.545500137784984e-05, 2.545500137784984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.545500137784984e-05

Optimization complete. Final v2v error: 4.387693881988525 mm

Highest mean error: 4.714447975158691 mm for frame 181

Lowest mean error: 4.085700511932373 mm for frame 44

Saving results

Total time: 59.40295767784119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538152
Iteration 2/25 | Loss: 0.00118593
Iteration 3/25 | Loss: 0.00107882
Iteration 4/25 | Loss: 0.00106712
Iteration 5/25 | Loss: 0.00106423
Iteration 6/25 | Loss: 0.00106381
Iteration 7/25 | Loss: 0.00106381
Iteration 8/25 | Loss: 0.00106381
Iteration 9/25 | Loss: 0.00106381
Iteration 10/25 | Loss: 0.00106381
Iteration 11/25 | Loss: 0.00106381
Iteration 12/25 | Loss: 0.00106381
Iteration 13/25 | Loss: 0.00106381
Iteration 14/25 | Loss: 0.00106381
Iteration 15/25 | Loss: 0.00106381
Iteration 16/25 | Loss: 0.00106381
Iteration 17/25 | Loss: 0.00106381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010638140374794602, 0.0010638140374794602, 0.0010638140374794602, 0.0010638140374794602, 0.0010638140374794602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010638140374794602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 18.51457214
Iteration 2/25 | Loss: 0.00139684
Iteration 3/25 | Loss: 0.00139676
Iteration 4/25 | Loss: 0.00139675
Iteration 5/25 | Loss: 0.00139675
Iteration 6/25 | Loss: 0.00139675
Iteration 7/25 | Loss: 0.00139675
Iteration 8/25 | Loss: 0.00139675
Iteration 9/25 | Loss: 0.00139675
Iteration 10/25 | Loss: 0.00139675
Iteration 11/25 | Loss: 0.00139675
Iteration 12/25 | Loss: 0.00139675
Iteration 13/25 | Loss: 0.00139675
Iteration 14/25 | Loss: 0.00139675
Iteration 15/25 | Loss: 0.00139675
Iteration 16/25 | Loss: 0.00139675
Iteration 17/25 | Loss: 0.00139675
Iteration 18/25 | Loss: 0.00139675
Iteration 19/25 | Loss: 0.00139675
Iteration 20/25 | Loss: 0.00139675
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013967504492029548, 0.0013967504492029548, 0.0013967504492029548, 0.0013967504492029548, 0.0013967504492029548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013967504492029548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139675
Iteration 2/1000 | Loss: 0.00002340
Iteration 3/1000 | Loss: 0.00001613
Iteration 4/1000 | Loss: 0.00001489
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001311
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001236
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001232
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001223
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001210
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001202
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001202
Iteration 39/1000 | Loss: 0.00001202
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001199
Iteration 46/1000 | Loss: 0.00001199
Iteration 47/1000 | Loss: 0.00001199
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001198
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001196
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001194
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001192
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001187
Iteration 130/1000 | Loss: 0.00001187
Iteration 131/1000 | Loss: 0.00001187
Iteration 132/1000 | Loss: 0.00001187
Iteration 133/1000 | Loss: 0.00001187
Iteration 134/1000 | Loss: 0.00001187
Iteration 135/1000 | Loss: 0.00001187
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001186
Iteration 143/1000 | Loss: 0.00001186
Iteration 144/1000 | Loss: 0.00001186
Iteration 145/1000 | Loss: 0.00001186
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001185
Iteration 149/1000 | Loss: 0.00001185
Iteration 150/1000 | Loss: 0.00001185
Iteration 151/1000 | Loss: 0.00001185
Iteration 152/1000 | Loss: 0.00001185
Iteration 153/1000 | Loss: 0.00001185
Iteration 154/1000 | Loss: 0.00001185
Iteration 155/1000 | Loss: 0.00001185
Iteration 156/1000 | Loss: 0.00001185
Iteration 157/1000 | Loss: 0.00001185
Iteration 158/1000 | Loss: 0.00001185
Iteration 159/1000 | Loss: 0.00001185
Iteration 160/1000 | Loss: 0.00001185
Iteration 161/1000 | Loss: 0.00001185
Iteration 162/1000 | Loss: 0.00001185
Iteration 163/1000 | Loss: 0.00001185
Iteration 164/1000 | Loss: 0.00001185
Iteration 165/1000 | Loss: 0.00001185
Iteration 166/1000 | Loss: 0.00001185
Iteration 167/1000 | Loss: 0.00001185
Iteration 168/1000 | Loss: 0.00001185
Iteration 169/1000 | Loss: 0.00001185
Iteration 170/1000 | Loss: 0.00001185
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.1847774658235721e-05, 1.1847774658235721e-05, 1.1847774658235721e-05, 1.1847774658235721e-05, 1.1847774658235721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1847774658235721e-05

Optimization complete. Final v2v error: 3.0386803150177 mm

Highest mean error: 3.430922746658325 mm for frame 137

Lowest mean error: 2.624683380126953 mm for frame 0

Saving results

Total time: 39.76737070083618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402286
Iteration 2/25 | Loss: 0.00120153
Iteration 3/25 | Loss: 0.00110588
Iteration 4/25 | Loss: 0.00109617
Iteration 5/25 | Loss: 0.00109337
Iteration 6/25 | Loss: 0.00109252
Iteration 7/25 | Loss: 0.00109243
Iteration 8/25 | Loss: 0.00109243
Iteration 9/25 | Loss: 0.00109243
Iteration 10/25 | Loss: 0.00109243
Iteration 11/25 | Loss: 0.00109243
Iteration 12/25 | Loss: 0.00109243
Iteration 13/25 | Loss: 0.00109243
Iteration 14/25 | Loss: 0.00109243
Iteration 15/25 | Loss: 0.00109243
Iteration 16/25 | Loss: 0.00109243
Iteration 17/25 | Loss: 0.00109243
Iteration 18/25 | Loss: 0.00109243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010924255475401878, 0.0010924255475401878, 0.0010924255475401878, 0.0010924255475401878, 0.0010924255475401878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010924255475401878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33296692
Iteration 2/25 | Loss: 0.00143890
Iteration 3/25 | Loss: 0.00143890
Iteration 4/25 | Loss: 0.00143889
Iteration 5/25 | Loss: 0.00143889
Iteration 6/25 | Loss: 0.00143889
Iteration 7/25 | Loss: 0.00143889
Iteration 8/25 | Loss: 0.00143889
Iteration 9/25 | Loss: 0.00143889
Iteration 10/25 | Loss: 0.00143889
Iteration 11/25 | Loss: 0.00143889
Iteration 12/25 | Loss: 0.00143889
Iteration 13/25 | Loss: 0.00143889
Iteration 14/25 | Loss: 0.00143889
Iteration 15/25 | Loss: 0.00143889
Iteration 16/25 | Loss: 0.00143889
Iteration 17/25 | Loss: 0.00143889
Iteration 18/25 | Loss: 0.00143889
Iteration 19/25 | Loss: 0.00143889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001438892213627696, 0.001438892213627696, 0.001438892213627696, 0.001438892213627696, 0.001438892213627696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001438892213627696

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143889
Iteration 2/1000 | Loss: 0.00003567
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001438
Iteration 5/1000 | Loss: 0.00001380
Iteration 6/1000 | Loss: 0.00001321
Iteration 7/1000 | Loss: 0.00001274
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001189
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001147
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001137
Iteration 18/1000 | Loss: 0.00001137
Iteration 19/1000 | Loss: 0.00001132
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001124
Iteration 26/1000 | Loss: 0.00001123
Iteration 27/1000 | Loss: 0.00001123
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001120
Iteration 30/1000 | Loss: 0.00001120
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001119
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001116
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001115
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001105
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001104
Iteration 71/1000 | Loss: 0.00001104
Iteration 72/1000 | Loss: 0.00001104
Iteration 73/1000 | Loss: 0.00001104
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001103
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001103
Iteration 80/1000 | Loss: 0.00001103
Iteration 81/1000 | Loss: 0.00001103
Iteration 82/1000 | Loss: 0.00001103
Iteration 83/1000 | Loss: 0.00001103
Iteration 84/1000 | Loss: 0.00001103
Iteration 85/1000 | Loss: 0.00001103
Iteration 86/1000 | Loss: 0.00001102
Iteration 87/1000 | Loss: 0.00001102
Iteration 88/1000 | Loss: 0.00001102
Iteration 89/1000 | Loss: 0.00001102
Iteration 90/1000 | Loss: 0.00001102
Iteration 91/1000 | Loss: 0.00001102
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001101
Iteration 98/1000 | Loss: 0.00001101
Iteration 99/1000 | Loss: 0.00001101
Iteration 100/1000 | Loss: 0.00001101
Iteration 101/1000 | Loss: 0.00001101
Iteration 102/1000 | Loss: 0.00001101
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001101
Iteration 108/1000 | Loss: 0.00001101
Iteration 109/1000 | Loss: 0.00001101
Iteration 110/1000 | Loss: 0.00001101
Iteration 111/1000 | Loss: 0.00001101
Iteration 112/1000 | Loss: 0.00001101
Iteration 113/1000 | Loss: 0.00001101
Iteration 114/1000 | Loss: 0.00001101
Iteration 115/1000 | Loss: 0.00001101
Iteration 116/1000 | Loss: 0.00001100
Iteration 117/1000 | Loss: 0.00001100
Iteration 118/1000 | Loss: 0.00001100
Iteration 119/1000 | Loss: 0.00001100
Iteration 120/1000 | Loss: 0.00001100
Iteration 121/1000 | Loss: 0.00001100
Iteration 122/1000 | Loss: 0.00001100
Iteration 123/1000 | Loss: 0.00001100
Iteration 124/1000 | Loss: 0.00001100
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001100
Iteration 137/1000 | Loss: 0.00001100
Iteration 138/1000 | Loss: 0.00001100
Iteration 139/1000 | Loss: 0.00001100
Iteration 140/1000 | Loss: 0.00001100
Iteration 141/1000 | Loss: 0.00001100
Iteration 142/1000 | Loss: 0.00001100
Iteration 143/1000 | Loss: 0.00001100
Iteration 144/1000 | Loss: 0.00001100
Iteration 145/1000 | Loss: 0.00001100
Iteration 146/1000 | Loss: 0.00001100
Iteration 147/1000 | Loss: 0.00001100
Iteration 148/1000 | Loss: 0.00001100
Iteration 149/1000 | Loss: 0.00001100
Iteration 150/1000 | Loss: 0.00001100
Iteration 151/1000 | Loss: 0.00001100
Iteration 152/1000 | Loss: 0.00001100
Iteration 153/1000 | Loss: 0.00001100
Iteration 154/1000 | Loss: 0.00001100
Iteration 155/1000 | Loss: 0.00001100
Iteration 156/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1001592611137312e-05, 1.1001592611137312e-05, 1.1001592611137312e-05, 1.1001592611137312e-05, 1.1001592611137312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1001592611137312e-05

Optimization complete. Final v2v error: 2.921140670776367 mm

Highest mean error: 3.4017372131347656 mm for frame 66

Lowest mean error: 2.716137647628784 mm for frame 34

Saving results

Total time: 34.5486843585968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444107
Iteration 2/25 | Loss: 0.00121197
Iteration 3/25 | Loss: 0.00109693
Iteration 4/25 | Loss: 0.00108235
Iteration 5/25 | Loss: 0.00107788
Iteration 6/25 | Loss: 0.00107632
Iteration 7/25 | Loss: 0.00107632
Iteration 8/25 | Loss: 0.00107632
Iteration 9/25 | Loss: 0.00107632
Iteration 10/25 | Loss: 0.00107632
Iteration 11/25 | Loss: 0.00107632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001076323096640408, 0.001076323096640408, 0.001076323096640408, 0.001076323096640408, 0.001076323096640408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001076323096640408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20276701
Iteration 2/25 | Loss: 0.00160451
Iteration 3/25 | Loss: 0.00160451
Iteration 4/25 | Loss: 0.00160451
Iteration 5/25 | Loss: 0.00160451
Iteration 6/25 | Loss: 0.00160451
Iteration 7/25 | Loss: 0.00160451
Iteration 8/25 | Loss: 0.00160450
Iteration 9/25 | Loss: 0.00160450
Iteration 10/25 | Loss: 0.00160450
Iteration 11/25 | Loss: 0.00160450
Iteration 12/25 | Loss: 0.00160450
Iteration 13/25 | Loss: 0.00160450
Iteration 14/25 | Loss: 0.00160450
Iteration 15/25 | Loss: 0.00160450
Iteration 16/25 | Loss: 0.00160450
Iteration 17/25 | Loss: 0.00160450
Iteration 18/25 | Loss: 0.00160450
Iteration 19/25 | Loss: 0.00160450
Iteration 20/25 | Loss: 0.00160450
Iteration 21/25 | Loss: 0.00160450
Iteration 22/25 | Loss: 0.00160450
Iteration 23/25 | Loss: 0.00160450
Iteration 24/25 | Loss: 0.00160450
Iteration 25/25 | Loss: 0.00160450

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160450
Iteration 2/1000 | Loss: 0.00003282
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001412
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001263
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001249
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001233
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001228
Iteration 52/1000 | Loss: 0.00001228
Iteration 53/1000 | Loss: 0.00001228
Iteration 54/1000 | Loss: 0.00001228
Iteration 55/1000 | Loss: 0.00001228
Iteration 56/1000 | Loss: 0.00001228
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001227
Iteration 59/1000 | Loss: 0.00001227
Iteration 60/1000 | Loss: 0.00001227
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001226
Iteration 63/1000 | Loss: 0.00001226
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001225
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001225
Iteration 70/1000 | Loss: 0.00001225
Iteration 71/1000 | Loss: 0.00001225
Iteration 72/1000 | Loss: 0.00001224
Iteration 73/1000 | Loss: 0.00001224
Iteration 74/1000 | Loss: 0.00001224
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001223
Iteration 77/1000 | Loss: 0.00001223
Iteration 78/1000 | Loss: 0.00001223
Iteration 79/1000 | Loss: 0.00001223
Iteration 80/1000 | Loss: 0.00001223
Iteration 81/1000 | Loss: 0.00001223
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001222
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001222
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001220
Iteration 99/1000 | Loss: 0.00001220
Iteration 100/1000 | Loss: 0.00001220
Iteration 101/1000 | Loss: 0.00001220
Iteration 102/1000 | Loss: 0.00001220
Iteration 103/1000 | Loss: 0.00001220
Iteration 104/1000 | Loss: 0.00001220
Iteration 105/1000 | Loss: 0.00001220
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001220
Iteration 109/1000 | Loss: 0.00001220
Iteration 110/1000 | Loss: 0.00001220
Iteration 111/1000 | Loss: 0.00001220
Iteration 112/1000 | Loss: 0.00001220
Iteration 113/1000 | Loss: 0.00001219
Iteration 114/1000 | Loss: 0.00001219
Iteration 115/1000 | Loss: 0.00001219
Iteration 116/1000 | Loss: 0.00001219
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001219
Iteration 139/1000 | Loss: 0.00001219
Iteration 140/1000 | Loss: 0.00001219
Iteration 141/1000 | Loss: 0.00001219
Iteration 142/1000 | Loss: 0.00001219
Iteration 143/1000 | Loss: 0.00001219
Iteration 144/1000 | Loss: 0.00001219
Iteration 145/1000 | Loss: 0.00001219
Iteration 146/1000 | Loss: 0.00001219
Iteration 147/1000 | Loss: 0.00001219
Iteration 148/1000 | Loss: 0.00001219
Iteration 149/1000 | Loss: 0.00001219
Iteration 150/1000 | Loss: 0.00001219
Iteration 151/1000 | Loss: 0.00001219
Iteration 152/1000 | Loss: 0.00001219
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001219
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001219
Iteration 165/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.2191160749353003e-05, 1.2191160749353003e-05, 1.2191160749353003e-05, 1.2191160749353003e-05, 1.2191160749353003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2191160749353003e-05

Optimization complete. Final v2v error: 2.980726718902588 mm

Highest mean error: 3.3229074478149414 mm for frame 89

Lowest mean error: 2.436849594116211 mm for frame 8

Saving results

Total time: 39.50790047645569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061699
Iteration 2/25 | Loss: 0.00375656
Iteration 3/25 | Loss: 0.00251049
Iteration 4/25 | Loss: 0.00233756
Iteration 5/25 | Loss: 0.00210389
Iteration 6/25 | Loss: 0.00200505
Iteration 7/25 | Loss: 0.00194666
Iteration 8/25 | Loss: 0.00191481
Iteration 9/25 | Loss: 0.00189909
Iteration 10/25 | Loss: 0.00189677
Iteration 11/25 | Loss: 0.00189696
Iteration 12/25 | Loss: 0.00190008
Iteration 13/25 | Loss: 0.00189763
Iteration 14/25 | Loss: 0.00186035
Iteration 15/25 | Loss: 0.00181325
Iteration 16/25 | Loss: 0.00178243
Iteration 17/25 | Loss: 0.00177938
Iteration 18/25 | Loss: 0.00178298
Iteration 19/25 | Loss: 0.00177483
Iteration 20/25 | Loss: 0.00177160
Iteration 21/25 | Loss: 0.00176909
Iteration 22/25 | Loss: 0.00175944
Iteration 23/25 | Loss: 0.00176118
Iteration 24/25 | Loss: 0.00176334
Iteration 25/25 | Loss: 0.00176096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18556535
Iteration 2/25 | Loss: 0.00973114
Iteration 3/25 | Loss: 0.00682078
Iteration 4/25 | Loss: 0.00689340
Iteration 5/25 | Loss: 0.00682077
Iteration 6/25 | Loss: 0.00682077
Iteration 7/25 | Loss: 0.00682077
Iteration 8/25 | Loss: 0.00682077
Iteration 9/25 | Loss: 0.00682077
Iteration 10/25 | Loss: 0.00682077
Iteration 11/25 | Loss: 0.00682077
Iteration 12/25 | Loss: 0.00682077
Iteration 13/25 | Loss: 0.00682077
Iteration 14/25 | Loss: 0.00682077
Iteration 15/25 | Loss: 0.00682077
Iteration 16/25 | Loss: 0.00682077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.006820765323936939, 0.006820765323936939, 0.006820765323936939, 0.006820765323936939, 0.006820765323936939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006820765323936939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00682077
Iteration 2/1000 | Loss: 0.00422393
Iteration 3/1000 | Loss: 0.00299798
Iteration 4/1000 | Loss: 0.00117746
Iteration 5/1000 | Loss: 0.00064613
Iteration 6/1000 | Loss: 0.00095500
Iteration 7/1000 | Loss: 0.00175330
Iteration 8/1000 | Loss: 0.00625249
Iteration 9/1000 | Loss: 0.00096378
Iteration 10/1000 | Loss: 0.00057333
Iteration 11/1000 | Loss: 0.00156386
Iteration 12/1000 | Loss: 0.00057817
Iteration 13/1000 | Loss: 0.00109291
Iteration 14/1000 | Loss: 0.00126967
Iteration 15/1000 | Loss: 0.00143579
Iteration 16/1000 | Loss: 0.00060313
Iteration 17/1000 | Loss: 0.00200201
Iteration 18/1000 | Loss: 0.00109045
Iteration 19/1000 | Loss: 0.00040990
Iteration 20/1000 | Loss: 0.00021215
Iteration 21/1000 | Loss: 0.00019749
Iteration 22/1000 | Loss: 0.00068592
Iteration 23/1000 | Loss: 0.00153772
Iteration 24/1000 | Loss: 0.00033208
Iteration 25/1000 | Loss: 0.00052182
Iteration 26/1000 | Loss: 0.00027631
Iteration 27/1000 | Loss: 0.00017126
Iteration 28/1000 | Loss: 0.00016661
Iteration 29/1000 | Loss: 0.00104769
Iteration 30/1000 | Loss: 0.00208887
Iteration 31/1000 | Loss: 0.00808574
Iteration 32/1000 | Loss: 0.01348077
Iteration 33/1000 | Loss: 0.01019673
Iteration 34/1000 | Loss: 0.00290140
Iteration 35/1000 | Loss: 0.00346510
Iteration 36/1000 | Loss: 0.00287585
Iteration 37/1000 | Loss: 0.00343115
Iteration 38/1000 | Loss: 0.00051979
Iteration 39/1000 | Loss: 0.00099617
Iteration 40/1000 | Loss: 0.00083995
Iteration 41/1000 | Loss: 0.00136794
Iteration 42/1000 | Loss: 0.00014233
Iteration 43/1000 | Loss: 0.00034203
Iteration 44/1000 | Loss: 0.00014954
Iteration 45/1000 | Loss: 0.00086456
Iteration 46/1000 | Loss: 0.00101494
Iteration 47/1000 | Loss: 0.00022269
Iteration 48/1000 | Loss: 0.00020123
Iteration 49/1000 | Loss: 0.00074668
Iteration 50/1000 | Loss: 0.00017419
Iteration 51/1000 | Loss: 0.00044399
Iteration 52/1000 | Loss: 0.00030366
Iteration 53/1000 | Loss: 0.00003841
Iteration 54/1000 | Loss: 0.00003179
Iteration 55/1000 | Loss: 0.00002944
Iteration 56/1000 | Loss: 0.00015422
Iteration 57/1000 | Loss: 0.00010698
Iteration 58/1000 | Loss: 0.00013274
Iteration 59/1000 | Loss: 0.00002561
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00013495
Iteration 62/1000 | Loss: 0.00026320
Iteration 63/1000 | Loss: 0.00003757
Iteration 64/1000 | Loss: 0.00007702
Iteration 65/1000 | Loss: 0.00057149
Iteration 66/1000 | Loss: 0.00004422
Iteration 67/1000 | Loss: 0.00003041
Iteration 68/1000 | Loss: 0.00009004
Iteration 69/1000 | Loss: 0.00009996
Iteration 70/1000 | Loss: 0.00002733
Iteration 71/1000 | Loss: 0.00002400
Iteration 72/1000 | Loss: 0.00010019
Iteration 73/1000 | Loss: 0.00002342
Iteration 74/1000 | Loss: 0.00004993
Iteration 75/1000 | Loss: 0.00003052
Iteration 76/1000 | Loss: 0.00004519
Iteration 77/1000 | Loss: 0.00002296
Iteration 78/1000 | Loss: 0.00002294
Iteration 79/1000 | Loss: 0.00009725
Iteration 80/1000 | Loss: 0.00006943
Iteration 81/1000 | Loss: 0.00004005
Iteration 82/1000 | Loss: 0.00004714
Iteration 83/1000 | Loss: 0.00008576
Iteration 84/1000 | Loss: 0.00002792
Iteration 85/1000 | Loss: 0.00002299
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002267
Iteration 88/1000 | Loss: 0.00006467
Iteration 89/1000 | Loss: 0.00002521
Iteration 90/1000 | Loss: 0.00014962
Iteration 91/1000 | Loss: 0.00002589
Iteration 92/1000 | Loss: 0.00003985
Iteration 93/1000 | Loss: 0.00004849
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002255
Iteration 96/1000 | Loss: 0.00002254
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00012583
Iteration 99/1000 | Loss: 0.00007513
Iteration 100/1000 | Loss: 0.00012097
Iteration 101/1000 | Loss: 0.00010841
Iteration 102/1000 | Loss: 0.00002918
Iteration 103/1000 | Loss: 0.00006019
Iteration 104/1000 | Loss: 0.00002257
Iteration 105/1000 | Loss: 0.00002253
Iteration 106/1000 | Loss: 0.00002247
Iteration 107/1000 | Loss: 0.00002246
Iteration 108/1000 | Loss: 0.00002245
Iteration 109/1000 | Loss: 0.00002244
Iteration 110/1000 | Loss: 0.00002244
Iteration 111/1000 | Loss: 0.00002243
Iteration 112/1000 | Loss: 0.00002242
Iteration 113/1000 | Loss: 0.00002242
Iteration 114/1000 | Loss: 0.00002242
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002242
Iteration 117/1000 | Loss: 0.00002242
Iteration 118/1000 | Loss: 0.00002242
Iteration 119/1000 | Loss: 0.00002242
Iteration 120/1000 | Loss: 0.00002242
Iteration 121/1000 | Loss: 0.00002242
Iteration 122/1000 | Loss: 0.00002242
Iteration 123/1000 | Loss: 0.00002242
Iteration 124/1000 | Loss: 0.00002242
Iteration 125/1000 | Loss: 0.00002242
Iteration 126/1000 | Loss: 0.00002242
Iteration 127/1000 | Loss: 0.00002242
Iteration 128/1000 | Loss: 0.00002242
Iteration 129/1000 | Loss: 0.00002242
Iteration 130/1000 | Loss: 0.00002242
Iteration 131/1000 | Loss: 0.00002242
Iteration 132/1000 | Loss: 0.00002242
Iteration 133/1000 | Loss: 0.00002242
Iteration 134/1000 | Loss: 0.00002242
Iteration 135/1000 | Loss: 0.00002242
Iteration 136/1000 | Loss: 0.00002242
Iteration 137/1000 | Loss: 0.00002242
Iteration 138/1000 | Loss: 0.00002242
Iteration 139/1000 | Loss: 0.00002242
Iteration 140/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.2423955670092255e-05, 2.2423955670092255e-05, 2.2423955670092255e-05, 2.2423955670092255e-05, 2.2423955670092255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2423955670092255e-05

Optimization complete. Final v2v error: 3.8543827533721924 mm

Highest mean error: 12.168916702270508 mm for frame 111

Lowest mean error: 3.3111038208007812 mm for frame 89

Saving results

Total time: 187.32379961013794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063470
Iteration 2/25 | Loss: 0.00217417
Iteration 3/25 | Loss: 0.00140185
Iteration 4/25 | Loss: 0.00130157
Iteration 5/25 | Loss: 0.00132711
Iteration 6/25 | Loss: 0.00128937
Iteration 7/25 | Loss: 0.00123815
Iteration 8/25 | Loss: 0.00120851
Iteration 9/25 | Loss: 0.00120318
Iteration 10/25 | Loss: 0.00118864
Iteration 11/25 | Loss: 0.00117614
Iteration 12/25 | Loss: 0.00117342
Iteration 13/25 | Loss: 0.00117249
Iteration 14/25 | Loss: 0.00117249
Iteration 15/25 | Loss: 0.00117289
Iteration 16/25 | Loss: 0.00117248
Iteration 17/25 | Loss: 0.00117269
Iteration 18/25 | Loss: 0.00117281
Iteration 19/25 | Loss: 0.00117280
Iteration 20/25 | Loss: 0.00117280
Iteration 21/25 | Loss: 0.00117262
Iteration 22/25 | Loss: 0.00117200
Iteration 23/25 | Loss: 0.00117184
Iteration 24/25 | Loss: 0.00117183
Iteration 25/25 | Loss: 0.00117183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64802384
Iteration 2/25 | Loss: 0.00163265
Iteration 3/25 | Loss: 0.00163263
Iteration 4/25 | Loss: 0.00163263
Iteration 5/25 | Loss: 0.00163263
Iteration 6/25 | Loss: 0.00163263
Iteration 7/25 | Loss: 0.00163263
Iteration 8/25 | Loss: 0.00163263
Iteration 9/25 | Loss: 0.00163263
Iteration 10/25 | Loss: 0.00163263
Iteration 11/25 | Loss: 0.00163263
Iteration 12/25 | Loss: 0.00163263
Iteration 13/25 | Loss: 0.00163263
Iteration 14/25 | Loss: 0.00163263
Iteration 15/25 | Loss: 0.00163263
Iteration 16/25 | Loss: 0.00163263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016326275654137135, 0.0016326275654137135, 0.0016326275654137135, 0.0016326275654137135, 0.0016326275654137135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016326275654137135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163263
Iteration 2/1000 | Loss: 0.00011391
Iteration 3/1000 | Loss: 0.00008738
Iteration 4/1000 | Loss: 0.00006936
Iteration 5/1000 | Loss: 0.00008466
Iteration 6/1000 | Loss: 0.00005493
Iteration 7/1000 | Loss: 0.00005230
Iteration 8/1000 | Loss: 0.00005272
Iteration 9/1000 | Loss: 0.00005083
Iteration 10/1000 | Loss: 0.00051998
Iteration 11/1000 | Loss: 0.00013136
Iteration 12/1000 | Loss: 0.00050376
Iteration 13/1000 | Loss: 0.00154141
Iteration 14/1000 | Loss: 0.00042463
Iteration 15/1000 | Loss: 0.00163774
Iteration 16/1000 | Loss: 0.00091141
Iteration 17/1000 | Loss: 0.00156150
Iteration 18/1000 | Loss: 0.00017377
Iteration 19/1000 | Loss: 0.00109809
Iteration 20/1000 | Loss: 0.00037368
Iteration 21/1000 | Loss: 0.00040223
Iteration 22/1000 | Loss: 0.00068979
Iteration 23/1000 | Loss: 0.00028778
Iteration 24/1000 | Loss: 0.00007752
Iteration 25/1000 | Loss: 0.00085828
Iteration 26/1000 | Loss: 0.00017320
Iteration 27/1000 | Loss: 0.00037877
Iteration 28/1000 | Loss: 0.00046973
Iteration 29/1000 | Loss: 0.00124194
Iteration 30/1000 | Loss: 0.00060068
Iteration 31/1000 | Loss: 0.00068657
Iteration 32/1000 | Loss: 0.00039370
Iteration 33/1000 | Loss: 0.00045349
Iteration 34/1000 | Loss: 0.00044281
Iteration 35/1000 | Loss: 0.00027719
Iteration 36/1000 | Loss: 0.00043673
Iteration 37/1000 | Loss: 0.00025835
Iteration 38/1000 | Loss: 0.00042882
Iteration 39/1000 | Loss: 0.00096011
Iteration 40/1000 | Loss: 0.00056512
Iteration 41/1000 | Loss: 0.00057128
Iteration 42/1000 | Loss: 0.00035994
Iteration 43/1000 | Loss: 0.00017562
Iteration 44/1000 | Loss: 0.00006712
Iteration 45/1000 | Loss: 0.00026375
Iteration 46/1000 | Loss: 0.00006348
Iteration 47/1000 | Loss: 0.00005212
Iteration 48/1000 | Loss: 0.00004663
Iteration 49/1000 | Loss: 0.00004525
Iteration 50/1000 | Loss: 0.00004376
Iteration 51/1000 | Loss: 0.00004174
Iteration 52/1000 | Loss: 0.00004075
Iteration 53/1000 | Loss: 0.00021307
Iteration 54/1000 | Loss: 0.00025769
Iteration 55/1000 | Loss: 0.00027102
Iteration 56/1000 | Loss: 0.00019304
Iteration 57/1000 | Loss: 0.00005464
Iteration 58/1000 | Loss: 0.00003963
Iteration 59/1000 | Loss: 0.00003830
Iteration 60/1000 | Loss: 0.00003703
Iteration 61/1000 | Loss: 0.00003743
Iteration 62/1000 | Loss: 0.00003647
Iteration 63/1000 | Loss: 0.00003615
Iteration 64/1000 | Loss: 0.00003577
Iteration 65/1000 | Loss: 0.00003615
Iteration 66/1000 | Loss: 0.00003588
Iteration 67/1000 | Loss: 0.00003657
Iteration 68/1000 | Loss: 0.00003543
Iteration 69/1000 | Loss: 0.00003522
Iteration 70/1000 | Loss: 0.00003522
Iteration 71/1000 | Loss: 0.00003521
Iteration 72/1000 | Loss: 0.00003521
Iteration 73/1000 | Loss: 0.00003521
Iteration 74/1000 | Loss: 0.00003521
Iteration 75/1000 | Loss: 0.00003521
Iteration 76/1000 | Loss: 0.00003521
Iteration 77/1000 | Loss: 0.00003521
Iteration 78/1000 | Loss: 0.00003521
Iteration 79/1000 | Loss: 0.00003521
Iteration 80/1000 | Loss: 0.00003610
Iteration 81/1000 | Loss: 0.00003540
Iteration 82/1000 | Loss: 0.00003577
Iteration 83/1000 | Loss: 0.00003747
Iteration 84/1000 | Loss: 0.00003602
Iteration 85/1000 | Loss: 0.00003837
Iteration 86/1000 | Loss: 0.00003597
Iteration 87/1000 | Loss: 0.00003786
Iteration 88/1000 | Loss: 0.00003617
Iteration 89/1000 | Loss: 0.00003895
Iteration 90/1000 | Loss: 0.00003667
Iteration 91/1000 | Loss: 0.00003971
Iteration 92/1000 | Loss: 0.00003713
Iteration 93/1000 | Loss: 0.00003910
Iteration 94/1000 | Loss: 0.00003670
Iteration 95/1000 | Loss: 0.00003901
Iteration 96/1000 | Loss: 0.00003813
Iteration 97/1000 | Loss: 0.00003855
Iteration 98/1000 | Loss: 0.00003721
Iteration 99/1000 | Loss: 0.00004049
Iteration 100/1000 | Loss: 0.00003630
Iteration 101/1000 | Loss: 0.00003517
Iteration 102/1000 | Loss: 0.00003685
Iteration 103/1000 | Loss: 0.00003788
Iteration 104/1000 | Loss: 0.00004014
Iteration 105/1000 | Loss: 0.00003833
Iteration 106/1000 | Loss: 0.00003588
Iteration 107/1000 | Loss: 0.00003531
Iteration 108/1000 | Loss: 0.00003512
Iteration 109/1000 | Loss: 0.00003511
Iteration 110/1000 | Loss: 0.00003511
Iteration 111/1000 | Loss: 0.00003511
Iteration 112/1000 | Loss: 0.00003511
Iteration 113/1000 | Loss: 0.00003511
Iteration 114/1000 | Loss: 0.00003511
Iteration 115/1000 | Loss: 0.00003511
Iteration 116/1000 | Loss: 0.00003511
Iteration 117/1000 | Loss: 0.00003511
Iteration 118/1000 | Loss: 0.00003511
Iteration 119/1000 | Loss: 0.00003511
Iteration 120/1000 | Loss: 0.00003510
Iteration 121/1000 | Loss: 0.00003510
Iteration 122/1000 | Loss: 0.00003509
Iteration 123/1000 | Loss: 0.00003509
Iteration 124/1000 | Loss: 0.00003826
Iteration 125/1000 | Loss: 0.00003702
Iteration 126/1000 | Loss: 0.00003509
Iteration 127/1000 | Loss: 0.00003508
Iteration 128/1000 | Loss: 0.00003508
Iteration 129/1000 | Loss: 0.00003807
Iteration 130/1000 | Loss: 0.00003654
Iteration 131/1000 | Loss: 0.00003832
Iteration 132/1000 | Loss: 0.00003763
Iteration 133/1000 | Loss: 0.00004105
Iteration 134/1000 | Loss: 0.00003679
Iteration 135/1000 | Loss: 0.00003511
Iteration 136/1000 | Loss: 0.00003510
Iteration 137/1000 | Loss: 0.00003509
Iteration 138/1000 | Loss: 0.00003509
Iteration 139/1000 | Loss: 0.00003508
Iteration 140/1000 | Loss: 0.00003827
Iteration 141/1000 | Loss: 0.00003864
Iteration 142/1000 | Loss: 0.00003988
Iteration 143/1000 | Loss: 0.00003756
Iteration 144/1000 | Loss: 0.00004009
Iteration 145/1000 | Loss: 0.00004009
Iteration 146/1000 | Loss: 0.00003903
Iteration 147/1000 | Loss: 0.00003978
Iteration 148/1000 | Loss: 0.00003763
Iteration 149/1000 | Loss: 0.00004280
Iteration 150/1000 | Loss: 0.00003905
Iteration 151/1000 | Loss: 0.00003809
Iteration 152/1000 | Loss: 0.00003804
Iteration 153/1000 | Loss: 0.00004079
Iteration 154/1000 | Loss: 0.00003752
Iteration 155/1000 | Loss: 0.00004014
Iteration 156/1000 | Loss: 0.00004061
Iteration 157/1000 | Loss: 0.00003978
Iteration 158/1000 | Loss: 0.00004060
Iteration 159/1000 | Loss: 0.00003929
Iteration 160/1000 | Loss: 0.00003839
Iteration 161/1000 | Loss: 0.00003938
Iteration 162/1000 | Loss: 0.00003831
Iteration 163/1000 | Loss: 0.00003814
Iteration 164/1000 | Loss: 0.00003664
Iteration 165/1000 | Loss: 0.00003543
Iteration 166/1000 | Loss: 0.00003866
Iteration 167/1000 | Loss: 0.00003596
Iteration 168/1000 | Loss: 0.00003576
Iteration 169/1000 | Loss: 0.00003520
Iteration 170/1000 | Loss: 0.00003511
Iteration 171/1000 | Loss: 0.00003511
Iteration 172/1000 | Loss: 0.00003511
Iteration 173/1000 | Loss: 0.00003511
Iteration 174/1000 | Loss: 0.00003510
Iteration 175/1000 | Loss: 0.00003510
Iteration 176/1000 | Loss: 0.00003510
Iteration 177/1000 | Loss: 0.00003510
Iteration 178/1000 | Loss: 0.00003509
Iteration 179/1000 | Loss: 0.00003508
Iteration 180/1000 | Loss: 0.00003526
Iteration 181/1000 | Loss: 0.00003526
Iteration 182/1000 | Loss: 0.00003526
Iteration 183/1000 | Loss: 0.00003526
Iteration 184/1000 | Loss: 0.00003524
Iteration 185/1000 | Loss: 0.00003523
Iteration 186/1000 | Loss: 0.00003522
Iteration 187/1000 | Loss: 0.00003521
Iteration 188/1000 | Loss: 0.00003520
Iteration 189/1000 | Loss: 0.00003520
Iteration 190/1000 | Loss: 0.00003520
Iteration 191/1000 | Loss: 0.00003520
Iteration 192/1000 | Loss: 0.00003520
Iteration 193/1000 | Loss: 0.00003519
Iteration 194/1000 | Loss: 0.00003519
Iteration 195/1000 | Loss: 0.00003519
Iteration 196/1000 | Loss: 0.00003519
Iteration 197/1000 | Loss: 0.00003519
Iteration 198/1000 | Loss: 0.00003519
Iteration 199/1000 | Loss: 0.00003518
Iteration 200/1000 | Loss: 0.00003518
Iteration 201/1000 | Loss: 0.00003517
Iteration 202/1000 | Loss: 0.00003517
Iteration 203/1000 | Loss: 0.00003517
Iteration 204/1000 | Loss: 0.00003573
Iteration 205/1000 | Loss: 0.00003524
Iteration 206/1000 | Loss: 0.00003507
Iteration 207/1000 | Loss: 0.00003507
Iteration 208/1000 | Loss: 0.00003506
Iteration 209/1000 | Loss: 0.00003506
Iteration 210/1000 | Loss: 0.00003505
Iteration 211/1000 | Loss: 0.00003505
Iteration 212/1000 | Loss: 0.00003505
Iteration 213/1000 | Loss: 0.00003505
Iteration 214/1000 | Loss: 0.00003505
Iteration 215/1000 | Loss: 0.00003505
Iteration 216/1000 | Loss: 0.00003504
Iteration 217/1000 | Loss: 0.00003504
Iteration 218/1000 | Loss: 0.00003504
Iteration 219/1000 | Loss: 0.00003504
Iteration 220/1000 | Loss: 0.00003504
Iteration 221/1000 | Loss: 0.00003504
Iteration 222/1000 | Loss: 0.00003504
Iteration 223/1000 | Loss: 0.00003504
Iteration 224/1000 | Loss: 0.00003504
Iteration 225/1000 | Loss: 0.00003504
Iteration 226/1000 | Loss: 0.00003504
Iteration 227/1000 | Loss: 0.00003504
Iteration 228/1000 | Loss: 0.00003504
Iteration 229/1000 | Loss: 0.00003504
Iteration 230/1000 | Loss: 0.00003504
Iteration 231/1000 | Loss: 0.00003504
Iteration 232/1000 | Loss: 0.00003504
Iteration 233/1000 | Loss: 0.00003504
Iteration 234/1000 | Loss: 0.00003503
Iteration 235/1000 | Loss: 0.00003503
Iteration 236/1000 | Loss: 0.00003503
Iteration 237/1000 | Loss: 0.00003503
Iteration 238/1000 | Loss: 0.00003503
Iteration 239/1000 | Loss: 0.00003503
Iteration 240/1000 | Loss: 0.00003503
Iteration 241/1000 | Loss: 0.00003503
Iteration 242/1000 | Loss: 0.00003503
Iteration 243/1000 | Loss: 0.00003503
Iteration 244/1000 | Loss: 0.00003503
Iteration 245/1000 | Loss: 0.00003503
Iteration 246/1000 | Loss: 0.00003503
Iteration 247/1000 | Loss: 0.00003503
Iteration 248/1000 | Loss: 0.00003503
Iteration 249/1000 | Loss: 0.00003503
Iteration 250/1000 | Loss: 0.00003503
Iteration 251/1000 | Loss: 0.00003503
Iteration 252/1000 | Loss: 0.00003503
Iteration 253/1000 | Loss: 0.00003503
Iteration 254/1000 | Loss: 0.00003503
Iteration 255/1000 | Loss: 0.00003503
Iteration 256/1000 | Loss: 0.00003503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [3.5033786843996495e-05, 3.5033786843996495e-05, 3.5033786843996495e-05, 3.5033786843996495e-05, 3.5033786843996495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5033786843996495e-05

Optimization complete. Final v2v error: 4.185093879699707 mm

Highest mean error: 22.10499382019043 mm for frame 120

Lowest mean error: 3.0444915294647217 mm for frame 2

Saving results

Total time: 215.96579575538635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_0176/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_0176/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01184074
Iteration 2/25 | Loss: 0.00181777
Iteration 3/25 | Loss: 0.00133390
Iteration 4/25 | Loss: 0.00129993
Iteration 5/25 | Loss: 0.00129389
Iteration 6/25 | Loss: 0.00129389
Iteration 7/25 | Loss: 0.00129389
Iteration 8/25 | Loss: 0.00129389
Iteration 9/25 | Loss: 0.00129389
Iteration 10/25 | Loss: 0.00129389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012938895961269736, 0.0012938895961269736, 0.0012938895961269736, 0.0012938895961269736, 0.0012938895961269736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012938895961269736

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94878566
Iteration 2/25 | Loss: 0.00101856
Iteration 3/25 | Loss: 0.00101856
Iteration 4/25 | Loss: 0.00101856
Iteration 5/25 | Loss: 0.00101856
Iteration 6/25 | Loss: 0.00101856
Iteration 7/25 | Loss: 0.00101856
Iteration 8/25 | Loss: 0.00101856
Iteration 9/25 | Loss: 0.00101856
Iteration 10/25 | Loss: 0.00101856
Iteration 11/25 | Loss: 0.00101856
Iteration 12/25 | Loss: 0.00101856
Iteration 13/25 | Loss: 0.00101856
Iteration 14/25 | Loss: 0.00101856
Iteration 15/25 | Loss: 0.00101856
Iteration 16/25 | Loss: 0.00101856
Iteration 17/25 | Loss: 0.00101856
Iteration 18/25 | Loss: 0.00101856
Iteration 19/25 | Loss: 0.00101856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010185581631958485, 0.0010185581631958485, 0.0010185581631958485, 0.0010185581631958485, 0.0010185581631958485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010185581631958485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101856
Iteration 2/1000 | Loss: 0.00005930
Iteration 3/1000 | Loss: 0.00003916
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00003371
Iteration 6/1000 | Loss: 0.00003249
Iteration 7/1000 | Loss: 0.00003157
Iteration 8/1000 | Loss: 0.00003090
Iteration 9/1000 | Loss: 0.00003028
Iteration 10/1000 | Loss: 0.00002982
Iteration 11/1000 | Loss: 0.00002956
Iteration 12/1000 | Loss: 0.00002940
Iteration 13/1000 | Loss: 0.00002930
Iteration 14/1000 | Loss: 0.00002917
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002907
Iteration 17/1000 | Loss: 0.00002905
Iteration 18/1000 | Loss: 0.00002903
Iteration 19/1000 | Loss: 0.00002902
Iteration 20/1000 | Loss: 0.00002897
Iteration 21/1000 | Loss: 0.00002893
Iteration 22/1000 | Loss: 0.00002892
Iteration 23/1000 | Loss: 0.00002891
Iteration 24/1000 | Loss: 0.00002890
Iteration 25/1000 | Loss: 0.00002888
Iteration 26/1000 | Loss: 0.00002887
Iteration 27/1000 | Loss: 0.00002887
Iteration 28/1000 | Loss: 0.00002886
Iteration 29/1000 | Loss: 0.00002883
Iteration 30/1000 | Loss: 0.00002882
Iteration 31/1000 | Loss: 0.00002880
Iteration 32/1000 | Loss: 0.00002880
Iteration 33/1000 | Loss: 0.00002879
Iteration 34/1000 | Loss: 0.00002878
Iteration 35/1000 | Loss: 0.00002877
Iteration 36/1000 | Loss: 0.00002876
Iteration 37/1000 | Loss: 0.00002876
Iteration 38/1000 | Loss: 0.00002876
Iteration 39/1000 | Loss: 0.00002875
Iteration 40/1000 | Loss: 0.00002875
Iteration 41/1000 | Loss: 0.00002874
Iteration 42/1000 | Loss: 0.00002873
Iteration 43/1000 | Loss: 0.00002873
Iteration 44/1000 | Loss: 0.00002872
Iteration 45/1000 | Loss: 0.00002872
Iteration 46/1000 | Loss: 0.00002872
Iteration 47/1000 | Loss: 0.00002872
Iteration 48/1000 | Loss: 0.00002872
Iteration 49/1000 | Loss: 0.00002872
Iteration 50/1000 | Loss: 0.00002872
Iteration 51/1000 | Loss: 0.00002872
Iteration 52/1000 | Loss: 0.00002870
Iteration 53/1000 | Loss: 0.00002869
Iteration 54/1000 | Loss: 0.00002867
Iteration 55/1000 | Loss: 0.00002867
Iteration 56/1000 | Loss: 0.00002867
Iteration 57/1000 | Loss: 0.00002866
Iteration 58/1000 | Loss: 0.00002866
Iteration 59/1000 | Loss: 0.00002866
Iteration 60/1000 | Loss: 0.00002866
Iteration 61/1000 | Loss: 0.00002866
Iteration 62/1000 | Loss: 0.00002866
Iteration 63/1000 | Loss: 0.00002865
Iteration 64/1000 | Loss: 0.00002865
Iteration 65/1000 | Loss: 0.00002865
Iteration 66/1000 | Loss: 0.00002865
Iteration 67/1000 | Loss: 0.00002865
Iteration 68/1000 | Loss: 0.00002865
Iteration 69/1000 | Loss: 0.00002865
Iteration 70/1000 | Loss: 0.00002864
Iteration 71/1000 | Loss: 0.00002863
Iteration 72/1000 | Loss: 0.00002863
Iteration 73/1000 | Loss: 0.00002863
Iteration 74/1000 | Loss: 0.00002862
Iteration 75/1000 | Loss: 0.00002862
Iteration 76/1000 | Loss: 0.00002862
Iteration 77/1000 | Loss: 0.00002862
Iteration 78/1000 | Loss: 0.00002861
Iteration 79/1000 | Loss: 0.00002861
Iteration 80/1000 | Loss: 0.00002861
Iteration 81/1000 | Loss: 0.00002861
Iteration 82/1000 | Loss: 0.00002860
Iteration 83/1000 | Loss: 0.00002860
Iteration 84/1000 | Loss: 0.00002857
Iteration 85/1000 | Loss: 0.00002857
Iteration 86/1000 | Loss: 0.00002857
Iteration 87/1000 | Loss: 0.00002857
Iteration 88/1000 | Loss: 0.00002857
Iteration 89/1000 | Loss: 0.00002857
Iteration 90/1000 | Loss: 0.00002856
Iteration 91/1000 | Loss: 0.00002856
Iteration 92/1000 | Loss: 0.00002856
Iteration 93/1000 | Loss: 0.00002855
Iteration 94/1000 | Loss: 0.00002855
Iteration 95/1000 | Loss: 0.00002855
Iteration 96/1000 | Loss: 0.00002855
Iteration 97/1000 | Loss: 0.00002855
Iteration 98/1000 | Loss: 0.00002855
Iteration 99/1000 | Loss: 0.00002855
Iteration 100/1000 | Loss: 0.00002855
Iteration 101/1000 | Loss: 0.00002855
Iteration 102/1000 | Loss: 0.00002855
Iteration 103/1000 | Loss: 0.00002855
Iteration 104/1000 | Loss: 0.00002854
Iteration 105/1000 | Loss: 0.00002854
Iteration 106/1000 | Loss: 0.00002854
Iteration 107/1000 | Loss: 0.00002854
Iteration 108/1000 | Loss: 0.00002854
Iteration 109/1000 | Loss: 0.00002854
Iteration 110/1000 | Loss: 0.00002854
Iteration 111/1000 | Loss: 0.00002854
Iteration 112/1000 | Loss: 0.00002853
Iteration 113/1000 | Loss: 0.00002853
Iteration 114/1000 | Loss: 0.00002853
Iteration 115/1000 | Loss: 0.00002853
Iteration 116/1000 | Loss: 0.00002853
Iteration 117/1000 | Loss: 0.00002853
Iteration 118/1000 | Loss: 0.00002853
Iteration 119/1000 | Loss: 0.00002852
Iteration 120/1000 | Loss: 0.00002852
Iteration 121/1000 | Loss: 0.00002851
Iteration 122/1000 | Loss: 0.00002851
Iteration 123/1000 | Loss: 0.00002851
Iteration 124/1000 | Loss: 0.00002851
Iteration 125/1000 | Loss: 0.00002851
Iteration 126/1000 | Loss: 0.00002851
Iteration 127/1000 | Loss: 0.00002851
Iteration 128/1000 | Loss: 0.00002851
Iteration 129/1000 | Loss: 0.00002851
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002850
Iteration 132/1000 | Loss: 0.00002850
Iteration 133/1000 | Loss: 0.00002850
Iteration 134/1000 | Loss: 0.00002850
Iteration 135/1000 | Loss: 0.00002850
Iteration 136/1000 | Loss: 0.00002850
Iteration 137/1000 | Loss: 0.00002850
Iteration 138/1000 | Loss: 0.00002849
Iteration 139/1000 | Loss: 0.00002849
Iteration 140/1000 | Loss: 0.00002849
Iteration 141/1000 | Loss: 0.00002849
Iteration 142/1000 | Loss: 0.00002848
Iteration 143/1000 | Loss: 0.00002848
Iteration 144/1000 | Loss: 0.00002848
Iteration 145/1000 | Loss: 0.00002848
Iteration 146/1000 | Loss: 0.00002848
Iteration 147/1000 | Loss: 0.00002848
Iteration 148/1000 | Loss: 0.00002848
Iteration 149/1000 | Loss: 0.00002848
Iteration 150/1000 | Loss: 0.00002848
Iteration 151/1000 | Loss: 0.00002848
Iteration 152/1000 | Loss: 0.00002848
Iteration 153/1000 | Loss: 0.00002848
Iteration 154/1000 | Loss: 0.00002848
Iteration 155/1000 | Loss: 0.00002848
Iteration 156/1000 | Loss: 0.00002848
Iteration 157/1000 | Loss: 0.00002848
Iteration 158/1000 | Loss: 0.00002848
Iteration 159/1000 | Loss: 0.00002848
Iteration 160/1000 | Loss: 0.00002848
Iteration 161/1000 | Loss: 0.00002848
Iteration 162/1000 | Loss: 0.00002848
Iteration 163/1000 | Loss: 0.00002848
Iteration 164/1000 | Loss: 0.00002848
Iteration 165/1000 | Loss: 0.00002848
Iteration 166/1000 | Loss: 0.00002848
Iteration 167/1000 | Loss: 0.00002848
Iteration 168/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.8480943001341075e-05, 2.8480943001341075e-05, 2.8480943001341075e-05, 2.8480943001341075e-05, 2.8480943001341075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8480943001341075e-05

Optimization complete. Final v2v error: 4.306642055511475 mm

Highest mean error: 5.742695331573486 mm for frame 72

Lowest mean error: 3.531383514404297 mm for frame 33

Saving results

Total time: 48.513760566711426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003158
Iteration 2/25 | Loss: 0.00149759
Iteration 3/25 | Loss: 0.00092563
Iteration 4/25 | Loss: 0.00089089
Iteration 5/25 | Loss: 0.00088092
Iteration 6/25 | Loss: 0.00087851
Iteration 7/25 | Loss: 0.00087801
Iteration 8/25 | Loss: 0.00087801
Iteration 9/25 | Loss: 0.00087801
Iteration 10/25 | Loss: 0.00087801
Iteration 11/25 | Loss: 0.00087801
Iteration 12/25 | Loss: 0.00087801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008780076750554144, 0.0008780076750554144, 0.0008780076750554144, 0.0008780076750554144, 0.0008780076750554144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008780076750554144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16920137
Iteration 2/25 | Loss: 0.00020629
Iteration 3/25 | Loss: 0.00020629
Iteration 4/25 | Loss: 0.00020629
Iteration 5/25 | Loss: 0.00020629
Iteration 6/25 | Loss: 0.00020629
Iteration 7/25 | Loss: 0.00020629
Iteration 8/25 | Loss: 0.00020629
Iteration 9/25 | Loss: 0.00020629
Iteration 10/25 | Loss: 0.00020629
Iteration 11/25 | Loss: 0.00020629
Iteration 12/25 | Loss: 0.00020629
Iteration 13/25 | Loss: 0.00020629
Iteration 14/25 | Loss: 0.00020629
Iteration 15/25 | Loss: 0.00020629
Iteration 16/25 | Loss: 0.00020629
Iteration 17/25 | Loss: 0.00020629
Iteration 18/25 | Loss: 0.00020629
Iteration 19/25 | Loss: 0.00020629
Iteration 20/25 | Loss: 0.00020629
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00020628885249607265, 0.00020628885249607265, 0.00020628885249607265, 0.00020628885249607265, 0.00020628885249607265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020628885249607265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020629
Iteration 2/1000 | Loss: 0.00007050
Iteration 3/1000 | Loss: 0.00005148
Iteration 4/1000 | Loss: 0.00004362
Iteration 5/1000 | Loss: 0.00004106
Iteration 6/1000 | Loss: 0.00003955
Iteration 7/1000 | Loss: 0.00003869
Iteration 8/1000 | Loss: 0.00003805
Iteration 9/1000 | Loss: 0.00003742
Iteration 10/1000 | Loss: 0.00003707
Iteration 11/1000 | Loss: 0.00003682
Iteration 12/1000 | Loss: 0.00003651
Iteration 13/1000 | Loss: 0.00003631
Iteration 14/1000 | Loss: 0.00003619
Iteration 15/1000 | Loss: 0.00003609
Iteration 16/1000 | Loss: 0.00003608
Iteration 17/1000 | Loss: 0.00003608
Iteration 18/1000 | Loss: 0.00003607
Iteration 19/1000 | Loss: 0.00003607
Iteration 20/1000 | Loss: 0.00003606
Iteration 21/1000 | Loss: 0.00003606
Iteration 22/1000 | Loss: 0.00003599
Iteration 23/1000 | Loss: 0.00003597
Iteration 24/1000 | Loss: 0.00003597
Iteration 25/1000 | Loss: 0.00003597
Iteration 26/1000 | Loss: 0.00003597
Iteration 27/1000 | Loss: 0.00003597
Iteration 28/1000 | Loss: 0.00003597
Iteration 29/1000 | Loss: 0.00003597
Iteration 30/1000 | Loss: 0.00003597
Iteration 31/1000 | Loss: 0.00003597
Iteration 32/1000 | Loss: 0.00003597
Iteration 33/1000 | Loss: 0.00003597
Iteration 34/1000 | Loss: 0.00003597
Iteration 35/1000 | Loss: 0.00003596
Iteration 36/1000 | Loss: 0.00003596
Iteration 37/1000 | Loss: 0.00003596
Iteration 38/1000 | Loss: 0.00003596
Iteration 39/1000 | Loss: 0.00003596
Iteration 40/1000 | Loss: 0.00003596
Iteration 41/1000 | Loss: 0.00003596
Iteration 42/1000 | Loss: 0.00003596
Iteration 43/1000 | Loss: 0.00003596
Iteration 44/1000 | Loss: 0.00003595
Iteration 45/1000 | Loss: 0.00003595
Iteration 46/1000 | Loss: 0.00003595
Iteration 47/1000 | Loss: 0.00003595
Iteration 48/1000 | Loss: 0.00003594
Iteration 49/1000 | Loss: 0.00003594
Iteration 50/1000 | Loss: 0.00003594
Iteration 51/1000 | Loss: 0.00003593
Iteration 52/1000 | Loss: 0.00003588
Iteration 53/1000 | Loss: 0.00003587
Iteration 54/1000 | Loss: 0.00003587
Iteration 55/1000 | Loss: 0.00003587
Iteration 56/1000 | Loss: 0.00003587
Iteration 57/1000 | Loss: 0.00003586
Iteration 58/1000 | Loss: 0.00003586
Iteration 59/1000 | Loss: 0.00003586
Iteration 60/1000 | Loss: 0.00003586
Iteration 61/1000 | Loss: 0.00003585
Iteration 62/1000 | Loss: 0.00003585
Iteration 63/1000 | Loss: 0.00003585
Iteration 64/1000 | Loss: 0.00003585
Iteration 65/1000 | Loss: 0.00003585
Iteration 66/1000 | Loss: 0.00003585
Iteration 67/1000 | Loss: 0.00003585
Iteration 68/1000 | Loss: 0.00003585
Iteration 69/1000 | Loss: 0.00003585
Iteration 70/1000 | Loss: 0.00003585
Iteration 71/1000 | Loss: 0.00003585
Iteration 72/1000 | Loss: 0.00003585
Iteration 73/1000 | Loss: 0.00003584
Iteration 74/1000 | Loss: 0.00003584
Iteration 75/1000 | Loss: 0.00003584
Iteration 76/1000 | Loss: 0.00003583
Iteration 77/1000 | Loss: 0.00003582
Iteration 78/1000 | Loss: 0.00003578
Iteration 79/1000 | Loss: 0.00003578
Iteration 80/1000 | Loss: 0.00003576
Iteration 81/1000 | Loss: 0.00003576
Iteration 82/1000 | Loss: 0.00003576
Iteration 83/1000 | Loss: 0.00003576
Iteration 84/1000 | Loss: 0.00003576
Iteration 85/1000 | Loss: 0.00003576
Iteration 86/1000 | Loss: 0.00003576
Iteration 87/1000 | Loss: 0.00003576
Iteration 88/1000 | Loss: 0.00003576
Iteration 89/1000 | Loss: 0.00003576
Iteration 90/1000 | Loss: 0.00003576
Iteration 91/1000 | Loss: 0.00003576
Iteration 92/1000 | Loss: 0.00003575
Iteration 93/1000 | Loss: 0.00003575
Iteration 94/1000 | Loss: 0.00003575
Iteration 95/1000 | Loss: 0.00003575
Iteration 96/1000 | Loss: 0.00003575
Iteration 97/1000 | Loss: 0.00003575
Iteration 98/1000 | Loss: 0.00003574
Iteration 99/1000 | Loss: 0.00003574
Iteration 100/1000 | Loss: 0.00003574
Iteration 101/1000 | Loss: 0.00003574
Iteration 102/1000 | Loss: 0.00003574
Iteration 103/1000 | Loss: 0.00003574
Iteration 104/1000 | Loss: 0.00003573
Iteration 105/1000 | Loss: 0.00003573
Iteration 106/1000 | Loss: 0.00003571
Iteration 107/1000 | Loss: 0.00003568
Iteration 108/1000 | Loss: 0.00003567
Iteration 109/1000 | Loss: 0.00003565
Iteration 110/1000 | Loss: 0.00003564
Iteration 111/1000 | Loss: 0.00003564
Iteration 112/1000 | Loss: 0.00003564
Iteration 113/1000 | Loss: 0.00003564
Iteration 114/1000 | Loss: 0.00003563
Iteration 115/1000 | Loss: 0.00003563
Iteration 116/1000 | Loss: 0.00003562
Iteration 117/1000 | Loss: 0.00003562
Iteration 118/1000 | Loss: 0.00003562
Iteration 119/1000 | Loss: 0.00003562
Iteration 120/1000 | Loss: 0.00003562
Iteration 121/1000 | Loss: 0.00003562
Iteration 122/1000 | Loss: 0.00003562
Iteration 123/1000 | Loss: 0.00003562
Iteration 124/1000 | Loss: 0.00003562
Iteration 125/1000 | Loss: 0.00003562
Iteration 126/1000 | Loss: 0.00003561
Iteration 127/1000 | Loss: 0.00003561
Iteration 128/1000 | Loss: 0.00003561
Iteration 129/1000 | Loss: 0.00003561
Iteration 130/1000 | Loss: 0.00003561
Iteration 131/1000 | Loss: 0.00003561
Iteration 132/1000 | Loss: 0.00003560
Iteration 133/1000 | Loss: 0.00003560
Iteration 134/1000 | Loss: 0.00003560
Iteration 135/1000 | Loss: 0.00003560
Iteration 136/1000 | Loss: 0.00003559
Iteration 137/1000 | Loss: 0.00003559
Iteration 138/1000 | Loss: 0.00003559
Iteration 139/1000 | Loss: 0.00003558
Iteration 140/1000 | Loss: 0.00003558
Iteration 141/1000 | Loss: 0.00003558
Iteration 142/1000 | Loss: 0.00003558
Iteration 143/1000 | Loss: 0.00003558
Iteration 144/1000 | Loss: 0.00003558
Iteration 145/1000 | Loss: 0.00003557
Iteration 146/1000 | Loss: 0.00003556
Iteration 147/1000 | Loss: 0.00003555
Iteration 148/1000 | Loss: 0.00003555
Iteration 149/1000 | Loss: 0.00003554
Iteration 150/1000 | Loss: 0.00003554
Iteration 151/1000 | Loss: 0.00003553
Iteration 152/1000 | Loss: 0.00003553
Iteration 153/1000 | Loss: 0.00003553
Iteration 154/1000 | Loss: 0.00003552
Iteration 155/1000 | Loss: 0.00003552
Iteration 156/1000 | Loss: 0.00003551
Iteration 157/1000 | Loss: 0.00003551
Iteration 158/1000 | Loss: 0.00003550
Iteration 159/1000 | Loss: 0.00003550
Iteration 160/1000 | Loss: 0.00003550
Iteration 161/1000 | Loss: 0.00003548
Iteration 162/1000 | Loss: 0.00003547
Iteration 163/1000 | Loss: 0.00003547
Iteration 164/1000 | Loss: 0.00003547
Iteration 165/1000 | Loss: 0.00003547
Iteration 166/1000 | Loss: 0.00003547
Iteration 167/1000 | Loss: 0.00003547
Iteration 168/1000 | Loss: 0.00003546
Iteration 169/1000 | Loss: 0.00003546
Iteration 170/1000 | Loss: 0.00003546
Iteration 171/1000 | Loss: 0.00003546
Iteration 172/1000 | Loss: 0.00003546
Iteration 173/1000 | Loss: 0.00003546
Iteration 174/1000 | Loss: 0.00003546
Iteration 175/1000 | Loss: 0.00003546
Iteration 176/1000 | Loss: 0.00003546
Iteration 177/1000 | Loss: 0.00003546
Iteration 178/1000 | Loss: 0.00003546
Iteration 179/1000 | Loss: 0.00003546
Iteration 180/1000 | Loss: 0.00003545
Iteration 181/1000 | Loss: 0.00003545
Iteration 182/1000 | Loss: 0.00003545
Iteration 183/1000 | Loss: 0.00003545
Iteration 184/1000 | Loss: 0.00003545
Iteration 185/1000 | Loss: 0.00003545
Iteration 186/1000 | Loss: 0.00003545
Iteration 187/1000 | Loss: 0.00003545
Iteration 188/1000 | Loss: 0.00003544
Iteration 189/1000 | Loss: 0.00003544
Iteration 190/1000 | Loss: 0.00003544
Iteration 191/1000 | Loss: 0.00003544
Iteration 192/1000 | Loss: 0.00003544
Iteration 193/1000 | Loss: 0.00003543
Iteration 194/1000 | Loss: 0.00003543
Iteration 195/1000 | Loss: 0.00003543
Iteration 196/1000 | Loss: 0.00003543
Iteration 197/1000 | Loss: 0.00003543
Iteration 198/1000 | Loss: 0.00003542
Iteration 199/1000 | Loss: 0.00003542
Iteration 200/1000 | Loss: 0.00003542
Iteration 201/1000 | Loss: 0.00003542
Iteration 202/1000 | Loss: 0.00003542
Iteration 203/1000 | Loss: 0.00003541
Iteration 204/1000 | Loss: 0.00003541
Iteration 205/1000 | Loss: 0.00003540
Iteration 206/1000 | Loss: 0.00003539
Iteration 207/1000 | Loss: 0.00003539
Iteration 208/1000 | Loss: 0.00003539
Iteration 209/1000 | Loss: 0.00003539
Iteration 210/1000 | Loss: 0.00003538
Iteration 211/1000 | Loss: 0.00003538
Iteration 212/1000 | Loss: 0.00003538
Iteration 213/1000 | Loss: 0.00003538
Iteration 214/1000 | Loss: 0.00003537
Iteration 215/1000 | Loss: 0.00003537
Iteration 216/1000 | Loss: 0.00003537
Iteration 217/1000 | Loss: 0.00003537
Iteration 218/1000 | Loss: 0.00003537
Iteration 219/1000 | Loss: 0.00003537
Iteration 220/1000 | Loss: 0.00003537
Iteration 221/1000 | Loss: 0.00003537
Iteration 222/1000 | Loss: 0.00003537
Iteration 223/1000 | Loss: 0.00003537
Iteration 224/1000 | Loss: 0.00003537
Iteration 225/1000 | Loss: 0.00003537
Iteration 226/1000 | Loss: 0.00003536
Iteration 227/1000 | Loss: 0.00003536
Iteration 228/1000 | Loss: 0.00003536
Iteration 229/1000 | Loss: 0.00003536
Iteration 230/1000 | Loss: 0.00003536
Iteration 231/1000 | Loss: 0.00003536
Iteration 232/1000 | Loss: 0.00003536
Iteration 233/1000 | Loss: 0.00003536
Iteration 234/1000 | Loss: 0.00003536
Iteration 235/1000 | Loss: 0.00003536
Iteration 236/1000 | Loss: 0.00003535
Iteration 237/1000 | Loss: 0.00003535
Iteration 238/1000 | Loss: 0.00003535
Iteration 239/1000 | Loss: 0.00003535
Iteration 240/1000 | Loss: 0.00003535
Iteration 241/1000 | Loss: 0.00003535
Iteration 242/1000 | Loss: 0.00003535
Iteration 243/1000 | Loss: 0.00003535
Iteration 244/1000 | Loss: 0.00003535
Iteration 245/1000 | Loss: 0.00003535
Iteration 246/1000 | Loss: 0.00003534
Iteration 247/1000 | Loss: 0.00003534
Iteration 248/1000 | Loss: 0.00003534
Iteration 249/1000 | Loss: 0.00003534
Iteration 250/1000 | Loss: 0.00003534
Iteration 251/1000 | Loss: 0.00003534
Iteration 252/1000 | Loss: 0.00003533
Iteration 253/1000 | Loss: 0.00003533
Iteration 254/1000 | Loss: 0.00003533
Iteration 255/1000 | Loss: 0.00003533
Iteration 256/1000 | Loss: 0.00003533
Iteration 257/1000 | Loss: 0.00003533
Iteration 258/1000 | Loss: 0.00003533
Iteration 259/1000 | Loss: 0.00003533
Iteration 260/1000 | Loss: 0.00003533
Iteration 261/1000 | Loss: 0.00003533
Iteration 262/1000 | Loss: 0.00003532
Iteration 263/1000 | Loss: 0.00003532
Iteration 264/1000 | Loss: 0.00003532
Iteration 265/1000 | Loss: 0.00003532
Iteration 266/1000 | Loss: 0.00003532
Iteration 267/1000 | Loss: 0.00003532
Iteration 268/1000 | Loss: 0.00003532
Iteration 269/1000 | Loss: 0.00003532
Iteration 270/1000 | Loss: 0.00003532
Iteration 271/1000 | Loss: 0.00003532
Iteration 272/1000 | Loss: 0.00003532
Iteration 273/1000 | Loss: 0.00003532
Iteration 274/1000 | Loss: 0.00003532
Iteration 275/1000 | Loss: 0.00003532
Iteration 276/1000 | Loss: 0.00003532
Iteration 277/1000 | Loss: 0.00003532
Iteration 278/1000 | Loss: 0.00003532
Iteration 279/1000 | Loss: 0.00003532
Iteration 280/1000 | Loss: 0.00003532
Iteration 281/1000 | Loss: 0.00003532
Iteration 282/1000 | Loss: 0.00003532
Iteration 283/1000 | Loss: 0.00003532
Iteration 284/1000 | Loss: 0.00003532
Iteration 285/1000 | Loss: 0.00003532
Iteration 286/1000 | Loss: 0.00003532
Iteration 287/1000 | Loss: 0.00003532
Iteration 288/1000 | Loss: 0.00003532
Iteration 289/1000 | Loss: 0.00003532
Iteration 290/1000 | Loss: 0.00003532
Iteration 291/1000 | Loss: 0.00003532
Iteration 292/1000 | Loss: 0.00003532
Iteration 293/1000 | Loss: 0.00003532
Iteration 294/1000 | Loss: 0.00003532
Iteration 295/1000 | Loss: 0.00003532
Iteration 296/1000 | Loss: 0.00003532
Iteration 297/1000 | Loss: 0.00003532
Iteration 298/1000 | Loss: 0.00003532
Iteration 299/1000 | Loss: 0.00003532
Iteration 300/1000 | Loss: 0.00003532
Iteration 301/1000 | Loss: 0.00003532
Iteration 302/1000 | Loss: 0.00003532
Iteration 303/1000 | Loss: 0.00003532
Iteration 304/1000 | Loss: 0.00003532
Iteration 305/1000 | Loss: 0.00003532
Iteration 306/1000 | Loss: 0.00003532
Iteration 307/1000 | Loss: 0.00003532
Iteration 308/1000 | Loss: 0.00003532
Iteration 309/1000 | Loss: 0.00003532
Iteration 310/1000 | Loss: 0.00003532
Iteration 311/1000 | Loss: 0.00003532
Iteration 312/1000 | Loss: 0.00003532
Iteration 313/1000 | Loss: 0.00003532
Iteration 314/1000 | Loss: 0.00003532
Iteration 315/1000 | Loss: 0.00003532
Iteration 316/1000 | Loss: 0.00003532
Iteration 317/1000 | Loss: 0.00003532
Iteration 318/1000 | Loss: 0.00003532
Iteration 319/1000 | Loss: 0.00003532
Iteration 320/1000 | Loss: 0.00003532
Iteration 321/1000 | Loss: 0.00003532
Iteration 322/1000 | Loss: 0.00003532
Iteration 323/1000 | Loss: 0.00003532
Iteration 324/1000 | Loss: 0.00003532
Iteration 325/1000 | Loss: 0.00003532
Iteration 326/1000 | Loss: 0.00003532
Iteration 327/1000 | Loss: 0.00003532
Iteration 328/1000 | Loss: 0.00003532
Iteration 329/1000 | Loss: 0.00003532
Iteration 330/1000 | Loss: 0.00003532
Iteration 331/1000 | Loss: 0.00003532
Iteration 332/1000 | Loss: 0.00003532
Iteration 333/1000 | Loss: 0.00003532
Iteration 334/1000 | Loss: 0.00003532
Iteration 335/1000 | Loss: 0.00003532
Iteration 336/1000 | Loss: 0.00003532
Iteration 337/1000 | Loss: 0.00003532
Iteration 338/1000 | Loss: 0.00003532
Iteration 339/1000 | Loss: 0.00003532
Iteration 340/1000 | Loss: 0.00003532
Iteration 341/1000 | Loss: 0.00003532
Iteration 342/1000 | Loss: 0.00003532
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [3.532091432134621e-05, 3.532091432134621e-05, 3.532091432134621e-05, 3.532091432134621e-05, 3.532091432134621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.532091432134621e-05

Optimization complete. Final v2v error: 4.763205528259277 mm

Highest mean error: 5.577960014343262 mm for frame 73

Lowest mean error: 4.017225742340088 mm for frame 86

Saving results

Total time: 53.10739088058472
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095638
Iteration 2/25 | Loss: 0.01095638
Iteration 3/25 | Loss: 0.01095638
Iteration 4/25 | Loss: 0.01095638
Iteration 5/25 | Loss: 0.01095638
Iteration 6/25 | Loss: 0.01095638
Iteration 7/25 | Loss: 0.01095638
Iteration 8/25 | Loss: 0.01095638
Iteration 9/25 | Loss: 0.01095638
Iteration 10/25 | Loss: 0.01095638
Iteration 11/25 | Loss: 0.01095637
Iteration 12/25 | Loss: 0.01095637
Iteration 13/25 | Loss: 0.01095637
Iteration 14/25 | Loss: 0.01095637
Iteration 15/25 | Loss: 0.01095637
Iteration 16/25 | Loss: 0.01095637
Iteration 17/25 | Loss: 0.01095637
Iteration 18/25 | Loss: 0.01095637
Iteration 19/25 | Loss: 0.01095637
Iteration 20/25 | Loss: 0.01095637
Iteration 21/25 | Loss: 0.01095637
Iteration 22/25 | Loss: 0.01095637
Iteration 23/25 | Loss: 0.01095636
Iteration 24/25 | Loss: 0.01095636
Iteration 25/25 | Loss: 0.01095636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79562676
Iteration 2/25 | Loss: 0.06411485
Iteration 3/25 | Loss: 0.06409922
Iteration 4/25 | Loss: 0.06409921
Iteration 5/25 | Loss: 0.06409921
Iteration 6/25 | Loss: 0.06409921
Iteration 7/25 | Loss: 0.06409920
Iteration 8/25 | Loss: 0.06409920
Iteration 9/25 | Loss: 0.06409920
Iteration 10/25 | Loss: 0.06409920
Iteration 11/25 | Loss: 0.06409920
Iteration 12/25 | Loss: 0.06409920
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.06409920006990433, 0.06409920006990433, 0.06409920006990433, 0.06409920006990433, 0.06409920006990433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06409920006990433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06409920
Iteration 2/1000 | Loss: 0.00539880
Iteration 3/1000 | Loss: 0.00186833
Iteration 4/1000 | Loss: 0.00082458
Iteration 5/1000 | Loss: 0.00716243
Iteration 6/1000 | Loss: 0.01106059
Iteration 7/1000 | Loss: 0.00704105
Iteration 8/1000 | Loss: 0.00060934
Iteration 9/1000 | Loss: 0.00116328
Iteration 10/1000 | Loss: 0.00014616
Iteration 11/1000 | Loss: 0.00074486
Iteration 12/1000 | Loss: 0.00122508
Iteration 13/1000 | Loss: 0.00083635
Iteration 14/1000 | Loss: 0.00032821
Iteration 15/1000 | Loss: 0.00040517
Iteration 16/1000 | Loss: 0.00131009
Iteration 17/1000 | Loss: 0.00030080
Iteration 18/1000 | Loss: 0.00057281
Iteration 19/1000 | Loss: 0.00005019
Iteration 20/1000 | Loss: 0.00013781
Iteration 21/1000 | Loss: 0.00008799
Iteration 22/1000 | Loss: 0.00006334
Iteration 23/1000 | Loss: 0.00009440
Iteration 24/1000 | Loss: 0.00040541
Iteration 25/1000 | Loss: 0.00034355
Iteration 26/1000 | Loss: 0.00116516
Iteration 27/1000 | Loss: 0.00003681
Iteration 28/1000 | Loss: 0.00011427
Iteration 29/1000 | Loss: 0.00004128
Iteration 30/1000 | Loss: 0.00002839
Iteration 31/1000 | Loss: 0.00027643
Iteration 32/1000 | Loss: 0.00003287
Iteration 33/1000 | Loss: 0.00018457
Iteration 34/1000 | Loss: 0.00007418
Iteration 35/1000 | Loss: 0.00022577
Iteration 36/1000 | Loss: 0.00046048
Iteration 37/1000 | Loss: 0.00002921
Iteration 38/1000 | Loss: 0.00008851
Iteration 39/1000 | Loss: 0.00002554
Iteration 40/1000 | Loss: 0.00003139
Iteration 41/1000 | Loss: 0.00002374
Iteration 42/1000 | Loss: 0.00002641
Iteration 43/1000 | Loss: 0.00059273
Iteration 44/1000 | Loss: 0.00019828
Iteration 45/1000 | Loss: 0.00003296
Iteration 46/1000 | Loss: 0.00006435
Iteration 47/1000 | Loss: 0.00045346
Iteration 48/1000 | Loss: 0.00002464
Iteration 49/1000 | Loss: 0.00004788
Iteration 50/1000 | Loss: 0.00002205
Iteration 51/1000 | Loss: 0.00010003
Iteration 52/1000 | Loss: 0.00019227
Iteration 53/1000 | Loss: 0.00015441
Iteration 54/1000 | Loss: 0.00002925
Iteration 55/1000 | Loss: 0.00005840
Iteration 56/1000 | Loss: 0.00021854
Iteration 57/1000 | Loss: 0.00003301
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00013243
Iteration 60/1000 | Loss: 0.00002895
Iteration 61/1000 | Loss: 0.00003384
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00002120
Iteration 64/1000 | Loss: 0.00002103
Iteration 65/1000 | Loss: 0.00004386
Iteration 66/1000 | Loss: 0.00032506
Iteration 67/1000 | Loss: 0.00005116
Iteration 68/1000 | Loss: 0.00005890
Iteration 69/1000 | Loss: 0.00006157
Iteration 70/1000 | Loss: 0.00004604
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00003660
Iteration 73/1000 | Loss: 0.00002077
Iteration 74/1000 | Loss: 0.00002076
Iteration 75/1000 | Loss: 0.00002073
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002115
Iteration 81/1000 | Loss: 0.00002063
Iteration 82/1000 | Loss: 0.00002063
Iteration 83/1000 | Loss: 0.00002063
Iteration 84/1000 | Loss: 0.00002063
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002062
Iteration 88/1000 | Loss: 0.00002062
Iteration 89/1000 | Loss: 0.00002062
Iteration 90/1000 | Loss: 0.00002062
Iteration 91/1000 | Loss: 0.00002062
Iteration 92/1000 | Loss: 0.00002062
Iteration 93/1000 | Loss: 0.00002062
Iteration 94/1000 | Loss: 0.00002062
Iteration 95/1000 | Loss: 0.00002062
Iteration 96/1000 | Loss: 0.00002062
Iteration 97/1000 | Loss: 0.00002062
Iteration 98/1000 | Loss: 0.00002062
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002061
Iteration 101/1000 | Loss: 0.00002061
Iteration 102/1000 | Loss: 0.00002061
Iteration 103/1000 | Loss: 0.00002061
Iteration 104/1000 | Loss: 0.00002070
Iteration 105/1000 | Loss: 0.00002061
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Iteration 110/1000 | Loss: 0.00002060
Iteration 111/1000 | Loss: 0.00002060
Iteration 112/1000 | Loss: 0.00002059
Iteration 113/1000 | Loss: 0.00002059
Iteration 114/1000 | Loss: 0.00002059
Iteration 115/1000 | Loss: 0.00002059
Iteration 116/1000 | Loss: 0.00002059
Iteration 117/1000 | Loss: 0.00002059
Iteration 118/1000 | Loss: 0.00002059
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002059
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002059
Iteration 125/1000 | Loss: 0.00002059
Iteration 126/1000 | Loss: 0.00002059
Iteration 127/1000 | Loss: 0.00002059
Iteration 128/1000 | Loss: 0.00002059
Iteration 129/1000 | Loss: 0.00002059
Iteration 130/1000 | Loss: 0.00002059
Iteration 131/1000 | Loss: 0.00002059
Iteration 132/1000 | Loss: 0.00002059
Iteration 133/1000 | Loss: 0.00002059
Iteration 134/1000 | Loss: 0.00002059
Iteration 135/1000 | Loss: 0.00002059
Iteration 136/1000 | Loss: 0.00002059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [2.0586569007718936e-05, 2.0586569007718936e-05, 2.0586569007718936e-05, 2.0586569007718936e-05, 2.0586569007718936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0586569007718936e-05

Optimization complete. Final v2v error: 3.658355712890625 mm

Highest mean error: 4.726980686187744 mm for frame 222

Lowest mean error: 2.7573323249816895 mm for frame 112

Saving results

Total time: 127.37093329429626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896883
Iteration 2/25 | Loss: 0.00147311
Iteration 3/25 | Loss: 0.00112053
Iteration 4/25 | Loss: 0.00105670
Iteration 5/25 | Loss: 0.00103358
Iteration 6/25 | Loss: 0.00103799
Iteration 7/25 | Loss: 0.00103629
Iteration 8/25 | Loss: 0.00104200
Iteration 9/25 | Loss: 0.00103434
Iteration 10/25 | Loss: 0.00101479
Iteration 11/25 | Loss: 0.00102178
Iteration 12/25 | Loss: 0.00100907
Iteration 13/25 | Loss: 0.00099705
Iteration 14/25 | Loss: 0.00098495
Iteration 15/25 | Loss: 0.00097849
Iteration 16/25 | Loss: 0.00097673
Iteration 17/25 | Loss: 0.00097601
Iteration 18/25 | Loss: 0.00097572
Iteration 19/25 | Loss: 0.00097561
Iteration 20/25 | Loss: 0.00097560
Iteration 21/25 | Loss: 0.00097559
Iteration 22/25 | Loss: 0.00097559
Iteration 23/25 | Loss: 0.00097559
Iteration 24/25 | Loss: 0.00097558
Iteration 25/25 | Loss: 0.00097558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18779194
Iteration 2/25 | Loss: 0.00247431
Iteration 3/25 | Loss: 0.00247425
Iteration 4/25 | Loss: 0.00247425
Iteration 5/25 | Loss: 0.00247425
Iteration 6/25 | Loss: 0.00247425
Iteration 7/25 | Loss: 0.00247425
Iteration 8/25 | Loss: 0.00247425
Iteration 9/25 | Loss: 0.00247425
Iteration 10/25 | Loss: 0.00247425
Iteration 11/25 | Loss: 0.00247425
Iteration 12/25 | Loss: 0.00247425
Iteration 13/25 | Loss: 0.00247425
Iteration 14/25 | Loss: 0.00247425
Iteration 15/25 | Loss: 0.00247425
Iteration 16/25 | Loss: 0.00247425
Iteration 17/25 | Loss: 0.00247425
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024742453824728727, 0.0024742453824728727, 0.0024742453824728727, 0.0024742453824728727, 0.0024742453824728727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024742453824728727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247425
Iteration 2/1000 | Loss: 0.00043876
Iteration 3/1000 | Loss: 0.00032950
Iteration 4/1000 | Loss: 0.00072005
Iteration 5/1000 | Loss: 0.00176627
Iteration 6/1000 | Loss: 0.00513937
Iteration 7/1000 | Loss: 0.00192001
Iteration 8/1000 | Loss: 0.00101230
Iteration 9/1000 | Loss: 0.00045489
Iteration 10/1000 | Loss: 0.00082022
Iteration 11/1000 | Loss: 0.00106267
Iteration 12/1000 | Loss: 0.00220834
Iteration 13/1000 | Loss: 0.00117022
Iteration 14/1000 | Loss: 0.00029412
Iteration 15/1000 | Loss: 0.00234011
Iteration 16/1000 | Loss: 0.00270808
Iteration 17/1000 | Loss: 0.00133853
Iteration 18/1000 | Loss: 0.00012637
Iteration 19/1000 | Loss: 0.00127240
Iteration 20/1000 | Loss: 0.00049547
Iteration 21/1000 | Loss: 0.00058878
Iteration 22/1000 | Loss: 0.00033874
Iteration 23/1000 | Loss: 0.00095953
Iteration 24/1000 | Loss: 0.00006494
Iteration 25/1000 | Loss: 0.00005418
Iteration 26/1000 | Loss: 0.00004780
Iteration 27/1000 | Loss: 0.00004436
Iteration 28/1000 | Loss: 0.00100864
Iteration 29/1000 | Loss: 0.00050334
Iteration 30/1000 | Loss: 0.00096088
Iteration 31/1000 | Loss: 0.00080685
Iteration 32/1000 | Loss: 0.00004184
Iteration 33/1000 | Loss: 0.00004051
Iteration 34/1000 | Loss: 0.00206304
Iteration 35/1000 | Loss: 0.00006025
Iteration 36/1000 | Loss: 0.00032404
Iteration 37/1000 | Loss: 0.00004441
Iteration 38/1000 | Loss: 0.00003931
Iteration 39/1000 | Loss: 0.00051315
Iteration 40/1000 | Loss: 0.00042644
Iteration 41/1000 | Loss: 0.00003695
Iteration 42/1000 | Loss: 0.00045814
Iteration 43/1000 | Loss: 0.00102209
Iteration 44/1000 | Loss: 0.00094022
Iteration 45/1000 | Loss: 0.00010158
Iteration 46/1000 | Loss: 0.00005172
Iteration 47/1000 | Loss: 0.00003884
Iteration 48/1000 | Loss: 0.00039030
Iteration 49/1000 | Loss: 0.00003957
Iteration 50/1000 | Loss: 0.00003312
Iteration 51/1000 | Loss: 0.00002979
Iteration 52/1000 | Loss: 0.00002744
Iteration 53/1000 | Loss: 0.00002549
Iteration 54/1000 | Loss: 0.00002471
Iteration 55/1000 | Loss: 0.00002420
Iteration 56/1000 | Loss: 0.00002393
Iteration 57/1000 | Loss: 0.00002367
Iteration 58/1000 | Loss: 0.00002344
Iteration 59/1000 | Loss: 0.00002342
Iteration 60/1000 | Loss: 0.00002327
Iteration 61/1000 | Loss: 0.00002323
Iteration 62/1000 | Loss: 0.00002323
Iteration 63/1000 | Loss: 0.00002323
Iteration 64/1000 | Loss: 0.00002322
Iteration 65/1000 | Loss: 0.00002322
Iteration 66/1000 | Loss: 0.00002321
Iteration 67/1000 | Loss: 0.00002320
Iteration 68/1000 | Loss: 0.00002320
Iteration 69/1000 | Loss: 0.00002319
Iteration 70/1000 | Loss: 0.00002319
Iteration 71/1000 | Loss: 0.00002315
Iteration 72/1000 | Loss: 0.00002315
Iteration 73/1000 | Loss: 0.00002313
Iteration 74/1000 | Loss: 0.00002313
Iteration 75/1000 | Loss: 0.00002312
Iteration 76/1000 | Loss: 0.00002312
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002307
Iteration 79/1000 | Loss: 0.00002303
Iteration 80/1000 | Loss: 0.00002303
Iteration 81/1000 | Loss: 0.00002302
Iteration 82/1000 | Loss: 0.00002302
Iteration 83/1000 | Loss: 0.00002301
Iteration 84/1000 | Loss: 0.00002300
Iteration 85/1000 | Loss: 0.00002300
Iteration 86/1000 | Loss: 0.00002300
Iteration 87/1000 | Loss: 0.00002299
Iteration 88/1000 | Loss: 0.00002299
Iteration 89/1000 | Loss: 0.00002299
Iteration 90/1000 | Loss: 0.00002298
Iteration 91/1000 | Loss: 0.00002298
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002296
Iteration 97/1000 | Loss: 0.00002296
Iteration 98/1000 | Loss: 0.00002296
Iteration 99/1000 | Loss: 0.00002295
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002294
Iteration 102/1000 | Loss: 0.00002294
Iteration 103/1000 | Loss: 0.00002294
Iteration 104/1000 | Loss: 0.00002294
Iteration 105/1000 | Loss: 0.00002294
Iteration 106/1000 | Loss: 0.00002294
Iteration 107/1000 | Loss: 0.00002294
Iteration 108/1000 | Loss: 0.00002294
Iteration 109/1000 | Loss: 0.00002294
Iteration 110/1000 | Loss: 0.00002293
Iteration 111/1000 | Loss: 0.00002293
Iteration 112/1000 | Loss: 0.00002292
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002290
Iteration 115/1000 | Loss: 0.00002290
Iteration 116/1000 | Loss: 0.00002290
Iteration 117/1000 | Loss: 0.00002290
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002290
Iteration 121/1000 | Loss: 0.00002290
Iteration 122/1000 | Loss: 0.00002290
Iteration 123/1000 | Loss: 0.00002290
Iteration 124/1000 | Loss: 0.00002290
Iteration 125/1000 | Loss: 0.00002289
Iteration 126/1000 | Loss: 0.00002289
Iteration 127/1000 | Loss: 0.00002289
Iteration 128/1000 | Loss: 0.00002289
Iteration 129/1000 | Loss: 0.00002289
Iteration 130/1000 | Loss: 0.00002289
Iteration 131/1000 | Loss: 0.00002289
Iteration 132/1000 | Loss: 0.00002289
Iteration 133/1000 | Loss: 0.00002288
Iteration 134/1000 | Loss: 0.00002288
Iteration 135/1000 | Loss: 0.00002288
Iteration 136/1000 | Loss: 0.00002288
Iteration 137/1000 | Loss: 0.00002288
Iteration 138/1000 | Loss: 0.00002288
Iteration 139/1000 | Loss: 0.00002288
Iteration 140/1000 | Loss: 0.00002288
Iteration 141/1000 | Loss: 0.00002288
Iteration 142/1000 | Loss: 0.00002288
Iteration 143/1000 | Loss: 0.00002288
Iteration 144/1000 | Loss: 0.00002288
Iteration 145/1000 | Loss: 0.00002288
Iteration 146/1000 | Loss: 0.00002288
Iteration 147/1000 | Loss: 0.00002288
Iteration 148/1000 | Loss: 0.00002288
Iteration 149/1000 | Loss: 0.00002288
Iteration 150/1000 | Loss: 0.00002288
Iteration 151/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.2881844415678643e-05, 2.2881844415678643e-05, 2.2881844415678643e-05, 2.2881844415678643e-05, 2.2881844415678643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2881844415678643e-05

Optimization complete. Final v2v error: 4.041927814483643 mm

Highest mean error: 4.778531074523926 mm for frame 114

Lowest mean error: 3.5886237621307373 mm for frame 13

Saving results

Total time: 122.21542119979858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384292
Iteration 2/25 | Loss: 0.00084193
Iteration 3/25 | Loss: 0.00067342
Iteration 4/25 | Loss: 0.00063781
Iteration 5/25 | Loss: 0.00062785
Iteration 6/25 | Loss: 0.00062620
Iteration 7/25 | Loss: 0.00062547
Iteration 8/25 | Loss: 0.00062534
Iteration 9/25 | Loss: 0.00062534
Iteration 10/25 | Loss: 0.00062534
Iteration 11/25 | Loss: 0.00062534
Iteration 12/25 | Loss: 0.00062534
Iteration 13/25 | Loss: 0.00062534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000625341257546097, 0.000625341257546097, 0.000625341257546097, 0.000625341257546097, 0.000625341257546097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000625341257546097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28874147
Iteration 2/25 | Loss: 0.00029260
Iteration 3/25 | Loss: 0.00029260
Iteration 4/25 | Loss: 0.00029260
Iteration 5/25 | Loss: 0.00029260
Iteration 6/25 | Loss: 0.00029259
Iteration 7/25 | Loss: 0.00029259
Iteration 8/25 | Loss: 0.00029259
Iteration 9/25 | Loss: 0.00029259
Iteration 10/25 | Loss: 0.00029259
Iteration 11/25 | Loss: 0.00029259
Iteration 12/25 | Loss: 0.00029259
Iteration 13/25 | Loss: 0.00029259
Iteration 14/25 | Loss: 0.00029259
Iteration 15/25 | Loss: 0.00029259
Iteration 16/25 | Loss: 0.00029259
Iteration 17/25 | Loss: 0.00029259
Iteration 18/25 | Loss: 0.00029259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002925933222286403, 0.0002925933222286403, 0.0002925933222286403, 0.0002925933222286403, 0.0002925933222286403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002925933222286403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029259
Iteration 2/1000 | Loss: 0.00004233
Iteration 3/1000 | Loss: 0.00002924
Iteration 4/1000 | Loss: 0.00002478
Iteration 5/1000 | Loss: 0.00002290
Iteration 6/1000 | Loss: 0.00002130
Iteration 7/1000 | Loss: 0.00002047
Iteration 8/1000 | Loss: 0.00001976
Iteration 9/1000 | Loss: 0.00001945
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001919
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001913
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001872
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00001866
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001866
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001862
Iteration 27/1000 | Loss: 0.00001861
Iteration 28/1000 | Loss: 0.00001860
Iteration 29/1000 | Loss: 0.00001860
Iteration 30/1000 | Loss: 0.00001860
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001859
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001857
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001856
Iteration 41/1000 | Loss: 0.00001856
Iteration 42/1000 | Loss: 0.00001856
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001856
Iteration 45/1000 | Loss: 0.00001856
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001855
Iteration 52/1000 | Loss: 0.00001855
Iteration 53/1000 | Loss: 0.00001855
Iteration 54/1000 | Loss: 0.00001855
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001855
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001854
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001854
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.8543687474448234e-05, 1.8543687474448234e-05, 1.8543687474448234e-05, 1.8543687474448234e-05, 1.8543687474448234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8543687474448234e-05

Optimization complete. Final v2v error: 3.6061928272247314 mm

Highest mean error: 4.019279479980469 mm for frame 40

Lowest mean error: 3.2725870609283447 mm for frame 120

Saving results

Total time: 34.24201416969299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01007252
Iteration 2/25 | Loss: 0.00410835
Iteration 3/25 | Loss: 0.00225985
Iteration 4/25 | Loss: 0.00198477
Iteration 5/25 | Loss: 0.00174507
Iteration 6/25 | Loss: 0.00163700
Iteration 7/25 | Loss: 0.00168070
Iteration 8/25 | Loss: 0.00156443
Iteration 9/25 | Loss: 0.00149060
Iteration 10/25 | Loss: 0.00141245
Iteration 11/25 | Loss: 0.00138372
Iteration 12/25 | Loss: 0.00146986
Iteration 13/25 | Loss: 0.00136038
Iteration 14/25 | Loss: 0.00126342
Iteration 15/25 | Loss: 0.00122709
Iteration 16/25 | Loss: 0.00120838
Iteration 17/25 | Loss: 0.00119982
Iteration 18/25 | Loss: 0.00118247
Iteration 19/25 | Loss: 0.00117835
Iteration 20/25 | Loss: 0.00117282
Iteration 21/25 | Loss: 0.00117112
Iteration 22/25 | Loss: 0.00117077
Iteration 23/25 | Loss: 0.00117371
Iteration 24/25 | Loss: 0.00117285
Iteration 25/25 | Loss: 0.00116893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25597501
Iteration 2/25 | Loss: 0.00304864
Iteration 3/25 | Loss: 0.00304754
Iteration 4/25 | Loss: 0.00304753
Iteration 5/25 | Loss: 0.00304753
Iteration 6/25 | Loss: 0.00304753
Iteration 7/25 | Loss: 0.00304753
Iteration 8/25 | Loss: 0.00304753
Iteration 9/25 | Loss: 0.00304753
Iteration 10/25 | Loss: 0.00304753
Iteration 11/25 | Loss: 0.00304753
Iteration 12/25 | Loss: 0.00304753
Iteration 13/25 | Loss: 0.00304753
Iteration 14/25 | Loss: 0.00304753
Iteration 15/25 | Loss: 0.00304753
Iteration 16/25 | Loss: 0.00304753
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0030475310049951077, 0.0030475310049951077, 0.0030475310049951077, 0.0030475310049951077, 0.0030475310049951077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030475310049951077

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304753
Iteration 2/1000 | Loss: 0.00062742
Iteration 3/1000 | Loss: 0.00043862
Iteration 4/1000 | Loss: 0.00033798
Iteration 5/1000 | Loss: 0.00064291
Iteration 6/1000 | Loss: 0.00399707
Iteration 7/1000 | Loss: 0.00218138
Iteration 8/1000 | Loss: 0.00110180
Iteration 9/1000 | Loss: 0.00082378
Iteration 10/1000 | Loss: 0.00064418
Iteration 11/1000 | Loss: 0.00039457
Iteration 12/1000 | Loss: 0.00127108
Iteration 13/1000 | Loss: 0.00157067
Iteration 14/1000 | Loss: 0.00034579
Iteration 15/1000 | Loss: 0.00045963
Iteration 16/1000 | Loss: 0.00030754
Iteration 17/1000 | Loss: 0.00038298
Iteration 18/1000 | Loss: 0.00016090
Iteration 19/1000 | Loss: 0.00043183
Iteration 20/1000 | Loss: 0.00339686
Iteration 21/1000 | Loss: 0.00073985
Iteration 22/1000 | Loss: 0.00051853
Iteration 23/1000 | Loss: 0.00011283
Iteration 24/1000 | Loss: 0.00082596
Iteration 25/1000 | Loss: 0.00024890
Iteration 26/1000 | Loss: 0.00044831
Iteration 27/1000 | Loss: 0.00008436
Iteration 28/1000 | Loss: 0.00012342
Iteration 29/1000 | Loss: 0.00005648
Iteration 30/1000 | Loss: 0.00020729
Iteration 31/1000 | Loss: 0.00004839
Iteration 32/1000 | Loss: 0.00004529
Iteration 33/1000 | Loss: 0.00006376
Iteration 34/1000 | Loss: 0.00021772
Iteration 35/1000 | Loss: 0.00007341
Iteration 36/1000 | Loss: 0.00004583
Iteration 37/1000 | Loss: 0.00005809
Iteration 38/1000 | Loss: 0.00004304
Iteration 39/1000 | Loss: 0.00003670
Iteration 40/1000 | Loss: 0.00003513
Iteration 41/1000 | Loss: 0.00010127
Iteration 42/1000 | Loss: 0.00003164
Iteration 43/1000 | Loss: 0.00003663
Iteration 44/1000 | Loss: 0.00004680
Iteration 45/1000 | Loss: 0.00003081
Iteration 46/1000 | Loss: 0.00003901
Iteration 47/1000 | Loss: 0.00003040
Iteration 48/1000 | Loss: 0.00003040
Iteration 49/1000 | Loss: 0.00003040
Iteration 50/1000 | Loss: 0.00003031
Iteration 51/1000 | Loss: 0.00003022
Iteration 52/1000 | Loss: 0.00003022
Iteration 53/1000 | Loss: 0.00003021
Iteration 54/1000 | Loss: 0.00003021
Iteration 55/1000 | Loss: 0.00003539
Iteration 56/1000 | Loss: 0.00003539
Iteration 57/1000 | Loss: 0.00005325
Iteration 58/1000 | Loss: 0.00003295
Iteration 59/1000 | Loss: 0.00002998
Iteration 60/1000 | Loss: 0.00002998
Iteration 61/1000 | Loss: 0.00002998
Iteration 62/1000 | Loss: 0.00002998
Iteration 63/1000 | Loss: 0.00002998
Iteration 64/1000 | Loss: 0.00002998
Iteration 65/1000 | Loss: 0.00002998
Iteration 66/1000 | Loss: 0.00002998
Iteration 67/1000 | Loss: 0.00002998
Iteration 68/1000 | Loss: 0.00002998
Iteration 69/1000 | Loss: 0.00002997
Iteration 70/1000 | Loss: 0.00002997
Iteration 71/1000 | Loss: 0.00003336
Iteration 72/1000 | Loss: 0.00003336
Iteration 73/1000 | Loss: 0.00007692
Iteration 74/1000 | Loss: 0.00003597
Iteration 75/1000 | Loss: 0.00002991
Iteration 76/1000 | Loss: 0.00002990
Iteration 77/1000 | Loss: 0.00002990
Iteration 78/1000 | Loss: 0.00002990
Iteration 79/1000 | Loss: 0.00002990
Iteration 80/1000 | Loss: 0.00002989
Iteration 81/1000 | Loss: 0.00002989
Iteration 82/1000 | Loss: 0.00002989
Iteration 83/1000 | Loss: 0.00002989
Iteration 84/1000 | Loss: 0.00002988
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002988
Iteration 87/1000 | Loss: 0.00002988
Iteration 88/1000 | Loss: 0.00002988
Iteration 89/1000 | Loss: 0.00002988
Iteration 90/1000 | Loss: 0.00002988
Iteration 91/1000 | Loss: 0.00002988
Iteration 92/1000 | Loss: 0.00002988
Iteration 93/1000 | Loss: 0.00002987
Iteration 94/1000 | Loss: 0.00002987
Iteration 95/1000 | Loss: 0.00002987
Iteration 96/1000 | Loss: 0.00002987
Iteration 97/1000 | Loss: 0.00002987
Iteration 98/1000 | Loss: 0.00002987
Iteration 99/1000 | Loss: 0.00002987
Iteration 100/1000 | Loss: 0.00002987
Iteration 101/1000 | Loss: 0.00002987
Iteration 102/1000 | Loss: 0.00002987
Iteration 103/1000 | Loss: 0.00002987
Iteration 104/1000 | Loss: 0.00002987
Iteration 105/1000 | Loss: 0.00002987
Iteration 106/1000 | Loss: 0.00002987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.9871376682422124e-05, 2.9871376682422124e-05, 2.9871376682422124e-05, 2.9871376682422124e-05, 2.9871376682422124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9871376682422124e-05

Optimization complete. Final v2v error: 4.629749298095703 mm

Highest mean error: 5.435896873474121 mm for frame 184

Lowest mean error: 4.051320552825928 mm for frame 233

Saving results

Total time: 141.38470578193665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051497
Iteration 2/25 | Loss: 0.00201461
Iteration 3/25 | Loss: 0.00117858
Iteration 4/25 | Loss: 0.00121411
Iteration 5/25 | Loss: 0.00099148
Iteration 6/25 | Loss: 0.00090798
Iteration 7/25 | Loss: 0.00086197
Iteration 8/25 | Loss: 0.00084782
Iteration 9/25 | Loss: 0.00083550
Iteration 10/25 | Loss: 0.00082715
Iteration 11/25 | Loss: 0.00082762
Iteration 12/25 | Loss: 0.00081409
Iteration 13/25 | Loss: 0.00081781
Iteration 14/25 | Loss: 0.00081634
Iteration 15/25 | Loss: 0.00081354
Iteration 16/25 | Loss: 0.00079927
Iteration 17/25 | Loss: 0.00078760
Iteration 18/25 | Loss: 0.00077569
Iteration 19/25 | Loss: 0.00077862
Iteration 20/25 | Loss: 0.00078075
Iteration 21/25 | Loss: 0.00077045
Iteration 22/25 | Loss: 0.00076481
Iteration 23/25 | Loss: 0.00075949
Iteration 24/25 | Loss: 0.00076902
Iteration 25/25 | Loss: 0.00076222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28515458
Iteration 2/25 | Loss: 0.00164565
Iteration 3/25 | Loss: 0.00136519
Iteration 4/25 | Loss: 0.00136519
Iteration 5/25 | Loss: 0.00136519
Iteration 6/25 | Loss: 0.00136519
Iteration 7/25 | Loss: 0.00136519
Iteration 8/25 | Loss: 0.00136519
Iteration 9/25 | Loss: 0.00136519
Iteration 10/25 | Loss: 0.00136519
Iteration 11/25 | Loss: 0.00136519
Iteration 12/25 | Loss: 0.00136519
Iteration 13/25 | Loss: 0.00136519
Iteration 14/25 | Loss: 0.00136519
Iteration 15/25 | Loss: 0.00136519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013651883928105235, 0.0013651883928105235, 0.0013651883928105235, 0.0013651883928105235, 0.0013651883928105235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013651883928105235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136519
Iteration 2/1000 | Loss: 0.00021979
Iteration 3/1000 | Loss: 0.00081154
Iteration 4/1000 | Loss: 0.00074700
Iteration 5/1000 | Loss: 0.00011650
Iteration 6/1000 | Loss: 0.00011537
Iteration 7/1000 | Loss: 0.00009320
Iteration 8/1000 | Loss: 0.00024145
Iteration 9/1000 | Loss: 0.00008383
Iteration 10/1000 | Loss: 0.00015861
Iteration 11/1000 | Loss: 0.00007868
Iteration 12/1000 | Loss: 0.00219638
Iteration 13/1000 | Loss: 0.00317608
Iteration 14/1000 | Loss: 0.00069211
Iteration 15/1000 | Loss: 0.00029125
Iteration 16/1000 | Loss: 0.00041430
Iteration 17/1000 | Loss: 0.00008085
Iteration 18/1000 | Loss: 0.00065314
Iteration 19/1000 | Loss: 0.00039400
Iteration 20/1000 | Loss: 0.00004460
Iteration 21/1000 | Loss: 0.00029928
Iteration 22/1000 | Loss: 0.00004521
Iteration 23/1000 | Loss: 0.00002477
Iteration 24/1000 | Loss: 0.00021861
Iteration 25/1000 | Loss: 0.00016669
Iteration 26/1000 | Loss: 0.00004130
Iteration 27/1000 | Loss: 0.00001946
Iteration 28/1000 | Loss: 0.00039816
Iteration 29/1000 | Loss: 0.00002808
Iteration 30/1000 | Loss: 0.00017485
Iteration 31/1000 | Loss: 0.00046312
Iteration 32/1000 | Loss: 0.00055544
Iteration 33/1000 | Loss: 0.00270707
Iteration 34/1000 | Loss: 0.00010138
Iteration 35/1000 | Loss: 0.00022084
Iteration 36/1000 | Loss: 0.00001763
Iteration 37/1000 | Loss: 0.00048293
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00024688
Iteration 41/1000 | Loss: 0.00001603
Iteration 42/1000 | Loss: 0.00001512
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00016309
Iteration 46/1000 | Loss: 0.00013108
Iteration 47/1000 | Loss: 0.00003406
Iteration 48/1000 | Loss: 0.00006513
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001456
Iteration 52/1000 | Loss: 0.00001452
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001440
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001439
Iteration 60/1000 | Loss: 0.00001439
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001437
Iteration 63/1000 | Loss: 0.00001437
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001436
Iteration 66/1000 | Loss: 0.00001436
Iteration 67/1000 | Loss: 0.00001435
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001433
Iteration 75/1000 | Loss: 0.00001433
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001431
Iteration 82/1000 | Loss: 0.00001431
Iteration 83/1000 | Loss: 0.00001431
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001431
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001431
Iteration 91/1000 | Loss: 0.00001431
Iteration 92/1000 | Loss: 0.00001431
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001431
Iteration 108/1000 | Loss: 0.00001431
Iteration 109/1000 | Loss: 0.00001431
Iteration 110/1000 | Loss: 0.00001431
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001431
Iteration 117/1000 | Loss: 0.00001431
Iteration 118/1000 | Loss: 0.00001431
Iteration 119/1000 | Loss: 0.00001431
Iteration 120/1000 | Loss: 0.00001431
Iteration 121/1000 | Loss: 0.00001431
Iteration 122/1000 | Loss: 0.00001431
Iteration 123/1000 | Loss: 0.00001431
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.4308175195765216e-05, 1.4308175195765216e-05, 1.4308175195765216e-05, 1.4308175195765216e-05, 1.4308175195765216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4308175195765216e-05

Optimization complete. Final v2v error: 3.1918516159057617 mm

Highest mean error: 3.745387077331543 mm for frame 53

Lowest mean error: 2.8947458267211914 mm for frame 88

Saving results

Total time: 116.61019349098206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560013
Iteration 2/25 | Loss: 0.00090622
Iteration 3/25 | Loss: 0.00065471
Iteration 4/25 | Loss: 0.00062549
Iteration 5/25 | Loss: 0.00061396
Iteration 6/25 | Loss: 0.00061204
Iteration 7/25 | Loss: 0.00061177
Iteration 8/25 | Loss: 0.00061177
Iteration 9/25 | Loss: 0.00061177
Iteration 10/25 | Loss: 0.00061177
Iteration 11/25 | Loss: 0.00061177
Iteration 12/25 | Loss: 0.00061177
Iteration 13/25 | Loss: 0.00061177
Iteration 14/25 | Loss: 0.00061177
Iteration 15/25 | Loss: 0.00061177
Iteration 16/25 | Loss: 0.00061177
Iteration 17/25 | Loss: 0.00061177
Iteration 18/25 | Loss: 0.00061177
Iteration 19/25 | Loss: 0.00061177
Iteration 20/25 | Loss: 0.00061177
Iteration 21/25 | Loss: 0.00061177
Iteration 22/25 | Loss: 0.00061177
Iteration 23/25 | Loss: 0.00061177
Iteration 24/25 | Loss: 0.00061177
Iteration 25/25 | Loss: 0.00061177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21255362
Iteration 2/25 | Loss: 0.00025767
Iteration 3/25 | Loss: 0.00025767
Iteration 4/25 | Loss: 0.00025767
Iteration 5/25 | Loss: 0.00025767
Iteration 6/25 | Loss: 0.00025767
Iteration 7/25 | Loss: 0.00025767
Iteration 8/25 | Loss: 0.00025766
Iteration 9/25 | Loss: 0.00025766
Iteration 10/25 | Loss: 0.00025766
Iteration 11/25 | Loss: 0.00025766
Iteration 12/25 | Loss: 0.00025766
Iteration 13/25 | Loss: 0.00025766
Iteration 14/25 | Loss: 0.00025766
Iteration 15/25 | Loss: 0.00025766
Iteration 16/25 | Loss: 0.00025766
Iteration 17/25 | Loss: 0.00025766
Iteration 18/25 | Loss: 0.00025766
Iteration 19/25 | Loss: 0.00025766
Iteration 20/25 | Loss: 0.00025766
Iteration 21/25 | Loss: 0.00025766
Iteration 22/25 | Loss: 0.00025766
Iteration 23/25 | Loss: 0.00025766
Iteration 24/25 | Loss: 0.00025766
Iteration 25/25 | Loss: 0.00025766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025766
Iteration 2/1000 | Loss: 0.00002830
Iteration 3/1000 | Loss: 0.00002060
Iteration 4/1000 | Loss: 0.00001811
Iteration 5/1000 | Loss: 0.00001736
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001620
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001540
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001526
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001520
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001507
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001503
Iteration 41/1000 | Loss: 0.00001503
Iteration 42/1000 | Loss: 0.00001503
Iteration 43/1000 | Loss: 0.00001502
Iteration 44/1000 | Loss: 0.00001502
Iteration 45/1000 | Loss: 0.00001502
Iteration 46/1000 | Loss: 0.00001502
Iteration 47/1000 | Loss: 0.00001502
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001501
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001501
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001500
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001499
Iteration 63/1000 | Loss: 0.00001499
Iteration 64/1000 | Loss: 0.00001499
Iteration 65/1000 | Loss: 0.00001499
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001497
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001497
Iteration 75/1000 | Loss: 0.00001496
Iteration 76/1000 | Loss: 0.00001496
Iteration 77/1000 | Loss: 0.00001496
Iteration 78/1000 | Loss: 0.00001496
Iteration 79/1000 | Loss: 0.00001495
Iteration 80/1000 | Loss: 0.00001495
Iteration 81/1000 | Loss: 0.00001495
Iteration 82/1000 | Loss: 0.00001494
Iteration 83/1000 | Loss: 0.00001494
Iteration 84/1000 | Loss: 0.00001494
Iteration 85/1000 | Loss: 0.00001493
Iteration 86/1000 | Loss: 0.00001493
Iteration 87/1000 | Loss: 0.00001493
Iteration 88/1000 | Loss: 0.00001493
Iteration 89/1000 | Loss: 0.00001492
Iteration 90/1000 | Loss: 0.00001492
Iteration 91/1000 | Loss: 0.00001492
Iteration 92/1000 | Loss: 0.00001492
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001491
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001490
Iteration 100/1000 | Loss: 0.00001490
Iteration 101/1000 | Loss: 0.00001489
Iteration 102/1000 | Loss: 0.00001489
Iteration 103/1000 | Loss: 0.00001489
Iteration 104/1000 | Loss: 0.00001489
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001489
Iteration 107/1000 | Loss: 0.00001489
Iteration 108/1000 | Loss: 0.00001489
Iteration 109/1000 | Loss: 0.00001488
Iteration 110/1000 | Loss: 0.00001488
Iteration 111/1000 | Loss: 0.00001488
Iteration 112/1000 | Loss: 0.00001488
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001486
Iteration 125/1000 | Loss: 0.00001486
Iteration 126/1000 | Loss: 0.00001486
Iteration 127/1000 | Loss: 0.00001486
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001485
Iteration 130/1000 | Loss: 0.00001485
Iteration 131/1000 | Loss: 0.00001485
Iteration 132/1000 | Loss: 0.00001485
Iteration 133/1000 | Loss: 0.00001485
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001484
Iteration 139/1000 | Loss: 0.00001484
Iteration 140/1000 | Loss: 0.00001484
Iteration 141/1000 | Loss: 0.00001484
Iteration 142/1000 | Loss: 0.00001484
Iteration 143/1000 | Loss: 0.00001484
Iteration 144/1000 | Loss: 0.00001484
Iteration 145/1000 | Loss: 0.00001483
Iteration 146/1000 | Loss: 0.00001483
Iteration 147/1000 | Loss: 0.00001483
Iteration 148/1000 | Loss: 0.00001483
Iteration 149/1000 | Loss: 0.00001482
Iteration 150/1000 | Loss: 0.00001482
Iteration 151/1000 | Loss: 0.00001482
Iteration 152/1000 | Loss: 0.00001482
Iteration 153/1000 | Loss: 0.00001482
Iteration 154/1000 | Loss: 0.00001482
Iteration 155/1000 | Loss: 0.00001482
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001482
Iteration 158/1000 | Loss: 0.00001482
Iteration 159/1000 | Loss: 0.00001482
Iteration 160/1000 | Loss: 0.00001482
Iteration 161/1000 | Loss: 0.00001482
Iteration 162/1000 | Loss: 0.00001482
Iteration 163/1000 | Loss: 0.00001482
Iteration 164/1000 | Loss: 0.00001482
Iteration 165/1000 | Loss: 0.00001482
Iteration 166/1000 | Loss: 0.00001482
Iteration 167/1000 | Loss: 0.00001482
Iteration 168/1000 | Loss: 0.00001482
Iteration 169/1000 | Loss: 0.00001482
Iteration 170/1000 | Loss: 0.00001482
Iteration 171/1000 | Loss: 0.00001482
Iteration 172/1000 | Loss: 0.00001482
Iteration 173/1000 | Loss: 0.00001482
Iteration 174/1000 | Loss: 0.00001482
Iteration 175/1000 | Loss: 0.00001482
Iteration 176/1000 | Loss: 0.00001482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4824950085312594e-05, 1.4824950085312594e-05, 1.4824950085312594e-05, 1.4824950085312594e-05, 1.4824950085312594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4824950085312594e-05

Optimization complete. Final v2v error: 3.232358694076538 mm

Highest mean error: 3.88773250579834 mm for frame 144

Lowest mean error: 2.453969955444336 mm for frame 0

Saving results

Total time: 43.86217951774597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807509
Iteration 2/25 | Loss: 0.00132108
Iteration 3/25 | Loss: 0.00082602
Iteration 4/25 | Loss: 0.00074406
Iteration 5/25 | Loss: 0.00073433
Iteration 6/25 | Loss: 0.00073203
Iteration 7/25 | Loss: 0.00073158
Iteration 8/25 | Loss: 0.00073158
Iteration 9/25 | Loss: 0.00073158
Iteration 10/25 | Loss: 0.00073158
Iteration 11/25 | Loss: 0.00073158
Iteration 12/25 | Loss: 0.00073158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007315801340155303, 0.0007315801340155303, 0.0007315801340155303, 0.0007315801340155303, 0.0007315801340155303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007315801340155303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28857994
Iteration 2/25 | Loss: 0.00028623
Iteration 3/25 | Loss: 0.00028622
Iteration 4/25 | Loss: 0.00028622
Iteration 5/25 | Loss: 0.00028622
Iteration 6/25 | Loss: 0.00028621
Iteration 7/25 | Loss: 0.00028621
Iteration 8/25 | Loss: 0.00028621
Iteration 9/25 | Loss: 0.00028621
Iteration 10/25 | Loss: 0.00028621
Iteration 11/25 | Loss: 0.00028621
Iteration 12/25 | Loss: 0.00028621
Iteration 13/25 | Loss: 0.00028621
Iteration 14/25 | Loss: 0.00028621
Iteration 15/25 | Loss: 0.00028621
Iteration 16/25 | Loss: 0.00028621
Iteration 17/25 | Loss: 0.00028621
Iteration 18/25 | Loss: 0.00028621
Iteration 19/25 | Loss: 0.00028621
Iteration 20/25 | Loss: 0.00028621
Iteration 21/25 | Loss: 0.00028621
Iteration 22/25 | Loss: 0.00028621
Iteration 23/25 | Loss: 0.00028621
Iteration 24/25 | Loss: 0.00028621
Iteration 25/25 | Loss: 0.00028621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028621
Iteration 2/1000 | Loss: 0.00002826
Iteration 3/1000 | Loss: 0.00002334
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00002021
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001880
Iteration 10/1000 | Loss: 0.00001872
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001854
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001852
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001845
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001838
Iteration 26/1000 | Loss: 0.00001837
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001834
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001831
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001830
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001827
Iteration 37/1000 | Loss: 0.00001826
Iteration 38/1000 | Loss: 0.00001826
Iteration 39/1000 | Loss: 0.00001826
Iteration 40/1000 | Loss: 0.00001826
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001826
Iteration 44/1000 | Loss: 0.00001826
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001825
Iteration 48/1000 | Loss: 0.00001823
Iteration 49/1000 | Loss: 0.00001823
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001822
Iteration 56/1000 | Loss: 0.00001821
Iteration 57/1000 | Loss: 0.00001821
Iteration 58/1000 | Loss: 0.00001821
Iteration 59/1000 | Loss: 0.00001821
Iteration 60/1000 | Loss: 0.00001820
Iteration 61/1000 | Loss: 0.00001820
Iteration 62/1000 | Loss: 0.00001820
Iteration 63/1000 | Loss: 0.00001820
Iteration 64/1000 | Loss: 0.00001820
Iteration 65/1000 | Loss: 0.00001820
Iteration 66/1000 | Loss: 0.00001820
Iteration 67/1000 | Loss: 0.00001820
Iteration 68/1000 | Loss: 0.00001820
Iteration 69/1000 | Loss: 0.00001820
Iteration 70/1000 | Loss: 0.00001820
Iteration 71/1000 | Loss: 0.00001819
Iteration 72/1000 | Loss: 0.00001819
Iteration 73/1000 | Loss: 0.00001819
Iteration 74/1000 | Loss: 0.00001819
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001819
Iteration 79/1000 | Loss: 0.00001819
Iteration 80/1000 | Loss: 0.00001819
Iteration 81/1000 | Loss: 0.00001819
Iteration 82/1000 | Loss: 0.00001819
Iteration 83/1000 | Loss: 0.00001819
Iteration 84/1000 | Loss: 0.00001819
Iteration 85/1000 | Loss: 0.00001819
Iteration 86/1000 | Loss: 0.00001819
Iteration 87/1000 | Loss: 0.00001819
Iteration 88/1000 | Loss: 0.00001819
Iteration 89/1000 | Loss: 0.00001819
Iteration 90/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.8194336007582024e-05, 1.8194336007582024e-05, 1.8194336007582024e-05, 1.8194336007582024e-05, 1.8194336007582024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8194336007582024e-05

Optimization complete. Final v2v error: 3.6217103004455566 mm

Highest mean error: 3.849876880645752 mm for frame 133

Lowest mean error: 3.250953435897827 mm for frame 28

Saving results

Total time: 31.268497228622437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939495
Iteration 2/25 | Loss: 0.00142877
Iteration 3/25 | Loss: 0.00084643
Iteration 4/25 | Loss: 0.00078635
Iteration 5/25 | Loss: 0.00076613
Iteration 6/25 | Loss: 0.00075990
Iteration 7/25 | Loss: 0.00075885
Iteration 8/25 | Loss: 0.00075878
Iteration 9/25 | Loss: 0.00075878
Iteration 10/25 | Loss: 0.00075878
Iteration 11/25 | Loss: 0.00075878
Iteration 12/25 | Loss: 0.00075878
Iteration 13/25 | Loss: 0.00075878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000758783018682152, 0.000758783018682152, 0.000758783018682152, 0.000758783018682152, 0.000758783018682152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000758783018682152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08233643
Iteration 2/25 | Loss: 0.00031581
Iteration 3/25 | Loss: 0.00031579
Iteration 4/25 | Loss: 0.00031579
Iteration 5/25 | Loss: 0.00031579
Iteration 6/25 | Loss: 0.00031579
Iteration 7/25 | Loss: 0.00031579
Iteration 8/25 | Loss: 0.00031579
Iteration 9/25 | Loss: 0.00031579
Iteration 10/25 | Loss: 0.00031579
Iteration 11/25 | Loss: 0.00031579
Iteration 12/25 | Loss: 0.00031579
Iteration 13/25 | Loss: 0.00031579
Iteration 14/25 | Loss: 0.00031579
Iteration 15/25 | Loss: 0.00031579
Iteration 16/25 | Loss: 0.00031579
Iteration 17/25 | Loss: 0.00031579
Iteration 18/25 | Loss: 0.00031579
Iteration 19/25 | Loss: 0.00031579
Iteration 20/25 | Loss: 0.00031579
Iteration 21/25 | Loss: 0.00031579
Iteration 22/25 | Loss: 0.00031579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000315789133310318, 0.000315789133310318, 0.000315789133310318, 0.000315789133310318, 0.000315789133310318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000315789133310318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031579
Iteration 2/1000 | Loss: 0.00006257
Iteration 3/1000 | Loss: 0.00004425
Iteration 4/1000 | Loss: 0.00003951
Iteration 5/1000 | Loss: 0.00003668
Iteration 6/1000 | Loss: 0.00003446
Iteration 7/1000 | Loss: 0.00003323
Iteration 8/1000 | Loss: 0.00003219
Iteration 9/1000 | Loss: 0.00003163
Iteration 10/1000 | Loss: 0.00003118
Iteration 11/1000 | Loss: 0.00003088
Iteration 12/1000 | Loss: 0.00003069
Iteration 13/1000 | Loss: 0.00003059
Iteration 14/1000 | Loss: 0.00003058
Iteration 15/1000 | Loss: 0.00003053
Iteration 16/1000 | Loss: 0.00003053
Iteration 17/1000 | Loss: 0.00003048
Iteration 18/1000 | Loss: 0.00003044
Iteration 19/1000 | Loss: 0.00003043
Iteration 20/1000 | Loss: 0.00003043
Iteration 21/1000 | Loss: 0.00003043
Iteration 22/1000 | Loss: 0.00003043
Iteration 23/1000 | Loss: 0.00003042
Iteration 24/1000 | Loss: 0.00003042
Iteration 25/1000 | Loss: 0.00003042
Iteration 26/1000 | Loss: 0.00003042
Iteration 27/1000 | Loss: 0.00003042
Iteration 28/1000 | Loss: 0.00003042
Iteration 29/1000 | Loss: 0.00003042
Iteration 30/1000 | Loss: 0.00003042
Iteration 31/1000 | Loss: 0.00003041
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00003041
Iteration 34/1000 | Loss: 0.00003041
Iteration 35/1000 | Loss: 0.00003041
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00003040
Iteration 38/1000 | Loss: 0.00003040
Iteration 39/1000 | Loss: 0.00003039
Iteration 40/1000 | Loss: 0.00003039
Iteration 41/1000 | Loss: 0.00003039
Iteration 42/1000 | Loss: 0.00003039
Iteration 43/1000 | Loss: 0.00003039
Iteration 44/1000 | Loss: 0.00003038
Iteration 45/1000 | Loss: 0.00003038
Iteration 46/1000 | Loss: 0.00003038
Iteration 47/1000 | Loss: 0.00003038
Iteration 48/1000 | Loss: 0.00003038
Iteration 49/1000 | Loss: 0.00003038
Iteration 50/1000 | Loss: 0.00003038
Iteration 51/1000 | Loss: 0.00003038
Iteration 52/1000 | Loss: 0.00003038
Iteration 53/1000 | Loss: 0.00003038
Iteration 54/1000 | Loss: 0.00003038
Iteration 55/1000 | Loss: 0.00003038
Iteration 56/1000 | Loss: 0.00003038
Iteration 57/1000 | Loss: 0.00003037
Iteration 58/1000 | Loss: 0.00003037
Iteration 59/1000 | Loss: 0.00003036
Iteration 60/1000 | Loss: 0.00003036
Iteration 61/1000 | Loss: 0.00003035
Iteration 62/1000 | Loss: 0.00003035
Iteration 63/1000 | Loss: 0.00003034
Iteration 64/1000 | Loss: 0.00003034
Iteration 65/1000 | Loss: 0.00003034
Iteration 66/1000 | Loss: 0.00003033
Iteration 67/1000 | Loss: 0.00003033
Iteration 68/1000 | Loss: 0.00003033
Iteration 69/1000 | Loss: 0.00003032
Iteration 70/1000 | Loss: 0.00003032
Iteration 71/1000 | Loss: 0.00003032
Iteration 72/1000 | Loss: 0.00003031
Iteration 73/1000 | Loss: 0.00003031
Iteration 74/1000 | Loss: 0.00003031
Iteration 75/1000 | Loss: 0.00003030
Iteration 76/1000 | Loss: 0.00003030
Iteration 77/1000 | Loss: 0.00003029
Iteration 78/1000 | Loss: 0.00003029
Iteration 79/1000 | Loss: 0.00003028
Iteration 80/1000 | Loss: 0.00003028
Iteration 81/1000 | Loss: 0.00003028
Iteration 82/1000 | Loss: 0.00003027
Iteration 83/1000 | Loss: 0.00003027
Iteration 84/1000 | Loss: 0.00003027
Iteration 85/1000 | Loss: 0.00003026
Iteration 86/1000 | Loss: 0.00003026
Iteration 87/1000 | Loss: 0.00003025
Iteration 88/1000 | Loss: 0.00003025
Iteration 89/1000 | Loss: 0.00003025
Iteration 90/1000 | Loss: 0.00003025
Iteration 91/1000 | Loss: 0.00003024
Iteration 92/1000 | Loss: 0.00003024
Iteration 93/1000 | Loss: 0.00003023
Iteration 94/1000 | Loss: 0.00003023
Iteration 95/1000 | Loss: 0.00003023
Iteration 96/1000 | Loss: 0.00003023
Iteration 97/1000 | Loss: 0.00003022
Iteration 98/1000 | Loss: 0.00003022
Iteration 99/1000 | Loss: 0.00003022
Iteration 100/1000 | Loss: 0.00003022
Iteration 101/1000 | Loss: 0.00003022
Iteration 102/1000 | Loss: 0.00003022
Iteration 103/1000 | Loss: 0.00003022
Iteration 104/1000 | Loss: 0.00003022
Iteration 105/1000 | Loss: 0.00003022
Iteration 106/1000 | Loss: 0.00003022
Iteration 107/1000 | Loss: 0.00003022
Iteration 108/1000 | Loss: 0.00003022
Iteration 109/1000 | Loss: 0.00003021
Iteration 110/1000 | Loss: 0.00003021
Iteration 111/1000 | Loss: 0.00003021
Iteration 112/1000 | Loss: 0.00003021
Iteration 113/1000 | Loss: 0.00003021
Iteration 114/1000 | Loss: 0.00003021
Iteration 115/1000 | Loss: 0.00003020
Iteration 116/1000 | Loss: 0.00003020
Iteration 117/1000 | Loss: 0.00003020
Iteration 118/1000 | Loss: 0.00003020
Iteration 119/1000 | Loss: 0.00003020
Iteration 120/1000 | Loss: 0.00003020
Iteration 121/1000 | Loss: 0.00003020
Iteration 122/1000 | Loss: 0.00003020
Iteration 123/1000 | Loss: 0.00003020
Iteration 124/1000 | Loss: 0.00003019
Iteration 125/1000 | Loss: 0.00003019
Iteration 126/1000 | Loss: 0.00003019
Iteration 127/1000 | Loss: 0.00003019
Iteration 128/1000 | Loss: 0.00003019
Iteration 129/1000 | Loss: 0.00003019
Iteration 130/1000 | Loss: 0.00003019
Iteration 131/1000 | Loss: 0.00003019
Iteration 132/1000 | Loss: 0.00003019
Iteration 133/1000 | Loss: 0.00003019
Iteration 134/1000 | Loss: 0.00003019
Iteration 135/1000 | Loss: 0.00003019
Iteration 136/1000 | Loss: 0.00003019
Iteration 137/1000 | Loss: 0.00003019
Iteration 138/1000 | Loss: 0.00003019
Iteration 139/1000 | Loss: 0.00003019
Iteration 140/1000 | Loss: 0.00003019
Iteration 141/1000 | Loss: 0.00003019
Iteration 142/1000 | Loss: 0.00003019
Iteration 143/1000 | Loss: 0.00003019
Iteration 144/1000 | Loss: 0.00003019
Iteration 145/1000 | Loss: 0.00003019
Iteration 146/1000 | Loss: 0.00003019
Iteration 147/1000 | Loss: 0.00003019
Iteration 148/1000 | Loss: 0.00003019
Iteration 149/1000 | Loss: 0.00003019
Iteration 150/1000 | Loss: 0.00003019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [3.019076757482253e-05, 3.019076757482253e-05, 3.019076757482253e-05, 3.019076757482253e-05, 3.019076757482253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.019076757482253e-05

Optimization complete. Final v2v error: 4.544418811798096 mm

Highest mean error: 5.3996052742004395 mm for frame 103

Lowest mean error: 3.870393991470337 mm for frame 43

Saving results

Total time: 39.54430103302002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574054
Iteration 2/25 | Loss: 0.00082089
Iteration 3/25 | Loss: 0.00066780
Iteration 4/25 | Loss: 0.00064411
Iteration 5/25 | Loss: 0.00063667
Iteration 6/25 | Loss: 0.00063536
Iteration 7/25 | Loss: 0.00063517
Iteration 8/25 | Loss: 0.00063517
Iteration 9/25 | Loss: 0.00063517
Iteration 10/25 | Loss: 0.00063517
Iteration 11/25 | Loss: 0.00063517
Iteration 12/25 | Loss: 0.00063517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006351693882606924, 0.0006351693882606924, 0.0006351693882606924, 0.0006351693882606924, 0.0006351693882606924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006351693882606924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86300039
Iteration 2/25 | Loss: 0.00026725
Iteration 3/25 | Loss: 0.00026725
Iteration 4/25 | Loss: 0.00026725
Iteration 5/25 | Loss: 0.00026725
Iteration 6/25 | Loss: 0.00026725
Iteration 7/25 | Loss: 0.00026725
Iteration 8/25 | Loss: 0.00026725
Iteration 9/25 | Loss: 0.00026725
Iteration 10/25 | Loss: 0.00026725
Iteration 11/25 | Loss: 0.00026725
Iteration 12/25 | Loss: 0.00026725
Iteration 13/25 | Loss: 0.00026725
Iteration 14/25 | Loss: 0.00026725
Iteration 15/25 | Loss: 0.00026725
Iteration 16/25 | Loss: 0.00026725
Iteration 17/25 | Loss: 0.00026725
Iteration 18/25 | Loss: 0.00026725
Iteration 19/25 | Loss: 0.00026725
Iteration 20/25 | Loss: 0.00026725
Iteration 21/25 | Loss: 0.00026725
Iteration 22/25 | Loss: 0.00026725
Iteration 23/25 | Loss: 0.00026725
Iteration 24/25 | Loss: 0.00026725
Iteration 25/25 | Loss: 0.00026725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026725
Iteration 2/1000 | Loss: 0.00002857
Iteration 3/1000 | Loss: 0.00001999
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001662
Iteration 7/1000 | Loss: 0.00001620
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001576
Iteration 11/1000 | Loss: 0.00001575
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001550
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001542
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001542
Iteration 21/1000 | Loss: 0.00001541
Iteration 22/1000 | Loss: 0.00001541
Iteration 23/1000 | Loss: 0.00001541
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001540
Iteration 27/1000 | Loss: 0.00001540
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001535
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001534
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001530
Iteration 44/1000 | Loss: 0.00001530
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001527
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001527
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001525
Iteration 60/1000 | Loss: 0.00001525
Iteration 61/1000 | Loss: 0.00001524
Iteration 62/1000 | Loss: 0.00001524
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001524
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001521
Iteration 73/1000 | Loss: 0.00001521
Iteration 74/1000 | Loss: 0.00001521
Iteration 75/1000 | Loss: 0.00001520
Iteration 76/1000 | Loss: 0.00001520
Iteration 77/1000 | Loss: 0.00001520
Iteration 78/1000 | Loss: 0.00001519
Iteration 79/1000 | Loss: 0.00001519
Iteration 80/1000 | Loss: 0.00001519
Iteration 81/1000 | Loss: 0.00001519
Iteration 82/1000 | Loss: 0.00001519
Iteration 83/1000 | Loss: 0.00001519
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001518
Iteration 87/1000 | Loss: 0.00001518
Iteration 88/1000 | Loss: 0.00001518
Iteration 89/1000 | Loss: 0.00001517
Iteration 90/1000 | Loss: 0.00001517
Iteration 91/1000 | Loss: 0.00001517
Iteration 92/1000 | Loss: 0.00001517
Iteration 93/1000 | Loss: 0.00001517
Iteration 94/1000 | Loss: 0.00001516
Iteration 95/1000 | Loss: 0.00001516
Iteration 96/1000 | Loss: 0.00001516
Iteration 97/1000 | Loss: 0.00001516
Iteration 98/1000 | Loss: 0.00001516
Iteration 99/1000 | Loss: 0.00001516
Iteration 100/1000 | Loss: 0.00001515
Iteration 101/1000 | Loss: 0.00001515
Iteration 102/1000 | Loss: 0.00001515
Iteration 103/1000 | Loss: 0.00001515
Iteration 104/1000 | Loss: 0.00001515
Iteration 105/1000 | Loss: 0.00001515
Iteration 106/1000 | Loss: 0.00001515
Iteration 107/1000 | Loss: 0.00001515
Iteration 108/1000 | Loss: 0.00001515
Iteration 109/1000 | Loss: 0.00001515
Iteration 110/1000 | Loss: 0.00001515
Iteration 111/1000 | Loss: 0.00001515
Iteration 112/1000 | Loss: 0.00001514
Iteration 113/1000 | Loss: 0.00001514
Iteration 114/1000 | Loss: 0.00001514
Iteration 115/1000 | Loss: 0.00001514
Iteration 116/1000 | Loss: 0.00001514
Iteration 117/1000 | Loss: 0.00001514
Iteration 118/1000 | Loss: 0.00001514
Iteration 119/1000 | Loss: 0.00001514
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001514
Iteration 122/1000 | Loss: 0.00001514
Iteration 123/1000 | Loss: 0.00001514
Iteration 124/1000 | Loss: 0.00001514
Iteration 125/1000 | Loss: 0.00001514
Iteration 126/1000 | Loss: 0.00001514
Iteration 127/1000 | Loss: 0.00001514
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001514
Iteration 131/1000 | Loss: 0.00001514
Iteration 132/1000 | Loss: 0.00001514
Iteration 133/1000 | Loss: 0.00001514
Iteration 134/1000 | Loss: 0.00001514
Iteration 135/1000 | Loss: 0.00001514
Iteration 136/1000 | Loss: 0.00001514
Iteration 137/1000 | Loss: 0.00001514
Iteration 138/1000 | Loss: 0.00001514
Iteration 139/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5144452845561318e-05, 1.5144452845561318e-05, 1.5144452845561318e-05, 1.5144452845561318e-05, 1.5144452845561318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5144452845561318e-05

Optimization complete. Final v2v error: 3.3533124923706055 mm

Highest mean error: 3.586008071899414 mm for frame 33

Lowest mean error: 3.1367506980895996 mm for frame 82

Saving results

Total time: 33.86062049865723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860448
Iteration 2/25 | Loss: 0.00094682
Iteration 3/25 | Loss: 0.00070153
Iteration 4/25 | Loss: 0.00065766
Iteration 5/25 | Loss: 0.00064993
Iteration 6/25 | Loss: 0.00064929
Iteration 7/25 | Loss: 0.00064929
Iteration 8/25 | Loss: 0.00064929
Iteration 9/25 | Loss: 0.00064929
Iteration 10/25 | Loss: 0.00064929
Iteration 11/25 | Loss: 0.00064929
Iteration 12/25 | Loss: 0.00064929
Iteration 13/25 | Loss: 0.00064929
Iteration 14/25 | Loss: 0.00064929
Iteration 15/25 | Loss: 0.00064929
Iteration 16/25 | Loss: 0.00064929
Iteration 17/25 | Loss: 0.00064929
Iteration 18/25 | Loss: 0.00064929
Iteration 19/25 | Loss: 0.00064929
Iteration 20/25 | Loss: 0.00064929
Iteration 21/25 | Loss: 0.00064929
Iteration 22/25 | Loss: 0.00064929
Iteration 23/25 | Loss: 0.00064929
Iteration 24/25 | Loss: 0.00064929
Iteration 25/25 | Loss: 0.00064929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27080274
Iteration 2/25 | Loss: 0.00033192
Iteration 3/25 | Loss: 0.00033188
Iteration 4/25 | Loss: 0.00033188
Iteration 5/25 | Loss: 0.00033188
Iteration 6/25 | Loss: 0.00033188
Iteration 7/25 | Loss: 0.00033188
Iteration 8/25 | Loss: 0.00033188
Iteration 9/25 | Loss: 0.00033188
Iteration 10/25 | Loss: 0.00033188
Iteration 11/25 | Loss: 0.00033188
Iteration 12/25 | Loss: 0.00033188
Iteration 13/25 | Loss: 0.00033188
Iteration 14/25 | Loss: 0.00033188
Iteration 15/25 | Loss: 0.00033188
Iteration 16/25 | Loss: 0.00033188
Iteration 17/25 | Loss: 0.00033188
Iteration 18/25 | Loss: 0.00033188
Iteration 19/25 | Loss: 0.00033188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003318771778140217, 0.0003318771778140217, 0.0003318771778140217, 0.0003318771778140217, 0.0003318771778140217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003318771778140217

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033188
Iteration 2/1000 | Loss: 0.00004471
Iteration 3/1000 | Loss: 0.00003368
Iteration 4/1000 | Loss: 0.00002822
Iteration 5/1000 | Loss: 0.00002577
Iteration 6/1000 | Loss: 0.00002390
Iteration 7/1000 | Loss: 0.00002262
Iteration 8/1000 | Loss: 0.00002202
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002144
Iteration 11/1000 | Loss: 0.00002128
Iteration 12/1000 | Loss: 0.00002115
Iteration 13/1000 | Loss: 0.00002110
Iteration 14/1000 | Loss: 0.00002106
Iteration 15/1000 | Loss: 0.00002104
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002102
Iteration 19/1000 | Loss: 0.00002101
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002100
Iteration 22/1000 | Loss: 0.00002100
Iteration 23/1000 | Loss: 0.00002100
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002099
Iteration 26/1000 | Loss: 0.00002099
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002099
Iteration 30/1000 | Loss: 0.00002099
Iteration 31/1000 | Loss: 0.00002099
Iteration 32/1000 | Loss: 0.00002099
Iteration 33/1000 | Loss: 0.00002099
Iteration 34/1000 | Loss: 0.00002099
Iteration 35/1000 | Loss: 0.00002098
Iteration 36/1000 | Loss: 0.00002098
Iteration 37/1000 | Loss: 0.00002098
Iteration 38/1000 | Loss: 0.00002098
Iteration 39/1000 | Loss: 0.00002098
Iteration 40/1000 | Loss: 0.00002097
Iteration 41/1000 | Loss: 0.00002097
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002096
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002095
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002092
Iteration 59/1000 | Loss: 0.00002092
Iteration 60/1000 | Loss: 0.00002092
Iteration 61/1000 | Loss: 0.00002092
Iteration 62/1000 | Loss: 0.00002092
Iteration 63/1000 | Loss: 0.00002092
Iteration 64/1000 | Loss: 0.00002091
Iteration 65/1000 | Loss: 0.00002091
Iteration 66/1000 | Loss: 0.00002091
Iteration 67/1000 | Loss: 0.00002091
Iteration 68/1000 | Loss: 0.00002091
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002090
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00002090
Iteration 74/1000 | Loss: 0.00002090
Iteration 75/1000 | Loss: 0.00002090
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002090
Iteration 78/1000 | Loss: 0.00002090
Iteration 79/1000 | Loss: 0.00002090
Iteration 80/1000 | Loss: 0.00002090
Iteration 81/1000 | Loss: 0.00002090
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002089
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002089
Iteration 98/1000 | Loss: 0.00002089
Iteration 99/1000 | Loss: 0.00002089
Iteration 100/1000 | Loss: 0.00002089
Iteration 101/1000 | Loss: 0.00002089
Iteration 102/1000 | Loss: 0.00002089
Iteration 103/1000 | Loss: 0.00002089
Iteration 104/1000 | Loss: 0.00002089
Iteration 105/1000 | Loss: 0.00002089
Iteration 106/1000 | Loss: 0.00002089
Iteration 107/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.08887013286585e-05, 2.08887013286585e-05, 2.08887013286585e-05, 2.08887013286585e-05, 2.08887013286585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.08887013286585e-05

Optimization complete. Final v2v error: 3.8751437664031982 mm

Highest mean error: 4.5605292320251465 mm for frame 38

Lowest mean error: 3.300290822982788 mm for frame 187

Saving results

Total time: 33.619513511657715
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609617
Iteration 2/25 | Loss: 0.00139710
Iteration 3/25 | Loss: 0.00086524
Iteration 4/25 | Loss: 0.00081525
Iteration 5/25 | Loss: 0.00080289
Iteration 6/25 | Loss: 0.00079934
Iteration 7/25 | Loss: 0.00079859
Iteration 8/25 | Loss: 0.00079859
Iteration 9/25 | Loss: 0.00079859
Iteration 10/25 | Loss: 0.00079859
Iteration 11/25 | Loss: 0.00079859
Iteration 12/25 | Loss: 0.00079859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007985889678820968, 0.0007985889678820968, 0.0007985889678820968, 0.0007985889678820968, 0.0007985889678820968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007985889678820968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93207449
Iteration 2/25 | Loss: 0.00029366
Iteration 3/25 | Loss: 0.00029365
Iteration 4/25 | Loss: 0.00029365
Iteration 5/25 | Loss: 0.00029365
Iteration 6/25 | Loss: 0.00029365
Iteration 7/25 | Loss: 0.00029365
Iteration 8/25 | Loss: 0.00029365
Iteration 9/25 | Loss: 0.00029365
Iteration 10/25 | Loss: 0.00029365
Iteration 11/25 | Loss: 0.00029365
Iteration 12/25 | Loss: 0.00029365
Iteration 13/25 | Loss: 0.00029365
Iteration 14/25 | Loss: 0.00029365
Iteration 15/25 | Loss: 0.00029365
Iteration 16/25 | Loss: 0.00029365
Iteration 17/25 | Loss: 0.00029365
Iteration 18/25 | Loss: 0.00029365
Iteration 19/25 | Loss: 0.00029365
Iteration 20/25 | Loss: 0.00029365
Iteration 21/25 | Loss: 0.00029365
Iteration 22/25 | Loss: 0.00029365
Iteration 23/25 | Loss: 0.00029365
Iteration 24/25 | Loss: 0.00029365
Iteration 25/25 | Loss: 0.00029365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029365
Iteration 2/1000 | Loss: 0.00005506
Iteration 3/1000 | Loss: 0.00003894
Iteration 4/1000 | Loss: 0.00003613
Iteration 5/1000 | Loss: 0.00003425
Iteration 6/1000 | Loss: 0.00003330
Iteration 7/1000 | Loss: 0.00003270
Iteration 8/1000 | Loss: 0.00003223
Iteration 9/1000 | Loss: 0.00003190
Iteration 10/1000 | Loss: 0.00003166
Iteration 11/1000 | Loss: 0.00003145
Iteration 12/1000 | Loss: 0.00003136
Iteration 13/1000 | Loss: 0.00003131
Iteration 14/1000 | Loss: 0.00003130
Iteration 15/1000 | Loss: 0.00003130
Iteration 16/1000 | Loss: 0.00003127
Iteration 17/1000 | Loss: 0.00003118
Iteration 18/1000 | Loss: 0.00003113
Iteration 19/1000 | Loss: 0.00003107
Iteration 20/1000 | Loss: 0.00003104
Iteration 21/1000 | Loss: 0.00003104
Iteration 22/1000 | Loss: 0.00003104
Iteration 23/1000 | Loss: 0.00003103
Iteration 24/1000 | Loss: 0.00003093
Iteration 25/1000 | Loss: 0.00003092
Iteration 26/1000 | Loss: 0.00003090
Iteration 27/1000 | Loss: 0.00003090
Iteration 28/1000 | Loss: 0.00003090
Iteration 29/1000 | Loss: 0.00003090
Iteration 30/1000 | Loss: 0.00003089
Iteration 31/1000 | Loss: 0.00003089
Iteration 32/1000 | Loss: 0.00003089
Iteration 33/1000 | Loss: 0.00003089
Iteration 34/1000 | Loss: 0.00003089
Iteration 35/1000 | Loss: 0.00003089
Iteration 36/1000 | Loss: 0.00003089
Iteration 37/1000 | Loss: 0.00003089
Iteration 38/1000 | Loss: 0.00003089
Iteration 39/1000 | Loss: 0.00003087
Iteration 40/1000 | Loss: 0.00003087
Iteration 41/1000 | Loss: 0.00003085
Iteration 42/1000 | Loss: 0.00003084
Iteration 43/1000 | Loss: 0.00003081
Iteration 44/1000 | Loss: 0.00003081
Iteration 45/1000 | Loss: 0.00003080
Iteration 46/1000 | Loss: 0.00003080
Iteration 47/1000 | Loss: 0.00003080
Iteration 48/1000 | Loss: 0.00003079
Iteration 49/1000 | Loss: 0.00003079
Iteration 50/1000 | Loss: 0.00003079
Iteration 51/1000 | Loss: 0.00003079
Iteration 52/1000 | Loss: 0.00003079
Iteration 53/1000 | Loss: 0.00003079
Iteration 54/1000 | Loss: 0.00003078
Iteration 55/1000 | Loss: 0.00003078
Iteration 56/1000 | Loss: 0.00003078
Iteration 57/1000 | Loss: 0.00003078
Iteration 58/1000 | Loss: 0.00003078
Iteration 59/1000 | Loss: 0.00003078
Iteration 60/1000 | Loss: 0.00003078
Iteration 61/1000 | Loss: 0.00003078
Iteration 62/1000 | Loss: 0.00003077
Iteration 63/1000 | Loss: 0.00003077
Iteration 64/1000 | Loss: 0.00003077
Iteration 65/1000 | Loss: 0.00003077
Iteration 66/1000 | Loss: 0.00003076
Iteration 67/1000 | Loss: 0.00003076
Iteration 68/1000 | Loss: 0.00003076
Iteration 69/1000 | Loss: 0.00003076
Iteration 70/1000 | Loss: 0.00003076
Iteration 71/1000 | Loss: 0.00003076
Iteration 72/1000 | Loss: 0.00003076
Iteration 73/1000 | Loss: 0.00003075
Iteration 74/1000 | Loss: 0.00003075
Iteration 75/1000 | Loss: 0.00003075
Iteration 76/1000 | Loss: 0.00003075
Iteration 77/1000 | Loss: 0.00003074
Iteration 78/1000 | Loss: 0.00003073
Iteration 79/1000 | Loss: 0.00003073
Iteration 80/1000 | Loss: 0.00003073
Iteration 81/1000 | Loss: 0.00003072
Iteration 82/1000 | Loss: 0.00003072
Iteration 83/1000 | Loss: 0.00003072
Iteration 84/1000 | Loss: 0.00003071
Iteration 85/1000 | Loss: 0.00003071
Iteration 86/1000 | Loss: 0.00003071
Iteration 87/1000 | Loss: 0.00003071
Iteration 88/1000 | Loss: 0.00003071
Iteration 89/1000 | Loss: 0.00003071
Iteration 90/1000 | Loss: 0.00003070
Iteration 91/1000 | Loss: 0.00003070
Iteration 92/1000 | Loss: 0.00003070
Iteration 93/1000 | Loss: 0.00003070
Iteration 94/1000 | Loss: 0.00003070
Iteration 95/1000 | Loss: 0.00003070
Iteration 96/1000 | Loss: 0.00003069
Iteration 97/1000 | Loss: 0.00003069
Iteration 98/1000 | Loss: 0.00003069
Iteration 99/1000 | Loss: 0.00003069
Iteration 100/1000 | Loss: 0.00003069
Iteration 101/1000 | Loss: 0.00003069
Iteration 102/1000 | Loss: 0.00003069
Iteration 103/1000 | Loss: 0.00003069
Iteration 104/1000 | Loss: 0.00003069
Iteration 105/1000 | Loss: 0.00003069
Iteration 106/1000 | Loss: 0.00003068
Iteration 107/1000 | Loss: 0.00003068
Iteration 108/1000 | Loss: 0.00003068
Iteration 109/1000 | Loss: 0.00003068
Iteration 110/1000 | Loss: 0.00003068
Iteration 111/1000 | Loss: 0.00003068
Iteration 112/1000 | Loss: 0.00003068
Iteration 113/1000 | Loss: 0.00003068
Iteration 114/1000 | Loss: 0.00003068
Iteration 115/1000 | Loss: 0.00003068
Iteration 116/1000 | Loss: 0.00003068
Iteration 117/1000 | Loss: 0.00003068
Iteration 118/1000 | Loss: 0.00003068
Iteration 119/1000 | Loss: 0.00003067
Iteration 120/1000 | Loss: 0.00003067
Iteration 121/1000 | Loss: 0.00003067
Iteration 122/1000 | Loss: 0.00003067
Iteration 123/1000 | Loss: 0.00003067
Iteration 124/1000 | Loss: 0.00003067
Iteration 125/1000 | Loss: 0.00003066
Iteration 126/1000 | Loss: 0.00003066
Iteration 127/1000 | Loss: 0.00003066
Iteration 128/1000 | Loss: 0.00003065
Iteration 129/1000 | Loss: 0.00003065
Iteration 130/1000 | Loss: 0.00003065
Iteration 131/1000 | Loss: 0.00003065
Iteration 132/1000 | Loss: 0.00003065
Iteration 133/1000 | Loss: 0.00003065
Iteration 134/1000 | Loss: 0.00003065
Iteration 135/1000 | Loss: 0.00003065
Iteration 136/1000 | Loss: 0.00003065
Iteration 137/1000 | Loss: 0.00003065
Iteration 138/1000 | Loss: 0.00003065
Iteration 139/1000 | Loss: 0.00003065
Iteration 140/1000 | Loss: 0.00003065
Iteration 141/1000 | Loss: 0.00003065
Iteration 142/1000 | Loss: 0.00003065
Iteration 143/1000 | Loss: 0.00003064
Iteration 144/1000 | Loss: 0.00003064
Iteration 145/1000 | Loss: 0.00003064
Iteration 146/1000 | Loss: 0.00003064
Iteration 147/1000 | Loss: 0.00003064
Iteration 148/1000 | Loss: 0.00003064
Iteration 149/1000 | Loss: 0.00003064
Iteration 150/1000 | Loss: 0.00003064
Iteration 151/1000 | Loss: 0.00003064
Iteration 152/1000 | Loss: 0.00003064
Iteration 153/1000 | Loss: 0.00003063
Iteration 154/1000 | Loss: 0.00003063
Iteration 155/1000 | Loss: 0.00003063
Iteration 156/1000 | Loss: 0.00003063
Iteration 157/1000 | Loss: 0.00003063
Iteration 158/1000 | Loss: 0.00003063
Iteration 159/1000 | Loss: 0.00003063
Iteration 160/1000 | Loss: 0.00003063
Iteration 161/1000 | Loss: 0.00003063
Iteration 162/1000 | Loss: 0.00003063
Iteration 163/1000 | Loss: 0.00003063
Iteration 164/1000 | Loss: 0.00003063
Iteration 165/1000 | Loss: 0.00003063
Iteration 166/1000 | Loss: 0.00003063
Iteration 167/1000 | Loss: 0.00003063
Iteration 168/1000 | Loss: 0.00003062
Iteration 169/1000 | Loss: 0.00003062
Iteration 170/1000 | Loss: 0.00003062
Iteration 171/1000 | Loss: 0.00003062
Iteration 172/1000 | Loss: 0.00003062
Iteration 173/1000 | Loss: 0.00003062
Iteration 174/1000 | Loss: 0.00003062
Iteration 175/1000 | Loss: 0.00003061
Iteration 176/1000 | Loss: 0.00003061
Iteration 177/1000 | Loss: 0.00003061
Iteration 178/1000 | Loss: 0.00003061
Iteration 179/1000 | Loss: 0.00003061
Iteration 180/1000 | Loss: 0.00003061
Iteration 181/1000 | Loss: 0.00003061
Iteration 182/1000 | Loss: 0.00003061
Iteration 183/1000 | Loss: 0.00003061
Iteration 184/1000 | Loss: 0.00003061
Iteration 185/1000 | Loss: 0.00003061
Iteration 186/1000 | Loss: 0.00003061
Iteration 187/1000 | Loss: 0.00003060
Iteration 188/1000 | Loss: 0.00003060
Iteration 189/1000 | Loss: 0.00003060
Iteration 190/1000 | Loss: 0.00003060
Iteration 191/1000 | Loss: 0.00003060
Iteration 192/1000 | Loss: 0.00003060
Iteration 193/1000 | Loss: 0.00003060
Iteration 194/1000 | Loss: 0.00003060
Iteration 195/1000 | Loss: 0.00003060
Iteration 196/1000 | Loss: 0.00003060
Iteration 197/1000 | Loss: 0.00003060
Iteration 198/1000 | Loss: 0.00003060
Iteration 199/1000 | Loss: 0.00003060
Iteration 200/1000 | Loss: 0.00003060
Iteration 201/1000 | Loss: 0.00003060
Iteration 202/1000 | Loss: 0.00003060
Iteration 203/1000 | Loss: 0.00003060
Iteration 204/1000 | Loss: 0.00003060
Iteration 205/1000 | Loss: 0.00003060
Iteration 206/1000 | Loss: 0.00003060
Iteration 207/1000 | Loss: 0.00003060
Iteration 208/1000 | Loss: 0.00003060
Iteration 209/1000 | Loss: 0.00003060
Iteration 210/1000 | Loss: 0.00003060
Iteration 211/1000 | Loss: 0.00003060
Iteration 212/1000 | Loss: 0.00003060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [3.059809023397975e-05, 3.059809023397975e-05, 3.059809023397975e-05, 3.059809023397975e-05, 3.059809023397975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.059809023397975e-05

Optimization complete. Final v2v error: 4.3710856437683105 mm

Highest mean error: 5.029582500457764 mm for frame 124

Lowest mean error: 3.5013322830200195 mm for frame 47

Saving results

Total time: 45.66325330734253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037982
Iteration 2/25 | Loss: 0.01037982
Iteration 3/25 | Loss: 0.00202618
Iteration 4/25 | Loss: 0.00117585
Iteration 5/25 | Loss: 0.00099714
Iteration 6/25 | Loss: 0.00094294
Iteration 7/25 | Loss: 0.00099525
Iteration 8/25 | Loss: 0.00092047
Iteration 9/25 | Loss: 0.00083389
Iteration 10/25 | Loss: 0.00079890
Iteration 11/25 | Loss: 0.00075245
Iteration 12/25 | Loss: 0.00073776
Iteration 13/25 | Loss: 0.00073428
Iteration 14/25 | Loss: 0.00073170
Iteration 15/25 | Loss: 0.00072727
Iteration 16/25 | Loss: 0.00072551
Iteration 17/25 | Loss: 0.00072484
Iteration 18/25 | Loss: 0.00072463
Iteration 19/25 | Loss: 0.00072455
Iteration 20/25 | Loss: 0.00072631
Iteration 21/25 | Loss: 0.00072363
Iteration 22/25 | Loss: 0.00072323
Iteration 23/25 | Loss: 0.00072312
Iteration 24/25 | Loss: 0.00072305
Iteration 25/25 | Loss: 0.00072305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27906382
Iteration 2/25 | Loss: 0.00036167
Iteration 3/25 | Loss: 0.00036167
Iteration 4/25 | Loss: 0.00036167
Iteration 5/25 | Loss: 0.00036167
Iteration 6/25 | Loss: 0.00036167
Iteration 7/25 | Loss: 0.00036167
Iteration 8/25 | Loss: 0.00036167
Iteration 9/25 | Loss: 0.00036167
Iteration 10/25 | Loss: 0.00036167
Iteration 11/25 | Loss: 0.00036167
Iteration 12/25 | Loss: 0.00036167
Iteration 13/25 | Loss: 0.00036167
Iteration 14/25 | Loss: 0.00036167
Iteration 15/25 | Loss: 0.00036167
Iteration 16/25 | Loss: 0.00036167
Iteration 17/25 | Loss: 0.00036167
Iteration 18/25 | Loss: 0.00036167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00036166503559798, 0.00036166503559798, 0.00036166503559798, 0.00036166503559798, 0.00036166503559798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036166503559798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036167
Iteration 2/1000 | Loss: 0.00003558
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002700
Iteration 5/1000 | Loss: 0.00002589
Iteration 6/1000 | Loss: 0.00002553
Iteration 7/1000 | Loss: 0.00002528
Iteration 8/1000 | Loss: 0.00002513
Iteration 9/1000 | Loss: 0.00002499
Iteration 10/1000 | Loss: 0.00002491
Iteration 11/1000 | Loss: 0.00002489
Iteration 12/1000 | Loss: 0.00002486
Iteration 13/1000 | Loss: 0.00002486
Iteration 14/1000 | Loss: 0.00002485
Iteration 15/1000 | Loss: 0.00002479
Iteration 16/1000 | Loss: 0.00002478
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002476
Iteration 19/1000 | Loss: 0.00002475
Iteration 20/1000 | Loss: 0.00002475
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002475
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002474
Iteration 25/1000 | Loss: 0.00002474
Iteration 26/1000 | Loss: 0.00002474
Iteration 27/1000 | Loss: 0.00002474
Iteration 28/1000 | Loss: 0.00002474
Iteration 29/1000 | Loss: 0.00002474
Iteration 30/1000 | Loss: 0.00002473
Iteration 31/1000 | Loss: 0.00002473
Iteration 32/1000 | Loss: 0.00002473
Iteration 33/1000 | Loss: 0.00002473
Iteration 34/1000 | Loss: 0.00002473
Iteration 35/1000 | Loss: 0.00002473
Iteration 36/1000 | Loss: 0.00002473
Iteration 37/1000 | Loss: 0.00002472
Iteration 38/1000 | Loss: 0.00002472
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002471
Iteration 41/1000 | Loss: 0.00002471
Iteration 42/1000 | Loss: 0.00002471
Iteration 43/1000 | Loss: 0.00002471
Iteration 44/1000 | Loss: 0.00002470
Iteration 45/1000 | Loss: 0.00002470
Iteration 46/1000 | Loss: 0.00002469
Iteration 47/1000 | Loss: 0.00002468
Iteration 48/1000 | Loss: 0.00002467
Iteration 49/1000 | Loss: 0.00002467
Iteration 50/1000 | Loss: 0.00002467
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002466
Iteration 54/1000 | Loss: 0.00002466
Iteration 55/1000 | Loss: 0.00002466
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002466
Iteration 58/1000 | Loss: 0.00002466
Iteration 59/1000 | Loss: 0.00002466
Iteration 60/1000 | Loss: 0.00002466
Iteration 61/1000 | Loss: 0.00002466
Iteration 62/1000 | Loss: 0.00002466
Iteration 63/1000 | Loss: 0.00002466
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002466
Iteration 67/1000 | Loss: 0.00002466
Iteration 68/1000 | Loss: 0.00002466
Iteration 69/1000 | Loss: 0.00002466
Iteration 70/1000 | Loss: 0.00002466
Iteration 71/1000 | Loss: 0.00002466
Iteration 72/1000 | Loss: 0.00002466
Iteration 73/1000 | Loss: 0.00002466
Iteration 74/1000 | Loss: 0.00002466
Iteration 75/1000 | Loss: 0.00002466
Iteration 76/1000 | Loss: 0.00002466
Iteration 77/1000 | Loss: 0.00002466
Iteration 78/1000 | Loss: 0.00002466
Iteration 79/1000 | Loss: 0.00002466
Iteration 80/1000 | Loss: 0.00002466
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002466
Iteration 83/1000 | Loss: 0.00002466
Iteration 84/1000 | Loss: 0.00002466
Iteration 85/1000 | Loss: 0.00002466
Iteration 86/1000 | Loss: 0.00002466
Iteration 87/1000 | Loss: 0.00002466
Iteration 88/1000 | Loss: 0.00002466
Iteration 89/1000 | Loss: 0.00002466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.4659468181198463e-05, 2.4659468181198463e-05, 2.4659468181198463e-05, 2.4659468181198463e-05, 2.4659468181198463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4659468181198463e-05

Optimization complete. Final v2v error: 4.062601089477539 mm

Highest mean error: 4.339622497558594 mm for frame 10

Lowest mean error: 3.8964645862579346 mm for frame 229

Saving results

Total time: 66.2490484714508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051674
Iteration 2/25 | Loss: 0.00220598
Iteration 3/25 | Loss: 0.00117969
Iteration 4/25 | Loss: 0.00102659
Iteration 5/25 | Loss: 0.00088269
Iteration 6/25 | Loss: 0.00085021
Iteration 7/25 | Loss: 0.00086102
Iteration 8/25 | Loss: 0.00080819
Iteration 9/25 | Loss: 0.00080646
Iteration 10/25 | Loss: 0.00078178
Iteration 11/25 | Loss: 0.00076594
Iteration 12/25 | Loss: 0.00076665
Iteration 13/25 | Loss: 0.00075655
Iteration 14/25 | Loss: 0.00075122
Iteration 15/25 | Loss: 0.00074860
Iteration 16/25 | Loss: 0.00074606
Iteration 17/25 | Loss: 0.00074532
Iteration 18/25 | Loss: 0.00074495
Iteration 19/25 | Loss: 0.00074486
Iteration 20/25 | Loss: 0.00074485
Iteration 21/25 | Loss: 0.00074485
Iteration 22/25 | Loss: 0.00074485
Iteration 23/25 | Loss: 0.00074485
Iteration 24/25 | Loss: 0.00074485
Iteration 25/25 | Loss: 0.00074485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27206957
Iteration 2/25 | Loss: 0.00077882
Iteration 3/25 | Loss: 0.00081716
Iteration 4/25 | Loss: 0.00058691
Iteration 5/25 | Loss: 0.00041668
Iteration 6/25 | Loss: 0.00041668
Iteration 7/25 | Loss: 0.00041668
Iteration 8/25 | Loss: 0.00041668
Iteration 9/25 | Loss: 0.00041668
Iteration 10/25 | Loss: 0.00041668
Iteration 11/25 | Loss: 0.00041668
Iteration 12/25 | Loss: 0.00041668
Iteration 13/25 | Loss: 0.00041668
Iteration 14/25 | Loss: 0.00041668
Iteration 15/25 | Loss: 0.00041668
Iteration 16/25 | Loss: 0.00041668
Iteration 17/25 | Loss: 0.00041668
Iteration 18/25 | Loss: 0.00041668
Iteration 19/25 | Loss: 0.00041668
Iteration 20/25 | Loss: 0.00041668
Iteration 21/25 | Loss: 0.00041668
Iteration 22/25 | Loss: 0.00041668
Iteration 23/25 | Loss: 0.00041668
Iteration 24/25 | Loss: 0.00041668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004166766884736717, 0.0004166766884736717, 0.0004166766884736717, 0.0004166766884736717, 0.0004166766884736717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004166766884736717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041668
Iteration 2/1000 | Loss: 0.00035550
Iteration 3/1000 | Loss: 0.00013035
Iteration 4/1000 | Loss: 0.00008863
Iteration 5/1000 | Loss: 0.00019281
Iteration 6/1000 | Loss: 0.00013140
Iteration 7/1000 | Loss: 0.00033661
Iteration 8/1000 | Loss: 0.00010802
Iteration 9/1000 | Loss: 0.00004211
Iteration 10/1000 | Loss: 0.00003797
Iteration 11/1000 | Loss: 0.00002794
Iteration 12/1000 | Loss: 0.00006249
Iteration 13/1000 | Loss: 0.00016458
Iteration 14/1000 | Loss: 0.00002712
Iteration 15/1000 | Loss: 0.00002663
Iteration 16/1000 | Loss: 0.00012205
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00009734
Iteration 19/1000 | Loss: 0.00002599
Iteration 20/1000 | Loss: 0.00009200
Iteration 21/1000 | Loss: 0.00002601
Iteration 22/1000 | Loss: 0.00002579
Iteration 23/1000 | Loss: 0.00002576
Iteration 24/1000 | Loss: 0.00002564
Iteration 25/1000 | Loss: 0.00004244
Iteration 26/1000 | Loss: 0.00003831
Iteration 27/1000 | Loss: 0.00008912
Iteration 28/1000 | Loss: 0.00003040
Iteration 29/1000 | Loss: 0.00002599
Iteration 30/1000 | Loss: 0.00006416
Iteration 31/1000 | Loss: 0.00009362
Iteration 32/1000 | Loss: 0.00005485
Iteration 33/1000 | Loss: 0.00003393
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00004090
Iteration 36/1000 | Loss: 0.00008092
Iteration 37/1000 | Loss: 0.00004197
Iteration 38/1000 | Loss: 0.00002640
Iteration 39/1000 | Loss: 0.00007916
Iteration 40/1000 | Loss: 0.00007266
Iteration 41/1000 | Loss: 0.00003318
Iteration 42/1000 | Loss: 0.00002549
Iteration 43/1000 | Loss: 0.00003098
Iteration 44/1000 | Loss: 0.00002529
Iteration 45/1000 | Loss: 0.00007405
Iteration 46/1000 | Loss: 0.00002604
Iteration 47/1000 | Loss: 0.00010098
Iteration 48/1000 | Loss: 0.00002911
Iteration 49/1000 | Loss: 0.00002524
Iteration 50/1000 | Loss: 0.00002977
Iteration 51/1000 | Loss: 0.00002597
Iteration 52/1000 | Loss: 0.00002778
Iteration 53/1000 | Loss: 0.00002537
Iteration 54/1000 | Loss: 0.00002738
Iteration 55/1000 | Loss: 0.00002521
Iteration 56/1000 | Loss: 0.00002498
Iteration 57/1000 | Loss: 0.00002498
Iteration 58/1000 | Loss: 0.00002497
Iteration 59/1000 | Loss: 0.00002497
Iteration 60/1000 | Loss: 0.00002497
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002497
Iteration 63/1000 | Loss: 0.00002497
Iteration 64/1000 | Loss: 0.00002497
Iteration 65/1000 | Loss: 0.00002497
Iteration 66/1000 | Loss: 0.00002497
Iteration 67/1000 | Loss: 0.00002497
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002495
Iteration 70/1000 | Loss: 0.00002942
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002494
Iteration 73/1000 | Loss: 0.00002494
Iteration 74/1000 | Loss: 0.00002493
Iteration 75/1000 | Loss: 0.00002493
Iteration 76/1000 | Loss: 0.00002493
Iteration 77/1000 | Loss: 0.00002493
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00002493
Iteration 80/1000 | Loss: 0.00002493
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002493
Iteration 83/1000 | Loss: 0.00002493
Iteration 84/1000 | Loss: 0.00002493
Iteration 85/1000 | Loss: 0.00002493
Iteration 86/1000 | Loss: 0.00002493
Iteration 87/1000 | Loss: 0.00002493
Iteration 88/1000 | Loss: 0.00002493
Iteration 89/1000 | Loss: 0.00002493
Iteration 90/1000 | Loss: 0.00002493
Iteration 91/1000 | Loss: 0.00002493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.492636485840194e-05, 2.492636485840194e-05, 2.492636485840194e-05, 2.492636485840194e-05, 2.492636485840194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.492636485840194e-05

Optimization complete. Final v2v error: 4.244865894317627 mm

Highest mean error: 5.570217132568359 mm for frame 42

Lowest mean error: 3.7608587741851807 mm for frame 0

Saving results

Total time: 128.06351947784424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_26_nl_4009/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_26_nl_4009/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444309
Iteration 2/25 | Loss: 0.00078972
Iteration 3/25 | Loss: 0.00066263
Iteration 4/25 | Loss: 0.00064003
Iteration 5/25 | Loss: 0.00063267
Iteration 6/25 | Loss: 0.00063054
Iteration 7/25 | Loss: 0.00063023
Iteration 8/25 | Loss: 0.00063023
Iteration 9/25 | Loss: 0.00063023
Iteration 10/25 | Loss: 0.00063023
Iteration 11/25 | Loss: 0.00063023
Iteration 12/25 | Loss: 0.00063023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006302255787886679, 0.0006302255787886679, 0.0006302255787886679, 0.0006302255787886679, 0.0006302255787886679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006302255787886679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30817652
Iteration 2/25 | Loss: 0.00036256
Iteration 3/25 | Loss: 0.00036256
Iteration 4/25 | Loss: 0.00036256
Iteration 5/25 | Loss: 0.00036256
Iteration 6/25 | Loss: 0.00036256
Iteration 7/25 | Loss: 0.00036256
Iteration 8/25 | Loss: 0.00036256
Iteration 9/25 | Loss: 0.00036256
Iteration 10/25 | Loss: 0.00036256
Iteration 11/25 | Loss: 0.00036256
Iteration 12/25 | Loss: 0.00036256
Iteration 13/25 | Loss: 0.00036256
Iteration 14/25 | Loss: 0.00036256
Iteration 15/25 | Loss: 0.00036256
Iteration 16/25 | Loss: 0.00036256
Iteration 17/25 | Loss: 0.00036256
Iteration 18/25 | Loss: 0.00036256
Iteration 19/25 | Loss: 0.00036256
Iteration 20/25 | Loss: 0.00036256
Iteration 21/25 | Loss: 0.00036256
Iteration 22/25 | Loss: 0.00036256
Iteration 23/25 | Loss: 0.00036256
Iteration 24/25 | Loss: 0.00036256
Iteration 25/25 | Loss: 0.00036256

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036256
Iteration 2/1000 | Loss: 0.00002700
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001502
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001466
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001442
Iteration 12/1000 | Loss: 0.00001441
Iteration 13/1000 | Loss: 0.00001438
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001422
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001410
Iteration 34/1000 | Loss: 0.00001410
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001410
Iteration 38/1000 | Loss: 0.00001410
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001407
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001407
Iteration 51/1000 | Loss: 0.00001407
Iteration 52/1000 | Loss: 0.00001407
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001406
Iteration 55/1000 | Loss: 0.00001406
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001405
Iteration 59/1000 | Loss: 0.00001405
Iteration 60/1000 | Loss: 0.00001404
Iteration 61/1000 | Loss: 0.00001404
Iteration 62/1000 | Loss: 0.00001404
Iteration 63/1000 | Loss: 0.00001404
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001403
Iteration 70/1000 | Loss: 0.00001403
Iteration 71/1000 | Loss: 0.00001403
Iteration 72/1000 | Loss: 0.00001403
Iteration 73/1000 | Loss: 0.00001403
Iteration 74/1000 | Loss: 0.00001403
Iteration 75/1000 | Loss: 0.00001403
Iteration 76/1000 | Loss: 0.00001402
Iteration 77/1000 | Loss: 0.00001402
Iteration 78/1000 | Loss: 0.00001402
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001401
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001400
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001400
Iteration 93/1000 | Loss: 0.00001400
Iteration 94/1000 | Loss: 0.00001400
Iteration 95/1000 | Loss: 0.00001400
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001400
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001399
Iteration 104/1000 | Loss: 0.00001399
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001397
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001396
Iteration 115/1000 | Loss: 0.00001396
Iteration 116/1000 | Loss: 0.00001396
Iteration 117/1000 | Loss: 0.00001396
Iteration 118/1000 | Loss: 0.00001396
Iteration 119/1000 | Loss: 0.00001395
Iteration 120/1000 | Loss: 0.00001395
Iteration 121/1000 | Loss: 0.00001395
Iteration 122/1000 | Loss: 0.00001395
Iteration 123/1000 | Loss: 0.00001394
Iteration 124/1000 | Loss: 0.00001394
Iteration 125/1000 | Loss: 0.00001394
Iteration 126/1000 | Loss: 0.00001394
Iteration 127/1000 | Loss: 0.00001394
Iteration 128/1000 | Loss: 0.00001394
Iteration 129/1000 | Loss: 0.00001394
Iteration 130/1000 | Loss: 0.00001394
Iteration 131/1000 | Loss: 0.00001394
Iteration 132/1000 | Loss: 0.00001394
Iteration 133/1000 | Loss: 0.00001394
Iteration 134/1000 | Loss: 0.00001393
Iteration 135/1000 | Loss: 0.00001393
Iteration 136/1000 | Loss: 0.00001392
Iteration 137/1000 | Loss: 0.00001392
Iteration 138/1000 | Loss: 0.00001392
Iteration 139/1000 | Loss: 0.00001392
Iteration 140/1000 | Loss: 0.00001392
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001392
Iteration 143/1000 | Loss: 0.00001392
Iteration 144/1000 | Loss: 0.00001392
Iteration 145/1000 | Loss: 0.00001392
Iteration 146/1000 | Loss: 0.00001392
Iteration 147/1000 | Loss: 0.00001392
Iteration 148/1000 | Loss: 0.00001391
Iteration 149/1000 | Loss: 0.00001391
Iteration 150/1000 | Loss: 0.00001391
Iteration 151/1000 | Loss: 0.00001391
Iteration 152/1000 | Loss: 0.00001391
Iteration 153/1000 | Loss: 0.00001391
Iteration 154/1000 | Loss: 0.00001391
Iteration 155/1000 | Loss: 0.00001391
Iteration 156/1000 | Loss: 0.00001391
Iteration 157/1000 | Loss: 0.00001391
Iteration 158/1000 | Loss: 0.00001391
Iteration 159/1000 | Loss: 0.00001391
Iteration 160/1000 | Loss: 0.00001391
Iteration 161/1000 | Loss: 0.00001390
Iteration 162/1000 | Loss: 0.00001390
Iteration 163/1000 | Loss: 0.00001390
Iteration 164/1000 | Loss: 0.00001390
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001388
Iteration 169/1000 | Loss: 0.00001388
Iteration 170/1000 | Loss: 0.00001388
Iteration 171/1000 | Loss: 0.00001388
Iteration 172/1000 | Loss: 0.00001388
Iteration 173/1000 | Loss: 0.00001388
Iteration 174/1000 | Loss: 0.00001388
Iteration 175/1000 | Loss: 0.00001388
Iteration 176/1000 | Loss: 0.00001387
Iteration 177/1000 | Loss: 0.00001387
Iteration 178/1000 | Loss: 0.00001387
Iteration 179/1000 | Loss: 0.00001387
Iteration 180/1000 | Loss: 0.00001387
Iteration 181/1000 | Loss: 0.00001387
Iteration 182/1000 | Loss: 0.00001387
Iteration 183/1000 | Loss: 0.00001387
Iteration 184/1000 | Loss: 0.00001387
Iteration 185/1000 | Loss: 0.00001387
Iteration 186/1000 | Loss: 0.00001387
Iteration 187/1000 | Loss: 0.00001387
Iteration 188/1000 | Loss: 0.00001387
Iteration 189/1000 | Loss: 0.00001387
Iteration 190/1000 | Loss: 0.00001387
Iteration 191/1000 | Loss: 0.00001387
Iteration 192/1000 | Loss: 0.00001387
Iteration 193/1000 | Loss: 0.00001387
Iteration 194/1000 | Loss: 0.00001387
Iteration 195/1000 | Loss: 0.00001387
Iteration 196/1000 | Loss: 0.00001387
Iteration 197/1000 | Loss: 0.00001387
Iteration 198/1000 | Loss: 0.00001387
Iteration 199/1000 | Loss: 0.00001387
Iteration 200/1000 | Loss: 0.00001387
Iteration 201/1000 | Loss: 0.00001387
Iteration 202/1000 | Loss: 0.00001387
Iteration 203/1000 | Loss: 0.00001387
Iteration 204/1000 | Loss: 0.00001387
Iteration 205/1000 | Loss: 0.00001387
Iteration 206/1000 | Loss: 0.00001387
Iteration 207/1000 | Loss: 0.00001387
Iteration 208/1000 | Loss: 0.00001387
Iteration 209/1000 | Loss: 0.00001387
Iteration 210/1000 | Loss: 0.00001387
Iteration 211/1000 | Loss: 0.00001387
Iteration 212/1000 | Loss: 0.00001387
Iteration 213/1000 | Loss: 0.00001387
Iteration 214/1000 | Loss: 0.00001387
Iteration 215/1000 | Loss: 0.00001387
Iteration 216/1000 | Loss: 0.00001387
Iteration 217/1000 | Loss: 0.00001387
Iteration 218/1000 | Loss: 0.00001387
Iteration 219/1000 | Loss: 0.00001387
Iteration 220/1000 | Loss: 0.00001387
Iteration 221/1000 | Loss: 0.00001387
Iteration 222/1000 | Loss: 0.00001387
Iteration 223/1000 | Loss: 0.00001387
Iteration 224/1000 | Loss: 0.00001387
Iteration 225/1000 | Loss: 0.00001387
Iteration 226/1000 | Loss: 0.00001387
Iteration 227/1000 | Loss: 0.00001387
Iteration 228/1000 | Loss: 0.00001387
Iteration 229/1000 | Loss: 0.00001387
Iteration 230/1000 | Loss: 0.00001387
Iteration 231/1000 | Loss: 0.00001387
Iteration 232/1000 | Loss: 0.00001387
Iteration 233/1000 | Loss: 0.00001387
Iteration 234/1000 | Loss: 0.00001387
Iteration 235/1000 | Loss: 0.00001387
Iteration 236/1000 | Loss: 0.00001387
Iteration 237/1000 | Loss: 0.00001387
Iteration 238/1000 | Loss: 0.00001387
Iteration 239/1000 | Loss: 0.00001387
Iteration 240/1000 | Loss: 0.00001387
Iteration 241/1000 | Loss: 0.00001387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.3866907465853728e-05, 1.3866907465853728e-05, 1.3866907465853728e-05, 1.3866907465853728e-05, 1.3866907465853728e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3866907465853728e-05

Optimization complete. Final v2v error: 3.128842830657959 mm

Highest mean error: 3.584192991256714 mm for frame 7

Lowest mean error: 2.241072416305542 mm for frame 147

Saving results

Total time: 38.075562477111816
