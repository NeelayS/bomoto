Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=241, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13496-13551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540652
Iteration 2/25 | Loss: 0.00115594
Iteration 3/25 | Loss: 0.00092730
Iteration 4/25 | Loss: 0.00091076
Iteration 5/25 | Loss: 0.00090852
Iteration 6/25 | Loss: 0.00090816
Iteration 7/25 | Loss: 0.00090816
Iteration 8/25 | Loss: 0.00090816
Iteration 9/25 | Loss: 0.00090816
Iteration 10/25 | Loss: 0.00090816
Iteration 11/25 | Loss: 0.00090816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009081626194529235, 0.0009081626194529235, 0.0009081626194529235, 0.0009081626194529235, 0.0009081626194529235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009081626194529235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40592182
Iteration 2/25 | Loss: 0.00048196
Iteration 3/25 | Loss: 0.00048191
Iteration 4/25 | Loss: 0.00048191
Iteration 5/25 | Loss: 0.00048191
Iteration 6/25 | Loss: 0.00048191
Iteration 7/25 | Loss: 0.00048191
Iteration 8/25 | Loss: 0.00048191
Iteration 9/25 | Loss: 0.00048191
Iteration 10/25 | Loss: 0.00048191
Iteration 11/25 | Loss: 0.00048191
Iteration 12/25 | Loss: 0.00048191
Iteration 13/25 | Loss: 0.00048191
Iteration 14/25 | Loss: 0.00048191
Iteration 15/25 | Loss: 0.00048191
Iteration 16/25 | Loss: 0.00048191
Iteration 17/25 | Loss: 0.00048191
Iteration 18/25 | Loss: 0.00048191
Iteration 19/25 | Loss: 0.00048191
Iteration 20/25 | Loss: 0.00048191
Iteration 21/25 | Loss: 0.00048191
Iteration 22/25 | Loss: 0.00048191
Iteration 23/25 | Loss: 0.00048191
Iteration 24/25 | Loss: 0.00048191
Iteration 25/25 | Loss: 0.00048191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048191
Iteration 2/1000 | Loss: 0.00004768
Iteration 3/1000 | Loss: 0.00003030
Iteration 4/1000 | Loss: 0.00002580
Iteration 5/1000 | Loss: 0.00002403
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002179
Iteration 8/1000 | Loss: 0.00002100
Iteration 9/1000 | Loss: 0.00002055
Iteration 10/1000 | Loss: 0.00002024
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001978
Iteration 13/1000 | Loss: 0.00001973
Iteration 14/1000 | Loss: 0.00001969
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001950
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001943
Iteration 19/1000 | Loss: 0.00001942
Iteration 20/1000 | Loss: 0.00001940
Iteration 21/1000 | Loss: 0.00001940
Iteration 22/1000 | Loss: 0.00001939
Iteration 23/1000 | Loss: 0.00001937
Iteration 24/1000 | Loss: 0.00001936
Iteration 25/1000 | Loss: 0.00001936
Iteration 26/1000 | Loss: 0.00001935
Iteration 27/1000 | Loss: 0.00001935
Iteration 28/1000 | Loss: 0.00001934
Iteration 29/1000 | Loss: 0.00001934
Iteration 30/1000 | Loss: 0.00001934
Iteration 31/1000 | Loss: 0.00001933
Iteration 32/1000 | Loss: 0.00001933
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001932
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001932
Iteration 38/1000 | Loss: 0.00001932
Iteration 39/1000 | Loss: 0.00001932
Iteration 40/1000 | Loss: 0.00001932
Iteration 41/1000 | Loss: 0.00001931
Iteration 42/1000 | Loss: 0.00001931
Iteration 43/1000 | Loss: 0.00001931
Iteration 44/1000 | Loss: 0.00001931
Iteration 45/1000 | Loss: 0.00001931
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001931
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001930
Iteration 51/1000 | Loss: 0.00001930
Iteration 52/1000 | Loss: 0.00001930
Iteration 53/1000 | Loss: 0.00001930
Iteration 54/1000 | Loss: 0.00001930
Iteration 55/1000 | Loss: 0.00001930
Iteration 56/1000 | Loss: 0.00001930
Iteration 57/1000 | Loss: 0.00001929
Iteration 58/1000 | Loss: 0.00001929
Iteration 59/1000 | Loss: 0.00001929
Iteration 60/1000 | Loss: 0.00001929
Iteration 61/1000 | Loss: 0.00001929
Iteration 62/1000 | Loss: 0.00001929
Iteration 63/1000 | Loss: 0.00001929
Iteration 64/1000 | Loss: 0.00001928
Iteration 65/1000 | Loss: 0.00001928
Iteration 66/1000 | Loss: 0.00001928
Iteration 67/1000 | Loss: 0.00001928
Iteration 68/1000 | Loss: 0.00001928
Iteration 69/1000 | Loss: 0.00001927
Iteration 70/1000 | Loss: 0.00001927
Iteration 71/1000 | Loss: 0.00001927
Iteration 72/1000 | Loss: 0.00001927
Iteration 73/1000 | Loss: 0.00001927
Iteration 74/1000 | Loss: 0.00001927
Iteration 75/1000 | Loss: 0.00001926
Iteration 76/1000 | Loss: 0.00001926
Iteration 77/1000 | Loss: 0.00001926
Iteration 78/1000 | Loss: 0.00001926
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00001925
Iteration 81/1000 | Loss: 0.00001925
Iteration 82/1000 | Loss: 0.00001925
Iteration 83/1000 | Loss: 0.00001925
Iteration 84/1000 | Loss: 0.00001925
Iteration 85/1000 | Loss: 0.00001925
Iteration 86/1000 | Loss: 0.00001925
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001924
Iteration 90/1000 | Loss: 0.00001924
Iteration 91/1000 | Loss: 0.00001924
Iteration 92/1000 | Loss: 0.00001924
Iteration 93/1000 | Loss: 0.00001924
Iteration 94/1000 | Loss: 0.00001924
Iteration 95/1000 | Loss: 0.00001924
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001924
Iteration 98/1000 | Loss: 0.00001924
Iteration 99/1000 | Loss: 0.00001924
Iteration 100/1000 | Loss: 0.00001924
Iteration 101/1000 | Loss: 0.00001924
Iteration 102/1000 | Loss: 0.00001924
Iteration 103/1000 | Loss: 0.00001924
Iteration 104/1000 | Loss: 0.00001924
Iteration 105/1000 | Loss: 0.00001924
Iteration 106/1000 | Loss: 0.00001924
Iteration 107/1000 | Loss: 0.00001923
Iteration 108/1000 | Loss: 0.00001923
Iteration 109/1000 | Loss: 0.00001923
Iteration 110/1000 | Loss: 0.00001923
Iteration 111/1000 | Loss: 0.00001923
Iteration 112/1000 | Loss: 0.00001923
Iteration 113/1000 | Loss: 0.00001923
Iteration 114/1000 | Loss: 0.00001923
Iteration 115/1000 | Loss: 0.00001923
Iteration 116/1000 | Loss: 0.00001923
Iteration 117/1000 | Loss: 0.00001923
Iteration 118/1000 | Loss: 0.00001923
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9228124074288644e-05, 1.9228124074288644e-05, 1.9228124074288644e-05, 1.9228124074288644e-05, 1.9228124074288644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9228124074288644e-05

Optimization complete. Final v2v error: 3.6042442321777344 mm

Highest mean error: 4.178438663482666 mm for frame 142

Lowest mean error: 3.005394697189331 mm for frame 19

Saving results

Total time: 38.99209713935852
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00940862
Iteration 2/25 | Loss: 0.00104805
Iteration 3/25 | Loss: 0.00085129
Iteration 4/25 | Loss: 0.00081326
Iteration 5/25 | Loss: 0.00079593
Iteration 6/25 | Loss: 0.00079162
Iteration 7/25 | Loss: 0.00079051
Iteration 8/25 | Loss: 0.00079040
Iteration 9/25 | Loss: 0.00079040
Iteration 10/25 | Loss: 0.00079040
Iteration 11/25 | Loss: 0.00079040
Iteration 12/25 | Loss: 0.00079040
Iteration 13/25 | Loss: 0.00079040
Iteration 14/25 | Loss: 0.00079040
Iteration 15/25 | Loss: 0.00079040
Iteration 16/25 | Loss: 0.00079040
Iteration 17/25 | Loss: 0.00079040
Iteration 18/25 | Loss: 0.00079040
Iteration 19/25 | Loss: 0.00079040
Iteration 20/25 | Loss: 0.00079040
Iteration 21/25 | Loss: 0.00079040
Iteration 22/25 | Loss: 0.00079040
Iteration 23/25 | Loss: 0.00079040
Iteration 24/25 | Loss: 0.00079040
Iteration 25/25 | Loss: 0.00079040

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44907510
Iteration 2/25 | Loss: 0.00029561
Iteration 3/25 | Loss: 0.00029559
Iteration 4/25 | Loss: 0.00029559
Iteration 5/25 | Loss: 0.00029559
Iteration 6/25 | Loss: 0.00029559
Iteration 7/25 | Loss: 0.00029559
Iteration 8/25 | Loss: 0.00029559
Iteration 9/25 | Loss: 0.00029559
Iteration 10/25 | Loss: 0.00029559
Iteration 11/25 | Loss: 0.00029559
Iteration 12/25 | Loss: 0.00029559
Iteration 13/25 | Loss: 0.00029559
Iteration 14/25 | Loss: 0.00029559
Iteration 15/25 | Loss: 0.00029559
Iteration 16/25 | Loss: 0.00029559
Iteration 17/25 | Loss: 0.00029559
Iteration 18/25 | Loss: 0.00029559
Iteration 19/25 | Loss: 0.00029559
Iteration 20/25 | Loss: 0.00029559
Iteration 21/25 | Loss: 0.00029559
Iteration 22/25 | Loss: 0.00029559
Iteration 23/25 | Loss: 0.00029559
Iteration 24/25 | Loss: 0.00029559
Iteration 25/25 | Loss: 0.00029559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029559
Iteration 2/1000 | Loss: 0.00004831
Iteration 3/1000 | Loss: 0.00003524
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00003085
Iteration 6/1000 | Loss: 0.00002947
Iteration 7/1000 | Loss: 0.00002858
Iteration 8/1000 | Loss: 0.00002784
Iteration 9/1000 | Loss: 0.00002731
Iteration 10/1000 | Loss: 0.00002701
Iteration 11/1000 | Loss: 0.00002680
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002642
Iteration 14/1000 | Loss: 0.00002636
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002628
Iteration 18/1000 | Loss: 0.00002628
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00002624
Iteration 21/1000 | Loss: 0.00002623
Iteration 22/1000 | Loss: 0.00002622
Iteration 23/1000 | Loss: 0.00002622
Iteration 24/1000 | Loss: 0.00002621
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002619
Iteration 27/1000 | Loss: 0.00002619
Iteration 28/1000 | Loss: 0.00002619
Iteration 29/1000 | Loss: 0.00002618
Iteration 30/1000 | Loss: 0.00002618
Iteration 31/1000 | Loss: 0.00002618
Iteration 32/1000 | Loss: 0.00002617
Iteration 33/1000 | Loss: 0.00002617
Iteration 34/1000 | Loss: 0.00002616
Iteration 35/1000 | Loss: 0.00002616
Iteration 36/1000 | Loss: 0.00002616
Iteration 37/1000 | Loss: 0.00002615
Iteration 38/1000 | Loss: 0.00002615
Iteration 39/1000 | Loss: 0.00002615
Iteration 40/1000 | Loss: 0.00002615
Iteration 41/1000 | Loss: 0.00002614
Iteration 42/1000 | Loss: 0.00002614
Iteration 43/1000 | Loss: 0.00002614
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002613
Iteration 47/1000 | Loss: 0.00002612
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002611
Iteration 50/1000 | Loss: 0.00002611
Iteration 51/1000 | Loss: 0.00002610
Iteration 52/1000 | Loss: 0.00002610
Iteration 53/1000 | Loss: 0.00002609
Iteration 54/1000 | Loss: 0.00002609
Iteration 55/1000 | Loss: 0.00002608
Iteration 56/1000 | Loss: 0.00002607
Iteration 57/1000 | Loss: 0.00002607
Iteration 58/1000 | Loss: 0.00002606
Iteration 59/1000 | Loss: 0.00002606
Iteration 60/1000 | Loss: 0.00002605
Iteration 61/1000 | Loss: 0.00002605
Iteration 62/1000 | Loss: 0.00002604
Iteration 63/1000 | Loss: 0.00002604
Iteration 64/1000 | Loss: 0.00002604
Iteration 65/1000 | Loss: 0.00002603
Iteration 66/1000 | Loss: 0.00002603
Iteration 67/1000 | Loss: 0.00002602
Iteration 68/1000 | Loss: 0.00002602
Iteration 69/1000 | Loss: 0.00002601
Iteration 70/1000 | Loss: 0.00002601
Iteration 71/1000 | Loss: 0.00002601
Iteration 72/1000 | Loss: 0.00002600
Iteration 73/1000 | Loss: 0.00002600
Iteration 74/1000 | Loss: 0.00002600
Iteration 75/1000 | Loss: 0.00002599
Iteration 76/1000 | Loss: 0.00002599
Iteration 77/1000 | Loss: 0.00002599
Iteration 78/1000 | Loss: 0.00002598
Iteration 79/1000 | Loss: 0.00002598
Iteration 80/1000 | Loss: 0.00002598
Iteration 81/1000 | Loss: 0.00002597
Iteration 82/1000 | Loss: 0.00002597
Iteration 83/1000 | Loss: 0.00002597
Iteration 84/1000 | Loss: 0.00002597
Iteration 85/1000 | Loss: 0.00002597
Iteration 86/1000 | Loss: 0.00002597
Iteration 87/1000 | Loss: 0.00002596
Iteration 88/1000 | Loss: 0.00002596
Iteration 89/1000 | Loss: 0.00002596
Iteration 90/1000 | Loss: 0.00002595
Iteration 91/1000 | Loss: 0.00002595
Iteration 92/1000 | Loss: 0.00002595
Iteration 93/1000 | Loss: 0.00002595
Iteration 94/1000 | Loss: 0.00002594
Iteration 95/1000 | Loss: 0.00002594
Iteration 96/1000 | Loss: 0.00002594
Iteration 97/1000 | Loss: 0.00002594
Iteration 98/1000 | Loss: 0.00002593
Iteration 99/1000 | Loss: 0.00002593
Iteration 100/1000 | Loss: 0.00002593
Iteration 101/1000 | Loss: 0.00002593
Iteration 102/1000 | Loss: 0.00002593
Iteration 103/1000 | Loss: 0.00002593
Iteration 104/1000 | Loss: 0.00002593
Iteration 105/1000 | Loss: 0.00002593
Iteration 106/1000 | Loss: 0.00002593
Iteration 107/1000 | Loss: 0.00002592
Iteration 108/1000 | Loss: 0.00002592
Iteration 109/1000 | Loss: 0.00002592
Iteration 110/1000 | Loss: 0.00002592
Iteration 111/1000 | Loss: 0.00002592
Iteration 112/1000 | Loss: 0.00002592
Iteration 113/1000 | Loss: 0.00002592
Iteration 114/1000 | Loss: 0.00002592
Iteration 115/1000 | Loss: 0.00002592
Iteration 116/1000 | Loss: 0.00002592
Iteration 117/1000 | Loss: 0.00002592
Iteration 118/1000 | Loss: 0.00002592
Iteration 119/1000 | Loss: 0.00002591
Iteration 120/1000 | Loss: 0.00002591
Iteration 121/1000 | Loss: 0.00002591
Iteration 122/1000 | Loss: 0.00002591
Iteration 123/1000 | Loss: 0.00002591
Iteration 124/1000 | Loss: 0.00002591
Iteration 125/1000 | Loss: 0.00002591
Iteration 126/1000 | Loss: 0.00002591
Iteration 127/1000 | Loss: 0.00002591
Iteration 128/1000 | Loss: 0.00002590
Iteration 129/1000 | Loss: 0.00002590
Iteration 130/1000 | Loss: 0.00002590
Iteration 131/1000 | Loss: 0.00002590
Iteration 132/1000 | Loss: 0.00002590
Iteration 133/1000 | Loss: 0.00002590
Iteration 134/1000 | Loss: 0.00002590
Iteration 135/1000 | Loss: 0.00002590
Iteration 136/1000 | Loss: 0.00002590
Iteration 137/1000 | Loss: 0.00002590
Iteration 138/1000 | Loss: 0.00002590
Iteration 139/1000 | Loss: 0.00002589
Iteration 140/1000 | Loss: 0.00002589
Iteration 141/1000 | Loss: 0.00002589
Iteration 142/1000 | Loss: 0.00002589
Iteration 143/1000 | Loss: 0.00002588
Iteration 144/1000 | Loss: 0.00002588
Iteration 145/1000 | Loss: 0.00002588
Iteration 146/1000 | Loss: 0.00002588
Iteration 147/1000 | Loss: 0.00002588
Iteration 148/1000 | Loss: 0.00002588
Iteration 149/1000 | Loss: 0.00002588
Iteration 150/1000 | Loss: 0.00002588
Iteration 151/1000 | Loss: 0.00002587
Iteration 152/1000 | Loss: 0.00002587
Iteration 153/1000 | Loss: 0.00002587
Iteration 154/1000 | Loss: 0.00002587
Iteration 155/1000 | Loss: 0.00002587
Iteration 156/1000 | Loss: 0.00002587
Iteration 157/1000 | Loss: 0.00002586
Iteration 158/1000 | Loss: 0.00002586
Iteration 159/1000 | Loss: 0.00002586
Iteration 160/1000 | Loss: 0.00002586
Iteration 161/1000 | Loss: 0.00002586
Iteration 162/1000 | Loss: 0.00002586
Iteration 163/1000 | Loss: 0.00002586
Iteration 164/1000 | Loss: 0.00002586
Iteration 165/1000 | Loss: 0.00002586
Iteration 166/1000 | Loss: 0.00002586
Iteration 167/1000 | Loss: 0.00002586
Iteration 168/1000 | Loss: 0.00002586
Iteration 169/1000 | Loss: 0.00002586
Iteration 170/1000 | Loss: 0.00002586
Iteration 171/1000 | Loss: 0.00002586
Iteration 172/1000 | Loss: 0.00002586
Iteration 173/1000 | Loss: 0.00002586
Iteration 174/1000 | Loss: 0.00002586
Iteration 175/1000 | Loss: 0.00002586
Iteration 176/1000 | Loss: 0.00002586
Iteration 177/1000 | Loss: 0.00002586
Iteration 178/1000 | Loss: 0.00002586
Iteration 179/1000 | Loss: 0.00002586
Iteration 180/1000 | Loss: 0.00002586
Iteration 181/1000 | Loss: 0.00002586
Iteration 182/1000 | Loss: 0.00002586
Iteration 183/1000 | Loss: 0.00002585
Iteration 184/1000 | Loss: 0.00002585
Iteration 185/1000 | Loss: 0.00002585
Iteration 186/1000 | Loss: 0.00002585
Iteration 187/1000 | Loss: 0.00002585
Iteration 188/1000 | Loss: 0.00002585
Iteration 189/1000 | Loss: 0.00002585
Iteration 190/1000 | Loss: 0.00002585
Iteration 191/1000 | Loss: 0.00002585
Iteration 192/1000 | Loss: 0.00002585
Iteration 193/1000 | Loss: 0.00002585
Iteration 194/1000 | Loss: 0.00002585
Iteration 195/1000 | Loss: 0.00002585
Iteration 196/1000 | Loss: 0.00002585
Iteration 197/1000 | Loss: 0.00002585
Iteration 198/1000 | Loss: 0.00002585
Iteration 199/1000 | Loss: 0.00002585
Iteration 200/1000 | Loss: 0.00002585
Iteration 201/1000 | Loss: 0.00002585
Iteration 202/1000 | Loss: 0.00002585
Iteration 203/1000 | Loss: 0.00002585
Iteration 204/1000 | Loss: 0.00002585
Iteration 205/1000 | Loss: 0.00002585
Iteration 206/1000 | Loss: 0.00002585
Iteration 207/1000 | Loss: 0.00002585
Iteration 208/1000 | Loss: 0.00002585
Iteration 209/1000 | Loss: 0.00002585
Iteration 210/1000 | Loss: 0.00002585
Iteration 211/1000 | Loss: 0.00002585
Iteration 212/1000 | Loss: 0.00002585
Iteration 213/1000 | Loss: 0.00002585
Iteration 214/1000 | Loss: 0.00002585
Iteration 215/1000 | Loss: 0.00002585
Iteration 216/1000 | Loss: 0.00002585
Iteration 217/1000 | Loss: 0.00002585
Iteration 218/1000 | Loss: 0.00002585
Iteration 219/1000 | Loss: 0.00002585
Iteration 220/1000 | Loss: 0.00002585
Iteration 221/1000 | Loss: 0.00002585
Iteration 222/1000 | Loss: 0.00002585
Iteration 223/1000 | Loss: 0.00002585
Iteration 224/1000 | Loss: 0.00002585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.585421498224605e-05, 2.585421498224605e-05, 2.585421498224605e-05, 2.585421498224605e-05, 2.585421498224605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.585421498224605e-05

Optimization complete. Final v2v error: 4.168144702911377 mm

Highest mean error: 6.314440727233887 mm for frame 70

Lowest mean error: 3.5564322471618652 mm for frame 95

Saving results

Total time: 45.150742053985596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830382
Iteration 2/25 | Loss: 0.00129713
Iteration 3/25 | Loss: 0.00087874
Iteration 4/25 | Loss: 0.00081494
Iteration 5/25 | Loss: 0.00079455
Iteration 6/25 | Loss: 0.00078877
Iteration 7/25 | Loss: 0.00078717
Iteration 8/25 | Loss: 0.00078702
Iteration 9/25 | Loss: 0.00078702
Iteration 10/25 | Loss: 0.00078702
Iteration 11/25 | Loss: 0.00078702
Iteration 12/25 | Loss: 0.00078702
Iteration 13/25 | Loss: 0.00078702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007870194385759532, 0.0007870194385759532, 0.0007870194385759532, 0.0007870194385759532, 0.0007870194385759532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007870194385759532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.27257109
Iteration 2/25 | Loss: 0.00028577
Iteration 3/25 | Loss: 0.00028572
Iteration 4/25 | Loss: 0.00028572
Iteration 5/25 | Loss: 0.00028572
Iteration 6/25 | Loss: 0.00028572
Iteration 7/25 | Loss: 0.00028572
Iteration 8/25 | Loss: 0.00028572
Iteration 9/25 | Loss: 0.00028572
Iteration 10/25 | Loss: 0.00028572
Iteration 11/25 | Loss: 0.00028572
Iteration 12/25 | Loss: 0.00028572
Iteration 13/25 | Loss: 0.00028572
Iteration 14/25 | Loss: 0.00028572
Iteration 15/25 | Loss: 0.00028572
Iteration 16/25 | Loss: 0.00028572
Iteration 17/25 | Loss: 0.00028572
Iteration 18/25 | Loss: 0.00028572
Iteration 19/25 | Loss: 0.00028572
Iteration 20/25 | Loss: 0.00028572
Iteration 21/25 | Loss: 0.00028572
Iteration 22/25 | Loss: 0.00028572
Iteration 23/25 | Loss: 0.00028572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002857208310160786, 0.0002857208310160786, 0.0002857208310160786, 0.0002857208310160786, 0.0002857208310160786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002857208310160786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028572
Iteration 2/1000 | Loss: 0.00003729
Iteration 3/1000 | Loss: 0.00002951
Iteration 4/1000 | Loss: 0.00002591
Iteration 5/1000 | Loss: 0.00002476
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002270
Iteration 9/1000 | Loss: 0.00002231
Iteration 10/1000 | Loss: 0.00002208
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002163
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002139
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002138
Iteration 17/1000 | Loss: 0.00002137
Iteration 18/1000 | Loss: 0.00002134
Iteration 19/1000 | Loss: 0.00002130
Iteration 20/1000 | Loss: 0.00002129
Iteration 21/1000 | Loss: 0.00002129
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002127
Iteration 24/1000 | Loss: 0.00002127
Iteration 25/1000 | Loss: 0.00002122
Iteration 26/1000 | Loss: 0.00002122
Iteration 27/1000 | Loss: 0.00002121
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002119
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002118
Iteration 35/1000 | Loss: 0.00002118
Iteration 36/1000 | Loss: 0.00002118
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002117
Iteration 41/1000 | Loss: 0.00002117
Iteration 42/1000 | Loss: 0.00002117
Iteration 43/1000 | Loss: 0.00002117
Iteration 44/1000 | Loss: 0.00002117
Iteration 45/1000 | Loss: 0.00002117
Iteration 46/1000 | Loss: 0.00002117
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 54. Stopping optimization.
Last 5 losses: [2.1166628357605077e-05, 2.1166628357605077e-05, 2.1166628357605077e-05, 2.1166628357605077e-05, 2.1166628357605077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1166628357605077e-05

Optimization complete. Final v2v error: 3.8274214267730713 mm

Highest mean error: 4.465084552764893 mm for frame 218

Lowest mean error: 3.0094964504241943 mm for frame 122

Saving results

Total time: 39.27837157249451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544958
Iteration 2/25 | Loss: 0.00107196
Iteration 3/25 | Loss: 0.00077472
Iteration 4/25 | Loss: 0.00072910
Iteration 5/25 | Loss: 0.00072620
Iteration 6/25 | Loss: 0.00071604
Iteration 7/25 | Loss: 0.00071916
Iteration 8/25 | Loss: 0.00070900
Iteration 9/25 | Loss: 0.00070877
Iteration 10/25 | Loss: 0.00070877
Iteration 11/25 | Loss: 0.00070877
Iteration 12/25 | Loss: 0.00070877
Iteration 13/25 | Loss: 0.00070877
Iteration 14/25 | Loss: 0.00070877
Iteration 15/25 | Loss: 0.00070877
Iteration 16/25 | Loss: 0.00070877
Iteration 17/25 | Loss: 0.00070877
Iteration 18/25 | Loss: 0.00070876
Iteration 19/25 | Loss: 0.00070876
Iteration 20/25 | Loss: 0.00070876
Iteration 21/25 | Loss: 0.00070876
Iteration 22/25 | Loss: 0.00070876
Iteration 23/25 | Loss: 0.00070876
Iteration 24/25 | Loss: 0.00070876
Iteration 25/25 | Loss: 0.00070876

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00348520
Iteration 2/25 | Loss: 0.00030882
Iteration 3/25 | Loss: 0.00026117
Iteration 4/25 | Loss: 0.00026116
Iteration 5/25 | Loss: 0.00026116
Iteration 6/25 | Loss: 0.00026116
Iteration 7/25 | Loss: 0.00026116
Iteration 8/25 | Loss: 0.00026116
Iteration 9/25 | Loss: 0.00026116
Iteration 10/25 | Loss: 0.00026116
Iteration 11/25 | Loss: 0.00026116
Iteration 12/25 | Loss: 0.00026116
Iteration 13/25 | Loss: 0.00026116
Iteration 14/25 | Loss: 0.00026116
Iteration 15/25 | Loss: 0.00026116
Iteration 16/25 | Loss: 0.00026116
Iteration 17/25 | Loss: 0.00026116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002611628733575344, 0.0002611628733575344, 0.0002611628733575344, 0.0002611628733575344, 0.0002611628733575344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002611628733575344

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026116
Iteration 2/1000 | Loss: 0.00004331
Iteration 3/1000 | Loss: 0.00004457
Iteration 4/1000 | Loss: 0.00002907
Iteration 5/1000 | Loss: 0.00003366
Iteration 6/1000 | Loss: 0.00011242
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001193
Iteration 11/1000 | Loss: 0.00004970
Iteration 12/1000 | Loss: 0.00001442
Iteration 13/1000 | Loss: 0.00001667
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00003591
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001156
Iteration 18/1000 | Loss: 0.00001155
Iteration 19/1000 | Loss: 0.00001153
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001147
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001146
Iteration 28/1000 | Loss: 0.00001146
Iteration 29/1000 | Loss: 0.00001145
Iteration 30/1000 | Loss: 0.00001145
Iteration 31/1000 | Loss: 0.00004640
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001142
Iteration 36/1000 | Loss: 0.00001142
Iteration 37/1000 | Loss: 0.00001142
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001141
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001141
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001140
Iteration 44/1000 | Loss: 0.00001140
Iteration 45/1000 | Loss: 0.00001140
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001137
Iteration 68/1000 | Loss: 0.00001137
Iteration 69/1000 | Loss: 0.00001137
Iteration 70/1000 | Loss: 0.00001137
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001136
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001133
Iteration 91/1000 | Loss: 0.00001133
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001133
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001132
Iteration 98/1000 | Loss: 0.00001132
Iteration 99/1000 | Loss: 0.00001132
Iteration 100/1000 | Loss: 0.00001131
Iteration 101/1000 | Loss: 0.00001131
Iteration 102/1000 | Loss: 0.00001131
Iteration 103/1000 | Loss: 0.00001131
Iteration 104/1000 | Loss: 0.00001130
Iteration 105/1000 | Loss: 0.00001130
Iteration 106/1000 | Loss: 0.00001130
Iteration 107/1000 | Loss: 0.00001130
Iteration 108/1000 | Loss: 0.00001130
Iteration 109/1000 | Loss: 0.00001129
Iteration 110/1000 | Loss: 0.00001129
Iteration 111/1000 | Loss: 0.00001129
Iteration 112/1000 | Loss: 0.00001129
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001128
Iteration 115/1000 | Loss: 0.00001128
Iteration 116/1000 | Loss: 0.00001128
Iteration 117/1000 | Loss: 0.00001128
Iteration 118/1000 | Loss: 0.00001128
Iteration 119/1000 | Loss: 0.00001128
Iteration 120/1000 | Loss: 0.00001128
Iteration 121/1000 | Loss: 0.00001128
Iteration 122/1000 | Loss: 0.00005844
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001126
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001124
Iteration 129/1000 | Loss: 0.00001124
Iteration 130/1000 | Loss: 0.00001124
Iteration 131/1000 | Loss: 0.00001124
Iteration 132/1000 | Loss: 0.00001124
Iteration 133/1000 | Loss: 0.00001124
Iteration 134/1000 | Loss: 0.00001124
Iteration 135/1000 | Loss: 0.00001123
Iteration 136/1000 | Loss: 0.00001123
Iteration 137/1000 | Loss: 0.00001123
Iteration 138/1000 | Loss: 0.00001123
Iteration 139/1000 | Loss: 0.00001123
Iteration 140/1000 | Loss: 0.00001123
Iteration 141/1000 | Loss: 0.00001123
Iteration 142/1000 | Loss: 0.00001123
Iteration 143/1000 | Loss: 0.00001123
Iteration 144/1000 | Loss: 0.00001123
Iteration 145/1000 | Loss: 0.00001122
Iteration 146/1000 | Loss: 0.00001122
Iteration 147/1000 | Loss: 0.00001122
Iteration 148/1000 | Loss: 0.00001122
Iteration 149/1000 | Loss: 0.00001122
Iteration 150/1000 | Loss: 0.00001122
Iteration 151/1000 | Loss: 0.00001122
Iteration 152/1000 | Loss: 0.00001122
Iteration 153/1000 | Loss: 0.00001122
Iteration 154/1000 | Loss: 0.00001122
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001122
Iteration 159/1000 | Loss: 0.00001122
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001121
Iteration 162/1000 | Loss: 0.00001121
Iteration 163/1000 | Loss: 0.00001121
Iteration 164/1000 | Loss: 0.00001121
Iteration 165/1000 | Loss: 0.00001121
Iteration 166/1000 | Loss: 0.00001121
Iteration 167/1000 | Loss: 0.00001121
Iteration 168/1000 | Loss: 0.00001121
Iteration 169/1000 | Loss: 0.00001121
Iteration 170/1000 | Loss: 0.00001121
Iteration 171/1000 | Loss: 0.00001121
Iteration 172/1000 | Loss: 0.00001121
Iteration 173/1000 | Loss: 0.00001121
Iteration 174/1000 | Loss: 0.00001121
Iteration 175/1000 | Loss: 0.00001121
Iteration 176/1000 | Loss: 0.00001121
Iteration 177/1000 | Loss: 0.00001120
Iteration 178/1000 | Loss: 0.00001120
Iteration 179/1000 | Loss: 0.00001120
Iteration 180/1000 | Loss: 0.00001120
Iteration 181/1000 | Loss: 0.00001120
Iteration 182/1000 | Loss: 0.00001120
Iteration 183/1000 | Loss: 0.00001120
Iteration 184/1000 | Loss: 0.00001120
Iteration 185/1000 | Loss: 0.00001120
Iteration 186/1000 | Loss: 0.00001120
Iteration 187/1000 | Loss: 0.00001120
Iteration 188/1000 | Loss: 0.00001120
Iteration 189/1000 | Loss: 0.00001120
Iteration 190/1000 | Loss: 0.00001120
Iteration 191/1000 | Loss: 0.00001120
Iteration 192/1000 | Loss: 0.00001120
Iteration 193/1000 | Loss: 0.00001119
Iteration 194/1000 | Loss: 0.00001119
Iteration 195/1000 | Loss: 0.00001119
Iteration 196/1000 | Loss: 0.00001119
Iteration 197/1000 | Loss: 0.00001119
Iteration 198/1000 | Loss: 0.00004772
Iteration 199/1000 | Loss: 0.00001118
Iteration 200/1000 | Loss: 0.00001118
Iteration 201/1000 | Loss: 0.00001118
Iteration 202/1000 | Loss: 0.00001118
Iteration 203/1000 | Loss: 0.00001118
Iteration 204/1000 | Loss: 0.00001118
Iteration 205/1000 | Loss: 0.00001118
Iteration 206/1000 | Loss: 0.00001118
Iteration 207/1000 | Loss: 0.00001118
Iteration 208/1000 | Loss: 0.00001118
Iteration 209/1000 | Loss: 0.00001117
Iteration 210/1000 | Loss: 0.00001117
Iteration 211/1000 | Loss: 0.00001117
Iteration 212/1000 | Loss: 0.00001117
Iteration 213/1000 | Loss: 0.00001117
Iteration 214/1000 | Loss: 0.00001117
Iteration 215/1000 | Loss: 0.00001117
Iteration 216/1000 | Loss: 0.00001117
Iteration 217/1000 | Loss: 0.00001117
Iteration 218/1000 | Loss: 0.00001117
Iteration 219/1000 | Loss: 0.00001117
Iteration 220/1000 | Loss: 0.00001117
Iteration 221/1000 | Loss: 0.00001117
Iteration 222/1000 | Loss: 0.00001117
Iteration 223/1000 | Loss: 0.00001117
Iteration 224/1000 | Loss: 0.00001117
Iteration 225/1000 | Loss: 0.00001117
Iteration 226/1000 | Loss: 0.00001116
Iteration 227/1000 | Loss: 0.00001116
Iteration 228/1000 | Loss: 0.00001116
Iteration 229/1000 | Loss: 0.00001116
Iteration 230/1000 | Loss: 0.00001116
Iteration 231/1000 | Loss: 0.00001116
Iteration 232/1000 | Loss: 0.00001116
Iteration 233/1000 | Loss: 0.00001116
Iteration 234/1000 | Loss: 0.00001116
Iteration 235/1000 | Loss: 0.00001116
Iteration 236/1000 | Loss: 0.00001116
Iteration 237/1000 | Loss: 0.00001116
Iteration 238/1000 | Loss: 0.00001116
Iteration 239/1000 | Loss: 0.00001116
Iteration 240/1000 | Loss: 0.00001116
Iteration 241/1000 | Loss: 0.00001116
Iteration 242/1000 | Loss: 0.00001116
Iteration 243/1000 | Loss: 0.00001116
Iteration 244/1000 | Loss: 0.00001116
Iteration 245/1000 | Loss: 0.00001116
Iteration 246/1000 | Loss: 0.00001116
Iteration 247/1000 | Loss: 0.00001116
Iteration 248/1000 | Loss: 0.00001116
Iteration 249/1000 | Loss: 0.00001116
Iteration 250/1000 | Loss: 0.00001116
Iteration 251/1000 | Loss: 0.00001116
Iteration 252/1000 | Loss: 0.00001116
Iteration 253/1000 | Loss: 0.00001116
Iteration 254/1000 | Loss: 0.00001116
Iteration 255/1000 | Loss: 0.00001116
Iteration 256/1000 | Loss: 0.00001116
Iteration 257/1000 | Loss: 0.00001116
Iteration 258/1000 | Loss: 0.00001116
Iteration 259/1000 | Loss: 0.00001116
Iteration 260/1000 | Loss: 0.00001116
Iteration 261/1000 | Loss: 0.00001116
Iteration 262/1000 | Loss: 0.00001116
Iteration 263/1000 | Loss: 0.00001116
Iteration 264/1000 | Loss: 0.00001116
Iteration 265/1000 | Loss: 0.00001116
Iteration 266/1000 | Loss: 0.00001116
Iteration 267/1000 | Loss: 0.00001116
Iteration 268/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [1.1164319403178524e-05, 1.1164319403178524e-05, 1.1164319403178524e-05, 1.1164319403178524e-05, 1.1164319403178524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1164319403178524e-05

Optimization complete. Final v2v error: 2.860004425048828 mm

Highest mean error: 3.351457118988037 mm for frame 198

Lowest mean error: 2.5712897777557373 mm for frame 117

Saving results

Total time: 64.60664463043213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344751
Iteration 2/25 | Loss: 0.00094088
Iteration 3/25 | Loss: 0.00079636
Iteration 4/25 | Loss: 0.00076021
Iteration 5/25 | Loss: 0.00074632
Iteration 6/25 | Loss: 0.00074366
Iteration 7/25 | Loss: 0.00074315
Iteration 8/25 | Loss: 0.00074315
Iteration 9/25 | Loss: 0.00074315
Iteration 10/25 | Loss: 0.00074315
Iteration 11/25 | Loss: 0.00074315
Iteration 12/25 | Loss: 0.00074315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007431491976603866, 0.0007431491976603866, 0.0007431491976603866, 0.0007431491976603866, 0.0007431491976603866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007431491976603866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44348586
Iteration 2/25 | Loss: 0.00026271
Iteration 3/25 | Loss: 0.00026271
Iteration 4/25 | Loss: 0.00026271
Iteration 5/25 | Loss: 0.00026271
Iteration 6/25 | Loss: 0.00026271
Iteration 7/25 | Loss: 0.00026271
Iteration 8/25 | Loss: 0.00026271
Iteration 9/25 | Loss: 0.00026271
Iteration 10/25 | Loss: 0.00026271
Iteration 11/25 | Loss: 0.00026271
Iteration 12/25 | Loss: 0.00026271
Iteration 13/25 | Loss: 0.00026271
Iteration 14/25 | Loss: 0.00026271
Iteration 15/25 | Loss: 0.00026271
Iteration 16/25 | Loss: 0.00026271
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00026270936359651387, 0.00026270936359651387, 0.00026270936359651387, 0.00026270936359651387, 0.00026270936359651387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00026270936359651387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026271
Iteration 2/1000 | Loss: 0.00004918
Iteration 3/1000 | Loss: 0.00003313
Iteration 4/1000 | Loss: 0.00002717
Iteration 5/1000 | Loss: 0.00002519
Iteration 6/1000 | Loss: 0.00002386
Iteration 7/1000 | Loss: 0.00002308
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002204
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002147
Iteration 12/1000 | Loss: 0.00002123
Iteration 13/1000 | Loss: 0.00002121
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002114
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002109
Iteration 20/1000 | Loss: 0.00002108
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002101
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002077
Iteration 29/1000 | Loss: 0.00002075
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002072
Iteration 32/1000 | Loss: 0.00002071
Iteration 33/1000 | Loss: 0.00002069
Iteration 34/1000 | Loss: 0.00002068
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002067
Iteration 37/1000 | Loss: 0.00002067
Iteration 38/1000 | Loss: 0.00002066
Iteration 39/1000 | Loss: 0.00002065
Iteration 40/1000 | Loss: 0.00002065
Iteration 41/1000 | Loss: 0.00002064
Iteration 42/1000 | Loss: 0.00002064
Iteration 43/1000 | Loss: 0.00002062
Iteration 44/1000 | Loss: 0.00002062
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002060
Iteration 48/1000 | Loss: 0.00002059
Iteration 49/1000 | Loss: 0.00002058
Iteration 50/1000 | Loss: 0.00002058
Iteration 51/1000 | Loss: 0.00002058
Iteration 52/1000 | Loss: 0.00002057
Iteration 53/1000 | Loss: 0.00002057
Iteration 54/1000 | Loss: 0.00002057
Iteration 55/1000 | Loss: 0.00002056
Iteration 56/1000 | Loss: 0.00002056
Iteration 57/1000 | Loss: 0.00002056
Iteration 58/1000 | Loss: 0.00002056
Iteration 59/1000 | Loss: 0.00002055
Iteration 60/1000 | Loss: 0.00002055
Iteration 61/1000 | Loss: 0.00002054
Iteration 62/1000 | Loss: 0.00002054
Iteration 63/1000 | Loss: 0.00002054
Iteration 64/1000 | Loss: 0.00002053
Iteration 65/1000 | Loss: 0.00002053
Iteration 66/1000 | Loss: 0.00002052
Iteration 67/1000 | Loss: 0.00002052
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002051
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002046
Iteration 74/1000 | Loss: 0.00002046
Iteration 75/1000 | Loss: 0.00002046
Iteration 76/1000 | Loss: 0.00002045
Iteration 77/1000 | Loss: 0.00002044
Iteration 78/1000 | Loss: 0.00002044
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002041
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002040
Iteration 91/1000 | Loss: 0.00002040
Iteration 92/1000 | Loss: 0.00002040
Iteration 93/1000 | Loss: 0.00002040
Iteration 94/1000 | Loss: 0.00002039
Iteration 95/1000 | Loss: 0.00002039
Iteration 96/1000 | Loss: 0.00002039
Iteration 97/1000 | Loss: 0.00002039
Iteration 98/1000 | Loss: 0.00002039
Iteration 99/1000 | Loss: 0.00002039
Iteration 100/1000 | Loss: 0.00002039
Iteration 101/1000 | Loss: 0.00002039
Iteration 102/1000 | Loss: 0.00002038
Iteration 103/1000 | Loss: 0.00002038
Iteration 104/1000 | Loss: 0.00002038
Iteration 105/1000 | Loss: 0.00002038
Iteration 106/1000 | Loss: 0.00002037
Iteration 107/1000 | Loss: 0.00002037
Iteration 108/1000 | Loss: 0.00002037
Iteration 109/1000 | Loss: 0.00002037
Iteration 110/1000 | Loss: 0.00002037
Iteration 111/1000 | Loss: 0.00002037
Iteration 112/1000 | Loss: 0.00002036
Iteration 113/1000 | Loss: 0.00002036
Iteration 114/1000 | Loss: 0.00002036
Iteration 115/1000 | Loss: 0.00002036
Iteration 116/1000 | Loss: 0.00002036
Iteration 117/1000 | Loss: 0.00002036
Iteration 118/1000 | Loss: 0.00002036
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002033
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002033
Iteration 146/1000 | Loss: 0.00002033
Iteration 147/1000 | Loss: 0.00002033
Iteration 148/1000 | Loss: 0.00002033
Iteration 149/1000 | Loss: 0.00002033
Iteration 150/1000 | Loss: 0.00002033
Iteration 151/1000 | Loss: 0.00002033
Iteration 152/1000 | Loss: 0.00002033
Iteration 153/1000 | Loss: 0.00002033
Iteration 154/1000 | Loss: 0.00002033
Iteration 155/1000 | Loss: 0.00002032
Iteration 156/1000 | Loss: 0.00002032
Iteration 157/1000 | Loss: 0.00002032
Iteration 158/1000 | Loss: 0.00002032
Iteration 159/1000 | Loss: 0.00002032
Iteration 160/1000 | Loss: 0.00002032
Iteration 161/1000 | Loss: 0.00002032
Iteration 162/1000 | Loss: 0.00002032
Iteration 163/1000 | Loss: 0.00002032
Iteration 164/1000 | Loss: 0.00002032
Iteration 165/1000 | Loss: 0.00002032
Iteration 166/1000 | Loss: 0.00002032
Iteration 167/1000 | Loss: 0.00002032
Iteration 168/1000 | Loss: 0.00002032
Iteration 169/1000 | Loss: 0.00002032
Iteration 170/1000 | Loss: 0.00002032
Iteration 171/1000 | Loss: 0.00002032
Iteration 172/1000 | Loss: 0.00002032
Iteration 173/1000 | Loss: 0.00002032
Iteration 174/1000 | Loss: 0.00002031
Iteration 175/1000 | Loss: 0.00002031
Iteration 176/1000 | Loss: 0.00002031
Iteration 177/1000 | Loss: 0.00002031
Iteration 178/1000 | Loss: 0.00002031
Iteration 179/1000 | Loss: 0.00002031
Iteration 180/1000 | Loss: 0.00002031
Iteration 181/1000 | Loss: 0.00002031
Iteration 182/1000 | Loss: 0.00002031
Iteration 183/1000 | Loss: 0.00002031
Iteration 184/1000 | Loss: 0.00002031
Iteration 185/1000 | Loss: 0.00002031
Iteration 186/1000 | Loss: 0.00002031
Iteration 187/1000 | Loss: 0.00002031
Iteration 188/1000 | Loss: 0.00002031
Iteration 189/1000 | Loss: 0.00002031
Iteration 190/1000 | Loss: 0.00002031
Iteration 191/1000 | Loss: 0.00002031
Iteration 192/1000 | Loss: 0.00002031
Iteration 193/1000 | Loss: 0.00002031
Iteration 194/1000 | Loss: 0.00002031
Iteration 195/1000 | Loss: 0.00002031
Iteration 196/1000 | Loss: 0.00002031
Iteration 197/1000 | Loss: 0.00002031
Iteration 198/1000 | Loss: 0.00002031
Iteration 199/1000 | Loss: 0.00002031
Iteration 200/1000 | Loss: 0.00002031
Iteration 201/1000 | Loss: 0.00002031
Iteration 202/1000 | Loss: 0.00002031
Iteration 203/1000 | Loss: 0.00002031
Iteration 204/1000 | Loss: 0.00002031
Iteration 205/1000 | Loss: 0.00002031
Iteration 206/1000 | Loss: 0.00002031
Iteration 207/1000 | Loss: 0.00002031
Iteration 208/1000 | Loss: 0.00002031
Iteration 209/1000 | Loss: 0.00002031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.03082017833367e-05, 2.03082017833367e-05, 2.03082017833367e-05, 2.03082017833367e-05, 2.03082017833367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.03082017833367e-05

Optimization complete. Final v2v error: 3.7328901290893555 mm

Highest mean error: 4.384125709533691 mm for frame 206

Lowest mean error: 3.342817544937134 mm for frame 192

Saving results

Total time: 53.36778378486633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949268
Iteration 2/25 | Loss: 0.00271540
Iteration 3/25 | Loss: 0.00174906
Iteration 4/25 | Loss: 0.00147573
Iteration 5/25 | Loss: 0.00134339
Iteration 6/25 | Loss: 0.00124832
Iteration 7/25 | Loss: 0.00121525
Iteration 8/25 | Loss: 0.00120870
Iteration 9/25 | Loss: 0.00111539
Iteration 10/25 | Loss: 0.00111984
Iteration 11/25 | Loss: 0.00106528
Iteration 12/25 | Loss: 0.00107439
Iteration 13/25 | Loss: 0.00105185
Iteration 14/25 | Loss: 0.00100292
Iteration 15/25 | Loss: 0.00098357
Iteration 16/25 | Loss: 0.00097432
Iteration 17/25 | Loss: 0.00095954
Iteration 18/25 | Loss: 0.00094755
Iteration 19/25 | Loss: 0.00094905
Iteration 20/25 | Loss: 0.00093656
Iteration 21/25 | Loss: 0.00093021
Iteration 22/25 | Loss: 0.00091994
Iteration 23/25 | Loss: 0.00092177
Iteration 24/25 | Loss: 0.00091345
Iteration 25/25 | Loss: 0.00091703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47122121
Iteration 2/25 | Loss: 0.00136017
Iteration 3/25 | Loss: 0.00101944
Iteration 4/25 | Loss: 0.00101944
Iteration 5/25 | Loss: 0.00101944
Iteration 6/25 | Loss: 0.00101944
Iteration 7/25 | Loss: 0.00101944
Iteration 8/25 | Loss: 0.00101944
Iteration 9/25 | Loss: 0.00101944
Iteration 10/25 | Loss: 0.00101944
Iteration 11/25 | Loss: 0.00101943
Iteration 12/25 | Loss: 0.00101943
Iteration 13/25 | Loss: 0.00101943
Iteration 14/25 | Loss: 0.00101943
Iteration 15/25 | Loss: 0.00101943
Iteration 16/25 | Loss: 0.00101943
Iteration 17/25 | Loss: 0.00101943
Iteration 18/25 | Loss: 0.00101943
Iteration 19/25 | Loss: 0.00101943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010194346541538835, 0.0010194346541538835, 0.0010194346541538835, 0.0010194346541538835, 0.0010194346541538835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010194346541538835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101943
Iteration 2/1000 | Loss: 0.00046898
Iteration 3/1000 | Loss: 0.00060266
Iteration 4/1000 | Loss: 0.00071271
Iteration 5/1000 | Loss: 0.00011951
Iteration 6/1000 | Loss: 0.00092052
Iteration 7/1000 | Loss: 0.00010071
Iteration 8/1000 | Loss: 0.00021508
Iteration 9/1000 | Loss: 0.00029236
Iteration 10/1000 | Loss: 0.00009917
Iteration 11/1000 | Loss: 0.00051033
Iteration 12/1000 | Loss: 0.00033838
Iteration 13/1000 | Loss: 0.00025232
Iteration 14/1000 | Loss: 0.00071737
Iteration 15/1000 | Loss: 0.00008885
Iteration 16/1000 | Loss: 0.00006998
Iteration 17/1000 | Loss: 0.00006424
Iteration 18/1000 | Loss: 0.00022177
Iteration 19/1000 | Loss: 0.00005628
Iteration 20/1000 | Loss: 0.00016183
Iteration 21/1000 | Loss: 0.00005155
Iteration 22/1000 | Loss: 0.00004828
Iteration 23/1000 | Loss: 0.00031218
Iteration 24/1000 | Loss: 0.00094395
Iteration 25/1000 | Loss: 0.00004559
Iteration 26/1000 | Loss: 0.00028271
Iteration 27/1000 | Loss: 0.00004388
Iteration 28/1000 | Loss: 0.00004310
Iteration 29/1000 | Loss: 0.00049606
Iteration 30/1000 | Loss: 0.00006517
Iteration 31/1000 | Loss: 0.00005260
Iteration 32/1000 | Loss: 0.00004708
Iteration 33/1000 | Loss: 0.00004403
Iteration 34/1000 | Loss: 0.00004198
Iteration 35/1000 | Loss: 0.00004106
Iteration 36/1000 | Loss: 0.00004056
Iteration 37/1000 | Loss: 0.00004007
Iteration 38/1000 | Loss: 0.00003950
Iteration 39/1000 | Loss: 0.00003906
Iteration 40/1000 | Loss: 0.00003869
Iteration 41/1000 | Loss: 0.00051595
Iteration 42/1000 | Loss: 0.00049932
Iteration 43/1000 | Loss: 0.00004894
Iteration 44/1000 | Loss: 0.00004404
Iteration 45/1000 | Loss: 0.00004225
Iteration 46/1000 | Loss: 0.00023814
Iteration 47/1000 | Loss: 0.00044518
Iteration 48/1000 | Loss: 0.00004285
Iteration 49/1000 | Loss: 0.00004071
Iteration 50/1000 | Loss: 0.00043998
Iteration 51/1000 | Loss: 0.00008778
Iteration 52/1000 | Loss: 0.00007508
Iteration 53/1000 | Loss: 0.00004943
Iteration 54/1000 | Loss: 0.00004740
Iteration 55/1000 | Loss: 0.00035979
Iteration 56/1000 | Loss: 0.00007800
Iteration 57/1000 | Loss: 0.00005450
Iteration 58/1000 | Loss: 0.00004422
Iteration 59/1000 | Loss: 0.00020666
Iteration 60/1000 | Loss: 0.00004266
Iteration 61/1000 | Loss: 0.00003993
Iteration 62/1000 | Loss: 0.00019071
Iteration 63/1000 | Loss: 0.00003977
Iteration 64/1000 | Loss: 0.00003864
Iteration 65/1000 | Loss: 0.00003804
Iteration 66/1000 | Loss: 0.00028413
Iteration 67/1000 | Loss: 0.00003875
Iteration 68/1000 | Loss: 0.00003747
Iteration 69/1000 | Loss: 0.00013656
Iteration 70/1000 | Loss: 0.00006261
Iteration 71/1000 | Loss: 0.00009236
Iteration 72/1000 | Loss: 0.00003711
Iteration 73/1000 | Loss: 0.00003695
Iteration 74/1000 | Loss: 0.00003694
Iteration 75/1000 | Loss: 0.00017304
Iteration 76/1000 | Loss: 0.00003724
Iteration 77/1000 | Loss: 0.00003680
Iteration 78/1000 | Loss: 0.00003680
Iteration 79/1000 | Loss: 0.00003675
Iteration 80/1000 | Loss: 0.00003674
Iteration 81/1000 | Loss: 0.00003674
Iteration 82/1000 | Loss: 0.00003663
Iteration 83/1000 | Loss: 0.00003659
Iteration 84/1000 | Loss: 0.00003658
Iteration 85/1000 | Loss: 0.00003655
Iteration 86/1000 | Loss: 0.00003654
Iteration 87/1000 | Loss: 0.00003654
Iteration 88/1000 | Loss: 0.00003653
Iteration 89/1000 | Loss: 0.00003652
Iteration 90/1000 | Loss: 0.00003652
Iteration 91/1000 | Loss: 0.00003652
Iteration 92/1000 | Loss: 0.00003650
Iteration 93/1000 | Loss: 0.00003650
Iteration 94/1000 | Loss: 0.00003650
Iteration 95/1000 | Loss: 0.00003650
Iteration 96/1000 | Loss: 0.00003650
Iteration 97/1000 | Loss: 0.00003649
Iteration 98/1000 | Loss: 0.00003649
Iteration 99/1000 | Loss: 0.00003649
Iteration 100/1000 | Loss: 0.00003648
Iteration 101/1000 | Loss: 0.00003648
Iteration 102/1000 | Loss: 0.00003648
Iteration 103/1000 | Loss: 0.00003648
Iteration 104/1000 | Loss: 0.00003648
Iteration 105/1000 | Loss: 0.00003648
Iteration 106/1000 | Loss: 0.00003648
Iteration 107/1000 | Loss: 0.00003647
Iteration 108/1000 | Loss: 0.00003647
Iteration 109/1000 | Loss: 0.00003647
Iteration 110/1000 | Loss: 0.00003647
Iteration 111/1000 | Loss: 0.00003646
Iteration 112/1000 | Loss: 0.00003646
Iteration 113/1000 | Loss: 0.00003646
Iteration 114/1000 | Loss: 0.00003646
Iteration 115/1000 | Loss: 0.00003645
Iteration 116/1000 | Loss: 0.00003645
Iteration 117/1000 | Loss: 0.00003645
Iteration 118/1000 | Loss: 0.00003645
Iteration 119/1000 | Loss: 0.00003645
Iteration 120/1000 | Loss: 0.00003645
Iteration 121/1000 | Loss: 0.00003645
Iteration 122/1000 | Loss: 0.00003645
Iteration 123/1000 | Loss: 0.00003645
Iteration 124/1000 | Loss: 0.00003645
Iteration 125/1000 | Loss: 0.00003645
Iteration 126/1000 | Loss: 0.00003645
Iteration 127/1000 | Loss: 0.00003644
Iteration 128/1000 | Loss: 0.00003644
Iteration 129/1000 | Loss: 0.00003644
Iteration 130/1000 | Loss: 0.00003644
Iteration 131/1000 | Loss: 0.00003643
Iteration 132/1000 | Loss: 0.00003643
Iteration 133/1000 | Loss: 0.00003643
Iteration 134/1000 | Loss: 0.00003643
Iteration 135/1000 | Loss: 0.00003643
Iteration 136/1000 | Loss: 0.00003643
Iteration 137/1000 | Loss: 0.00003643
Iteration 138/1000 | Loss: 0.00003643
Iteration 139/1000 | Loss: 0.00003642
Iteration 140/1000 | Loss: 0.00003642
Iteration 141/1000 | Loss: 0.00003642
Iteration 142/1000 | Loss: 0.00003642
Iteration 143/1000 | Loss: 0.00003641
Iteration 144/1000 | Loss: 0.00003641
Iteration 145/1000 | Loss: 0.00003641
Iteration 146/1000 | Loss: 0.00003641
Iteration 147/1000 | Loss: 0.00003641
Iteration 148/1000 | Loss: 0.00003641
Iteration 149/1000 | Loss: 0.00003641
Iteration 150/1000 | Loss: 0.00003641
Iteration 151/1000 | Loss: 0.00003641
Iteration 152/1000 | Loss: 0.00003641
Iteration 153/1000 | Loss: 0.00003641
Iteration 154/1000 | Loss: 0.00003641
Iteration 155/1000 | Loss: 0.00003641
Iteration 156/1000 | Loss: 0.00003641
Iteration 157/1000 | Loss: 0.00003641
Iteration 158/1000 | Loss: 0.00003641
Iteration 159/1000 | Loss: 0.00003641
Iteration 160/1000 | Loss: 0.00003641
Iteration 161/1000 | Loss: 0.00003641
Iteration 162/1000 | Loss: 0.00003641
Iteration 163/1000 | Loss: 0.00003641
Iteration 164/1000 | Loss: 0.00003641
Iteration 165/1000 | Loss: 0.00003641
Iteration 166/1000 | Loss: 0.00003641
Iteration 167/1000 | Loss: 0.00003641
Iteration 168/1000 | Loss: 0.00003641
Iteration 169/1000 | Loss: 0.00003641
Iteration 170/1000 | Loss: 0.00003641
Iteration 171/1000 | Loss: 0.00003641
Iteration 172/1000 | Loss: 0.00003641
Iteration 173/1000 | Loss: 0.00003641
Iteration 174/1000 | Loss: 0.00003641
Iteration 175/1000 | Loss: 0.00003641
Iteration 176/1000 | Loss: 0.00003641
Iteration 177/1000 | Loss: 0.00003641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [3.6411227483768016e-05, 3.6411227483768016e-05, 3.6411227483768016e-05, 3.6411227483768016e-05, 3.6411227483768016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6411227483768016e-05

Optimization complete. Final v2v error: 4.013368129730225 mm

Highest mean error: 6.186587333679199 mm for frame 20

Lowest mean error: 3.1225311756134033 mm for frame 120

Saving results

Total time: 160.83332896232605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383353
Iteration 2/25 | Loss: 0.00100636
Iteration 3/25 | Loss: 0.00073927
Iteration 4/25 | Loss: 0.00070466
Iteration 5/25 | Loss: 0.00069638
Iteration 6/25 | Loss: 0.00069466
Iteration 7/25 | Loss: 0.00069421
Iteration 8/25 | Loss: 0.00069421
Iteration 9/25 | Loss: 0.00069421
Iteration 10/25 | Loss: 0.00069421
Iteration 11/25 | Loss: 0.00069421
Iteration 12/25 | Loss: 0.00069421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006942087784409523, 0.0006942087784409523, 0.0006942087784409523, 0.0006942087784409523, 0.0006942087784409523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006942087784409523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42988253
Iteration 2/25 | Loss: 0.00020835
Iteration 3/25 | Loss: 0.00020833
Iteration 4/25 | Loss: 0.00020833
Iteration 5/25 | Loss: 0.00020833
Iteration 6/25 | Loss: 0.00020833
Iteration 7/25 | Loss: 0.00020833
Iteration 8/25 | Loss: 0.00020833
Iteration 9/25 | Loss: 0.00020833
Iteration 10/25 | Loss: 0.00020833
Iteration 11/25 | Loss: 0.00020833
Iteration 12/25 | Loss: 0.00020833
Iteration 13/25 | Loss: 0.00020833
Iteration 14/25 | Loss: 0.00020833
Iteration 15/25 | Loss: 0.00020833
Iteration 16/25 | Loss: 0.00020833
Iteration 17/25 | Loss: 0.00020833
Iteration 18/25 | Loss: 0.00020833
Iteration 19/25 | Loss: 0.00020833
Iteration 20/25 | Loss: 0.00020833
Iteration 21/25 | Loss: 0.00020833
Iteration 22/25 | Loss: 0.00020833
Iteration 23/25 | Loss: 0.00020833
Iteration 24/25 | Loss: 0.00020833
Iteration 25/25 | Loss: 0.00020833
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00020832743030041456, 0.00020832743030041456, 0.00020832743030041456, 0.00020832743030041456, 0.00020832743030041456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020832743030041456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020833
Iteration 2/1000 | Loss: 0.00003060
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001649
Iteration 5/1000 | Loss: 0.00001489
Iteration 6/1000 | Loss: 0.00001391
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001281
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001236
Iteration 11/1000 | Loss: 0.00001233
Iteration 12/1000 | Loss: 0.00001228
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001199
Iteration 18/1000 | Loss: 0.00001198
Iteration 19/1000 | Loss: 0.00001198
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001194
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001184
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001175
Iteration 33/1000 | Loss: 0.00001175
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001171
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001170
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001166
Iteration 52/1000 | Loss: 0.00001166
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001164
Iteration 57/1000 | Loss: 0.00001164
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Iteration 69/1000 | Loss: 0.00001162
Iteration 70/1000 | Loss: 0.00001162
Iteration 71/1000 | Loss: 0.00001162
Iteration 72/1000 | Loss: 0.00001162
Iteration 73/1000 | Loss: 0.00001161
Iteration 74/1000 | Loss: 0.00001161
Iteration 75/1000 | Loss: 0.00001161
Iteration 76/1000 | Loss: 0.00001161
Iteration 77/1000 | Loss: 0.00001160
Iteration 78/1000 | Loss: 0.00001160
Iteration 79/1000 | Loss: 0.00001160
Iteration 80/1000 | Loss: 0.00001160
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001160
Iteration 84/1000 | Loss: 0.00001160
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001159
Iteration 90/1000 | Loss: 0.00001159
Iteration 91/1000 | Loss: 0.00001159
Iteration 92/1000 | Loss: 0.00001159
Iteration 93/1000 | Loss: 0.00001159
Iteration 94/1000 | Loss: 0.00001159
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001159
Iteration 99/1000 | Loss: 0.00001159
Iteration 100/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.1592775990720838e-05, 1.1592775990720838e-05, 1.1592775990720838e-05, 1.1592775990720838e-05, 1.1592775990720838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1592775990720838e-05

Optimization complete. Final v2v error: 2.8959991931915283 mm

Highest mean error: 3.4401702880859375 mm for frame 104

Lowest mean error: 2.5420260429382324 mm for frame 7

Saving results

Total time: 35.07728338241577
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773779
Iteration 2/25 | Loss: 0.00119797
Iteration 3/25 | Loss: 0.00083866
Iteration 4/25 | Loss: 0.00077246
Iteration 5/25 | Loss: 0.00075309
Iteration 6/25 | Loss: 0.00074446
Iteration 7/25 | Loss: 0.00074513
Iteration 8/25 | Loss: 0.00074116
Iteration 9/25 | Loss: 0.00074070
Iteration 10/25 | Loss: 0.00073699
Iteration 11/25 | Loss: 0.00073291
Iteration 12/25 | Loss: 0.00073130
Iteration 13/25 | Loss: 0.00073054
Iteration 14/25 | Loss: 0.00073022
Iteration 15/25 | Loss: 0.00073003
Iteration 16/25 | Loss: 0.00072993
Iteration 17/25 | Loss: 0.00072987
Iteration 18/25 | Loss: 0.00072987
Iteration 19/25 | Loss: 0.00072987
Iteration 20/25 | Loss: 0.00072986
Iteration 21/25 | Loss: 0.00072986
Iteration 22/25 | Loss: 0.00072986
Iteration 23/25 | Loss: 0.00072986
Iteration 24/25 | Loss: 0.00072986
Iteration 25/25 | Loss: 0.00072986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08667326
Iteration 2/25 | Loss: 0.00022768
Iteration 3/25 | Loss: 0.00022768
Iteration 4/25 | Loss: 0.00022768
Iteration 5/25 | Loss: 0.00022767
Iteration 6/25 | Loss: 0.00022767
Iteration 7/25 | Loss: 0.00022767
Iteration 8/25 | Loss: 0.00022767
Iteration 9/25 | Loss: 0.00022767
Iteration 10/25 | Loss: 0.00022767
Iteration 11/25 | Loss: 0.00022767
Iteration 12/25 | Loss: 0.00022767
Iteration 13/25 | Loss: 0.00022767
Iteration 14/25 | Loss: 0.00022767
Iteration 15/25 | Loss: 0.00022767
Iteration 16/25 | Loss: 0.00022767
Iteration 17/25 | Loss: 0.00022767
Iteration 18/25 | Loss: 0.00022767
Iteration 19/25 | Loss: 0.00022767
Iteration 20/25 | Loss: 0.00022767
Iteration 21/25 | Loss: 0.00022767
Iteration 22/25 | Loss: 0.00022767
Iteration 23/25 | Loss: 0.00022767
Iteration 24/25 | Loss: 0.00022767
Iteration 25/25 | Loss: 0.00022767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022767
Iteration 2/1000 | Loss: 0.00001994
Iteration 3/1000 | Loss: 0.00001565
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001351
Iteration 12/1000 | Loss: 0.00001350
Iteration 13/1000 | Loss: 0.00001345
Iteration 14/1000 | Loss: 0.00001345
Iteration 15/1000 | Loss: 0.00001344
Iteration 16/1000 | Loss: 0.00001343
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001333
Iteration 20/1000 | Loss: 0.00001330
Iteration 21/1000 | Loss: 0.00001329
Iteration 22/1000 | Loss: 0.00001327
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001307
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001306
Iteration 30/1000 | Loss: 0.00001301
Iteration 31/1000 | Loss: 0.00001301
Iteration 32/1000 | Loss: 0.00001301
Iteration 33/1000 | Loss: 0.00001300
Iteration 34/1000 | Loss: 0.00001299
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001298
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001295
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001292
Iteration 55/1000 | Loss: 0.00001292
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001291
Iteration 61/1000 | Loss: 0.00001291
Iteration 62/1000 | Loss: 0.00001291
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001285
Iteration 79/1000 | Loss: 0.00001285
Iteration 80/1000 | Loss: 0.00001285
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001285
Iteration 84/1000 | Loss: 0.00001285
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001284
Iteration 87/1000 | Loss: 0.00001284
Iteration 88/1000 | Loss: 0.00001284
Iteration 89/1000 | Loss: 0.00001284
Iteration 90/1000 | Loss: 0.00001284
Iteration 91/1000 | Loss: 0.00001284
Iteration 92/1000 | Loss: 0.00001284
Iteration 93/1000 | Loss: 0.00001284
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Iteration 115/1000 | Loss: 0.00001284
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001284
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001284
Iteration 121/1000 | Loss: 0.00001284
Iteration 122/1000 | Loss: 0.00001284
Iteration 123/1000 | Loss: 0.00001284
Iteration 124/1000 | Loss: 0.00001284
Iteration 125/1000 | Loss: 0.00001284
Iteration 126/1000 | Loss: 0.00001284
Iteration 127/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2835371308028698e-05, 1.2835371308028698e-05, 1.2835371308028698e-05, 1.2835371308028698e-05, 1.2835371308028698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2835371308028698e-05

Optimization complete. Final v2v error: 3.0587682723999023 mm

Highest mean error: 3.3527491092681885 mm for frame 108

Lowest mean error: 2.7895970344543457 mm for frame 198

Saving results

Total time: 58.864502906799316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430186
Iteration 2/25 | Loss: 0.00088564
Iteration 3/25 | Loss: 0.00075656
Iteration 4/25 | Loss: 0.00073353
Iteration 5/25 | Loss: 0.00072489
Iteration 6/25 | Loss: 0.00072338
Iteration 7/25 | Loss: 0.00072310
Iteration 8/25 | Loss: 0.00072310
Iteration 9/25 | Loss: 0.00072310
Iteration 10/25 | Loss: 0.00072310
Iteration 11/25 | Loss: 0.00072310
Iteration 12/25 | Loss: 0.00072310
Iteration 13/25 | Loss: 0.00072310
Iteration 14/25 | Loss: 0.00072310
Iteration 15/25 | Loss: 0.00072310
Iteration 16/25 | Loss: 0.00072310
Iteration 17/25 | Loss: 0.00072310
Iteration 18/25 | Loss: 0.00072310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007230999763123691, 0.0007230999763123691, 0.0007230999763123691, 0.0007230999763123691, 0.0007230999763123691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007230999763123691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44210780
Iteration 2/25 | Loss: 0.00027390
Iteration 3/25 | Loss: 0.00027390
Iteration 4/25 | Loss: 0.00027390
Iteration 5/25 | Loss: 0.00027390
Iteration 6/25 | Loss: 0.00027390
Iteration 7/25 | Loss: 0.00027390
Iteration 8/25 | Loss: 0.00027389
Iteration 9/25 | Loss: 0.00027389
Iteration 10/25 | Loss: 0.00027389
Iteration 11/25 | Loss: 0.00027389
Iteration 12/25 | Loss: 0.00027389
Iteration 13/25 | Loss: 0.00027389
Iteration 14/25 | Loss: 0.00027389
Iteration 15/25 | Loss: 0.00027389
Iteration 16/25 | Loss: 0.00027389
Iteration 17/25 | Loss: 0.00027389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002738941984716803, 0.0002738941984716803, 0.0002738941984716803, 0.0002738941984716803, 0.0002738941984716803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002738941984716803

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027389
Iteration 2/1000 | Loss: 0.00002156
Iteration 3/1000 | Loss: 0.00001596
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001382
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001325
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001296
Iteration 21/1000 | Loss: 0.00001296
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001295
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001295
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001290
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001289
Iteration 40/1000 | Loss: 0.00001289
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001287
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001285
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001284
Iteration 48/1000 | Loss: 0.00001283
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001280
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001279
Iteration 55/1000 | Loss: 0.00001277
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001276
Iteration 59/1000 | Loss: 0.00001276
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001275
Iteration 62/1000 | Loss: 0.00001275
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001274
Iteration 69/1000 | Loss: 0.00001274
Iteration 70/1000 | Loss: 0.00001273
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001270
Iteration 74/1000 | Loss: 0.00001269
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001264
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001263
Iteration 102/1000 | Loss: 0.00001263
Iteration 103/1000 | Loss: 0.00001263
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001263
Iteration 108/1000 | Loss: 0.00001263
Iteration 109/1000 | Loss: 0.00001263
Iteration 110/1000 | Loss: 0.00001263
Iteration 111/1000 | Loss: 0.00001262
Iteration 112/1000 | Loss: 0.00001262
Iteration 113/1000 | Loss: 0.00001262
Iteration 114/1000 | Loss: 0.00001262
Iteration 115/1000 | Loss: 0.00001262
Iteration 116/1000 | Loss: 0.00001262
Iteration 117/1000 | Loss: 0.00001262
Iteration 118/1000 | Loss: 0.00001262
Iteration 119/1000 | Loss: 0.00001262
Iteration 120/1000 | Loss: 0.00001262
Iteration 121/1000 | Loss: 0.00001261
Iteration 122/1000 | Loss: 0.00001261
Iteration 123/1000 | Loss: 0.00001261
Iteration 124/1000 | Loss: 0.00001261
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001261
Iteration 128/1000 | Loss: 0.00001261
Iteration 129/1000 | Loss: 0.00001261
Iteration 130/1000 | Loss: 0.00001261
Iteration 131/1000 | Loss: 0.00001261
Iteration 132/1000 | Loss: 0.00001261
Iteration 133/1000 | Loss: 0.00001261
Iteration 134/1000 | Loss: 0.00001261
Iteration 135/1000 | Loss: 0.00001261
Iteration 136/1000 | Loss: 0.00001261
Iteration 137/1000 | Loss: 0.00001261
Iteration 138/1000 | Loss: 0.00001261
Iteration 139/1000 | Loss: 0.00001261
Iteration 140/1000 | Loss: 0.00001261
Iteration 141/1000 | Loss: 0.00001261
Iteration 142/1000 | Loss: 0.00001261
Iteration 143/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.2605779375007842e-05, 1.2605779375007842e-05, 1.2605779375007842e-05, 1.2605779375007842e-05, 1.2605779375007842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2605779375007842e-05

Optimization complete. Final v2v error: 2.984358072280884 mm

Highest mean error: 3.2328052520751953 mm for frame 47

Lowest mean error: 2.665302276611328 mm for frame 10

Saving results

Total time: 41.557082653045654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397983
Iteration 2/25 | Loss: 0.00081009
Iteration 3/25 | Loss: 0.00071788
Iteration 4/25 | Loss: 0.00069742
Iteration 5/25 | Loss: 0.00069073
Iteration 6/25 | Loss: 0.00068935
Iteration 7/25 | Loss: 0.00068902
Iteration 8/25 | Loss: 0.00068902
Iteration 9/25 | Loss: 0.00068902
Iteration 10/25 | Loss: 0.00068902
Iteration 11/25 | Loss: 0.00068902
Iteration 12/25 | Loss: 0.00068902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000689022010192275, 0.000689022010192275, 0.000689022010192275, 0.000689022010192275, 0.000689022010192275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000689022010192275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.19727778
Iteration 2/25 | Loss: 0.00024258
Iteration 3/25 | Loss: 0.00024258
Iteration 4/25 | Loss: 0.00024257
Iteration 5/25 | Loss: 0.00024257
Iteration 6/25 | Loss: 0.00024257
Iteration 7/25 | Loss: 0.00024257
Iteration 8/25 | Loss: 0.00024257
Iteration 9/25 | Loss: 0.00024257
Iteration 10/25 | Loss: 0.00024257
Iteration 11/25 | Loss: 0.00024257
Iteration 12/25 | Loss: 0.00024257
Iteration 13/25 | Loss: 0.00024257
Iteration 14/25 | Loss: 0.00024257
Iteration 15/25 | Loss: 0.00024257
Iteration 16/25 | Loss: 0.00024257
Iteration 17/25 | Loss: 0.00024257
Iteration 18/25 | Loss: 0.00024257
Iteration 19/25 | Loss: 0.00024257
Iteration 20/25 | Loss: 0.00024257
Iteration 21/25 | Loss: 0.00024257
Iteration 22/25 | Loss: 0.00024257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00024257146287709475, 0.00024257146287709475, 0.00024257146287709475, 0.00024257146287709475, 0.00024257146287709475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024257146287709475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024257
Iteration 2/1000 | Loss: 0.00002301
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001523
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001436
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001375
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001367
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001365
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001356
Iteration 20/1000 | Loss: 0.00001352
Iteration 21/1000 | Loss: 0.00001351
Iteration 22/1000 | Loss: 0.00001351
Iteration 23/1000 | Loss: 0.00001351
Iteration 24/1000 | Loss: 0.00001351
Iteration 25/1000 | Loss: 0.00001350
Iteration 26/1000 | Loss: 0.00001350
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001350
Iteration 31/1000 | Loss: 0.00001350
Iteration 32/1000 | Loss: 0.00001350
Iteration 33/1000 | Loss: 0.00001348
Iteration 34/1000 | Loss: 0.00001347
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001345
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001340
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001336
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001329
Iteration 63/1000 | Loss: 0.00001329
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001329
Iteration 66/1000 | Loss: 0.00001329
Iteration 67/1000 | Loss: 0.00001329
Iteration 68/1000 | Loss: 0.00001329
Iteration 69/1000 | Loss: 0.00001329
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001328
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001327
Iteration 75/1000 | Loss: 0.00001327
Iteration 76/1000 | Loss: 0.00001327
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001324
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001324
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001323
Iteration 114/1000 | Loss: 0.00001323
Iteration 115/1000 | Loss: 0.00001323
Iteration 116/1000 | Loss: 0.00001323
Iteration 117/1000 | Loss: 0.00001323
Iteration 118/1000 | Loss: 0.00001323
Iteration 119/1000 | Loss: 0.00001323
Iteration 120/1000 | Loss: 0.00001323
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001322
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001320
Iteration 149/1000 | Loss: 0.00001320
Iteration 150/1000 | Loss: 0.00001320
Iteration 151/1000 | Loss: 0.00001320
Iteration 152/1000 | Loss: 0.00001320
Iteration 153/1000 | Loss: 0.00001320
Iteration 154/1000 | Loss: 0.00001320
Iteration 155/1000 | Loss: 0.00001320
Iteration 156/1000 | Loss: 0.00001320
Iteration 157/1000 | Loss: 0.00001320
Iteration 158/1000 | Loss: 0.00001320
Iteration 159/1000 | Loss: 0.00001320
Iteration 160/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.3198375199863221e-05, 1.3198375199863221e-05, 1.3198375199863221e-05, 1.3198375199863221e-05, 1.3198375199863221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3198375199863221e-05

Optimization complete. Final v2v error: 3.0966873168945312 mm

Highest mean error: 3.335200786590576 mm for frame 76

Lowest mean error: 2.9500653743743896 mm for frame 35

Saving results

Total time: 37.44345307350159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490249
Iteration 2/25 | Loss: 0.00099723
Iteration 3/25 | Loss: 0.00084508
Iteration 4/25 | Loss: 0.00079675
Iteration 5/25 | Loss: 0.00078108
Iteration 6/25 | Loss: 0.00077710
Iteration 7/25 | Loss: 0.00077556
Iteration 8/25 | Loss: 0.00077546
Iteration 9/25 | Loss: 0.00077546
Iteration 10/25 | Loss: 0.00077546
Iteration 11/25 | Loss: 0.00077546
Iteration 12/25 | Loss: 0.00077546
Iteration 13/25 | Loss: 0.00077546
Iteration 14/25 | Loss: 0.00077546
Iteration 15/25 | Loss: 0.00077546
Iteration 16/25 | Loss: 0.00077546
Iteration 17/25 | Loss: 0.00077546
Iteration 18/25 | Loss: 0.00077546
Iteration 19/25 | Loss: 0.00077546
Iteration 20/25 | Loss: 0.00077546
Iteration 21/25 | Loss: 0.00077546
Iteration 22/25 | Loss: 0.00077546
Iteration 23/25 | Loss: 0.00077546
Iteration 24/25 | Loss: 0.00077546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007754620164632797, 0.0007754620164632797, 0.0007754620164632797, 0.0007754620164632797, 0.0007754620164632797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007754620164632797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65042937
Iteration 2/25 | Loss: 0.00030707
Iteration 3/25 | Loss: 0.00030706
Iteration 4/25 | Loss: 0.00030706
Iteration 5/25 | Loss: 0.00030706
Iteration 6/25 | Loss: 0.00030706
Iteration 7/25 | Loss: 0.00030706
Iteration 8/25 | Loss: 0.00030706
Iteration 9/25 | Loss: 0.00030706
Iteration 10/25 | Loss: 0.00030706
Iteration 11/25 | Loss: 0.00030706
Iteration 12/25 | Loss: 0.00030706
Iteration 13/25 | Loss: 0.00030706
Iteration 14/25 | Loss: 0.00030706
Iteration 15/25 | Loss: 0.00030706
Iteration 16/25 | Loss: 0.00030706
Iteration 17/25 | Loss: 0.00030706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003070592356380075, 0.0003070592356380075, 0.0003070592356380075, 0.0003070592356380075, 0.0003070592356380075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003070592356380075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030706
Iteration 2/1000 | Loss: 0.00004110
Iteration 3/1000 | Loss: 0.00003041
Iteration 4/1000 | Loss: 0.00002713
Iteration 5/1000 | Loss: 0.00002561
Iteration 6/1000 | Loss: 0.00002452
Iteration 7/1000 | Loss: 0.00002393
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002307
Iteration 10/1000 | Loss: 0.00002280
Iteration 11/1000 | Loss: 0.00002258
Iteration 12/1000 | Loss: 0.00002242
Iteration 13/1000 | Loss: 0.00002225
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002210
Iteration 16/1000 | Loss: 0.00002205
Iteration 17/1000 | Loss: 0.00002200
Iteration 18/1000 | Loss: 0.00002198
Iteration 19/1000 | Loss: 0.00002197
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002196
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002193
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002189
Iteration 30/1000 | Loss: 0.00002189
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002188
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002186
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002185
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002185
Iteration 46/1000 | Loss: 0.00002185
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002185
Iteration 49/1000 | Loss: 0.00002185
Iteration 50/1000 | Loss: 0.00002184
Iteration 51/1000 | Loss: 0.00002184
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002184
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002181
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002181
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002180
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002177
Iteration 79/1000 | Loss: 0.00002177
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002176
Iteration 82/1000 | Loss: 0.00002176
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002176
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002174
Iteration 89/1000 | Loss: 0.00002174
Iteration 90/1000 | Loss: 0.00002174
Iteration 91/1000 | Loss: 0.00002174
Iteration 92/1000 | Loss: 0.00002174
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002174
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002172
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002172
Iteration 113/1000 | Loss: 0.00002172
Iteration 114/1000 | Loss: 0.00002172
Iteration 115/1000 | Loss: 0.00002172
Iteration 116/1000 | Loss: 0.00002172
Iteration 117/1000 | Loss: 0.00002172
Iteration 118/1000 | Loss: 0.00002172
Iteration 119/1000 | Loss: 0.00002172
Iteration 120/1000 | Loss: 0.00002172
Iteration 121/1000 | Loss: 0.00002172
Iteration 122/1000 | Loss: 0.00002172
Iteration 123/1000 | Loss: 0.00002172
Iteration 124/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.1721825760323554e-05, 2.1721825760323554e-05, 2.1721825760323554e-05, 2.1721825760323554e-05, 2.1721825760323554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1721825760323554e-05

Optimization complete. Final v2v error: 3.8767330646514893 mm

Highest mean error: 5.091993808746338 mm for frame 54

Lowest mean error: 2.937546730041504 mm for frame 239

Saving results

Total time: 45.82145595550537
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565121
Iteration 2/25 | Loss: 0.00107874
Iteration 3/25 | Loss: 0.00082865
Iteration 4/25 | Loss: 0.00079672
Iteration 5/25 | Loss: 0.00078508
Iteration 6/25 | Loss: 0.00078283
Iteration 7/25 | Loss: 0.00078256
Iteration 8/25 | Loss: 0.00078256
Iteration 9/25 | Loss: 0.00078256
Iteration 10/25 | Loss: 0.00078256
Iteration 11/25 | Loss: 0.00078256
Iteration 12/25 | Loss: 0.00078256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007825615466572344, 0.0007825615466572344, 0.0007825615466572344, 0.0007825615466572344, 0.0007825615466572344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007825615466572344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.84857130
Iteration 2/25 | Loss: 0.00023892
Iteration 3/25 | Loss: 0.00023882
Iteration 4/25 | Loss: 0.00023882
Iteration 5/25 | Loss: 0.00023882
Iteration 6/25 | Loss: 0.00023882
Iteration 7/25 | Loss: 0.00023882
Iteration 8/25 | Loss: 0.00023882
Iteration 9/25 | Loss: 0.00023882
Iteration 10/25 | Loss: 0.00023882
Iteration 11/25 | Loss: 0.00023882
Iteration 12/25 | Loss: 0.00023882
Iteration 13/25 | Loss: 0.00023882
Iteration 14/25 | Loss: 0.00023882
Iteration 15/25 | Loss: 0.00023882
Iteration 16/25 | Loss: 0.00023882
Iteration 17/25 | Loss: 0.00023882
Iteration 18/25 | Loss: 0.00023882
Iteration 19/25 | Loss: 0.00023882
Iteration 20/25 | Loss: 0.00023882
Iteration 21/25 | Loss: 0.00023882
Iteration 22/25 | Loss: 0.00023882
Iteration 23/25 | Loss: 0.00023882
Iteration 24/25 | Loss: 0.00023882
Iteration 25/25 | Loss: 0.00023882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023882
Iteration 2/1000 | Loss: 0.00002756
Iteration 3/1000 | Loss: 0.00002063
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001785
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001773
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001765
Iteration 16/1000 | Loss: 0.00001755
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001752
Iteration 19/1000 | Loss: 0.00001752
Iteration 20/1000 | Loss: 0.00001752
Iteration 21/1000 | Loss: 0.00001752
Iteration 22/1000 | Loss: 0.00001752
Iteration 23/1000 | Loss: 0.00001751
Iteration 24/1000 | Loss: 0.00001751
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001750
Iteration 27/1000 | Loss: 0.00001750
Iteration 28/1000 | Loss: 0.00001750
Iteration 29/1000 | Loss: 0.00001749
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001749
Iteration 33/1000 | Loss: 0.00001749
Iteration 34/1000 | Loss: 0.00001749
Iteration 35/1000 | Loss: 0.00001749
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001745
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001745
Iteration 63/1000 | Loss: 0.00001745
Iteration 64/1000 | Loss: 0.00001745
Iteration 65/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.7446511265006848e-05, 1.7446511265006848e-05, 1.7446511265006848e-05, 1.7446511265006848e-05, 1.7446511265006848e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7446511265006848e-05

Optimization complete. Final v2v error: 3.538480520248413 mm

Highest mean error: 3.815169095993042 mm for frame 225

Lowest mean error: 3.317789077758789 mm for frame 4

Saving results

Total time: 33.56903862953186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415479
Iteration 2/25 | Loss: 0.00085589
Iteration 3/25 | Loss: 0.00076079
Iteration 4/25 | Loss: 0.00073148
Iteration 5/25 | Loss: 0.00072093
Iteration 6/25 | Loss: 0.00071939
Iteration 7/25 | Loss: 0.00071885
Iteration 8/25 | Loss: 0.00071885
Iteration 9/25 | Loss: 0.00071885
Iteration 10/25 | Loss: 0.00071885
Iteration 11/25 | Loss: 0.00071885
Iteration 12/25 | Loss: 0.00071885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007188476156443357, 0.0007188476156443357, 0.0007188476156443357, 0.0007188476156443357, 0.0007188476156443357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007188476156443357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59398401
Iteration 2/25 | Loss: 0.00022733
Iteration 3/25 | Loss: 0.00022733
Iteration 4/25 | Loss: 0.00022733
Iteration 5/25 | Loss: 0.00022733
Iteration 6/25 | Loss: 0.00022732
Iteration 7/25 | Loss: 0.00022732
Iteration 8/25 | Loss: 0.00022732
Iteration 9/25 | Loss: 0.00022732
Iteration 10/25 | Loss: 0.00022732
Iteration 11/25 | Loss: 0.00022732
Iteration 12/25 | Loss: 0.00022732
Iteration 13/25 | Loss: 0.00022732
Iteration 14/25 | Loss: 0.00022732
Iteration 15/25 | Loss: 0.00022732
Iteration 16/25 | Loss: 0.00022732
Iteration 17/25 | Loss: 0.00022732
Iteration 18/25 | Loss: 0.00022732
Iteration 19/25 | Loss: 0.00022732
Iteration 20/25 | Loss: 0.00022732
Iteration 21/25 | Loss: 0.00022732
Iteration 22/25 | Loss: 0.00022732
Iteration 23/25 | Loss: 0.00022732
Iteration 24/25 | Loss: 0.00022732
Iteration 25/25 | Loss: 0.00022732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022732
Iteration 2/1000 | Loss: 0.00002681
Iteration 3/1000 | Loss: 0.00002022
Iteration 4/1000 | Loss: 0.00001894
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001729
Iteration 7/1000 | Loss: 0.00001670
Iteration 8/1000 | Loss: 0.00001666
Iteration 9/1000 | Loss: 0.00001652
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001600
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001593
Iteration 17/1000 | Loss: 0.00001593
Iteration 18/1000 | Loss: 0.00001592
Iteration 19/1000 | Loss: 0.00001591
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001585
Iteration 22/1000 | Loss: 0.00001584
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001581
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001578
Iteration 36/1000 | Loss: 0.00001577
Iteration 37/1000 | Loss: 0.00001577
Iteration 38/1000 | Loss: 0.00001577
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00001576
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001574
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001572
Iteration 49/1000 | Loss: 0.00001572
Iteration 50/1000 | Loss: 0.00001571
Iteration 51/1000 | Loss: 0.00001571
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001566
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001562
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001559
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001559
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001558
Iteration 80/1000 | Loss: 0.00001558
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001555
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001555
Iteration 103/1000 | Loss: 0.00001555
Iteration 104/1000 | Loss: 0.00001555
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001555
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001553
Iteration 124/1000 | Loss: 0.00001553
Iteration 125/1000 | Loss: 0.00001553
Iteration 126/1000 | Loss: 0.00001553
Iteration 127/1000 | Loss: 0.00001553
Iteration 128/1000 | Loss: 0.00001553
Iteration 129/1000 | Loss: 0.00001553
Iteration 130/1000 | Loss: 0.00001553
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001552
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001552
Iteration 138/1000 | Loss: 0.00001552
Iteration 139/1000 | Loss: 0.00001552
Iteration 140/1000 | Loss: 0.00001552
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001551
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001550
Iteration 150/1000 | Loss: 0.00001550
Iteration 151/1000 | Loss: 0.00001550
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001549
Iteration 158/1000 | Loss: 0.00001549
Iteration 159/1000 | Loss: 0.00001549
Iteration 160/1000 | Loss: 0.00001549
Iteration 161/1000 | Loss: 0.00001549
Iteration 162/1000 | Loss: 0.00001549
Iteration 163/1000 | Loss: 0.00001549
Iteration 164/1000 | Loss: 0.00001549
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001548
Iteration 167/1000 | Loss: 0.00001548
Iteration 168/1000 | Loss: 0.00001548
Iteration 169/1000 | Loss: 0.00001548
Iteration 170/1000 | Loss: 0.00001548
Iteration 171/1000 | Loss: 0.00001548
Iteration 172/1000 | Loss: 0.00001548
Iteration 173/1000 | Loss: 0.00001548
Iteration 174/1000 | Loss: 0.00001548
Iteration 175/1000 | Loss: 0.00001548
Iteration 176/1000 | Loss: 0.00001548
Iteration 177/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.5480683941859752e-05, 1.5480683941859752e-05, 1.5480683941859752e-05, 1.5480683941859752e-05, 1.5480683941859752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5480683941859752e-05

Optimization complete. Final v2v error: 3.317655086517334 mm

Highest mean error: 3.671618938446045 mm for frame 78

Lowest mean error: 3.166065216064453 mm for frame 108

Saving results

Total time: 39.1962513923645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851174
Iteration 2/25 | Loss: 0.00088914
Iteration 3/25 | Loss: 0.00072047
Iteration 4/25 | Loss: 0.00070027
Iteration 5/25 | Loss: 0.00069338
Iteration 6/25 | Loss: 0.00069142
Iteration 7/25 | Loss: 0.00069119
Iteration 8/25 | Loss: 0.00069119
Iteration 9/25 | Loss: 0.00069119
Iteration 10/25 | Loss: 0.00069119
Iteration 11/25 | Loss: 0.00069119
Iteration 12/25 | Loss: 0.00069119
Iteration 13/25 | Loss: 0.00069119
Iteration 14/25 | Loss: 0.00069119
Iteration 15/25 | Loss: 0.00069119
Iteration 16/25 | Loss: 0.00069119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000691187393385917, 0.000691187393385917, 0.000691187393385917, 0.000691187393385917, 0.000691187393385917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000691187393385917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45124078
Iteration 2/25 | Loss: 0.00018482
Iteration 3/25 | Loss: 0.00018481
Iteration 4/25 | Loss: 0.00018481
Iteration 5/25 | Loss: 0.00018481
Iteration 6/25 | Loss: 0.00018481
Iteration 7/25 | Loss: 0.00018481
Iteration 8/25 | Loss: 0.00018481
Iteration 9/25 | Loss: 0.00018481
Iteration 10/25 | Loss: 0.00018481
Iteration 11/25 | Loss: 0.00018481
Iteration 12/25 | Loss: 0.00018481
Iteration 13/25 | Loss: 0.00018481
Iteration 14/25 | Loss: 0.00018481
Iteration 15/25 | Loss: 0.00018481
Iteration 16/25 | Loss: 0.00018481
Iteration 17/25 | Loss: 0.00018481
Iteration 18/25 | Loss: 0.00018481
Iteration 19/25 | Loss: 0.00018481
Iteration 20/25 | Loss: 0.00018481
Iteration 21/25 | Loss: 0.00018481
Iteration 22/25 | Loss: 0.00018481
Iteration 23/25 | Loss: 0.00018481
Iteration 24/25 | Loss: 0.00018481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0001848087558755651, 0.0001848087558755651, 0.0001848087558755651, 0.0001848087558755651, 0.0001848087558755651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001848087558755651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018481
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001471
Iteration 6/1000 | Loss: 0.00001371
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001291
Iteration 11/1000 | Loss: 0.00001290
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001260
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001228
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001225
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001217
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001215
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001214
Iteration 34/1000 | Loss: 0.00001214
Iteration 35/1000 | Loss: 0.00001214
Iteration 36/1000 | Loss: 0.00001213
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001211
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001201
Iteration 54/1000 | Loss: 0.00001201
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001196
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001190
Iteration 107/1000 | Loss: 0.00001190
Iteration 108/1000 | Loss: 0.00001190
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001189
Iteration 120/1000 | Loss: 0.00001189
Iteration 121/1000 | Loss: 0.00001189
Iteration 122/1000 | Loss: 0.00001189
Iteration 123/1000 | Loss: 0.00001189
Iteration 124/1000 | Loss: 0.00001189
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001188
Iteration 133/1000 | Loss: 0.00001188
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001187
Iteration 137/1000 | Loss: 0.00001187
Iteration 138/1000 | Loss: 0.00001187
Iteration 139/1000 | Loss: 0.00001187
Iteration 140/1000 | Loss: 0.00001187
Iteration 141/1000 | Loss: 0.00001187
Iteration 142/1000 | Loss: 0.00001187
Iteration 143/1000 | Loss: 0.00001187
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001187
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001187
Iteration 158/1000 | Loss: 0.00001187
Iteration 159/1000 | Loss: 0.00001187
Iteration 160/1000 | Loss: 0.00001187
Iteration 161/1000 | Loss: 0.00001187
Iteration 162/1000 | Loss: 0.00001187
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001187
Iteration 170/1000 | Loss: 0.00001187
Iteration 171/1000 | Loss: 0.00001187
Iteration 172/1000 | Loss: 0.00001187
Iteration 173/1000 | Loss: 0.00001187
Iteration 174/1000 | Loss: 0.00001187
Iteration 175/1000 | Loss: 0.00001187
Iteration 176/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.1869236004713457e-05, 1.1869236004713457e-05, 1.1869236004713457e-05, 1.1869236004713457e-05, 1.1869236004713457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1869236004713457e-05

Optimization complete. Final v2v error: 2.891127824783325 mm

Highest mean error: 3.0280919075012207 mm for frame 61

Lowest mean error: 2.803058385848999 mm for frame 117

Saving results

Total time: 39.818883657455444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825165
Iteration 2/25 | Loss: 0.00083870
Iteration 3/25 | Loss: 0.00070402
Iteration 4/25 | Loss: 0.00068766
Iteration 5/25 | Loss: 0.00068166
Iteration 6/25 | Loss: 0.00068076
Iteration 7/25 | Loss: 0.00068076
Iteration 8/25 | Loss: 0.00068076
Iteration 9/25 | Loss: 0.00068076
Iteration 10/25 | Loss: 0.00068076
Iteration 11/25 | Loss: 0.00068076
Iteration 12/25 | Loss: 0.00068076
Iteration 13/25 | Loss: 0.00068076
Iteration 14/25 | Loss: 0.00068076
Iteration 15/25 | Loss: 0.00068076
Iteration 16/25 | Loss: 0.00068076
Iteration 17/25 | Loss: 0.00068076
Iteration 18/25 | Loss: 0.00068076
Iteration 19/25 | Loss: 0.00068076
Iteration 20/25 | Loss: 0.00068076
Iteration 21/25 | Loss: 0.00068076
Iteration 22/25 | Loss: 0.00068076
Iteration 23/25 | Loss: 0.00068076
Iteration 24/25 | Loss: 0.00068076
Iteration 25/25 | Loss: 0.00068076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46233392
Iteration 2/25 | Loss: 0.00020514
Iteration 3/25 | Loss: 0.00020514
Iteration 4/25 | Loss: 0.00020514
Iteration 5/25 | Loss: 0.00020514
Iteration 6/25 | Loss: 0.00020514
Iteration 7/25 | Loss: 0.00020514
Iteration 8/25 | Loss: 0.00020514
Iteration 9/25 | Loss: 0.00020514
Iteration 10/25 | Loss: 0.00020514
Iteration 11/25 | Loss: 0.00020514
Iteration 12/25 | Loss: 0.00020514
Iteration 13/25 | Loss: 0.00020514
Iteration 14/25 | Loss: 0.00020514
Iteration 15/25 | Loss: 0.00020514
Iteration 16/25 | Loss: 0.00020514
Iteration 17/25 | Loss: 0.00020514
Iteration 18/25 | Loss: 0.00020514
Iteration 19/25 | Loss: 0.00020514
Iteration 20/25 | Loss: 0.00020514
Iteration 21/25 | Loss: 0.00020514
Iteration 22/25 | Loss: 0.00020514
Iteration 23/25 | Loss: 0.00020514
Iteration 24/25 | Loss: 0.00020514
Iteration 25/25 | Loss: 0.00020514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020514
Iteration 2/1000 | Loss: 0.00002079
Iteration 3/1000 | Loss: 0.00001342
Iteration 4/1000 | Loss: 0.00001192
Iteration 5/1000 | Loss: 0.00001138
Iteration 6/1000 | Loss: 0.00001076
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001034
Iteration 9/1000 | Loss: 0.00001033
Iteration 10/1000 | Loss: 0.00001027
Iteration 11/1000 | Loss: 0.00001023
Iteration 12/1000 | Loss: 0.00001018
Iteration 13/1000 | Loss: 0.00001017
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001016
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001012
Iteration 18/1000 | Loss: 0.00001012
Iteration 19/1000 | Loss: 0.00001012
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001011
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001008
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001007
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001004
Iteration 29/1000 | Loss: 0.00001000
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000998
Iteration 33/1000 | Loss: 0.00000998
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000997
Iteration 36/1000 | Loss: 0.00000997
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000995
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000993
Iteration 43/1000 | Loss: 0.00000993
Iteration 44/1000 | Loss: 0.00000993
Iteration 45/1000 | Loss: 0.00000992
Iteration 46/1000 | Loss: 0.00000992
Iteration 47/1000 | Loss: 0.00000991
Iteration 48/1000 | Loss: 0.00000991
Iteration 49/1000 | Loss: 0.00000991
Iteration 50/1000 | Loss: 0.00000990
Iteration 51/1000 | Loss: 0.00000990
Iteration 52/1000 | Loss: 0.00000990
Iteration 53/1000 | Loss: 0.00000989
Iteration 54/1000 | Loss: 0.00000988
Iteration 55/1000 | Loss: 0.00000988
Iteration 56/1000 | Loss: 0.00000987
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000986
Iteration 59/1000 | Loss: 0.00000985
Iteration 60/1000 | Loss: 0.00000985
Iteration 61/1000 | Loss: 0.00000985
Iteration 62/1000 | Loss: 0.00000985
Iteration 63/1000 | Loss: 0.00000985
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000985
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000984
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000984
Iteration 71/1000 | Loss: 0.00000984
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000983
Iteration 76/1000 | Loss: 0.00000983
Iteration 77/1000 | Loss: 0.00000983
Iteration 78/1000 | Loss: 0.00000983
Iteration 79/1000 | Loss: 0.00000983
Iteration 80/1000 | Loss: 0.00000983
Iteration 81/1000 | Loss: 0.00000982
Iteration 82/1000 | Loss: 0.00000982
Iteration 83/1000 | Loss: 0.00000982
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000982
Iteration 87/1000 | Loss: 0.00000982
Iteration 88/1000 | Loss: 0.00000982
Iteration 89/1000 | Loss: 0.00000981
Iteration 90/1000 | Loss: 0.00000981
Iteration 91/1000 | Loss: 0.00000981
Iteration 92/1000 | Loss: 0.00000981
Iteration 93/1000 | Loss: 0.00000981
Iteration 94/1000 | Loss: 0.00000981
Iteration 95/1000 | Loss: 0.00000980
Iteration 96/1000 | Loss: 0.00000980
Iteration 97/1000 | Loss: 0.00000980
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000980
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000979
Iteration 107/1000 | Loss: 0.00000979
Iteration 108/1000 | Loss: 0.00000979
Iteration 109/1000 | Loss: 0.00000979
Iteration 110/1000 | Loss: 0.00000979
Iteration 111/1000 | Loss: 0.00000978
Iteration 112/1000 | Loss: 0.00000978
Iteration 113/1000 | Loss: 0.00000978
Iteration 114/1000 | Loss: 0.00000978
Iteration 115/1000 | Loss: 0.00000978
Iteration 116/1000 | Loss: 0.00000978
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000977
Iteration 119/1000 | Loss: 0.00000977
Iteration 120/1000 | Loss: 0.00000977
Iteration 121/1000 | Loss: 0.00000977
Iteration 122/1000 | Loss: 0.00000977
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000975
Iteration 127/1000 | Loss: 0.00000975
Iteration 128/1000 | Loss: 0.00000975
Iteration 129/1000 | Loss: 0.00000974
Iteration 130/1000 | Loss: 0.00000974
Iteration 131/1000 | Loss: 0.00000974
Iteration 132/1000 | Loss: 0.00000974
Iteration 133/1000 | Loss: 0.00000973
Iteration 134/1000 | Loss: 0.00000973
Iteration 135/1000 | Loss: 0.00000973
Iteration 136/1000 | Loss: 0.00000973
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000972
Iteration 140/1000 | Loss: 0.00000972
Iteration 141/1000 | Loss: 0.00000972
Iteration 142/1000 | Loss: 0.00000972
Iteration 143/1000 | Loss: 0.00000972
Iteration 144/1000 | Loss: 0.00000972
Iteration 145/1000 | Loss: 0.00000972
Iteration 146/1000 | Loss: 0.00000972
Iteration 147/1000 | Loss: 0.00000972
Iteration 148/1000 | Loss: 0.00000972
Iteration 149/1000 | Loss: 0.00000971
Iteration 150/1000 | Loss: 0.00000971
Iteration 151/1000 | Loss: 0.00000971
Iteration 152/1000 | Loss: 0.00000971
Iteration 153/1000 | Loss: 0.00000971
Iteration 154/1000 | Loss: 0.00000971
Iteration 155/1000 | Loss: 0.00000971
Iteration 156/1000 | Loss: 0.00000971
Iteration 157/1000 | Loss: 0.00000971
Iteration 158/1000 | Loss: 0.00000971
Iteration 159/1000 | Loss: 0.00000971
Iteration 160/1000 | Loss: 0.00000970
Iteration 161/1000 | Loss: 0.00000970
Iteration 162/1000 | Loss: 0.00000970
Iteration 163/1000 | Loss: 0.00000970
Iteration 164/1000 | Loss: 0.00000970
Iteration 165/1000 | Loss: 0.00000970
Iteration 166/1000 | Loss: 0.00000970
Iteration 167/1000 | Loss: 0.00000970
Iteration 168/1000 | Loss: 0.00000970
Iteration 169/1000 | Loss: 0.00000970
Iteration 170/1000 | Loss: 0.00000970
Iteration 171/1000 | Loss: 0.00000970
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000968
Iteration 181/1000 | Loss: 0.00000968
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000968
Iteration 187/1000 | Loss: 0.00000968
Iteration 188/1000 | Loss: 0.00000968
Iteration 189/1000 | Loss: 0.00000968
Iteration 190/1000 | Loss: 0.00000968
Iteration 191/1000 | Loss: 0.00000968
Iteration 192/1000 | Loss: 0.00000968
Iteration 193/1000 | Loss: 0.00000968
Iteration 194/1000 | Loss: 0.00000968
Iteration 195/1000 | Loss: 0.00000968
Iteration 196/1000 | Loss: 0.00000968
Iteration 197/1000 | Loss: 0.00000968
Iteration 198/1000 | Loss: 0.00000968
Iteration 199/1000 | Loss: 0.00000968
Iteration 200/1000 | Loss: 0.00000968
Iteration 201/1000 | Loss: 0.00000968
Iteration 202/1000 | Loss: 0.00000968
Iteration 203/1000 | Loss: 0.00000968
Iteration 204/1000 | Loss: 0.00000968
Iteration 205/1000 | Loss: 0.00000968
Iteration 206/1000 | Loss: 0.00000968
Iteration 207/1000 | Loss: 0.00000968
Iteration 208/1000 | Loss: 0.00000968
Iteration 209/1000 | Loss: 0.00000968
Iteration 210/1000 | Loss: 0.00000968
Iteration 211/1000 | Loss: 0.00000968
Iteration 212/1000 | Loss: 0.00000968
Iteration 213/1000 | Loss: 0.00000968
Iteration 214/1000 | Loss: 0.00000968
Iteration 215/1000 | Loss: 0.00000968
Iteration 216/1000 | Loss: 0.00000968
Iteration 217/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [9.680691618996207e-06, 9.680691618996207e-06, 9.680691618996207e-06, 9.680691618996207e-06, 9.680691618996207e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.680691618996207e-06

Optimization complete. Final v2v error: 2.6381921768188477 mm

Highest mean error: 2.798820972442627 mm for frame 55

Lowest mean error: 2.5505776405334473 mm for frame 115

Saving results

Total time: 36.90690636634827
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060248
Iteration 2/25 | Loss: 0.00278474
Iteration 3/25 | Loss: 0.00174709
Iteration 4/25 | Loss: 0.00159610
Iteration 5/25 | Loss: 0.00141712
Iteration 6/25 | Loss: 0.00123875
Iteration 7/25 | Loss: 0.00120932
Iteration 8/25 | Loss: 0.00118281
Iteration 9/25 | Loss: 0.00111233
Iteration 10/25 | Loss: 0.00109863
Iteration 11/25 | Loss: 0.00109654
Iteration 12/25 | Loss: 0.00106876
Iteration 13/25 | Loss: 0.00105082
Iteration 14/25 | Loss: 0.00104830
Iteration 15/25 | Loss: 0.00104242
Iteration 16/25 | Loss: 0.00103375
Iteration 17/25 | Loss: 0.00103633
Iteration 18/25 | Loss: 0.00103526
Iteration 19/25 | Loss: 0.00102847
Iteration 20/25 | Loss: 0.00102330
Iteration 21/25 | Loss: 0.00101906
Iteration 22/25 | Loss: 0.00101734
Iteration 23/25 | Loss: 0.00101689
Iteration 24/25 | Loss: 0.00102492
Iteration 25/25 | Loss: 0.00101587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89413357
Iteration 2/25 | Loss: 0.00226368
Iteration 3/25 | Loss: 0.00226368
Iteration 4/25 | Loss: 0.00226368
Iteration 5/25 | Loss: 0.00226368
Iteration 6/25 | Loss: 0.00226368
Iteration 7/25 | Loss: 0.00226368
Iteration 8/25 | Loss: 0.00226367
Iteration 9/25 | Loss: 0.00226367
Iteration 10/25 | Loss: 0.00226367
Iteration 11/25 | Loss: 0.00226367
Iteration 12/25 | Loss: 0.00226367
Iteration 13/25 | Loss: 0.00226367
Iteration 14/25 | Loss: 0.00226367
Iteration 15/25 | Loss: 0.00226367
Iteration 16/25 | Loss: 0.00226367
Iteration 17/25 | Loss: 0.00226367
Iteration 18/25 | Loss: 0.00226367
Iteration 19/25 | Loss: 0.00226367
Iteration 20/25 | Loss: 0.00226367
Iteration 21/25 | Loss: 0.00226367
Iteration 22/25 | Loss: 0.00226367
Iteration 23/25 | Loss: 0.00226367
Iteration 24/25 | Loss: 0.00226367
Iteration 25/25 | Loss: 0.00226367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226367
Iteration 2/1000 | Loss: 0.00096960
Iteration 3/1000 | Loss: 0.00066054
Iteration 4/1000 | Loss: 0.00068352
Iteration 5/1000 | Loss: 0.00065237
Iteration 6/1000 | Loss: 0.00020924
Iteration 7/1000 | Loss: 0.00017643
Iteration 8/1000 | Loss: 0.00015169
Iteration 9/1000 | Loss: 0.00013533
Iteration 10/1000 | Loss: 0.00012577
Iteration 11/1000 | Loss: 0.00012192
Iteration 12/1000 | Loss: 0.00011841
Iteration 13/1000 | Loss: 0.00011638
Iteration 14/1000 | Loss: 0.00017828
Iteration 15/1000 | Loss: 0.00011816
Iteration 16/1000 | Loss: 0.00011399
Iteration 17/1000 | Loss: 0.00073124
Iteration 18/1000 | Loss: 0.00067506
Iteration 19/1000 | Loss: 0.00120051
Iteration 20/1000 | Loss: 0.00070352
Iteration 21/1000 | Loss: 0.00026273
Iteration 22/1000 | Loss: 0.00012170
Iteration 23/1000 | Loss: 0.00011045
Iteration 24/1000 | Loss: 0.00086544
Iteration 25/1000 | Loss: 0.00115067
Iteration 26/1000 | Loss: 0.00013906
Iteration 27/1000 | Loss: 0.00068441
Iteration 28/1000 | Loss: 0.00044759
Iteration 29/1000 | Loss: 0.00011454
Iteration 30/1000 | Loss: 0.00065287
Iteration 31/1000 | Loss: 0.00019161
Iteration 32/1000 | Loss: 0.00013905
Iteration 33/1000 | Loss: 0.00012501
Iteration 34/1000 | Loss: 0.00011716
Iteration 35/1000 | Loss: 0.00042239
Iteration 36/1000 | Loss: 0.00040882
Iteration 37/1000 | Loss: 0.00076110
Iteration 38/1000 | Loss: 0.00022841
Iteration 39/1000 | Loss: 0.00047945
Iteration 40/1000 | Loss: 0.00048639
Iteration 41/1000 | Loss: 0.00068659
Iteration 42/1000 | Loss: 0.00031616
Iteration 43/1000 | Loss: 0.00011900
Iteration 44/1000 | Loss: 0.00010946
Iteration 45/1000 | Loss: 0.00010541
Iteration 46/1000 | Loss: 0.00053058
Iteration 47/1000 | Loss: 0.00030959
Iteration 48/1000 | Loss: 0.00040819
Iteration 49/1000 | Loss: 0.00057270
Iteration 50/1000 | Loss: 0.00049226
Iteration 51/1000 | Loss: 0.00113734
Iteration 52/1000 | Loss: 0.00032480
Iteration 53/1000 | Loss: 0.00046729
Iteration 54/1000 | Loss: 0.00050518
Iteration 55/1000 | Loss: 0.00025764
Iteration 56/1000 | Loss: 0.00047721
Iteration 57/1000 | Loss: 0.00088317
Iteration 58/1000 | Loss: 0.00042017
Iteration 59/1000 | Loss: 0.00020496
Iteration 60/1000 | Loss: 0.00045718
Iteration 61/1000 | Loss: 0.00020505
Iteration 62/1000 | Loss: 0.00044198
Iteration 63/1000 | Loss: 0.00023109
Iteration 64/1000 | Loss: 0.00027325
Iteration 65/1000 | Loss: 0.00021264
Iteration 66/1000 | Loss: 0.00019291
Iteration 67/1000 | Loss: 0.00047631
Iteration 68/1000 | Loss: 0.00013736
Iteration 69/1000 | Loss: 0.00023728
Iteration 70/1000 | Loss: 0.00040848
Iteration 71/1000 | Loss: 0.00049131
Iteration 72/1000 | Loss: 0.00016798
Iteration 73/1000 | Loss: 0.00048682
Iteration 74/1000 | Loss: 0.00078070
Iteration 75/1000 | Loss: 0.00043993
Iteration 76/1000 | Loss: 0.00026839
Iteration 77/1000 | Loss: 0.00018846
Iteration 78/1000 | Loss: 0.00009791
Iteration 79/1000 | Loss: 0.00009699
Iteration 80/1000 | Loss: 0.00021419
Iteration 81/1000 | Loss: 0.00043368
Iteration 82/1000 | Loss: 0.00046483
Iteration 83/1000 | Loss: 0.00046308
Iteration 84/1000 | Loss: 0.00011075
Iteration 85/1000 | Loss: 0.00025283
Iteration 86/1000 | Loss: 0.00010130
Iteration 87/1000 | Loss: 0.00009796
Iteration 88/1000 | Loss: 0.00026383
Iteration 89/1000 | Loss: 0.00022046
Iteration 90/1000 | Loss: 0.00021198
Iteration 91/1000 | Loss: 0.00039670
Iteration 92/1000 | Loss: 0.00023566
Iteration 93/1000 | Loss: 0.00037328
Iteration 94/1000 | Loss: 0.00049267
Iteration 95/1000 | Loss: 0.00065046
Iteration 96/1000 | Loss: 0.00036716
Iteration 97/1000 | Loss: 0.00080032
Iteration 98/1000 | Loss: 0.00020035
Iteration 99/1000 | Loss: 0.00026375
Iteration 100/1000 | Loss: 0.00062898
Iteration 101/1000 | Loss: 0.00015932
Iteration 102/1000 | Loss: 0.00027712
Iteration 103/1000 | Loss: 0.00021594
Iteration 104/1000 | Loss: 0.00020460
Iteration 105/1000 | Loss: 0.00039124
Iteration 106/1000 | Loss: 0.00068623
Iteration 107/1000 | Loss: 0.00050161
Iteration 108/1000 | Loss: 0.00034071
Iteration 109/1000 | Loss: 0.00038796
Iteration 110/1000 | Loss: 0.00034058
Iteration 111/1000 | Loss: 0.00050818
Iteration 112/1000 | Loss: 0.00038817
Iteration 113/1000 | Loss: 0.00010249
Iteration 114/1000 | Loss: 0.00009905
Iteration 115/1000 | Loss: 0.00027333
Iteration 116/1000 | Loss: 0.00048323
Iteration 117/1000 | Loss: 0.00055141
Iteration 118/1000 | Loss: 0.00012933
Iteration 119/1000 | Loss: 0.00010251
Iteration 120/1000 | Loss: 0.00009774
Iteration 121/1000 | Loss: 0.00009565
Iteration 122/1000 | Loss: 0.00068998
Iteration 123/1000 | Loss: 0.00027159
Iteration 124/1000 | Loss: 0.00049148
Iteration 125/1000 | Loss: 0.00011951
Iteration 126/1000 | Loss: 0.00010121
Iteration 127/1000 | Loss: 0.00009834
Iteration 128/1000 | Loss: 0.00009638
Iteration 129/1000 | Loss: 0.00009562
Iteration 130/1000 | Loss: 0.00009508
Iteration 131/1000 | Loss: 0.00009472
Iteration 132/1000 | Loss: 0.00009424
Iteration 133/1000 | Loss: 0.00009388
Iteration 134/1000 | Loss: 0.00107421
Iteration 135/1000 | Loss: 0.00474445
Iteration 136/1000 | Loss: 0.00655500
Iteration 137/1000 | Loss: 0.00955569
Iteration 138/1000 | Loss: 0.00468684
Iteration 139/1000 | Loss: 0.00348405
Iteration 140/1000 | Loss: 0.00542767
Iteration 141/1000 | Loss: 0.00157347
Iteration 142/1000 | Loss: 0.00145443
Iteration 143/1000 | Loss: 0.00044643
Iteration 144/1000 | Loss: 0.00359720
Iteration 145/1000 | Loss: 0.00123856
Iteration 146/1000 | Loss: 0.00174644
Iteration 147/1000 | Loss: 0.00567837
Iteration 148/1000 | Loss: 0.00197193
Iteration 149/1000 | Loss: 0.00174724
Iteration 150/1000 | Loss: 0.00196567
Iteration 151/1000 | Loss: 0.00227824
Iteration 152/1000 | Loss: 0.00238971
Iteration 153/1000 | Loss: 0.00188435
Iteration 154/1000 | Loss: 0.00105565
Iteration 155/1000 | Loss: 0.00057390
Iteration 156/1000 | Loss: 0.00127519
Iteration 157/1000 | Loss: 0.00132664
Iteration 158/1000 | Loss: 0.00164569
Iteration 159/1000 | Loss: 0.00317033
Iteration 160/1000 | Loss: 0.00169809
Iteration 161/1000 | Loss: 0.00025916
Iteration 162/1000 | Loss: 0.00121678
Iteration 163/1000 | Loss: 0.00057083
Iteration 164/1000 | Loss: 0.00012336
Iteration 165/1000 | Loss: 0.00033698
Iteration 166/1000 | Loss: 0.00010511
Iteration 167/1000 | Loss: 0.00009937
Iteration 168/1000 | Loss: 0.00113572
Iteration 169/1000 | Loss: 0.00044913
Iteration 170/1000 | Loss: 0.00035945
Iteration 171/1000 | Loss: 0.00086941
Iteration 172/1000 | Loss: 0.00067629
Iteration 173/1000 | Loss: 0.00060817
Iteration 174/1000 | Loss: 0.00078653
Iteration 175/1000 | Loss: 0.00029753
Iteration 176/1000 | Loss: 0.00068802
Iteration 177/1000 | Loss: 0.00116367
Iteration 178/1000 | Loss: 0.00097505
Iteration 179/1000 | Loss: 0.00124660
Iteration 180/1000 | Loss: 0.00124612
Iteration 181/1000 | Loss: 0.00083709
Iteration 182/1000 | Loss: 0.00081163
Iteration 183/1000 | Loss: 0.00052526
Iteration 184/1000 | Loss: 0.00016765
Iteration 185/1000 | Loss: 0.00065603
Iteration 186/1000 | Loss: 0.00022448
Iteration 187/1000 | Loss: 0.00018388
Iteration 188/1000 | Loss: 0.00120482
Iteration 189/1000 | Loss: 0.00045056
Iteration 190/1000 | Loss: 0.00084625
Iteration 191/1000 | Loss: 0.00072554
Iteration 192/1000 | Loss: 0.00052137
Iteration 193/1000 | Loss: 0.00037085
Iteration 194/1000 | Loss: 0.00135612
Iteration 195/1000 | Loss: 0.00088558
Iteration 196/1000 | Loss: 0.00033733
Iteration 197/1000 | Loss: 0.00049197
Iteration 198/1000 | Loss: 0.00029456
Iteration 199/1000 | Loss: 0.00043387
Iteration 200/1000 | Loss: 0.00046455
Iteration 201/1000 | Loss: 0.00048684
Iteration 202/1000 | Loss: 0.00068469
Iteration 203/1000 | Loss: 0.00023629
Iteration 204/1000 | Loss: 0.00037405
Iteration 205/1000 | Loss: 0.00021162
Iteration 206/1000 | Loss: 0.00031535
Iteration 207/1000 | Loss: 0.00036431
Iteration 208/1000 | Loss: 0.00008100
Iteration 209/1000 | Loss: 0.00007336
Iteration 210/1000 | Loss: 0.00034561
Iteration 211/1000 | Loss: 0.00046824
Iteration 212/1000 | Loss: 0.00061942
Iteration 213/1000 | Loss: 0.00046388
Iteration 214/1000 | Loss: 0.00033050
Iteration 215/1000 | Loss: 0.00032162
Iteration 216/1000 | Loss: 0.00039340
Iteration 217/1000 | Loss: 0.00071407
Iteration 218/1000 | Loss: 0.00103898
Iteration 219/1000 | Loss: 0.00074125
Iteration 220/1000 | Loss: 0.00034021
Iteration 221/1000 | Loss: 0.00027688
Iteration 222/1000 | Loss: 0.00008923
Iteration 223/1000 | Loss: 0.00025869
Iteration 224/1000 | Loss: 0.00026074
Iteration 225/1000 | Loss: 0.00009661
Iteration 226/1000 | Loss: 0.00025105
Iteration 227/1000 | Loss: 0.00122646
Iteration 228/1000 | Loss: 0.00054521
Iteration 229/1000 | Loss: 0.00006638
Iteration 230/1000 | Loss: 0.00005678
Iteration 231/1000 | Loss: 0.00005227
Iteration 232/1000 | Loss: 0.00004927
Iteration 233/1000 | Loss: 0.00004703
Iteration 234/1000 | Loss: 0.00018245
Iteration 235/1000 | Loss: 0.00004592
Iteration 236/1000 | Loss: 0.00004529
Iteration 237/1000 | Loss: 0.00004463
Iteration 238/1000 | Loss: 0.00038201
Iteration 239/1000 | Loss: 0.00029258
Iteration 240/1000 | Loss: 0.00037152
Iteration 241/1000 | Loss: 0.00022999
Iteration 242/1000 | Loss: 0.00026826
Iteration 243/1000 | Loss: 0.00004533
Iteration 244/1000 | Loss: 0.00004437
Iteration 245/1000 | Loss: 0.00004388
Iteration 246/1000 | Loss: 0.00069829
Iteration 247/1000 | Loss: 0.00034904
Iteration 248/1000 | Loss: 0.00026553
Iteration 249/1000 | Loss: 0.00037076
Iteration 250/1000 | Loss: 0.00023567
Iteration 251/1000 | Loss: 0.00007287
Iteration 252/1000 | Loss: 0.00005511
Iteration 253/1000 | Loss: 0.00004638
Iteration 254/1000 | Loss: 0.00004337
Iteration 255/1000 | Loss: 0.00033791
Iteration 256/1000 | Loss: 0.00040107
Iteration 257/1000 | Loss: 0.00048540
Iteration 258/1000 | Loss: 0.00050446
Iteration 259/1000 | Loss: 0.00036584
Iteration 260/1000 | Loss: 0.00029670
Iteration 261/1000 | Loss: 0.00035755
Iteration 262/1000 | Loss: 0.00048258
Iteration 263/1000 | Loss: 0.00028771
Iteration 264/1000 | Loss: 0.00039287
Iteration 265/1000 | Loss: 0.00034698
Iteration 266/1000 | Loss: 0.00007118
Iteration 267/1000 | Loss: 0.00038898
Iteration 268/1000 | Loss: 0.00008294
Iteration 269/1000 | Loss: 0.00006076
Iteration 270/1000 | Loss: 0.00033686
Iteration 271/1000 | Loss: 0.00004841
Iteration 272/1000 | Loss: 0.00036231
Iteration 273/1000 | Loss: 0.00030709
Iteration 274/1000 | Loss: 0.00045073
Iteration 275/1000 | Loss: 0.00038777
Iteration 276/1000 | Loss: 0.00006722
Iteration 277/1000 | Loss: 0.00006001
Iteration 278/1000 | Loss: 0.00005471
Iteration 279/1000 | Loss: 0.00005143
Iteration 280/1000 | Loss: 0.00034837
Iteration 281/1000 | Loss: 0.00004937
Iteration 282/1000 | Loss: 0.00008138
Iteration 283/1000 | Loss: 0.00004445
Iteration 284/1000 | Loss: 0.00004265
Iteration 285/1000 | Loss: 0.00004113
Iteration 286/1000 | Loss: 0.00004007
Iteration 287/1000 | Loss: 0.00003958
Iteration 288/1000 | Loss: 0.00003923
Iteration 289/1000 | Loss: 0.00003885
Iteration 290/1000 | Loss: 0.00003862
Iteration 291/1000 | Loss: 0.00003858
Iteration 292/1000 | Loss: 0.00003853
Iteration 293/1000 | Loss: 0.00003853
Iteration 294/1000 | Loss: 0.00003853
Iteration 295/1000 | Loss: 0.00003852
Iteration 296/1000 | Loss: 0.00003848
Iteration 297/1000 | Loss: 0.00003844
Iteration 298/1000 | Loss: 0.00003843
Iteration 299/1000 | Loss: 0.00003843
Iteration 300/1000 | Loss: 0.00003843
Iteration 301/1000 | Loss: 0.00003843
Iteration 302/1000 | Loss: 0.00003843
Iteration 303/1000 | Loss: 0.00003843
Iteration 304/1000 | Loss: 0.00003843
Iteration 305/1000 | Loss: 0.00003843
Iteration 306/1000 | Loss: 0.00003843
Iteration 307/1000 | Loss: 0.00003843
Iteration 308/1000 | Loss: 0.00003843
Iteration 309/1000 | Loss: 0.00003842
Iteration 310/1000 | Loss: 0.00003842
Iteration 311/1000 | Loss: 0.00003836
Iteration 312/1000 | Loss: 0.00003836
Iteration 313/1000 | Loss: 0.00003836
Iteration 314/1000 | Loss: 0.00003834
Iteration 315/1000 | Loss: 0.00003832
Iteration 316/1000 | Loss: 0.00003831
Iteration 317/1000 | Loss: 0.00003831
Iteration 318/1000 | Loss: 0.00003826
Iteration 319/1000 | Loss: 0.00003822
Iteration 320/1000 | Loss: 0.00003822
Iteration 321/1000 | Loss: 0.00003820
Iteration 322/1000 | Loss: 0.00003819
Iteration 323/1000 | Loss: 0.00003819
Iteration 324/1000 | Loss: 0.00003819
Iteration 325/1000 | Loss: 0.00003819
Iteration 326/1000 | Loss: 0.00003819
Iteration 327/1000 | Loss: 0.00003819
Iteration 328/1000 | Loss: 0.00003819
Iteration 329/1000 | Loss: 0.00003819
Iteration 330/1000 | Loss: 0.00003818
Iteration 331/1000 | Loss: 0.00003818
Iteration 332/1000 | Loss: 0.00003818
Iteration 333/1000 | Loss: 0.00003818
Iteration 334/1000 | Loss: 0.00003818
Iteration 335/1000 | Loss: 0.00003818
Iteration 336/1000 | Loss: 0.00003818
Iteration 337/1000 | Loss: 0.00003818
Iteration 338/1000 | Loss: 0.00003818
Iteration 339/1000 | Loss: 0.00003818
Iteration 340/1000 | Loss: 0.00003818
Iteration 341/1000 | Loss: 0.00003818
Iteration 342/1000 | Loss: 0.00003817
Iteration 343/1000 | Loss: 0.00003817
Iteration 344/1000 | Loss: 0.00003816
Iteration 345/1000 | Loss: 0.00003813
Iteration 346/1000 | Loss: 0.00003812
Iteration 347/1000 | Loss: 0.00003812
Iteration 348/1000 | Loss: 0.00003812
Iteration 349/1000 | Loss: 0.00003811
Iteration 350/1000 | Loss: 0.00003810
Iteration 351/1000 | Loss: 0.00003810
Iteration 352/1000 | Loss: 0.00003809
Iteration 353/1000 | Loss: 0.00003809
Iteration 354/1000 | Loss: 0.00003809
Iteration 355/1000 | Loss: 0.00003809
Iteration 356/1000 | Loss: 0.00003809
Iteration 357/1000 | Loss: 0.00003809
Iteration 358/1000 | Loss: 0.00003809
Iteration 359/1000 | Loss: 0.00003809
Iteration 360/1000 | Loss: 0.00003809
Iteration 361/1000 | Loss: 0.00003809
Iteration 362/1000 | Loss: 0.00003809
Iteration 363/1000 | Loss: 0.00003809
Iteration 364/1000 | Loss: 0.00003808
Iteration 365/1000 | Loss: 0.00003808
Iteration 366/1000 | Loss: 0.00003808
Iteration 367/1000 | Loss: 0.00003808
Iteration 368/1000 | Loss: 0.00003808
Iteration 369/1000 | Loss: 0.00003808
Iteration 370/1000 | Loss: 0.00003808
Iteration 371/1000 | Loss: 0.00003808
Iteration 372/1000 | Loss: 0.00003808
Iteration 373/1000 | Loss: 0.00003808
Iteration 374/1000 | Loss: 0.00003808
Iteration 375/1000 | Loss: 0.00003808
Iteration 376/1000 | Loss: 0.00003808
Iteration 377/1000 | Loss: 0.00003807
Iteration 378/1000 | Loss: 0.00003807
Iteration 379/1000 | Loss: 0.00003807
Iteration 380/1000 | Loss: 0.00003806
Iteration 381/1000 | Loss: 0.00003806
Iteration 382/1000 | Loss: 0.00003805
Iteration 383/1000 | Loss: 0.00003805
Iteration 384/1000 | Loss: 0.00003805
Iteration 385/1000 | Loss: 0.00003805
Iteration 386/1000 | Loss: 0.00003804
Iteration 387/1000 | Loss: 0.00003804
Iteration 388/1000 | Loss: 0.00003804
Iteration 389/1000 | Loss: 0.00003804
Iteration 390/1000 | Loss: 0.00003801
Iteration 391/1000 | Loss: 0.00003801
Iteration 392/1000 | Loss: 0.00003801
Iteration 393/1000 | Loss: 0.00003800
Iteration 394/1000 | Loss: 0.00003799
Iteration 395/1000 | Loss: 0.00003799
Iteration 396/1000 | Loss: 0.00003799
Iteration 397/1000 | Loss: 0.00003799
Iteration 398/1000 | Loss: 0.00003799
Iteration 399/1000 | Loss: 0.00003799
Iteration 400/1000 | Loss: 0.00003799
Iteration 401/1000 | Loss: 0.00003798
Iteration 402/1000 | Loss: 0.00003798
Iteration 403/1000 | Loss: 0.00003798
Iteration 404/1000 | Loss: 0.00003798
Iteration 405/1000 | Loss: 0.00003798
Iteration 406/1000 | Loss: 0.00003797
Iteration 407/1000 | Loss: 0.00003797
Iteration 408/1000 | Loss: 0.00003797
Iteration 409/1000 | Loss: 0.00003796
Iteration 410/1000 | Loss: 0.00003796
Iteration 411/1000 | Loss: 0.00003796
Iteration 412/1000 | Loss: 0.00003796
Iteration 413/1000 | Loss: 0.00003796
Iteration 414/1000 | Loss: 0.00003796
Iteration 415/1000 | Loss: 0.00003796
Iteration 416/1000 | Loss: 0.00003796
Iteration 417/1000 | Loss: 0.00003796
Iteration 418/1000 | Loss: 0.00003796
Iteration 419/1000 | Loss: 0.00003796
Iteration 420/1000 | Loss: 0.00003795
Iteration 421/1000 | Loss: 0.00003795
Iteration 422/1000 | Loss: 0.00003795
Iteration 423/1000 | Loss: 0.00003795
Iteration 424/1000 | Loss: 0.00003794
Iteration 425/1000 | Loss: 0.00003794
Iteration 426/1000 | Loss: 0.00003794
Iteration 427/1000 | Loss: 0.00003794
Iteration 428/1000 | Loss: 0.00003793
Iteration 429/1000 | Loss: 0.00003793
Iteration 430/1000 | Loss: 0.00003793
Iteration 431/1000 | Loss: 0.00003793
Iteration 432/1000 | Loss: 0.00003793
Iteration 433/1000 | Loss: 0.00003793
Iteration 434/1000 | Loss: 0.00003793
Iteration 435/1000 | Loss: 0.00003792
Iteration 436/1000 | Loss: 0.00003792
Iteration 437/1000 | Loss: 0.00003792
Iteration 438/1000 | Loss: 0.00003792
Iteration 439/1000 | Loss: 0.00003792
Iteration 440/1000 | Loss: 0.00003792
Iteration 441/1000 | Loss: 0.00003792
Iteration 442/1000 | Loss: 0.00003792
Iteration 443/1000 | Loss: 0.00003792
Iteration 444/1000 | Loss: 0.00003792
Iteration 445/1000 | Loss: 0.00003792
Iteration 446/1000 | Loss: 0.00003792
Iteration 447/1000 | Loss: 0.00003792
Iteration 448/1000 | Loss: 0.00003791
Iteration 449/1000 | Loss: 0.00003791
Iteration 450/1000 | Loss: 0.00003791
Iteration 451/1000 | Loss: 0.00003791
Iteration 452/1000 | Loss: 0.00003791
Iteration 453/1000 | Loss: 0.00003791
Iteration 454/1000 | Loss: 0.00003791
Iteration 455/1000 | Loss: 0.00003791
Iteration 456/1000 | Loss: 0.00003791
Iteration 457/1000 | Loss: 0.00003791
Iteration 458/1000 | Loss: 0.00003790
Iteration 459/1000 | Loss: 0.00003790
Iteration 460/1000 | Loss: 0.00003790
Iteration 461/1000 | Loss: 0.00003790
Iteration 462/1000 | Loss: 0.00003790
Iteration 463/1000 | Loss: 0.00003789
Iteration 464/1000 | Loss: 0.00003789
Iteration 465/1000 | Loss: 0.00003789
Iteration 466/1000 | Loss: 0.00003789
Iteration 467/1000 | Loss: 0.00003789
Iteration 468/1000 | Loss: 0.00003789
Iteration 469/1000 | Loss: 0.00003789
Iteration 470/1000 | Loss: 0.00003789
Iteration 471/1000 | Loss: 0.00003788
Iteration 472/1000 | Loss: 0.00003788
Iteration 473/1000 | Loss: 0.00003788
Iteration 474/1000 | Loss: 0.00003788
Iteration 475/1000 | Loss: 0.00003788
Iteration 476/1000 | Loss: 0.00003788
Iteration 477/1000 | Loss: 0.00003788
Iteration 478/1000 | Loss: 0.00003788
Iteration 479/1000 | Loss: 0.00003788
Iteration 480/1000 | Loss: 0.00003788
Iteration 481/1000 | Loss: 0.00003788
Iteration 482/1000 | Loss: 0.00003788
Iteration 483/1000 | Loss: 0.00003788
Iteration 484/1000 | Loss: 0.00003788
Iteration 485/1000 | Loss: 0.00003787
Iteration 486/1000 | Loss: 0.00003787
Iteration 487/1000 | Loss: 0.00003787
Iteration 488/1000 | Loss: 0.00003787
Iteration 489/1000 | Loss: 0.00003787
Iteration 490/1000 | Loss: 0.00003787
Iteration 491/1000 | Loss: 0.00003787
Iteration 492/1000 | Loss: 0.00003787
Iteration 493/1000 | Loss: 0.00003787
Iteration 494/1000 | Loss: 0.00003787
Iteration 495/1000 | Loss: 0.00003787
Iteration 496/1000 | Loss: 0.00003787
Iteration 497/1000 | Loss: 0.00003787
Iteration 498/1000 | Loss: 0.00003787
Iteration 499/1000 | Loss: 0.00003787
Iteration 500/1000 | Loss: 0.00003787
Iteration 501/1000 | Loss: 0.00003787
Iteration 502/1000 | Loss: 0.00003787
Iteration 503/1000 | Loss: 0.00003787
Iteration 504/1000 | Loss: 0.00003787
Iteration 505/1000 | Loss: 0.00003787
Iteration 506/1000 | Loss: 0.00003787
Iteration 507/1000 | Loss: 0.00003787
Iteration 508/1000 | Loss: 0.00003787
Iteration 509/1000 | Loss: 0.00003787
Iteration 510/1000 | Loss: 0.00003787
Iteration 511/1000 | Loss: 0.00003787
Iteration 512/1000 | Loss: 0.00003787
Iteration 513/1000 | Loss: 0.00003787
Iteration 514/1000 | Loss: 0.00003787
Iteration 515/1000 | Loss: 0.00003787
Iteration 516/1000 | Loss: 0.00003787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 516. Stopping optimization.
Last 5 losses: [3.7867903301957995e-05, 3.7867903301957995e-05, 3.7867903301957995e-05, 3.7867903301957995e-05, 3.7867903301957995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7867903301957995e-05

Optimization complete. Final v2v error: 3.9806878566741943 mm

Highest mean error: 12.235407829284668 mm for frame 52

Lowest mean error: 3.0437171459198 mm for frame 6

Saving results

Total time: 473.5509765148163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00697146
Iteration 2/25 | Loss: 0.00117811
Iteration 3/25 | Loss: 0.00093419
Iteration 4/25 | Loss: 0.00087903
Iteration 5/25 | Loss: 0.00085159
Iteration 6/25 | Loss: 0.00084154
Iteration 7/25 | Loss: 0.00084421
Iteration 8/25 | Loss: 0.00084013
Iteration 9/25 | Loss: 0.00086228
Iteration 10/25 | Loss: 0.00083218
Iteration 11/25 | Loss: 0.00082553
Iteration 12/25 | Loss: 0.00082461
Iteration 13/25 | Loss: 0.00082452
Iteration 14/25 | Loss: 0.00082452
Iteration 15/25 | Loss: 0.00082452
Iteration 16/25 | Loss: 0.00082450
Iteration 17/25 | Loss: 0.00082450
Iteration 18/25 | Loss: 0.00082449
Iteration 19/25 | Loss: 0.00082449
Iteration 20/25 | Loss: 0.00082449
Iteration 21/25 | Loss: 0.00082449
Iteration 22/25 | Loss: 0.00082449
Iteration 23/25 | Loss: 0.00082448
Iteration 24/25 | Loss: 0.00082448
Iteration 25/25 | Loss: 0.00082447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50522113
Iteration 2/25 | Loss: 0.00037779
Iteration 3/25 | Loss: 0.00037778
Iteration 4/25 | Loss: 0.00037778
Iteration 5/25 | Loss: 0.00037777
Iteration 6/25 | Loss: 0.00037777
Iteration 7/25 | Loss: 0.00037777
Iteration 8/25 | Loss: 0.00037777
Iteration 9/25 | Loss: 0.00037777
Iteration 10/25 | Loss: 0.00037777
Iteration 11/25 | Loss: 0.00037777
Iteration 12/25 | Loss: 0.00037777
Iteration 13/25 | Loss: 0.00037777
Iteration 14/25 | Loss: 0.00037777
Iteration 15/25 | Loss: 0.00037777
Iteration 16/25 | Loss: 0.00037777
Iteration 17/25 | Loss: 0.00037777
Iteration 18/25 | Loss: 0.00037777
Iteration 19/25 | Loss: 0.00037777
Iteration 20/25 | Loss: 0.00037777
Iteration 21/25 | Loss: 0.00037777
Iteration 22/25 | Loss: 0.00037777
Iteration 23/25 | Loss: 0.00037777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00037777310353703797, 0.00037777310353703797, 0.00037777310353703797, 0.00037777310353703797, 0.00037777310353703797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037777310353703797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037777
Iteration 2/1000 | Loss: 0.00005411
Iteration 3/1000 | Loss: 0.00003569
Iteration 4/1000 | Loss: 0.00002876
Iteration 5/1000 | Loss: 0.00002749
Iteration 6/1000 | Loss: 0.00002644
Iteration 7/1000 | Loss: 0.00002590
Iteration 8/1000 | Loss: 0.00002555
Iteration 9/1000 | Loss: 0.00002528
Iteration 10/1000 | Loss: 0.00002495
Iteration 11/1000 | Loss: 0.00002472
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002436
Iteration 14/1000 | Loss: 0.00002426
Iteration 15/1000 | Loss: 0.00002418
Iteration 16/1000 | Loss: 0.00002417
Iteration 17/1000 | Loss: 0.00002416
Iteration 18/1000 | Loss: 0.00002412
Iteration 19/1000 | Loss: 0.00002409
Iteration 20/1000 | Loss: 0.00002408
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002407
Iteration 23/1000 | Loss: 0.00002406
Iteration 24/1000 | Loss: 0.00002406
Iteration 25/1000 | Loss: 0.00002406
Iteration 26/1000 | Loss: 0.00002405
Iteration 27/1000 | Loss: 0.00002405
Iteration 28/1000 | Loss: 0.00002404
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00002403
Iteration 31/1000 | Loss: 0.00002402
Iteration 32/1000 | Loss: 0.00002402
Iteration 33/1000 | Loss: 0.00002401
Iteration 34/1000 | Loss: 0.00002401
Iteration 35/1000 | Loss: 0.00002401
Iteration 36/1000 | Loss: 0.00002400
Iteration 37/1000 | Loss: 0.00002400
Iteration 38/1000 | Loss: 0.00002400
Iteration 39/1000 | Loss: 0.00002399
Iteration 40/1000 | Loss: 0.00002399
Iteration 41/1000 | Loss: 0.00002398
Iteration 42/1000 | Loss: 0.00002397
Iteration 43/1000 | Loss: 0.00002397
Iteration 44/1000 | Loss: 0.00002397
Iteration 45/1000 | Loss: 0.00002397
Iteration 46/1000 | Loss: 0.00002396
Iteration 47/1000 | Loss: 0.00002396
Iteration 48/1000 | Loss: 0.00002396
Iteration 49/1000 | Loss: 0.00002396
Iteration 50/1000 | Loss: 0.00002396
Iteration 51/1000 | Loss: 0.00002395
Iteration 52/1000 | Loss: 0.00002395
Iteration 53/1000 | Loss: 0.00002395
Iteration 54/1000 | Loss: 0.00002395
Iteration 55/1000 | Loss: 0.00002395
Iteration 56/1000 | Loss: 0.00002394
Iteration 57/1000 | Loss: 0.00002394
Iteration 58/1000 | Loss: 0.00002394
Iteration 59/1000 | Loss: 0.00002394
Iteration 60/1000 | Loss: 0.00002394
Iteration 61/1000 | Loss: 0.00002394
Iteration 62/1000 | Loss: 0.00002394
Iteration 63/1000 | Loss: 0.00002393
Iteration 64/1000 | Loss: 0.00002393
Iteration 65/1000 | Loss: 0.00002393
Iteration 66/1000 | Loss: 0.00002393
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00002393
Iteration 69/1000 | Loss: 0.00002392
Iteration 70/1000 | Loss: 0.00002392
Iteration 71/1000 | Loss: 0.00002392
Iteration 72/1000 | Loss: 0.00002392
Iteration 73/1000 | Loss: 0.00002392
Iteration 74/1000 | Loss: 0.00002392
Iteration 75/1000 | Loss: 0.00002392
Iteration 76/1000 | Loss: 0.00002392
Iteration 77/1000 | Loss: 0.00002392
Iteration 78/1000 | Loss: 0.00002392
Iteration 79/1000 | Loss: 0.00002392
Iteration 80/1000 | Loss: 0.00002392
Iteration 81/1000 | Loss: 0.00002392
Iteration 82/1000 | Loss: 0.00002392
Iteration 83/1000 | Loss: 0.00002392
Iteration 84/1000 | Loss: 0.00002392
Iteration 85/1000 | Loss: 0.00002392
Iteration 86/1000 | Loss: 0.00002392
Iteration 87/1000 | Loss: 0.00002392
Iteration 88/1000 | Loss: 0.00002392
Iteration 89/1000 | Loss: 0.00002392
Iteration 90/1000 | Loss: 0.00002392
Iteration 91/1000 | Loss: 0.00002392
Iteration 92/1000 | Loss: 0.00002392
Iteration 93/1000 | Loss: 0.00002392
Iteration 94/1000 | Loss: 0.00002392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.391582529526204e-05, 2.391582529526204e-05, 2.391582529526204e-05, 2.391582529526204e-05, 2.391582529526204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.391582529526204e-05

Optimization complete. Final v2v error: 4.1250081062316895 mm

Highest mean error: 4.648428916931152 mm for frame 122

Lowest mean error: 3.0734124183654785 mm for frame 180

Saving results

Total time: 51.39090085029602
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034692
Iteration 2/25 | Loss: 0.00206655
Iteration 3/25 | Loss: 0.00168926
Iteration 4/25 | Loss: 0.00171549
Iteration 5/25 | Loss: 0.00130628
Iteration 6/25 | Loss: 0.00126805
Iteration 7/25 | Loss: 0.00127201
Iteration 8/25 | Loss: 0.00105624
Iteration 9/25 | Loss: 0.00091422
Iteration 10/25 | Loss: 0.00088876
Iteration 11/25 | Loss: 0.00086924
Iteration 12/25 | Loss: 0.00086340
Iteration 13/25 | Loss: 0.00086564
Iteration 14/25 | Loss: 0.00086657
Iteration 15/25 | Loss: 0.00086216
Iteration 16/25 | Loss: 0.00086005
Iteration 17/25 | Loss: 0.00085953
Iteration 18/25 | Loss: 0.00085941
Iteration 19/25 | Loss: 0.00085931
Iteration 20/25 | Loss: 0.00085930
Iteration 21/25 | Loss: 0.00085930
Iteration 22/25 | Loss: 0.00085930
Iteration 23/25 | Loss: 0.00085930
Iteration 24/25 | Loss: 0.00085930
Iteration 25/25 | Loss: 0.00085929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45359337
Iteration 2/25 | Loss: 0.00085336
Iteration 3/25 | Loss: 0.00085336
Iteration 4/25 | Loss: 0.00085336
Iteration 5/25 | Loss: 0.00085336
Iteration 6/25 | Loss: 0.00085336
Iteration 7/25 | Loss: 0.00085335
Iteration 8/25 | Loss: 0.00085335
Iteration 9/25 | Loss: 0.00085335
Iteration 10/25 | Loss: 0.00085335
Iteration 11/25 | Loss: 0.00085335
Iteration 12/25 | Loss: 0.00085335
Iteration 13/25 | Loss: 0.00085335
Iteration 14/25 | Loss: 0.00085335
Iteration 15/25 | Loss: 0.00085335
Iteration 16/25 | Loss: 0.00085335
Iteration 17/25 | Loss: 0.00085335
Iteration 18/25 | Loss: 0.00085335
Iteration 19/25 | Loss: 0.00085335
Iteration 20/25 | Loss: 0.00085335
Iteration 21/25 | Loss: 0.00085335
Iteration 22/25 | Loss: 0.00085335
Iteration 23/25 | Loss: 0.00085335
Iteration 24/25 | Loss: 0.00085335
Iteration 25/25 | Loss: 0.00085335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085335
Iteration 2/1000 | Loss: 0.00006570
Iteration 3/1000 | Loss: 0.00004294
Iteration 4/1000 | Loss: 0.00003828
Iteration 5/1000 | Loss: 0.00003642
Iteration 6/1000 | Loss: 0.00003445
Iteration 7/1000 | Loss: 0.00003341
Iteration 8/1000 | Loss: 0.00003267
Iteration 9/1000 | Loss: 0.00020535
Iteration 10/1000 | Loss: 0.00080538
Iteration 11/1000 | Loss: 0.00006949
Iteration 12/1000 | Loss: 0.00004197
Iteration 13/1000 | Loss: 0.00003194
Iteration 14/1000 | Loss: 0.00002815
Iteration 15/1000 | Loss: 0.00002463
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002067
Iteration 18/1000 | Loss: 0.00002005
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001912
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001875
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001867
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001862
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001859
Iteration 43/1000 | Loss: 0.00001859
Iteration 44/1000 | Loss: 0.00001859
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001858
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001858
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001858
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001857
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001856
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001855
Iteration 81/1000 | Loss: 0.00001855
Iteration 82/1000 | Loss: 0.00001855
Iteration 83/1000 | Loss: 0.00001855
Iteration 84/1000 | Loss: 0.00001855
Iteration 85/1000 | Loss: 0.00001855
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001855
Iteration 99/1000 | Loss: 0.00001855
Iteration 100/1000 | Loss: 0.00001855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.8550450477050617e-05, 1.8550450477050617e-05, 1.8550450477050617e-05, 1.8550450477050617e-05, 1.8550450477050617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8550450477050617e-05

Optimization complete. Final v2v error: 3.6042282581329346 mm

Highest mean error: 4.09529447555542 mm for frame 191

Lowest mean error: 3.428354024887085 mm for frame 237

Saving results

Total time: 82.6468448638916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422338
Iteration 2/25 | Loss: 0.00093714
Iteration 3/25 | Loss: 0.00076476
Iteration 4/25 | Loss: 0.00073322
Iteration 5/25 | Loss: 0.00072762
Iteration 6/25 | Loss: 0.00072581
Iteration 7/25 | Loss: 0.00072525
Iteration 8/25 | Loss: 0.00072525
Iteration 9/25 | Loss: 0.00072525
Iteration 10/25 | Loss: 0.00072525
Iteration 11/25 | Loss: 0.00072525
Iteration 12/25 | Loss: 0.00072525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007252476061694324, 0.0007252476061694324, 0.0007252476061694324, 0.0007252476061694324, 0.0007252476061694324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007252476061694324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45407093
Iteration 2/25 | Loss: 0.00022193
Iteration 3/25 | Loss: 0.00022193
Iteration 4/25 | Loss: 0.00022193
Iteration 5/25 | Loss: 0.00022193
Iteration 6/25 | Loss: 0.00022193
Iteration 7/25 | Loss: 0.00022193
Iteration 8/25 | Loss: 0.00022193
Iteration 9/25 | Loss: 0.00022193
Iteration 10/25 | Loss: 0.00022193
Iteration 11/25 | Loss: 0.00022193
Iteration 12/25 | Loss: 0.00022193
Iteration 13/25 | Loss: 0.00022193
Iteration 14/25 | Loss: 0.00022193
Iteration 15/25 | Loss: 0.00022193
Iteration 16/25 | Loss: 0.00022193
Iteration 17/25 | Loss: 0.00022193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00022192724281921983, 0.00022192724281921983, 0.00022192724281921983, 0.00022192724281921983, 0.00022192724281921983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022192724281921983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022193
Iteration 2/1000 | Loss: 0.00002214
Iteration 3/1000 | Loss: 0.00001853
Iteration 4/1000 | Loss: 0.00001708
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001551
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001509
Iteration 9/1000 | Loss: 0.00001485
Iteration 10/1000 | Loss: 0.00001474
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001452
Iteration 13/1000 | Loss: 0.00001444
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001442
Iteration 16/1000 | Loss: 0.00001442
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001441
Iteration 19/1000 | Loss: 0.00001441
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001441
Iteration 24/1000 | Loss: 0.00001441
Iteration 25/1000 | Loss: 0.00001441
Iteration 26/1000 | Loss: 0.00001438
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001438
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001435
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001434
Iteration 37/1000 | Loss: 0.00001434
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001432
Iteration 44/1000 | Loss: 0.00001432
Iteration 45/1000 | Loss: 0.00001432
Iteration 46/1000 | Loss: 0.00001431
Iteration 47/1000 | Loss: 0.00001431
Iteration 48/1000 | Loss: 0.00001431
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001430
Iteration 59/1000 | Loss: 0.00001430
Iteration 60/1000 | Loss: 0.00001430
Iteration 61/1000 | Loss: 0.00001430
Iteration 62/1000 | Loss: 0.00001430
Iteration 63/1000 | Loss: 0.00001430
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001430
Iteration 66/1000 | Loss: 0.00001430
Iteration 67/1000 | Loss: 0.00001429
Iteration 68/1000 | Loss: 0.00001428
Iteration 69/1000 | Loss: 0.00001428
Iteration 70/1000 | Loss: 0.00001428
Iteration 71/1000 | Loss: 0.00001427
Iteration 72/1000 | Loss: 0.00001427
Iteration 73/1000 | Loss: 0.00001427
Iteration 74/1000 | Loss: 0.00001427
Iteration 75/1000 | Loss: 0.00001427
Iteration 76/1000 | Loss: 0.00001427
Iteration 77/1000 | Loss: 0.00001427
Iteration 78/1000 | Loss: 0.00001427
Iteration 79/1000 | Loss: 0.00001427
Iteration 80/1000 | Loss: 0.00001427
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001427
Iteration 83/1000 | Loss: 0.00001427
Iteration 84/1000 | Loss: 0.00001426
Iteration 85/1000 | Loss: 0.00001426
Iteration 86/1000 | Loss: 0.00001425
Iteration 87/1000 | Loss: 0.00001425
Iteration 88/1000 | Loss: 0.00001425
Iteration 89/1000 | Loss: 0.00001425
Iteration 90/1000 | Loss: 0.00001425
Iteration 91/1000 | Loss: 0.00001425
Iteration 92/1000 | Loss: 0.00001425
Iteration 93/1000 | Loss: 0.00001425
Iteration 94/1000 | Loss: 0.00001424
Iteration 95/1000 | Loss: 0.00001424
Iteration 96/1000 | Loss: 0.00001424
Iteration 97/1000 | Loss: 0.00001424
Iteration 98/1000 | Loss: 0.00001424
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001423
Iteration 101/1000 | Loss: 0.00001423
Iteration 102/1000 | Loss: 0.00001423
Iteration 103/1000 | Loss: 0.00001423
Iteration 104/1000 | Loss: 0.00001423
Iteration 105/1000 | Loss: 0.00001422
Iteration 106/1000 | Loss: 0.00001422
Iteration 107/1000 | Loss: 0.00001422
Iteration 108/1000 | Loss: 0.00001422
Iteration 109/1000 | Loss: 0.00001422
Iteration 110/1000 | Loss: 0.00001422
Iteration 111/1000 | Loss: 0.00001421
Iteration 112/1000 | Loss: 0.00001421
Iteration 113/1000 | Loss: 0.00001421
Iteration 114/1000 | Loss: 0.00001421
Iteration 115/1000 | Loss: 0.00001420
Iteration 116/1000 | Loss: 0.00001420
Iteration 117/1000 | Loss: 0.00001420
Iteration 118/1000 | Loss: 0.00001420
Iteration 119/1000 | Loss: 0.00001420
Iteration 120/1000 | Loss: 0.00001420
Iteration 121/1000 | Loss: 0.00001420
Iteration 122/1000 | Loss: 0.00001420
Iteration 123/1000 | Loss: 0.00001420
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001420
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001419
Iteration 130/1000 | Loss: 0.00001419
Iteration 131/1000 | Loss: 0.00001419
Iteration 132/1000 | Loss: 0.00001419
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001418
Iteration 138/1000 | Loss: 0.00001418
Iteration 139/1000 | Loss: 0.00001418
Iteration 140/1000 | Loss: 0.00001418
Iteration 141/1000 | Loss: 0.00001418
Iteration 142/1000 | Loss: 0.00001417
Iteration 143/1000 | Loss: 0.00001417
Iteration 144/1000 | Loss: 0.00001417
Iteration 145/1000 | Loss: 0.00001417
Iteration 146/1000 | Loss: 0.00001417
Iteration 147/1000 | Loss: 0.00001417
Iteration 148/1000 | Loss: 0.00001417
Iteration 149/1000 | Loss: 0.00001416
Iteration 150/1000 | Loss: 0.00001416
Iteration 151/1000 | Loss: 0.00001416
Iteration 152/1000 | Loss: 0.00001416
Iteration 153/1000 | Loss: 0.00001416
Iteration 154/1000 | Loss: 0.00001416
Iteration 155/1000 | Loss: 0.00001416
Iteration 156/1000 | Loss: 0.00001416
Iteration 157/1000 | Loss: 0.00001416
Iteration 158/1000 | Loss: 0.00001416
Iteration 159/1000 | Loss: 0.00001415
Iteration 160/1000 | Loss: 0.00001415
Iteration 161/1000 | Loss: 0.00001415
Iteration 162/1000 | Loss: 0.00001415
Iteration 163/1000 | Loss: 0.00001415
Iteration 164/1000 | Loss: 0.00001414
Iteration 165/1000 | Loss: 0.00001414
Iteration 166/1000 | Loss: 0.00001414
Iteration 167/1000 | Loss: 0.00001414
Iteration 168/1000 | Loss: 0.00001414
Iteration 169/1000 | Loss: 0.00001414
Iteration 170/1000 | Loss: 0.00001414
Iteration 171/1000 | Loss: 0.00001414
Iteration 172/1000 | Loss: 0.00001414
Iteration 173/1000 | Loss: 0.00001414
Iteration 174/1000 | Loss: 0.00001414
Iteration 175/1000 | Loss: 0.00001414
Iteration 176/1000 | Loss: 0.00001414
Iteration 177/1000 | Loss: 0.00001414
Iteration 178/1000 | Loss: 0.00001414
Iteration 179/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.4139847735350486e-05, 1.4139847735350486e-05, 1.4139847735350486e-05, 1.4139847735350486e-05, 1.4139847735350486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4139847735350486e-05

Optimization complete. Final v2v error: 3.1823890209198 mm

Highest mean error: 3.4491257667541504 mm for frame 105

Lowest mean error: 2.892838478088379 mm for frame 13

Saving results

Total time: 38.40303611755371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00350639
Iteration 2/25 | Loss: 0.00098387
Iteration 3/25 | Loss: 0.00079593
Iteration 4/25 | Loss: 0.00075597
Iteration 5/25 | Loss: 0.00074042
Iteration 6/25 | Loss: 0.00073653
Iteration 7/25 | Loss: 0.00073517
Iteration 8/25 | Loss: 0.00073462
Iteration 9/25 | Loss: 0.00073462
Iteration 10/25 | Loss: 0.00073462
Iteration 11/25 | Loss: 0.00073462
Iteration 12/25 | Loss: 0.00073462
Iteration 13/25 | Loss: 0.00073462
Iteration 14/25 | Loss: 0.00073462
Iteration 15/25 | Loss: 0.00073462
Iteration 16/25 | Loss: 0.00073462
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007346236961893737, 0.0007346236961893737, 0.0007346236961893737, 0.0007346236961893737, 0.0007346236961893737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007346236961893737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47140777
Iteration 2/25 | Loss: 0.00028082
Iteration 3/25 | Loss: 0.00028082
Iteration 4/25 | Loss: 0.00028082
Iteration 5/25 | Loss: 0.00028082
Iteration 6/25 | Loss: 0.00028082
Iteration 7/25 | Loss: 0.00028082
Iteration 8/25 | Loss: 0.00028082
Iteration 9/25 | Loss: 0.00028082
Iteration 10/25 | Loss: 0.00028082
Iteration 11/25 | Loss: 0.00028082
Iteration 12/25 | Loss: 0.00028082
Iteration 13/25 | Loss: 0.00028082
Iteration 14/25 | Loss: 0.00028082
Iteration 15/25 | Loss: 0.00028082
Iteration 16/25 | Loss: 0.00028082
Iteration 17/25 | Loss: 0.00028082
Iteration 18/25 | Loss: 0.00028082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002808185527101159, 0.0002808185527101159, 0.0002808185527101159, 0.0002808185527101159, 0.0002808185527101159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002808185527101159

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028082
Iteration 2/1000 | Loss: 0.00003991
Iteration 3/1000 | Loss: 0.00002888
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002208
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002034
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001944
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001927
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001881
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001864
Iteration 18/1000 | Loss: 0.00001864
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001859
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001850
Iteration 25/1000 | Loss: 0.00001849
Iteration 26/1000 | Loss: 0.00001849
Iteration 27/1000 | Loss: 0.00001848
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001846
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001843
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001842
Iteration 41/1000 | Loss: 0.00001842
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001840
Iteration 47/1000 | Loss: 0.00001840
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001839
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001838
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001836
Iteration 57/1000 | Loss: 0.00001836
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001830
Iteration 72/1000 | Loss: 0.00001830
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001827
Iteration 85/1000 | Loss: 0.00001827
Iteration 86/1000 | Loss: 0.00001827
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001826
Iteration 90/1000 | Loss: 0.00001826
Iteration 91/1000 | Loss: 0.00001826
Iteration 92/1000 | Loss: 0.00001826
Iteration 93/1000 | Loss: 0.00001826
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001825
Iteration 99/1000 | Loss: 0.00001825
Iteration 100/1000 | Loss: 0.00001825
Iteration 101/1000 | Loss: 0.00001825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.825276376621332e-05, 1.825276376621332e-05, 1.825276376621332e-05, 1.825276376621332e-05, 1.825276376621332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.825276376621332e-05

Optimization complete. Final v2v error: 3.58608341217041 mm

Highest mean error: 4.390233039855957 mm for frame 149

Lowest mean error: 2.9363768100738525 mm for frame 59

Saving results

Total time: 47.16995143890381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414901
Iteration 2/25 | Loss: 0.00089109
Iteration 3/25 | Loss: 0.00077241
Iteration 4/25 | Loss: 0.00075491
Iteration 5/25 | Loss: 0.00074978
Iteration 6/25 | Loss: 0.00074892
Iteration 7/25 | Loss: 0.00074892
Iteration 8/25 | Loss: 0.00074892
Iteration 9/25 | Loss: 0.00074892
Iteration 10/25 | Loss: 0.00074892
Iteration 11/25 | Loss: 0.00074892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007489225245080888, 0.0007489225245080888, 0.0007489225245080888, 0.0007489225245080888, 0.0007489225245080888]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007489225245080888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46273780
Iteration 2/25 | Loss: 0.00036809
Iteration 3/25 | Loss: 0.00036809
Iteration 4/25 | Loss: 0.00036809
Iteration 5/25 | Loss: 0.00036809
Iteration 6/25 | Loss: 0.00036809
Iteration 7/25 | Loss: 0.00036809
Iteration 8/25 | Loss: 0.00036809
Iteration 9/25 | Loss: 0.00036809
Iteration 10/25 | Loss: 0.00036809
Iteration 11/25 | Loss: 0.00036809
Iteration 12/25 | Loss: 0.00036809
Iteration 13/25 | Loss: 0.00036809
Iteration 14/25 | Loss: 0.00036809
Iteration 15/25 | Loss: 0.00036809
Iteration 16/25 | Loss: 0.00036809
Iteration 17/25 | Loss: 0.00036809
Iteration 18/25 | Loss: 0.00036809
Iteration 19/25 | Loss: 0.00036809
Iteration 20/25 | Loss: 0.00036809
Iteration 21/25 | Loss: 0.00036809
Iteration 22/25 | Loss: 0.00036809
Iteration 23/25 | Loss: 0.00036809
Iteration 24/25 | Loss: 0.00036809
Iteration 25/25 | Loss: 0.00036809

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036809
Iteration 2/1000 | Loss: 0.00002558
Iteration 3/1000 | Loss: 0.00001477
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001201
Iteration 6/1000 | Loss: 0.00001175
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001148
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001141
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001140
Iteration 13/1000 | Loss: 0.00001140
Iteration 14/1000 | Loss: 0.00001140
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001139
Iteration 17/1000 | Loss: 0.00001138
Iteration 18/1000 | Loss: 0.00001136
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001135
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001135
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001133
Iteration 29/1000 | Loss: 0.00001132
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001131
Iteration 32/1000 | Loss: 0.00001131
Iteration 33/1000 | Loss: 0.00001131
Iteration 34/1000 | Loss: 0.00001131
Iteration 35/1000 | Loss: 0.00001130
Iteration 36/1000 | Loss: 0.00001128
Iteration 37/1000 | Loss: 0.00001128
Iteration 38/1000 | Loss: 0.00001128
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001126
Iteration 42/1000 | Loss: 0.00001126
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001124
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001123
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001123
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001122
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001116
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001116
Iteration 85/1000 | Loss: 0.00001116
Iteration 86/1000 | Loss: 0.00001116
Iteration 87/1000 | Loss: 0.00001116
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001116
Iteration 95/1000 | Loss: 0.00001115
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001115
Iteration 98/1000 | Loss: 0.00001115
Iteration 99/1000 | Loss: 0.00001115
Iteration 100/1000 | Loss: 0.00001115
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001114
Iteration 111/1000 | Loss: 0.00001114
Iteration 112/1000 | Loss: 0.00001114
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001114
Iteration 122/1000 | Loss: 0.00001114
Iteration 123/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.1144457857881207e-05, 1.1144457857881207e-05, 1.1144457857881207e-05, 1.1144457857881207e-05, 1.1144457857881207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1144457857881207e-05

Optimization complete. Final v2v error: 2.848069667816162 mm

Highest mean error: 3.168832540512085 mm for frame 32

Lowest mean error: 2.6483190059661865 mm for frame 97

Saving results

Total time: 26.950608253479004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01096791
Iteration 2/25 | Loss: 0.00534145
Iteration 3/25 | Loss: 0.00349757
Iteration 4/25 | Loss: 0.00315505
Iteration 5/25 | Loss: 0.00272602
Iteration 6/25 | Loss: 0.00253606
Iteration 7/25 | Loss: 0.00218571
Iteration 8/25 | Loss: 0.00202129
Iteration 9/25 | Loss: 0.00196604
Iteration 10/25 | Loss: 0.00179519
Iteration 11/25 | Loss: 0.00174648
Iteration 12/25 | Loss: 0.00170159
Iteration 13/25 | Loss: 0.00168485
Iteration 14/25 | Loss: 0.00172166
Iteration 15/25 | Loss: 0.00174766
Iteration 16/25 | Loss: 0.00172117
Iteration 17/25 | Loss: 0.00159395
Iteration 18/25 | Loss: 0.00154300
Iteration 19/25 | Loss: 0.00151708
Iteration 20/25 | Loss: 0.00150098
Iteration 21/25 | Loss: 0.00148951
Iteration 22/25 | Loss: 0.00147862
Iteration 23/25 | Loss: 0.00146466
Iteration 24/25 | Loss: 0.00146831
Iteration 25/25 | Loss: 0.00144667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43892652
Iteration 2/25 | Loss: 0.00731177
Iteration 3/25 | Loss: 0.00298979
Iteration 4/25 | Loss: 0.00298979
Iteration 5/25 | Loss: 0.00298979
Iteration 6/25 | Loss: 0.00298979
Iteration 7/25 | Loss: 0.00298978
Iteration 8/25 | Loss: 0.00298978
Iteration 9/25 | Loss: 0.00298978
Iteration 10/25 | Loss: 0.00298978
Iteration 11/25 | Loss: 0.00298978
Iteration 12/25 | Loss: 0.00298978
Iteration 13/25 | Loss: 0.00298978
Iteration 14/25 | Loss: 0.00298978
Iteration 15/25 | Loss: 0.00298978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002989782951772213, 0.002989782951772213, 0.002989782951772213, 0.002989782951772213, 0.002989782951772213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002989782951772213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298978
Iteration 2/1000 | Loss: 0.00210343
Iteration 3/1000 | Loss: 0.00193306
Iteration 4/1000 | Loss: 0.00406332
Iteration 5/1000 | Loss: 0.00179669
Iteration 6/1000 | Loss: 0.00220619
Iteration 7/1000 | Loss: 0.00120701
Iteration 8/1000 | Loss: 0.00110254
Iteration 9/1000 | Loss: 0.00140179
Iteration 10/1000 | Loss: 0.00083391
Iteration 11/1000 | Loss: 0.00155005
Iteration 12/1000 | Loss: 0.00096240
Iteration 13/1000 | Loss: 0.00071539
Iteration 14/1000 | Loss: 0.00121767
Iteration 15/1000 | Loss: 0.00107197
Iteration 16/1000 | Loss: 0.00059907
Iteration 17/1000 | Loss: 0.00053459
Iteration 18/1000 | Loss: 0.00129592
Iteration 19/1000 | Loss: 0.00106960
Iteration 20/1000 | Loss: 0.00076250
Iteration 21/1000 | Loss: 0.00096243
Iteration 22/1000 | Loss: 0.00098996
Iteration 23/1000 | Loss: 0.00136346
Iteration 24/1000 | Loss: 0.00087455
Iteration 25/1000 | Loss: 0.00059836
Iteration 26/1000 | Loss: 0.00040563
Iteration 27/1000 | Loss: 0.00044973
Iteration 28/1000 | Loss: 0.00079180
Iteration 29/1000 | Loss: 0.00076910
Iteration 30/1000 | Loss: 0.00084453
Iteration 31/1000 | Loss: 0.00067706
Iteration 32/1000 | Loss: 0.00064498
Iteration 33/1000 | Loss: 0.00100031
Iteration 34/1000 | Loss: 0.00126058
Iteration 35/1000 | Loss: 0.00069811
Iteration 36/1000 | Loss: 0.00070789
Iteration 37/1000 | Loss: 0.00134288
Iteration 38/1000 | Loss: 0.00113379
Iteration 39/1000 | Loss: 0.00103387
Iteration 40/1000 | Loss: 0.00081328
Iteration 41/1000 | Loss: 0.00088154
Iteration 42/1000 | Loss: 0.00061038
Iteration 43/1000 | Loss: 0.00047794
Iteration 44/1000 | Loss: 0.00055298
Iteration 45/1000 | Loss: 0.00092273
Iteration 46/1000 | Loss: 0.00043501
Iteration 47/1000 | Loss: 0.00178084
Iteration 48/1000 | Loss: 0.00146739
Iteration 49/1000 | Loss: 0.00150696
Iteration 50/1000 | Loss: 0.00104388
Iteration 51/1000 | Loss: 0.00161538
Iteration 52/1000 | Loss: 0.00174969
Iteration 53/1000 | Loss: 0.00074928
Iteration 54/1000 | Loss: 0.00065145
Iteration 55/1000 | Loss: 0.00229879
Iteration 56/1000 | Loss: 0.00118395
Iteration 57/1000 | Loss: 0.00084663
Iteration 58/1000 | Loss: 0.00094564
Iteration 59/1000 | Loss: 0.00101739
Iteration 60/1000 | Loss: 0.00075622
Iteration 61/1000 | Loss: 0.00088330
Iteration 62/1000 | Loss: 0.00181845
Iteration 63/1000 | Loss: 0.00109152
Iteration 64/1000 | Loss: 0.00156201
Iteration 65/1000 | Loss: 0.00128813
Iteration 66/1000 | Loss: 0.00102347
Iteration 67/1000 | Loss: 0.00084487
Iteration 68/1000 | Loss: 0.00070757
Iteration 69/1000 | Loss: 0.00081738
Iteration 70/1000 | Loss: 0.00072477
Iteration 71/1000 | Loss: 0.00101920
Iteration 72/1000 | Loss: 0.00079781
Iteration 73/1000 | Loss: 0.00099925
Iteration 74/1000 | Loss: 0.00109922
Iteration 75/1000 | Loss: 0.00065465
Iteration 76/1000 | Loss: 0.00074399
Iteration 77/1000 | Loss: 0.00150085
Iteration 78/1000 | Loss: 0.00085509
Iteration 79/1000 | Loss: 0.00264879
Iteration 80/1000 | Loss: 0.00185451
Iteration 81/1000 | Loss: 0.00135370
Iteration 82/1000 | Loss: 0.00056756
Iteration 83/1000 | Loss: 0.00074509
Iteration 84/1000 | Loss: 0.00174824
Iteration 85/1000 | Loss: 0.00179605
Iteration 86/1000 | Loss: 0.00121973
Iteration 87/1000 | Loss: 0.00148531
Iteration 88/1000 | Loss: 0.00140647
Iteration 89/1000 | Loss: 0.00154848
Iteration 90/1000 | Loss: 0.00125482
Iteration 91/1000 | Loss: 0.00131267
Iteration 92/1000 | Loss: 0.00134918
Iteration 93/1000 | Loss: 0.00116224
Iteration 94/1000 | Loss: 0.00138981
Iteration 95/1000 | Loss: 0.00170644
Iteration 96/1000 | Loss: 0.00138004
Iteration 97/1000 | Loss: 0.00043525
Iteration 98/1000 | Loss: 0.00048508
Iteration 99/1000 | Loss: 0.00071522
Iteration 100/1000 | Loss: 0.00110284
Iteration 101/1000 | Loss: 0.00058136
Iteration 102/1000 | Loss: 0.00091301
Iteration 103/1000 | Loss: 0.00085587
Iteration 104/1000 | Loss: 0.00083151
Iteration 105/1000 | Loss: 0.00079363
Iteration 106/1000 | Loss: 0.00130540
Iteration 107/1000 | Loss: 0.00143086
Iteration 108/1000 | Loss: 0.00158626
Iteration 109/1000 | Loss: 0.00130447
Iteration 110/1000 | Loss: 0.00101860
Iteration 111/1000 | Loss: 0.00124838
Iteration 112/1000 | Loss: 0.00098834
Iteration 113/1000 | Loss: 0.00099193
Iteration 114/1000 | Loss: 0.00068791
Iteration 115/1000 | Loss: 0.00086828
Iteration 116/1000 | Loss: 0.00141085
Iteration 117/1000 | Loss: 0.00146184
Iteration 118/1000 | Loss: 0.00139301
Iteration 119/1000 | Loss: 0.00085371
Iteration 120/1000 | Loss: 0.00090977
Iteration 121/1000 | Loss: 0.00052588
Iteration 122/1000 | Loss: 0.00056773
Iteration 123/1000 | Loss: 0.00091783
Iteration 124/1000 | Loss: 0.00070768
Iteration 125/1000 | Loss: 0.00052301
Iteration 126/1000 | Loss: 0.00060153
Iteration 127/1000 | Loss: 0.00104138
Iteration 128/1000 | Loss: 0.00066191
Iteration 129/1000 | Loss: 0.00050340
Iteration 130/1000 | Loss: 0.00029895
Iteration 131/1000 | Loss: 0.00070739
Iteration 132/1000 | Loss: 0.00061874
Iteration 133/1000 | Loss: 0.00075281
Iteration 134/1000 | Loss: 0.00149862
Iteration 135/1000 | Loss: 0.00050744
Iteration 136/1000 | Loss: 0.00046793
Iteration 137/1000 | Loss: 0.00033101
Iteration 138/1000 | Loss: 0.00041139
Iteration 139/1000 | Loss: 0.00029795
Iteration 140/1000 | Loss: 0.00059370
Iteration 141/1000 | Loss: 0.00067773
Iteration 142/1000 | Loss: 0.00057029
Iteration 143/1000 | Loss: 0.00050129
Iteration 144/1000 | Loss: 0.00050537
Iteration 145/1000 | Loss: 0.00092260
Iteration 146/1000 | Loss: 0.00085607
Iteration 147/1000 | Loss: 0.00086032
Iteration 148/1000 | Loss: 0.00072129
Iteration 149/1000 | Loss: 0.00080122
Iteration 150/1000 | Loss: 0.00057838
Iteration 151/1000 | Loss: 0.00056115
Iteration 152/1000 | Loss: 0.00045493
Iteration 153/1000 | Loss: 0.00049185
Iteration 154/1000 | Loss: 0.00040007
Iteration 155/1000 | Loss: 0.00071617
Iteration 156/1000 | Loss: 0.00094331
Iteration 157/1000 | Loss: 0.00058496
Iteration 158/1000 | Loss: 0.00097999
Iteration 159/1000 | Loss: 0.00040494
Iteration 160/1000 | Loss: 0.00039092
Iteration 161/1000 | Loss: 0.00032909
Iteration 162/1000 | Loss: 0.00043263
Iteration 163/1000 | Loss: 0.00042876
Iteration 164/1000 | Loss: 0.00060840
Iteration 165/1000 | Loss: 0.00053938
Iteration 166/1000 | Loss: 0.00093979
Iteration 167/1000 | Loss: 0.00063521
Iteration 168/1000 | Loss: 0.00046207
Iteration 169/1000 | Loss: 0.00042165
Iteration 170/1000 | Loss: 0.00107549
Iteration 171/1000 | Loss: 0.00155799
Iteration 172/1000 | Loss: 0.00113025
Iteration 173/1000 | Loss: 0.00042529
Iteration 174/1000 | Loss: 0.00046547
Iteration 175/1000 | Loss: 0.00048897
Iteration 176/1000 | Loss: 0.00054012
Iteration 177/1000 | Loss: 0.00066437
Iteration 178/1000 | Loss: 0.00051641
Iteration 179/1000 | Loss: 0.00045402
Iteration 180/1000 | Loss: 0.00036492
Iteration 181/1000 | Loss: 0.00032968
Iteration 182/1000 | Loss: 0.00031776
Iteration 183/1000 | Loss: 0.00041581
Iteration 184/1000 | Loss: 0.00050288
Iteration 185/1000 | Loss: 0.00063354
Iteration 186/1000 | Loss: 0.00038302
Iteration 187/1000 | Loss: 0.00044936
Iteration 188/1000 | Loss: 0.00021105
Iteration 189/1000 | Loss: 0.00074702
Iteration 190/1000 | Loss: 0.00057518
Iteration 191/1000 | Loss: 0.00096949
Iteration 192/1000 | Loss: 0.00031743
Iteration 193/1000 | Loss: 0.00138157
Iteration 194/1000 | Loss: 0.00040318
Iteration 195/1000 | Loss: 0.00190374
Iteration 196/1000 | Loss: 0.00071355
Iteration 197/1000 | Loss: 0.00078345
Iteration 198/1000 | Loss: 0.00234646
Iteration 199/1000 | Loss: 0.00129707
Iteration 200/1000 | Loss: 0.00082108
Iteration 201/1000 | Loss: 0.00069119
Iteration 202/1000 | Loss: 0.00086996
Iteration 203/1000 | Loss: 0.00099452
Iteration 204/1000 | Loss: 0.00076133
Iteration 205/1000 | Loss: 0.00068085
Iteration 206/1000 | Loss: 0.00070245
Iteration 207/1000 | Loss: 0.00040792
Iteration 208/1000 | Loss: 0.00035859
Iteration 209/1000 | Loss: 0.00047618
Iteration 210/1000 | Loss: 0.00046350
Iteration 211/1000 | Loss: 0.00045731
Iteration 212/1000 | Loss: 0.00064571
Iteration 213/1000 | Loss: 0.00045845
Iteration 214/1000 | Loss: 0.00091088
Iteration 215/1000 | Loss: 0.00111062
Iteration 216/1000 | Loss: 0.00094563
Iteration 217/1000 | Loss: 0.00091107
Iteration 218/1000 | Loss: 0.00031828
Iteration 219/1000 | Loss: 0.00109676
Iteration 220/1000 | Loss: 0.00044973
Iteration 221/1000 | Loss: 0.00036964
Iteration 222/1000 | Loss: 0.00076522
Iteration 223/1000 | Loss: 0.00062274
Iteration 224/1000 | Loss: 0.00111320
Iteration 225/1000 | Loss: 0.00072858
Iteration 226/1000 | Loss: 0.00075792
Iteration 227/1000 | Loss: 0.00111525
Iteration 228/1000 | Loss: 0.00076117
Iteration 229/1000 | Loss: 0.00060257
Iteration 230/1000 | Loss: 0.00083959
Iteration 231/1000 | Loss: 0.00037939
Iteration 232/1000 | Loss: 0.00098907
Iteration 233/1000 | Loss: 0.00090715
Iteration 234/1000 | Loss: 0.00043971
Iteration 235/1000 | Loss: 0.00046792
Iteration 236/1000 | Loss: 0.00042364
Iteration 237/1000 | Loss: 0.00039713
Iteration 238/1000 | Loss: 0.00057297
Iteration 239/1000 | Loss: 0.00094926
Iteration 240/1000 | Loss: 0.00093712
Iteration 241/1000 | Loss: 0.00055323
Iteration 242/1000 | Loss: 0.00043764
Iteration 243/1000 | Loss: 0.00064102
Iteration 244/1000 | Loss: 0.00043752
Iteration 245/1000 | Loss: 0.00040627
Iteration 246/1000 | Loss: 0.00032720
Iteration 247/1000 | Loss: 0.00035464
Iteration 248/1000 | Loss: 0.00022916
Iteration 249/1000 | Loss: 0.00033576
Iteration 250/1000 | Loss: 0.00033738
Iteration 251/1000 | Loss: 0.00120953
Iteration 252/1000 | Loss: 0.00066928
Iteration 253/1000 | Loss: 0.00054319
Iteration 254/1000 | Loss: 0.00038336
Iteration 255/1000 | Loss: 0.00036843
Iteration 256/1000 | Loss: 0.00094499
Iteration 257/1000 | Loss: 0.00018144
Iteration 258/1000 | Loss: 0.00017364
Iteration 259/1000 | Loss: 0.00027578
Iteration 260/1000 | Loss: 0.00031685
Iteration 261/1000 | Loss: 0.00028075
Iteration 262/1000 | Loss: 0.00027330
Iteration 263/1000 | Loss: 0.00019334
Iteration 264/1000 | Loss: 0.00036230
Iteration 265/1000 | Loss: 0.00016783
Iteration 266/1000 | Loss: 0.00013584
Iteration 267/1000 | Loss: 0.00024483
Iteration 268/1000 | Loss: 0.00035896
Iteration 269/1000 | Loss: 0.00033806
Iteration 270/1000 | Loss: 0.00020484
Iteration 271/1000 | Loss: 0.00015047
Iteration 272/1000 | Loss: 0.00033208
Iteration 273/1000 | Loss: 0.00043011
Iteration 274/1000 | Loss: 0.00033016
Iteration 275/1000 | Loss: 0.00038719
Iteration 276/1000 | Loss: 0.00015743
Iteration 277/1000 | Loss: 0.00014603
Iteration 278/1000 | Loss: 0.00013276
Iteration 279/1000 | Loss: 0.00033582
Iteration 280/1000 | Loss: 0.00033342
Iteration 281/1000 | Loss: 0.00034741
Iteration 282/1000 | Loss: 0.00030852
Iteration 283/1000 | Loss: 0.00035536
Iteration 284/1000 | Loss: 0.00016749
Iteration 285/1000 | Loss: 0.00022478
Iteration 286/1000 | Loss: 0.00019707
Iteration 287/1000 | Loss: 0.00019846
Iteration 288/1000 | Loss: 0.00016074
Iteration 289/1000 | Loss: 0.00023562
Iteration 290/1000 | Loss: 0.00027520
Iteration 291/1000 | Loss: 0.00027608
Iteration 292/1000 | Loss: 0.00033425
Iteration 293/1000 | Loss: 0.00021711
Iteration 294/1000 | Loss: 0.00019991
Iteration 295/1000 | Loss: 0.00021223
Iteration 296/1000 | Loss: 0.00033592
Iteration 297/1000 | Loss: 0.00021953
Iteration 298/1000 | Loss: 0.00059452
Iteration 299/1000 | Loss: 0.00048016
Iteration 300/1000 | Loss: 0.00066945
Iteration 301/1000 | Loss: 0.00071175
Iteration 302/1000 | Loss: 0.00036157
Iteration 303/1000 | Loss: 0.00039777
Iteration 304/1000 | Loss: 0.00037473
Iteration 305/1000 | Loss: 0.00039190
Iteration 306/1000 | Loss: 0.00029804
Iteration 307/1000 | Loss: 0.00029356
Iteration 308/1000 | Loss: 0.00037274
Iteration 309/1000 | Loss: 0.00021037
Iteration 310/1000 | Loss: 0.00021409
Iteration 311/1000 | Loss: 0.00022232
Iteration 312/1000 | Loss: 0.00057832
Iteration 313/1000 | Loss: 0.00036580
Iteration 314/1000 | Loss: 0.00037049
Iteration 315/1000 | Loss: 0.00061115
Iteration 316/1000 | Loss: 0.00045408
Iteration 317/1000 | Loss: 0.00027691
Iteration 318/1000 | Loss: 0.00027650
Iteration 319/1000 | Loss: 0.00034748
Iteration 320/1000 | Loss: 0.00036095
Iteration 321/1000 | Loss: 0.00024237
Iteration 322/1000 | Loss: 0.00036377
Iteration 323/1000 | Loss: 0.00049046
Iteration 324/1000 | Loss: 0.00034130
Iteration 325/1000 | Loss: 0.00024802
Iteration 326/1000 | Loss: 0.00026055
Iteration 327/1000 | Loss: 0.00024936
Iteration 328/1000 | Loss: 0.00025544
Iteration 329/1000 | Loss: 0.00015207
Iteration 330/1000 | Loss: 0.00015712
Iteration 331/1000 | Loss: 0.00094883
Iteration 332/1000 | Loss: 0.00028910
Iteration 333/1000 | Loss: 0.00026235
Iteration 334/1000 | Loss: 0.00021659
Iteration 335/1000 | Loss: 0.00015066
Iteration 336/1000 | Loss: 0.00018322
Iteration 337/1000 | Loss: 0.00017055
Iteration 338/1000 | Loss: 0.00013452
Iteration 339/1000 | Loss: 0.00013071
Iteration 340/1000 | Loss: 0.00013742
Iteration 341/1000 | Loss: 0.00012739
Iteration 342/1000 | Loss: 0.00018533
Iteration 343/1000 | Loss: 0.00016057
Iteration 344/1000 | Loss: 0.00012554
Iteration 345/1000 | Loss: 0.00013203
Iteration 346/1000 | Loss: 0.00019562
Iteration 347/1000 | Loss: 0.00016207
Iteration 348/1000 | Loss: 0.00019495
Iteration 349/1000 | Loss: 0.00016064
Iteration 350/1000 | Loss: 0.00045944
Iteration 351/1000 | Loss: 0.00012746
Iteration 352/1000 | Loss: 0.00013139
Iteration 353/1000 | Loss: 0.00012870
Iteration 354/1000 | Loss: 0.00012756
Iteration 355/1000 | Loss: 0.00012890
Iteration 356/1000 | Loss: 0.00021173
Iteration 357/1000 | Loss: 0.00019832
Iteration 358/1000 | Loss: 0.00016552
Iteration 359/1000 | Loss: 0.00016553
Iteration 360/1000 | Loss: 0.00011731
Iteration 361/1000 | Loss: 0.00021540
Iteration 362/1000 | Loss: 0.00018185
Iteration 363/1000 | Loss: 0.00016586
Iteration 364/1000 | Loss: 0.00016367
Iteration 365/1000 | Loss: 0.00020677
Iteration 366/1000 | Loss: 0.00017321
Iteration 367/1000 | Loss: 0.00020640
Iteration 368/1000 | Loss: 0.00023747
Iteration 369/1000 | Loss: 0.00024729
Iteration 370/1000 | Loss: 0.00023071
Iteration 371/1000 | Loss: 0.00026784
Iteration 372/1000 | Loss: 0.00025381
Iteration 373/1000 | Loss: 0.00014160
Iteration 374/1000 | Loss: 0.00019450
Iteration 375/1000 | Loss: 0.00018325
Iteration 376/1000 | Loss: 0.00013048
Iteration 377/1000 | Loss: 0.00023305
Iteration 378/1000 | Loss: 0.00014686
Iteration 379/1000 | Loss: 0.00016618
Iteration 380/1000 | Loss: 0.00021905
Iteration 381/1000 | Loss: 0.00020612
Iteration 382/1000 | Loss: 0.00019425
Iteration 383/1000 | Loss: 0.00020280
Iteration 384/1000 | Loss: 0.00013004
Iteration 385/1000 | Loss: 0.00013027
Iteration 386/1000 | Loss: 0.00012174
Iteration 387/1000 | Loss: 0.00011963
Iteration 388/1000 | Loss: 0.00019127
Iteration 389/1000 | Loss: 0.00016827
Iteration 390/1000 | Loss: 0.00024835
Iteration 391/1000 | Loss: 0.00023671
Iteration 392/1000 | Loss: 0.00023897
Iteration 393/1000 | Loss: 0.00026393
Iteration 394/1000 | Loss: 0.00029322
Iteration 395/1000 | Loss: 0.00025413
Iteration 396/1000 | Loss: 0.00030147
Iteration 397/1000 | Loss: 0.00024057
Iteration 398/1000 | Loss: 0.00029363
Iteration 399/1000 | Loss: 0.00025389
Iteration 400/1000 | Loss: 0.00025446
Iteration 401/1000 | Loss: 0.00015141
Iteration 402/1000 | Loss: 0.00012540
Iteration 403/1000 | Loss: 0.00027129
Iteration 404/1000 | Loss: 0.00010509
Iteration 405/1000 | Loss: 0.00021748
Iteration 406/1000 | Loss: 0.00012777
Iteration 407/1000 | Loss: 0.00019126
Iteration 408/1000 | Loss: 0.00012516
Iteration 409/1000 | Loss: 0.00012568
Iteration 410/1000 | Loss: 0.00013227
Iteration 411/1000 | Loss: 0.00017335
Iteration 412/1000 | Loss: 0.00014199
Iteration 413/1000 | Loss: 0.00010956
Iteration 414/1000 | Loss: 0.00018340
Iteration 415/1000 | Loss: 0.00021499
Iteration 416/1000 | Loss: 0.00076944
Iteration 417/1000 | Loss: 0.00014456
Iteration 418/1000 | Loss: 0.00014141
Iteration 419/1000 | Loss: 0.00010234
Iteration 420/1000 | Loss: 0.00021139
Iteration 421/1000 | Loss: 0.00064895
Iteration 422/1000 | Loss: 0.00012554
Iteration 423/1000 | Loss: 0.00012178
Iteration 424/1000 | Loss: 0.00028236
Iteration 425/1000 | Loss: 0.00027538
Iteration 426/1000 | Loss: 0.00013014
Iteration 427/1000 | Loss: 0.00012475
Iteration 428/1000 | Loss: 0.00019643
Iteration 429/1000 | Loss: 0.00017569
Iteration 430/1000 | Loss: 0.00012364
Iteration 431/1000 | Loss: 0.00012635
Iteration 432/1000 | Loss: 0.00012726
Iteration 433/1000 | Loss: 0.00012727
Iteration 434/1000 | Loss: 0.00012336
Iteration 435/1000 | Loss: 0.00012517
Iteration 436/1000 | Loss: 0.00016938
Iteration 437/1000 | Loss: 0.00015665
Iteration 438/1000 | Loss: 0.00019300
Iteration 439/1000 | Loss: 0.00019031
Iteration 440/1000 | Loss: 0.00018751
Iteration 441/1000 | Loss: 0.00017384
Iteration 442/1000 | Loss: 0.00016961
Iteration 443/1000 | Loss: 0.00017441
Iteration 444/1000 | Loss: 0.00011069
Iteration 445/1000 | Loss: 0.00010554
Iteration 446/1000 | Loss: 0.00010207
Iteration 447/1000 | Loss: 0.00012387
Iteration 448/1000 | Loss: 0.00012076
Iteration 449/1000 | Loss: 0.00012116
Iteration 450/1000 | Loss: 0.00012062
Iteration 451/1000 | Loss: 0.00012169
Iteration 452/1000 | Loss: 0.00074696
Iteration 453/1000 | Loss: 0.00010467
Iteration 454/1000 | Loss: 0.00011829
Iteration 455/1000 | Loss: 0.00029882
Iteration 456/1000 | Loss: 0.00018075
Iteration 457/1000 | Loss: 0.00012015
Iteration 458/1000 | Loss: 0.00010370
Iteration 459/1000 | Loss: 0.00011956
Iteration 460/1000 | Loss: 0.00010881
Iteration 461/1000 | Loss: 0.00012150
Iteration 462/1000 | Loss: 0.00011906
Iteration 463/1000 | Loss: 0.00011411
Iteration 464/1000 | Loss: 0.00029473
Iteration 465/1000 | Loss: 0.00012443
Iteration 466/1000 | Loss: 0.00010298
Iteration 467/1000 | Loss: 0.00011864
Iteration 468/1000 | Loss: 0.00011977
Iteration 469/1000 | Loss: 0.00011357
Iteration 470/1000 | Loss: 0.00011699
Iteration 471/1000 | Loss: 0.00011606
Iteration 472/1000 | Loss: 0.00011586
Iteration 473/1000 | Loss: 0.00011752
Iteration 474/1000 | Loss: 0.00010080
Iteration 475/1000 | Loss: 0.00010815
Iteration 476/1000 | Loss: 0.00011682
Iteration 477/1000 | Loss: 0.00009851
Iteration 478/1000 | Loss: 0.00009274
Iteration 479/1000 | Loss: 0.00011721
Iteration 480/1000 | Loss: 0.00011582
Iteration 481/1000 | Loss: 0.00027495
Iteration 482/1000 | Loss: 0.00028558
Iteration 483/1000 | Loss: 0.00027333
Iteration 484/1000 | Loss: 0.00012114
Iteration 485/1000 | Loss: 0.00011722
Iteration 486/1000 | Loss: 0.00011139
Iteration 487/1000 | Loss: 0.00011286
Iteration 488/1000 | Loss: 0.00011085
Iteration 489/1000 | Loss: 0.00011184
Iteration 490/1000 | Loss: 0.00011529
Iteration 491/1000 | Loss: 0.00009340
Iteration 492/1000 | Loss: 0.00011050
Iteration 493/1000 | Loss: 0.00013333
Iteration 494/1000 | Loss: 0.00008385
Iteration 495/1000 | Loss: 0.00007530
Iteration 496/1000 | Loss: 0.00008121
Iteration 497/1000 | Loss: 0.00006849
Iteration 498/1000 | Loss: 0.00006638
Iteration 499/1000 | Loss: 0.00007696
Iteration 500/1000 | Loss: 0.00007580
Iteration 501/1000 | Loss: 0.00007397
Iteration 502/1000 | Loss: 0.00007257
Iteration 503/1000 | Loss: 0.00007133
Iteration 504/1000 | Loss: 0.00007241
Iteration 505/1000 | Loss: 0.00006876
Iteration 506/1000 | Loss: 0.00007266
Iteration 507/1000 | Loss: 0.00008380
Iteration 508/1000 | Loss: 0.00007389
Iteration 509/1000 | Loss: 0.00007572
Iteration 510/1000 | Loss: 0.00006949
Iteration 511/1000 | Loss: 0.00006578
Iteration 512/1000 | Loss: 0.00007680
Iteration 513/1000 | Loss: 0.00008544
Iteration 514/1000 | Loss: 0.00007870
Iteration 515/1000 | Loss: 0.00007907
Iteration 516/1000 | Loss: 0.00006619
Iteration 517/1000 | Loss: 0.00007521
Iteration 518/1000 | Loss: 0.00007402
Iteration 519/1000 | Loss: 0.00007465
Iteration 520/1000 | Loss: 0.00007683
Iteration 521/1000 | Loss: 0.00008377
Iteration 522/1000 | Loss: 0.00008475
Iteration 523/1000 | Loss: 0.00049683
Iteration 524/1000 | Loss: 0.00007376
Iteration 525/1000 | Loss: 0.00006733
Iteration 526/1000 | Loss: 0.00006479
Iteration 527/1000 | Loss: 0.00006368
Iteration 528/1000 | Loss: 0.00006249
Iteration 529/1000 | Loss: 0.00006166
Iteration 530/1000 | Loss: 0.00006119
Iteration 531/1000 | Loss: 0.00006097
Iteration 532/1000 | Loss: 0.00006073
Iteration 533/1000 | Loss: 0.00006062
Iteration 534/1000 | Loss: 0.00006053
Iteration 535/1000 | Loss: 0.00006041
Iteration 536/1000 | Loss: 0.00006031
Iteration 537/1000 | Loss: 0.00006030
Iteration 538/1000 | Loss: 0.00006028
Iteration 539/1000 | Loss: 0.00006028
Iteration 540/1000 | Loss: 0.00006027
Iteration 541/1000 | Loss: 0.00006027
Iteration 542/1000 | Loss: 0.00006020
Iteration 543/1000 | Loss: 0.00006018
Iteration 544/1000 | Loss: 0.00006018
Iteration 545/1000 | Loss: 0.00006018
Iteration 546/1000 | Loss: 0.00006018
Iteration 547/1000 | Loss: 0.00006017
Iteration 548/1000 | Loss: 0.00006017
Iteration 549/1000 | Loss: 0.00006017
Iteration 550/1000 | Loss: 0.00006017
Iteration 551/1000 | Loss: 0.00006017
Iteration 552/1000 | Loss: 0.00006017
Iteration 553/1000 | Loss: 0.00006017
Iteration 554/1000 | Loss: 0.00006017
Iteration 555/1000 | Loss: 0.00006017
Iteration 556/1000 | Loss: 0.00006017
Iteration 557/1000 | Loss: 0.00006017
Iteration 558/1000 | Loss: 0.00006016
Iteration 559/1000 | Loss: 0.00006016
Iteration 560/1000 | Loss: 0.00006016
Iteration 561/1000 | Loss: 0.00006016
Iteration 562/1000 | Loss: 0.00006016
Iteration 563/1000 | Loss: 0.00006016
Iteration 564/1000 | Loss: 0.00006015
Iteration 565/1000 | Loss: 0.00006015
Iteration 566/1000 | Loss: 0.00006015
Iteration 567/1000 | Loss: 0.00006015
Iteration 568/1000 | Loss: 0.00006014
Iteration 569/1000 | Loss: 0.00006013
Iteration 570/1000 | Loss: 0.00006013
Iteration 571/1000 | Loss: 0.00006013
Iteration 572/1000 | Loss: 0.00006012
Iteration 573/1000 | Loss: 0.00006012
Iteration 574/1000 | Loss: 0.00006012
Iteration 575/1000 | Loss: 0.00006012
Iteration 576/1000 | Loss: 0.00006011
Iteration 577/1000 | Loss: 0.00006011
Iteration 578/1000 | Loss: 0.00006011
Iteration 579/1000 | Loss: 0.00006010
Iteration 580/1000 | Loss: 0.00006010
Iteration 581/1000 | Loss: 0.00006010
Iteration 582/1000 | Loss: 0.00006009
Iteration 583/1000 | Loss: 0.00006009
Iteration 584/1000 | Loss: 0.00006009
Iteration 585/1000 | Loss: 0.00006009
Iteration 586/1000 | Loss: 0.00006008
Iteration 587/1000 | Loss: 0.00006008
Iteration 588/1000 | Loss: 0.00006008
Iteration 589/1000 | Loss: 0.00006008
Iteration 590/1000 | Loss: 0.00006008
Iteration 591/1000 | Loss: 0.00006008
Iteration 592/1000 | Loss: 0.00006008
Iteration 593/1000 | Loss: 0.00006008
Iteration 594/1000 | Loss: 0.00006008
Iteration 595/1000 | Loss: 0.00006007
Iteration 596/1000 | Loss: 0.00006007
Iteration 597/1000 | Loss: 0.00006007
Iteration 598/1000 | Loss: 0.00006007
Iteration 599/1000 | Loss: 0.00006007
Iteration 600/1000 | Loss: 0.00006007
Iteration 601/1000 | Loss: 0.00006007
Iteration 602/1000 | Loss: 0.00006006
Iteration 603/1000 | Loss: 0.00006006
Iteration 604/1000 | Loss: 0.00006006
Iteration 605/1000 | Loss: 0.00006006
Iteration 606/1000 | Loss: 0.00006006
Iteration 607/1000 | Loss: 0.00006006
Iteration 608/1000 | Loss: 0.00006006
Iteration 609/1000 | Loss: 0.00006005
Iteration 610/1000 | Loss: 0.00006005
Iteration 611/1000 | Loss: 0.00006005
Iteration 612/1000 | Loss: 0.00006005
Iteration 613/1000 | Loss: 0.00006005
Iteration 614/1000 | Loss: 0.00006005
Iteration 615/1000 | Loss: 0.00006005
Iteration 616/1000 | Loss: 0.00006004
Iteration 617/1000 | Loss: 0.00006004
Iteration 618/1000 | Loss: 0.00006003
Iteration 619/1000 | Loss: 0.00006003
Iteration 620/1000 | Loss: 0.00006003
Iteration 621/1000 | Loss: 0.00006003
Iteration 622/1000 | Loss: 0.00006003
Iteration 623/1000 | Loss: 0.00006003
Iteration 624/1000 | Loss: 0.00006003
Iteration 625/1000 | Loss: 0.00006003
Iteration 626/1000 | Loss: 0.00006002
Iteration 627/1000 | Loss: 0.00006002
Iteration 628/1000 | Loss: 0.00006002
Iteration 629/1000 | Loss: 0.00006002
Iteration 630/1000 | Loss: 0.00006002
Iteration 631/1000 | Loss: 0.00006001
Iteration 632/1000 | Loss: 0.00006001
Iteration 633/1000 | Loss: 0.00006001
Iteration 634/1000 | Loss: 0.00006001
Iteration 635/1000 | Loss: 0.00006001
Iteration 636/1000 | Loss: 0.00006001
Iteration 637/1000 | Loss: 0.00006001
Iteration 638/1000 | Loss: 0.00006001
Iteration 639/1000 | Loss: 0.00006001
Iteration 640/1000 | Loss: 0.00006001
Iteration 641/1000 | Loss: 0.00006001
Iteration 642/1000 | Loss: 0.00006001
Iteration 643/1000 | Loss: 0.00006001
Iteration 644/1000 | Loss: 0.00006000
Iteration 645/1000 | Loss: 0.00006000
Iteration 646/1000 | Loss: 0.00006000
Iteration 647/1000 | Loss: 0.00006000
Iteration 648/1000 | Loss: 0.00006000
Iteration 649/1000 | Loss: 0.00006000
Iteration 650/1000 | Loss: 0.00006000
Iteration 651/1000 | Loss: 0.00006000
Iteration 652/1000 | Loss: 0.00006000
Iteration 653/1000 | Loss: 0.00006000
Iteration 654/1000 | Loss: 0.00006000
Iteration 655/1000 | Loss: 0.00006000
Iteration 656/1000 | Loss: 0.00006000
Iteration 657/1000 | Loss: 0.00006000
Iteration 658/1000 | Loss: 0.00006000
Iteration 659/1000 | Loss: 0.00006000
Iteration 660/1000 | Loss: 0.00006000
Iteration 661/1000 | Loss: 0.00005999
Iteration 662/1000 | Loss: 0.00005999
Iteration 663/1000 | Loss: 0.00005999
Iteration 664/1000 | Loss: 0.00005998
Iteration 665/1000 | Loss: 0.00005998
Iteration 666/1000 | Loss: 0.00005998
Iteration 667/1000 | Loss: 0.00005998
Iteration 668/1000 | Loss: 0.00005998
Iteration 669/1000 | Loss: 0.00005998
Iteration 670/1000 | Loss: 0.00005998
Iteration 671/1000 | Loss: 0.00005997
Iteration 672/1000 | Loss: 0.00005997
Iteration 673/1000 | Loss: 0.00005997
Iteration 674/1000 | Loss: 0.00005997
Iteration 675/1000 | Loss: 0.00005997
Iteration 676/1000 | Loss: 0.00005997
Iteration 677/1000 | Loss: 0.00005997
Iteration 678/1000 | Loss: 0.00005997
Iteration 679/1000 | Loss: 0.00005997
Iteration 680/1000 | Loss: 0.00005996
Iteration 681/1000 | Loss: 0.00005996
Iteration 682/1000 | Loss: 0.00005996
Iteration 683/1000 | Loss: 0.00005996
Iteration 684/1000 | Loss: 0.00005996
Iteration 685/1000 | Loss: 0.00005996
Iteration 686/1000 | Loss: 0.00005996
Iteration 687/1000 | Loss: 0.00005996
Iteration 688/1000 | Loss: 0.00005996
Iteration 689/1000 | Loss: 0.00005996
Iteration 690/1000 | Loss: 0.00005996
Iteration 691/1000 | Loss: 0.00005996
Iteration 692/1000 | Loss: 0.00005996
Iteration 693/1000 | Loss: 0.00005996
Iteration 694/1000 | Loss: 0.00005995
Iteration 695/1000 | Loss: 0.00005995
Iteration 696/1000 | Loss: 0.00005995
Iteration 697/1000 | Loss: 0.00005995
Iteration 698/1000 | Loss: 0.00005995
Iteration 699/1000 | Loss: 0.00005995
Iteration 700/1000 | Loss: 0.00005995
Iteration 701/1000 | Loss: 0.00005995
Iteration 702/1000 | Loss: 0.00005995
Iteration 703/1000 | Loss: 0.00005995
Iteration 704/1000 | Loss: 0.00005995
Iteration 705/1000 | Loss: 0.00005995
Iteration 706/1000 | Loss: 0.00005995
Iteration 707/1000 | Loss: 0.00005995
Iteration 708/1000 | Loss: 0.00005995
Iteration 709/1000 | Loss: 0.00005995
Iteration 710/1000 | Loss: 0.00005995
Iteration 711/1000 | Loss: 0.00005995
Iteration 712/1000 | Loss: 0.00005995
Iteration 713/1000 | Loss: 0.00005995
Iteration 714/1000 | Loss: 0.00005995
Iteration 715/1000 | Loss: 0.00005995
Iteration 716/1000 | Loss: 0.00005995
Iteration 717/1000 | Loss: 0.00005995
Iteration 718/1000 | Loss: 0.00005995
Iteration 719/1000 | Loss: 0.00005995
Iteration 720/1000 | Loss: 0.00005995
Iteration 721/1000 | Loss: 0.00005995
Iteration 722/1000 | Loss: 0.00005995
Iteration 723/1000 | Loss: 0.00005995
Iteration 724/1000 | Loss: 0.00005995
Iteration 725/1000 | Loss: 0.00005995
Iteration 726/1000 | Loss: 0.00005995
Iteration 727/1000 | Loss: 0.00005995
Iteration 728/1000 | Loss: 0.00005995
Iteration 729/1000 | Loss: 0.00005995
Iteration 730/1000 | Loss: 0.00005995
Iteration 731/1000 | Loss: 0.00005995
Iteration 732/1000 | Loss: 0.00005995
Iteration 733/1000 | Loss: 0.00005995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 733. Stopping optimization.
Last 5 losses: [5.9952959418296814e-05, 5.9952959418296814e-05, 5.9952959418296814e-05, 5.9952959418296814e-05, 5.9952959418296814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.9952959418296814e-05

Optimization complete. Final v2v error: 5.865991115570068 mm

Highest mean error: 6.635397434234619 mm for frame 60

Lowest mean error: 4.627696514129639 mm for frame 13

Saving results

Total time: 933.4061603546143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700795
Iteration 2/25 | Loss: 0.00086985
Iteration 3/25 | Loss: 0.00074271
Iteration 4/25 | Loss: 0.00071608
Iteration 5/25 | Loss: 0.00070978
Iteration 6/25 | Loss: 0.00070725
Iteration 7/25 | Loss: 0.00070659
Iteration 8/25 | Loss: 0.00070659
Iteration 9/25 | Loss: 0.00070659
Iteration 10/25 | Loss: 0.00070659
Iteration 11/25 | Loss: 0.00070659
Iteration 12/25 | Loss: 0.00070659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000706594786606729, 0.000706594786606729, 0.000706594786606729, 0.000706594786606729, 0.000706594786606729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000706594786606729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.81198502
Iteration 2/25 | Loss: 0.00020267
Iteration 3/25 | Loss: 0.00020265
Iteration 4/25 | Loss: 0.00020265
Iteration 5/25 | Loss: 0.00020265
Iteration 6/25 | Loss: 0.00020265
Iteration 7/25 | Loss: 0.00020265
Iteration 8/25 | Loss: 0.00020265
Iteration 9/25 | Loss: 0.00020265
Iteration 10/25 | Loss: 0.00020265
Iteration 11/25 | Loss: 0.00020265
Iteration 12/25 | Loss: 0.00020265
Iteration 13/25 | Loss: 0.00020265
Iteration 14/25 | Loss: 0.00020265
Iteration 15/25 | Loss: 0.00020265
Iteration 16/25 | Loss: 0.00020265
Iteration 17/25 | Loss: 0.00020265
Iteration 18/25 | Loss: 0.00020265
Iteration 19/25 | Loss: 0.00020265
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002026494184974581, 0.0002026494184974581, 0.0002026494184974581, 0.0002026494184974581, 0.0002026494184974581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002026494184974581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020265
Iteration 2/1000 | Loss: 0.00002288
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001595
Iteration 5/1000 | Loss: 0.00001519
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001446
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001423
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001402
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001396
Iteration 19/1000 | Loss: 0.00001395
Iteration 20/1000 | Loss: 0.00001395
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001394
Iteration 23/1000 | Loss: 0.00001394
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001392
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001390
Iteration 29/1000 | Loss: 0.00001390
Iteration 30/1000 | Loss: 0.00001389
Iteration 31/1000 | Loss: 0.00001389
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001387
Iteration 34/1000 | Loss: 0.00001387
Iteration 35/1000 | Loss: 0.00001387
Iteration 36/1000 | Loss: 0.00001386
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001385
Iteration 39/1000 | Loss: 0.00001384
Iteration 40/1000 | Loss: 0.00001384
Iteration 41/1000 | Loss: 0.00001383
Iteration 42/1000 | Loss: 0.00001383
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001382
Iteration 45/1000 | Loss: 0.00001382
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001381
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001380
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001379
Iteration 55/1000 | Loss: 0.00001379
Iteration 56/1000 | Loss: 0.00001379
Iteration 57/1000 | Loss: 0.00001379
Iteration 58/1000 | Loss: 0.00001379
Iteration 59/1000 | Loss: 0.00001378
Iteration 60/1000 | Loss: 0.00001378
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001377
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001373
Iteration 71/1000 | Loss: 0.00001372
Iteration 72/1000 | Loss: 0.00001372
Iteration 73/1000 | Loss: 0.00001372
Iteration 74/1000 | Loss: 0.00001372
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001371
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001368
Iteration 91/1000 | Loss: 0.00001368
Iteration 92/1000 | Loss: 0.00001368
Iteration 93/1000 | Loss: 0.00001368
Iteration 94/1000 | Loss: 0.00001368
Iteration 95/1000 | Loss: 0.00001368
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001364
Iteration 107/1000 | Loss: 0.00001364
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001364
Iteration 110/1000 | Loss: 0.00001364
Iteration 111/1000 | Loss: 0.00001363
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001363
Iteration 114/1000 | Loss: 0.00001363
Iteration 115/1000 | Loss: 0.00001363
Iteration 116/1000 | Loss: 0.00001363
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3631283763970714e-05, 1.3631283763970714e-05, 1.3631283763970714e-05, 1.3631283763970714e-05, 1.3631283763970714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3631283763970714e-05

Optimization complete. Final v2v error: 3.176121950149536 mm

Highest mean error: 3.424821138381958 mm for frame 98

Lowest mean error: 2.9065229892730713 mm for frame 34

Saving results

Total time: 34.95877742767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802494
Iteration 2/25 | Loss: 0.00131111
Iteration 3/25 | Loss: 0.00096579
Iteration 4/25 | Loss: 0.00092371
Iteration 5/25 | Loss: 0.00091438
Iteration 6/25 | Loss: 0.00091252
Iteration 7/25 | Loss: 0.00091218
Iteration 8/25 | Loss: 0.00091218
Iteration 9/25 | Loss: 0.00091218
Iteration 10/25 | Loss: 0.00091218
Iteration 11/25 | Loss: 0.00091218
Iteration 12/25 | Loss: 0.00091218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009121836046688259, 0.0009121836046688259, 0.0009121836046688259, 0.0009121836046688259, 0.0009121836046688259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009121836046688259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44809079
Iteration 2/25 | Loss: 0.00031371
Iteration 3/25 | Loss: 0.00031368
Iteration 4/25 | Loss: 0.00031368
Iteration 5/25 | Loss: 0.00031368
Iteration 6/25 | Loss: 0.00031368
Iteration 7/25 | Loss: 0.00031368
Iteration 8/25 | Loss: 0.00031368
Iteration 9/25 | Loss: 0.00031368
Iteration 10/25 | Loss: 0.00031368
Iteration 11/25 | Loss: 0.00031368
Iteration 12/25 | Loss: 0.00031368
Iteration 13/25 | Loss: 0.00031368
Iteration 14/25 | Loss: 0.00031368
Iteration 15/25 | Loss: 0.00031368
Iteration 16/25 | Loss: 0.00031368
Iteration 17/25 | Loss: 0.00031368
Iteration 18/25 | Loss: 0.00031368
Iteration 19/25 | Loss: 0.00031368
Iteration 20/25 | Loss: 0.00031368
Iteration 21/25 | Loss: 0.00031368
Iteration 22/25 | Loss: 0.00031368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00031367538031190634, 0.00031367538031190634, 0.00031367538031190634, 0.00031367538031190634, 0.00031367538031190634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031367538031190634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031368
Iteration 2/1000 | Loss: 0.00004820
Iteration 3/1000 | Loss: 0.00003441
Iteration 4/1000 | Loss: 0.00003155
Iteration 5/1000 | Loss: 0.00003025
Iteration 6/1000 | Loss: 0.00002954
Iteration 7/1000 | Loss: 0.00002908
Iteration 8/1000 | Loss: 0.00002872
Iteration 9/1000 | Loss: 0.00002853
Iteration 10/1000 | Loss: 0.00002838
Iteration 11/1000 | Loss: 0.00002837
Iteration 12/1000 | Loss: 0.00002836
Iteration 13/1000 | Loss: 0.00002831
Iteration 14/1000 | Loss: 0.00002827
Iteration 15/1000 | Loss: 0.00002823
Iteration 16/1000 | Loss: 0.00002822
Iteration 17/1000 | Loss: 0.00002822
Iteration 18/1000 | Loss: 0.00002821
Iteration 19/1000 | Loss: 0.00002820
Iteration 20/1000 | Loss: 0.00002820
Iteration 21/1000 | Loss: 0.00002820
Iteration 22/1000 | Loss: 0.00002820
Iteration 23/1000 | Loss: 0.00002819
Iteration 24/1000 | Loss: 0.00002819
Iteration 25/1000 | Loss: 0.00002819
Iteration 26/1000 | Loss: 0.00002819
Iteration 27/1000 | Loss: 0.00002819
Iteration 28/1000 | Loss: 0.00002819
Iteration 29/1000 | Loss: 0.00002819
Iteration 30/1000 | Loss: 0.00002819
Iteration 31/1000 | Loss: 0.00002819
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00002819
Iteration 34/1000 | Loss: 0.00002819
Iteration 35/1000 | Loss: 0.00002818
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002818
Iteration 38/1000 | Loss: 0.00002818
Iteration 39/1000 | Loss: 0.00002818
Iteration 40/1000 | Loss: 0.00002818
Iteration 41/1000 | Loss: 0.00002818
Iteration 42/1000 | Loss: 0.00002818
Iteration 43/1000 | Loss: 0.00002818
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002818
Iteration 47/1000 | Loss: 0.00002817
Iteration 48/1000 | Loss: 0.00002817
Iteration 49/1000 | Loss: 0.00002817
Iteration 50/1000 | Loss: 0.00002817
Iteration 51/1000 | Loss: 0.00002816
Iteration 52/1000 | Loss: 0.00002816
Iteration 53/1000 | Loss: 0.00002816
Iteration 54/1000 | Loss: 0.00002816
Iteration 55/1000 | Loss: 0.00002816
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002816
Iteration 58/1000 | Loss: 0.00002815
Iteration 59/1000 | Loss: 0.00002815
Iteration 60/1000 | Loss: 0.00002815
Iteration 61/1000 | Loss: 0.00002815
Iteration 62/1000 | Loss: 0.00002815
Iteration 63/1000 | Loss: 0.00002815
Iteration 64/1000 | Loss: 0.00002815
Iteration 65/1000 | Loss: 0.00002815
Iteration 66/1000 | Loss: 0.00002815
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002815
Iteration 69/1000 | Loss: 0.00002815
Iteration 70/1000 | Loss: 0.00002815
Iteration 71/1000 | Loss: 0.00002814
Iteration 72/1000 | Loss: 0.00002814
Iteration 73/1000 | Loss: 0.00002814
Iteration 74/1000 | Loss: 0.00002814
Iteration 75/1000 | Loss: 0.00002814
Iteration 76/1000 | Loss: 0.00002814
Iteration 77/1000 | Loss: 0.00002814
Iteration 78/1000 | Loss: 0.00002814
Iteration 79/1000 | Loss: 0.00002814
Iteration 80/1000 | Loss: 0.00002814
Iteration 81/1000 | Loss: 0.00002814
Iteration 82/1000 | Loss: 0.00002814
Iteration 83/1000 | Loss: 0.00002814
Iteration 84/1000 | Loss: 0.00002814
Iteration 85/1000 | Loss: 0.00002814
Iteration 86/1000 | Loss: 0.00002814
Iteration 87/1000 | Loss: 0.00002814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [2.8137797926319763e-05, 2.8137797926319763e-05, 2.8137797926319763e-05, 2.8137797926319763e-05, 2.8137797926319763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8137797926319763e-05

Optimization complete. Final v2v error: 4.211948394775391 mm

Highest mean error: 4.575953960418701 mm for frame 80

Lowest mean error: 3.219407320022583 mm for frame 4

Saving results

Total time: 30.176374673843384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001212
Iteration 2/25 | Loss: 0.00257358
Iteration 3/25 | Loss: 0.00138041
Iteration 4/25 | Loss: 0.00125204
Iteration 5/25 | Loss: 0.00119359
Iteration 6/25 | Loss: 0.00124459
Iteration 7/25 | Loss: 0.00120086
Iteration 8/25 | Loss: 0.00115848
Iteration 9/25 | Loss: 0.00112979
Iteration 10/25 | Loss: 0.00111051
Iteration 11/25 | Loss: 0.00111434
Iteration 12/25 | Loss: 0.00112214
Iteration 13/25 | Loss: 0.00112163
Iteration 14/25 | Loss: 0.00111579
Iteration 15/25 | Loss: 0.00110568
Iteration 16/25 | Loss: 0.00110249
Iteration 17/25 | Loss: 0.00111354
Iteration 18/25 | Loss: 0.00111300
Iteration 19/25 | Loss: 0.00111518
Iteration 20/25 | Loss: 0.00111228
Iteration 21/25 | Loss: 0.00110770
Iteration 22/25 | Loss: 0.00111119
Iteration 23/25 | Loss: 0.00110781
Iteration 24/25 | Loss: 0.00110503
Iteration 25/25 | Loss: 0.00109677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48702478
Iteration 2/25 | Loss: 0.00269643
Iteration 3/25 | Loss: 0.00252376
Iteration 4/25 | Loss: 0.00252376
Iteration 5/25 | Loss: 0.00252376
Iteration 6/25 | Loss: 0.00252376
Iteration 7/25 | Loss: 0.00252376
Iteration 8/25 | Loss: 0.00252376
Iteration 9/25 | Loss: 0.00252376
Iteration 10/25 | Loss: 0.00252376
Iteration 11/25 | Loss: 0.00252376
Iteration 12/25 | Loss: 0.00252376
Iteration 13/25 | Loss: 0.00252376
Iteration 14/25 | Loss: 0.00252376
Iteration 15/25 | Loss: 0.00252376
Iteration 16/25 | Loss: 0.00252376
Iteration 17/25 | Loss: 0.00252376
Iteration 18/25 | Loss: 0.00252376
Iteration 19/25 | Loss: 0.00252376
Iteration 20/25 | Loss: 0.00252376
Iteration 21/25 | Loss: 0.00252376
Iteration 22/25 | Loss: 0.00252376
Iteration 23/25 | Loss: 0.00252376
Iteration 24/25 | Loss: 0.00252376
Iteration 25/25 | Loss: 0.00252376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252376
Iteration 2/1000 | Loss: 0.00086001
Iteration 3/1000 | Loss: 0.00051706
Iteration 4/1000 | Loss: 0.00191627
Iteration 5/1000 | Loss: 0.00120831
Iteration 6/1000 | Loss: 0.00148979
Iteration 7/1000 | Loss: 0.00108006
Iteration 8/1000 | Loss: 0.00209113
Iteration 9/1000 | Loss: 0.00094604
Iteration 10/1000 | Loss: 0.00070632
Iteration 11/1000 | Loss: 0.00093086
Iteration 12/1000 | Loss: 0.00072292
Iteration 13/1000 | Loss: 0.00038806
Iteration 14/1000 | Loss: 0.00048974
Iteration 15/1000 | Loss: 0.00156751
Iteration 16/1000 | Loss: 0.00145070
Iteration 17/1000 | Loss: 0.00060407
Iteration 18/1000 | Loss: 0.00050113
Iteration 19/1000 | Loss: 0.00055398
Iteration 20/1000 | Loss: 0.00085755
Iteration 21/1000 | Loss: 0.00083714
Iteration 22/1000 | Loss: 0.00038215
Iteration 23/1000 | Loss: 0.00048436
Iteration 24/1000 | Loss: 0.00122229
Iteration 25/1000 | Loss: 0.00059808
Iteration 26/1000 | Loss: 0.00054676
Iteration 27/1000 | Loss: 0.00037409
Iteration 28/1000 | Loss: 0.00055307
Iteration 29/1000 | Loss: 0.00187224
Iteration 30/1000 | Loss: 0.00056386
Iteration 31/1000 | Loss: 0.00029369
Iteration 32/1000 | Loss: 0.00062206
Iteration 33/1000 | Loss: 0.00033105
Iteration 34/1000 | Loss: 0.00069157
Iteration 35/1000 | Loss: 0.00017181
Iteration 36/1000 | Loss: 0.00095828
Iteration 37/1000 | Loss: 0.00085986
Iteration 38/1000 | Loss: 0.00129859
Iteration 39/1000 | Loss: 0.00033892
Iteration 40/1000 | Loss: 0.00063709
Iteration 41/1000 | Loss: 0.00467415
Iteration 42/1000 | Loss: 0.00147280
Iteration 43/1000 | Loss: 0.00101967
Iteration 44/1000 | Loss: 0.00080475
Iteration 45/1000 | Loss: 0.00166080
Iteration 46/1000 | Loss: 0.00048725
Iteration 47/1000 | Loss: 0.00038717
Iteration 48/1000 | Loss: 0.00051483
Iteration 49/1000 | Loss: 0.00101327
Iteration 50/1000 | Loss: 0.00403589
Iteration 51/1000 | Loss: 0.00219314
Iteration 52/1000 | Loss: 0.00109368
Iteration 53/1000 | Loss: 0.00038367
Iteration 54/1000 | Loss: 0.00080866
Iteration 55/1000 | Loss: 0.00017725
Iteration 56/1000 | Loss: 0.00117513
Iteration 57/1000 | Loss: 0.00027053
Iteration 58/1000 | Loss: 0.00011086
Iteration 59/1000 | Loss: 0.00008809
Iteration 60/1000 | Loss: 0.00169738
Iteration 61/1000 | Loss: 0.00013641
Iteration 62/1000 | Loss: 0.00010363
Iteration 63/1000 | Loss: 0.00031133
Iteration 64/1000 | Loss: 0.00023013
Iteration 65/1000 | Loss: 0.00006102
Iteration 66/1000 | Loss: 0.00025551
Iteration 67/1000 | Loss: 0.00005468
Iteration 68/1000 | Loss: 0.00004766
Iteration 69/1000 | Loss: 0.00016969
Iteration 70/1000 | Loss: 0.00014835
Iteration 71/1000 | Loss: 0.00016551
Iteration 72/1000 | Loss: 0.00013084
Iteration 73/1000 | Loss: 0.00015299
Iteration 74/1000 | Loss: 0.00021070
Iteration 75/1000 | Loss: 0.00015819
Iteration 76/1000 | Loss: 0.00003960
Iteration 77/1000 | Loss: 0.00003824
Iteration 78/1000 | Loss: 0.00003667
Iteration 79/1000 | Loss: 0.00021884
Iteration 80/1000 | Loss: 0.00004695
Iteration 81/1000 | Loss: 0.00003852
Iteration 82/1000 | Loss: 0.00003638
Iteration 83/1000 | Loss: 0.00003553
Iteration 84/1000 | Loss: 0.00003480
Iteration 85/1000 | Loss: 0.00037712
Iteration 86/1000 | Loss: 0.00020977
Iteration 87/1000 | Loss: 0.00003487
Iteration 88/1000 | Loss: 0.00048532
Iteration 89/1000 | Loss: 0.00013787
Iteration 90/1000 | Loss: 0.00004161
Iteration 91/1000 | Loss: 0.00003715
Iteration 92/1000 | Loss: 0.00003565
Iteration 93/1000 | Loss: 0.00003501
Iteration 94/1000 | Loss: 0.00045632
Iteration 95/1000 | Loss: 0.00057501
Iteration 96/1000 | Loss: 0.00069473
Iteration 97/1000 | Loss: 0.00025451
Iteration 98/1000 | Loss: 0.00025485
Iteration 99/1000 | Loss: 0.00054567
Iteration 100/1000 | Loss: 0.00033201
Iteration 101/1000 | Loss: 0.00017482
Iteration 102/1000 | Loss: 0.00013005
Iteration 103/1000 | Loss: 0.00005296
Iteration 104/1000 | Loss: 0.00004621
Iteration 105/1000 | Loss: 0.00004240
Iteration 106/1000 | Loss: 0.00003982
Iteration 107/1000 | Loss: 0.00003705
Iteration 108/1000 | Loss: 0.00004662
Iteration 109/1000 | Loss: 0.00024264
Iteration 110/1000 | Loss: 0.00025645
Iteration 111/1000 | Loss: 0.00013909
Iteration 112/1000 | Loss: 0.00015868
Iteration 113/1000 | Loss: 0.00015159
Iteration 114/1000 | Loss: 0.00031058
Iteration 115/1000 | Loss: 0.00015879
Iteration 116/1000 | Loss: 0.00028293
Iteration 117/1000 | Loss: 0.00010820
Iteration 118/1000 | Loss: 0.00032693
Iteration 119/1000 | Loss: 0.00004721
Iteration 120/1000 | Loss: 0.00003951
Iteration 121/1000 | Loss: 0.00003612
Iteration 122/1000 | Loss: 0.00003354
Iteration 123/1000 | Loss: 0.00003225
Iteration 124/1000 | Loss: 0.00003116
Iteration 125/1000 | Loss: 0.00003047
Iteration 126/1000 | Loss: 0.00002962
Iteration 127/1000 | Loss: 0.00002909
Iteration 128/1000 | Loss: 0.00002860
Iteration 129/1000 | Loss: 0.00002806
Iteration 130/1000 | Loss: 0.00002751
Iteration 131/1000 | Loss: 0.00043872
Iteration 132/1000 | Loss: 0.00003184
Iteration 133/1000 | Loss: 0.00002748
Iteration 134/1000 | Loss: 0.00002606
Iteration 135/1000 | Loss: 0.00002503
Iteration 136/1000 | Loss: 0.00002428
Iteration 137/1000 | Loss: 0.00002388
Iteration 138/1000 | Loss: 0.00002378
Iteration 139/1000 | Loss: 0.00002374
Iteration 140/1000 | Loss: 0.00002361
Iteration 141/1000 | Loss: 0.00002361
Iteration 142/1000 | Loss: 0.00002357
Iteration 143/1000 | Loss: 0.00002347
Iteration 144/1000 | Loss: 0.00002345
Iteration 145/1000 | Loss: 0.00002345
Iteration 146/1000 | Loss: 0.00002341
Iteration 147/1000 | Loss: 0.00002340
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002339
Iteration 150/1000 | Loss: 0.00002338
Iteration 151/1000 | Loss: 0.00002336
Iteration 152/1000 | Loss: 0.00002330
Iteration 153/1000 | Loss: 0.00002327
Iteration 154/1000 | Loss: 0.00002327
Iteration 155/1000 | Loss: 0.00002327
Iteration 156/1000 | Loss: 0.00002326
Iteration 157/1000 | Loss: 0.00002326
Iteration 158/1000 | Loss: 0.00002326
Iteration 159/1000 | Loss: 0.00002325
Iteration 160/1000 | Loss: 0.00002325
Iteration 161/1000 | Loss: 0.00002325
Iteration 162/1000 | Loss: 0.00002325
Iteration 163/1000 | Loss: 0.00002325
Iteration 164/1000 | Loss: 0.00002324
Iteration 165/1000 | Loss: 0.00002324
Iteration 166/1000 | Loss: 0.00002324
Iteration 167/1000 | Loss: 0.00002324
Iteration 168/1000 | Loss: 0.00002323
Iteration 169/1000 | Loss: 0.00002323
Iteration 170/1000 | Loss: 0.00002323
Iteration 171/1000 | Loss: 0.00002323
Iteration 172/1000 | Loss: 0.00002323
Iteration 173/1000 | Loss: 0.00002322
Iteration 174/1000 | Loss: 0.00002321
Iteration 175/1000 | Loss: 0.00002321
Iteration 176/1000 | Loss: 0.00002321
Iteration 177/1000 | Loss: 0.00002320
Iteration 178/1000 | Loss: 0.00002320
Iteration 179/1000 | Loss: 0.00002320
Iteration 180/1000 | Loss: 0.00002320
Iteration 181/1000 | Loss: 0.00002320
Iteration 182/1000 | Loss: 0.00002320
Iteration 183/1000 | Loss: 0.00002320
Iteration 184/1000 | Loss: 0.00002320
Iteration 185/1000 | Loss: 0.00002320
Iteration 186/1000 | Loss: 0.00002320
Iteration 187/1000 | Loss: 0.00002319
Iteration 188/1000 | Loss: 0.00002319
Iteration 189/1000 | Loss: 0.00002319
Iteration 190/1000 | Loss: 0.00002319
Iteration 191/1000 | Loss: 0.00002319
Iteration 192/1000 | Loss: 0.00002319
Iteration 193/1000 | Loss: 0.00002319
Iteration 194/1000 | Loss: 0.00002319
Iteration 195/1000 | Loss: 0.00002319
Iteration 196/1000 | Loss: 0.00002319
Iteration 197/1000 | Loss: 0.00002319
Iteration 198/1000 | Loss: 0.00002319
Iteration 199/1000 | Loss: 0.00002319
Iteration 200/1000 | Loss: 0.00002319
Iteration 201/1000 | Loss: 0.00002319
Iteration 202/1000 | Loss: 0.00002319
Iteration 203/1000 | Loss: 0.00002319
Iteration 204/1000 | Loss: 0.00002319
Iteration 205/1000 | Loss: 0.00002319
Iteration 206/1000 | Loss: 0.00002318
Iteration 207/1000 | Loss: 0.00002318
Iteration 208/1000 | Loss: 0.00002318
Iteration 209/1000 | Loss: 0.00002318
Iteration 210/1000 | Loss: 0.00002318
Iteration 211/1000 | Loss: 0.00002318
Iteration 212/1000 | Loss: 0.00002318
Iteration 213/1000 | Loss: 0.00002318
Iteration 214/1000 | Loss: 0.00002318
Iteration 215/1000 | Loss: 0.00002318
Iteration 216/1000 | Loss: 0.00002318
Iteration 217/1000 | Loss: 0.00002318
Iteration 218/1000 | Loss: 0.00002318
Iteration 219/1000 | Loss: 0.00002318
Iteration 220/1000 | Loss: 0.00002318
Iteration 221/1000 | Loss: 0.00002318
Iteration 222/1000 | Loss: 0.00002318
Iteration 223/1000 | Loss: 0.00002318
Iteration 224/1000 | Loss: 0.00002318
Iteration 225/1000 | Loss: 0.00002318
Iteration 226/1000 | Loss: 0.00002318
Iteration 227/1000 | Loss: 0.00002318
Iteration 228/1000 | Loss: 0.00002318
Iteration 229/1000 | Loss: 0.00002318
Iteration 230/1000 | Loss: 0.00002318
Iteration 231/1000 | Loss: 0.00002318
Iteration 232/1000 | Loss: 0.00002318
Iteration 233/1000 | Loss: 0.00002318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.3176693503046408e-05, 2.3176693503046408e-05, 2.3176693503046408e-05, 2.3176693503046408e-05, 2.3176693503046408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3176693503046408e-05

Optimization complete. Final v2v error: 3.9241762161254883 mm

Highest mean error: 6.123661518096924 mm for frame 4

Lowest mean error: 3.4932687282562256 mm for frame 103

Saving results

Total time: 255.52233266830444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354534
Iteration 2/25 | Loss: 0.00099901
Iteration 3/25 | Loss: 0.00080609
Iteration 4/25 | Loss: 0.00075947
Iteration 5/25 | Loss: 0.00074118
Iteration 6/25 | Loss: 0.00073574
Iteration 7/25 | Loss: 0.00073398
Iteration 8/25 | Loss: 0.00073321
Iteration 9/25 | Loss: 0.00073316
Iteration 10/25 | Loss: 0.00073316
Iteration 11/25 | Loss: 0.00073316
Iteration 12/25 | Loss: 0.00073316
Iteration 13/25 | Loss: 0.00073316
Iteration 14/25 | Loss: 0.00073316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007331555243581533, 0.0007331555243581533, 0.0007331555243581533, 0.0007331555243581533, 0.0007331555243581533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007331555243581533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50077271
Iteration 2/25 | Loss: 0.00029133
Iteration 3/25 | Loss: 0.00029133
Iteration 4/25 | Loss: 0.00029133
Iteration 5/25 | Loss: 0.00029133
Iteration 6/25 | Loss: 0.00029133
Iteration 7/25 | Loss: 0.00029133
Iteration 8/25 | Loss: 0.00029133
Iteration 9/25 | Loss: 0.00029133
Iteration 10/25 | Loss: 0.00029133
Iteration 11/25 | Loss: 0.00029133
Iteration 12/25 | Loss: 0.00029133
Iteration 13/25 | Loss: 0.00029133
Iteration 14/25 | Loss: 0.00029133
Iteration 15/25 | Loss: 0.00029133
Iteration 16/25 | Loss: 0.00029133
Iteration 17/25 | Loss: 0.00029133
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002913257048930973, 0.0002913257048930973, 0.0002913257048930973, 0.0002913257048930973, 0.0002913257048930973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002913257048930973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029133
Iteration 2/1000 | Loss: 0.00003671
Iteration 3/1000 | Loss: 0.00002423
Iteration 4/1000 | Loss: 0.00002072
Iteration 5/1000 | Loss: 0.00001918
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001751
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001658
Iteration 12/1000 | Loss: 0.00001658
Iteration 13/1000 | Loss: 0.00001651
Iteration 14/1000 | Loss: 0.00001646
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001640
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001634
Iteration 20/1000 | Loss: 0.00001630
Iteration 21/1000 | Loss: 0.00001626
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001621
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001618
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001614
Iteration 31/1000 | Loss: 0.00001614
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001612
Iteration 35/1000 | Loss: 0.00001611
Iteration 36/1000 | Loss: 0.00001611
Iteration 37/1000 | Loss: 0.00001611
Iteration 38/1000 | Loss: 0.00001611
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001605
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001604
Iteration 55/1000 | Loss: 0.00001604
Iteration 56/1000 | Loss: 0.00001604
Iteration 57/1000 | Loss: 0.00001604
Iteration 58/1000 | Loss: 0.00001604
Iteration 59/1000 | Loss: 0.00001603
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001603
Iteration 62/1000 | Loss: 0.00001603
Iteration 63/1000 | Loss: 0.00001603
Iteration 64/1000 | Loss: 0.00001603
Iteration 65/1000 | Loss: 0.00001603
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001603
Iteration 68/1000 | Loss: 0.00001602
Iteration 69/1000 | Loss: 0.00001602
Iteration 70/1000 | Loss: 0.00001602
Iteration 71/1000 | Loss: 0.00001601
Iteration 72/1000 | Loss: 0.00001601
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001601
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001600
Iteration 79/1000 | Loss: 0.00001600
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001599
Iteration 82/1000 | Loss: 0.00001599
Iteration 83/1000 | Loss: 0.00001599
Iteration 84/1000 | Loss: 0.00001599
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001599
Iteration 87/1000 | Loss: 0.00001599
Iteration 88/1000 | Loss: 0.00001598
Iteration 89/1000 | Loss: 0.00001598
Iteration 90/1000 | Loss: 0.00001598
Iteration 91/1000 | Loss: 0.00001598
Iteration 92/1000 | Loss: 0.00001598
Iteration 93/1000 | Loss: 0.00001598
Iteration 94/1000 | Loss: 0.00001598
Iteration 95/1000 | Loss: 0.00001598
Iteration 96/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.5983117918949574e-05, 1.5983117918949574e-05, 1.5983117918949574e-05, 1.5983117918949574e-05, 1.5983117918949574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5983117918949574e-05

Optimization complete. Final v2v error: 3.32132625579834 mm

Highest mean error: 4.710960865020752 mm for frame 151

Lowest mean error: 2.476484775543213 mm for frame 166

Saving results

Total time: 39.0408935546875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475290
Iteration 2/25 | Loss: 0.00088068
Iteration 3/25 | Loss: 0.00073927
Iteration 4/25 | Loss: 0.00072102
Iteration 5/25 | Loss: 0.00071770
Iteration 6/25 | Loss: 0.00071757
Iteration 7/25 | Loss: 0.00071757
Iteration 8/25 | Loss: 0.00071757
Iteration 9/25 | Loss: 0.00071757
Iteration 10/25 | Loss: 0.00071757
Iteration 11/25 | Loss: 0.00071757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007175697828643024, 0.0007175697828643024, 0.0007175697828643024, 0.0007175697828643024, 0.0007175697828643024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007175697828643024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12010860
Iteration 2/25 | Loss: 0.00022706
Iteration 3/25 | Loss: 0.00022706
Iteration 4/25 | Loss: 0.00022706
Iteration 5/25 | Loss: 0.00022706
Iteration 6/25 | Loss: 0.00022706
Iteration 7/25 | Loss: 0.00022706
Iteration 8/25 | Loss: 0.00022706
Iteration 9/25 | Loss: 0.00022706
Iteration 10/25 | Loss: 0.00022706
Iteration 11/25 | Loss: 0.00022706
Iteration 12/25 | Loss: 0.00022706
Iteration 13/25 | Loss: 0.00022706
Iteration 14/25 | Loss: 0.00022706
Iteration 15/25 | Loss: 0.00022706
Iteration 16/25 | Loss: 0.00022706
Iteration 17/25 | Loss: 0.00022706
Iteration 18/25 | Loss: 0.00022706
Iteration 19/25 | Loss: 0.00022706
Iteration 20/25 | Loss: 0.00022706
Iteration 21/25 | Loss: 0.00022706
Iteration 22/25 | Loss: 0.00022706
Iteration 23/25 | Loss: 0.00022706
Iteration 24/25 | Loss: 0.00022706
Iteration 25/25 | Loss: 0.00022706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022706
Iteration 2/1000 | Loss: 0.00002082
Iteration 3/1000 | Loss: 0.00001698
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001504
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001399
Iteration 12/1000 | Loss: 0.00001387
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001369
Iteration 16/1000 | Loss: 0.00001356
Iteration 17/1000 | Loss: 0.00001354
Iteration 18/1000 | Loss: 0.00001354
Iteration 19/1000 | Loss: 0.00001353
Iteration 20/1000 | Loss: 0.00001353
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001348
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001340
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001324
Iteration 32/1000 | Loss: 0.00001324
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001321
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001319
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001318
Iteration 65/1000 | Loss: 0.00001318
Iteration 66/1000 | Loss: 0.00001318
Iteration 67/1000 | Loss: 0.00001318
Iteration 68/1000 | Loss: 0.00001318
Iteration 69/1000 | Loss: 0.00001318
Iteration 70/1000 | Loss: 0.00001317
Iteration 71/1000 | Loss: 0.00001317
Iteration 72/1000 | Loss: 0.00001317
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001316
Iteration 76/1000 | Loss: 0.00001316
Iteration 77/1000 | Loss: 0.00001316
Iteration 78/1000 | Loss: 0.00001316
Iteration 79/1000 | Loss: 0.00001315
Iteration 80/1000 | Loss: 0.00001315
Iteration 81/1000 | Loss: 0.00001315
Iteration 82/1000 | Loss: 0.00001315
Iteration 83/1000 | Loss: 0.00001314
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001314
Iteration 94/1000 | Loss: 0.00001314
Iteration 95/1000 | Loss: 0.00001314
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001313
Iteration 99/1000 | Loss: 0.00001313
Iteration 100/1000 | Loss: 0.00001313
Iteration 101/1000 | Loss: 0.00001313
Iteration 102/1000 | Loss: 0.00001313
Iteration 103/1000 | Loss: 0.00001313
Iteration 104/1000 | Loss: 0.00001313
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001312
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001312
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001312
Iteration 129/1000 | Loss: 0.00001312
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.312330823566299e-05, 1.312330823566299e-05, 1.312330823566299e-05, 1.312330823566299e-05, 1.312330823566299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.312330823566299e-05

Optimization complete. Final v2v error: 3.085204601287842 mm

Highest mean error: 3.383225440979004 mm for frame 228

Lowest mean error: 2.919832944869995 mm for frame 170

Saving results

Total time: 38.8231885433197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051734
Iteration 2/25 | Loss: 0.00165136
Iteration 3/25 | Loss: 0.00114183
Iteration 4/25 | Loss: 0.00100347
Iteration 5/25 | Loss: 0.00095640
Iteration 6/25 | Loss: 0.00096783
Iteration 7/25 | Loss: 0.00101778
Iteration 8/25 | Loss: 0.00099715
Iteration 9/25 | Loss: 0.00095199
Iteration 10/25 | Loss: 0.00084967
Iteration 11/25 | Loss: 0.00082638
Iteration 12/25 | Loss: 0.00081259
Iteration 13/25 | Loss: 0.00081144
Iteration 14/25 | Loss: 0.00080889
Iteration 15/25 | Loss: 0.00079739
Iteration 16/25 | Loss: 0.00080383
Iteration 17/25 | Loss: 0.00080742
Iteration 18/25 | Loss: 0.00080877
Iteration 19/25 | Loss: 0.00079303
Iteration 20/25 | Loss: 0.00078623
Iteration 21/25 | Loss: 0.00078127
Iteration 22/25 | Loss: 0.00077779
Iteration 23/25 | Loss: 0.00078312
Iteration 24/25 | Loss: 0.00077088
Iteration 25/25 | Loss: 0.00078878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51569712
Iteration 2/25 | Loss: 0.00049799
Iteration 3/25 | Loss: 0.00049799
Iteration 4/25 | Loss: 0.00049799
Iteration 5/25 | Loss: 0.00049799
Iteration 6/25 | Loss: 0.00049799
Iteration 7/25 | Loss: 0.00049799
Iteration 8/25 | Loss: 0.00049799
Iteration 9/25 | Loss: 0.00049799
Iteration 10/25 | Loss: 0.00049799
Iteration 11/25 | Loss: 0.00049799
Iteration 12/25 | Loss: 0.00049799
Iteration 13/25 | Loss: 0.00049799
Iteration 14/25 | Loss: 0.00049799
Iteration 15/25 | Loss: 0.00049799
Iteration 16/25 | Loss: 0.00049799
Iteration 17/25 | Loss: 0.00049799
Iteration 18/25 | Loss: 0.00049799
Iteration 19/25 | Loss: 0.00049799
Iteration 20/25 | Loss: 0.00049799
Iteration 21/25 | Loss: 0.00049799
Iteration 22/25 | Loss: 0.00049799
Iteration 23/25 | Loss: 0.00049799
Iteration 24/25 | Loss: 0.00049799
Iteration 25/25 | Loss: 0.00049799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049799
Iteration 2/1000 | Loss: 0.00074716
Iteration 3/1000 | Loss: 0.00019707
Iteration 4/1000 | Loss: 0.00019292
Iteration 5/1000 | Loss: 0.00026053
Iteration 6/1000 | Loss: 0.00076049
Iteration 7/1000 | Loss: 0.00046865
Iteration 8/1000 | Loss: 0.00032980
Iteration 9/1000 | Loss: 0.00062858
Iteration 10/1000 | Loss: 0.00079345
Iteration 11/1000 | Loss: 0.00163342
Iteration 12/1000 | Loss: 0.00036623
Iteration 13/1000 | Loss: 0.00035600
Iteration 14/1000 | Loss: 0.00053820
Iteration 15/1000 | Loss: 0.00031506
Iteration 16/1000 | Loss: 0.00023999
Iteration 17/1000 | Loss: 0.00023148
Iteration 18/1000 | Loss: 0.00020787
Iteration 19/1000 | Loss: 0.00020013
Iteration 20/1000 | Loss: 0.00033739
Iteration 21/1000 | Loss: 0.00021766
Iteration 22/1000 | Loss: 0.00014386
Iteration 23/1000 | Loss: 0.00070435
Iteration 24/1000 | Loss: 0.00030438
Iteration 25/1000 | Loss: 0.00038890
Iteration 26/1000 | Loss: 0.00070503
Iteration 27/1000 | Loss: 0.00070437
Iteration 28/1000 | Loss: 0.00106667
Iteration 29/1000 | Loss: 0.00106358
Iteration 30/1000 | Loss: 0.00023209
Iteration 31/1000 | Loss: 0.00021807
Iteration 32/1000 | Loss: 0.00009639
Iteration 33/1000 | Loss: 0.00023668
Iteration 34/1000 | Loss: 0.00047708
Iteration 35/1000 | Loss: 0.00108105
Iteration 36/1000 | Loss: 0.00077363
Iteration 37/1000 | Loss: 0.00093247
Iteration 38/1000 | Loss: 0.00061629
Iteration 39/1000 | Loss: 0.00041570
Iteration 40/1000 | Loss: 0.00061640
Iteration 41/1000 | Loss: 0.00060026
Iteration 42/1000 | Loss: 0.00024340
Iteration 43/1000 | Loss: 0.00061846
Iteration 44/1000 | Loss: 0.00077658
Iteration 45/1000 | Loss: 0.00071909
Iteration 46/1000 | Loss: 0.00045898
Iteration 47/1000 | Loss: 0.00048025
Iteration 48/1000 | Loss: 0.00030727
Iteration 49/1000 | Loss: 0.00037130
Iteration 50/1000 | Loss: 0.00044454
Iteration 51/1000 | Loss: 0.00155714
Iteration 52/1000 | Loss: 0.00067835
Iteration 53/1000 | Loss: 0.00073866
Iteration 54/1000 | Loss: 0.00043805
Iteration 55/1000 | Loss: 0.00023533
Iteration 56/1000 | Loss: 0.00067102
Iteration 57/1000 | Loss: 0.00024961
Iteration 58/1000 | Loss: 0.00014319
Iteration 59/1000 | Loss: 0.00021428
Iteration 60/1000 | Loss: 0.00025549
Iteration 61/1000 | Loss: 0.00020307
Iteration 62/1000 | Loss: 0.00018003
Iteration 63/1000 | Loss: 0.00010542
Iteration 64/1000 | Loss: 0.00004830
Iteration 65/1000 | Loss: 0.00023058
Iteration 66/1000 | Loss: 0.00017522
Iteration 67/1000 | Loss: 0.00011518
Iteration 68/1000 | Loss: 0.00006569
Iteration 69/1000 | Loss: 0.00029825
Iteration 70/1000 | Loss: 0.00038184
Iteration 71/1000 | Loss: 0.00135487
Iteration 72/1000 | Loss: 0.00049848
Iteration 73/1000 | Loss: 0.00011844
Iteration 74/1000 | Loss: 0.00014528
Iteration 75/1000 | Loss: 0.00026266
Iteration 76/1000 | Loss: 0.00034997
Iteration 77/1000 | Loss: 0.00044715
Iteration 78/1000 | Loss: 0.00037576
Iteration 79/1000 | Loss: 0.00004722
Iteration 80/1000 | Loss: 0.00014470
Iteration 81/1000 | Loss: 0.00050489
Iteration 82/1000 | Loss: 0.00010082
Iteration 83/1000 | Loss: 0.00011230
Iteration 84/1000 | Loss: 0.00038710
Iteration 85/1000 | Loss: 0.00013898
Iteration 86/1000 | Loss: 0.00006906
Iteration 87/1000 | Loss: 0.00015298
Iteration 88/1000 | Loss: 0.00018674
Iteration 89/1000 | Loss: 0.00006846
Iteration 90/1000 | Loss: 0.00004477
Iteration 91/1000 | Loss: 0.00003859
Iteration 92/1000 | Loss: 0.00003595
Iteration 93/1000 | Loss: 0.00016409
Iteration 94/1000 | Loss: 0.00003589
Iteration 95/1000 | Loss: 0.00038764
Iteration 96/1000 | Loss: 0.00028871
Iteration 97/1000 | Loss: 0.00098811
Iteration 98/1000 | Loss: 0.00065767
Iteration 99/1000 | Loss: 0.00008534
Iteration 100/1000 | Loss: 0.00052856
Iteration 101/1000 | Loss: 0.00029593
Iteration 102/1000 | Loss: 0.00021135
Iteration 103/1000 | Loss: 0.00009896
Iteration 104/1000 | Loss: 0.00009008
Iteration 105/1000 | Loss: 0.00009873
Iteration 106/1000 | Loss: 0.00013255
Iteration 107/1000 | Loss: 0.00100303
Iteration 108/1000 | Loss: 0.00031330
Iteration 109/1000 | Loss: 0.00052136
Iteration 110/1000 | Loss: 0.00063742
Iteration 111/1000 | Loss: 0.00062620
Iteration 112/1000 | Loss: 0.00085101
Iteration 113/1000 | Loss: 0.00102459
Iteration 114/1000 | Loss: 0.00033080
Iteration 115/1000 | Loss: 0.00054821
Iteration 116/1000 | Loss: 0.00065993
Iteration 117/1000 | Loss: 0.00030585
Iteration 118/1000 | Loss: 0.00043184
Iteration 119/1000 | Loss: 0.00042872
Iteration 120/1000 | Loss: 0.00033636
Iteration 121/1000 | Loss: 0.00005891
Iteration 122/1000 | Loss: 0.00013267
Iteration 123/1000 | Loss: 0.00011641
Iteration 124/1000 | Loss: 0.00010132
Iteration 125/1000 | Loss: 0.00003379
Iteration 126/1000 | Loss: 0.00002821
Iteration 127/1000 | Loss: 0.00019709
Iteration 128/1000 | Loss: 0.00003673
Iteration 129/1000 | Loss: 0.00002882
Iteration 130/1000 | Loss: 0.00002483
Iteration 131/1000 | Loss: 0.00002326
Iteration 132/1000 | Loss: 0.00002163
Iteration 133/1000 | Loss: 0.00012418
Iteration 134/1000 | Loss: 0.00043756
Iteration 135/1000 | Loss: 0.00003503
Iteration 136/1000 | Loss: 0.00002256
Iteration 137/1000 | Loss: 0.00001990
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001593
Iteration 141/1000 | Loss: 0.00001506
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001386
Iteration 144/1000 | Loss: 0.00001349
Iteration 145/1000 | Loss: 0.00001346
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001307
Iteration 149/1000 | Loss: 0.00001291
Iteration 150/1000 | Loss: 0.00001290
Iteration 151/1000 | Loss: 0.00001287
Iteration 152/1000 | Loss: 0.00001286
Iteration 153/1000 | Loss: 0.00001286
Iteration 154/1000 | Loss: 0.00001286
Iteration 155/1000 | Loss: 0.00001285
Iteration 156/1000 | Loss: 0.00001285
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001281
Iteration 162/1000 | Loss: 0.00001281
Iteration 163/1000 | Loss: 0.00001280
Iteration 164/1000 | Loss: 0.00001280
Iteration 165/1000 | Loss: 0.00001280
Iteration 166/1000 | Loss: 0.00001279
Iteration 167/1000 | Loss: 0.00001279
Iteration 168/1000 | Loss: 0.00001279
Iteration 169/1000 | Loss: 0.00001279
Iteration 170/1000 | Loss: 0.00001278
Iteration 171/1000 | Loss: 0.00001278
Iteration 172/1000 | Loss: 0.00001278
Iteration 173/1000 | Loss: 0.00001277
Iteration 174/1000 | Loss: 0.00001277
Iteration 175/1000 | Loss: 0.00001277
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001276
Iteration 179/1000 | Loss: 0.00001275
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001275
Iteration 182/1000 | Loss: 0.00001274
Iteration 183/1000 | Loss: 0.00001274
Iteration 184/1000 | Loss: 0.00001274
Iteration 185/1000 | Loss: 0.00001273
Iteration 186/1000 | Loss: 0.00001273
Iteration 187/1000 | Loss: 0.00001273
Iteration 188/1000 | Loss: 0.00001273
Iteration 189/1000 | Loss: 0.00001272
Iteration 190/1000 | Loss: 0.00001272
Iteration 191/1000 | Loss: 0.00001272
Iteration 192/1000 | Loss: 0.00001272
Iteration 193/1000 | Loss: 0.00001272
Iteration 194/1000 | Loss: 0.00001272
Iteration 195/1000 | Loss: 0.00001272
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Iteration 208/1000 | Loss: 0.00001271
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001270
Iteration 211/1000 | Loss: 0.00001270
Iteration 212/1000 | Loss: 0.00001270
Iteration 213/1000 | Loss: 0.00001270
Iteration 214/1000 | Loss: 0.00001270
Iteration 215/1000 | Loss: 0.00001270
Iteration 216/1000 | Loss: 0.00001270
Iteration 217/1000 | Loss: 0.00001270
Iteration 218/1000 | Loss: 0.00001269
Iteration 219/1000 | Loss: 0.00001269
Iteration 220/1000 | Loss: 0.00001269
Iteration 221/1000 | Loss: 0.00001269
Iteration 222/1000 | Loss: 0.00001269
Iteration 223/1000 | Loss: 0.00001269
Iteration 224/1000 | Loss: 0.00001269
Iteration 225/1000 | Loss: 0.00001269
Iteration 226/1000 | Loss: 0.00001269
Iteration 227/1000 | Loss: 0.00001269
Iteration 228/1000 | Loss: 0.00001269
Iteration 229/1000 | Loss: 0.00001268
Iteration 230/1000 | Loss: 0.00001268
Iteration 231/1000 | Loss: 0.00001268
Iteration 232/1000 | Loss: 0.00001268
Iteration 233/1000 | Loss: 0.00001268
Iteration 234/1000 | Loss: 0.00001268
Iteration 235/1000 | Loss: 0.00001267
Iteration 236/1000 | Loss: 0.00001267
Iteration 237/1000 | Loss: 0.00001267
Iteration 238/1000 | Loss: 0.00001267
Iteration 239/1000 | Loss: 0.00001267
Iteration 240/1000 | Loss: 0.00001267
Iteration 241/1000 | Loss: 0.00001267
Iteration 242/1000 | Loss: 0.00001267
Iteration 243/1000 | Loss: 0.00001267
Iteration 244/1000 | Loss: 0.00001267
Iteration 245/1000 | Loss: 0.00001267
Iteration 246/1000 | Loss: 0.00001267
Iteration 247/1000 | Loss: 0.00001267
Iteration 248/1000 | Loss: 0.00001267
Iteration 249/1000 | Loss: 0.00001267
Iteration 250/1000 | Loss: 0.00001267
Iteration 251/1000 | Loss: 0.00001267
Iteration 252/1000 | Loss: 0.00001267
Iteration 253/1000 | Loss: 0.00001267
Iteration 254/1000 | Loss: 0.00001267
Iteration 255/1000 | Loss: 0.00001267
Iteration 256/1000 | Loss: 0.00001267
Iteration 257/1000 | Loss: 0.00001267
Iteration 258/1000 | Loss: 0.00001267
Iteration 259/1000 | Loss: 0.00001267
Iteration 260/1000 | Loss: 0.00001267
Iteration 261/1000 | Loss: 0.00001267
Iteration 262/1000 | Loss: 0.00001267
Iteration 263/1000 | Loss: 0.00001267
Iteration 264/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.2666927432292141e-05, 1.2666927432292141e-05, 1.2666927432292141e-05, 1.2666927432292141e-05, 1.2666927432292141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2666927432292141e-05

Optimization complete. Final v2v error: 2.956775426864624 mm

Highest mean error: 4.252232551574707 mm for frame 63

Lowest mean error: 2.5518550872802734 mm for frame 148

Saving results

Total time: 263.39626955986023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christopher_posed_008/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christopher_posed_008/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629658
Iteration 2/25 | Loss: 0.00134984
Iteration 3/25 | Loss: 0.00105256
Iteration 4/25 | Loss: 0.00099847
Iteration 5/25 | Loss: 0.00098004
Iteration 6/25 | Loss: 0.00096666
Iteration 7/25 | Loss: 0.00095832
Iteration 8/25 | Loss: 0.00095372
Iteration 9/25 | Loss: 0.00094824
Iteration 10/25 | Loss: 0.00094575
Iteration 11/25 | Loss: 0.00094184
Iteration 12/25 | Loss: 0.00093883
Iteration 13/25 | Loss: 0.00093688
Iteration 14/25 | Loss: 0.00093926
Iteration 15/25 | Loss: 0.00093935
Iteration 16/25 | Loss: 0.00093606
Iteration 17/25 | Loss: 0.00093329
Iteration 18/25 | Loss: 0.00093167
Iteration 19/25 | Loss: 0.00093103
Iteration 20/25 | Loss: 0.00093088
Iteration 21/25 | Loss: 0.00093649
Iteration 22/25 | Loss: 0.00094397
Iteration 23/25 | Loss: 0.00093565
Iteration 24/25 | Loss: 0.00092636
Iteration 25/25 | Loss: 0.00092260

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45509732
Iteration 2/25 | Loss: 0.00141931
Iteration 3/25 | Loss: 0.00141928
Iteration 4/25 | Loss: 0.00141928
Iteration 5/25 | Loss: 0.00141927
Iteration 6/25 | Loss: 0.00141927
Iteration 7/25 | Loss: 0.00141927
Iteration 8/25 | Loss: 0.00141927
Iteration 9/25 | Loss: 0.00141927
Iteration 10/25 | Loss: 0.00141927
Iteration 11/25 | Loss: 0.00141927
Iteration 12/25 | Loss: 0.00141927
Iteration 13/25 | Loss: 0.00141927
Iteration 14/25 | Loss: 0.00141927
Iteration 15/25 | Loss: 0.00141927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014192734379321337, 0.0014192734379321337, 0.0014192734379321337, 0.0014192734379321337, 0.0014192734379321337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014192734379321337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141927
Iteration 2/1000 | Loss: 0.00106431
Iteration 3/1000 | Loss: 0.00020138
Iteration 4/1000 | Loss: 0.00080099
Iteration 5/1000 | Loss: 0.00015987
Iteration 6/1000 | Loss: 0.00243703
Iteration 7/1000 | Loss: 0.00227292
Iteration 8/1000 | Loss: 0.00133569
Iteration 9/1000 | Loss: 0.00112024
Iteration 10/1000 | Loss: 0.00083496
Iteration 11/1000 | Loss: 0.00116813
Iteration 12/1000 | Loss: 0.00126891
Iteration 13/1000 | Loss: 0.00014743
Iteration 14/1000 | Loss: 0.00054592
Iteration 15/1000 | Loss: 0.00117797
Iteration 16/1000 | Loss: 0.00056954
Iteration 17/1000 | Loss: 0.00012760
Iteration 18/1000 | Loss: 0.00010356
Iteration 19/1000 | Loss: 0.00009311
Iteration 20/1000 | Loss: 0.00015920
Iteration 21/1000 | Loss: 0.00100540
Iteration 22/1000 | Loss: 0.00015921
Iteration 23/1000 | Loss: 0.00009009
Iteration 24/1000 | Loss: 0.00021655
Iteration 25/1000 | Loss: 0.00007506
Iteration 26/1000 | Loss: 0.00056428
Iteration 27/1000 | Loss: 0.00073045
Iteration 28/1000 | Loss: 0.00010357
Iteration 29/1000 | Loss: 0.00007255
Iteration 30/1000 | Loss: 0.00006538
Iteration 31/1000 | Loss: 0.00006223
Iteration 32/1000 | Loss: 0.00015909
Iteration 33/1000 | Loss: 0.00005719
Iteration 34/1000 | Loss: 0.00005561
Iteration 35/1000 | Loss: 0.00005378
Iteration 36/1000 | Loss: 0.00050302
Iteration 37/1000 | Loss: 0.00007053
Iteration 38/1000 | Loss: 0.00005442
Iteration 39/1000 | Loss: 0.00005056
Iteration 40/1000 | Loss: 0.00004922
Iteration 41/1000 | Loss: 0.00074342
Iteration 42/1000 | Loss: 0.00061489
Iteration 43/1000 | Loss: 0.00005254
Iteration 44/1000 | Loss: 0.00004911
Iteration 45/1000 | Loss: 0.00041349
Iteration 46/1000 | Loss: 0.00110096
Iteration 47/1000 | Loss: 0.00340553
Iteration 48/1000 | Loss: 0.00072772
Iteration 49/1000 | Loss: 0.00006023
Iteration 50/1000 | Loss: 0.00005008
Iteration 51/1000 | Loss: 0.00004153
Iteration 52/1000 | Loss: 0.00003453
Iteration 53/1000 | Loss: 0.00003036
Iteration 54/1000 | Loss: 0.00002766
Iteration 55/1000 | Loss: 0.00026722
Iteration 56/1000 | Loss: 0.00002978
Iteration 57/1000 | Loss: 0.00002445
Iteration 58/1000 | Loss: 0.00002348
Iteration 59/1000 | Loss: 0.00002265
Iteration 60/1000 | Loss: 0.00002211
Iteration 61/1000 | Loss: 0.00002174
Iteration 62/1000 | Loss: 0.00002138
Iteration 63/1000 | Loss: 0.00002125
Iteration 64/1000 | Loss: 0.00002116
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00002105
Iteration 67/1000 | Loss: 0.00002102
Iteration 68/1000 | Loss: 0.00002097
Iteration 69/1000 | Loss: 0.00002096
Iteration 70/1000 | Loss: 0.00002094
Iteration 71/1000 | Loss: 0.00002094
Iteration 72/1000 | Loss: 0.00002093
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002091
Iteration 75/1000 | Loss: 0.00002091
Iteration 76/1000 | Loss: 0.00002090
Iteration 77/1000 | Loss: 0.00002090
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002088
Iteration 82/1000 | Loss: 0.00002088
Iteration 83/1000 | Loss: 0.00002086
Iteration 84/1000 | Loss: 0.00002086
Iteration 85/1000 | Loss: 0.00002086
Iteration 86/1000 | Loss: 0.00002086
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002085
Iteration 89/1000 | Loss: 0.00002085
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002083
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002083
Iteration 100/1000 | Loss: 0.00002083
Iteration 101/1000 | Loss: 0.00002083
Iteration 102/1000 | Loss: 0.00002083
Iteration 103/1000 | Loss: 0.00002082
Iteration 104/1000 | Loss: 0.00002082
Iteration 105/1000 | Loss: 0.00002082
Iteration 106/1000 | Loss: 0.00002082
Iteration 107/1000 | Loss: 0.00002082
Iteration 108/1000 | Loss: 0.00002082
Iteration 109/1000 | Loss: 0.00002082
Iteration 110/1000 | Loss: 0.00002082
Iteration 111/1000 | Loss: 0.00002082
Iteration 112/1000 | Loss: 0.00002082
Iteration 113/1000 | Loss: 0.00002081
Iteration 114/1000 | Loss: 0.00002081
Iteration 115/1000 | Loss: 0.00002081
Iteration 116/1000 | Loss: 0.00002081
Iteration 117/1000 | Loss: 0.00002081
Iteration 118/1000 | Loss: 0.00002081
Iteration 119/1000 | Loss: 0.00002081
Iteration 120/1000 | Loss: 0.00002081
Iteration 121/1000 | Loss: 0.00002081
Iteration 122/1000 | Loss: 0.00002081
Iteration 123/1000 | Loss: 0.00002080
Iteration 124/1000 | Loss: 0.00002080
Iteration 125/1000 | Loss: 0.00002080
Iteration 126/1000 | Loss: 0.00002080
Iteration 127/1000 | Loss: 0.00002080
Iteration 128/1000 | Loss: 0.00002080
Iteration 129/1000 | Loss: 0.00002080
Iteration 130/1000 | Loss: 0.00002080
Iteration 131/1000 | Loss: 0.00002080
Iteration 132/1000 | Loss: 0.00002080
Iteration 133/1000 | Loss: 0.00002079
Iteration 134/1000 | Loss: 0.00002079
Iteration 135/1000 | Loss: 0.00002079
Iteration 136/1000 | Loss: 0.00002079
Iteration 137/1000 | Loss: 0.00002079
Iteration 138/1000 | Loss: 0.00002079
Iteration 139/1000 | Loss: 0.00002079
Iteration 140/1000 | Loss: 0.00002079
Iteration 141/1000 | Loss: 0.00002079
Iteration 142/1000 | Loss: 0.00002079
Iteration 143/1000 | Loss: 0.00002078
Iteration 144/1000 | Loss: 0.00002078
Iteration 145/1000 | Loss: 0.00002078
Iteration 146/1000 | Loss: 0.00002078
Iteration 147/1000 | Loss: 0.00002078
Iteration 148/1000 | Loss: 0.00002078
Iteration 149/1000 | Loss: 0.00002077
Iteration 150/1000 | Loss: 0.00002077
Iteration 151/1000 | Loss: 0.00002077
Iteration 152/1000 | Loss: 0.00002077
Iteration 153/1000 | Loss: 0.00002077
Iteration 154/1000 | Loss: 0.00002077
Iteration 155/1000 | Loss: 0.00002076
Iteration 156/1000 | Loss: 0.00002076
Iteration 157/1000 | Loss: 0.00002076
Iteration 158/1000 | Loss: 0.00002076
Iteration 159/1000 | Loss: 0.00002076
Iteration 160/1000 | Loss: 0.00002076
Iteration 161/1000 | Loss: 0.00002076
Iteration 162/1000 | Loss: 0.00002076
Iteration 163/1000 | Loss: 0.00002075
Iteration 164/1000 | Loss: 0.00002075
Iteration 165/1000 | Loss: 0.00002075
Iteration 166/1000 | Loss: 0.00002075
Iteration 167/1000 | Loss: 0.00002075
Iteration 168/1000 | Loss: 0.00002075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.075482188956812e-05, 2.075482188956812e-05, 2.075482188956812e-05, 2.075482188956812e-05, 2.075482188956812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.075482188956812e-05

Optimization complete. Final v2v error: 3.734194755554199 mm

Highest mean error: 5.0694804191589355 mm for frame 177

Lowest mean error: 3.0320680141448975 mm for frame 208

Saving results

Total time: 164.7554476261139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430164
Iteration 2/25 | Loss: 0.00128486
Iteration 3/25 | Loss: 0.00121321
Iteration 4/25 | Loss: 0.00120306
Iteration 5/25 | Loss: 0.00120020
Iteration 6/25 | Loss: 0.00120006
Iteration 7/25 | Loss: 0.00120003
Iteration 8/25 | Loss: 0.00120001
Iteration 9/25 | Loss: 0.00120001
Iteration 10/25 | Loss: 0.00120001
Iteration 11/25 | Loss: 0.00120001
Iteration 12/25 | Loss: 0.00120001
Iteration 13/25 | Loss: 0.00120001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012000135611742735, 0.0012000135611742735, 0.0012000135611742735, 0.0012000135611742735, 0.0012000135611742735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012000135611742735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.24562907
Iteration 2/25 | Loss: 0.00136560
Iteration 3/25 | Loss: 0.00136560
Iteration 4/25 | Loss: 0.00136560
Iteration 5/25 | Loss: 0.00136560
Iteration 6/25 | Loss: 0.00136560
Iteration 7/25 | Loss: 0.00136560
Iteration 8/25 | Loss: 0.00136560
Iteration 9/25 | Loss: 0.00136560
Iteration 10/25 | Loss: 0.00136560
Iteration 11/25 | Loss: 0.00136560
Iteration 12/25 | Loss: 0.00136560
Iteration 13/25 | Loss: 0.00136560
Iteration 14/25 | Loss: 0.00136560
Iteration 15/25 | Loss: 0.00136560
Iteration 16/25 | Loss: 0.00136560
Iteration 17/25 | Loss: 0.00136560
Iteration 18/25 | Loss: 0.00136560
Iteration 19/25 | Loss: 0.00136560
Iteration 20/25 | Loss: 0.00136560
Iteration 21/25 | Loss: 0.00136560
Iteration 22/25 | Loss: 0.00136560
Iteration 23/25 | Loss: 0.00136560
Iteration 24/25 | Loss: 0.00136560
Iteration 25/25 | Loss: 0.00136560

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136560
Iteration 2/1000 | Loss: 0.00002099
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001544
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001337
Iteration 9/1000 | Loss: 0.00001307
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001273
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001257
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001242
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001229
Iteration 23/1000 | Loss: 0.00001226
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001226
Iteration 26/1000 | Loss: 0.00001226
Iteration 27/1000 | Loss: 0.00001225
Iteration 28/1000 | Loss: 0.00001225
Iteration 29/1000 | Loss: 0.00001225
Iteration 30/1000 | Loss: 0.00001225
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001223
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001222
Iteration 37/1000 | Loss: 0.00001222
Iteration 38/1000 | Loss: 0.00001221
Iteration 39/1000 | Loss: 0.00001221
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001218
Iteration 45/1000 | Loss: 0.00001218
Iteration 46/1000 | Loss: 0.00001218
Iteration 47/1000 | Loss: 0.00001217
Iteration 48/1000 | Loss: 0.00001217
Iteration 49/1000 | Loss: 0.00001217
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001216
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001211
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001210
Iteration 64/1000 | Loss: 0.00001210
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001203
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001202
Iteration 88/1000 | Loss: 0.00001202
Iteration 89/1000 | Loss: 0.00001202
Iteration 90/1000 | Loss: 0.00001202
Iteration 91/1000 | Loss: 0.00001202
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001201
Iteration 94/1000 | Loss: 0.00001201
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001198
Iteration 107/1000 | Loss: 0.00001198
Iteration 108/1000 | Loss: 0.00001198
Iteration 109/1000 | Loss: 0.00001198
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001197
Iteration 112/1000 | Loss: 0.00001197
Iteration 113/1000 | Loss: 0.00001197
Iteration 114/1000 | Loss: 0.00001197
Iteration 115/1000 | Loss: 0.00001197
Iteration 116/1000 | Loss: 0.00001197
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001196
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001195
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00001195
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001194
Iteration 139/1000 | Loss: 0.00001194
Iteration 140/1000 | Loss: 0.00001194
Iteration 141/1000 | Loss: 0.00001194
Iteration 142/1000 | Loss: 0.00001194
Iteration 143/1000 | Loss: 0.00001194
Iteration 144/1000 | Loss: 0.00001194
Iteration 145/1000 | Loss: 0.00001194
Iteration 146/1000 | Loss: 0.00001194
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001193
Iteration 149/1000 | Loss: 0.00001193
Iteration 150/1000 | Loss: 0.00001193
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001192
Iteration 158/1000 | Loss: 0.00001192
Iteration 159/1000 | Loss: 0.00001192
Iteration 160/1000 | Loss: 0.00001192
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001192
Iteration 165/1000 | Loss: 0.00001192
Iteration 166/1000 | Loss: 0.00001192
Iteration 167/1000 | Loss: 0.00001192
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001192
Iteration 179/1000 | Loss: 0.00001192
Iteration 180/1000 | Loss: 0.00001192
Iteration 181/1000 | Loss: 0.00001192
Iteration 182/1000 | Loss: 0.00001192
Iteration 183/1000 | Loss: 0.00001192
Iteration 184/1000 | Loss: 0.00001192
Iteration 185/1000 | Loss: 0.00001192
Iteration 186/1000 | Loss: 0.00001192
Iteration 187/1000 | Loss: 0.00001192
Iteration 188/1000 | Loss: 0.00001192
Iteration 189/1000 | Loss: 0.00001192
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001192
Iteration 195/1000 | Loss: 0.00001192
Iteration 196/1000 | Loss: 0.00001192
Iteration 197/1000 | Loss: 0.00001192
Iteration 198/1000 | Loss: 0.00001192
Iteration 199/1000 | Loss: 0.00001192
Iteration 200/1000 | Loss: 0.00001192
Iteration 201/1000 | Loss: 0.00001192
Iteration 202/1000 | Loss: 0.00001192
Iteration 203/1000 | Loss: 0.00001192
Iteration 204/1000 | Loss: 0.00001192
Iteration 205/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1924870705115609e-05, 1.1924870705115609e-05, 1.1924870705115609e-05, 1.1924870705115609e-05, 1.1924870705115609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1924870705115609e-05

Optimization complete. Final v2v error: 2.967956304550171 mm

Highest mean error: 3.23992919921875 mm for frame 108

Lowest mean error: 2.8246562480926514 mm for frame 3

Saving results

Total time: 42.06049370765686
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476743
Iteration 2/25 | Loss: 0.00136617
Iteration 3/25 | Loss: 0.00125137
Iteration 4/25 | Loss: 0.00123128
Iteration 5/25 | Loss: 0.00122472
Iteration 6/25 | Loss: 0.00122349
Iteration 7/25 | Loss: 0.00122349
Iteration 8/25 | Loss: 0.00122349
Iteration 9/25 | Loss: 0.00122349
Iteration 10/25 | Loss: 0.00122349
Iteration 11/25 | Loss: 0.00122349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012234932510182261, 0.0012234932510182261, 0.0012234932510182261, 0.0012234932510182261, 0.0012234932510182261]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012234932510182261

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22162151
Iteration 2/25 | Loss: 0.00195266
Iteration 3/25 | Loss: 0.00195266
Iteration 4/25 | Loss: 0.00195266
Iteration 5/25 | Loss: 0.00195266
Iteration 6/25 | Loss: 0.00195266
Iteration 7/25 | Loss: 0.00195266
Iteration 8/25 | Loss: 0.00195266
Iteration 9/25 | Loss: 0.00195266
Iteration 10/25 | Loss: 0.00195266
Iteration 11/25 | Loss: 0.00195266
Iteration 12/25 | Loss: 0.00195266
Iteration 13/25 | Loss: 0.00195266
Iteration 14/25 | Loss: 0.00195266
Iteration 15/25 | Loss: 0.00195266
Iteration 16/25 | Loss: 0.00195266
Iteration 17/25 | Loss: 0.00195266
Iteration 18/25 | Loss: 0.00195266
Iteration 19/25 | Loss: 0.00195266
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001952656893990934, 0.001952656893990934, 0.001952656893990934, 0.001952656893990934, 0.001952656893990934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001952656893990934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195266
Iteration 2/1000 | Loss: 0.00006890
Iteration 3/1000 | Loss: 0.00003907
Iteration 4/1000 | Loss: 0.00003193
Iteration 5/1000 | Loss: 0.00002962
Iteration 6/1000 | Loss: 0.00002861
Iteration 7/1000 | Loss: 0.00002789
Iteration 8/1000 | Loss: 0.00002741
Iteration 9/1000 | Loss: 0.00002719
Iteration 10/1000 | Loss: 0.00002696
Iteration 11/1000 | Loss: 0.00002675
Iteration 12/1000 | Loss: 0.00002663
Iteration 13/1000 | Loss: 0.00002648
Iteration 14/1000 | Loss: 0.00002633
Iteration 15/1000 | Loss: 0.00002630
Iteration 16/1000 | Loss: 0.00002627
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002613
Iteration 19/1000 | Loss: 0.00002604
Iteration 20/1000 | Loss: 0.00002603
Iteration 21/1000 | Loss: 0.00002598
Iteration 22/1000 | Loss: 0.00002596
Iteration 23/1000 | Loss: 0.00002596
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002595
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002594
Iteration 28/1000 | Loss: 0.00002594
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00002594
Iteration 31/1000 | Loss: 0.00002594
Iteration 32/1000 | Loss: 0.00002594
Iteration 33/1000 | Loss: 0.00002594
Iteration 34/1000 | Loss: 0.00002594
Iteration 35/1000 | Loss: 0.00002593
Iteration 36/1000 | Loss: 0.00002593
Iteration 37/1000 | Loss: 0.00002593
Iteration 38/1000 | Loss: 0.00002593
Iteration 39/1000 | Loss: 0.00002593
Iteration 40/1000 | Loss: 0.00002592
Iteration 41/1000 | Loss: 0.00002592
Iteration 42/1000 | Loss: 0.00002591
Iteration 43/1000 | Loss: 0.00002591
Iteration 44/1000 | Loss: 0.00002591
Iteration 45/1000 | Loss: 0.00002590
Iteration 46/1000 | Loss: 0.00002589
Iteration 47/1000 | Loss: 0.00002589
Iteration 48/1000 | Loss: 0.00002588
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002588
Iteration 53/1000 | Loss: 0.00002588
Iteration 54/1000 | Loss: 0.00002588
Iteration 55/1000 | Loss: 0.00002587
Iteration 56/1000 | Loss: 0.00002587
Iteration 57/1000 | Loss: 0.00002586
Iteration 58/1000 | Loss: 0.00002586
Iteration 59/1000 | Loss: 0.00002586
Iteration 60/1000 | Loss: 0.00002585
Iteration 61/1000 | Loss: 0.00002585
Iteration 62/1000 | Loss: 0.00002585
Iteration 63/1000 | Loss: 0.00002584
Iteration 64/1000 | Loss: 0.00002584
Iteration 65/1000 | Loss: 0.00002584
Iteration 66/1000 | Loss: 0.00002584
Iteration 67/1000 | Loss: 0.00002584
Iteration 68/1000 | Loss: 0.00002584
Iteration 69/1000 | Loss: 0.00002583
Iteration 70/1000 | Loss: 0.00002583
Iteration 71/1000 | Loss: 0.00002583
Iteration 72/1000 | Loss: 0.00002583
Iteration 73/1000 | Loss: 0.00002583
Iteration 74/1000 | Loss: 0.00002582
Iteration 75/1000 | Loss: 0.00002582
Iteration 76/1000 | Loss: 0.00002582
Iteration 77/1000 | Loss: 0.00002582
Iteration 78/1000 | Loss: 0.00002582
Iteration 79/1000 | Loss: 0.00002582
Iteration 80/1000 | Loss: 0.00002582
Iteration 81/1000 | Loss: 0.00002581
Iteration 82/1000 | Loss: 0.00002581
Iteration 83/1000 | Loss: 0.00002581
Iteration 84/1000 | Loss: 0.00002581
Iteration 85/1000 | Loss: 0.00002581
Iteration 86/1000 | Loss: 0.00002580
Iteration 87/1000 | Loss: 0.00002580
Iteration 88/1000 | Loss: 0.00002580
Iteration 89/1000 | Loss: 0.00002580
Iteration 90/1000 | Loss: 0.00002579
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002579
Iteration 94/1000 | Loss: 0.00002578
Iteration 95/1000 | Loss: 0.00002578
Iteration 96/1000 | Loss: 0.00002577
Iteration 97/1000 | Loss: 0.00002577
Iteration 98/1000 | Loss: 0.00002577
Iteration 99/1000 | Loss: 0.00002576
Iteration 100/1000 | Loss: 0.00002575
Iteration 101/1000 | Loss: 0.00002575
Iteration 102/1000 | Loss: 0.00002575
Iteration 103/1000 | Loss: 0.00002575
Iteration 104/1000 | Loss: 0.00002575
Iteration 105/1000 | Loss: 0.00002575
Iteration 106/1000 | Loss: 0.00002575
Iteration 107/1000 | Loss: 0.00002575
Iteration 108/1000 | Loss: 0.00002575
Iteration 109/1000 | Loss: 0.00002575
Iteration 110/1000 | Loss: 0.00002575
Iteration 111/1000 | Loss: 0.00002575
Iteration 112/1000 | Loss: 0.00002575
Iteration 113/1000 | Loss: 0.00002574
Iteration 114/1000 | Loss: 0.00002574
Iteration 115/1000 | Loss: 0.00002574
Iteration 116/1000 | Loss: 0.00002574
Iteration 117/1000 | Loss: 0.00002574
Iteration 118/1000 | Loss: 0.00002574
Iteration 119/1000 | Loss: 0.00002574
Iteration 120/1000 | Loss: 0.00002574
Iteration 121/1000 | Loss: 0.00002574
Iteration 122/1000 | Loss: 0.00002574
Iteration 123/1000 | Loss: 0.00002574
Iteration 124/1000 | Loss: 0.00002574
Iteration 125/1000 | Loss: 0.00002574
Iteration 126/1000 | Loss: 0.00002573
Iteration 127/1000 | Loss: 0.00002573
Iteration 128/1000 | Loss: 0.00002573
Iteration 129/1000 | Loss: 0.00002573
Iteration 130/1000 | Loss: 0.00002573
Iteration 131/1000 | Loss: 0.00002573
Iteration 132/1000 | Loss: 0.00002573
Iteration 133/1000 | Loss: 0.00002573
Iteration 134/1000 | Loss: 0.00002573
Iteration 135/1000 | Loss: 0.00002573
Iteration 136/1000 | Loss: 0.00002573
Iteration 137/1000 | Loss: 0.00002573
Iteration 138/1000 | Loss: 0.00002573
Iteration 139/1000 | Loss: 0.00002572
Iteration 140/1000 | Loss: 0.00002572
Iteration 141/1000 | Loss: 0.00002572
Iteration 142/1000 | Loss: 0.00002572
Iteration 143/1000 | Loss: 0.00002572
Iteration 144/1000 | Loss: 0.00002572
Iteration 145/1000 | Loss: 0.00002572
Iteration 146/1000 | Loss: 0.00002572
Iteration 147/1000 | Loss: 0.00002572
Iteration 148/1000 | Loss: 0.00002572
Iteration 149/1000 | Loss: 0.00002572
Iteration 150/1000 | Loss: 0.00002572
Iteration 151/1000 | Loss: 0.00002572
Iteration 152/1000 | Loss: 0.00002572
Iteration 153/1000 | Loss: 0.00002572
Iteration 154/1000 | Loss: 0.00002572
Iteration 155/1000 | Loss: 0.00002572
Iteration 156/1000 | Loss: 0.00002572
Iteration 157/1000 | Loss: 0.00002572
Iteration 158/1000 | Loss: 0.00002572
Iteration 159/1000 | Loss: 0.00002572
Iteration 160/1000 | Loss: 0.00002572
Iteration 161/1000 | Loss: 0.00002572
Iteration 162/1000 | Loss: 0.00002572
Iteration 163/1000 | Loss: 0.00002572
Iteration 164/1000 | Loss: 0.00002572
Iteration 165/1000 | Loss: 0.00002572
Iteration 166/1000 | Loss: 0.00002572
Iteration 167/1000 | Loss: 0.00002572
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002572
Iteration 170/1000 | Loss: 0.00002572
Iteration 171/1000 | Loss: 0.00002572
Iteration 172/1000 | Loss: 0.00002572
Iteration 173/1000 | Loss: 0.00002572
Iteration 174/1000 | Loss: 0.00002572
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.5724542865646072e-05, 2.5724542865646072e-05, 2.5724542865646072e-05, 2.5724542865646072e-05, 2.5724542865646072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5724542865646072e-05

Optimization complete. Final v2v error: 4.0428314208984375 mm

Highest mean error: 4.402608394622803 mm for frame 40

Lowest mean error: 3.7354609966278076 mm for frame 169

Saving results

Total time: 42.542810678482056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433477
Iteration 2/25 | Loss: 0.00141370
Iteration 3/25 | Loss: 0.00129793
Iteration 4/25 | Loss: 0.00128054
Iteration 5/25 | Loss: 0.00127693
Iteration 6/25 | Loss: 0.00127585
Iteration 7/25 | Loss: 0.00127571
Iteration 8/25 | Loss: 0.00127571
Iteration 9/25 | Loss: 0.00127571
Iteration 10/25 | Loss: 0.00127571
Iteration 11/25 | Loss: 0.00127571
Iteration 12/25 | Loss: 0.00127571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012757108779624104, 0.0012757108779624104, 0.0012757108779624104, 0.0012757108779624104, 0.0012757108779624104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012757108779624104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.90775013
Iteration 2/25 | Loss: 0.00120806
Iteration 3/25 | Loss: 0.00120803
Iteration 4/25 | Loss: 0.00120803
Iteration 5/25 | Loss: 0.00120803
Iteration 6/25 | Loss: 0.00120803
Iteration 7/25 | Loss: 0.00120803
Iteration 8/25 | Loss: 0.00120803
Iteration 9/25 | Loss: 0.00120803
Iteration 10/25 | Loss: 0.00120803
Iteration 11/25 | Loss: 0.00120803
Iteration 12/25 | Loss: 0.00120803
Iteration 13/25 | Loss: 0.00120803
Iteration 14/25 | Loss: 0.00120803
Iteration 15/25 | Loss: 0.00120803
Iteration 16/25 | Loss: 0.00120803
Iteration 17/25 | Loss: 0.00120803
Iteration 18/25 | Loss: 0.00120803
Iteration 19/25 | Loss: 0.00120803
Iteration 20/25 | Loss: 0.00120803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012080265441909432, 0.0012080265441909432, 0.0012080265441909432, 0.0012080265441909432, 0.0012080265441909432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012080265441909432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120803
Iteration 2/1000 | Loss: 0.00003851
Iteration 3/1000 | Loss: 0.00002616
Iteration 4/1000 | Loss: 0.00002420
Iteration 5/1000 | Loss: 0.00002325
Iteration 6/1000 | Loss: 0.00002263
Iteration 7/1000 | Loss: 0.00002184
Iteration 8/1000 | Loss: 0.00002137
Iteration 9/1000 | Loss: 0.00002093
Iteration 10/1000 | Loss: 0.00002059
Iteration 11/1000 | Loss: 0.00002035
Iteration 12/1000 | Loss: 0.00002017
Iteration 13/1000 | Loss: 0.00002013
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001994
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001990
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001976
Iteration 22/1000 | Loss: 0.00001975
Iteration 23/1000 | Loss: 0.00001975
Iteration 24/1000 | Loss: 0.00001975
Iteration 25/1000 | Loss: 0.00001975
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001974
Iteration 28/1000 | Loss: 0.00001971
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001970
Iteration 31/1000 | Loss: 0.00001970
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001968
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001965
Iteration 44/1000 | Loss: 0.00001965
Iteration 45/1000 | Loss: 0.00001965
Iteration 46/1000 | Loss: 0.00001964
Iteration 47/1000 | Loss: 0.00001964
Iteration 48/1000 | Loss: 0.00001964
Iteration 49/1000 | Loss: 0.00001963
Iteration 50/1000 | Loss: 0.00001963
Iteration 51/1000 | Loss: 0.00001963
Iteration 52/1000 | Loss: 0.00001963
Iteration 53/1000 | Loss: 0.00001963
Iteration 54/1000 | Loss: 0.00001963
Iteration 55/1000 | Loss: 0.00001963
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001961
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001961
Iteration 63/1000 | Loss: 0.00001961
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001960
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001960
Iteration 69/1000 | Loss: 0.00001960
Iteration 70/1000 | Loss: 0.00001960
Iteration 71/1000 | Loss: 0.00001960
Iteration 72/1000 | Loss: 0.00001960
Iteration 73/1000 | Loss: 0.00001960
Iteration 74/1000 | Loss: 0.00001960
Iteration 75/1000 | Loss: 0.00001959
Iteration 76/1000 | Loss: 0.00001959
Iteration 77/1000 | Loss: 0.00001959
Iteration 78/1000 | Loss: 0.00001958
Iteration 79/1000 | Loss: 0.00001958
Iteration 80/1000 | Loss: 0.00001958
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001958
Iteration 86/1000 | Loss: 0.00001958
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001954
Iteration 104/1000 | Loss: 0.00001954
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001954
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001953
Iteration 117/1000 | Loss: 0.00001953
Iteration 118/1000 | Loss: 0.00001953
Iteration 119/1000 | Loss: 0.00001953
Iteration 120/1000 | Loss: 0.00001953
Iteration 121/1000 | Loss: 0.00001953
Iteration 122/1000 | Loss: 0.00001953
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001952
Iteration 126/1000 | Loss: 0.00001952
Iteration 127/1000 | Loss: 0.00001952
Iteration 128/1000 | Loss: 0.00001952
Iteration 129/1000 | Loss: 0.00001952
Iteration 130/1000 | Loss: 0.00001952
Iteration 131/1000 | Loss: 0.00001952
Iteration 132/1000 | Loss: 0.00001952
Iteration 133/1000 | Loss: 0.00001952
Iteration 134/1000 | Loss: 0.00001952
Iteration 135/1000 | Loss: 0.00001952
Iteration 136/1000 | Loss: 0.00001952
Iteration 137/1000 | Loss: 0.00001952
Iteration 138/1000 | Loss: 0.00001952
Iteration 139/1000 | Loss: 0.00001952
Iteration 140/1000 | Loss: 0.00001952
Iteration 141/1000 | Loss: 0.00001952
Iteration 142/1000 | Loss: 0.00001952
Iteration 143/1000 | Loss: 0.00001952
Iteration 144/1000 | Loss: 0.00001952
Iteration 145/1000 | Loss: 0.00001952
Iteration 146/1000 | Loss: 0.00001952
Iteration 147/1000 | Loss: 0.00001952
Iteration 148/1000 | Loss: 0.00001952
Iteration 149/1000 | Loss: 0.00001952
Iteration 150/1000 | Loss: 0.00001952
Iteration 151/1000 | Loss: 0.00001952
Iteration 152/1000 | Loss: 0.00001952
Iteration 153/1000 | Loss: 0.00001952
Iteration 154/1000 | Loss: 0.00001952
Iteration 155/1000 | Loss: 0.00001952
Iteration 156/1000 | Loss: 0.00001952
Iteration 157/1000 | Loss: 0.00001952
Iteration 158/1000 | Loss: 0.00001952
Iteration 159/1000 | Loss: 0.00001952
Iteration 160/1000 | Loss: 0.00001952
Iteration 161/1000 | Loss: 0.00001952
Iteration 162/1000 | Loss: 0.00001952
Iteration 163/1000 | Loss: 0.00001952
Iteration 164/1000 | Loss: 0.00001952
Iteration 165/1000 | Loss: 0.00001952
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.95185566553846e-05, 1.95185566553846e-05, 1.95185566553846e-05, 1.95185566553846e-05, 1.95185566553846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.95185566553846e-05

Optimization complete. Final v2v error: 3.759958028793335 mm

Highest mean error: 4.243006706237793 mm for frame 59

Lowest mean error: 3.3347976207733154 mm for frame 4

Saving results

Total time: 39.08249831199646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019923
Iteration 2/25 | Loss: 0.00208756
Iteration 3/25 | Loss: 0.00156860
Iteration 4/25 | Loss: 0.00159377
Iteration 5/25 | Loss: 0.00148234
Iteration 6/25 | Loss: 0.00153305
Iteration 7/25 | Loss: 0.00145046
Iteration 8/25 | Loss: 0.00144858
Iteration 9/25 | Loss: 0.00135652
Iteration 10/25 | Loss: 0.00132234
Iteration 11/25 | Loss: 0.00129796
Iteration 12/25 | Loss: 0.00128485
Iteration 13/25 | Loss: 0.00127759
Iteration 14/25 | Loss: 0.00128478
Iteration 15/25 | Loss: 0.00128531
Iteration 16/25 | Loss: 0.00126896
Iteration 17/25 | Loss: 0.00128969
Iteration 18/25 | Loss: 0.00125184
Iteration 19/25 | Loss: 0.00124646
Iteration 20/25 | Loss: 0.00124372
Iteration 21/25 | Loss: 0.00124297
Iteration 22/25 | Loss: 0.00124240
Iteration 23/25 | Loss: 0.00124245
Iteration 24/25 | Loss: 0.00124209
Iteration 25/25 | Loss: 0.00124152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57483137
Iteration 2/25 | Loss: 0.00168057
Iteration 3/25 | Loss: 0.00168057
Iteration 4/25 | Loss: 0.00168057
Iteration 5/25 | Loss: 0.00168057
Iteration 6/25 | Loss: 0.00168057
Iteration 7/25 | Loss: 0.00168057
Iteration 8/25 | Loss: 0.00168057
Iteration 9/25 | Loss: 0.00168057
Iteration 10/25 | Loss: 0.00168057
Iteration 11/25 | Loss: 0.00168057
Iteration 12/25 | Loss: 0.00168057
Iteration 13/25 | Loss: 0.00168057
Iteration 14/25 | Loss: 0.00168057
Iteration 15/25 | Loss: 0.00168057
Iteration 16/25 | Loss: 0.00168057
Iteration 17/25 | Loss: 0.00168057
Iteration 18/25 | Loss: 0.00168057
Iteration 19/25 | Loss: 0.00168057
Iteration 20/25 | Loss: 0.00168057
Iteration 21/25 | Loss: 0.00168057
Iteration 22/25 | Loss: 0.00168057
Iteration 23/25 | Loss: 0.00168057
Iteration 24/25 | Loss: 0.00168057
Iteration 25/25 | Loss: 0.00168057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168057
Iteration 2/1000 | Loss: 0.00008007
Iteration 3/1000 | Loss: 0.00005502
Iteration 4/1000 | Loss: 0.00004594
Iteration 5/1000 | Loss: 0.00003717
Iteration 6/1000 | Loss: 0.00004629
Iteration 7/1000 | Loss: 0.00004598
Iteration 8/1000 | Loss: 0.00004420
Iteration 9/1000 | Loss: 0.00004443
Iteration 10/1000 | Loss: 0.00004304
Iteration 11/1000 | Loss: 0.00004711
Iteration 12/1000 | Loss: 0.00004110
Iteration 13/1000 | Loss: 0.00004768
Iteration 14/1000 | Loss: 0.00004291
Iteration 15/1000 | Loss: 0.00004490
Iteration 16/1000 | Loss: 0.00004431
Iteration 17/1000 | Loss: 0.00004568
Iteration 18/1000 | Loss: 0.00004057
Iteration 19/1000 | Loss: 0.00004525
Iteration 20/1000 | Loss: 0.00004217
Iteration 21/1000 | Loss: 0.00004844
Iteration 22/1000 | Loss: 0.00004209
Iteration 23/1000 | Loss: 0.00004791
Iteration 24/1000 | Loss: 0.00004257
Iteration 25/1000 | Loss: 0.00004518
Iteration 26/1000 | Loss: 0.00004553
Iteration 27/1000 | Loss: 0.00005020
Iteration 28/1000 | Loss: 0.00004557
Iteration 29/1000 | Loss: 0.00004278
Iteration 30/1000 | Loss: 0.00004942
Iteration 31/1000 | Loss: 0.00004173
Iteration 32/1000 | Loss: 0.00004913
Iteration 33/1000 | Loss: 0.00004378
Iteration 34/1000 | Loss: 0.00005772
Iteration 35/1000 | Loss: 0.00003203
Iteration 36/1000 | Loss: 0.00002975
Iteration 37/1000 | Loss: 0.00002829
Iteration 38/1000 | Loss: 0.00002739
Iteration 39/1000 | Loss: 0.00002666
Iteration 40/1000 | Loss: 0.00002628
Iteration 41/1000 | Loss: 0.00032503
Iteration 42/1000 | Loss: 0.00059812
Iteration 43/1000 | Loss: 0.00040987
Iteration 44/1000 | Loss: 0.00002528
Iteration 45/1000 | Loss: 0.00002171
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001673
Iteration 49/1000 | Loss: 0.00028255
Iteration 50/1000 | Loss: 0.00001680
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001440
Iteration 53/1000 | Loss: 0.00001357
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001277
Iteration 57/1000 | Loss: 0.00001270
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001260
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001256
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001255
Iteration 69/1000 | Loss: 0.00001255
Iteration 70/1000 | Loss: 0.00001255
Iteration 71/1000 | Loss: 0.00001255
Iteration 72/1000 | Loss: 0.00001254
Iteration 73/1000 | Loss: 0.00001254
Iteration 74/1000 | Loss: 0.00001253
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001247
Iteration 79/1000 | Loss: 0.00001247
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001244
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001244
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001242
Iteration 88/1000 | Loss: 0.00001242
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001237
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001235
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001234
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001232
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001230
Iteration 110/1000 | Loss: 0.00001230
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001226
Iteration 138/1000 | Loss: 0.00001226
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001224
Iteration 146/1000 | Loss: 0.00001224
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001223
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001221
Iteration 160/1000 | Loss: 0.00001221
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001220
Iteration 166/1000 | Loss: 0.00001220
Iteration 167/1000 | Loss: 0.00001220
Iteration 168/1000 | Loss: 0.00001220
Iteration 169/1000 | Loss: 0.00001220
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001220
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001219
Iteration 175/1000 | Loss: 0.00001219
Iteration 176/1000 | Loss: 0.00001219
Iteration 177/1000 | Loss: 0.00001219
Iteration 178/1000 | Loss: 0.00001219
Iteration 179/1000 | Loss: 0.00001219
Iteration 180/1000 | Loss: 0.00001219
Iteration 181/1000 | Loss: 0.00001218
Iteration 182/1000 | Loss: 0.00001218
Iteration 183/1000 | Loss: 0.00001218
Iteration 184/1000 | Loss: 0.00001218
Iteration 185/1000 | Loss: 0.00001218
Iteration 186/1000 | Loss: 0.00001218
Iteration 187/1000 | Loss: 0.00001217
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001217
Iteration 191/1000 | Loss: 0.00001217
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001216
Iteration 194/1000 | Loss: 0.00001216
Iteration 195/1000 | Loss: 0.00001216
Iteration 196/1000 | Loss: 0.00001216
Iteration 197/1000 | Loss: 0.00001216
Iteration 198/1000 | Loss: 0.00001215
Iteration 199/1000 | Loss: 0.00001215
Iteration 200/1000 | Loss: 0.00001215
Iteration 201/1000 | Loss: 0.00001215
Iteration 202/1000 | Loss: 0.00001215
Iteration 203/1000 | Loss: 0.00001215
Iteration 204/1000 | Loss: 0.00001215
Iteration 205/1000 | Loss: 0.00001215
Iteration 206/1000 | Loss: 0.00001215
Iteration 207/1000 | Loss: 0.00001215
Iteration 208/1000 | Loss: 0.00001215
Iteration 209/1000 | Loss: 0.00001215
Iteration 210/1000 | Loss: 0.00001215
Iteration 211/1000 | Loss: 0.00001215
Iteration 212/1000 | Loss: 0.00001215
Iteration 213/1000 | Loss: 0.00001215
Iteration 214/1000 | Loss: 0.00001214
Iteration 215/1000 | Loss: 0.00001214
Iteration 216/1000 | Loss: 0.00001214
Iteration 217/1000 | Loss: 0.00001214
Iteration 218/1000 | Loss: 0.00001214
Iteration 219/1000 | Loss: 0.00001214
Iteration 220/1000 | Loss: 0.00001214
Iteration 221/1000 | Loss: 0.00001214
Iteration 222/1000 | Loss: 0.00001214
Iteration 223/1000 | Loss: 0.00001214
Iteration 224/1000 | Loss: 0.00001214
Iteration 225/1000 | Loss: 0.00001214
Iteration 226/1000 | Loss: 0.00001214
Iteration 227/1000 | Loss: 0.00001214
Iteration 228/1000 | Loss: 0.00001213
Iteration 229/1000 | Loss: 0.00001213
Iteration 230/1000 | Loss: 0.00001213
Iteration 231/1000 | Loss: 0.00001213
Iteration 232/1000 | Loss: 0.00001213
Iteration 233/1000 | Loss: 0.00001213
Iteration 234/1000 | Loss: 0.00001213
Iteration 235/1000 | Loss: 0.00001213
Iteration 236/1000 | Loss: 0.00001213
Iteration 237/1000 | Loss: 0.00001213
Iteration 238/1000 | Loss: 0.00001213
Iteration 239/1000 | Loss: 0.00001213
Iteration 240/1000 | Loss: 0.00001213
Iteration 241/1000 | Loss: 0.00001213
Iteration 242/1000 | Loss: 0.00001213
Iteration 243/1000 | Loss: 0.00001213
Iteration 244/1000 | Loss: 0.00001213
Iteration 245/1000 | Loss: 0.00001213
Iteration 246/1000 | Loss: 0.00001213
Iteration 247/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.213089853990823e-05, 1.213089853990823e-05, 1.213089853990823e-05, 1.213089853990823e-05, 1.213089853990823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.213089853990823e-05

Optimization complete. Final v2v error: 2.956507444381714 mm

Highest mean error: 5.34320592880249 mm for frame 93

Lowest mean error: 2.599914789199829 mm for frame 19

Saving results

Total time: 142.69434189796448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541724
Iteration 2/25 | Loss: 0.00151651
Iteration 3/25 | Loss: 0.00129707
Iteration 4/25 | Loss: 0.00123410
Iteration 5/25 | Loss: 0.00123449
Iteration 6/25 | Loss: 0.00122879
Iteration 7/25 | Loss: 0.00122678
Iteration 8/25 | Loss: 0.00122529
Iteration 9/25 | Loss: 0.00122520
Iteration 10/25 | Loss: 0.00122516
Iteration 11/25 | Loss: 0.00122516
Iteration 12/25 | Loss: 0.00122515
Iteration 13/25 | Loss: 0.00122515
Iteration 14/25 | Loss: 0.00122515
Iteration 15/25 | Loss: 0.00122515
Iteration 16/25 | Loss: 0.00122515
Iteration 17/25 | Loss: 0.00122515
Iteration 18/25 | Loss: 0.00122515
Iteration 19/25 | Loss: 0.00122515
Iteration 20/25 | Loss: 0.00122515
Iteration 21/25 | Loss: 0.00122514
Iteration 22/25 | Loss: 0.00122514
Iteration 23/25 | Loss: 0.00122514
Iteration 24/25 | Loss: 0.00122514
Iteration 25/25 | Loss: 0.00122514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.15984058
Iteration 2/25 | Loss: 0.00151217
Iteration 3/25 | Loss: 0.00151215
Iteration 4/25 | Loss: 0.00151215
Iteration 5/25 | Loss: 0.00151215
Iteration 6/25 | Loss: 0.00151215
Iteration 7/25 | Loss: 0.00151215
Iteration 8/25 | Loss: 0.00151215
Iteration 9/25 | Loss: 0.00151215
Iteration 10/25 | Loss: 0.00151215
Iteration 11/25 | Loss: 0.00151215
Iteration 12/25 | Loss: 0.00151215
Iteration 13/25 | Loss: 0.00151215
Iteration 14/25 | Loss: 0.00151215
Iteration 15/25 | Loss: 0.00151215
Iteration 16/25 | Loss: 0.00151215
Iteration 17/25 | Loss: 0.00151215
Iteration 18/25 | Loss: 0.00151215
Iteration 19/25 | Loss: 0.00151215
Iteration 20/25 | Loss: 0.00151215
Iteration 21/25 | Loss: 0.00151215
Iteration 22/25 | Loss: 0.00151215
Iteration 23/25 | Loss: 0.00151215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001512147719040513, 0.001512147719040513, 0.001512147719040513, 0.001512147719040513, 0.001512147719040513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001512147719040513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151215
Iteration 2/1000 | Loss: 0.00002060
Iteration 3/1000 | Loss: 0.00001496
Iteration 4/1000 | Loss: 0.00001384
Iteration 5/1000 | Loss: 0.00001892
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001248
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001169
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001152
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001140
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001134
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001128
Iteration 23/1000 | Loss: 0.00001128
Iteration 24/1000 | Loss: 0.00001127
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001124
Iteration 31/1000 | Loss: 0.00001124
Iteration 32/1000 | Loss: 0.00001123
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001123
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001121
Iteration 40/1000 | Loss: 0.00001120
Iteration 41/1000 | Loss: 0.00001119
Iteration 42/1000 | Loss: 0.00001118
Iteration 43/1000 | Loss: 0.00001118
Iteration 44/1000 | Loss: 0.00001117
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001113
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001113
Iteration 52/1000 | Loss: 0.00001113
Iteration 53/1000 | Loss: 0.00001113
Iteration 54/1000 | Loss: 0.00001113
Iteration 55/1000 | Loss: 0.00001113
Iteration 56/1000 | Loss: 0.00001113
Iteration 57/1000 | Loss: 0.00001113
Iteration 58/1000 | Loss: 0.00001113
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001112
Iteration 61/1000 | Loss: 0.00001112
Iteration 62/1000 | Loss: 0.00001112
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001109
Iteration 70/1000 | Loss: 0.00001109
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001109
Iteration 73/1000 | Loss: 0.00001108
Iteration 74/1000 | Loss: 0.00001108
Iteration 75/1000 | Loss: 0.00001108
Iteration 76/1000 | Loss: 0.00001107
Iteration 77/1000 | Loss: 0.00001107
Iteration 78/1000 | Loss: 0.00001107
Iteration 79/1000 | Loss: 0.00001106
Iteration 80/1000 | Loss: 0.00001106
Iteration 81/1000 | Loss: 0.00001106
Iteration 82/1000 | Loss: 0.00001106
Iteration 83/1000 | Loss: 0.00001106
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001105
Iteration 86/1000 | Loss: 0.00001105
Iteration 87/1000 | Loss: 0.00001105
Iteration 88/1000 | Loss: 0.00001104
Iteration 89/1000 | Loss: 0.00001104
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001103
Iteration 93/1000 | Loss: 0.00001103
Iteration 94/1000 | Loss: 0.00001103
Iteration 95/1000 | Loss: 0.00001103
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001102
Iteration 101/1000 | Loss: 0.00001102
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.1024056220776401e-05, 1.1024056220776401e-05, 1.1024056220776401e-05, 1.1024056220776401e-05, 1.1024056220776401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1024056220776401e-05

Optimization complete. Final v2v error: 2.811692953109741 mm

Highest mean error: 3.484234571456909 mm for frame 166

Lowest mean error: 2.5015645027160645 mm for frame 90

Saving results

Total time: 46.852298974990845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978248
Iteration 2/25 | Loss: 0.00206501
Iteration 3/25 | Loss: 0.00147976
Iteration 4/25 | Loss: 0.00137034
Iteration 5/25 | Loss: 0.00134377
Iteration 6/25 | Loss: 0.00132253
Iteration 7/25 | Loss: 0.00132466
Iteration 8/25 | Loss: 0.00133255
Iteration 9/25 | Loss: 0.00133903
Iteration 10/25 | Loss: 0.00134580
Iteration 11/25 | Loss: 0.00135277
Iteration 12/25 | Loss: 0.00134672
Iteration 13/25 | Loss: 0.00131589
Iteration 14/25 | Loss: 0.00128986
Iteration 15/25 | Loss: 0.00127731
Iteration 16/25 | Loss: 0.00127466
Iteration 17/25 | Loss: 0.00127358
Iteration 18/25 | Loss: 0.00127305
Iteration 19/25 | Loss: 0.00127295
Iteration 20/25 | Loss: 0.00127295
Iteration 21/25 | Loss: 0.00127295
Iteration 22/25 | Loss: 0.00127295
Iteration 23/25 | Loss: 0.00127295
Iteration 24/25 | Loss: 0.00127295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012729461304843426, 0.0012729461304843426, 0.0012729461304843426, 0.0012729461304843426, 0.0012729461304843426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012729461304843426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58085561
Iteration 2/25 | Loss: 0.00110439
Iteration 3/25 | Loss: 0.00110439
Iteration 4/25 | Loss: 0.00108392
Iteration 5/25 | Loss: 0.00108392
Iteration 6/25 | Loss: 0.00108392
Iteration 7/25 | Loss: 0.00108392
Iteration 8/25 | Loss: 0.00108392
Iteration 9/25 | Loss: 0.00108392
Iteration 10/25 | Loss: 0.00108392
Iteration 11/25 | Loss: 0.00108392
Iteration 12/25 | Loss: 0.00108392
Iteration 13/25 | Loss: 0.00108392
Iteration 14/25 | Loss: 0.00108392
Iteration 15/25 | Loss: 0.00108392
Iteration 16/25 | Loss: 0.00108392
Iteration 17/25 | Loss: 0.00108392
Iteration 18/25 | Loss: 0.00108392
Iteration 19/25 | Loss: 0.00108392
Iteration 20/25 | Loss: 0.00108392
Iteration 21/25 | Loss: 0.00108392
Iteration 22/25 | Loss: 0.00108392
Iteration 23/25 | Loss: 0.00108392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001083920942619443, 0.001083920942619443, 0.001083920942619443, 0.001083920942619443, 0.001083920942619443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001083920942619443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108392
Iteration 2/1000 | Loss: 0.00034137
Iteration 3/1000 | Loss: 0.00057436
Iteration 4/1000 | Loss: 0.00017169
Iteration 5/1000 | Loss: 0.00018522
Iteration 6/1000 | Loss: 0.00136330
Iteration 7/1000 | Loss: 0.00012376
Iteration 8/1000 | Loss: 0.00007754
Iteration 9/1000 | Loss: 0.00019594
Iteration 10/1000 | Loss: 0.00003351
Iteration 11/1000 | Loss: 0.00003105
Iteration 12/1000 | Loss: 0.00002940
Iteration 13/1000 | Loss: 0.00004944
Iteration 14/1000 | Loss: 0.00003898
Iteration 15/1000 | Loss: 0.00008516
Iteration 16/1000 | Loss: 0.00010558
Iteration 17/1000 | Loss: 0.00014356
Iteration 18/1000 | Loss: 0.00031668
Iteration 19/1000 | Loss: 0.00016478
Iteration 20/1000 | Loss: 0.00003847
Iteration 21/1000 | Loss: 0.00003167
Iteration 22/1000 | Loss: 0.00003692
Iteration 23/1000 | Loss: 0.00003557
Iteration 24/1000 | Loss: 0.00002744
Iteration 25/1000 | Loss: 0.00002683
Iteration 26/1000 | Loss: 0.00004798
Iteration 27/1000 | Loss: 0.00002991
Iteration 28/1000 | Loss: 0.00005026
Iteration 29/1000 | Loss: 0.00002992
Iteration 30/1000 | Loss: 0.00005587
Iteration 31/1000 | Loss: 0.00003006
Iteration 32/1000 | Loss: 0.00030450
Iteration 33/1000 | Loss: 0.00009876
Iteration 34/1000 | Loss: 0.00023602
Iteration 35/1000 | Loss: 0.00005055
Iteration 36/1000 | Loss: 0.00004523
Iteration 37/1000 | Loss: 0.00004895
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00005948
Iteration 40/1000 | Loss: 0.00002335
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00002070
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00002007
Iteration 46/1000 | Loss: 0.00005994
Iteration 47/1000 | Loss: 0.00007370
Iteration 48/1000 | Loss: 0.00002267
Iteration 49/1000 | Loss: 0.00001975
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001931
Iteration 53/1000 | Loss: 0.00006163
Iteration 54/1000 | Loss: 0.00002976
Iteration 55/1000 | Loss: 0.00002220
Iteration 56/1000 | Loss: 0.00001919
Iteration 57/1000 | Loss: 0.00002676
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00002557
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00009726
Iteration 62/1000 | Loss: 0.00001942
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00006832
Iteration 69/1000 | Loss: 0.00003489
Iteration 70/1000 | Loss: 0.00001924
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00027550
Iteration 74/1000 | Loss: 0.00041782
Iteration 75/1000 | Loss: 0.00004566
Iteration 76/1000 | Loss: 0.00003025
Iteration 77/1000 | Loss: 0.00002438
Iteration 78/1000 | Loss: 0.00003688
Iteration 79/1000 | Loss: 0.00009539
Iteration 80/1000 | Loss: 0.00001981
Iteration 81/1000 | Loss: 0.00005152
Iteration 82/1000 | Loss: 0.00002745
Iteration 83/1000 | Loss: 0.00002561
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001659
Iteration 88/1000 | Loss: 0.00001925
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001664
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001618
Iteration 95/1000 | Loss: 0.00001617
Iteration 96/1000 | Loss: 0.00001671
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001603
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001601
Iteration 111/1000 | Loss: 0.00005353
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001591
Iteration 114/1000 | Loss: 0.00001591
Iteration 115/1000 | Loss: 0.00001588
Iteration 116/1000 | Loss: 0.00001588
Iteration 117/1000 | Loss: 0.00001588
Iteration 118/1000 | Loss: 0.00001588
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001588
Iteration 121/1000 | Loss: 0.00001587
Iteration 122/1000 | Loss: 0.00001587
Iteration 123/1000 | Loss: 0.00001587
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001587
Iteration 127/1000 | Loss: 0.00001587
Iteration 128/1000 | Loss: 0.00001587
Iteration 129/1000 | Loss: 0.00001587
Iteration 130/1000 | Loss: 0.00001587
Iteration 131/1000 | Loss: 0.00001587
Iteration 132/1000 | Loss: 0.00001586
Iteration 133/1000 | Loss: 0.00001586
Iteration 134/1000 | Loss: 0.00001586
Iteration 135/1000 | Loss: 0.00001586
Iteration 136/1000 | Loss: 0.00001586
Iteration 137/1000 | Loss: 0.00001586
Iteration 138/1000 | Loss: 0.00001586
Iteration 139/1000 | Loss: 0.00001585
Iteration 140/1000 | Loss: 0.00001585
Iteration 141/1000 | Loss: 0.00001585
Iteration 142/1000 | Loss: 0.00001585
Iteration 143/1000 | Loss: 0.00001585
Iteration 144/1000 | Loss: 0.00001584
Iteration 145/1000 | Loss: 0.00001584
Iteration 146/1000 | Loss: 0.00001584
Iteration 147/1000 | Loss: 0.00001584
Iteration 148/1000 | Loss: 0.00001584
Iteration 149/1000 | Loss: 0.00001584
Iteration 150/1000 | Loss: 0.00001584
Iteration 151/1000 | Loss: 0.00001584
Iteration 152/1000 | Loss: 0.00001584
Iteration 153/1000 | Loss: 0.00001584
Iteration 154/1000 | Loss: 0.00001584
Iteration 155/1000 | Loss: 0.00001584
Iteration 156/1000 | Loss: 0.00001584
Iteration 157/1000 | Loss: 0.00001584
Iteration 158/1000 | Loss: 0.00001584
Iteration 159/1000 | Loss: 0.00001583
Iteration 160/1000 | Loss: 0.00001583
Iteration 161/1000 | Loss: 0.00001583
Iteration 162/1000 | Loss: 0.00001583
Iteration 163/1000 | Loss: 0.00001583
Iteration 164/1000 | Loss: 0.00001583
Iteration 165/1000 | Loss: 0.00001583
Iteration 166/1000 | Loss: 0.00001583
Iteration 167/1000 | Loss: 0.00001583
Iteration 168/1000 | Loss: 0.00001583
Iteration 169/1000 | Loss: 0.00001583
Iteration 170/1000 | Loss: 0.00001583
Iteration 171/1000 | Loss: 0.00001583
Iteration 172/1000 | Loss: 0.00001583
Iteration 173/1000 | Loss: 0.00001583
Iteration 174/1000 | Loss: 0.00001583
Iteration 175/1000 | Loss: 0.00001582
Iteration 176/1000 | Loss: 0.00001582
Iteration 177/1000 | Loss: 0.00001582
Iteration 178/1000 | Loss: 0.00001582
Iteration 179/1000 | Loss: 0.00001582
Iteration 180/1000 | Loss: 0.00001582
Iteration 181/1000 | Loss: 0.00001582
Iteration 182/1000 | Loss: 0.00001582
Iteration 183/1000 | Loss: 0.00001582
Iteration 184/1000 | Loss: 0.00001581
Iteration 185/1000 | Loss: 0.00001581
Iteration 186/1000 | Loss: 0.00001581
Iteration 187/1000 | Loss: 0.00001581
Iteration 188/1000 | Loss: 0.00001581
Iteration 189/1000 | Loss: 0.00001581
Iteration 190/1000 | Loss: 0.00001581
Iteration 191/1000 | Loss: 0.00004966
Iteration 192/1000 | Loss: 0.00002144
Iteration 193/1000 | Loss: 0.00002897
Iteration 194/1000 | Loss: 0.00001583
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001581
Iteration 198/1000 | Loss: 0.00001581
Iteration 199/1000 | Loss: 0.00001581
Iteration 200/1000 | Loss: 0.00001581
Iteration 201/1000 | Loss: 0.00001581
Iteration 202/1000 | Loss: 0.00001581
Iteration 203/1000 | Loss: 0.00001581
Iteration 204/1000 | Loss: 0.00001581
Iteration 205/1000 | Loss: 0.00001580
Iteration 206/1000 | Loss: 0.00001580
Iteration 207/1000 | Loss: 0.00001580
Iteration 208/1000 | Loss: 0.00001580
Iteration 209/1000 | Loss: 0.00001580
Iteration 210/1000 | Loss: 0.00001580
Iteration 211/1000 | Loss: 0.00001580
Iteration 212/1000 | Loss: 0.00001580
Iteration 213/1000 | Loss: 0.00001580
Iteration 214/1000 | Loss: 0.00001580
Iteration 215/1000 | Loss: 0.00002153
Iteration 216/1000 | Loss: 0.00001580
Iteration 217/1000 | Loss: 0.00001580
Iteration 218/1000 | Loss: 0.00001579
Iteration 219/1000 | Loss: 0.00001579
Iteration 220/1000 | Loss: 0.00001579
Iteration 221/1000 | Loss: 0.00001579
Iteration 222/1000 | Loss: 0.00001579
Iteration 223/1000 | Loss: 0.00001579
Iteration 224/1000 | Loss: 0.00001579
Iteration 225/1000 | Loss: 0.00001579
Iteration 226/1000 | Loss: 0.00001579
Iteration 227/1000 | Loss: 0.00001579
Iteration 228/1000 | Loss: 0.00001579
Iteration 229/1000 | Loss: 0.00001578
Iteration 230/1000 | Loss: 0.00001578
Iteration 231/1000 | Loss: 0.00001578
Iteration 232/1000 | Loss: 0.00001578
Iteration 233/1000 | Loss: 0.00001578
Iteration 234/1000 | Loss: 0.00001578
Iteration 235/1000 | Loss: 0.00001578
Iteration 236/1000 | Loss: 0.00001578
Iteration 237/1000 | Loss: 0.00001833
Iteration 238/1000 | Loss: 0.00001659
Iteration 239/1000 | Loss: 0.00001578
Iteration 240/1000 | Loss: 0.00001577
Iteration 241/1000 | Loss: 0.00001577
Iteration 242/1000 | Loss: 0.00001577
Iteration 243/1000 | Loss: 0.00001577
Iteration 244/1000 | Loss: 0.00001577
Iteration 245/1000 | Loss: 0.00001577
Iteration 246/1000 | Loss: 0.00001577
Iteration 247/1000 | Loss: 0.00001576
Iteration 248/1000 | Loss: 0.00001576
Iteration 249/1000 | Loss: 0.00001576
Iteration 250/1000 | Loss: 0.00001576
Iteration 251/1000 | Loss: 0.00001576
Iteration 252/1000 | Loss: 0.00001576
Iteration 253/1000 | Loss: 0.00001576
Iteration 254/1000 | Loss: 0.00001576
Iteration 255/1000 | Loss: 0.00001576
Iteration 256/1000 | Loss: 0.00001576
Iteration 257/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.5763409464852884e-05, 1.5763409464852884e-05, 1.5763409464852884e-05, 1.5763409464852884e-05, 1.5763409464852884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5763409464852884e-05

Optimization complete. Final v2v error: 3.231827974319458 mm

Highest mean error: 4.868931293487549 mm for frame 78

Lowest mean error: 2.6893575191497803 mm for frame 129

Saving results

Total time: 172.694518327713
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850200
Iteration 2/25 | Loss: 0.00149933
Iteration 3/25 | Loss: 0.00132289
Iteration 4/25 | Loss: 0.00127885
Iteration 5/25 | Loss: 0.00127288
Iteration 6/25 | Loss: 0.00127126
Iteration 7/25 | Loss: 0.00127100
Iteration 8/25 | Loss: 0.00127100
Iteration 9/25 | Loss: 0.00127100
Iteration 10/25 | Loss: 0.00127100
Iteration 11/25 | Loss: 0.00127100
Iteration 12/25 | Loss: 0.00127100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012709987349808216, 0.0012709987349808216, 0.0012709987349808216, 0.0012709987349808216, 0.0012709987349808216]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012709987349808216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22014046
Iteration 2/25 | Loss: 0.00160295
Iteration 3/25 | Loss: 0.00160295
Iteration 4/25 | Loss: 0.00160295
Iteration 5/25 | Loss: 0.00160295
Iteration 6/25 | Loss: 0.00160295
Iteration 7/25 | Loss: 0.00160295
Iteration 8/25 | Loss: 0.00160295
Iteration 9/25 | Loss: 0.00160295
Iteration 10/25 | Loss: 0.00160295
Iteration 11/25 | Loss: 0.00160295
Iteration 12/25 | Loss: 0.00160295
Iteration 13/25 | Loss: 0.00160295
Iteration 14/25 | Loss: 0.00160295
Iteration 15/25 | Loss: 0.00160295
Iteration 16/25 | Loss: 0.00160295
Iteration 17/25 | Loss: 0.00160295
Iteration 18/25 | Loss: 0.00160295
Iteration 19/25 | Loss: 0.00160295
Iteration 20/25 | Loss: 0.00160295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016029461985453963, 0.0016029461985453963, 0.0016029461985453963, 0.0016029461985453963, 0.0016029461985453963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016029461985453963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160295
Iteration 2/1000 | Loss: 0.00010093
Iteration 3/1000 | Loss: 0.00007026
Iteration 4/1000 | Loss: 0.00003950
Iteration 5/1000 | Loss: 0.00003297
Iteration 6/1000 | Loss: 0.00003092
Iteration 7/1000 | Loss: 0.00002926
Iteration 8/1000 | Loss: 0.00002852
Iteration 9/1000 | Loss: 0.00002774
Iteration 10/1000 | Loss: 0.00002749
Iteration 11/1000 | Loss: 0.00002729
Iteration 12/1000 | Loss: 0.00002705
Iteration 13/1000 | Loss: 0.00002684
Iteration 14/1000 | Loss: 0.00002663
Iteration 15/1000 | Loss: 0.00002660
Iteration 16/1000 | Loss: 0.00002643
Iteration 17/1000 | Loss: 0.00002627
Iteration 18/1000 | Loss: 0.00002627
Iteration 19/1000 | Loss: 0.00002625
Iteration 20/1000 | Loss: 0.00002623
Iteration 21/1000 | Loss: 0.00002620
Iteration 22/1000 | Loss: 0.00002620
Iteration 23/1000 | Loss: 0.00002617
Iteration 24/1000 | Loss: 0.00002617
Iteration 25/1000 | Loss: 0.00002616
Iteration 26/1000 | Loss: 0.00002613
Iteration 27/1000 | Loss: 0.00002613
Iteration 28/1000 | Loss: 0.00002612
Iteration 29/1000 | Loss: 0.00002611
Iteration 30/1000 | Loss: 0.00002611
Iteration 31/1000 | Loss: 0.00002611
Iteration 32/1000 | Loss: 0.00002611
Iteration 33/1000 | Loss: 0.00002610
Iteration 34/1000 | Loss: 0.00002607
Iteration 35/1000 | Loss: 0.00002607
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00002607
Iteration 38/1000 | Loss: 0.00002605
Iteration 39/1000 | Loss: 0.00002604
Iteration 40/1000 | Loss: 0.00002604
Iteration 41/1000 | Loss: 0.00002603
Iteration 42/1000 | Loss: 0.00002603
Iteration 43/1000 | Loss: 0.00002602
Iteration 44/1000 | Loss: 0.00002602
Iteration 45/1000 | Loss: 0.00002602
Iteration 46/1000 | Loss: 0.00002602
Iteration 47/1000 | Loss: 0.00002601
Iteration 48/1000 | Loss: 0.00002601
Iteration 49/1000 | Loss: 0.00002601
Iteration 50/1000 | Loss: 0.00002600
Iteration 51/1000 | Loss: 0.00002600
Iteration 52/1000 | Loss: 0.00002600
Iteration 53/1000 | Loss: 0.00002600
Iteration 54/1000 | Loss: 0.00002599
Iteration 55/1000 | Loss: 0.00002599
Iteration 56/1000 | Loss: 0.00002598
Iteration 57/1000 | Loss: 0.00002598
Iteration 58/1000 | Loss: 0.00002598
Iteration 59/1000 | Loss: 0.00002597
Iteration 60/1000 | Loss: 0.00002597
Iteration 61/1000 | Loss: 0.00002597
Iteration 62/1000 | Loss: 0.00002596
Iteration 63/1000 | Loss: 0.00002596
Iteration 64/1000 | Loss: 0.00002596
Iteration 65/1000 | Loss: 0.00002595
Iteration 66/1000 | Loss: 0.00002595
Iteration 67/1000 | Loss: 0.00002595
Iteration 68/1000 | Loss: 0.00002594
Iteration 69/1000 | Loss: 0.00002594
Iteration 70/1000 | Loss: 0.00002594
Iteration 71/1000 | Loss: 0.00002594
Iteration 72/1000 | Loss: 0.00002593
Iteration 73/1000 | Loss: 0.00002593
Iteration 74/1000 | Loss: 0.00002593
Iteration 75/1000 | Loss: 0.00002592
Iteration 76/1000 | Loss: 0.00002592
Iteration 77/1000 | Loss: 0.00002592
Iteration 78/1000 | Loss: 0.00002592
Iteration 79/1000 | Loss: 0.00002591
Iteration 80/1000 | Loss: 0.00002591
Iteration 81/1000 | Loss: 0.00002591
Iteration 82/1000 | Loss: 0.00002591
Iteration 83/1000 | Loss: 0.00002591
Iteration 84/1000 | Loss: 0.00002591
Iteration 85/1000 | Loss: 0.00002591
Iteration 86/1000 | Loss: 0.00002591
Iteration 87/1000 | Loss: 0.00002591
Iteration 88/1000 | Loss: 0.00002591
Iteration 89/1000 | Loss: 0.00002590
Iteration 90/1000 | Loss: 0.00002590
Iteration 91/1000 | Loss: 0.00002590
Iteration 92/1000 | Loss: 0.00002589
Iteration 93/1000 | Loss: 0.00002589
Iteration 94/1000 | Loss: 0.00002589
Iteration 95/1000 | Loss: 0.00002589
Iteration 96/1000 | Loss: 0.00002589
Iteration 97/1000 | Loss: 0.00002589
Iteration 98/1000 | Loss: 0.00002588
Iteration 99/1000 | Loss: 0.00002588
Iteration 100/1000 | Loss: 0.00002588
Iteration 101/1000 | Loss: 0.00002588
Iteration 102/1000 | Loss: 0.00002588
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002588
Iteration 105/1000 | Loss: 0.00002587
Iteration 106/1000 | Loss: 0.00002587
Iteration 107/1000 | Loss: 0.00002587
Iteration 108/1000 | Loss: 0.00002587
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00002587
Iteration 111/1000 | Loss: 0.00002587
Iteration 112/1000 | Loss: 0.00002587
Iteration 113/1000 | Loss: 0.00002586
Iteration 114/1000 | Loss: 0.00002586
Iteration 115/1000 | Loss: 0.00002586
Iteration 116/1000 | Loss: 0.00002586
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002586
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002586
Iteration 123/1000 | Loss: 0.00002586
Iteration 124/1000 | Loss: 0.00002586
Iteration 125/1000 | Loss: 0.00002585
Iteration 126/1000 | Loss: 0.00002585
Iteration 127/1000 | Loss: 0.00002585
Iteration 128/1000 | Loss: 0.00002585
Iteration 129/1000 | Loss: 0.00002585
Iteration 130/1000 | Loss: 0.00002585
Iteration 131/1000 | Loss: 0.00002585
Iteration 132/1000 | Loss: 0.00002585
Iteration 133/1000 | Loss: 0.00002585
Iteration 134/1000 | Loss: 0.00002585
Iteration 135/1000 | Loss: 0.00002585
Iteration 136/1000 | Loss: 0.00002585
Iteration 137/1000 | Loss: 0.00002585
Iteration 138/1000 | Loss: 0.00002584
Iteration 139/1000 | Loss: 0.00002584
Iteration 140/1000 | Loss: 0.00002584
Iteration 141/1000 | Loss: 0.00002584
Iteration 142/1000 | Loss: 0.00002584
Iteration 143/1000 | Loss: 0.00002584
Iteration 144/1000 | Loss: 0.00002584
Iteration 145/1000 | Loss: 0.00002584
Iteration 146/1000 | Loss: 0.00002584
Iteration 147/1000 | Loss: 0.00002584
Iteration 148/1000 | Loss: 0.00002584
Iteration 149/1000 | Loss: 0.00002583
Iteration 150/1000 | Loss: 0.00002583
Iteration 151/1000 | Loss: 0.00002583
Iteration 152/1000 | Loss: 0.00002583
Iteration 153/1000 | Loss: 0.00002583
Iteration 154/1000 | Loss: 0.00002583
Iteration 155/1000 | Loss: 0.00002583
Iteration 156/1000 | Loss: 0.00002583
Iteration 157/1000 | Loss: 0.00002583
Iteration 158/1000 | Loss: 0.00002583
Iteration 159/1000 | Loss: 0.00002583
Iteration 160/1000 | Loss: 0.00002583
Iteration 161/1000 | Loss: 0.00002583
Iteration 162/1000 | Loss: 0.00002583
Iteration 163/1000 | Loss: 0.00002583
Iteration 164/1000 | Loss: 0.00002583
Iteration 165/1000 | Loss: 0.00002583
Iteration 166/1000 | Loss: 0.00002583
Iteration 167/1000 | Loss: 0.00002583
Iteration 168/1000 | Loss: 0.00002583
Iteration 169/1000 | Loss: 0.00002583
Iteration 170/1000 | Loss: 0.00002583
Iteration 171/1000 | Loss: 0.00002583
Iteration 172/1000 | Loss: 0.00002583
Iteration 173/1000 | Loss: 0.00002583
Iteration 174/1000 | Loss: 0.00002583
Iteration 175/1000 | Loss: 0.00002583
Iteration 176/1000 | Loss: 0.00002583
Iteration 177/1000 | Loss: 0.00002583
Iteration 178/1000 | Loss: 0.00002583
Iteration 179/1000 | Loss: 0.00002583
Iteration 180/1000 | Loss: 0.00002583
Iteration 181/1000 | Loss: 0.00002583
Iteration 182/1000 | Loss: 0.00002583
Iteration 183/1000 | Loss: 0.00002583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.5829593141679652e-05, 2.5829593141679652e-05, 2.5829593141679652e-05, 2.5829593141679652e-05, 2.5829593141679652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5829593141679652e-05

Optimization complete. Final v2v error: 4.274852275848389 mm

Highest mean error: 4.746939659118652 mm for frame 75

Lowest mean error: 3.3316447734832764 mm for frame 7

Saving results

Total time: 43.366196155548096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417886
Iteration 2/25 | Loss: 0.00131924
Iteration 3/25 | Loss: 0.00123787
Iteration 4/25 | Loss: 0.00122628
Iteration 5/25 | Loss: 0.00122306
Iteration 6/25 | Loss: 0.00122306
Iteration 7/25 | Loss: 0.00122306
Iteration 8/25 | Loss: 0.00122306
Iteration 9/25 | Loss: 0.00122306
Iteration 10/25 | Loss: 0.00122306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012230646098032594, 0.0012230646098032594, 0.0012230646098032594, 0.0012230646098032594, 0.0012230646098032594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012230646098032594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32682812
Iteration 2/25 | Loss: 0.00163972
Iteration 3/25 | Loss: 0.00163972
Iteration 4/25 | Loss: 0.00163971
Iteration 5/25 | Loss: 0.00163971
Iteration 6/25 | Loss: 0.00163971
Iteration 7/25 | Loss: 0.00163971
Iteration 8/25 | Loss: 0.00163971
Iteration 9/25 | Loss: 0.00163971
Iteration 10/25 | Loss: 0.00163971
Iteration 11/25 | Loss: 0.00163971
Iteration 12/25 | Loss: 0.00163971
Iteration 13/25 | Loss: 0.00163971
Iteration 14/25 | Loss: 0.00163971
Iteration 15/25 | Loss: 0.00163971
Iteration 16/25 | Loss: 0.00163971
Iteration 17/25 | Loss: 0.00163971
Iteration 18/25 | Loss: 0.00163971
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016397112049162388, 0.0016397112049162388, 0.0016397112049162388, 0.0016397112049162388, 0.0016397112049162388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016397112049162388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163971
Iteration 2/1000 | Loss: 0.00002191
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001230
Iteration 12/1000 | Loss: 0.00001227
Iteration 13/1000 | Loss: 0.00001225
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001216
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001208
Iteration 24/1000 | Loss: 0.00001201
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001199
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001188
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001178
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001176
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001174
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001173
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001169
Iteration 130/1000 | Loss: 0.00001169
Iteration 131/1000 | Loss: 0.00001169
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001169
Iteration 135/1000 | Loss: 0.00001169
Iteration 136/1000 | Loss: 0.00001169
Iteration 137/1000 | Loss: 0.00001169
Iteration 138/1000 | Loss: 0.00001169
Iteration 139/1000 | Loss: 0.00001169
Iteration 140/1000 | Loss: 0.00001169
Iteration 141/1000 | Loss: 0.00001169
Iteration 142/1000 | Loss: 0.00001169
Iteration 143/1000 | Loss: 0.00001169
Iteration 144/1000 | Loss: 0.00001169
Iteration 145/1000 | Loss: 0.00001169
Iteration 146/1000 | Loss: 0.00001169
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1694952263496816e-05, 1.1694952263496816e-05, 1.1694952263496816e-05, 1.1694952263496816e-05, 1.1694952263496816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1694952263496816e-05

Optimization complete. Final v2v error: 2.8895978927612305 mm

Highest mean error: 3.0246410369873047 mm for frame 167

Lowest mean error: 2.741729497909546 mm for frame 47

Saving results

Total time: 36.867960691452026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00684302
Iteration 2/25 | Loss: 0.00151964
Iteration 3/25 | Loss: 0.00131976
Iteration 4/25 | Loss: 0.00130189
Iteration 5/25 | Loss: 0.00127879
Iteration 6/25 | Loss: 0.00128887
Iteration 7/25 | Loss: 0.00126765
Iteration 8/25 | Loss: 0.00126925
Iteration 9/25 | Loss: 0.00126956
Iteration 10/25 | Loss: 0.00126588
Iteration 11/25 | Loss: 0.00126473
Iteration 12/25 | Loss: 0.00126429
Iteration 13/25 | Loss: 0.00126409
Iteration 14/25 | Loss: 0.00126407
Iteration 15/25 | Loss: 0.00126407
Iteration 16/25 | Loss: 0.00126407
Iteration 17/25 | Loss: 0.00126406
Iteration 18/25 | Loss: 0.00126406
Iteration 19/25 | Loss: 0.00126406
Iteration 20/25 | Loss: 0.00126406
Iteration 21/25 | Loss: 0.00126406
Iteration 22/25 | Loss: 0.00126406
Iteration 23/25 | Loss: 0.00126406
Iteration 24/25 | Loss: 0.00126406
Iteration 25/25 | Loss: 0.00126405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39415169
Iteration 2/25 | Loss: 0.00186612
Iteration 3/25 | Loss: 0.00186611
Iteration 4/25 | Loss: 0.00186611
Iteration 5/25 | Loss: 0.00186611
Iteration 6/25 | Loss: 0.00186611
Iteration 7/25 | Loss: 0.00186611
Iteration 8/25 | Loss: 0.00186611
Iteration 9/25 | Loss: 0.00186611
Iteration 10/25 | Loss: 0.00186611
Iteration 11/25 | Loss: 0.00186611
Iteration 12/25 | Loss: 0.00186611
Iteration 13/25 | Loss: 0.00186611
Iteration 14/25 | Loss: 0.00186611
Iteration 15/25 | Loss: 0.00186611
Iteration 16/25 | Loss: 0.00186611
Iteration 17/25 | Loss: 0.00186611
Iteration 18/25 | Loss: 0.00186611
Iteration 19/25 | Loss: 0.00186611
Iteration 20/25 | Loss: 0.00186611
Iteration 21/25 | Loss: 0.00186611
Iteration 22/25 | Loss: 0.00186611
Iteration 23/25 | Loss: 0.00186611
Iteration 24/25 | Loss: 0.00186611
Iteration 25/25 | Loss: 0.00186611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186611
Iteration 2/1000 | Loss: 0.00004305
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002285
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002062
Iteration 7/1000 | Loss: 0.00002007
Iteration 8/1000 | Loss: 0.00001965
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001885
Iteration 13/1000 | Loss: 0.00001871
Iteration 14/1000 | Loss: 0.00001866
Iteration 15/1000 | Loss: 0.00001858
Iteration 16/1000 | Loss: 0.00001857
Iteration 17/1000 | Loss: 0.00001857
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001854
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001853
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001850
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001849
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001844
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001843
Iteration 38/1000 | Loss: 0.00001842
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001840
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001833
Iteration 53/1000 | Loss: 0.00001833
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001832
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001831
Iteration 61/1000 | Loss: 0.00001831
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001830
Iteration 64/1000 | Loss: 0.00001830
Iteration 65/1000 | Loss: 0.00001830
Iteration 66/1000 | Loss: 0.00001830
Iteration 67/1000 | Loss: 0.00001830
Iteration 68/1000 | Loss: 0.00001830
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001829
Iteration 71/1000 | Loss: 0.00001829
Iteration 72/1000 | Loss: 0.00001829
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001829
Iteration 75/1000 | Loss: 0.00001829
Iteration 76/1000 | Loss: 0.00001829
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001827
Iteration 85/1000 | Loss: 0.00001827
Iteration 86/1000 | Loss: 0.00001827
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001826
Iteration 89/1000 | Loss: 0.00001826
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001825
Iteration 95/1000 | Loss: 0.00001825
Iteration 96/1000 | Loss: 0.00001825
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001824
Iteration 99/1000 | Loss: 0.00001824
Iteration 100/1000 | Loss: 0.00001824
Iteration 101/1000 | Loss: 0.00001824
Iteration 102/1000 | Loss: 0.00001824
Iteration 103/1000 | Loss: 0.00001824
Iteration 104/1000 | Loss: 0.00001823
Iteration 105/1000 | Loss: 0.00001823
Iteration 106/1000 | Loss: 0.00001823
Iteration 107/1000 | Loss: 0.00001822
Iteration 108/1000 | Loss: 0.00001822
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001822
Iteration 111/1000 | Loss: 0.00001822
Iteration 112/1000 | Loss: 0.00001821
Iteration 113/1000 | Loss: 0.00001821
Iteration 114/1000 | Loss: 0.00001821
Iteration 115/1000 | Loss: 0.00001821
Iteration 116/1000 | Loss: 0.00001820
Iteration 117/1000 | Loss: 0.00001820
Iteration 118/1000 | Loss: 0.00001820
Iteration 119/1000 | Loss: 0.00001820
Iteration 120/1000 | Loss: 0.00001820
Iteration 121/1000 | Loss: 0.00001820
Iteration 122/1000 | Loss: 0.00001820
Iteration 123/1000 | Loss: 0.00001820
Iteration 124/1000 | Loss: 0.00001820
Iteration 125/1000 | Loss: 0.00001820
Iteration 126/1000 | Loss: 0.00001820
Iteration 127/1000 | Loss: 0.00001820
Iteration 128/1000 | Loss: 0.00001820
Iteration 129/1000 | Loss: 0.00001820
Iteration 130/1000 | Loss: 0.00001820
Iteration 131/1000 | Loss: 0.00001820
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.8203987565357238e-05, 1.8203987565357238e-05, 1.8203987565357238e-05, 1.8203987565357238e-05, 1.8203987565357238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8203987565357238e-05

Optimization complete. Final v2v error: 3.599987745285034 mm

Highest mean error: 4.295309543609619 mm for frame 83

Lowest mean error: 3.115126371383667 mm for frame 6

Saving results

Total time: 58.385762453079224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757913
Iteration 2/25 | Loss: 0.00138998
Iteration 3/25 | Loss: 0.00128704
Iteration 4/25 | Loss: 0.00127549
Iteration 5/25 | Loss: 0.00127177
Iteration 6/25 | Loss: 0.00127152
Iteration 7/25 | Loss: 0.00127152
Iteration 8/25 | Loss: 0.00127152
Iteration 9/25 | Loss: 0.00127152
Iteration 10/25 | Loss: 0.00127152
Iteration 11/25 | Loss: 0.00127152
Iteration 12/25 | Loss: 0.00127152
Iteration 13/25 | Loss: 0.00127152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012715219054371119, 0.0012715219054371119, 0.0012715219054371119, 0.0012715219054371119, 0.0012715219054371119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012715219054371119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32226920
Iteration 2/25 | Loss: 0.00143016
Iteration 3/25 | Loss: 0.00143016
Iteration 4/25 | Loss: 0.00143016
Iteration 5/25 | Loss: 0.00143016
Iteration 6/25 | Loss: 0.00143016
Iteration 7/25 | Loss: 0.00143016
Iteration 8/25 | Loss: 0.00143016
Iteration 9/25 | Loss: 0.00143016
Iteration 10/25 | Loss: 0.00143016
Iteration 11/25 | Loss: 0.00143016
Iteration 12/25 | Loss: 0.00143016
Iteration 13/25 | Loss: 0.00143016
Iteration 14/25 | Loss: 0.00143016
Iteration 15/25 | Loss: 0.00143016
Iteration 16/25 | Loss: 0.00143016
Iteration 17/25 | Loss: 0.00143016
Iteration 18/25 | Loss: 0.00143016
Iteration 19/25 | Loss: 0.00143016
Iteration 20/25 | Loss: 0.00143016
Iteration 21/25 | Loss: 0.00143016
Iteration 22/25 | Loss: 0.00143016
Iteration 23/25 | Loss: 0.00143016
Iteration 24/25 | Loss: 0.00143016
Iteration 25/25 | Loss: 0.00143016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143016
Iteration 2/1000 | Loss: 0.00003599
Iteration 3/1000 | Loss: 0.00002168
Iteration 4/1000 | Loss: 0.00001962
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001779
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001701
Iteration 9/1000 | Loss: 0.00001677
Iteration 10/1000 | Loss: 0.00001654
Iteration 11/1000 | Loss: 0.00001632
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001609
Iteration 14/1000 | Loss: 0.00001605
Iteration 15/1000 | Loss: 0.00001603
Iteration 16/1000 | Loss: 0.00001602
Iteration 17/1000 | Loss: 0.00001600
Iteration 18/1000 | Loss: 0.00001600
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001596
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001587
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001571
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001564
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001562
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001560
Iteration 42/1000 | Loss: 0.00001559
Iteration 43/1000 | Loss: 0.00001559
Iteration 44/1000 | Loss: 0.00001558
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001554
Iteration 66/1000 | Loss: 0.00001554
Iteration 67/1000 | Loss: 0.00001553
Iteration 68/1000 | Loss: 0.00001553
Iteration 69/1000 | Loss: 0.00001553
Iteration 70/1000 | Loss: 0.00001553
Iteration 71/1000 | Loss: 0.00001553
Iteration 72/1000 | Loss: 0.00001553
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001552
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001552
Iteration 80/1000 | Loss: 0.00001552
Iteration 81/1000 | Loss: 0.00001552
Iteration 82/1000 | Loss: 0.00001551
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001549
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001547
Iteration 91/1000 | Loss: 0.00001547
Iteration 92/1000 | Loss: 0.00001547
Iteration 93/1000 | Loss: 0.00001547
Iteration 94/1000 | Loss: 0.00001547
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001546
Iteration 98/1000 | Loss: 0.00001546
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001543
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001542
Iteration 111/1000 | Loss: 0.00001542
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001538
Iteration 128/1000 | Loss: 0.00001538
Iteration 129/1000 | Loss: 0.00001538
Iteration 130/1000 | Loss: 0.00001538
Iteration 131/1000 | Loss: 0.00001538
Iteration 132/1000 | Loss: 0.00001538
Iteration 133/1000 | Loss: 0.00001538
Iteration 134/1000 | Loss: 0.00001538
Iteration 135/1000 | Loss: 0.00001537
Iteration 136/1000 | Loss: 0.00001537
Iteration 137/1000 | Loss: 0.00001537
Iteration 138/1000 | Loss: 0.00001536
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001536
Iteration 142/1000 | Loss: 0.00001536
Iteration 143/1000 | Loss: 0.00001535
Iteration 144/1000 | Loss: 0.00001535
Iteration 145/1000 | Loss: 0.00001535
Iteration 146/1000 | Loss: 0.00001534
Iteration 147/1000 | Loss: 0.00001534
Iteration 148/1000 | Loss: 0.00001534
Iteration 149/1000 | Loss: 0.00001534
Iteration 150/1000 | Loss: 0.00001534
Iteration 151/1000 | Loss: 0.00001534
Iteration 152/1000 | Loss: 0.00001534
Iteration 153/1000 | Loss: 0.00001534
Iteration 154/1000 | Loss: 0.00001533
Iteration 155/1000 | Loss: 0.00001533
Iteration 156/1000 | Loss: 0.00001533
Iteration 157/1000 | Loss: 0.00001533
Iteration 158/1000 | Loss: 0.00001533
Iteration 159/1000 | Loss: 0.00001533
Iteration 160/1000 | Loss: 0.00001533
Iteration 161/1000 | Loss: 0.00001533
Iteration 162/1000 | Loss: 0.00001533
Iteration 163/1000 | Loss: 0.00001533
Iteration 164/1000 | Loss: 0.00001533
Iteration 165/1000 | Loss: 0.00001533
Iteration 166/1000 | Loss: 0.00001533
Iteration 167/1000 | Loss: 0.00001532
Iteration 168/1000 | Loss: 0.00001532
Iteration 169/1000 | Loss: 0.00001532
Iteration 170/1000 | Loss: 0.00001532
Iteration 171/1000 | Loss: 0.00001532
Iteration 172/1000 | Loss: 0.00001532
Iteration 173/1000 | Loss: 0.00001531
Iteration 174/1000 | Loss: 0.00001531
Iteration 175/1000 | Loss: 0.00001531
Iteration 176/1000 | Loss: 0.00001531
Iteration 177/1000 | Loss: 0.00001531
Iteration 178/1000 | Loss: 0.00001531
Iteration 179/1000 | Loss: 0.00001531
Iteration 180/1000 | Loss: 0.00001531
Iteration 181/1000 | Loss: 0.00001531
Iteration 182/1000 | Loss: 0.00001531
Iteration 183/1000 | Loss: 0.00001530
Iteration 184/1000 | Loss: 0.00001530
Iteration 185/1000 | Loss: 0.00001530
Iteration 186/1000 | Loss: 0.00001530
Iteration 187/1000 | Loss: 0.00001530
Iteration 188/1000 | Loss: 0.00001530
Iteration 189/1000 | Loss: 0.00001530
Iteration 190/1000 | Loss: 0.00001530
Iteration 191/1000 | Loss: 0.00001530
Iteration 192/1000 | Loss: 0.00001530
Iteration 193/1000 | Loss: 0.00001529
Iteration 194/1000 | Loss: 0.00001529
Iteration 195/1000 | Loss: 0.00001529
Iteration 196/1000 | Loss: 0.00001529
Iteration 197/1000 | Loss: 0.00001529
Iteration 198/1000 | Loss: 0.00001529
Iteration 199/1000 | Loss: 0.00001529
Iteration 200/1000 | Loss: 0.00001529
Iteration 201/1000 | Loss: 0.00001529
Iteration 202/1000 | Loss: 0.00001529
Iteration 203/1000 | Loss: 0.00001529
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001529
Iteration 207/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5288836948457174e-05, 1.5288836948457174e-05, 1.5288836948457174e-05, 1.5288836948457174e-05, 1.5288836948457174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5288836948457174e-05

Optimization complete. Final v2v error: 3.2975974082946777 mm

Highest mean error: 4.2891340255737305 mm for frame 142

Lowest mean error: 2.887216329574585 mm for frame 114

Saving results

Total time: 49.892523527145386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805050
Iteration 2/25 | Loss: 0.00147729
Iteration 3/25 | Loss: 0.00125092
Iteration 4/25 | Loss: 0.00122505
Iteration 5/25 | Loss: 0.00122115
Iteration 6/25 | Loss: 0.00122018
Iteration 7/25 | Loss: 0.00121977
Iteration 8/25 | Loss: 0.00121953
Iteration 9/25 | Loss: 0.00121936
Iteration 10/25 | Loss: 0.00122131
Iteration 11/25 | Loss: 0.00122172
Iteration 12/25 | Loss: 0.00122079
Iteration 13/25 | Loss: 0.00121846
Iteration 14/25 | Loss: 0.00121724
Iteration 15/25 | Loss: 0.00121876
Iteration 16/25 | Loss: 0.00121772
Iteration 17/25 | Loss: 0.00121791
Iteration 18/25 | Loss: 0.00121808
Iteration 19/25 | Loss: 0.00121843
Iteration 20/25 | Loss: 0.00121843
Iteration 21/25 | Loss: 0.00121797
Iteration 22/25 | Loss: 0.00121838
Iteration 23/25 | Loss: 0.00121838
Iteration 24/25 | Loss: 0.00121838
Iteration 25/25 | Loss: 0.00121785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28776383
Iteration 2/25 | Loss: 0.00148986
Iteration 3/25 | Loss: 0.00148986
Iteration 4/25 | Loss: 0.00148986
Iteration 5/25 | Loss: 0.00148986
Iteration 6/25 | Loss: 0.00148986
Iteration 7/25 | Loss: 0.00148986
Iteration 8/25 | Loss: 0.00148986
Iteration 9/25 | Loss: 0.00148986
Iteration 10/25 | Loss: 0.00148986
Iteration 11/25 | Loss: 0.00148986
Iteration 12/25 | Loss: 0.00148986
Iteration 13/25 | Loss: 0.00148986
Iteration 14/25 | Loss: 0.00148986
Iteration 15/25 | Loss: 0.00148986
Iteration 16/25 | Loss: 0.00148986
Iteration 17/25 | Loss: 0.00148986
Iteration 18/25 | Loss: 0.00148986
Iteration 19/25 | Loss: 0.00148986
Iteration 20/25 | Loss: 0.00148986
Iteration 21/25 | Loss: 0.00148986
Iteration 22/25 | Loss: 0.00148986
Iteration 23/25 | Loss: 0.00148986
Iteration 24/25 | Loss: 0.00148986
Iteration 25/25 | Loss: 0.00148986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148986
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00002115
Iteration 4/1000 | Loss: 0.00002409
Iteration 5/1000 | Loss: 0.00002172
Iteration 6/1000 | Loss: 0.00002627
Iteration 7/1000 | Loss: 0.00002168
Iteration 8/1000 | Loss: 0.00002721
Iteration 9/1000 | Loss: 0.00002246
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00002974
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00002699
Iteration 14/1000 | Loss: 0.00002188
Iteration 15/1000 | Loss: 0.00002775
Iteration 16/1000 | Loss: 0.00003519
Iteration 17/1000 | Loss: 0.00002910
Iteration 18/1000 | Loss: 0.00002149
Iteration 19/1000 | Loss: 0.00003450
Iteration 20/1000 | Loss: 0.00002624
Iteration 21/1000 | Loss: 0.00003490
Iteration 22/1000 | Loss: 0.00002672
Iteration 23/1000 | Loss: 0.00003373
Iteration 24/1000 | Loss: 0.00002808
Iteration 25/1000 | Loss: 0.00003463
Iteration 26/1000 | Loss: 0.00002783
Iteration 27/1000 | Loss: 0.00003632
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001108
Iteration 34/1000 | Loss: 0.00001107
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001089
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001073
Iteration 39/1000 | Loss: 0.00001072
Iteration 40/1000 | Loss: 0.00001071
Iteration 41/1000 | Loss: 0.00001071
Iteration 42/1000 | Loss: 0.00001054
Iteration 43/1000 | Loss: 0.00001032
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001014
Iteration 46/1000 | Loss: 0.00001013
Iteration 47/1000 | Loss: 0.00001013
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001008
Iteration 52/1000 | Loss: 0.00001007
Iteration 53/1000 | Loss: 0.00001006
Iteration 54/1000 | Loss: 0.00001006
Iteration 55/1000 | Loss: 0.00001005
Iteration 56/1000 | Loss: 0.00001005
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001004
Iteration 62/1000 | Loss: 0.00001004
Iteration 63/1000 | Loss: 0.00001003
Iteration 64/1000 | Loss: 0.00001003
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000998
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000997
Iteration 73/1000 | Loss: 0.00000997
Iteration 74/1000 | Loss: 0.00000996
Iteration 75/1000 | Loss: 0.00000996
Iteration 76/1000 | Loss: 0.00000996
Iteration 77/1000 | Loss: 0.00000996
Iteration 78/1000 | Loss: 0.00000996
Iteration 79/1000 | Loss: 0.00000996
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000996
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000995
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000993
Iteration 100/1000 | Loss: 0.00000993
Iteration 101/1000 | Loss: 0.00000993
Iteration 102/1000 | Loss: 0.00000993
Iteration 103/1000 | Loss: 0.00000993
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000993
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000992
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000991
Iteration 121/1000 | Loss: 0.00000991
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000991
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000990
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000989
Iteration 134/1000 | Loss: 0.00000989
Iteration 135/1000 | Loss: 0.00000989
Iteration 136/1000 | Loss: 0.00000989
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000988
Iteration 140/1000 | Loss: 0.00000988
Iteration 141/1000 | Loss: 0.00000988
Iteration 142/1000 | Loss: 0.00000988
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000986
Iteration 151/1000 | Loss: 0.00000986
Iteration 152/1000 | Loss: 0.00000986
Iteration 153/1000 | Loss: 0.00000986
Iteration 154/1000 | Loss: 0.00000986
Iteration 155/1000 | Loss: 0.00000986
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000986
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000985
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000984
Iteration 170/1000 | Loss: 0.00000984
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000984
Iteration 173/1000 | Loss: 0.00000984
Iteration 174/1000 | Loss: 0.00000984
Iteration 175/1000 | Loss: 0.00000984
Iteration 176/1000 | Loss: 0.00000984
Iteration 177/1000 | Loss: 0.00000984
Iteration 178/1000 | Loss: 0.00000984
Iteration 179/1000 | Loss: 0.00000984
Iteration 180/1000 | Loss: 0.00000984
Iteration 181/1000 | Loss: 0.00000984
Iteration 182/1000 | Loss: 0.00000984
Iteration 183/1000 | Loss: 0.00000984
Iteration 184/1000 | Loss: 0.00000983
Iteration 185/1000 | Loss: 0.00000983
Iteration 186/1000 | Loss: 0.00000983
Iteration 187/1000 | Loss: 0.00000983
Iteration 188/1000 | Loss: 0.00000983
Iteration 189/1000 | Loss: 0.00000983
Iteration 190/1000 | Loss: 0.00000983
Iteration 191/1000 | Loss: 0.00000982
Iteration 192/1000 | Loss: 0.00000982
Iteration 193/1000 | Loss: 0.00000982
Iteration 194/1000 | Loss: 0.00000980
Iteration 195/1000 | Loss: 0.00000980
Iteration 196/1000 | Loss: 0.00000980
Iteration 197/1000 | Loss: 0.00000979
Iteration 198/1000 | Loss: 0.00000978
Iteration 199/1000 | Loss: 0.00000978
Iteration 200/1000 | Loss: 0.00000977
Iteration 201/1000 | Loss: 0.00000977
Iteration 202/1000 | Loss: 0.00000976
Iteration 203/1000 | Loss: 0.00000976
Iteration 204/1000 | Loss: 0.00000975
Iteration 205/1000 | Loss: 0.00000975
Iteration 206/1000 | Loss: 0.00000974
Iteration 207/1000 | Loss: 0.00000974
Iteration 208/1000 | Loss: 0.00000974
Iteration 209/1000 | Loss: 0.00000974
Iteration 210/1000 | Loss: 0.00000973
Iteration 211/1000 | Loss: 0.00000973
Iteration 212/1000 | Loss: 0.00000973
Iteration 213/1000 | Loss: 0.00000973
Iteration 214/1000 | Loss: 0.00000972
Iteration 215/1000 | Loss: 0.00000972
Iteration 216/1000 | Loss: 0.00000972
Iteration 217/1000 | Loss: 0.00000972
Iteration 218/1000 | Loss: 0.00000972
Iteration 219/1000 | Loss: 0.00000972
Iteration 220/1000 | Loss: 0.00000972
Iteration 221/1000 | Loss: 0.00000972
Iteration 222/1000 | Loss: 0.00000972
Iteration 223/1000 | Loss: 0.00000972
Iteration 224/1000 | Loss: 0.00000972
Iteration 225/1000 | Loss: 0.00000971
Iteration 226/1000 | Loss: 0.00000971
Iteration 227/1000 | Loss: 0.00000971
Iteration 228/1000 | Loss: 0.00000971
Iteration 229/1000 | Loss: 0.00000971
Iteration 230/1000 | Loss: 0.00000971
Iteration 231/1000 | Loss: 0.00000971
Iteration 232/1000 | Loss: 0.00000971
Iteration 233/1000 | Loss: 0.00000971
Iteration 234/1000 | Loss: 0.00000971
Iteration 235/1000 | Loss: 0.00000971
Iteration 236/1000 | Loss: 0.00000971
Iteration 237/1000 | Loss: 0.00000971
Iteration 238/1000 | Loss: 0.00000971
Iteration 239/1000 | Loss: 0.00000971
Iteration 240/1000 | Loss: 0.00000970
Iteration 241/1000 | Loss: 0.00000970
Iteration 242/1000 | Loss: 0.00000970
Iteration 243/1000 | Loss: 0.00000970
Iteration 244/1000 | Loss: 0.00000970
Iteration 245/1000 | Loss: 0.00000970
Iteration 246/1000 | Loss: 0.00000970
Iteration 247/1000 | Loss: 0.00000970
Iteration 248/1000 | Loss: 0.00000970
Iteration 249/1000 | Loss: 0.00000970
Iteration 250/1000 | Loss: 0.00000970
Iteration 251/1000 | Loss: 0.00000970
Iteration 252/1000 | Loss: 0.00000970
Iteration 253/1000 | Loss: 0.00000970
Iteration 254/1000 | Loss: 0.00000970
Iteration 255/1000 | Loss: 0.00000970
Iteration 256/1000 | Loss: 0.00000969
Iteration 257/1000 | Loss: 0.00000969
Iteration 258/1000 | Loss: 0.00000969
Iteration 259/1000 | Loss: 0.00000969
Iteration 260/1000 | Loss: 0.00000969
Iteration 261/1000 | Loss: 0.00000969
Iteration 262/1000 | Loss: 0.00000969
Iteration 263/1000 | Loss: 0.00000969
Iteration 264/1000 | Loss: 0.00000969
Iteration 265/1000 | Loss: 0.00000969
Iteration 266/1000 | Loss: 0.00000969
Iteration 267/1000 | Loss: 0.00000969
Iteration 268/1000 | Loss: 0.00000969
Iteration 269/1000 | Loss: 0.00000969
Iteration 270/1000 | Loss: 0.00000969
Iteration 271/1000 | Loss: 0.00000969
Iteration 272/1000 | Loss: 0.00000969
Iteration 273/1000 | Loss: 0.00000969
Iteration 274/1000 | Loss: 0.00000969
Iteration 275/1000 | Loss: 0.00000969
Iteration 276/1000 | Loss: 0.00000969
Iteration 277/1000 | Loss: 0.00000969
Iteration 278/1000 | Loss: 0.00000969
Iteration 279/1000 | Loss: 0.00000969
Iteration 280/1000 | Loss: 0.00000969
Iteration 281/1000 | Loss: 0.00000969
Iteration 282/1000 | Loss: 0.00000969
Iteration 283/1000 | Loss: 0.00000969
Iteration 284/1000 | Loss: 0.00000969
Iteration 285/1000 | Loss: 0.00000969
Iteration 286/1000 | Loss: 0.00000969
Iteration 287/1000 | Loss: 0.00000969
Iteration 288/1000 | Loss: 0.00000969
Iteration 289/1000 | Loss: 0.00000969
Iteration 290/1000 | Loss: 0.00000969
Iteration 291/1000 | Loss: 0.00000969
Iteration 292/1000 | Loss: 0.00000969
Iteration 293/1000 | Loss: 0.00000969
Iteration 294/1000 | Loss: 0.00000969
Iteration 295/1000 | Loss: 0.00000969
Iteration 296/1000 | Loss: 0.00000969
Iteration 297/1000 | Loss: 0.00000969
Iteration 298/1000 | Loss: 0.00000969
Iteration 299/1000 | Loss: 0.00000969
Iteration 300/1000 | Loss: 0.00000969
Iteration 301/1000 | Loss: 0.00000969
Iteration 302/1000 | Loss: 0.00000969
Iteration 303/1000 | Loss: 0.00000969
Iteration 304/1000 | Loss: 0.00000969
Iteration 305/1000 | Loss: 0.00000969
Iteration 306/1000 | Loss: 0.00000969
Iteration 307/1000 | Loss: 0.00000969
Iteration 308/1000 | Loss: 0.00000969
Iteration 309/1000 | Loss: 0.00000969
Iteration 310/1000 | Loss: 0.00000969
Iteration 311/1000 | Loss: 0.00000969
Iteration 312/1000 | Loss: 0.00000969
Iteration 313/1000 | Loss: 0.00000969
Iteration 314/1000 | Loss: 0.00000969
Iteration 315/1000 | Loss: 0.00000969
Iteration 316/1000 | Loss: 0.00000969
Iteration 317/1000 | Loss: 0.00000969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [9.690624210634269e-06, 9.690624210634269e-06, 9.690624210634269e-06, 9.690624210634269e-06, 9.690624210634269e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.690624210634269e-06

Optimization complete. Final v2v error: 2.6846704483032227 mm

Highest mean error: 4.192754745483398 mm for frame 63

Lowest mean error: 2.5546977519989014 mm for frame 3

Saving results

Total time: 112.54981660842896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678431
Iteration 2/25 | Loss: 0.00158816
Iteration 3/25 | Loss: 0.00135339
Iteration 4/25 | Loss: 0.00131642
Iteration 5/25 | Loss: 0.00127939
Iteration 6/25 | Loss: 0.00127223
Iteration 7/25 | Loss: 0.00126987
Iteration 8/25 | Loss: 0.00126916
Iteration 9/25 | Loss: 0.00126891
Iteration 10/25 | Loss: 0.00126886
Iteration 11/25 | Loss: 0.00126886
Iteration 12/25 | Loss: 0.00126886
Iteration 13/25 | Loss: 0.00126886
Iteration 14/25 | Loss: 0.00126886
Iteration 15/25 | Loss: 0.00126885
Iteration 16/25 | Loss: 0.00126885
Iteration 17/25 | Loss: 0.00126885
Iteration 18/25 | Loss: 0.00126885
Iteration 19/25 | Loss: 0.00126885
Iteration 20/25 | Loss: 0.00126885
Iteration 21/25 | Loss: 0.00126885
Iteration 22/25 | Loss: 0.00126885
Iteration 23/25 | Loss: 0.00126885
Iteration 24/25 | Loss: 0.00126884
Iteration 25/25 | Loss: 0.00126884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32274663
Iteration 2/25 | Loss: 0.00235799
Iteration 3/25 | Loss: 0.00235798
Iteration 4/25 | Loss: 0.00235798
Iteration 5/25 | Loss: 0.00235798
Iteration 6/25 | Loss: 0.00235798
Iteration 7/25 | Loss: 0.00235798
Iteration 8/25 | Loss: 0.00235798
Iteration 9/25 | Loss: 0.00235798
Iteration 10/25 | Loss: 0.00235798
Iteration 11/25 | Loss: 0.00235798
Iteration 12/25 | Loss: 0.00235798
Iteration 13/25 | Loss: 0.00235798
Iteration 14/25 | Loss: 0.00235798
Iteration 15/25 | Loss: 0.00235798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002357982099056244, 0.002357982099056244, 0.002357982099056244, 0.002357982099056244, 0.002357982099056244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002357982099056244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235798
Iteration 2/1000 | Loss: 0.00007079
Iteration 3/1000 | Loss: 0.00004437
Iteration 4/1000 | Loss: 0.00003208
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002769
Iteration 7/1000 | Loss: 0.00002656
Iteration 8/1000 | Loss: 0.00002584
Iteration 9/1000 | Loss: 0.00002535
Iteration 10/1000 | Loss: 0.00002497
Iteration 11/1000 | Loss: 0.00002462
Iteration 12/1000 | Loss: 0.00002431
Iteration 13/1000 | Loss: 0.00002404
Iteration 14/1000 | Loss: 0.00002384
Iteration 15/1000 | Loss: 0.00002367
Iteration 16/1000 | Loss: 0.00002348
Iteration 17/1000 | Loss: 0.00002333
Iteration 18/1000 | Loss: 0.00002328
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002322
Iteration 21/1000 | Loss: 0.00002321
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002317
Iteration 24/1000 | Loss: 0.00002315
Iteration 25/1000 | Loss: 0.00002315
Iteration 26/1000 | Loss: 0.00002315
Iteration 27/1000 | Loss: 0.00002315
Iteration 28/1000 | Loss: 0.00002315
Iteration 29/1000 | Loss: 0.00002315
Iteration 30/1000 | Loss: 0.00002315
Iteration 31/1000 | Loss: 0.00002315
Iteration 32/1000 | Loss: 0.00002315
Iteration 33/1000 | Loss: 0.00002314
Iteration 34/1000 | Loss: 0.00002314
Iteration 35/1000 | Loss: 0.00002314
Iteration 36/1000 | Loss: 0.00002314
Iteration 37/1000 | Loss: 0.00002314
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002313
Iteration 40/1000 | Loss: 0.00002313
Iteration 41/1000 | Loss: 0.00002313
Iteration 42/1000 | Loss: 0.00002313
Iteration 43/1000 | Loss: 0.00002313
Iteration 44/1000 | Loss: 0.00002312
Iteration 45/1000 | Loss: 0.00002312
Iteration 46/1000 | Loss: 0.00002312
Iteration 47/1000 | Loss: 0.00002311
Iteration 48/1000 | Loss: 0.00002311
Iteration 49/1000 | Loss: 0.00002311
Iteration 50/1000 | Loss: 0.00002310
Iteration 51/1000 | Loss: 0.00002310
Iteration 52/1000 | Loss: 0.00002310
Iteration 53/1000 | Loss: 0.00002309
Iteration 54/1000 | Loss: 0.00002309
Iteration 55/1000 | Loss: 0.00002309
Iteration 56/1000 | Loss: 0.00002309
Iteration 57/1000 | Loss: 0.00002309
Iteration 58/1000 | Loss: 0.00002309
Iteration 59/1000 | Loss: 0.00002309
Iteration 60/1000 | Loss: 0.00002308
Iteration 61/1000 | Loss: 0.00002308
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002307
Iteration 65/1000 | Loss: 0.00002307
Iteration 66/1000 | Loss: 0.00002307
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002306
Iteration 70/1000 | Loss: 0.00002306
Iteration 71/1000 | Loss: 0.00002306
Iteration 72/1000 | Loss: 0.00002306
Iteration 73/1000 | Loss: 0.00002306
Iteration 74/1000 | Loss: 0.00002306
Iteration 75/1000 | Loss: 0.00002305
Iteration 76/1000 | Loss: 0.00002305
Iteration 77/1000 | Loss: 0.00002305
Iteration 78/1000 | Loss: 0.00002305
Iteration 79/1000 | Loss: 0.00002305
Iteration 80/1000 | Loss: 0.00002305
Iteration 81/1000 | Loss: 0.00002305
Iteration 82/1000 | Loss: 0.00002305
Iteration 83/1000 | Loss: 0.00002305
Iteration 84/1000 | Loss: 0.00002304
Iteration 85/1000 | Loss: 0.00002304
Iteration 86/1000 | Loss: 0.00002304
Iteration 87/1000 | Loss: 0.00002304
Iteration 88/1000 | Loss: 0.00002304
Iteration 89/1000 | Loss: 0.00002304
Iteration 90/1000 | Loss: 0.00002304
Iteration 91/1000 | Loss: 0.00002304
Iteration 92/1000 | Loss: 0.00002304
Iteration 93/1000 | Loss: 0.00002304
Iteration 94/1000 | Loss: 0.00002303
Iteration 95/1000 | Loss: 0.00002303
Iteration 96/1000 | Loss: 0.00002303
Iteration 97/1000 | Loss: 0.00002303
Iteration 98/1000 | Loss: 0.00002303
Iteration 99/1000 | Loss: 0.00002303
Iteration 100/1000 | Loss: 0.00002303
Iteration 101/1000 | Loss: 0.00002303
Iteration 102/1000 | Loss: 0.00002303
Iteration 103/1000 | Loss: 0.00002303
Iteration 104/1000 | Loss: 0.00002302
Iteration 105/1000 | Loss: 0.00002302
Iteration 106/1000 | Loss: 0.00002302
Iteration 107/1000 | Loss: 0.00002302
Iteration 108/1000 | Loss: 0.00002302
Iteration 109/1000 | Loss: 0.00002302
Iteration 110/1000 | Loss: 0.00002302
Iteration 111/1000 | Loss: 0.00002302
Iteration 112/1000 | Loss: 0.00002302
Iteration 113/1000 | Loss: 0.00002302
Iteration 114/1000 | Loss: 0.00002302
Iteration 115/1000 | Loss: 0.00002302
Iteration 116/1000 | Loss: 0.00002302
Iteration 117/1000 | Loss: 0.00002301
Iteration 118/1000 | Loss: 0.00002301
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00002301
Iteration 121/1000 | Loss: 0.00002301
Iteration 122/1000 | Loss: 0.00002301
Iteration 123/1000 | Loss: 0.00002301
Iteration 124/1000 | Loss: 0.00002300
Iteration 125/1000 | Loss: 0.00002300
Iteration 126/1000 | Loss: 0.00002300
Iteration 127/1000 | Loss: 0.00002300
Iteration 128/1000 | Loss: 0.00002300
Iteration 129/1000 | Loss: 0.00002300
Iteration 130/1000 | Loss: 0.00002300
Iteration 131/1000 | Loss: 0.00002300
Iteration 132/1000 | Loss: 0.00002299
Iteration 133/1000 | Loss: 0.00002299
Iteration 134/1000 | Loss: 0.00002299
Iteration 135/1000 | Loss: 0.00002299
Iteration 136/1000 | Loss: 0.00002299
Iteration 137/1000 | Loss: 0.00002299
Iteration 138/1000 | Loss: 0.00002299
Iteration 139/1000 | Loss: 0.00002299
Iteration 140/1000 | Loss: 0.00002299
Iteration 141/1000 | Loss: 0.00002299
Iteration 142/1000 | Loss: 0.00002299
Iteration 143/1000 | Loss: 0.00002299
Iteration 144/1000 | Loss: 0.00002299
Iteration 145/1000 | Loss: 0.00002299
Iteration 146/1000 | Loss: 0.00002299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.29886263696244e-05, 2.29886263696244e-05, 2.29886263696244e-05, 2.29886263696244e-05, 2.29886263696244e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.29886263696244e-05

Optimization complete. Final v2v error: 4.044085502624512 mm

Highest mean error: 4.673016548156738 mm for frame 122

Lowest mean error: 2.8083128929138184 mm for frame 179

Saving results

Total time: 51.626305103302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998086
Iteration 2/25 | Loss: 0.00998086
Iteration 3/25 | Loss: 0.00998086
Iteration 4/25 | Loss: 0.00998086
Iteration 5/25 | Loss: 0.00998085
Iteration 6/25 | Loss: 0.00998085
Iteration 7/25 | Loss: 0.00998084
Iteration 8/25 | Loss: 0.00998084
Iteration 9/25 | Loss: 0.00998083
Iteration 10/25 | Loss: 0.00440665
Iteration 11/25 | Loss: 0.00330837
Iteration 12/25 | Loss: 0.00231018
Iteration 13/25 | Loss: 0.00215447
Iteration 14/25 | Loss: 0.00220266
Iteration 15/25 | Loss: 0.00208388
Iteration 16/25 | Loss: 0.00196517
Iteration 17/25 | Loss: 0.00187869
Iteration 18/25 | Loss: 0.00184509
Iteration 19/25 | Loss: 0.00177267
Iteration 20/25 | Loss: 0.00174982
Iteration 21/25 | Loss: 0.00170464
Iteration 22/25 | Loss: 0.00167785
Iteration 23/25 | Loss: 0.00165776
Iteration 24/25 | Loss: 0.00163550
Iteration 25/25 | Loss: 0.00162234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23736680
Iteration 2/25 | Loss: 0.00496171
Iteration 3/25 | Loss: 0.00371018
Iteration 4/25 | Loss: 0.00371017
Iteration 5/25 | Loss: 0.00371017
Iteration 6/25 | Loss: 0.00371017
Iteration 7/25 | Loss: 0.00371017
Iteration 8/25 | Loss: 0.00371017
Iteration 9/25 | Loss: 0.00371017
Iteration 10/25 | Loss: 0.00371017
Iteration 11/25 | Loss: 0.00371017
Iteration 12/25 | Loss: 0.00371017
Iteration 13/25 | Loss: 0.00371017
Iteration 14/25 | Loss: 0.00371017
Iteration 15/25 | Loss: 0.00371017
Iteration 16/25 | Loss: 0.00371017
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003710172139108181, 0.003710172139108181, 0.003710172139108181, 0.003710172139108181, 0.003710172139108181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003710172139108181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00371017
Iteration 2/1000 | Loss: 0.00437702
Iteration 3/1000 | Loss: 0.00192249
Iteration 4/1000 | Loss: 0.00098440
Iteration 5/1000 | Loss: 0.00057032
Iteration 6/1000 | Loss: 0.00096540
Iteration 7/1000 | Loss: 0.00144737
Iteration 8/1000 | Loss: 0.00050242
Iteration 9/1000 | Loss: 0.00068897
Iteration 10/1000 | Loss: 0.00059700
Iteration 11/1000 | Loss: 0.00094559
Iteration 12/1000 | Loss: 0.00127541
Iteration 13/1000 | Loss: 0.00075049
Iteration 14/1000 | Loss: 0.00085188
Iteration 15/1000 | Loss: 0.00072506
Iteration 16/1000 | Loss: 0.00060323
Iteration 17/1000 | Loss: 0.00065969
Iteration 18/1000 | Loss: 0.00055141
Iteration 19/1000 | Loss: 0.00084518
Iteration 20/1000 | Loss: 0.00102971
Iteration 21/1000 | Loss: 0.00071148
Iteration 22/1000 | Loss: 0.00068801
Iteration 23/1000 | Loss: 0.00098733
Iteration 24/1000 | Loss: 0.00147297
Iteration 25/1000 | Loss: 0.00101622
Iteration 26/1000 | Loss: 0.00090765
Iteration 27/1000 | Loss: 0.00084176
Iteration 28/1000 | Loss: 0.00072731
Iteration 29/1000 | Loss: 0.00065575
Iteration 30/1000 | Loss: 0.00068595
Iteration 31/1000 | Loss: 0.00073552
Iteration 32/1000 | Loss: 0.00133280
Iteration 33/1000 | Loss: 0.00223402
Iteration 34/1000 | Loss: 0.00083825
Iteration 35/1000 | Loss: 0.00094416
Iteration 36/1000 | Loss: 0.00041870
Iteration 37/1000 | Loss: 0.00135756
Iteration 38/1000 | Loss: 0.00064879
Iteration 39/1000 | Loss: 0.00124506
Iteration 40/1000 | Loss: 0.00061945
Iteration 41/1000 | Loss: 0.00080210
Iteration 42/1000 | Loss: 0.00080012
Iteration 43/1000 | Loss: 0.00064181
Iteration 44/1000 | Loss: 0.00110574
Iteration 45/1000 | Loss: 0.00159881
Iteration 46/1000 | Loss: 0.00255668
Iteration 47/1000 | Loss: 0.00126944
Iteration 48/1000 | Loss: 0.00037898
Iteration 49/1000 | Loss: 0.00032489
Iteration 50/1000 | Loss: 0.00035506
Iteration 51/1000 | Loss: 0.00040756
Iteration 52/1000 | Loss: 0.00079128
Iteration 53/1000 | Loss: 0.00053086
Iteration 54/1000 | Loss: 0.00044678
Iteration 55/1000 | Loss: 0.00058893
Iteration 56/1000 | Loss: 0.00101725
Iteration 57/1000 | Loss: 0.00040008
Iteration 58/1000 | Loss: 0.00089242
Iteration 59/1000 | Loss: 0.00069749
Iteration 60/1000 | Loss: 0.00072601
Iteration 61/1000 | Loss: 0.00112466
Iteration 62/1000 | Loss: 0.00069538
Iteration 63/1000 | Loss: 0.00045667
Iteration 64/1000 | Loss: 0.00030941
Iteration 65/1000 | Loss: 0.00023071
Iteration 66/1000 | Loss: 0.00029325
Iteration 67/1000 | Loss: 0.00070712
Iteration 68/1000 | Loss: 0.00040559
Iteration 69/1000 | Loss: 0.00031225
Iteration 70/1000 | Loss: 0.00032411
Iteration 71/1000 | Loss: 0.00020763
Iteration 72/1000 | Loss: 0.00037504
Iteration 73/1000 | Loss: 0.00012522
Iteration 74/1000 | Loss: 0.00014740
Iteration 75/1000 | Loss: 0.00020197
Iteration 76/1000 | Loss: 0.00074525
Iteration 77/1000 | Loss: 0.00078408
Iteration 78/1000 | Loss: 0.00092908
Iteration 79/1000 | Loss: 0.00126960
Iteration 80/1000 | Loss: 0.00167123
Iteration 81/1000 | Loss: 0.00111818
Iteration 82/1000 | Loss: 0.00151883
Iteration 83/1000 | Loss: 0.00012522
Iteration 84/1000 | Loss: 0.00045245
Iteration 85/1000 | Loss: 0.00039616
Iteration 86/1000 | Loss: 0.00033485
Iteration 87/1000 | Loss: 0.00088231
Iteration 88/1000 | Loss: 0.00094374
Iteration 89/1000 | Loss: 0.00068830
Iteration 90/1000 | Loss: 0.00057913
Iteration 91/1000 | Loss: 0.00070955
Iteration 92/1000 | Loss: 0.00101326
Iteration 93/1000 | Loss: 0.00099714
Iteration 94/1000 | Loss: 0.00077959
Iteration 95/1000 | Loss: 0.00091689
Iteration 96/1000 | Loss: 0.00056470
Iteration 97/1000 | Loss: 0.00078794
Iteration 98/1000 | Loss: 0.00120693
Iteration 99/1000 | Loss: 0.00130190
Iteration 100/1000 | Loss: 0.00058046
Iteration 101/1000 | Loss: 0.00082626
Iteration 102/1000 | Loss: 0.00075741
Iteration 103/1000 | Loss: 0.00029558
Iteration 104/1000 | Loss: 0.00038077
Iteration 105/1000 | Loss: 0.00076508
Iteration 106/1000 | Loss: 0.00022393
Iteration 107/1000 | Loss: 0.00047986
Iteration 108/1000 | Loss: 0.00037390
Iteration 109/1000 | Loss: 0.00066436
Iteration 110/1000 | Loss: 0.00101693
Iteration 111/1000 | Loss: 0.00102975
Iteration 112/1000 | Loss: 0.00086751
Iteration 113/1000 | Loss: 0.00067196
Iteration 114/1000 | Loss: 0.00019507
Iteration 115/1000 | Loss: 0.00009027
Iteration 116/1000 | Loss: 0.00034705
Iteration 117/1000 | Loss: 0.00041517
Iteration 118/1000 | Loss: 0.00039503
Iteration 119/1000 | Loss: 0.00020375
Iteration 120/1000 | Loss: 0.00051181
Iteration 121/1000 | Loss: 0.00011118
Iteration 122/1000 | Loss: 0.00007028
Iteration 123/1000 | Loss: 0.00012218
Iteration 124/1000 | Loss: 0.00021707
Iteration 125/1000 | Loss: 0.00022652
Iteration 126/1000 | Loss: 0.00066314
Iteration 127/1000 | Loss: 0.00046705
Iteration 128/1000 | Loss: 0.00024700
Iteration 129/1000 | Loss: 0.00019493
Iteration 130/1000 | Loss: 0.00042202
Iteration 131/1000 | Loss: 0.00044159
Iteration 132/1000 | Loss: 0.00022884
Iteration 133/1000 | Loss: 0.00007754
Iteration 134/1000 | Loss: 0.00013139
Iteration 135/1000 | Loss: 0.00019155
Iteration 136/1000 | Loss: 0.00020223
Iteration 137/1000 | Loss: 0.00020660
Iteration 138/1000 | Loss: 0.00031544
Iteration 139/1000 | Loss: 0.00046729
Iteration 140/1000 | Loss: 0.00029793
Iteration 141/1000 | Loss: 0.00023249
Iteration 142/1000 | Loss: 0.00018179
Iteration 143/1000 | Loss: 0.00020876
Iteration 144/1000 | Loss: 0.00007211
Iteration 145/1000 | Loss: 0.00006338
Iteration 146/1000 | Loss: 0.00006374
Iteration 147/1000 | Loss: 0.00006659
Iteration 148/1000 | Loss: 0.00016277
Iteration 149/1000 | Loss: 0.00039240
Iteration 150/1000 | Loss: 0.00046115
Iteration 151/1000 | Loss: 0.00029647
Iteration 152/1000 | Loss: 0.00035081
Iteration 153/1000 | Loss: 0.00021188
Iteration 154/1000 | Loss: 0.00020284
Iteration 155/1000 | Loss: 0.00016404
Iteration 156/1000 | Loss: 0.00005443
Iteration 157/1000 | Loss: 0.00005469
Iteration 158/1000 | Loss: 0.00005580
Iteration 159/1000 | Loss: 0.00046390
Iteration 160/1000 | Loss: 0.00035253
Iteration 161/1000 | Loss: 0.00026322
Iteration 162/1000 | Loss: 0.00035778
Iteration 163/1000 | Loss: 0.00007138
Iteration 164/1000 | Loss: 0.00020550
Iteration 165/1000 | Loss: 0.00011250
Iteration 166/1000 | Loss: 0.00007709
Iteration 167/1000 | Loss: 0.00015083
Iteration 168/1000 | Loss: 0.00039777
Iteration 169/1000 | Loss: 0.00020184
Iteration 170/1000 | Loss: 0.00020904
Iteration 171/1000 | Loss: 0.00015995
Iteration 172/1000 | Loss: 0.00006272
Iteration 173/1000 | Loss: 0.00006703
Iteration 174/1000 | Loss: 0.00009924
Iteration 175/1000 | Loss: 0.00006565
Iteration 176/1000 | Loss: 0.00007086
Iteration 177/1000 | Loss: 0.00006216
Iteration 178/1000 | Loss: 0.00007459
Iteration 179/1000 | Loss: 0.00042766
Iteration 180/1000 | Loss: 0.00014582
Iteration 181/1000 | Loss: 0.00027685
Iteration 182/1000 | Loss: 0.00011338
Iteration 183/1000 | Loss: 0.00012278
Iteration 184/1000 | Loss: 0.00060028
Iteration 185/1000 | Loss: 0.00047068
Iteration 186/1000 | Loss: 0.00067528
Iteration 187/1000 | Loss: 0.00064878
Iteration 188/1000 | Loss: 0.00033884
Iteration 189/1000 | Loss: 0.00007636
Iteration 190/1000 | Loss: 0.00035706
Iteration 191/1000 | Loss: 0.00018824
Iteration 192/1000 | Loss: 0.00035525
Iteration 193/1000 | Loss: 0.00074176
Iteration 194/1000 | Loss: 0.00014063
Iteration 195/1000 | Loss: 0.00011085
Iteration 196/1000 | Loss: 0.00011192
Iteration 197/1000 | Loss: 0.00029089
Iteration 198/1000 | Loss: 0.00009264
Iteration 199/1000 | Loss: 0.00005749
Iteration 200/1000 | Loss: 0.00006922
Iteration 201/1000 | Loss: 0.00007092
Iteration 202/1000 | Loss: 0.00005025
Iteration 203/1000 | Loss: 0.00022565
Iteration 204/1000 | Loss: 0.00017717
Iteration 205/1000 | Loss: 0.00010305
Iteration 206/1000 | Loss: 0.00006136
Iteration 207/1000 | Loss: 0.00008673
Iteration 208/1000 | Loss: 0.00045443
Iteration 209/1000 | Loss: 0.00025487
Iteration 210/1000 | Loss: 0.00036005
Iteration 211/1000 | Loss: 0.00015961
Iteration 212/1000 | Loss: 0.00014368
Iteration 213/1000 | Loss: 0.00009055
Iteration 214/1000 | Loss: 0.00005864
Iteration 215/1000 | Loss: 0.00006432
Iteration 216/1000 | Loss: 0.00019044
Iteration 217/1000 | Loss: 0.00006468
Iteration 218/1000 | Loss: 0.00017867
Iteration 219/1000 | Loss: 0.00012499
Iteration 220/1000 | Loss: 0.00017875
Iteration 221/1000 | Loss: 0.00036534
Iteration 222/1000 | Loss: 0.00005962
Iteration 223/1000 | Loss: 0.00009340
Iteration 224/1000 | Loss: 0.00007920
Iteration 225/1000 | Loss: 0.00016457
Iteration 226/1000 | Loss: 0.00006467
Iteration 227/1000 | Loss: 0.00006699
Iteration 228/1000 | Loss: 0.00008269
Iteration 229/1000 | Loss: 0.00026860
Iteration 230/1000 | Loss: 0.00006749
Iteration 231/1000 | Loss: 0.00006590
Iteration 232/1000 | Loss: 0.00020247
Iteration 233/1000 | Loss: 0.00010000
Iteration 234/1000 | Loss: 0.00007815
Iteration 235/1000 | Loss: 0.00019638
Iteration 236/1000 | Loss: 0.00009517
Iteration 237/1000 | Loss: 0.00008176
Iteration 238/1000 | Loss: 0.00021552
Iteration 239/1000 | Loss: 0.00009821
Iteration 240/1000 | Loss: 0.00005455
Iteration 241/1000 | Loss: 0.00006312
Iteration 242/1000 | Loss: 0.00005742
Iteration 243/1000 | Loss: 0.00006070
Iteration 244/1000 | Loss: 0.00005006
Iteration 245/1000 | Loss: 0.00020617
Iteration 246/1000 | Loss: 0.00017914
Iteration 247/1000 | Loss: 0.00006378
Iteration 248/1000 | Loss: 0.00005781
Iteration 249/1000 | Loss: 0.00004597
Iteration 250/1000 | Loss: 0.00005158
Iteration 251/1000 | Loss: 0.00019246
Iteration 252/1000 | Loss: 0.00014733
Iteration 253/1000 | Loss: 0.00004530
Iteration 254/1000 | Loss: 0.00021688
Iteration 255/1000 | Loss: 0.00014028
Iteration 256/1000 | Loss: 0.00022198
Iteration 257/1000 | Loss: 0.00017783
Iteration 258/1000 | Loss: 0.00016713
Iteration 259/1000 | Loss: 0.00005264
Iteration 260/1000 | Loss: 0.00004927
Iteration 261/1000 | Loss: 0.00004906
Iteration 262/1000 | Loss: 0.00007436
Iteration 263/1000 | Loss: 0.00025562
Iteration 264/1000 | Loss: 0.00018884
Iteration 265/1000 | Loss: 0.00009064
Iteration 266/1000 | Loss: 0.00025810
Iteration 267/1000 | Loss: 0.00021182
Iteration 268/1000 | Loss: 0.00026026
Iteration 269/1000 | Loss: 0.00008404
Iteration 270/1000 | Loss: 0.00006061
Iteration 271/1000 | Loss: 0.00007329
Iteration 272/1000 | Loss: 0.00006353
Iteration 273/1000 | Loss: 0.00005934
Iteration 274/1000 | Loss: 0.00007512
Iteration 275/1000 | Loss: 0.00004654
Iteration 276/1000 | Loss: 0.00004275
Iteration 277/1000 | Loss: 0.00005182
Iteration 278/1000 | Loss: 0.00009596
Iteration 279/1000 | Loss: 0.00007115
Iteration 280/1000 | Loss: 0.00004931
Iteration 281/1000 | Loss: 0.00004308
Iteration 282/1000 | Loss: 0.00004178
Iteration 283/1000 | Loss: 0.00007707
Iteration 284/1000 | Loss: 0.00005366
Iteration 285/1000 | Loss: 0.00004143
Iteration 286/1000 | Loss: 0.00004259
Iteration 287/1000 | Loss: 0.00004136
Iteration 288/1000 | Loss: 0.00004136
Iteration 289/1000 | Loss: 0.00004136
Iteration 290/1000 | Loss: 0.00004136
Iteration 291/1000 | Loss: 0.00004136
Iteration 292/1000 | Loss: 0.00004136
Iteration 293/1000 | Loss: 0.00004136
Iteration 294/1000 | Loss: 0.00004136
Iteration 295/1000 | Loss: 0.00004135
Iteration 296/1000 | Loss: 0.00004135
Iteration 297/1000 | Loss: 0.00004135
Iteration 298/1000 | Loss: 0.00008244
Iteration 299/1000 | Loss: 0.00008859
Iteration 300/1000 | Loss: 0.00044852
Iteration 301/1000 | Loss: 0.00009063
Iteration 302/1000 | Loss: 0.00004187
Iteration 303/1000 | Loss: 0.00007991
Iteration 304/1000 | Loss: 0.00004125
Iteration 305/1000 | Loss: 0.00004315
Iteration 306/1000 | Loss: 0.00004113
Iteration 307/1000 | Loss: 0.00004111
Iteration 308/1000 | Loss: 0.00004111
Iteration 309/1000 | Loss: 0.00004111
Iteration 310/1000 | Loss: 0.00004111
Iteration 311/1000 | Loss: 0.00004110
Iteration 312/1000 | Loss: 0.00004105
Iteration 313/1000 | Loss: 0.00004103
Iteration 314/1000 | Loss: 0.00004101
Iteration 315/1000 | Loss: 0.00005432
Iteration 316/1000 | Loss: 0.00004099
Iteration 317/1000 | Loss: 0.00006136
Iteration 318/1000 | Loss: 0.00095756
Iteration 319/1000 | Loss: 0.00019304
Iteration 320/1000 | Loss: 0.00006912
Iteration 321/1000 | Loss: 0.00007551
Iteration 322/1000 | Loss: 0.00008470
Iteration 323/1000 | Loss: 0.00004140
Iteration 324/1000 | Loss: 0.00017901
Iteration 325/1000 | Loss: 0.00024576
Iteration 326/1000 | Loss: 0.00020850
Iteration 327/1000 | Loss: 0.00015433
Iteration 328/1000 | Loss: 0.00020837
Iteration 329/1000 | Loss: 0.00010733
Iteration 330/1000 | Loss: 0.00017405
Iteration 331/1000 | Loss: 0.00025280
Iteration 332/1000 | Loss: 0.00026941
Iteration 333/1000 | Loss: 0.00039867
Iteration 334/1000 | Loss: 0.00035225
Iteration 335/1000 | Loss: 0.00008887
Iteration 336/1000 | Loss: 0.00004520
Iteration 337/1000 | Loss: 0.00018915
Iteration 338/1000 | Loss: 0.00010096
Iteration 339/1000 | Loss: 0.00010643
Iteration 340/1000 | Loss: 0.00005093
Iteration 341/1000 | Loss: 0.00006243
Iteration 342/1000 | Loss: 0.00004183
Iteration 343/1000 | Loss: 0.00008216
Iteration 344/1000 | Loss: 0.00004445
Iteration 345/1000 | Loss: 0.00007886
Iteration 346/1000 | Loss: 0.00004004
Iteration 347/1000 | Loss: 0.00009863
Iteration 348/1000 | Loss: 0.00005057
Iteration 349/1000 | Loss: 0.00003928
Iteration 350/1000 | Loss: 0.00004917
Iteration 351/1000 | Loss: 0.00008378
Iteration 352/1000 | Loss: 0.00061028
Iteration 353/1000 | Loss: 0.00061305
Iteration 354/1000 | Loss: 0.00033791
Iteration 355/1000 | Loss: 0.00053436
Iteration 356/1000 | Loss: 0.00047048
Iteration 357/1000 | Loss: 0.00009395
Iteration 358/1000 | Loss: 0.00040580
Iteration 359/1000 | Loss: 0.00023198
Iteration 360/1000 | Loss: 0.00021525
Iteration 361/1000 | Loss: 0.00019928
Iteration 362/1000 | Loss: 0.00017735
Iteration 363/1000 | Loss: 0.00021858
Iteration 364/1000 | Loss: 0.00032497
Iteration 365/1000 | Loss: 0.00025227
Iteration 366/1000 | Loss: 0.00024371
Iteration 367/1000 | Loss: 0.00020346
Iteration 368/1000 | Loss: 0.00009688
Iteration 369/1000 | Loss: 0.00017264
Iteration 370/1000 | Loss: 0.00010106
Iteration 371/1000 | Loss: 0.00008301
Iteration 372/1000 | Loss: 0.00007957
Iteration 373/1000 | Loss: 0.00005078
Iteration 374/1000 | Loss: 0.00005392
Iteration 375/1000 | Loss: 0.00004200
Iteration 376/1000 | Loss: 0.00005313
Iteration 377/1000 | Loss: 0.00011449
Iteration 378/1000 | Loss: 0.00004647
Iteration 379/1000 | Loss: 0.00004147
Iteration 380/1000 | Loss: 0.00007100
Iteration 381/1000 | Loss: 0.00003561
Iteration 382/1000 | Loss: 0.00025463
Iteration 383/1000 | Loss: 0.00067466
Iteration 384/1000 | Loss: 0.00047371
Iteration 385/1000 | Loss: 0.00079305
Iteration 386/1000 | Loss: 0.00047166
Iteration 387/1000 | Loss: 0.00013224
Iteration 388/1000 | Loss: 0.00030532
Iteration 389/1000 | Loss: 0.00022383
Iteration 390/1000 | Loss: 0.00006781
Iteration 391/1000 | Loss: 0.00024952
Iteration 392/1000 | Loss: 0.00040906
Iteration 393/1000 | Loss: 0.00021002
Iteration 394/1000 | Loss: 0.00006035
Iteration 395/1000 | Loss: 0.00023566
Iteration 396/1000 | Loss: 0.00004783
Iteration 397/1000 | Loss: 0.00003892
Iteration 398/1000 | Loss: 0.00005009
Iteration 399/1000 | Loss: 0.00003495
Iteration 400/1000 | Loss: 0.00003842
Iteration 401/1000 | Loss: 0.00004232
Iteration 402/1000 | Loss: 0.00003240
Iteration 403/1000 | Loss: 0.00006254
Iteration 404/1000 | Loss: 0.00003064
Iteration 405/1000 | Loss: 0.00004397
Iteration 406/1000 | Loss: 0.00002964
Iteration 407/1000 | Loss: 0.00003153
Iteration 408/1000 | Loss: 0.00003880
Iteration 409/1000 | Loss: 0.00005045
Iteration 410/1000 | Loss: 0.00042495
Iteration 411/1000 | Loss: 0.00030298
Iteration 412/1000 | Loss: 0.00019934
Iteration 413/1000 | Loss: 0.00018176
Iteration 414/1000 | Loss: 0.00011492
Iteration 415/1000 | Loss: 0.00011306
Iteration 416/1000 | Loss: 0.00011321
Iteration 417/1000 | Loss: 0.00006128
Iteration 418/1000 | Loss: 0.00005486
Iteration 419/1000 | Loss: 0.00008943
Iteration 420/1000 | Loss: 0.00018983
Iteration 421/1000 | Loss: 0.00007115
Iteration 422/1000 | Loss: 0.00005538
Iteration 423/1000 | Loss: 0.00007892
Iteration 424/1000 | Loss: 0.00005968
Iteration 425/1000 | Loss: 0.00005666
Iteration 426/1000 | Loss: 0.00004701
Iteration 427/1000 | Loss: 0.00005125
Iteration 428/1000 | Loss: 0.00004243
Iteration 429/1000 | Loss: 0.00006572
Iteration 430/1000 | Loss: 0.00004443
Iteration 431/1000 | Loss: 0.00005201
Iteration 432/1000 | Loss: 0.00023320
Iteration 433/1000 | Loss: 0.00016100
Iteration 434/1000 | Loss: 0.00025491
Iteration 435/1000 | Loss: 0.00018424
Iteration 436/1000 | Loss: 0.00011447
Iteration 437/1000 | Loss: 0.00015323
Iteration 438/1000 | Loss: 0.00005658
Iteration 439/1000 | Loss: 0.00032327
Iteration 440/1000 | Loss: 0.00019223
Iteration 441/1000 | Loss: 0.00013278
Iteration 442/1000 | Loss: 0.00013919
Iteration 443/1000 | Loss: 0.00017447
Iteration 444/1000 | Loss: 0.00005276
Iteration 445/1000 | Loss: 0.00004643
Iteration 446/1000 | Loss: 0.00005415
Iteration 447/1000 | Loss: 0.00016998
Iteration 448/1000 | Loss: 0.00005431
Iteration 449/1000 | Loss: 0.00005388
Iteration 450/1000 | Loss: 0.00003048
Iteration 451/1000 | Loss: 0.00005179
Iteration 452/1000 | Loss: 0.00003472
Iteration 453/1000 | Loss: 0.00003534
Iteration 454/1000 | Loss: 0.00004477
Iteration 455/1000 | Loss: 0.00003593
Iteration 456/1000 | Loss: 0.00004176
Iteration 457/1000 | Loss: 0.00007685
Iteration 458/1000 | Loss: 0.00009593
Iteration 459/1000 | Loss: 0.00004865
Iteration 460/1000 | Loss: 0.00003510
Iteration 461/1000 | Loss: 0.00003326
Iteration 462/1000 | Loss: 0.00003858
Iteration 463/1000 | Loss: 0.00004564
Iteration 464/1000 | Loss: 0.00003230
Iteration 465/1000 | Loss: 0.00004002
Iteration 466/1000 | Loss: 0.00003645
Iteration 467/1000 | Loss: 0.00004161
Iteration 468/1000 | Loss: 0.00004040
Iteration 469/1000 | Loss: 0.00004483
Iteration 470/1000 | Loss: 0.00003927
Iteration 471/1000 | Loss: 0.00004015
Iteration 472/1000 | Loss: 0.00008152
Iteration 473/1000 | Loss: 0.00006985
Iteration 474/1000 | Loss: 0.00003846
Iteration 475/1000 | Loss: 0.00003210
Iteration 476/1000 | Loss: 0.00005408
Iteration 477/1000 | Loss: 0.00003662
Iteration 478/1000 | Loss: 0.00025372
Iteration 479/1000 | Loss: 0.00017350
Iteration 480/1000 | Loss: 0.00004522
Iteration 481/1000 | Loss: 0.00003872
Iteration 482/1000 | Loss: 0.00003879
Iteration 483/1000 | Loss: 0.00025949
Iteration 484/1000 | Loss: 0.00015220
Iteration 485/1000 | Loss: 0.00003677
Iteration 486/1000 | Loss: 0.00011151
Iteration 487/1000 | Loss: 0.00023591
Iteration 488/1000 | Loss: 0.00019518
Iteration 489/1000 | Loss: 0.00002881
Iteration 490/1000 | Loss: 0.00003626
Iteration 491/1000 | Loss: 0.00025026
Iteration 492/1000 | Loss: 0.00026632
Iteration 493/1000 | Loss: 0.00005864
Iteration 494/1000 | Loss: 0.00011288
Iteration 495/1000 | Loss: 0.00003483
Iteration 496/1000 | Loss: 0.00003730
Iteration 497/1000 | Loss: 0.00002491
Iteration 498/1000 | Loss: 0.00003337
Iteration 499/1000 | Loss: 0.00002484
Iteration 500/1000 | Loss: 0.00025462
Iteration 501/1000 | Loss: 0.00016453
Iteration 502/1000 | Loss: 0.00027095
Iteration 503/1000 | Loss: 0.00003321
Iteration 504/1000 | Loss: 0.00004344
Iteration 505/1000 | Loss: 0.00027848
Iteration 506/1000 | Loss: 0.00003688
Iteration 507/1000 | Loss: 0.00002827
Iteration 508/1000 | Loss: 0.00004463
Iteration 509/1000 | Loss: 0.00013076
Iteration 510/1000 | Loss: 0.00003426
Iteration 511/1000 | Loss: 0.00005037
Iteration 512/1000 | Loss: 0.00002629
Iteration 513/1000 | Loss: 0.00002587
Iteration 514/1000 | Loss: 0.00002557
Iteration 515/1000 | Loss: 0.00004384
Iteration 516/1000 | Loss: 0.00002649
Iteration 517/1000 | Loss: 0.00002538
Iteration 518/1000 | Loss: 0.00002538
Iteration 519/1000 | Loss: 0.00002538
Iteration 520/1000 | Loss: 0.00002538
Iteration 521/1000 | Loss: 0.00002538
Iteration 522/1000 | Loss: 0.00002537
Iteration 523/1000 | Loss: 0.00002537
Iteration 524/1000 | Loss: 0.00002537
Iteration 525/1000 | Loss: 0.00002537
Iteration 526/1000 | Loss: 0.00002537
Iteration 527/1000 | Loss: 0.00002535
Iteration 528/1000 | Loss: 0.00002534
Iteration 529/1000 | Loss: 0.00002533
Iteration 530/1000 | Loss: 0.00002532
Iteration 531/1000 | Loss: 0.00003342
Iteration 532/1000 | Loss: 0.00005429
Iteration 533/1000 | Loss: 0.00003196
Iteration 534/1000 | Loss: 0.00003953
Iteration 535/1000 | Loss: 0.00002829
Iteration 536/1000 | Loss: 0.00002529
Iteration 537/1000 | Loss: 0.00002528
Iteration 538/1000 | Loss: 0.00002528
Iteration 539/1000 | Loss: 0.00002528
Iteration 540/1000 | Loss: 0.00002528
Iteration 541/1000 | Loss: 0.00002528
Iteration 542/1000 | Loss: 0.00002528
Iteration 543/1000 | Loss: 0.00002528
Iteration 544/1000 | Loss: 0.00002528
Iteration 545/1000 | Loss: 0.00002528
Iteration 546/1000 | Loss: 0.00002528
Iteration 547/1000 | Loss: 0.00002527
Iteration 548/1000 | Loss: 0.00002527
Iteration 549/1000 | Loss: 0.00002527
Iteration 550/1000 | Loss: 0.00002527
Iteration 551/1000 | Loss: 0.00002527
Iteration 552/1000 | Loss: 0.00002526
Iteration 553/1000 | Loss: 0.00002526
Iteration 554/1000 | Loss: 0.00002526
Iteration 555/1000 | Loss: 0.00002526
Iteration 556/1000 | Loss: 0.00002525
Iteration 557/1000 | Loss: 0.00002525
Iteration 558/1000 | Loss: 0.00002524
Iteration 559/1000 | Loss: 0.00002769
Iteration 560/1000 | Loss: 0.00003147
Iteration 561/1000 | Loss: 0.00002514
Iteration 562/1000 | Loss: 0.00005073
Iteration 563/1000 | Loss: 0.00003579
Iteration 564/1000 | Loss: 0.00003578
Iteration 565/1000 | Loss: 0.00010661
Iteration 566/1000 | Loss: 0.00002527
Iteration 567/1000 | Loss: 0.00003632
Iteration 568/1000 | Loss: 0.00003392
Iteration 569/1000 | Loss: 0.00002492
Iteration 570/1000 | Loss: 0.00002487
Iteration 571/1000 | Loss: 0.00002487
Iteration 572/1000 | Loss: 0.00002487
Iteration 573/1000 | Loss: 0.00002487
Iteration 574/1000 | Loss: 0.00002487
Iteration 575/1000 | Loss: 0.00002487
Iteration 576/1000 | Loss: 0.00002487
Iteration 577/1000 | Loss: 0.00002487
Iteration 578/1000 | Loss: 0.00002486
Iteration 579/1000 | Loss: 0.00002547
Iteration 580/1000 | Loss: 0.00002486
Iteration 581/1000 | Loss: 0.00002486
Iteration 582/1000 | Loss: 0.00002486
Iteration 583/1000 | Loss: 0.00002486
Iteration 584/1000 | Loss: 0.00002486
Iteration 585/1000 | Loss: 0.00002486
Iteration 586/1000 | Loss: 0.00002486
Iteration 587/1000 | Loss: 0.00002485
Iteration 588/1000 | Loss: 0.00002485
Iteration 589/1000 | Loss: 0.00002485
Iteration 590/1000 | Loss: 0.00002485
Iteration 591/1000 | Loss: 0.00002485
Iteration 592/1000 | Loss: 0.00002485
Iteration 593/1000 | Loss: 0.00002485
Iteration 594/1000 | Loss: 0.00002484
Iteration 595/1000 | Loss: 0.00002497
Iteration 596/1000 | Loss: 0.00002485
Iteration 597/1000 | Loss: 0.00002485
Iteration 598/1000 | Loss: 0.00002485
Iteration 599/1000 | Loss: 0.00002485
Iteration 600/1000 | Loss: 0.00002485
Iteration 601/1000 | Loss: 0.00002485
Iteration 602/1000 | Loss: 0.00002485
Iteration 603/1000 | Loss: 0.00002484
Iteration 604/1000 | Loss: 0.00002484
Iteration 605/1000 | Loss: 0.00002484
Iteration 606/1000 | Loss: 0.00002484
Iteration 607/1000 | Loss: 0.00002484
Iteration 608/1000 | Loss: 0.00002484
Iteration 609/1000 | Loss: 0.00002484
Iteration 610/1000 | Loss: 0.00002484
Iteration 611/1000 | Loss: 0.00002484
Iteration 612/1000 | Loss: 0.00002484
Iteration 613/1000 | Loss: 0.00002484
Iteration 614/1000 | Loss: 0.00002484
Iteration 615/1000 | Loss: 0.00002484
Iteration 616/1000 | Loss: 0.00002484
Iteration 617/1000 | Loss: 0.00002484
Iteration 618/1000 | Loss: 0.00002484
Iteration 619/1000 | Loss: 0.00002484
Iteration 620/1000 | Loss: 0.00002484
Iteration 621/1000 | Loss: 0.00002484
Iteration 622/1000 | Loss: 0.00002484
Iteration 623/1000 | Loss: 0.00002484
Iteration 624/1000 | Loss: 0.00002484
Iteration 625/1000 | Loss: 0.00002484
Iteration 626/1000 | Loss: 0.00002484
Iteration 627/1000 | Loss: 0.00002484
Iteration 628/1000 | Loss: 0.00002484
Iteration 629/1000 | Loss: 0.00002484
Iteration 630/1000 | Loss: 0.00002484
Iteration 631/1000 | Loss: 0.00002484
Iteration 632/1000 | Loss: 0.00002484
Iteration 633/1000 | Loss: 0.00002484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 633. Stopping optimization.
Last 5 losses: [2.4835209842422046e-05, 2.4835209842422046e-05, 2.4835209842422046e-05, 2.4835209842422046e-05, 2.4835209842422046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4835209842422046e-05

Optimization complete. Final v2v error: 3.5085463523864746 mm

Highest mean error: 11.234112739562988 mm for frame 127

Lowest mean error: 3.0621697902679443 mm for frame 43

Saving results

Total time: 867.1143264770508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00377172
Iteration 2/25 | Loss: 0.00125890
Iteration 3/25 | Loss: 0.00118762
Iteration 4/25 | Loss: 0.00117928
Iteration 5/25 | Loss: 0.00117714
Iteration 6/25 | Loss: 0.00117712
Iteration 7/25 | Loss: 0.00117712
Iteration 8/25 | Loss: 0.00117712
Iteration 9/25 | Loss: 0.00117712
Iteration 10/25 | Loss: 0.00117712
Iteration 11/25 | Loss: 0.00117712
Iteration 12/25 | Loss: 0.00117712
Iteration 13/25 | Loss: 0.00117712
Iteration 14/25 | Loss: 0.00117712
Iteration 15/25 | Loss: 0.00117712
Iteration 16/25 | Loss: 0.00117712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011771153658628464, 0.0011771153658628464, 0.0011771153658628464, 0.0011771153658628464, 0.0011771153658628464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771153658628464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28983951
Iteration 2/25 | Loss: 0.00160287
Iteration 3/25 | Loss: 0.00160287
Iteration 4/25 | Loss: 0.00160287
Iteration 5/25 | Loss: 0.00160287
Iteration 6/25 | Loss: 0.00160287
Iteration 7/25 | Loss: 0.00160287
Iteration 8/25 | Loss: 0.00160287
Iteration 9/25 | Loss: 0.00160287
Iteration 10/25 | Loss: 0.00160287
Iteration 11/25 | Loss: 0.00160287
Iteration 12/25 | Loss: 0.00160287
Iteration 13/25 | Loss: 0.00160287
Iteration 14/25 | Loss: 0.00160287
Iteration 15/25 | Loss: 0.00160287
Iteration 16/25 | Loss: 0.00160287
Iteration 17/25 | Loss: 0.00160287
Iteration 18/25 | Loss: 0.00160287
Iteration 19/25 | Loss: 0.00160287
Iteration 20/25 | Loss: 0.00160287
Iteration 21/25 | Loss: 0.00160287
Iteration 22/25 | Loss: 0.00160287
Iteration 23/25 | Loss: 0.00160287
Iteration 24/25 | Loss: 0.00160287
Iteration 25/25 | Loss: 0.00160287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160287
Iteration 2/1000 | Loss: 0.00001879
Iteration 3/1000 | Loss: 0.00001270
Iteration 4/1000 | Loss: 0.00001104
Iteration 5/1000 | Loss: 0.00001023
Iteration 6/1000 | Loss: 0.00000967
Iteration 7/1000 | Loss: 0.00000945
Iteration 8/1000 | Loss: 0.00000919
Iteration 9/1000 | Loss: 0.00000917
Iteration 10/1000 | Loss: 0.00000902
Iteration 11/1000 | Loss: 0.00000878
Iteration 12/1000 | Loss: 0.00000870
Iteration 13/1000 | Loss: 0.00000868
Iteration 14/1000 | Loss: 0.00000867
Iteration 15/1000 | Loss: 0.00000866
Iteration 16/1000 | Loss: 0.00000857
Iteration 17/1000 | Loss: 0.00000856
Iteration 18/1000 | Loss: 0.00000854
Iteration 19/1000 | Loss: 0.00000850
Iteration 20/1000 | Loss: 0.00000849
Iteration 21/1000 | Loss: 0.00000849
Iteration 22/1000 | Loss: 0.00000847
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000846
Iteration 25/1000 | Loss: 0.00000836
Iteration 26/1000 | Loss: 0.00000835
Iteration 27/1000 | Loss: 0.00000834
Iteration 28/1000 | Loss: 0.00000828
Iteration 29/1000 | Loss: 0.00000828
Iteration 30/1000 | Loss: 0.00000828
Iteration 31/1000 | Loss: 0.00000828
Iteration 32/1000 | Loss: 0.00000827
Iteration 33/1000 | Loss: 0.00000826
Iteration 34/1000 | Loss: 0.00000826
Iteration 35/1000 | Loss: 0.00000826
Iteration 36/1000 | Loss: 0.00000823
Iteration 37/1000 | Loss: 0.00000821
Iteration 38/1000 | Loss: 0.00000820
Iteration 39/1000 | Loss: 0.00000818
Iteration 40/1000 | Loss: 0.00000818
Iteration 41/1000 | Loss: 0.00000817
Iteration 42/1000 | Loss: 0.00000817
Iteration 43/1000 | Loss: 0.00000815
Iteration 44/1000 | Loss: 0.00000814
Iteration 45/1000 | Loss: 0.00000814
Iteration 46/1000 | Loss: 0.00000814
Iteration 47/1000 | Loss: 0.00000814
Iteration 48/1000 | Loss: 0.00000814
Iteration 49/1000 | Loss: 0.00000814
Iteration 50/1000 | Loss: 0.00000814
Iteration 51/1000 | Loss: 0.00000814
Iteration 52/1000 | Loss: 0.00000814
Iteration 53/1000 | Loss: 0.00000814
Iteration 54/1000 | Loss: 0.00000813
Iteration 55/1000 | Loss: 0.00000813
Iteration 56/1000 | Loss: 0.00000811
Iteration 57/1000 | Loss: 0.00000811
Iteration 58/1000 | Loss: 0.00000810
Iteration 59/1000 | Loss: 0.00000810
Iteration 60/1000 | Loss: 0.00000809
Iteration 61/1000 | Loss: 0.00000808
Iteration 62/1000 | Loss: 0.00000808
Iteration 63/1000 | Loss: 0.00000808
Iteration 64/1000 | Loss: 0.00000808
Iteration 65/1000 | Loss: 0.00000807
Iteration 66/1000 | Loss: 0.00000807
Iteration 67/1000 | Loss: 0.00000806
Iteration 68/1000 | Loss: 0.00000806
Iteration 69/1000 | Loss: 0.00000806
Iteration 70/1000 | Loss: 0.00000806
Iteration 71/1000 | Loss: 0.00000806
Iteration 72/1000 | Loss: 0.00000805
Iteration 73/1000 | Loss: 0.00000805
Iteration 74/1000 | Loss: 0.00000805
Iteration 75/1000 | Loss: 0.00000805
Iteration 76/1000 | Loss: 0.00000804
Iteration 77/1000 | Loss: 0.00000803
Iteration 78/1000 | Loss: 0.00000803
Iteration 79/1000 | Loss: 0.00000802
Iteration 80/1000 | Loss: 0.00000802
Iteration 81/1000 | Loss: 0.00000802
Iteration 82/1000 | Loss: 0.00000802
Iteration 83/1000 | Loss: 0.00000801
Iteration 84/1000 | Loss: 0.00000801
Iteration 85/1000 | Loss: 0.00000801
Iteration 86/1000 | Loss: 0.00000801
Iteration 87/1000 | Loss: 0.00000801
Iteration 88/1000 | Loss: 0.00000801
Iteration 89/1000 | Loss: 0.00000801
Iteration 90/1000 | Loss: 0.00000800
Iteration 91/1000 | Loss: 0.00000800
Iteration 92/1000 | Loss: 0.00000800
Iteration 93/1000 | Loss: 0.00000800
Iteration 94/1000 | Loss: 0.00000799
Iteration 95/1000 | Loss: 0.00000798
Iteration 96/1000 | Loss: 0.00000798
Iteration 97/1000 | Loss: 0.00000798
Iteration 98/1000 | Loss: 0.00000797
Iteration 99/1000 | Loss: 0.00000797
Iteration 100/1000 | Loss: 0.00000797
Iteration 101/1000 | Loss: 0.00000797
Iteration 102/1000 | Loss: 0.00000797
Iteration 103/1000 | Loss: 0.00000797
Iteration 104/1000 | Loss: 0.00000796
Iteration 105/1000 | Loss: 0.00000796
Iteration 106/1000 | Loss: 0.00000796
Iteration 107/1000 | Loss: 0.00000796
Iteration 108/1000 | Loss: 0.00000796
Iteration 109/1000 | Loss: 0.00000795
Iteration 110/1000 | Loss: 0.00000795
Iteration 111/1000 | Loss: 0.00000795
Iteration 112/1000 | Loss: 0.00000795
Iteration 113/1000 | Loss: 0.00000795
Iteration 114/1000 | Loss: 0.00000795
Iteration 115/1000 | Loss: 0.00000794
Iteration 116/1000 | Loss: 0.00000794
Iteration 117/1000 | Loss: 0.00000794
Iteration 118/1000 | Loss: 0.00000794
Iteration 119/1000 | Loss: 0.00000794
Iteration 120/1000 | Loss: 0.00000794
Iteration 121/1000 | Loss: 0.00000794
Iteration 122/1000 | Loss: 0.00000794
Iteration 123/1000 | Loss: 0.00000794
Iteration 124/1000 | Loss: 0.00000794
Iteration 125/1000 | Loss: 0.00000794
Iteration 126/1000 | Loss: 0.00000794
Iteration 127/1000 | Loss: 0.00000794
Iteration 128/1000 | Loss: 0.00000794
Iteration 129/1000 | Loss: 0.00000794
Iteration 130/1000 | Loss: 0.00000794
Iteration 131/1000 | Loss: 0.00000794
Iteration 132/1000 | Loss: 0.00000794
Iteration 133/1000 | Loss: 0.00000794
Iteration 134/1000 | Loss: 0.00000794
Iteration 135/1000 | Loss: 0.00000794
Iteration 136/1000 | Loss: 0.00000794
Iteration 137/1000 | Loss: 0.00000794
Iteration 138/1000 | Loss: 0.00000794
Iteration 139/1000 | Loss: 0.00000794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [7.93782965047285e-06, 7.93782965047285e-06, 7.93782965047285e-06, 7.93782965047285e-06, 7.93782965047285e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.93782965047285e-06

Optimization complete. Final v2v error: 2.4468650817871094 mm

Highest mean error: 2.566216468811035 mm for frame 76

Lowest mean error: 2.3930726051330566 mm for frame 137

Saving results

Total time: 35.998246908187866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954281
Iteration 2/25 | Loss: 0.00219866
Iteration 3/25 | Loss: 0.00150899
Iteration 4/25 | Loss: 0.00146667
Iteration 5/25 | Loss: 0.00145483
Iteration 6/25 | Loss: 0.00132403
Iteration 7/25 | Loss: 0.00130389
Iteration 8/25 | Loss: 0.00128728
Iteration 9/25 | Loss: 0.00128601
Iteration 10/25 | Loss: 0.00128357
Iteration 11/25 | Loss: 0.00128058
Iteration 12/25 | Loss: 0.00128112
Iteration 13/25 | Loss: 0.00127683
Iteration 14/25 | Loss: 0.00127265
Iteration 15/25 | Loss: 0.00127115
Iteration 16/25 | Loss: 0.00127412
Iteration 17/25 | Loss: 0.00127018
Iteration 18/25 | Loss: 0.00126925
Iteration 19/25 | Loss: 0.00126908
Iteration 20/25 | Loss: 0.00126905
Iteration 21/25 | Loss: 0.00126904
Iteration 22/25 | Loss: 0.00126904
Iteration 23/25 | Loss: 0.00126904
Iteration 24/25 | Loss: 0.00126904
Iteration 25/25 | Loss: 0.00126903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28035116
Iteration 2/25 | Loss: 0.00108287
Iteration 3/25 | Loss: 0.00108287
Iteration 4/25 | Loss: 0.00108287
Iteration 5/25 | Loss: 0.00108287
Iteration 6/25 | Loss: 0.00108287
Iteration 7/25 | Loss: 0.00108287
Iteration 8/25 | Loss: 0.00108287
Iteration 9/25 | Loss: 0.00108287
Iteration 10/25 | Loss: 0.00108287
Iteration 11/25 | Loss: 0.00108287
Iteration 12/25 | Loss: 0.00108287
Iteration 13/25 | Loss: 0.00108287
Iteration 14/25 | Loss: 0.00108287
Iteration 15/25 | Loss: 0.00108287
Iteration 16/25 | Loss: 0.00108287
Iteration 17/25 | Loss: 0.00108287
Iteration 18/25 | Loss: 0.00108287
Iteration 19/25 | Loss: 0.00108287
Iteration 20/25 | Loss: 0.00108287
Iteration 21/25 | Loss: 0.00108287
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010828656377270818, 0.0010828656377270818, 0.0010828656377270818, 0.0010828656377270818, 0.0010828656377270818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010828656377270818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108287
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00008477
Iteration 4/1000 | Loss: 0.00002961
Iteration 5/1000 | Loss: 0.00001845
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001695
Iteration 8/1000 | Loss: 0.00001658
Iteration 9/1000 | Loss: 0.00002304
Iteration 10/1000 | Loss: 0.00001699
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001554
Iteration 14/1000 | Loss: 0.00010166
Iteration 15/1000 | Loss: 0.00010166
Iteration 16/1000 | Loss: 0.00010165
Iteration 17/1000 | Loss: 0.00045101
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00010055
Iteration 24/1000 | Loss: 0.00062998
Iteration 25/1000 | Loss: 0.00010276
Iteration 26/1000 | Loss: 0.00003537
Iteration 27/1000 | Loss: 0.00001513
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001482
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001481
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00006582
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00003598
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001588
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001470
Iteration 53/1000 | Loss: 0.00001470
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001451
Iteration 72/1000 | Loss: 0.00001451
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001450
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001442
Iteration 111/1000 | Loss: 0.00001442
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001442
Iteration 114/1000 | Loss: 0.00001442
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001442
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001441
Iteration 137/1000 | Loss: 0.00001441
Iteration 138/1000 | Loss: 0.00001441
Iteration 139/1000 | Loss: 0.00001441
Iteration 140/1000 | Loss: 0.00001441
Iteration 141/1000 | Loss: 0.00001441
Iteration 142/1000 | Loss: 0.00001441
Iteration 143/1000 | Loss: 0.00001441
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001440
Iteration 153/1000 | Loss: 0.00001440
Iteration 154/1000 | Loss: 0.00001440
Iteration 155/1000 | Loss: 0.00001440
Iteration 156/1000 | Loss: 0.00009595
Iteration 157/1000 | Loss: 0.00009595
Iteration 158/1000 | Loss: 0.00001989
Iteration 159/1000 | Loss: 0.00001652
Iteration 160/1000 | Loss: 0.00001479
Iteration 161/1000 | Loss: 0.00001447
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001442
Iteration 164/1000 | Loss: 0.00001442
Iteration 165/1000 | Loss: 0.00001441
Iteration 166/1000 | Loss: 0.00001441
Iteration 167/1000 | Loss: 0.00001441
Iteration 168/1000 | Loss: 0.00001441
Iteration 169/1000 | Loss: 0.00001441
Iteration 170/1000 | Loss: 0.00001441
Iteration 171/1000 | Loss: 0.00001441
Iteration 172/1000 | Loss: 0.00001441
Iteration 173/1000 | Loss: 0.00001441
Iteration 174/1000 | Loss: 0.00001441
Iteration 175/1000 | Loss: 0.00001441
Iteration 176/1000 | Loss: 0.00001441
Iteration 177/1000 | Loss: 0.00001440
Iteration 178/1000 | Loss: 0.00001440
Iteration 179/1000 | Loss: 0.00001440
Iteration 180/1000 | Loss: 0.00001440
Iteration 181/1000 | Loss: 0.00001440
Iteration 182/1000 | Loss: 0.00001440
Iteration 183/1000 | Loss: 0.00001440
Iteration 184/1000 | Loss: 0.00001440
Iteration 185/1000 | Loss: 0.00001440
Iteration 186/1000 | Loss: 0.00001440
Iteration 187/1000 | Loss: 0.00001440
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00001439
Iteration 190/1000 | Loss: 0.00001439
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001439
Iteration 197/1000 | Loss: 0.00001439
Iteration 198/1000 | Loss: 0.00001439
Iteration 199/1000 | Loss: 0.00001439
Iteration 200/1000 | Loss: 0.00001439
Iteration 201/1000 | Loss: 0.00001439
Iteration 202/1000 | Loss: 0.00001439
Iteration 203/1000 | Loss: 0.00001439
Iteration 204/1000 | Loss: 0.00001439
Iteration 205/1000 | Loss: 0.00001439
Iteration 206/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.438763410988031e-05, 1.438763410988031e-05, 1.438763410988031e-05, 1.438763410988031e-05, 1.438763410988031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438763410988031e-05

Optimization complete. Final v2v error: 3.19252610206604 mm

Highest mean error: 4.503927230834961 mm for frame 22

Lowest mean error: 2.687084913253784 mm for frame 146

Saving results

Total time: 109.8493640422821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805251
Iteration 2/25 | Loss: 0.00139545
Iteration 3/25 | Loss: 0.00124197
Iteration 4/25 | Loss: 0.00120499
Iteration 5/25 | Loss: 0.00121620
Iteration 6/25 | Loss: 0.00119858
Iteration 7/25 | Loss: 0.00119805
Iteration 8/25 | Loss: 0.00119783
Iteration 9/25 | Loss: 0.00119769
Iteration 10/25 | Loss: 0.00119767
Iteration 11/25 | Loss: 0.00119765
Iteration 12/25 | Loss: 0.00119764
Iteration 13/25 | Loss: 0.00119764
Iteration 14/25 | Loss: 0.00119764
Iteration 15/25 | Loss: 0.00119764
Iteration 16/25 | Loss: 0.00119764
Iteration 17/25 | Loss: 0.00119764
Iteration 18/25 | Loss: 0.00119764
Iteration 19/25 | Loss: 0.00119763
Iteration 20/25 | Loss: 0.00119763
Iteration 21/25 | Loss: 0.00119763
Iteration 22/25 | Loss: 0.00119763
Iteration 23/25 | Loss: 0.00119763
Iteration 24/25 | Loss: 0.00119763
Iteration 25/25 | Loss: 0.00119763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40649366
Iteration 2/25 | Loss: 0.00150266
Iteration 3/25 | Loss: 0.00150266
Iteration 4/25 | Loss: 0.00150266
Iteration 5/25 | Loss: 0.00150266
Iteration 6/25 | Loss: 0.00150266
Iteration 7/25 | Loss: 0.00150266
Iteration 8/25 | Loss: 0.00150266
Iteration 9/25 | Loss: 0.00150266
Iteration 10/25 | Loss: 0.00150266
Iteration 11/25 | Loss: 0.00150266
Iteration 12/25 | Loss: 0.00150266
Iteration 13/25 | Loss: 0.00150266
Iteration 14/25 | Loss: 0.00150266
Iteration 15/25 | Loss: 0.00150266
Iteration 16/25 | Loss: 0.00150266
Iteration 17/25 | Loss: 0.00150266
Iteration 18/25 | Loss: 0.00150266
Iteration 19/25 | Loss: 0.00150266
Iteration 20/25 | Loss: 0.00150266
Iteration 21/25 | Loss: 0.00150266
Iteration 22/25 | Loss: 0.00150266
Iteration 23/25 | Loss: 0.00150266
Iteration 24/25 | Loss: 0.00150266
Iteration 25/25 | Loss: 0.00150266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150266
Iteration 2/1000 | Loss: 0.00002455
Iteration 3/1000 | Loss: 0.00001595
Iteration 4/1000 | Loss: 0.00001444
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001323
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001223
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001130
Iteration 18/1000 | Loss: 0.00001123
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001117
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001109
Iteration 24/1000 | Loss: 0.00001109
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001109
Iteration 27/1000 | Loss: 0.00001109
Iteration 28/1000 | Loss: 0.00001106
Iteration 29/1000 | Loss: 0.00001104
Iteration 30/1000 | Loss: 0.00001104
Iteration 31/1000 | Loss: 0.00001103
Iteration 32/1000 | Loss: 0.00001103
Iteration 33/1000 | Loss: 0.00001102
Iteration 34/1000 | Loss: 0.00001102
Iteration 35/1000 | Loss: 0.00001102
Iteration 36/1000 | Loss: 0.00001101
Iteration 37/1000 | Loss: 0.00001100
Iteration 38/1000 | Loss: 0.00001098
Iteration 39/1000 | Loss: 0.00001097
Iteration 40/1000 | Loss: 0.00001096
Iteration 41/1000 | Loss: 0.00001096
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001095
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001093
Iteration 50/1000 | Loss: 0.00001093
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001093
Iteration 59/1000 | Loss: 0.00001093
Iteration 60/1000 | Loss: 0.00001093
Iteration 61/1000 | Loss: 0.00001093
Iteration 62/1000 | Loss: 0.00001093
Iteration 63/1000 | Loss: 0.00001093
Iteration 64/1000 | Loss: 0.00001093
Iteration 65/1000 | Loss: 0.00001093
Iteration 66/1000 | Loss: 0.00001093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.0926627510343678e-05, 1.0926627510343678e-05, 1.0926627510343678e-05, 1.0926627510343678e-05, 1.0926627510343678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0926627510343678e-05

Optimization complete. Final v2v error: 2.869016647338867 mm

Highest mean error: 3.178088426589966 mm for frame 49

Lowest mean error: 2.624318838119507 mm for frame 157

Saving results

Total time: 42.88992643356323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775168
Iteration 2/25 | Loss: 0.00142090
Iteration 3/25 | Loss: 0.00123900
Iteration 4/25 | Loss: 0.00121431
Iteration 5/25 | Loss: 0.00120737
Iteration 6/25 | Loss: 0.00120565
Iteration 7/25 | Loss: 0.00120543
Iteration 8/25 | Loss: 0.00120543
Iteration 9/25 | Loss: 0.00120543
Iteration 10/25 | Loss: 0.00120543
Iteration 11/25 | Loss: 0.00120543
Iteration 12/25 | Loss: 0.00120543
Iteration 13/25 | Loss: 0.00120543
Iteration 14/25 | Loss: 0.00120543
Iteration 15/25 | Loss: 0.00120543
Iteration 16/25 | Loss: 0.00120543
Iteration 17/25 | Loss: 0.00120543
Iteration 18/25 | Loss: 0.00120543
Iteration 19/25 | Loss: 0.00120543
Iteration 20/25 | Loss: 0.00120543
Iteration 21/25 | Loss: 0.00120543
Iteration 22/25 | Loss: 0.00120543
Iteration 23/25 | Loss: 0.00120543
Iteration 24/25 | Loss: 0.00120543
Iteration 25/25 | Loss: 0.00120543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23358583
Iteration 2/25 | Loss: 0.00173794
Iteration 3/25 | Loss: 0.00173794
Iteration 4/25 | Loss: 0.00173794
Iteration 5/25 | Loss: 0.00173794
Iteration 6/25 | Loss: 0.00173794
Iteration 7/25 | Loss: 0.00173794
Iteration 8/25 | Loss: 0.00173794
Iteration 9/25 | Loss: 0.00173794
Iteration 10/25 | Loss: 0.00173794
Iteration 11/25 | Loss: 0.00173794
Iteration 12/25 | Loss: 0.00173794
Iteration 13/25 | Loss: 0.00173794
Iteration 14/25 | Loss: 0.00173794
Iteration 15/25 | Loss: 0.00173794
Iteration 16/25 | Loss: 0.00173794
Iteration 17/25 | Loss: 0.00173794
Iteration 18/25 | Loss: 0.00173794
Iteration 19/25 | Loss: 0.00173794
Iteration 20/25 | Loss: 0.00173794
Iteration 21/25 | Loss: 0.00173794
Iteration 22/25 | Loss: 0.00173794
Iteration 23/25 | Loss: 0.00173794
Iteration 24/25 | Loss: 0.00173794
Iteration 25/25 | Loss: 0.00173794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173794
Iteration 2/1000 | Loss: 0.00005622
Iteration 3/1000 | Loss: 0.00003767
Iteration 4/1000 | Loss: 0.00003020
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002358
Iteration 8/1000 | Loss: 0.00002294
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002212
Iteration 11/1000 | Loss: 0.00002186
Iteration 12/1000 | Loss: 0.00002167
Iteration 13/1000 | Loss: 0.00002156
Iteration 14/1000 | Loss: 0.00002146
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002137
Iteration 17/1000 | Loss: 0.00002134
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002130
Iteration 23/1000 | Loss: 0.00002129
Iteration 24/1000 | Loss: 0.00002122
Iteration 25/1000 | Loss: 0.00002122
Iteration 26/1000 | Loss: 0.00002121
Iteration 27/1000 | Loss: 0.00002121
Iteration 28/1000 | Loss: 0.00002120
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002114
Iteration 32/1000 | Loss: 0.00002113
Iteration 33/1000 | Loss: 0.00002112
Iteration 34/1000 | Loss: 0.00002111
Iteration 35/1000 | Loss: 0.00002110
Iteration 36/1000 | Loss: 0.00002107
Iteration 37/1000 | Loss: 0.00002106
Iteration 38/1000 | Loss: 0.00002106
Iteration 39/1000 | Loss: 0.00002105
Iteration 40/1000 | Loss: 0.00002101
Iteration 41/1000 | Loss: 0.00002100
Iteration 42/1000 | Loss: 0.00002100
Iteration 43/1000 | Loss: 0.00002099
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002098
Iteration 46/1000 | Loss: 0.00002098
Iteration 47/1000 | Loss: 0.00002096
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002092
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002090
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002090
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002090
Iteration 64/1000 | Loss: 0.00002090
Iteration 65/1000 | Loss: 0.00002090
Iteration 66/1000 | Loss: 0.00002090
Iteration 67/1000 | Loss: 0.00002090
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00002089
Iteration 73/1000 | Loss: 0.00002089
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00002089
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002088
Iteration 82/1000 | Loss: 0.00002088
Iteration 83/1000 | Loss: 0.00002088
Iteration 84/1000 | Loss: 0.00002088
Iteration 85/1000 | Loss: 0.00002088
Iteration 86/1000 | Loss: 0.00002088
Iteration 87/1000 | Loss: 0.00002088
Iteration 88/1000 | Loss: 0.00002088
Iteration 89/1000 | Loss: 0.00002088
Iteration 90/1000 | Loss: 0.00002087
Iteration 91/1000 | Loss: 0.00002087
Iteration 92/1000 | Loss: 0.00002087
Iteration 93/1000 | Loss: 0.00002087
Iteration 94/1000 | Loss: 0.00002087
Iteration 95/1000 | Loss: 0.00002087
Iteration 96/1000 | Loss: 0.00002087
Iteration 97/1000 | Loss: 0.00002087
Iteration 98/1000 | Loss: 0.00002087
Iteration 99/1000 | Loss: 0.00002086
Iteration 100/1000 | Loss: 0.00002086
Iteration 101/1000 | Loss: 0.00002086
Iteration 102/1000 | Loss: 0.00002086
Iteration 103/1000 | Loss: 0.00002085
Iteration 104/1000 | Loss: 0.00002085
Iteration 105/1000 | Loss: 0.00002085
Iteration 106/1000 | Loss: 0.00002084
Iteration 107/1000 | Loss: 0.00002084
Iteration 108/1000 | Loss: 0.00002084
Iteration 109/1000 | Loss: 0.00002083
Iteration 110/1000 | Loss: 0.00002083
Iteration 111/1000 | Loss: 0.00002083
Iteration 112/1000 | Loss: 0.00002082
Iteration 113/1000 | Loss: 0.00002082
Iteration 114/1000 | Loss: 0.00002082
Iteration 115/1000 | Loss: 0.00002082
Iteration 116/1000 | Loss: 0.00002082
Iteration 117/1000 | Loss: 0.00002082
Iteration 118/1000 | Loss: 0.00002082
Iteration 119/1000 | Loss: 0.00002082
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002082
Iteration 124/1000 | Loss: 0.00002082
Iteration 125/1000 | Loss: 0.00002082
Iteration 126/1000 | Loss: 0.00002082
Iteration 127/1000 | Loss: 0.00002082
Iteration 128/1000 | Loss: 0.00002082
Iteration 129/1000 | Loss: 0.00002082
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002082
Iteration 132/1000 | Loss: 0.00002082
Iteration 133/1000 | Loss: 0.00002082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.0815377865801565e-05, 2.0815377865801565e-05, 2.0815377865801565e-05, 2.0815377865801565e-05, 2.0815377865801565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0815377865801565e-05

Optimization complete. Final v2v error: 3.7685930728912354 mm

Highest mean error: 4.341965675354004 mm for frame 124

Lowest mean error: 2.745508909225464 mm for frame 0

Saving results

Total time: 40.18292951583862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499343
Iteration 2/25 | Loss: 0.00145786
Iteration 3/25 | Loss: 0.00128150
Iteration 4/25 | Loss: 0.00124517
Iteration 5/25 | Loss: 0.00123757
Iteration 6/25 | Loss: 0.00123685
Iteration 7/25 | Loss: 0.00123685
Iteration 8/25 | Loss: 0.00123685
Iteration 9/25 | Loss: 0.00123685
Iteration 10/25 | Loss: 0.00123685
Iteration 11/25 | Loss: 0.00123685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012368496973067522, 0.0012368496973067522, 0.0012368496973067522, 0.0012368496973067522, 0.0012368496973067522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012368496973067522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22054005
Iteration 2/25 | Loss: 0.00140150
Iteration 3/25 | Loss: 0.00140149
Iteration 4/25 | Loss: 0.00140149
Iteration 5/25 | Loss: 0.00140149
Iteration 6/25 | Loss: 0.00140149
Iteration 7/25 | Loss: 0.00140149
Iteration 8/25 | Loss: 0.00140149
Iteration 9/25 | Loss: 0.00140149
Iteration 10/25 | Loss: 0.00140148
Iteration 11/25 | Loss: 0.00140148
Iteration 12/25 | Loss: 0.00140148
Iteration 13/25 | Loss: 0.00140148
Iteration 14/25 | Loss: 0.00140148
Iteration 15/25 | Loss: 0.00140148
Iteration 16/25 | Loss: 0.00140148
Iteration 17/25 | Loss: 0.00140148
Iteration 18/25 | Loss: 0.00140148
Iteration 19/25 | Loss: 0.00140148
Iteration 20/25 | Loss: 0.00140148
Iteration 21/25 | Loss: 0.00140148
Iteration 22/25 | Loss: 0.00140148
Iteration 23/25 | Loss: 0.00140148
Iteration 24/25 | Loss: 0.00140148
Iteration 25/25 | Loss: 0.00140148
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001401484594680369, 0.001401484594680369, 0.001401484594680369, 0.001401484594680369, 0.001401484594680369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001401484594680369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140148
Iteration 2/1000 | Loss: 0.00003771
Iteration 3/1000 | Loss: 0.00002425
Iteration 4/1000 | Loss: 0.00002082
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001914
Iteration 7/1000 | Loss: 0.00001827
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001695
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001662
Iteration 14/1000 | Loss: 0.00001662
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001645
Iteration 21/1000 | Loss: 0.00001645
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001634
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001632
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001631
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001626
Iteration 37/1000 | Loss: 0.00001624
Iteration 38/1000 | Loss: 0.00001624
Iteration 39/1000 | Loss: 0.00001624
Iteration 40/1000 | Loss: 0.00001623
Iteration 41/1000 | Loss: 0.00001623
Iteration 42/1000 | Loss: 0.00001623
Iteration 43/1000 | Loss: 0.00001622
Iteration 44/1000 | Loss: 0.00001622
Iteration 45/1000 | Loss: 0.00001622
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001622
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001622
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001621
Iteration 53/1000 | Loss: 0.00001621
Iteration 54/1000 | Loss: 0.00001621
Iteration 55/1000 | Loss: 0.00001621
Iteration 56/1000 | Loss: 0.00001621
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001621
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001618
Iteration 66/1000 | Loss: 0.00001617
Iteration 67/1000 | Loss: 0.00001617
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001616
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001615
Iteration 76/1000 | Loss: 0.00001615
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001615
Iteration 82/1000 | Loss: 0.00001615
Iteration 83/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.6150765077327378e-05, 1.6150765077327378e-05, 1.6150765077327378e-05, 1.6150765077327378e-05, 1.6150765077327378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6150765077327378e-05

Optimization complete. Final v2v error: 3.474053144454956 mm

Highest mean error: 3.7354769706726074 mm for frame 134

Lowest mean error: 3.3470754623413086 mm for frame 72

Saving results

Total time: 33.08197021484375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984706
Iteration 2/25 | Loss: 0.00146490
Iteration 3/25 | Loss: 0.00129683
Iteration 4/25 | Loss: 0.00126737
Iteration 5/25 | Loss: 0.00127112
Iteration 6/25 | Loss: 0.00125741
Iteration 7/25 | Loss: 0.00124972
Iteration 8/25 | Loss: 0.00124530
Iteration 9/25 | Loss: 0.00124135
Iteration 10/25 | Loss: 0.00124027
Iteration 11/25 | Loss: 0.00123924
Iteration 12/25 | Loss: 0.00123856
Iteration 13/25 | Loss: 0.00123840
Iteration 14/25 | Loss: 0.00123839
Iteration 15/25 | Loss: 0.00123839
Iteration 16/25 | Loss: 0.00123836
Iteration 17/25 | Loss: 0.00123836
Iteration 18/25 | Loss: 0.00123835
Iteration 19/25 | Loss: 0.00123835
Iteration 20/25 | Loss: 0.00123835
Iteration 21/25 | Loss: 0.00123835
Iteration 22/25 | Loss: 0.00123835
Iteration 23/25 | Loss: 0.00123835
Iteration 24/25 | Loss: 0.00123835
Iteration 25/25 | Loss: 0.00123835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81397581
Iteration 2/25 | Loss: 0.00155519
Iteration 3/25 | Loss: 0.00155519
Iteration 4/25 | Loss: 0.00155519
Iteration 5/25 | Loss: 0.00155519
Iteration 6/25 | Loss: 0.00155519
Iteration 7/25 | Loss: 0.00155519
Iteration 8/25 | Loss: 0.00155519
Iteration 9/25 | Loss: 0.00155519
Iteration 10/25 | Loss: 0.00155519
Iteration 11/25 | Loss: 0.00155519
Iteration 12/25 | Loss: 0.00155519
Iteration 13/25 | Loss: 0.00155519
Iteration 14/25 | Loss: 0.00155519
Iteration 15/25 | Loss: 0.00155519
Iteration 16/25 | Loss: 0.00155519
Iteration 17/25 | Loss: 0.00155519
Iteration 18/25 | Loss: 0.00155519
Iteration 19/25 | Loss: 0.00155519
Iteration 20/25 | Loss: 0.00155519
Iteration 21/25 | Loss: 0.00155519
Iteration 22/25 | Loss: 0.00155519
Iteration 23/25 | Loss: 0.00155519
Iteration 24/25 | Loss: 0.00155519
Iteration 25/25 | Loss: 0.00155519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155519
Iteration 2/1000 | Loss: 0.00002876
Iteration 3/1000 | Loss: 0.00001844
Iteration 4/1000 | Loss: 0.00001580
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001449
Iteration 7/1000 | Loss: 0.00001404
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001314
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001254
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001248
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001226
Iteration 21/1000 | Loss: 0.00001225
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001221
Iteration 29/1000 | Loss: 0.00001219
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001215
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001213
Iteration 44/1000 | Loss: 0.00001213
Iteration 45/1000 | Loss: 0.00001213
Iteration 46/1000 | Loss: 0.00001213
Iteration 47/1000 | Loss: 0.00001213
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001211
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001211
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001210
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001209
Iteration 62/1000 | Loss: 0.00001209
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001206
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001204
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001204
Iteration 90/1000 | Loss: 0.00001204
Iteration 91/1000 | Loss: 0.00001204
Iteration 92/1000 | Loss: 0.00001204
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001197
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001197
Iteration 135/1000 | Loss: 0.00001197
Iteration 136/1000 | Loss: 0.00001197
Iteration 137/1000 | Loss: 0.00001197
Iteration 138/1000 | Loss: 0.00001197
Iteration 139/1000 | Loss: 0.00001197
Iteration 140/1000 | Loss: 0.00001197
Iteration 141/1000 | Loss: 0.00001197
Iteration 142/1000 | Loss: 0.00001197
Iteration 143/1000 | Loss: 0.00001197
Iteration 144/1000 | Loss: 0.00001197
Iteration 145/1000 | Loss: 0.00001197
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001197
Iteration 154/1000 | Loss: 0.00001197
Iteration 155/1000 | Loss: 0.00001197
Iteration 156/1000 | Loss: 0.00001197
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001197
Iteration 160/1000 | Loss: 0.00001197
Iteration 161/1000 | Loss: 0.00001197
Iteration 162/1000 | Loss: 0.00001197
Iteration 163/1000 | Loss: 0.00001197
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001197
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001197
Iteration 181/1000 | Loss: 0.00001197
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1970560080953874e-05, 1.1970560080953874e-05, 1.1970560080953874e-05, 1.1970560080953874e-05, 1.1970560080953874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1970560080953874e-05

Optimization complete. Final v2v error: 2.9700210094451904 mm

Highest mean error: 3.316230297088623 mm for frame 99

Lowest mean error: 2.667670726776123 mm for frame 112

Saving results

Total time: 53.85317611694336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412812
Iteration 2/25 | Loss: 0.00137673
Iteration 3/25 | Loss: 0.00125235
Iteration 4/25 | Loss: 0.00123390
Iteration 5/25 | Loss: 0.00122908
Iteration 6/25 | Loss: 0.00122769
Iteration 7/25 | Loss: 0.00122731
Iteration 8/25 | Loss: 0.00122725
Iteration 9/25 | Loss: 0.00122725
Iteration 10/25 | Loss: 0.00122725
Iteration 11/25 | Loss: 0.00122725
Iteration 12/25 | Loss: 0.00122725
Iteration 13/25 | Loss: 0.00122725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001227250206284225, 0.001227250206284225, 0.001227250206284225, 0.001227250206284225, 0.001227250206284225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001227250206284225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33670068
Iteration 2/25 | Loss: 0.00137969
Iteration 3/25 | Loss: 0.00137969
Iteration 4/25 | Loss: 0.00137969
Iteration 5/25 | Loss: 0.00137969
Iteration 6/25 | Loss: 0.00137969
Iteration 7/25 | Loss: 0.00137968
Iteration 8/25 | Loss: 0.00137968
Iteration 9/25 | Loss: 0.00137968
Iteration 10/25 | Loss: 0.00137968
Iteration 11/25 | Loss: 0.00137968
Iteration 12/25 | Loss: 0.00137968
Iteration 13/25 | Loss: 0.00137968
Iteration 14/25 | Loss: 0.00137968
Iteration 15/25 | Loss: 0.00137968
Iteration 16/25 | Loss: 0.00137968
Iteration 17/25 | Loss: 0.00137968
Iteration 18/25 | Loss: 0.00137968
Iteration 19/25 | Loss: 0.00137968
Iteration 20/25 | Loss: 0.00137968
Iteration 21/25 | Loss: 0.00137968
Iteration 22/25 | Loss: 0.00137968
Iteration 23/25 | Loss: 0.00137968
Iteration 24/25 | Loss: 0.00137968
Iteration 25/25 | Loss: 0.00137968
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013796827988699079, 0.0013796827988699079, 0.0013796827988699079, 0.0013796827988699079, 0.0013796827988699079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013796827988699079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137968
Iteration 2/1000 | Loss: 0.00003213
Iteration 3/1000 | Loss: 0.00002291
Iteration 4/1000 | Loss: 0.00001779
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001539
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001462
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001421
Iteration 12/1000 | Loss: 0.00001408
Iteration 13/1000 | Loss: 0.00001404
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001386
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001371
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001369
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001366
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001361
Iteration 44/1000 | Loss: 0.00001361
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001360
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001357
Iteration 62/1000 | Loss: 0.00001357
Iteration 63/1000 | Loss: 0.00001357
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001355
Iteration 68/1000 | Loss: 0.00001355
Iteration 69/1000 | Loss: 0.00001355
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001351
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001350
Iteration 89/1000 | Loss: 0.00001350
Iteration 90/1000 | Loss: 0.00001350
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001346
Iteration 110/1000 | Loss: 0.00001346
Iteration 111/1000 | Loss: 0.00001346
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001344
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001342
Iteration 132/1000 | Loss: 0.00001342
Iteration 133/1000 | Loss: 0.00001342
Iteration 134/1000 | Loss: 0.00001342
Iteration 135/1000 | Loss: 0.00001342
Iteration 136/1000 | Loss: 0.00001342
Iteration 137/1000 | Loss: 0.00001342
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001341
Iteration 146/1000 | Loss: 0.00001340
Iteration 147/1000 | Loss: 0.00001340
Iteration 148/1000 | Loss: 0.00001340
Iteration 149/1000 | Loss: 0.00001340
Iteration 150/1000 | Loss: 0.00001340
Iteration 151/1000 | Loss: 0.00001340
Iteration 152/1000 | Loss: 0.00001340
Iteration 153/1000 | Loss: 0.00001340
Iteration 154/1000 | Loss: 0.00001340
Iteration 155/1000 | Loss: 0.00001339
Iteration 156/1000 | Loss: 0.00001339
Iteration 157/1000 | Loss: 0.00001339
Iteration 158/1000 | Loss: 0.00001339
Iteration 159/1000 | Loss: 0.00001339
Iteration 160/1000 | Loss: 0.00001339
Iteration 161/1000 | Loss: 0.00001339
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001338
Iteration 164/1000 | Loss: 0.00001338
Iteration 165/1000 | Loss: 0.00001338
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001338
Iteration 171/1000 | Loss: 0.00001338
Iteration 172/1000 | Loss: 0.00001337
Iteration 173/1000 | Loss: 0.00001337
Iteration 174/1000 | Loss: 0.00001337
Iteration 175/1000 | Loss: 0.00001337
Iteration 176/1000 | Loss: 0.00001337
Iteration 177/1000 | Loss: 0.00001337
Iteration 178/1000 | Loss: 0.00001337
Iteration 179/1000 | Loss: 0.00001337
Iteration 180/1000 | Loss: 0.00001337
Iteration 181/1000 | Loss: 0.00001337
Iteration 182/1000 | Loss: 0.00001337
Iteration 183/1000 | Loss: 0.00001337
Iteration 184/1000 | Loss: 0.00001336
Iteration 185/1000 | Loss: 0.00001336
Iteration 186/1000 | Loss: 0.00001336
Iteration 187/1000 | Loss: 0.00001336
Iteration 188/1000 | Loss: 0.00001336
Iteration 189/1000 | Loss: 0.00001336
Iteration 190/1000 | Loss: 0.00001336
Iteration 191/1000 | Loss: 0.00001336
Iteration 192/1000 | Loss: 0.00001336
Iteration 193/1000 | Loss: 0.00001336
Iteration 194/1000 | Loss: 0.00001336
Iteration 195/1000 | Loss: 0.00001336
Iteration 196/1000 | Loss: 0.00001335
Iteration 197/1000 | Loss: 0.00001335
Iteration 198/1000 | Loss: 0.00001335
Iteration 199/1000 | Loss: 0.00001335
Iteration 200/1000 | Loss: 0.00001335
Iteration 201/1000 | Loss: 0.00001335
Iteration 202/1000 | Loss: 0.00001335
Iteration 203/1000 | Loss: 0.00001335
Iteration 204/1000 | Loss: 0.00001335
Iteration 205/1000 | Loss: 0.00001335
Iteration 206/1000 | Loss: 0.00001335
Iteration 207/1000 | Loss: 0.00001335
Iteration 208/1000 | Loss: 0.00001335
Iteration 209/1000 | Loss: 0.00001335
Iteration 210/1000 | Loss: 0.00001335
Iteration 211/1000 | Loss: 0.00001335
Iteration 212/1000 | Loss: 0.00001335
Iteration 213/1000 | Loss: 0.00001334
Iteration 214/1000 | Loss: 0.00001334
Iteration 215/1000 | Loss: 0.00001334
Iteration 216/1000 | Loss: 0.00001334
Iteration 217/1000 | Loss: 0.00001334
Iteration 218/1000 | Loss: 0.00001334
Iteration 219/1000 | Loss: 0.00001334
Iteration 220/1000 | Loss: 0.00001334
Iteration 221/1000 | Loss: 0.00001334
Iteration 222/1000 | Loss: 0.00001334
Iteration 223/1000 | Loss: 0.00001334
Iteration 224/1000 | Loss: 0.00001334
Iteration 225/1000 | Loss: 0.00001334
Iteration 226/1000 | Loss: 0.00001334
Iteration 227/1000 | Loss: 0.00001334
Iteration 228/1000 | Loss: 0.00001334
Iteration 229/1000 | Loss: 0.00001334
Iteration 230/1000 | Loss: 0.00001334
Iteration 231/1000 | Loss: 0.00001334
Iteration 232/1000 | Loss: 0.00001334
Iteration 233/1000 | Loss: 0.00001334
Iteration 234/1000 | Loss: 0.00001334
Iteration 235/1000 | Loss: 0.00001334
Iteration 236/1000 | Loss: 0.00001334
Iteration 237/1000 | Loss: 0.00001334
Iteration 238/1000 | Loss: 0.00001334
Iteration 239/1000 | Loss: 0.00001334
Iteration 240/1000 | Loss: 0.00001334
Iteration 241/1000 | Loss: 0.00001334
Iteration 242/1000 | Loss: 0.00001334
Iteration 243/1000 | Loss: 0.00001334
Iteration 244/1000 | Loss: 0.00001334
Iteration 245/1000 | Loss: 0.00001334
Iteration 246/1000 | Loss: 0.00001334
Iteration 247/1000 | Loss: 0.00001334
Iteration 248/1000 | Loss: 0.00001334
Iteration 249/1000 | Loss: 0.00001334
Iteration 250/1000 | Loss: 0.00001334
Iteration 251/1000 | Loss: 0.00001334
Iteration 252/1000 | Loss: 0.00001334
Iteration 253/1000 | Loss: 0.00001334
Iteration 254/1000 | Loss: 0.00001334
Iteration 255/1000 | Loss: 0.00001334
Iteration 256/1000 | Loss: 0.00001334
Iteration 257/1000 | Loss: 0.00001334
Iteration 258/1000 | Loss: 0.00001334
Iteration 259/1000 | Loss: 0.00001334
Iteration 260/1000 | Loss: 0.00001334
Iteration 261/1000 | Loss: 0.00001334
Iteration 262/1000 | Loss: 0.00001334
Iteration 263/1000 | Loss: 0.00001334
Iteration 264/1000 | Loss: 0.00001334
Iteration 265/1000 | Loss: 0.00001334
Iteration 266/1000 | Loss: 0.00001334
Iteration 267/1000 | Loss: 0.00001334
Iteration 268/1000 | Loss: 0.00001334
Iteration 269/1000 | Loss: 0.00001334
Iteration 270/1000 | Loss: 0.00001334
Iteration 271/1000 | Loss: 0.00001334
Iteration 272/1000 | Loss: 0.00001334
Iteration 273/1000 | Loss: 0.00001334
Iteration 274/1000 | Loss: 0.00001334
Iteration 275/1000 | Loss: 0.00001334
Iteration 276/1000 | Loss: 0.00001334
Iteration 277/1000 | Loss: 0.00001334
Iteration 278/1000 | Loss: 0.00001334
Iteration 279/1000 | Loss: 0.00001334
Iteration 280/1000 | Loss: 0.00001334
Iteration 281/1000 | Loss: 0.00001334
Iteration 282/1000 | Loss: 0.00001334
Iteration 283/1000 | Loss: 0.00001334
Iteration 284/1000 | Loss: 0.00001334
Iteration 285/1000 | Loss: 0.00001334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.3335198673303239e-05, 1.3335198673303239e-05, 1.3335198673303239e-05, 1.3335198673303239e-05, 1.3335198673303239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3335198673303239e-05

Optimization complete. Final v2v error: 3.0648627281188965 mm

Highest mean error: 4.3589630126953125 mm for frame 54

Lowest mean error: 2.5458712577819824 mm for frame 114

Saving results

Total time: 47.53070688247681
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417877
Iteration 2/25 | Loss: 0.00130435
Iteration 3/25 | Loss: 0.00123658
Iteration 4/25 | Loss: 0.00122608
Iteration 5/25 | Loss: 0.00122312
Iteration 6/25 | Loss: 0.00122312
Iteration 7/25 | Loss: 0.00122312
Iteration 8/25 | Loss: 0.00122312
Iteration 9/25 | Loss: 0.00122312
Iteration 10/25 | Loss: 0.00122312
Iteration 11/25 | Loss: 0.00122312
Iteration 12/25 | Loss: 0.00122312
Iteration 13/25 | Loss: 0.00122312
Iteration 14/25 | Loss: 0.00122312
Iteration 15/25 | Loss: 0.00122312
Iteration 16/25 | Loss: 0.00122312
Iteration 17/25 | Loss: 0.00122312
Iteration 18/25 | Loss: 0.00122312
Iteration 19/25 | Loss: 0.00122312
Iteration 20/25 | Loss: 0.00122312
Iteration 21/25 | Loss: 0.00122312
Iteration 22/25 | Loss: 0.00122312
Iteration 23/25 | Loss: 0.00122312
Iteration 24/25 | Loss: 0.00122312
Iteration 25/25 | Loss: 0.00122312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32676530
Iteration 2/25 | Loss: 0.00163777
Iteration 3/25 | Loss: 0.00163777
Iteration 4/25 | Loss: 0.00163777
Iteration 5/25 | Loss: 0.00163777
Iteration 6/25 | Loss: 0.00163777
Iteration 7/25 | Loss: 0.00163776
Iteration 8/25 | Loss: 0.00163776
Iteration 9/25 | Loss: 0.00163776
Iteration 10/25 | Loss: 0.00163776
Iteration 11/25 | Loss: 0.00163776
Iteration 12/25 | Loss: 0.00163776
Iteration 13/25 | Loss: 0.00163776
Iteration 14/25 | Loss: 0.00163776
Iteration 15/25 | Loss: 0.00163776
Iteration 16/25 | Loss: 0.00163776
Iteration 17/25 | Loss: 0.00163776
Iteration 18/25 | Loss: 0.00163776
Iteration 19/25 | Loss: 0.00163776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016377642750740051, 0.0016377642750740051, 0.0016377642750740051, 0.0016377642750740051, 0.0016377642750740051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016377642750740051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163776
Iteration 2/1000 | Loss: 0.00002477
Iteration 3/1000 | Loss: 0.00001798
Iteration 4/1000 | Loss: 0.00001514
Iteration 5/1000 | Loss: 0.00001435
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001296
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001245
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001226
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001202
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001199
Iteration 26/1000 | Loss: 0.00001199
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001198
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001194
Iteration 34/1000 | Loss: 0.00001194
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001190
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001187
Iteration 43/1000 | Loss: 0.00001186
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001185
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001175
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001172
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001168
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001168
Iteration 133/1000 | Loss: 0.00001167
Iteration 134/1000 | Loss: 0.00001167
Iteration 135/1000 | Loss: 0.00001167
Iteration 136/1000 | Loss: 0.00001167
Iteration 137/1000 | Loss: 0.00001167
Iteration 138/1000 | Loss: 0.00001167
Iteration 139/1000 | Loss: 0.00001167
Iteration 140/1000 | Loss: 0.00001167
Iteration 141/1000 | Loss: 0.00001167
Iteration 142/1000 | Loss: 0.00001167
Iteration 143/1000 | Loss: 0.00001167
Iteration 144/1000 | Loss: 0.00001167
Iteration 145/1000 | Loss: 0.00001167
Iteration 146/1000 | Loss: 0.00001167
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Iteration 156/1000 | Loss: 0.00001166
Iteration 157/1000 | Loss: 0.00001166
Iteration 158/1000 | Loss: 0.00001166
Iteration 159/1000 | Loss: 0.00001166
Iteration 160/1000 | Loss: 0.00001166
Iteration 161/1000 | Loss: 0.00001166
Iteration 162/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1664298654068261e-05, 1.1664298654068261e-05, 1.1664298654068261e-05, 1.1664298654068261e-05, 1.1664298654068261e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1664298654068261e-05

Optimization complete. Final v2v error: 2.885589838027954 mm

Highest mean error: 3.0322580337524414 mm for frame 166

Lowest mean error: 2.7016398906707764 mm for frame 49

Saving results

Total time: 39.49431896209717
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677900
Iteration 2/25 | Loss: 0.00135515
Iteration 3/25 | Loss: 0.00126012
Iteration 4/25 | Loss: 0.00123207
Iteration 5/25 | Loss: 0.00122635
Iteration 6/25 | Loss: 0.00121198
Iteration 7/25 | Loss: 0.00120637
Iteration 8/25 | Loss: 0.00120525
Iteration 9/25 | Loss: 0.00120488
Iteration 10/25 | Loss: 0.00120467
Iteration 11/25 | Loss: 0.00120458
Iteration 12/25 | Loss: 0.00120458
Iteration 13/25 | Loss: 0.00120458
Iteration 14/25 | Loss: 0.00120458
Iteration 15/25 | Loss: 0.00120458
Iteration 16/25 | Loss: 0.00120457
Iteration 17/25 | Loss: 0.00120457
Iteration 18/25 | Loss: 0.00120457
Iteration 19/25 | Loss: 0.00120457
Iteration 20/25 | Loss: 0.00120457
Iteration 21/25 | Loss: 0.00120457
Iteration 22/25 | Loss: 0.00120457
Iteration 23/25 | Loss: 0.00120457
Iteration 24/25 | Loss: 0.00120457
Iteration 25/25 | Loss: 0.00120457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40165305
Iteration 2/25 | Loss: 0.00146706
Iteration 3/25 | Loss: 0.00146706
Iteration 4/25 | Loss: 0.00146706
Iteration 5/25 | Loss: 0.00146706
Iteration 6/25 | Loss: 0.00146706
Iteration 7/25 | Loss: 0.00146706
Iteration 8/25 | Loss: 0.00146706
Iteration 9/25 | Loss: 0.00146706
Iteration 10/25 | Loss: 0.00146706
Iteration 11/25 | Loss: 0.00146706
Iteration 12/25 | Loss: 0.00146706
Iteration 13/25 | Loss: 0.00146706
Iteration 14/25 | Loss: 0.00146706
Iteration 15/25 | Loss: 0.00146706
Iteration 16/25 | Loss: 0.00146706
Iteration 17/25 | Loss: 0.00146706
Iteration 18/25 | Loss: 0.00146706
Iteration 19/25 | Loss: 0.00146706
Iteration 20/25 | Loss: 0.00146706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001467060879804194, 0.001467060879804194, 0.001467060879804194, 0.001467060879804194, 0.001467060879804194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001467060879804194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146706
Iteration 2/1000 | Loss: 0.00001955
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001435
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001290
Iteration 7/1000 | Loss: 0.00001265
Iteration 8/1000 | Loss: 0.00001228
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001203
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001176
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001166
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001159
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001148
Iteration 29/1000 | Loss: 0.00001142
Iteration 30/1000 | Loss: 0.00001141
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001135
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001127
Iteration 77/1000 | Loss: 0.00001127
Iteration 78/1000 | Loss: 0.00001127
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001125
Iteration 85/1000 | Loss: 0.00001125
Iteration 86/1000 | Loss: 0.00001125
Iteration 87/1000 | Loss: 0.00001124
Iteration 88/1000 | Loss: 0.00001124
Iteration 89/1000 | Loss: 0.00001124
Iteration 90/1000 | Loss: 0.00001124
Iteration 91/1000 | Loss: 0.00001123
Iteration 92/1000 | Loss: 0.00001123
Iteration 93/1000 | Loss: 0.00001123
Iteration 94/1000 | Loss: 0.00001123
Iteration 95/1000 | Loss: 0.00001123
Iteration 96/1000 | Loss: 0.00001123
Iteration 97/1000 | Loss: 0.00001123
Iteration 98/1000 | Loss: 0.00001123
Iteration 99/1000 | Loss: 0.00001123
Iteration 100/1000 | Loss: 0.00001122
Iteration 101/1000 | Loss: 0.00001122
Iteration 102/1000 | Loss: 0.00001122
Iteration 103/1000 | Loss: 0.00001122
Iteration 104/1000 | Loss: 0.00001122
Iteration 105/1000 | Loss: 0.00001122
Iteration 106/1000 | Loss: 0.00001122
Iteration 107/1000 | Loss: 0.00001122
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001121
Iteration 115/1000 | Loss: 0.00001121
Iteration 116/1000 | Loss: 0.00001121
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001120
Iteration 119/1000 | Loss: 0.00001120
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001120
Iteration 124/1000 | Loss: 0.00001120
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001119
Iteration 128/1000 | Loss: 0.00001119
Iteration 129/1000 | Loss: 0.00001118
Iteration 130/1000 | Loss: 0.00001118
Iteration 131/1000 | Loss: 0.00001118
Iteration 132/1000 | Loss: 0.00001118
Iteration 133/1000 | Loss: 0.00001118
Iteration 134/1000 | Loss: 0.00001118
Iteration 135/1000 | Loss: 0.00001118
Iteration 136/1000 | Loss: 0.00001118
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001117
Iteration 142/1000 | Loss: 0.00001117
Iteration 143/1000 | Loss: 0.00001117
Iteration 144/1000 | Loss: 0.00001117
Iteration 145/1000 | Loss: 0.00001117
Iteration 146/1000 | Loss: 0.00001117
Iteration 147/1000 | Loss: 0.00001117
Iteration 148/1000 | Loss: 0.00001117
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.1162543160025962e-05, 1.1162543160025962e-05, 1.1162543160025962e-05, 1.1162543160025962e-05, 1.1162543160025962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1162543160025962e-05

Optimization complete. Final v2v error: 2.898010492324829 mm

Highest mean error: 3.1344845294952393 mm for frame 64

Lowest mean error: 2.657982587814331 mm for frame 26

Saving results

Total time: 57.0886504650116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997998
Iteration 2/25 | Loss: 0.00997998
Iteration 3/25 | Loss: 0.00997998
Iteration 4/25 | Loss: 0.00997998
Iteration 5/25 | Loss: 0.00997998
Iteration 6/25 | Loss: 0.00997998
Iteration 7/25 | Loss: 0.00997998
Iteration 8/25 | Loss: 0.00997998
Iteration 9/25 | Loss: 0.00997998
Iteration 10/25 | Loss: 0.00997998
Iteration 11/25 | Loss: 0.00997998
Iteration 12/25 | Loss: 0.00997997
Iteration 13/25 | Loss: 0.00997997
Iteration 14/25 | Loss: 0.00997997
Iteration 15/25 | Loss: 0.00997997
Iteration 16/25 | Loss: 0.00997997
Iteration 17/25 | Loss: 0.00997997
Iteration 18/25 | Loss: 0.00997997
Iteration 19/25 | Loss: 0.00997997
Iteration 20/25 | Loss: 0.00997997
Iteration 21/25 | Loss: 0.00997997
Iteration 22/25 | Loss: 0.00997997
Iteration 23/25 | Loss: 0.00997997
Iteration 24/25 | Loss: 0.00997997
Iteration 25/25 | Loss: 0.00997997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51319480
Iteration 2/25 | Loss: 0.11965659
Iteration 3/25 | Loss: 0.11957533
Iteration 4/25 | Loss: 0.11858971
Iteration 5/25 | Loss: 0.11858969
Iteration 6/25 | Loss: 0.11858969
Iteration 7/25 | Loss: 0.11858968
Iteration 8/25 | Loss: 0.11858968
Iteration 9/25 | Loss: 0.11858967
Iteration 10/25 | Loss: 0.11858967
Iteration 11/25 | Loss: 0.11858968
Iteration 12/25 | Loss: 0.11858967
Iteration 13/25 | Loss: 0.11858967
Iteration 14/25 | Loss: 0.11858967
Iteration 15/25 | Loss: 0.11858967
Iteration 16/25 | Loss: 0.11858967
Iteration 17/25 | Loss: 0.11858967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.11858966946601868, 0.11858966946601868, 0.11858966946601868, 0.11858966946601868, 0.11858966946601868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11858966946601868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11858967
Iteration 2/1000 | Loss: 0.00293758
Iteration 3/1000 | Loss: 0.00709178
Iteration 4/1000 | Loss: 0.00140916
Iteration 5/1000 | Loss: 0.00140235
Iteration 6/1000 | Loss: 0.00086767
Iteration 7/1000 | Loss: 0.00050834
Iteration 8/1000 | Loss: 0.00069808
Iteration 9/1000 | Loss: 0.00025833
Iteration 10/1000 | Loss: 0.00007815
Iteration 11/1000 | Loss: 0.00116351
Iteration 12/1000 | Loss: 0.00021205
Iteration 13/1000 | Loss: 0.00085502
Iteration 14/1000 | Loss: 0.00005516
Iteration 15/1000 | Loss: 0.00028636
Iteration 16/1000 | Loss: 0.00010632
Iteration 17/1000 | Loss: 0.00006085
Iteration 18/1000 | Loss: 0.00050743
Iteration 19/1000 | Loss: 0.00006230
Iteration 20/1000 | Loss: 0.00023393
Iteration 21/1000 | Loss: 0.00011682
Iteration 22/1000 | Loss: 0.00007623
Iteration 23/1000 | Loss: 0.00118993
Iteration 24/1000 | Loss: 0.00010845
Iteration 25/1000 | Loss: 0.00003991
Iteration 26/1000 | Loss: 0.00007152
Iteration 27/1000 | Loss: 0.00005094
Iteration 28/1000 | Loss: 0.00010710
Iteration 29/1000 | Loss: 0.00006597
Iteration 30/1000 | Loss: 0.00006292
Iteration 31/1000 | Loss: 0.00029043
Iteration 32/1000 | Loss: 0.00003078
Iteration 33/1000 | Loss: 0.00005603
Iteration 34/1000 | Loss: 0.00005940
Iteration 35/1000 | Loss: 0.00008096
Iteration 36/1000 | Loss: 0.00020882
Iteration 37/1000 | Loss: 0.00003884
Iteration 38/1000 | Loss: 0.00002602
Iteration 39/1000 | Loss: 0.00006360
Iteration 40/1000 | Loss: 0.00003918
Iteration 41/1000 | Loss: 0.00004185
Iteration 42/1000 | Loss: 0.00003606
Iteration 43/1000 | Loss: 0.00013526
Iteration 44/1000 | Loss: 0.00007099
Iteration 45/1000 | Loss: 0.00007476
Iteration 46/1000 | Loss: 0.00009870
Iteration 47/1000 | Loss: 0.00003660
Iteration 48/1000 | Loss: 0.00003705
Iteration 49/1000 | Loss: 0.00002037
Iteration 50/1000 | Loss: 0.00014592
Iteration 51/1000 | Loss: 0.00010125
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00002074
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00004904
Iteration 57/1000 | Loss: 0.00002798
Iteration 58/1000 | Loss: 0.00002544
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00002974
Iteration 61/1000 | Loss: 0.00047739
Iteration 62/1000 | Loss: 0.00002961
Iteration 63/1000 | Loss: 0.00012703
Iteration 64/1000 | Loss: 0.00006668
Iteration 65/1000 | Loss: 0.00033955
Iteration 66/1000 | Loss: 0.00049858
Iteration 67/1000 | Loss: 0.00028866
Iteration 68/1000 | Loss: 0.00006466
Iteration 69/1000 | Loss: 0.00002470
Iteration 70/1000 | Loss: 0.00002061
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00002906
Iteration 73/1000 | Loss: 0.00016402
Iteration 74/1000 | Loss: 0.00061250
Iteration 75/1000 | Loss: 0.00002374
Iteration 76/1000 | Loss: 0.00002056
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001476
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001476
Iteration 98/1000 | Loss: 0.00001476
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001803
Iteration 107/1000 | Loss: 0.00001514
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001482
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001474
Iteration 117/1000 | Loss: 0.00001474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.4738348909304477e-05, 1.4738348909304477e-05, 1.4738348909304477e-05, 1.4738348909304477e-05, 1.4738348909304477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4738348909304477e-05

Optimization complete. Final v2v error: 3.2824485301971436 mm

Highest mean error: 3.692577600479126 mm for frame 95

Lowest mean error: 3.1891517639160156 mm for frame 234

Saving results

Total time: 133.03311204910278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998026
Iteration 2/25 | Loss: 0.00998026
Iteration 3/25 | Loss: 0.00998026
Iteration 4/25 | Loss: 0.00998026
Iteration 5/25 | Loss: 0.00998026
Iteration 6/25 | Loss: 0.00998026
Iteration 7/25 | Loss: 0.00998026
Iteration 8/25 | Loss: 0.00998026
Iteration 9/25 | Loss: 0.00998025
Iteration 10/25 | Loss: 0.00998025
Iteration 11/25 | Loss: 0.00998025
Iteration 12/25 | Loss: 0.00998025
Iteration 13/25 | Loss: 0.00998025
Iteration 14/25 | Loss: 0.00998025
Iteration 15/25 | Loss: 0.00998025
Iteration 16/25 | Loss: 0.00998025
Iteration 17/25 | Loss: 0.00998025
Iteration 18/25 | Loss: 0.00998025
Iteration 19/25 | Loss: 0.00998025
Iteration 20/25 | Loss: 0.00998025
Iteration 21/25 | Loss: 0.00998024
Iteration 22/25 | Loss: 0.00998024
Iteration 23/25 | Loss: 0.00998024
Iteration 24/25 | Loss: 0.00998024
Iteration 25/25 | Loss: 0.00998024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51333690
Iteration 2/25 | Loss: 0.11868419
Iteration 3/25 | Loss: 0.11852603
Iteration 4/25 | Loss: 0.11852602
Iteration 5/25 | Loss: 0.11852602
Iteration 6/25 | Loss: 0.11852602
Iteration 7/25 | Loss: 0.11852602
Iteration 8/25 | Loss: 0.11852602
Iteration 9/25 | Loss: 0.11852602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.11852601915597916, 0.11852601915597916, 0.11852601915597916, 0.11852601915597916, 0.11852601915597916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.11852601915597916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11852602
Iteration 2/1000 | Loss: 0.00285393
Iteration 3/1000 | Loss: 0.00474387
Iteration 4/1000 | Loss: 0.00126536
Iteration 5/1000 | Loss: 0.00063440
Iteration 6/1000 | Loss: 0.00037501
Iteration 7/1000 | Loss: 0.00009646
Iteration 8/1000 | Loss: 0.00006373
Iteration 9/1000 | Loss: 0.00008794
Iteration 10/1000 | Loss: 0.00011058
Iteration 11/1000 | Loss: 0.00022609
Iteration 12/1000 | Loss: 0.00008717
Iteration 13/1000 | Loss: 0.00038831
Iteration 14/1000 | Loss: 0.00004121
Iteration 15/1000 | Loss: 0.00003520
Iteration 16/1000 | Loss: 0.00008156
Iteration 17/1000 | Loss: 0.00008344
Iteration 18/1000 | Loss: 0.00004471
Iteration 19/1000 | Loss: 0.00004658
Iteration 20/1000 | Loss: 0.00009610
Iteration 21/1000 | Loss: 0.00172499
Iteration 22/1000 | Loss: 0.00009532
Iteration 23/1000 | Loss: 0.00018933
Iteration 24/1000 | Loss: 0.00006548
Iteration 25/1000 | Loss: 0.00006150
Iteration 26/1000 | Loss: 0.00001867
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00007097
Iteration 29/1000 | Loss: 0.00017395
Iteration 30/1000 | Loss: 0.00008331
Iteration 31/1000 | Loss: 0.00004416
Iteration 32/1000 | Loss: 0.00002131
Iteration 33/1000 | Loss: 0.00006812
Iteration 34/1000 | Loss: 0.00005407
Iteration 35/1000 | Loss: 0.00006880
Iteration 36/1000 | Loss: 0.00003562
Iteration 37/1000 | Loss: 0.00010364
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00003842
Iteration 40/1000 | Loss: 0.00003045
Iteration 41/1000 | Loss: 0.00005541
Iteration 42/1000 | Loss: 0.00003932
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00003185
Iteration 45/1000 | Loss: 0.00002077
Iteration 46/1000 | Loss: 0.00003172
Iteration 47/1000 | Loss: 0.00002261
Iteration 48/1000 | Loss: 0.00001496
Iteration 49/1000 | Loss: 0.00011980
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00006299
Iteration 52/1000 | Loss: 0.00008664
Iteration 53/1000 | Loss: 0.00002122
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00002266
Iteration 57/1000 | Loss: 0.00003817
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00002664
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001457
Iteration 63/1000 | Loss: 0.00001457
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001457
Iteration 66/1000 | Loss: 0.00001456
Iteration 67/1000 | Loss: 0.00001456
Iteration 68/1000 | Loss: 0.00001456
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001456
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001456
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.4556837413692847e-05, 1.4556837413692847e-05, 1.4556837413692847e-05, 1.4556837413692847e-05, 1.4556837413692847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4556837413692847e-05

Optimization complete. Final v2v error: 3.246399164199829 mm

Highest mean error: 3.404111862182617 mm for frame 223

Lowest mean error: 3.160663604736328 mm for frame 197

Saving results

Total time: 102.32295179367065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967726
Iteration 2/25 | Loss: 0.00160770
Iteration 3/25 | Loss: 0.00141356
Iteration 4/25 | Loss: 0.00138953
Iteration 5/25 | Loss: 0.00137624
Iteration 6/25 | Loss: 0.00140922
Iteration 7/25 | Loss: 0.00137603
Iteration 8/25 | Loss: 0.00133176
Iteration 9/25 | Loss: 0.00132681
Iteration 10/25 | Loss: 0.00132982
Iteration 11/25 | Loss: 0.00126124
Iteration 12/25 | Loss: 0.00125473
Iteration 13/25 | Loss: 0.00125166
Iteration 14/25 | Loss: 0.00124393
Iteration 15/25 | Loss: 0.00124279
Iteration 16/25 | Loss: 0.00124763
Iteration 17/25 | Loss: 0.00124090
Iteration 18/25 | Loss: 0.00123886
Iteration 19/25 | Loss: 0.00123837
Iteration 20/25 | Loss: 0.00123821
Iteration 21/25 | Loss: 0.00123820
Iteration 22/25 | Loss: 0.00123812
Iteration 23/25 | Loss: 0.00123812
Iteration 24/25 | Loss: 0.00123812
Iteration 25/25 | Loss: 0.00123812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91201615
Iteration 2/25 | Loss: 0.00159108
Iteration 3/25 | Loss: 0.00159107
Iteration 4/25 | Loss: 0.00159107
Iteration 5/25 | Loss: 0.00159107
Iteration 6/25 | Loss: 0.00159106
Iteration 7/25 | Loss: 0.00159106
Iteration 8/25 | Loss: 0.00159106
Iteration 9/25 | Loss: 0.00159106
Iteration 10/25 | Loss: 0.00159106
Iteration 11/25 | Loss: 0.00159106
Iteration 12/25 | Loss: 0.00159106
Iteration 13/25 | Loss: 0.00159106
Iteration 14/25 | Loss: 0.00159106
Iteration 15/25 | Loss: 0.00159106
Iteration 16/25 | Loss: 0.00159106
Iteration 17/25 | Loss: 0.00159106
Iteration 18/25 | Loss: 0.00159106
Iteration 19/25 | Loss: 0.00159106
Iteration 20/25 | Loss: 0.00159106
Iteration 21/25 | Loss: 0.00159106
Iteration 22/25 | Loss: 0.00159106
Iteration 23/25 | Loss: 0.00159106
Iteration 24/25 | Loss: 0.00159106
Iteration 25/25 | Loss: 0.00159106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159106
Iteration 2/1000 | Loss: 0.00004676
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002304
Iteration 6/1000 | Loss: 0.00002180
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00035229
Iteration 9/1000 | Loss: 0.00095143
Iteration 10/1000 | Loss: 0.00009683
Iteration 11/1000 | Loss: 0.00004812
Iteration 12/1000 | Loss: 0.00003246
Iteration 13/1000 | Loss: 0.00002690
Iteration 14/1000 | Loss: 0.00002473
Iteration 15/1000 | Loss: 0.00020861
Iteration 16/1000 | Loss: 0.00002796
Iteration 17/1000 | Loss: 0.00002355
Iteration 18/1000 | Loss: 0.00002130
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001815
Iteration 22/1000 | Loss: 0.00001797
Iteration 23/1000 | Loss: 0.00001780
Iteration 24/1000 | Loss: 0.00001759
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001726
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001711
Iteration 36/1000 | Loss: 0.00001710
Iteration 37/1000 | Loss: 0.00001710
Iteration 38/1000 | Loss: 0.00001709
Iteration 39/1000 | Loss: 0.00001709
Iteration 40/1000 | Loss: 0.00001708
Iteration 41/1000 | Loss: 0.00001708
Iteration 42/1000 | Loss: 0.00001708
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001706
Iteration 50/1000 | Loss: 0.00001703
Iteration 51/1000 | Loss: 0.00001703
Iteration 52/1000 | Loss: 0.00001702
Iteration 53/1000 | Loss: 0.00001702
Iteration 54/1000 | Loss: 0.00001701
Iteration 55/1000 | Loss: 0.00001701
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001701
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001701
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001700
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001698
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001697
Iteration 81/1000 | Loss: 0.00001697
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001697
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001697
Iteration 91/1000 | Loss: 0.00001697
Iteration 92/1000 | Loss: 0.00001697
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001697
Iteration 97/1000 | Loss: 0.00001697
Iteration 98/1000 | Loss: 0.00001697
Iteration 99/1000 | Loss: 0.00001697
Iteration 100/1000 | Loss: 0.00001696
Iteration 101/1000 | Loss: 0.00001696
Iteration 102/1000 | Loss: 0.00001696
Iteration 103/1000 | Loss: 0.00001696
Iteration 104/1000 | Loss: 0.00001696
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001696
Iteration 111/1000 | Loss: 0.00001696
Iteration 112/1000 | Loss: 0.00001696
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001696
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001696
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001696
Iteration 147/1000 | Loss: 0.00001696
Iteration 148/1000 | Loss: 0.00001696
Iteration 149/1000 | Loss: 0.00001696
Iteration 150/1000 | Loss: 0.00001696
Iteration 151/1000 | Loss: 0.00001696
Iteration 152/1000 | Loss: 0.00001696
Iteration 153/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.6963456801022403e-05, 1.6963456801022403e-05, 1.6963456801022403e-05, 1.6963456801022403e-05, 1.6963456801022403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6963456801022403e-05

Optimization complete. Final v2v error: 3.2218117713928223 mm

Highest mean error: 12.00491714477539 mm for frame 88

Lowest mean error: 2.65937876701355 mm for frame 32

Saving results

Total time: 81.10013628005981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996985
Iteration 2/25 | Loss: 0.00221744
Iteration 3/25 | Loss: 0.00178817
Iteration 4/25 | Loss: 0.00161029
Iteration 5/25 | Loss: 0.00158697
Iteration 6/25 | Loss: 0.00134920
Iteration 7/25 | Loss: 0.00131811
Iteration 8/25 | Loss: 0.00131360
Iteration 9/25 | Loss: 0.00131287
Iteration 10/25 | Loss: 0.00131251
Iteration 11/25 | Loss: 0.00131237
Iteration 12/25 | Loss: 0.00131236
Iteration 13/25 | Loss: 0.00131236
Iteration 14/25 | Loss: 0.00131236
Iteration 15/25 | Loss: 0.00131236
Iteration 16/25 | Loss: 0.00131236
Iteration 17/25 | Loss: 0.00131236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013123591197654605, 0.0013123591197654605, 0.0013123591197654605, 0.0013123591197654605, 0.0013123591197654605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013123591197654605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24800551
Iteration 2/25 | Loss: 0.00127258
Iteration 3/25 | Loss: 0.00127258
Iteration 4/25 | Loss: 0.00127258
Iteration 5/25 | Loss: 0.00127258
Iteration 6/25 | Loss: 0.00127258
Iteration 7/25 | Loss: 0.00127258
Iteration 8/25 | Loss: 0.00127258
Iteration 9/25 | Loss: 0.00127258
Iteration 10/25 | Loss: 0.00127257
Iteration 11/25 | Loss: 0.00127257
Iteration 12/25 | Loss: 0.00127257
Iteration 13/25 | Loss: 0.00127257
Iteration 14/25 | Loss: 0.00127257
Iteration 15/25 | Loss: 0.00127257
Iteration 16/25 | Loss: 0.00127257
Iteration 17/25 | Loss: 0.00127257
Iteration 18/25 | Loss: 0.00127257
Iteration 19/25 | Loss: 0.00127257
Iteration 20/25 | Loss: 0.00127257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001272574532777071, 0.001272574532777071, 0.001272574532777071, 0.001272574532777071, 0.001272574532777071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001272574532777071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127257
Iteration 2/1000 | Loss: 0.00003367
Iteration 3/1000 | Loss: 0.00002596
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002404
Iteration 6/1000 | Loss: 0.00002334
Iteration 7/1000 | Loss: 0.00002289
Iteration 8/1000 | Loss: 0.00002251
Iteration 9/1000 | Loss: 0.00002218
Iteration 10/1000 | Loss: 0.00002198
Iteration 11/1000 | Loss: 0.00002181
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00002158
Iteration 14/1000 | Loss: 0.00002158
Iteration 15/1000 | Loss: 0.00002158
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00002157
Iteration 18/1000 | Loss: 0.00002156
Iteration 19/1000 | Loss: 0.00002154
Iteration 20/1000 | Loss: 0.00002154
Iteration 21/1000 | Loss: 0.00002153
Iteration 22/1000 | Loss: 0.00002150
Iteration 23/1000 | Loss: 0.00002149
Iteration 24/1000 | Loss: 0.00002149
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002149
Iteration 27/1000 | Loss: 0.00002149
Iteration 28/1000 | Loss: 0.00002149
Iteration 29/1000 | Loss: 0.00002149
Iteration 30/1000 | Loss: 0.00002149
Iteration 31/1000 | Loss: 0.00002149
Iteration 32/1000 | Loss: 0.00002149
Iteration 33/1000 | Loss: 0.00002148
Iteration 34/1000 | Loss: 0.00002148
Iteration 35/1000 | Loss: 0.00002147
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002142
Iteration 38/1000 | Loss: 0.00002142
Iteration 39/1000 | Loss: 0.00002142
Iteration 40/1000 | Loss: 0.00002142
Iteration 41/1000 | Loss: 0.00002142
Iteration 42/1000 | Loss: 0.00002142
Iteration 43/1000 | Loss: 0.00002142
Iteration 44/1000 | Loss: 0.00002142
Iteration 45/1000 | Loss: 0.00002142
Iteration 46/1000 | Loss: 0.00002142
Iteration 47/1000 | Loss: 0.00002142
Iteration 48/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [2.1419920813059434e-05, 2.1419920813059434e-05, 2.1419920813059434e-05, 2.1419920813059434e-05, 2.1419920813059434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1419920813059434e-05

Optimization complete. Final v2v error: 3.993863344192505 mm

Highest mean error: 4.089565277099609 mm for frame 177

Lowest mean error: 3.93320894241333 mm for frame 115

Saving results

Total time: 43.122819662094116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_006/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_006/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426726
Iteration 2/25 | Loss: 0.00144441
Iteration 3/25 | Loss: 0.00128644
Iteration 4/25 | Loss: 0.00126707
Iteration 5/25 | Loss: 0.00126263
Iteration 6/25 | Loss: 0.00126141
Iteration 7/25 | Loss: 0.00126141
Iteration 8/25 | Loss: 0.00126141
Iteration 9/25 | Loss: 0.00126141
Iteration 10/25 | Loss: 0.00126141
Iteration 11/25 | Loss: 0.00126141
Iteration 12/25 | Loss: 0.00126141
Iteration 13/25 | Loss: 0.00126141
Iteration 14/25 | Loss: 0.00126141
Iteration 15/25 | Loss: 0.00126141
Iteration 16/25 | Loss: 0.00126141
Iteration 17/25 | Loss: 0.00126141
Iteration 18/25 | Loss: 0.00126141
Iteration 19/25 | Loss: 0.00126141
Iteration 20/25 | Loss: 0.00126141
Iteration 21/25 | Loss: 0.00126141
Iteration 22/25 | Loss: 0.00126141
Iteration 23/25 | Loss: 0.00126141
Iteration 24/25 | Loss: 0.00126141
Iteration 25/25 | Loss: 0.00126141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.27517414
Iteration 2/25 | Loss: 0.00126188
Iteration 3/25 | Loss: 0.00126185
Iteration 4/25 | Loss: 0.00126185
Iteration 5/25 | Loss: 0.00126185
Iteration 6/25 | Loss: 0.00126185
Iteration 7/25 | Loss: 0.00126185
Iteration 8/25 | Loss: 0.00126185
Iteration 9/25 | Loss: 0.00126185
Iteration 10/25 | Loss: 0.00126185
Iteration 11/25 | Loss: 0.00126185
Iteration 12/25 | Loss: 0.00126185
Iteration 13/25 | Loss: 0.00126185
Iteration 14/25 | Loss: 0.00126185
Iteration 15/25 | Loss: 0.00126185
Iteration 16/25 | Loss: 0.00126185
Iteration 17/25 | Loss: 0.00126185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012618477921932936, 0.0012618477921932936, 0.0012618477921932936, 0.0012618477921932936, 0.0012618477921932936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012618477921932936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126185
Iteration 2/1000 | Loss: 0.00003332
Iteration 3/1000 | Loss: 0.00002402
Iteration 4/1000 | Loss: 0.00002253
Iteration 5/1000 | Loss: 0.00002166
Iteration 6/1000 | Loss: 0.00002087
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00002002
Iteration 9/1000 | Loss: 0.00001956
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001903
Iteration 12/1000 | Loss: 0.00001882
Iteration 13/1000 | Loss: 0.00001872
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001867
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001851
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001847
Iteration 21/1000 | Loss: 0.00001843
Iteration 22/1000 | Loss: 0.00001843
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001842
Iteration 27/1000 | Loss: 0.00001839
Iteration 28/1000 | Loss: 0.00001838
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001838
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001835
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001834
Iteration 40/1000 | Loss: 0.00001834
Iteration 41/1000 | Loss: 0.00001834
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001834
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001833
Iteration 51/1000 | Loss: 0.00001832
Iteration 52/1000 | Loss: 0.00001832
Iteration 53/1000 | Loss: 0.00001832
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001831
Iteration 59/1000 | Loss: 0.00001831
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001830
Iteration 62/1000 | Loss: 0.00001830
Iteration 63/1000 | Loss: 0.00001830
Iteration 64/1000 | Loss: 0.00001829
Iteration 65/1000 | Loss: 0.00001829
Iteration 66/1000 | Loss: 0.00001829
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001828
Iteration 69/1000 | Loss: 0.00001828
Iteration 70/1000 | Loss: 0.00001828
Iteration 71/1000 | Loss: 0.00001827
Iteration 72/1000 | Loss: 0.00001827
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001826
Iteration 75/1000 | Loss: 0.00001826
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001823
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001822
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001819
Iteration 111/1000 | Loss: 0.00001819
Iteration 112/1000 | Loss: 0.00001819
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001818
Iteration 121/1000 | Loss: 0.00001818
Iteration 122/1000 | Loss: 0.00001818
Iteration 123/1000 | Loss: 0.00001818
Iteration 124/1000 | Loss: 0.00001818
Iteration 125/1000 | Loss: 0.00001818
Iteration 126/1000 | Loss: 0.00001818
Iteration 127/1000 | Loss: 0.00001818
Iteration 128/1000 | Loss: 0.00001818
Iteration 129/1000 | Loss: 0.00001818
Iteration 130/1000 | Loss: 0.00001818
Iteration 131/1000 | Loss: 0.00001818
Iteration 132/1000 | Loss: 0.00001818
Iteration 133/1000 | Loss: 0.00001818
Iteration 134/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.8178208847530186e-05, 1.8178208847530186e-05, 1.8178208847530186e-05, 1.8178208847530186e-05, 1.8178208847530186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8178208847530186e-05

Optimization complete. Final v2v error: 3.6265172958374023 mm

Highest mean error: 4.465703964233398 mm for frame 143

Lowest mean error: 3.267987012863159 mm for frame 76

Saving results

Total time: 40.41851568222046
