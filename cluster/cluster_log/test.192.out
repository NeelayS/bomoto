Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=192, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10752-10807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978973
Iteration 2/25 | Loss: 0.00194299
Iteration 3/25 | Loss: 0.00155919
Iteration 4/25 | Loss: 0.00148667
Iteration 5/25 | Loss: 0.00153706
Iteration 6/25 | Loss: 0.00141544
Iteration 7/25 | Loss: 0.00138091
Iteration 8/25 | Loss: 0.00133665
Iteration 9/25 | Loss: 0.00132226
Iteration 10/25 | Loss: 0.00131670
Iteration 11/25 | Loss: 0.00131479
Iteration 12/25 | Loss: 0.00132128
Iteration 13/25 | Loss: 0.00130996
Iteration 14/25 | Loss: 0.00130822
Iteration 15/25 | Loss: 0.00130810
Iteration 16/25 | Loss: 0.00130807
Iteration 17/25 | Loss: 0.00130807
Iteration 18/25 | Loss: 0.00130807
Iteration 19/25 | Loss: 0.00130807
Iteration 20/25 | Loss: 0.00130807
Iteration 21/25 | Loss: 0.00130807
Iteration 22/25 | Loss: 0.00130807
Iteration 23/25 | Loss: 0.00130807
Iteration 24/25 | Loss: 0.00130807
Iteration 25/25 | Loss: 0.00130807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90136719
Iteration 2/25 | Loss: 0.00096962
Iteration 3/25 | Loss: 0.00096962
Iteration 4/25 | Loss: 0.00093131
Iteration 5/25 | Loss: 0.00093128
Iteration 6/25 | Loss: 0.00093128
Iteration 7/25 | Loss: 0.00093128
Iteration 8/25 | Loss: 0.00093128
Iteration 9/25 | Loss: 0.00093128
Iteration 10/25 | Loss: 0.00093128
Iteration 11/25 | Loss: 0.00093128
Iteration 12/25 | Loss: 0.00093128
Iteration 13/25 | Loss: 0.00093128
Iteration 14/25 | Loss: 0.00093128
Iteration 15/25 | Loss: 0.00093128
Iteration 16/25 | Loss: 0.00093128
Iteration 17/25 | Loss: 0.00093128
Iteration 18/25 | Loss: 0.00093128
Iteration 19/25 | Loss: 0.00093128
Iteration 20/25 | Loss: 0.00093128
Iteration 21/25 | Loss: 0.00093128
Iteration 22/25 | Loss: 0.00093128
Iteration 23/25 | Loss: 0.00093128
Iteration 24/25 | Loss: 0.00093128
Iteration 25/25 | Loss: 0.00093128

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093128
Iteration 2/1000 | Loss: 0.00006860
Iteration 3/1000 | Loss: 0.00003026
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00002377
Iteration 6/1000 | Loss: 0.00002294
Iteration 7/1000 | Loss: 0.00002221
Iteration 8/1000 | Loss: 0.00002153
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00041909
Iteration 15/1000 | Loss: 0.00002631
Iteration 16/1000 | Loss: 0.00002254
Iteration 17/1000 | Loss: 0.00002081
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001744
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001739
Iteration 25/1000 | Loss: 0.00001733
Iteration 26/1000 | Loss: 0.00001727
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001711
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001702
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001697
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001693
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001692
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001691
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001686
Iteration 125/1000 | Loss: 0.00001686
Iteration 126/1000 | Loss: 0.00001686
Iteration 127/1000 | Loss: 0.00001686
Iteration 128/1000 | Loss: 0.00001686
Iteration 129/1000 | Loss: 0.00001685
Iteration 130/1000 | Loss: 0.00001685
Iteration 131/1000 | Loss: 0.00001685
Iteration 132/1000 | Loss: 0.00001685
Iteration 133/1000 | Loss: 0.00001685
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001685
Iteration 136/1000 | Loss: 0.00001685
Iteration 137/1000 | Loss: 0.00001685
Iteration 138/1000 | Loss: 0.00001685
Iteration 139/1000 | Loss: 0.00001685
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001684
Iteration 143/1000 | Loss: 0.00001684
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001683
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001682
Iteration 161/1000 | Loss: 0.00001682
Iteration 162/1000 | Loss: 0.00001682
Iteration 163/1000 | Loss: 0.00001682
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001682
Iteration 174/1000 | Loss: 0.00001682
Iteration 175/1000 | Loss: 0.00001682
Iteration 176/1000 | Loss: 0.00001682
Iteration 177/1000 | Loss: 0.00001682
Iteration 178/1000 | Loss: 0.00001682
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.682408583292272e-05, 1.682408583292272e-05, 1.682408583292272e-05, 1.682408583292272e-05, 1.682408583292272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.682408583292272e-05

Optimization complete. Final v2v error: 3.453660488128662 mm

Highest mean error: 4.221156120300293 mm for frame 73

Lowest mean error: 3.1895265579223633 mm for frame 120

Saving results

Total time: 72.35896682739258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993711
Iteration 2/25 | Loss: 0.00286025
Iteration 3/25 | Loss: 0.00209805
Iteration 4/25 | Loss: 0.00192822
Iteration 5/25 | Loss: 0.00183078
Iteration 6/25 | Loss: 0.00160365
Iteration 7/25 | Loss: 0.00139225
Iteration 8/25 | Loss: 0.00132711
Iteration 9/25 | Loss: 0.00130040
Iteration 10/25 | Loss: 0.00127706
Iteration 11/25 | Loss: 0.00127358
Iteration 12/25 | Loss: 0.00126579
Iteration 13/25 | Loss: 0.00127177
Iteration 14/25 | Loss: 0.00126921
Iteration 15/25 | Loss: 0.00126538
Iteration 16/25 | Loss: 0.00125683
Iteration 17/25 | Loss: 0.00124850
Iteration 18/25 | Loss: 0.00125013
Iteration 19/25 | Loss: 0.00124772
Iteration 20/25 | Loss: 0.00124192
Iteration 21/25 | Loss: 0.00124024
Iteration 22/25 | Loss: 0.00123863
Iteration 23/25 | Loss: 0.00123809
Iteration 24/25 | Loss: 0.00124017
Iteration 25/25 | Loss: 0.00123957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32354927
Iteration 2/25 | Loss: 0.00108272
Iteration 3/25 | Loss: 0.00108272
Iteration 4/25 | Loss: 0.00108272
Iteration 5/25 | Loss: 0.00108271
Iteration 6/25 | Loss: 0.00107187
Iteration 7/25 | Loss: 0.00107187
Iteration 8/25 | Loss: 0.00107187
Iteration 9/25 | Loss: 0.00107187
Iteration 10/25 | Loss: 0.00107187
Iteration 11/25 | Loss: 0.00107187
Iteration 12/25 | Loss: 0.00107187
Iteration 13/25 | Loss: 0.00107187
Iteration 14/25 | Loss: 0.00107187
Iteration 15/25 | Loss: 0.00107187
Iteration 16/25 | Loss: 0.00107187
Iteration 17/25 | Loss: 0.00107187
Iteration 18/25 | Loss: 0.00107187
Iteration 19/25 | Loss: 0.00107187
Iteration 20/25 | Loss: 0.00107187
Iteration 21/25 | Loss: 0.00107187
Iteration 22/25 | Loss: 0.00107187
Iteration 23/25 | Loss: 0.00107187
Iteration 24/25 | Loss: 0.00107187
Iteration 25/25 | Loss: 0.00107187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107187
Iteration 2/1000 | Loss: 0.00009030
Iteration 3/1000 | Loss: 0.00007781
Iteration 4/1000 | Loss: 0.00009348
Iteration 5/1000 | Loss: 0.00008961
Iteration 6/1000 | Loss: 0.00008017
Iteration 7/1000 | Loss: 0.00008044
Iteration 8/1000 | Loss: 0.00012661
Iteration 9/1000 | Loss: 0.00005439
Iteration 10/1000 | Loss: 0.00009653
Iteration 11/1000 | Loss: 0.00008409
Iteration 12/1000 | Loss: 0.00008014
Iteration 13/1000 | Loss: 0.00007323
Iteration 14/1000 | Loss: 0.00008559
Iteration 15/1000 | Loss: 0.00008714
Iteration 16/1000 | Loss: 0.00007892
Iteration 17/1000 | Loss: 0.00008683
Iteration 18/1000 | Loss: 0.00009947
Iteration 19/1000 | Loss: 0.00007859
Iteration 20/1000 | Loss: 0.00008639
Iteration 21/1000 | Loss: 0.00007996
Iteration 22/1000 | Loss: 0.00008440
Iteration 23/1000 | Loss: 0.00008447
Iteration 24/1000 | Loss: 0.00009367
Iteration 25/1000 | Loss: 0.00009062
Iteration 26/1000 | Loss: 0.00008963
Iteration 27/1000 | Loss: 0.00008252
Iteration 28/1000 | Loss: 0.00009085
Iteration 29/1000 | Loss: 0.00008809
Iteration 30/1000 | Loss: 0.00008667
Iteration 31/1000 | Loss: 0.00008779
Iteration 32/1000 | Loss: 0.00011780
Iteration 33/1000 | Loss: 0.00009029
Iteration 34/1000 | Loss: 0.00008306
Iteration 35/1000 | Loss: 0.00005918
Iteration 36/1000 | Loss: 0.00005207
Iteration 37/1000 | Loss: 0.00005903
Iteration 38/1000 | Loss: 0.00005591
Iteration 39/1000 | Loss: 0.00002672
Iteration 40/1000 | Loss: 0.00005741
Iteration 41/1000 | Loss: 0.00002659
Iteration 42/1000 | Loss: 0.00004500
Iteration 43/1000 | Loss: 0.00003388
Iteration 44/1000 | Loss: 0.00006848
Iteration 45/1000 | Loss: 0.00005505
Iteration 46/1000 | Loss: 0.00004678
Iteration 47/1000 | Loss: 0.00003349
Iteration 48/1000 | Loss: 0.00002079
Iteration 49/1000 | Loss: 0.00004292
Iteration 50/1000 | Loss: 0.00003415
Iteration 51/1000 | Loss: 0.00004228
Iteration 52/1000 | Loss: 0.00003408
Iteration 53/1000 | Loss: 0.00004977
Iteration 54/1000 | Loss: 0.00002600
Iteration 55/1000 | Loss: 0.00002659
Iteration 56/1000 | Loss: 0.00004070
Iteration 57/1000 | Loss: 0.00003862
Iteration 58/1000 | Loss: 0.00004175
Iteration 59/1000 | Loss: 0.00003104
Iteration 60/1000 | Loss: 0.00004107
Iteration 61/1000 | Loss: 0.00003329
Iteration 62/1000 | Loss: 0.00004222
Iteration 63/1000 | Loss: 0.00004284
Iteration 64/1000 | Loss: 0.00004444
Iteration 65/1000 | Loss: 0.00003734
Iteration 66/1000 | Loss: 0.00004766
Iteration 67/1000 | Loss: 0.00003414
Iteration 68/1000 | Loss: 0.00003350
Iteration 69/1000 | Loss: 0.00003061
Iteration 70/1000 | Loss: 0.00003884
Iteration 71/1000 | Loss: 0.00003443
Iteration 72/1000 | Loss: 0.00004172
Iteration 73/1000 | Loss: 0.00003585
Iteration 74/1000 | Loss: 0.00003961
Iteration 75/1000 | Loss: 0.00003161
Iteration 76/1000 | Loss: 0.00004214
Iteration 77/1000 | Loss: 0.00003485
Iteration 78/1000 | Loss: 0.00004499
Iteration 79/1000 | Loss: 0.00003105
Iteration 80/1000 | Loss: 0.00004525
Iteration 81/1000 | Loss: 0.00003471
Iteration 82/1000 | Loss: 0.00004570
Iteration 83/1000 | Loss: 0.00003609
Iteration 84/1000 | Loss: 0.00003690
Iteration 85/1000 | Loss: 0.00005952
Iteration 86/1000 | Loss: 0.00004195
Iteration 87/1000 | Loss: 0.00003439
Iteration 88/1000 | Loss: 0.00002676
Iteration 89/1000 | Loss: 0.00004240
Iteration 90/1000 | Loss: 0.00004168
Iteration 91/1000 | Loss: 0.00003378
Iteration 92/1000 | Loss: 0.00003633
Iteration 93/1000 | Loss: 0.00003448
Iteration 94/1000 | Loss: 0.00003722
Iteration 95/1000 | Loss: 0.00003569
Iteration 96/1000 | Loss: 0.00003872
Iteration 97/1000 | Loss: 0.00003464
Iteration 98/1000 | Loss: 0.00003863
Iteration 99/1000 | Loss: 0.00003547
Iteration 100/1000 | Loss: 0.00003396
Iteration 101/1000 | Loss: 0.00003755
Iteration 102/1000 | Loss: 0.00005743
Iteration 103/1000 | Loss: 0.00004407
Iteration 104/1000 | Loss: 0.00005387
Iteration 105/1000 | Loss: 0.00003768
Iteration 106/1000 | Loss: 0.00055486
Iteration 107/1000 | Loss: 0.00059807
Iteration 108/1000 | Loss: 0.00041307
Iteration 109/1000 | Loss: 0.00004631
Iteration 110/1000 | Loss: 0.00003999
Iteration 111/1000 | Loss: 0.00004929
Iteration 112/1000 | Loss: 0.00005425
Iteration 113/1000 | Loss: 0.00002635
Iteration 114/1000 | Loss: 0.00005952
Iteration 115/1000 | Loss: 0.00003051
Iteration 116/1000 | Loss: 0.00005553
Iteration 117/1000 | Loss: 0.00004365
Iteration 118/1000 | Loss: 0.00004334
Iteration 119/1000 | Loss: 0.00004216
Iteration 120/1000 | Loss: 0.00003946
Iteration 121/1000 | Loss: 0.00004091
Iteration 122/1000 | Loss: 0.00003639
Iteration 123/1000 | Loss: 0.00003997
Iteration 124/1000 | Loss: 0.00002572
Iteration 125/1000 | Loss: 0.00003771
Iteration 126/1000 | Loss: 0.00002482
Iteration 127/1000 | Loss: 0.00005199
Iteration 128/1000 | Loss: 0.00002522
Iteration 129/1000 | Loss: 0.00005160
Iteration 130/1000 | Loss: 0.00004006
Iteration 131/1000 | Loss: 0.00003297
Iteration 132/1000 | Loss: 0.00006011
Iteration 133/1000 | Loss: 0.00003625
Iteration 134/1000 | Loss: 0.00005815
Iteration 135/1000 | Loss: 0.00001914
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00001670
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001918
Iteration 140/1000 | Loss: 0.00001368
Iteration 141/1000 | Loss: 0.00001357
Iteration 142/1000 | Loss: 0.00001351
Iteration 143/1000 | Loss: 0.00001349
Iteration 144/1000 | Loss: 0.00003291
Iteration 145/1000 | Loss: 0.00001334
Iteration 146/1000 | Loss: 0.00001316
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001310
Iteration 155/1000 | Loss: 0.00001310
Iteration 156/1000 | Loss: 0.00001482
Iteration 157/1000 | Loss: 0.00001304
Iteration 158/1000 | Loss: 0.00001304
Iteration 159/1000 | Loss: 0.00001304
Iteration 160/1000 | Loss: 0.00001304
Iteration 161/1000 | Loss: 0.00001304
Iteration 162/1000 | Loss: 0.00001304
Iteration 163/1000 | Loss: 0.00001304
Iteration 164/1000 | Loss: 0.00001304
Iteration 165/1000 | Loss: 0.00001304
Iteration 166/1000 | Loss: 0.00001303
Iteration 167/1000 | Loss: 0.00001303
Iteration 168/1000 | Loss: 0.00001303
Iteration 169/1000 | Loss: 0.00001303
Iteration 170/1000 | Loss: 0.00001303
Iteration 171/1000 | Loss: 0.00001302
Iteration 172/1000 | Loss: 0.00001302
Iteration 173/1000 | Loss: 0.00001301
Iteration 174/1000 | Loss: 0.00001301
Iteration 175/1000 | Loss: 0.00001300
Iteration 176/1000 | Loss: 0.00001300
Iteration 177/1000 | Loss: 0.00001299
Iteration 178/1000 | Loss: 0.00001299
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00002448
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001284
Iteration 183/1000 | Loss: 0.00001284
Iteration 184/1000 | Loss: 0.00001284
Iteration 185/1000 | Loss: 0.00001284
Iteration 186/1000 | Loss: 0.00001284
Iteration 187/1000 | Loss: 0.00001284
Iteration 188/1000 | Loss: 0.00001283
Iteration 189/1000 | Loss: 0.00001283
Iteration 190/1000 | Loss: 0.00001283
Iteration 191/1000 | Loss: 0.00001283
Iteration 192/1000 | Loss: 0.00001283
Iteration 193/1000 | Loss: 0.00001283
Iteration 194/1000 | Loss: 0.00001283
Iteration 195/1000 | Loss: 0.00001283
Iteration 196/1000 | Loss: 0.00002742
Iteration 197/1000 | Loss: 0.00001281
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001268
Iteration 201/1000 | Loss: 0.00001268
Iteration 202/1000 | Loss: 0.00001268
Iteration 203/1000 | Loss: 0.00001268
Iteration 204/1000 | Loss: 0.00001268
Iteration 205/1000 | Loss: 0.00001268
Iteration 206/1000 | Loss: 0.00001268
Iteration 207/1000 | Loss: 0.00001268
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001267
Iteration 211/1000 | Loss: 0.00001266
Iteration 212/1000 | Loss: 0.00001266
Iteration 213/1000 | Loss: 0.00001265
Iteration 214/1000 | Loss: 0.00001265
Iteration 215/1000 | Loss: 0.00001265
Iteration 216/1000 | Loss: 0.00001265
Iteration 217/1000 | Loss: 0.00001265
Iteration 218/1000 | Loss: 0.00001265
Iteration 219/1000 | Loss: 0.00001264
Iteration 220/1000 | Loss: 0.00001264
Iteration 221/1000 | Loss: 0.00001264
Iteration 222/1000 | Loss: 0.00001264
Iteration 223/1000 | Loss: 0.00001264
Iteration 224/1000 | Loss: 0.00001264
Iteration 225/1000 | Loss: 0.00001264
Iteration 226/1000 | Loss: 0.00001264
Iteration 227/1000 | Loss: 0.00001264
Iteration 228/1000 | Loss: 0.00001263
Iteration 229/1000 | Loss: 0.00001263
Iteration 230/1000 | Loss: 0.00001263
Iteration 231/1000 | Loss: 0.00004300
Iteration 232/1000 | Loss: 0.00001578
Iteration 233/1000 | Loss: 0.00001599
Iteration 234/1000 | Loss: 0.00001894
Iteration 235/1000 | Loss: 0.00005978
Iteration 236/1000 | Loss: 0.00001259
Iteration 237/1000 | Loss: 0.00001259
Iteration 238/1000 | Loss: 0.00001259
Iteration 239/1000 | Loss: 0.00001259
Iteration 240/1000 | Loss: 0.00001259
Iteration 241/1000 | Loss: 0.00001259
Iteration 242/1000 | Loss: 0.00001259
Iteration 243/1000 | Loss: 0.00001259
Iteration 244/1000 | Loss: 0.00001259
Iteration 245/1000 | Loss: 0.00001259
Iteration 246/1000 | Loss: 0.00001259
Iteration 247/1000 | Loss: 0.00001259
Iteration 248/1000 | Loss: 0.00001259
Iteration 249/1000 | Loss: 0.00001259
Iteration 250/1000 | Loss: 0.00001259
Iteration 251/1000 | Loss: 0.00001259
Iteration 252/1000 | Loss: 0.00001259
Iteration 253/1000 | Loss: 0.00001259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.258615156984888e-05, 1.258615156984888e-05, 1.258615156984888e-05, 1.258615156984888e-05, 1.258615156984888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.258615156984888e-05

Optimization complete. Final v2v error: 2.996903657913208 mm

Highest mean error: 4.000453948974609 mm for frame 44

Lowest mean error: 2.612741470336914 mm for frame 84

Saving results

Total time: 263.54600858688354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439574
Iteration 2/25 | Loss: 0.00133891
Iteration 3/25 | Loss: 0.00125118
Iteration 4/25 | Loss: 0.00123600
Iteration 5/25 | Loss: 0.00123173
Iteration 6/25 | Loss: 0.00123145
Iteration 7/25 | Loss: 0.00123145
Iteration 8/25 | Loss: 0.00123145
Iteration 9/25 | Loss: 0.00123145
Iteration 10/25 | Loss: 0.00123145
Iteration 11/25 | Loss: 0.00123145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012314536143094301, 0.0012314536143094301, 0.0012314536143094301, 0.0012314536143094301, 0.0012314536143094301]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012314536143094301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33096766
Iteration 2/25 | Loss: 0.00081847
Iteration 3/25 | Loss: 0.00081846
Iteration 4/25 | Loss: 0.00081846
Iteration 5/25 | Loss: 0.00081846
Iteration 6/25 | Loss: 0.00081846
Iteration 7/25 | Loss: 0.00081846
Iteration 8/25 | Loss: 0.00081846
Iteration 9/25 | Loss: 0.00081846
Iteration 10/25 | Loss: 0.00081846
Iteration 11/25 | Loss: 0.00081846
Iteration 12/25 | Loss: 0.00081846
Iteration 13/25 | Loss: 0.00081846
Iteration 14/25 | Loss: 0.00081846
Iteration 15/25 | Loss: 0.00081846
Iteration 16/25 | Loss: 0.00081846
Iteration 17/25 | Loss: 0.00081846
Iteration 18/25 | Loss: 0.00081846
Iteration 19/25 | Loss: 0.00081846
Iteration 20/25 | Loss: 0.00081846
Iteration 21/25 | Loss: 0.00081846
Iteration 22/25 | Loss: 0.00081846
Iteration 23/25 | Loss: 0.00081846
Iteration 24/25 | Loss: 0.00081846
Iteration 25/25 | Loss: 0.00081846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081846
Iteration 2/1000 | Loss: 0.00002900
Iteration 3/1000 | Loss: 0.00002142
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001874
Iteration 6/1000 | Loss: 0.00001806
Iteration 7/1000 | Loss: 0.00001761
Iteration 8/1000 | Loss: 0.00001728
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001665
Iteration 12/1000 | Loss: 0.00001650
Iteration 13/1000 | Loss: 0.00001634
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001615
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001611
Iteration 18/1000 | Loss: 0.00001610
Iteration 19/1000 | Loss: 0.00001608
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001602
Iteration 24/1000 | Loss: 0.00001602
Iteration 25/1000 | Loss: 0.00001602
Iteration 26/1000 | Loss: 0.00001602
Iteration 27/1000 | Loss: 0.00001600
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001595
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001592
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001591
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001590
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001589
Iteration 49/1000 | Loss: 0.00001589
Iteration 50/1000 | Loss: 0.00001588
Iteration 51/1000 | Loss: 0.00001588
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001587
Iteration 56/1000 | Loss: 0.00001586
Iteration 57/1000 | Loss: 0.00001586
Iteration 58/1000 | Loss: 0.00001585
Iteration 59/1000 | Loss: 0.00001585
Iteration 60/1000 | Loss: 0.00001585
Iteration 61/1000 | Loss: 0.00001585
Iteration 62/1000 | Loss: 0.00001584
Iteration 63/1000 | Loss: 0.00001584
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001581
Iteration 67/1000 | Loss: 0.00001580
Iteration 68/1000 | Loss: 0.00001580
Iteration 69/1000 | Loss: 0.00001580
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001579
Iteration 72/1000 | Loss: 0.00001578
Iteration 73/1000 | Loss: 0.00001578
Iteration 74/1000 | Loss: 0.00001574
Iteration 75/1000 | Loss: 0.00001574
Iteration 76/1000 | Loss: 0.00001574
Iteration 77/1000 | Loss: 0.00001574
Iteration 78/1000 | Loss: 0.00001574
Iteration 79/1000 | Loss: 0.00001574
Iteration 80/1000 | Loss: 0.00001574
Iteration 81/1000 | Loss: 0.00001574
Iteration 82/1000 | Loss: 0.00001574
Iteration 83/1000 | Loss: 0.00001573
Iteration 84/1000 | Loss: 0.00001573
Iteration 85/1000 | Loss: 0.00001573
Iteration 86/1000 | Loss: 0.00001573
Iteration 87/1000 | Loss: 0.00001572
Iteration 88/1000 | Loss: 0.00001572
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001568
Iteration 98/1000 | Loss: 0.00001568
Iteration 99/1000 | Loss: 0.00001568
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001567
Iteration 102/1000 | Loss: 0.00001567
Iteration 103/1000 | Loss: 0.00001567
Iteration 104/1000 | Loss: 0.00001567
Iteration 105/1000 | Loss: 0.00001566
Iteration 106/1000 | Loss: 0.00001566
Iteration 107/1000 | Loss: 0.00001566
Iteration 108/1000 | Loss: 0.00001566
Iteration 109/1000 | Loss: 0.00001566
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001565
Iteration 113/1000 | Loss: 0.00001565
Iteration 114/1000 | Loss: 0.00001565
Iteration 115/1000 | Loss: 0.00001565
Iteration 116/1000 | Loss: 0.00001565
Iteration 117/1000 | Loss: 0.00001564
Iteration 118/1000 | Loss: 0.00001564
Iteration 119/1000 | Loss: 0.00001564
Iteration 120/1000 | Loss: 0.00001564
Iteration 121/1000 | Loss: 0.00001564
Iteration 122/1000 | Loss: 0.00001564
Iteration 123/1000 | Loss: 0.00001564
Iteration 124/1000 | Loss: 0.00001564
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001564
Iteration 130/1000 | Loss: 0.00001564
Iteration 131/1000 | Loss: 0.00001564
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001563
Iteration 141/1000 | Loss: 0.00001563
Iteration 142/1000 | Loss: 0.00001563
Iteration 143/1000 | Loss: 0.00001563
Iteration 144/1000 | Loss: 0.00001563
Iteration 145/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5630246707587503e-05, 1.5630246707587503e-05, 1.5630246707587503e-05, 1.5630246707587503e-05, 1.5630246707587503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5630246707587503e-05

Optimization complete. Final v2v error: 3.364694595336914 mm

Highest mean error: 3.588775634765625 mm for frame 177

Lowest mean error: 3.0173861980438232 mm for frame 191

Saving results

Total time: 44.313010692596436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844402
Iteration 2/25 | Loss: 0.00162390
Iteration 3/25 | Loss: 0.00134755
Iteration 4/25 | Loss: 0.00132383
Iteration 5/25 | Loss: 0.00131889
Iteration 6/25 | Loss: 0.00131859
Iteration 7/25 | Loss: 0.00131859
Iteration 8/25 | Loss: 0.00131859
Iteration 9/25 | Loss: 0.00131859
Iteration 10/25 | Loss: 0.00131859
Iteration 11/25 | Loss: 0.00131859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013185905991122127, 0.0013185905991122127, 0.0013185905991122127, 0.0013185905991122127, 0.0013185905991122127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013185905991122127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.15415320
Iteration 2/25 | Loss: 0.00080817
Iteration 3/25 | Loss: 0.00080815
Iteration 4/25 | Loss: 0.00080815
Iteration 5/25 | Loss: 0.00080815
Iteration 6/25 | Loss: 0.00080815
Iteration 7/25 | Loss: 0.00080815
Iteration 8/25 | Loss: 0.00080815
Iteration 9/25 | Loss: 0.00080815
Iteration 10/25 | Loss: 0.00080815
Iteration 11/25 | Loss: 0.00080815
Iteration 12/25 | Loss: 0.00080815
Iteration 13/25 | Loss: 0.00080815
Iteration 14/25 | Loss: 0.00080815
Iteration 15/25 | Loss: 0.00080815
Iteration 16/25 | Loss: 0.00080815
Iteration 17/25 | Loss: 0.00080815
Iteration 18/25 | Loss: 0.00080815
Iteration 19/25 | Loss: 0.00080815
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008081484702415764, 0.0008081484702415764, 0.0008081484702415764, 0.0008081484702415764, 0.0008081484702415764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008081484702415764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080815
Iteration 2/1000 | Loss: 0.00005752
Iteration 3/1000 | Loss: 0.00003230
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00002673
Iteration 6/1000 | Loss: 0.00002574
Iteration 7/1000 | Loss: 0.00002514
Iteration 8/1000 | Loss: 0.00002467
Iteration 9/1000 | Loss: 0.00002432
Iteration 10/1000 | Loss: 0.00002407
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002381
Iteration 13/1000 | Loss: 0.00002363
Iteration 14/1000 | Loss: 0.00002359
Iteration 15/1000 | Loss: 0.00002353
Iteration 16/1000 | Loss: 0.00002350
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002347
Iteration 19/1000 | Loss: 0.00002347
Iteration 20/1000 | Loss: 0.00002347
Iteration 21/1000 | Loss: 0.00002346
Iteration 22/1000 | Loss: 0.00002346
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002345
Iteration 27/1000 | Loss: 0.00002345
Iteration 28/1000 | Loss: 0.00002344
Iteration 29/1000 | Loss: 0.00002344
Iteration 30/1000 | Loss: 0.00002344
Iteration 31/1000 | Loss: 0.00002344
Iteration 32/1000 | Loss: 0.00002344
Iteration 33/1000 | Loss: 0.00002344
Iteration 34/1000 | Loss: 0.00002344
Iteration 35/1000 | Loss: 0.00002343
Iteration 36/1000 | Loss: 0.00002343
Iteration 37/1000 | Loss: 0.00002343
Iteration 38/1000 | Loss: 0.00002342
Iteration 39/1000 | Loss: 0.00002341
Iteration 40/1000 | Loss: 0.00002341
Iteration 41/1000 | Loss: 0.00002338
Iteration 42/1000 | Loss: 0.00002338
Iteration 43/1000 | Loss: 0.00002338
Iteration 44/1000 | Loss: 0.00002338
Iteration 45/1000 | Loss: 0.00002338
Iteration 46/1000 | Loss: 0.00002338
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002337
Iteration 49/1000 | Loss: 0.00002337
Iteration 50/1000 | Loss: 0.00002337
Iteration 51/1000 | Loss: 0.00002337
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002337
Iteration 54/1000 | Loss: 0.00002337
Iteration 55/1000 | Loss: 0.00002337
Iteration 56/1000 | Loss: 0.00002336
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002334
Iteration 60/1000 | Loss: 0.00002333
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Iteration 63/1000 | Loss: 0.00002333
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002333
Iteration 66/1000 | Loss: 0.00002333
Iteration 67/1000 | Loss: 0.00002332
Iteration 68/1000 | Loss: 0.00002332
Iteration 69/1000 | Loss: 0.00002332
Iteration 70/1000 | Loss: 0.00002331
Iteration 71/1000 | Loss: 0.00002331
Iteration 72/1000 | Loss: 0.00002331
Iteration 73/1000 | Loss: 0.00002331
Iteration 74/1000 | Loss: 0.00002331
Iteration 75/1000 | Loss: 0.00002331
Iteration 76/1000 | Loss: 0.00002331
Iteration 77/1000 | Loss: 0.00002331
Iteration 78/1000 | Loss: 0.00002330
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002330
Iteration 81/1000 | Loss: 0.00002330
Iteration 82/1000 | Loss: 0.00002330
Iteration 83/1000 | Loss: 0.00002330
Iteration 84/1000 | Loss: 0.00002330
Iteration 85/1000 | Loss: 0.00002330
Iteration 86/1000 | Loss: 0.00002330
Iteration 87/1000 | Loss: 0.00002330
Iteration 88/1000 | Loss: 0.00002330
Iteration 89/1000 | Loss: 0.00002330
Iteration 90/1000 | Loss: 0.00002329
Iteration 91/1000 | Loss: 0.00002329
Iteration 92/1000 | Loss: 0.00002329
Iteration 93/1000 | Loss: 0.00002329
Iteration 94/1000 | Loss: 0.00002329
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002326
Iteration 108/1000 | Loss: 0.00002326
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002324
Iteration 114/1000 | Loss: 0.00002324
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002324
Iteration 120/1000 | Loss: 0.00002324
Iteration 121/1000 | Loss: 0.00002324
Iteration 122/1000 | Loss: 0.00002324
Iteration 123/1000 | Loss: 0.00002324
Iteration 124/1000 | Loss: 0.00002324
Iteration 125/1000 | Loss: 0.00002323
Iteration 126/1000 | Loss: 0.00002323
Iteration 127/1000 | Loss: 0.00002323
Iteration 128/1000 | Loss: 0.00002323
Iteration 129/1000 | Loss: 0.00002323
Iteration 130/1000 | Loss: 0.00002323
Iteration 131/1000 | Loss: 0.00002323
Iteration 132/1000 | Loss: 0.00002323
Iteration 133/1000 | Loss: 0.00002323
Iteration 134/1000 | Loss: 0.00002323
Iteration 135/1000 | Loss: 0.00002323
Iteration 136/1000 | Loss: 0.00002323
Iteration 137/1000 | Loss: 0.00002323
Iteration 138/1000 | Loss: 0.00002322
Iteration 139/1000 | Loss: 0.00002322
Iteration 140/1000 | Loss: 0.00002322
Iteration 141/1000 | Loss: 0.00002322
Iteration 142/1000 | Loss: 0.00002321
Iteration 143/1000 | Loss: 0.00002321
Iteration 144/1000 | Loss: 0.00002321
Iteration 145/1000 | Loss: 0.00002321
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002320
Iteration 148/1000 | Loss: 0.00002320
Iteration 149/1000 | Loss: 0.00002320
Iteration 150/1000 | Loss: 0.00002320
Iteration 151/1000 | Loss: 0.00002320
Iteration 152/1000 | Loss: 0.00002320
Iteration 153/1000 | Loss: 0.00002320
Iteration 154/1000 | Loss: 0.00002320
Iteration 155/1000 | Loss: 0.00002320
Iteration 156/1000 | Loss: 0.00002320
Iteration 157/1000 | Loss: 0.00002320
Iteration 158/1000 | Loss: 0.00002320
Iteration 159/1000 | Loss: 0.00002320
Iteration 160/1000 | Loss: 0.00002320
Iteration 161/1000 | Loss: 0.00002320
Iteration 162/1000 | Loss: 0.00002320
Iteration 163/1000 | Loss: 0.00002320
Iteration 164/1000 | Loss: 0.00002320
Iteration 165/1000 | Loss: 0.00002320
Iteration 166/1000 | Loss: 0.00002320
Iteration 167/1000 | Loss: 0.00002320
Iteration 168/1000 | Loss: 0.00002320
Iteration 169/1000 | Loss: 0.00002320
Iteration 170/1000 | Loss: 0.00002320
Iteration 171/1000 | Loss: 0.00002320
Iteration 172/1000 | Loss: 0.00002320
Iteration 173/1000 | Loss: 0.00002320
Iteration 174/1000 | Loss: 0.00002320
Iteration 175/1000 | Loss: 0.00002320
Iteration 176/1000 | Loss: 0.00002320
Iteration 177/1000 | Loss: 0.00002320
Iteration 178/1000 | Loss: 0.00002320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.320014755241573e-05, 2.320014755241573e-05, 2.320014755241573e-05, 2.320014755241573e-05, 2.320014755241573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.320014755241573e-05

Optimization complete. Final v2v error: 4.092748165130615 mm

Highest mean error: 4.259748458862305 mm for frame 52

Lowest mean error: 3.687960624694824 mm for frame 102

Saving results

Total time: 42.060218334198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447010
Iteration 2/25 | Loss: 0.00138559
Iteration 3/25 | Loss: 0.00124926
Iteration 4/25 | Loss: 0.00123235
Iteration 5/25 | Loss: 0.00122811
Iteration 6/25 | Loss: 0.00122728
Iteration 7/25 | Loss: 0.00122728
Iteration 8/25 | Loss: 0.00122728
Iteration 9/25 | Loss: 0.00122728
Iteration 10/25 | Loss: 0.00122728
Iteration 11/25 | Loss: 0.00122728
Iteration 12/25 | Loss: 0.00122728
Iteration 13/25 | Loss: 0.00122728
Iteration 14/25 | Loss: 0.00122728
Iteration 15/25 | Loss: 0.00122728
Iteration 16/25 | Loss: 0.00122728
Iteration 17/25 | Loss: 0.00122728
Iteration 18/25 | Loss: 0.00122728
Iteration 19/25 | Loss: 0.00122728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012272802414372563, 0.0012272802414372563, 0.0012272802414372563, 0.0012272802414372563, 0.0012272802414372563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012272802414372563

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55940390
Iteration 2/25 | Loss: 0.00102099
Iteration 3/25 | Loss: 0.00102097
Iteration 4/25 | Loss: 0.00102097
Iteration 5/25 | Loss: 0.00102097
Iteration 6/25 | Loss: 0.00102097
Iteration 7/25 | Loss: 0.00102097
Iteration 8/25 | Loss: 0.00102097
Iteration 9/25 | Loss: 0.00102097
Iteration 10/25 | Loss: 0.00102097
Iteration 11/25 | Loss: 0.00102097
Iteration 12/25 | Loss: 0.00102097
Iteration 13/25 | Loss: 0.00102097
Iteration 14/25 | Loss: 0.00102097
Iteration 15/25 | Loss: 0.00102097
Iteration 16/25 | Loss: 0.00102097
Iteration 17/25 | Loss: 0.00102097
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010209697065874934, 0.0010209697065874934, 0.0010209697065874934, 0.0010209697065874934, 0.0010209697065874934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010209697065874934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102097
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001726
Iteration 4/1000 | Loss: 0.00001553
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001345
Iteration 10/1000 | Loss: 0.00001325
Iteration 11/1000 | Loss: 0.00001309
Iteration 12/1000 | Loss: 0.00001300
Iteration 13/1000 | Loss: 0.00001292
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001284
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001280
Iteration 21/1000 | Loss: 0.00001280
Iteration 22/1000 | Loss: 0.00001279
Iteration 23/1000 | Loss: 0.00001277
Iteration 24/1000 | Loss: 0.00001277
Iteration 25/1000 | Loss: 0.00001276
Iteration 26/1000 | Loss: 0.00001276
Iteration 27/1000 | Loss: 0.00001276
Iteration 28/1000 | Loss: 0.00001275
Iteration 29/1000 | Loss: 0.00001274
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001271
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001267
Iteration 45/1000 | Loss: 0.00001267
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001265
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001262
Iteration 55/1000 | Loss: 0.00001262
Iteration 56/1000 | Loss: 0.00001262
Iteration 57/1000 | Loss: 0.00001261
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001253
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001246
Iteration 117/1000 | Loss: 0.00001246
Iteration 118/1000 | Loss: 0.00001246
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001245
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001245
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001245
Iteration 133/1000 | Loss: 0.00001245
Iteration 134/1000 | Loss: 0.00001245
Iteration 135/1000 | Loss: 0.00001245
Iteration 136/1000 | Loss: 0.00001245
Iteration 137/1000 | Loss: 0.00001245
Iteration 138/1000 | Loss: 0.00001245
Iteration 139/1000 | Loss: 0.00001245
Iteration 140/1000 | Loss: 0.00001245
Iteration 141/1000 | Loss: 0.00001245
Iteration 142/1000 | Loss: 0.00001245
Iteration 143/1000 | Loss: 0.00001245
Iteration 144/1000 | Loss: 0.00001245
Iteration 145/1000 | Loss: 0.00001245
Iteration 146/1000 | Loss: 0.00001245
Iteration 147/1000 | Loss: 0.00001245
Iteration 148/1000 | Loss: 0.00001245
Iteration 149/1000 | Loss: 0.00001245
Iteration 150/1000 | Loss: 0.00001245
Iteration 151/1000 | Loss: 0.00001245
Iteration 152/1000 | Loss: 0.00001245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.245316434506094e-05, 1.245316434506094e-05, 1.245316434506094e-05, 1.245316434506094e-05, 1.245316434506094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.245316434506094e-05

Optimization complete. Final v2v error: 3.004444122314453 mm

Highest mean error: 3.747748374938965 mm for frame 95

Lowest mean error: 2.6976025104522705 mm for frame 154

Saving results

Total time: 38.559131145477295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946064
Iteration 2/25 | Loss: 0.00461990
Iteration 3/25 | Loss: 0.00341989
Iteration 4/25 | Loss: 0.00274279
Iteration 5/25 | Loss: 0.00235950
Iteration 6/25 | Loss: 0.00218212
Iteration 7/25 | Loss: 0.00211030
Iteration 8/25 | Loss: 0.00201913
Iteration 9/25 | Loss: 0.00194622
Iteration 10/25 | Loss: 0.00190455
Iteration 11/25 | Loss: 0.00184610
Iteration 12/25 | Loss: 0.00180027
Iteration 13/25 | Loss: 0.00177528
Iteration 14/25 | Loss: 0.00176429
Iteration 15/25 | Loss: 0.00175971
Iteration 16/25 | Loss: 0.00175293
Iteration 17/25 | Loss: 0.00174668
Iteration 18/25 | Loss: 0.00173634
Iteration 19/25 | Loss: 0.00173309
Iteration 20/25 | Loss: 0.00173255
Iteration 21/25 | Loss: 0.00173206
Iteration 22/25 | Loss: 0.00173435
Iteration 23/25 | Loss: 0.00173315
Iteration 24/25 | Loss: 0.00172896
Iteration 25/25 | Loss: 0.00172833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33006525
Iteration 2/25 | Loss: 0.00531294
Iteration 3/25 | Loss: 0.00531294
Iteration 4/25 | Loss: 0.00528119
Iteration 5/25 | Loss: 0.00528119
Iteration 6/25 | Loss: 0.00528119
Iteration 7/25 | Loss: 0.00528119
Iteration 8/25 | Loss: 0.00528119
Iteration 9/25 | Loss: 0.00528119
Iteration 10/25 | Loss: 0.00528118
Iteration 11/25 | Loss: 0.00528118
Iteration 12/25 | Loss: 0.00528118
Iteration 13/25 | Loss: 0.00528118
Iteration 14/25 | Loss: 0.00528118
Iteration 15/25 | Loss: 0.00528118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005281184334307909, 0.005281184334307909, 0.005281184334307909, 0.005281184334307909, 0.005281184334307909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005281184334307909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00528118
Iteration 2/1000 | Loss: 0.00068999
Iteration 3/1000 | Loss: 0.00056219
Iteration 4/1000 | Loss: 0.00063707
Iteration 5/1000 | Loss: 0.00043957
Iteration 6/1000 | Loss: 0.00032796
Iteration 7/1000 | Loss: 0.00029356
Iteration 8/1000 | Loss: 0.00027237
Iteration 9/1000 | Loss: 0.00025130
Iteration 10/1000 | Loss: 0.00023997
Iteration 11/1000 | Loss: 0.00023208
Iteration 12/1000 | Loss: 0.00022620
Iteration 13/1000 | Loss: 0.00022076
Iteration 14/1000 | Loss: 0.00021686
Iteration 15/1000 | Loss: 0.00065145
Iteration 16/1000 | Loss: 0.00457505
Iteration 17/1000 | Loss: 0.01368977
Iteration 18/1000 | Loss: 0.00052919
Iteration 19/1000 | Loss: 0.00031122
Iteration 20/1000 | Loss: 0.00023973
Iteration 21/1000 | Loss: 0.00018372
Iteration 22/1000 | Loss: 0.00014423
Iteration 23/1000 | Loss: 0.00012087
Iteration 24/1000 | Loss: 0.00010815
Iteration 25/1000 | Loss: 0.00009895
Iteration 26/1000 | Loss: 0.00009205
Iteration 27/1000 | Loss: 0.00008684
Iteration 28/1000 | Loss: 0.00008281
Iteration 29/1000 | Loss: 0.00008014
Iteration 30/1000 | Loss: 0.00007775
Iteration 31/1000 | Loss: 0.00007559
Iteration 32/1000 | Loss: 0.00007334
Iteration 33/1000 | Loss: 0.00007141
Iteration 34/1000 | Loss: 0.00006962
Iteration 35/1000 | Loss: 0.00006819
Iteration 36/1000 | Loss: 0.00006686
Iteration 37/1000 | Loss: 0.00006567
Iteration 38/1000 | Loss: 0.00006476
Iteration 39/1000 | Loss: 0.00006395
Iteration 40/1000 | Loss: 0.00006343
Iteration 41/1000 | Loss: 0.00006286
Iteration 42/1000 | Loss: 0.00006245
Iteration 43/1000 | Loss: 0.00006206
Iteration 44/1000 | Loss: 0.00006167
Iteration 45/1000 | Loss: 0.00006136
Iteration 46/1000 | Loss: 0.00006116
Iteration 47/1000 | Loss: 0.00006111
Iteration 48/1000 | Loss: 0.00006101
Iteration 49/1000 | Loss: 0.00006101
Iteration 50/1000 | Loss: 0.00006093
Iteration 51/1000 | Loss: 0.00006089
Iteration 52/1000 | Loss: 0.00006076
Iteration 53/1000 | Loss: 0.00006074
Iteration 54/1000 | Loss: 0.00006073
Iteration 55/1000 | Loss: 0.00006070
Iteration 56/1000 | Loss: 0.00006064
Iteration 57/1000 | Loss: 0.00006060
Iteration 58/1000 | Loss: 0.00006060
Iteration 59/1000 | Loss: 0.00006059
Iteration 60/1000 | Loss: 0.00006058
Iteration 61/1000 | Loss: 0.00006057
Iteration 62/1000 | Loss: 0.00006056
Iteration 63/1000 | Loss: 0.00006056
Iteration 64/1000 | Loss: 0.00006056
Iteration 65/1000 | Loss: 0.00006055
Iteration 66/1000 | Loss: 0.00006054
Iteration 67/1000 | Loss: 0.00006053
Iteration 68/1000 | Loss: 0.00006051
Iteration 69/1000 | Loss: 0.00006050
Iteration 70/1000 | Loss: 0.00006050
Iteration 71/1000 | Loss: 0.00006050
Iteration 72/1000 | Loss: 0.00006050
Iteration 73/1000 | Loss: 0.00006050
Iteration 74/1000 | Loss: 0.00006050
Iteration 75/1000 | Loss: 0.00006050
Iteration 76/1000 | Loss: 0.00006049
Iteration 77/1000 | Loss: 0.00006049
Iteration 78/1000 | Loss: 0.00006049
Iteration 79/1000 | Loss: 0.00006049
Iteration 80/1000 | Loss: 0.00006049
Iteration 81/1000 | Loss: 0.00006048
Iteration 82/1000 | Loss: 0.00006047
Iteration 83/1000 | Loss: 0.00006047
Iteration 84/1000 | Loss: 0.00006047
Iteration 85/1000 | Loss: 0.00006047
Iteration 86/1000 | Loss: 0.00006047
Iteration 87/1000 | Loss: 0.00006047
Iteration 88/1000 | Loss: 0.00006047
Iteration 89/1000 | Loss: 0.00006046
Iteration 90/1000 | Loss: 0.00006046
Iteration 91/1000 | Loss: 0.00006045
Iteration 92/1000 | Loss: 0.00006045
Iteration 93/1000 | Loss: 0.00006045
Iteration 94/1000 | Loss: 0.00006045
Iteration 95/1000 | Loss: 0.00006045
Iteration 96/1000 | Loss: 0.00006044
Iteration 97/1000 | Loss: 0.00006044
Iteration 98/1000 | Loss: 0.00006044
Iteration 99/1000 | Loss: 0.00006044
Iteration 100/1000 | Loss: 0.00006043
Iteration 101/1000 | Loss: 0.00006043
Iteration 102/1000 | Loss: 0.00006043
Iteration 103/1000 | Loss: 0.00006043
Iteration 104/1000 | Loss: 0.00006042
Iteration 105/1000 | Loss: 0.00006042
Iteration 106/1000 | Loss: 0.00006042
Iteration 107/1000 | Loss: 0.00006041
Iteration 108/1000 | Loss: 0.00006041
Iteration 109/1000 | Loss: 0.00006041
Iteration 110/1000 | Loss: 0.00006041
Iteration 111/1000 | Loss: 0.00006041
Iteration 112/1000 | Loss: 0.00006041
Iteration 113/1000 | Loss: 0.00006041
Iteration 114/1000 | Loss: 0.00006040
Iteration 115/1000 | Loss: 0.00006040
Iteration 116/1000 | Loss: 0.00006040
Iteration 117/1000 | Loss: 0.00006040
Iteration 118/1000 | Loss: 0.00006040
Iteration 119/1000 | Loss: 0.00006040
Iteration 120/1000 | Loss: 0.00006040
Iteration 121/1000 | Loss: 0.00006039
Iteration 122/1000 | Loss: 0.00006039
Iteration 123/1000 | Loss: 0.00006039
Iteration 124/1000 | Loss: 0.00006039
Iteration 125/1000 | Loss: 0.00006039
Iteration 126/1000 | Loss: 0.00006039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [6.039492291165516e-05, 6.039492291165516e-05, 6.039492291165516e-05, 6.039492291165516e-05, 6.039492291165516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.039492291165516e-05

Optimization complete. Final v2v error: 4.535995960235596 mm

Highest mean error: 10.779516220092773 mm for frame 220

Lowest mean error: 3.5674798488616943 mm for frame 233

Saving results

Total time: 139.37461948394775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804735
Iteration 2/25 | Loss: 0.00145261
Iteration 3/25 | Loss: 0.00125018
Iteration 4/25 | Loss: 0.00123067
Iteration 5/25 | Loss: 0.00122840
Iteration 6/25 | Loss: 0.00122781
Iteration 7/25 | Loss: 0.00122745
Iteration 8/25 | Loss: 0.00122723
Iteration 9/25 | Loss: 0.00122710
Iteration 10/25 | Loss: 0.00122697
Iteration 11/25 | Loss: 0.00123110
Iteration 12/25 | Loss: 0.00122898
Iteration 13/25 | Loss: 0.00122925
Iteration 14/25 | Loss: 0.00122402
Iteration 15/25 | Loss: 0.00122293
Iteration 16/25 | Loss: 0.00122234
Iteration 17/25 | Loss: 0.00122217
Iteration 18/25 | Loss: 0.00122196
Iteration 19/25 | Loss: 0.00122187
Iteration 20/25 | Loss: 0.00122187
Iteration 21/25 | Loss: 0.00122186
Iteration 22/25 | Loss: 0.00122186
Iteration 23/25 | Loss: 0.00122186
Iteration 24/25 | Loss: 0.00122186
Iteration 25/25 | Loss: 0.00122186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35380054
Iteration 2/25 | Loss: 0.00101298
Iteration 3/25 | Loss: 0.00101298
Iteration 4/25 | Loss: 0.00101297
Iteration 5/25 | Loss: 0.00101297
Iteration 6/25 | Loss: 0.00101297
Iteration 7/25 | Loss: 0.00101297
Iteration 8/25 | Loss: 0.00101297
Iteration 9/25 | Loss: 0.00101297
Iteration 10/25 | Loss: 0.00101297
Iteration 11/25 | Loss: 0.00101297
Iteration 12/25 | Loss: 0.00101297
Iteration 13/25 | Loss: 0.00101297
Iteration 14/25 | Loss: 0.00101297
Iteration 15/25 | Loss: 0.00101297
Iteration 16/25 | Loss: 0.00101297
Iteration 17/25 | Loss: 0.00101297
Iteration 18/25 | Loss: 0.00101297
Iteration 19/25 | Loss: 0.00101297
Iteration 20/25 | Loss: 0.00101297
Iteration 21/25 | Loss: 0.00101297
Iteration 22/25 | Loss: 0.00101297
Iteration 23/25 | Loss: 0.00101297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001012970693409443, 0.001012970693409443, 0.001012970693409443, 0.001012970693409443, 0.001012970693409443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001012970693409443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101297
Iteration 2/1000 | Loss: 0.00002431
Iteration 3/1000 | Loss: 0.00001712
Iteration 4/1000 | Loss: 0.00001439
Iteration 5/1000 | Loss: 0.00001348
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001205
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001141
Iteration 12/1000 | Loss: 0.00001137
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001133
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001130
Iteration 20/1000 | Loss: 0.00001129
Iteration 21/1000 | Loss: 0.00001127
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001124
Iteration 24/1000 | Loss: 0.00001123
Iteration 25/1000 | Loss: 0.00001122
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001120
Iteration 28/1000 | Loss: 0.00001119
Iteration 29/1000 | Loss: 0.00001119
Iteration 30/1000 | Loss: 0.00001118
Iteration 31/1000 | Loss: 0.00001118
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001117
Iteration 34/1000 | Loss: 0.00001117
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001115
Iteration 38/1000 | Loss: 0.00001114
Iteration 39/1000 | Loss: 0.00001114
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001113
Iteration 42/1000 | Loss: 0.00001113
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001109
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001108
Iteration 48/1000 | Loss: 0.00001108
Iteration 49/1000 | Loss: 0.00001108
Iteration 50/1000 | Loss: 0.00001107
Iteration 51/1000 | Loss: 0.00001107
Iteration 52/1000 | Loss: 0.00001107
Iteration 53/1000 | Loss: 0.00001106
Iteration 54/1000 | Loss: 0.00001106
Iteration 55/1000 | Loss: 0.00001106
Iteration 56/1000 | Loss: 0.00001106
Iteration 57/1000 | Loss: 0.00001106
Iteration 58/1000 | Loss: 0.00001105
Iteration 59/1000 | Loss: 0.00001105
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001105
Iteration 63/1000 | Loss: 0.00001105
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001104
Iteration 67/1000 | Loss: 0.00001104
Iteration 68/1000 | Loss: 0.00001104
Iteration 69/1000 | Loss: 0.00001104
Iteration 70/1000 | Loss: 0.00001103
Iteration 71/1000 | Loss: 0.00001103
Iteration 72/1000 | Loss: 0.00001103
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001103
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001102
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001102
Iteration 81/1000 | Loss: 0.00001102
Iteration 82/1000 | Loss: 0.00001102
Iteration 83/1000 | Loss: 0.00001102
Iteration 84/1000 | Loss: 0.00001102
Iteration 85/1000 | Loss: 0.00001102
Iteration 86/1000 | Loss: 0.00001101
Iteration 87/1000 | Loss: 0.00001101
Iteration 88/1000 | Loss: 0.00001101
Iteration 89/1000 | Loss: 0.00001100
Iteration 90/1000 | Loss: 0.00001100
Iteration 91/1000 | Loss: 0.00001100
Iteration 92/1000 | Loss: 0.00001100
Iteration 93/1000 | Loss: 0.00001100
Iteration 94/1000 | Loss: 0.00001100
Iteration 95/1000 | Loss: 0.00001099
Iteration 96/1000 | Loss: 0.00001099
Iteration 97/1000 | Loss: 0.00001099
Iteration 98/1000 | Loss: 0.00001099
Iteration 99/1000 | Loss: 0.00001098
Iteration 100/1000 | Loss: 0.00001098
Iteration 101/1000 | Loss: 0.00001098
Iteration 102/1000 | Loss: 0.00001098
Iteration 103/1000 | Loss: 0.00001098
Iteration 104/1000 | Loss: 0.00001097
Iteration 105/1000 | Loss: 0.00001097
Iteration 106/1000 | Loss: 0.00001097
Iteration 107/1000 | Loss: 0.00001097
Iteration 108/1000 | Loss: 0.00001097
Iteration 109/1000 | Loss: 0.00001096
Iteration 110/1000 | Loss: 0.00001096
Iteration 111/1000 | Loss: 0.00001096
Iteration 112/1000 | Loss: 0.00001095
Iteration 113/1000 | Loss: 0.00001095
Iteration 114/1000 | Loss: 0.00001094
Iteration 115/1000 | Loss: 0.00001094
Iteration 116/1000 | Loss: 0.00001094
Iteration 117/1000 | Loss: 0.00001094
Iteration 118/1000 | Loss: 0.00001094
Iteration 119/1000 | Loss: 0.00001094
Iteration 120/1000 | Loss: 0.00001093
Iteration 121/1000 | Loss: 0.00001093
Iteration 122/1000 | Loss: 0.00001093
Iteration 123/1000 | Loss: 0.00001093
Iteration 124/1000 | Loss: 0.00001093
Iteration 125/1000 | Loss: 0.00001092
Iteration 126/1000 | Loss: 0.00001092
Iteration 127/1000 | Loss: 0.00001092
Iteration 128/1000 | Loss: 0.00001092
Iteration 129/1000 | Loss: 0.00001092
Iteration 130/1000 | Loss: 0.00001092
Iteration 131/1000 | Loss: 0.00001092
Iteration 132/1000 | Loss: 0.00001091
Iteration 133/1000 | Loss: 0.00001091
Iteration 134/1000 | Loss: 0.00001091
Iteration 135/1000 | Loss: 0.00001091
Iteration 136/1000 | Loss: 0.00001091
Iteration 137/1000 | Loss: 0.00001091
Iteration 138/1000 | Loss: 0.00001091
Iteration 139/1000 | Loss: 0.00001091
Iteration 140/1000 | Loss: 0.00001091
Iteration 141/1000 | Loss: 0.00001091
Iteration 142/1000 | Loss: 0.00001091
Iteration 143/1000 | Loss: 0.00001091
Iteration 144/1000 | Loss: 0.00001091
Iteration 145/1000 | Loss: 0.00001091
Iteration 146/1000 | Loss: 0.00001091
Iteration 147/1000 | Loss: 0.00001091
Iteration 148/1000 | Loss: 0.00001091
Iteration 149/1000 | Loss: 0.00001091
Iteration 150/1000 | Loss: 0.00001091
Iteration 151/1000 | Loss: 0.00001091
Iteration 152/1000 | Loss: 0.00001091
Iteration 153/1000 | Loss: 0.00001091
Iteration 154/1000 | Loss: 0.00001091
Iteration 155/1000 | Loss: 0.00001091
Iteration 156/1000 | Loss: 0.00001091
Iteration 157/1000 | Loss: 0.00001091
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.091077683668118e-05, 1.091077683668118e-05, 1.091077683668118e-05, 1.091077683668118e-05, 1.091077683668118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.091077683668118e-05

Optimization complete. Final v2v error: 2.847773313522339 mm

Highest mean error: 3.1226003170013428 mm for frame 97

Lowest mean error: 2.7283904552459717 mm for frame 7

Saving results

Total time: 60.69598150253296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827887
Iteration 2/25 | Loss: 0.00242783
Iteration 3/25 | Loss: 0.00193361
Iteration 4/25 | Loss: 0.00180139
Iteration 5/25 | Loss: 0.00165618
Iteration 6/25 | Loss: 0.00147304
Iteration 7/25 | Loss: 0.00143556
Iteration 8/25 | Loss: 0.00142306
Iteration 9/25 | Loss: 0.00141642
Iteration 10/25 | Loss: 0.00140986
Iteration 11/25 | Loss: 0.00140835
Iteration 12/25 | Loss: 0.00140821
Iteration 13/25 | Loss: 0.00141087
Iteration 14/25 | Loss: 0.00140880
Iteration 15/25 | Loss: 0.00140349
Iteration 16/25 | Loss: 0.00140085
Iteration 17/25 | Loss: 0.00139953
Iteration 18/25 | Loss: 0.00139675
Iteration 19/25 | Loss: 0.00139601
Iteration 20/25 | Loss: 0.00139582
Iteration 21/25 | Loss: 0.00139517
Iteration 22/25 | Loss: 0.00139541
Iteration 23/25 | Loss: 0.00139582
Iteration 24/25 | Loss: 0.00139652
Iteration 25/25 | Loss: 0.00139554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34431839
Iteration 2/25 | Loss: 0.00079597
Iteration 3/25 | Loss: 0.00079595
Iteration 4/25 | Loss: 0.00079595
Iteration 5/25 | Loss: 0.00079595
Iteration 6/25 | Loss: 0.00079595
Iteration 7/25 | Loss: 0.00079595
Iteration 8/25 | Loss: 0.00079595
Iteration 9/25 | Loss: 0.00079595
Iteration 10/25 | Loss: 0.00079595
Iteration 11/25 | Loss: 0.00079595
Iteration 12/25 | Loss: 0.00079595
Iteration 13/25 | Loss: 0.00079595
Iteration 14/25 | Loss: 0.00079595
Iteration 15/25 | Loss: 0.00079595
Iteration 16/25 | Loss: 0.00079595
Iteration 17/25 | Loss: 0.00079595
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007959482609294355, 0.0007959482609294355, 0.0007959482609294355, 0.0007959482609294355, 0.0007959482609294355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007959482609294355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079595
Iteration 2/1000 | Loss: 0.00004146
Iteration 3/1000 | Loss: 0.00003972
Iteration 4/1000 | Loss: 0.00002684
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002808
Iteration 7/1000 | Loss: 0.00003005
Iteration 8/1000 | Loss: 0.00002765
Iteration 9/1000 | Loss: 0.00002684
Iteration 10/1000 | Loss: 0.00003953
Iteration 11/1000 | Loss: 0.00003495
Iteration 12/1000 | Loss: 0.00003600
Iteration 13/1000 | Loss: 0.00003100
Iteration 14/1000 | Loss: 0.00003616
Iteration 15/1000 | Loss: 0.00002484
Iteration 16/1000 | Loss: 0.00002384
Iteration 17/1000 | Loss: 0.00003406
Iteration 18/1000 | Loss: 0.00003299
Iteration 19/1000 | Loss: 0.00003400
Iteration 20/1000 | Loss: 0.00003359
Iteration 21/1000 | Loss: 0.00003420
Iteration 22/1000 | Loss: 0.00002969
Iteration 23/1000 | Loss: 0.00002486
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002294
Iteration 26/1000 | Loss: 0.00002248
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002203
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002199
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002173
Iteration 34/1000 | Loss: 0.00002154
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002122
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002116
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002110
Iteration 43/1000 | Loss: 0.00002102
Iteration 44/1000 | Loss: 0.00002099
Iteration 45/1000 | Loss: 0.00002099
Iteration 46/1000 | Loss: 0.00002099
Iteration 47/1000 | Loss: 0.00002098
Iteration 48/1000 | Loss: 0.00002098
Iteration 49/1000 | Loss: 0.00002097
Iteration 50/1000 | Loss: 0.00002097
Iteration 51/1000 | Loss: 0.00002097
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002096
Iteration 54/1000 | Loss: 0.00002095
Iteration 55/1000 | Loss: 0.00002095
Iteration 56/1000 | Loss: 0.00002095
Iteration 57/1000 | Loss: 0.00002095
Iteration 58/1000 | Loss: 0.00002094
Iteration 59/1000 | Loss: 0.00002094
Iteration 60/1000 | Loss: 0.00002094
Iteration 61/1000 | Loss: 0.00002094
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00002092
Iteration 66/1000 | Loss: 0.00002092
Iteration 67/1000 | Loss: 0.00002092
Iteration 68/1000 | Loss: 0.00002092
Iteration 69/1000 | Loss: 0.00002092
Iteration 70/1000 | Loss: 0.00002092
Iteration 71/1000 | Loss: 0.00002092
Iteration 72/1000 | Loss: 0.00002092
Iteration 73/1000 | Loss: 0.00002092
Iteration 74/1000 | Loss: 0.00002092
Iteration 75/1000 | Loss: 0.00002092
Iteration 76/1000 | Loss: 0.00002092
Iteration 77/1000 | Loss: 0.00002092
Iteration 78/1000 | Loss: 0.00002092
Iteration 79/1000 | Loss: 0.00002092
Iteration 80/1000 | Loss: 0.00002092
Iteration 81/1000 | Loss: 0.00002092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.0915591449011117e-05, 2.0915591449011117e-05, 2.0915591449011117e-05, 2.0915591449011117e-05, 2.0915591449011117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0915591449011117e-05

Optimization complete. Final v2v error: 3.8572440147399902 mm

Highest mean error: 5.320217609405518 mm for frame 197

Lowest mean error: 3.631211757659912 mm for frame 43

Saving results

Total time: 109.77036380767822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826649
Iteration 2/25 | Loss: 0.00137150
Iteration 3/25 | Loss: 0.00123228
Iteration 4/25 | Loss: 0.00120140
Iteration 5/25 | Loss: 0.00119959
Iteration 6/25 | Loss: 0.00119612
Iteration 7/25 | Loss: 0.00119506
Iteration 8/25 | Loss: 0.00119529
Iteration 9/25 | Loss: 0.00119413
Iteration 10/25 | Loss: 0.00119440
Iteration 11/25 | Loss: 0.00119411
Iteration 12/25 | Loss: 0.00119401
Iteration 13/25 | Loss: 0.00119401
Iteration 14/25 | Loss: 0.00119401
Iteration 15/25 | Loss: 0.00119401
Iteration 16/25 | Loss: 0.00119401
Iteration 17/25 | Loss: 0.00119401
Iteration 18/25 | Loss: 0.00119400
Iteration 19/25 | Loss: 0.00119400
Iteration 20/25 | Loss: 0.00119400
Iteration 21/25 | Loss: 0.00119400
Iteration 22/25 | Loss: 0.00119400
Iteration 23/25 | Loss: 0.00119400
Iteration 24/25 | Loss: 0.00119400
Iteration 25/25 | Loss: 0.00119405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71752357
Iteration 2/25 | Loss: 0.00102801
Iteration 3/25 | Loss: 0.00102800
Iteration 4/25 | Loss: 0.00102800
Iteration 5/25 | Loss: 0.00102800
Iteration 6/25 | Loss: 0.00102800
Iteration 7/25 | Loss: 0.00102800
Iteration 8/25 | Loss: 0.00102800
Iteration 9/25 | Loss: 0.00102800
Iteration 10/25 | Loss: 0.00102800
Iteration 11/25 | Loss: 0.00102800
Iteration 12/25 | Loss: 0.00102800
Iteration 13/25 | Loss: 0.00102800
Iteration 14/25 | Loss: 0.00102800
Iteration 15/25 | Loss: 0.00102800
Iteration 16/25 | Loss: 0.00102800
Iteration 17/25 | Loss: 0.00102800
Iteration 18/25 | Loss: 0.00102800
Iteration 19/25 | Loss: 0.00102800
Iteration 20/25 | Loss: 0.00102800
Iteration 21/25 | Loss: 0.00102800
Iteration 22/25 | Loss: 0.00102800
Iteration 23/25 | Loss: 0.00102800
Iteration 24/25 | Loss: 0.00102800
Iteration 25/25 | Loss: 0.00102800

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102800
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00001499
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001379
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001212
Iteration 9/1000 | Loss: 0.00001183
Iteration 10/1000 | Loss: 0.00001161
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001135
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001122
Iteration 17/1000 | Loss: 0.00001121
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001119
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001117
Iteration 23/1000 | Loss: 0.00001111
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001105
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001091
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001086
Iteration 33/1000 | Loss: 0.00001468
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001079
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001078
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001078
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001078
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001074
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001073
Iteration 67/1000 | Loss: 0.00001073
Iteration 68/1000 | Loss: 0.00001073
Iteration 69/1000 | Loss: 0.00001073
Iteration 70/1000 | Loss: 0.00001073
Iteration 71/1000 | Loss: 0.00001073
Iteration 72/1000 | Loss: 0.00001073
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001073
Iteration 79/1000 | Loss: 0.00001073
Iteration 80/1000 | Loss: 0.00001073
Iteration 81/1000 | Loss: 0.00001073
Iteration 82/1000 | Loss: 0.00001073
Iteration 83/1000 | Loss: 0.00001073
Iteration 84/1000 | Loss: 0.00001073
Iteration 85/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.0729210771387443e-05, 1.0729210771387443e-05, 1.0729210771387443e-05, 1.0729210771387443e-05, 1.0729210771387443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0729210771387443e-05

Optimization complete. Final v2v error: 2.834418535232544 mm

Highest mean error: 3.1994824409484863 mm for frame 40

Lowest mean error: 2.6047117710113525 mm for frame 16

Saving results

Total time: 57.39611315727234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859677
Iteration 2/25 | Loss: 0.00182603
Iteration 3/25 | Loss: 0.00140360
Iteration 4/25 | Loss: 0.00132698
Iteration 5/25 | Loss: 0.00138831
Iteration 6/25 | Loss: 0.00129893
Iteration 7/25 | Loss: 0.00126966
Iteration 8/25 | Loss: 0.00125572
Iteration 9/25 | Loss: 0.00124330
Iteration 10/25 | Loss: 0.00124221
Iteration 11/25 | Loss: 0.00124244
Iteration 12/25 | Loss: 0.00124164
Iteration 13/25 | Loss: 0.00124160
Iteration 14/25 | Loss: 0.00124160
Iteration 15/25 | Loss: 0.00124160
Iteration 16/25 | Loss: 0.00124160
Iteration 17/25 | Loss: 0.00124160
Iteration 18/25 | Loss: 0.00124160
Iteration 19/25 | Loss: 0.00124160
Iteration 20/25 | Loss: 0.00124160
Iteration 21/25 | Loss: 0.00124159
Iteration 22/25 | Loss: 0.00124159
Iteration 23/25 | Loss: 0.00124159
Iteration 24/25 | Loss: 0.00124159
Iteration 25/25 | Loss: 0.00124159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43100536
Iteration 2/25 | Loss: 0.00110142
Iteration 3/25 | Loss: 0.00106057
Iteration 4/25 | Loss: 0.00106057
Iteration 5/25 | Loss: 0.00106057
Iteration 6/25 | Loss: 0.00106057
Iteration 7/25 | Loss: 0.00106057
Iteration 8/25 | Loss: 0.00106057
Iteration 9/25 | Loss: 0.00106057
Iteration 10/25 | Loss: 0.00106057
Iteration 11/25 | Loss: 0.00106057
Iteration 12/25 | Loss: 0.00106057
Iteration 13/25 | Loss: 0.00106057
Iteration 14/25 | Loss: 0.00106057
Iteration 15/25 | Loss: 0.00106057
Iteration 16/25 | Loss: 0.00106057
Iteration 17/25 | Loss: 0.00106057
Iteration 18/25 | Loss: 0.00106057
Iteration 19/25 | Loss: 0.00106057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010605669813230634, 0.0010605669813230634, 0.0010605669813230634, 0.0010605669813230634, 0.0010605669813230634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010605669813230634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106057
Iteration 2/1000 | Loss: 0.00003774
Iteration 3/1000 | Loss: 0.00007805
Iteration 4/1000 | Loss: 0.00005372
Iteration 5/1000 | Loss: 0.00007993
Iteration 6/1000 | Loss: 0.00024672
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00011057
Iteration 9/1000 | Loss: 0.00005645
Iteration 10/1000 | Loss: 0.00008220
Iteration 11/1000 | Loss: 0.00001757
Iteration 12/1000 | Loss: 0.00003375
Iteration 13/1000 | Loss: 0.00001696
Iteration 14/1000 | Loss: 0.00001668
Iteration 15/1000 | Loss: 0.00001646
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00006228
Iteration 18/1000 | Loss: 0.00001610
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001588
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00007533
Iteration 31/1000 | Loss: 0.00001585
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001571
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001570
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001568
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001563
Iteration 44/1000 | Loss: 0.00001562
Iteration 45/1000 | Loss: 0.00001561
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001560
Iteration 48/1000 | Loss: 0.00001560
Iteration 49/1000 | Loss: 0.00001560
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00005539
Iteration 57/1000 | Loss: 0.00014030
Iteration 58/1000 | Loss: 0.00061406
Iteration 59/1000 | Loss: 0.00002071
Iteration 60/1000 | Loss: 0.00001565
Iteration 61/1000 | Loss: 0.00001553
Iteration 62/1000 | Loss: 0.00001552
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001545
Iteration 78/1000 | Loss: 0.00001545
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00004879
Iteration 81/1000 | Loss: 0.00004868
Iteration 82/1000 | Loss: 0.00001688
Iteration 83/1000 | Loss: 0.00002868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00001547
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001546
Iteration 92/1000 | Loss: 0.00001546
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001543
Iteration 107/1000 | Loss: 0.00001543
Iteration 108/1000 | Loss: 0.00001543
Iteration 109/1000 | Loss: 0.00001543
Iteration 110/1000 | Loss: 0.00001543
Iteration 111/1000 | Loss: 0.00001543
Iteration 112/1000 | Loss: 0.00001542
Iteration 113/1000 | Loss: 0.00001542
Iteration 114/1000 | Loss: 0.00001542
Iteration 115/1000 | Loss: 0.00001541
Iteration 116/1000 | Loss: 0.00001541
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00004478
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00001538
Iteration 123/1000 | Loss: 0.00001538
Iteration 124/1000 | Loss: 0.00001538
Iteration 125/1000 | Loss: 0.00001538
Iteration 126/1000 | Loss: 0.00001538
Iteration 127/1000 | Loss: 0.00001537
Iteration 128/1000 | Loss: 0.00001537
Iteration 129/1000 | Loss: 0.00001537
Iteration 130/1000 | Loss: 0.00001537
Iteration 131/1000 | Loss: 0.00001536
Iteration 132/1000 | Loss: 0.00001536
Iteration 133/1000 | Loss: 0.00001536
Iteration 134/1000 | Loss: 0.00001536
Iteration 135/1000 | Loss: 0.00001536
Iteration 136/1000 | Loss: 0.00001536
Iteration 137/1000 | Loss: 0.00001536
Iteration 138/1000 | Loss: 0.00001536
Iteration 139/1000 | Loss: 0.00001536
Iteration 140/1000 | Loss: 0.00001536
Iteration 141/1000 | Loss: 0.00001536
Iteration 142/1000 | Loss: 0.00001536
Iteration 143/1000 | Loss: 0.00001536
Iteration 144/1000 | Loss: 0.00001536
Iteration 145/1000 | Loss: 0.00001536
Iteration 146/1000 | Loss: 0.00001536
Iteration 147/1000 | Loss: 0.00001536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.535551382403355e-05, 1.535551382403355e-05, 1.535551382403355e-05, 1.535551382403355e-05, 1.535551382403355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.535551382403355e-05

Optimization complete. Final v2v error: 3.353595495223999 mm

Highest mean error: 4.14207124710083 mm for frame 73

Lowest mean error: 3.0426766872406006 mm for frame 114

Saving results

Total time: 77.85886430740356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583754
Iteration 2/25 | Loss: 0.00132066
Iteration 3/25 | Loss: 0.00125203
Iteration 4/25 | Loss: 0.00123447
Iteration 5/25 | Loss: 0.00122920
Iteration 6/25 | Loss: 0.00122868
Iteration 7/25 | Loss: 0.00122868
Iteration 8/25 | Loss: 0.00122868
Iteration 9/25 | Loss: 0.00122868
Iteration 10/25 | Loss: 0.00122868
Iteration 11/25 | Loss: 0.00122868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012286752462387085, 0.0012286752462387085, 0.0012286752462387085, 0.0012286752462387085, 0.0012286752462387085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012286752462387085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14784384
Iteration 2/25 | Loss: 0.00100868
Iteration 3/25 | Loss: 0.00100868
Iteration 4/25 | Loss: 0.00100868
Iteration 5/25 | Loss: 0.00100868
Iteration 6/25 | Loss: 0.00100868
Iteration 7/25 | Loss: 0.00100868
Iteration 8/25 | Loss: 0.00100868
Iteration 9/25 | Loss: 0.00100868
Iteration 10/25 | Loss: 0.00100868
Iteration 11/25 | Loss: 0.00100868
Iteration 12/25 | Loss: 0.00100868
Iteration 13/25 | Loss: 0.00100868
Iteration 14/25 | Loss: 0.00100868
Iteration 15/25 | Loss: 0.00100868
Iteration 16/25 | Loss: 0.00100868
Iteration 17/25 | Loss: 0.00100868
Iteration 18/25 | Loss: 0.00100868
Iteration 19/25 | Loss: 0.00100868
Iteration 20/25 | Loss: 0.00100868
Iteration 21/25 | Loss: 0.00100868
Iteration 22/25 | Loss: 0.00100868
Iteration 23/25 | Loss: 0.00100868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00100867566652596, 0.00100867566652596, 0.00100867566652596, 0.00100867566652596, 0.00100867566652596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00100867566652596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100868
Iteration 2/1000 | Loss: 0.00002972
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00002110
Iteration 5/1000 | Loss: 0.00002035
Iteration 6/1000 | Loss: 0.00001969
Iteration 7/1000 | Loss: 0.00001918
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001851
Iteration 10/1000 | Loss: 0.00001825
Iteration 11/1000 | Loss: 0.00001804
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001767
Iteration 16/1000 | Loss: 0.00001763
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001760
Iteration 21/1000 | Loss: 0.00001760
Iteration 22/1000 | Loss: 0.00001759
Iteration 23/1000 | Loss: 0.00001759
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001750
Iteration 26/1000 | Loss: 0.00001748
Iteration 27/1000 | Loss: 0.00001748
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001744
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001743
Iteration 35/1000 | Loss: 0.00001743
Iteration 36/1000 | Loss: 0.00001742
Iteration 37/1000 | Loss: 0.00001742
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001739
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001737
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001736
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001733
Iteration 60/1000 | Loss: 0.00001733
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001723
Iteration 122/1000 | Loss: 0.00001723
Iteration 123/1000 | Loss: 0.00001723
Iteration 124/1000 | Loss: 0.00001723
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001722
Iteration 130/1000 | Loss: 0.00001722
Iteration 131/1000 | Loss: 0.00001722
Iteration 132/1000 | Loss: 0.00001722
Iteration 133/1000 | Loss: 0.00001722
Iteration 134/1000 | Loss: 0.00001722
Iteration 135/1000 | Loss: 0.00001722
Iteration 136/1000 | Loss: 0.00001722
Iteration 137/1000 | Loss: 0.00001722
Iteration 138/1000 | Loss: 0.00001722
Iteration 139/1000 | Loss: 0.00001722
Iteration 140/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.721899570839014e-05, 1.721899570839014e-05, 1.721899570839014e-05, 1.721899570839014e-05, 1.721899570839014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.721899570839014e-05

Optimization complete. Final v2v error: 3.540421962738037 mm

Highest mean error: 3.8385088443756104 mm for frame 95

Lowest mean error: 3.3304669857025146 mm for frame 123

Saving results

Total time: 38.170538663864136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367221
Iteration 2/25 | Loss: 0.00124471
Iteration 3/25 | Loss: 0.00118943
Iteration 4/25 | Loss: 0.00118130
Iteration 5/25 | Loss: 0.00117956
Iteration 6/25 | Loss: 0.00117956
Iteration 7/25 | Loss: 0.00117956
Iteration 8/25 | Loss: 0.00117956
Iteration 9/25 | Loss: 0.00117956
Iteration 10/25 | Loss: 0.00117956
Iteration 11/25 | Loss: 0.00117956
Iteration 12/25 | Loss: 0.00117956
Iteration 13/25 | Loss: 0.00117956
Iteration 14/25 | Loss: 0.00117956
Iteration 15/25 | Loss: 0.00117956
Iteration 16/25 | Loss: 0.00117956
Iteration 17/25 | Loss: 0.00117956
Iteration 18/25 | Loss: 0.00117956
Iteration 19/25 | Loss: 0.00117956
Iteration 20/25 | Loss: 0.00117956
Iteration 21/25 | Loss: 0.00117956
Iteration 22/25 | Loss: 0.00117956
Iteration 23/25 | Loss: 0.00117956
Iteration 24/25 | Loss: 0.00117956
Iteration 25/25 | Loss: 0.00117956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62041450
Iteration 2/25 | Loss: 0.00100928
Iteration 3/25 | Loss: 0.00100928
Iteration 4/25 | Loss: 0.00100928
Iteration 5/25 | Loss: 0.00100928
Iteration 6/25 | Loss: 0.00100928
Iteration 7/25 | Loss: 0.00100928
Iteration 8/25 | Loss: 0.00100928
Iteration 9/25 | Loss: 0.00100928
Iteration 10/25 | Loss: 0.00100928
Iteration 11/25 | Loss: 0.00100928
Iteration 12/25 | Loss: 0.00100928
Iteration 13/25 | Loss: 0.00100928
Iteration 14/25 | Loss: 0.00100928
Iteration 15/25 | Loss: 0.00100928
Iteration 16/25 | Loss: 0.00100928
Iteration 17/25 | Loss: 0.00100928
Iteration 18/25 | Loss: 0.00100928
Iteration 19/25 | Loss: 0.00100928
Iteration 20/25 | Loss: 0.00100928
Iteration 21/25 | Loss: 0.00100928
Iteration 22/25 | Loss: 0.00100928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00100928102619946, 0.00100928102619946, 0.00100928102619946, 0.00100928102619946, 0.00100928102619946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00100928102619946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100928
Iteration 2/1000 | Loss: 0.00001659
Iteration 3/1000 | Loss: 0.00001301
Iteration 4/1000 | Loss: 0.00001199
Iteration 5/1000 | Loss: 0.00001146
Iteration 6/1000 | Loss: 0.00001101
Iteration 7/1000 | Loss: 0.00001072
Iteration 8/1000 | Loss: 0.00001038
Iteration 9/1000 | Loss: 0.00001034
Iteration 10/1000 | Loss: 0.00001029
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001011
Iteration 13/1000 | Loss: 0.00001010
Iteration 14/1000 | Loss: 0.00001008
Iteration 15/1000 | Loss: 0.00001008
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001005
Iteration 18/1000 | Loss: 0.00001000
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000992
Iteration 22/1000 | Loss: 0.00000990
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000989
Iteration 25/1000 | Loss: 0.00000989
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000988
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000980
Iteration 32/1000 | Loss: 0.00000976
Iteration 33/1000 | Loss: 0.00000972
Iteration 34/1000 | Loss: 0.00000972
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000962
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000959
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000957
Iteration 44/1000 | Loss: 0.00000957
Iteration 45/1000 | Loss: 0.00000957
Iteration 46/1000 | Loss: 0.00000957
Iteration 47/1000 | Loss: 0.00000957
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000957
Iteration 50/1000 | Loss: 0.00000957
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000956
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000955
Iteration 59/1000 | Loss: 0.00000955
Iteration 60/1000 | Loss: 0.00000954
Iteration 61/1000 | Loss: 0.00000953
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000953
Iteration 65/1000 | Loss: 0.00000953
Iteration 66/1000 | Loss: 0.00000953
Iteration 67/1000 | Loss: 0.00000952
Iteration 68/1000 | Loss: 0.00000952
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000951
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000951
Iteration 73/1000 | Loss: 0.00000951
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000950
Iteration 76/1000 | Loss: 0.00000950
Iteration 77/1000 | Loss: 0.00000950
Iteration 78/1000 | Loss: 0.00000949
Iteration 79/1000 | Loss: 0.00000949
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000948
Iteration 82/1000 | Loss: 0.00000948
Iteration 83/1000 | Loss: 0.00000948
Iteration 84/1000 | Loss: 0.00000947
Iteration 85/1000 | Loss: 0.00000947
Iteration 86/1000 | Loss: 0.00000947
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000946
Iteration 92/1000 | Loss: 0.00000946
Iteration 93/1000 | Loss: 0.00000946
Iteration 94/1000 | Loss: 0.00000946
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000945
Iteration 97/1000 | Loss: 0.00000945
Iteration 98/1000 | Loss: 0.00000945
Iteration 99/1000 | Loss: 0.00000944
Iteration 100/1000 | Loss: 0.00000944
Iteration 101/1000 | Loss: 0.00000944
Iteration 102/1000 | Loss: 0.00000944
Iteration 103/1000 | Loss: 0.00000943
Iteration 104/1000 | Loss: 0.00000943
Iteration 105/1000 | Loss: 0.00000943
Iteration 106/1000 | Loss: 0.00000943
Iteration 107/1000 | Loss: 0.00000943
Iteration 108/1000 | Loss: 0.00000943
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000943
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000943
Iteration 114/1000 | Loss: 0.00000943
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000941
Iteration 118/1000 | Loss: 0.00000941
Iteration 119/1000 | Loss: 0.00000941
Iteration 120/1000 | Loss: 0.00000941
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000938
Iteration 128/1000 | Loss: 0.00000938
Iteration 129/1000 | Loss: 0.00000938
Iteration 130/1000 | Loss: 0.00000937
Iteration 131/1000 | Loss: 0.00000937
Iteration 132/1000 | Loss: 0.00000937
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000935
Iteration 137/1000 | Loss: 0.00000935
Iteration 138/1000 | Loss: 0.00000934
Iteration 139/1000 | Loss: 0.00000934
Iteration 140/1000 | Loss: 0.00000934
Iteration 141/1000 | Loss: 0.00000934
Iteration 142/1000 | Loss: 0.00000934
Iteration 143/1000 | Loss: 0.00000934
Iteration 144/1000 | Loss: 0.00000933
Iteration 145/1000 | Loss: 0.00000933
Iteration 146/1000 | Loss: 0.00000933
Iteration 147/1000 | Loss: 0.00000933
Iteration 148/1000 | Loss: 0.00000933
Iteration 149/1000 | Loss: 0.00000933
Iteration 150/1000 | Loss: 0.00000933
Iteration 151/1000 | Loss: 0.00000933
Iteration 152/1000 | Loss: 0.00000933
Iteration 153/1000 | Loss: 0.00000933
Iteration 154/1000 | Loss: 0.00000933
Iteration 155/1000 | Loss: 0.00000932
Iteration 156/1000 | Loss: 0.00000932
Iteration 157/1000 | Loss: 0.00000932
Iteration 158/1000 | Loss: 0.00000932
Iteration 159/1000 | Loss: 0.00000932
Iteration 160/1000 | Loss: 0.00000932
Iteration 161/1000 | Loss: 0.00000932
Iteration 162/1000 | Loss: 0.00000932
Iteration 163/1000 | Loss: 0.00000932
Iteration 164/1000 | Loss: 0.00000932
Iteration 165/1000 | Loss: 0.00000932
Iteration 166/1000 | Loss: 0.00000932
Iteration 167/1000 | Loss: 0.00000932
Iteration 168/1000 | Loss: 0.00000932
Iteration 169/1000 | Loss: 0.00000932
Iteration 170/1000 | Loss: 0.00000932
Iteration 171/1000 | Loss: 0.00000932
Iteration 172/1000 | Loss: 0.00000932
Iteration 173/1000 | Loss: 0.00000932
Iteration 174/1000 | Loss: 0.00000931
Iteration 175/1000 | Loss: 0.00000931
Iteration 176/1000 | Loss: 0.00000931
Iteration 177/1000 | Loss: 0.00000931
Iteration 178/1000 | Loss: 0.00000931
Iteration 179/1000 | Loss: 0.00000931
Iteration 180/1000 | Loss: 0.00000931
Iteration 181/1000 | Loss: 0.00000931
Iteration 182/1000 | Loss: 0.00000931
Iteration 183/1000 | Loss: 0.00000931
Iteration 184/1000 | Loss: 0.00000931
Iteration 185/1000 | Loss: 0.00000931
Iteration 186/1000 | Loss: 0.00000931
Iteration 187/1000 | Loss: 0.00000931
Iteration 188/1000 | Loss: 0.00000931
Iteration 189/1000 | Loss: 0.00000931
Iteration 190/1000 | Loss: 0.00000930
Iteration 191/1000 | Loss: 0.00000930
Iteration 192/1000 | Loss: 0.00000930
Iteration 193/1000 | Loss: 0.00000930
Iteration 194/1000 | Loss: 0.00000930
Iteration 195/1000 | Loss: 0.00000930
Iteration 196/1000 | Loss: 0.00000930
Iteration 197/1000 | Loss: 0.00000930
Iteration 198/1000 | Loss: 0.00000930
Iteration 199/1000 | Loss: 0.00000930
Iteration 200/1000 | Loss: 0.00000930
Iteration 201/1000 | Loss: 0.00000930
Iteration 202/1000 | Loss: 0.00000930
Iteration 203/1000 | Loss: 0.00000930
Iteration 204/1000 | Loss: 0.00000930
Iteration 205/1000 | Loss: 0.00000930
Iteration 206/1000 | Loss: 0.00000930
Iteration 207/1000 | Loss: 0.00000930
Iteration 208/1000 | Loss: 0.00000930
Iteration 209/1000 | Loss: 0.00000930
Iteration 210/1000 | Loss: 0.00000930
Iteration 211/1000 | Loss: 0.00000930
Iteration 212/1000 | Loss: 0.00000930
Iteration 213/1000 | Loss: 0.00000930
Iteration 214/1000 | Loss: 0.00000930
Iteration 215/1000 | Loss: 0.00000930
Iteration 216/1000 | Loss: 0.00000930
Iteration 217/1000 | Loss: 0.00000930
Iteration 218/1000 | Loss: 0.00000930
Iteration 219/1000 | Loss: 0.00000930
Iteration 220/1000 | Loss: 0.00000930
Iteration 221/1000 | Loss: 0.00000930
Iteration 222/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [9.297882570535876e-06, 9.297882570535876e-06, 9.297882570535876e-06, 9.297882570535876e-06, 9.297882570535876e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.297882570535876e-06

Optimization complete. Final v2v error: 2.635411262512207 mm

Highest mean error: 3.0413765907287598 mm for frame 136

Lowest mean error: 2.544617176055908 mm for frame 7

Saving results

Total time: 46.11869668960571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835147
Iteration 2/25 | Loss: 0.00176098
Iteration 3/25 | Loss: 0.00154012
Iteration 4/25 | Loss: 0.00151339
Iteration 5/25 | Loss: 0.00150558
Iteration 6/25 | Loss: 0.00151287
Iteration 7/25 | Loss: 0.00150789
Iteration 8/25 | Loss: 0.00150508
Iteration 9/25 | Loss: 0.00150898
Iteration 10/25 | Loss: 0.00150066
Iteration 11/25 | Loss: 0.00149843
Iteration 12/25 | Loss: 0.00149565
Iteration 13/25 | Loss: 0.00148777
Iteration 14/25 | Loss: 0.00148389
Iteration 15/25 | Loss: 0.00148242
Iteration 16/25 | Loss: 0.00148699
Iteration 17/25 | Loss: 0.00148483
Iteration 18/25 | Loss: 0.00148129
Iteration 19/25 | Loss: 0.00147789
Iteration 20/25 | Loss: 0.00147199
Iteration 21/25 | Loss: 0.00147110
Iteration 22/25 | Loss: 0.00147096
Iteration 23/25 | Loss: 0.00147092
Iteration 24/25 | Loss: 0.00147091
Iteration 25/25 | Loss: 0.00147091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27177238
Iteration 2/25 | Loss: 0.00217514
Iteration 3/25 | Loss: 0.00217507
Iteration 4/25 | Loss: 0.00217507
Iteration 5/25 | Loss: 0.00217507
Iteration 6/25 | Loss: 0.00217507
Iteration 7/25 | Loss: 0.00217507
Iteration 8/25 | Loss: 0.00217507
Iteration 9/25 | Loss: 0.00217507
Iteration 10/25 | Loss: 0.00217507
Iteration 11/25 | Loss: 0.00217507
Iteration 12/25 | Loss: 0.00217507
Iteration 13/25 | Loss: 0.00217507
Iteration 14/25 | Loss: 0.00217507
Iteration 15/25 | Loss: 0.00217507
Iteration 16/25 | Loss: 0.00217507
Iteration 17/25 | Loss: 0.00217507
Iteration 18/25 | Loss: 0.00217507
Iteration 19/25 | Loss: 0.00217507
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0021750701125711203, 0.0021750701125711203, 0.0021750701125711203, 0.0021750701125711203, 0.0021750701125711203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021750701125711203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00217507
Iteration 2/1000 | Loss: 0.00033925
Iteration 3/1000 | Loss: 0.00023127
Iteration 4/1000 | Loss: 0.00019653
Iteration 5/1000 | Loss: 0.00017045
Iteration 6/1000 | Loss: 0.00072293
Iteration 7/1000 | Loss: 0.00058042
Iteration 8/1000 | Loss: 0.00015010
Iteration 9/1000 | Loss: 0.00013530
Iteration 10/1000 | Loss: 0.00012775
Iteration 11/1000 | Loss: 0.00012412
Iteration 12/1000 | Loss: 0.00011988
Iteration 13/1000 | Loss: 0.00089662
Iteration 14/1000 | Loss: 0.00053410
Iteration 15/1000 | Loss: 0.00061539
Iteration 16/1000 | Loss: 0.00012048
Iteration 17/1000 | Loss: 0.00011607
Iteration 18/1000 | Loss: 0.00011287
Iteration 19/1000 | Loss: 0.00010931
Iteration 20/1000 | Loss: 0.00010721
Iteration 21/1000 | Loss: 0.00010582
Iteration 22/1000 | Loss: 0.00010470
Iteration 23/1000 | Loss: 0.00010354
Iteration 24/1000 | Loss: 0.00010276
Iteration 25/1000 | Loss: 0.00010200
Iteration 26/1000 | Loss: 0.00010132
Iteration 27/1000 | Loss: 0.00010074
Iteration 28/1000 | Loss: 0.00010019
Iteration 29/1000 | Loss: 0.00009984
Iteration 30/1000 | Loss: 0.00009948
Iteration 31/1000 | Loss: 0.00009912
Iteration 32/1000 | Loss: 0.00088620
Iteration 33/1000 | Loss: 0.00361407
Iteration 34/1000 | Loss: 0.00491294
Iteration 35/1000 | Loss: 0.00091647
Iteration 36/1000 | Loss: 0.00063713
Iteration 37/1000 | Loss: 0.00010042
Iteration 38/1000 | Loss: 0.00008890
Iteration 39/1000 | Loss: 0.00008055
Iteration 40/1000 | Loss: 0.00073387
Iteration 41/1000 | Loss: 0.00106306
Iteration 42/1000 | Loss: 0.00010271
Iteration 43/1000 | Loss: 0.00007728
Iteration 44/1000 | Loss: 0.00007060
Iteration 45/1000 | Loss: 0.00061629
Iteration 46/1000 | Loss: 0.00006724
Iteration 47/1000 | Loss: 0.00006222
Iteration 48/1000 | Loss: 0.00005884
Iteration 49/1000 | Loss: 0.00005648
Iteration 50/1000 | Loss: 0.00005447
Iteration 51/1000 | Loss: 0.00005338
Iteration 52/1000 | Loss: 0.00005273
Iteration 53/1000 | Loss: 0.00005203
Iteration 54/1000 | Loss: 0.00005140
Iteration 55/1000 | Loss: 0.00062264
Iteration 56/1000 | Loss: 0.00015406
Iteration 57/1000 | Loss: 0.00008021
Iteration 58/1000 | Loss: 0.00005966
Iteration 59/1000 | Loss: 0.00005347
Iteration 60/1000 | Loss: 0.00005085
Iteration 61/1000 | Loss: 0.00004958
Iteration 62/1000 | Loss: 0.00004878
Iteration 63/1000 | Loss: 0.00004805
Iteration 64/1000 | Loss: 0.00004743
Iteration 65/1000 | Loss: 0.00004713
Iteration 66/1000 | Loss: 0.00004690
Iteration 67/1000 | Loss: 0.00004662
Iteration 68/1000 | Loss: 0.00004651
Iteration 69/1000 | Loss: 0.00004650
Iteration 70/1000 | Loss: 0.00004646
Iteration 71/1000 | Loss: 0.00004639
Iteration 72/1000 | Loss: 0.00004622
Iteration 73/1000 | Loss: 0.00004620
Iteration 74/1000 | Loss: 0.00004620
Iteration 75/1000 | Loss: 0.00004619
Iteration 76/1000 | Loss: 0.00004619
Iteration 77/1000 | Loss: 0.00004618
Iteration 78/1000 | Loss: 0.00004618
Iteration 79/1000 | Loss: 0.00004617
Iteration 80/1000 | Loss: 0.00004617
Iteration 81/1000 | Loss: 0.00004617
Iteration 82/1000 | Loss: 0.00004616
Iteration 83/1000 | Loss: 0.00004616
Iteration 84/1000 | Loss: 0.00004615
Iteration 85/1000 | Loss: 0.00004615
Iteration 86/1000 | Loss: 0.00004615
Iteration 87/1000 | Loss: 0.00004615
Iteration 88/1000 | Loss: 0.00004615
Iteration 89/1000 | Loss: 0.00004615
Iteration 90/1000 | Loss: 0.00004615
Iteration 91/1000 | Loss: 0.00004614
Iteration 92/1000 | Loss: 0.00004614
Iteration 93/1000 | Loss: 0.00004614
Iteration 94/1000 | Loss: 0.00004613
Iteration 95/1000 | Loss: 0.00064464
Iteration 96/1000 | Loss: 0.00005139
Iteration 97/1000 | Loss: 0.00004726
Iteration 98/1000 | Loss: 0.00004578
Iteration 99/1000 | Loss: 0.00004505
Iteration 100/1000 | Loss: 0.00004438
Iteration 101/1000 | Loss: 0.00004381
Iteration 102/1000 | Loss: 0.00004361
Iteration 103/1000 | Loss: 0.00004341
Iteration 104/1000 | Loss: 0.00004329
Iteration 105/1000 | Loss: 0.00004321
Iteration 106/1000 | Loss: 0.00073732
Iteration 107/1000 | Loss: 0.00004595
Iteration 108/1000 | Loss: 0.00004344
Iteration 109/1000 | Loss: 0.00004246
Iteration 110/1000 | Loss: 0.00004177
Iteration 111/1000 | Loss: 0.00004113
Iteration 112/1000 | Loss: 0.00004076
Iteration 113/1000 | Loss: 0.00004053
Iteration 114/1000 | Loss: 0.00069147
Iteration 115/1000 | Loss: 0.00004325
Iteration 116/1000 | Loss: 0.00004053
Iteration 117/1000 | Loss: 0.00003963
Iteration 118/1000 | Loss: 0.00003894
Iteration 119/1000 | Loss: 0.00003832
Iteration 120/1000 | Loss: 0.00003792
Iteration 121/1000 | Loss: 0.00003770
Iteration 122/1000 | Loss: 0.00003764
Iteration 123/1000 | Loss: 0.00003753
Iteration 124/1000 | Loss: 0.00003739
Iteration 125/1000 | Loss: 0.00003738
Iteration 126/1000 | Loss: 0.00003737
Iteration 127/1000 | Loss: 0.00003734
Iteration 128/1000 | Loss: 0.00003733
Iteration 129/1000 | Loss: 0.00003732
Iteration 130/1000 | Loss: 0.00003732
Iteration 131/1000 | Loss: 0.00003732
Iteration 132/1000 | Loss: 0.00003732
Iteration 133/1000 | Loss: 0.00003731
Iteration 134/1000 | Loss: 0.00003731
Iteration 135/1000 | Loss: 0.00003731
Iteration 136/1000 | Loss: 0.00003730
Iteration 137/1000 | Loss: 0.00003729
Iteration 138/1000 | Loss: 0.00003729
Iteration 139/1000 | Loss: 0.00003729
Iteration 140/1000 | Loss: 0.00003728
Iteration 141/1000 | Loss: 0.00003728
Iteration 142/1000 | Loss: 0.00003728
Iteration 143/1000 | Loss: 0.00003728
Iteration 144/1000 | Loss: 0.00003728
Iteration 145/1000 | Loss: 0.00003728
Iteration 146/1000 | Loss: 0.00003727
Iteration 147/1000 | Loss: 0.00003727
Iteration 148/1000 | Loss: 0.00003727
Iteration 149/1000 | Loss: 0.00003726
Iteration 150/1000 | Loss: 0.00003726
Iteration 151/1000 | Loss: 0.00003726
Iteration 152/1000 | Loss: 0.00003725
Iteration 153/1000 | Loss: 0.00003725
Iteration 154/1000 | Loss: 0.00003725
Iteration 155/1000 | Loss: 0.00003725
Iteration 156/1000 | Loss: 0.00003725
Iteration 157/1000 | Loss: 0.00003725
Iteration 158/1000 | Loss: 0.00003724
Iteration 159/1000 | Loss: 0.00003724
Iteration 160/1000 | Loss: 0.00003724
Iteration 161/1000 | Loss: 0.00003724
Iteration 162/1000 | Loss: 0.00003724
Iteration 163/1000 | Loss: 0.00003724
Iteration 164/1000 | Loss: 0.00003724
Iteration 165/1000 | Loss: 0.00003724
Iteration 166/1000 | Loss: 0.00003724
Iteration 167/1000 | Loss: 0.00003724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.7240188248688355e-05, 3.7240188248688355e-05, 3.7240188248688355e-05, 3.7240188248688355e-05, 3.7240188248688355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7240188248688355e-05

Optimization complete. Final v2v error: 4.522653102874756 mm

Highest mean error: 10.722946166992188 mm for frame 105

Lowest mean error: 3.1292996406555176 mm for frame 4

Saving results

Total time: 181.87577295303345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442623
Iteration 2/25 | Loss: 0.00134084
Iteration 3/25 | Loss: 0.00124403
Iteration 4/25 | Loss: 0.00123065
Iteration 5/25 | Loss: 0.00122609
Iteration 6/25 | Loss: 0.00122491
Iteration 7/25 | Loss: 0.00122491
Iteration 8/25 | Loss: 0.00122491
Iteration 9/25 | Loss: 0.00122491
Iteration 10/25 | Loss: 0.00122491
Iteration 11/25 | Loss: 0.00122491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012249110732227564, 0.0012249110732227564, 0.0012249110732227564, 0.0012249110732227564, 0.0012249110732227564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012249110732227564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36810923
Iteration 2/25 | Loss: 0.00127097
Iteration 3/25 | Loss: 0.00127097
Iteration 4/25 | Loss: 0.00127097
Iteration 5/25 | Loss: 0.00127097
Iteration 6/25 | Loss: 0.00127096
Iteration 7/25 | Loss: 0.00127096
Iteration 8/25 | Loss: 0.00127096
Iteration 9/25 | Loss: 0.00127096
Iteration 10/25 | Loss: 0.00127096
Iteration 11/25 | Loss: 0.00127096
Iteration 12/25 | Loss: 0.00127096
Iteration 13/25 | Loss: 0.00127096
Iteration 14/25 | Loss: 0.00127096
Iteration 15/25 | Loss: 0.00127096
Iteration 16/25 | Loss: 0.00127096
Iteration 17/25 | Loss: 0.00127096
Iteration 18/25 | Loss: 0.00127096
Iteration 19/25 | Loss: 0.00127096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012709632283076644, 0.0012709632283076644, 0.0012709632283076644, 0.0012709632283076644, 0.0012709632283076644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012709632283076644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127096
Iteration 2/1000 | Loss: 0.00002690
Iteration 3/1000 | Loss: 0.00001927
Iteration 4/1000 | Loss: 0.00001704
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001456
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001376
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001341
Iteration 13/1000 | Loss: 0.00001335
Iteration 14/1000 | Loss: 0.00001332
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001323
Iteration 18/1000 | Loss: 0.00001322
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001320
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001319
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001318
Iteration 30/1000 | Loss: 0.00001318
Iteration 31/1000 | Loss: 0.00001318
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001317
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001316
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001316
Iteration 41/1000 | Loss: 0.00001315
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001314
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001313
Iteration 49/1000 | Loss: 0.00001312
Iteration 50/1000 | Loss: 0.00001312
Iteration 51/1000 | Loss: 0.00001312
Iteration 52/1000 | Loss: 0.00001312
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001311
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001308
Iteration 63/1000 | Loss: 0.00001308
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001307
Iteration 67/1000 | Loss: 0.00001307
Iteration 68/1000 | Loss: 0.00001307
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001305
Iteration 74/1000 | Loss: 0.00001305
Iteration 75/1000 | Loss: 0.00001305
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001304
Iteration 78/1000 | Loss: 0.00001304
Iteration 79/1000 | Loss: 0.00001304
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001304
Iteration 85/1000 | Loss: 0.00001304
Iteration 86/1000 | Loss: 0.00001304
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001304
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001304
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001304
Iteration 96/1000 | Loss: 0.00001304
Iteration 97/1000 | Loss: 0.00001304
Iteration 98/1000 | Loss: 0.00001304
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001304
Iteration 105/1000 | Loss: 0.00001304
Iteration 106/1000 | Loss: 0.00001304
Iteration 107/1000 | Loss: 0.00001304
Iteration 108/1000 | Loss: 0.00001304
Iteration 109/1000 | Loss: 0.00001304
Iteration 110/1000 | Loss: 0.00001304
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001304
Iteration 118/1000 | Loss: 0.00001304
Iteration 119/1000 | Loss: 0.00001304
Iteration 120/1000 | Loss: 0.00001304
Iteration 121/1000 | Loss: 0.00001304
Iteration 122/1000 | Loss: 0.00001304
Iteration 123/1000 | Loss: 0.00001304
Iteration 124/1000 | Loss: 0.00001304
Iteration 125/1000 | Loss: 0.00001304
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.3038746146776248e-05, 1.3038746146776248e-05, 1.3038746146776248e-05, 1.3038746146776248e-05, 1.3038746146776248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3038746146776248e-05

Optimization complete. Final v2v error: 3.041012763977051 mm

Highest mean error: 3.6148037910461426 mm for frame 160

Lowest mean error: 2.6419005393981934 mm for frame 169

Saving results

Total time: 35.81684994697571
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485634
Iteration 2/25 | Loss: 0.00127069
Iteration 3/25 | Loss: 0.00121273
Iteration 4/25 | Loss: 0.00120112
Iteration 5/25 | Loss: 0.00119804
Iteration 6/25 | Loss: 0.00119804
Iteration 7/25 | Loss: 0.00119804
Iteration 8/25 | Loss: 0.00119804
Iteration 9/25 | Loss: 0.00119804
Iteration 10/25 | Loss: 0.00119804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011980365961790085, 0.0011980365961790085, 0.0011980365961790085, 0.0011980365961790085, 0.0011980365961790085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011980365961790085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33014512
Iteration 2/25 | Loss: 0.00096198
Iteration 3/25 | Loss: 0.00096197
Iteration 4/25 | Loss: 0.00096197
Iteration 5/25 | Loss: 0.00096197
Iteration 6/25 | Loss: 0.00096197
Iteration 7/25 | Loss: 0.00096197
Iteration 8/25 | Loss: 0.00096197
Iteration 9/25 | Loss: 0.00096197
Iteration 10/25 | Loss: 0.00096197
Iteration 11/25 | Loss: 0.00096197
Iteration 12/25 | Loss: 0.00096197
Iteration 13/25 | Loss: 0.00096197
Iteration 14/25 | Loss: 0.00096197
Iteration 15/25 | Loss: 0.00096197
Iteration 16/25 | Loss: 0.00096197
Iteration 17/25 | Loss: 0.00096197
Iteration 18/25 | Loss: 0.00096197
Iteration 19/25 | Loss: 0.00096197
Iteration 20/25 | Loss: 0.00096197
Iteration 21/25 | Loss: 0.00096197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009619714110158384, 0.0009619714110158384, 0.0009619714110158384, 0.0009619714110158384, 0.0009619714110158384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009619714110158384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096197
Iteration 2/1000 | Loss: 0.00002050
Iteration 3/1000 | Loss: 0.00001662
Iteration 4/1000 | Loss: 0.00001490
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001227
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001189
Iteration 20/1000 | Loss: 0.00001180
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001177
Iteration 24/1000 | Loss: 0.00001177
Iteration 25/1000 | Loss: 0.00001176
Iteration 26/1000 | Loss: 0.00001175
Iteration 27/1000 | Loss: 0.00001174
Iteration 28/1000 | Loss: 0.00001174
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001172
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001172
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001171
Iteration 39/1000 | Loss: 0.00001171
Iteration 40/1000 | Loss: 0.00001170
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001168
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001165
Iteration 48/1000 | Loss: 0.00001164
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001164
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001163
Iteration 53/1000 | Loss: 0.00001163
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001163
Iteration 59/1000 | Loss: 0.00001163
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001163
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001163
Iteration 67/1000 | Loss: 0.00001163
Iteration 68/1000 | Loss: 0.00001163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [1.163018441729946e-05, 1.163018441729946e-05, 1.163018441729946e-05, 1.163018441729946e-05, 1.163018441729946e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.163018441729946e-05

Optimization complete. Final v2v error: 2.9652676582336426 mm

Highest mean error: 3.2100329399108887 mm for frame 170

Lowest mean error: 2.8415889739990234 mm for frame 14

Saving results

Total time: 36.41577196121216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00555388
Iteration 2/25 | Loss: 0.00167353
Iteration 3/25 | Loss: 0.00139835
Iteration 4/25 | Loss: 0.00137933
Iteration 5/25 | Loss: 0.00137651
Iteration 6/25 | Loss: 0.00137596
Iteration 7/25 | Loss: 0.00137596
Iteration 8/25 | Loss: 0.00137596
Iteration 9/25 | Loss: 0.00137596
Iteration 10/25 | Loss: 0.00137596
Iteration 11/25 | Loss: 0.00137596
Iteration 12/25 | Loss: 0.00137596
Iteration 13/25 | Loss: 0.00137596
Iteration 14/25 | Loss: 0.00137596
Iteration 15/25 | Loss: 0.00137596
Iteration 16/25 | Loss: 0.00137596
Iteration 17/25 | Loss: 0.00137596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013759637949988246, 0.0013759637949988246, 0.0013759637949988246, 0.0013759637949988246, 0.0013759637949988246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013759637949988246

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99026668
Iteration 2/25 | Loss: 0.00112247
Iteration 3/25 | Loss: 0.00112246
Iteration 4/25 | Loss: 0.00112246
Iteration 5/25 | Loss: 0.00112245
Iteration 6/25 | Loss: 0.00112245
Iteration 7/25 | Loss: 0.00112245
Iteration 8/25 | Loss: 0.00112245
Iteration 9/25 | Loss: 0.00112245
Iteration 10/25 | Loss: 0.00112245
Iteration 11/25 | Loss: 0.00112245
Iteration 12/25 | Loss: 0.00112245
Iteration 13/25 | Loss: 0.00112245
Iteration 14/25 | Loss: 0.00112245
Iteration 15/25 | Loss: 0.00112245
Iteration 16/25 | Loss: 0.00112245
Iteration 17/25 | Loss: 0.00112245
Iteration 18/25 | Loss: 0.00112245
Iteration 19/25 | Loss: 0.00112245
Iteration 20/25 | Loss: 0.00112245
Iteration 21/25 | Loss: 0.00112245
Iteration 22/25 | Loss: 0.00112245
Iteration 23/25 | Loss: 0.00112245
Iteration 24/25 | Loss: 0.00112245
Iteration 25/25 | Loss: 0.00112245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112245
Iteration 2/1000 | Loss: 0.00005046
Iteration 3/1000 | Loss: 0.00003508
Iteration 4/1000 | Loss: 0.00002822
Iteration 5/1000 | Loss: 0.00002633
Iteration 6/1000 | Loss: 0.00002551
Iteration 7/1000 | Loss: 0.00002511
Iteration 8/1000 | Loss: 0.00002452
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002348
Iteration 12/1000 | Loss: 0.00002318
Iteration 13/1000 | Loss: 0.00002292
Iteration 14/1000 | Loss: 0.00002272
Iteration 15/1000 | Loss: 0.00002251
Iteration 16/1000 | Loss: 0.00002237
Iteration 17/1000 | Loss: 0.00002232
Iteration 18/1000 | Loss: 0.00002231
Iteration 19/1000 | Loss: 0.00002229
Iteration 20/1000 | Loss: 0.00002227
Iteration 21/1000 | Loss: 0.00002214
Iteration 22/1000 | Loss: 0.00002211
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002206
Iteration 25/1000 | Loss: 0.00002206
Iteration 26/1000 | Loss: 0.00002205
Iteration 27/1000 | Loss: 0.00002204
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002203
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002195
Iteration 36/1000 | Loss: 0.00002195
Iteration 37/1000 | Loss: 0.00002192
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002189
Iteration 40/1000 | Loss: 0.00002189
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002185
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002183
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002181
Iteration 56/1000 | Loss: 0.00002181
Iteration 57/1000 | Loss: 0.00002180
Iteration 58/1000 | Loss: 0.00002180
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00002178
Iteration 62/1000 | Loss: 0.00002178
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002178
Iteration 70/1000 | Loss: 0.00002178
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.1776017092633992e-05, 2.1776017092633992e-05, 2.1776017092633992e-05, 2.1776017092633992e-05, 2.1776017092633992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1776017092633992e-05

Optimization complete. Final v2v error: 3.7465827465057373 mm

Highest mean error: 4.7889556884765625 mm for frame 59

Lowest mean error: 2.9194064140319824 mm for frame 137

Saving results

Total time: 41.9361412525177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753276
Iteration 2/25 | Loss: 0.00141231
Iteration 3/25 | Loss: 0.00131508
Iteration 4/25 | Loss: 0.00129627
Iteration 5/25 | Loss: 0.00128972
Iteration 6/25 | Loss: 0.00128859
Iteration 7/25 | Loss: 0.00128859
Iteration 8/25 | Loss: 0.00128859
Iteration 9/25 | Loss: 0.00128859
Iteration 10/25 | Loss: 0.00128859
Iteration 11/25 | Loss: 0.00128859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001288594794459641, 0.001288594794459641, 0.001288594794459641, 0.001288594794459641, 0.001288594794459641]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001288594794459641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34286344
Iteration 2/25 | Loss: 0.00112543
Iteration 3/25 | Loss: 0.00112543
Iteration 4/25 | Loss: 0.00112543
Iteration 5/25 | Loss: 0.00112543
Iteration 6/25 | Loss: 0.00112543
Iteration 7/25 | Loss: 0.00112543
Iteration 8/25 | Loss: 0.00112543
Iteration 9/25 | Loss: 0.00112543
Iteration 10/25 | Loss: 0.00112543
Iteration 11/25 | Loss: 0.00112543
Iteration 12/25 | Loss: 0.00112542
Iteration 13/25 | Loss: 0.00112542
Iteration 14/25 | Loss: 0.00112542
Iteration 15/25 | Loss: 0.00112542
Iteration 16/25 | Loss: 0.00112542
Iteration 17/25 | Loss: 0.00112542
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011254249839112163, 0.0011254249839112163, 0.0011254249839112163, 0.0011254249839112163, 0.0011254249839112163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011254249839112163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112542
Iteration 2/1000 | Loss: 0.00005054
Iteration 3/1000 | Loss: 0.00003777
Iteration 4/1000 | Loss: 0.00003360
Iteration 5/1000 | Loss: 0.00003168
Iteration 6/1000 | Loss: 0.00003057
Iteration 7/1000 | Loss: 0.00002976
Iteration 8/1000 | Loss: 0.00002915
Iteration 9/1000 | Loss: 0.00002862
Iteration 10/1000 | Loss: 0.00002835
Iteration 11/1000 | Loss: 0.00002807
Iteration 12/1000 | Loss: 0.00002783
Iteration 13/1000 | Loss: 0.00002778
Iteration 14/1000 | Loss: 0.00002769
Iteration 15/1000 | Loss: 0.00002753
Iteration 16/1000 | Loss: 0.00002747
Iteration 17/1000 | Loss: 0.00002738
Iteration 18/1000 | Loss: 0.00002734
Iteration 19/1000 | Loss: 0.00002733
Iteration 20/1000 | Loss: 0.00002732
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002729
Iteration 23/1000 | Loss: 0.00002728
Iteration 24/1000 | Loss: 0.00002728
Iteration 25/1000 | Loss: 0.00002727
Iteration 26/1000 | Loss: 0.00002727
Iteration 27/1000 | Loss: 0.00002726
Iteration 28/1000 | Loss: 0.00002725
Iteration 29/1000 | Loss: 0.00002725
Iteration 30/1000 | Loss: 0.00002724
Iteration 31/1000 | Loss: 0.00002724
Iteration 32/1000 | Loss: 0.00002718
Iteration 33/1000 | Loss: 0.00002715
Iteration 34/1000 | Loss: 0.00002714
Iteration 35/1000 | Loss: 0.00002713
Iteration 36/1000 | Loss: 0.00002713
Iteration 37/1000 | Loss: 0.00002713
Iteration 38/1000 | Loss: 0.00002712
Iteration 39/1000 | Loss: 0.00002712
Iteration 40/1000 | Loss: 0.00002712
Iteration 41/1000 | Loss: 0.00002712
Iteration 42/1000 | Loss: 0.00002711
Iteration 43/1000 | Loss: 0.00002711
Iteration 44/1000 | Loss: 0.00002710
Iteration 45/1000 | Loss: 0.00002710
Iteration 46/1000 | Loss: 0.00002709
Iteration 47/1000 | Loss: 0.00002709
Iteration 48/1000 | Loss: 0.00002709
Iteration 49/1000 | Loss: 0.00002708
Iteration 50/1000 | Loss: 0.00002708
Iteration 51/1000 | Loss: 0.00002708
Iteration 52/1000 | Loss: 0.00002708
Iteration 53/1000 | Loss: 0.00002707
Iteration 54/1000 | Loss: 0.00002707
Iteration 55/1000 | Loss: 0.00002706
Iteration 56/1000 | Loss: 0.00002706
Iteration 57/1000 | Loss: 0.00002706
Iteration 58/1000 | Loss: 0.00002705
Iteration 59/1000 | Loss: 0.00002705
Iteration 60/1000 | Loss: 0.00002705
Iteration 61/1000 | Loss: 0.00002705
Iteration 62/1000 | Loss: 0.00002705
Iteration 63/1000 | Loss: 0.00002705
Iteration 64/1000 | Loss: 0.00002705
Iteration 65/1000 | Loss: 0.00002704
Iteration 66/1000 | Loss: 0.00002704
Iteration 67/1000 | Loss: 0.00002702
Iteration 68/1000 | Loss: 0.00002702
Iteration 69/1000 | Loss: 0.00002702
Iteration 70/1000 | Loss: 0.00002701
Iteration 71/1000 | Loss: 0.00002701
Iteration 72/1000 | Loss: 0.00002701
Iteration 73/1000 | Loss: 0.00002701
Iteration 74/1000 | Loss: 0.00002700
Iteration 75/1000 | Loss: 0.00002700
Iteration 76/1000 | Loss: 0.00002700
Iteration 77/1000 | Loss: 0.00002700
Iteration 78/1000 | Loss: 0.00002699
Iteration 79/1000 | Loss: 0.00002699
Iteration 80/1000 | Loss: 0.00002699
Iteration 81/1000 | Loss: 0.00002698
Iteration 82/1000 | Loss: 0.00002698
Iteration 83/1000 | Loss: 0.00002698
Iteration 84/1000 | Loss: 0.00002698
Iteration 85/1000 | Loss: 0.00002698
Iteration 86/1000 | Loss: 0.00002698
Iteration 87/1000 | Loss: 0.00002698
Iteration 88/1000 | Loss: 0.00002697
Iteration 89/1000 | Loss: 0.00002697
Iteration 90/1000 | Loss: 0.00002697
Iteration 91/1000 | Loss: 0.00002697
Iteration 92/1000 | Loss: 0.00002696
Iteration 93/1000 | Loss: 0.00002696
Iteration 94/1000 | Loss: 0.00002696
Iteration 95/1000 | Loss: 0.00002696
Iteration 96/1000 | Loss: 0.00002696
Iteration 97/1000 | Loss: 0.00002696
Iteration 98/1000 | Loss: 0.00002696
Iteration 99/1000 | Loss: 0.00002696
Iteration 100/1000 | Loss: 0.00002696
Iteration 101/1000 | Loss: 0.00002696
Iteration 102/1000 | Loss: 0.00002696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.6958214220940135e-05, 2.6958214220940135e-05, 2.6958214220940135e-05, 2.6958214220940135e-05, 2.6958214220940135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6958214220940135e-05

Optimization complete. Final v2v error: 4.316866874694824 mm

Highest mean error: 5.354503631591797 mm for frame 21

Lowest mean error: 3.4320502281188965 mm for frame 130

Saving results

Total time: 38.50054097175598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015298
Iteration 2/25 | Loss: 0.00245612
Iteration 3/25 | Loss: 0.00199306
Iteration 4/25 | Loss: 0.00184962
Iteration 5/25 | Loss: 0.00174071
Iteration 6/25 | Loss: 0.00162314
Iteration 7/25 | Loss: 0.00160781
Iteration 8/25 | Loss: 0.00158248
Iteration 9/25 | Loss: 0.00154853
Iteration 10/25 | Loss: 0.00154943
Iteration 11/25 | Loss: 0.00154777
Iteration 12/25 | Loss: 0.00153976
Iteration 13/25 | Loss: 0.00152585
Iteration 14/25 | Loss: 0.00152528
Iteration 15/25 | Loss: 0.00152329
Iteration 16/25 | Loss: 0.00152045
Iteration 17/25 | Loss: 0.00151043
Iteration 18/25 | Loss: 0.00150666
Iteration 19/25 | Loss: 0.00150620
Iteration 20/25 | Loss: 0.00150598
Iteration 21/25 | Loss: 0.00150584
Iteration 22/25 | Loss: 0.00150557
Iteration 23/25 | Loss: 0.00150500
Iteration 24/25 | Loss: 0.00150386
Iteration 25/25 | Loss: 0.00150289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79835320
Iteration 2/25 | Loss: 0.00255593
Iteration 3/25 | Loss: 0.00255593
Iteration 4/25 | Loss: 0.00255593
Iteration 5/25 | Loss: 0.00255593
Iteration 6/25 | Loss: 0.00255593
Iteration 7/25 | Loss: 0.00255593
Iteration 8/25 | Loss: 0.00255593
Iteration 9/25 | Loss: 0.00255593
Iteration 10/25 | Loss: 0.00255593
Iteration 11/25 | Loss: 0.00255593
Iteration 12/25 | Loss: 0.00255593
Iteration 13/25 | Loss: 0.00255593
Iteration 14/25 | Loss: 0.00255593
Iteration 15/25 | Loss: 0.00255593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025559274945408106, 0.0025559274945408106, 0.0025559274945408106, 0.0025559274945408106, 0.0025559274945408106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025559274945408106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255593
Iteration 2/1000 | Loss: 0.00028856
Iteration 3/1000 | Loss: 0.00077418
Iteration 4/1000 | Loss: 0.00065204
Iteration 5/1000 | Loss: 0.00016655
Iteration 6/1000 | Loss: 0.00046596
Iteration 7/1000 | Loss: 0.00014163
Iteration 8/1000 | Loss: 0.00012791
Iteration 9/1000 | Loss: 0.00011388
Iteration 10/1000 | Loss: 0.00010575
Iteration 11/1000 | Loss: 0.00010129
Iteration 12/1000 | Loss: 0.00009857
Iteration 13/1000 | Loss: 0.00009632
Iteration 14/1000 | Loss: 0.00009422
Iteration 15/1000 | Loss: 0.00009237
Iteration 16/1000 | Loss: 0.00009096
Iteration 17/1000 | Loss: 0.00012700
Iteration 18/1000 | Loss: 0.00009370
Iteration 19/1000 | Loss: 0.00009035
Iteration 20/1000 | Loss: 0.00008870
Iteration 21/1000 | Loss: 0.00051216
Iteration 22/1000 | Loss: 0.00010024
Iteration 23/1000 | Loss: 0.00008903
Iteration 24/1000 | Loss: 0.00008739
Iteration 25/1000 | Loss: 0.00008573
Iteration 26/1000 | Loss: 0.00008445
Iteration 27/1000 | Loss: 0.00008280
Iteration 28/1000 | Loss: 0.00008179
Iteration 29/1000 | Loss: 0.00008095
Iteration 30/1000 | Loss: 0.00008056
Iteration 31/1000 | Loss: 0.00008020
Iteration 32/1000 | Loss: 0.00007977
Iteration 33/1000 | Loss: 0.00007939
Iteration 34/1000 | Loss: 0.00007905
Iteration 35/1000 | Loss: 0.00007871
Iteration 36/1000 | Loss: 0.00007848
Iteration 37/1000 | Loss: 0.00034146
Iteration 38/1000 | Loss: 0.00020041
Iteration 39/1000 | Loss: 0.00008171
Iteration 40/1000 | Loss: 0.00007978
Iteration 41/1000 | Loss: 0.00007844
Iteration 42/1000 | Loss: 0.00007838
Iteration 43/1000 | Loss: 0.00007838
Iteration 44/1000 | Loss: 0.00033589
Iteration 45/1000 | Loss: 0.00021029
Iteration 46/1000 | Loss: 0.00029583
Iteration 47/1000 | Loss: 0.00020372
Iteration 48/1000 | Loss: 0.00009256
Iteration 49/1000 | Loss: 0.00008653
Iteration 50/1000 | Loss: 0.00007985
Iteration 51/1000 | Loss: 0.00023547
Iteration 52/1000 | Loss: 0.00010322
Iteration 53/1000 | Loss: 0.00007953
Iteration 54/1000 | Loss: 0.00020768
Iteration 55/1000 | Loss: 0.00009743
Iteration 56/1000 | Loss: 0.00014756
Iteration 57/1000 | Loss: 0.00011005
Iteration 58/1000 | Loss: 0.00010196
Iteration 59/1000 | Loss: 0.00007938
Iteration 60/1000 | Loss: 0.00007849
Iteration 61/1000 | Loss: 0.00007744
Iteration 62/1000 | Loss: 0.00007658
Iteration 63/1000 | Loss: 0.00007629
Iteration 64/1000 | Loss: 0.00007610
Iteration 65/1000 | Loss: 0.00007590
Iteration 66/1000 | Loss: 0.00007572
Iteration 67/1000 | Loss: 0.00007555
Iteration 68/1000 | Loss: 0.00007552
Iteration 69/1000 | Loss: 0.00007550
Iteration 70/1000 | Loss: 0.00007549
Iteration 71/1000 | Loss: 0.00007548
Iteration 72/1000 | Loss: 0.00007541
Iteration 73/1000 | Loss: 0.00007541
Iteration 74/1000 | Loss: 0.00007540
Iteration 75/1000 | Loss: 0.00007540
Iteration 76/1000 | Loss: 0.00007539
Iteration 77/1000 | Loss: 0.00007538
Iteration 78/1000 | Loss: 0.00007537
Iteration 79/1000 | Loss: 0.00007536
Iteration 80/1000 | Loss: 0.00007535
Iteration 81/1000 | Loss: 0.00007535
Iteration 82/1000 | Loss: 0.00007532
Iteration 83/1000 | Loss: 0.00007531
Iteration 84/1000 | Loss: 0.00007530
Iteration 85/1000 | Loss: 0.00007530
Iteration 86/1000 | Loss: 0.00007530
Iteration 87/1000 | Loss: 0.00007529
Iteration 88/1000 | Loss: 0.00007529
Iteration 89/1000 | Loss: 0.00007529
Iteration 90/1000 | Loss: 0.00007528
Iteration 91/1000 | Loss: 0.00007528
Iteration 92/1000 | Loss: 0.00007528
Iteration 93/1000 | Loss: 0.00007527
Iteration 94/1000 | Loss: 0.00007527
Iteration 95/1000 | Loss: 0.00007527
Iteration 96/1000 | Loss: 0.00007527
Iteration 97/1000 | Loss: 0.00007526
Iteration 98/1000 | Loss: 0.00007526
Iteration 99/1000 | Loss: 0.00007524
Iteration 100/1000 | Loss: 0.00007523
Iteration 101/1000 | Loss: 0.00007521
Iteration 102/1000 | Loss: 0.00007521
Iteration 103/1000 | Loss: 0.00007521
Iteration 104/1000 | Loss: 0.00007521
Iteration 105/1000 | Loss: 0.00007521
Iteration 106/1000 | Loss: 0.00007521
Iteration 107/1000 | Loss: 0.00007521
Iteration 108/1000 | Loss: 0.00007520
Iteration 109/1000 | Loss: 0.00007520
Iteration 110/1000 | Loss: 0.00007520
Iteration 111/1000 | Loss: 0.00007520
Iteration 112/1000 | Loss: 0.00007518
Iteration 113/1000 | Loss: 0.00007518
Iteration 114/1000 | Loss: 0.00007518
Iteration 115/1000 | Loss: 0.00007518
Iteration 116/1000 | Loss: 0.00007518
Iteration 117/1000 | Loss: 0.00007518
Iteration 118/1000 | Loss: 0.00007518
Iteration 119/1000 | Loss: 0.00007518
Iteration 120/1000 | Loss: 0.00007518
Iteration 121/1000 | Loss: 0.00007517
Iteration 122/1000 | Loss: 0.00007517
Iteration 123/1000 | Loss: 0.00007517
Iteration 124/1000 | Loss: 0.00007517
Iteration 125/1000 | Loss: 0.00007517
Iteration 126/1000 | Loss: 0.00007517
Iteration 127/1000 | Loss: 0.00007516
Iteration 128/1000 | Loss: 0.00007516
Iteration 129/1000 | Loss: 0.00007516
Iteration 130/1000 | Loss: 0.00007516
Iteration 131/1000 | Loss: 0.00007515
Iteration 132/1000 | Loss: 0.00007515
Iteration 133/1000 | Loss: 0.00007515
Iteration 134/1000 | Loss: 0.00007515
Iteration 135/1000 | Loss: 0.00007515
Iteration 136/1000 | Loss: 0.00007515
Iteration 137/1000 | Loss: 0.00007515
Iteration 138/1000 | Loss: 0.00007515
Iteration 139/1000 | Loss: 0.00007515
Iteration 140/1000 | Loss: 0.00007514
Iteration 141/1000 | Loss: 0.00007514
Iteration 142/1000 | Loss: 0.00007514
Iteration 143/1000 | Loss: 0.00007514
Iteration 144/1000 | Loss: 0.00007514
Iteration 145/1000 | Loss: 0.00007514
Iteration 146/1000 | Loss: 0.00007514
Iteration 147/1000 | Loss: 0.00007514
Iteration 148/1000 | Loss: 0.00007514
Iteration 149/1000 | Loss: 0.00007514
Iteration 150/1000 | Loss: 0.00007514
Iteration 151/1000 | Loss: 0.00007514
Iteration 152/1000 | Loss: 0.00007513
Iteration 153/1000 | Loss: 0.00007513
Iteration 154/1000 | Loss: 0.00007513
Iteration 155/1000 | Loss: 0.00007513
Iteration 156/1000 | Loss: 0.00007513
Iteration 157/1000 | Loss: 0.00007513
Iteration 158/1000 | Loss: 0.00007513
Iteration 159/1000 | Loss: 0.00007513
Iteration 160/1000 | Loss: 0.00007512
Iteration 161/1000 | Loss: 0.00007512
Iteration 162/1000 | Loss: 0.00007512
Iteration 163/1000 | Loss: 0.00007512
Iteration 164/1000 | Loss: 0.00007512
Iteration 165/1000 | Loss: 0.00007511
Iteration 166/1000 | Loss: 0.00007511
Iteration 167/1000 | Loss: 0.00007511
Iteration 168/1000 | Loss: 0.00007511
Iteration 169/1000 | Loss: 0.00007511
Iteration 170/1000 | Loss: 0.00007511
Iteration 171/1000 | Loss: 0.00007511
Iteration 172/1000 | Loss: 0.00007511
Iteration 173/1000 | Loss: 0.00007511
Iteration 174/1000 | Loss: 0.00007511
Iteration 175/1000 | Loss: 0.00007511
Iteration 176/1000 | Loss: 0.00007510
Iteration 177/1000 | Loss: 0.00007510
Iteration 178/1000 | Loss: 0.00007510
Iteration 179/1000 | Loss: 0.00007510
Iteration 180/1000 | Loss: 0.00007510
Iteration 181/1000 | Loss: 0.00007510
Iteration 182/1000 | Loss: 0.00007510
Iteration 183/1000 | Loss: 0.00007510
Iteration 184/1000 | Loss: 0.00007510
Iteration 185/1000 | Loss: 0.00007510
Iteration 186/1000 | Loss: 0.00007510
Iteration 187/1000 | Loss: 0.00007510
Iteration 188/1000 | Loss: 0.00007510
Iteration 189/1000 | Loss: 0.00007510
Iteration 190/1000 | Loss: 0.00007510
Iteration 191/1000 | Loss: 0.00007510
Iteration 192/1000 | Loss: 0.00007509
Iteration 193/1000 | Loss: 0.00007509
Iteration 194/1000 | Loss: 0.00007509
Iteration 195/1000 | Loss: 0.00007509
Iteration 196/1000 | Loss: 0.00007509
Iteration 197/1000 | Loss: 0.00007509
Iteration 198/1000 | Loss: 0.00007509
Iteration 199/1000 | Loss: 0.00007509
Iteration 200/1000 | Loss: 0.00007509
Iteration 201/1000 | Loss: 0.00007509
Iteration 202/1000 | Loss: 0.00007509
Iteration 203/1000 | Loss: 0.00007509
Iteration 204/1000 | Loss: 0.00007509
Iteration 205/1000 | Loss: 0.00007509
Iteration 206/1000 | Loss: 0.00007508
Iteration 207/1000 | Loss: 0.00007508
Iteration 208/1000 | Loss: 0.00007508
Iteration 209/1000 | Loss: 0.00007508
Iteration 210/1000 | Loss: 0.00007508
Iteration 211/1000 | Loss: 0.00007508
Iteration 212/1000 | Loss: 0.00007508
Iteration 213/1000 | Loss: 0.00007508
Iteration 214/1000 | Loss: 0.00007508
Iteration 215/1000 | Loss: 0.00007508
Iteration 216/1000 | Loss: 0.00007508
Iteration 217/1000 | Loss: 0.00007508
Iteration 218/1000 | Loss: 0.00007508
Iteration 219/1000 | Loss: 0.00007508
Iteration 220/1000 | Loss: 0.00007508
Iteration 221/1000 | Loss: 0.00007508
Iteration 222/1000 | Loss: 0.00007508
Iteration 223/1000 | Loss: 0.00007508
Iteration 224/1000 | Loss: 0.00007508
Iteration 225/1000 | Loss: 0.00007508
Iteration 226/1000 | Loss: 0.00007508
Iteration 227/1000 | Loss: 0.00007508
Iteration 228/1000 | Loss: 0.00007508
Iteration 229/1000 | Loss: 0.00007508
Iteration 230/1000 | Loss: 0.00007508
Iteration 231/1000 | Loss: 0.00007508
Iteration 232/1000 | Loss: 0.00007508
Iteration 233/1000 | Loss: 0.00007508
Iteration 234/1000 | Loss: 0.00007508
Iteration 235/1000 | Loss: 0.00007508
Iteration 236/1000 | Loss: 0.00007508
Iteration 237/1000 | Loss: 0.00007508
Iteration 238/1000 | Loss: 0.00007508
Iteration 239/1000 | Loss: 0.00007508
Iteration 240/1000 | Loss: 0.00007508
Iteration 241/1000 | Loss: 0.00007508
Iteration 242/1000 | Loss: 0.00007508
Iteration 243/1000 | Loss: 0.00007508
Iteration 244/1000 | Loss: 0.00007508
Iteration 245/1000 | Loss: 0.00007508
Iteration 246/1000 | Loss: 0.00007508
Iteration 247/1000 | Loss: 0.00007508
Iteration 248/1000 | Loss: 0.00007508
Iteration 249/1000 | Loss: 0.00007508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [7.508083945140243e-05, 7.508083945140243e-05, 7.508083945140243e-05, 7.508083945140243e-05, 7.508083945140243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.508083945140243e-05

Optimization complete. Final v2v error: 4.873920917510986 mm

Highest mean error: 11.631547927856445 mm for frame 51

Lowest mean error: 3.195847988128662 mm for frame 7

Saving results

Total time: 146.86920857429504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00426034
Iteration 2/25 | Loss: 0.00128684
Iteration 3/25 | Loss: 0.00122638
Iteration 4/25 | Loss: 0.00121120
Iteration 5/25 | Loss: 0.00120686
Iteration 6/25 | Loss: 0.00120615
Iteration 7/25 | Loss: 0.00120615
Iteration 8/25 | Loss: 0.00120615
Iteration 9/25 | Loss: 0.00120615
Iteration 10/25 | Loss: 0.00120615
Iteration 11/25 | Loss: 0.00120615
Iteration 12/25 | Loss: 0.00120615
Iteration 13/25 | Loss: 0.00120615
Iteration 14/25 | Loss: 0.00120615
Iteration 15/25 | Loss: 0.00120615
Iteration 16/25 | Loss: 0.00120615
Iteration 17/25 | Loss: 0.00120615
Iteration 18/25 | Loss: 0.00120615
Iteration 19/25 | Loss: 0.00120615
Iteration 20/25 | Loss: 0.00120615
Iteration 21/25 | Loss: 0.00120615
Iteration 22/25 | Loss: 0.00120615
Iteration 23/25 | Loss: 0.00120615
Iteration 24/25 | Loss: 0.00120615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012061534216627479, 0.0012061534216627479, 0.0012061534216627479, 0.0012061534216627479, 0.0012061534216627479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012061534216627479

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35409582
Iteration 2/25 | Loss: 0.00094544
Iteration 3/25 | Loss: 0.00094544
Iteration 4/25 | Loss: 0.00094544
Iteration 5/25 | Loss: 0.00094544
Iteration 6/25 | Loss: 0.00094544
Iteration 7/25 | Loss: 0.00094544
Iteration 8/25 | Loss: 0.00094544
Iteration 9/25 | Loss: 0.00094544
Iteration 10/25 | Loss: 0.00094544
Iteration 11/25 | Loss: 0.00094544
Iteration 12/25 | Loss: 0.00094544
Iteration 13/25 | Loss: 0.00094544
Iteration 14/25 | Loss: 0.00094544
Iteration 15/25 | Loss: 0.00094544
Iteration 16/25 | Loss: 0.00094544
Iteration 17/25 | Loss: 0.00094544
Iteration 18/25 | Loss: 0.00094544
Iteration 19/25 | Loss: 0.00094544
Iteration 20/25 | Loss: 0.00094544
Iteration 21/25 | Loss: 0.00094544
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009454403771087527, 0.0009454403771087527, 0.0009454403771087527, 0.0009454403771087527, 0.0009454403771087527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009454403771087527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094544
Iteration 2/1000 | Loss: 0.00002172
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00001459
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001384
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001375
Iteration 12/1000 | Loss: 0.00001373
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001359
Iteration 16/1000 | Loss: 0.00001353
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001345
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001343
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00001342
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001321
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001320
Iteration 29/1000 | Loss: 0.00001320
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001320
Iteration 32/1000 | Loss: 0.00001318
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001317
Iteration 35/1000 | Loss: 0.00001317
Iteration 36/1000 | Loss: 0.00001317
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001317
Iteration 40/1000 | Loss: 0.00001317
Iteration 41/1000 | Loss: 0.00001316
Iteration 42/1000 | Loss: 0.00001315
Iteration 43/1000 | Loss: 0.00001315
Iteration 44/1000 | Loss: 0.00001314
Iteration 45/1000 | Loss: 0.00001314
Iteration 46/1000 | Loss: 0.00001313
Iteration 47/1000 | Loss: 0.00001313
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001311
Iteration 53/1000 | Loss: 0.00001311
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001308
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001303
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001303
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001300
Iteration 67/1000 | Loss: 0.00001300
Iteration 68/1000 | Loss: 0.00001299
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001298
Iteration 72/1000 | Loss: 0.00001298
Iteration 73/1000 | Loss: 0.00001298
Iteration 74/1000 | Loss: 0.00001297
Iteration 75/1000 | Loss: 0.00001297
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001294
Iteration 78/1000 | Loss: 0.00001294
Iteration 79/1000 | Loss: 0.00001294
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00001294
Iteration 87/1000 | Loss: 0.00001293
Iteration 88/1000 | Loss: 0.00001293
Iteration 89/1000 | Loss: 0.00001293
Iteration 90/1000 | Loss: 0.00001293
Iteration 91/1000 | Loss: 0.00001293
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001290
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001288
Iteration 112/1000 | Loss: 0.00001288
Iteration 113/1000 | Loss: 0.00001288
Iteration 114/1000 | Loss: 0.00001288
Iteration 115/1000 | Loss: 0.00001288
Iteration 116/1000 | Loss: 0.00001288
Iteration 117/1000 | Loss: 0.00001288
Iteration 118/1000 | Loss: 0.00001287
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001285
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001285
Iteration 131/1000 | Loss: 0.00001285
Iteration 132/1000 | Loss: 0.00001284
Iteration 133/1000 | Loss: 0.00001284
Iteration 134/1000 | Loss: 0.00001284
Iteration 135/1000 | Loss: 0.00001284
Iteration 136/1000 | Loss: 0.00001284
Iteration 137/1000 | Loss: 0.00001284
Iteration 138/1000 | Loss: 0.00001284
Iteration 139/1000 | Loss: 0.00001284
Iteration 140/1000 | Loss: 0.00001284
Iteration 141/1000 | Loss: 0.00001284
Iteration 142/1000 | Loss: 0.00001284
Iteration 143/1000 | Loss: 0.00001284
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001283
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001283
Iteration 158/1000 | Loss: 0.00001283
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001282
Iteration 165/1000 | Loss: 0.00001282
Iteration 166/1000 | Loss: 0.00001282
Iteration 167/1000 | Loss: 0.00001282
Iteration 168/1000 | Loss: 0.00001282
Iteration 169/1000 | Loss: 0.00001282
Iteration 170/1000 | Loss: 0.00001282
Iteration 171/1000 | Loss: 0.00001282
Iteration 172/1000 | Loss: 0.00001282
Iteration 173/1000 | Loss: 0.00001282
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001282
Iteration 176/1000 | Loss: 0.00001282
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001281
Iteration 180/1000 | Loss: 0.00001281
Iteration 181/1000 | Loss: 0.00001281
Iteration 182/1000 | Loss: 0.00001281
Iteration 183/1000 | Loss: 0.00001281
Iteration 184/1000 | Loss: 0.00001281
Iteration 185/1000 | Loss: 0.00001281
Iteration 186/1000 | Loss: 0.00001281
Iteration 187/1000 | Loss: 0.00001281
Iteration 188/1000 | Loss: 0.00001281
Iteration 189/1000 | Loss: 0.00001281
Iteration 190/1000 | Loss: 0.00001281
Iteration 191/1000 | Loss: 0.00001281
Iteration 192/1000 | Loss: 0.00001281
Iteration 193/1000 | Loss: 0.00001281
Iteration 194/1000 | Loss: 0.00001281
Iteration 195/1000 | Loss: 0.00001281
Iteration 196/1000 | Loss: 0.00001281
Iteration 197/1000 | Loss: 0.00001281
Iteration 198/1000 | Loss: 0.00001281
Iteration 199/1000 | Loss: 0.00001281
Iteration 200/1000 | Loss: 0.00001281
Iteration 201/1000 | Loss: 0.00001281
Iteration 202/1000 | Loss: 0.00001281
Iteration 203/1000 | Loss: 0.00001281
Iteration 204/1000 | Loss: 0.00001281
Iteration 205/1000 | Loss: 0.00001281
Iteration 206/1000 | Loss: 0.00001281
Iteration 207/1000 | Loss: 0.00001281
Iteration 208/1000 | Loss: 0.00001281
Iteration 209/1000 | Loss: 0.00001281
Iteration 210/1000 | Loss: 0.00001281
Iteration 211/1000 | Loss: 0.00001281
Iteration 212/1000 | Loss: 0.00001281
Iteration 213/1000 | Loss: 0.00001281
Iteration 214/1000 | Loss: 0.00001281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.281363711314043e-05, 1.281363711314043e-05, 1.281363711314043e-05, 1.281363711314043e-05, 1.281363711314043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.281363711314043e-05

Optimization complete. Final v2v error: 3.0903942584991455 mm

Highest mean error: 3.2894859313964844 mm for frame 106

Lowest mean error: 2.9359543323516846 mm for frame 157

Saving results

Total time: 39.6902859210968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817422
Iteration 2/25 | Loss: 0.00135371
Iteration 3/25 | Loss: 0.00127889
Iteration 4/25 | Loss: 0.00126317
Iteration 5/25 | Loss: 0.00125889
Iteration 6/25 | Loss: 0.00125813
Iteration 7/25 | Loss: 0.00125813
Iteration 8/25 | Loss: 0.00125813
Iteration 9/25 | Loss: 0.00125803
Iteration 10/25 | Loss: 0.00125803
Iteration 11/25 | Loss: 0.00125803
Iteration 12/25 | Loss: 0.00125803
Iteration 13/25 | Loss: 0.00125803
Iteration 14/25 | Loss: 0.00125803
Iteration 15/25 | Loss: 0.00125803
Iteration 16/25 | Loss: 0.00125803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012580264592543244, 0.0012580264592543244, 0.0012580264592543244, 0.0012580264592543244, 0.0012580264592543244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012580264592543244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71805453
Iteration 2/25 | Loss: 0.00120126
Iteration 3/25 | Loss: 0.00120123
Iteration 4/25 | Loss: 0.00120123
Iteration 5/25 | Loss: 0.00120123
Iteration 6/25 | Loss: 0.00120123
Iteration 7/25 | Loss: 0.00120123
Iteration 8/25 | Loss: 0.00120123
Iteration 9/25 | Loss: 0.00120123
Iteration 10/25 | Loss: 0.00120123
Iteration 11/25 | Loss: 0.00120123
Iteration 12/25 | Loss: 0.00120123
Iteration 13/25 | Loss: 0.00120123
Iteration 14/25 | Loss: 0.00120123
Iteration 15/25 | Loss: 0.00120123
Iteration 16/25 | Loss: 0.00120123
Iteration 17/25 | Loss: 0.00120123
Iteration 18/25 | Loss: 0.00120123
Iteration 19/25 | Loss: 0.00120123
Iteration 20/25 | Loss: 0.00120123
Iteration 21/25 | Loss: 0.00120123
Iteration 22/25 | Loss: 0.00120123
Iteration 23/25 | Loss: 0.00120123
Iteration 24/25 | Loss: 0.00120123
Iteration 25/25 | Loss: 0.00120123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120123
Iteration 2/1000 | Loss: 0.00003994
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00001903
Iteration 5/1000 | Loss: 0.00001816
Iteration 6/1000 | Loss: 0.00001755
Iteration 7/1000 | Loss: 0.00001711
Iteration 8/1000 | Loss: 0.00001668
Iteration 9/1000 | Loss: 0.00001651
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001591
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001569
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001561
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001555
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001542
Iteration 23/1000 | Loss: 0.00001538
Iteration 24/1000 | Loss: 0.00001538
Iteration 25/1000 | Loss: 0.00001538
Iteration 26/1000 | Loss: 0.00001537
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001535
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001535
Iteration 43/1000 | Loss: 0.00001534
Iteration 44/1000 | Loss: 0.00001534
Iteration 45/1000 | Loss: 0.00001533
Iteration 46/1000 | Loss: 0.00001533
Iteration 47/1000 | Loss: 0.00001533
Iteration 48/1000 | Loss: 0.00001532
Iteration 49/1000 | Loss: 0.00001530
Iteration 50/1000 | Loss: 0.00001530
Iteration 51/1000 | Loss: 0.00001529
Iteration 52/1000 | Loss: 0.00001528
Iteration 53/1000 | Loss: 0.00001527
Iteration 54/1000 | Loss: 0.00001527
Iteration 55/1000 | Loss: 0.00001527
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001525
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001524
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001520
Iteration 72/1000 | Loss: 0.00001520
Iteration 73/1000 | Loss: 0.00001520
Iteration 74/1000 | Loss: 0.00001520
Iteration 75/1000 | Loss: 0.00001519
Iteration 76/1000 | Loss: 0.00001519
Iteration 77/1000 | Loss: 0.00001518
Iteration 78/1000 | Loss: 0.00001518
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001518
Iteration 81/1000 | Loss: 0.00001518
Iteration 82/1000 | Loss: 0.00001518
Iteration 83/1000 | Loss: 0.00001518
Iteration 84/1000 | Loss: 0.00001518
Iteration 85/1000 | Loss: 0.00001518
Iteration 86/1000 | Loss: 0.00001517
Iteration 87/1000 | Loss: 0.00001517
Iteration 88/1000 | Loss: 0.00001516
Iteration 89/1000 | Loss: 0.00001516
Iteration 90/1000 | Loss: 0.00001516
Iteration 91/1000 | Loss: 0.00001515
Iteration 92/1000 | Loss: 0.00001515
Iteration 93/1000 | Loss: 0.00001515
Iteration 94/1000 | Loss: 0.00001515
Iteration 95/1000 | Loss: 0.00001515
Iteration 96/1000 | Loss: 0.00001514
Iteration 97/1000 | Loss: 0.00001514
Iteration 98/1000 | Loss: 0.00001514
Iteration 99/1000 | Loss: 0.00001513
Iteration 100/1000 | Loss: 0.00001513
Iteration 101/1000 | Loss: 0.00001513
Iteration 102/1000 | Loss: 0.00001513
Iteration 103/1000 | Loss: 0.00001513
Iteration 104/1000 | Loss: 0.00001513
Iteration 105/1000 | Loss: 0.00001513
Iteration 106/1000 | Loss: 0.00001513
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001513
Iteration 109/1000 | Loss: 0.00001512
Iteration 110/1000 | Loss: 0.00001512
Iteration 111/1000 | Loss: 0.00001512
Iteration 112/1000 | Loss: 0.00001512
Iteration 113/1000 | Loss: 0.00001511
Iteration 114/1000 | Loss: 0.00001511
Iteration 115/1000 | Loss: 0.00001510
Iteration 116/1000 | Loss: 0.00001510
Iteration 117/1000 | Loss: 0.00001510
Iteration 118/1000 | Loss: 0.00001510
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001509
Iteration 121/1000 | Loss: 0.00001509
Iteration 122/1000 | Loss: 0.00001509
Iteration 123/1000 | Loss: 0.00001509
Iteration 124/1000 | Loss: 0.00001509
Iteration 125/1000 | Loss: 0.00001509
Iteration 126/1000 | Loss: 0.00001509
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Iteration 130/1000 | Loss: 0.00001508
Iteration 131/1000 | Loss: 0.00001508
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001508
Iteration 134/1000 | Loss: 0.00001508
Iteration 135/1000 | Loss: 0.00001508
Iteration 136/1000 | Loss: 0.00001508
Iteration 137/1000 | Loss: 0.00001508
Iteration 138/1000 | Loss: 0.00001508
Iteration 139/1000 | Loss: 0.00001508
Iteration 140/1000 | Loss: 0.00001507
Iteration 141/1000 | Loss: 0.00001507
Iteration 142/1000 | Loss: 0.00001507
Iteration 143/1000 | Loss: 0.00001507
Iteration 144/1000 | Loss: 0.00001507
Iteration 145/1000 | Loss: 0.00001507
Iteration 146/1000 | Loss: 0.00001507
Iteration 147/1000 | Loss: 0.00001507
Iteration 148/1000 | Loss: 0.00001507
Iteration 149/1000 | Loss: 0.00001507
Iteration 150/1000 | Loss: 0.00001507
Iteration 151/1000 | Loss: 0.00001507
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001507
Iteration 154/1000 | Loss: 0.00001507
Iteration 155/1000 | Loss: 0.00001507
Iteration 156/1000 | Loss: 0.00001507
Iteration 157/1000 | Loss: 0.00001507
Iteration 158/1000 | Loss: 0.00001507
Iteration 159/1000 | Loss: 0.00001507
Iteration 160/1000 | Loss: 0.00001507
Iteration 161/1000 | Loss: 0.00001507
Iteration 162/1000 | Loss: 0.00001507
Iteration 163/1000 | Loss: 0.00001507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.506545868323883e-05, 1.506545868323883e-05, 1.506545868323883e-05, 1.506545868323883e-05, 1.506545868323883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.506545868323883e-05

Optimization complete. Final v2v error: 3.2818713188171387 mm

Highest mean error: 3.5212252140045166 mm for frame 112

Lowest mean error: 2.9484307765960693 mm for frame 0

Saving results

Total time: 39.03202748298645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862931
Iteration 2/25 | Loss: 0.00173217
Iteration 3/25 | Loss: 0.00141832
Iteration 4/25 | Loss: 0.00141790
Iteration 5/25 | Loss: 0.00136770
Iteration 6/25 | Loss: 0.00134229
Iteration 7/25 | Loss: 0.00131819
Iteration 8/25 | Loss: 0.00131581
Iteration 9/25 | Loss: 0.00130979
Iteration 10/25 | Loss: 0.00131350
Iteration 11/25 | Loss: 0.00130630
Iteration 12/25 | Loss: 0.00130888
Iteration 13/25 | Loss: 0.00130620
Iteration 14/25 | Loss: 0.00130619
Iteration 15/25 | Loss: 0.00130619
Iteration 16/25 | Loss: 0.00130619
Iteration 17/25 | Loss: 0.00130619
Iteration 18/25 | Loss: 0.00130618
Iteration 19/25 | Loss: 0.00130618
Iteration 20/25 | Loss: 0.00130618
Iteration 21/25 | Loss: 0.00130618
Iteration 22/25 | Loss: 0.00130618
Iteration 23/25 | Loss: 0.00130618
Iteration 24/25 | Loss: 0.00130618
Iteration 25/25 | Loss: 0.00130618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52656841
Iteration 2/25 | Loss: 0.00097266
Iteration 3/25 | Loss: 0.00097260
Iteration 4/25 | Loss: 0.00097260
Iteration 5/25 | Loss: 0.00097260
Iteration 6/25 | Loss: 0.00097260
Iteration 7/25 | Loss: 0.00097260
Iteration 8/25 | Loss: 0.00097260
Iteration 9/25 | Loss: 0.00097260
Iteration 10/25 | Loss: 0.00097260
Iteration 11/25 | Loss: 0.00097260
Iteration 12/25 | Loss: 0.00097260
Iteration 13/25 | Loss: 0.00097260
Iteration 14/25 | Loss: 0.00097260
Iteration 15/25 | Loss: 0.00097260
Iteration 16/25 | Loss: 0.00097260
Iteration 17/25 | Loss: 0.00097260
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009726003627292812, 0.0009726003627292812, 0.0009726003627292812, 0.0009726003627292812, 0.0009726003627292812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009726003627292812

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097260
Iteration 2/1000 | Loss: 0.00005820
Iteration 3/1000 | Loss: 0.00003947
Iteration 4/1000 | Loss: 0.00003387
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00003015
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002840
Iteration 9/1000 | Loss: 0.00002791
Iteration 10/1000 | Loss: 0.00002752
Iteration 11/1000 | Loss: 0.00017934
Iteration 12/1000 | Loss: 0.00003145
Iteration 13/1000 | Loss: 0.00005323
Iteration 14/1000 | Loss: 0.00002706
Iteration 15/1000 | Loss: 0.00004052
Iteration 16/1000 | Loss: 0.00004276
Iteration 17/1000 | Loss: 0.00003494
Iteration 18/1000 | Loss: 0.00002526
Iteration 19/1000 | Loss: 0.00004237
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002492
Iteration 22/1000 | Loss: 0.00005232
Iteration 23/1000 | Loss: 0.00002492
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002486
Iteration 26/1000 | Loss: 0.00002485
Iteration 27/1000 | Loss: 0.00002485
Iteration 28/1000 | Loss: 0.00002480
Iteration 29/1000 | Loss: 0.00002479
Iteration 30/1000 | Loss: 0.00002479
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002478
Iteration 33/1000 | Loss: 0.00002478
Iteration 34/1000 | Loss: 0.00002477
Iteration 35/1000 | Loss: 0.00002475
Iteration 36/1000 | Loss: 0.00002473
Iteration 37/1000 | Loss: 0.00004310
Iteration 38/1000 | Loss: 0.00006419
Iteration 39/1000 | Loss: 0.00002472
Iteration 40/1000 | Loss: 0.00002470
Iteration 41/1000 | Loss: 0.00002470
Iteration 42/1000 | Loss: 0.00002470
Iteration 43/1000 | Loss: 0.00002470
Iteration 44/1000 | Loss: 0.00002470
Iteration 45/1000 | Loss: 0.00002470
Iteration 46/1000 | Loss: 0.00002470
Iteration 47/1000 | Loss: 0.00002470
Iteration 48/1000 | Loss: 0.00002470
Iteration 49/1000 | Loss: 0.00002470
Iteration 50/1000 | Loss: 0.00002470
Iteration 51/1000 | Loss: 0.00002469
Iteration 52/1000 | Loss: 0.00002469
Iteration 53/1000 | Loss: 0.00002469
Iteration 54/1000 | Loss: 0.00002469
Iteration 55/1000 | Loss: 0.00002469
Iteration 56/1000 | Loss: 0.00002469
Iteration 57/1000 | Loss: 0.00002469
Iteration 58/1000 | Loss: 0.00002469
Iteration 59/1000 | Loss: 0.00002469
Iteration 60/1000 | Loss: 0.00002469
Iteration 61/1000 | Loss: 0.00002469
Iteration 62/1000 | Loss: 0.00002469
Iteration 63/1000 | Loss: 0.00002469
Iteration 64/1000 | Loss: 0.00002469
Iteration 65/1000 | Loss: 0.00002468
Iteration 66/1000 | Loss: 0.00002468
Iteration 67/1000 | Loss: 0.00002468
Iteration 68/1000 | Loss: 0.00002468
Iteration 69/1000 | Loss: 0.00002468
Iteration 70/1000 | Loss: 0.00002468
Iteration 71/1000 | Loss: 0.00002468
Iteration 72/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.4684093659743667e-05, 2.4684093659743667e-05, 2.4684093659743667e-05, 2.4684093659743667e-05, 2.4684093659743667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4684093659743667e-05

Optimization complete. Final v2v error: 4.164089679718018 mm

Highest mean error: 5.659315586090088 mm for frame 53

Lowest mean error: 3.464077949523926 mm for frame 151

Saving results

Total time: 70.47948980331421
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842059
Iteration 2/25 | Loss: 0.00144842
Iteration 3/25 | Loss: 0.00130150
Iteration 4/25 | Loss: 0.00128132
Iteration 5/25 | Loss: 0.00127386
Iteration 6/25 | Loss: 0.00127187
Iteration 7/25 | Loss: 0.00127117
Iteration 8/25 | Loss: 0.00127117
Iteration 9/25 | Loss: 0.00127117
Iteration 10/25 | Loss: 0.00127117
Iteration 11/25 | Loss: 0.00127117
Iteration 12/25 | Loss: 0.00127117
Iteration 13/25 | Loss: 0.00127117
Iteration 14/25 | Loss: 0.00127117
Iteration 15/25 | Loss: 0.00127117
Iteration 16/25 | Loss: 0.00127117
Iteration 17/25 | Loss: 0.00127117
Iteration 18/25 | Loss: 0.00127117
Iteration 19/25 | Loss: 0.00127117
Iteration 20/25 | Loss: 0.00127117
Iteration 21/25 | Loss: 0.00127117
Iteration 22/25 | Loss: 0.00127117
Iteration 23/25 | Loss: 0.00127117
Iteration 24/25 | Loss: 0.00127117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012711721938103437, 0.0012711721938103437, 0.0012711721938103437, 0.0012711721938103437, 0.0012711721938103437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012711721938103437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36688566
Iteration 2/25 | Loss: 0.00112215
Iteration 3/25 | Loss: 0.00112214
Iteration 4/25 | Loss: 0.00112214
Iteration 5/25 | Loss: 0.00112214
Iteration 6/25 | Loss: 0.00112214
Iteration 7/25 | Loss: 0.00112214
Iteration 8/25 | Loss: 0.00112214
Iteration 9/25 | Loss: 0.00112214
Iteration 10/25 | Loss: 0.00112214
Iteration 11/25 | Loss: 0.00112214
Iteration 12/25 | Loss: 0.00112214
Iteration 13/25 | Loss: 0.00112214
Iteration 14/25 | Loss: 0.00112214
Iteration 15/25 | Loss: 0.00112214
Iteration 16/25 | Loss: 0.00112214
Iteration 17/25 | Loss: 0.00112214
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011221368331462145, 0.0011221368331462145, 0.0011221368331462145, 0.0011221368331462145, 0.0011221368331462145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011221368331462145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112214
Iteration 2/1000 | Loss: 0.00004708
Iteration 3/1000 | Loss: 0.00003039
Iteration 4/1000 | Loss: 0.00002467
Iteration 5/1000 | Loss: 0.00002271
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00001998
Iteration 9/1000 | Loss: 0.00001954
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001835
Iteration 15/1000 | Loss: 0.00001832
Iteration 16/1000 | Loss: 0.00001827
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001823
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001816
Iteration 24/1000 | Loss: 0.00001815
Iteration 25/1000 | Loss: 0.00001814
Iteration 26/1000 | Loss: 0.00001812
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001809
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001805
Iteration 31/1000 | Loss: 0.00001805
Iteration 32/1000 | Loss: 0.00001804
Iteration 33/1000 | Loss: 0.00001801
Iteration 34/1000 | Loss: 0.00001801
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001796
Iteration 39/1000 | Loss: 0.00001795
Iteration 40/1000 | Loss: 0.00001791
Iteration 41/1000 | Loss: 0.00001790
Iteration 42/1000 | Loss: 0.00001789
Iteration 43/1000 | Loss: 0.00001789
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001785
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001783
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001782
Iteration 71/1000 | Loss: 0.00001782
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001781
Iteration 76/1000 | Loss: 0.00001781
Iteration 77/1000 | Loss: 0.00001781
Iteration 78/1000 | Loss: 0.00001781
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001780
Iteration 83/1000 | Loss: 0.00001779
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001779
Iteration 86/1000 | Loss: 0.00001779
Iteration 87/1000 | Loss: 0.00001779
Iteration 88/1000 | Loss: 0.00001779
Iteration 89/1000 | Loss: 0.00001779
Iteration 90/1000 | Loss: 0.00001779
Iteration 91/1000 | Loss: 0.00001779
Iteration 92/1000 | Loss: 0.00001778
Iteration 93/1000 | Loss: 0.00001778
Iteration 94/1000 | Loss: 0.00001778
Iteration 95/1000 | Loss: 0.00001778
Iteration 96/1000 | Loss: 0.00001778
Iteration 97/1000 | Loss: 0.00001778
Iteration 98/1000 | Loss: 0.00001778
Iteration 99/1000 | Loss: 0.00001777
Iteration 100/1000 | Loss: 0.00001777
Iteration 101/1000 | Loss: 0.00001777
Iteration 102/1000 | Loss: 0.00001777
Iteration 103/1000 | Loss: 0.00001776
Iteration 104/1000 | Loss: 0.00001776
Iteration 105/1000 | Loss: 0.00001776
Iteration 106/1000 | Loss: 0.00001776
Iteration 107/1000 | Loss: 0.00001776
Iteration 108/1000 | Loss: 0.00001776
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001775
Iteration 111/1000 | Loss: 0.00001775
Iteration 112/1000 | Loss: 0.00001775
Iteration 113/1000 | Loss: 0.00001775
Iteration 114/1000 | Loss: 0.00001775
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001775
Iteration 117/1000 | Loss: 0.00001775
Iteration 118/1000 | Loss: 0.00001775
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Iteration 121/1000 | Loss: 0.00001774
Iteration 122/1000 | Loss: 0.00001774
Iteration 123/1000 | Loss: 0.00001773
Iteration 124/1000 | Loss: 0.00001773
Iteration 125/1000 | Loss: 0.00001773
Iteration 126/1000 | Loss: 0.00001773
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001771
Iteration 136/1000 | Loss: 0.00001771
Iteration 137/1000 | Loss: 0.00001771
Iteration 138/1000 | Loss: 0.00001771
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001770
Iteration 141/1000 | Loss: 0.00001770
Iteration 142/1000 | Loss: 0.00001770
Iteration 143/1000 | Loss: 0.00001770
Iteration 144/1000 | Loss: 0.00001770
Iteration 145/1000 | Loss: 0.00001770
Iteration 146/1000 | Loss: 0.00001770
Iteration 147/1000 | Loss: 0.00001770
Iteration 148/1000 | Loss: 0.00001769
Iteration 149/1000 | Loss: 0.00001769
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001769
Iteration 153/1000 | Loss: 0.00001769
Iteration 154/1000 | Loss: 0.00001769
Iteration 155/1000 | Loss: 0.00001769
Iteration 156/1000 | Loss: 0.00001769
Iteration 157/1000 | Loss: 0.00001769
Iteration 158/1000 | Loss: 0.00001769
Iteration 159/1000 | Loss: 0.00001769
Iteration 160/1000 | Loss: 0.00001768
Iteration 161/1000 | Loss: 0.00001768
Iteration 162/1000 | Loss: 0.00001768
Iteration 163/1000 | Loss: 0.00001768
Iteration 164/1000 | Loss: 0.00001767
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Iteration 169/1000 | Loss: 0.00001767
Iteration 170/1000 | Loss: 0.00001767
Iteration 171/1000 | Loss: 0.00001766
Iteration 172/1000 | Loss: 0.00001766
Iteration 173/1000 | Loss: 0.00001766
Iteration 174/1000 | Loss: 0.00001766
Iteration 175/1000 | Loss: 0.00001766
Iteration 176/1000 | Loss: 0.00001765
Iteration 177/1000 | Loss: 0.00001765
Iteration 178/1000 | Loss: 0.00001765
Iteration 179/1000 | Loss: 0.00001765
Iteration 180/1000 | Loss: 0.00001765
Iteration 181/1000 | Loss: 0.00001764
Iteration 182/1000 | Loss: 0.00001764
Iteration 183/1000 | Loss: 0.00001764
Iteration 184/1000 | Loss: 0.00001764
Iteration 185/1000 | Loss: 0.00001764
Iteration 186/1000 | Loss: 0.00001764
Iteration 187/1000 | Loss: 0.00001764
Iteration 188/1000 | Loss: 0.00001764
Iteration 189/1000 | Loss: 0.00001764
Iteration 190/1000 | Loss: 0.00001764
Iteration 191/1000 | Loss: 0.00001764
Iteration 192/1000 | Loss: 0.00001763
Iteration 193/1000 | Loss: 0.00001763
Iteration 194/1000 | Loss: 0.00001763
Iteration 195/1000 | Loss: 0.00001763
Iteration 196/1000 | Loss: 0.00001763
Iteration 197/1000 | Loss: 0.00001763
Iteration 198/1000 | Loss: 0.00001763
Iteration 199/1000 | Loss: 0.00001763
Iteration 200/1000 | Loss: 0.00001762
Iteration 201/1000 | Loss: 0.00001762
Iteration 202/1000 | Loss: 0.00001762
Iteration 203/1000 | Loss: 0.00001762
Iteration 204/1000 | Loss: 0.00001762
Iteration 205/1000 | Loss: 0.00001762
Iteration 206/1000 | Loss: 0.00001762
Iteration 207/1000 | Loss: 0.00001762
Iteration 208/1000 | Loss: 0.00001762
Iteration 209/1000 | Loss: 0.00001762
Iteration 210/1000 | Loss: 0.00001762
Iteration 211/1000 | Loss: 0.00001762
Iteration 212/1000 | Loss: 0.00001762
Iteration 213/1000 | Loss: 0.00001762
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001761
Iteration 216/1000 | Loss: 0.00001761
Iteration 217/1000 | Loss: 0.00001761
Iteration 218/1000 | Loss: 0.00001761
Iteration 219/1000 | Loss: 0.00001761
Iteration 220/1000 | Loss: 0.00001761
Iteration 221/1000 | Loss: 0.00001761
Iteration 222/1000 | Loss: 0.00001761
Iteration 223/1000 | Loss: 0.00001761
Iteration 224/1000 | Loss: 0.00001761
Iteration 225/1000 | Loss: 0.00001761
Iteration 226/1000 | Loss: 0.00001761
Iteration 227/1000 | Loss: 0.00001761
Iteration 228/1000 | Loss: 0.00001761
Iteration 229/1000 | Loss: 0.00001760
Iteration 230/1000 | Loss: 0.00001760
Iteration 231/1000 | Loss: 0.00001760
Iteration 232/1000 | Loss: 0.00001760
Iteration 233/1000 | Loss: 0.00001760
Iteration 234/1000 | Loss: 0.00001760
Iteration 235/1000 | Loss: 0.00001760
Iteration 236/1000 | Loss: 0.00001760
Iteration 237/1000 | Loss: 0.00001760
Iteration 238/1000 | Loss: 0.00001760
Iteration 239/1000 | Loss: 0.00001760
Iteration 240/1000 | Loss: 0.00001760
Iteration 241/1000 | Loss: 0.00001760
Iteration 242/1000 | Loss: 0.00001760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.760317900334485e-05, 1.760317900334485e-05, 1.760317900334485e-05, 1.760317900334485e-05, 1.760317900334485e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.760317900334485e-05

Optimization complete. Final v2v error: 3.471057891845703 mm

Highest mean error: 5.648021697998047 mm for frame 71

Lowest mean error: 2.7215564250946045 mm for frame 126

Saving results

Total time: 48.731550216674805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866404
Iteration 2/25 | Loss: 0.00139452
Iteration 3/25 | Loss: 0.00129817
Iteration 4/25 | Loss: 0.00127680
Iteration 5/25 | Loss: 0.00126974
Iteration 6/25 | Loss: 0.00126822
Iteration 7/25 | Loss: 0.00126822
Iteration 8/25 | Loss: 0.00126822
Iteration 9/25 | Loss: 0.00126822
Iteration 10/25 | Loss: 0.00126822
Iteration 11/25 | Loss: 0.00126822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012682205997407436, 0.0012682205997407436, 0.0012682205997407436, 0.0012682205997407436, 0.0012682205997407436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012682205997407436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36922944
Iteration 2/25 | Loss: 0.00100279
Iteration 3/25 | Loss: 0.00100276
Iteration 4/25 | Loss: 0.00100276
Iteration 5/25 | Loss: 0.00100276
Iteration 6/25 | Loss: 0.00100276
Iteration 7/25 | Loss: 0.00100276
Iteration 8/25 | Loss: 0.00100276
Iteration 9/25 | Loss: 0.00100276
Iteration 10/25 | Loss: 0.00100276
Iteration 11/25 | Loss: 0.00100276
Iteration 12/25 | Loss: 0.00100276
Iteration 13/25 | Loss: 0.00100276
Iteration 14/25 | Loss: 0.00100276
Iteration 15/25 | Loss: 0.00100276
Iteration 16/25 | Loss: 0.00100276
Iteration 17/25 | Loss: 0.00100276
Iteration 18/25 | Loss: 0.00100276
Iteration 19/25 | Loss: 0.00100276
Iteration 20/25 | Loss: 0.00100276
Iteration 21/25 | Loss: 0.00100276
Iteration 22/25 | Loss: 0.00100276
Iteration 23/25 | Loss: 0.00100276
Iteration 24/25 | Loss: 0.00100276
Iteration 25/25 | Loss: 0.00100276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100276
Iteration 2/1000 | Loss: 0.00005356
Iteration 3/1000 | Loss: 0.00003466
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002533
Iteration 7/1000 | Loss: 0.00002440
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002262
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002243
Iteration 15/1000 | Loss: 0.00002234
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002224
Iteration 18/1000 | Loss: 0.00002218
Iteration 19/1000 | Loss: 0.00002215
Iteration 20/1000 | Loss: 0.00002215
Iteration 21/1000 | Loss: 0.00002210
Iteration 22/1000 | Loss: 0.00002209
Iteration 23/1000 | Loss: 0.00002208
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002205
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002200
Iteration 28/1000 | Loss: 0.00002200
Iteration 29/1000 | Loss: 0.00002199
Iteration 30/1000 | Loss: 0.00002199
Iteration 31/1000 | Loss: 0.00002198
Iteration 32/1000 | Loss: 0.00002198
Iteration 33/1000 | Loss: 0.00002198
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002196
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002196
Iteration 44/1000 | Loss: 0.00002196
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002195
Iteration 47/1000 | Loss: 0.00002195
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002193
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002192
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002192
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002192
Iteration 61/1000 | Loss: 0.00002192
Iteration 62/1000 | Loss: 0.00002192
Iteration 63/1000 | Loss: 0.00002191
Iteration 64/1000 | Loss: 0.00002191
Iteration 65/1000 | Loss: 0.00002191
Iteration 66/1000 | Loss: 0.00002191
Iteration 67/1000 | Loss: 0.00002190
Iteration 68/1000 | Loss: 0.00002190
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002189
Iteration 71/1000 | Loss: 0.00002189
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002188
Iteration 75/1000 | Loss: 0.00002188
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002187
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002185
Iteration 88/1000 | Loss: 0.00002184
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002184
Iteration 91/1000 | Loss: 0.00002184
Iteration 92/1000 | Loss: 0.00002184
Iteration 93/1000 | Loss: 0.00002184
Iteration 94/1000 | Loss: 0.00002184
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Iteration 98/1000 | Loss: 0.00002183
Iteration 99/1000 | Loss: 0.00002183
Iteration 100/1000 | Loss: 0.00002183
Iteration 101/1000 | Loss: 0.00002183
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002182
Iteration 110/1000 | Loss: 0.00002181
Iteration 111/1000 | Loss: 0.00002181
Iteration 112/1000 | Loss: 0.00002181
Iteration 113/1000 | Loss: 0.00002181
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002179
Iteration 119/1000 | Loss: 0.00002179
Iteration 120/1000 | Loss: 0.00002179
Iteration 121/1000 | Loss: 0.00002179
Iteration 122/1000 | Loss: 0.00002179
Iteration 123/1000 | Loss: 0.00002178
Iteration 124/1000 | Loss: 0.00002178
Iteration 125/1000 | Loss: 0.00002178
Iteration 126/1000 | Loss: 0.00002178
Iteration 127/1000 | Loss: 0.00002178
Iteration 128/1000 | Loss: 0.00002178
Iteration 129/1000 | Loss: 0.00002178
Iteration 130/1000 | Loss: 0.00002178
Iteration 131/1000 | Loss: 0.00002178
Iteration 132/1000 | Loss: 0.00002178
Iteration 133/1000 | Loss: 0.00002178
Iteration 134/1000 | Loss: 0.00002178
Iteration 135/1000 | Loss: 0.00002178
Iteration 136/1000 | Loss: 0.00002178
Iteration 137/1000 | Loss: 0.00002178
Iteration 138/1000 | Loss: 0.00002178
Iteration 139/1000 | Loss: 0.00002178
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002178
Iteration 144/1000 | Loss: 0.00002178
Iteration 145/1000 | Loss: 0.00002178
Iteration 146/1000 | Loss: 0.00002178
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002178
Iteration 150/1000 | Loss: 0.00002178
Iteration 151/1000 | Loss: 0.00002178
Iteration 152/1000 | Loss: 0.00002178
Iteration 153/1000 | Loss: 0.00002178
Iteration 154/1000 | Loss: 0.00002178
Iteration 155/1000 | Loss: 0.00002178
Iteration 156/1000 | Loss: 0.00002178
Iteration 157/1000 | Loss: 0.00002178
Iteration 158/1000 | Loss: 0.00002178
Iteration 159/1000 | Loss: 0.00002178
Iteration 160/1000 | Loss: 0.00002178
Iteration 161/1000 | Loss: 0.00002178
Iteration 162/1000 | Loss: 0.00002178
Iteration 163/1000 | Loss: 0.00002178
Iteration 164/1000 | Loss: 0.00002178
Iteration 165/1000 | Loss: 0.00002178
Iteration 166/1000 | Loss: 0.00002178
Iteration 167/1000 | Loss: 0.00002178
Iteration 168/1000 | Loss: 0.00002178
Iteration 169/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.1779384042019956e-05, 2.1779384042019956e-05, 2.1779384042019956e-05, 2.1779384042019956e-05, 2.1779384042019956e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1779384042019956e-05

Optimization complete. Final v2v error: 3.9078261852264404 mm

Highest mean error: 5.517075538635254 mm for frame 67

Lowest mean error: 3.3161938190460205 mm for frame 98

Saving results

Total time: 39.43127703666687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855917
Iteration 2/25 | Loss: 0.00165429
Iteration 3/25 | Loss: 0.00141717
Iteration 4/25 | Loss: 0.00138927
Iteration 5/25 | Loss: 0.00137131
Iteration 6/25 | Loss: 0.00136084
Iteration 7/25 | Loss: 0.00135546
Iteration 8/25 | Loss: 0.00135406
Iteration 9/25 | Loss: 0.00135357
Iteration 10/25 | Loss: 0.00135350
Iteration 11/25 | Loss: 0.00135350
Iteration 12/25 | Loss: 0.00135350
Iteration 13/25 | Loss: 0.00135350
Iteration 14/25 | Loss: 0.00135350
Iteration 15/25 | Loss: 0.00135350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013534955214709044, 0.0013534955214709044, 0.0013534955214709044, 0.0013534955214709044, 0.0013534955214709044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013534955214709044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26464045
Iteration 2/25 | Loss: 0.00109854
Iteration 3/25 | Loss: 0.00109850
Iteration 4/25 | Loss: 0.00109850
Iteration 5/25 | Loss: 0.00109850
Iteration 6/25 | Loss: 0.00109850
Iteration 7/25 | Loss: 0.00109850
Iteration 8/25 | Loss: 0.00109850
Iteration 9/25 | Loss: 0.00109850
Iteration 10/25 | Loss: 0.00109850
Iteration 11/25 | Loss: 0.00109850
Iteration 12/25 | Loss: 0.00109850
Iteration 13/25 | Loss: 0.00109850
Iteration 14/25 | Loss: 0.00109850
Iteration 15/25 | Loss: 0.00109850
Iteration 16/25 | Loss: 0.00109850
Iteration 17/25 | Loss: 0.00109850
Iteration 18/25 | Loss: 0.00109850
Iteration 19/25 | Loss: 0.00109850
Iteration 20/25 | Loss: 0.00109850
Iteration 21/25 | Loss: 0.00109850
Iteration 22/25 | Loss: 0.00109850
Iteration 23/25 | Loss: 0.00109850
Iteration 24/25 | Loss: 0.00109850
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010984950931742787, 0.0010984950931742787, 0.0010984950931742787, 0.0010984950931742787, 0.0010984950931742787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010984950931742787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109850
Iteration 2/1000 | Loss: 0.00006860
Iteration 3/1000 | Loss: 0.00003857
Iteration 4/1000 | Loss: 0.00003162
Iteration 5/1000 | Loss: 0.00002912
Iteration 6/1000 | Loss: 0.00002762
Iteration 7/1000 | Loss: 0.00002667
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002530
Iteration 10/1000 | Loss: 0.00002496
Iteration 11/1000 | Loss: 0.00002464
Iteration 12/1000 | Loss: 0.00002440
Iteration 13/1000 | Loss: 0.00002419
Iteration 14/1000 | Loss: 0.00002404
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002393
Iteration 17/1000 | Loss: 0.00002391
Iteration 18/1000 | Loss: 0.00002389
Iteration 19/1000 | Loss: 0.00002384
Iteration 20/1000 | Loss: 0.00002380
Iteration 21/1000 | Loss: 0.00002380
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002378
Iteration 24/1000 | Loss: 0.00002378
Iteration 25/1000 | Loss: 0.00002377
Iteration 26/1000 | Loss: 0.00002377
Iteration 27/1000 | Loss: 0.00002376
Iteration 28/1000 | Loss: 0.00002376
Iteration 29/1000 | Loss: 0.00002375
Iteration 30/1000 | Loss: 0.00002375
Iteration 31/1000 | Loss: 0.00002374
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002374
Iteration 34/1000 | Loss: 0.00002373
Iteration 35/1000 | Loss: 0.00002373
Iteration 36/1000 | Loss: 0.00002372
Iteration 37/1000 | Loss: 0.00002372
Iteration 38/1000 | Loss: 0.00002371
Iteration 39/1000 | Loss: 0.00002371
Iteration 40/1000 | Loss: 0.00002371
Iteration 41/1000 | Loss: 0.00002370
Iteration 42/1000 | Loss: 0.00002370
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002369
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00002368
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002366
Iteration 52/1000 | Loss: 0.00002366
Iteration 53/1000 | Loss: 0.00002366
Iteration 54/1000 | Loss: 0.00002365
Iteration 55/1000 | Loss: 0.00002365
Iteration 56/1000 | Loss: 0.00002364
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00002364
Iteration 59/1000 | Loss: 0.00002363
Iteration 60/1000 | Loss: 0.00002363
Iteration 61/1000 | Loss: 0.00002363
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002362
Iteration 66/1000 | Loss: 0.00002362
Iteration 67/1000 | Loss: 0.00002362
Iteration 68/1000 | Loss: 0.00002362
Iteration 69/1000 | Loss: 0.00002361
Iteration 70/1000 | Loss: 0.00002361
Iteration 71/1000 | Loss: 0.00002361
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00002361
Iteration 74/1000 | Loss: 0.00002361
Iteration 75/1000 | Loss: 0.00002361
Iteration 76/1000 | Loss: 0.00002361
Iteration 77/1000 | Loss: 0.00002361
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002360
Iteration 81/1000 | Loss: 0.00002360
Iteration 82/1000 | Loss: 0.00002360
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002359
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002359
Iteration 95/1000 | Loss: 0.00002359
Iteration 96/1000 | Loss: 0.00002359
Iteration 97/1000 | Loss: 0.00002359
Iteration 98/1000 | Loss: 0.00002359
Iteration 99/1000 | Loss: 0.00002359
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002359
Iteration 102/1000 | Loss: 0.00002359
Iteration 103/1000 | Loss: 0.00002359
Iteration 104/1000 | Loss: 0.00002358
Iteration 105/1000 | Loss: 0.00002358
Iteration 106/1000 | Loss: 0.00002358
Iteration 107/1000 | Loss: 0.00002358
Iteration 108/1000 | Loss: 0.00002358
Iteration 109/1000 | Loss: 0.00002358
Iteration 110/1000 | Loss: 0.00002358
Iteration 111/1000 | Loss: 0.00002358
Iteration 112/1000 | Loss: 0.00002358
Iteration 113/1000 | Loss: 0.00002358
Iteration 114/1000 | Loss: 0.00002358
Iteration 115/1000 | Loss: 0.00002358
Iteration 116/1000 | Loss: 0.00002358
Iteration 117/1000 | Loss: 0.00002358
Iteration 118/1000 | Loss: 0.00002358
Iteration 119/1000 | Loss: 0.00002358
Iteration 120/1000 | Loss: 0.00002358
Iteration 121/1000 | Loss: 0.00002358
Iteration 122/1000 | Loss: 0.00002358
Iteration 123/1000 | Loss: 0.00002358
Iteration 124/1000 | Loss: 0.00002358
Iteration 125/1000 | Loss: 0.00002358
Iteration 126/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.3582842914038338e-05, 2.3582842914038338e-05, 2.3582842914038338e-05, 2.3582842914038338e-05, 2.3582842914038338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3582842914038338e-05

Optimization complete. Final v2v error: 4.110033988952637 mm

Highest mean error: 4.647195816040039 mm for frame 220

Lowest mean error: 3.6721580028533936 mm for frame 30

Saving results

Total time: 50.14074087142944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556102
Iteration 2/25 | Loss: 0.00140667
Iteration 3/25 | Loss: 0.00127367
Iteration 4/25 | Loss: 0.00125274
Iteration 5/25 | Loss: 0.00125773
Iteration 6/25 | Loss: 0.00124610
Iteration 7/25 | Loss: 0.00124553
Iteration 8/25 | Loss: 0.00125366
Iteration 9/25 | Loss: 0.00124353
Iteration 10/25 | Loss: 0.00124499
Iteration 11/25 | Loss: 0.00124151
Iteration 12/25 | Loss: 0.00124148
Iteration 13/25 | Loss: 0.00124148
Iteration 14/25 | Loss: 0.00124148
Iteration 15/25 | Loss: 0.00124148
Iteration 16/25 | Loss: 0.00124148
Iteration 17/25 | Loss: 0.00124148
Iteration 18/25 | Loss: 0.00124148
Iteration 19/25 | Loss: 0.00124148
Iteration 20/25 | Loss: 0.00124148
Iteration 21/25 | Loss: 0.00124148
Iteration 22/25 | Loss: 0.00124148
Iteration 23/25 | Loss: 0.00124148
Iteration 24/25 | Loss: 0.00124148
Iteration 25/25 | Loss: 0.00124148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21496296
Iteration 2/25 | Loss: 0.00102493
Iteration 3/25 | Loss: 0.00096956
Iteration 4/25 | Loss: 0.00096956
Iteration 5/25 | Loss: 0.00096956
Iteration 6/25 | Loss: 0.00096956
Iteration 7/25 | Loss: 0.00096956
Iteration 8/25 | Loss: 0.00096956
Iteration 9/25 | Loss: 0.00096956
Iteration 10/25 | Loss: 0.00096956
Iteration 11/25 | Loss: 0.00096956
Iteration 12/25 | Loss: 0.00096956
Iteration 13/25 | Loss: 0.00096956
Iteration 14/25 | Loss: 0.00096956
Iteration 15/25 | Loss: 0.00096956
Iteration 16/25 | Loss: 0.00096956
Iteration 17/25 | Loss: 0.00096956
Iteration 18/25 | Loss: 0.00096956
Iteration 19/25 | Loss: 0.00096956
Iteration 20/25 | Loss: 0.00096956
Iteration 21/25 | Loss: 0.00096956
Iteration 22/25 | Loss: 0.00096956
Iteration 23/25 | Loss: 0.00096956
Iteration 24/25 | Loss: 0.00096956
Iteration 25/25 | Loss: 0.00096956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096956
Iteration 2/1000 | Loss: 0.00008744
Iteration 3/1000 | Loss: 0.00001804
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00003428
Iteration 7/1000 | Loss: 0.00002150
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001795
Iteration 10/1000 | Loss: 0.00002042
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001370
Iteration 18/1000 | Loss: 0.00001369
Iteration 19/1000 | Loss: 0.00001369
Iteration 20/1000 | Loss: 0.00001368
Iteration 21/1000 | Loss: 0.00005280
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001347
Iteration 25/1000 | Loss: 0.00001346
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001345
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001344
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001342
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001341
Iteration 44/1000 | Loss: 0.00001341
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001340
Iteration 51/1000 | Loss: 0.00001340
Iteration 52/1000 | Loss: 0.00001340
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001336
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00003196
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001328
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00005994
Iteration 99/1000 | Loss: 0.00003544
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001321
Iteration 102/1000 | Loss: 0.00001320
Iteration 103/1000 | Loss: 0.00001320
Iteration 104/1000 | Loss: 0.00001320
Iteration 105/1000 | Loss: 0.00001320
Iteration 106/1000 | Loss: 0.00001319
Iteration 107/1000 | Loss: 0.00001319
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001316
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001315
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001314
Iteration 130/1000 | Loss: 0.00001314
Iteration 131/1000 | Loss: 0.00001314
Iteration 132/1000 | Loss: 0.00001314
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001313
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.3133398169884458e-05, 1.3133398169884458e-05, 1.3133398169884458e-05, 1.3133398169884458e-05, 1.3133398169884458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3133398169884458e-05

Optimization complete. Final v2v error: 3.072021245956421 mm

Highest mean error: 3.428170680999756 mm for frame 190

Lowest mean error: 2.844644069671631 mm for frame 171

Saving results

Total time: 65.65923595428467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00700963
Iteration 2/25 | Loss: 0.00183781
Iteration 3/25 | Loss: 0.00152612
Iteration 4/25 | Loss: 0.00144684
Iteration 5/25 | Loss: 0.00138172
Iteration 6/25 | Loss: 0.00133125
Iteration 7/25 | Loss: 0.00132211
Iteration 8/25 | Loss: 0.00131041
Iteration 9/25 | Loss: 0.00130781
Iteration 10/25 | Loss: 0.00130595
Iteration 11/25 | Loss: 0.00130444
Iteration 12/25 | Loss: 0.00130315
Iteration 13/25 | Loss: 0.00130194
Iteration 14/25 | Loss: 0.00130115
Iteration 15/25 | Loss: 0.00130081
Iteration 16/25 | Loss: 0.00130054
Iteration 17/25 | Loss: 0.00130032
Iteration 18/25 | Loss: 0.00130017
Iteration 19/25 | Loss: 0.00130014
Iteration 20/25 | Loss: 0.00130014
Iteration 21/25 | Loss: 0.00130014
Iteration 22/25 | Loss: 0.00130014
Iteration 23/25 | Loss: 0.00130014
Iteration 24/25 | Loss: 0.00130014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013001359766349196, 0.0013001359766349196, 0.0013001359766349196, 0.0013001359766349196, 0.0013001359766349196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013001359766349196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.69324207
Iteration 2/25 | Loss: 0.00324110
Iteration 3/25 | Loss: 0.00188933
Iteration 4/25 | Loss: 0.00188933
Iteration 5/25 | Loss: 0.00188933
Iteration 6/25 | Loss: 0.00188933
Iteration 7/25 | Loss: 0.00188933
Iteration 8/25 | Loss: 0.00188933
Iteration 9/25 | Loss: 0.00188933
Iteration 10/25 | Loss: 0.00188933
Iteration 11/25 | Loss: 0.00188933
Iteration 12/25 | Loss: 0.00188933
Iteration 13/25 | Loss: 0.00188933
Iteration 14/25 | Loss: 0.00188933
Iteration 15/25 | Loss: 0.00188933
Iteration 16/25 | Loss: 0.00188933
Iteration 17/25 | Loss: 0.00188933
Iteration 18/25 | Loss: 0.00188933
Iteration 19/25 | Loss: 0.00188933
Iteration 20/25 | Loss: 0.00188933
Iteration 21/25 | Loss: 0.00188933
Iteration 22/25 | Loss: 0.00188933
Iteration 23/25 | Loss: 0.00188933
Iteration 24/25 | Loss: 0.00188933
Iteration 25/25 | Loss: 0.00188933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188933
Iteration 2/1000 | Loss: 0.00032603
Iteration 3/1000 | Loss: 0.00100494
Iteration 4/1000 | Loss: 0.00231323
Iteration 5/1000 | Loss: 0.00130894
Iteration 6/1000 | Loss: 0.00009722
Iteration 7/1000 | Loss: 0.00312075
Iteration 8/1000 | Loss: 0.00727820
Iteration 9/1000 | Loss: 0.00379593
Iteration 10/1000 | Loss: 0.00484558
Iteration 11/1000 | Loss: 0.00653873
Iteration 12/1000 | Loss: 0.00185457
Iteration 13/1000 | Loss: 0.00341288
Iteration 14/1000 | Loss: 0.00673585
Iteration 15/1000 | Loss: 0.00455552
Iteration 16/1000 | Loss: 0.00461496
Iteration 17/1000 | Loss: 0.00299597
Iteration 18/1000 | Loss: 0.00482206
Iteration 19/1000 | Loss: 0.00344245
Iteration 20/1000 | Loss: 0.00209268
Iteration 21/1000 | Loss: 0.00100663
Iteration 22/1000 | Loss: 0.00064897
Iteration 23/1000 | Loss: 0.00079485
Iteration 24/1000 | Loss: 0.00136286
Iteration 25/1000 | Loss: 0.00091934
Iteration 26/1000 | Loss: 0.00076262
Iteration 27/1000 | Loss: 0.00147424
Iteration 28/1000 | Loss: 0.00074373
Iteration 29/1000 | Loss: 0.00033952
Iteration 30/1000 | Loss: 0.00079822
Iteration 31/1000 | Loss: 0.00035553
Iteration 32/1000 | Loss: 0.00034772
Iteration 33/1000 | Loss: 0.00018827
Iteration 34/1000 | Loss: 0.00014339
Iteration 35/1000 | Loss: 0.00042337
Iteration 36/1000 | Loss: 0.00022019
Iteration 37/1000 | Loss: 0.00017766
Iteration 38/1000 | Loss: 0.00004654
Iteration 39/1000 | Loss: 0.00009235
Iteration 40/1000 | Loss: 0.00004174
Iteration 41/1000 | Loss: 0.00011565
Iteration 42/1000 | Loss: 0.00011826
Iteration 43/1000 | Loss: 0.00036570
Iteration 44/1000 | Loss: 0.00010189
Iteration 45/1000 | Loss: 0.00011647
Iteration 46/1000 | Loss: 0.00012386
Iteration 47/1000 | Loss: 0.00011230
Iteration 48/1000 | Loss: 0.00011285
Iteration 49/1000 | Loss: 0.00057242
Iteration 50/1000 | Loss: 0.00014555
Iteration 51/1000 | Loss: 0.00007977
Iteration 52/1000 | Loss: 0.00007320
Iteration 53/1000 | Loss: 0.00011898
Iteration 54/1000 | Loss: 0.00039651
Iteration 55/1000 | Loss: 0.00036540
Iteration 56/1000 | Loss: 0.00042822
Iteration 57/1000 | Loss: 0.00015980
Iteration 58/1000 | Loss: 0.00011920
Iteration 59/1000 | Loss: 0.00050989
Iteration 60/1000 | Loss: 0.00027853
Iteration 61/1000 | Loss: 0.00030714
Iteration 62/1000 | Loss: 0.00025296
Iteration 63/1000 | Loss: 0.00024938
Iteration 64/1000 | Loss: 0.00061022
Iteration 65/1000 | Loss: 0.00016930
Iteration 66/1000 | Loss: 0.00029824
Iteration 67/1000 | Loss: 0.00015852
Iteration 68/1000 | Loss: 0.00012467
Iteration 69/1000 | Loss: 0.00026000
Iteration 70/1000 | Loss: 0.00016897
Iteration 71/1000 | Loss: 0.00016009
Iteration 72/1000 | Loss: 0.00013143
Iteration 73/1000 | Loss: 0.00014416
Iteration 74/1000 | Loss: 0.00016885
Iteration 75/1000 | Loss: 0.00028880
Iteration 76/1000 | Loss: 0.00016423
Iteration 77/1000 | Loss: 0.00030076
Iteration 78/1000 | Loss: 0.00009154
Iteration 79/1000 | Loss: 0.00017615
Iteration 80/1000 | Loss: 0.00018435
Iteration 81/1000 | Loss: 0.00046646
Iteration 82/1000 | Loss: 0.00013549
Iteration 83/1000 | Loss: 0.00013233
Iteration 84/1000 | Loss: 0.00012439
Iteration 85/1000 | Loss: 0.00011565
Iteration 86/1000 | Loss: 0.00034299
Iteration 87/1000 | Loss: 0.00011216
Iteration 88/1000 | Loss: 0.00017966
Iteration 89/1000 | Loss: 0.00029641
Iteration 90/1000 | Loss: 0.00009535
Iteration 91/1000 | Loss: 0.00010774
Iteration 92/1000 | Loss: 0.00006419
Iteration 93/1000 | Loss: 0.00007076
Iteration 94/1000 | Loss: 0.00007267
Iteration 95/1000 | Loss: 0.00021578
Iteration 96/1000 | Loss: 0.00040541
Iteration 97/1000 | Loss: 0.00020941
Iteration 98/1000 | Loss: 0.00015255
Iteration 99/1000 | Loss: 0.00016291
Iteration 100/1000 | Loss: 0.00011240
Iteration 101/1000 | Loss: 0.00014334
Iteration 102/1000 | Loss: 0.00007933
Iteration 103/1000 | Loss: 0.00009829
Iteration 104/1000 | Loss: 0.00064173
Iteration 105/1000 | Loss: 0.00022542
Iteration 106/1000 | Loss: 0.00005675
Iteration 107/1000 | Loss: 0.00005241
Iteration 108/1000 | Loss: 0.00003332
Iteration 109/1000 | Loss: 0.00018674
Iteration 110/1000 | Loss: 0.00005003
Iteration 111/1000 | Loss: 0.00025281
Iteration 112/1000 | Loss: 0.00017408
Iteration 113/1000 | Loss: 0.00003052
Iteration 114/1000 | Loss: 0.00002669
Iteration 115/1000 | Loss: 0.00002490
Iteration 116/1000 | Loss: 0.00002381
Iteration 117/1000 | Loss: 0.00034417
Iteration 118/1000 | Loss: 0.00002304
Iteration 119/1000 | Loss: 0.00002227
Iteration 120/1000 | Loss: 0.00002185
Iteration 121/1000 | Loss: 0.00002165
Iteration 122/1000 | Loss: 0.00002142
Iteration 123/1000 | Loss: 0.00031745
Iteration 124/1000 | Loss: 0.00002737
Iteration 125/1000 | Loss: 0.00002232
Iteration 126/1000 | Loss: 0.00029807
Iteration 127/1000 | Loss: 0.00002613
Iteration 128/1000 | Loss: 0.00002046
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001753
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001683
Iteration 135/1000 | Loss: 0.00001681
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001664
Iteration 139/1000 | Loss: 0.00001664
Iteration 140/1000 | Loss: 0.00001659
Iteration 141/1000 | Loss: 0.00001643
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001623
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001612
Iteration 149/1000 | Loss: 0.00001612
Iteration 150/1000 | Loss: 0.00001612
Iteration 151/1000 | Loss: 0.00001612
Iteration 152/1000 | Loss: 0.00001611
Iteration 153/1000 | Loss: 0.00001609
Iteration 154/1000 | Loss: 0.00001609
Iteration 155/1000 | Loss: 0.00001609
Iteration 156/1000 | Loss: 0.00001608
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001599
Iteration 160/1000 | Loss: 0.00001597
Iteration 161/1000 | Loss: 0.00001592
Iteration 162/1000 | Loss: 0.00001592
Iteration 163/1000 | Loss: 0.00001592
Iteration 164/1000 | Loss: 0.00001592
Iteration 165/1000 | Loss: 0.00001592
Iteration 166/1000 | Loss: 0.00001592
Iteration 167/1000 | Loss: 0.00001592
Iteration 168/1000 | Loss: 0.00001592
Iteration 169/1000 | Loss: 0.00001591
Iteration 170/1000 | Loss: 0.00001590
Iteration 171/1000 | Loss: 0.00001588
Iteration 172/1000 | Loss: 0.00001588
Iteration 173/1000 | Loss: 0.00001588
Iteration 174/1000 | Loss: 0.00001587
Iteration 175/1000 | Loss: 0.00001587
Iteration 176/1000 | Loss: 0.00001587
Iteration 177/1000 | Loss: 0.00001587
Iteration 178/1000 | Loss: 0.00001587
Iteration 179/1000 | Loss: 0.00001587
Iteration 180/1000 | Loss: 0.00001587
Iteration 181/1000 | Loss: 0.00001587
Iteration 182/1000 | Loss: 0.00001587
Iteration 183/1000 | Loss: 0.00001586
Iteration 184/1000 | Loss: 0.00001586
Iteration 185/1000 | Loss: 0.00001586
Iteration 186/1000 | Loss: 0.00001586
Iteration 187/1000 | Loss: 0.00001586
Iteration 188/1000 | Loss: 0.00001586
Iteration 189/1000 | Loss: 0.00001585
Iteration 190/1000 | Loss: 0.00001584
Iteration 191/1000 | Loss: 0.00001583
Iteration 192/1000 | Loss: 0.00001583
Iteration 193/1000 | Loss: 0.00001583
Iteration 194/1000 | Loss: 0.00001583
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001581
Iteration 198/1000 | Loss: 0.00001581
Iteration 199/1000 | Loss: 0.00001581
Iteration 200/1000 | Loss: 0.00001580
Iteration 201/1000 | Loss: 0.00001580
Iteration 202/1000 | Loss: 0.00001580
Iteration 203/1000 | Loss: 0.00001579
Iteration 204/1000 | Loss: 0.00001578
Iteration 205/1000 | Loss: 0.00001578
Iteration 206/1000 | Loss: 0.00001578
Iteration 207/1000 | Loss: 0.00001578
Iteration 208/1000 | Loss: 0.00001577
Iteration 209/1000 | Loss: 0.00001577
Iteration 210/1000 | Loss: 0.00001577
Iteration 211/1000 | Loss: 0.00001576
Iteration 212/1000 | Loss: 0.00001576
Iteration 213/1000 | Loss: 0.00001576
Iteration 214/1000 | Loss: 0.00001576
Iteration 215/1000 | Loss: 0.00001576
Iteration 216/1000 | Loss: 0.00001576
Iteration 217/1000 | Loss: 0.00001576
Iteration 218/1000 | Loss: 0.00001576
Iteration 219/1000 | Loss: 0.00001576
Iteration 220/1000 | Loss: 0.00001576
Iteration 221/1000 | Loss: 0.00001576
Iteration 222/1000 | Loss: 0.00001576
Iteration 223/1000 | Loss: 0.00001576
Iteration 224/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.576197064423468e-05, 1.576197064423468e-05, 1.576197064423468e-05, 1.576197064423468e-05, 1.576197064423468e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.576197064423468e-05

Optimization complete. Final v2v error: 3.2113492488861084 mm

Highest mean error: 12.875689506530762 mm for frame 0

Lowest mean error: 2.883080005645752 mm for frame 78

Saving results

Total time: 263.52577900886536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764124
Iteration 2/25 | Loss: 0.00138496
Iteration 3/25 | Loss: 0.00125659
Iteration 4/25 | Loss: 0.00121578
Iteration 5/25 | Loss: 0.00120255
Iteration 6/25 | Loss: 0.00119699
Iteration 7/25 | Loss: 0.00119521
Iteration 8/25 | Loss: 0.00119472
Iteration 9/25 | Loss: 0.00119459
Iteration 10/25 | Loss: 0.00119456
Iteration 11/25 | Loss: 0.00119455
Iteration 12/25 | Loss: 0.00119455
Iteration 13/25 | Loss: 0.00119455
Iteration 14/25 | Loss: 0.00119455
Iteration 15/25 | Loss: 0.00119455
Iteration 16/25 | Loss: 0.00119455
Iteration 17/25 | Loss: 0.00119455
Iteration 18/25 | Loss: 0.00119455
Iteration 19/25 | Loss: 0.00119454
Iteration 20/25 | Loss: 0.00119454
Iteration 21/25 | Loss: 0.00119454
Iteration 22/25 | Loss: 0.00119454
Iteration 23/25 | Loss: 0.00119454
Iteration 24/25 | Loss: 0.00119454
Iteration 25/25 | Loss: 0.00119454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38870668
Iteration 2/25 | Loss: 0.00150690
Iteration 3/25 | Loss: 0.00150687
Iteration 4/25 | Loss: 0.00150686
Iteration 5/25 | Loss: 0.00150686
Iteration 6/25 | Loss: 0.00150686
Iteration 7/25 | Loss: 0.00150686
Iteration 8/25 | Loss: 0.00150686
Iteration 9/25 | Loss: 0.00150686
Iteration 10/25 | Loss: 0.00150686
Iteration 11/25 | Loss: 0.00150686
Iteration 12/25 | Loss: 0.00150686
Iteration 13/25 | Loss: 0.00150686
Iteration 14/25 | Loss: 0.00150686
Iteration 15/25 | Loss: 0.00150686
Iteration 16/25 | Loss: 0.00150686
Iteration 17/25 | Loss: 0.00150686
Iteration 18/25 | Loss: 0.00150686
Iteration 19/25 | Loss: 0.00150686
Iteration 20/25 | Loss: 0.00150686
Iteration 21/25 | Loss: 0.00150686
Iteration 22/25 | Loss: 0.00150686
Iteration 23/25 | Loss: 0.00150686
Iteration 24/25 | Loss: 0.00150686
Iteration 25/25 | Loss: 0.00150686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150686
Iteration 2/1000 | Loss: 0.00004228
Iteration 3/1000 | Loss: 0.00002709
Iteration 4/1000 | Loss: 0.00002018
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001687
Iteration 8/1000 | Loss: 0.00001628
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001536
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001510
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001488
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001484
Iteration 23/1000 | Loss: 0.00001484
Iteration 24/1000 | Loss: 0.00001483
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001478
Iteration 29/1000 | Loss: 0.00001478
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001475
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001474
Iteration 50/1000 | Loss: 0.00001474
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001470
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001469
Iteration 62/1000 | Loss: 0.00001469
Iteration 63/1000 | Loss: 0.00001469
Iteration 64/1000 | Loss: 0.00001469
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001468
Iteration 67/1000 | Loss: 0.00001468
Iteration 68/1000 | Loss: 0.00001468
Iteration 69/1000 | Loss: 0.00001468
Iteration 70/1000 | Loss: 0.00001468
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001467
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001467
Iteration 77/1000 | Loss: 0.00001467
Iteration 78/1000 | Loss: 0.00001467
Iteration 79/1000 | Loss: 0.00001467
Iteration 80/1000 | Loss: 0.00001467
Iteration 81/1000 | Loss: 0.00001467
Iteration 82/1000 | Loss: 0.00001467
Iteration 83/1000 | Loss: 0.00001466
Iteration 84/1000 | Loss: 0.00001466
Iteration 85/1000 | Loss: 0.00001466
Iteration 86/1000 | Loss: 0.00001465
Iteration 87/1000 | Loss: 0.00001465
Iteration 88/1000 | Loss: 0.00001465
Iteration 89/1000 | Loss: 0.00001465
Iteration 90/1000 | Loss: 0.00001465
Iteration 91/1000 | Loss: 0.00001465
Iteration 92/1000 | Loss: 0.00001465
Iteration 93/1000 | Loss: 0.00001465
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001464
Iteration 97/1000 | Loss: 0.00001464
Iteration 98/1000 | Loss: 0.00001464
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001463
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001462
Iteration 107/1000 | Loss: 0.00001462
Iteration 108/1000 | Loss: 0.00001462
Iteration 109/1000 | Loss: 0.00001462
Iteration 110/1000 | Loss: 0.00001461
Iteration 111/1000 | Loss: 0.00001461
Iteration 112/1000 | Loss: 0.00001461
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00001460
Iteration 115/1000 | Loss: 0.00001460
Iteration 116/1000 | Loss: 0.00001460
Iteration 117/1000 | Loss: 0.00001460
Iteration 118/1000 | Loss: 0.00001460
Iteration 119/1000 | Loss: 0.00001460
Iteration 120/1000 | Loss: 0.00001460
Iteration 121/1000 | Loss: 0.00001460
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001460
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001459
Iteration 129/1000 | Loss: 0.00001459
Iteration 130/1000 | Loss: 0.00001459
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001458
Iteration 144/1000 | Loss: 0.00001458
Iteration 145/1000 | Loss: 0.00001458
Iteration 146/1000 | Loss: 0.00001458
Iteration 147/1000 | Loss: 0.00001458
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001458
Iteration 155/1000 | Loss: 0.00001458
Iteration 156/1000 | Loss: 0.00001458
Iteration 157/1000 | Loss: 0.00001458
Iteration 158/1000 | Loss: 0.00001458
Iteration 159/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4580292372556869e-05, 1.4580292372556869e-05, 1.4580292372556869e-05, 1.4580292372556869e-05, 1.4580292372556869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4580292372556869e-05

Optimization complete. Final v2v error: 3.2901206016540527 mm

Highest mean error: 3.7868497371673584 mm for frame 54

Lowest mean error: 2.944018840789795 mm for frame 117

Saving results

Total time: 47.51474571228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00939376
Iteration 2/25 | Loss: 0.00303016
Iteration 3/25 | Loss: 0.00238410
Iteration 4/25 | Loss: 0.00203916
Iteration 5/25 | Loss: 0.00194770
Iteration 6/25 | Loss: 0.00190919
Iteration 7/25 | Loss: 0.00182558
Iteration 8/25 | Loss: 0.00177734
Iteration 9/25 | Loss: 0.00172936
Iteration 10/25 | Loss: 0.00170424
Iteration 11/25 | Loss: 0.00166316
Iteration 12/25 | Loss: 0.00166686
Iteration 13/25 | Loss: 0.00163589
Iteration 14/25 | Loss: 0.00163643
Iteration 15/25 | Loss: 0.00165493
Iteration 16/25 | Loss: 0.00160619
Iteration 17/25 | Loss: 0.00159359
Iteration 18/25 | Loss: 0.00157632
Iteration 19/25 | Loss: 0.00157328
Iteration 20/25 | Loss: 0.00156718
Iteration 21/25 | Loss: 0.00156688
Iteration 22/25 | Loss: 0.00156675
Iteration 23/25 | Loss: 0.00156666
Iteration 24/25 | Loss: 0.00156652
Iteration 25/25 | Loss: 0.00156633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32663083
Iteration 2/25 | Loss: 0.00338037
Iteration 3/25 | Loss: 0.00298122
Iteration 4/25 | Loss: 0.00298122
Iteration 5/25 | Loss: 0.00298122
Iteration 6/25 | Loss: 0.00298122
Iteration 7/25 | Loss: 0.00298122
Iteration 8/25 | Loss: 0.00298122
Iteration 9/25 | Loss: 0.00298122
Iteration 10/25 | Loss: 0.00298122
Iteration 11/25 | Loss: 0.00298122
Iteration 12/25 | Loss: 0.00298122
Iteration 13/25 | Loss: 0.00298122
Iteration 14/25 | Loss: 0.00298122
Iteration 15/25 | Loss: 0.00298122
Iteration 16/25 | Loss: 0.00298122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029812203720211983, 0.0029812203720211983, 0.0029812203720211983, 0.0029812203720211983, 0.0029812203720211983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029812203720211983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298122
Iteration 2/1000 | Loss: 0.00078215
Iteration 3/1000 | Loss: 0.00032532
Iteration 4/1000 | Loss: 0.00077335
Iteration 5/1000 | Loss: 0.00055363
Iteration 6/1000 | Loss: 0.00127767
Iteration 7/1000 | Loss: 0.00020687
Iteration 8/1000 | Loss: 0.00069701
Iteration 9/1000 | Loss: 0.00053365
Iteration 10/1000 | Loss: 0.00104252
Iteration 11/1000 | Loss: 0.00071687
Iteration 12/1000 | Loss: 0.00047374
Iteration 13/1000 | Loss: 0.00076313
Iteration 14/1000 | Loss: 0.00028438
Iteration 15/1000 | Loss: 0.00022910
Iteration 16/1000 | Loss: 0.00071033
Iteration 17/1000 | Loss: 0.00016054
Iteration 18/1000 | Loss: 0.00022978
Iteration 19/1000 | Loss: 0.00013211
Iteration 20/1000 | Loss: 0.00025662
Iteration 21/1000 | Loss: 0.00014116
Iteration 22/1000 | Loss: 0.00017957
Iteration 23/1000 | Loss: 0.00045301
Iteration 24/1000 | Loss: 0.00012644
Iteration 25/1000 | Loss: 0.00012000
Iteration 26/1000 | Loss: 0.00011781
Iteration 27/1000 | Loss: 0.00011591
Iteration 28/1000 | Loss: 0.00018419
Iteration 29/1000 | Loss: 0.00104176
Iteration 30/1000 | Loss: 0.00020640
Iteration 31/1000 | Loss: 0.00014778
Iteration 32/1000 | Loss: 0.00020974
Iteration 33/1000 | Loss: 0.00011154
Iteration 34/1000 | Loss: 0.00011029
Iteration 35/1000 | Loss: 0.00010943
Iteration 36/1000 | Loss: 0.00041474
Iteration 37/1000 | Loss: 0.00010886
Iteration 38/1000 | Loss: 0.00010823
Iteration 39/1000 | Loss: 0.00010794
Iteration 40/1000 | Loss: 0.00010758
Iteration 41/1000 | Loss: 0.00031453
Iteration 42/1000 | Loss: 0.00240037
Iteration 43/1000 | Loss: 0.00414790
Iteration 44/1000 | Loss: 0.00074510
Iteration 45/1000 | Loss: 0.00085252
Iteration 46/1000 | Loss: 0.00150966
Iteration 47/1000 | Loss: 0.00039734
Iteration 48/1000 | Loss: 0.00017786
Iteration 49/1000 | Loss: 0.00054475
Iteration 50/1000 | Loss: 0.00012492
Iteration 51/1000 | Loss: 0.00015302
Iteration 52/1000 | Loss: 0.00013358
Iteration 53/1000 | Loss: 0.00052467
Iteration 54/1000 | Loss: 0.00026061
Iteration 55/1000 | Loss: 0.00013409
Iteration 56/1000 | Loss: 0.00011642
Iteration 57/1000 | Loss: 0.00006629
Iteration 58/1000 | Loss: 0.00014496
Iteration 59/1000 | Loss: 0.00157551
Iteration 60/1000 | Loss: 0.00045708
Iteration 61/1000 | Loss: 0.00032181
Iteration 62/1000 | Loss: 0.00062635
Iteration 63/1000 | Loss: 0.00050475
Iteration 64/1000 | Loss: 0.00027128
Iteration 65/1000 | Loss: 0.00011053
Iteration 66/1000 | Loss: 0.00023106
Iteration 67/1000 | Loss: 0.00031789
Iteration 68/1000 | Loss: 0.00035190
Iteration 69/1000 | Loss: 0.00028043
Iteration 70/1000 | Loss: 0.00170882
Iteration 71/1000 | Loss: 0.00077857
Iteration 72/1000 | Loss: 0.00063397
Iteration 73/1000 | Loss: 0.00008769
Iteration 74/1000 | Loss: 0.00045106
Iteration 75/1000 | Loss: 0.00154953
Iteration 76/1000 | Loss: 0.00035950
Iteration 77/1000 | Loss: 0.00006115
Iteration 78/1000 | Loss: 0.00040810
Iteration 79/1000 | Loss: 0.00032779
Iteration 80/1000 | Loss: 0.00023088
Iteration 81/1000 | Loss: 0.00017927
Iteration 82/1000 | Loss: 0.00008625
Iteration 83/1000 | Loss: 0.00007163
Iteration 84/1000 | Loss: 0.00005700
Iteration 85/1000 | Loss: 0.00008372
Iteration 86/1000 | Loss: 0.00005173
Iteration 87/1000 | Loss: 0.00007327
Iteration 88/1000 | Loss: 0.00005045
Iteration 89/1000 | Loss: 0.00004978
Iteration 90/1000 | Loss: 0.00005256
Iteration 91/1000 | Loss: 0.00005031
Iteration 92/1000 | Loss: 0.00004892
Iteration 93/1000 | Loss: 0.00004826
Iteration 94/1000 | Loss: 0.00004781
Iteration 95/1000 | Loss: 0.00004744
Iteration 96/1000 | Loss: 0.00004718
Iteration 97/1000 | Loss: 0.00004693
Iteration 98/1000 | Loss: 0.00004667
Iteration 99/1000 | Loss: 0.00004660
Iteration 100/1000 | Loss: 0.00004660
Iteration 101/1000 | Loss: 0.00004660
Iteration 102/1000 | Loss: 0.00004660
Iteration 103/1000 | Loss: 0.00004659
Iteration 104/1000 | Loss: 0.00004659
Iteration 105/1000 | Loss: 0.00004659
Iteration 106/1000 | Loss: 0.00004659
Iteration 107/1000 | Loss: 0.00004659
Iteration 108/1000 | Loss: 0.00004659
Iteration 109/1000 | Loss: 0.00004659
Iteration 110/1000 | Loss: 0.00004659
Iteration 111/1000 | Loss: 0.00018768
Iteration 112/1000 | Loss: 0.00018768
Iteration 113/1000 | Loss: 0.00015080
Iteration 114/1000 | Loss: 0.00007619
Iteration 115/1000 | Loss: 0.00004697
Iteration 116/1000 | Loss: 0.00004656
Iteration 117/1000 | Loss: 0.00004654
Iteration 118/1000 | Loss: 0.00004654
Iteration 119/1000 | Loss: 0.00004653
Iteration 120/1000 | Loss: 0.00004653
Iteration 121/1000 | Loss: 0.00004652
Iteration 122/1000 | Loss: 0.00004652
Iteration 123/1000 | Loss: 0.00004652
Iteration 124/1000 | Loss: 0.00004652
Iteration 125/1000 | Loss: 0.00004651
Iteration 126/1000 | Loss: 0.00004651
Iteration 127/1000 | Loss: 0.00004651
Iteration 128/1000 | Loss: 0.00004651
Iteration 129/1000 | Loss: 0.00004651
Iteration 130/1000 | Loss: 0.00004650
Iteration 131/1000 | Loss: 0.00004650
Iteration 132/1000 | Loss: 0.00004650
Iteration 133/1000 | Loss: 0.00004649
Iteration 134/1000 | Loss: 0.00004648
Iteration 135/1000 | Loss: 0.00004648
Iteration 136/1000 | Loss: 0.00004648
Iteration 137/1000 | Loss: 0.00004648
Iteration 138/1000 | Loss: 0.00004648
Iteration 139/1000 | Loss: 0.00004648
Iteration 140/1000 | Loss: 0.00004648
Iteration 141/1000 | Loss: 0.00004648
Iteration 142/1000 | Loss: 0.00004648
Iteration 143/1000 | Loss: 0.00004648
Iteration 144/1000 | Loss: 0.00004648
Iteration 145/1000 | Loss: 0.00004648
Iteration 146/1000 | Loss: 0.00004648
Iteration 147/1000 | Loss: 0.00004647
Iteration 148/1000 | Loss: 0.00004647
Iteration 149/1000 | Loss: 0.00004647
Iteration 150/1000 | Loss: 0.00004647
Iteration 151/1000 | Loss: 0.00004647
Iteration 152/1000 | Loss: 0.00004647
Iteration 153/1000 | Loss: 0.00004647
Iteration 154/1000 | Loss: 0.00004647
Iteration 155/1000 | Loss: 0.00004647
Iteration 156/1000 | Loss: 0.00004647
Iteration 157/1000 | Loss: 0.00004646
Iteration 158/1000 | Loss: 0.00004646
Iteration 159/1000 | Loss: 0.00004646
Iteration 160/1000 | Loss: 0.00004646
Iteration 161/1000 | Loss: 0.00004646
Iteration 162/1000 | Loss: 0.00004646
Iteration 163/1000 | Loss: 0.00004646
Iteration 164/1000 | Loss: 0.00004646
Iteration 165/1000 | Loss: 0.00004646
Iteration 166/1000 | Loss: 0.00004646
Iteration 167/1000 | Loss: 0.00004646
Iteration 168/1000 | Loss: 0.00004646
Iteration 169/1000 | Loss: 0.00004645
Iteration 170/1000 | Loss: 0.00004645
Iteration 171/1000 | Loss: 0.00022594
Iteration 172/1000 | Loss: 0.00007202
Iteration 173/1000 | Loss: 0.00004673
Iteration 174/1000 | Loss: 0.00011933
Iteration 175/1000 | Loss: 0.00004820
Iteration 176/1000 | Loss: 0.00006770
Iteration 177/1000 | Loss: 0.00004978
Iteration 178/1000 | Loss: 0.00005010
Iteration 179/1000 | Loss: 0.00004657
Iteration 180/1000 | Loss: 0.00004657
Iteration 181/1000 | Loss: 0.00004657
Iteration 182/1000 | Loss: 0.00004657
Iteration 183/1000 | Loss: 0.00004657
Iteration 184/1000 | Loss: 0.00004656
Iteration 185/1000 | Loss: 0.00004656
Iteration 186/1000 | Loss: 0.00004655
Iteration 187/1000 | Loss: 0.00004652
Iteration 188/1000 | Loss: 0.00004652
Iteration 189/1000 | Loss: 0.00004649
Iteration 190/1000 | Loss: 0.00004649
Iteration 191/1000 | Loss: 0.00004647
Iteration 192/1000 | Loss: 0.00004647
Iteration 193/1000 | Loss: 0.00004647
Iteration 194/1000 | Loss: 0.00004646
Iteration 195/1000 | Loss: 0.00004646
Iteration 196/1000 | Loss: 0.00004646
Iteration 197/1000 | Loss: 0.00004645
Iteration 198/1000 | Loss: 0.00004644
Iteration 199/1000 | Loss: 0.00004644
Iteration 200/1000 | Loss: 0.00004644
Iteration 201/1000 | Loss: 0.00004644
Iteration 202/1000 | Loss: 0.00004644
Iteration 203/1000 | Loss: 0.00004643
Iteration 204/1000 | Loss: 0.00004643
Iteration 205/1000 | Loss: 0.00004643
Iteration 206/1000 | Loss: 0.00004643
Iteration 207/1000 | Loss: 0.00004643
Iteration 208/1000 | Loss: 0.00004643
Iteration 209/1000 | Loss: 0.00004643
Iteration 210/1000 | Loss: 0.00004643
Iteration 211/1000 | Loss: 0.00004642
Iteration 212/1000 | Loss: 0.00004642
Iteration 213/1000 | Loss: 0.00004642
Iteration 214/1000 | Loss: 0.00004642
Iteration 215/1000 | Loss: 0.00004642
Iteration 216/1000 | Loss: 0.00004641
Iteration 217/1000 | Loss: 0.00004641
Iteration 218/1000 | Loss: 0.00004641
Iteration 219/1000 | Loss: 0.00004641
Iteration 220/1000 | Loss: 0.00004641
Iteration 221/1000 | Loss: 0.00004641
Iteration 222/1000 | Loss: 0.00004641
Iteration 223/1000 | Loss: 0.00004641
Iteration 224/1000 | Loss: 0.00004641
Iteration 225/1000 | Loss: 0.00004641
Iteration 226/1000 | Loss: 0.00004641
Iteration 227/1000 | Loss: 0.00004641
Iteration 228/1000 | Loss: 0.00004641
Iteration 229/1000 | Loss: 0.00004641
Iteration 230/1000 | Loss: 0.00004641
Iteration 231/1000 | Loss: 0.00004641
Iteration 232/1000 | Loss: 0.00004640
Iteration 233/1000 | Loss: 0.00004640
Iteration 234/1000 | Loss: 0.00004640
Iteration 235/1000 | Loss: 0.00004640
Iteration 236/1000 | Loss: 0.00004640
Iteration 237/1000 | Loss: 0.00004640
Iteration 238/1000 | Loss: 0.00004640
Iteration 239/1000 | Loss: 0.00004640
Iteration 240/1000 | Loss: 0.00004640
Iteration 241/1000 | Loss: 0.00004640
Iteration 242/1000 | Loss: 0.00004640
Iteration 243/1000 | Loss: 0.00004640
Iteration 244/1000 | Loss: 0.00004640
Iteration 245/1000 | Loss: 0.00004639
Iteration 246/1000 | Loss: 0.00004639
Iteration 247/1000 | Loss: 0.00004639
Iteration 248/1000 | Loss: 0.00004639
Iteration 249/1000 | Loss: 0.00004639
Iteration 250/1000 | Loss: 0.00004639
Iteration 251/1000 | Loss: 0.00004639
Iteration 252/1000 | Loss: 0.00004639
Iteration 253/1000 | Loss: 0.00004639
Iteration 254/1000 | Loss: 0.00004639
Iteration 255/1000 | Loss: 0.00004639
Iteration 256/1000 | Loss: 0.00004639
Iteration 257/1000 | Loss: 0.00004639
Iteration 258/1000 | Loss: 0.00004639
Iteration 259/1000 | Loss: 0.00004639
Iteration 260/1000 | Loss: 0.00004639
Iteration 261/1000 | Loss: 0.00004639
Iteration 262/1000 | Loss: 0.00004639
Iteration 263/1000 | Loss: 0.00004639
Iteration 264/1000 | Loss: 0.00004639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [4.638804603018798e-05, 4.638804603018798e-05, 4.638804603018798e-05, 4.638804603018798e-05, 4.638804603018798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.638804603018798e-05

Optimization complete. Final v2v error: 4.025465488433838 mm

Highest mean error: 10.69347858428955 mm for frame 91

Lowest mean error: 2.8705320358276367 mm for frame 14

Saving results

Total time: 210.2591791152954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405289
Iteration 2/25 | Loss: 0.00132553
Iteration 3/25 | Loss: 0.00121657
Iteration 4/25 | Loss: 0.00120416
Iteration 5/25 | Loss: 0.00120194
Iteration 6/25 | Loss: 0.00120176
Iteration 7/25 | Loss: 0.00120176
Iteration 8/25 | Loss: 0.00120176
Iteration 9/25 | Loss: 0.00120176
Iteration 10/25 | Loss: 0.00120176
Iteration 11/25 | Loss: 0.00120176
Iteration 12/25 | Loss: 0.00120176
Iteration 13/25 | Loss: 0.00120176
Iteration 14/25 | Loss: 0.00120176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012017575791105628, 0.0012017575791105628, 0.0012017575791105628, 0.0012017575791105628, 0.0012017575791105628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012017575791105628

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34570670
Iteration 2/25 | Loss: 0.00081912
Iteration 3/25 | Loss: 0.00081912
Iteration 4/25 | Loss: 0.00081912
Iteration 5/25 | Loss: 0.00081912
Iteration 6/25 | Loss: 0.00081912
Iteration 7/25 | Loss: 0.00081912
Iteration 8/25 | Loss: 0.00081912
Iteration 9/25 | Loss: 0.00081912
Iteration 10/25 | Loss: 0.00081912
Iteration 11/25 | Loss: 0.00081912
Iteration 12/25 | Loss: 0.00081912
Iteration 13/25 | Loss: 0.00081912
Iteration 14/25 | Loss: 0.00081912
Iteration 15/25 | Loss: 0.00081912
Iteration 16/25 | Loss: 0.00081912
Iteration 17/25 | Loss: 0.00081912
Iteration 18/25 | Loss: 0.00081912
Iteration 19/25 | Loss: 0.00081912
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008191154920496047, 0.0008191154920496047, 0.0008191154920496047, 0.0008191154920496047, 0.0008191154920496047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008191154920496047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081912
Iteration 2/1000 | Loss: 0.00002343
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001678
Iteration 5/1000 | Loss: 0.00001575
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001461
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001378
Iteration 11/1000 | Loss: 0.00001377
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001342
Iteration 16/1000 | Loss: 0.00001341
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001329
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001325
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001320
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001302
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001299
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001292
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001291
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001291
Iteration 42/1000 | Loss: 0.00001291
Iteration 43/1000 | Loss: 0.00001290
Iteration 44/1000 | Loss: 0.00001290
Iteration 45/1000 | Loss: 0.00001290
Iteration 46/1000 | Loss: 0.00001290
Iteration 47/1000 | Loss: 0.00001290
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001287
Iteration 53/1000 | Loss: 0.00001287
Iteration 54/1000 | Loss: 0.00001287
Iteration 55/1000 | Loss: 0.00001287
Iteration 56/1000 | Loss: 0.00001286
Iteration 57/1000 | Loss: 0.00001286
Iteration 58/1000 | Loss: 0.00001286
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001280
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001280
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001278
Iteration 79/1000 | Loss: 0.00001278
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001277
Iteration 85/1000 | Loss: 0.00001277
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001275
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001273
Iteration 108/1000 | Loss: 0.00001273
Iteration 109/1000 | Loss: 0.00001273
Iteration 110/1000 | Loss: 0.00001273
Iteration 111/1000 | Loss: 0.00001272
Iteration 112/1000 | Loss: 0.00001272
Iteration 113/1000 | Loss: 0.00001272
Iteration 114/1000 | Loss: 0.00001272
Iteration 115/1000 | Loss: 0.00001272
Iteration 116/1000 | Loss: 0.00001272
Iteration 117/1000 | Loss: 0.00001272
Iteration 118/1000 | Loss: 0.00001272
Iteration 119/1000 | Loss: 0.00001272
Iteration 120/1000 | Loss: 0.00001272
Iteration 121/1000 | Loss: 0.00001272
Iteration 122/1000 | Loss: 0.00001272
Iteration 123/1000 | Loss: 0.00001272
Iteration 124/1000 | Loss: 0.00001272
Iteration 125/1000 | Loss: 0.00001271
Iteration 126/1000 | Loss: 0.00001271
Iteration 127/1000 | Loss: 0.00001271
Iteration 128/1000 | Loss: 0.00001271
Iteration 129/1000 | Loss: 0.00001271
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001271
Iteration 132/1000 | Loss: 0.00001271
Iteration 133/1000 | Loss: 0.00001271
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001270
Iteration 137/1000 | Loss: 0.00001270
Iteration 138/1000 | Loss: 0.00001270
Iteration 139/1000 | Loss: 0.00001270
Iteration 140/1000 | Loss: 0.00001270
Iteration 141/1000 | Loss: 0.00001270
Iteration 142/1000 | Loss: 0.00001270
Iteration 143/1000 | Loss: 0.00001270
Iteration 144/1000 | Loss: 0.00001270
Iteration 145/1000 | Loss: 0.00001270
Iteration 146/1000 | Loss: 0.00001270
Iteration 147/1000 | Loss: 0.00001270
Iteration 148/1000 | Loss: 0.00001270
Iteration 149/1000 | Loss: 0.00001270
Iteration 150/1000 | Loss: 0.00001270
Iteration 151/1000 | Loss: 0.00001269
Iteration 152/1000 | Loss: 0.00001269
Iteration 153/1000 | Loss: 0.00001269
Iteration 154/1000 | Loss: 0.00001269
Iteration 155/1000 | Loss: 0.00001269
Iteration 156/1000 | Loss: 0.00001269
Iteration 157/1000 | Loss: 0.00001269
Iteration 158/1000 | Loss: 0.00001269
Iteration 159/1000 | Loss: 0.00001269
Iteration 160/1000 | Loss: 0.00001268
Iteration 161/1000 | Loss: 0.00001268
Iteration 162/1000 | Loss: 0.00001268
Iteration 163/1000 | Loss: 0.00001268
Iteration 164/1000 | Loss: 0.00001268
Iteration 165/1000 | Loss: 0.00001268
Iteration 166/1000 | Loss: 0.00001268
Iteration 167/1000 | Loss: 0.00001268
Iteration 168/1000 | Loss: 0.00001268
Iteration 169/1000 | Loss: 0.00001268
Iteration 170/1000 | Loss: 0.00001268
Iteration 171/1000 | Loss: 0.00001268
Iteration 172/1000 | Loss: 0.00001267
Iteration 173/1000 | Loss: 0.00001267
Iteration 174/1000 | Loss: 0.00001267
Iteration 175/1000 | Loss: 0.00001267
Iteration 176/1000 | Loss: 0.00001267
Iteration 177/1000 | Loss: 0.00001267
Iteration 178/1000 | Loss: 0.00001267
Iteration 179/1000 | Loss: 0.00001267
Iteration 180/1000 | Loss: 0.00001267
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001267
Iteration 183/1000 | Loss: 0.00001267
Iteration 184/1000 | Loss: 0.00001267
Iteration 185/1000 | Loss: 0.00001267
Iteration 186/1000 | Loss: 0.00001267
Iteration 187/1000 | Loss: 0.00001267
Iteration 188/1000 | Loss: 0.00001267
Iteration 189/1000 | Loss: 0.00001267
Iteration 190/1000 | Loss: 0.00001267
Iteration 191/1000 | Loss: 0.00001267
Iteration 192/1000 | Loss: 0.00001267
Iteration 193/1000 | Loss: 0.00001267
Iteration 194/1000 | Loss: 0.00001267
Iteration 195/1000 | Loss: 0.00001267
Iteration 196/1000 | Loss: 0.00001267
Iteration 197/1000 | Loss: 0.00001267
Iteration 198/1000 | Loss: 0.00001267
Iteration 199/1000 | Loss: 0.00001267
Iteration 200/1000 | Loss: 0.00001267
Iteration 201/1000 | Loss: 0.00001267
Iteration 202/1000 | Loss: 0.00001267
Iteration 203/1000 | Loss: 0.00001267
Iteration 204/1000 | Loss: 0.00001267
Iteration 205/1000 | Loss: 0.00001267
Iteration 206/1000 | Loss: 0.00001267
Iteration 207/1000 | Loss: 0.00001267
Iteration 208/1000 | Loss: 0.00001267
Iteration 209/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.2669645002461039e-05, 1.2669645002461039e-05, 1.2669645002461039e-05, 1.2669645002461039e-05, 1.2669645002461039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2669645002461039e-05

Optimization complete. Final v2v error: 3.041332483291626 mm

Highest mean error: 3.2864115238189697 mm for frame 120

Lowest mean error: 2.7946598529815674 mm for frame 7

Saving results

Total time: 43.15028643608093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997493
Iteration 2/25 | Loss: 0.00347479
Iteration 3/25 | Loss: 0.00227145
Iteration 4/25 | Loss: 0.00214226
Iteration 5/25 | Loss: 0.00213819
Iteration 6/25 | Loss: 0.00174237
Iteration 7/25 | Loss: 0.00140006
Iteration 8/25 | Loss: 0.00135758
Iteration 9/25 | Loss: 0.00134252
Iteration 10/25 | Loss: 0.00132881
Iteration 11/25 | Loss: 0.00130025
Iteration 12/25 | Loss: 0.00126782
Iteration 13/25 | Loss: 0.00125537
Iteration 14/25 | Loss: 0.00124812
Iteration 15/25 | Loss: 0.00124831
Iteration 16/25 | Loss: 0.00124745
Iteration 17/25 | Loss: 0.00124619
Iteration 18/25 | Loss: 0.00124746
Iteration 19/25 | Loss: 0.00124597
Iteration 20/25 | Loss: 0.00124636
Iteration 21/25 | Loss: 0.00124798
Iteration 22/25 | Loss: 0.00124386
Iteration 23/25 | Loss: 0.00124445
Iteration 24/25 | Loss: 0.00123829
Iteration 25/25 | Loss: 0.00123807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33336353
Iteration 2/25 | Loss: 0.00136233
Iteration 3/25 | Loss: 0.00134848
Iteration 4/25 | Loss: 0.00134732
Iteration 5/25 | Loss: 0.00134732
Iteration 6/25 | Loss: 0.00134732
Iteration 7/25 | Loss: 0.00134732
Iteration 8/25 | Loss: 0.00134732
Iteration 9/25 | Loss: 0.00134732
Iteration 10/25 | Loss: 0.00134732
Iteration 11/25 | Loss: 0.00134732
Iteration 12/25 | Loss: 0.00134732
Iteration 13/25 | Loss: 0.00134732
Iteration 14/25 | Loss: 0.00134732
Iteration 15/25 | Loss: 0.00134732
Iteration 16/25 | Loss: 0.00134732
Iteration 17/25 | Loss: 0.00134732
Iteration 18/25 | Loss: 0.00134732
Iteration 19/25 | Loss: 0.00134732
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013473150320351124, 0.0013473150320351124, 0.0013473150320351124, 0.0013473150320351124, 0.0013473150320351124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013473150320351124

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134732
Iteration 2/1000 | Loss: 0.00008776
Iteration 3/1000 | Loss: 0.00005120
Iteration 4/1000 | Loss: 0.00004540
Iteration 5/1000 | Loss: 0.00004156
Iteration 6/1000 | Loss: 0.00004103
Iteration 7/1000 | Loss: 0.00003955
Iteration 8/1000 | Loss: 0.00003851
Iteration 9/1000 | Loss: 0.00003587
Iteration 10/1000 | Loss: 0.00003667
Iteration 11/1000 | Loss: 0.00004792
Iteration 12/1000 | Loss: 0.00003711
Iteration 13/1000 | Loss: 0.00003429
Iteration 14/1000 | Loss: 0.00003391
Iteration 15/1000 | Loss: 0.00003688
Iteration 16/1000 | Loss: 0.00004663
Iteration 17/1000 | Loss: 0.00003549
Iteration 18/1000 | Loss: 0.00008542
Iteration 19/1000 | Loss: 0.00004075
Iteration 20/1000 | Loss: 0.00003319
Iteration 21/1000 | Loss: 0.00003349
Iteration 22/1000 | Loss: 0.00003292
Iteration 23/1000 | Loss: 0.00003292
Iteration 24/1000 | Loss: 0.00003292
Iteration 25/1000 | Loss: 0.00003292
Iteration 26/1000 | Loss: 0.00003292
Iteration 27/1000 | Loss: 0.00003292
Iteration 28/1000 | Loss: 0.00003292
Iteration 29/1000 | Loss: 0.00003292
Iteration 30/1000 | Loss: 0.00003292
Iteration 31/1000 | Loss: 0.00003292
Iteration 32/1000 | Loss: 0.00003506
Iteration 33/1000 | Loss: 0.00003749
Iteration 34/1000 | Loss: 0.00003353
Iteration 35/1000 | Loss: 0.00007557
Iteration 36/1000 | Loss: 0.00003411
Iteration 37/1000 | Loss: 0.00003253
Iteration 38/1000 | Loss: 0.00003310
Iteration 39/1000 | Loss: 0.00003466
Iteration 40/1000 | Loss: 0.00003245
Iteration 41/1000 | Loss: 0.00003245
Iteration 42/1000 | Loss: 0.00003245
Iteration 43/1000 | Loss: 0.00003245
Iteration 44/1000 | Loss: 0.00003284
Iteration 45/1000 | Loss: 0.00003280
Iteration 46/1000 | Loss: 0.00003625
Iteration 47/1000 | Loss: 0.00003522
Iteration 48/1000 | Loss: 0.00003256
Iteration 49/1000 | Loss: 0.00003256
Iteration 50/1000 | Loss: 0.00003278
Iteration 51/1000 | Loss: 0.00003270
Iteration 52/1000 | Loss: 0.00003238
Iteration 53/1000 | Loss: 0.00003343
Iteration 54/1000 | Loss: 0.00003221
Iteration 55/1000 | Loss: 0.00003220
Iteration 56/1000 | Loss: 0.00003220
Iteration 57/1000 | Loss: 0.00003220
Iteration 58/1000 | Loss: 0.00003220
Iteration 59/1000 | Loss: 0.00003220
Iteration 60/1000 | Loss: 0.00003220
Iteration 61/1000 | Loss: 0.00003220
Iteration 62/1000 | Loss: 0.00003220
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003219
Iteration 66/1000 | Loss: 0.00003219
Iteration 67/1000 | Loss: 0.00003219
Iteration 68/1000 | Loss: 0.00003219
Iteration 69/1000 | Loss: 0.00003341
Iteration 70/1000 | Loss: 0.00003805
Iteration 71/1000 | Loss: 0.00003335
Iteration 72/1000 | Loss: 0.00004133
Iteration 73/1000 | Loss: 0.00003767
Iteration 74/1000 | Loss: 0.00003321
Iteration 75/1000 | Loss: 0.00003439
Iteration 76/1000 | Loss: 0.00003558
Iteration 77/1000 | Loss: 0.00003796
Iteration 78/1000 | Loss: 0.00003800
Iteration 79/1000 | Loss: 0.00003221
Iteration 80/1000 | Loss: 0.00003202
Iteration 81/1000 | Loss: 0.00003202
Iteration 82/1000 | Loss: 0.00003202
Iteration 83/1000 | Loss: 0.00003202
Iteration 84/1000 | Loss: 0.00003202
Iteration 85/1000 | Loss: 0.00003202
Iteration 86/1000 | Loss: 0.00003202
Iteration 87/1000 | Loss: 0.00003202
Iteration 88/1000 | Loss: 0.00003202
Iteration 89/1000 | Loss: 0.00003202
Iteration 90/1000 | Loss: 0.00003202
Iteration 91/1000 | Loss: 0.00003202
Iteration 92/1000 | Loss: 0.00003202
Iteration 93/1000 | Loss: 0.00003202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [3.201710569555871e-05, 3.201710569555871e-05, 3.201710569555871e-05, 3.201710569555871e-05, 3.201710569555871e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.201710569555871e-05

Optimization complete. Final v2v error: 3.344318151473999 mm

Highest mean error: 9.85151195526123 mm for frame 28

Lowest mean error: 2.480768918991089 mm for frame 14

Saving results

Total time: 105.66534185409546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594883
Iteration 2/25 | Loss: 0.00147803
Iteration 3/25 | Loss: 0.00135750
Iteration 4/25 | Loss: 0.00134218
Iteration 5/25 | Loss: 0.00133894
Iteration 6/25 | Loss: 0.00133892
Iteration 7/25 | Loss: 0.00133858
Iteration 8/25 | Loss: 0.00133766
Iteration 9/25 | Loss: 0.00133745
Iteration 10/25 | Loss: 0.00133732
Iteration 11/25 | Loss: 0.00133822
Iteration 12/25 | Loss: 0.00133884
Iteration 13/25 | Loss: 0.00133845
Iteration 14/25 | Loss: 0.00133871
Iteration 15/25 | Loss: 0.00133833
Iteration 16/25 | Loss: 0.00133814
Iteration 17/25 | Loss: 0.00133846
Iteration 18/25 | Loss: 0.00133846
Iteration 19/25 | Loss: 0.00133810
Iteration 20/25 | Loss: 0.00133807
Iteration 21/25 | Loss: 0.00133803
Iteration 22/25 | Loss: 0.00133822
Iteration 23/25 | Loss: 0.00133806
Iteration 24/25 | Loss: 0.00133823
Iteration 25/25 | Loss: 0.00133806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33273602
Iteration 2/25 | Loss: 0.00114592
Iteration 3/25 | Loss: 0.00114590
Iteration 4/25 | Loss: 0.00114590
Iteration 5/25 | Loss: 0.00114590
Iteration 6/25 | Loss: 0.00114590
Iteration 7/25 | Loss: 0.00114590
Iteration 8/25 | Loss: 0.00114590
Iteration 9/25 | Loss: 0.00114590
Iteration 10/25 | Loss: 0.00114590
Iteration 11/25 | Loss: 0.00114590
Iteration 12/25 | Loss: 0.00114590
Iteration 13/25 | Loss: 0.00114590
Iteration 14/25 | Loss: 0.00114590
Iteration 15/25 | Loss: 0.00114590
Iteration 16/25 | Loss: 0.00114590
Iteration 17/25 | Loss: 0.00114590
Iteration 18/25 | Loss: 0.00114590
Iteration 19/25 | Loss: 0.00114590
Iteration 20/25 | Loss: 0.00114590
Iteration 21/25 | Loss: 0.00114590
Iteration 22/25 | Loss: 0.00114590
Iteration 23/25 | Loss: 0.00114590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011458982480689883, 0.0011458982480689883, 0.0011458982480689883, 0.0011458982480689883, 0.0011458982480689883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011458982480689883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114590
Iteration 2/1000 | Loss: 0.00003913
Iteration 3/1000 | Loss: 0.00002965
Iteration 4/1000 | Loss: 0.00002612
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002372
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002663
Iteration 10/1000 | Loss: 0.00002693
Iteration 11/1000 | Loss: 0.00002612
Iteration 12/1000 | Loss: 0.00002299
Iteration 13/1000 | Loss: 0.00002990
Iteration 14/1000 | Loss: 0.00002531
Iteration 15/1000 | Loss: 0.00002261
Iteration 16/1000 | Loss: 0.00003000
Iteration 17/1000 | Loss: 0.00002464
Iteration 18/1000 | Loss: 0.00002258
Iteration 19/1000 | Loss: 0.00002937
Iteration 20/1000 | Loss: 0.00002277
Iteration 21/1000 | Loss: 0.00002122
Iteration 22/1000 | Loss: 0.00002081
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002074
Iteration 25/1000 | Loss: 0.00002072
Iteration 26/1000 | Loss: 0.00002072
Iteration 27/1000 | Loss: 0.00002071
Iteration 28/1000 | Loss: 0.00002071
Iteration 29/1000 | Loss: 0.00002070
Iteration 30/1000 | Loss: 0.00002069
Iteration 31/1000 | Loss: 0.00002069
Iteration 32/1000 | Loss: 0.00002065
Iteration 33/1000 | Loss: 0.00002065
Iteration 34/1000 | Loss: 0.00002062
Iteration 35/1000 | Loss: 0.00002062
Iteration 36/1000 | Loss: 0.00002062
Iteration 37/1000 | Loss: 0.00002061
Iteration 38/1000 | Loss: 0.00002061
Iteration 39/1000 | Loss: 0.00002061
Iteration 40/1000 | Loss: 0.00002058
Iteration 41/1000 | Loss: 0.00002052
Iteration 42/1000 | Loss: 0.00002052
Iteration 43/1000 | Loss: 0.00002052
Iteration 44/1000 | Loss: 0.00002052
Iteration 45/1000 | Loss: 0.00002052
Iteration 46/1000 | Loss: 0.00002051
Iteration 47/1000 | Loss: 0.00002051
Iteration 48/1000 | Loss: 0.00002051
Iteration 49/1000 | Loss: 0.00002051
Iteration 50/1000 | Loss: 0.00002051
Iteration 51/1000 | Loss: 0.00002051
Iteration 52/1000 | Loss: 0.00002051
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002050
Iteration 55/1000 | Loss: 0.00002050
Iteration 56/1000 | Loss: 0.00002049
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002048
Iteration 60/1000 | Loss: 0.00002048
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002046
Iteration 67/1000 | Loss: 0.00002046
Iteration 68/1000 | Loss: 0.00002046
Iteration 69/1000 | Loss: 0.00002045
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002045
Iteration 73/1000 | Loss: 0.00002044
Iteration 74/1000 | Loss: 0.00002044
Iteration 75/1000 | Loss: 0.00002044
Iteration 76/1000 | Loss: 0.00002044
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002041
Iteration 96/1000 | Loss: 0.00002040
Iteration 97/1000 | Loss: 0.00002040
Iteration 98/1000 | Loss: 0.00002040
Iteration 99/1000 | Loss: 0.00002039
Iteration 100/1000 | Loss: 0.00002039
Iteration 101/1000 | Loss: 0.00002039
Iteration 102/1000 | Loss: 0.00002039
Iteration 103/1000 | Loss: 0.00002039
Iteration 104/1000 | Loss: 0.00002039
Iteration 105/1000 | Loss: 0.00002039
Iteration 106/1000 | Loss: 0.00002039
Iteration 107/1000 | Loss: 0.00002039
Iteration 108/1000 | Loss: 0.00002038
Iteration 109/1000 | Loss: 0.00002038
Iteration 110/1000 | Loss: 0.00002038
Iteration 111/1000 | Loss: 0.00002037
Iteration 112/1000 | Loss: 0.00002037
Iteration 113/1000 | Loss: 0.00002037
Iteration 114/1000 | Loss: 0.00002037
Iteration 115/1000 | Loss: 0.00002036
Iteration 116/1000 | Loss: 0.00002036
Iteration 117/1000 | Loss: 0.00002036
Iteration 118/1000 | Loss: 0.00002036
Iteration 119/1000 | Loss: 0.00002035
Iteration 120/1000 | Loss: 0.00002035
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002035
Iteration 123/1000 | Loss: 0.00002035
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002033
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002032
Iteration 150/1000 | Loss: 0.00002032
Iteration 151/1000 | Loss: 0.00002032
Iteration 152/1000 | Loss: 0.00002032
Iteration 153/1000 | Loss: 0.00002032
Iteration 154/1000 | Loss: 0.00002032
Iteration 155/1000 | Loss: 0.00002032
Iteration 156/1000 | Loss: 0.00002032
Iteration 157/1000 | Loss: 0.00002032
Iteration 158/1000 | Loss: 0.00002032
Iteration 159/1000 | Loss: 0.00002032
Iteration 160/1000 | Loss: 0.00002031
Iteration 161/1000 | Loss: 0.00002031
Iteration 162/1000 | Loss: 0.00002031
Iteration 163/1000 | Loss: 0.00002031
Iteration 164/1000 | Loss: 0.00002031
Iteration 165/1000 | Loss: 0.00002031
Iteration 166/1000 | Loss: 0.00002031
Iteration 167/1000 | Loss: 0.00002031
Iteration 168/1000 | Loss: 0.00002030
Iteration 169/1000 | Loss: 0.00002030
Iteration 170/1000 | Loss: 0.00002030
Iteration 171/1000 | Loss: 0.00002030
Iteration 172/1000 | Loss: 0.00002030
Iteration 173/1000 | Loss: 0.00002030
Iteration 174/1000 | Loss: 0.00002030
Iteration 175/1000 | Loss: 0.00002030
Iteration 176/1000 | Loss: 0.00002030
Iteration 177/1000 | Loss: 0.00002030
Iteration 178/1000 | Loss: 0.00002030
Iteration 179/1000 | Loss: 0.00002029
Iteration 180/1000 | Loss: 0.00002029
Iteration 181/1000 | Loss: 0.00002029
Iteration 182/1000 | Loss: 0.00002029
Iteration 183/1000 | Loss: 0.00002029
Iteration 184/1000 | Loss: 0.00002029
Iteration 185/1000 | Loss: 0.00002029
Iteration 186/1000 | Loss: 0.00002029
Iteration 187/1000 | Loss: 0.00002029
Iteration 188/1000 | Loss: 0.00002029
Iteration 189/1000 | Loss: 0.00002029
Iteration 190/1000 | Loss: 0.00002029
Iteration 191/1000 | Loss: 0.00002029
Iteration 192/1000 | Loss: 0.00002029
Iteration 193/1000 | Loss: 0.00002029
Iteration 194/1000 | Loss: 0.00002029
Iteration 195/1000 | Loss: 0.00002029
Iteration 196/1000 | Loss: 0.00002029
Iteration 197/1000 | Loss: 0.00002029
Iteration 198/1000 | Loss: 0.00002029
Iteration 199/1000 | Loss: 0.00002029
Iteration 200/1000 | Loss: 0.00002029
Iteration 201/1000 | Loss: 0.00002029
Iteration 202/1000 | Loss: 0.00002029
Iteration 203/1000 | Loss: 0.00002029
Iteration 204/1000 | Loss: 0.00002029
Iteration 205/1000 | Loss: 0.00002029
Iteration 206/1000 | Loss: 0.00002029
Iteration 207/1000 | Loss: 0.00002029
Iteration 208/1000 | Loss: 0.00002029
Iteration 209/1000 | Loss: 0.00002029
Iteration 210/1000 | Loss: 0.00002029
Iteration 211/1000 | Loss: 0.00002029
Iteration 212/1000 | Loss: 0.00002029
Iteration 213/1000 | Loss: 0.00002029
Iteration 214/1000 | Loss: 0.00002029
Iteration 215/1000 | Loss: 0.00002029
Iteration 216/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.028907329076901e-05, 2.028907329076901e-05, 2.028907329076901e-05, 2.028907329076901e-05, 2.028907329076901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.028907329076901e-05

Optimization complete. Final v2v error: 3.818490982055664 mm

Highest mean error: 4.8210129737854 mm for frame 60

Lowest mean error: 3.516383409500122 mm for frame 116

Saving results

Total time: 102.50540256500244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461947
Iteration 2/25 | Loss: 0.00142866
Iteration 3/25 | Loss: 0.00127427
Iteration 4/25 | Loss: 0.00126089
Iteration 5/25 | Loss: 0.00125742
Iteration 6/25 | Loss: 0.00125667
Iteration 7/25 | Loss: 0.00125667
Iteration 8/25 | Loss: 0.00125667
Iteration 9/25 | Loss: 0.00125667
Iteration 10/25 | Loss: 0.00125667
Iteration 11/25 | Loss: 0.00125667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012566704535856843, 0.0012566704535856843, 0.0012566704535856843, 0.0012566704535856843, 0.0012566704535856843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012566704535856843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46773946
Iteration 2/25 | Loss: 0.00092010
Iteration 3/25 | Loss: 0.00092009
Iteration 4/25 | Loss: 0.00092009
Iteration 5/25 | Loss: 0.00092009
Iteration 6/25 | Loss: 0.00092008
Iteration 7/25 | Loss: 0.00092008
Iteration 8/25 | Loss: 0.00092008
Iteration 9/25 | Loss: 0.00092008
Iteration 10/25 | Loss: 0.00092008
Iteration 11/25 | Loss: 0.00092008
Iteration 12/25 | Loss: 0.00092008
Iteration 13/25 | Loss: 0.00092008
Iteration 14/25 | Loss: 0.00092008
Iteration 15/25 | Loss: 0.00092008
Iteration 16/25 | Loss: 0.00092008
Iteration 17/25 | Loss: 0.00092008
Iteration 18/25 | Loss: 0.00092008
Iteration 19/25 | Loss: 0.00092008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009200834319926798, 0.0009200834319926798, 0.0009200834319926798, 0.0009200834319926798, 0.0009200834319926798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009200834319926798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092008
Iteration 2/1000 | Loss: 0.00003083
Iteration 3/1000 | Loss: 0.00002420
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00002019
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001925
Iteration 9/1000 | Loss: 0.00001893
Iteration 10/1000 | Loss: 0.00001864
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001838
Iteration 14/1000 | Loss: 0.00001837
Iteration 15/1000 | Loss: 0.00001828
Iteration 16/1000 | Loss: 0.00001812
Iteration 17/1000 | Loss: 0.00001807
Iteration 18/1000 | Loss: 0.00001805
Iteration 19/1000 | Loss: 0.00001798
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001773
Iteration 31/1000 | Loss: 0.00001772
Iteration 32/1000 | Loss: 0.00001771
Iteration 33/1000 | Loss: 0.00001771
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001765
Iteration 38/1000 | Loss: 0.00001761
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001759
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001758
Iteration 53/1000 | Loss: 0.00001758
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001756
Iteration 56/1000 | Loss: 0.00001756
Iteration 57/1000 | Loss: 0.00001756
Iteration 58/1000 | Loss: 0.00001755
Iteration 59/1000 | Loss: 0.00001755
Iteration 60/1000 | Loss: 0.00001755
Iteration 61/1000 | Loss: 0.00001755
Iteration 62/1000 | Loss: 0.00001755
Iteration 63/1000 | Loss: 0.00001755
Iteration 64/1000 | Loss: 0.00001755
Iteration 65/1000 | Loss: 0.00001755
Iteration 66/1000 | Loss: 0.00001755
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001754
Iteration 70/1000 | Loss: 0.00001754
Iteration 71/1000 | Loss: 0.00001754
Iteration 72/1000 | Loss: 0.00001754
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001753
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001751
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001751
Iteration 86/1000 | Loss: 0.00001751
Iteration 87/1000 | Loss: 0.00001751
Iteration 88/1000 | Loss: 0.00001751
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001750
Iteration 93/1000 | Loss: 0.00001750
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00001750
Iteration 96/1000 | Loss: 0.00001750
Iteration 97/1000 | Loss: 0.00001750
Iteration 98/1000 | Loss: 0.00001750
Iteration 99/1000 | Loss: 0.00001750
Iteration 100/1000 | Loss: 0.00001750
Iteration 101/1000 | Loss: 0.00001750
Iteration 102/1000 | Loss: 0.00001750
Iteration 103/1000 | Loss: 0.00001750
Iteration 104/1000 | Loss: 0.00001750
Iteration 105/1000 | Loss: 0.00001749
Iteration 106/1000 | Loss: 0.00001749
Iteration 107/1000 | Loss: 0.00001749
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001748
Iteration 115/1000 | Loss: 0.00001748
Iteration 116/1000 | Loss: 0.00001748
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001747
Iteration 121/1000 | Loss: 0.00001747
Iteration 122/1000 | Loss: 0.00001747
Iteration 123/1000 | Loss: 0.00001747
Iteration 124/1000 | Loss: 0.00001747
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001746
Iteration 132/1000 | Loss: 0.00001746
Iteration 133/1000 | Loss: 0.00001746
Iteration 134/1000 | Loss: 0.00001746
Iteration 135/1000 | Loss: 0.00001746
Iteration 136/1000 | Loss: 0.00001745
Iteration 137/1000 | Loss: 0.00001745
Iteration 138/1000 | Loss: 0.00001745
Iteration 139/1000 | Loss: 0.00001745
Iteration 140/1000 | Loss: 0.00001744
Iteration 141/1000 | Loss: 0.00001744
Iteration 142/1000 | Loss: 0.00001744
Iteration 143/1000 | Loss: 0.00001744
Iteration 144/1000 | Loss: 0.00001744
Iteration 145/1000 | Loss: 0.00001744
Iteration 146/1000 | Loss: 0.00001744
Iteration 147/1000 | Loss: 0.00001744
Iteration 148/1000 | Loss: 0.00001744
Iteration 149/1000 | Loss: 0.00001744
Iteration 150/1000 | Loss: 0.00001744
Iteration 151/1000 | Loss: 0.00001744
Iteration 152/1000 | Loss: 0.00001744
Iteration 153/1000 | Loss: 0.00001744
Iteration 154/1000 | Loss: 0.00001744
Iteration 155/1000 | Loss: 0.00001744
Iteration 156/1000 | Loss: 0.00001744
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.74422293639509e-05, 1.74422293639509e-05, 1.74422293639509e-05, 1.74422293639509e-05, 1.74422293639509e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.74422293639509e-05

Optimization complete. Final v2v error: 3.4470419883728027 mm

Highest mean error: 4.2476983070373535 mm for frame 147

Lowest mean error: 2.6567485332489014 mm for frame 0

Saving results

Total time: 42.39693641662598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019934
Iteration 2/25 | Loss: 0.00163317
Iteration 3/25 | Loss: 0.00139149
Iteration 4/25 | Loss: 0.00137110
Iteration 5/25 | Loss: 0.00138380
Iteration 6/25 | Loss: 0.00135261
Iteration 7/25 | Loss: 0.00131300
Iteration 8/25 | Loss: 0.00129846
Iteration 9/25 | Loss: 0.00128974
Iteration 10/25 | Loss: 0.00128625
Iteration 11/25 | Loss: 0.00128519
Iteration 12/25 | Loss: 0.00128493
Iteration 13/25 | Loss: 0.00128484
Iteration 14/25 | Loss: 0.00128484
Iteration 15/25 | Loss: 0.00128483
Iteration 16/25 | Loss: 0.00128483
Iteration 17/25 | Loss: 0.00128483
Iteration 18/25 | Loss: 0.00128483
Iteration 19/25 | Loss: 0.00128483
Iteration 20/25 | Loss: 0.00128483
Iteration 21/25 | Loss: 0.00128482
Iteration 22/25 | Loss: 0.00128482
Iteration 23/25 | Loss: 0.00128482
Iteration 24/25 | Loss: 0.00128482
Iteration 25/25 | Loss: 0.00128482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55138171
Iteration 2/25 | Loss: 0.00115849
Iteration 3/25 | Loss: 0.00115848
Iteration 4/25 | Loss: 0.00115847
Iteration 5/25 | Loss: 0.00115847
Iteration 6/25 | Loss: 0.00115847
Iteration 7/25 | Loss: 0.00115847
Iteration 8/25 | Loss: 0.00115847
Iteration 9/25 | Loss: 0.00115847
Iteration 10/25 | Loss: 0.00115847
Iteration 11/25 | Loss: 0.00115847
Iteration 12/25 | Loss: 0.00115847
Iteration 13/25 | Loss: 0.00115847
Iteration 14/25 | Loss: 0.00115847
Iteration 15/25 | Loss: 0.00115847
Iteration 16/25 | Loss: 0.00115847
Iteration 17/25 | Loss: 0.00115847
Iteration 18/25 | Loss: 0.00115847
Iteration 19/25 | Loss: 0.00115847
Iteration 20/25 | Loss: 0.00115847
Iteration 21/25 | Loss: 0.00115847
Iteration 22/25 | Loss: 0.00115847
Iteration 23/25 | Loss: 0.00115847
Iteration 24/25 | Loss: 0.00115847
Iteration 25/25 | Loss: 0.00115847

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115847
Iteration 2/1000 | Loss: 0.00004553
Iteration 3/1000 | Loss: 0.00002855
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00001951
Iteration 6/1000 | Loss: 0.00001867
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001758
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001696
Iteration 11/1000 | Loss: 0.00001678
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001654
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001647
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001626
Iteration 29/1000 | Loss: 0.00001624
Iteration 30/1000 | Loss: 0.00001624
Iteration 31/1000 | Loss: 0.00001622
Iteration 32/1000 | Loss: 0.00001621
Iteration 33/1000 | Loss: 0.00001619
Iteration 34/1000 | Loss: 0.00001619
Iteration 35/1000 | Loss: 0.00001618
Iteration 36/1000 | Loss: 0.00001615
Iteration 37/1000 | Loss: 0.00001615
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001612
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001612
Iteration 50/1000 | Loss: 0.00001612
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001611
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001607
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001606
Iteration 76/1000 | Loss: 0.00001606
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001606
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001605
Iteration 90/1000 | Loss: 0.00001605
Iteration 91/1000 | Loss: 0.00001605
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001605
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001603
Iteration 112/1000 | Loss: 0.00001603
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001603
Iteration 117/1000 | Loss: 0.00001603
Iteration 118/1000 | Loss: 0.00001603
Iteration 119/1000 | Loss: 0.00001603
Iteration 120/1000 | Loss: 0.00001603
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001602
Iteration 142/1000 | Loss: 0.00001602
Iteration 143/1000 | Loss: 0.00001601
Iteration 144/1000 | Loss: 0.00001601
Iteration 145/1000 | Loss: 0.00001601
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001600
Iteration 149/1000 | Loss: 0.00001600
Iteration 150/1000 | Loss: 0.00001600
Iteration 151/1000 | Loss: 0.00001600
Iteration 152/1000 | Loss: 0.00001600
Iteration 153/1000 | Loss: 0.00001600
Iteration 154/1000 | Loss: 0.00001600
Iteration 155/1000 | Loss: 0.00001600
Iteration 156/1000 | Loss: 0.00001600
Iteration 157/1000 | Loss: 0.00001600
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001600
Iteration 160/1000 | Loss: 0.00001600
Iteration 161/1000 | Loss: 0.00001600
Iteration 162/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.5998379240045324e-05, 1.5998379240045324e-05, 1.5998379240045324e-05, 1.5998379240045324e-05, 1.5998379240045324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5998379240045324e-05

Optimization complete. Final v2v error: 3.3099045753479004 mm

Highest mean error: 4.977163314819336 mm for frame 0

Lowest mean error: 2.869709014892578 mm for frame 56

Saving results

Total time: 53.145251750946045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524931
Iteration 2/25 | Loss: 0.00137344
Iteration 3/25 | Loss: 0.00126950
Iteration 4/25 | Loss: 0.00126029
Iteration 5/25 | Loss: 0.00125830
Iteration 6/25 | Loss: 0.00125811
Iteration 7/25 | Loss: 0.00125811
Iteration 8/25 | Loss: 0.00125811
Iteration 9/25 | Loss: 0.00125811
Iteration 10/25 | Loss: 0.00125811
Iteration 11/25 | Loss: 0.00125811
Iteration 12/25 | Loss: 0.00125811
Iteration 13/25 | Loss: 0.00125811
Iteration 14/25 | Loss: 0.00125811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012581124901771545, 0.0012581124901771545, 0.0012581124901771545, 0.0012581124901771545, 0.0012581124901771545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012581124901771545

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41516399
Iteration 2/25 | Loss: 0.00090414
Iteration 3/25 | Loss: 0.00090412
Iteration 4/25 | Loss: 0.00090412
Iteration 5/25 | Loss: 0.00090412
Iteration 6/25 | Loss: 0.00090412
Iteration 7/25 | Loss: 0.00090412
Iteration 8/25 | Loss: 0.00090412
Iteration 9/25 | Loss: 0.00090412
Iteration 10/25 | Loss: 0.00090412
Iteration 11/25 | Loss: 0.00090412
Iteration 12/25 | Loss: 0.00090412
Iteration 13/25 | Loss: 0.00090412
Iteration 14/25 | Loss: 0.00090412
Iteration 15/25 | Loss: 0.00090412
Iteration 16/25 | Loss: 0.00090412
Iteration 17/25 | Loss: 0.00090412
Iteration 18/25 | Loss: 0.00090412
Iteration 19/25 | Loss: 0.00090412
Iteration 20/25 | Loss: 0.00090412
Iteration 21/25 | Loss: 0.00090412
Iteration 22/25 | Loss: 0.00090412
Iteration 23/25 | Loss: 0.00090412
Iteration 24/25 | Loss: 0.00090412
Iteration 25/25 | Loss: 0.00090412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090412
Iteration 2/1000 | Loss: 0.00003186
Iteration 3/1000 | Loss: 0.00002232
Iteration 4/1000 | Loss: 0.00001864
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001696
Iteration 7/1000 | Loss: 0.00001651
Iteration 8/1000 | Loss: 0.00001607
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001544
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001480
Iteration 17/1000 | Loss: 0.00001479
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001477
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001467
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001466
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001466
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001466
Iteration 30/1000 | Loss: 0.00001466
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001460
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001457
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001455
Iteration 50/1000 | Loss: 0.00001454
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001451
Iteration 57/1000 | Loss: 0.00001451
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001447
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001443
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001442
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001441
Iteration 80/1000 | Loss: 0.00001441
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001435
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001432
Iteration 130/1000 | Loss: 0.00001432
Iteration 131/1000 | Loss: 0.00001432
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001431
Iteration 138/1000 | Loss: 0.00001431
Iteration 139/1000 | Loss: 0.00001431
Iteration 140/1000 | Loss: 0.00001431
Iteration 141/1000 | Loss: 0.00001431
Iteration 142/1000 | Loss: 0.00001431
Iteration 143/1000 | Loss: 0.00001431
Iteration 144/1000 | Loss: 0.00001431
Iteration 145/1000 | Loss: 0.00001431
Iteration 146/1000 | Loss: 0.00001431
Iteration 147/1000 | Loss: 0.00001430
Iteration 148/1000 | Loss: 0.00001430
Iteration 149/1000 | Loss: 0.00001430
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001430
Iteration 155/1000 | Loss: 0.00001430
Iteration 156/1000 | Loss: 0.00001430
Iteration 157/1000 | Loss: 0.00001430
Iteration 158/1000 | Loss: 0.00001430
Iteration 159/1000 | Loss: 0.00001430
Iteration 160/1000 | Loss: 0.00001430
Iteration 161/1000 | Loss: 0.00001430
Iteration 162/1000 | Loss: 0.00001430
Iteration 163/1000 | Loss: 0.00001430
Iteration 164/1000 | Loss: 0.00001430
Iteration 165/1000 | Loss: 0.00001430
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001429
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001429
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Iteration 176/1000 | Loss: 0.00001429
Iteration 177/1000 | Loss: 0.00001429
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001429
Iteration 185/1000 | Loss: 0.00001429
Iteration 186/1000 | Loss: 0.00001429
Iteration 187/1000 | Loss: 0.00001429
Iteration 188/1000 | Loss: 0.00001429
Iteration 189/1000 | Loss: 0.00001429
Iteration 190/1000 | Loss: 0.00001428
Iteration 191/1000 | Loss: 0.00001428
Iteration 192/1000 | Loss: 0.00001428
Iteration 193/1000 | Loss: 0.00001428
Iteration 194/1000 | Loss: 0.00001428
Iteration 195/1000 | Loss: 0.00001428
Iteration 196/1000 | Loss: 0.00001428
Iteration 197/1000 | Loss: 0.00001428
Iteration 198/1000 | Loss: 0.00001428
Iteration 199/1000 | Loss: 0.00001428
Iteration 200/1000 | Loss: 0.00001428
Iteration 201/1000 | Loss: 0.00001428
Iteration 202/1000 | Loss: 0.00001428
Iteration 203/1000 | Loss: 0.00001428
Iteration 204/1000 | Loss: 0.00001428
Iteration 205/1000 | Loss: 0.00001428
Iteration 206/1000 | Loss: 0.00001428
Iteration 207/1000 | Loss: 0.00001427
Iteration 208/1000 | Loss: 0.00001427
Iteration 209/1000 | Loss: 0.00001427
Iteration 210/1000 | Loss: 0.00001427
Iteration 211/1000 | Loss: 0.00001427
Iteration 212/1000 | Loss: 0.00001427
Iteration 213/1000 | Loss: 0.00001427
Iteration 214/1000 | Loss: 0.00001427
Iteration 215/1000 | Loss: 0.00001427
Iteration 216/1000 | Loss: 0.00001427
Iteration 217/1000 | Loss: 0.00001427
Iteration 218/1000 | Loss: 0.00001427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.4272661246650387e-05, 1.4272661246650387e-05, 1.4272661246650387e-05, 1.4272661246650387e-05, 1.4272661246650387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4272661246650387e-05

Optimization complete. Final v2v error: 3.162062406539917 mm

Highest mean error: 3.811833381652832 mm for frame 69

Lowest mean error: 2.7757246494293213 mm for frame 26

Saving results

Total time: 42.3435320854187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008968
Iteration 2/25 | Loss: 0.00209577
Iteration 3/25 | Loss: 0.00155272
Iteration 4/25 | Loss: 0.00141637
Iteration 5/25 | Loss: 0.00138821
Iteration 6/25 | Loss: 0.00136742
Iteration 7/25 | Loss: 0.00136343
Iteration 8/25 | Loss: 0.00136213
Iteration 9/25 | Loss: 0.00136068
Iteration 10/25 | Loss: 0.00135853
Iteration 11/25 | Loss: 0.00135751
Iteration 12/25 | Loss: 0.00135727
Iteration 13/25 | Loss: 0.00135724
Iteration 14/25 | Loss: 0.00135724
Iteration 15/25 | Loss: 0.00135724
Iteration 16/25 | Loss: 0.00135724
Iteration 17/25 | Loss: 0.00135724
Iteration 18/25 | Loss: 0.00135724
Iteration 19/25 | Loss: 0.00135724
Iteration 20/25 | Loss: 0.00135724
Iteration 21/25 | Loss: 0.00135724
Iteration 22/25 | Loss: 0.00135724
Iteration 23/25 | Loss: 0.00135724
Iteration 24/25 | Loss: 0.00135724
Iteration 25/25 | Loss: 0.00135724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25518644
Iteration 2/25 | Loss: 0.00167641
Iteration 3/25 | Loss: 0.00153754
Iteration 4/25 | Loss: 0.00153754
Iteration 5/25 | Loss: 0.00153754
Iteration 6/25 | Loss: 0.00153754
Iteration 7/25 | Loss: 0.00153754
Iteration 8/25 | Loss: 0.00153754
Iteration 9/25 | Loss: 0.00153754
Iteration 10/25 | Loss: 0.00153754
Iteration 11/25 | Loss: 0.00153754
Iteration 12/25 | Loss: 0.00153754
Iteration 13/25 | Loss: 0.00153754
Iteration 14/25 | Loss: 0.00153754
Iteration 15/25 | Loss: 0.00153754
Iteration 16/25 | Loss: 0.00153754
Iteration 17/25 | Loss: 0.00153754
Iteration 18/25 | Loss: 0.00153754
Iteration 19/25 | Loss: 0.00153754
Iteration 20/25 | Loss: 0.00153754
Iteration 21/25 | Loss: 0.00153754
Iteration 22/25 | Loss: 0.00153754
Iteration 23/25 | Loss: 0.00153754
Iteration 24/25 | Loss: 0.00153754
Iteration 25/25 | Loss: 0.00153754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153754
Iteration 2/1000 | Loss: 0.00012017
Iteration 3/1000 | Loss: 0.00005131
Iteration 4/1000 | Loss: 0.00003952
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00013096
Iteration 7/1000 | Loss: 0.00004158
Iteration 8/1000 | Loss: 0.00002980
Iteration 9/1000 | Loss: 0.00002739
Iteration 10/1000 | Loss: 0.00002580
Iteration 11/1000 | Loss: 0.00002410
Iteration 12/1000 | Loss: 0.00002289
Iteration 13/1000 | Loss: 0.00002245
Iteration 14/1000 | Loss: 0.00002201
Iteration 15/1000 | Loss: 0.00002177
Iteration 16/1000 | Loss: 0.00002173
Iteration 17/1000 | Loss: 0.00002166
Iteration 18/1000 | Loss: 0.00002159
Iteration 19/1000 | Loss: 0.00002153
Iteration 20/1000 | Loss: 0.00002153
Iteration 21/1000 | Loss: 0.00002146
Iteration 22/1000 | Loss: 0.00002139
Iteration 23/1000 | Loss: 0.00002139
Iteration 24/1000 | Loss: 0.00002138
Iteration 25/1000 | Loss: 0.00002137
Iteration 26/1000 | Loss: 0.00002137
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002130
Iteration 32/1000 | Loss: 0.00002129
Iteration 33/1000 | Loss: 0.00002129
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002125
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002119
Iteration 41/1000 | Loss: 0.00002118
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002118
Iteration 44/1000 | Loss: 0.00002118
Iteration 45/1000 | Loss: 0.00002118
Iteration 46/1000 | Loss: 0.00002118
Iteration 47/1000 | Loss: 0.00002117
Iteration 48/1000 | Loss: 0.00002117
Iteration 49/1000 | Loss: 0.00002117
Iteration 50/1000 | Loss: 0.00002117
Iteration 51/1000 | Loss: 0.00002117
Iteration 52/1000 | Loss: 0.00002117
Iteration 53/1000 | Loss: 0.00002117
Iteration 54/1000 | Loss: 0.00002117
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002116
Iteration 57/1000 | Loss: 0.00002116
Iteration 58/1000 | Loss: 0.00002115
Iteration 59/1000 | Loss: 0.00002115
Iteration 60/1000 | Loss: 0.00002114
Iteration 61/1000 | Loss: 0.00002114
Iteration 62/1000 | Loss: 0.00002113
Iteration 63/1000 | Loss: 0.00002113
Iteration 64/1000 | Loss: 0.00002113
Iteration 65/1000 | Loss: 0.00002113
Iteration 66/1000 | Loss: 0.00002112
Iteration 67/1000 | Loss: 0.00002112
Iteration 68/1000 | Loss: 0.00002111
Iteration 69/1000 | Loss: 0.00002111
Iteration 70/1000 | Loss: 0.00002111
Iteration 71/1000 | Loss: 0.00002110
Iteration 72/1000 | Loss: 0.00002110
Iteration 73/1000 | Loss: 0.00002110
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002110
Iteration 77/1000 | Loss: 0.00002110
Iteration 78/1000 | Loss: 0.00002109
Iteration 79/1000 | Loss: 0.00002109
Iteration 80/1000 | Loss: 0.00002109
Iteration 81/1000 | Loss: 0.00002109
Iteration 82/1000 | Loss: 0.00002109
Iteration 83/1000 | Loss: 0.00002109
Iteration 84/1000 | Loss: 0.00002109
Iteration 85/1000 | Loss: 0.00002108
Iteration 86/1000 | Loss: 0.00002108
Iteration 87/1000 | Loss: 0.00002107
Iteration 88/1000 | Loss: 0.00002107
Iteration 89/1000 | Loss: 0.00002107
Iteration 90/1000 | Loss: 0.00002107
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002105
Iteration 95/1000 | Loss: 0.00002105
Iteration 96/1000 | Loss: 0.00002105
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00002104
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002104
Iteration 105/1000 | Loss: 0.00002104
Iteration 106/1000 | Loss: 0.00002104
Iteration 107/1000 | Loss: 0.00002104
Iteration 108/1000 | Loss: 0.00002104
Iteration 109/1000 | Loss: 0.00002104
Iteration 110/1000 | Loss: 0.00002103
Iteration 111/1000 | Loss: 0.00002103
Iteration 112/1000 | Loss: 0.00002103
Iteration 113/1000 | Loss: 0.00002103
Iteration 114/1000 | Loss: 0.00002103
Iteration 115/1000 | Loss: 0.00002102
Iteration 116/1000 | Loss: 0.00002102
Iteration 117/1000 | Loss: 0.00002102
Iteration 118/1000 | Loss: 0.00002102
Iteration 119/1000 | Loss: 0.00002102
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00002101
Iteration 125/1000 | Loss: 0.00002101
Iteration 126/1000 | Loss: 0.00002101
Iteration 127/1000 | Loss: 0.00002101
Iteration 128/1000 | Loss: 0.00002101
Iteration 129/1000 | Loss: 0.00002101
Iteration 130/1000 | Loss: 0.00002101
Iteration 131/1000 | Loss: 0.00002101
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002100
Iteration 137/1000 | Loss: 0.00002100
Iteration 138/1000 | Loss: 0.00002100
Iteration 139/1000 | Loss: 0.00002100
Iteration 140/1000 | Loss: 0.00002100
Iteration 141/1000 | Loss: 0.00002100
Iteration 142/1000 | Loss: 0.00002100
Iteration 143/1000 | Loss: 0.00002100
Iteration 144/1000 | Loss: 0.00002100
Iteration 145/1000 | Loss: 0.00002100
Iteration 146/1000 | Loss: 0.00002100
Iteration 147/1000 | Loss: 0.00002100
Iteration 148/1000 | Loss: 0.00002100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.1001675122533925e-05, 2.1001675122533925e-05, 2.1001675122533925e-05, 2.1001675122533925e-05, 2.1001675122533925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1001675122533925e-05

Optimization complete. Final v2v error: 3.771876811981201 mm

Highest mean error: 5.383601665496826 mm for frame 225

Lowest mean error: 3.343834638595581 mm for frame 64

Saving results

Total time: 65.10200500488281
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053312
Iteration 2/25 | Loss: 0.00191557
Iteration 3/25 | Loss: 0.00169414
Iteration 4/25 | Loss: 0.00130678
Iteration 5/25 | Loss: 0.00128500
Iteration 6/25 | Loss: 0.00126177
Iteration 7/25 | Loss: 0.00125633
Iteration 8/25 | Loss: 0.00125818
Iteration 9/25 | Loss: 0.00124389
Iteration 10/25 | Loss: 0.00124540
Iteration 11/25 | Loss: 0.00124243
Iteration 12/25 | Loss: 0.00124218
Iteration 13/25 | Loss: 0.00124116
Iteration 14/25 | Loss: 0.00124442
Iteration 15/25 | Loss: 0.00124093
Iteration 16/25 | Loss: 0.00124093
Iteration 17/25 | Loss: 0.00124093
Iteration 18/25 | Loss: 0.00124093
Iteration 19/25 | Loss: 0.00124093
Iteration 20/25 | Loss: 0.00124093
Iteration 21/25 | Loss: 0.00124093
Iteration 22/25 | Loss: 0.00124093
Iteration 23/25 | Loss: 0.00124093
Iteration 24/25 | Loss: 0.00124093
Iteration 25/25 | Loss: 0.00124093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86528134
Iteration 2/25 | Loss: 0.00117869
Iteration 3/25 | Loss: 0.00109025
Iteration 4/25 | Loss: 0.00109025
Iteration 5/25 | Loss: 0.00109024
Iteration 6/25 | Loss: 0.00109024
Iteration 7/25 | Loss: 0.00109024
Iteration 8/25 | Loss: 0.00109024
Iteration 9/25 | Loss: 0.00109024
Iteration 10/25 | Loss: 0.00109024
Iteration 11/25 | Loss: 0.00109024
Iteration 12/25 | Loss: 0.00109024
Iteration 13/25 | Loss: 0.00109024
Iteration 14/25 | Loss: 0.00109024
Iteration 15/25 | Loss: 0.00109024
Iteration 16/25 | Loss: 0.00109024
Iteration 17/25 | Loss: 0.00109024
Iteration 18/25 | Loss: 0.00109024
Iteration 19/25 | Loss: 0.00109024
Iteration 20/25 | Loss: 0.00109024
Iteration 21/25 | Loss: 0.00109024
Iteration 22/25 | Loss: 0.00109024
Iteration 23/25 | Loss: 0.00109024
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010902417125180364, 0.0010902417125180364, 0.0010902417125180364, 0.0010902417125180364, 0.0010902417125180364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010902417125180364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109024
Iteration 2/1000 | Loss: 0.00011988
Iteration 3/1000 | Loss: 0.00003578
Iteration 4/1000 | Loss: 0.00005598
Iteration 5/1000 | Loss: 0.00005052
Iteration 6/1000 | Loss: 0.00002757
Iteration 7/1000 | Loss: 0.00003362
Iteration 8/1000 | Loss: 0.00004640
Iteration 9/1000 | Loss: 0.00002563
Iteration 10/1000 | Loss: 0.00006740
Iteration 11/1000 | Loss: 0.00002506
Iteration 12/1000 | Loss: 0.00035142
Iteration 13/1000 | Loss: 0.00002953
Iteration 14/1000 | Loss: 0.00002507
Iteration 15/1000 | Loss: 0.00006312
Iteration 16/1000 | Loss: 0.00055063
Iteration 17/1000 | Loss: 0.00002790
Iteration 18/1000 | Loss: 0.00007057
Iteration 19/1000 | Loss: 0.00010405
Iteration 20/1000 | Loss: 0.00002249
Iteration 21/1000 | Loss: 0.00002285
Iteration 22/1000 | Loss: 0.00005445
Iteration 23/1000 | Loss: 0.00002587
Iteration 24/1000 | Loss: 0.00002347
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002172
Iteration 27/1000 | Loss: 0.00002171
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002169
Iteration 31/1000 | Loss: 0.00002169
Iteration 32/1000 | Loss: 0.00002166
Iteration 33/1000 | Loss: 0.00002164
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00006243
Iteration 37/1000 | Loss: 0.00004665
Iteration 38/1000 | Loss: 0.00026979
Iteration 39/1000 | Loss: 0.00002231
Iteration 40/1000 | Loss: 0.00004722
Iteration 41/1000 | Loss: 0.00003361
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002935
Iteration 44/1000 | Loss: 0.00002150
Iteration 45/1000 | Loss: 0.00002149
Iteration 46/1000 | Loss: 0.00002146
Iteration 47/1000 | Loss: 0.00002141
Iteration 48/1000 | Loss: 0.00002137
Iteration 49/1000 | Loss: 0.00002136
Iteration 50/1000 | Loss: 0.00002136
Iteration 51/1000 | Loss: 0.00002136
Iteration 52/1000 | Loss: 0.00002135
Iteration 53/1000 | Loss: 0.00002135
Iteration 54/1000 | Loss: 0.00002134
Iteration 55/1000 | Loss: 0.00002134
Iteration 56/1000 | Loss: 0.00002134
Iteration 57/1000 | Loss: 0.00002133
Iteration 58/1000 | Loss: 0.00002133
Iteration 59/1000 | Loss: 0.00002133
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002131
Iteration 62/1000 | Loss: 0.00003177
Iteration 63/1000 | Loss: 0.00020617
Iteration 64/1000 | Loss: 0.00002455
Iteration 65/1000 | Loss: 0.00003187
Iteration 66/1000 | Loss: 0.00002920
Iteration 67/1000 | Loss: 0.00005864
Iteration 68/1000 | Loss: 0.00002686
Iteration 69/1000 | Loss: 0.00002320
Iteration 70/1000 | Loss: 0.00002131
Iteration 71/1000 | Loss: 0.00002130
Iteration 72/1000 | Loss: 0.00002130
Iteration 73/1000 | Loss: 0.00002129
Iteration 74/1000 | Loss: 0.00002128
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00002125
Iteration 77/1000 | Loss: 0.00002124
Iteration 78/1000 | Loss: 0.00002123
Iteration 79/1000 | Loss: 0.00002122
Iteration 80/1000 | Loss: 0.00002117
Iteration 81/1000 | Loss: 0.00002116
Iteration 82/1000 | Loss: 0.00004740
Iteration 83/1000 | Loss: 0.00002132
Iteration 84/1000 | Loss: 0.00003584
Iteration 85/1000 | Loss: 0.00002111
Iteration 86/1000 | Loss: 0.00002111
Iteration 87/1000 | Loss: 0.00002111
Iteration 88/1000 | Loss: 0.00002111
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002110
Iteration 91/1000 | Loss: 0.00002110
Iteration 92/1000 | Loss: 0.00002110
Iteration 93/1000 | Loss: 0.00002109
Iteration 94/1000 | Loss: 0.00002109
Iteration 95/1000 | Loss: 0.00002385
Iteration 96/1000 | Loss: 0.00002109
Iteration 97/1000 | Loss: 0.00002109
Iteration 98/1000 | Loss: 0.00002108
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002107
Iteration 101/1000 | Loss: 0.00002106
Iteration 102/1000 | Loss: 0.00002106
Iteration 103/1000 | Loss: 0.00002106
Iteration 104/1000 | Loss: 0.00002106
Iteration 105/1000 | Loss: 0.00002105
Iteration 106/1000 | Loss: 0.00002105
Iteration 107/1000 | Loss: 0.00002105
Iteration 108/1000 | Loss: 0.00002105
Iteration 109/1000 | Loss: 0.00002105
Iteration 110/1000 | Loss: 0.00002105
Iteration 111/1000 | Loss: 0.00002105
Iteration 112/1000 | Loss: 0.00002105
Iteration 113/1000 | Loss: 0.00002105
Iteration 114/1000 | Loss: 0.00002105
Iteration 115/1000 | Loss: 0.00002105
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002104
Iteration 119/1000 | Loss: 0.00002104
Iteration 120/1000 | Loss: 0.00002104
Iteration 121/1000 | Loss: 0.00002103
Iteration 122/1000 | Loss: 0.00002103
Iteration 123/1000 | Loss: 0.00002103
Iteration 124/1000 | Loss: 0.00002103
Iteration 125/1000 | Loss: 0.00002102
Iteration 126/1000 | Loss: 0.00002102
Iteration 127/1000 | Loss: 0.00002102
Iteration 128/1000 | Loss: 0.00002102
Iteration 129/1000 | Loss: 0.00002102
Iteration 130/1000 | Loss: 0.00002102
Iteration 131/1000 | Loss: 0.00002102
Iteration 132/1000 | Loss: 0.00002102
Iteration 133/1000 | Loss: 0.00002102
Iteration 134/1000 | Loss: 0.00002102
Iteration 135/1000 | Loss: 0.00002102
Iteration 136/1000 | Loss: 0.00002102
Iteration 137/1000 | Loss: 0.00002102
Iteration 138/1000 | Loss: 0.00002102
Iteration 139/1000 | Loss: 0.00002102
Iteration 140/1000 | Loss: 0.00002102
Iteration 141/1000 | Loss: 0.00002102
Iteration 142/1000 | Loss: 0.00002102
Iteration 143/1000 | Loss: 0.00002102
Iteration 144/1000 | Loss: 0.00002102
Iteration 145/1000 | Loss: 0.00002102
Iteration 146/1000 | Loss: 0.00002102
Iteration 147/1000 | Loss: 0.00002102
Iteration 148/1000 | Loss: 0.00002102
Iteration 149/1000 | Loss: 0.00002102
Iteration 150/1000 | Loss: 0.00002102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.102251528413035e-05, 2.102251528413035e-05, 2.102251528413035e-05, 2.102251528413035e-05, 2.102251528413035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.102251528413035e-05

Optimization complete. Final v2v error: 3.9320099353790283 mm

Highest mean error: 4.242112159729004 mm for frame 2

Lowest mean error: 3.722395896911621 mm for frame 115

Saving results

Total time: 98.04866480827332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034880
Iteration 2/25 | Loss: 0.00194406
Iteration 3/25 | Loss: 0.00160080
Iteration 4/25 | Loss: 0.00154127
Iteration 5/25 | Loss: 0.00156687
Iteration 6/25 | Loss: 0.00148669
Iteration 7/25 | Loss: 0.00145907
Iteration 8/25 | Loss: 0.00147497
Iteration 9/25 | Loss: 0.00135721
Iteration 10/25 | Loss: 0.00136530
Iteration 11/25 | Loss: 0.00134899
Iteration 12/25 | Loss: 0.00134795
Iteration 13/25 | Loss: 0.00133915
Iteration 14/25 | Loss: 0.00131138
Iteration 15/25 | Loss: 0.00130874
Iteration 16/25 | Loss: 0.00132004
Iteration 17/25 | Loss: 0.00129937
Iteration 18/25 | Loss: 0.00131740
Iteration 19/25 | Loss: 0.00128700
Iteration 20/25 | Loss: 0.00130437
Iteration 21/25 | Loss: 0.00128510
Iteration 22/25 | Loss: 0.00126873
Iteration 23/25 | Loss: 0.00126594
Iteration 24/25 | Loss: 0.00126560
Iteration 25/25 | Loss: 0.00128008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38415790
Iteration 2/25 | Loss: 0.00115549
Iteration 3/25 | Loss: 0.00115549
Iteration 4/25 | Loss: 0.00115549
Iteration 5/25 | Loss: 0.00115549
Iteration 6/25 | Loss: 0.00115549
Iteration 7/25 | Loss: 0.00115548
Iteration 8/25 | Loss: 0.00115548
Iteration 9/25 | Loss: 0.00115548
Iteration 10/25 | Loss: 0.00115548
Iteration 11/25 | Loss: 0.00115548
Iteration 12/25 | Loss: 0.00115548
Iteration 13/25 | Loss: 0.00115548
Iteration 14/25 | Loss: 0.00115548
Iteration 15/25 | Loss: 0.00115548
Iteration 16/25 | Loss: 0.00115548
Iteration 17/25 | Loss: 0.00115548
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011554836528375745, 0.0011554836528375745, 0.0011554836528375745, 0.0011554836528375745, 0.0011554836528375745]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011554836528375745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115548
Iteration 2/1000 | Loss: 0.00119710
Iteration 3/1000 | Loss: 0.00005751
Iteration 4/1000 | Loss: 0.00051572
Iteration 5/1000 | Loss: 0.00065728
Iteration 6/1000 | Loss: 0.00055699
Iteration 7/1000 | Loss: 0.00062689
Iteration 8/1000 | Loss: 0.00033751
Iteration 9/1000 | Loss: 0.00033690
Iteration 10/1000 | Loss: 0.00033861
Iteration 11/1000 | Loss: 0.00029683
Iteration 12/1000 | Loss: 0.00004940
Iteration 13/1000 | Loss: 0.00003203
Iteration 14/1000 | Loss: 0.00002626
Iteration 15/1000 | Loss: 0.00002255
Iteration 16/1000 | Loss: 0.00009966
Iteration 17/1000 | Loss: 0.00003544
Iteration 18/1000 | Loss: 0.00004063
Iteration 19/1000 | Loss: 0.00001993
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00002751
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00002765
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001512
Iteration 31/1000 | Loss: 0.00001507
Iteration 32/1000 | Loss: 0.00002424
Iteration 33/1000 | Loss: 0.00001669
Iteration 34/1000 | Loss: 0.00001668
Iteration 35/1000 | Loss: 0.00001499
Iteration 36/1000 | Loss: 0.00001498
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001497
Iteration 39/1000 | Loss: 0.00001497
Iteration 40/1000 | Loss: 0.00001496
Iteration 41/1000 | Loss: 0.00001496
Iteration 42/1000 | Loss: 0.00001496
Iteration 43/1000 | Loss: 0.00001496
Iteration 44/1000 | Loss: 0.00001495
Iteration 45/1000 | Loss: 0.00001495
Iteration 46/1000 | Loss: 0.00001495
Iteration 47/1000 | Loss: 0.00001495
Iteration 48/1000 | Loss: 0.00001494
Iteration 49/1000 | Loss: 0.00001494
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001530
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001486
Iteration 56/1000 | Loss: 0.00001486
Iteration 57/1000 | Loss: 0.00001485
Iteration 58/1000 | Loss: 0.00001485
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001483
Iteration 61/1000 | Loss: 0.00001498
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001469
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00003192
Iteration 69/1000 | Loss: 0.00001567
Iteration 70/1000 | Loss: 0.00006084
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001449
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001448
Iteration 79/1000 | Loss: 0.00001447
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001446
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001445
Iteration 88/1000 | Loss: 0.00001445
Iteration 89/1000 | Loss: 0.00001445
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001442
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00002738
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00002039
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001711
Iteration 123/1000 | Loss: 0.00002379
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.4359258784679696e-05, 1.4359258784679696e-05, 1.4359258784679696e-05, 1.4359258784679696e-05, 1.4359258784679696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4359258784679696e-05

Optimization complete. Final v2v error: 3.1633076667785645 mm

Highest mean error: 4.936622619628906 mm for frame 68

Lowest mean error: 2.742894172668457 mm for frame 130

Saving results

Total time: 111.17482829093933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00963305
Iteration 2/25 | Loss: 0.00283423
Iteration 3/25 | Loss: 0.00222298
Iteration 4/25 | Loss: 0.00210033
Iteration 5/25 | Loss: 0.00200247
Iteration 6/25 | Loss: 0.00184289
Iteration 7/25 | Loss: 0.00179561
Iteration 8/25 | Loss: 0.00174856
Iteration 9/25 | Loss: 0.00171434
Iteration 10/25 | Loss: 0.00170305
Iteration 11/25 | Loss: 0.00168016
Iteration 12/25 | Loss: 0.00166656
Iteration 13/25 | Loss: 0.00165216
Iteration 14/25 | Loss: 0.00164561
Iteration 15/25 | Loss: 0.00164095
Iteration 16/25 | Loss: 0.00164347
Iteration 17/25 | Loss: 0.00163751
Iteration 18/25 | Loss: 0.00163576
Iteration 19/25 | Loss: 0.00163523
Iteration 20/25 | Loss: 0.00163508
Iteration 21/25 | Loss: 0.00163503
Iteration 22/25 | Loss: 0.00163503
Iteration 23/25 | Loss: 0.00163503
Iteration 24/25 | Loss: 0.00163503
Iteration 25/25 | Loss: 0.00163503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33753729
Iteration 2/25 | Loss: 0.00306116
Iteration 3/25 | Loss: 0.00306116
Iteration 4/25 | Loss: 0.00306116
Iteration 5/25 | Loss: 0.00306115
Iteration 6/25 | Loss: 0.00306115
Iteration 7/25 | Loss: 0.00306115
Iteration 8/25 | Loss: 0.00306115
Iteration 9/25 | Loss: 0.00306115
Iteration 10/25 | Loss: 0.00306115
Iteration 11/25 | Loss: 0.00306115
Iteration 12/25 | Loss: 0.00306115
Iteration 13/25 | Loss: 0.00306115
Iteration 14/25 | Loss: 0.00306115
Iteration 15/25 | Loss: 0.00306115
Iteration 16/25 | Loss: 0.00306115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003061152994632721, 0.003061152994632721, 0.003061152994632721, 0.003061152994632721, 0.003061152994632721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003061152994632721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306115
Iteration 2/1000 | Loss: 0.00057006
Iteration 3/1000 | Loss: 0.00032390
Iteration 4/1000 | Loss: 0.00027982
Iteration 5/1000 | Loss: 0.00025399
Iteration 6/1000 | Loss: 0.00023357
Iteration 7/1000 | Loss: 0.00021869
Iteration 8/1000 | Loss: 0.00020479
Iteration 9/1000 | Loss: 0.00026235
Iteration 10/1000 | Loss: 0.00019236
Iteration 11/1000 | Loss: 0.00018340
Iteration 12/1000 | Loss: 0.00017628
Iteration 13/1000 | Loss: 0.00016880
Iteration 14/1000 | Loss: 0.00202144
Iteration 15/1000 | Loss: 0.00873960
Iteration 16/1000 | Loss: 0.01367379
Iteration 17/1000 | Loss: 0.00421860
Iteration 18/1000 | Loss: 0.00456899
Iteration 19/1000 | Loss: 0.00034606
Iteration 20/1000 | Loss: 0.00024125
Iteration 21/1000 | Loss: 0.00016834
Iteration 22/1000 | Loss: 0.00011478
Iteration 23/1000 | Loss: 0.00007416
Iteration 24/1000 | Loss: 0.00005596
Iteration 25/1000 | Loss: 0.00004683
Iteration 26/1000 | Loss: 0.00004053
Iteration 27/1000 | Loss: 0.00003582
Iteration 28/1000 | Loss: 0.00003233
Iteration 29/1000 | Loss: 0.00002982
Iteration 30/1000 | Loss: 0.00002732
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00002486
Iteration 33/1000 | Loss: 0.00002402
Iteration 34/1000 | Loss: 0.00002337
Iteration 35/1000 | Loss: 0.00002301
Iteration 36/1000 | Loss: 0.00002271
Iteration 37/1000 | Loss: 0.00002248
Iteration 38/1000 | Loss: 0.00002235
Iteration 39/1000 | Loss: 0.00002234
Iteration 40/1000 | Loss: 0.00002233
Iteration 41/1000 | Loss: 0.00002228
Iteration 42/1000 | Loss: 0.00002228
Iteration 43/1000 | Loss: 0.00002227
Iteration 44/1000 | Loss: 0.00002227
Iteration 45/1000 | Loss: 0.00002227
Iteration 46/1000 | Loss: 0.00002225
Iteration 47/1000 | Loss: 0.00002221
Iteration 48/1000 | Loss: 0.00002221
Iteration 49/1000 | Loss: 0.00002220
Iteration 50/1000 | Loss: 0.00002219
Iteration 51/1000 | Loss: 0.00002218
Iteration 52/1000 | Loss: 0.00002218
Iteration 53/1000 | Loss: 0.00002218
Iteration 54/1000 | Loss: 0.00002218
Iteration 55/1000 | Loss: 0.00002216
Iteration 56/1000 | Loss: 0.00002216
Iteration 57/1000 | Loss: 0.00002215
Iteration 58/1000 | Loss: 0.00002215
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002214
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002214
Iteration 64/1000 | Loss: 0.00002214
Iteration 65/1000 | Loss: 0.00002214
Iteration 66/1000 | Loss: 0.00002214
Iteration 67/1000 | Loss: 0.00002214
Iteration 68/1000 | Loss: 0.00002214
Iteration 69/1000 | Loss: 0.00002214
Iteration 70/1000 | Loss: 0.00002214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.2141479348647408e-05, 2.2141479348647408e-05, 2.2141479348647408e-05, 2.2141479348647408e-05, 2.2141479348647408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2141479348647408e-05

Optimization complete. Final v2v error: 3.9960811138153076 mm

Highest mean error: 4.33839750289917 mm for frame 226

Lowest mean error: 3.8788697719573975 mm for frame 87

Saving results

Total time: 105.1255030632019
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503859
Iteration 2/25 | Loss: 0.00133726
Iteration 3/25 | Loss: 0.00125910
Iteration 4/25 | Loss: 0.00123956
Iteration 5/25 | Loss: 0.00123334
Iteration 6/25 | Loss: 0.00123191
Iteration 7/25 | Loss: 0.00123180
Iteration 8/25 | Loss: 0.00123180
Iteration 9/25 | Loss: 0.00123180
Iteration 10/25 | Loss: 0.00123180
Iteration 11/25 | Loss: 0.00123180
Iteration 12/25 | Loss: 0.00123180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012317952932789922, 0.0012317952932789922, 0.0012317952932789922, 0.0012317952932789922, 0.0012317952932789922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012317952932789922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.68423653
Iteration 2/25 | Loss: 0.00105405
Iteration 3/25 | Loss: 0.00105405
Iteration 4/25 | Loss: 0.00105405
Iteration 5/25 | Loss: 0.00105405
Iteration 6/25 | Loss: 0.00105405
Iteration 7/25 | Loss: 0.00105405
Iteration 8/25 | Loss: 0.00105405
Iteration 9/25 | Loss: 0.00105405
Iteration 10/25 | Loss: 0.00105405
Iteration 11/25 | Loss: 0.00105405
Iteration 12/25 | Loss: 0.00105405
Iteration 13/25 | Loss: 0.00105405
Iteration 14/25 | Loss: 0.00105405
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001054047024808824, 0.001054047024808824, 0.001054047024808824, 0.001054047024808824, 0.001054047024808824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001054047024808824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105405
Iteration 2/1000 | Loss: 0.00004111
Iteration 3/1000 | Loss: 0.00002918
Iteration 4/1000 | Loss: 0.00002494
Iteration 5/1000 | Loss: 0.00002368
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002191
Iteration 8/1000 | Loss: 0.00002127
Iteration 9/1000 | Loss: 0.00002082
Iteration 10/1000 | Loss: 0.00002043
Iteration 11/1000 | Loss: 0.00002018
Iteration 12/1000 | Loss: 0.00001988
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001961
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001932
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001913
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001907
Iteration 32/1000 | Loss: 0.00001907
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001905
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001903
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001903
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001902
Iteration 49/1000 | Loss: 0.00001902
Iteration 50/1000 | Loss: 0.00001902
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001901
Iteration 53/1000 | Loss: 0.00001901
Iteration 54/1000 | Loss: 0.00001901
Iteration 55/1000 | Loss: 0.00001900
Iteration 56/1000 | Loss: 0.00001900
Iteration 57/1000 | Loss: 0.00001900
Iteration 58/1000 | Loss: 0.00001900
Iteration 59/1000 | Loss: 0.00001900
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001899
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001898
Iteration 64/1000 | Loss: 0.00001898
Iteration 65/1000 | Loss: 0.00001898
Iteration 66/1000 | Loss: 0.00001898
Iteration 67/1000 | Loss: 0.00001898
Iteration 68/1000 | Loss: 0.00001897
Iteration 69/1000 | Loss: 0.00001897
Iteration 70/1000 | Loss: 0.00001897
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001896
Iteration 74/1000 | Loss: 0.00001896
Iteration 75/1000 | Loss: 0.00001895
Iteration 76/1000 | Loss: 0.00001895
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001889
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001888
Iteration 104/1000 | Loss: 0.00001888
Iteration 105/1000 | Loss: 0.00001888
Iteration 106/1000 | Loss: 0.00001888
Iteration 107/1000 | Loss: 0.00001888
Iteration 108/1000 | Loss: 0.00001887
Iteration 109/1000 | Loss: 0.00001887
Iteration 110/1000 | Loss: 0.00001887
Iteration 111/1000 | Loss: 0.00001887
Iteration 112/1000 | Loss: 0.00001887
Iteration 113/1000 | Loss: 0.00001887
Iteration 114/1000 | Loss: 0.00001887
Iteration 115/1000 | Loss: 0.00001887
Iteration 116/1000 | Loss: 0.00001886
Iteration 117/1000 | Loss: 0.00001886
Iteration 118/1000 | Loss: 0.00001886
Iteration 119/1000 | Loss: 0.00001886
Iteration 120/1000 | Loss: 0.00001886
Iteration 121/1000 | Loss: 0.00001885
Iteration 122/1000 | Loss: 0.00001885
Iteration 123/1000 | Loss: 0.00001885
Iteration 124/1000 | Loss: 0.00001884
Iteration 125/1000 | Loss: 0.00001884
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001884
Iteration 128/1000 | Loss: 0.00001884
Iteration 129/1000 | Loss: 0.00001884
Iteration 130/1000 | Loss: 0.00001884
Iteration 131/1000 | Loss: 0.00001884
Iteration 132/1000 | Loss: 0.00001884
Iteration 133/1000 | Loss: 0.00001884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.884379526018165e-05, 1.884379526018165e-05, 1.884379526018165e-05, 1.884379526018165e-05, 1.884379526018165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.884379526018165e-05

Optimization complete. Final v2v error: 3.684812068939209 mm

Highest mean error: 3.997511148452759 mm for frame 51

Lowest mean error: 3.264641046524048 mm for frame 35

Saving results

Total time: 41.681761026382446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990045
Iteration 2/25 | Loss: 0.00990044
Iteration 3/25 | Loss: 0.00344149
Iteration 4/25 | Loss: 0.00254043
Iteration 5/25 | Loss: 0.00230130
Iteration 6/25 | Loss: 0.00215727
Iteration 7/25 | Loss: 0.00212121
Iteration 8/25 | Loss: 0.00213020
Iteration 9/25 | Loss: 0.00229268
Iteration 10/25 | Loss: 0.00198490
Iteration 11/25 | Loss: 0.00190848
Iteration 12/25 | Loss: 0.00188927
Iteration 13/25 | Loss: 0.00187705
Iteration 14/25 | Loss: 0.00186618
Iteration 15/25 | Loss: 0.00185618
Iteration 16/25 | Loss: 0.00185454
Iteration 17/25 | Loss: 0.00184952
Iteration 18/25 | Loss: 0.00185420
Iteration 19/25 | Loss: 0.00185021
Iteration 20/25 | Loss: 0.00183854
Iteration 21/25 | Loss: 0.00184256
Iteration 22/25 | Loss: 0.00182998
Iteration 23/25 | Loss: 0.00182434
Iteration 24/25 | Loss: 0.00182725
Iteration 25/25 | Loss: 0.00182216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30114019
Iteration 2/25 | Loss: 0.00752964
Iteration 3/25 | Loss: 0.00564224
Iteration 4/25 | Loss: 0.00564224
Iteration 5/25 | Loss: 0.00564224
Iteration 6/25 | Loss: 0.00564224
Iteration 7/25 | Loss: 0.00564224
Iteration 8/25 | Loss: 0.00564224
Iteration 9/25 | Loss: 0.00564224
Iteration 10/25 | Loss: 0.00564224
Iteration 11/25 | Loss: 0.00564224
Iteration 12/25 | Loss: 0.00564224
Iteration 13/25 | Loss: 0.00564224
Iteration 14/25 | Loss: 0.00564224
Iteration 15/25 | Loss: 0.00564224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0056422376073896885, 0.0056422376073896885, 0.0056422376073896885, 0.0056422376073896885, 0.0056422376073896885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0056422376073896885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00564224
Iteration 2/1000 | Loss: 0.00929613
Iteration 3/1000 | Loss: 0.00112538
Iteration 4/1000 | Loss: 0.00704027
Iteration 5/1000 | Loss: 0.00124908
Iteration 6/1000 | Loss: 0.00049515
Iteration 7/1000 | Loss: 0.00043642
Iteration 8/1000 | Loss: 0.00070062
Iteration 9/1000 | Loss: 0.00053013
Iteration 10/1000 | Loss: 0.00039641
Iteration 11/1000 | Loss: 0.00033120
Iteration 12/1000 | Loss: 0.00031436
Iteration 13/1000 | Loss: 0.00029981
Iteration 14/1000 | Loss: 0.00035442
Iteration 15/1000 | Loss: 0.00052939
Iteration 16/1000 | Loss: 0.00031021
Iteration 17/1000 | Loss: 0.00046419
Iteration 18/1000 | Loss: 0.00060487
Iteration 19/1000 | Loss: 0.00062015
Iteration 20/1000 | Loss: 0.00079456
Iteration 21/1000 | Loss: 0.00036594
Iteration 22/1000 | Loss: 0.00087523
Iteration 23/1000 | Loss: 0.00129808
Iteration 24/1000 | Loss: 0.00106903
Iteration 25/1000 | Loss: 0.00116934
Iteration 26/1000 | Loss: 0.00276768
Iteration 27/1000 | Loss: 0.00830279
Iteration 28/1000 | Loss: 0.01790209
Iteration 29/1000 | Loss: 0.01621816
Iteration 30/1000 | Loss: 0.00510421
Iteration 31/1000 | Loss: 0.00385044
Iteration 32/1000 | Loss: 0.00205201
Iteration 33/1000 | Loss: 0.00047699
Iteration 34/1000 | Loss: 0.00036054
Iteration 35/1000 | Loss: 0.00029486
Iteration 36/1000 | Loss: 0.00021050
Iteration 37/1000 | Loss: 0.00104971
Iteration 38/1000 | Loss: 0.00015304
Iteration 39/1000 | Loss: 0.00081067
Iteration 40/1000 | Loss: 0.00018955
Iteration 41/1000 | Loss: 0.00015942
Iteration 42/1000 | Loss: 0.00006868
Iteration 43/1000 | Loss: 0.00018328
Iteration 44/1000 | Loss: 0.00021359
Iteration 45/1000 | Loss: 0.00025147
Iteration 46/1000 | Loss: 0.00005831
Iteration 47/1000 | Loss: 0.00005628
Iteration 48/1000 | Loss: 0.00009558
Iteration 49/1000 | Loss: 0.00010147
Iteration 50/1000 | Loss: 0.00004363
Iteration 51/1000 | Loss: 0.00011187
Iteration 52/1000 | Loss: 0.00005271
Iteration 53/1000 | Loss: 0.00007172
Iteration 54/1000 | Loss: 0.00015765
Iteration 55/1000 | Loss: 0.00006905
Iteration 56/1000 | Loss: 0.00002891
Iteration 57/1000 | Loss: 0.00006309
Iteration 58/1000 | Loss: 0.00009253
Iteration 59/1000 | Loss: 0.00013161
Iteration 60/1000 | Loss: 0.00016868
Iteration 61/1000 | Loss: 0.00006335
Iteration 62/1000 | Loss: 0.00003641
Iteration 63/1000 | Loss: 0.00006632
Iteration 64/1000 | Loss: 0.00016889
Iteration 65/1000 | Loss: 0.00003455
Iteration 66/1000 | Loss: 0.00002628
Iteration 67/1000 | Loss: 0.00003528
Iteration 68/1000 | Loss: 0.00003158
Iteration 69/1000 | Loss: 0.00002514
Iteration 70/1000 | Loss: 0.00006197
Iteration 71/1000 | Loss: 0.00005805
Iteration 72/1000 | Loss: 0.00006978
Iteration 73/1000 | Loss: 0.00002772
Iteration 74/1000 | Loss: 0.00004780
Iteration 75/1000 | Loss: 0.00004333
Iteration 76/1000 | Loss: 0.00007796
Iteration 77/1000 | Loss: 0.00049430
Iteration 78/1000 | Loss: 0.00005474
Iteration 79/1000 | Loss: 0.00004871
Iteration 80/1000 | Loss: 0.00005039
Iteration 81/1000 | Loss: 0.00002893
Iteration 82/1000 | Loss: 0.00003068
Iteration 83/1000 | Loss: 0.00003068
Iteration 84/1000 | Loss: 0.00003677
Iteration 85/1000 | Loss: 0.00006542
Iteration 86/1000 | Loss: 0.00002919
Iteration 87/1000 | Loss: 0.00004506
Iteration 88/1000 | Loss: 0.00002654
Iteration 89/1000 | Loss: 0.00002339
Iteration 90/1000 | Loss: 0.00004455
Iteration 91/1000 | Loss: 0.00003023
Iteration 92/1000 | Loss: 0.00006022
Iteration 93/1000 | Loss: 0.00002745
Iteration 94/1000 | Loss: 0.00003452
Iteration 95/1000 | Loss: 0.00002348
Iteration 96/1000 | Loss: 0.00002365
Iteration 97/1000 | Loss: 0.00002587
Iteration 98/1000 | Loss: 0.00002587
Iteration 99/1000 | Loss: 0.00002587
Iteration 100/1000 | Loss: 0.00002586
Iteration 101/1000 | Loss: 0.00010160
Iteration 102/1000 | Loss: 0.00006091
Iteration 103/1000 | Loss: 0.00002400
Iteration 104/1000 | Loss: 0.00003458
Iteration 105/1000 | Loss: 0.00002424
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002709
Iteration 108/1000 | Loss: 0.00002283
Iteration 109/1000 | Loss: 0.00002264
Iteration 110/1000 | Loss: 0.00002263
Iteration 111/1000 | Loss: 0.00002262
Iteration 112/1000 | Loss: 0.00002262
Iteration 113/1000 | Loss: 0.00002564
Iteration 114/1000 | Loss: 0.00003695
Iteration 115/1000 | Loss: 0.00003695
Iteration 116/1000 | Loss: 0.00012261
Iteration 117/1000 | Loss: 0.00002657
Iteration 118/1000 | Loss: 0.00003524
Iteration 119/1000 | Loss: 0.00005412
Iteration 120/1000 | Loss: 0.00009125
Iteration 121/1000 | Loss: 0.00003465
Iteration 122/1000 | Loss: 0.00002403
Iteration 123/1000 | Loss: 0.00006173
Iteration 124/1000 | Loss: 0.00013430
Iteration 125/1000 | Loss: 0.00003603
Iteration 126/1000 | Loss: 0.00011668
Iteration 127/1000 | Loss: 0.00005406
Iteration 128/1000 | Loss: 0.00005001
Iteration 129/1000 | Loss: 0.00003392
Iteration 130/1000 | Loss: 0.00002799
Iteration 131/1000 | Loss: 0.00003650
Iteration 132/1000 | Loss: 0.00002688
Iteration 133/1000 | Loss: 0.00002277
Iteration 134/1000 | Loss: 0.00004212
Iteration 135/1000 | Loss: 0.00002304
Iteration 136/1000 | Loss: 0.00002290
Iteration 137/1000 | Loss: 0.00002655
Iteration 138/1000 | Loss: 0.00002236
Iteration 139/1000 | Loss: 0.00002236
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00003508
Iteration 142/1000 | Loss: 0.00002715
Iteration 143/1000 | Loss: 0.00002367
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002463
Iteration 148/1000 | Loss: 0.00003112
Iteration 149/1000 | Loss: 0.00004897
Iteration 150/1000 | Loss: 0.00002298
Iteration 151/1000 | Loss: 0.00002335
Iteration 152/1000 | Loss: 0.00002227
Iteration 153/1000 | Loss: 0.00002225
Iteration 154/1000 | Loss: 0.00002224
Iteration 155/1000 | Loss: 0.00002223
Iteration 156/1000 | Loss: 0.00002223
Iteration 157/1000 | Loss: 0.00002223
Iteration 158/1000 | Loss: 0.00002223
Iteration 159/1000 | Loss: 0.00002331
Iteration 160/1000 | Loss: 0.00002247
Iteration 161/1000 | Loss: 0.00002224
Iteration 162/1000 | Loss: 0.00002224
Iteration 163/1000 | Loss: 0.00002224
Iteration 164/1000 | Loss: 0.00002223
Iteration 165/1000 | Loss: 0.00002223
Iteration 166/1000 | Loss: 0.00002223
Iteration 167/1000 | Loss: 0.00002448
Iteration 168/1000 | Loss: 0.00002513
Iteration 169/1000 | Loss: 0.00002317
Iteration 170/1000 | Loss: 0.00002223
Iteration 171/1000 | Loss: 0.00002222
Iteration 172/1000 | Loss: 0.00002221
Iteration 173/1000 | Loss: 0.00002221
Iteration 174/1000 | Loss: 0.00002865
Iteration 175/1000 | Loss: 0.00002406
Iteration 176/1000 | Loss: 0.00002222
Iteration 177/1000 | Loss: 0.00002222
Iteration 178/1000 | Loss: 0.00002222
Iteration 179/1000 | Loss: 0.00002222
Iteration 180/1000 | Loss: 0.00002222
Iteration 181/1000 | Loss: 0.00002222
Iteration 182/1000 | Loss: 0.00002222
Iteration 183/1000 | Loss: 0.00002222
Iteration 184/1000 | Loss: 0.00002221
Iteration 185/1000 | Loss: 0.00002221
Iteration 186/1000 | Loss: 0.00002240
Iteration 187/1000 | Loss: 0.00002231
Iteration 188/1000 | Loss: 0.00002526
Iteration 189/1000 | Loss: 0.00002375
Iteration 190/1000 | Loss: 0.00002513
Iteration 191/1000 | Loss: 0.00002241
Iteration 192/1000 | Loss: 0.00002524
Iteration 193/1000 | Loss: 0.00002895
Iteration 194/1000 | Loss: 0.00002228
Iteration 195/1000 | Loss: 0.00002228
Iteration 196/1000 | Loss: 0.00002378
Iteration 197/1000 | Loss: 0.00002223
Iteration 198/1000 | Loss: 0.00002219
Iteration 199/1000 | Loss: 0.00002224
Iteration 200/1000 | Loss: 0.00002243
Iteration 201/1000 | Loss: 0.00002220
Iteration 202/1000 | Loss: 0.00002227
Iteration 203/1000 | Loss: 0.00002226
Iteration 204/1000 | Loss: 0.00002226
Iteration 205/1000 | Loss: 0.00002226
Iteration 206/1000 | Loss: 0.00002307
Iteration 207/1000 | Loss: 0.00002305
Iteration 208/1000 | Loss: 0.00002469
Iteration 209/1000 | Loss: 0.00003052
Iteration 210/1000 | Loss: 0.00002546
Iteration 211/1000 | Loss: 0.00002266
Iteration 212/1000 | Loss: 0.00002512
Iteration 213/1000 | Loss: 0.00002475
Iteration 214/1000 | Loss: 0.00006044
Iteration 215/1000 | Loss: 0.00009276
Iteration 216/1000 | Loss: 0.00002449
Iteration 217/1000 | Loss: 0.00002237
Iteration 218/1000 | Loss: 0.00002371
Iteration 219/1000 | Loss: 0.00004553
Iteration 220/1000 | Loss: 0.00002270
Iteration 221/1000 | Loss: 0.00002323
Iteration 222/1000 | Loss: 0.00002228
Iteration 223/1000 | Loss: 0.00002228
Iteration 224/1000 | Loss: 0.00002222
Iteration 225/1000 | Loss: 0.00002216
Iteration 226/1000 | Loss: 0.00002216
Iteration 227/1000 | Loss: 0.00002216
Iteration 228/1000 | Loss: 0.00002216
Iteration 229/1000 | Loss: 0.00002216
Iteration 230/1000 | Loss: 0.00002216
Iteration 231/1000 | Loss: 0.00002216
Iteration 232/1000 | Loss: 0.00002216
Iteration 233/1000 | Loss: 0.00002216
Iteration 234/1000 | Loss: 0.00002216
Iteration 235/1000 | Loss: 0.00002216
Iteration 236/1000 | Loss: 0.00002216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [2.2159423679113388e-05, 2.2159423679113388e-05, 2.2159423679113388e-05, 2.2159423679113388e-05, 2.2159423679113388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2159423679113388e-05

Optimization complete. Final v2v error: 3.6449670791625977 mm

Highest mean error: 12.164864540100098 mm for frame 83

Lowest mean error: 3.0824944972991943 mm for frame 48

Saving results

Total time: 305.4128911495209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786625
Iteration 2/25 | Loss: 0.00151629
Iteration 3/25 | Loss: 0.00137094
Iteration 4/25 | Loss: 0.00133242
Iteration 5/25 | Loss: 0.00134061
Iteration 6/25 | Loss: 0.00132164
Iteration 7/25 | Loss: 0.00131185
Iteration 8/25 | Loss: 0.00130940
Iteration 9/25 | Loss: 0.00130821
Iteration 10/25 | Loss: 0.00130765
Iteration 11/25 | Loss: 0.00130747
Iteration 12/25 | Loss: 0.00130743
Iteration 13/25 | Loss: 0.00130743
Iteration 14/25 | Loss: 0.00130743
Iteration 15/25 | Loss: 0.00130743
Iteration 16/25 | Loss: 0.00130742
Iteration 17/25 | Loss: 0.00130742
Iteration 18/25 | Loss: 0.00130742
Iteration 19/25 | Loss: 0.00130742
Iteration 20/25 | Loss: 0.00130742
Iteration 21/25 | Loss: 0.00130742
Iteration 22/25 | Loss: 0.00130742
Iteration 23/25 | Loss: 0.00130742
Iteration 24/25 | Loss: 0.00130742
Iteration 25/25 | Loss: 0.00130742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.59096718
Iteration 2/25 | Loss: 0.00086918
Iteration 3/25 | Loss: 0.00086917
Iteration 4/25 | Loss: 0.00086917
Iteration 5/25 | Loss: 0.00086917
Iteration 6/25 | Loss: 0.00086917
Iteration 7/25 | Loss: 0.00086917
Iteration 8/25 | Loss: 0.00086917
Iteration 9/25 | Loss: 0.00086917
Iteration 10/25 | Loss: 0.00086917
Iteration 11/25 | Loss: 0.00086917
Iteration 12/25 | Loss: 0.00086917
Iteration 13/25 | Loss: 0.00086917
Iteration 14/25 | Loss: 0.00086917
Iteration 15/25 | Loss: 0.00086917
Iteration 16/25 | Loss: 0.00086917
Iteration 17/25 | Loss: 0.00086917
Iteration 18/25 | Loss: 0.00086917
Iteration 19/25 | Loss: 0.00086917
Iteration 20/25 | Loss: 0.00086917
Iteration 21/25 | Loss: 0.00086917
Iteration 22/25 | Loss: 0.00086917
Iteration 23/25 | Loss: 0.00086917
Iteration 24/25 | Loss: 0.00086917
Iteration 25/25 | Loss: 0.00086917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086917
Iteration 2/1000 | Loss: 0.00004352
Iteration 3/1000 | Loss: 0.00002663
Iteration 4/1000 | Loss: 0.00002281
Iteration 5/1000 | Loss: 0.00002166
Iteration 6/1000 | Loss: 0.00002113
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002003
Iteration 9/1000 | Loss: 0.00001973
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001925
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001906
Iteration 14/1000 | Loss: 0.00001905
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001881
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001879
Iteration 21/1000 | Loss: 0.00001877
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001872
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001871
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001868
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001865
Iteration 50/1000 | Loss: 0.00001865
Iteration 51/1000 | Loss: 0.00001865
Iteration 52/1000 | Loss: 0.00001864
Iteration 53/1000 | Loss: 0.00001864
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001864
Iteration 56/1000 | Loss: 0.00001864
Iteration 57/1000 | Loss: 0.00001864
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001862
Iteration 68/1000 | Loss: 0.00001862
Iteration 69/1000 | Loss: 0.00001862
Iteration 70/1000 | Loss: 0.00001862
Iteration 71/1000 | Loss: 0.00001861
Iteration 72/1000 | Loss: 0.00001861
Iteration 73/1000 | Loss: 0.00001861
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001860
Iteration 81/1000 | Loss: 0.00001860
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001859
Iteration 92/1000 | Loss: 0.00001858
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001857
Iteration 100/1000 | Loss: 0.00001857
Iteration 101/1000 | Loss: 0.00001857
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001856
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001855
Iteration 110/1000 | Loss: 0.00001855
Iteration 111/1000 | Loss: 0.00001855
Iteration 112/1000 | Loss: 0.00001855
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001854
Iteration 117/1000 | Loss: 0.00001854
Iteration 118/1000 | Loss: 0.00001854
Iteration 119/1000 | Loss: 0.00001854
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001852
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001851
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001851
Iteration 148/1000 | Loss: 0.00001851
Iteration 149/1000 | Loss: 0.00001851
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00001851
Iteration 152/1000 | Loss: 0.00001851
Iteration 153/1000 | Loss: 0.00001851
Iteration 154/1000 | Loss: 0.00001851
Iteration 155/1000 | Loss: 0.00001851
Iteration 156/1000 | Loss: 0.00001851
Iteration 157/1000 | Loss: 0.00001851
Iteration 158/1000 | Loss: 0.00001851
Iteration 159/1000 | Loss: 0.00001851
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001851
Iteration 173/1000 | Loss: 0.00001851
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001851
Iteration 176/1000 | Loss: 0.00001851
Iteration 177/1000 | Loss: 0.00001851
Iteration 178/1000 | Loss: 0.00001851
Iteration 179/1000 | Loss: 0.00001851
Iteration 180/1000 | Loss: 0.00001851
Iteration 181/1000 | Loss: 0.00001851
Iteration 182/1000 | Loss: 0.00001851
Iteration 183/1000 | Loss: 0.00001851
Iteration 184/1000 | Loss: 0.00001851
Iteration 185/1000 | Loss: 0.00001851
Iteration 186/1000 | Loss: 0.00001851
Iteration 187/1000 | Loss: 0.00001851
Iteration 188/1000 | Loss: 0.00001851
Iteration 189/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.8505450498196296e-05, 1.8505450498196296e-05, 1.8505450498196296e-05, 1.8505450498196296e-05, 1.8505450498196296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8505450498196296e-05

Optimization complete. Final v2v error: 3.680304527282715 mm

Highest mean error: 4.26313591003418 mm for frame 68

Lowest mean error: 3.2785086631774902 mm for frame 182

Saving results

Total time: 58.79592299461365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796508
Iteration 2/25 | Loss: 0.00145556
Iteration 3/25 | Loss: 0.00128409
Iteration 4/25 | Loss: 0.00125188
Iteration 5/25 | Loss: 0.00124219
Iteration 6/25 | Loss: 0.00123945
Iteration 7/25 | Loss: 0.00123879
Iteration 8/25 | Loss: 0.00123879
Iteration 9/25 | Loss: 0.00123879
Iteration 10/25 | Loss: 0.00123879
Iteration 11/25 | Loss: 0.00123879
Iteration 12/25 | Loss: 0.00123879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012387861497700214, 0.0012387861497700214, 0.0012387861497700214, 0.0012387861497700214, 0.0012387861497700214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012387861497700214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.46148992
Iteration 2/25 | Loss: 0.00132419
Iteration 3/25 | Loss: 0.00132417
Iteration 4/25 | Loss: 0.00132417
Iteration 5/25 | Loss: 0.00132417
Iteration 6/25 | Loss: 0.00132417
Iteration 7/25 | Loss: 0.00132417
Iteration 8/25 | Loss: 0.00132417
Iteration 9/25 | Loss: 0.00132417
Iteration 10/25 | Loss: 0.00132417
Iteration 11/25 | Loss: 0.00132417
Iteration 12/25 | Loss: 0.00132417
Iteration 13/25 | Loss: 0.00132417
Iteration 14/25 | Loss: 0.00132417
Iteration 15/25 | Loss: 0.00132417
Iteration 16/25 | Loss: 0.00132417
Iteration 17/25 | Loss: 0.00132417
Iteration 18/25 | Loss: 0.00132417
Iteration 19/25 | Loss: 0.00132417
Iteration 20/25 | Loss: 0.00132417
Iteration 21/25 | Loss: 0.00132417
Iteration 22/25 | Loss: 0.00132417
Iteration 23/25 | Loss: 0.00132417
Iteration 24/25 | Loss: 0.00132417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013241686392575502, 0.0013241686392575502, 0.0013241686392575502, 0.0013241686392575502, 0.0013241686392575502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013241686392575502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132417
Iteration 2/1000 | Loss: 0.00004656
Iteration 3/1000 | Loss: 0.00002883
Iteration 4/1000 | Loss: 0.00002331
Iteration 5/1000 | Loss: 0.00002127
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001945
Iteration 8/1000 | Loss: 0.00001893
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001811
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001789
Iteration 14/1000 | Loss: 0.00001784
Iteration 15/1000 | Loss: 0.00001782
Iteration 16/1000 | Loss: 0.00001779
Iteration 17/1000 | Loss: 0.00001763
Iteration 18/1000 | Loss: 0.00001756
Iteration 19/1000 | Loss: 0.00001754
Iteration 20/1000 | Loss: 0.00001753
Iteration 21/1000 | Loss: 0.00001753
Iteration 22/1000 | Loss: 0.00001752
Iteration 23/1000 | Loss: 0.00001746
Iteration 24/1000 | Loss: 0.00001745
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001726
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001723
Iteration 34/1000 | Loss: 0.00001722
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001712
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001712
Iteration 83/1000 | Loss: 0.00001712
Iteration 84/1000 | Loss: 0.00001712
Iteration 85/1000 | Loss: 0.00001712
Iteration 86/1000 | Loss: 0.00001712
Iteration 87/1000 | Loss: 0.00001712
Iteration 88/1000 | Loss: 0.00001712
Iteration 89/1000 | Loss: 0.00001712
Iteration 90/1000 | Loss: 0.00001712
Iteration 91/1000 | Loss: 0.00001712
Iteration 92/1000 | Loss: 0.00001712
Iteration 93/1000 | Loss: 0.00001712
Iteration 94/1000 | Loss: 0.00001712
Iteration 95/1000 | Loss: 0.00001712
Iteration 96/1000 | Loss: 0.00001712
Iteration 97/1000 | Loss: 0.00001712
Iteration 98/1000 | Loss: 0.00001712
Iteration 99/1000 | Loss: 0.00001712
Iteration 100/1000 | Loss: 0.00001712
Iteration 101/1000 | Loss: 0.00001712
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001712
Iteration 107/1000 | Loss: 0.00001712
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001712
Iteration 112/1000 | Loss: 0.00001712
Iteration 113/1000 | Loss: 0.00001712
Iteration 114/1000 | Loss: 0.00001712
Iteration 115/1000 | Loss: 0.00001712
Iteration 116/1000 | Loss: 0.00001712
Iteration 117/1000 | Loss: 0.00001712
Iteration 118/1000 | Loss: 0.00001712
Iteration 119/1000 | Loss: 0.00001712
Iteration 120/1000 | Loss: 0.00001712
Iteration 121/1000 | Loss: 0.00001712
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.711588993202895e-05, 1.711588993202895e-05, 1.711588993202895e-05, 1.711588993202895e-05, 1.711588993202895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.711588993202895e-05

Optimization complete. Final v2v error: 3.5195934772491455 mm

Highest mean error: 4.315838813781738 mm for frame 11

Lowest mean error: 2.792135000228882 mm for frame 148

Saving results

Total time: 37.38541889190674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502650
Iteration 2/25 | Loss: 0.00147868
Iteration 3/25 | Loss: 0.00127294
Iteration 4/25 | Loss: 0.00124732
Iteration 5/25 | Loss: 0.00124412
Iteration 6/25 | Loss: 0.00124364
Iteration 7/25 | Loss: 0.00124364
Iteration 8/25 | Loss: 0.00124364
Iteration 9/25 | Loss: 0.00124364
Iteration 10/25 | Loss: 0.00124364
Iteration 11/25 | Loss: 0.00124364
Iteration 12/25 | Loss: 0.00124364
Iteration 13/25 | Loss: 0.00124364
Iteration 14/25 | Loss: 0.00124364
Iteration 15/25 | Loss: 0.00124364
Iteration 16/25 | Loss: 0.00124364
Iteration 17/25 | Loss: 0.00124364
Iteration 18/25 | Loss: 0.00124364
Iteration 19/25 | Loss: 0.00124364
Iteration 20/25 | Loss: 0.00124364
Iteration 21/25 | Loss: 0.00124364
Iteration 22/25 | Loss: 0.00124364
Iteration 23/25 | Loss: 0.00124364
Iteration 24/25 | Loss: 0.00124364
Iteration 25/25 | Loss: 0.00124364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36700141
Iteration 2/25 | Loss: 0.00101165
Iteration 3/25 | Loss: 0.00101165
Iteration 4/25 | Loss: 0.00101165
Iteration 5/25 | Loss: 0.00101165
Iteration 6/25 | Loss: 0.00101165
Iteration 7/25 | Loss: 0.00101165
Iteration 8/25 | Loss: 0.00101165
Iteration 9/25 | Loss: 0.00101165
Iteration 10/25 | Loss: 0.00101165
Iteration 11/25 | Loss: 0.00101165
Iteration 12/25 | Loss: 0.00101165
Iteration 13/25 | Loss: 0.00101165
Iteration 14/25 | Loss: 0.00101165
Iteration 15/25 | Loss: 0.00101165
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010116506600752473, 0.0010116506600752473, 0.0010116506600752473, 0.0010116506600752473, 0.0010116506600752473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010116506600752473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101165
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002093
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001626
Iteration 6/1000 | Loss: 0.00001541
Iteration 7/1000 | Loss: 0.00001487
Iteration 8/1000 | Loss: 0.00001455
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001386
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001365
Iteration 19/1000 | Loss: 0.00001362
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001361
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001360
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001355
Iteration 36/1000 | Loss: 0.00001355
Iteration 37/1000 | Loss: 0.00001355
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001353
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001343
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001342
Iteration 53/1000 | Loss: 0.00001342
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001340
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001339
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001338
Iteration 64/1000 | Loss: 0.00001338
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001337
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001336
Iteration 69/1000 | Loss: 0.00001336
Iteration 70/1000 | Loss: 0.00001336
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001327
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001326
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001324
Iteration 107/1000 | Loss: 0.00001324
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001323
Iteration 111/1000 | Loss: 0.00001323
Iteration 112/1000 | Loss: 0.00001323
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001320
Iteration 128/1000 | Loss: 0.00001320
Iteration 129/1000 | Loss: 0.00001320
Iteration 130/1000 | Loss: 0.00001320
Iteration 131/1000 | Loss: 0.00001320
Iteration 132/1000 | Loss: 0.00001320
Iteration 133/1000 | Loss: 0.00001320
Iteration 134/1000 | Loss: 0.00001320
Iteration 135/1000 | Loss: 0.00001320
Iteration 136/1000 | Loss: 0.00001320
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001320
Iteration 142/1000 | Loss: 0.00001320
Iteration 143/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.3197716725699138e-05, 1.3197716725699138e-05, 1.3197716725699138e-05, 1.3197716725699138e-05, 1.3197716725699138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3197716725699138e-05

Optimization complete. Final v2v error: 2.975289821624756 mm

Highest mean error: 4.624627113342285 mm for frame 82

Lowest mean error: 2.581926107406616 mm for frame 168

Saving results

Total time: 41.47352385520935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979901
Iteration 2/25 | Loss: 0.00979901
Iteration 3/25 | Loss: 0.00979901
Iteration 4/25 | Loss: 0.00979901
Iteration 5/25 | Loss: 0.00979901
Iteration 6/25 | Loss: 0.00979900
Iteration 7/25 | Loss: 0.00979900
Iteration 8/25 | Loss: 0.00979900
Iteration 9/25 | Loss: 0.00979900
Iteration 10/25 | Loss: 0.00979900
Iteration 11/25 | Loss: 0.00979900
Iteration 12/25 | Loss: 0.00979900
Iteration 13/25 | Loss: 0.00979900
Iteration 14/25 | Loss: 0.00979900
Iteration 15/25 | Loss: 0.00979899
Iteration 16/25 | Loss: 0.00979899
Iteration 17/25 | Loss: 0.00979899
Iteration 18/25 | Loss: 0.00979899
Iteration 19/25 | Loss: 0.00979899
Iteration 20/25 | Loss: 0.00979899
Iteration 21/25 | Loss: 0.00979899
Iteration 22/25 | Loss: 0.00979899
Iteration 23/25 | Loss: 0.00979898
Iteration 24/25 | Loss: 0.00979898
Iteration 25/25 | Loss: 0.00979898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43466496
Iteration 2/25 | Loss: 0.18617456
Iteration 3/25 | Loss: 0.18616927
Iteration 4/25 | Loss: 0.18616927
Iteration 5/25 | Loss: 0.18616922
Iteration 6/25 | Loss: 0.18616922
Iteration 7/25 | Loss: 0.18616921
Iteration 8/25 | Loss: 0.18616919
Iteration 9/25 | Loss: 0.18616919
Iteration 10/25 | Loss: 0.18616919
Iteration 11/25 | Loss: 0.18616919
Iteration 12/25 | Loss: 0.18616919
Iteration 13/25 | Loss: 0.18616919
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.18616919219493866, 0.18616919219493866, 0.18616919219493866, 0.18616919219493866, 0.18616919219493866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18616919219493866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18616919
Iteration 2/1000 | Loss: 0.00595236
Iteration 3/1000 | Loss: 0.00235791
Iteration 4/1000 | Loss: 0.00084952
Iteration 5/1000 | Loss: 0.00081961
Iteration 6/1000 | Loss: 0.00305418
Iteration 7/1000 | Loss: 0.00086134
Iteration 8/1000 | Loss: 0.00033843
Iteration 9/1000 | Loss: 0.00034746
Iteration 10/1000 | Loss: 0.00020928
Iteration 11/1000 | Loss: 0.00012426
Iteration 12/1000 | Loss: 0.00015057
Iteration 13/1000 | Loss: 0.00039561
Iteration 14/1000 | Loss: 0.00005000
Iteration 15/1000 | Loss: 0.00028555
Iteration 16/1000 | Loss: 0.00004744
Iteration 17/1000 | Loss: 0.00017652
Iteration 18/1000 | Loss: 0.00014263
Iteration 19/1000 | Loss: 0.00042718
Iteration 20/1000 | Loss: 0.00043638
Iteration 21/1000 | Loss: 0.00007926
Iteration 22/1000 | Loss: 0.00005779
Iteration 23/1000 | Loss: 0.00012775
Iteration 24/1000 | Loss: 0.00008481
Iteration 25/1000 | Loss: 0.00008910
Iteration 26/1000 | Loss: 0.00007670
Iteration 27/1000 | Loss: 0.00009241
Iteration 28/1000 | Loss: 0.00002723
Iteration 29/1000 | Loss: 0.00003790
Iteration 30/1000 | Loss: 0.00005267
Iteration 31/1000 | Loss: 0.00004846
Iteration 32/1000 | Loss: 0.00002403
Iteration 33/1000 | Loss: 0.00010732
Iteration 34/1000 | Loss: 0.00002319
Iteration 35/1000 | Loss: 0.00009196
Iteration 36/1000 | Loss: 0.00002251
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00009922
Iteration 39/1000 | Loss: 0.00002238
Iteration 40/1000 | Loss: 0.00002138
Iteration 41/1000 | Loss: 0.00007186
Iteration 42/1000 | Loss: 0.00057457
Iteration 43/1000 | Loss: 0.00002263
Iteration 44/1000 | Loss: 0.00009072
Iteration 45/1000 | Loss: 0.00003352
Iteration 46/1000 | Loss: 0.00003277
Iteration 47/1000 | Loss: 0.00002091
Iteration 48/1000 | Loss: 0.00002090
Iteration 49/1000 | Loss: 0.00002090
Iteration 50/1000 | Loss: 0.00002090
Iteration 51/1000 | Loss: 0.00002089
Iteration 52/1000 | Loss: 0.00002089
Iteration 53/1000 | Loss: 0.00002089
Iteration 54/1000 | Loss: 0.00002089
Iteration 55/1000 | Loss: 0.00002089
Iteration 56/1000 | Loss: 0.00002089
Iteration 57/1000 | Loss: 0.00002089
Iteration 58/1000 | Loss: 0.00002089
Iteration 59/1000 | Loss: 0.00002088
Iteration 60/1000 | Loss: 0.00002079
Iteration 61/1000 | Loss: 0.00004238
Iteration 62/1000 | Loss: 0.00021209
Iteration 63/1000 | Loss: 0.00003394
Iteration 64/1000 | Loss: 0.00010371
Iteration 65/1000 | Loss: 0.00002282
Iteration 66/1000 | Loss: 0.00003260
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002639
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002064
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002060
Iteration 74/1000 | Loss: 0.00002060
Iteration 75/1000 | Loss: 0.00002060
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002059
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002057
Iteration 87/1000 | Loss: 0.00002057
Iteration 88/1000 | Loss: 0.00002057
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002056
Iteration 92/1000 | Loss: 0.00002056
Iteration 93/1000 | Loss: 0.00002055
Iteration 94/1000 | Loss: 0.00002055
Iteration 95/1000 | Loss: 0.00002055
Iteration 96/1000 | Loss: 0.00002054
Iteration 97/1000 | Loss: 0.00002054
Iteration 98/1000 | Loss: 0.00002054
Iteration 99/1000 | Loss: 0.00002054
Iteration 100/1000 | Loss: 0.00002054
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002054
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002053
Iteration 106/1000 | Loss: 0.00002053
Iteration 107/1000 | Loss: 0.00002052
Iteration 108/1000 | Loss: 0.00002052
Iteration 109/1000 | Loss: 0.00002052
Iteration 110/1000 | Loss: 0.00002052
Iteration 111/1000 | Loss: 0.00002052
Iteration 112/1000 | Loss: 0.00002052
Iteration 113/1000 | Loss: 0.00002052
Iteration 114/1000 | Loss: 0.00002052
Iteration 115/1000 | Loss: 0.00002052
Iteration 116/1000 | Loss: 0.00002052
Iteration 117/1000 | Loss: 0.00002052
Iteration 118/1000 | Loss: 0.00002052
Iteration 119/1000 | Loss: 0.00002052
Iteration 120/1000 | Loss: 0.00002051
Iteration 121/1000 | Loss: 0.00002051
Iteration 122/1000 | Loss: 0.00002051
Iteration 123/1000 | Loss: 0.00002051
Iteration 124/1000 | Loss: 0.00002050
Iteration 125/1000 | Loss: 0.00002050
Iteration 126/1000 | Loss: 0.00002050
Iteration 127/1000 | Loss: 0.00002050
Iteration 128/1000 | Loss: 0.00002050
Iteration 129/1000 | Loss: 0.00002050
Iteration 130/1000 | Loss: 0.00002050
Iteration 131/1000 | Loss: 0.00002050
Iteration 132/1000 | Loss: 0.00002050
Iteration 133/1000 | Loss: 0.00002049
Iteration 134/1000 | Loss: 0.00002049
Iteration 135/1000 | Loss: 0.00002049
Iteration 136/1000 | Loss: 0.00002049
Iteration 137/1000 | Loss: 0.00002049
Iteration 138/1000 | Loss: 0.00002049
Iteration 139/1000 | Loss: 0.00002049
Iteration 140/1000 | Loss: 0.00002049
Iteration 141/1000 | Loss: 0.00002049
Iteration 142/1000 | Loss: 0.00002048
Iteration 143/1000 | Loss: 0.00002048
Iteration 144/1000 | Loss: 0.00002048
Iteration 145/1000 | Loss: 0.00002048
Iteration 146/1000 | Loss: 0.00002048
Iteration 147/1000 | Loss: 0.00002048
Iteration 148/1000 | Loss: 0.00002048
Iteration 149/1000 | Loss: 0.00002048
Iteration 150/1000 | Loss: 0.00002048
Iteration 151/1000 | Loss: 0.00002048
Iteration 152/1000 | Loss: 0.00002048
Iteration 153/1000 | Loss: 0.00002048
Iteration 154/1000 | Loss: 0.00002047
Iteration 155/1000 | Loss: 0.00002047
Iteration 156/1000 | Loss: 0.00002047
Iteration 157/1000 | Loss: 0.00002046
Iteration 158/1000 | Loss: 0.00002046
Iteration 159/1000 | Loss: 0.00002046
Iteration 160/1000 | Loss: 0.00002046
Iteration 161/1000 | Loss: 0.00002046
Iteration 162/1000 | Loss: 0.00002046
Iteration 163/1000 | Loss: 0.00002046
Iteration 164/1000 | Loss: 0.00002046
Iteration 165/1000 | Loss: 0.00002046
Iteration 166/1000 | Loss: 0.00002046
Iteration 167/1000 | Loss: 0.00002045
Iteration 168/1000 | Loss: 0.00002045
Iteration 169/1000 | Loss: 0.00002045
Iteration 170/1000 | Loss: 0.00002045
Iteration 171/1000 | Loss: 0.00002045
Iteration 172/1000 | Loss: 0.00002045
Iteration 173/1000 | Loss: 0.00002045
Iteration 174/1000 | Loss: 0.00002045
Iteration 175/1000 | Loss: 0.00002045
Iteration 176/1000 | Loss: 0.00002045
Iteration 177/1000 | Loss: 0.00002045
Iteration 178/1000 | Loss: 0.00002045
Iteration 179/1000 | Loss: 0.00002045
Iteration 180/1000 | Loss: 0.00002045
Iteration 181/1000 | Loss: 0.00002044
Iteration 182/1000 | Loss: 0.00002044
Iteration 183/1000 | Loss: 0.00002044
Iteration 184/1000 | Loss: 0.00002044
Iteration 185/1000 | Loss: 0.00002044
Iteration 186/1000 | Loss: 0.00002044
Iteration 187/1000 | Loss: 0.00002044
Iteration 188/1000 | Loss: 0.00002044
Iteration 189/1000 | Loss: 0.00002044
Iteration 190/1000 | Loss: 0.00002044
Iteration 191/1000 | Loss: 0.00004181
Iteration 192/1000 | Loss: 0.00004181
Iteration 193/1000 | Loss: 0.00007768
Iteration 194/1000 | Loss: 0.00002083
Iteration 195/1000 | Loss: 0.00002052
Iteration 196/1000 | Loss: 0.00006013
Iteration 197/1000 | Loss: 0.00002323
Iteration 198/1000 | Loss: 0.00003106
Iteration 199/1000 | Loss: 0.00002046
Iteration 200/1000 | Loss: 0.00002044
Iteration 201/1000 | Loss: 0.00002044
Iteration 202/1000 | Loss: 0.00002043
Iteration 203/1000 | Loss: 0.00002043
Iteration 204/1000 | Loss: 0.00002043
Iteration 205/1000 | Loss: 0.00002043
Iteration 206/1000 | Loss: 0.00002043
Iteration 207/1000 | Loss: 0.00002043
Iteration 208/1000 | Loss: 0.00002043
Iteration 209/1000 | Loss: 0.00002043
Iteration 210/1000 | Loss: 0.00002043
Iteration 211/1000 | Loss: 0.00002043
Iteration 212/1000 | Loss: 0.00002043
Iteration 213/1000 | Loss: 0.00002043
Iteration 214/1000 | Loss: 0.00002042
Iteration 215/1000 | Loss: 0.00002042
Iteration 216/1000 | Loss: 0.00002042
Iteration 217/1000 | Loss: 0.00002042
Iteration 218/1000 | Loss: 0.00002041
Iteration 219/1000 | Loss: 0.00002041
Iteration 220/1000 | Loss: 0.00002041
Iteration 221/1000 | Loss: 0.00002041
Iteration 222/1000 | Loss: 0.00002041
Iteration 223/1000 | Loss: 0.00002041
Iteration 224/1000 | Loss: 0.00002041
Iteration 225/1000 | Loss: 0.00002041
Iteration 226/1000 | Loss: 0.00002041
Iteration 227/1000 | Loss: 0.00002041
Iteration 228/1000 | Loss: 0.00002041
Iteration 229/1000 | Loss: 0.00002041
Iteration 230/1000 | Loss: 0.00002041
Iteration 231/1000 | Loss: 0.00002041
Iteration 232/1000 | Loss: 0.00002041
Iteration 233/1000 | Loss: 0.00002040
Iteration 234/1000 | Loss: 0.00002040
Iteration 235/1000 | Loss: 0.00002040
Iteration 236/1000 | Loss: 0.00002040
Iteration 237/1000 | Loss: 0.00002040
Iteration 238/1000 | Loss: 0.00002427
Iteration 239/1000 | Loss: 0.00002042
Iteration 240/1000 | Loss: 0.00002084
Iteration 241/1000 | Loss: 0.00002041
Iteration 242/1000 | Loss: 0.00002041
Iteration 243/1000 | Loss: 0.00002041
Iteration 244/1000 | Loss: 0.00002041
Iteration 245/1000 | Loss: 0.00002041
Iteration 246/1000 | Loss: 0.00002041
Iteration 247/1000 | Loss: 0.00002041
Iteration 248/1000 | Loss: 0.00002041
Iteration 249/1000 | Loss: 0.00002041
Iteration 250/1000 | Loss: 0.00002041
Iteration 251/1000 | Loss: 0.00002041
Iteration 252/1000 | Loss: 0.00002041
Iteration 253/1000 | Loss: 0.00002041
Iteration 254/1000 | Loss: 0.00002041
Iteration 255/1000 | Loss: 0.00002041
Iteration 256/1000 | Loss: 0.00002041
Iteration 257/1000 | Loss: 0.00002041
Iteration 258/1000 | Loss: 0.00002041
Iteration 259/1000 | Loss: 0.00002041
Iteration 260/1000 | Loss: 0.00002041
Iteration 261/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.040832805505488e-05, 2.040832805505488e-05, 2.040832805505488e-05, 2.040832805505488e-05, 2.040832805505488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.040832805505488e-05

Optimization complete. Final v2v error: 3.86102557182312 mm

Highest mean error: 4.320735931396484 mm for frame 29

Lowest mean error: 3.581951856613159 mm for frame 139

Saving results

Total time: 119.88126564025879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00605960
Iteration 2/25 | Loss: 0.00166339
Iteration 3/25 | Loss: 0.00150661
Iteration 4/25 | Loss: 0.00149904
Iteration 5/25 | Loss: 0.00144300
Iteration 6/25 | Loss: 0.00140429
Iteration 7/25 | Loss: 0.00138442
Iteration 8/25 | Loss: 0.00137667
Iteration 9/25 | Loss: 0.00136903
Iteration 10/25 | Loss: 0.00136294
Iteration 11/25 | Loss: 0.00135918
Iteration 12/25 | Loss: 0.00135833
Iteration 13/25 | Loss: 0.00135973
Iteration 14/25 | Loss: 0.00135835
Iteration 15/25 | Loss: 0.00135819
Iteration 16/25 | Loss: 0.00135284
Iteration 17/25 | Loss: 0.00134982
Iteration 18/25 | Loss: 0.00134922
Iteration 19/25 | Loss: 0.00134889
Iteration 20/25 | Loss: 0.00134867
Iteration 21/25 | Loss: 0.00134860
Iteration 22/25 | Loss: 0.00134939
Iteration 23/25 | Loss: 0.00134629
Iteration 24/25 | Loss: 0.00134591
Iteration 25/25 | Loss: 0.00134578

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25132668
Iteration 2/25 | Loss: 0.00152771
Iteration 3/25 | Loss: 0.00152763
Iteration 4/25 | Loss: 0.00152763
Iteration 5/25 | Loss: 0.00152763
Iteration 6/25 | Loss: 0.00152763
Iteration 7/25 | Loss: 0.00152762
Iteration 8/25 | Loss: 0.00152762
Iteration 9/25 | Loss: 0.00152762
Iteration 10/25 | Loss: 0.00152762
Iteration 11/25 | Loss: 0.00152762
Iteration 12/25 | Loss: 0.00152762
Iteration 13/25 | Loss: 0.00152762
Iteration 14/25 | Loss: 0.00152762
Iteration 15/25 | Loss: 0.00152762
Iteration 16/25 | Loss: 0.00152762
Iteration 17/25 | Loss: 0.00152762
Iteration 18/25 | Loss: 0.00152762
Iteration 19/25 | Loss: 0.00152762
Iteration 20/25 | Loss: 0.00152762
Iteration 21/25 | Loss: 0.00152762
Iteration 22/25 | Loss: 0.00152762
Iteration 23/25 | Loss: 0.00152762
Iteration 24/25 | Loss: 0.00152762
Iteration 25/25 | Loss: 0.00152762

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152762
Iteration 2/1000 | Loss: 0.00011148
Iteration 3/1000 | Loss: 0.00005385
Iteration 4/1000 | Loss: 0.00004290
Iteration 5/1000 | Loss: 0.00003962
Iteration 6/1000 | Loss: 0.00003696
Iteration 7/1000 | Loss: 0.00003493
Iteration 8/1000 | Loss: 0.00003340
Iteration 9/1000 | Loss: 0.00003245
Iteration 10/1000 | Loss: 0.00003171
Iteration 11/1000 | Loss: 0.00003108
Iteration 12/1000 | Loss: 0.00003063
Iteration 13/1000 | Loss: 0.00003037
Iteration 14/1000 | Loss: 0.00003006
Iteration 15/1000 | Loss: 0.00002984
Iteration 16/1000 | Loss: 0.00002964
Iteration 17/1000 | Loss: 0.00002954
Iteration 18/1000 | Loss: 0.00002950
Iteration 19/1000 | Loss: 0.00002949
Iteration 20/1000 | Loss: 0.00002948
Iteration 21/1000 | Loss: 0.00002945
Iteration 22/1000 | Loss: 0.00002942
Iteration 23/1000 | Loss: 0.00002941
Iteration 24/1000 | Loss: 0.00002936
Iteration 25/1000 | Loss: 0.00002932
Iteration 26/1000 | Loss: 0.00002928
Iteration 27/1000 | Loss: 0.00002927
Iteration 28/1000 | Loss: 0.00002926
Iteration 29/1000 | Loss: 0.00002923
Iteration 30/1000 | Loss: 0.00002922
Iteration 31/1000 | Loss: 0.00002920
Iteration 32/1000 | Loss: 0.00002915
Iteration 33/1000 | Loss: 0.00002912
Iteration 34/1000 | Loss: 0.00002912
Iteration 35/1000 | Loss: 0.00002911
Iteration 36/1000 | Loss: 0.00002911
Iteration 37/1000 | Loss: 0.00002910
Iteration 38/1000 | Loss: 0.00002908
Iteration 39/1000 | Loss: 0.00002907
Iteration 40/1000 | Loss: 0.00002906
Iteration 41/1000 | Loss: 0.00002906
Iteration 42/1000 | Loss: 0.00002905
Iteration 43/1000 | Loss: 0.00002903
Iteration 44/1000 | Loss: 0.00002903
Iteration 45/1000 | Loss: 0.00002902
Iteration 46/1000 | Loss: 0.00002902
Iteration 47/1000 | Loss: 0.00002901
Iteration 48/1000 | Loss: 0.00002901
Iteration 49/1000 | Loss: 0.00002898
Iteration 50/1000 | Loss: 0.00002898
Iteration 51/1000 | Loss: 0.00002897
Iteration 52/1000 | Loss: 0.00002897
Iteration 53/1000 | Loss: 0.00002897
Iteration 54/1000 | Loss: 0.00002896
Iteration 55/1000 | Loss: 0.00002895
Iteration 56/1000 | Loss: 0.00002895
Iteration 57/1000 | Loss: 0.00002894
Iteration 58/1000 | Loss: 0.00002894
Iteration 59/1000 | Loss: 0.00002893
Iteration 60/1000 | Loss: 0.00002893
Iteration 61/1000 | Loss: 0.00002891
Iteration 62/1000 | Loss: 0.00002891
Iteration 63/1000 | Loss: 0.00002890
Iteration 64/1000 | Loss: 0.00002890
Iteration 65/1000 | Loss: 0.00002889
Iteration 66/1000 | Loss: 0.00002889
Iteration 67/1000 | Loss: 0.00002888
Iteration 68/1000 | Loss: 0.00002888
Iteration 69/1000 | Loss: 0.00002887
Iteration 70/1000 | Loss: 0.00002887
Iteration 71/1000 | Loss: 0.00002886
Iteration 72/1000 | Loss: 0.00002886
Iteration 73/1000 | Loss: 0.00002886
Iteration 74/1000 | Loss: 0.00002885
Iteration 75/1000 | Loss: 0.00002885
Iteration 76/1000 | Loss: 0.00002884
Iteration 77/1000 | Loss: 0.00002883
Iteration 78/1000 | Loss: 0.00002882
Iteration 79/1000 | Loss: 0.00002882
Iteration 80/1000 | Loss: 0.00002882
Iteration 81/1000 | Loss: 0.00002881
Iteration 82/1000 | Loss: 0.00002881
Iteration 83/1000 | Loss: 0.00002881
Iteration 84/1000 | Loss: 0.00002881
Iteration 85/1000 | Loss: 0.00002881
Iteration 86/1000 | Loss: 0.00002881
Iteration 87/1000 | Loss: 0.00002880
Iteration 88/1000 | Loss: 0.00002880
Iteration 89/1000 | Loss: 0.00002880
Iteration 90/1000 | Loss: 0.00002880
Iteration 91/1000 | Loss: 0.00002880
Iteration 92/1000 | Loss: 0.00002879
Iteration 93/1000 | Loss: 0.00002879
Iteration 94/1000 | Loss: 0.00002879
Iteration 95/1000 | Loss: 0.00002879
Iteration 96/1000 | Loss: 0.00002879
Iteration 97/1000 | Loss: 0.00002879
Iteration 98/1000 | Loss: 0.00002879
Iteration 99/1000 | Loss: 0.00002878
Iteration 100/1000 | Loss: 0.00002878
Iteration 101/1000 | Loss: 0.00002878
Iteration 102/1000 | Loss: 0.00002878
Iteration 103/1000 | Loss: 0.00002877
Iteration 104/1000 | Loss: 0.00002877
Iteration 105/1000 | Loss: 0.00002877
Iteration 106/1000 | Loss: 0.00002876
Iteration 107/1000 | Loss: 0.00002876
Iteration 108/1000 | Loss: 0.00002876
Iteration 109/1000 | Loss: 0.00002876
Iteration 110/1000 | Loss: 0.00002875
Iteration 111/1000 | Loss: 0.00002875
Iteration 112/1000 | Loss: 0.00002875
Iteration 113/1000 | Loss: 0.00002875
Iteration 114/1000 | Loss: 0.00002874
Iteration 115/1000 | Loss: 0.00002874
Iteration 116/1000 | Loss: 0.00002874
Iteration 117/1000 | Loss: 0.00002873
Iteration 118/1000 | Loss: 0.00002873
Iteration 119/1000 | Loss: 0.00002873
Iteration 120/1000 | Loss: 0.00002873
Iteration 121/1000 | Loss: 0.00002873
Iteration 122/1000 | Loss: 0.00002872
Iteration 123/1000 | Loss: 0.00002872
Iteration 124/1000 | Loss: 0.00002872
Iteration 125/1000 | Loss: 0.00002871
Iteration 126/1000 | Loss: 0.00002871
Iteration 127/1000 | Loss: 0.00002871
Iteration 128/1000 | Loss: 0.00002871
Iteration 129/1000 | Loss: 0.00002870
Iteration 130/1000 | Loss: 0.00002870
Iteration 131/1000 | Loss: 0.00002870
Iteration 132/1000 | Loss: 0.00002870
Iteration 133/1000 | Loss: 0.00002870
Iteration 134/1000 | Loss: 0.00002870
Iteration 135/1000 | Loss: 0.00002870
Iteration 136/1000 | Loss: 0.00002869
Iteration 137/1000 | Loss: 0.00002869
Iteration 138/1000 | Loss: 0.00002869
Iteration 139/1000 | Loss: 0.00002869
Iteration 140/1000 | Loss: 0.00002869
Iteration 141/1000 | Loss: 0.00002869
Iteration 142/1000 | Loss: 0.00002868
Iteration 143/1000 | Loss: 0.00002868
Iteration 144/1000 | Loss: 0.00002868
Iteration 145/1000 | Loss: 0.00002868
Iteration 146/1000 | Loss: 0.00002868
Iteration 147/1000 | Loss: 0.00002868
Iteration 148/1000 | Loss: 0.00002868
Iteration 149/1000 | Loss: 0.00002868
Iteration 150/1000 | Loss: 0.00002867
Iteration 151/1000 | Loss: 0.00002867
Iteration 152/1000 | Loss: 0.00002867
Iteration 153/1000 | Loss: 0.00002867
Iteration 154/1000 | Loss: 0.00002866
Iteration 155/1000 | Loss: 0.00002866
Iteration 156/1000 | Loss: 0.00002866
Iteration 157/1000 | Loss: 0.00002866
Iteration 158/1000 | Loss: 0.00002866
Iteration 159/1000 | Loss: 0.00002866
Iteration 160/1000 | Loss: 0.00002865
Iteration 161/1000 | Loss: 0.00002865
Iteration 162/1000 | Loss: 0.00002865
Iteration 163/1000 | Loss: 0.00002865
Iteration 164/1000 | Loss: 0.00002865
Iteration 165/1000 | Loss: 0.00002865
Iteration 166/1000 | Loss: 0.00002865
Iteration 167/1000 | Loss: 0.00002865
Iteration 168/1000 | Loss: 0.00002865
Iteration 169/1000 | Loss: 0.00002865
Iteration 170/1000 | Loss: 0.00002865
Iteration 171/1000 | Loss: 0.00002865
Iteration 172/1000 | Loss: 0.00002865
Iteration 173/1000 | Loss: 0.00002864
Iteration 174/1000 | Loss: 0.00002864
Iteration 175/1000 | Loss: 0.00002864
Iteration 176/1000 | Loss: 0.00002864
Iteration 177/1000 | Loss: 0.00002864
Iteration 178/1000 | Loss: 0.00002864
Iteration 179/1000 | Loss: 0.00002864
Iteration 180/1000 | Loss: 0.00002864
Iteration 181/1000 | Loss: 0.00002864
Iteration 182/1000 | Loss: 0.00002864
Iteration 183/1000 | Loss: 0.00002864
Iteration 184/1000 | Loss: 0.00002863
Iteration 185/1000 | Loss: 0.00002863
Iteration 186/1000 | Loss: 0.00002863
Iteration 187/1000 | Loss: 0.00002863
Iteration 188/1000 | Loss: 0.00002863
Iteration 189/1000 | Loss: 0.00002863
Iteration 190/1000 | Loss: 0.00002863
Iteration 191/1000 | Loss: 0.00002863
Iteration 192/1000 | Loss: 0.00002863
Iteration 193/1000 | Loss: 0.00002863
Iteration 194/1000 | Loss: 0.00002863
Iteration 195/1000 | Loss: 0.00002863
Iteration 196/1000 | Loss: 0.00002862
Iteration 197/1000 | Loss: 0.00002862
Iteration 198/1000 | Loss: 0.00002862
Iteration 199/1000 | Loss: 0.00002862
Iteration 200/1000 | Loss: 0.00002862
Iteration 201/1000 | Loss: 0.00002862
Iteration 202/1000 | Loss: 0.00002862
Iteration 203/1000 | Loss: 0.00002862
Iteration 204/1000 | Loss: 0.00002862
Iteration 205/1000 | Loss: 0.00002862
Iteration 206/1000 | Loss: 0.00002862
Iteration 207/1000 | Loss: 0.00002862
Iteration 208/1000 | Loss: 0.00002861
Iteration 209/1000 | Loss: 0.00002861
Iteration 210/1000 | Loss: 0.00002861
Iteration 211/1000 | Loss: 0.00002861
Iteration 212/1000 | Loss: 0.00002861
Iteration 213/1000 | Loss: 0.00002861
Iteration 214/1000 | Loss: 0.00002861
Iteration 215/1000 | Loss: 0.00002861
Iteration 216/1000 | Loss: 0.00002861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.861304528778419e-05, 2.861304528778419e-05, 2.861304528778419e-05, 2.861304528778419e-05, 2.861304528778419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.861304528778419e-05

Optimization complete. Final v2v error: 3.9802896976470947 mm

Highest mean error: 11.163492202758789 mm for frame 137

Lowest mean error: 3.140348196029663 mm for frame 77

Saving results

Total time: 88.7283992767334
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959378
Iteration 2/25 | Loss: 0.00173695
Iteration 3/25 | Loss: 0.00147639
Iteration 4/25 | Loss: 0.00145690
Iteration 5/25 | Loss: 0.00145153
Iteration 6/25 | Loss: 0.00145101
Iteration 7/25 | Loss: 0.00145101
Iteration 8/25 | Loss: 0.00145101
Iteration 9/25 | Loss: 0.00145101
Iteration 10/25 | Loss: 0.00145101
Iteration 11/25 | Loss: 0.00145101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014510061591863632, 0.0014510061591863632, 0.0014510061591863632, 0.0014510061591863632, 0.0014510061591863632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014510061591863632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.52914941
Iteration 2/25 | Loss: 0.00114593
Iteration 3/25 | Loss: 0.00114593
Iteration 4/25 | Loss: 0.00114593
Iteration 5/25 | Loss: 0.00114593
Iteration 6/25 | Loss: 0.00114593
Iteration 7/25 | Loss: 0.00114592
Iteration 8/25 | Loss: 0.00114592
Iteration 9/25 | Loss: 0.00114592
Iteration 10/25 | Loss: 0.00114592
Iteration 11/25 | Loss: 0.00114592
Iteration 12/25 | Loss: 0.00114592
Iteration 13/25 | Loss: 0.00114592
Iteration 14/25 | Loss: 0.00114592
Iteration 15/25 | Loss: 0.00114592
Iteration 16/25 | Loss: 0.00114592
Iteration 17/25 | Loss: 0.00114592
Iteration 18/25 | Loss: 0.00114592
Iteration 19/25 | Loss: 0.00114592
Iteration 20/25 | Loss: 0.00114592
Iteration 21/25 | Loss: 0.00114592
Iteration 22/25 | Loss: 0.00114592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011459231609478593, 0.0011459231609478593, 0.0011459231609478593, 0.0011459231609478593, 0.0011459231609478593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011459231609478593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114592
Iteration 2/1000 | Loss: 0.00006223
Iteration 3/1000 | Loss: 0.00004404
Iteration 4/1000 | Loss: 0.00003982
Iteration 5/1000 | Loss: 0.00003841
Iteration 6/1000 | Loss: 0.00003763
Iteration 7/1000 | Loss: 0.00003668
Iteration 8/1000 | Loss: 0.00003618
Iteration 9/1000 | Loss: 0.00003579
Iteration 10/1000 | Loss: 0.00003544
Iteration 11/1000 | Loss: 0.00003513
Iteration 12/1000 | Loss: 0.00003483
Iteration 13/1000 | Loss: 0.00003453
Iteration 14/1000 | Loss: 0.00003425
Iteration 15/1000 | Loss: 0.00003398
Iteration 16/1000 | Loss: 0.00003373
Iteration 17/1000 | Loss: 0.00003347
Iteration 18/1000 | Loss: 0.00003333
Iteration 19/1000 | Loss: 0.00003322
Iteration 20/1000 | Loss: 0.00003321
Iteration 21/1000 | Loss: 0.00003312
Iteration 22/1000 | Loss: 0.00003307
Iteration 23/1000 | Loss: 0.00003304
Iteration 24/1000 | Loss: 0.00003304
Iteration 25/1000 | Loss: 0.00003302
Iteration 26/1000 | Loss: 0.00003301
Iteration 27/1000 | Loss: 0.00003301
Iteration 28/1000 | Loss: 0.00003299
Iteration 29/1000 | Loss: 0.00003299
Iteration 30/1000 | Loss: 0.00003298
Iteration 31/1000 | Loss: 0.00003298
Iteration 32/1000 | Loss: 0.00003298
Iteration 33/1000 | Loss: 0.00003298
Iteration 34/1000 | Loss: 0.00003297
Iteration 35/1000 | Loss: 0.00003296
Iteration 36/1000 | Loss: 0.00003296
Iteration 37/1000 | Loss: 0.00003296
Iteration 38/1000 | Loss: 0.00003296
Iteration 39/1000 | Loss: 0.00003295
Iteration 40/1000 | Loss: 0.00003295
Iteration 41/1000 | Loss: 0.00003295
Iteration 42/1000 | Loss: 0.00003295
Iteration 43/1000 | Loss: 0.00003295
Iteration 44/1000 | Loss: 0.00003295
Iteration 45/1000 | Loss: 0.00003295
Iteration 46/1000 | Loss: 0.00003295
Iteration 47/1000 | Loss: 0.00003295
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003294
Iteration 50/1000 | Loss: 0.00003294
Iteration 51/1000 | Loss: 0.00003294
Iteration 52/1000 | Loss: 0.00003294
Iteration 53/1000 | Loss: 0.00003294
Iteration 54/1000 | Loss: 0.00003294
Iteration 55/1000 | Loss: 0.00003294
Iteration 56/1000 | Loss: 0.00003293
Iteration 57/1000 | Loss: 0.00003292
Iteration 58/1000 | Loss: 0.00003292
Iteration 59/1000 | Loss: 0.00003292
Iteration 60/1000 | Loss: 0.00003292
Iteration 61/1000 | Loss: 0.00003292
Iteration 62/1000 | Loss: 0.00003292
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003292
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003292
Iteration 67/1000 | Loss: 0.00003292
Iteration 68/1000 | Loss: 0.00003292
Iteration 69/1000 | Loss: 0.00003291
Iteration 70/1000 | Loss: 0.00003291
Iteration 71/1000 | Loss: 0.00003291
Iteration 72/1000 | Loss: 0.00003291
Iteration 73/1000 | Loss: 0.00003291
Iteration 74/1000 | Loss: 0.00003290
Iteration 75/1000 | Loss: 0.00003290
Iteration 76/1000 | Loss: 0.00003290
Iteration 77/1000 | Loss: 0.00003290
Iteration 78/1000 | Loss: 0.00003290
Iteration 79/1000 | Loss: 0.00003290
Iteration 80/1000 | Loss: 0.00003290
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003290
Iteration 83/1000 | Loss: 0.00003290
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003290
Iteration 86/1000 | Loss: 0.00003290
Iteration 87/1000 | Loss: 0.00003290
Iteration 88/1000 | Loss: 0.00003289
Iteration 89/1000 | Loss: 0.00003289
Iteration 90/1000 | Loss: 0.00003289
Iteration 91/1000 | Loss: 0.00003289
Iteration 92/1000 | Loss: 0.00003289
Iteration 93/1000 | Loss: 0.00003289
Iteration 94/1000 | Loss: 0.00003289
Iteration 95/1000 | Loss: 0.00003288
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003288
Iteration 99/1000 | Loss: 0.00003288
Iteration 100/1000 | Loss: 0.00003288
Iteration 101/1000 | Loss: 0.00003288
Iteration 102/1000 | Loss: 0.00003287
Iteration 103/1000 | Loss: 0.00003287
Iteration 104/1000 | Loss: 0.00003287
Iteration 105/1000 | Loss: 0.00003287
Iteration 106/1000 | Loss: 0.00003287
Iteration 107/1000 | Loss: 0.00003287
Iteration 108/1000 | Loss: 0.00003287
Iteration 109/1000 | Loss: 0.00003287
Iteration 110/1000 | Loss: 0.00003286
Iteration 111/1000 | Loss: 0.00003286
Iteration 112/1000 | Loss: 0.00003286
Iteration 113/1000 | Loss: 0.00003286
Iteration 114/1000 | Loss: 0.00003286
Iteration 115/1000 | Loss: 0.00003286
Iteration 116/1000 | Loss: 0.00003286
Iteration 117/1000 | Loss: 0.00003286
Iteration 118/1000 | Loss: 0.00003286
Iteration 119/1000 | Loss: 0.00003286
Iteration 120/1000 | Loss: 0.00003286
Iteration 121/1000 | Loss: 0.00003285
Iteration 122/1000 | Loss: 0.00003285
Iteration 123/1000 | Loss: 0.00003285
Iteration 124/1000 | Loss: 0.00003285
Iteration 125/1000 | Loss: 0.00003285
Iteration 126/1000 | Loss: 0.00003285
Iteration 127/1000 | Loss: 0.00003285
Iteration 128/1000 | Loss: 0.00003285
Iteration 129/1000 | Loss: 0.00003285
Iteration 130/1000 | Loss: 0.00003285
Iteration 131/1000 | Loss: 0.00003285
Iteration 132/1000 | Loss: 0.00003285
Iteration 133/1000 | Loss: 0.00003285
Iteration 134/1000 | Loss: 0.00003284
Iteration 135/1000 | Loss: 0.00003284
Iteration 136/1000 | Loss: 0.00003284
Iteration 137/1000 | Loss: 0.00003284
Iteration 138/1000 | Loss: 0.00003284
Iteration 139/1000 | Loss: 0.00003284
Iteration 140/1000 | Loss: 0.00003284
Iteration 141/1000 | Loss: 0.00003284
Iteration 142/1000 | Loss: 0.00003284
Iteration 143/1000 | Loss: 0.00003284
Iteration 144/1000 | Loss: 0.00003284
Iteration 145/1000 | Loss: 0.00003284
Iteration 146/1000 | Loss: 0.00003284
Iteration 147/1000 | Loss: 0.00003284
Iteration 148/1000 | Loss: 0.00003284
Iteration 149/1000 | Loss: 0.00003284
Iteration 150/1000 | Loss: 0.00003284
Iteration 151/1000 | Loss: 0.00003284
Iteration 152/1000 | Loss: 0.00003284
Iteration 153/1000 | Loss: 0.00003284
Iteration 154/1000 | Loss: 0.00003284
Iteration 155/1000 | Loss: 0.00003284
Iteration 156/1000 | Loss: 0.00003284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [3.283774640294723e-05, 3.283774640294723e-05, 3.283774640294723e-05, 3.283774640294723e-05, 3.283774640294723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.283774640294723e-05

Optimization complete. Final v2v error: 4.771674633026123 mm

Highest mean error: 5.602363109588623 mm for frame 7

Lowest mean error: 4.397130966186523 mm for frame 62

Saving results

Total time: 45.38207769393921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00877574
Iteration 2/25 | Loss: 0.00153717
Iteration 3/25 | Loss: 0.00133179
Iteration 4/25 | Loss: 0.00131548
Iteration 5/25 | Loss: 0.00131481
Iteration 6/25 | Loss: 0.00131905
Iteration 7/25 | Loss: 0.00131228
Iteration 8/25 | Loss: 0.00131387
Iteration 9/25 | Loss: 0.00130849
Iteration 10/25 | Loss: 0.00131392
Iteration 11/25 | Loss: 0.00131255
Iteration 12/25 | Loss: 0.00130982
Iteration 13/25 | Loss: 0.00130718
Iteration 14/25 | Loss: 0.00130662
Iteration 15/25 | Loss: 0.00130654
Iteration 16/25 | Loss: 0.00130654
Iteration 17/25 | Loss: 0.00130654
Iteration 18/25 | Loss: 0.00130653
Iteration 19/25 | Loss: 0.00130653
Iteration 20/25 | Loss: 0.00130653
Iteration 21/25 | Loss: 0.00130653
Iteration 22/25 | Loss: 0.00130653
Iteration 23/25 | Loss: 0.00130653
Iteration 24/25 | Loss: 0.00130653
Iteration 25/25 | Loss: 0.00130653

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.34032726
Iteration 2/25 | Loss: 0.00087026
Iteration 3/25 | Loss: 0.00087015
Iteration 4/25 | Loss: 0.00087015
Iteration 5/25 | Loss: 0.00087015
Iteration 6/25 | Loss: 0.00087015
Iteration 7/25 | Loss: 0.00087015
Iteration 8/25 | Loss: 0.00087015
Iteration 9/25 | Loss: 0.00087015
Iteration 10/25 | Loss: 0.00087014
Iteration 11/25 | Loss: 0.00087014
Iteration 12/25 | Loss: 0.00087014
Iteration 13/25 | Loss: 0.00087014
Iteration 14/25 | Loss: 0.00087014
Iteration 15/25 | Loss: 0.00087014
Iteration 16/25 | Loss: 0.00087014
Iteration 17/25 | Loss: 0.00087014
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000870144460350275, 0.000870144460350275, 0.000870144460350275, 0.000870144460350275, 0.000870144460350275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000870144460350275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087014
Iteration 2/1000 | Loss: 0.00004344
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002765
Iteration 5/1000 | Loss: 0.00002612
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00002390
Iteration 9/1000 | Loss: 0.00002340
Iteration 10/1000 | Loss: 0.00002308
Iteration 11/1000 | Loss: 0.00002290
Iteration 12/1000 | Loss: 0.00002271
Iteration 13/1000 | Loss: 0.00002269
Iteration 14/1000 | Loss: 0.00002259
Iteration 15/1000 | Loss: 0.00002253
Iteration 16/1000 | Loss: 0.00002253
Iteration 17/1000 | Loss: 0.00002248
Iteration 18/1000 | Loss: 0.00002248
Iteration 19/1000 | Loss: 0.00002245
Iteration 20/1000 | Loss: 0.00002245
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002243
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002240
Iteration 25/1000 | Loss: 0.00002240
Iteration 26/1000 | Loss: 0.00002240
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002239
Iteration 29/1000 | Loss: 0.00002239
Iteration 30/1000 | Loss: 0.00002238
Iteration 31/1000 | Loss: 0.00002237
Iteration 32/1000 | Loss: 0.00002237
Iteration 33/1000 | Loss: 0.00002237
Iteration 34/1000 | Loss: 0.00002236
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002236
Iteration 37/1000 | Loss: 0.00002236
Iteration 38/1000 | Loss: 0.00002235
Iteration 39/1000 | Loss: 0.00002235
Iteration 40/1000 | Loss: 0.00002234
Iteration 41/1000 | Loss: 0.00002234
Iteration 42/1000 | Loss: 0.00002233
Iteration 43/1000 | Loss: 0.00002233
Iteration 44/1000 | Loss: 0.00002233
Iteration 45/1000 | Loss: 0.00002233
Iteration 46/1000 | Loss: 0.00002233
Iteration 47/1000 | Loss: 0.00002233
Iteration 48/1000 | Loss: 0.00002233
Iteration 49/1000 | Loss: 0.00002232
Iteration 50/1000 | Loss: 0.00002232
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002229
Iteration 55/1000 | Loss: 0.00002227
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002226
Iteration 58/1000 | Loss: 0.00002226
Iteration 59/1000 | Loss: 0.00002226
Iteration 60/1000 | Loss: 0.00002225
Iteration 61/1000 | Loss: 0.00002225
Iteration 62/1000 | Loss: 0.00002225
Iteration 63/1000 | Loss: 0.00002224
Iteration 64/1000 | Loss: 0.00002224
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00002224
Iteration 67/1000 | Loss: 0.00002223
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00002223
Iteration 70/1000 | Loss: 0.00002223
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002223
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002221
Iteration 79/1000 | Loss: 0.00002221
Iteration 80/1000 | Loss: 0.00002221
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002220
Iteration 83/1000 | Loss: 0.00002220
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002219
Iteration 87/1000 | Loss: 0.00002219
Iteration 88/1000 | Loss: 0.00002219
Iteration 89/1000 | Loss: 0.00002219
Iteration 90/1000 | Loss: 0.00002219
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002218
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002218
Iteration 97/1000 | Loss: 0.00002217
Iteration 98/1000 | Loss: 0.00002217
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002216
Iteration 101/1000 | Loss: 0.00002216
Iteration 102/1000 | Loss: 0.00002216
Iteration 103/1000 | Loss: 0.00002216
Iteration 104/1000 | Loss: 0.00002216
Iteration 105/1000 | Loss: 0.00002215
Iteration 106/1000 | Loss: 0.00002215
Iteration 107/1000 | Loss: 0.00002215
Iteration 108/1000 | Loss: 0.00002215
Iteration 109/1000 | Loss: 0.00002215
Iteration 110/1000 | Loss: 0.00002215
Iteration 111/1000 | Loss: 0.00002214
Iteration 112/1000 | Loss: 0.00002214
Iteration 113/1000 | Loss: 0.00002214
Iteration 114/1000 | Loss: 0.00002214
Iteration 115/1000 | Loss: 0.00002214
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002213
Iteration 118/1000 | Loss: 0.00002213
Iteration 119/1000 | Loss: 0.00002213
Iteration 120/1000 | Loss: 0.00002213
Iteration 121/1000 | Loss: 0.00002213
Iteration 122/1000 | Loss: 0.00002213
Iteration 123/1000 | Loss: 0.00002213
Iteration 124/1000 | Loss: 0.00002212
Iteration 125/1000 | Loss: 0.00002212
Iteration 126/1000 | Loss: 0.00002212
Iteration 127/1000 | Loss: 0.00002212
Iteration 128/1000 | Loss: 0.00002211
Iteration 129/1000 | Loss: 0.00002211
Iteration 130/1000 | Loss: 0.00002211
Iteration 131/1000 | Loss: 0.00002211
Iteration 132/1000 | Loss: 0.00002211
Iteration 133/1000 | Loss: 0.00002211
Iteration 134/1000 | Loss: 0.00002211
Iteration 135/1000 | Loss: 0.00002211
Iteration 136/1000 | Loss: 0.00002211
Iteration 137/1000 | Loss: 0.00002211
Iteration 138/1000 | Loss: 0.00002211
Iteration 139/1000 | Loss: 0.00002211
Iteration 140/1000 | Loss: 0.00002211
Iteration 141/1000 | Loss: 0.00002211
Iteration 142/1000 | Loss: 0.00002211
Iteration 143/1000 | Loss: 0.00002211
Iteration 144/1000 | Loss: 0.00002211
Iteration 145/1000 | Loss: 0.00002211
Iteration 146/1000 | Loss: 0.00002211
Iteration 147/1000 | Loss: 0.00002211
Iteration 148/1000 | Loss: 0.00002211
Iteration 149/1000 | Loss: 0.00002211
Iteration 150/1000 | Loss: 0.00002211
Iteration 151/1000 | Loss: 0.00002211
Iteration 152/1000 | Loss: 0.00002211
Iteration 153/1000 | Loss: 0.00002211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.2109827114036307e-05, 2.2109827114036307e-05, 2.2109827114036307e-05, 2.2109827114036307e-05, 2.2109827114036307e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2109827114036307e-05

Optimization complete. Final v2v error: 3.9228265285491943 mm

Highest mean error: 4.826401233673096 mm for frame 30

Lowest mean error: 3.346299171447754 mm for frame 207

Saving results

Total time: 62.50685501098633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812557
Iteration 2/25 | Loss: 0.00128826
Iteration 3/25 | Loss: 0.00119946
Iteration 4/25 | Loss: 0.00118830
Iteration 5/25 | Loss: 0.00118592
Iteration 6/25 | Loss: 0.00118592
Iteration 7/25 | Loss: 0.00118592
Iteration 8/25 | Loss: 0.00118592
Iteration 9/25 | Loss: 0.00118592
Iteration 10/25 | Loss: 0.00118592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011859198566526175, 0.0011859198566526175, 0.0011859198566526175, 0.0011859198566526175, 0.0011859198566526175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859198566526175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34926021
Iteration 2/25 | Loss: 0.00100205
Iteration 3/25 | Loss: 0.00100205
Iteration 4/25 | Loss: 0.00100205
Iteration 5/25 | Loss: 0.00100205
Iteration 6/25 | Loss: 0.00100205
Iteration 7/25 | Loss: 0.00100204
Iteration 8/25 | Loss: 0.00100204
Iteration 9/25 | Loss: 0.00100204
Iteration 10/25 | Loss: 0.00100204
Iteration 11/25 | Loss: 0.00100204
Iteration 12/25 | Loss: 0.00100204
Iteration 13/25 | Loss: 0.00100204
Iteration 14/25 | Loss: 0.00100204
Iteration 15/25 | Loss: 0.00100204
Iteration 16/25 | Loss: 0.00100204
Iteration 17/25 | Loss: 0.00100204
Iteration 18/25 | Loss: 0.00100204
Iteration 19/25 | Loss: 0.00100204
Iteration 20/25 | Loss: 0.00100204
Iteration 21/25 | Loss: 0.00100204
Iteration 22/25 | Loss: 0.00100204
Iteration 23/25 | Loss: 0.00100204
Iteration 24/25 | Loss: 0.00100204
Iteration 25/25 | Loss: 0.00100204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100204
Iteration 2/1000 | Loss: 0.00001954
Iteration 3/1000 | Loss: 0.00001525
Iteration 4/1000 | Loss: 0.00001339
Iteration 5/1000 | Loss: 0.00001258
Iteration 6/1000 | Loss: 0.00001187
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001088
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001065
Iteration 13/1000 | Loss: 0.00001056
Iteration 14/1000 | Loss: 0.00001055
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001054
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001047
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001041
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001040
Iteration 27/1000 | Loss: 0.00001039
Iteration 28/1000 | Loss: 0.00001039
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001034
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001032
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001031
Iteration 38/1000 | Loss: 0.00001030
Iteration 39/1000 | Loss: 0.00001029
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001026
Iteration 44/1000 | Loss: 0.00001024
Iteration 45/1000 | Loss: 0.00001022
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001014
Iteration 48/1000 | Loss: 0.00001013
Iteration 49/1000 | Loss: 0.00001013
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001006
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001005
Iteration 60/1000 | Loss: 0.00001004
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001000
Iteration 63/1000 | Loss: 0.00001000
Iteration 64/1000 | Loss: 0.00001000
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000998
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000997
Iteration 74/1000 | Loss: 0.00000997
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000997
Iteration 78/1000 | Loss: 0.00000997
Iteration 79/1000 | Loss: 0.00000996
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000996
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000996
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000995
Iteration 87/1000 | Loss: 0.00000995
Iteration 88/1000 | Loss: 0.00000995
Iteration 89/1000 | Loss: 0.00000995
Iteration 90/1000 | Loss: 0.00000995
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000994
Iteration 95/1000 | Loss: 0.00000994
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000994
Iteration 102/1000 | Loss: 0.00000994
Iteration 103/1000 | Loss: 0.00000994
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000993
Iteration 109/1000 | Loss: 0.00000993
Iteration 110/1000 | Loss: 0.00000993
Iteration 111/1000 | Loss: 0.00000993
Iteration 112/1000 | Loss: 0.00000993
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000990
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000989
Iteration 132/1000 | Loss: 0.00000988
Iteration 133/1000 | Loss: 0.00000987
Iteration 134/1000 | Loss: 0.00000987
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000986
Iteration 138/1000 | Loss: 0.00000986
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000985
Iteration 141/1000 | Loss: 0.00000985
Iteration 142/1000 | Loss: 0.00000985
Iteration 143/1000 | Loss: 0.00000984
Iteration 144/1000 | Loss: 0.00000984
Iteration 145/1000 | Loss: 0.00000984
Iteration 146/1000 | Loss: 0.00000983
Iteration 147/1000 | Loss: 0.00000983
Iteration 148/1000 | Loss: 0.00000983
Iteration 149/1000 | Loss: 0.00000983
Iteration 150/1000 | Loss: 0.00000983
Iteration 151/1000 | Loss: 0.00000982
Iteration 152/1000 | Loss: 0.00000982
Iteration 153/1000 | Loss: 0.00000982
Iteration 154/1000 | Loss: 0.00000982
Iteration 155/1000 | Loss: 0.00000981
Iteration 156/1000 | Loss: 0.00000981
Iteration 157/1000 | Loss: 0.00000981
Iteration 158/1000 | Loss: 0.00000981
Iteration 159/1000 | Loss: 0.00000981
Iteration 160/1000 | Loss: 0.00000981
Iteration 161/1000 | Loss: 0.00000980
Iteration 162/1000 | Loss: 0.00000980
Iteration 163/1000 | Loss: 0.00000980
Iteration 164/1000 | Loss: 0.00000980
Iteration 165/1000 | Loss: 0.00000980
Iteration 166/1000 | Loss: 0.00000980
Iteration 167/1000 | Loss: 0.00000980
Iteration 168/1000 | Loss: 0.00000980
Iteration 169/1000 | Loss: 0.00000979
Iteration 170/1000 | Loss: 0.00000979
Iteration 171/1000 | Loss: 0.00000979
Iteration 172/1000 | Loss: 0.00000979
Iteration 173/1000 | Loss: 0.00000978
Iteration 174/1000 | Loss: 0.00000978
Iteration 175/1000 | Loss: 0.00000978
Iteration 176/1000 | Loss: 0.00000978
Iteration 177/1000 | Loss: 0.00000978
Iteration 178/1000 | Loss: 0.00000978
Iteration 179/1000 | Loss: 0.00000978
Iteration 180/1000 | Loss: 0.00000978
Iteration 181/1000 | Loss: 0.00000978
Iteration 182/1000 | Loss: 0.00000978
Iteration 183/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [9.779821994015947e-06, 9.779821994015947e-06, 9.779821994015947e-06, 9.779821994015947e-06, 9.779821994015947e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.779821994015947e-06

Optimization complete. Final v2v error: 2.6801390647888184 mm

Highest mean error: 2.914673089981079 mm for frame 105

Lowest mean error: 2.523362874984741 mm for frame 42

Saving results

Total time: 45.247039556503296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724584
Iteration 2/25 | Loss: 0.00141497
Iteration 3/25 | Loss: 0.00128152
Iteration 4/25 | Loss: 0.00122047
Iteration 5/25 | Loss: 0.00122163
Iteration 6/25 | Loss: 0.00122343
Iteration 7/25 | Loss: 0.00120361
Iteration 8/25 | Loss: 0.00119914
Iteration 9/25 | Loss: 0.00120203
Iteration 10/25 | Loss: 0.00120039
Iteration 11/25 | Loss: 0.00119881
Iteration 12/25 | Loss: 0.00120210
Iteration 13/25 | Loss: 0.00120237
Iteration 14/25 | Loss: 0.00120095
Iteration 15/25 | Loss: 0.00119875
Iteration 16/25 | Loss: 0.00119809
Iteration 17/25 | Loss: 0.00119791
Iteration 18/25 | Loss: 0.00119791
Iteration 19/25 | Loss: 0.00119790
Iteration 20/25 | Loss: 0.00119790
Iteration 21/25 | Loss: 0.00119790
Iteration 22/25 | Loss: 0.00119790
Iteration 23/25 | Loss: 0.00119790
Iteration 24/25 | Loss: 0.00119790
Iteration 25/25 | Loss: 0.00119790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.70027113
Iteration 2/25 | Loss: 0.00142153
Iteration 3/25 | Loss: 0.00142148
Iteration 4/25 | Loss: 0.00142148
Iteration 5/25 | Loss: 0.00142148
Iteration 6/25 | Loss: 0.00142148
Iteration 7/25 | Loss: 0.00142148
Iteration 8/25 | Loss: 0.00142148
Iteration 9/25 | Loss: 0.00142148
Iteration 10/25 | Loss: 0.00142148
Iteration 11/25 | Loss: 0.00142148
Iteration 12/25 | Loss: 0.00142148
Iteration 13/25 | Loss: 0.00142148
Iteration 14/25 | Loss: 0.00142148
Iteration 15/25 | Loss: 0.00142148
Iteration 16/25 | Loss: 0.00142148
Iteration 17/25 | Loss: 0.00142148
Iteration 18/25 | Loss: 0.00142148
Iteration 19/25 | Loss: 0.00142148
Iteration 20/25 | Loss: 0.00142148
Iteration 21/25 | Loss: 0.00142148
Iteration 22/25 | Loss: 0.00142148
Iteration 23/25 | Loss: 0.00142148
Iteration 24/25 | Loss: 0.00142148
Iteration 25/25 | Loss: 0.00142148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142148
Iteration 2/1000 | Loss: 0.00018029
Iteration 3/1000 | Loss: 0.00009699
Iteration 4/1000 | Loss: 0.00012884
Iteration 5/1000 | Loss: 0.00020099
Iteration 6/1000 | Loss: 0.00020109
Iteration 7/1000 | Loss: 0.00014795
Iteration 8/1000 | Loss: 0.00015685
Iteration 9/1000 | Loss: 0.00024816
Iteration 10/1000 | Loss: 0.00010704
Iteration 11/1000 | Loss: 0.00013233
Iteration 12/1000 | Loss: 0.00015546
Iteration 13/1000 | Loss: 0.00019790
Iteration 14/1000 | Loss: 0.00014379
Iteration 15/1000 | Loss: 0.00020348
Iteration 16/1000 | Loss: 0.00012803
Iteration 17/1000 | Loss: 0.00021816
Iteration 18/1000 | Loss: 0.00012435
Iteration 19/1000 | Loss: 0.00021372
Iteration 20/1000 | Loss: 0.00018752
Iteration 21/1000 | Loss: 0.00020615
Iteration 22/1000 | Loss: 0.00016595
Iteration 23/1000 | Loss: 0.00012056
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00001970
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001612
Iteration 28/1000 | Loss: 0.00001555
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001444
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001444
Iteration 36/1000 | Loss: 0.00001444
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001440
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001439
Iteration 46/1000 | Loss: 0.00001434
Iteration 47/1000 | Loss: 0.00001434
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001431
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001429
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001427
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001425
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001414
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001413
Iteration 71/1000 | Loss: 0.00001413
Iteration 72/1000 | Loss: 0.00001413
Iteration 73/1000 | Loss: 0.00001413
Iteration 74/1000 | Loss: 0.00001413
Iteration 75/1000 | Loss: 0.00001413
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001413
Iteration 78/1000 | Loss: 0.00001413
Iteration 79/1000 | Loss: 0.00001412
Iteration 80/1000 | Loss: 0.00001412
Iteration 81/1000 | Loss: 0.00001412
Iteration 82/1000 | Loss: 0.00001412
Iteration 83/1000 | Loss: 0.00001410
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001408
Iteration 86/1000 | Loss: 0.00001408
Iteration 87/1000 | Loss: 0.00001408
Iteration 88/1000 | Loss: 0.00001408
Iteration 89/1000 | Loss: 0.00001407
Iteration 90/1000 | Loss: 0.00001407
Iteration 91/1000 | Loss: 0.00001407
Iteration 92/1000 | Loss: 0.00001407
Iteration 93/1000 | Loss: 0.00001407
Iteration 94/1000 | Loss: 0.00001407
Iteration 95/1000 | Loss: 0.00001406
Iteration 96/1000 | Loss: 0.00001406
Iteration 97/1000 | Loss: 0.00001406
Iteration 98/1000 | Loss: 0.00001406
Iteration 99/1000 | Loss: 0.00001406
Iteration 100/1000 | Loss: 0.00001405
Iteration 101/1000 | Loss: 0.00001405
Iteration 102/1000 | Loss: 0.00001405
Iteration 103/1000 | Loss: 0.00001405
Iteration 104/1000 | Loss: 0.00001405
Iteration 105/1000 | Loss: 0.00001405
Iteration 106/1000 | Loss: 0.00001405
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001404
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00001404
Iteration 114/1000 | Loss: 0.00001402
Iteration 115/1000 | Loss: 0.00001402
Iteration 116/1000 | Loss: 0.00001401
Iteration 117/1000 | Loss: 0.00001401
Iteration 118/1000 | Loss: 0.00001401
Iteration 119/1000 | Loss: 0.00001401
Iteration 120/1000 | Loss: 0.00001401
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001400
Iteration 128/1000 | Loss: 0.00001400
Iteration 129/1000 | Loss: 0.00001400
Iteration 130/1000 | Loss: 0.00001400
Iteration 131/1000 | Loss: 0.00001400
Iteration 132/1000 | Loss: 0.00001399
Iteration 133/1000 | Loss: 0.00001399
Iteration 134/1000 | Loss: 0.00001399
Iteration 135/1000 | Loss: 0.00001399
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001398
Iteration 138/1000 | Loss: 0.00001398
Iteration 139/1000 | Loss: 0.00001398
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001398
Iteration 142/1000 | Loss: 0.00001397
Iteration 143/1000 | Loss: 0.00001397
Iteration 144/1000 | Loss: 0.00001397
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001396
Iteration 149/1000 | Loss: 0.00001396
Iteration 150/1000 | Loss: 0.00001396
Iteration 151/1000 | Loss: 0.00001396
Iteration 152/1000 | Loss: 0.00001396
Iteration 153/1000 | Loss: 0.00001396
Iteration 154/1000 | Loss: 0.00001396
Iteration 155/1000 | Loss: 0.00001396
Iteration 156/1000 | Loss: 0.00001395
Iteration 157/1000 | Loss: 0.00001395
Iteration 158/1000 | Loss: 0.00001395
Iteration 159/1000 | Loss: 0.00001395
Iteration 160/1000 | Loss: 0.00001395
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001394
Iteration 171/1000 | Loss: 0.00001394
Iteration 172/1000 | Loss: 0.00001394
Iteration 173/1000 | Loss: 0.00001394
Iteration 174/1000 | Loss: 0.00001394
Iteration 175/1000 | Loss: 0.00001394
Iteration 176/1000 | Loss: 0.00001394
Iteration 177/1000 | Loss: 0.00001394
Iteration 178/1000 | Loss: 0.00001394
Iteration 179/1000 | Loss: 0.00001394
Iteration 180/1000 | Loss: 0.00001394
Iteration 181/1000 | Loss: 0.00001394
Iteration 182/1000 | Loss: 0.00001394
Iteration 183/1000 | Loss: 0.00001394
Iteration 184/1000 | Loss: 0.00001394
Iteration 185/1000 | Loss: 0.00001394
Iteration 186/1000 | Loss: 0.00001394
Iteration 187/1000 | Loss: 0.00001394
Iteration 188/1000 | Loss: 0.00001394
Iteration 189/1000 | Loss: 0.00001394
Iteration 190/1000 | Loss: 0.00001394
Iteration 191/1000 | Loss: 0.00001394
Iteration 192/1000 | Loss: 0.00001394
Iteration 193/1000 | Loss: 0.00001394
Iteration 194/1000 | Loss: 0.00001394
Iteration 195/1000 | Loss: 0.00001394
Iteration 196/1000 | Loss: 0.00001394
Iteration 197/1000 | Loss: 0.00001394
Iteration 198/1000 | Loss: 0.00001394
Iteration 199/1000 | Loss: 0.00001394
Iteration 200/1000 | Loss: 0.00001394
Iteration 201/1000 | Loss: 0.00001394
Iteration 202/1000 | Loss: 0.00001394
Iteration 203/1000 | Loss: 0.00001394
Iteration 204/1000 | Loss: 0.00001394
Iteration 205/1000 | Loss: 0.00001394
Iteration 206/1000 | Loss: 0.00001394
Iteration 207/1000 | Loss: 0.00001394
Iteration 208/1000 | Loss: 0.00001394
Iteration 209/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.393661841575522e-05, 1.393661841575522e-05, 1.393661841575522e-05, 1.393661841575522e-05, 1.393661841575522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.393661841575522e-05

Optimization complete. Final v2v error: 3.1974422931671143 mm

Highest mean error: 3.952988862991333 mm for frame 20

Lowest mean error: 2.885829448699951 mm for frame 126

Saving results

Total time: 88.143141746521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998060
Iteration 2/25 | Loss: 0.00267758
Iteration 3/25 | Loss: 0.00212852
Iteration 4/25 | Loss: 0.00186630
Iteration 5/25 | Loss: 0.00175987
Iteration 6/25 | Loss: 0.00173408
Iteration 7/25 | Loss: 0.00171746
Iteration 8/25 | Loss: 0.00164311
Iteration 9/25 | Loss: 0.00157888
Iteration 10/25 | Loss: 0.00154661
Iteration 11/25 | Loss: 0.00153109
Iteration 12/25 | Loss: 0.00152149
Iteration 13/25 | Loss: 0.00151296
Iteration 14/25 | Loss: 0.00150558
Iteration 15/25 | Loss: 0.00149949
Iteration 16/25 | Loss: 0.00149986
Iteration 17/25 | Loss: 0.00149331
Iteration 18/25 | Loss: 0.00148993
Iteration 19/25 | Loss: 0.00148844
Iteration 20/25 | Loss: 0.00148591
Iteration 21/25 | Loss: 0.00148419
Iteration 22/25 | Loss: 0.00148150
Iteration 23/25 | Loss: 0.00148071
Iteration 24/25 | Loss: 0.00148094
Iteration 25/25 | Loss: 0.00148598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38645232
Iteration 2/25 | Loss: 0.00257813
Iteration 3/25 | Loss: 0.00245792
Iteration 4/25 | Loss: 0.00245791
Iteration 5/25 | Loss: 0.00245791
Iteration 6/25 | Loss: 0.00245791
Iteration 7/25 | Loss: 0.00245791
Iteration 8/25 | Loss: 0.00245791
Iteration 9/25 | Loss: 0.00245791
Iteration 10/25 | Loss: 0.00245791
Iteration 11/25 | Loss: 0.00245791
Iteration 12/25 | Loss: 0.00245791
Iteration 13/25 | Loss: 0.00245791
Iteration 14/25 | Loss: 0.00245791
Iteration 15/25 | Loss: 0.00245791
Iteration 16/25 | Loss: 0.00245791
Iteration 17/25 | Loss: 0.00245791
Iteration 18/25 | Loss: 0.00245791
Iteration 19/25 | Loss: 0.00245791
Iteration 20/25 | Loss: 0.00245791
Iteration 21/25 | Loss: 0.00245791
Iteration 22/25 | Loss: 0.00245791
Iteration 23/25 | Loss: 0.00245791
Iteration 24/25 | Loss: 0.00245791
Iteration 25/25 | Loss: 0.00245791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245791
Iteration 2/1000 | Loss: 0.00118487
Iteration 3/1000 | Loss: 0.00028227
Iteration 4/1000 | Loss: 0.00020221
Iteration 5/1000 | Loss: 0.00024012
Iteration 6/1000 | Loss: 0.00033694
Iteration 7/1000 | Loss: 0.00026022
Iteration 8/1000 | Loss: 0.00020743
Iteration 9/1000 | Loss: 0.00049096
Iteration 10/1000 | Loss: 0.00026130
Iteration 11/1000 | Loss: 0.00014250
Iteration 12/1000 | Loss: 0.00038444
Iteration 13/1000 | Loss: 0.00041460
Iteration 14/1000 | Loss: 0.00028625
Iteration 15/1000 | Loss: 0.00030480
Iteration 16/1000 | Loss: 0.00023751
Iteration 17/1000 | Loss: 0.00038262
Iteration 18/1000 | Loss: 0.00047156
Iteration 19/1000 | Loss: 0.00031376
Iteration 20/1000 | Loss: 0.00024125
Iteration 21/1000 | Loss: 0.00013904
Iteration 22/1000 | Loss: 0.00015031
Iteration 23/1000 | Loss: 0.00020074
Iteration 24/1000 | Loss: 0.00015586
Iteration 25/1000 | Loss: 0.00021310
Iteration 26/1000 | Loss: 0.00016021
Iteration 27/1000 | Loss: 0.00016886
Iteration 28/1000 | Loss: 0.00017558
Iteration 29/1000 | Loss: 0.00018324
Iteration 30/1000 | Loss: 0.00015257
Iteration 31/1000 | Loss: 0.00015412
Iteration 32/1000 | Loss: 0.00020930
Iteration 33/1000 | Loss: 0.00098727
Iteration 34/1000 | Loss: 0.00082634
Iteration 35/1000 | Loss: 0.00016605
Iteration 36/1000 | Loss: 0.00023243
Iteration 37/1000 | Loss: 0.00027183
Iteration 38/1000 | Loss: 0.00103547
Iteration 39/1000 | Loss: 0.00048965
Iteration 40/1000 | Loss: 0.00025315
Iteration 41/1000 | Loss: 0.00084804
Iteration 42/1000 | Loss: 0.00100506
Iteration 43/1000 | Loss: 0.00016846
Iteration 44/1000 | Loss: 0.00020755
Iteration 45/1000 | Loss: 0.00039921
Iteration 46/1000 | Loss: 0.00031313
Iteration 47/1000 | Loss: 0.00018838
Iteration 48/1000 | Loss: 0.00036472
Iteration 49/1000 | Loss: 0.00038478
Iteration 50/1000 | Loss: 0.00036902
Iteration 51/1000 | Loss: 0.00013898
Iteration 52/1000 | Loss: 0.00018425
Iteration 53/1000 | Loss: 0.00014511
Iteration 54/1000 | Loss: 0.00011295
Iteration 55/1000 | Loss: 0.00010770
Iteration 56/1000 | Loss: 0.00010109
Iteration 57/1000 | Loss: 0.00020043
Iteration 58/1000 | Loss: 0.00016304
Iteration 59/1000 | Loss: 0.00019736
Iteration 60/1000 | Loss: 0.00012789
Iteration 61/1000 | Loss: 0.00010312
Iteration 62/1000 | Loss: 0.00010529
Iteration 63/1000 | Loss: 0.00010010
Iteration 64/1000 | Loss: 0.00024044
Iteration 65/1000 | Loss: 0.00012188
Iteration 66/1000 | Loss: 0.00023788
Iteration 67/1000 | Loss: 0.00024632
Iteration 68/1000 | Loss: 0.00216581
Iteration 69/1000 | Loss: 0.00054809
Iteration 70/1000 | Loss: 0.00013077
Iteration 71/1000 | Loss: 0.00010043
Iteration 72/1000 | Loss: 0.00029239
Iteration 73/1000 | Loss: 0.00008827
Iteration 74/1000 | Loss: 0.00008383
Iteration 75/1000 | Loss: 0.00009330
Iteration 76/1000 | Loss: 0.00008739
Iteration 77/1000 | Loss: 0.00008555
Iteration 78/1000 | Loss: 0.00038522
Iteration 79/1000 | Loss: 0.00011865
Iteration 80/1000 | Loss: 0.00015537
Iteration 81/1000 | Loss: 0.00022133
Iteration 82/1000 | Loss: 0.00008251
Iteration 83/1000 | Loss: 0.00009297
Iteration 84/1000 | Loss: 0.00008902
Iteration 85/1000 | Loss: 0.00008476
Iteration 86/1000 | Loss: 0.00008865
Iteration 87/1000 | Loss: 0.00007920
Iteration 88/1000 | Loss: 0.00007384
Iteration 89/1000 | Loss: 0.00007072
Iteration 90/1000 | Loss: 0.00006838
Iteration 91/1000 | Loss: 0.00007524
Iteration 92/1000 | Loss: 0.00008086
Iteration 93/1000 | Loss: 0.00021495
Iteration 94/1000 | Loss: 0.00010471
Iteration 95/1000 | Loss: 0.00007209
Iteration 96/1000 | Loss: 0.00008036
Iteration 97/1000 | Loss: 0.00021989
Iteration 98/1000 | Loss: 0.00041765
Iteration 99/1000 | Loss: 0.00008570
Iteration 100/1000 | Loss: 0.00007360
Iteration 101/1000 | Loss: 0.00009039
Iteration 102/1000 | Loss: 0.00011070
Iteration 103/1000 | Loss: 0.00009164
Iteration 104/1000 | Loss: 0.00011414
Iteration 105/1000 | Loss: 0.00006312
Iteration 106/1000 | Loss: 0.00006085
Iteration 107/1000 | Loss: 0.00005872
Iteration 108/1000 | Loss: 0.00005679
Iteration 109/1000 | Loss: 0.00005521
Iteration 110/1000 | Loss: 0.00005431
Iteration 111/1000 | Loss: 0.00005380
Iteration 112/1000 | Loss: 0.00018540
Iteration 113/1000 | Loss: 0.00024717
Iteration 114/1000 | Loss: 0.00006002
Iteration 115/1000 | Loss: 0.00005401
Iteration 116/1000 | Loss: 0.00019282
Iteration 117/1000 | Loss: 0.00007088
Iteration 118/1000 | Loss: 0.00005831
Iteration 119/1000 | Loss: 0.00005541
Iteration 120/1000 | Loss: 0.00021530
Iteration 121/1000 | Loss: 0.00020759
Iteration 122/1000 | Loss: 0.00018227
Iteration 123/1000 | Loss: 0.00005531
Iteration 124/1000 | Loss: 0.00005308
Iteration 125/1000 | Loss: 0.00021890
Iteration 126/1000 | Loss: 0.00006131
Iteration 127/1000 | Loss: 0.00005370
Iteration 128/1000 | Loss: 0.00005117
Iteration 129/1000 | Loss: 0.00005047
Iteration 130/1000 | Loss: 0.00004989
Iteration 131/1000 | Loss: 0.00004925
Iteration 132/1000 | Loss: 0.00004879
Iteration 133/1000 | Loss: 0.00004837
Iteration 134/1000 | Loss: 0.00004803
Iteration 135/1000 | Loss: 0.00004785
Iteration 136/1000 | Loss: 0.00004783
Iteration 137/1000 | Loss: 0.00004768
Iteration 138/1000 | Loss: 0.00004761
Iteration 139/1000 | Loss: 0.00004760
Iteration 140/1000 | Loss: 0.00004760
Iteration 141/1000 | Loss: 0.00004754
Iteration 142/1000 | Loss: 0.00004742
Iteration 143/1000 | Loss: 0.00004735
Iteration 144/1000 | Loss: 0.00004732
Iteration 145/1000 | Loss: 0.00004722
Iteration 146/1000 | Loss: 0.00004720
Iteration 147/1000 | Loss: 0.00004720
Iteration 148/1000 | Loss: 0.00004719
Iteration 149/1000 | Loss: 0.00004714
Iteration 150/1000 | Loss: 0.00004714
Iteration 151/1000 | Loss: 0.00004713
Iteration 152/1000 | Loss: 0.00004712
Iteration 153/1000 | Loss: 0.00004712
Iteration 154/1000 | Loss: 0.00004711
Iteration 155/1000 | Loss: 0.00004711
Iteration 156/1000 | Loss: 0.00004711
Iteration 157/1000 | Loss: 0.00004711
Iteration 158/1000 | Loss: 0.00004710
Iteration 159/1000 | Loss: 0.00004710
Iteration 160/1000 | Loss: 0.00004710
Iteration 161/1000 | Loss: 0.00004710
Iteration 162/1000 | Loss: 0.00004710
Iteration 163/1000 | Loss: 0.00004710
Iteration 164/1000 | Loss: 0.00004709
Iteration 165/1000 | Loss: 0.00004709
Iteration 166/1000 | Loss: 0.00004709
Iteration 167/1000 | Loss: 0.00004709
Iteration 168/1000 | Loss: 0.00004709
Iteration 169/1000 | Loss: 0.00004709
Iteration 170/1000 | Loss: 0.00004709
Iteration 171/1000 | Loss: 0.00004709
Iteration 172/1000 | Loss: 0.00004709
Iteration 173/1000 | Loss: 0.00004709
Iteration 174/1000 | Loss: 0.00004708
Iteration 175/1000 | Loss: 0.00004706
Iteration 176/1000 | Loss: 0.00004706
Iteration 177/1000 | Loss: 0.00004706
Iteration 178/1000 | Loss: 0.00004705
Iteration 179/1000 | Loss: 0.00004705
Iteration 180/1000 | Loss: 0.00004705
Iteration 181/1000 | Loss: 0.00004705
Iteration 182/1000 | Loss: 0.00004705
Iteration 183/1000 | Loss: 0.00004705
Iteration 184/1000 | Loss: 0.00004705
Iteration 185/1000 | Loss: 0.00004705
Iteration 186/1000 | Loss: 0.00004705
Iteration 187/1000 | Loss: 0.00004704
Iteration 188/1000 | Loss: 0.00004704
Iteration 189/1000 | Loss: 0.00004704
Iteration 190/1000 | Loss: 0.00004704
Iteration 191/1000 | Loss: 0.00004704
Iteration 192/1000 | Loss: 0.00004704
Iteration 193/1000 | Loss: 0.00004704
Iteration 194/1000 | Loss: 0.00004703
Iteration 195/1000 | Loss: 0.00004703
Iteration 196/1000 | Loss: 0.00004702
Iteration 197/1000 | Loss: 0.00004702
Iteration 198/1000 | Loss: 0.00004702
Iteration 199/1000 | Loss: 0.00004702
Iteration 200/1000 | Loss: 0.00004701
Iteration 201/1000 | Loss: 0.00004701
Iteration 202/1000 | Loss: 0.00004701
Iteration 203/1000 | Loss: 0.00004701
Iteration 204/1000 | Loss: 0.00004701
Iteration 205/1000 | Loss: 0.00004701
Iteration 206/1000 | Loss: 0.00004701
Iteration 207/1000 | Loss: 0.00004701
Iteration 208/1000 | Loss: 0.00004700
Iteration 209/1000 | Loss: 0.00004700
Iteration 210/1000 | Loss: 0.00004700
Iteration 211/1000 | Loss: 0.00004700
Iteration 212/1000 | Loss: 0.00004700
Iteration 213/1000 | Loss: 0.00004700
Iteration 214/1000 | Loss: 0.00004700
Iteration 215/1000 | Loss: 0.00004700
Iteration 216/1000 | Loss: 0.00004700
Iteration 217/1000 | Loss: 0.00004700
Iteration 218/1000 | Loss: 0.00004700
Iteration 219/1000 | Loss: 0.00004700
Iteration 220/1000 | Loss: 0.00004699
Iteration 221/1000 | Loss: 0.00004699
Iteration 222/1000 | Loss: 0.00004699
Iteration 223/1000 | Loss: 0.00004699
Iteration 224/1000 | Loss: 0.00004699
Iteration 225/1000 | Loss: 0.00004699
Iteration 226/1000 | Loss: 0.00004699
Iteration 227/1000 | Loss: 0.00004699
Iteration 228/1000 | Loss: 0.00004699
Iteration 229/1000 | Loss: 0.00004699
Iteration 230/1000 | Loss: 0.00004699
Iteration 231/1000 | Loss: 0.00004699
Iteration 232/1000 | Loss: 0.00004698
Iteration 233/1000 | Loss: 0.00004698
Iteration 234/1000 | Loss: 0.00004698
Iteration 235/1000 | Loss: 0.00004698
Iteration 236/1000 | Loss: 0.00004698
Iteration 237/1000 | Loss: 0.00004698
Iteration 238/1000 | Loss: 0.00004698
Iteration 239/1000 | Loss: 0.00004697
Iteration 240/1000 | Loss: 0.00004697
Iteration 241/1000 | Loss: 0.00004697
Iteration 242/1000 | Loss: 0.00004697
Iteration 243/1000 | Loss: 0.00004696
Iteration 244/1000 | Loss: 0.00004696
Iteration 245/1000 | Loss: 0.00004696
Iteration 246/1000 | Loss: 0.00004695
Iteration 247/1000 | Loss: 0.00004695
Iteration 248/1000 | Loss: 0.00004695
Iteration 249/1000 | Loss: 0.00004694
Iteration 250/1000 | Loss: 0.00004694
Iteration 251/1000 | Loss: 0.00004694
Iteration 252/1000 | Loss: 0.00004694
Iteration 253/1000 | Loss: 0.00004693
Iteration 254/1000 | Loss: 0.00004693
Iteration 255/1000 | Loss: 0.00004693
Iteration 256/1000 | Loss: 0.00017670
Iteration 257/1000 | Loss: 0.00004818
Iteration 258/1000 | Loss: 0.00004705
Iteration 259/1000 | Loss: 0.00004627
Iteration 260/1000 | Loss: 0.00004584
Iteration 261/1000 | Loss: 0.00004564
Iteration 262/1000 | Loss: 0.00004547
Iteration 263/1000 | Loss: 0.00004525
Iteration 264/1000 | Loss: 0.00004507
Iteration 265/1000 | Loss: 0.00004497
Iteration 266/1000 | Loss: 0.00004497
Iteration 267/1000 | Loss: 0.00004497
Iteration 268/1000 | Loss: 0.00004496
Iteration 269/1000 | Loss: 0.00004496
Iteration 270/1000 | Loss: 0.00004496
Iteration 271/1000 | Loss: 0.00004496
Iteration 272/1000 | Loss: 0.00004496
Iteration 273/1000 | Loss: 0.00004496
Iteration 274/1000 | Loss: 0.00004496
Iteration 275/1000 | Loss: 0.00004495
Iteration 276/1000 | Loss: 0.00004495
Iteration 277/1000 | Loss: 0.00004495
Iteration 278/1000 | Loss: 0.00004495
Iteration 279/1000 | Loss: 0.00004495
Iteration 280/1000 | Loss: 0.00004495
Iteration 281/1000 | Loss: 0.00004495
Iteration 282/1000 | Loss: 0.00004494
Iteration 283/1000 | Loss: 0.00004494
Iteration 284/1000 | Loss: 0.00004494
Iteration 285/1000 | Loss: 0.00004494
Iteration 286/1000 | Loss: 0.00004494
Iteration 287/1000 | Loss: 0.00004494
Iteration 288/1000 | Loss: 0.00004494
Iteration 289/1000 | Loss: 0.00004493
Iteration 290/1000 | Loss: 0.00004493
Iteration 291/1000 | Loss: 0.00004493
Iteration 292/1000 | Loss: 0.00004493
Iteration 293/1000 | Loss: 0.00004493
Iteration 294/1000 | Loss: 0.00004492
Iteration 295/1000 | Loss: 0.00004492
Iteration 296/1000 | Loss: 0.00004492
Iteration 297/1000 | Loss: 0.00004492
Iteration 298/1000 | Loss: 0.00004491
Iteration 299/1000 | Loss: 0.00004491
Iteration 300/1000 | Loss: 0.00004491
Iteration 301/1000 | Loss: 0.00004490
Iteration 302/1000 | Loss: 0.00004490
Iteration 303/1000 | Loss: 0.00004489
Iteration 304/1000 | Loss: 0.00004489
Iteration 305/1000 | Loss: 0.00004489
Iteration 306/1000 | Loss: 0.00004489
Iteration 307/1000 | Loss: 0.00004489
Iteration 308/1000 | Loss: 0.00004489
Iteration 309/1000 | Loss: 0.00004489
Iteration 310/1000 | Loss: 0.00004489
Iteration 311/1000 | Loss: 0.00004489
Iteration 312/1000 | Loss: 0.00004489
Iteration 313/1000 | Loss: 0.00004488
Iteration 314/1000 | Loss: 0.00004488
Iteration 315/1000 | Loss: 0.00004488
Iteration 316/1000 | Loss: 0.00004488
Iteration 317/1000 | Loss: 0.00004488
Iteration 318/1000 | Loss: 0.00004488
Iteration 319/1000 | Loss: 0.00004488
Iteration 320/1000 | Loss: 0.00004488
Iteration 321/1000 | Loss: 0.00004488
Iteration 322/1000 | Loss: 0.00004488
Iteration 323/1000 | Loss: 0.00004488
Iteration 324/1000 | Loss: 0.00004488
Iteration 325/1000 | Loss: 0.00004488
Iteration 326/1000 | Loss: 0.00004488
Iteration 327/1000 | Loss: 0.00004488
Iteration 328/1000 | Loss: 0.00004488
Iteration 329/1000 | Loss: 0.00004488
Iteration 330/1000 | Loss: 0.00004488
Iteration 331/1000 | Loss: 0.00004488
Iteration 332/1000 | Loss: 0.00004488
Iteration 333/1000 | Loss: 0.00004488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [4.4877975597046316e-05, 4.4877975597046316e-05, 4.4877975597046316e-05, 4.4877975597046316e-05, 4.4877975597046316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4877975597046316e-05

Optimization complete. Final v2v error: 4.26076602935791 mm

Highest mean error: 14.6109037399292 mm for frame 194

Lowest mean error: 3.1419968605041504 mm for frame 6

Saving results

Total time: 305.1056799888611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442407
Iteration 2/25 | Loss: 0.00140727
Iteration 3/25 | Loss: 0.00125814
Iteration 4/25 | Loss: 0.00123819
Iteration 5/25 | Loss: 0.00123345
Iteration 6/25 | Loss: 0.00123190
Iteration 7/25 | Loss: 0.00123190
Iteration 8/25 | Loss: 0.00123190
Iteration 9/25 | Loss: 0.00123190
Iteration 10/25 | Loss: 0.00123190
Iteration 11/25 | Loss: 0.00123190
Iteration 12/25 | Loss: 0.00123190
Iteration 13/25 | Loss: 0.00123190
Iteration 14/25 | Loss: 0.00123190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012319026282057166, 0.0012319026282057166, 0.0012319026282057166, 0.0012319026282057166, 0.0012319026282057166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012319026282057166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35395348
Iteration 2/25 | Loss: 0.00104495
Iteration 3/25 | Loss: 0.00104495
Iteration 4/25 | Loss: 0.00104495
Iteration 5/25 | Loss: 0.00104495
Iteration 6/25 | Loss: 0.00104495
Iteration 7/25 | Loss: 0.00104495
Iteration 8/25 | Loss: 0.00104495
Iteration 9/25 | Loss: 0.00104495
Iteration 10/25 | Loss: 0.00104495
Iteration 11/25 | Loss: 0.00104495
Iteration 12/25 | Loss: 0.00104495
Iteration 13/25 | Loss: 0.00104495
Iteration 14/25 | Loss: 0.00104495
Iteration 15/25 | Loss: 0.00104495
Iteration 16/25 | Loss: 0.00104495
Iteration 17/25 | Loss: 0.00104495
Iteration 18/25 | Loss: 0.00104495
Iteration 19/25 | Loss: 0.00104495
Iteration 20/25 | Loss: 0.00104495
Iteration 21/25 | Loss: 0.00104495
Iteration 22/25 | Loss: 0.00104495
Iteration 23/25 | Loss: 0.00104495
Iteration 24/25 | Loss: 0.00104495
Iteration 25/25 | Loss: 0.00104495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104495
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001760
Iteration 5/1000 | Loss: 0.00001655
Iteration 6/1000 | Loss: 0.00001588
Iteration 7/1000 | Loss: 0.00001533
Iteration 8/1000 | Loss: 0.00001502
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001454
Iteration 11/1000 | Loss: 0.00001431
Iteration 12/1000 | Loss: 0.00001428
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001410
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001403
Iteration 21/1000 | Loss: 0.00001402
Iteration 22/1000 | Loss: 0.00001401
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001399
Iteration 25/1000 | Loss: 0.00001399
Iteration 26/1000 | Loss: 0.00001398
Iteration 27/1000 | Loss: 0.00001398
Iteration 28/1000 | Loss: 0.00001397
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001395
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001394
Iteration 33/1000 | Loss: 0.00001393
Iteration 34/1000 | Loss: 0.00001392
Iteration 35/1000 | Loss: 0.00001392
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001390
Iteration 39/1000 | Loss: 0.00001390
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001388
Iteration 43/1000 | Loss: 0.00001388
Iteration 44/1000 | Loss: 0.00001388
Iteration 45/1000 | Loss: 0.00001387
Iteration 46/1000 | Loss: 0.00001387
Iteration 47/1000 | Loss: 0.00001386
Iteration 48/1000 | Loss: 0.00001386
Iteration 49/1000 | Loss: 0.00001386
Iteration 50/1000 | Loss: 0.00001385
Iteration 51/1000 | Loss: 0.00001385
Iteration 52/1000 | Loss: 0.00001385
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001385
Iteration 55/1000 | Loss: 0.00001385
Iteration 56/1000 | Loss: 0.00001385
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001384
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001384
Iteration 64/1000 | Loss: 0.00001384
Iteration 65/1000 | Loss: 0.00001383
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001380
Iteration 73/1000 | Loss: 0.00001380
Iteration 74/1000 | Loss: 0.00001380
Iteration 75/1000 | Loss: 0.00001380
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001379
Iteration 78/1000 | Loss: 0.00001373
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001370
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001369
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001368
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001366
Iteration 96/1000 | Loss: 0.00001366
Iteration 97/1000 | Loss: 0.00001366
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001365
Iteration 109/1000 | Loss: 0.00001365
Iteration 110/1000 | Loss: 0.00001365
Iteration 111/1000 | Loss: 0.00001364
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001364
Iteration 115/1000 | Loss: 0.00001364
Iteration 116/1000 | Loss: 0.00001364
Iteration 117/1000 | Loss: 0.00001364
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001363
Iteration 121/1000 | Loss: 0.00001363
Iteration 122/1000 | Loss: 0.00001363
Iteration 123/1000 | Loss: 0.00001363
Iteration 124/1000 | Loss: 0.00001363
Iteration 125/1000 | Loss: 0.00001363
Iteration 126/1000 | Loss: 0.00001363
Iteration 127/1000 | Loss: 0.00001363
Iteration 128/1000 | Loss: 0.00001362
Iteration 129/1000 | Loss: 0.00001362
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001362
Iteration 134/1000 | Loss: 0.00001362
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001361
Iteration 139/1000 | Loss: 0.00001361
Iteration 140/1000 | Loss: 0.00001361
Iteration 141/1000 | Loss: 0.00001361
Iteration 142/1000 | Loss: 0.00001361
Iteration 143/1000 | Loss: 0.00001361
Iteration 144/1000 | Loss: 0.00001361
Iteration 145/1000 | Loss: 0.00001361
Iteration 146/1000 | Loss: 0.00001361
Iteration 147/1000 | Loss: 0.00001360
Iteration 148/1000 | Loss: 0.00001360
Iteration 149/1000 | Loss: 0.00001360
Iteration 150/1000 | Loss: 0.00001360
Iteration 151/1000 | Loss: 0.00001360
Iteration 152/1000 | Loss: 0.00001360
Iteration 153/1000 | Loss: 0.00001360
Iteration 154/1000 | Loss: 0.00001360
Iteration 155/1000 | Loss: 0.00001360
Iteration 156/1000 | Loss: 0.00001360
Iteration 157/1000 | Loss: 0.00001360
Iteration 158/1000 | Loss: 0.00001360
Iteration 159/1000 | Loss: 0.00001360
Iteration 160/1000 | Loss: 0.00001360
Iteration 161/1000 | Loss: 0.00001360
Iteration 162/1000 | Loss: 0.00001360
Iteration 163/1000 | Loss: 0.00001360
Iteration 164/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.3596241842606105e-05, 1.3596241842606105e-05, 1.3596241842606105e-05, 1.3596241842606105e-05, 1.3596241842606105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3596241842606105e-05

Optimization complete. Final v2v error: 3.158458709716797 mm

Highest mean error: 4.0743913650512695 mm for frame 79

Lowest mean error: 2.756117820739746 mm for frame 35

Saving results

Total time: 45.460102558135986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803126
Iteration 2/25 | Loss: 0.00126880
Iteration 3/25 | Loss: 0.00119763
Iteration 4/25 | Loss: 0.00119324
Iteration 5/25 | Loss: 0.00119218
Iteration 6/25 | Loss: 0.00119218
Iteration 7/25 | Loss: 0.00119218
Iteration 8/25 | Loss: 0.00119218
Iteration 9/25 | Loss: 0.00119218
Iteration 10/25 | Loss: 0.00119218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011921830009669065, 0.0011921830009669065, 0.0011921830009669065, 0.0011921830009669065, 0.0011921830009669065]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011921830009669065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35503221
Iteration 2/25 | Loss: 0.00095127
Iteration 3/25 | Loss: 0.00095127
Iteration 4/25 | Loss: 0.00095127
Iteration 5/25 | Loss: 0.00095126
Iteration 6/25 | Loss: 0.00095126
Iteration 7/25 | Loss: 0.00095126
Iteration 8/25 | Loss: 0.00095126
Iteration 9/25 | Loss: 0.00095126
Iteration 10/25 | Loss: 0.00095126
Iteration 11/25 | Loss: 0.00095126
Iteration 12/25 | Loss: 0.00095126
Iteration 13/25 | Loss: 0.00095126
Iteration 14/25 | Loss: 0.00095126
Iteration 15/25 | Loss: 0.00095126
Iteration 16/25 | Loss: 0.00095126
Iteration 17/25 | Loss: 0.00095126
Iteration 18/25 | Loss: 0.00095126
Iteration 19/25 | Loss: 0.00095126
Iteration 20/25 | Loss: 0.00095126
Iteration 21/25 | Loss: 0.00095126
Iteration 22/25 | Loss: 0.00095126
Iteration 23/25 | Loss: 0.00095126
Iteration 24/25 | Loss: 0.00095126
Iteration 25/25 | Loss: 0.00095126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095126
Iteration 2/1000 | Loss: 0.00002092
Iteration 3/1000 | Loss: 0.00001539
Iteration 4/1000 | Loss: 0.00001371
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001159
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001117
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001082
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001073
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001071
Iteration 17/1000 | Loss: 0.00001069
Iteration 18/1000 | Loss: 0.00001068
Iteration 19/1000 | Loss: 0.00001067
Iteration 20/1000 | Loss: 0.00001061
Iteration 21/1000 | Loss: 0.00001061
Iteration 22/1000 | Loss: 0.00001059
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001057
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001049
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001046
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001045
Iteration 38/1000 | Loss: 0.00001045
Iteration 39/1000 | Loss: 0.00001045
Iteration 40/1000 | Loss: 0.00001044
Iteration 41/1000 | Loss: 0.00001044
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001042
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001042
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001040
Iteration 55/1000 | Loss: 0.00001040
Iteration 56/1000 | Loss: 0.00001039
Iteration 57/1000 | Loss: 0.00001038
Iteration 58/1000 | Loss: 0.00001038
Iteration 59/1000 | Loss: 0.00001036
Iteration 60/1000 | Loss: 0.00001036
Iteration 61/1000 | Loss: 0.00001036
Iteration 62/1000 | Loss: 0.00001036
Iteration 63/1000 | Loss: 0.00001035
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001031
Iteration 66/1000 | Loss: 0.00001031
Iteration 67/1000 | Loss: 0.00001031
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001025
Iteration 72/1000 | Loss: 0.00001024
Iteration 73/1000 | Loss: 0.00001024
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001019
Iteration 84/1000 | Loss: 0.00001019
Iteration 85/1000 | Loss: 0.00001018
Iteration 86/1000 | Loss: 0.00001018
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001017
Iteration 92/1000 | Loss: 0.00001016
Iteration 93/1000 | Loss: 0.00001016
Iteration 94/1000 | Loss: 0.00001016
Iteration 95/1000 | Loss: 0.00001016
Iteration 96/1000 | Loss: 0.00001016
Iteration 97/1000 | Loss: 0.00001016
Iteration 98/1000 | Loss: 0.00001015
Iteration 99/1000 | Loss: 0.00001015
Iteration 100/1000 | Loss: 0.00001015
Iteration 101/1000 | Loss: 0.00001015
Iteration 102/1000 | Loss: 0.00001014
Iteration 103/1000 | Loss: 0.00001014
Iteration 104/1000 | Loss: 0.00001014
Iteration 105/1000 | Loss: 0.00001014
Iteration 106/1000 | Loss: 0.00001013
Iteration 107/1000 | Loss: 0.00001013
Iteration 108/1000 | Loss: 0.00001013
Iteration 109/1000 | Loss: 0.00001013
Iteration 110/1000 | Loss: 0.00001012
Iteration 111/1000 | Loss: 0.00001012
Iteration 112/1000 | Loss: 0.00001012
Iteration 113/1000 | Loss: 0.00001012
Iteration 114/1000 | Loss: 0.00001012
Iteration 115/1000 | Loss: 0.00001011
Iteration 116/1000 | Loss: 0.00001011
Iteration 117/1000 | Loss: 0.00001011
Iteration 118/1000 | Loss: 0.00001011
Iteration 119/1000 | Loss: 0.00001011
Iteration 120/1000 | Loss: 0.00001011
Iteration 121/1000 | Loss: 0.00001010
Iteration 122/1000 | Loss: 0.00001010
Iteration 123/1000 | Loss: 0.00001010
Iteration 124/1000 | Loss: 0.00001010
Iteration 125/1000 | Loss: 0.00001010
Iteration 126/1000 | Loss: 0.00001010
Iteration 127/1000 | Loss: 0.00001010
Iteration 128/1000 | Loss: 0.00001010
Iteration 129/1000 | Loss: 0.00001010
Iteration 130/1000 | Loss: 0.00001010
Iteration 131/1000 | Loss: 0.00001010
Iteration 132/1000 | Loss: 0.00001010
Iteration 133/1000 | Loss: 0.00001010
Iteration 134/1000 | Loss: 0.00001010
Iteration 135/1000 | Loss: 0.00001009
Iteration 136/1000 | Loss: 0.00001009
Iteration 137/1000 | Loss: 0.00001009
Iteration 138/1000 | Loss: 0.00001009
Iteration 139/1000 | Loss: 0.00001009
Iteration 140/1000 | Loss: 0.00001009
Iteration 141/1000 | Loss: 0.00001009
Iteration 142/1000 | Loss: 0.00001008
Iteration 143/1000 | Loss: 0.00001008
Iteration 144/1000 | Loss: 0.00001008
Iteration 145/1000 | Loss: 0.00001008
Iteration 146/1000 | Loss: 0.00001008
Iteration 147/1000 | Loss: 0.00001007
Iteration 148/1000 | Loss: 0.00001007
Iteration 149/1000 | Loss: 0.00001007
Iteration 150/1000 | Loss: 0.00001007
Iteration 151/1000 | Loss: 0.00001007
Iteration 152/1000 | Loss: 0.00001006
Iteration 153/1000 | Loss: 0.00001006
Iteration 154/1000 | Loss: 0.00001006
Iteration 155/1000 | Loss: 0.00001006
Iteration 156/1000 | Loss: 0.00001005
Iteration 157/1000 | Loss: 0.00001005
Iteration 158/1000 | Loss: 0.00001005
Iteration 159/1000 | Loss: 0.00001005
Iteration 160/1000 | Loss: 0.00001005
Iteration 161/1000 | Loss: 0.00001005
Iteration 162/1000 | Loss: 0.00001004
Iteration 163/1000 | Loss: 0.00001004
Iteration 164/1000 | Loss: 0.00001004
Iteration 165/1000 | Loss: 0.00001004
Iteration 166/1000 | Loss: 0.00001004
Iteration 167/1000 | Loss: 0.00001004
Iteration 168/1000 | Loss: 0.00001004
Iteration 169/1000 | Loss: 0.00001004
Iteration 170/1000 | Loss: 0.00001004
Iteration 171/1000 | Loss: 0.00001004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.0043240763479844e-05, 1.0043240763479844e-05, 1.0043240763479844e-05, 1.0043240763479844e-05, 1.0043240763479844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0043240763479844e-05

Optimization complete. Final v2v error: 2.7075393199920654 mm

Highest mean error: 2.8541417121887207 mm for frame 54

Lowest mean error: 2.5804402828216553 mm for frame 167

Saving results

Total time: 39.378488540649414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403005
Iteration 2/25 | Loss: 0.00124388
Iteration 3/25 | Loss: 0.00118220
Iteration 4/25 | Loss: 0.00117319
Iteration 5/25 | Loss: 0.00117092
Iteration 6/25 | Loss: 0.00117092
Iteration 7/25 | Loss: 0.00117092
Iteration 8/25 | Loss: 0.00117092
Iteration 9/25 | Loss: 0.00117092
Iteration 10/25 | Loss: 0.00117092
Iteration 11/25 | Loss: 0.00117092
Iteration 12/25 | Loss: 0.00117092
Iteration 13/25 | Loss: 0.00117092
Iteration 14/25 | Loss: 0.00117092
Iteration 15/25 | Loss: 0.00117092
Iteration 16/25 | Loss: 0.00117092
Iteration 17/25 | Loss: 0.00117092
Iteration 18/25 | Loss: 0.00117092
Iteration 19/25 | Loss: 0.00117092
Iteration 20/25 | Loss: 0.00117092
Iteration 21/25 | Loss: 0.00117092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011709170648828149, 0.0011709170648828149, 0.0011709170648828149, 0.0011709170648828149, 0.0011709170648828149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011709170648828149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93457210
Iteration 2/25 | Loss: 0.00092278
Iteration 3/25 | Loss: 0.00092278
Iteration 4/25 | Loss: 0.00092278
Iteration 5/25 | Loss: 0.00092277
Iteration 6/25 | Loss: 0.00092277
Iteration 7/25 | Loss: 0.00092277
Iteration 8/25 | Loss: 0.00092277
Iteration 9/25 | Loss: 0.00092277
Iteration 10/25 | Loss: 0.00092277
Iteration 11/25 | Loss: 0.00092277
Iteration 12/25 | Loss: 0.00092277
Iteration 13/25 | Loss: 0.00092277
Iteration 14/25 | Loss: 0.00092277
Iteration 15/25 | Loss: 0.00092277
Iteration 16/25 | Loss: 0.00092277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009227731497958302, 0.0009227731497958302, 0.0009227731497958302, 0.0009227731497958302, 0.0009227731497958302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009227731497958302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092277
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001409
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001226
Iteration 9/1000 | Loss: 0.00001201
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001119
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001109
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001104
Iteration 25/1000 | Loss: 0.00001103
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001092
Iteration 29/1000 | Loss: 0.00001092
Iteration 30/1000 | Loss: 0.00001091
Iteration 31/1000 | Loss: 0.00001087
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001080
Iteration 36/1000 | Loss: 0.00001080
Iteration 37/1000 | Loss: 0.00001080
Iteration 38/1000 | Loss: 0.00001079
Iteration 39/1000 | Loss: 0.00001079
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001077
Iteration 44/1000 | Loss: 0.00001077
Iteration 45/1000 | Loss: 0.00001076
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001074
Iteration 51/1000 | Loss: 0.00001074
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001073
Iteration 55/1000 | Loss: 0.00001072
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001071
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001070
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001070
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001068
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001066
Iteration 78/1000 | Loss: 0.00001066
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001064
Iteration 85/1000 | Loss: 0.00001064
Iteration 86/1000 | Loss: 0.00001064
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001064
Iteration 89/1000 | Loss: 0.00001063
Iteration 90/1000 | Loss: 0.00001063
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001062
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001061
Iteration 99/1000 | Loss: 0.00001061
Iteration 100/1000 | Loss: 0.00001061
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001061
Iteration 103/1000 | Loss: 0.00001061
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001060
Iteration 108/1000 | Loss: 0.00001060
Iteration 109/1000 | Loss: 0.00001060
Iteration 110/1000 | Loss: 0.00001060
Iteration 111/1000 | Loss: 0.00001060
Iteration 112/1000 | Loss: 0.00001060
Iteration 113/1000 | Loss: 0.00001060
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001059
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001059
Iteration 122/1000 | Loss: 0.00001059
Iteration 123/1000 | Loss: 0.00001059
Iteration 124/1000 | Loss: 0.00001059
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001057
Iteration 138/1000 | Loss: 0.00001057
Iteration 139/1000 | Loss: 0.00001057
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001056
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001055
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001054
Iteration 159/1000 | Loss: 0.00001054
Iteration 160/1000 | Loss: 0.00001054
Iteration 161/1000 | Loss: 0.00001054
Iteration 162/1000 | Loss: 0.00001054
Iteration 163/1000 | Loss: 0.00001054
Iteration 164/1000 | Loss: 0.00001054
Iteration 165/1000 | Loss: 0.00001054
Iteration 166/1000 | Loss: 0.00001054
Iteration 167/1000 | Loss: 0.00001054
Iteration 168/1000 | Loss: 0.00001054
Iteration 169/1000 | Loss: 0.00001054
Iteration 170/1000 | Loss: 0.00001054
Iteration 171/1000 | Loss: 0.00001053
Iteration 172/1000 | Loss: 0.00001053
Iteration 173/1000 | Loss: 0.00001053
Iteration 174/1000 | Loss: 0.00001053
Iteration 175/1000 | Loss: 0.00001053
Iteration 176/1000 | Loss: 0.00001053
Iteration 177/1000 | Loss: 0.00001053
Iteration 178/1000 | Loss: 0.00001052
Iteration 179/1000 | Loss: 0.00001052
Iteration 180/1000 | Loss: 0.00001052
Iteration 181/1000 | Loss: 0.00001052
Iteration 182/1000 | Loss: 0.00001052
Iteration 183/1000 | Loss: 0.00001052
Iteration 184/1000 | Loss: 0.00001052
Iteration 185/1000 | Loss: 0.00001052
Iteration 186/1000 | Loss: 0.00001052
Iteration 187/1000 | Loss: 0.00001052
Iteration 188/1000 | Loss: 0.00001052
Iteration 189/1000 | Loss: 0.00001051
Iteration 190/1000 | Loss: 0.00001051
Iteration 191/1000 | Loss: 0.00001051
Iteration 192/1000 | Loss: 0.00001051
Iteration 193/1000 | Loss: 0.00001051
Iteration 194/1000 | Loss: 0.00001051
Iteration 195/1000 | Loss: 0.00001051
Iteration 196/1000 | Loss: 0.00001051
Iteration 197/1000 | Loss: 0.00001051
Iteration 198/1000 | Loss: 0.00001051
Iteration 199/1000 | Loss: 0.00001051
Iteration 200/1000 | Loss: 0.00001051
Iteration 201/1000 | Loss: 0.00001051
Iteration 202/1000 | Loss: 0.00001051
Iteration 203/1000 | Loss: 0.00001051
Iteration 204/1000 | Loss: 0.00001051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.0510505489946809e-05, 1.0510505489946809e-05, 1.0510505489946809e-05, 1.0510505489946809e-05, 1.0510505489946809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0510505489946809e-05

Optimization complete. Final v2v error: 2.7993197441101074 mm

Highest mean error: 3.166522979736328 mm for frame 88

Lowest mean error: 2.7125773429870605 mm for frame 135

Saving results

Total time: 42.432392835617065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741362
Iteration 2/25 | Loss: 0.00132154
Iteration 3/25 | Loss: 0.00118890
Iteration 4/25 | Loss: 0.00117556
Iteration 5/25 | Loss: 0.00117238
Iteration 6/25 | Loss: 0.00117229
Iteration 7/25 | Loss: 0.00117229
Iteration 8/25 | Loss: 0.00117229
Iteration 9/25 | Loss: 0.00117229
Iteration 10/25 | Loss: 0.00117229
Iteration 11/25 | Loss: 0.00117229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011722864583134651, 0.0011722864583134651, 0.0011722864583134651, 0.0011722864583134651, 0.0011722864583134651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011722864583134651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30991769
Iteration 2/25 | Loss: 0.00079359
Iteration 3/25 | Loss: 0.00079356
Iteration 4/25 | Loss: 0.00079355
Iteration 5/25 | Loss: 0.00079355
Iteration 6/25 | Loss: 0.00079355
Iteration 7/25 | Loss: 0.00079355
Iteration 8/25 | Loss: 0.00079355
Iteration 9/25 | Loss: 0.00079355
Iteration 10/25 | Loss: 0.00079355
Iteration 11/25 | Loss: 0.00079355
Iteration 12/25 | Loss: 0.00079355
Iteration 13/25 | Loss: 0.00079355
Iteration 14/25 | Loss: 0.00079355
Iteration 15/25 | Loss: 0.00079355
Iteration 16/25 | Loss: 0.00079355
Iteration 17/25 | Loss: 0.00079355
Iteration 18/25 | Loss: 0.00079355
Iteration 19/25 | Loss: 0.00079355
Iteration 20/25 | Loss: 0.00079355
Iteration 21/25 | Loss: 0.00079355
Iteration 22/25 | Loss: 0.00079355
Iteration 23/25 | Loss: 0.00079355
Iteration 24/25 | Loss: 0.00079355
Iteration 25/25 | Loss: 0.00079355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079355
Iteration 2/1000 | Loss: 0.00002675
Iteration 3/1000 | Loss: 0.00002087
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001755
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001505
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001433
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001355
Iteration 15/1000 | Loss: 0.00001351
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001345
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001335
Iteration 25/1000 | Loss: 0.00001335
Iteration 26/1000 | Loss: 0.00001335
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001331
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001329
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001328
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001327
Iteration 41/1000 | Loss: 0.00001326
Iteration 42/1000 | Loss: 0.00001325
Iteration 43/1000 | Loss: 0.00001325
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001324
Iteration 46/1000 | Loss: 0.00001323
Iteration 47/1000 | Loss: 0.00001323
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001322
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001314
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001311
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001310
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001307
Iteration 81/1000 | Loss: 0.00001307
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001305
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001304
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001302
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001302
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001302
Iteration 107/1000 | Loss: 0.00001302
Iteration 108/1000 | Loss: 0.00001302
Iteration 109/1000 | Loss: 0.00001302
Iteration 110/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.3016849152336363e-05, 1.3016849152336363e-05, 1.3016849152336363e-05, 1.3016849152336363e-05, 1.3016849152336363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3016849152336363e-05

Optimization complete. Final v2v error: 3.099846839904785 mm

Highest mean error: 3.5137336254119873 mm for frame 193

Lowest mean error: 2.7808988094329834 mm for frame 65

Saving results

Total time: 43.02335262298584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553480
Iteration 2/25 | Loss: 0.00144534
Iteration 3/25 | Loss: 0.00129694
Iteration 4/25 | Loss: 0.00128228
Iteration 5/25 | Loss: 0.00127817
Iteration 6/25 | Loss: 0.00127767
Iteration 7/25 | Loss: 0.00127767
Iteration 8/25 | Loss: 0.00127767
Iteration 9/25 | Loss: 0.00127767
Iteration 10/25 | Loss: 0.00127767
Iteration 11/25 | Loss: 0.00127767
Iteration 12/25 | Loss: 0.00127767
Iteration 13/25 | Loss: 0.00127767
Iteration 14/25 | Loss: 0.00127767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012776703806594014, 0.0012776703806594014, 0.0012776703806594014, 0.0012776703806594014, 0.0012776703806594014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012776703806594014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70482862
Iteration 2/25 | Loss: 0.00129506
Iteration 3/25 | Loss: 0.00129504
Iteration 4/25 | Loss: 0.00129504
Iteration 5/25 | Loss: 0.00129503
Iteration 6/25 | Loss: 0.00129503
Iteration 7/25 | Loss: 0.00129503
Iteration 8/25 | Loss: 0.00129503
Iteration 9/25 | Loss: 0.00129503
Iteration 10/25 | Loss: 0.00129503
Iteration 11/25 | Loss: 0.00129503
Iteration 12/25 | Loss: 0.00129503
Iteration 13/25 | Loss: 0.00129503
Iteration 14/25 | Loss: 0.00129503
Iteration 15/25 | Loss: 0.00129503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012950327945873141, 0.0012950327945873141, 0.0012950327945873141, 0.0012950327945873141, 0.0012950327945873141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012950327945873141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129503
Iteration 2/1000 | Loss: 0.00005213
Iteration 3/1000 | Loss: 0.00003130
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002574
Iteration 6/1000 | Loss: 0.00002424
Iteration 7/1000 | Loss: 0.00002339
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002225
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002141
Iteration 12/1000 | Loss: 0.00002116
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002090
Iteration 16/1000 | Loss: 0.00002082
Iteration 17/1000 | Loss: 0.00002079
Iteration 18/1000 | Loss: 0.00002075
Iteration 19/1000 | Loss: 0.00002072
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002059
Iteration 24/1000 | Loss: 0.00002059
Iteration 25/1000 | Loss: 0.00002057
Iteration 26/1000 | Loss: 0.00002056
Iteration 27/1000 | Loss: 0.00002055
Iteration 28/1000 | Loss: 0.00002055
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002050
Iteration 33/1000 | Loss: 0.00002050
Iteration 34/1000 | Loss: 0.00002049
Iteration 35/1000 | Loss: 0.00002049
Iteration 36/1000 | Loss: 0.00002049
Iteration 37/1000 | Loss: 0.00002048
Iteration 38/1000 | Loss: 0.00002046
Iteration 39/1000 | Loss: 0.00002046
Iteration 40/1000 | Loss: 0.00002046
Iteration 41/1000 | Loss: 0.00002046
Iteration 42/1000 | Loss: 0.00002046
Iteration 43/1000 | Loss: 0.00002046
Iteration 44/1000 | Loss: 0.00002046
Iteration 45/1000 | Loss: 0.00002046
Iteration 46/1000 | Loss: 0.00002045
Iteration 47/1000 | Loss: 0.00002045
Iteration 48/1000 | Loss: 0.00002045
Iteration 49/1000 | Loss: 0.00002045
Iteration 50/1000 | Loss: 0.00002045
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002042
Iteration 55/1000 | Loss: 0.00002042
Iteration 56/1000 | Loss: 0.00002042
Iteration 57/1000 | Loss: 0.00002041
Iteration 58/1000 | Loss: 0.00002041
Iteration 59/1000 | Loss: 0.00002041
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002040
Iteration 62/1000 | Loss: 0.00002040
Iteration 63/1000 | Loss: 0.00002039
Iteration 64/1000 | Loss: 0.00002039
Iteration 65/1000 | Loss: 0.00002039
Iteration 66/1000 | Loss: 0.00002039
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002038
Iteration 70/1000 | Loss: 0.00002037
Iteration 71/1000 | Loss: 0.00002037
Iteration 72/1000 | Loss: 0.00002037
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00002035
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002034
Iteration 81/1000 | Loss: 0.00002034
Iteration 82/1000 | Loss: 0.00002034
Iteration 83/1000 | Loss: 0.00002034
Iteration 84/1000 | Loss: 0.00002034
Iteration 85/1000 | Loss: 0.00002034
Iteration 86/1000 | Loss: 0.00002034
Iteration 87/1000 | Loss: 0.00002033
Iteration 88/1000 | Loss: 0.00002033
Iteration 89/1000 | Loss: 0.00002033
Iteration 90/1000 | Loss: 0.00002033
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00002032
Iteration 93/1000 | Loss: 0.00002032
Iteration 94/1000 | Loss: 0.00002032
Iteration 95/1000 | Loss: 0.00002032
Iteration 96/1000 | Loss: 0.00002031
Iteration 97/1000 | Loss: 0.00002031
Iteration 98/1000 | Loss: 0.00002031
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002029
Iteration 105/1000 | Loss: 0.00002029
Iteration 106/1000 | Loss: 0.00002029
Iteration 107/1000 | Loss: 0.00002029
Iteration 108/1000 | Loss: 0.00002028
Iteration 109/1000 | Loss: 0.00002028
Iteration 110/1000 | Loss: 0.00002028
Iteration 111/1000 | Loss: 0.00002027
Iteration 112/1000 | Loss: 0.00002027
Iteration 113/1000 | Loss: 0.00002027
Iteration 114/1000 | Loss: 0.00002027
Iteration 115/1000 | Loss: 0.00002026
Iteration 116/1000 | Loss: 0.00002026
Iteration 117/1000 | Loss: 0.00002026
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00002025
Iteration 120/1000 | Loss: 0.00002025
Iteration 121/1000 | Loss: 0.00002025
Iteration 122/1000 | Loss: 0.00002025
Iteration 123/1000 | Loss: 0.00002024
Iteration 124/1000 | Loss: 0.00002024
Iteration 125/1000 | Loss: 0.00002024
Iteration 126/1000 | Loss: 0.00002024
Iteration 127/1000 | Loss: 0.00002023
Iteration 128/1000 | Loss: 0.00002023
Iteration 129/1000 | Loss: 0.00002023
Iteration 130/1000 | Loss: 0.00002023
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002022
Iteration 135/1000 | Loss: 0.00002022
Iteration 136/1000 | Loss: 0.00002022
Iteration 137/1000 | Loss: 0.00002022
Iteration 138/1000 | Loss: 0.00002021
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00002020
Iteration 150/1000 | Loss: 0.00002020
Iteration 151/1000 | Loss: 0.00002020
Iteration 152/1000 | Loss: 0.00002020
Iteration 153/1000 | Loss: 0.00002020
Iteration 154/1000 | Loss: 0.00002020
Iteration 155/1000 | Loss: 0.00002020
Iteration 156/1000 | Loss: 0.00002019
Iteration 157/1000 | Loss: 0.00002019
Iteration 158/1000 | Loss: 0.00002019
Iteration 159/1000 | Loss: 0.00002019
Iteration 160/1000 | Loss: 0.00002019
Iteration 161/1000 | Loss: 0.00002019
Iteration 162/1000 | Loss: 0.00002019
Iteration 163/1000 | Loss: 0.00002019
Iteration 164/1000 | Loss: 0.00002019
Iteration 165/1000 | Loss: 0.00002019
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002018
Iteration 169/1000 | Loss: 0.00002018
Iteration 170/1000 | Loss: 0.00002018
Iteration 171/1000 | Loss: 0.00002018
Iteration 172/1000 | Loss: 0.00002018
Iteration 173/1000 | Loss: 0.00002018
Iteration 174/1000 | Loss: 0.00002018
Iteration 175/1000 | Loss: 0.00002018
Iteration 176/1000 | Loss: 0.00002018
Iteration 177/1000 | Loss: 0.00002018
Iteration 178/1000 | Loss: 0.00002018
Iteration 179/1000 | Loss: 0.00002018
Iteration 180/1000 | Loss: 0.00002018
Iteration 181/1000 | Loss: 0.00002018
Iteration 182/1000 | Loss: 0.00002018
Iteration 183/1000 | Loss: 0.00002018
Iteration 184/1000 | Loss: 0.00002018
Iteration 185/1000 | Loss: 0.00002018
Iteration 186/1000 | Loss: 0.00002018
Iteration 187/1000 | Loss: 0.00002018
Iteration 188/1000 | Loss: 0.00002018
Iteration 189/1000 | Loss: 0.00002018
Iteration 190/1000 | Loss: 0.00002018
Iteration 191/1000 | Loss: 0.00002018
Iteration 192/1000 | Loss: 0.00002018
Iteration 193/1000 | Loss: 0.00002018
Iteration 194/1000 | Loss: 0.00002018
Iteration 195/1000 | Loss: 0.00002018
Iteration 196/1000 | Loss: 0.00002018
Iteration 197/1000 | Loss: 0.00002018
Iteration 198/1000 | Loss: 0.00002018
Iteration 199/1000 | Loss: 0.00002018
Iteration 200/1000 | Loss: 0.00002018
Iteration 201/1000 | Loss: 0.00002018
Iteration 202/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.0176619727863e-05, 2.0176619727863e-05, 2.0176619727863e-05, 2.0176619727863e-05, 2.0176619727863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0176619727863e-05

Optimization complete. Final v2v error: 3.6904287338256836 mm

Highest mean error: 4.370152950286865 mm for frame 232

Lowest mean error: 3.240079402923584 mm for frame 203

Saving results

Total time: 51.101022720336914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_026/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_026/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955955
Iteration 2/25 | Loss: 0.00355857
Iteration 3/25 | Loss: 0.00220885
Iteration 4/25 | Loss: 0.00201936
Iteration 5/25 | Loss: 0.00182732
Iteration 6/25 | Loss: 0.00184834
Iteration 7/25 | Loss: 0.00191030
Iteration 8/25 | Loss: 0.00179122
Iteration 9/25 | Loss: 0.00163719
Iteration 10/25 | Loss: 0.00158687
Iteration 11/25 | Loss: 0.00156424
Iteration 12/25 | Loss: 0.00154388
Iteration 13/25 | Loss: 0.00152577
Iteration 14/25 | Loss: 0.00152590
Iteration 15/25 | Loss: 0.00152554
Iteration 16/25 | Loss: 0.00151673
Iteration 17/25 | Loss: 0.00151379
Iteration 18/25 | Loss: 0.00150530
Iteration 19/25 | Loss: 0.00149896
Iteration 20/25 | Loss: 0.00150183
Iteration 21/25 | Loss: 0.00150160
Iteration 22/25 | Loss: 0.00149613
Iteration 23/25 | Loss: 0.00149360
Iteration 24/25 | Loss: 0.00149284
Iteration 25/25 | Loss: 0.00149382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34196126
Iteration 2/25 | Loss: 0.00305804
Iteration 3/25 | Loss: 0.00284630
Iteration 4/25 | Loss: 0.00284629
Iteration 5/25 | Loss: 0.00284629
Iteration 6/25 | Loss: 0.00284629
Iteration 7/25 | Loss: 0.00284629
Iteration 8/25 | Loss: 0.00284629
Iteration 9/25 | Loss: 0.00284629
Iteration 10/25 | Loss: 0.00284629
Iteration 11/25 | Loss: 0.00284629
Iteration 12/25 | Loss: 0.00284629
Iteration 13/25 | Loss: 0.00284629
Iteration 14/25 | Loss: 0.00284629
Iteration 15/25 | Loss: 0.00284629
Iteration 16/25 | Loss: 0.00284629
Iteration 17/25 | Loss: 0.00284629
Iteration 18/25 | Loss: 0.00284629
Iteration 19/25 | Loss: 0.00284629
Iteration 20/25 | Loss: 0.00284629
Iteration 21/25 | Loss: 0.00284629
Iteration 22/25 | Loss: 0.00284629
Iteration 23/25 | Loss: 0.00284629
Iteration 24/25 | Loss: 0.00284629
Iteration 25/25 | Loss: 0.00284629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284629
Iteration 2/1000 | Loss: 0.00052290
Iteration 3/1000 | Loss: 0.00032858
Iteration 4/1000 | Loss: 0.00055407
Iteration 5/1000 | Loss: 0.00038939
Iteration 6/1000 | Loss: 0.00123977
Iteration 7/1000 | Loss: 0.00038086
Iteration 8/1000 | Loss: 0.00066339
Iteration 9/1000 | Loss: 0.00037424
Iteration 10/1000 | Loss: 0.00132462
Iteration 11/1000 | Loss: 0.00192714
Iteration 12/1000 | Loss: 0.00323237
Iteration 13/1000 | Loss: 0.00170035
Iteration 14/1000 | Loss: 0.00018583
Iteration 15/1000 | Loss: 0.00062502
Iteration 16/1000 | Loss: 0.00052329
Iteration 17/1000 | Loss: 0.00073232
Iteration 18/1000 | Loss: 0.00022732
Iteration 19/1000 | Loss: 0.00015608
Iteration 20/1000 | Loss: 0.00017906
Iteration 21/1000 | Loss: 0.00017364
Iteration 22/1000 | Loss: 0.00034740
Iteration 23/1000 | Loss: 0.00014918
Iteration 24/1000 | Loss: 0.00018429
Iteration 25/1000 | Loss: 0.00011003
Iteration 26/1000 | Loss: 0.00053795
Iteration 27/1000 | Loss: 0.00025136
Iteration 28/1000 | Loss: 0.00174221
Iteration 29/1000 | Loss: 0.00643313
Iteration 30/1000 | Loss: 0.00417811
Iteration 31/1000 | Loss: 0.00232079
Iteration 32/1000 | Loss: 0.00093332
Iteration 33/1000 | Loss: 0.00022978
Iteration 34/1000 | Loss: 0.00034091
Iteration 35/1000 | Loss: 0.00072649
Iteration 36/1000 | Loss: 0.00053152
Iteration 37/1000 | Loss: 0.00044927
Iteration 38/1000 | Loss: 0.00008926
Iteration 39/1000 | Loss: 0.00039232
Iteration 40/1000 | Loss: 0.00017318
Iteration 41/1000 | Loss: 0.00009854
Iteration 42/1000 | Loss: 0.00015312
Iteration 43/1000 | Loss: 0.00023172
Iteration 44/1000 | Loss: 0.00033415
Iteration 45/1000 | Loss: 0.00046425
Iteration 46/1000 | Loss: 0.00027499
Iteration 47/1000 | Loss: 0.00016573
Iteration 48/1000 | Loss: 0.00005652
Iteration 49/1000 | Loss: 0.00016111
Iteration 50/1000 | Loss: 0.00004262
Iteration 51/1000 | Loss: 0.00011396
Iteration 52/1000 | Loss: 0.00003362
Iteration 53/1000 | Loss: 0.00006457
Iteration 54/1000 | Loss: 0.00003983
Iteration 55/1000 | Loss: 0.00003596
Iteration 56/1000 | Loss: 0.00002717
Iteration 57/1000 | Loss: 0.00002894
Iteration 58/1000 | Loss: 0.00002686
Iteration 59/1000 | Loss: 0.00007092
Iteration 60/1000 | Loss: 0.00003737
Iteration 61/1000 | Loss: 0.00003098
Iteration 62/1000 | Loss: 0.00002597
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002111
Iteration 65/1000 | Loss: 0.00003054
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002011
Iteration 68/1000 | Loss: 0.00005200
Iteration 69/1000 | Loss: 0.00022850
Iteration 70/1000 | Loss: 0.00007247
Iteration 71/1000 | Loss: 0.00007344
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002124
Iteration 75/1000 | Loss: 0.00003172
Iteration 76/1000 | Loss: 0.00002045
Iteration 77/1000 | Loss: 0.00003069
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00001990
Iteration 80/1000 | Loss: 0.00001990
Iteration 81/1000 | Loss: 0.00001990
Iteration 82/1000 | Loss: 0.00001989
Iteration 83/1000 | Loss: 0.00001989
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001987
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001962
Iteration 88/1000 | Loss: 0.00001959
Iteration 89/1000 | Loss: 0.00002664
Iteration 90/1000 | Loss: 0.00001927
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001921
Iteration 94/1000 | Loss: 0.00001926
Iteration 95/1000 | Loss: 0.00001925
Iteration 96/1000 | Loss: 0.00001924
Iteration 97/1000 | Loss: 0.00001914
Iteration 98/1000 | Loss: 0.00001914
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001909
Iteration 120/1000 | Loss: 0.00001921
Iteration 121/1000 | Loss: 0.00001921
Iteration 122/1000 | Loss: 0.00002727
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00002178
Iteration 125/1000 | Loss: 0.00002419
Iteration 126/1000 | Loss: 0.00001889
Iteration 127/1000 | Loss: 0.00002309
Iteration 128/1000 | Loss: 0.00001960
Iteration 129/1000 | Loss: 0.00002861
Iteration 130/1000 | Loss: 0.00001937
Iteration 131/1000 | Loss: 0.00001872
Iteration 132/1000 | Loss: 0.00002532
Iteration 133/1000 | Loss: 0.00001864
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001863
Iteration 136/1000 | Loss: 0.00001862
Iteration 137/1000 | Loss: 0.00001862
Iteration 138/1000 | Loss: 0.00001862
Iteration 139/1000 | Loss: 0.00001861
Iteration 140/1000 | Loss: 0.00001860
Iteration 141/1000 | Loss: 0.00001859
Iteration 142/1000 | Loss: 0.00001859
Iteration 143/1000 | Loss: 0.00001859
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001853
Iteration 147/1000 | Loss: 0.00001853
Iteration 148/1000 | Loss: 0.00001849
Iteration 149/1000 | Loss: 0.00001844
Iteration 150/1000 | Loss: 0.00001841
Iteration 151/1000 | Loss: 0.00001841
Iteration 152/1000 | Loss: 0.00001841
Iteration 153/1000 | Loss: 0.00001841
Iteration 154/1000 | Loss: 0.00001841
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001841
Iteration 157/1000 | Loss: 0.00001841
Iteration 158/1000 | Loss: 0.00001840
Iteration 159/1000 | Loss: 0.00001840
Iteration 160/1000 | Loss: 0.00001840
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001836
Iteration 164/1000 | Loss: 0.00001836
Iteration 165/1000 | Loss: 0.00001836
Iteration 166/1000 | Loss: 0.00001836
Iteration 167/1000 | Loss: 0.00001836
Iteration 168/1000 | Loss: 0.00001836
Iteration 169/1000 | Loss: 0.00001836
Iteration 170/1000 | Loss: 0.00001836
Iteration 171/1000 | Loss: 0.00001835
Iteration 172/1000 | Loss: 0.00001835
Iteration 173/1000 | Loss: 0.00001835
Iteration 174/1000 | Loss: 0.00001835
Iteration 175/1000 | Loss: 0.00001835
Iteration 176/1000 | Loss: 0.00001835
Iteration 177/1000 | Loss: 0.00001835
Iteration 178/1000 | Loss: 0.00001835
Iteration 179/1000 | Loss: 0.00001835
Iteration 180/1000 | Loss: 0.00001834
Iteration 181/1000 | Loss: 0.00001834
Iteration 182/1000 | Loss: 0.00001834
Iteration 183/1000 | Loss: 0.00001834
Iteration 184/1000 | Loss: 0.00001834
Iteration 185/1000 | Loss: 0.00001834
Iteration 186/1000 | Loss: 0.00001834
Iteration 187/1000 | Loss: 0.00001833
Iteration 188/1000 | Loss: 0.00001833
Iteration 189/1000 | Loss: 0.00001833
Iteration 190/1000 | Loss: 0.00001833
Iteration 191/1000 | Loss: 0.00001833
Iteration 192/1000 | Loss: 0.00001833
Iteration 193/1000 | Loss: 0.00001833
Iteration 194/1000 | Loss: 0.00001833
Iteration 195/1000 | Loss: 0.00001833
Iteration 196/1000 | Loss: 0.00001833
Iteration 197/1000 | Loss: 0.00001832
Iteration 198/1000 | Loss: 0.00001832
Iteration 199/1000 | Loss: 0.00001832
Iteration 200/1000 | Loss: 0.00001832
Iteration 201/1000 | Loss: 0.00001832
Iteration 202/1000 | Loss: 0.00001832
Iteration 203/1000 | Loss: 0.00001831
Iteration 204/1000 | Loss: 0.00001831
Iteration 205/1000 | Loss: 0.00001831
Iteration 206/1000 | Loss: 0.00001830
Iteration 207/1000 | Loss: 0.00002662
Iteration 208/1000 | Loss: 0.00001831
Iteration 209/1000 | Loss: 0.00001830
Iteration 210/1000 | Loss: 0.00001830
Iteration 211/1000 | Loss: 0.00001830
Iteration 212/1000 | Loss: 0.00001830
Iteration 213/1000 | Loss: 0.00001829
Iteration 214/1000 | Loss: 0.00001829
Iteration 215/1000 | Loss: 0.00001829
Iteration 216/1000 | Loss: 0.00001829
Iteration 217/1000 | Loss: 0.00001829
Iteration 218/1000 | Loss: 0.00001829
Iteration 219/1000 | Loss: 0.00001829
Iteration 220/1000 | Loss: 0.00001829
Iteration 221/1000 | Loss: 0.00001829
Iteration 222/1000 | Loss: 0.00001829
Iteration 223/1000 | Loss: 0.00001829
Iteration 224/1000 | Loss: 0.00001829
Iteration 225/1000 | Loss: 0.00001828
Iteration 226/1000 | Loss: 0.00001828
Iteration 227/1000 | Loss: 0.00001828
Iteration 228/1000 | Loss: 0.00001828
Iteration 229/1000 | Loss: 0.00001828
Iteration 230/1000 | Loss: 0.00001828
Iteration 231/1000 | Loss: 0.00001828
Iteration 232/1000 | Loss: 0.00001828
Iteration 233/1000 | Loss: 0.00001827
Iteration 234/1000 | Loss: 0.00001827
Iteration 235/1000 | Loss: 0.00001827
Iteration 236/1000 | Loss: 0.00001827
Iteration 237/1000 | Loss: 0.00001827
Iteration 238/1000 | Loss: 0.00001827
Iteration 239/1000 | Loss: 0.00002468
Iteration 240/1000 | Loss: 0.00001828
Iteration 241/1000 | Loss: 0.00001828
Iteration 242/1000 | Loss: 0.00001826
Iteration 243/1000 | Loss: 0.00001826
Iteration 244/1000 | Loss: 0.00001826
Iteration 245/1000 | Loss: 0.00001826
Iteration 246/1000 | Loss: 0.00001826
Iteration 247/1000 | Loss: 0.00001826
Iteration 248/1000 | Loss: 0.00001825
Iteration 249/1000 | Loss: 0.00001825
Iteration 250/1000 | Loss: 0.00001825
Iteration 251/1000 | Loss: 0.00001825
Iteration 252/1000 | Loss: 0.00001825
Iteration 253/1000 | Loss: 0.00001825
Iteration 254/1000 | Loss: 0.00001825
Iteration 255/1000 | Loss: 0.00001825
Iteration 256/1000 | Loss: 0.00001825
Iteration 257/1000 | Loss: 0.00001825
Iteration 258/1000 | Loss: 0.00001825
Iteration 259/1000 | Loss: 0.00001825
Iteration 260/1000 | Loss: 0.00001824
Iteration 261/1000 | Loss: 0.00001824
Iteration 262/1000 | Loss: 0.00001824
Iteration 263/1000 | Loss: 0.00001824
Iteration 264/1000 | Loss: 0.00001824
Iteration 265/1000 | Loss: 0.00001824
Iteration 266/1000 | Loss: 0.00001824
Iteration 267/1000 | Loss: 0.00001824
Iteration 268/1000 | Loss: 0.00001824
Iteration 269/1000 | Loss: 0.00001824
Iteration 270/1000 | Loss: 0.00001824
Iteration 271/1000 | Loss: 0.00001824
Iteration 272/1000 | Loss: 0.00001824
Iteration 273/1000 | Loss: 0.00001824
Iteration 274/1000 | Loss: 0.00001824
Iteration 275/1000 | Loss: 0.00001824
Iteration 276/1000 | Loss: 0.00001824
Iteration 277/1000 | Loss: 0.00001823
Iteration 278/1000 | Loss: 0.00001823
Iteration 279/1000 | Loss: 0.00001823
Iteration 280/1000 | Loss: 0.00001823
Iteration 281/1000 | Loss: 0.00001823
Iteration 282/1000 | Loss: 0.00001823
Iteration 283/1000 | Loss: 0.00001823
Iteration 284/1000 | Loss: 0.00009492
Iteration 285/1000 | Loss: 0.00001923
Iteration 286/1000 | Loss: 0.00001844
Iteration 287/1000 | Loss: 0.00003017
Iteration 288/1000 | Loss: 0.00001750
Iteration 289/1000 | Loss: 0.00002006
Iteration 290/1000 | Loss: 0.00001686
Iteration 291/1000 | Loss: 0.00002090
Iteration 292/1000 | Loss: 0.00001678
Iteration 293/1000 | Loss: 0.00001676
Iteration 294/1000 | Loss: 0.00001675
Iteration 295/1000 | Loss: 0.00002024
Iteration 296/1000 | Loss: 0.00001667
Iteration 297/1000 | Loss: 0.00001667
Iteration 298/1000 | Loss: 0.00001667
Iteration 299/1000 | Loss: 0.00001667
Iteration 300/1000 | Loss: 0.00001667
Iteration 301/1000 | Loss: 0.00001667
Iteration 302/1000 | Loss: 0.00001667
Iteration 303/1000 | Loss: 0.00001667
Iteration 304/1000 | Loss: 0.00001667
Iteration 305/1000 | Loss: 0.00001667
Iteration 306/1000 | Loss: 0.00001667
Iteration 307/1000 | Loss: 0.00001667
Iteration 308/1000 | Loss: 0.00001667
Iteration 309/1000 | Loss: 0.00001666
Iteration 310/1000 | Loss: 0.00001666
Iteration 311/1000 | Loss: 0.00001666
Iteration 312/1000 | Loss: 0.00001665
Iteration 313/1000 | Loss: 0.00001665
Iteration 314/1000 | Loss: 0.00001664
Iteration 315/1000 | Loss: 0.00001663
Iteration 316/1000 | Loss: 0.00001663
Iteration 317/1000 | Loss: 0.00001663
Iteration 318/1000 | Loss: 0.00001663
Iteration 319/1000 | Loss: 0.00001663
Iteration 320/1000 | Loss: 0.00001663
Iteration 321/1000 | Loss: 0.00001663
Iteration 322/1000 | Loss: 0.00001662
Iteration 323/1000 | Loss: 0.00001662
Iteration 324/1000 | Loss: 0.00001662
Iteration 325/1000 | Loss: 0.00001662
Iteration 326/1000 | Loss: 0.00001662
Iteration 327/1000 | Loss: 0.00001662
Iteration 328/1000 | Loss: 0.00001662
Iteration 329/1000 | Loss: 0.00001662
Iteration 330/1000 | Loss: 0.00001661
Iteration 331/1000 | Loss: 0.00001661
Iteration 332/1000 | Loss: 0.00001660
Iteration 333/1000 | Loss: 0.00002011
Iteration 334/1000 | Loss: 0.00001662
Iteration 335/1000 | Loss: 0.00001954
Iteration 336/1000 | Loss: 0.00001864
Iteration 337/1000 | Loss: 0.00001673
Iteration 338/1000 | Loss: 0.00001659
Iteration 339/1000 | Loss: 0.00001659
Iteration 340/1000 | Loss: 0.00001659
Iteration 341/1000 | Loss: 0.00001659
Iteration 342/1000 | Loss: 0.00001659
Iteration 343/1000 | Loss: 0.00001659
Iteration 344/1000 | Loss: 0.00001659
Iteration 345/1000 | Loss: 0.00001659
Iteration 346/1000 | Loss: 0.00001659
Iteration 347/1000 | Loss: 0.00001658
Iteration 348/1000 | Loss: 0.00001661
Iteration 349/1000 | Loss: 0.00001658
Iteration 350/1000 | Loss: 0.00001658
Iteration 351/1000 | Loss: 0.00001658
Iteration 352/1000 | Loss: 0.00001658
Iteration 353/1000 | Loss: 0.00001657
Iteration 354/1000 | Loss: 0.00001657
Iteration 355/1000 | Loss: 0.00001657
Iteration 356/1000 | Loss: 0.00001657
Iteration 357/1000 | Loss: 0.00001657
Iteration 358/1000 | Loss: 0.00001657
Iteration 359/1000 | Loss: 0.00001657
Iteration 360/1000 | Loss: 0.00001657
Iteration 361/1000 | Loss: 0.00001657
Iteration 362/1000 | Loss: 0.00001657
Iteration 363/1000 | Loss: 0.00001657
Iteration 364/1000 | Loss: 0.00001657
Iteration 365/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [1.6572103049838915e-05, 1.6572103049838915e-05, 1.6572103049838915e-05, 1.6572103049838915e-05, 1.6572103049838915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6572103049838915e-05

Optimization complete. Final v2v error: 3.249119281768799 mm

Highest mean error: 10.505450248718262 mm for frame 230

Lowest mean error: 2.8076584339141846 mm for frame 233

Saving results

Total time: 242.30623483657837
