Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=4, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 224-279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_henry_posed_014/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_henry_posed_014/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029761
Iteration 2/25 | Loss: 0.00278611
Iteration 3/25 | Loss: 0.00209653
Iteration 4/25 | Loss: 0.00163118
Iteration 5/25 | Loss: 0.00159498
Iteration 6/25 | Loss: 0.00133027
Iteration 7/25 | Loss: 0.00125251
Iteration 8/25 | Loss: 0.00120934
Iteration 9/25 | Loss: 0.00119180
Iteration 10/25 | Loss: 0.00116917
Iteration 11/25 | Loss: 0.00113589
Iteration 12/25 | Loss: 0.00111945
Iteration 13/25 | Loss: 0.00108213
Iteration 14/25 | Loss: 0.00105120
Iteration 15/25 | Loss: 0.00104428
Iteration 16/25 | Loss: 0.00104068
Iteration 17/25 | Loss: 0.00103735
Iteration 18/25 | Loss: 0.00103806
Iteration 19/25 | Loss: 0.00103672
Iteration 20/25 | Loss: 0.00103736
Iteration 21/25 | Loss: 0.00103774
Iteration 22/25 | Loss: 0.00103799
Iteration 23/25 | Loss: 0.00104006
Iteration 24/25 | Loss: 0.00103789
Iteration 25/25 | Loss: 0.00103996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45149899
Iteration 2/25 | Loss: 0.00073620
Iteration 3/25 | Loss: 0.00066429
Iteration 4/25 | Loss: 0.00066429
Iteration 5/25 | Loss: 0.00066429
Iteration 6/25 | Loss: 0.00066429
Iteration 7/25 | Loss: 0.00066429
Iteration 8/25 | Loss: 0.00066429
Iteration 9/25 | Loss: 0.00066429
Iteration 10/25 | Loss: 0.00066429
Iteration 11/25 | Loss: 0.00066429
Iteration 12/25 | Loss: 0.00066429
Iteration 13/25 | Loss: 0.00066429
Iteration 14/25 | Loss: 0.00066429
Iteration 15/25 | Loss: 0.00066429
Iteration 16/25 | Loss: 0.00066429
Iteration 17/25 | Loss: 0.00066429
Iteration 18/25 | Loss: 0.00066429
Iteration 19/25 | Loss: 0.00066429
Iteration 20/25 | Loss: 0.00066429
Iteration 21/25 | Loss: 0.00066429
Iteration 22/25 | Loss: 0.00066429
Iteration 23/25 | Loss: 0.00066429
Iteration 24/25 | Loss: 0.00066429
Iteration 25/25 | Loss: 0.00066429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066429
Iteration 2/1000 | Loss: 0.00043012
Iteration 3/1000 | Loss: 0.00009803
Iteration 4/1000 | Loss: 0.00006582
Iteration 5/1000 | Loss: 0.00005717
Iteration 6/1000 | Loss: 0.00004621
Iteration 7/1000 | Loss: 0.00006277
Iteration 8/1000 | Loss: 0.00006666
Iteration 9/1000 | Loss: 0.00006309
Iteration 10/1000 | Loss: 0.00006340
Iteration 11/1000 | Loss: 0.00006473
Iteration 12/1000 | Loss: 0.00010060
Iteration 13/1000 | Loss: 0.00003673
Iteration 14/1000 | Loss: 0.00003376
Iteration 15/1000 | Loss: 0.00003235
Iteration 16/1000 | Loss: 0.00003131
Iteration 17/1000 | Loss: 0.00003071
Iteration 18/1000 | Loss: 0.00003025
Iteration 19/1000 | Loss: 0.00002985
Iteration 20/1000 | Loss: 0.00002951
Iteration 21/1000 | Loss: 0.00002930
Iteration 22/1000 | Loss: 0.00002930
Iteration 23/1000 | Loss: 0.00002930
Iteration 24/1000 | Loss: 0.00002919
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00002903
Iteration 27/1000 | Loss: 0.00002899
Iteration 28/1000 | Loss: 0.00002897
Iteration 29/1000 | Loss: 0.00002896
Iteration 30/1000 | Loss: 0.00002896
Iteration 31/1000 | Loss: 0.00002896
Iteration 32/1000 | Loss: 0.00002896
Iteration 33/1000 | Loss: 0.00002895
Iteration 34/1000 | Loss: 0.00002895
Iteration 35/1000 | Loss: 0.00002894
Iteration 36/1000 | Loss: 0.00002894
Iteration 37/1000 | Loss: 0.00002893
Iteration 38/1000 | Loss: 0.00002892
Iteration 39/1000 | Loss: 0.00002892
Iteration 40/1000 | Loss: 0.00002891
Iteration 41/1000 | Loss: 0.00002891
Iteration 42/1000 | Loss: 0.00002890
Iteration 43/1000 | Loss: 0.00002890
Iteration 44/1000 | Loss: 0.00002889
Iteration 45/1000 | Loss: 0.00002889
Iteration 46/1000 | Loss: 0.00002889
Iteration 47/1000 | Loss: 0.00002888
Iteration 48/1000 | Loss: 0.00002888
Iteration 49/1000 | Loss: 0.00002888
Iteration 50/1000 | Loss: 0.00002888
Iteration 51/1000 | Loss: 0.00002887
Iteration 52/1000 | Loss: 0.00002887
Iteration 53/1000 | Loss: 0.00002887
Iteration 54/1000 | Loss: 0.00002887
Iteration 55/1000 | Loss: 0.00002886
Iteration 56/1000 | Loss: 0.00002886
Iteration 57/1000 | Loss: 0.00002885
Iteration 58/1000 | Loss: 0.00002885
Iteration 59/1000 | Loss: 0.00002885
Iteration 60/1000 | Loss: 0.00002884
Iteration 61/1000 | Loss: 0.00002884
Iteration 62/1000 | Loss: 0.00002884
Iteration 63/1000 | Loss: 0.00002884
Iteration 64/1000 | Loss: 0.00002884
Iteration 65/1000 | Loss: 0.00002884
Iteration 66/1000 | Loss: 0.00002884
Iteration 67/1000 | Loss: 0.00002884
Iteration 68/1000 | Loss: 0.00002884
Iteration 69/1000 | Loss: 0.00002883
Iteration 70/1000 | Loss: 0.00002883
Iteration 71/1000 | Loss: 0.00002883
Iteration 72/1000 | Loss: 0.00002882
Iteration 73/1000 | Loss: 0.00002882
Iteration 74/1000 | Loss: 0.00002882
Iteration 75/1000 | Loss: 0.00002881
Iteration 76/1000 | Loss: 0.00002881
Iteration 77/1000 | Loss: 0.00002880
Iteration 78/1000 | Loss: 0.00002880
Iteration 79/1000 | Loss: 0.00002879
Iteration 80/1000 | Loss: 0.00002879
Iteration 81/1000 | Loss: 0.00002879
Iteration 82/1000 | Loss: 0.00002878
Iteration 83/1000 | Loss: 0.00002878
Iteration 84/1000 | Loss: 0.00002877
Iteration 85/1000 | Loss: 0.00002877
Iteration 86/1000 | Loss: 0.00002876
Iteration 87/1000 | Loss: 0.00002876
Iteration 88/1000 | Loss: 0.00002876
Iteration 89/1000 | Loss: 0.00002876
Iteration 90/1000 | Loss: 0.00002875
Iteration 91/1000 | Loss: 0.00002875
Iteration 92/1000 | Loss: 0.00002875
Iteration 93/1000 | Loss: 0.00002875
Iteration 94/1000 | Loss: 0.00002875
Iteration 95/1000 | Loss: 0.00002875
Iteration 96/1000 | Loss: 0.00002874
Iteration 97/1000 | Loss: 0.00002874
Iteration 98/1000 | Loss: 0.00002874
Iteration 99/1000 | Loss: 0.00002874
Iteration 100/1000 | Loss: 0.00002874
Iteration 101/1000 | Loss: 0.00002873
Iteration 102/1000 | Loss: 0.00002873
Iteration 103/1000 | Loss: 0.00002873
Iteration 104/1000 | Loss: 0.00002873
Iteration 105/1000 | Loss: 0.00002873
Iteration 106/1000 | Loss: 0.00002873
Iteration 107/1000 | Loss: 0.00002873
Iteration 108/1000 | Loss: 0.00002873
Iteration 109/1000 | Loss: 0.00002872
Iteration 110/1000 | Loss: 0.00002872
Iteration 111/1000 | Loss: 0.00002872
Iteration 112/1000 | Loss: 0.00002872
Iteration 113/1000 | Loss: 0.00002872
Iteration 114/1000 | Loss: 0.00002872
Iteration 115/1000 | Loss: 0.00002872
Iteration 116/1000 | Loss: 0.00002872
Iteration 117/1000 | Loss: 0.00002872
Iteration 118/1000 | Loss: 0.00002871
Iteration 119/1000 | Loss: 0.00002871
Iteration 120/1000 | Loss: 0.00002871
Iteration 121/1000 | Loss: 0.00002871
Iteration 122/1000 | Loss: 0.00002871
Iteration 123/1000 | Loss: 0.00002871
Iteration 124/1000 | Loss: 0.00002870
Iteration 125/1000 | Loss: 0.00002870
Iteration 126/1000 | Loss: 0.00002870
Iteration 127/1000 | Loss: 0.00002870
Iteration 128/1000 | Loss: 0.00002870
Iteration 129/1000 | Loss: 0.00002870
Iteration 130/1000 | Loss: 0.00002870
Iteration 131/1000 | Loss: 0.00002870
Iteration 132/1000 | Loss: 0.00002870
Iteration 133/1000 | Loss: 0.00002870
Iteration 134/1000 | Loss: 0.00002870
Iteration 135/1000 | Loss: 0.00002870
Iteration 136/1000 | Loss: 0.00002870
Iteration 137/1000 | Loss: 0.00002870
Iteration 138/1000 | Loss: 0.00002870
Iteration 139/1000 | Loss: 0.00002870
Iteration 140/1000 | Loss: 0.00002870
Iteration 141/1000 | Loss: 0.00002870
Iteration 142/1000 | Loss: 0.00002870
Iteration 143/1000 | Loss: 0.00002870
Iteration 144/1000 | Loss: 0.00002870
Iteration 145/1000 | Loss: 0.00002869
Iteration 146/1000 | Loss: 0.00002869
Iteration 147/1000 | Loss: 0.00002869
Iteration 148/1000 | Loss: 0.00002869
Iteration 149/1000 | Loss: 0.00002869
Iteration 150/1000 | Loss: 0.00002869
Iteration 151/1000 | Loss: 0.00002869
Iteration 152/1000 | Loss: 0.00002869
Iteration 153/1000 | Loss: 0.00002869
Iteration 154/1000 | Loss: 0.00002869
Iteration 155/1000 | Loss: 0.00002869
Iteration 156/1000 | Loss: 0.00002869
Iteration 157/1000 | Loss: 0.00002869
Iteration 158/1000 | Loss: 0.00002869
Iteration 159/1000 | Loss: 0.00002869
Iteration 160/1000 | Loss: 0.00002869
Iteration 161/1000 | Loss: 0.00002869
Iteration 162/1000 | Loss: 0.00002869
Iteration 163/1000 | Loss: 0.00002869
Iteration 164/1000 | Loss: 0.00002869
Iteration 165/1000 | Loss: 0.00002869
Iteration 166/1000 | Loss: 0.00002869
Iteration 167/1000 | Loss: 0.00002869
Iteration 168/1000 | Loss: 0.00002869
Iteration 169/1000 | Loss: 0.00002869
Iteration 170/1000 | Loss: 0.00002869
Iteration 171/1000 | Loss: 0.00002869
Iteration 172/1000 | Loss: 0.00002869
Iteration 173/1000 | Loss: 0.00002869
Iteration 174/1000 | Loss: 0.00002869
Iteration 175/1000 | Loss: 0.00002869
Iteration 176/1000 | Loss: 0.00002869
Iteration 177/1000 | Loss: 0.00002869
Iteration 178/1000 | Loss: 0.00002869
Iteration 179/1000 | Loss: 0.00002869
Iteration 180/1000 | Loss: 0.00002869
Iteration 181/1000 | Loss: 0.00002869
Iteration 182/1000 | Loss: 0.00002869
Iteration 183/1000 | Loss: 0.00002869
Iteration 184/1000 | Loss: 0.00002869
Iteration 185/1000 | Loss: 0.00002869
Iteration 186/1000 | Loss: 0.00002869
Iteration 187/1000 | Loss: 0.00002869
Iteration 188/1000 | Loss: 0.00002869
Iteration 189/1000 | Loss: 0.00002869
Iteration 190/1000 | Loss: 0.00002869
Iteration 191/1000 | Loss: 0.00002869
Iteration 192/1000 | Loss: 0.00002869
Iteration 193/1000 | Loss: 0.00002869
Iteration 194/1000 | Loss: 0.00002869
Iteration 195/1000 | Loss: 0.00002869
Iteration 196/1000 | Loss: 0.00002869
Iteration 197/1000 | Loss: 0.00002869
Iteration 198/1000 | Loss: 0.00002869
Iteration 199/1000 | Loss: 0.00002869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.8688500606222078e-05, 2.8688500606222078e-05, 2.8688500606222078e-05, 2.8688500606222078e-05, 2.8688500606222078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8688500606222078e-05

Optimization complete. Final v2v error: 4.523630142211914 mm

Highest mean error: 5.267351150512695 mm for frame 192

Lowest mean error: 4.2492828369140625 mm for frame 52

Saving results

Total time: 275.66662526130676
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858937
Iteration 2/25 | Loss: 0.00122627
Iteration 3/25 | Loss: 0.00111371
Iteration 4/25 | Loss: 0.00108977
Iteration 5/25 | Loss: 0.00108124
Iteration 6/25 | Loss: 0.00107941
Iteration 7/25 | Loss: 0.00107927
Iteration 8/25 | Loss: 0.00107927
Iteration 9/25 | Loss: 0.00107927
Iteration 10/25 | Loss: 0.00107927
Iteration 11/25 | Loss: 0.00107927
Iteration 12/25 | Loss: 0.00107927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010792652610689402, 0.0010792652610689402, 0.0010792652610689402, 0.0010792652610689402, 0.0010792652610689402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010792652610689402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23524249
Iteration 2/25 | Loss: 0.00153022
Iteration 3/25 | Loss: 0.00153021
Iteration 4/25 | Loss: 0.00153021
Iteration 5/25 | Loss: 0.00153021
Iteration 6/25 | Loss: 0.00153021
Iteration 7/25 | Loss: 0.00153021
Iteration 8/25 | Loss: 0.00153021
Iteration 9/25 | Loss: 0.00153021
Iteration 10/25 | Loss: 0.00153021
Iteration 11/25 | Loss: 0.00153021
Iteration 12/25 | Loss: 0.00153021
Iteration 13/25 | Loss: 0.00153021
Iteration 14/25 | Loss: 0.00153021
Iteration 15/25 | Loss: 0.00153021
Iteration 16/25 | Loss: 0.00153021
Iteration 17/25 | Loss: 0.00153021
Iteration 18/25 | Loss: 0.00153021
Iteration 19/25 | Loss: 0.00153021
Iteration 20/25 | Loss: 0.00153021
Iteration 21/25 | Loss: 0.00153021
Iteration 22/25 | Loss: 0.00153021
Iteration 23/25 | Loss: 0.00153021
Iteration 24/25 | Loss: 0.00153021
Iteration 25/25 | Loss: 0.00153021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153021
Iteration 2/1000 | Loss: 0.00004920
Iteration 3/1000 | Loss: 0.00003015
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002459
Iteration 7/1000 | Loss: 0.00002388
Iteration 8/1000 | Loss: 0.00002352
Iteration 9/1000 | Loss: 0.00002325
Iteration 10/1000 | Loss: 0.00002306
Iteration 11/1000 | Loss: 0.00002286
Iteration 12/1000 | Loss: 0.00002285
Iteration 13/1000 | Loss: 0.00002276
Iteration 14/1000 | Loss: 0.00002267
Iteration 15/1000 | Loss: 0.00002266
Iteration 16/1000 | Loss: 0.00002264
Iteration 17/1000 | Loss: 0.00002263
Iteration 18/1000 | Loss: 0.00002263
Iteration 19/1000 | Loss: 0.00002263
Iteration 20/1000 | Loss: 0.00002263
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002262
Iteration 23/1000 | Loss: 0.00002262
Iteration 24/1000 | Loss: 0.00002262
Iteration 25/1000 | Loss: 0.00002261
Iteration 26/1000 | Loss: 0.00002261
Iteration 27/1000 | Loss: 0.00002261
Iteration 28/1000 | Loss: 0.00002260
Iteration 29/1000 | Loss: 0.00002260
Iteration 30/1000 | Loss: 0.00002260
Iteration 31/1000 | Loss: 0.00002259
Iteration 32/1000 | Loss: 0.00002259
Iteration 33/1000 | Loss: 0.00002259
Iteration 34/1000 | Loss: 0.00002259
Iteration 35/1000 | Loss: 0.00002259
Iteration 36/1000 | Loss: 0.00002259
Iteration 37/1000 | Loss: 0.00002258
Iteration 38/1000 | Loss: 0.00002258
Iteration 39/1000 | Loss: 0.00002258
Iteration 40/1000 | Loss: 0.00002258
Iteration 41/1000 | Loss: 0.00002256
Iteration 42/1000 | Loss: 0.00002256
Iteration 43/1000 | Loss: 0.00002256
Iteration 44/1000 | Loss: 0.00002256
Iteration 45/1000 | Loss: 0.00002256
Iteration 46/1000 | Loss: 0.00002256
Iteration 47/1000 | Loss: 0.00002256
Iteration 48/1000 | Loss: 0.00002256
Iteration 49/1000 | Loss: 0.00002256
Iteration 50/1000 | Loss: 0.00002256
Iteration 51/1000 | Loss: 0.00002256
Iteration 52/1000 | Loss: 0.00002256
Iteration 53/1000 | Loss: 0.00002255
Iteration 54/1000 | Loss: 0.00002255
Iteration 55/1000 | Loss: 0.00002255
Iteration 56/1000 | Loss: 0.00002255
Iteration 57/1000 | Loss: 0.00002255
Iteration 58/1000 | Loss: 0.00002255
Iteration 59/1000 | Loss: 0.00002254
Iteration 60/1000 | Loss: 0.00002254
Iteration 61/1000 | Loss: 0.00002253
Iteration 62/1000 | Loss: 0.00002253
Iteration 63/1000 | Loss: 0.00002253
Iteration 64/1000 | Loss: 0.00002252
Iteration 65/1000 | Loss: 0.00002252
Iteration 66/1000 | Loss: 0.00002252
Iteration 67/1000 | Loss: 0.00002252
Iteration 68/1000 | Loss: 0.00002251
Iteration 69/1000 | Loss: 0.00002251
Iteration 70/1000 | Loss: 0.00002251
Iteration 71/1000 | Loss: 0.00002251
Iteration 72/1000 | Loss: 0.00002251
Iteration 73/1000 | Loss: 0.00002251
Iteration 74/1000 | Loss: 0.00002251
Iteration 75/1000 | Loss: 0.00002251
Iteration 76/1000 | Loss: 0.00002251
Iteration 77/1000 | Loss: 0.00002250
Iteration 78/1000 | Loss: 0.00002250
Iteration 79/1000 | Loss: 0.00002250
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00002250
Iteration 82/1000 | Loss: 0.00002250
Iteration 83/1000 | Loss: 0.00002250
Iteration 84/1000 | Loss: 0.00002250
Iteration 85/1000 | Loss: 0.00002250
Iteration 86/1000 | Loss: 0.00002249
Iteration 87/1000 | Loss: 0.00002249
Iteration 88/1000 | Loss: 0.00002249
Iteration 89/1000 | Loss: 0.00002249
Iteration 90/1000 | Loss: 0.00002249
Iteration 91/1000 | Loss: 0.00002249
Iteration 92/1000 | Loss: 0.00002249
Iteration 93/1000 | Loss: 0.00002249
Iteration 94/1000 | Loss: 0.00002249
Iteration 95/1000 | Loss: 0.00002249
Iteration 96/1000 | Loss: 0.00002249
Iteration 97/1000 | Loss: 0.00002249
Iteration 98/1000 | Loss: 0.00002249
Iteration 99/1000 | Loss: 0.00002249
Iteration 100/1000 | Loss: 0.00002248
Iteration 101/1000 | Loss: 0.00002248
Iteration 102/1000 | Loss: 0.00002248
Iteration 103/1000 | Loss: 0.00002248
Iteration 104/1000 | Loss: 0.00002248
Iteration 105/1000 | Loss: 0.00002248
Iteration 106/1000 | Loss: 0.00002248
Iteration 107/1000 | Loss: 0.00002248
Iteration 108/1000 | Loss: 0.00002248
Iteration 109/1000 | Loss: 0.00002247
Iteration 110/1000 | Loss: 0.00002247
Iteration 111/1000 | Loss: 0.00002247
Iteration 112/1000 | Loss: 0.00002247
Iteration 113/1000 | Loss: 0.00002247
Iteration 114/1000 | Loss: 0.00002247
Iteration 115/1000 | Loss: 0.00002247
Iteration 116/1000 | Loss: 0.00002246
Iteration 117/1000 | Loss: 0.00002246
Iteration 118/1000 | Loss: 0.00002246
Iteration 119/1000 | Loss: 0.00002246
Iteration 120/1000 | Loss: 0.00002246
Iteration 121/1000 | Loss: 0.00002246
Iteration 122/1000 | Loss: 0.00002246
Iteration 123/1000 | Loss: 0.00002245
Iteration 124/1000 | Loss: 0.00002245
Iteration 125/1000 | Loss: 0.00002245
Iteration 126/1000 | Loss: 0.00002245
Iteration 127/1000 | Loss: 0.00002245
Iteration 128/1000 | Loss: 0.00002245
Iteration 129/1000 | Loss: 0.00002245
Iteration 130/1000 | Loss: 0.00002245
Iteration 131/1000 | Loss: 0.00002245
Iteration 132/1000 | Loss: 0.00002244
Iteration 133/1000 | Loss: 0.00002244
Iteration 134/1000 | Loss: 0.00002244
Iteration 135/1000 | Loss: 0.00002243
Iteration 136/1000 | Loss: 0.00002243
Iteration 137/1000 | Loss: 0.00002243
Iteration 138/1000 | Loss: 0.00002243
Iteration 139/1000 | Loss: 0.00002243
Iteration 140/1000 | Loss: 0.00002243
Iteration 141/1000 | Loss: 0.00002242
Iteration 142/1000 | Loss: 0.00002242
Iteration 143/1000 | Loss: 0.00002242
Iteration 144/1000 | Loss: 0.00002242
Iteration 145/1000 | Loss: 0.00002241
Iteration 146/1000 | Loss: 0.00002241
Iteration 147/1000 | Loss: 0.00002241
Iteration 148/1000 | Loss: 0.00002241
Iteration 149/1000 | Loss: 0.00002241
Iteration 150/1000 | Loss: 0.00002241
Iteration 151/1000 | Loss: 0.00002241
Iteration 152/1000 | Loss: 0.00002241
Iteration 153/1000 | Loss: 0.00002241
Iteration 154/1000 | Loss: 0.00002241
Iteration 155/1000 | Loss: 0.00002241
Iteration 156/1000 | Loss: 0.00002241
Iteration 157/1000 | Loss: 0.00002241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.240823960164562e-05, 2.240823960164562e-05, 2.240823960164562e-05, 2.240823960164562e-05, 2.240823960164562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.240823960164562e-05

Optimization complete. Final v2v error: 4.090918064117432 mm

Highest mean error: 4.419010162353516 mm for frame 89

Lowest mean error: 3.8866779804229736 mm for frame 37

Saving results

Total time: 80.65910315513611
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549229
Iteration 2/25 | Loss: 0.00130796
Iteration 3/25 | Loss: 0.00107837
Iteration 4/25 | Loss: 0.00105556
Iteration 5/25 | Loss: 0.00104773
Iteration 6/25 | Loss: 0.00104637
Iteration 7/25 | Loss: 0.00104611
Iteration 8/25 | Loss: 0.00104611
Iteration 9/25 | Loss: 0.00104611
Iteration 10/25 | Loss: 0.00104611
Iteration 11/25 | Loss: 0.00104611
Iteration 12/25 | Loss: 0.00104611
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010461114579811692, 0.0010461114579811692, 0.0010461114579811692, 0.0010461114579811692, 0.0010461114579811692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010461114579811692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66636819
Iteration 2/25 | Loss: 0.00085276
Iteration 3/25 | Loss: 0.00085276
Iteration 4/25 | Loss: 0.00085276
Iteration 5/25 | Loss: 0.00085276
Iteration 6/25 | Loss: 0.00085276
Iteration 7/25 | Loss: 0.00085276
Iteration 8/25 | Loss: 0.00085276
Iteration 9/25 | Loss: 0.00085275
Iteration 10/25 | Loss: 0.00085275
Iteration 11/25 | Loss: 0.00085275
Iteration 12/25 | Loss: 0.00085275
Iteration 13/25 | Loss: 0.00085275
Iteration 14/25 | Loss: 0.00085275
Iteration 15/25 | Loss: 0.00085275
Iteration 16/25 | Loss: 0.00085275
Iteration 17/25 | Loss: 0.00085275
Iteration 18/25 | Loss: 0.00085275
Iteration 19/25 | Loss: 0.00085275
Iteration 20/25 | Loss: 0.00085275
Iteration 21/25 | Loss: 0.00085275
Iteration 22/25 | Loss: 0.00085275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008527542813681066, 0.0008527542813681066, 0.0008527542813681066, 0.0008527542813681066, 0.0008527542813681066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008527542813681066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085275
Iteration 2/1000 | Loss: 0.00005454
Iteration 3/1000 | Loss: 0.00002950
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002268
Iteration 6/1000 | Loss: 0.00002135
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002029
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00001979
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001942
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001937
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001934
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001934
Iteration 19/1000 | Loss: 0.00001934
Iteration 20/1000 | Loss: 0.00001933
Iteration 21/1000 | Loss: 0.00001933
Iteration 22/1000 | Loss: 0.00001932
Iteration 23/1000 | Loss: 0.00001932
Iteration 24/1000 | Loss: 0.00001930
Iteration 25/1000 | Loss: 0.00001930
Iteration 26/1000 | Loss: 0.00001930
Iteration 27/1000 | Loss: 0.00001930
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001930
Iteration 30/1000 | Loss: 0.00001930
Iteration 31/1000 | Loss: 0.00001930
Iteration 32/1000 | Loss: 0.00001930
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001930
Iteration 35/1000 | Loss: 0.00001929
Iteration 36/1000 | Loss: 0.00001929
Iteration 37/1000 | Loss: 0.00001929
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001929
Iteration 48/1000 | Loss: 0.00001929
Iteration 49/1000 | Loss: 0.00001929
Iteration 50/1000 | Loss: 0.00001928
Iteration 51/1000 | Loss: 0.00001928
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001928
Iteration 57/1000 | Loss: 0.00001928
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001927
Iteration 65/1000 | Loss: 0.00001927
Iteration 66/1000 | Loss: 0.00001927
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001926
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001925
Iteration 75/1000 | Loss: 0.00001924
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001923
Iteration 85/1000 | Loss: 0.00001923
Iteration 86/1000 | Loss: 0.00001923
Iteration 87/1000 | Loss: 0.00001923
Iteration 88/1000 | Loss: 0.00001923
Iteration 89/1000 | Loss: 0.00001922
Iteration 90/1000 | Loss: 0.00001922
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001922
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00001921
Iteration 98/1000 | Loss: 0.00001921
Iteration 99/1000 | Loss: 0.00001921
Iteration 100/1000 | Loss: 0.00001921
Iteration 101/1000 | Loss: 0.00001921
Iteration 102/1000 | Loss: 0.00001921
Iteration 103/1000 | Loss: 0.00001921
Iteration 104/1000 | Loss: 0.00001921
Iteration 105/1000 | Loss: 0.00001921
Iteration 106/1000 | Loss: 0.00001920
Iteration 107/1000 | Loss: 0.00001920
Iteration 108/1000 | Loss: 0.00001920
Iteration 109/1000 | Loss: 0.00001919
Iteration 110/1000 | Loss: 0.00001919
Iteration 111/1000 | Loss: 0.00001919
Iteration 112/1000 | Loss: 0.00001919
Iteration 113/1000 | Loss: 0.00001919
Iteration 114/1000 | Loss: 0.00001919
Iteration 115/1000 | Loss: 0.00001919
Iteration 116/1000 | Loss: 0.00001919
Iteration 117/1000 | Loss: 0.00001919
Iteration 118/1000 | Loss: 0.00001919
Iteration 119/1000 | Loss: 0.00001918
Iteration 120/1000 | Loss: 0.00001918
Iteration 121/1000 | Loss: 0.00001918
Iteration 122/1000 | Loss: 0.00001918
Iteration 123/1000 | Loss: 0.00001918
Iteration 124/1000 | Loss: 0.00001918
Iteration 125/1000 | Loss: 0.00001918
Iteration 126/1000 | Loss: 0.00001918
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001917
Iteration 129/1000 | Loss: 0.00001917
Iteration 130/1000 | Loss: 0.00001917
Iteration 131/1000 | Loss: 0.00001917
Iteration 132/1000 | Loss: 0.00001917
Iteration 133/1000 | Loss: 0.00001917
Iteration 134/1000 | Loss: 0.00001917
Iteration 135/1000 | Loss: 0.00001917
Iteration 136/1000 | Loss: 0.00001917
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001916
Iteration 139/1000 | Loss: 0.00001916
Iteration 140/1000 | Loss: 0.00001916
Iteration 141/1000 | Loss: 0.00001916
Iteration 142/1000 | Loss: 0.00001916
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001915
Iteration 153/1000 | Loss: 0.00001915
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001914
Iteration 159/1000 | Loss: 0.00001914
Iteration 160/1000 | Loss: 0.00001914
Iteration 161/1000 | Loss: 0.00001914
Iteration 162/1000 | Loss: 0.00001914
Iteration 163/1000 | Loss: 0.00001914
Iteration 164/1000 | Loss: 0.00001914
Iteration 165/1000 | Loss: 0.00001914
Iteration 166/1000 | Loss: 0.00001913
Iteration 167/1000 | Loss: 0.00001913
Iteration 168/1000 | Loss: 0.00001913
Iteration 169/1000 | Loss: 0.00001913
Iteration 170/1000 | Loss: 0.00001913
Iteration 171/1000 | Loss: 0.00001913
Iteration 172/1000 | Loss: 0.00001913
Iteration 173/1000 | Loss: 0.00001912
Iteration 174/1000 | Loss: 0.00001912
Iteration 175/1000 | Loss: 0.00001912
Iteration 176/1000 | Loss: 0.00001912
Iteration 177/1000 | Loss: 0.00001912
Iteration 178/1000 | Loss: 0.00001912
Iteration 179/1000 | Loss: 0.00001912
Iteration 180/1000 | Loss: 0.00001912
Iteration 181/1000 | Loss: 0.00001912
Iteration 182/1000 | Loss: 0.00001912
Iteration 183/1000 | Loss: 0.00001912
Iteration 184/1000 | Loss: 0.00001912
Iteration 185/1000 | Loss: 0.00001912
Iteration 186/1000 | Loss: 0.00001912
Iteration 187/1000 | Loss: 0.00001912
Iteration 188/1000 | Loss: 0.00001912
Iteration 189/1000 | Loss: 0.00001912
Iteration 190/1000 | Loss: 0.00001912
Iteration 191/1000 | Loss: 0.00001912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.9117398551316e-05, 1.9117398551316e-05, 1.9117398551316e-05, 1.9117398551316e-05, 1.9117398551316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9117398551316e-05

Optimization complete. Final v2v error: 3.7513234615325928 mm

Highest mean error: 4.652811050415039 mm for frame 8

Lowest mean error: 3.462303876876831 mm for frame 123

Saving results

Total time: 87.16542196273804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01139974
Iteration 2/25 | Loss: 0.00160626
Iteration 3/25 | Loss: 0.00132380
Iteration 4/25 | Loss: 0.00122657
Iteration 5/25 | Loss: 0.00116004
Iteration 6/25 | Loss: 0.00115451
Iteration 7/25 | Loss: 0.00116345
Iteration 8/25 | Loss: 0.00115621
Iteration 9/25 | Loss: 0.00113628
Iteration 10/25 | Loss: 0.00113691
Iteration 11/25 | Loss: 0.00113126
Iteration 12/25 | Loss: 0.00113433
Iteration 13/25 | Loss: 0.00113782
Iteration 14/25 | Loss: 0.00112935
Iteration 15/25 | Loss: 0.00112555
Iteration 16/25 | Loss: 0.00112168
Iteration 17/25 | Loss: 0.00112489
Iteration 18/25 | Loss: 0.00113110
Iteration 19/25 | Loss: 0.00112661
Iteration 20/25 | Loss: 0.00112787
Iteration 21/25 | Loss: 0.00114364
Iteration 22/25 | Loss: 0.00113381
Iteration 23/25 | Loss: 0.00112369
Iteration 24/25 | Loss: 0.00112438
Iteration 25/25 | Loss: 0.00111744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13414562
Iteration 2/25 | Loss: 0.00170182
Iteration 3/25 | Loss: 0.00170182
Iteration 4/25 | Loss: 0.00170182
Iteration 5/25 | Loss: 0.00170182
Iteration 6/25 | Loss: 0.00170182
Iteration 7/25 | Loss: 0.00170182
Iteration 8/25 | Loss: 0.00170182
Iteration 9/25 | Loss: 0.00170182
Iteration 10/25 | Loss: 0.00170182
Iteration 11/25 | Loss: 0.00170182
Iteration 12/25 | Loss: 0.00170182
Iteration 13/25 | Loss: 0.00170182
Iteration 14/25 | Loss: 0.00170182
Iteration 15/25 | Loss: 0.00170182
Iteration 16/25 | Loss: 0.00170182
Iteration 17/25 | Loss: 0.00170182
Iteration 18/25 | Loss: 0.00170182
Iteration 19/25 | Loss: 0.00170182
Iteration 20/25 | Loss: 0.00170182
Iteration 21/25 | Loss: 0.00170182
Iteration 22/25 | Loss: 0.00170182
Iteration 23/25 | Loss: 0.00170182
Iteration 24/25 | Loss: 0.00170182
Iteration 25/25 | Loss: 0.00170182

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170182
Iteration 2/1000 | Loss: 0.00023799
Iteration 3/1000 | Loss: 0.00039784
Iteration 4/1000 | Loss: 0.00013343
Iteration 5/1000 | Loss: 0.00024532
Iteration 6/1000 | Loss: 0.00025648
Iteration 7/1000 | Loss: 0.00055737
Iteration 8/1000 | Loss: 0.00064306
Iteration 9/1000 | Loss: 0.00080981
Iteration 10/1000 | Loss: 0.00130021
Iteration 11/1000 | Loss: 0.00064292
Iteration 12/1000 | Loss: 0.00080042
Iteration 13/1000 | Loss: 0.00062779
Iteration 14/1000 | Loss: 0.00110569
Iteration 15/1000 | Loss: 0.00063837
Iteration 16/1000 | Loss: 0.00111524
Iteration 17/1000 | Loss: 0.00070808
Iteration 18/1000 | Loss: 0.00022873
Iteration 19/1000 | Loss: 0.00029212
Iteration 20/1000 | Loss: 0.00036396
Iteration 21/1000 | Loss: 0.00072953
Iteration 22/1000 | Loss: 0.00005416
Iteration 23/1000 | Loss: 0.00005008
Iteration 24/1000 | Loss: 0.00004950
Iteration 25/1000 | Loss: 0.00007596
Iteration 26/1000 | Loss: 0.00007779
Iteration 27/1000 | Loss: 0.00005736
Iteration 28/1000 | Loss: 0.00007190
Iteration 29/1000 | Loss: 0.00005298
Iteration 30/1000 | Loss: 0.00004334
Iteration 31/1000 | Loss: 0.00006307
Iteration 32/1000 | Loss: 0.00005051
Iteration 33/1000 | Loss: 0.00005018
Iteration 34/1000 | Loss: 0.00004532
Iteration 35/1000 | Loss: 0.00003870
Iteration 36/1000 | Loss: 0.00006446
Iteration 37/1000 | Loss: 0.00009728
Iteration 38/1000 | Loss: 0.00113455
Iteration 39/1000 | Loss: 0.00018707
Iteration 40/1000 | Loss: 0.00006777
Iteration 41/1000 | Loss: 0.00006924
Iteration 42/1000 | Loss: 0.00063310
Iteration 43/1000 | Loss: 0.00030447
Iteration 44/1000 | Loss: 0.00017905
Iteration 45/1000 | Loss: 0.00006603
Iteration 46/1000 | Loss: 0.00005812
Iteration 47/1000 | Loss: 0.00106627
Iteration 48/1000 | Loss: 0.00010009
Iteration 49/1000 | Loss: 0.00005323
Iteration 50/1000 | Loss: 0.00030305
Iteration 51/1000 | Loss: 0.00008307
Iteration 52/1000 | Loss: 0.00012258
Iteration 53/1000 | Loss: 0.00006762
Iteration 54/1000 | Loss: 0.00029138
Iteration 55/1000 | Loss: 0.00009458
Iteration 56/1000 | Loss: 0.00007330
Iteration 57/1000 | Loss: 0.00026570
Iteration 58/1000 | Loss: 0.00005617
Iteration 59/1000 | Loss: 0.00003828
Iteration 60/1000 | Loss: 0.00017038
Iteration 61/1000 | Loss: 0.00011941
Iteration 62/1000 | Loss: 0.00013911
Iteration 63/1000 | Loss: 0.00004583
Iteration 64/1000 | Loss: 0.00005672
Iteration 65/1000 | Loss: 0.00057776
Iteration 66/1000 | Loss: 0.00023792
Iteration 67/1000 | Loss: 0.00011368
Iteration 68/1000 | Loss: 0.00005324
Iteration 69/1000 | Loss: 0.00003651
Iteration 70/1000 | Loss: 0.00005741
Iteration 71/1000 | Loss: 0.00004626
Iteration 72/1000 | Loss: 0.00011614
Iteration 73/1000 | Loss: 0.00004734
Iteration 74/1000 | Loss: 0.00004564
Iteration 75/1000 | Loss: 0.00003208
Iteration 76/1000 | Loss: 0.00004717
Iteration 77/1000 | Loss: 0.00004209
Iteration 78/1000 | Loss: 0.00021813
Iteration 79/1000 | Loss: 0.00004390
Iteration 80/1000 | Loss: 0.00005058
Iteration 81/1000 | Loss: 0.00004554
Iteration 82/1000 | Loss: 0.00004210
Iteration 83/1000 | Loss: 0.00009881
Iteration 84/1000 | Loss: 0.00004088
Iteration 85/1000 | Loss: 0.00003064
Iteration 86/1000 | Loss: 0.00012752
Iteration 87/1000 | Loss: 0.00012084
Iteration 88/1000 | Loss: 0.00004978
Iteration 89/1000 | Loss: 0.00004366
Iteration 90/1000 | Loss: 0.00010886
Iteration 91/1000 | Loss: 0.00003304
Iteration 92/1000 | Loss: 0.00005020
Iteration 93/1000 | Loss: 0.00005020
Iteration 94/1000 | Loss: 0.00008480
Iteration 95/1000 | Loss: 0.00006800
Iteration 96/1000 | Loss: 0.00003286
Iteration 97/1000 | Loss: 0.00003841
Iteration 98/1000 | Loss: 0.00003225
Iteration 99/1000 | Loss: 0.00003181
Iteration 100/1000 | Loss: 0.00003013
Iteration 101/1000 | Loss: 0.00003013
Iteration 102/1000 | Loss: 0.00003013
Iteration 103/1000 | Loss: 0.00003013
Iteration 104/1000 | Loss: 0.00003013
Iteration 105/1000 | Loss: 0.00003012
Iteration 106/1000 | Loss: 0.00003012
Iteration 107/1000 | Loss: 0.00003012
Iteration 108/1000 | Loss: 0.00003011
Iteration 109/1000 | Loss: 0.00003038
Iteration 110/1000 | Loss: 0.00003468
Iteration 111/1000 | Loss: 0.00005698
Iteration 112/1000 | Loss: 0.00003325
Iteration 113/1000 | Loss: 0.00003251
Iteration 114/1000 | Loss: 0.00006856
Iteration 115/1000 | Loss: 0.00003281
Iteration 116/1000 | Loss: 0.00005302
Iteration 117/1000 | Loss: 0.00003784
Iteration 118/1000 | Loss: 0.00004458
Iteration 119/1000 | Loss: 0.00003552
Iteration 120/1000 | Loss: 0.00003211
Iteration 121/1000 | Loss: 0.00002990
Iteration 122/1000 | Loss: 0.00002990
Iteration 123/1000 | Loss: 0.00002990
Iteration 124/1000 | Loss: 0.00003848
Iteration 125/1000 | Loss: 0.00003848
Iteration 126/1000 | Loss: 0.00015604
Iteration 127/1000 | Loss: 0.00010551
Iteration 128/1000 | Loss: 0.00004032
Iteration 129/1000 | Loss: 0.00003215
Iteration 130/1000 | Loss: 0.00004221
Iteration 131/1000 | Loss: 0.00004212
Iteration 132/1000 | Loss: 0.00003154
Iteration 133/1000 | Loss: 0.00003129
Iteration 134/1000 | Loss: 0.00003158
Iteration 135/1000 | Loss: 0.00004465
Iteration 136/1000 | Loss: 0.00009479
Iteration 137/1000 | Loss: 0.00003381
Iteration 138/1000 | Loss: 0.00004851
Iteration 139/1000 | Loss: 0.00008225
Iteration 140/1000 | Loss: 0.00007681
Iteration 141/1000 | Loss: 0.00003466
Iteration 142/1000 | Loss: 0.00003114
Iteration 143/1000 | Loss: 0.00003642
Iteration 144/1000 | Loss: 0.00004171
Iteration 145/1000 | Loss: 0.00003250
Iteration 146/1000 | Loss: 0.00003232
Iteration 147/1000 | Loss: 0.00003942
Iteration 148/1000 | Loss: 0.00003943
Iteration 149/1000 | Loss: 0.00008349
Iteration 150/1000 | Loss: 0.00007789
Iteration 151/1000 | Loss: 0.00003062
Iteration 152/1000 | Loss: 0.00003110
Iteration 153/1000 | Loss: 0.00004449
Iteration 154/1000 | Loss: 0.00003297
Iteration 155/1000 | Loss: 0.00005688
Iteration 156/1000 | Loss: 0.00003090
Iteration 157/1000 | Loss: 0.00003450
Iteration 158/1000 | Loss: 0.00003201
Iteration 159/1000 | Loss: 0.00003061
Iteration 160/1000 | Loss: 0.00003005
Iteration 161/1000 | Loss: 0.00002973
Iteration 162/1000 | Loss: 0.00002972
Iteration 163/1000 | Loss: 0.00002972
Iteration 164/1000 | Loss: 0.00002972
Iteration 165/1000 | Loss: 0.00002972
Iteration 166/1000 | Loss: 0.00002972
Iteration 167/1000 | Loss: 0.00002972
Iteration 168/1000 | Loss: 0.00002972
Iteration 169/1000 | Loss: 0.00002972
Iteration 170/1000 | Loss: 0.00002972
Iteration 171/1000 | Loss: 0.00002972
Iteration 172/1000 | Loss: 0.00002971
Iteration 173/1000 | Loss: 0.00002971
Iteration 174/1000 | Loss: 0.00002971
Iteration 175/1000 | Loss: 0.00002971
Iteration 176/1000 | Loss: 0.00002971
Iteration 177/1000 | Loss: 0.00002971
Iteration 178/1000 | Loss: 0.00002971
Iteration 179/1000 | Loss: 0.00002971
Iteration 180/1000 | Loss: 0.00002971
Iteration 181/1000 | Loss: 0.00002971
Iteration 182/1000 | Loss: 0.00002971
Iteration 183/1000 | Loss: 0.00002971
Iteration 184/1000 | Loss: 0.00002971
Iteration 185/1000 | Loss: 0.00002971
Iteration 186/1000 | Loss: 0.00002971
Iteration 187/1000 | Loss: 0.00002971
Iteration 188/1000 | Loss: 0.00002971
Iteration 189/1000 | Loss: 0.00002971
Iteration 190/1000 | Loss: 0.00002971
Iteration 191/1000 | Loss: 0.00002971
Iteration 192/1000 | Loss: 0.00002971
Iteration 193/1000 | Loss: 0.00002971
Iteration 194/1000 | Loss: 0.00002971
Iteration 195/1000 | Loss: 0.00002971
Iteration 196/1000 | Loss: 0.00002971
Iteration 197/1000 | Loss: 0.00002971
Iteration 198/1000 | Loss: 0.00002971
Iteration 199/1000 | Loss: 0.00002971
Iteration 200/1000 | Loss: 0.00002971
Iteration 201/1000 | Loss: 0.00002971
Iteration 202/1000 | Loss: 0.00002971
Iteration 203/1000 | Loss: 0.00002971
Iteration 204/1000 | Loss: 0.00002971
Iteration 205/1000 | Loss: 0.00002971
Iteration 206/1000 | Loss: 0.00002971
Iteration 207/1000 | Loss: 0.00002971
Iteration 208/1000 | Loss: 0.00002971
Iteration 209/1000 | Loss: 0.00002971
Iteration 210/1000 | Loss: 0.00002971
Iteration 211/1000 | Loss: 0.00002971
Iteration 212/1000 | Loss: 0.00002971
Iteration 213/1000 | Loss: 0.00002971
Iteration 214/1000 | Loss: 0.00002971
Iteration 215/1000 | Loss: 0.00002971
Iteration 216/1000 | Loss: 0.00002971
Iteration 217/1000 | Loss: 0.00002971
Iteration 218/1000 | Loss: 0.00002971
Iteration 219/1000 | Loss: 0.00002971
Iteration 220/1000 | Loss: 0.00002971
Iteration 221/1000 | Loss: 0.00002971
Iteration 222/1000 | Loss: 0.00002971
Iteration 223/1000 | Loss: 0.00002971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.9713133699260652e-05, 2.9713133699260652e-05, 2.9713133699260652e-05, 2.9713133699260652e-05, 2.9713133699260652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9713133699260652e-05

Optimization complete. Final v2v error: 4.091405391693115 mm

Highest mean error: 21.996458053588867 mm for frame 144

Lowest mean error: 3.6426966190338135 mm for frame 136

Saving results

Total time: 549.2526993751526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00897640
Iteration 2/25 | Loss: 0.00149996
Iteration 3/25 | Loss: 0.00123041
Iteration 4/25 | Loss: 0.00117981
Iteration 5/25 | Loss: 0.00116784
Iteration 6/25 | Loss: 0.00116608
Iteration 7/25 | Loss: 0.00116601
Iteration 8/25 | Loss: 0.00116601
Iteration 9/25 | Loss: 0.00116601
Iteration 10/25 | Loss: 0.00116601
Iteration 11/25 | Loss: 0.00116601
Iteration 12/25 | Loss: 0.00116601
Iteration 13/25 | Loss: 0.00116601
Iteration 14/25 | Loss: 0.00116601
Iteration 15/25 | Loss: 0.00116601
Iteration 16/25 | Loss: 0.00116601
Iteration 17/25 | Loss: 0.00116601
Iteration 18/25 | Loss: 0.00116601
Iteration 19/25 | Loss: 0.00116601
Iteration 20/25 | Loss: 0.00116601
Iteration 21/25 | Loss: 0.00116601
Iteration 22/25 | Loss: 0.00116601
Iteration 23/25 | Loss: 0.00116601
Iteration 24/25 | Loss: 0.00116601
Iteration 25/25 | Loss: 0.00116601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83190006
Iteration 2/25 | Loss: 0.00127928
Iteration 3/25 | Loss: 0.00127927
Iteration 4/25 | Loss: 0.00127927
Iteration 5/25 | Loss: 0.00127927
Iteration 6/25 | Loss: 0.00127927
Iteration 7/25 | Loss: 0.00127927
Iteration 8/25 | Loss: 0.00127927
Iteration 9/25 | Loss: 0.00127927
Iteration 10/25 | Loss: 0.00127927
Iteration 11/25 | Loss: 0.00127927
Iteration 12/25 | Loss: 0.00127927
Iteration 13/25 | Loss: 0.00127927
Iteration 14/25 | Loss: 0.00127927
Iteration 15/25 | Loss: 0.00127927
Iteration 16/25 | Loss: 0.00127927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012792657362297177, 0.0012792657362297177, 0.0012792657362297177, 0.0012792657362297177, 0.0012792657362297177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012792657362297177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127927
Iteration 2/1000 | Loss: 0.00009449
Iteration 3/1000 | Loss: 0.00006013
Iteration 4/1000 | Loss: 0.00004956
Iteration 5/1000 | Loss: 0.00004562
Iteration 6/1000 | Loss: 0.00004243
Iteration 7/1000 | Loss: 0.00004052
Iteration 8/1000 | Loss: 0.00003931
Iteration 9/1000 | Loss: 0.00003826
Iteration 10/1000 | Loss: 0.00003731
Iteration 11/1000 | Loss: 0.00003691
Iteration 12/1000 | Loss: 0.00003655
Iteration 13/1000 | Loss: 0.00003632
Iteration 14/1000 | Loss: 0.00003630
Iteration 15/1000 | Loss: 0.00003629
Iteration 16/1000 | Loss: 0.00003627
Iteration 17/1000 | Loss: 0.00003626
Iteration 18/1000 | Loss: 0.00003625
Iteration 19/1000 | Loss: 0.00003625
Iteration 20/1000 | Loss: 0.00003625
Iteration 21/1000 | Loss: 0.00003623
Iteration 22/1000 | Loss: 0.00003621
Iteration 23/1000 | Loss: 0.00003620
Iteration 24/1000 | Loss: 0.00003619
Iteration 25/1000 | Loss: 0.00003619
Iteration 26/1000 | Loss: 0.00003617
Iteration 27/1000 | Loss: 0.00003617
Iteration 28/1000 | Loss: 0.00003617
Iteration 29/1000 | Loss: 0.00003617
Iteration 30/1000 | Loss: 0.00003617
Iteration 31/1000 | Loss: 0.00003617
Iteration 32/1000 | Loss: 0.00003617
Iteration 33/1000 | Loss: 0.00003617
Iteration 34/1000 | Loss: 0.00003617
Iteration 35/1000 | Loss: 0.00003617
Iteration 36/1000 | Loss: 0.00003617
Iteration 37/1000 | Loss: 0.00003616
Iteration 38/1000 | Loss: 0.00003615
Iteration 39/1000 | Loss: 0.00003615
Iteration 40/1000 | Loss: 0.00003615
Iteration 41/1000 | Loss: 0.00003615
Iteration 42/1000 | Loss: 0.00003615
Iteration 43/1000 | Loss: 0.00003615
Iteration 44/1000 | Loss: 0.00003614
Iteration 45/1000 | Loss: 0.00003614
Iteration 46/1000 | Loss: 0.00003614
Iteration 47/1000 | Loss: 0.00003614
Iteration 48/1000 | Loss: 0.00003614
Iteration 49/1000 | Loss: 0.00003614
Iteration 50/1000 | Loss: 0.00003614
Iteration 51/1000 | Loss: 0.00003613
Iteration 52/1000 | Loss: 0.00003613
Iteration 53/1000 | Loss: 0.00003613
Iteration 54/1000 | Loss: 0.00003613
Iteration 55/1000 | Loss: 0.00003613
Iteration 56/1000 | Loss: 0.00003613
Iteration 57/1000 | Loss: 0.00003613
Iteration 58/1000 | Loss: 0.00003612
Iteration 59/1000 | Loss: 0.00003612
Iteration 60/1000 | Loss: 0.00003612
Iteration 61/1000 | Loss: 0.00003612
Iteration 62/1000 | Loss: 0.00003611
Iteration 63/1000 | Loss: 0.00003611
Iteration 64/1000 | Loss: 0.00003611
Iteration 65/1000 | Loss: 0.00003610
Iteration 66/1000 | Loss: 0.00003610
Iteration 67/1000 | Loss: 0.00003610
Iteration 68/1000 | Loss: 0.00003609
Iteration 69/1000 | Loss: 0.00003609
Iteration 70/1000 | Loss: 0.00003609
Iteration 71/1000 | Loss: 0.00003609
Iteration 72/1000 | Loss: 0.00003608
Iteration 73/1000 | Loss: 0.00003608
Iteration 74/1000 | Loss: 0.00003608
Iteration 75/1000 | Loss: 0.00003608
Iteration 76/1000 | Loss: 0.00003608
Iteration 77/1000 | Loss: 0.00003607
Iteration 78/1000 | Loss: 0.00003607
Iteration 79/1000 | Loss: 0.00003607
Iteration 80/1000 | Loss: 0.00003607
Iteration 81/1000 | Loss: 0.00003607
Iteration 82/1000 | Loss: 0.00003607
Iteration 83/1000 | Loss: 0.00003607
Iteration 84/1000 | Loss: 0.00003607
Iteration 85/1000 | Loss: 0.00003607
Iteration 86/1000 | Loss: 0.00003607
Iteration 87/1000 | Loss: 0.00003607
Iteration 88/1000 | Loss: 0.00003607
Iteration 89/1000 | Loss: 0.00003607
Iteration 90/1000 | Loss: 0.00003607
Iteration 91/1000 | Loss: 0.00003607
Iteration 92/1000 | Loss: 0.00003607
Iteration 93/1000 | Loss: 0.00003607
Iteration 94/1000 | Loss: 0.00003607
Iteration 95/1000 | Loss: 0.00003607
Iteration 96/1000 | Loss: 0.00003607
Iteration 97/1000 | Loss: 0.00003607
Iteration 98/1000 | Loss: 0.00003607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [3.607003600336611e-05, 3.607003600336611e-05, 3.607003600336611e-05, 3.607003600336611e-05, 3.607003600336611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.607003600336611e-05

Optimization complete. Final v2v error: 5.01810884475708 mm

Highest mean error: 5.4227399826049805 mm for frame 47

Lowest mean error: 4.682969093322754 mm for frame 86

Saving results

Total time: 77.54512596130371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860673
Iteration 2/25 | Loss: 0.00122862
Iteration 3/25 | Loss: 0.00113721
Iteration 4/25 | Loss: 0.00110164
Iteration 5/25 | Loss: 0.00109399
Iteration 6/25 | Loss: 0.00109331
Iteration 7/25 | Loss: 0.00109331
Iteration 8/25 | Loss: 0.00109331
Iteration 9/25 | Loss: 0.00109331
Iteration 10/25 | Loss: 0.00109331
Iteration 11/25 | Loss: 0.00109331
Iteration 12/25 | Loss: 0.00109331
Iteration 13/25 | Loss: 0.00109331
Iteration 14/25 | Loss: 0.00109331
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010933115845546126, 0.0010933115845546126, 0.0010933115845546126, 0.0010933115845546126, 0.0010933115845546126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010933115845546126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22674525
Iteration 2/25 | Loss: 0.00159278
Iteration 3/25 | Loss: 0.00159278
Iteration 4/25 | Loss: 0.00159278
Iteration 5/25 | Loss: 0.00159278
Iteration 6/25 | Loss: 0.00159278
Iteration 7/25 | Loss: 0.00159278
Iteration 8/25 | Loss: 0.00159278
Iteration 9/25 | Loss: 0.00159278
Iteration 10/25 | Loss: 0.00159278
Iteration 11/25 | Loss: 0.00159278
Iteration 12/25 | Loss: 0.00159278
Iteration 13/25 | Loss: 0.00159278
Iteration 14/25 | Loss: 0.00159278
Iteration 15/25 | Loss: 0.00159278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015927784843370318, 0.0015927784843370318, 0.0015927784843370318, 0.0015927784843370318, 0.0015927784843370318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015927784843370318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159278
Iteration 2/1000 | Loss: 0.00005833
Iteration 3/1000 | Loss: 0.00003721
Iteration 4/1000 | Loss: 0.00003201
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002794
Iteration 7/1000 | Loss: 0.00002705
Iteration 8/1000 | Loss: 0.00002632
Iteration 9/1000 | Loss: 0.00002588
Iteration 10/1000 | Loss: 0.00002551
Iteration 11/1000 | Loss: 0.00002526
Iteration 12/1000 | Loss: 0.00002514
Iteration 13/1000 | Loss: 0.00002514
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002510
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002509
Iteration 19/1000 | Loss: 0.00002508
Iteration 20/1000 | Loss: 0.00002508
Iteration 21/1000 | Loss: 0.00002506
Iteration 22/1000 | Loss: 0.00002505
Iteration 23/1000 | Loss: 0.00002505
Iteration 24/1000 | Loss: 0.00002505
Iteration 25/1000 | Loss: 0.00002505
Iteration 26/1000 | Loss: 0.00002504
Iteration 27/1000 | Loss: 0.00002503
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002502
Iteration 30/1000 | Loss: 0.00002502
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002501
Iteration 33/1000 | Loss: 0.00002501
Iteration 34/1000 | Loss: 0.00002501
Iteration 35/1000 | Loss: 0.00002500
Iteration 36/1000 | Loss: 0.00002500
Iteration 37/1000 | Loss: 0.00002500
Iteration 38/1000 | Loss: 0.00002500
Iteration 39/1000 | Loss: 0.00002500
Iteration 40/1000 | Loss: 0.00002500
Iteration 41/1000 | Loss: 0.00002499
Iteration 42/1000 | Loss: 0.00002499
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002496
Iteration 45/1000 | Loss: 0.00002496
Iteration 46/1000 | Loss: 0.00002495
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002494
Iteration 50/1000 | Loss: 0.00002494
Iteration 51/1000 | Loss: 0.00002494
Iteration 52/1000 | Loss: 0.00002494
Iteration 53/1000 | Loss: 0.00002493
Iteration 54/1000 | Loss: 0.00002493
Iteration 55/1000 | Loss: 0.00002492
Iteration 56/1000 | Loss: 0.00002492
Iteration 57/1000 | Loss: 0.00002492
Iteration 58/1000 | Loss: 0.00002492
Iteration 59/1000 | Loss: 0.00002492
Iteration 60/1000 | Loss: 0.00002491
Iteration 61/1000 | Loss: 0.00002490
Iteration 62/1000 | Loss: 0.00002489
Iteration 63/1000 | Loss: 0.00002489
Iteration 64/1000 | Loss: 0.00002489
Iteration 65/1000 | Loss: 0.00002489
Iteration 66/1000 | Loss: 0.00002489
Iteration 67/1000 | Loss: 0.00002488
Iteration 68/1000 | Loss: 0.00002488
Iteration 69/1000 | Loss: 0.00002488
Iteration 70/1000 | Loss: 0.00002488
Iteration 71/1000 | Loss: 0.00002487
Iteration 72/1000 | Loss: 0.00002487
Iteration 73/1000 | Loss: 0.00002486
Iteration 74/1000 | Loss: 0.00002486
Iteration 75/1000 | Loss: 0.00002485
Iteration 76/1000 | Loss: 0.00002485
Iteration 77/1000 | Loss: 0.00002485
Iteration 78/1000 | Loss: 0.00002485
Iteration 79/1000 | Loss: 0.00002485
Iteration 80/1000 | Loss: 0.00002484
Iteration 81/1000 | Loss: 0.00002484
Iteration 82/1000 | Loss: 0.00002484
Iteration 83/1000 | Loss: 0.00002484
Iteration 84/1000 | Loss: 0.00002484
Iteration 85/1000 | Loss: 0.00002483
Iteration 86/1000 | Loss: 0.00002483
Iteration 87/1000 | Loss: 0.00002483
Iteration 88/1000 | Loss: 0.00002483
Iteration 89/1000 | Loss: 0.00002483
Iteration 90/1000 | Loss: 0.00002483
Iteration 91/1000 | Loss: 0.00002483
Iteration 92/1000 | Loss: 0.00002483
Iteration 93/1000 | Loss: 0.00002483
Iteration 94/1000 | Loss: 0.00002483
Iteration 95/1000 | Loss: 0.00002483
Iteration 96/1000 | Loss: 0.00002483
Iteration 97/1000 | Loss: 0.00002483
Iteration 98/1000 | Loss: 0.00002483
Iteration 99/1000 | Loss: 0.00002482
Iteration 100/1000 | Loss: 0.00002482
Iteration 101/1000 | Loss: 0.00002482
Iteration 102/1000 | Loss: 0.00002482
Iteration 103/1000 | Loss: 0.00002482
Iteration 104/1000 | Loss: 0.00002482
Iteration 105/1000 | Loss: 0.00002482
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002482
Iteration 108/1000 | Loss: 0.00002482
Iteration 109/1000 | Loss: 0.00002481
Iteration 110/1000 | Loss: 0.00002481
Iteration 111/1000 | Loss: 0.00002481
Iteration 112/1000 | Loss: 0.00002481
Iteration 113/1000 | Loss: 0.00002481
Iteration 114/1000 | Loss: 0.00002481
Iteration 115/1000 | Loss: 0.00002481
Iteration 116/1000 | Loss: 0.00002481
Iteration 117/1000 | Loss: 0.00002481
Iteration 118/1000 | Loss: 0.00002481
Iteration 119/1000 | Loss: 0.00002481
Iteration 120/1000 | Loss: 0.00002481
Iteration 121/1000 | Loss: 0.00002481
Iteration 122/1000 | Loss: 0.00002481
Iteration 123/1000 | Loss: 0.00002481
Iteration 124/1000 | Loss: 0.00002481
Iteration 125/1000 | Loss: 0.00002481
Iteration 126/1000 | Loss: 0.00002481
Iteration 127/1000 | Loss: 0.00002481
Iteration 128/1000 | Loss: 0.00002481
Iteration 129/1000 | Loss: 0.00002481
Iteration 130/1000 | Loss: 0.00002481
Iteration 131/1000 | Loss: 0.00002481
Iteration 132/1000 | Loss: 0.00002481
Iteration 133/1000 | Loss: 0.00002481
Iteration 134/1000 | Loss: 0.00002481
Iteration 135/1000 | Loss: 0.00002481
Iteration 136/1000 | Loss: 0.00002481
Iteration 137/1000 | Loss: 0.00002481
Iteration 138/1000 | Loss: 0.00002481
Iteration 139/1000 | Loss: 0.00002481
Iteration 140/1000 | Loss: 0.00002481
Iteration 141/1000 | Loss: 0.00002481
Iteration 142/1000 | Loss: 0.00002481
Iteration 143/1000 | Loss: 0.00002481
Iteration 144/1000 | Loss: 0.00002481
Iteration 145/1000 | Loss: 0.00002481
Iteration 146/1000 | Loss: 0.00002481
Iteration 147/1000 | Loss: 0.00002481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.4809263777569868e-05, 2.4809263777569868e-05, 2.4809263777569868e-05, 2.4809263777569868e-05, 2.4809263777569868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4809263777569868e-05

Optimization complete. Final v2v error: 4.217670440673828 mm

Highest mean error: 4.629914283752441 mm for frame 59

Lowest mean error: 3.7514419555664062 mm for frame 187

Saving results

Total time: 85.62427568435669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533190
Iteration 2/25 | Loss: 0.00151889
Iteration 3/25 | Loss: 0.00133460
Iteration 4/25 | Loss: 0.00130162
Iteration 5/25 | Loss: 0.00125363
Iteration 6/25 | Loss: 0.00122671
Iteration 7/25 | Loss: 0.00117759
Iteration 8/25 | Loss: 0.00116380
Iteration 9/25 | Loss: 0.00115823
Iteration 10/25 | Loss: 0.00115667
Iteration 11/25 | Loss: 0.00115617
Iteration 12/25 | Loss: 0.00115793
Iteration 13/25 | Loss: 0.00115539
Iteration 14/25 | Loss: 0.00115411
Iteration 15/25 | Loss: 0.00115371
Iteration 16/25 | Loss: 0.00115368
Iteration 17/25 | Loss: 0.00115368
Iteration 18/25 | Loss: 0.00115368
Iteration 19/25 | Loss: 0.00115368
Iteration 20/25 | Loss: 0.00115367
Iteration 21/25 | Loss: 0.00115367
Iteration 22/25 | Loss: 0.00115367
Iteration 23/25 | Loss: 0.00115367
Iteration 24/25 | Loss: 0.00115367
Iteration 25/25 | Loss: 0.00115367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20484102
Iteration 2/25 | Loss: 0.00222582
Iteration 3/25 | Loss: 0.00222582
Iteration 4/25 | Loss: 0.00222581
Iteration 5/25 | Loss: 0.00222581
Iteration 6/25 | Loss: 0.00222581
Iteration 7/25 | Loss: 0.00222581
Iteration 8/25 | Loss: 0.00222581
Iteration 9/25 | Loss: 0.00222581
Iteration 10/25 | Loss: 0.00222581
Iteration 11/25 | Loss: 0.00222581
Iteration 12/25 | Loss: 0.00222581
Iteration 13/25 | Loss: 0.00222581
Iteration 14/25 | Loss: 0.00222581
Iteration 15/25 | Loss: 0.00222581
Iteration 16/25 | Loss: 0.00222581
Iteration 17/25 | Loss: 0.00222581
Iteration 18/25 | Loss: 0.00222581
Iteration 19/25 | Loss: 0.00222581
Iteration 20/25 | Loss: 0.00222581
Iteration 21/25 | Loss: 0.00222581
Iteration 22/25 | Loss: 0.00222581
Iteration 23/25 | Loss: 0.00222581
Iteration 24/25 | Loss: 0.00222581
Iteration 25/25 | Loss: 0.00222581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222581
Iteration 2/1000 | Loss: 0.00009867
Iteration 3/1000 | Loss: 0.00010109
Iteration 4/1000 | Loss: 0.00011602
Iteration 5/1000 | Loss: 0.00007175
Iteration 6/1000 | Loss: 0.00007110
Iteration 7/1000 | Loss: 0.00010233
Iteration 8/1000 | Loss: 0.00007231
Iteration 9/1000 | Loss: 0.00006475
Iteration 10/1000 | Loss: 0.00008206
Iteration 11/1000 | Loss: 0.00003993
Iteration 12/1000 | Loss: 0.00003425
Iteration 13/1000 | Loss: 0.00003265
Iteration 14/1000 | Loss: 0.00003105
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00002816
Iteration 17/1000 | Loss: 0.00002749
Iteration 18/1000 | Loss: 0.00002712
Iteration 19/1000 | Loss: 0.00002684
Iteration 20/1000 | Loss: 0.00002648
Iteration 21/1000 | Loss: 0.00002623
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002582
Iteration 24/1000 | Loss: 0.00002581
Iteration 25/1000 | Loss: 0.00002580
Iteration 26/1000 | Loss: 0.00002579
Iteration 27/1000 | Loss: 0.00002578
Iteration 28/1000 | Loss: 0.00002578
Iteration 29/1000 | Loss: 0.00002574
Iteration 30/1000 | Loss: 0.00002573
Iteration 31/1000 | Loss: 0.00002572
Iteration 32/1000 | Loss: 0.00002572
Iteration 33/1000 | Loss: 0.00002571
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00002570
Iteration 36/1000 | Loss: 0.00002570
Iteration 37/1000 | Loss: 0.00002569
Iteration 38/1000 | Loss: 0.00002568
Iteration 39/1000 | Loss: 0.00002567
Iteration 40/1000 | Loss: 0.00002567
Iteration 41/1000 | Loss: 0.00002567
Iteration 42/1000 | Loss: 0.00002566
Iteration 43/1000 | Loss: 0.00002566
Iteration 44/1000 | Loss: 0.00002566
Iteration 45/1000 | Loss: 0.00002566
Iteration 46/1000 | Loss: 0.00002565
Iteration 47/1000 | Loss: 0.00002565
Iteration 48/1000 | Loss: 0.00002565
Iteration 49/1000 | Loss: 0.00002565
Iteration 50/1000 | Loss: 0.00002565
Iteration 51/1000 | Loss: 0.00002565
Iteration 52/1000 | Loss: 0.00002565
Iteration 53/1000 | Loss: 0.00002565
Iteration 54/1000 | Loss: 0.00002564
Iteration 55/1000 | Loss: 0.00002564
Iteration 56/1000 | Loss: 0.00002564
Iteration 57/1000 | Loss: 0.00002564
Iteration 58/1000 | Loss: 0.00002564
Iteration 59/1000 | Loss: 0.00002564
Iteration 60/1000 | Loss: 0.00002564
Iteration 61/1000 | Loss: 0.00002564
Iteration 62/1000 | Loss: 0.00002563
Iteration 63/1000 | Loss: 0.00002563
Iteration 64/1000 | Loss: 0.00002563
Iteration 65/1000 | Loss: 0.00002563
Iteration 66/1000 | Loss: 0.00002563
Iteration 67/1000 | Loss: 0.00002563
Iteration 68/1000 | Loss: 0.00002562
Iteration 69/1000 | Loss: 0.00002562
Iteration 70/1000 | Loss: 0.00002562
Iteration 71/1000 | Loss: 0.00002562
Iteration 72/1000 | Loss: 0.00002562
Iteration 73/1000 | Loss: 0.00002561
Iteration 74/1000 | Loss: 0.00002561
Iteration 75/1000 | Loss: 0.00002561
Iteration 76/1000 | Loss: 0.00002561
Iteration 77/1000 | Loss: 0.00002560
Iteration 78/1000 | Loss: 0.00002560
Iteration 79/1000 | Loss: 0.00002560
Iteration 80/1000 | Loss: 0.00002560
Iteration 81/1000 | Loss: 0.00002559
Iteration 82/1000 | Loss: 0.00002559
Iteration 83/1000 | Loss: 0.00002559
Iteration 84/1000 | Loss: 0.00002559
Iteration 85/1000 | Loss: 0.00002559
Iteration 86/1000 | Loss: 0.00002559
Iteration 87/1000 | Loss: 0.00002559
Iteration 88/1000 | Loss: 0.00002559
Iteration 89/1000 | Loss: 0.00002559
Iteration 90/1000 | Loss: 0.00002559
Iteration 91/1000 | Loss: 0.00002558
Iteration 92/1000 | Loss: 0.00002558
Iteration 93/1000 | Loss: 0.00002558
Iteration 94/1000 | Loss: 0.00002558
Iteration 95/1000 | Loss: 0.00002557
Iteration 96/1000 | Loss: 0.00002557
Iteration 97/1000 | Loss: 0.00002557
Iteration 98/1000 | Loss: 0.00002557
Iteration 99/1000 | Loss: 0.00002556
Iteration 100/1000 | Loss: 0.00002556
Iteration 101/1000 | Loss: 0.00002556
Iteration 102/1000 | Loss: 0.00002555
Iteration 103/1000 | Loss: 0.00002555
Iteration 104/1000 | Loss: 0.00002555
Iteration 105/1000 | Loss: 0.00002555
Iteration 106/1000 | Loss: 0.00002554
Iteration 107/1000 | Loss: 0.00002554
Iteration 108/1000 | Loss: 0.00002553
Iteration 109/1000 | Loss: 0.00002553
Iteration 110/1000 | Loss: 0.00002553
Iteration 111/1000 | Loss: 0.00002552
Iteration 112/1000 | Loss: 0.00002552
Iteration 113/1000 | Loss: 0.00002552
Iteration 114/1000 | Loss: 0.00002551
Iteration 115/1000 | Loss: 0.00002551
Iteration 116/1000 | Loss: 0.00002551
Iteration 117/1000 | Loss: 0.00002551
Iteration 118/1000 | Loss: 0.00002551
Iteration 119/1000 | Loss: 0.00002551
Iteration 120/1000 | Loss: 0.00002551
Iteration 121/1000 | Loss: 0.00002551
Iteration 122/1000 | Loss: 0.00002551
Iteration 123/1000 | Loss: 0.00002551
Iteration 124/1000 | Loss: 0.00002551
Iteration 125/1000 | Loss: 0.00002550
Iteration 126/1000 | Loss: 0.00002550
Iteration 127/1000 | Loss: 0.00002550
Iteration 128/1000 | Loss: 0.00002550
Iteration 129/1000 | Loss: 0.00002550
Iteration 130/1000 | Loss: 0.00002550
Iteration 131/1000 | Loss: 0.00002550
Iteration 132/1000 | Loss: 0.00002549
Iteration 133/1000 | Loss: 0.00002549
Iteration 134/1000 | Loss: 0.00002549
Iteration 135/1000 | Loss: 0.00002549
Iteration 136/1000 | Loss: 0.00002549
Iteration 137/1000 | Loss: 0.00002549
Iteration 138/1000 | Loss: 0.00002548
Iteration 139/1000 | Loss: 0.00002548
Iteration 140/1000 | Loss: 0.00002548
Iteration 141/1000 | Loss: 0.00002547
Iteration 142/1000 | Loss: 0.00002547
Iteration 143/1000 | Loss: 0.00002547
Iteration 144/1000 | Loss: 0.00002546
Iteration 145/1000 | Loss: 0.00002546
Iteration 146/1000 | Loss: 0.00002546
Iteration 147/1000 | Loss: 0.00002546
Iteration 148/1000 | Loss: 0.00002546
Iteration 149/1000 | Loss: 0.00002546
Iteration 150/1000 | Loss: 0.00002545
Iteration 151/1000 | Loss: 0.00002545
Iteration 152/1000 | Loss: 0.00002545
Iteration 153/1000 | Loss: 0.00002545
Iteration 154/1000 | Loss: 0.00002545
Iteration 155/1000 | Loss: 0.00002545
Iteration 156/1000 | Loss: 0.00002545
Iteration 157/1000 | Loss: 0.00002545
Iteration 158/1000 | Loss: 0.00002545
Iteration 159/1000 | Loss: 0.00002545
Iteration 160/1000 | Loss: 0.00002545
Iteration 161/1000 | Loss: 0.00002545
Iteration 162/1000 | Loss: 0.00002545
Iteration 163/1000 | Loss: 0.00002545
Iteration 164/1000 | Loss: 0.00002544
Iteration 165/1000 | Loss: 0.00002544
Iteration 166/1000 | Loss: 0.00002544
Iteration 167/1000 | Loss: 0.00002544
Iteration 168/1000 | Loss: 0.00002544
Iteration 169/1000 | Loss: 0.00002544
Iteration 170/1000 | Loss: 0.00002544
Iteration 171/1000 | Loss: 0.00002544
Iteration 172/1000 | Loss: 0.00002544
Iteration 173/1000 | Loss: 0.00002544
Iteration 174/1000 | Loss: 0.00002544
Iteration 175/1000 | Loss: 0.00002544
Iteration 176/1000 | Loss: 0.00002543
Iteration 177/1000 | Loss: 0.00002543
Iteration 178/1000 | Loss: 0.00002543
Iteration 179/1000 | Loss: 0.00002543
Iteration 180/1000 | Loss: 0.00002543
Iteration 181/1000 | Loss: 0.00002543
Iteration 182/1000 | Loss: 0.00002543
Iteration 183/1000 | Loss: 0.00002543
Iteration 184/1000 | Loss: 0.00002543
Iteration 185/1000 | Loss: 0.00002543
Iteration 186/1000 | Loss: 0.00002542
Iteration 187/1000 | Loss: 0.00002542
Iteration 188/1000 | Loss: 0.00002542
Iteration 189/1000 | Loss: 0.00002542
Iteration 190/1000 | Loss: 0.00002542
Iteration 191/1000 | Loss: 0.00002542
Iteration 192/1000 | Loss: 0.00002542
Iteration 193/1000 | Loss: 0.00002542
Iteration 194/1000 | Loss: 0.00002542
Iteration 195/1000 | Loss: 0.00002541
Iteration 196/1000 | Loss: 0.00002541
Iteration 197/1000 | Loss: 0.00002541
Iteration 198/1000 | Loss: 0.00002541
Iteration 199/1000 | Loss: 0.00002541
Iteration 200/1000 | Loss: 0.00002541
Iteration 201/1000 | Loss: 0.00002540
Iteration 202/1000 | Loss: 0.00002540
Iteration 203/1000 | Loss: 0.00002540
Iteration 204/1000 | Loss: 0.00002540
Iteration 205/1000 | Loss: 0.00002540
Iteration 206/1000 | Loss: 0.00002540
Iteration 207/1000 | Loss: 0.00002540
Iteration 208/1000 | Loss: 0.00002540
Iteration 209/1000 | Loss: 0.00002539
Iteration 210/1000 | Loss: 0.00002539
Iteration 211/1000 | Loss: 0.00002539
Iteration 212/1000 | Loss: 0.00002539
Iteration 213/1000 | Loss: 0.00002539
Iteration 214/1000 | Loss: 0.00002539
Iteration 215/1000 | Loss: 0.00002539
Iteration 216/1000 | Loss: 0.00002539
Iteration 217/1000 | Loss: 0.00002539
Iteration 218/1000 | Loss: 0.00002539
Iteration 219/1000 | Loss: 0.00002539
Iteration 220/1000 | Loss: 0.00002538
Iteration 221/1000 | Loss: 0.00002538
Iteration 222/1000 | Loss: 0.00002538
Iteration 223/1000 | Loss: 0.00002538
Iteration 224/1000 | Loss: 0.00002538
Iteration 225/1000 | Loss: 0.00002538
Iteration 226/1000 | Loss: 0.00002538
Iteration 227/1000 | Loss: 0.00002538
Iteration 228/1000 | Loss: 0.00002538
Iteration 229/1000 | Loss: 0.00002538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.538055196055211e-05, 2.538055196055211e-05, 2.538055196055211e-05, 2.538055196055211e-05, 2.538055196055211e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.538055196055211e-05

Optimization complete. Final v2v error: 4.0749993324279785 mm

Highest mean error: 5.55780553817749 mm for frame 25

Lowest mean error: 3.373342990875244 mm for frame 162

Saving results

Total time: 187.87239956855774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842115
Iteration 2/25 | Loss: 0.00123723
Iteration 3/25 | Loss: 0.00107536
Iteration 4/25 | Loss: 0.00104975
Iteration 5/25 | Loss: 0.00104230
Iteration 6/25 | Loss: 0.00103988
Iteration 7/25 | Loss: 0.00103988
Iteration 8/25 | Loss: 0.00103988
Iteration 9/25 | Loss: 0.00103988
Iteration 10/25 | Loss: 0.00103988
Iteration 11/25 | Loss: 0.00103988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010398818412795663, 0.0010398818412795663, 0.0010398818412795663, 0.0010398818412795663, 0.0010398818412795663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010398818412795663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20416880
Iteration 2/25 | Loss: 0.00157293
Iteration 3/25 | Loss: 0.00157293
Iteration 4/25 | Loss: 0.00157293
Iteration 5/25 | Loss: 0.00157293
Iteration 6/25 | Loss: 0.00157293
Iteration 7/25 | Loss: 0.00157293
Iteration 8/25 | Loss: 0.00157293
Iteration 9/25 | Loss: 0.00157293
Iteration 10/25 | Loss: 0.00157293
Iteration 11/25 | Loss: 0.00157293
Iteration 12/25 | Loss: 0.00157293
Iteration 13/25 | Loss: 0.00157293
Iteration 14/25 | Loss: 0.00157293
Iteration 15/25 | Loss: 0.00157293
Iteration 16/25 | Loss: 0.00157293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015729309525340796, 0.0015729309525340796, 0.0015729309525340796, 0.0015729309525340796, 0.0015729309525340796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015729309525340796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157293
Iteration 2/1000 | Loss: 0.00004432
Iteration 3/1000 | Loss: 0.00002593
Iteration 4/1000 | Loss: 0.00002252
Iteration 5/1000 | Loss: 0.00002105
Iteration 6/1000 | Loss: 0.00001988
Iteration 7/1000 | Loss: 0.00001925
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001811
Iteration 11/1000 | Loss: 0.00001783
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001749
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001745
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001744
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001737
Iteration 23/1000 | Loss: 0.00001737
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001736
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001734
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001733
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001733
Iteration 42/1000 | Loss: 0.00001733
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001733
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001733
Iteration 49/1000 | Loss: 0.00001733
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001732
Iteration 53/1000 | Loss: 0.00001732
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001731
Iteration 56/1000 | Loss: 0.00001731
Iteration 57/1000 | Loss: 0.00001730
Iteration 58/1000 | Loss: 0.00001730
Iteration 59/1000 | Loss: 0.00001730
Iteration 60/1000 | Loss: 0.00001730
Iteration 61/1000 | Loss: 0.00001730
Iteration 62/1000 | Loss: 0.00001729
Iteration 63/1000 | Loss: 0.00001729
Iteration 64/1000 | Loss: 0.00001729
Iteration 65/1000 | Loss: 0.00001729
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001728
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001727
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001727
Iteration 98/1000 | Loss: 0.00001727
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.7267882867599837e-05, 1.7267882867599837e-05, 1.7267882867599837e-05, 1.7267882867599837e-05, 1.7267882867599837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7267882867599837e-05

Optimization complete. Final v2v error: 3.60006046295166 mm

Highest mean error: 3.9549050331115723 mm for frame 78

Lowest mean error: 3.3199751377105713 mm for frame 161

Saving results

Total time: 96.58110761642456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047991
Iteration 2/25 | Loss: 0.00327814
Iteration 3/25 | Loss: 0.00237617
Iteration 4/25 | Loss: 0.00216098
Iteration 5/25 | Loss: 0.00180404
Iteration 6/25 | Loss: 0.00167928
Iteration 7/25 | Loss: 0.00149269
Iteration 8/25 | Loss: 0.00144343
Iteration 9/25 | Loss: 0.00143223
Iteration 10/25 | Loss: 0.00142928
Iteration 11/25 | Loss: 0.00141028
Iteration 12/25 | Loss: 0.00137178
Iteration 13/25 | Loss: 0.00135310
Iteration 14/25 | Loss: 0.00134660
Iteration 15/25 | Loss: 0.00134766
Iteration 16/25 | Loss: 0.00134168
Iteration 17/25 | Loss: 0.00133969
Iteration 18/25 | Loss: 0.00133819
Iteration 19/25 | Loss: 0.00133981
Iteration 20/25 | Loss: 0.00133508
Iteration 21/25 | Loss: 0.00133449
Iteration 22/25 | Loss: 0.00132568
Iteration 23/25 | Loss: 0.00132560
Iteration 24/25 | Loss: 0.00132976
Iteration 25/25 | Loss: 0.00132462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15840662
Iteration 2/25 | Loss: 0.00430586
Iteration 3/25 | Loss: 0.00387746
Iteration 4/25 | Loss: 0.00387746
Iteration 5/25 | Loss: 0.00387746
Iteration 6/25 | Loss: 0.00387746
Iteration 7/25 | Loss: 0.00387746
Iteration 8/25 | Loss: 0.00387746
Iteration 9/25 | Loss: 0.00387746
Iteration 10/25 | Loss: 0.00387745
Iteration 11/25 | Loss: 0.00387745
Iteration 12/25 | Loss: 0.00387745
Iteration 13/25 | Loss: 0.00387745
Iteration 14/25 | Loss: 0.00387745
Iteration 15/25 | Loss: 0.00387745
Iteration 16/25 | Loss: 0.00387745
Iteration 17/25 | Loss: 0.00387745
Iteration 18/25 | Loss: 0.00387745
Iteration 19/25 | Loss: 0.00387745
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0038774542044848204, 0.0038774542044848204, 0.0038774542044848204, 0.0038774542044848204, 0.0038774542044848204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038774542044848204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387745
Iteration 2/1000 | Loss: 0.00092040
Iteration 3/1000 | Loss: 0.00069099
Iteration 4/1000 | Loss: 0.00037032
Iteration 5/1000 | Loss: 0.00150433
Iteration 6/1000 | Loss: 0.00072274
Iteration 7/1000 | Loss: 0.00076755
Iteration 8/1000 | Loss: 0.00051923
Iteration 9/1000 | Loss: 0.00035194
Iteration 10/1000 | Loss: 0.00035477
Iteration 11/1000 | Loss: 0.00038418
Iteration 12/1000 | Loss: 0.00029076
Iteration 13/1000 | Loss: 0.00035720
Iteration 14/1000 | Loss: 0.00048944
Iteration 15/1000 | Loss: 0.00028925
Iteration 16/1000 | Loss: 0.00088498
Iteration 17/1000 | Loss: 0.00027886
Iteration 18/1000 | Loss: 0.00058887
Iteration 19/1000 | Loss: 0.00017781
Iteration 20/1000 | Loss: 0.00030278
Iteration 21/1000 | Loss: 0.00018399
Iteration 22/1000 | Loss: 0.00031087
Iteration 23/1000 | Loss: 0.00035515
Iteration 24/1000 | Loss: 0.00063428
Iteration 25/1000 | Loss: 0.00030682
Iteration 26/1000 | Loss: 0.00064529
Iteration 27/1000 | Loss: 0.00018771
Iteration 28/1000 | Loss: 0.00016490
Iteration 29/1000 | Loss: 0.00018444
Iteration 30/1000 | Loss: 0.00030648
Iteration 31/1000 | Loss: 0.00016734
Iteration 32/1000 | Loss: 0.00033464
Iteration 33/1000 | Loss: 0.00185511
Iteration 34/1000 | Loss: 0.00045124
Iteration 35/1000 | Loss: 0.00060933
Iteration 36/1000 | Loss: 0.00029641
Iteration 37/1000 | Loss: 0.00032885
Iteration 38/1000 | Loss: 0.00021101
Iteration 39/1000 | Loss: 0.00021264
Iteration 40/1000 | Loss: 0.00014537
Iteration 41/1000 | Loss: 0.00022646
Iteration 42/1000 | Loss: 0.00013144
Iteration 43/1000 | Loss: 0.00018505
Iteration 44/1000 | Loss: 0.00021923
Iteration 45/1000 | Loss: 0.00035671
Iteration 46/1000 | Loss: 0.00034929
Iteration 47/1000 | Loss: 0.00015103
Iteration 48/1000 | Loss: 0.00027622
Iteration 49/1000 | Loss: 0.00011145
Iteration 50/1000 | Loss: 0.00012912
Iteration 51/1000 | Loss: 0.00010765
Iteration 52/1000 | Loss: 0.00052059
Iteration 53/1000 | Loss: 0.00046414
Iteration 54/1000 | Loss: 0.00018332
Iteration 55/1000 | Loss: 0.00012955
Iteration 56/1000 | Loss: 0.00017490
Iteration 57/1000 | Loss: 0.00023299
Iteration 58/1000 | Loss: 0.00008738
Iteration 59/1000 | Loss: 0.00022085
Iteration 60/1000 | Loss: 0.00008465
Iteration 61/1000 | Loss: 0.00015283
Iteration 62/1000 | Loss: 0.00008444
Iteration 63/1000 | Loss: 0.00007768
Iteration 64/1000 | Loss: 0.00017724
Iteration 65/1000 | Loss: 0.00017606
Iteration 66/1000 | Loss: 0.00127939
Iteration 67/1000 | Loss: 0.00011049
Iteration 68/1000 | Loss: 0.00007189
Iteration 69/1000 | Loss: 0.00007251
Iteration 70/1000 | Loss: 0.00007821
Iteration 71/1000 | Loss: 0.00023058
Iteration 72/1000 | Loss: 0.00025896
Iteration 73/1000 | Loss: 0.00011502
Iteration 74/1000 | Loss: 0.00007857
Iteration 75/1000 | Loss: 0.00008741
Iteration 76/1000 | Loss: 0.00008631
Iteration 77/1000 | Loss: 0.00007082
Iteration 78/1000 | Loss: 0.00021663
Iteration 79/1000 | Loss: 0.00015461
Iteration 80/1000 | Loss: 0.00052993
Iteration 81/1000 | Loss: 0.00007613
Iteration 82/1000 | Loss: 0.00006278
Iteration 83/1000 | Loss: 0.00016050
Iteration 84/1000 | Loss: 0.00005978
Iteration 85/1000 | Loss: 0.00013162
Iteration 86/1000 | Loss: 0.00006208
Iteration 87/1000 | Loss: 0.00005328
Iteration 88/1000 | Loss: 0.00005090
Iteration 89/1000 | Loss: 0.00004945
Iteration 90/1000 | Loss: 0.00004858
Iteration 91/1000 | Loss: 0.00004809
Iteration 92/1000 | Loss: 0.00004954
Iteration 93/1000 | Loss: 0.00004757
Iteration 94/1000 | Loss: 0.00024881
Iteration 95/1000 | Loss: 0.00014987
Iteration 96/1000 | Loss: 0.00008601
Iteration 97/1000 | Loss: 0.00005764
Iteration 98/1000 | Loss: 0.00004965
Iteration 99/1000 | Loss: 0.00004792
Iteration 100/1000 | Loss: 0.00004617
Iteration 101/1000 | Loss: 0.00004472
Iteration 102/1000 | Loss: 0.00004405
Iteration 103/1000 | Loss: 0.00004351
Iteration 104/1000 | Loss: 0.00004314
Iteration 105/1000 | Loss: 0.00004290
Iteration 106/1000 | Loss: 0.00004283
Iteration 107/1000 | Loss: 0.00004279
Iteration 108/1000 | Loss: 0.00004277
Iteration 109/1000 | Loss: 0.00004264
Iteration 110/1000 | Loss: 0.00004257
Iteration 111/1000 | Loss: 0.00004251
Iteration 112/1000 | Loss: 0.00004250
Iteration 113/1000 | Loss: 0.00004246
Iteration 114/1000 | Loss: 0.00004246
Iteration 115/1000 | Loss: 0.00004246
Iteration 116/1000 | Loss: 0.00004246
Iteration 117/1000 | Loss: 0.00004246
Iteration 118/1000 | Loss: 0.00004246
Iteration 119/1000 | Loss: 0.00004246
Iteration 120/1000 | Loss: 0.00004246
Iteration 121/1000 | Loss: 0.00004246
Iteration 122/1000 | Loss: 0.00004246
Iteration 123/1000 | Loss: 0.00004246
Iteration 124/1000 | Loss: 0.00004246
Iteration 125/1000 | Loss: 0.00004246
Iteration 126/1000 | Loss: 0.00004246
Iteration 127/1000 | Loss: 0.00004245
Iteration 128/1000 | Loss: 0.00004245
Iteration 129/1000 | Loss: 0.00004245
Iteration 130/1000 | Loss: 0.00004245
Iteration 131/1000 | Loss: 0.00004245
Iteration 132/1000 | Loss: 0.00012940
Iteration 133/1000 | Loss: 0.00015651
Iteration 134/1000 | Loss: 0.00007014
Iteration 135/1000 | Loss: 0.00004736
Iteration 136/1000 | Loss: 0.00004507
Iteration 137/1000 | Loss: 0.00004205
Iteration 138/1000 | Loss: 0.00004161
Iteration 139/1000 | Loss: 0.00004129
Iteration 140/1000 | Loss: 0.00004110
Iteration 141/1000 | Loss: 0.00004102
Iteration 142/1000 | Loss: 0.00004097
Iteration 143/1000 | Loss: 0.00004097
Iteration 144/1000 | Loss: 0.00004096
Iteration 145/1000 | Loss: 0.00004096
Iteration 146/1000 | Loss: 0.00004096
Iteration 147/1000 | Loss: 0.00004096
Iteration 148/1000 | Loss: 0.00004096
Iteration 149/1000 | Loss: 0.00004096
Iteration 150/1000 | Loss: 0.00004095
Iteration 151/1000 | Loss: 0.00004094
Iteration 152/1000 | Loss: 0.00004094
Iteration 153/1000 | Loss: 0.00004094
Iteration 154/1000 | Loss: 0.00004093
Iteration 155/1000 | Loss: 0.00004092
Iteration 156/1000 | Loss: 0.00004092
Iteration 157/1000 | Loss: 0.00004092
Iteration 158/1000 | Loss: 0.00004091
Iteration 159/1000 | Loss: 0.00004091
Iteration 160/1000 | Loss: 0.00004091
Iteration 161/1000 | Loss: 0.00004090
Iteration 162/1000 | Loss: 0.00004090
Iteration 163/1000 | Loss: 0.00004090
Iteration 164/1000 | Loss: 0.00004090
Iteration 165/1000 | Loss: 0.00004090
Iteration 166/1000 | Loss: 0.00004090
Iteration 167/1000 | Loss: 0.00004090
Iteration 168/1000 | Loss: 0.00004090
Iteration 169/1000 | Loss: 0.00004089
Iteration 170/1000 | Loss: 0.00004089
Iteration 171/1000 | Loss: 0.00004089
Iteration 172/1000 | Loss: 0.00004089
Iteration 173/1000 | Loss: 0.00004089
Iteration 174/1000 | Loss: 0.00004089
Iteration 175/1000 | Loss: 0.00004089
Iteration 176/1000 | Loss: 0.00004089
Iteration 177/1000 | Loss: 0.00004089
Iteration 178/1000 | Loss: 0.00004089
Iteration 179/1000 | Loss: 0.00004089
Iteration 180/1000 | Loss: 0.00004089
Iteration 181/1000 | Loss: 0.00004089
Iteration 182/1000 | Loss: 0.00004089
Iteration 183/1000 | Loss: 0.00004088
Iteration 184/1000 | Loss: 0.00004088
Iteration 185/1000 | Loss: 0.00004088
Iteration 186/1000 | Loss: 0.00004088
Iteration 187/1000 | Loss: 0.00004088
Iteration 188/1000 | Loss: 0.00004088
Iteration 189/1000 | Loss: 0.00004088
Iteration 190/1000 | Loss: 0.00004088
Iteration 191/1000 | Loss: 0.00004088
Iteration 192/1000 | Loss: 0.00004088
Iteration 193/1000 | Loss: 0.00004088
Iteration 194/1000 | Loss: 0.00004088
Iteration 195/1000 | Loss: 0.00004088
Iteration 196/1000 | Loss: 0.00004088
Iteration 197/1000 | Loss: 0.00004088
Iteration 198/1000 | Loss: 0.00004088
Iteration 199/1000 | Loss: 0.00004088
Iteration 200/1000 | Loss: 0.00004088
Iteration 201/1000 | Loss: 0.00004088
Iteration 202/1000 | Loss: 0.00004088
Iteration 203/1000 | Loss: 0.00004088
Iteration 204/1000 | Loss: 0.00004088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [4.0884377085603774e-05, 4.0884377085603774e-05, 4.0884377085603774e-05, 4.0884377085603774e-05, 4.0884377085603774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0884377085603774e-05

Optimization complete. Final v2v error: 4.8590168952941895 mm

Highest mean error: 13.554685592651367 mm for frame 17

Lowest mean error: 4.247442245483398 mm for frame 58

Saving results

Total time: 649.7186081409454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01141114
Iteration 2/25 | Loss: 0.01141113
Iteration 3/25 | Loss: 0.01141113
Iteration 4/25 | Loss: 0.01141113
Iteration 5/25 | Loss: 0.00248562
Iteration 6/25 | Loss: 0.00237448
Iteration 7/25 | Loss: 0.00153297
Iteration 8/25 | Loss: 0.00144232
Iteration 9/25 | Loss: 0.00146218
Iteration 10/25 | Loss: 0.00141167
Iteration 11/25 | Loss: 0.00135384
Iteration 12/25 | Loss: 0.00130270
Iteration 13/25 | Loss: 0.00122844
Iteration 14/25 | Loss: 0.00119181
Iteration 15/25 | Loss: 0.00121088
Iteration 16/25 | Loss: 0.00117449
Iteration 17/25 | Loss: 0.00116183
Iteration 18/25 | Loss: 0.00117725
Iteration 19/25 | Loss: 0.00115737
Iteration 20/25 | Loss: 0.00116505
Iteration 21/25 | Loss: 0.00118260
Iteration 22/25 | Loss: 0.00114414
Iteration 23/25 | Loss: 0.00113301
Iteration 24/25 | Loss: 0.00113219
Iteration 25/25 | Loss: 0.00113060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54347956
Iteration 2/25 | Loss: 0.00265415
Iteration 3/25 | Loss: 0.00248363
Iteration 4/25 | Loss: 0.00248363
Iteration 5/25 | Loss: 0.00248363
Iteration 6/25 | Loss: 0.00248363
Iteration 7/25 | Loss: 0.00248363
Iteration 8/25 | Loss: 0.00248363
Iteration 9/25 | Loss: 0.00248363
Iteration 10/25 | Loss: 0.00248363
Iteration 11/25 | Loss: 0.00248363
Iteration 12/25 | Loss: 0.00248363
Iteration 13/25 | Loss: 0.00248363
Iteration 14/25 | Loss: 0.00248363
Iteration 15/25 | Loss: 0.00248363
Iteration 16/25 | Loss: 0.00248363
Iteration 17/25 | Loss: 0.00248363
Iteration 18/25 | Loss: 0.00248363
Iteration 19/25 | Loss: 0.00248363
Iteration 20/25 | Loss: 0.00248363
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024836251977831125, 0.0024836251977831125, 0.0024836251977831125, 0.0024836251977831125, 0.0024836251977831125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024836251977831125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248363
Iteration 2/1000 | Loss: 0.00318501
Iteration 3/1000 | Loss: 0.00094757
Iteration 4/1000 | Loss: 0.00470598
Iteration 5/1000 | Loss: 0.00362649
Iteration 6/1000 | Loss: 0.00163084
Iteration 7/1000 | Loss: 0.00332937
Iteration 8/1000 | Loss: 0.00019996
Iteration 9/1000 | Loss: 0.00008782
Iteration 10/1000 | Loss: 0.00006118
Iteration 11/1000 | Loss: 0.00306604
Iteration 12/1000 | Loss: 0.00249909
Iteration 13/1000 | Loss: 0.00273763
Iteration 14/1000 | Loss: 0.00066858
Iteration 15/1000 | Loss: 0.00097588
Iteration 16/1000 | Loss: 0.00074576
Iteration 17/1000 | Loss: 0.00090447
Iteration 18/1000 | Loss: 0.00007660
Iteration 19/1000 | Loss: 0.00006123
Iteration 20/1000 | Loss: 0.00113155
Iteration 21/1000 | Loss: 0.00088243
Iteration 22/1000 | Loss: 0.00189632
Iteration 23/1000 | Loss: 0.00118270
Iteration 24/1000 | Loss: 0.00214800
Iteration 25/1000 | Loss: 0.00079854
Iteration 26/1000 | Loss: 0.00037480
Iteration 27/1000 | Loss: 0.00051538
Iteration 28/1000 | Loss: 0.00118764
Iteration 29/1000 | Loss: 0.00060327
Iteration 30/1000 | Loss: 0.00056517
Iteration 31/1000 | Loss: 0.00093617
Iteration 32/1000 | Loss: 0.00063401
Iteration 33/1000 | Loss: 0.00069726
Iteration 34/1000 | Loss: 0.00043801
Iteration 35/1000 | Loss: 0.00069991
Iteration 36/1000 | Loss: 0.00081930
Iteration 37/1000 | Loss: 0.00086138
Iteration 38/1000 | Loss: 0.00073588
Iteration 39/1000 | Loss: 0.00053695
Iteration 40/1000 | Loss: 0.00006536
Iteration 41/1000 | Loss: 0.00005348
Iteration 42/1000 | Loss: 0.00004765
Iteration 43/1000 | Loss: 0.00192437
Iteration 44/1000 | Loss: 0.00153895
Iteration 45/1000 | Loss: 0.00118333
Iteration 46/1000 | Loss: 0.00029625
Iteration 47/1000 | Loss: 0.00007674
Iteration 48/1000 | Loss: 0.00047881
Iteration 49/1000 | Loss: 0.00057092
Iteration 50/1000 | Loss: 0.00040716
Iteration 51/1000 | Loss: 0.00024936
Iteration 52/1000 | Loss: 0.00028562
Iteration 53/1000 | Loss: 0.00032530
Iteration 54/1000 | Loss: 0.00035498
Iteration 55/1000 | Loss: 0.00021836
Iteration 56/1000 | Loss: 0.00026904
Iteration 57/1000 | Loss: 0.00035791
Iteration 58/1000 | Loss: 0.00033233
Iteration 59/1000 | Loss: 0.00019583
Iteration 60/1000 | Loss: 0.00074771
Iteration 61/1000 | Loss: 0.00047821
Iteration 62/1000 | Loss: 0.00006492
Iteration 63/1000 | Loss: 0.00054475
Iteration 64/1000 | Loss: 0.00045208
Iteration 65/1000 | Loss: 0.00023891
Iteration 66/1000 | Loss: 0.00011356
Iteration 67/1000 | Loss: 0.00013662
Iteration 68/1000 | Loss: 0.00015829
Iteration 69/1000 | Loss: 0.00012496
Iteration 70/1000 | Loss: 0.00037182
Iteration 71/1000 | Loss: 0.00014407
Iteration 72/1000 | Loss: 0.00015940
Iteration 73/1000 | Loss: 0.00030689
Iteration 74/1000 | Loss: 0.00025757
Iteration 75/1000 | Loss: 0.00035294
Iteration 76/1000 | Loss: 0.00047313
Iteration 77/1000 | Loss: 0.00052015
Iteration 78/1000 | Loss: 0.00054686
Iteration 79/1000 | Loss: 0.00043227
Iteration 80/1000 | Loss: 0.00011715
Iteration 81/1000 | Loss: 0.00003892
Iteration 82/1000 | Loss: 0.00003563
Iteration 83/1000 | Loss: 0.00003363
Iteration 84/1000 | Loss: 0.00003227
Iteration 85/1000 | Loss: 0.00003141
Iteration 86/1000 | Loss: 0.00013019
Iteration 87/1000 | Loss: 0.00003785
Iteration 88/1000 | Loss: 0.00012232
Iteration 89/1000 | Loss: 0.00011795
Iteration 90/1000 | Loss: 0.00012294
Iteration 91/1000 | Loss: 0.00011646
Iteration 92/1000 | Loss: 0.00008623
Iteration 93/1000 | Loss: 0.00011834
Iteration 94/1000 | Loss: 0.00008633
Iteration 95/1000 | Loss: 0.00010495
Iteration 96/1000 | Loss: 0.00006606
Iteration 97/1000 | Loss: 0.00010506
Iteration 98/1000 | Loss: 0.00011422
Iteration 99/1000 | Loss: 0.00020197
Iteration 100/1000 | Loss: 0.00015746
Iteration 101/1000 | Loss: 0.00019601
Iteration 102/1000 | Loss: 0.00003032
Iteration 103/1000 | Loss: 0.00002857
Iteration 104/1000 | Loss: 0.00002728
Iteration 105/1000 | Loss: 0.00002662
Iteration 106/1000 | Loss: 0.00002625
Iteration 107/1000 | Loss: 0.00002604
Iteration 108/1000 | Loss: 0.00002580
Iteration 109/1000 | Loss: 0.00002573
Iteration 110/1000 | Loss: 0.00002572
Iteration 111/1000 | Loss: 0.00002570
Iteration 112/1000 | Loss: 0.00002568
Iteration 113/1000 | Loss: 0.00002568
Iteration 114/1000 | Loss: 0.00002567
Iteration 115/1000 | Loss: 0.00002566
Iteration 116/1000 | Loss: 0.00002566
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002557
Iteration 119/1000 | Loss: 0.00002556
Iteration 120/1000 | Loss: 0.00002550
Iteration 121/1000 | Loss: 0.00002548
Iteration 122/1000 | Loss: 0.00002548
Iteration 123/1000 | Loss: 0.00002547
Iteration 124/1000 | Loss: 0.00002546
Iteration 125/1000 | Loss: 0.00002545
Iteration 126/1000 | Loss: 0.00002544
Iteration 127/1000 | Loss: 0.00002543
Iteration 128/1000 | Loss: 0.00002543
Iteration 129/1000 | Loss: 0.00002543
Iteration 130/1000 | Loss: 0.00002543
Iteration 131/1000 | Loss: 0.00002543
Iteration 132/1000 | Loss: 0.00002543
Iteration 133/1000 | Loss: 0.00002542
Iteration 134/1000 | Loss: 0.00002542
Iteration 135/1000 | Loss: 0.00002541
Iteration 136/1000 | Loss: 0.00002541
Iteration 137/1000 | Loss: 0.00002541
Iteration 138/1000 | Loss: 0.00002540
Iteration 139/1000 | Loss: 0.00002540
Iteration 140/1000 | Loss: 0.00002540
Iteration 141/1000 | Loss: 0.00002540
Iteration 142/1000 | Loss: 0.00002540
Iteration 143/1000 | Loss: 0.00002539
Iteration 144/1000 | Loss: 0.00002539
Iteration 145/1000 | Loss: 0.00002539
Iteration 146/1000 | Loss: 0.00002539
Iteration 147/1000 | Loss: 0.00002538
Iteration 148/1000 | Loss: 0.00002538
Iteration 149/1000 | Loss: 0.00002538
Iteration 150/1000 | Loss: 0.00002538
Iteration 151/1000 | Loss: 0.00002538
Iteration 152/1000 | Loss: 0.00002537
Iteration 153/1000 | Loss: 0.00002537
Iteration 154/1000 | Loss: 0.00002537
Iteration 155/1000 | Loss: 0.00002537
Iteration 156/1000 | Loss: 0.00002537
Iteration 157/1000 | Loss: 0.00002536
Iteration 158/1000 | Loss: 0.00002536
Iteration 159/1000 | Loss: 0.00002536
Iteration 160/1000 | Loss: 0.00002536
Iteration 161/1000 | Loss: 0.00002536
Iteration 162/1000 | Loss: 0.00002536
Iteration 163/1000 | Loss: 0.00002536
Iteration 164/1000 | Loss: 0.00002536
Iteration 165/1000 | Loss: 0.00002536
Iteration 166/1000 | Loss: 0.00002536
Iteration 167/1000 | Loss: 0.00002536
Iteration 168/1000 | Loss: 0.00002536
Iteration 169/1000 | Loss: 0.00002536
Iteration 170/1000 | Loss: 0.00002536
Iteration 171/1000 | Loss: 0.00002536
Iteration 172/1000 | Loss: 0.00002536
Iteration 173/1000 | Loss: 0.00002536
Iteration 174/1000 | Loss: 0.00002536
Iteration 175/1000 | Loss: 0.00002536
Iteration 176/1000 | Loss: 0.00002536
Iteration 177/1000 | Loss: 0.00002536
Iteration 178/1000 | Loss: 0.00002536
Iteration 179/1000 | Loss: 0.00002536
Iteration 180/1000 | Loss: 0.00002536
Iteration 181/1000 | Loss: 0.00002536
Iteration 182/1000 | Loss: 0.00002536
Iteration 183/1000 | Loss: 0.00002536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.536112879170105e-05, 2.536112879170105e-05, 2.536112879170105e-05, 2.536112879170105e-05, 2.536112879170105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.536112879170105e-05

Optimization complete. Final v2v error: 4.316431045532227 mm

Highest mean error: 11.127379417419434 mm for frame 217

Lowest mean error: 3.956989288330078 mm for frame 203

Saving results

Total time: 606.3862662315369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530779
Iteration 2/25 | Loss: 0.00124735
Iteration 3/25 | Loss: 0.00115106
Iteration 4/25 | Loss: 0.00112275
Iteration 5/25 | Loss: 0.00111301
Iteration 6/25 | Loss: 0.00111110
Iteration 7/25 | Loss: 0.00111110
Iteration 8/25 | Loss: 0.00111110
Iteration 9/25 | Loss: 0.00111110
Iteration 10/25 | Loss: 0.00111110
Iteration 11/25 | Loss: 0.00111110
Iteration 12/25 | Loss: 0.00111110
Iteration 13/25 | Loss: 0.00111110
Iteration 14/25 | Loss: 0.00111110
Iteration 15/25 | Loss: 0.00111110
Iteration 16/25 | Loss: 0.00111110
Iteration 17/25 | Loss: 0.00111110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011110982159152627, 0.0011110982159152627, 0.0011110982159152627, 0.0011110982159152627, 0.0011110982159152627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011110982159152627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23419511
Iteration 2/25 | Loss: 0.00150739
Iteration 3/25 | Loss: 0.00150737
Iteration 4/25 | Loss: 0.00150736
Iteration 5/25 | Loss: 0.00150736
Iteration 6/25 | Loss: 0.00150736
Iteration 7/25 | Loss: 0.00150736
Iteration 8/25 | Loss: 0.00150736
Iteration 9/25 | Loss: 0.00150736
Iteration 10/25 | Loss: 0.00150736
Iteration 11/25 | Loss: 0.00150736
Iteration 12/25 | Loss: 0.00150736
Iteration 13/25 | Loss: 0.00150736
Iteration 14/25 | Loss: 0.00150736
Iteration 15/25 | Loss: 0.00150736
Iteration 16/25 | Loss: 0.00150736
Iteration 17/25 | Loss: 0.00150736
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015073613030835986, 0.0015073613030835986, 0.0015073613030835986, 0.0015073613030835986, 0.0015073613030835986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015073613030835986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150736
Iteration 2/1000 | Loss: 0.00006132
Iteration 3/1000 | Loss: 0.00003851
Iteration 4/1000 | Loss: 0.00003229
Iteration 5/1000 | Loss: 0.00003071
Iteration 6/1000 | Loss: 0.00002879
Iteration 7/1000 | Loss: 0.00002787
Iteration 8/1000 | Loss: 0.00002710
Iteration 9/1000 | Loss: 0.00002649
Iteration 10/1000 | Loss: 0.00002600
Iteration 11/1000 | Loss: 0.00002564
Iteration 12/1000 | Loss: 0.00002540
Iteration 13/1000 | Loss: 0.00002522
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002506
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002505
Iteration 20/1000 | Loss: 0.00002505
Iteration 21/1000 | Loss: 0.00002504
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002502
Iteration 25/1000 | Loss: 0.00002501
Iteration 26/1000 | Loss: 0.00002501
Iteration 27/1000 | Loss: 0.00002500
Iteration 28/1000 | Loss: 0.00002499
Iteration 29/1000 | Loss: 0.00002499
Iteration 30/1000 | Loss: 0.00002499
Iteration 31/1000 | Loss: 0.00002499
Iteration 32/1000 | Loss: 0.00002499
Iteration 33/1000 | Loss: 0.00002499
Iteration 34/1000 | Loss: 0.00002499
Iteration 35/1000 | Loss: 0.00002499
Iteration 36/1000 | Loss: 0.00002499
Iteration 37/1000 | Loss: 0.00002499
Iteration 38/1000 | Loss: 0.00002498
Iteration 39/1000 | Loss: 0.00002498
Iteration 40/1000 | Loss: 0.00002498
Iteration 41/1000 | Loss: 0.00002498
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002498
Iteration 44/1000 | Loss: 0.00002496
Iteration 45/1000 | Loss: 0.00002495
Iteration 46/1000 | Loss: 0.00002495
Iteration 47/1000 | Loss: 0.00002495
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002495
Iteration 51/1000 | Loss: 0.00002495
Iteration 52/1000 | Loss: 0.00002495
Iteration 53/1000 | Loss: 0.00002495
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00002495
Iteration 56/1000 | Loss: 0.00002494
Iteration 57/1000 | Loss: 0.00002494
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002494
Iteration 61/1000 | Loss: 0.00002494
Iteration 62/1000 | Loss: 0.00002494
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00002493
Iteration 65/1000 | Loss: 0.00002492
Iteration 66/1000 | Loss: 0.00002492
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00002491
Iteration 69/1000 | Loss: 0.00002491
Iteration 70/1000 | Loss: 0.00002490
Iteration 71/1000 | Loss: 0.00002490
Iteration 72/1000 | Loss: 0.00002490
Iteration 73/1000 | Loss: 0.00002490
Iteration 74/1000 | Loss: 0.00002490
Iteration 75/1000 | Loss: 0.00002490
Iteration 76/1000 | Loss: 0.00002490
Iteration 77/1000 | Loss: 0.00002489
Iteration 78/1000 | Loss: 0.00002489
Iteration 79/1000 | Loss: 0.00002489
Iteration 80/1000 | Loss: 0.00002489
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002488
Iteration 83/1000 | Loss: 0.00002488
Iteration 84/1000 | Loss: 0.00002488
Iteration 85/1000 | Loss: 0.00002487
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00002486
Iteration 96/1000 | Loss: 0.00002486
Iteration 97/1000 | Loss: 0.00002486
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002485
Iteration 113/1000 | Loss: 0.00002485
Iteration 114/1000 | Loss: 0.00002485
Iteration 115/1000 | Loss: 0.00002485
Iteration 116/1000 | Loss: 0.00002485
Iteration 117/1000 | Loss: 0.00002485
Iteration 118/1000 | Loss: 0.00002484
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002484
Iteration 121/1000 | Loss: 0.00002484
Iteration 122/1000 | Loss: 0.00002484
Iteration 123/1000 | Loss: 0.00002484
Iteration 124/1000 | Loss: 0.00002484
Iteration 125/1000 | Loss: 0.00002484
Iteration 126/1000 | Loss: 0.00002484
Iteration 127/1000 | Loss: 0.00002484
Iteration 128/1000 | Loss: 0.00002484
Iteration 129/1000 | Loss: 0.00002484
Iteration 130/1000 | Loss: 0.00002484
Iteration 131/1000 | Loss: 0.00002484
Iteration 132/1000 | Loss: 0.00002484
Iteration 133/1000 | Loss: 0.00002484
Iteration 134/1000 | Loss: 0.00002484
Iteration 135/1000 | Loss: 0.00002484
Iteration 136/1000 | Loss: 0.00002484
Iteration 137/1000 | Loss: 0.00002483
Iteration 138/1000 | Loss: 0.00002483
Iteration 139/1000 | Loss: 0.00002483
Iteration 140/1000 | Loss: 0.00002483
Iteration 141/1000 | Loss: 0.00002483
Iteration 142/1000 | Loss: 0.00002483
Iteration 143/1000 | Loss: 0.00002483
Iteration 144/1000 | Loss: 0.00002483
Iteration 145/1000 | Loss: 0.00002483
Iteration 146/1000 | Loss: 0.00002483
Iteration 147/1000 | Loss: 0.00002483
Iteration 148/1000 | Loss: 0.00002483
Iteration 149/1000 | Loss: 0.00002483
Iteration 150/1000 | Loss: 0.00002482
Iteration 151/1000 | Loss: 0.00002482
Iteration 152/1000 | Loss: 0.00002482
Iteration 153/1000 | Loss: 0.00002482
Iteration 154/1000 | Loss: 0.00002482
Iteration 155/1000 | Loss: 0.00002482
Iteration 156/1000 | Loss: 0.00002482
Iteration 157/1000 | Loss: 0.00002482
Iteration 158/1000 | Loss: 0.00002482
Iteration 159/1000 | Loss: 0.00002482
Iteration 160/1000 | Loss: 0.00002482
Iteration 161/1000 | Loss: 0.00002482
Iteration 162/1000 | Loss: 0.00002482
Iteration 163/1000 | Loss: 0.00002482
Iteration 164/1000 | Loss: 0.00002482
Iteration 165/1000 | Loss: 0.00002482
Iteration 166/1000 | Loss: 0.00002482
Iteration 167/1000 | Loss: 0.00002482
Iteration 168/1000 | Loss: 0.00002482
Iteration 169/1000 | Loss: 0.00002482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.4822567866067402e-05, 2.4822567866067402e-05, 2.4822567866067402e-05, 2.4822567866067402e-05, 2.4822567866067402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4822567866067402e-05

Optimization complete. Final v2v error: 4.251671314239502 mm

Highest mean error: 5.059316635131836 mm for frame 157

Lowest mean error: 3.4699769020080566 mm for frame 8

Saving results

Total time: 119.25344395637512
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527713
Iteration 2/25 | Loss: 0.00128160
Iteration 3/25 | Loss: 0.00111344
Iteration 4/25 | Loss: 0.00108862
Iteration 5/25 | Loss: 0.00108098
Iteration 6/25 | Loss: 0.00107850
Iteration 7/25 | Loss: 0.00107816
Iteration 8/25 | Loss: 0.00107816
Iteration 9/25 | Loss: 0.00107816
Iteration 10/25 | Loss: 0.00107816
Iteration 11/25 | Loss: 0.00107816
Iteration 12/25 | Loss: 0.00107816
Iteration 13/25 | Loss: 0.00107816
Iteration 14/25 | Loss: 0.00107816
Iteration 15/25 | Loss: 0.00107816
Iteration 16/25 | Loss: 0.00107816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010781630408018827, 0.0010781630408018827, 0.0010781630408018827, 0.0010781630408018827, 0.0010781630408018827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010781630408018827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.76798224
Iteration 2/25 | Loss: 0.00145546
Iteration 3/25 | Loss: 0.00145545
Iteration 4/25 | Loss: 0.00145545
Iteration 5/25 | Loss: 0.00145545
Iteration 6/25 | Loss: 0.00145545
Iteration 7/25 | Loss: 0.00145545
Iteration 8/25 | Loss: 0.00145545
Iteration 9/25 | Loss: 0.00145545
Iteration 10/25 | Loss: 0.00145545
Iteration 11/25 | Loss: 0.00145545
Iteration 12/25 | Loss: 0.00145545
Iteration 13/25 | Loss: 0.00145545
Iteration 14/25 | Loss: 0.00145545
Iteration 15/25 | Loss: 0.00145545
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014554507797583938, 0.0014554507797583938, 0.0014554507797583938, 0.0014554507797583938, 0.0014554507797583938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014554507797583938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145545
Iteration 2/1000 | Loss: 0.00005333
Iteration 3/1000 | Loss: 0.00003556
Iteration 4/1000 | Loss: 0.00002880
Iteration 5/1000 | Loss: 0.00002684
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00002470
Iteration 8/1000 | Loss: 0.00002418
Iteration 9/1000 | Loss: 0.00002380
Iteration 10/1000 | Loss: 0.00002338
Iteration 11/1000 | Loss: 0.00002309
Iteration 12/1000 | Loss: 0.00002285
Iteration 13/1000 | Loss: 0.00002283
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002265
Iteration 16/1000 | Loss: 0.00002264
Iteration 17/1000 | Loss: 0.00002262
Iteration 18/1000 | Loss: 0.00002261
Iteration 19/1000 | Loss: 0.00002257
Iteration 20/1000 | Loss: 0.00002256
Iteration 21/1000 | Loss: 0.00002256
Iteration 22/1000 | Loss: 0.00002256
Iteration 23/1000 | Loss: 0.00002255
Iteration 24/1000 | Loss: 0.00002255
Iteration 25/1000 | Loss: 0.00002254
Iteration 26/1000 | Loss: 0.00002252
Iteration 27/1000 | Loss: 0.00002252
Iteration 28/1000 | Loss: 0.00002252
Iteration 29/1000 | Loss: 0.00002251
Iteration 30/1000 | Loss: 0.00002251
Iteration 31/1000 | Loss: 0.00002250
Iteration 32/1000 | Loss: 0.00002250
Iteration 33/1000 | Loss: 0.00002250
Iteration 34/1000 | Loss: 0.00002250
Iteration 35/1000 | Loss: 0.00002250
Iteration 36/1000 | Loss: 0.00002250
Iteration 37/1000 | Loss: 0.00002247
Iteration 38/1000 | Loss: 0.00002247
Iteration 39/1000 | Loss: 0.00002246
Iteration 40/1000 | Loss: 0.00002246
Iteration 41/1000 | Loss: 0.00002245
Iteration 42/1000 | Loss: 0.00002245
Iteration 43/1000 | Loss: 0.00002245
Iteration 44/1000 | Loss: 0.00002244
Iteration 45/1000 | Loss: 0.00002244
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002243
Iteration 49/1000 | Loss: 0.00002242
Iteration 50/1000 | Loss: 0.00002242
Iteration 51/1000 | Loss: 0.00002242
Iteration 52/1000 | Loss: 0.00002242
Iteration 53/1000 | Loss: 0.00002242
Iteration 54/1000 | Loss: 0.00002242
Iteration 55/1000 | Loss: 0.00002242
Iteration 56/1000 | Loss: 0.00002242
Iteration 57/1000 | Loss: 0.00002242
Iteration 58/1000 | Loss: 0.00002242
Iteration 59/1000 | Loss: 0.00002241
Iteration 60/1000 | Loss: 0.00002241
Iteration 61/1000 | Loss: 0.00002241
Iteration 62/1000 | Loss: 0.00002241
Iteration 63/1000 | Loss: 0.00002241
Iteration 64/1000 | Loss: 0.00002241
Iteration 65/1000 | Loss: 0.00002241
Iteration 66/1000 | Loss: 0.00002241
Iteration 67/1000 | Loss: 0.00002241
Iteration 68/1000 | Loss: 0.00002241
Iteration 69/1000 | Loss: 0.00002240
Iteration 70/1000 | Loss: 0.00002240
Iteration 71/1000 | Loss: 0.00002240
Iteration 72/1000 | Loss: 0.00002239
Iteration 73/1000 | Loss: 0.00002239
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00002239
Iteration 76/1000 | Loss: 0.00002239
Iteration 77/1000 | Loss: 0.00002238
Iteration 78/1000 | Loss: 0.00002238
Iteration 79/1000 | Loss: 0.00002238
Iteration 80/1000 | Loss: 0.00002238
Iteration 81/1000 | Loss: 0.00002238
Iteration 82/1000 | Loss: 0.00002237
Iteration 83/1000 | Loss: 0.00002237
Iteration 84/1000 | Loss: 0.00002237
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002234
Iteration 95/1000 | Loss: 0.00002234
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002232
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002231
Iteration 105/1000 | Loss: 0.00002231
Iteration 106/1000 | Loss: 0.00002231
Iteration 107/1000 | Loss: 0.00002231
Iteration 108/1000 | Loss: 0.00002231
Iteration 109/1000 | Loss: 0.00002231
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002230
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00002230
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002228
Iteration 140/1000 | Loss: 0.00002228
Iteration 141/1000 | Loss: 0.00002228
Iteration 142/1000 | Loss: 0.00002228
Iteration 143/1000 | Loss: 0.00002228
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002228
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002228
Iteration 150/1000 | Loss: 0.00002227
Iteration 151/1000 | Loss: 0.00002227
Iteration 152/1000 | Loss: 0.00002227
Iteration 153/1000 | Loss: 0.00002227
Iteration 154/1000 | Loss: 0.00002227
Iteration 155/1000 | Loss: 0.00002227
Iteration 156/1000 | Loss: 0.00002227
Iteration 157/1000 | Loss: 0.00002227
Iteration 158/1000 | Loss: 0.00002227
Iteration 159/1000 | Loss: 0.00002227
Iteration 160/1000 | Loss: 0.00002227
Iteration 161/1000 | Loss: 0.00002227
Iteration 162/1000 | Loss: 0.00002227
Iteration 163/1000 | Loss: 0.00002227
Iteration 164/1000 | Loss: 0.00002227
Iteration 165/1000 | Loss: 0.00002227
Iteration 166/1000 | Loss: 0.00002227
Iteration 167/1000 | Loss: 0.00002227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.2271580746746622e-05, 2.2271580746746622e-05, 2.2271580746746622e-05, 2.2271580746746622e-05, 2.2271580746746622e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2271580746746622e-05

Optimization complete. Final v2v error: 4.0283050537109375 mm

Highest mean error: 4.765812397003174 mm for frame 60

Lowest mean error: 3.2676496505737305 mm for frame 13

Saving results

Total time: 121.79946851730347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912295
Iteration 2/25 | Loss: 0.00144927
Iteration 3/25 | Loss: 0.00117470
Iteration 4/25 | Loss: 0.00113031
Iteration 5/25 | Loss: 0.00112585
Iteration 6/25 | Loss: 0.00112556
Iteration 7/25 | Loss: 0.00112556
Iteration 8/25 | Loss: 0.00112556
Iteration 9/25 | Loss: 0.00112556
Iteration 10/25 | Loss: 0.00112556
Iteration 11/25 | Loss: 0.00112556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011255585122853518, 0.0011255585122853518, 0.0011255585122853518, 0.0011255585122853518, 0.0011255585122853518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011255585122853518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83096266
Iteration 2/25 | Loss: 0.00095518
Iteration 3/25 | Loss: 0.00095517
Iteration 4/25 | Loss: 0.00095517
Iteration 5/25 | Loss: 0.00095517
Iteration 6/25 | Loss: 0.00095517
Iteration 7/25 | Loss: 0.00095517
Iteration 8/25 | Loss: 0.00095517
Iteration 9/25 | Loss: 0.00095517
Iteration 10/25 | Loss: 0.00095517
Iteration 11/25 | Loss: 0.00095517
Iteration 12/25 | Loss: 0.00095517
Iteration 13/25 | Loss: 0.00095517
Iteration 14/25 | Loss: 0.00095517
Iteration 15/25 | Loss: 0.00095517
Iteration 16/25 | Loss: 0.00095517
Iteration 17/25 | Loss: 0.00095517
Iteration 18/25 | Loss: 0.00095517
Iteration 19/25 | Loss: 0.00095517
Iteration 20/25 | Loss: 0.00095517
Iteration 21/25 | Loss: 0.00095517
Iteration 22/25 | Loss: 0.00095517
Iteration 23/25 | Loss: 0.00095517
Iteration 24/25 | Loss: 0.00095517
Iteration 25/25 | Loss: 0.00095517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095517
Iteration 2/1000 | Loss: 0.00006656
Iteration 3/1000 | Loss: 0.00004772
Iteration 4/1000 | Loss: 0.00004374
Iteration 5/1000 | Loss: 0.00004106
Iteration 6/1000 | Loss: 0.00003947
Iteration 7/1000 | Loss: 0.00003810
Iteration 8/1000 | Loss: 0.00003711
Iteration 9/1000 | Loss: 0.00003639
Iteration 10/1000 | Loss: 0.00003598
Iteration 11/1000 | Loss: 0.00003574
Iteration 12/1000 | Loss: 0.00003560
Iteration 13/1000 | Loss: 0.00003547
Iteration 14/1000 | Loss: 0.00003547
Iteration 15/1000 | Loss: 0.00003547
Iteration 16/1000 | Loss: 0.00003545
Iteration 17/1000 | Loss: 0.00003543
Iteration 18/1000 | Loss: 0.00003540
Iteration 19/1000 | Loss: 0.00003540
Iteration 20/1000 | Loss: 0.00003536
Iteration 21/1000 | Loss: 0.00003535
Iteration 22/1000 | Loss: 0.00003535
Iteration 23/1000 | Loss: 0.00003533
Iteration 24/1000 | Loss: 0.00003533
Iteration 25/1000 | Loss: 0.00003533
Iteration 26/1000 | Loss: 0.00003532
Iteration 27/1000 | Loss: 0.00003532
Iteration 28/1000 | Loss: 0.00003532
Iteration 29/1000 | Loss: 0.00003532
Iteration 30/1000 | Loss: 0.00003532
Iteration 31/1000 | Loss: 0.00003532
Iteration 32/1000 | Loss: 0.00003531
Iteration 33/1000 | Loss: 0.00003531
Iteration 34/1000 | Loss: 0.00003530
Iteration 35/1000 | Loss: 0.00003525
Iteration 36/1000 | Loss: 0.00003525
Iteration 37/1000 | Loss: 0.00003525
Iteration 38/1000 | Loss: 0.00003525
Iteration 39/1000 | Loss: 0.00003525
Iteration 40/1000 | Loss: 0.00003525
Iteration 41/1000 | Loss: 0.00003525
Iteration 42/1000 | Loss: 0.00003525
Iteration 43/1000 | Loss: 0.00003524
Iteration 44/1000 | Loss: 0.00003524
Iteration 45/1000 | Loss: 0.00003524
Iteration 46/1000 | Loss: 0.00003524
Iteration 47/1000 | Loss: 0.00003524
Iteration 48/1000 | Loss: 0.00003524
Iteration 49/1000 | Loss: 0.00003523
Iteration 50/1000 | Loss: 0.00003523
Iteration 51/1000 | Loss: 0.00003523
Iteration 52/1000 | Loss: 0.00003523
Iteration 53/1000 | Loss: 0.00003523
Iteration 54/1000 | Loss: 0.00003522
Iteration 55/1000 | Loss: 0.00003522
Iteration 56/1000 | Loss: 0.00003522
Iteration 57/1000 | Loss: 0.00003522
Iteration 58/1000 | Loss: 0.00003522
Iteration 59/1000 | Loss: 0.00003522
Iteration 60/1000 | Loss: 0.00003522
Iteration 61/1000 | Loss: 0.00003522
Iteration 62/1000 | Loss: 0.00003522
Iteration 63/1000 | Loss: 0.00003522
Iteration 64/1000 | Loss: 0.00003522
Iteration 65/1000 | Loss: 0.00003522
Iteration 66/1000 | Loss: 0.00003522
Iteration 67/1000 | Loss: 0.00003521
Iteration 68/1000 | Loss: 0.00003521
Iteration 69/1000 | Loss: 0.00003521
Iteration 70/1000 | Loss: 0.00003521
Iteration 71/1000 | Loss: 0.00003521
Iteration 72/1000 | Loss: 0.00003521
Iteration 73/1000 | Loss: 0.00003520
Iteration 74/1000 | Loss: 0.00003520
Iteration 75/1000 | Loss: 0.00003520
Iteration 76/1000 | Loss: 0.00003520
Iteration 77/1000 | Loss: 0.00003520
Iteration 78/1000 | Loss: 0.00003520
Iteration 79/1000 | Loss: 0.00003519
Iteration 80/1000 | Loss: 0.00003519
Iteration 81/1000 | Loss: 0.00003519
Iteration 82/1000 | Loss: 0.00003519
Iteration 83/1000 | Loss: 0.00003519
Iteration 84/1000 | Loss: 0.00003519
Iteration 85/1000 | Loss: 0.00003519
Iteration 86/1000 | Loss: 0.00003519
Iteration 87/1000 | Loss: 0.00003519
Iteration 88/1000 | Loss: 0.00003519
Iteration 89/1000 | Loss: 0.00003519
Iteration 90/1000 | Loss: 0.00003519
Iteration 91/1000 | Loss: 0.00003519
Iteration 92/1000 | Loss: 0.00003519
Iteration 93/1000 | Loss: 0.00003519
Iteration 94/1000 | Loss: 0.00003519
Iteration 95/1000 | Loss: 0.00003519
Iteration 96/1000 | Loss: 0.00003519
Iteration 97/1000 | Loss: 0.00003519
Iteration 98/1000 | Loss: 0.00003519
Iteration 99/1000 | Loss: 0.00003519
Iteration 100/1000 | Loss: 0.00003519
Iteration 101/1000 | Loss: 0.00003519
Iteration 102/1000 | Loss: 0.00003519
Iteration 103/1000 | Loss: 0.00003519
Iteration 104/1000 | Loss: 0.00003519
Iteration 105/1000 | Loss: 0.00003519
Iteration 106/1000 | Loss: 0.00003519
Iteration 107/1000 | Loss: 0.00003519
Iteration 108/1000 | Loss: 0.00003519
Iteration 109/1000 | Loss: 0.00003519
Iteration 110/1000 | Loss: 0.00003519
Iteration 111/1000 | Loss: 0.00003519
Iteration 112/1000 | Loss: 0.00003519
Iteration 113/1000 | Loss: 0.00003519
Iteration 114/1000 | Loss: 0.00003519
Iteration 115/1000 | Loss: 0.00003519
Iteration 116/1000 | Loss: 0.00003519
Iteration 117/1000 | Loss: 0.00003519
Iteration 118/1000 | Loss: 0.00003519
Iteration 119/1000 | Loss: 0.00003519
Iteration 120/1000 | Loss: 0.00003519
Iteration 121/1000 | Loss: 0.00003519
Iteration 122/1000 | Loss: 0.00003519
Iteration 123/1000 | Loss: 0.00003519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.5186920285923406e-05, 3.5186920285923406e-05, 3.5186920285923406e-05, 3.5186920285923406e-05, 3.5186920285923406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5186920285923406e-05

Optimization complete. Final v2v error: 5.051977157592773 mm

Highest mean error: 5.325602054595947 mm for frame 125

Lowest mean error: 4.792063236236572 mm for frame 0

Saving results

Total time: 79.33802127838135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839903
Iteration 2/25 | Loss: 0.00159600
Iteration 3/25 | Loss: 0.00130198
Iteration 4/25 | Loss: 0.00123837
Iteration 5/25 | Loss: 0.00121658
Iteration 6/25 | Loss: 0.00121122
Iteration 7/25 | Loss: 0.00120908
Iteration 8/25 | Loss: 0.00120890
Iteration 9/25 | Loss: 0.00120890
Iteration 10/25 | Loss: 0.00120890
Iteration 11/25 | Loss: 0.00120890
Iteration 12/25 | Loss: 0.00120890
Iteration 13/25 | Loss: 0.00120890
Iteration 14/25 | Loss: 0.00120890
Iteration 15/25 | Loss: 0.00120890
Iteration 16/25 | Loss: 0.00120890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012088962830603123, 0.0012088962830603123, 0.0012088962830603123, 0.0012088962830603123, 0.0012088962830603123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012088962830603123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20024645
Iteration 2/25 | Loss: 0.00163911
Iteration 3/25 | Loss: 0.00163911
Iteration 4/25 | Loss: 0.00163911
Iteration 5/25 | Loss: 0.00163911
Iteration 6/25 | Loss: 0.00163911
Iteration 7/25 | Loss: 0.00163911
Iteration 8/25 | Loss: 0.00163911
Iteration 9/25 | Loss: 0.00163911
Iteration 10/25 | Loss: 0.00163911
Iteration 11/25 | Loss: 0.00163911
Iteration 12/25 | Loss: 0.00163911
Iteration 13/25 | Loss: 0.00163911
Iteration 14/25 | Loss: 0.00163911
Iteration 15/25 | Loss: 0.00163911
Iteration 16/25 | Loss: 0.00163911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016391113167628646, 0.0016391113167628646, 0.0016391113167628646, 0.0016391113167628646, 0.0016391113167628646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016391113167628646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163911
Iteration 2/1000 | Loss: 0.00009328
Iteration 3/1000 | Loss: 0.00005989
Iteration 4/1000 | Loss: 0.00005127
Iteration 5/1000 | Loss: 0.00004745
Iteration 6/1000 | Loss: 0.00004485
Iteration 7/1000 | Loss: 0.00004343
Iteration 8/1000 | Loss: 0.00004231
Iteration 9/1000 | Loss: 0.00004139
Iteration 10/1000 | Loss: 0.00004081
Iteration 11/1000 | Loss: 0.00004038
Iteration 12/1000 | Loss: 0.00003992
Iteration 13/1000 | Loss: 0.00003959
Iteration 14/1000 | Loss: 0.00003932
Iteration 15/1000 | Loss: 0.00003924
Iteration 16/1000 | Loss: 0.00003917
Iteration 17/1000 | Loss: 0.00003905
Iteration 18/1000 | Loss: 0.00003905
Iteration 19/1000 | Loss: 0.00003904
Iteration 20/1000 | Loss: 0.00003901
Iteration 21/1000 | Loss: 0.00003901
Iteration 22/1000 | Loss: 0.00003900
Iteration 23/1000 | Loss: 0.00003900
Iteration 24/1000 | Loss: 0.00003899
Iteration 25/1000 | Loss: 0.00003897
Iteration 26/1000 | Loss: 0.00003896
Iteration 27/1000 | Loss: 0.00003895
Iteration 28/1000 | Loss: 0.00003894
Iteration 29/1000 | Loss: 0.00003894
Iteration 30/1000 | Loss: 0.00003894
Iteration 31/1000 | Loss: 0.00003894
Iteration 32/1000 | Loss: 0.00003893
Iteration 33/1000 | Loss: 0.00003893
Iteration 34/1000 | Loss: 0.00003892
Iteration 35/1000 | Loss: 0.00003892
Iteration 36/1000 | Loss: 0.00003892
Iteration 37/1000 | Loss: 0.00003892
Iteration 38/1000 | Loss: 0.00003892
Iteration 39/1000 | Loss: 0.00003892
Iteration 40/1000 | Loss: 0.00003892
Iteration 41/1000 | Loss: 0.00003891
Iteration 42/1000 | Loss: 0.00003891
Iteration 43/1000 | Loss: 0.00003891
Iteration 44/1000 | Loss: 0.00003891
Iteration 45/1000 | Loss: 0.00003891
Iteration 46/1000 | Loss: 0.00003891
Iteration 47/1000 | Loss: 0.00003891
Iteration 48/1000 | Loss: 0.00003890
Iteration 49/1000 | Loss: 0.00003889
Iteration 50/1000 | Loss: 0.00003889
Iteration 51/1000 | Loss: 0.00003889
Iteration 52/1000 | Loss: 0.00003889
Iteration 53/1000 | Loss: 0.00003888
Iteration 54/1000 | Loss: 0.00003888
Iteration 55/1000 | Loss: 0.00003888
Iteration 56/1000 | Loss: 0.00003888
Iteration 57/1000 | Loss: 0.00003888
Iteration 58/1000 | Loss: 0.00003888
Iteration 59/1000 | Loss: 0.00003887
Iteration 60/1000 | Loss: 0.00003887
Iteration 61/1000 | Loss: 0.00003887
Iteration 62/1000 | Loss: 0.00003887
Iteration 63/1000 | Loss: 0.00003887
Iteration 64/1000 | Loss: 0.00003887
Iteration 65/1000 | Loss: 0.00003887
Iteration 66/1000 | Loss: 0.00003887
Iteration 67/1000 | Loss: 0.00003887
Iteration 68/1000 | Loss: 0.00003886
Iteration 69/1000 | Loss: 0.00003886
Iteration 70/1000 | Loss: 0.00003886
Iteration 71/1000 | Loss: 0.00003886
Iteration 72/1000 | Loss: 0.00003886
Iteration 73/1000 | Loss: 0.00003886
Iteration 74/1000 | Loss: 0.00003885
Iteration 75/1000 | Loss: 0.00003885
Iteration 76/1000 | Loss: 0.00003885
Iteration 77/1000 | Loss: 0.00003885
Iteration 78/1000 | Loss: 0.00003885
Iteration 79/1000 | Loss: 0.00003885
Iteration 80/1000 | Loss: 0.00003885
Iteration 81/1000 | Loss: 0.00003885
Iteration 82/1000 | Loss: 0.00003885
Iteration 83/1000 | Loss: 0.00003885
Iteration 84/1000 | Loss: 0.00003885
Iteration 85/1000 | Loss: 0.00003885
Iteration 86/1000 | Loss: 0.00003885
Iteration 87/1000 | Loss: 0.00003885
Iteration 88/1000 | Loss: 0.00003885
Iteration 89/1000 | Loss: 0.00003885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.885109617840499e-05, 3.885109617840499e-05, 3.885109617840499e-05, 3.885109617840499e-05, 3.885109617840499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.885109617840499e-05

Optimization complete. Final v2v error: 5.208085060119629 mm

Highest mean error: 6.48588228225708 mm for frame 74

Lowest mean error: 4.31795597076416 mm for frame 4

Saving results

Total time: 116.72374296188354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032172
Iteration 2/25 | Loss: 0.01032172
Iteration 3/25 | Loss: 0.00417725
Iteration 4/25 | Loss: 0.00238252
Iteration 5/25 | Loss: 0.00192896
Iteration 6/25 | Loss: 0.00194665
Iteration 7/25 | Loss: 0.00169995
Iteration 8/25 | Loss: 0.00156890
Iteration 9/25 | Loss: 0.00147296
Iteration 10/25 | Loss: 0.00143902
Iteration 11/25 | Loss: 0.00141074
Iteration 12/25 | Loss: 0.00140418
Iteration 13/25 | Loss: 0.00139793
Iteration 14/25 | Loss: 0.00139181
Iteration 15/25 | Loss: 0.00138590
Iteration 16/25 | Loss: 0.00138324
Iteration 17/25 | Loss: 0.00138273
Iteration 18/25 | Loss: 0.00138131
Iteration 19/25 | Loss: 0.00138173
Iteration 20/25 | Loss: 0.00137765
Iteration 21/25 | Loss: 0.00137527
Iteration 22/25 | Loss: 0.00137770
Iteration 23/25 | Loss: 0.00137695
Iteration 24/25 | Loss: 0.00137833
Iteration 25/25 | Loss: 0.00137699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15125132
Iteration 2/25 | Loss: 0.00582928
Iteration 3/25 | Loss: 0.00582927
Iteration 4/25 | Loss: 0.00582927
Iteration 5/25 | Loss: 0.00582927
Iteration 6/25 | Loss: 0.00582927
Iteration 7/25 | Loss: 0.00582927
Iteration 8/25 | Loss: 0.00582927
Iteration 9/25 | Loss: 0.00582927
Iteration 10/25 | Loss: 0.00582927
Iteration 11/25 | Loss: 0.00582927
Iteration 12/25 | Loss: 0.00582927
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.005829272326081991, 0.005829272326081991, 0.005829272326081991, 0.005829272326081991, 0.005829272326081991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005829272326081991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00582927
Iteration 2/1000 | Loss: 0.00067917
Iteration 3/1000 | Loss: 0.00051419
Iteration 4/1000 | Loss: 0.00038483
Iteration 5/1000 | Loss: 0.00033805
Iteration 6/1000 | Loss: 0.00062701
Iteration 7/1000 | Loss: 0.00078486
Iteration 8/1000 | Loss: 0.00032646
Iteration 9/1000 | Loss: 0.00083964
Iteration 10/1000 | Loss: 0.00028495
Iteration 11/1000 | Loss: 0.00056802
Iteration 12/1000 | Loss: 0.00022621
Iteration 13/1000 | Loss: 0.00066739
Iteration 14/1000 | Loss: 0.00021553
Iteration 15/1000 | Loss: 0.00050622
Iteration 16/1000 | Loss: 0.00020423
Iteration 17/1000 | Loss: 0.00019684
Iteration 18/1000 | Loss: 0.00030340
Iteration 19/1000 | Loss: 0.00032630
Iteration 20/1000 | Loss: 0.00018943
Iteration 21/1000 | Loss: 0.00018700
Iteration 22/1000 | Loss: 0.00018522
Iteration 23/1000 | Loss: 0.00018353
Iteration 24/1000 | Loss: 0.00018211
Iteration 25/1000 | Loss: 0.00029905
Iteration 26/1000 | Loss: 0.00018633
Iteration 27/1000 | Loss: 0.00018167
Iteration 28/1000 | Loss: 0.00017967
Iteration 29/1000 | Loss: 0.00017811
Iteration 30/1000 | Loss: 0.00017717
Iteration 31/1000 | Loss: 0.00017681
Iteration 32/1000 | Loss: 0.00017656
Iteration 33/1000 | Loss: 0.00017637
Iteration 34/1000 | Loss: 0.00017623
Iteration 35/1000 | Loss: 0.00017613
Iteration 36/1000 | Loss: 0.00017613
Iteration 37/1000 | Loss: 0.00017613
Iteration 38/1000 | Loss: 0.00017613
Iteration 39/1000 | Loss: 0.00017613
Iteration 40/1000 | Loss: 0.00017613
Iteration 41/1000 | Loss: 0.00017612
Iteration 42/1000 | Loss: 0.00017612
Iteration 43/1000 | Loss: 0.00017612
Iteration 44/1000 | Loss: 0.00017612
Iteration 45/1000 | Loss: 0.00017612
Iteration 46/1000 | Loss: 0.00017612
Iteration 47/1000 | Loss: 0.00017609
Iteration 48/1000 | Loss: 0.00017609
Iteration 49/1000 | Loss: 0.00017608
Iteration 50/1000 | Loss: 0.00017608
Iteration 51/1000 | Loss: 0.00017608
Iteration 52/1000 | Loss: 0.00017607
Iteration 53/1000 | Loss: 0.00017607
Iteration 54/1000 | Loss: 0.00017606
Iteration 55/1000 | Loss: 0.00017605
Iteration 56/1000 | Loss: 0.00017604
Iteration 57/1000 | Loss: 0.00017603
Iteration 58/1000 | Loss: 0.00017602
Iteration 59/1000 | Loss: 0.00017601
Iteration 60/1000 | Loss: 0.00017601
Iteration 61/1000 | Loss: 0.00017601
Iteration 62/1000 | Loss: 0.00017600
Iteration 63/1000 | Loss: 0.00017600
Iteration 64/1000 | Loss: 0.00017599
Iteration 65/1000 | Loss: 0.00017599
Iteration 66/1000 | Loss: 0.00017599
Iteration 67/1000 | Loss: 0.00017599
Iteration 68/1000 | Loss: 0.00017599
Iteration 69/1000 | Loss: 0.00017599
Iteration 70/1000 | Loss: 0.00017599
Iteration 71/1000 | Loss: 0.00017599
Iteration 72/1000 | Loss: 0.00017599
Iteration 73/1000 | Loss: 0.00017599
Iteration 74/1000 | Loss: 0.00017599
Iteration 75/1000 | Loss: 0.00017598
Iteration 76/1000 | Loss: 0.00017598
Iteration 77/1000 | Loss: 0.00017598
Iteration 78/1000 | Loss: 0.00017598
Iteration 79/1000 | Loss: 0.00017597
Iteration 80/1000 | Loss: 0.00017597
Iteration 81/1000 | Loss: 0.00017597
Iteration 82/1000 | Loss: 0.00017597
Iteration 83/1000 | Loss: 0.00017597
Iteration 84/1000 | Loss: 0.00017596
Iteration 85/1000 | Loss: 0.00017596
Iteration 86/1000 | Loss: 0.00017596
Iteration 87/1000 | Loss: 0.00017596
Iteration 88/1000 | Loss: 0.00017596
Iteration 89/1000 | Loss: 0.00017596
Iteration 90/1000 | Loss: 0.00017596
Iteration 91/1000 | Loss: 0.00017596
Iteration 92/1000 | Loss: 0.00017596
Iteration 93/1000 | Loss: 0.00017595
Iteration 94/1000 | Loss: 0.00017595
Iteration 95/1000 | Loss: 0.00017595
Iteration 96/1000 | Loss: 0.00017595
Iteration 97/1000 | Loss: 0.00017595
Iteration 98/1000 | Loss: 0.00017595
Iteration 99/1000 | Loss: 0.00017595
Iteration 100/1000 | Loss: 0.00017595
Iteration 101/1000 | Loss: 0.00017595
Iteration 102/1000 | Loss: 0.00017595
Iteration 103/1000 | Loss: 0.00017595
Iteration 104/1000 | Loss: 0.00017595
Iteration 105/1000 | Loss: 0.00017595
Iteration 106/1000 | Loss: 0.00017595
Iteration 107/1000 | Loss: 0.00017595
Iteration 108/1000 | Loss: 0.00017595
Iteration 109/1000 | Loss: 0.00017595
Iteration 110/1000 | Loss: 0.00017595
Iteration 111/1000 | Loss: 0.00017595
Iteration 112/1000 | Loss: 0.00017595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [0.0001759513543220237, 0.0001759513543220237, 0.0001759513543220237, 0.0001759513543220237, 0.0001759513543220237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001759513543220237

Optimization complete. Final v2v error: 7.884449481964111 mm

Highest mean error: 11.734292030334473 mm for frame 141

Lowest mean error: 5.223445415496826 mm for frame 191

Saving results

Total time: 284.31404066085815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560091
Iteration 2/25 | Loss: 0.00123499
Iteration 3/25 | Loss: 0.00112431
Iteration 4/25 | Loss: 0.00110575
Iteration 5/25 | Loss: 0.00109968
Iteration 6/25 | Loss: 0.00109800
Iteration 7/25 | Loss: 0.00109791
Iteration 8/25 | Loss: 0.00109791
Iteration 9/25 | Loss: 0.00109791
Iteration 10/25 | Loss: 0.00109791
Iteration 11/25 | Loss: 0.00109791
Iteration 12/25 | Loss: 0.00109791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010979060316458344, 0.0010979060316458344, 0.0010979060316458344, 0.0010979060316458344, 0.0010979060316458344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010979060316458344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.92406774
Iteration 2/25 | Loss: 0.00153605
Iteration 3/25 | Loss: 0.00153605
Iteration 4/25 | Loss: 0.00153604
Iteration 5/25 | Loss: 0.00153604
Iteration 6/25 | Loss: 0.00153604
Iteration 7/25 | Loss: 0.00153604
Iteration 8/25 | Loss: 0.00153604
Iteration 9/25 | Loss: 0.00153604
Iteration 10/25 | Loss: 0.00153604
Iteration 11/25 | Loss: 0.00153604
Iteration 12/25 | Loss: 0.00153604
Iteration 13/25 | Loss: 0.00153604
Iteration 14/25 | Loss: 0.00153604
Iteration 15/25 | Loss: 0.00153604
Iteration 16/25 | Loss: 0.00153604
Iteration 17/25 | Loss: 0.00153604
Iteration 18/25 | Loss: 0.00153604
Iteration 19/25 | Loss: 0.00153604
Iteration 20/25 | Loss: 0.00153604
Iteration 21/25 | Loss: 0.00153604
Iteration 22/25 | Loss: 0.00153604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015360425459221005, 0.0015360425459221005, 0.0015360425459221005, 0.0015360425459221005, 0.0015360425459221005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015360425459221005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153604
Iteration 2/1000 | Loss: 0.00004699
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002103
Iteration 6/1000 | Loss: 0.00001978
Iteration 7/1000 | Loss: 0.00001896
Iteration 8/1000 | Loss: 0.00001847
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001788
Iteration 11/1000 | Loss: 0.00001769
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001750
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001737
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001733
Iteration 32/1000 | Loss: 0.00001733
Iteration 33/1000 | Loss: 0.00001733
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001732
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001732
Iteration 38/1000 | Loss: 0.00001731
Iteration 39/1000 | Loss: 0.00001731
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001730
Iteration 43/1000 | Loss: 0.00001730
Iteration 44/1000 | Loss: 0.00001730
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001729
Iteration 58/1000 | Loss: 0.00001729
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001728
Iteration 63/1000 | Loss: 0.00001728
Iteration 64/1000 | Loss: 0.00001728
Iteration 65/1000 | Loss: 0.00001728
Iteration 66/1000 | Loss: 0.00001728
Iteration 67/1000 | Loss: 0.00001728
Iteration 68/1000 | Loss: 0.00001728
Iteration 69/1000 | Loss: 0.00001728
Iteration 70/1000 | Loss: 0.00001728
Iteration 71/1000 | Loss: 0.00001728
Iteration 72/1000 | Loss: 0.00001727
Iteration 73/1000 | Loss: 0.00001727
Iteration 74/1000 | Loss: 0.00001727
Iteration 75/1000 | Loss: 0.00001727
Iteration 76/1000 | Loss: 0.00001727
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001727
Iteration 83/1000 | Loss: 0.00001727
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001725
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001724
Iteration 119/1000 | Loss: 0.00001724
Iteration 120/1000 | Loss: 0.00001724
Iteration 121/1000 | Loss: 0.00001724
Iteration 122/1000 | Loss: 0.00001724
Iteration 123/1000 | Loss: 0.00001724
Iteration 124/1000 | Loss: 0.00001723
Iteration 125/1000 | Loss: 0.00001723
Iteration 126/1000 | Loss: 0.00001723
Iteration 127/1000 | Loss: 0.00001723
Iteration 128/1000 | Loss: 0.00001723
Iteration 129/1000 | Loss: 0.00001723
Iteration 130/1000 | Loss: 0.00001723
Iteration 131/1000 | Loss: 0.00001723
Iteration 132/1000 | Loss: 0.00001723
Iteration 133/1000 | Loss: 0.00001723
Iteration 134/1000 | Loss: 0.00001722
Iteration 135/1000 | Loss: 0.00001722
Iteration 136/1000 | Loss: 0.00001722
Iteration 137/1000 | Loss: 0.00001722
Iteration 138/1000 | Loss: 0.00001722
Iteration 139/1000 | Loss: 0.00001722
Iteration 140/1000 | Loss: 0.00001722
Iteration 141/1000 | Loss: 0.00001722
Iteration 142/1000 | Loss: 0.00001722
Iteration 143/1000 | Loss: 0.00001721
Iteration 144/1000 | Loss: 0.00001721
Iteration 145/1000 | Loss: 0.00001721
Iteration 146/1000 | Loss: 0.00001721
Iteration 147/1000 | Loss: 0.00001720
Iteration 148/1000 | Loss: 0.00001720
Iteration 149/1000 | Loss: 0.00001720
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001720
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001720
Iteration 154/1000 | Loss: 0.00001720
Iteration 155/1000 | Loss: 0.00001720
Iteration 156/1000 | Loss: 0.00001720
Iteration 157/1000 | Loss: 0.00001720
Iteration 158/1000 | Loss: 0.00001720
Iteration 159/1000 | Loss: 0.00001719
Iteration 160/1000 | Loss: 0.00001719
Iteration 161/1000 | Loss: 0.00001719
Iteration 162/1000 | Loss: 0.00001719
Iteration 163/1000 | Loss: 0.00001719
Iteration 164/1000 | Loss: 0.00001719
Iteration 165/1000 | Loss: 0.00001719
Iteration 166/1000 | Loss: 0.00001719
Iteration 167/1000 | Loss: 0.00001719
Iteration 168/1000 | Loss: 0.00001719
Iteration 169/1000 | Loss: 0.00001719
Iteration 170/1000 | Loss: 0.00001719
Iteration 171/1000 | Loss: 0.00001719
Iteration 172/1000 | Loss: 0.00001718
Iteration 173/1000 | Loss: 0.00001718
Iteration 174/1000 | Loss: 0.00001718
Iteration 175/1000 | Loss: 0.00001718
Iteration 176/1000 | Loss: 0.00001718
Iteration 177/1000 | Loss: 0.00001718
Iteration 178/1000 | Loss: 0.00001718
Iteration 179/1000 | Loss: 0.00001718
Iteration 180/1000 | Loss: 0.00001718
Iteration 181/1000 | Loss: 0.00001718
Iteration 182/1000 | Loss: 0.00001718
Iteration 183/1000 | Loss: 0.00001718
Iteration 184/1000 | Loss: 0.00001718
Iteration 185/1000 | Loss: 0.00001718
Iteration 186/1000 | Loss: 0.00001718
Iteration 187/1000 | Loss: 0.00001718
Iteration 188/1000 | Loss: 0.00001718
Iteration 189/1000 | Loss: 0.00001718
Iteration 190/1000 | Loss: 0.00001718
Iteration 191/1000 | Loss: 0.00001718
Iteration 192/1000 | Loss: 0.00001718
Iteration 193/1000 | Loss: 0.00001717
Iteration 194/1000 | Loss: 0.00001717
Iteration 195/1000 | Loss: 0.00001717
Iteration 196/1000 | Loss: 0.00001717
Iteration 197/1000 | Loss: 0.00001717
Iteration 198/1000 | Loss: 0.00001717
Iteration 199/1000 | Loss: 0.00001717
Iteration 200/1000 | Loss: 0.00001717
Iteration 201/1000 | Loss: 0.00001717
Iteration 202/1000 | Loss: 0.00001717
Iteration 203/1000 | Loss: 0.00001717
Iteration 204/1000 | Loss: 0.00001717
Iteration 205/1000 | Loss: 0.00001717
Iteration 206/1000 | Loss: 0.00001717
Iteration 207/1000 | Loss: 0.00001717
Iteration 208/1000 | Loss: 0.00001717
Iteration 209/1000 | Loss: 0.00001717
Iteration 210/1000 | Loss: 0.00001717
Iteration 211/1000 | Loss: 0.00001717
Iteration 212/1000 | Loss: 0.00001717
Iteration 213/1000 | Loss: 0.00001717
Iteration 214/1000 | Loss: 0.00001717
Iteration 215/1000 | Loss: 0.00001717
Iteration 216/1000 | Loss: 0.00001717
Iteration 217/1000 | Loss: 0.00001717
Iteration 218/1000 | Loss: 0.00001717
Iteration 219/1000 | Loss: 0.00001717
Iteration 220/1000 | Loss: 0.00001717
Iteration 221/1000 | Loss: 0.00001717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.716627593850717e-05, 1.716627593850717e-05, 1.716627593850717e-05, 1.716627593850717e-05, 1.716627593850717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.716627593850717e-05

Optimization complete. Final v2v error: 3.4902782440185547 mm

Highest mean error: 3.800523281097412 mm for frame 113

Lowest mean error: 3.263315200805664 mm for frame 139

Saving results

Total time: 90.57316327095032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095049
Iteration 2/25 | Loss: 0.00332652
Iteration 3/25 | Loss: 0.00239809
Iteration 4/25 | Loss: 0.00194245
Iteration 5/25 | Loss: 0.00204653
Iteration 6/25 | Loss: 0.00191136
Iteration 7/25 | Loss: 0.00144327
Iteration 8/25 | Loss: 0.00137585
Iteration 9/25 | Loss: 0.00118242
Iteration 10/25 | Loss: 0.00116503
Iteration 11/25 | Loss: 0.00112553
Iteration 12/25 | Loss: 0.00110842
Iteration 13/25 | Loss: 0.00110560
Iteration 14/25 | Loss: 0.00110462
Iteration 15/25 | Loss: 0.00110270
Iteration 16/25 | Loss: 0.00110263
Iteration 17/25 | Loss: 0.00110131
Iteration 18/25 | Loss: 0.00110317
Iteration 19/25 | Loss: 0.00110324
Iteration 20/25 | Loss: 0.00110279
Iteration 21/25 | Loss: 0.00110275
Iteration 22/25 | Loss: 0.00110165
Iteration 23/25 | Loss: 0.00110358
Iteration 24/25 | Loss: 0.00110173
Iteration 25/25 | Loss: 0.00109885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21358109
Iteration 2/25 | Loss: 0.00168795
Iteration 3/25 | Loss: 0.00168795
Iteration 4/25 | Loss: 0.00168795
Iteration 5/25 | Loss: 0.00168795
Iteration 6/25 | Loss: 0.00168795
Iteration 7/25 | Loss: 0.00168795
Iteration 8/25 | Loss: 0.00168795
Iteration 9/25 | Loss: 0.00168795
Iteration 10/25 | Loss: 0.00168795
Iteration 11/25 | Loss: 0.00168795
Iteration 12/25 | Loss: 0.00168795
Iteration 13/25 | Loss: 0.00168795
Iteration 14/25 | Loss: 0.00168795
Iteration 15/25 | Loss: 0.00168795
Iteration 16/25 | Loss: 0.00168795
Iteration 17/25 | Loss: 0.00168795
Iteration 18/25 | Loss: 0.00168795
Iteration 19/25 | Loss: 0.00168795
Iteration 20/25 | Loss: 0.00168795
Iteration 21/25 | Loss: 0.00168795
Iteration 22/25 | Loss: 0.00168795
Iteration 23/25 | Loss: 0.00168795
Iteration 24/25 | Loss: 0.00168795
Iteration 25/25 | Loss: 0.00168795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168795
Iteration 2/1000 | Loss: 0.00014926
Iteration 3/1000 | Loss: 0.00016916
Iteration 4/1000 | Loss: 0.00015133
Iteration 5/1000 | Loss: 0.00017490
Iteration 6/1000 | Loss: 0.00004068
Iteration 7/1000 | Loss: 0.00003604
Iteration 8/1000 | Loss: 0.00003364
Iteration 9/1000 | Loss: 0.00020249
Iteration 10/1000 | Loss: 0.00036245
Iteration 11/1000 | Loss: 0.00012045
Iteration 12/1000 | Loss: 0.00022574
Iteration 13/1000 | Loss: 0.00009984
Iteration 14/1000 | Loss: 0.00004963
Iteration 15/1000 | Loss: 0.00003140
Iteration 16/1000 | Loss: 0.00080038
Iteration 17/1000 | Loss: 0.00010469
Iteration 18/1000 | Loss: 0.00018460
Iteration 19/1000 | Loss: 0.00014340
Iteration 20/1000 | Loss: 0.00005576
Iteration 21/1000 | Loss: 0.00025602
Iteration 22/1000 | Loss: 0.00018821
Iteration 23/1000 | Loss: 0.00003313
Iteration 24/1000 | Loss: 0.00043994
Iteration 25/1000 | Loss: 0.00034054
Iteration 26/1000 | Loss: 0.00009472
Iteration 27/1000 | Loss: 0.00013036
Iteration 28/1000 | Loss: 0.00010946
Iteration 29/1000 | Loss: 0.00011954
Iteration 30/1000 | Loss: 0.00010344
Iteration 31/1000 | Loss: 0.00005811
Iteration 32/1000 | Loss: 0.00008844
Iteration 33/1000 | Loss: 0.00008187
Iteration 34/1000 | Loss: 0.00025640
Iteration 35/1000 | Loss: 0.00016214
Iteration 36/1000 | Loss: 0.00016056
Iteration 37/1000 | Loss: 0.00004744
Iteration 38/1000 | Loss: 0.00003458
Iteration 39/1000 | Loss: 0.00003104
Iteration 40/1000 | Loss: 0.00004918
Iteration 41/1000 | Loss: 0.00003139
Iteration 42/1000 | Loss: 0.00002732
Iteration 43/1000 | Loss: 0.00002638
Iteration 44/1000 | Loss: 0.00002586
Iteration 45/1000 | Loss: 0.00002543
Iteration 46/1000 | Loss: 0.00002491
Iteration 47/1000 | Loss: 0.00002447
Iteration 48/1000 | Loss: 0.00002430
Iteration 49/1000 | Loss: 0.00002429
Iteration 50/1000 | Loss: 0.00002425
Iteration 51/1000 | Loss: 0.00002417
Iteration 52/1000 | Loss: 0.00002415
Iteration 53/1000 | Loss: 0.00002415
Iteration 54/1000 | Loss: 0.00002414
Iteration 55/1000 | Loss: 0.00002413
Iteration 56/1000 | Loss: 0.00002412
Iteration 57/1000 | Loss: 0.00002408
Iteration 58/1000 | Loss: 0.00002405
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002401
Iteration 61/1000 | Loss: 0.00002401
Iteration 62/1000 | Loss: 0.00002400
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00002399
Iteration 65/1000 | Loss: 0.00002398
Iteration 66/1000 | Loss: 0.00002398
Iteration 67/1000 | Loss: 0.00002398
Iteration 68/1000 | Loss: 0.00002397
Iteration 69/1000 | Loss: 0.00002397
Iteration 70/1000 | Loss: 0.00002397
Iteration 71/1000 | Loss: 0.00002397
Iteration 72/1000 | Loss: 0.00002396
Iteration 73/1000 | Loss: 0.00002396
Iteration 74/1000 | Loss: 0.00002396
Iteration 75/1000 | Loss: 0.00002396
Iteration 76/1000 | Loss: 0.00002395
Iteration 77/1000 | Loss: 0.00002395
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002394
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002394
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002394
Iteration 89/1000 | Loss: 0.00002394
Iteration 90/1000 | Loss: 0.00002394
Iteration 91/1000 | Loss: 0.00002394
Iteration 92/1000 | Loss: 0.00002394
Iteration 93/1000 | Loss: 0.00002394
Iteration 94/1000 | Loss: 0.00002394
Iteration 95/1000 | Loss: 0.00002394
Iteration 96/1000 | Loss: 0.00002393
Iteration 97/1000 | Loss: 0.00002393
Iteration 98/1000 | Loss: 0.00002393
Iteration 99/1000 | Loss: 0.00002393
Iteration 100/1000 | Loss: 0.00002393
Iteration 101/1000 | Loss: 0.00002393
Iteration 102/1000 | Loss: 0.00002393
Iteration 103/1000 | Loss: 0.00002392
Iteration 104/1000 | Loss: 0.00002392
Iteration 105/1000 | Loss: 0.00002392
Iteration 106/1000 | Loss: 0.00002392
Iteration 107/1000 | Loss: 0.00002392
Iteration 108/1000 | Loss: 0.00002392
Iteration 109/1000 | Loss: 0.00002392
Iteration 110/1000 | Loss: 0.00002392
Iteration 111/1000 | Loss: 0.00002392
Iteration 112/1000 | Loss: 0.00002392
Iteration 113/1000 | Loss: 0.00002392
Iteration 114/1000 | Loss: 0.00002392
Iteration 115/1000 | Loss: 0.00002392
Iteration 116/1000 | Loss: 0.00002392
Iteration 117/1000 | Loss: 0.00002392
Iteration 118/1000 | Loss: 0.00002392
Iteration 119/1000 | Loss: 0.00002392
Iteration 120/1000 | Loss: 0.00002391
Iteration 121/1000 | Loss: 0.00002391
Iteration 122/1000 | Loss: 0.00002391
Iteration 123/1000 | Loss: 0.00002391
Iteration 124/1000 | Loss: 0.00002391
Iteration 125/1000 | Loss: 0.00002391
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00002391
Iteration 128/1000 | Loss: 0.00002390
Iteration 129/1000 | Loss: 0.00002390
Iteration 130/1000 | Loss: 0.00002390
Iteration 131/1000 | Loss: 0.00002390
Iteration 132/1000 | Loss: 0.00002390
Iteration 133/1000 | Loss: 0.00002390
Iteration 134/1000 | Loss: 0.00002390
Iteration 135/1000 | Loss: 0.00002390
Iteration 136/1000 | Loss: 0.00002390
Iteration 137/1000 | Loss: 0.00002390
Iteration 138/1000 | Loss: 0.00002390
Iteration 139/1000 | Loss: 0.00002390
Iteration 140/1000 | Loss: 0.00002390
Iteration 141/1000 | Loss: 0.00002390
Iteration 142/1000 | Loss: 0.00002390
Iteration 143/1000 | Loss: 0.00002390
Iteration 144/1000 | Loss: 0.00002390
Iteration 145/1000 | Loss: 0.00002390
Iteration 146/1000 | Loss: 0.00002390
Iteration 147/1000 | Loss: 0.00002390
Iteration 148/1000 | Loss: 0.00002390
Iteration 149/1000 | Loss: 0.00002389
Iteration 150/1000 | Loss: 0.00002389
Iteration 151/1000 | Loss: 0.00002389
Iteration 152/1000 | Loss: 0.00002389
Iteration 153/1000 | Loss: 0.00002389
Iteration 154/1000 | Loss: 0.00002389
Iteration 155/1000 | Loss: 0.00002389
Iteration 156/1000 | Loss: 0.00002389
Iteration 157/1000 | Loss: 0.00002389
Iteration 158/1000 | Loss: 0.00002389
Iteration 159/1000 | Loss: 0.00002389
Iteration 160/1000 | Loss: 0.00002389
Iteration 161/1000 | Loss: 0.00002389
Iteration 162/1000 | Loss: 0.00002389
Iteration 163/1000 | Loss: 0.00002389
Iteration 164/1000 | Loss: 0.00002389
Iteration 165/1000 | Loss: 0.00002389
Iteration 166/1000 | Loss: 0.00002389
Iteration 167/1000 | Loss: 0.00002389
Iteration 168/1000 | Loss: 0.00002389
Iteration 169/1000 | Loss: 0.00002389
Iteration 170/1000 | Loss: 0.00002389
Iteration 171/1000 | Loss: 0.00002389
Iteration 172/1000 | Loss: 0.00002389
Iteration 173/1000 | Loss: 0.00002389
Iteration 174/1000 | Loss: 0.00002389
Iteration 175/1000 | Loss: 0.00002389
Iteration 176/1000 | Loss: 0.00002389
Iteration 177/1000 | Loss: 0.00002389
Iteration 178/1000 | Loss: 0.00002389
Iteration 179/1000 | Loss: 0.00002389
Iteration 180/1000 | Loss: 0.00002389
Iteration 181/1000 | Loss: 0.00002389
Iteration 182/1000 | Loss: 0.00002389
Iteration 183/1000 | Loss: 0.00002389
Iteration 184/1000 | Loss: 0.00002389
Iteration 185/1000 | Loss: 0.00002389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.3890672309789807e-05, 2.3890672309789807e-05, 2.3890672309789807e-05, 2.3890672309789807e-05, 2.3890672309789807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3890672309789807e-05

Optimization complete. Final v2v error: 4.231091499328613 mm

Highest mean error: 4.937143802642822 mm for frame 76

Lowest mean error: 3.590742588043213 mm for frame 142

Saving results

Total time: 280.62037348747253
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044561
Iteration 2/25 | Loss: 0.00483885
Iteration 3/25 | Loss: 0.00298154
Iteration 4/25 | Loss: 0.00258565
Iteration 5/25 | Loss: 0.00233165
Iteration 6/25 | Loss: 0.00234948
Iteration 7/25 | Loss: 0.00216215
Iteration 8/25 | Loss: 0.00205796
Iteration 9/25 | Loss: 0.00199133
Iteration 10/25 | Loss: 0.00197018
Iteration 11/25 | Loss: 0.00192162
Iteration 12/25 | Loss: 0.00187981
Iteration 13/25 | Loss: 0.00184977
Iteration 14/25 | Loss: 0.00184215
Iteration 15/25 | Loss: 0.00182184
Iteration 16/25 | Loss: 0.00181460
Iteration 17/25 | Loss: 0.00181311
Iteration 18/25 | Loss: 0.00181181
Iteration 19/25 | Loss: 0.00181590
Iteration 20/25 | Loss: 0.00181172
Iteration 21/25 | Loss: 0.00181617
Iteration 22/25 | Loss: 0.00181001
Iteration 23/25 | Loss: 0.00180911
Iteration 24/25 | Loss: 0.00181218
Iteration 25/25 | Loss: 0.00181257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19242370
Iteration 2/25 | Loss: 0.01214057
Iteration 3/25 | Loss: 0.01009243
Iteration 4/25 | Loss: 0.01009243
Iteration 5/25 | Loss: 0.01009243
Iteration 6/25 | Loss: 0.01009243
Iteration 7/25 | Loss: 0.01009243
Iteration 8/25 | Loss: 0.01009243
Iteration 9/25 | Loss: 0.01009243
Iteration 10/25 | Loss: 0.01009243
Iteration 11/25 | Loss: 0.01009243
Iteration 12/25 | Loss: 0.01009243
Iteration 13/25 | Loss: 0.01009243
Iteration 14/25 | Loss: 0.01009243
Iteration 15/25 | Loss: 0.01009243
Iteration 16/25 | Loss: 0.01009243
Iteration 17/25 | Loss: 0.01009243
Iteration 18/25 | Loss: 0.01009243
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.010092426091432571, 0.010092426091432571, 0.010092426091432571, 0.010092426091432571, 0.010092426091432571]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.010092426091432571

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01009243
Iteration 2/1000 | Loss: 0.00673453
Iteration 3/1000 | Loss: 0.00181619
Iteration 4/1000 | Loss: 0.00458148
Iteration 5/1000 | Loss: 0.00102355
Iteration 6/1000 | Loss: 0.00082338
Iteration 7/1000 | Loss: 0.00053077
Iteration 8/1000 | Loss: 0.00052788
Iteration 9/1000 | Loss: 0.00049123
Iteration 10/1000 | Loss: 0.00038652
Iteration 11/1000 | Loss: 0.00120479
Iteration 12/1000 | Loss: 0.00740090
Iteration 13/1000 | Loss: 0.01487743
Iteration 14/1000 | Loss: 0.00588521
Iteration 15/1000 | Loss: 0.00428816
Iteration 16/1000 | Loss: 0.00048917
Iteration 17/1000 | Loss: 0.00143957
Iteration 18/1000 | Loss: 0.00058875
Iteration 19/1000 | Loss: 0.00020961
Iteration 20/1000 | Loss: 0.00111446
Iteration 21/1000 | Loss: 0.00082903
Iteration 22/1000 | Loss: 0.00160369
Iteration 23/1000 | Loss: 0.00051408
Iteration 24/1000 | Loss: 0.00010244
Iteration 25/1000 | Loss: 0.00103573
Iteration 26/1000 | Loss: 0.00134853
Iteration 27/1000 | Loss: 0.00118722
Iteration 28/1000 | Loss: 0.00010759
Iteration 29/1000 | Loss: 0.00007087
Iteration 30/1000 | Loss: 0.00006950
Iteration 31/1000 | Loss: 0.00016631
Iteration 32/1000 | Loss: 0.00005866
Iteration 33/1000 | Loss: 0.00006256
Iteration 34/1000 | Loss: 0.00005301
Iteration 35/1000 | Loss: 0.00005026
Iteration 36/1000 | Loss: 0.00030938
Iteration 37/1000 | Loss: 0.00024574
Iteration 38/1000 | Loss: 0.00011772
Iteration 39/1000 | Loss: 0.00038293
Iteration 40/1000 | Loss: 0.00004756
Iteration 41/1000 | Loss: 0.00004549
Iteration 42/1000 | Loss: 0.00026452
Iteration 43/1000 | Loss: 0.00012399
Iteration 44/1000 | Loss: 0.00011223
Iteration 45/1000 | Loss: 0.00006749
Iteration 46/1000 | Loss: 0.00004395
Iteration 47/1000 | Loss: 0.00004184
Iteration 48/1000 | Loss: 0.00004063
Iteration 49/1000 | Loss: 0.00012625
Iteration 50/1000 | Loss: 0.00005736
Iteration 51/1000 | Loss: 0.00006917
Iteration 52/1000 | Loss: 0.00003983
Iteration 53/1000 | Loss: 0.00003930
Iteration 54/1000 | Loss: 0.00003918
Iteration 55/1000 | Loss: 0.00011627
Iteration 56/1000 | Loss: 0.00007050
Iteration 57/1000 | Loss: 0.00027176
Iteration 58/1000 | Loss: 0.00024201
Iteration 59/1000 | Loss: 0.00011036
Iteration 60/1000 | Loss: 0.00004056
Iteration 61/1000 | Loss: 0.00005122
Iteration 62/1000 | Loss: 0.00011422
Iteration 63/1000 | Loss: 0.00003782
Iteration 64/1000 | Loss: 0.00003724
Iteration 65/1000 | Loss: 0.00003698
Iteration 66/1000 | Loss: 0.00003681
Iteration 67/1000 | Loss: 0.00003680
Iteration 68/1000 | Loss: 0.00003673
Iteration 69/1000 | Loss: 0.00003669
Iteration 70/1000 | Loss: 0.00003667
Iteration 71/1000 | Loss: 0.00014181
Iteration 72/1000 | Loss: 0.00003723
Iteration 73/1000 | Loss: 0.00003663
Iteration 74/1000 | Loss: 0.00003660
Iteration 75/1000 | Loss: 0.00003660
Iteration 76/1000 | Loss: 0.00003659
Iteration 77/1000 | Loss: 0.00003659
Iteration 78/1000 | Loss: 0.00003659
Iteration 79/1000 | Loss: 0.00003659
Iteration 80/1000 | Loss: 0.00003657
Iteration 81/1000 | Loss: 0.00003657
Iteration 82/1000 | Loss: 0.00003656
Iteration 83/1000 | Loss: 0.00003656
Iteration 84/1000 | Loss: 0.00003656
Iteration 85/1000 | Loss: 0.00003656
Iteration 86/1000 | Loss: 0.00003656
Iteration 87/1000 | Loss: 0.00003656
Iteration 88/1000 | Loss: 0.00003655
Iteration 89/1000 | Loss: 0.00003655
Iteration 90/1000 | Loss: 0.00003655
Iteration 91/1000 | Loss: 0.00003655
Iteration 92/1000 | Loss: 0.00003655
Iteration 93/1000 | Loss: 0.00003655
Iteration 94/1000 | Loss: 0.00003655
Iteration 95/1000 | Loss: 0.00003655
Iteration 96/1000 | Loss: 0.00003655
Iteration 97/1000 | Loss: 0.00003655
Iteration 98/1000 | Loss: 0.00003655
Iteration 99/1000 | Loss: 0.00003654
Iteration 100/1000 | Loss: 0.00003654
Iteration 101/1000 | Loss: 0.00003654
Iteration 102/1000 | Loss: 0.00003654
Iteration 103/1000 | Loss: 0.00003653
Iteration 104/1000 | Loss: 0.00003653
Iteration 105/1000 | Loss: 0.00003653
Iteration 106/1000 | Loss: 0.00003653
Iteration 107/1000 | Loss: 0.00003653
Iteration 108/1000 | Loss: 0.00003653
Iteration 109/1000 | Loss: 0.00003653
Iteration 110/1000 | Loss: 0.00003653
Iteration 111/1000 | Loss: 0.00003653
Iteration 112/1000 | Loss: 0.00003653
Iteration 113/1000 | Loss: 0.00003653
Iteration 114/1000 | Loss: 0.00003653
Iteration 115/1000 | Loss: 0.00003653
Iteration 116/1000 | Loss: 0.00003653
Iteration 117/1000 | Loss: 0.00003653
Iteration 118/1000 | Loss: 0.00003653
Iteration 119/1000 | Loss: 0.00003653
Iteration 120/1000 | Loss: 0.00003653
Iteration 121/1000 | Loss: 0.00003653
Iteration 122/1000 | Loss: 0.00003653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [3.653034582384862e-05, 3.653034582384862e-05, 3.653034582384862e-05, 3.653034582384862e-05, 3.653034582384862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.653034582384862e-05

Optimization complete. Final v2v error: 5.068908214569092 mm

Highest mean error: 12.492620468139648 mm for frame 179

Lowest mean error: 4.340703010559082 mm for frame 9

Saving results

Total time: 438.041401386261
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440098
Iteration 2/25 | Loss: 0.00120334
Iteration 3/25 | Loss: 0.00106800
Iteration 4/25 | Loss: 0.00105472
Iteration 5/25 | Loss: 0.00105108
Iteration 6/25 | Loss: 0.00105023
Iteration 7/25 | Loss: 0.00105023
Iteration 8/25 | Loss: 0.00105023
Iteration 9/25 | Loss: 0.00105023
Iteration 10/25 | Loss: 0.00105023
Iteration 11/25 | Loss: 0.00105023
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010502311633899808, 0.0010502311633899808, 0.0010502311633899808, 0.0010502311633899808, 0.0010502311633899808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010502311633899808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23520958
Iteration 2/25 | Loss: 0.00148038
Iteration 3/25 | Loss: 0.00148037
Iteration 4/25 | Loss: 0.00148037
Iteration 5/25 | Loss: 0.00148037
Iteration 6/25 | Loss: 0.00148037
Iteration 7/25 | Loss: 0.00148037
Iteration 8/25 | Loss: 0.00148037
Iteration 9/25 | Loss: 0.00148037
Iteration 10/25 | Loss: 0.00148037
Iteration 11/25 | Loss: 0.00148037
Iteration 12/25 | Loss: 0.00148037
Iteration 13/25 | Loss: 0.00148037
Iteration 14/25 | Loss: 0.00148037
Iteration 15/25 | Loss: 0.00148037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014803721569478512, 0.0014803721569478512, 0.0014803721569478512, 0.0014803721569478512, 0.0014803721569478512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014803721569478512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148037
Iteration 2/1000 | Loss: 0.00004021
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00002200
Iteration 5/1000 | Loss: 0.00002009
Iteration 6/1000 | Loss: 0.00001950
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00001894
Iteration 9/1000 | Loss: 0.00001882
Iteration 10/1000 | Loss: 0.00001863
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001841
Iteration 14/1000 | Loss: 0.00001836
Iteration 15/1000 | Loss: 0.00001835
Iteration 16/1000 | Loss: 0.00001835
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001827
Iteration 20/1000 | Loss: 0.00001825
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001821
Iteration 28/1000 | Loss: 0.00001821
Iteration 29/1000 | Loss: 0.00001821
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001821
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001821
Iteration 36/1000 | Loss: 0.00001821
Iteration 37/1000 | Loss: 0.00001821
Iteration 38/1000 | Loss: 0.00001821
Iteration 39/1000 | Loss: 0.00001821
Iteration 40/1000 | Loss: 0.00001820
Iteration 41/1000 | Loss: 0.00001820
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001819
Iteration 45/1000 | Loss: 0.00001819
Iteration 46/1000 | Loss: 0.00001819
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001819
Iteration 49/1000 | Loss: 0.00001819
Iteration 50/1000 | Loss: 0.00001819
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001819
Iteration 53/1000 | Loss: 0.00001819
Iteration 54/1000 | Loss: 0.00001819
Iteration 55/1000 | Loss: 0.00001819
Iteration 56/1000 | Loss: 0.00001819
Iteration 57/1000 | Loss: 0.00001819
Iteration 58/1000 | Loss: 0.00001819
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001818
Iteration 65/1000 | Loss: 0.00001818
Iteration 66/1000 | Loss: 0.00001818
Iteration 67/1000 | Loss: 0.00001817
Iteration 68/1000 | Loss: 0.00001817
Iteration 69/1000 | Loss: 0.00001817
Iteration 70/1000 | Loss: 0.00001817
Iteration 71/1000 | Loss: 0.00001817
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001817
Iteration 86/1000 | Loss: 0.00001817
Iteration 87/1000 | Loss: 0.00001817
Iteration 88/1000 | Loss: 0.00001817
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001816
Iteration 98/1000 | Loss: 0.00001816
Iteration 99/1000 | Loss: 0.00001816
Iteration 100/1000 | Loss: 0.00001816
Iteration 101/1000 | Loss: 0.00001816
Iteration 102/1000 | Loss: 0.00001816
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001815
Iteration 108/1000 | Loss: 0.00001815
Iteration 109/1000 | Loss: 0.00001815
Iteration 110/1000 | Loss: 0.00001815
Iteration 111/1000 | Loss: 0.00001815
Iteration 112/1000 | Loss: 0.00001815
Iteration 113/1000 | Loss: 0.00001815
Iteration 114/1000 | Loss: 0.00001815
Iteration 115/1000 | Loss: 0.00001815
Iteration 116/1000 | Loss: 0.00001815
Iteration 117/1000 | Loss: 0.00001815
Iteration 118/1000 | Loss: 0.00001815
Iteration 119/1000 | Loss: 0.00001815
Iteration 120/1000 | Loss: 0.00001815
Iteration 121/1000 | Loss: 0.00001815
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001815
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8152424672734924e-05, 1.8152424672734924e-05, 1.8152424672734924e-05, 1.8152424672734924e-05, 1.8152424672734924e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8152424672734924e-05

Optimization complete. Final v2v error: 3.7228662967681885 mm

Highest mean error: 3.9781458377838135 mm for frame 30

Lowest mean error: 3.4252569675445557 mm for frame 9

Saving results

Total time: 74.68301105499268
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681805
Iteration 2/25 | Loss: 0.00121381
Iteration 3/25 | Loss: 0.00109842
Iteration 4/25 | Loss: 0.00107978
Iteration 5/25 | Loss: 0.00107392
Iteration 6/25 | Loss: 0.00107266
Iteration 7/25 | Loss: 0.00107263
Iteration 8/25 | Loss: 0.00107263
Iteration 9/25 | Loss: 0.00107263
Iteration 10/25 | Loss: 0.00107263
Iteration 11/25 | Loss: 0.00107263
Iteration 12/25 | Loss: 0.00107263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001072634942829609, 0.001072634942829609, 0.001072634942829609, 0.001072634942829609, 0.001072634942829609]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001072634942829609

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.94730854
Iteration 2/25 | Loss: 0.00148771
Iteration 3/25 | Loss: 0.00148760
Iteration 4/25 | Loss: 0.00148760
Iteration 5/25 | Loss: 0.00148760
Iteration 6/25 | Loss: 0.00148760
Iteration 7/25 | Loss: 0.00148760
Iteration 8/25 | Loss: 0.00148760
Iteration 9/25 | Loss: 0.00148760
Iteration 10/25 | Loss: 0.00148760
Iteration 11/25 | Loss: 0.00148760
Iteration 12/25 | Loss: 0.00148760
Iteration 13/25 | Loss: 0.00148760
Iteration 14/25 | Loss: 0.00148760
Iteration 15/25 | Loss: 0.00148760
Iteration 16/25 | Loss: 0.00148760
Iteration 17/25 | Loss: 0.00148760
Iteration 18/25 | Loss: 0.00148760
Iteration 19/25 | Loss: 0.00148760
Iteration 20/25 | Loss: 0.00148760
Iteration 21/25 | Loss: 0.00148760
Iteration 22/25 | Loss: 0.00148760
Iteration 23/25 | Loss: 0.00148760
Iteration 24/25 | Loss: 0.00148760
Iteration 25/25 | Loss: 0.00148760

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148760
Iteration 2/1000 | Loss: 0.00005358
Iteration 3/1000 | Loss: 0.00003665
Iteration 4/1000 | Loss: 0.00003223
Iteration 5/1000 | Loss: 0.00002990
Iteration 6/1000 | Loss: 0.00002801
Iteration 7/1000 | Loss: 0.00002702
Iteration 8/1000 | Loss: 0.00002635
Iteration 9/1000 | Loss: 0.00002593
Iteration 10/1000 | Loss: 0.00002550
Iteration 11/1000 | Loss: 0.00002522
Iteration 12/1000 | Loss: 0.00002505
Iteration 13/1000 | Loss: 0.00002503
Iteration 14/1000 | Loss: 0.00002501
Iteration 15/1000 | Loss: 0.00002496
Iteration 16/1000 | Loss: 0.00002496
Iteration 17/1000 | Loss: 0.00002496
Iteration 18/1000 | Loss: 0.00002496
Iteration 19/1000 | Loss: 0.00002496
Iteration 20/1000 | Loss: 0.00002496
Iteration 21/1000 | Loss: 0.00002496
Iteration 22/1000 | Loss: 0.00002496
Iteration 23/1000 | Loss: 0.00002492
Iteration 24/1000 | Loss: 0.00002492
Iteration 25/1000 | Loss: 0.00002491
Iteration 26/1000 | Loss: 0.00002491
Iteration 27/1000 | Loss: 0.00002491
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002490
Iteration 31/1000 | Loss: 0.00002489
Iteration 32/1000 | Loss: 0.00002489
Iteration 33/1000 | Loss: 0.00002488
Iteration 34/1000 | Loss: 0.00002488
Iteration 35/1000 | Loss: 0.00002487
Iteration 36/1000 | Loss: 0.00002487
Iteration 37/1000 | Loss: 0.00002487
Iteration 38/1000 | Loss: 0.00002487
Iteration 39/1000 | Loss: 0.00002486
Iteration 40/1000 | Loss: 0.00002485
Iteration 41/1000 | Loss: 0.00002485
Iteration 42/1000 | Loss: 0.00002485
Iteration 43/1000 | Loss: 0.00002484
Iteration 44/1000 | Loss: 0.00002484
Iteration 45/1000 | Loss: 0.00002484
Iteration 46/1000 | Loss: 0.00002484
Iteration 47/1000 | Loss: 0.00002484
Iteration 48/1000 | Loss: 0.00002484
Iteration 49/1000 | Loss: 0.00002483
Iteration 50/1000 | Loss: 0.00002483
Iteration 51/1000 | Loss: 0.00002483
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002482
Iteration 54/1000 | Loss: 0.00002482
Iteration 55/1000 | Loss: 0.00002481
Iteration 56/1000 | Loss: 0.00002481
Iteration 57/1000 | Loss: 0.00002480
Iteration 58/1000 | Loss: 0.00002480
Iteration 59/1000 | Loss: 0.00002480
Iteration 60/1000 | Loss: 0.00002480
Iteration 61/1000 | Loss: 0.00002479
Iteration 62/1000 | Loss: 0.00002479
Iteration 63/1000 | Loss: 0.00002479
Iteration 64/1000 | Loss: 0.00002479
Iteration 65/1000 | Loss: 0.00002479
Iteration 66/1000 | Loss: 0.00002479
Iteration 67/1000 | Loss: 0.00002479
Iteration 68/1000 | Loss: 0.00002479
Iteration 69/1000 | Loss: 0.00002478
Iteration 70/1000 | Loss: 0.00002478
Iteration 71/1000 | Loss: 0.00002477
Iteration 72/1000 | Loss: 0.00002476
Iteration 73/1000 | Loss: 0.00002476
Iteration 74/1000 | Loss: 0.00002476
Iteration 75/1000 | Loss: 0.00002476
Iteration 76/1000 | Loss: 0.00002476
Iteration 77/1000 | Loss: 0.00002476
Iteration 78/1000 | Loss: 0.00002476
Iteration 79/1000 | Loss: 0.00002476
Iteration 80/1000 | Loss: 0.00002476
Iteration 81/1000 | Loss: 0.00002475
Iteration 82/1000 | Loss: 0.00002475
Iteration 83/1000 | Loss: 0.00002475
Iteration 84/1000 | Loss: 0.00002474
Iteration 85/1000 | Loss: 0.00002474
Iteration 86/1000 | Loss: 0.00002473
Iteration 87/1000 | Loss: 0.00002473
Iteration 88/1000 | Loss: 0.00002473
Iteration 89/1000 | Loss: 0.00002473
Iteration 90/1000 | Loss: 0.00002473
Iteration 91/1000 | Loss: 0.00002473
Iteration 92/1000 | Loss: 0.00002473
Iteration 93/1000 | Loss: 0.00002473
Iteration 94/1000 | Loss: 0.00002473
Iteration 95/1000 | Loss: 0.00002472
Iteration 96/1000 | Loss: 0.00002472
Iteration 97/1000 | Loss: 0.00002472
Iteration 98/1000 | Loss: 0.00002472
Iteration 99/1000 | Loss: 0.00002472
Iteration 100/1000 | Loss: 0.00002472
Iteration 101/1000 | Loss: 0.00002472
Iteration 102/1000 | Loss: 0.00002472
Iteration 103/1000 | Loss: 0.00002471
Iteration 104/1000 | Loss: 0.00002471
Iteration 105/1000 | Loss: 0.00002471
Iteration 106/1000 | Loss: 0.00002471
Iteration 107/1000 | Loss: 0.00002471
Iteration 108/1000 | Loss: 0.00002471
Iteration 109/1000 | Loss: 0.00002471
Iteration 110/1000 | Loss: 0.00002471
Iteration 111/1000 | Loss: 0.00002471
Iteration 112/1000 | Loss: 0.00002471
Iteration 113/1000 | Loss: 0.00002471
Iteration 114/1000 | Loss: 0.00002471
Iteration 115/1000 | Loss: 0.00002471
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002470
Iteration 118/1000 | Loss: 0.00002470
Iteration 119/1000 | Loss: 0.00002470
Iteration 120/1000 | Loss: 0.00002470
Iteration 121/1000 | Loss: 0.00002470
Iteration 122/1000 | Loss: 0.00002470
Iteration 123/1000 | Loss: 0.00002470
Iteration 124/1000 | Loss: 0.00002470
Iteration 125/1000 | Loss: 0.00002470
Iteration 126/1000 | Loss: 0.00002470
Iteration 127/1000 | Loss: 0.00002470
Iteration 128/1000 | Loss: 0.00002470
Iteration 129/1000 | Loss: 0.00002470
Iteration 130/1000 | Loss: 0.00002470
Iteration 131/1000 | Loss: 0.00002470
Iteration 132/1000 | Loss: 0.00002469
Iteration 133/1000 | Loss: 0.00002469
Iteration 134/1000 | Loss: 0.00002469
Iteration 135/1000 | Loss: 0.00002469
Iteration 136/1000 | Loss: 0.00002469
Iteration 137/1000 | Loss: 0.00002469
Iteration 138/1000 | Loss: 0.00002469
Iteration 139/1000 | Loss: 0.00002469
Iteration 140/1000 | Loss: 0.00002469
Iteration 141/1000 | Loss: 0.00002469
Iteration 142/1000 | Loss: 0.00002469
Iteration 143/1000 | Loss: 0.00002469
Iteration 144/1000 | Loss: 0.00002469
Iteration 145/1000 | Loss: 0.00002469
Iteration 146/1000 | Loss: 0.00002469
Iteration 147/1000 | Loss: 0.00002469
Iteration 148/1000 | Loss: 0.00002469
Iteration 149/1000 | Loss: 0.00002469
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002469
Iteration 152/1000 | Loss: 0.00002469
Iteration 153/1000 | Loss: 0.00002469
Iteration 154/1000 | Loss: 0.00002469
Iteration 155/1000 | Loss: 0.00002469
Iteration 156/1000 | Loss: 0.00002469
Iteration 157/1000 | Loss: 0.00002469
Iteration 158/1000 | Loss: 0.00002469
Iteration 159/1000 | Loss: 0.00002469
Iteration 160/1000 | Loss: 0.00002469
Iteration 161/1000 | Loss: 0.00002469
Iteration 162/1000 | Loss: 0.00002469
Iteration 163/1000 | Loss: 0.00002469
Iteration 164/1000 | Loss: 0.00002469
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.469280116201844e-05, 2.469280116201844e-05, 2.469280116201844e-05, 2.469280116201844e-05, 2.469280116201844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.469280116201844e-05

Optimization complete. Final v2v error: 4.298449993133545 mm

Highest mean error: 4.612414360046387 mm for frame 153

Lowest mean error: 3.9866981506347656 mm for frame 0

Saving results

Total time: 90.02388548851013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00345964
Iteration 2/25 | Loss: 0.00127822
Iteration 3/25 | Loss: 0.00114166
Iteration 4/25 | Loss: 0.00111061
Iteration 5/25 | Loss: 0.00110342
Iteration 6/25 | Loss: 0.00110267
Iteration 7/25 | Loss: 0.00110267
Iteration 8/25 | Loss: 0.00110267
Iteration 9/25 | Loss: 0.00110267
Iteration 10/25 | Loss: 0.00110267
Iteration 11/25 | Loss: 0.00110267
Iteration 12/25 | Loss: 0.00110267
Iteration 13/25 | Loss: 0.00110267
Iteration 14/25 | Loss: 0.00110267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001102669513784349, 0.001102669513784349, 0.001102669513784349, 0.001102669513784349, 0.001102669513784349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001102669513784349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18403900
Iteration 2/25 | Loss: 0.00190933
Iteration 3/25 | Loss: 0.00190932
Iteration 4/25 | Loss: 0.00190932
Iteration 5/25 | Loss: 0.00190932
Iteration 6/25 | Loss: 0.00190932
Iteration 7/25 | Loss: 0.00190932
Iteration 8/25 | Loss: 0.00190932
Iteration 9/25 | Loss: 0.00190932
Iteration 10/25 | Loss: 0.00190932
Iteration 11/25 | Loss: 0.00190932
Iteration 12/25 | Loss: 0.00190932
Iteration 13/25 | Loss: 0.00190932
Iteration 14/25 | Loss: 0.00190932
Iteration 15/25 | Loss: 0.00190932
Iteration 16/25 | Loss: 0.00190932
Iteration 17/25 | Loss: 0.00190932
Iteration 18/25 | Loss: 0.00190932
Iteration 19/25 | Loss: 0.00190932
Iteration 20/25 | Loss: 0.00190932
Iteration 21/25 | Loss: 0.00190932
Iteration 22/25 | Loss: 0.00190932
Iteration 23/25 | Loss: 0.00190932
Iteration 24/25 | Loss: 0.00190932
Iteration 25/25 | Loss: 0.00190932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190932
Iteration 2/1000 | Loss: 0.00005893
Iteration 3/1000 | Loss: 0.00003773
Iteration 4/1000 | Loss: 0.00002975
Iteration 5/1000 | Loss: 0.00002744
Iteration 6/1000 | Loss: 0.00002557
Iteration 7/1000 | Loss: 0.00002467
Iteration 8/1000 | Loss: 0.00002400
Iteration 9/1000 | Loss: 0.00002358
Iteration 10/1000 | Loss: 0.00002311
Iteration 11/1000 | Loss: 0.00002287
Iteration 12/1000 | Loss: 0.00002269
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002252
Iteration 15/1000 | Loss: 0.00002252
Iteration 16/1000 | Loss: 0.00002251
Iteration 17/1000 | Loss: 0.00002250
Iteration 18/1000 | Loss: 0.00002249
Iteration 19/1000 | Loss: 0.00002249
Iteration 20/1000 | Loss: 0.00002244
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002241
Iteration 23/1000 | Loss: 0.00002240
Iteration 24/1000 | Loss: 0.00002240
Iteration 25/1000 | Loss: 0.00002239
Iteration 26/1000 | Loss: 0.00002238
Iteration 27/1000 | Loss: 0.00002236
Iteration 28/1000 | Loss: 0.00002236
Iteration 29/1000 | Loss: 0.00002236
Iteration 30/1000 | Loss: 0.00002236
Iteration 31/1000 | Loss: 0.00002235
Iteration 32/1000 | Loss: 0.00002235
Iteration 33/1000 | Loss: 0.00002234
Iteration 34/1000 | Loss: 0.00002232
Iteration 35/1000 | Loss: 0.00002232
Iteration 36/1000 | Loss: 0.00002232
Iteration 37/1000 | Loss: 0.00002231
Iteration 38/1000 | Loss: 0.00002231
Iteration 39/1000 | Loss: 0.00002230
Iteration 40/1000 | Loss: 0.00002230
Iteration 41/1000 | Loss: 0.00002230
Iteration 42/1000 | Loss: 0.00002230
Iteration 43/1000 | Loss: 0.00002229
Iteration 44/1000 | Loss: 0.00002229
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00002229
Iteration 47/1000 | Loss: 0.00002229
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002228
Iteration 50/1000 | Loss: 0.00002228
Iteration 51/1000 | Loss: 0.00002228
Iteration 52/1000 | Loss: 0.00002227
Iteration 53/1000 | Loss: 0.00002227
Iteration 54/1000 | Loss: 0.00002227
Iteration 55/1000 | Loss: 0.00002227
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002227
Iteration 58/1000 | Loss: 0.00002226
Iteration 59/1000 | Loss: 0.00002226
Iteration 60/1000 | Loss: 0.00002226
Iteration 61/1000 | Loss: 0.00002226
Iteration 62/1000 | Loss: 0.00002225
Iteration 63/1000 | Loss: 0.00002225
Iteration 64/1000 | Loss: 0.00002225
Iteration 65/1000 | Loss: 0.00002225
Iteration 66/1000 | Loss: 0.00002224
Iteration 67/1000 | Loss: 0.00002224
Iteration 68/1000 | Loss: 0.00002224
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002224
Iteration 71/1000 | Loss: 0.00002224
Iteration 72/1000 | Loss: 0.00002224
Iteration 73/1000 | Loss: 0.00002224
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002224
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002223
Iteration 78/1000 | Loss: 0.00002223
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002223
Iteration 81/1000 | Loss: 0.00002223
Iteration 82/1000 | Loss: 0.00002223
Iteration 83/1000 | Loss: 0.00002222
Iteration 84/1000 | Loss: 0.00002222
Iteration 85/1000 | Loss: 0.00002222
Iteration 86/1000 | Loss: 0.00002222
Iteration 87/1000 | Loss: 0.00002222
Iteration 88/1000 | Loss: 0.00002222
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002222
Iteration 91/1000 | Loss: 0.00002222
Iteration 92/1000 | Loss: 0.00002222
Iteration 93/1000 | Loss: 0.00002222
Iteration 94/1000 | Loss: 0.00002222
Iteration 95/1000 | Loss: 0.00002222
Iteration 96/1000 | Loss: 0.00002222
Iteration 97/1000 | Loss: 0.00002222
Iteration 98/1000 | Loss: 0.00002222
Iteration 99/1000 | Loss: 0.00002222
Iteration 100/1000 | Loss: 0.00002222
Iteration 101/1000 | Loss: 0.00002222
Iteration 102/1000 | Loss: 0.00002222
Iteration 103/1000 | Loss: 0.00002222
Iteration 104/1000 | Loss: 0.00002222
Iteration 105/1000 | Loss: 0.00002222
Iteration 106/1000 | Loss: 0.00002222
Iteration 107/1000 | Loss: 0.00002222
Iteration 108/1000 | Loss: 0.00002222
Iteration 109/1000 | Loss: 0.00002222
Iteration 110/1000 | Loss: 0.00002222
Iteration 111/1000 | Loss: 0.00002222
Iteration 112/1000 | Loss: 0.00002222
Iteration 113/1000 | Loss: 0.00002222
Iteration 114/1000 | Loss: 0.00002222
Iteration 115/1000 | Loss: 0.00002222
Iteration 116/1000 | Loss: 0.00002222
Iteration 117/1000 | Loss: 0.00002222
Iteration 118/1000 | Loss: 0.00002222
Iteration 119/1000 | Loss: 0.00002222
Iteration 120/1000 | Loss: 0.00002222
Iteration 121/1000 | Loss: 0.00002222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.2216063371161e-05, 2.2216063371161e-05, 2.2216063371161e-05, 2.2216063371161e-05, 2.2216063371161e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2216063371161e-05

Optimization complete. Final v2v error: 4.079442977905273 mm

Highest mean error: 4.563986301422119 mm for frame 213

Lowest mean error: 3.6497914791107178 mm for frame 64

Saving results

Total time: 110.56519961357117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01116318
Iteration 2/25 | Loss: 0.01116318
Iteration 3/25 | Loss: 0.01116317
Iteration 4/25 | Loss: 0.01116317
Iteration 5/25 | Loss: 0.01116317
Iteration 6/25 | Loss: 0.01116317
Iteration 7/25 | Loss: 0.01116316
Iteration 8/25 | Loss: 0.01116316
Iteration 9/25 | Loss: 0.01116316
Iteration 10/25 | Loss: 0.01116316
Iteration 11/25 | Loss: 0.01116316
Iteration 12/25 | Loss: 0.01116316
Iteration 13/25 | Loss: 0.01116315
Iteration 14/25 | Loss: 0.01116315
Iteration 15/25 | Loss: 0.01116315
Iteration 16/25 | Loss: 0.01116315
Iteration 17/25 | Loss: 0.01116315
Iteration 18/25 | Loss: 0.01116315
Iteration 19/25 | Loss: 0.01116314
Iteration 20/25 | Loss: 0.01116314
Iteration 21/25 | Loss: 0.01116314
Iteration 22/25 | Loss: 0.01116314
Iteration 23/25 | Loss: 0.01116314
Iteration 24/25 | Loss: 0.01116313
Iteration 25/25 | Loss: 0.01116313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26339436
Iteration 2/25 | Loss: 0.14234376
Iteration 3/25 | Loss: 0.14215828
Iteration 4/25 | Loss: 0.14177565
Iteration 5/25 | Loss: 0.14177561
Iteration 6/25 | Loss: 0.14177559
Iteration 7/25 | Loss: 0.14177559
Iteration 8/25 | Loss: 0.14177559
Iteration 9/25 | Loss: 0.14177559
Iteration 10/25 | Loss: 0.14177558
Iteration 11/25 | Loss: 0.14177558
Iteration 12/25 | Loss: 0.14177558
Iteration 13/25 | Loss: 0.14177558
Iteration 14/25 | Loss: 0.14177558
Iteration 15/25 | Loss: 0.14177558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.14177557826042175, 0.14177557826042175, 0.14177557826042175, 0.14177557826042175, 0.14177557826042175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14177557826042175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14177558
Iteration 2/1000 | Loss: 0.00401356
Iteration 3/1000 | Loss: 0.00068672
Iteration 4/1000 | Loss: 0.00032133
Iteration 5/1000 | Loss: 0.00101023
Iteration 6/1000 | Loss: 0.00250952
Iteration 7/1000 | Loss: 0.00031232
Iteration 8/1000 | Loss: 0.00006513
Iteration 9/1000 | Loss: 0.00042597
Iteration 10/1000 | Loss: 0.00004988
Iteration 11/1000 | Loss: 0.00004588
Iteration 12/1000 | Loss: 0.00016930
Iteration 13/1000 | Loss: 0.00004019
Iteration 14/1000 | Loss: 0.00024800
Iteration 15/1000 | Loss: 0.00025421
Iteration 16/1000 | Loss: 0.00004831
Iteration 17/1000 | Loss: 0.00003847
Iteration 18/1000 | Loss: 0.00003666
Iteration 19/1000 | Loss: 0.00003526
Iteration 20/1000 | Loss: 0.00023521
Iteration 21/1000 | Loss: 0.00020562
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00023486
Iteration 24/1000 | Loss: 0.00024270
Iteration 25/1000 | Loss: 0.00003340
Iteration 26/1000 | Loss: 0.00003199
Iteration 27/1000 | Loss: 0.00003105
Iteration 28/1000 | Loss: 0.00027554
Iteration 29/1000 | Loss: 0.00004363
Iteration 30/1000 | Loss: 0.00003705
Iteration 31/1000 | Loss: 0.00003113
Iteration 32/1000 | Loss: 0.00003028
Iteration 33/1000 | Loss: 0.00020899
Iteration 34/1000 | Loss: 0.00004509
Iteration 35/1000 | Loss: 0.00003764
Iteration 36/1000 | Loss: 0.00003452
Iteration 37/1000 | Loss: 0.00003074
Iteration 38/1000 | Loss: 0.00002976
Iteration 39/1000 | Loss: 0.00023426
Iteration 40/1000 | Loss: 0.00003006
Iteration 41/1000 | Loss: 0.00002941
Iteration 42/1000 | Loss: 0.00002940
Iteration 43/1000 | Loss: 0.00002940
Iteration 44/1000 | Loss: 0.00002939
Iteration 45/1000 | Loss: 0.00002925
Iteration 46/1000 | Loss: 0.00002922
Iteration 47/1000 | Loss: 0.00002922
Iteration 48/1000 | Loss: 0.00002922
Iteration 49/1000 | Loss: 0.00002922
Iteration 50/1000 | Loss: 0.00002922
Iteration 51/1000 | Loss: 0.00002921
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002920
Iteration 55/1000 | Loss: 0.00002916
Iteration 56/1000 | Loss: 0.00002916
Iteration 57/1000 | Loss: 0.00002915
Iteration 58/1000 | Loss: 0.00002915
Iteration 59/1000 | Loss: 0.00002915
Iteration 60/1000 | Loss: 0.00002914
Iteration 61/1000 | Loss: 0.00002912
Iteration 62/1000 | Loss: 0.00002912
Iteration 63/1000 | Loss: 0.00002912
Iteration 64/1000 | Loss: 0.00002912
Iteration 65/1000 | Loss: 0.00002912
Iteration 66/1000 | Loss: 0.00002912
Iteration 67/1000 | Loss: 0.00002912
Iteration 68/1000 | Loss: 0.00002912
Iteration 69/1000 | Loss: 0.00002912
Iteration 70/1000 | Loss: 0.00002911
Iteration 71/1000 | Loss: 0.00002911
Iteration 72/1000 | Loss: 0.00002908
Iteration 73/1000 | Loss: 0.00002908
Iteration 74/1000 | Loss: 0.00002908
Iteration 75/1000 | Loss: 0.00002908
Iteration 76/1000 | Loss: 0.00002908
Iteration 77/1000 | Loss: 0.00002908
Iteration 78/1000 | Loss: 0.00002907
Iteration 79/1000 | Loss: 0.00002906
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002903
Iteration 82/1000 | Loss: 0.00002903
Iteration 83/1000 | Loss: 0.00002903
Iteration 84/1000 | Loss: 0.00002902
Iteration 85/1000 | Loss: 0.00002901
Iteration 86/1000 | Loss: 0.00002900
Iteration 87/1000 | Loss: 0.00002900
Iteration 88/1000 | Loss: 0.00002900
Iteration 89/1000 | Loss: 0.00002899
Iteration 90/1000 | Loss: 0.00002899
Iteration 91/1000 | Loss: 0.00002899
Iteration 92/1000 | Loss: 0.00002899
Iteration 93/1000 | Loss: 0.00002899
Iteration 94/1000 | Loss: 0.00002899
Iteration 95/1000 | Loss: 0.00002899
Iteration 96/1000 | Loss: 0.00002899
Iteration 97/1000 | Loss: 0.00002898
Iteration 98/1000 | Loss: 0.00002898
Iteration 99/1000 | Loss: 0.00002898
Iteration 100/1000 | Loss: 0.00002898
Iteration 101/1000 | Loss: 0.00002898
Iteration 102/1000 | Loss: 0.00002898
Iteration 103/1000 | Loss: 0.00002898
Iteration 104/1000 | Loss: 0.00002898
Iteration 105/1000 | Loss: 0.00002898
Iteration 106/1000 | Loss: 0.00002897
Iteration 107/1000 | Loss: 0.00002897
Iteration 108/1000 | Loss: 0.00002897
Iteration 109/1000 | Loss: 0.00002897
Iteration 110/1000 | Loss: 0.00002897
Iteration 111/1000 | Loss: 0.00002897
Iteration 112/1000 | Loss: 0.00002897
Iteration 113/1000 | Loss: 0.00002897
Iteration 114/1000 | Loss: 0.00002897
Iteration 115/1000 | Loss: 0.00002897
Iteration 116/1000 | Loss: 0.00002897
Iteration 117/1000 | Loss: 0.00002897
Iteration 118/1000 | Loss: 0.00002896
Iteration 119/1000 | Loss: 0.00002896
Iteration 120/1000 | Loss: 0.00002896
Iteration 121/1000 | Loss: 0.00002896
Iteration 122/1000 | Loss: 0.00002896
Iteration 123/1000 | Loss: 0.00002896
Iteration 124/1000 | Loss: 0.00002896
Iteration 125/1000 | Loss: 0.00002896
Iteration 126/1000 | Loss: 0.00002896
Iteration 127/1000 | Loss: 0.00002896
Iteration 128/1000 | Loss: 0.00002896
Iteration 129/1000 | Loss: 0.00002896
Iteration 130/1000 | Loss: 0.00002896
Iteration 131/1000 | Loss: 0.00002896
Iteration 132/1000 | Loss: 0.00002896
Iteration 133/1000 | Loss: 0.00002895
Iteration 134/1000 | Loss: 0.00002895
Iteration 135/1000 | Loss: 0.00002895
Iteration 136/1000 | Loss: 0.00002895
Iteration 137/1000 | Loss: 0.00002895
Iteration 138/1000 | Loss: 0.00002895
Iteration 139/1000 | Loss: 0.00002895
Iteration 140/1000 | Loss: 0.00002895
Iteration 141/1000 | Loss: 0.00002895
Iteration 142/1000 | Loss: 0.00002895
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002895
Iteration 146/1000 | Loss: 0.00002895
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002895
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.8952375942026265e-05, 2.8952375942026265e-05, 2.8952375942026265e-05, 2.8952375942026265e-05, 2.8952375942026265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8952375942026265e-05

Optimization complete. Final v2v error: 4.517132759094238 mm

Highest mean error: 9.58195686340332 mm for frame 75

Lowest mean error: 4.078538417816162 mm for frame 239

Saving results

Total time: 226.7819628715515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455529
Iteration 2/25 | Loss: 0.00117199
Iteration 3/25 | Loss: 0.00106445
Iteration 4/25 | Loss: 0.00104778
Iteration 5/25 | Loss: 0.00104167
Iteration 6/25 | Loss: 0.00104063
Iteration 7/25 | Loss: 0.00104063
Iteration 8/25 | Loss: 0.00104063
Iteration 9/25 | Loss: 0.00104063
Iteration 10/25 | Loss: 0.00104063
Iteration 11/25 | Loss: 0.00104063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001040625269524753, 0.001040625269524753, 0.001040625269524753, 0.001040625269524753, 0.001040625269524753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040625269524753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19214988
Iteration 2/25 | Loss: 0.00145922
Iteration 3/25 | Loss: 0.00145922
Iteration 4/25 | Loss: 0.00145922
Iteration 5/25 | Loss: 0.00145922
Iteration 6/25 | Loss: 0.00145922
Iteration 7/25 | Loss: 0.00145922
Iteration 8/25 | Loss: 0.00145922
Iteration 9/25 | Loss: 0.00145922
Iteration 10/25 | Loss: 0.00145922
Iteration 11/25 | Loss: 0.00145922
Iteration 12/25 | Loss: 0.00145922
Iteration 13/25 | Loss: 0.00145922
Iteration 14/25 | Loss: 0.00145922
Iteration 15/25 | Loss: 0.00145922
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001459216233342886, 0.001459216233342886, 0.001459216233342886, 0.001459216233342886, 0.001459216233342886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001459216233342886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145922
Iteration 2/1000 | Loss: 0.00003704
Iteration 3/1000 | Loss: 0.00002645
Iteration 4/1000 | Loss: 0.00002415
Iteration 5/1000 | Loss: 0.00002274
Iteration 6/1000 | Loss: 0.00002197
Iteration 7/1000 | Loss: 0.00002162
Iteration 8/1000 | Loss: 0.00002119
Iteration 9/1000 | Loss: 0.00002097
Iteration 10/1000 | Loss: 0.00002092
Iteration 11/1000 | Loss: 0.00002088
Iteration 12/1000 | Loss: 0.00002087
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002054
Iteration 18/1000 | Loss: 0.00002053
Iteration 19/1000 | Loss: 0.00002052
Iteration 20/1000 | Loss: 0.00002052
Iteration 21/1000 | Loss: 0.00002052
Iteration 22/1000 | Loss: 0.00002051
Iteration 23/1000 | Loss: 0.00002051
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00002050
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002046
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002043
Iteration 43/1000 | Loss: 0.00002043
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002043
Iteration 47/1000 | Loss: 0.00002043
Iteration 48/1000 | Loss: 0.00002043
Iteration 49/1000 | Loss: 0.00002043
Iteration 50/1000 | Loss: 0.00002043
Iteration 51/1000 | Loss: 0.00002043
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002043
Iteration 55/1000 | Loss: 0.00002043
Iteration 56/1000 | Loss: 0.00002043
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00002043
Iteration 59/1000 | Loss: 0.00002043
Iteration 60/1000 | Loss: 0.00002043
Iteration 61/1000 | Loss: 0.00002043
Iteration 62/1000 | Loss: 0.00002043
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00002043
Iteration 66/1000 | Loss: 0.00002043
Iteration 67/1000 | Loss: 0.00002043
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002043
Iteration 72/1000 | Loss: 0.00002043
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002043
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.0425914044608362e-05, 2.0425914044608362e-05, 2.0425914044608362e-05, 2.0425914044608362e-05, 2.0425914044608362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0425914044608362e-05

Optimization complete. Final v2v error: 3.882357597351074 mm

Highest mean error: 4.478939533233643 mm for frame 219

Lowest mean error: 3.545393466949463 mm for frame 58

Saving results

Total time: 87.29326677322388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895865
Iteration 2/25 | Loss: 0.00122219
Iteration 3/25 | Loss: 0.00108896
Iteration 4/25 | Loss: 0.00106692
Iteration 5/25 | Loss: 0.00106021
Iteration 6/25 | Loss: 0.00105870
Iteration 7/25 | Loss: 0.00105860
Iteration 8/25 | Loss: 0.00105860
Iteration 9/25 | Loss: 0.00105860
Iteration 10/25 | Loss: 0.00105860
Iteration 11/25 | Loss: 0.00105860
Iteration 12/25 | Loss: 0.00105860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010585990967229009, 0.0010585990967229009, 0.0010585990967229009, 0.0010585990967229009, 0.0010585990967229009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010585990967229009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21499133
Iteration 2/25 | Loss: 0.00142321
Iteration 3/25 | Loss: 0.00142320
Iteration 4/25 | Loss: 0.00142320
Iteration 5/25 | Loss: 0.00142320
Iteration 6/25 | Loss: 0.00142319
Iteration 7/25 | Loss: 0.00142319
Iteration 8/25 | Loss: 0.00142319
Iteration 9/25 | Loss: 0.00142319
Iteration 10/25 | Loss: 0.00142319
Iteration 11/25 | Loss: 0.00142319
Iteration 12/25 | Loss: 0.00142319
Iteration 13/25 | Loss: 0.00142319
Iteration 14/25 | Loss: 0.00142319
Iteration 15/25 | Loss: 0.00142319
Iteration 16/25 | Loss: 0.00142319
Iteration 17/25 | Loss: 0.00142319
Iteration 18/25 | Loss: 0.00142319
Iteration 19/25 | Loss: 0.00142319
Iteration 20/25 | Loss: 0.00142319
Iteration 21/25 | Loss: 0.00142319
Iteration 22/25 | Loss: 0.00142319
Iteration 23/25 | Loss: 0.00142319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014231931418180466, 0.0014231931418180466, 0.0014231931418180466, 0.0014231931418180466, 0.0014231931418180466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014231931418180466

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142319
Iteration 2/1000 | Loss: 0.00003943
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002465
Iteration 5/1000 | Loss: 0.00002319
Iteration 6/1000 | Loss: 0.00002231
Iteration 7/1000 | Loss: 0.00002150
Iteration 8/1000 | Loss: 0.00002111
Iteration 9/1000 | Loss: 0.00002084
Iteration 10/1000 | Loss: 0.00002054
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002037
Iteration 13/1000 | Loss: 0.00002029
Iteration 14/1000 | Loss: 0.00002020
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002016
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002015
Iteration 19/1000 | Loss: 0.00002013
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00002011
Iteration 22/1000 | Loss: 0.00002011
Iteration 23/1000 | Loss: 0.00002010
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00002010
Iteration 26/1000 | Loss: 0.00002009
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002007
Iteration 38/1000 | Loss: 0.00002007
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002006
Iteration 43/1000 | Loss: 0.00002006
Iteration 44/1000 | Loss: 0.00002006
Iteration 45/1000 | Loss: 0.00002006
Iteration 46/1000 | Loss: 0.00002005
Iteration 47/1000 | Loss: 0.00002005
Iteration 48/1000 | Loss: 0.00002005
Iteration 49/1000 | Loss: 0.00002005
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002004
Iteration 52/1000 | Loss: 0.00002004
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002004
Iteration 58/1000 | Loss: 0.00002003
Iteration 59/1000 | Loss: 0.00002003
Iteration 60/1000 | Loss: 0.00002003
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00002002
Iteration 64/1000 | Loss: 0.00002002
Iteration 65/1000 | Loss: 0.00002002
Iteration 66/1000 | Loss: 0.00002002
Iteration 67/1000 | Loss: 0.00002002
Iteration 68/1000 | Loss: 0.00002002
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002001
Iteration 72/1000 | Loss: 0.00002001
Iteration 73/1000 | Loss: 0.00002001
Iteration 74/1000 | Loss: 0.00002001
Iteration 75/1000 | Loss: 0.00002001
Iteration 76/1000 | Loss: 0.00002001
Iteration 77/1000 | Loss: 0.00002000
Iteration 78/1000 | Loss: 0.00002000
Iteration 79/1000 | Loss: 0.00002000
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00002000
Iteration 84/1000 | Loss: 0.00002000
Iteration 85/1000 | Loss: 0.00002000
Iteration 86/1000 | Loss: 0.00002000
Iteration 87/1000 | Loss: 0.00002000
Iteration 88/1000 | Loss: 0.00002000
Iteration 89/1000 | Loss: 0.00002000
Iteration 90/1000 | Loss: 0.00002000
Iteration 91/1000 | Loss: 0.00002000
Iteration 92/1000 | Loss: 0.00002000
Iteration 93/1000 | Loss: 0.00002000
Iteration 94/1000 | Loss: 0.00002000
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00002000
Iteration 97/1000 | Loss: 0.00002000
Iteration 98/1000 | Loss: 0.00002000
Iteration 99/1000 | Loss: 0.00002000
Iteration 100/1000 | Loss: 0.00002000
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00002000
Iteration 104/1000 | Loss: 0.00002000
Iteration 105/1000 | Loss: 0.00002000
Iteration 106/1000 | Loss: 0.00002000
Iteration 107/1000 | Loss: 0.00002000
Iteration 108/1000 | Loss: 0.00002000
Iteration 109/1000 | Loss: 0.00002000
Iteration 110/1000 | Loss: 0.00002000
Iteration 111/1000 | Loss: 0.00002000
Iteration 112/1000 | Loss: 0.00002000
Iteration 113/1000 | Loss: 0.00002000
Iteration 114/1000 | Loss: 0.00002000
Iteration 115/1000 | Loss: 0.00002000
Iteration 116/1000 | Loss: 0.00002000
Iteration 117/1000 | Loss: 0.00002000
Iteration 118/1000 | Loss: 0.00002000
Iteration 119/1000 | Loss: 0.00002000
Iteration 120/1000 | Loss: 0.00002000
Iteration 121/1000 | Loss: 0.00002000
Iteration 122/1000 | Loss: 0.00002000
Iteration 123/1000 | Loss: 0.00002000
Iteration 124/1000 | Loss: 0.00002000
Iteration 125/1000 | Loss: 0.00002000
Iteration 126/1000 | Loss: 0.00002000
Iteration 127/1000 | Loss: 0.00002000
Iteration 128/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.0003833924420178e-05, 2.0003833924420178e-05, 2.0003833924420178e-05, 2.0003833924420178e-05, 2.0003833924420178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0003833924420178e-05

Optimization complete. Final v2v error: 3.901074171066284 mm

Highest mean error: 4.181678295135498 mm for frame 82

Lowest mean error: 3.5822975635528564 mm for frame 129

Saving results

Total time: 74.17315340042114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517484
Iteration 2/25 | Loss: 0.00142965
Iteration 3/25 | Loss: 0.00124167
Iteration 4/25 | Loss: 0.00121240
Iteration 5/25 | Loss: 0.00119981
Iteration 6/25 | Loss: 0.00119876
Iteration 7/25 | Loss: 0.00119876
Iteration 8/25 | Loss: 0.00119876
Iteration 9/25 | Loss: 0.00119876
Iteration 10/25 | Loss: 0.00119876
Iteration 11/25 | Loss: 0.00119876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011987598845735192, 0.0011987598845735192, 0.0011987598845735192, 0.0011987598845735192, 0.0011987598845735192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011987598845735192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85847247
Iteration 2/25 | Loss: 0.00158980
Iteration 3/25 | Loss: 0.00158976
Iteration 4/25 | Loss: 0.00158976
Iteration 5/25 | Loss: 0.00158976
Iteration 6/25 | Loss: 0.00158976
Iteration 7/25 | Loss: 0.00158976
Iteration 8/25 | Loss: 0.00158976
Iteration 9/25 | Loss: 0.00158976
Iteration 10/25 | Loss: 0.00158976
Iteration 11/25 | Loss: 0.00158976
Iteration 12/25 | Loss: 0.00158976
Iteration 13/25 | Loss: 0.00158976
Iteration 14/25 | Loss: 0.00158976
Iteration 15/25 | Loss: 0.00158976
Iteration 16/25 | Loss: 0.00158976
Iteration 17/25 | Loss: 0.00158976
Iteration 18/25 | Loss: 0.00158976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015897563425824046, 0.0015897563425824046, 0.0015897563425824046, 0.0015897563425824046, 0.0015897563425824046]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015897563425824046

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158976
Iteration 2/1000 | Loss: 0.00005884
Iteration 3/1000 | Loss: 0.00004304
Iteration 4/1000 | Loss: 0.00003389
Iteration 5/1000 | Loss: 0.00003188
Iteration 6/1000 | Loss: 0.00003073
Iteration 7/1000 | Loss: 0.00002972
Iteration 8/1000 | Loss: 0.00002884
Iteration 9/1000 | Loss: 0.00002829
Iteration 10/1000 | Loss: 0.00002783
Iteration 11/1000 | Loss: 0.00002749
Iteration 12/1000 | Loss: 0.00002731
Iteration 13/1000 | Loss: 0.00002725
Iteration 14/1000 | Loss: 0.00002719
Iteration 15/1000 | Loss: 0.00002717
Iteration 16/1000 | Loss: 0.00002716
Iteration 17/1000 | Loss: 0.00002712
Iteration 18/1000 | Loss: 0.00002710
Iteration 19/1000 | Loss: 0.00002709
Iteration 20/1000 | Loss: 0.00002709
Iteration 21/1000 | Loss: 0.00002708
Iteration 22/1000 | Loss: 0.00002705
Iteration 23/1000 | Loss: 0.00002705
Iteration 24/1000 | Loss: 0.00002702
Iteration 25/1000 | Loss: 0.00002698
Iteration 26/1000 | Loss: 0.00002695
Iteration 27/1000 | Loss: 0.00002689
Iteration 28/1000 | Loss: 0.00002687
Iteration 29/1000 | Loss: 0.00002686
Iteration 30/1000 | Loss: 0.00002686
Iteration 31/1000 | Loss: 0.00002685
Iteration 32/1000 | Loss: 0.00002685
Iteration 33/1000 | Loss: 0.00002685
Iteration 34/1000 | Loss: 0.00002684
Iteration 35/1000 | Loss: 0.00002684
Iteration 36/1000 | Loss: 0.00002684
Iteration 37/1000 | Loss: 0.00002684
Iteration 38/1000 | Loss: 0.00002684
Iteration 39/1000 | Loss: 0.00002684
Iteration 40/1000 | Loss: 0.00002684
Iteration 41/1000 | Loss: 0.00002684
Iteration 42/1000 | Loss: 0.00002683
Iteration 43/1000 | Loss: 0.00002683
Iteration 44/1000 | Loss: 0.00002683
Iteration 45/1000 | Loss: 0.00002682
Iteration 46/1000 | Loss: 0.00002682
Iteration 47/1000 | Loss: 0.00002682
Iteration 48/1000 | Loss: 0.00002682
Iteration 49/1000 | Loss: 0.00002682
Iteration 50/1000 | Loss: 0.00002682
Iteration 51/1000 | Loss: 0.00002681
Iteration 52/1000 | Loss: 0.00002681
Iteration 53/1000 | Loss: 0.00002681
Iteration 54/1000 | Loss: 0.00002681
Iteration 55/1000 | Loss: 0.00002680
Iteration 56/1000 | Loss: 0.00002680
Iteration 57/1000 | Loss: 0.00002680
Iteration 58/1000 | Loss: 0.00002680
Iteration 59/1000 | Loss: 0.00002680
Iteration 60/1000 | Loss: 0.00002679
Iteration 61/1000 | Loss: 0.00002679
Iteration 62/1000 | Loss: 0.00002678
Iteration 63/1000 | Loss: 0.00002678
Iteration 64/1000 | Loss: 0.00002677
Iteration 65/1000 | Loss: 0.00002677
Iteration 66/1000 | Loss: 0.00002677
Iteration 67/1000 | Loss: 0.00002677
Iteration 68/1000 | Loss: 0.00002677
Iteration 69/1000 | Loss: 0.00002677
Iteration 70/1000 | Loss: 0.00002676
Iteration 71/1000 | Loss: 0.00002676
Iteration 72/1000 | Loss: 0.00002676
Iteration 73/1000 | Loss: 0.00002676
Iteration 74/1000 | Loss: 0.00002676
Iteration 75/1000 | Loss: 0.00002676
Iteration 76/1000 | Loss: 0.00002676
Iteration 77/1000 | Loss: 0.00002676
Iteration 78/1000 | Loss: 0.00002676
Iteration 79/1000 | Loss: 0.00002676
Iteration 80/1000 | Loss: 0.00002676
Iteration 81/1000 | Loss: 0.00002676
Iteration 82/1000 | Loss: 0.00002676
Iteration 83/1000 | Loss: 0.00002676
Iteration 84/1000 | Loss: 0.00002676
Iteration 85/1000 | Loss: 0.00002676
Iteration 86/1000 | Loss: 0.00002676
Iteration 87/1000 | Loss: 0.00002676
Iteration 88/1000 | Loss: 0.00002675
Iteration 89/1000 | Loss: 0.00002675
Iteration 90/1000 | Loss: 0.00002675
Iteration 91/1000 | Loss: 0.00002675
Iteration 92/1000 | Loss: 0.00002675
Iteration 93/1000 | Loss: 0.00002674
Iteration 94/1000 | Loss: 0.00002674
Iteration 95/1000 | Loss: 0.00002673
Iteration 96/1000 | Loss: 0.00002673
Iteration 97/1000 | Loss: 0.00002673
Iteration 98/1000 | Loss: 0.00002673
Iteration 99/1000 | Loss: 0.00002673
Iteration 100/1000 | Loss: 0.00002673
Iteration 101/1000 | Loss: 0.00002673
Iteration 102/1000 | Loss: 0.00002673
Iteration 103/1000 | Loss: 0.00002673
Iteration 104/1000 | Loss: 0.00002673
Iteration 105/1000 | Loss: 0.00002672
Iteration 106/1000 | Loss: 0.00002672
Iteration 107/1000 | Loss: 0.00002672
Iteration 108/1000 | Loss: 0.00002671
Iteration 109/1000 | Loss: 0.00002670
Iteration 110/1000 | Loss: 0.00002670
Iteration 111/1000 | Loss: 0.00002670
Iteration 112/1000 | Loss: 0.00002669
Iteration 113/1000 | Loss: 0.00002669
Iteration 114/1000 | Loss: 0.00002669
Iteration 115/1000 | Loss: 0.00002668
Iteration 116/1000 | Loss: 0.00002668
Iteration 117/1000 | Loss: 0.00002667
Iteration 118/1000 | Loss: 0.00002667
Iteration 119/1000 | Loss: 0.00002667
Iteration 120/1000 | Loss: 0.00002667
Iteration 121/1000 | Loss: 0.00002667
Iteration 122/1000 | Loss: 0.00002667
Iteration 123/1000 | Loss: 0.00002667
Iteration 124/1000 | Loss: 0.00002667
Iteration 125/1000 | Loss: 0.00002667
Iteration 126/1000 | Loss: 0.00002667
Iteration 127/1000 | Loss: 0.00002667
Iteration 128/1000 | Loss: 0.00002667
Iteration 129/1000 | Loss: 0.00002666
Iteration 130/1000 | Loss: 0.00002666
Iteration 131/1000 | Loss: 0.00002666
Iteration 132/1000 | Loss: 0.00002666
Iteration 133/1000 | Loss: 0.00002665
Iteration 134/1000 | Loss: 0.00002665
Iteration 135/1000 | Loss: 0.00002665
Iteration 136/1000 | Loss: 0.00002665
Iteration 137/1000 | Loss: 0.00002665
Iteration 138/1000 | Loss: 0.00002664
Iteration 139/1000 | Loss: 0.00002664
Iteration 140/1000 | Loss: 0.00002664
Iteration 141/1000 | Loss: 0.00002664
Iteration 142/1000 | Loss: 0.00002664
Iteration 143/1000 | Loss: 0.00002664
Iteration 144/1000 | Loss: 0.00002664
Iteration 145/1000 | Loss: 0.00002663
Iteration 146/1000 | Loss: 0.00002663
Iteration 147/1000 | Loss: 0.00002663
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002663
Iteration 151/1000 | Loss: 0.00002663
Iteration 152/1000 | Loss: 0.00002663
Iteration 153/1000 | Loss: 0.00002663
Iteration 154/1000 | Loss: 0.00002663
Iteration 155/1000 | Loss: 0.00002663
Iteration 156/1000 | Loss: 0.00002663
Iteration 157/1000 | Loss: 0.00002663
Iteration 158/1000 | Loss: 0.00002663
Iteration 159/1000 | Loss: 0.00002663
Iteration 160/1000 | Loss: 0.00002663
Iteration 161/1000 | Loss: 0.00002663
Iteration 162/1000 | Loss: 0.00002663
Iteration 163/1000 | Loss: 0.00002663
Iteration 164/1000 | Loss: 0.00002663
Iteration 165/1000 | Loss: 0.00002663
Iteration 166/1000 | Loss: 0.00002663
Iteration 167/1000 | Loss: 0.00002663
Iteration 168/1000 | Loss: 0.00002663
Iteration 169/1000 | Loss: 0.00002663
Iteration 170/1000 | Loss: 0.00002663
Iteration 171/1000 | Loss: 0.00002663
Iteration 172/1000 | Loss: 0.00002663
Iteration 173/1000 | Loss: 0.00002663
Iteration 174/1000 | Loss: 0.00002663
Iteration 175/1000 | Loss: 0.00002663
Iteration 176/1000 | Loss: 0.00002663
Iteration 177/1000 | Loss: 0.00002663
Iteration 178/1000 | Loss: 0.00002663
Iteration 179/1000 | Loss: 0.00002663
Iteration 180/1000 | Loss: 0.00002663
Iteration 181/1000 | Loss: 0.00002663
Iteration 182/1000 | Loss: 0.00002663
Iteration 183/1000 | Loss: 0.00002663
Iteration 184/1000 | Loss: 0.00002663
Iteration 185/1000 | Loss: 0.00002663
Iteration 186/1000 | Loss: 0.00002663
Iteration 187/1000 | Loss: 0.00002663
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.663402665348258e-05, 2.663402665348258e-05, 2.663402665348258e-05, 2.663402665348258e-05, 2.663402665348258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.663402665348258e-05

Optimization complete. Final v2v error: 4.310973644256592 mm

Highest mean error: 4.461667537689209 mm for frame 109

Lowest mean error: 4.197745323181152 mm for frame 1

Saving results

Total time: 84.25791478157043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_nl_6537/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_nl_6537/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864665
Iteration 2/25 | Loss: 0.00130862
Iteration 3/25 | Loss: 0.00120021
Iteration 4/25 | Loss: 0.00116268
Iteration 5/25 | Loss: 0.00114498
Iteration 6/25 | Loss: 0.00114063
Iteration 7/25 | Loss: 0.00113872
Iteration 8/25 | Loss: 0.00113832
Iteration 9/25 | Loss: 0.00113832
Iteration 10/25 | Loss: 0.00113832
Iteration 11/25 | Loss: 0.00113832
Iteration 12/25 | Loss: 0.00113832
Iteration 13/25 | Loss: 0.00113832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011383218225091696, 0.0011383218225091696, 0.0011383218225091696, 0.0011383218225091696, 0.0011383218225091696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011383218225091696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31619227
Iteration 2/25 | Loss: 0.00185907
Iteration 3/25 | Loss: 0.00185907
Iteration 4/25 | Loss: 0.00185907
Iteration 5/25 | Loss: 0.00185907
Iteration 6/25 | Loss: 0.00185907
Iteration 7/25 | Loss: 0.00185907
Iteration 8/25 | Loss: 0.00185907
Iteration 9/25 | Loss: 0.00185907
Iteration 10/25 | Loss: 0.00185907
Iteration 11/25 | Loss: 0.00185907
Iteration 12/25 | Loss: 0.00185907
Iteration 13/25 | Loss: 0.00185907
Iteration 14/25 | Loss: 0.00185907
Iteration 15/25 | Loss: 0.00185907
Iteration 16/25 | Loss: 0.00185907
Iteration 17/25 | Loss: 0.00185907
Iteration 18/25 | Loss: 0.00185907
Iteration 19/25 | Loss: 0.00185907
Iteration 20/25 | Loss: 0.00185907
Iteration 21/25 | Loss: 0.00185907
Iteration 22/25 | Loss: 0.00185907
Iteration 23/25 | Loss: 0.00185907
Iteration 24/25 | Loss: 0.00185907
Iteration 25/25 | Loss: 0.00185907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185907
Iteration 2/1000 | Loss: 0.00010471
Iteration 3/1000 | Loss: 0.00005328
Iteration 4/1000 | Loss: 0.00003813
Iteration 5/1000 | Loss: 0.00003306
Iteration 6/1000 | Loss: 0.00003100
Iteration 7/1000 | Loss: 0.00002962
Iteration 8/1000 | Loss: 0.00002849
Iteration 9/1000 | Loss: 0.00002778
Iteration 10/1000 | Loss: 0.00002719
Iteration 11/1000 | Loss: 0.00002683
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002633
Iteration 14/1000 | Loss: 0.00002610
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00002574
Iteration 17/1000 | Loss: 0.00002561
Iteration 18/1000 | Loss: 0.00002558
Iteration 19/1000 | Loss: 0.00002551
Iteration 20/1000 | Loss: 0.00002547
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002546
Iteration 23/1000 | Loss: 0.00002546
Iteration 24/1000 | Loss: 0.00002545
Iteration 25/1000 | Loss: 0.00002545
Iteration 26/1000 | Loss: 0.00002544
Iteration 27/1000 | Loss: 0.00002544
Iteration 28/1000 | Loss: 0.00002544
Iteration 29/1000 | Loss: 0.00002543
Iteration 30/1000 | Loss: 0.00002543
Iteration 31/1000 | Loss: 0.00002543
Iteration 32/1000 | Loss: 0.00002542
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00002542
Iteration 35/1000 | Loss: 0.00002541
Iteration 36/1000 | Loss: 0.00002541
Iteration 37/1000 | Loss: 0.00002540
Iteration 38/1000 | Loss: 0.00002540
Iteration 39/1000 | Loss: 0.00002539
Iteration 40/1000 | Loss: 0.00002539
Iteration 41/1000 | Loss: 0.00002539
Iteration 42/1000 | Loss: 0.00002539
Iteration 43/1000 | Loss: 0.00002538
Iteration 44/1000 | Loss: 0.00002538
Iteration 45/1000 | Loss: 0.00002538
Iteration 46/1000 | Loss: 0.00002537
Iteration 47/1000 | Loss: 0.00002537
Iteration 48/1000 | Loss: 0.00002537
Iteration 49/1000 | Loss: 0.00002536
Iteration 50/1000 | Loss: 0.00002536
Iteration 51/1000 | Loss: 0.00002535
Iteration 52/1000 | Loss: 0.00002535
Iteration 53/1000 | Loss: 0.00002534
Iteration 54/1000 | Loss: 0.00002534
Iteration 55/1000 | Loss: 0.00002534
Iteration 56/1000 | Loss: 0.00002534
Iteration 57/1000 | Loss: 0.00002534
Iteration 58/1000 | Loss: 0.00002534
Iteration 59/1000 | Loss: 0.00002533
Iteration 60/1000 | Loss: 0.00002533
Iteration 61/1000 | Loss: 0.00002533
Iteration 62/1000 | Loss: 0.00002533
Iteration 63/1000 | Loss: 0.00002533
Iteration 64/1000 | Loss: 0.00002532
Iteration 65/1000 | Loss: 0.00002532
Iteration 66/1000 | Loss: 0.00002532
Iteration 67/1000 | Loss: 0.00002532
Iteration 68/1000 | Loss: 0.00002532
Iteration 69/1000 | Loss: 0.00002532
Iteration 70/1000 | Loss: 0.00002532
Iteration 71/1000 | Loss: 0.00002531
Iteration 72/1000 | Loss: 0.00002531
Iteration 73/1000 | Loss: 0.00002531
Iteration 74/1000 | Loss: 0.00002531
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002530
Iteration 77/1000 | Loss: 0.00002530
Iteration 78/1000 | Loss: 0.00002530
Iteration 79/1000 | Loss: 0.00002529
Iteration 80/1000 | Loss: 0.00002529
Iteration 81/1000 | Loss: 0.00002529
Iteration 82/1000 | Loss: 0.00002528
Iteration 83/1000 | Loss: 0.00002528
Iteration 84/1000 | Loss: 0.00002528
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002526
Iteration 91/1000 | Loss: 0.00002526
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002526
Iteration 94/1000 | Loss: 0.00002526
Iteration 95/1000 | Loss: 0.00002526
Iteration 96/1000 | Loss: 0.00002525
Iteration 97/1000 | Loss: 0.00002525
Iteration 98/1000 | Loss: 0.00002525
Iteration 99/1000 | Loss: 0.00002524
Iteration 100/1000 | Loss: 0.00002523
Iteration 101/1000 | Loss: 0.00002523
Iteration 102/1000 | Loss: 0.00002523
Iteration 103/1000 | Loss: 0.00002523
Iteration 104/1000 | Loss: 0.00002523
Iteration 105/1000 | Loss: 0.00002522
Iteration 106/1000 | Loss: 0.00002522
Iteration 107/1000 | Loss: 0.00002522
Iteration 108/1000 | Loss: 0.00002522
Iteration 109/1000 | Loss: 0.00002521
Iteration 110/1000 | Loss: 0.00002521
Iteration 111/1000 | Loss: 0.00002521
Iteration 112/1000 | Loss: 0.00002521
Iteration 113/1000 | Loss: 0.00002521
Iteration 114/1000 | Loss: 0.00002521
Iteration 115/1000 | Loss: 0.00002521
Iteration 116/1000 | Loss: 0.00002521
Iteration 117/1000 | Loss: 0.00002521
Iteration 118/1000 | Loss: 0.00002521
Iteration 119/1000 | Loss: 0.00002521
Iteration 120/1000 | Loss: 0.00002520
Iteration 121/1000 | Loss: 0.00002520
Iteration 122/1000 | Loss: 0.00002520
Iteration 123/1000 | Loss: 0.00002519
Iteration 124/1000 | Loss: 0.00002519
Iteration 125/1000 | Loss: 0.00002519
Iteration 126/1000 | Loss: 0.00002519
Iteration 127/1000 | Loss: 0.00002519
Iteration 128/1000 | Loss: 0.00002519
Iteration 129/1000 | Loss: 0.00002519
Iteration 130/1000 | Loss: 0.00002519
Iteration 131/1000 | Loss: 0.00002519
Iteration 132/1000 | Loss: 0.00002518
Iteration 133/1000 | Loss: 0.00002518
Iteration 134/1000 | Loss: 0.00002518
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002518
Iteration 137/1000 | Loss: 0.00002518
Iteration 138/1000 | Loss: 0.00002518
Iteration 139/1000 | Loss: 0.00002518
Iteration 140/1000 | Loss: 0.00002518
Iteration 141/1000 | Loss: 0.00002518
Iteration 142/1000 | Loss: 0.00002518
Iteration 143/1000 | Loss: 0.00002518
Iteration 144/1000 | Loss: 0.00002518
Iteration 145/1000 | Loss: 0.00002518
Iteration 146/1000 | Loss: 0.00002518
Iteration 147/1000 | Loss: 0.00002518
Iteration 148/1000 | Loss: 0.00002518
Iteration 149/1000 | Loss: 0.00002517
Iteration 150/1000 | Loss: 0.00002517
Iteration 151/1000 | Loss: 0.00002517
Iteration 152/1000 | Loss: 0.00002517
Iteration 153/1000 | Loss: 0.00002517
Iteration 154/1000 | Loss: 0.00002517
Iteration 155/1000 | Loss: 0.00002517
Iteration 156/1000 | Loss: 0.00002517
Iteration 157/1000 | Loss: 0.00002517
Iteration 158/1000 | Loss: 0.00002517
Iteration 159/1000 | Loss: 0.00002516
Iteration 160/1000 | Loss: 0.00002516
Iteration 161/1000 | Loss: 0.00002516
Iteration 162/1000 | Loss: 0.00002516
Iteration 163/1000 | Loss: 0.00002516
Iteration 164/1000 | Loss: 0.00002516
Iteration 165/1000 | Loss: 0.00002516
Iteration 166/1000 | Loss: 0.00002516
Iteration 167/1000 | Loss: 0.00002515
Iteration 168/1000 | Loss: 0.00002515
Iteration 169/1000 | Loss: 0.00002515
Iteration 170/1000 | Loss: 0.00002515
Iteration 171/1000 | Loss: 0.00002515
Iteration 172/1000 | Loss: 0.00002515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.515468622732442e-05, 2.515468622732442e-05, 2.515468622732442e-05, 2.515468622732442e-05, 2.515468622732442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.515468622732442e-05

Optimization complete. Final v2v error: 4.245763301849365 mm

Highest mean error: 5.907750129699707 mm for frame 127

Lowest mean error: 3.5529098510742188 mm for frame 69

Saving results

Total time: 108.358145236969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061805
Iteration 2/25 | Loss: 0.01061805
Iteration 3/25 | Loss: 0.01061805
Iteration 4/25 | Loss: 0.01061805
Iteration 5/25 | Loss: 0.01061805
Iteration 6/25 | Loss: 0.01061804
Iteration 7/25 | Loss: 0.01061804
Iteration 8/25 | Loss: 0.01061804
Iteration 9/25 | Loss: 0.01061804
Iteration 10/25 | Loss: 0.01061804
Iteration 11/25 | Loss: 0.01061804
Iteration 12/25 | Loss: 0.01061804
Iteration 13/25 | Loss: 0.01061804
Iteration 14/25 | Loss: 0.01061803
Iteration 15/25 | Loss: 0.01061803
Iteration 16/25 | Loss: 0.01061803
Iteration 17/25 | Loss: 0.01061803
Iteration 18/25 | Loss: 0.01061803
Iteration 19/25 | Loss: 0.01061803
Iteration 20/25 | Loss: 0.01061803
Iteration 21/25 | Loss: 0.01061803
Iteration 22/25 | Loss: 0.01061803
Iteration 23/25 | Loss: 0.01061802
Iteration 24/25 | Loss: 0.01061802
Iteration 25/25 | Loss: 0.01061802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60906696
Iteration 2/25 | Loss: 0.06583214
Iteration 3/25 | Loss: 0.06552008
Iteration 4/25 | Loss: 0.06552007
Iteration 5/25 | Loss: 0.06552006
Iteration 6/25 | Loss: 0.06552006
Iteration 7/25 | Loss: 0.06552006
Iteration 8/25 | Loss: 0.06552006
Iteration 9/25 | Loss: 0.06552006
Iteration 10/25 | Loss: 0.06552006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.06552006304264069, 0.06552006304264069, 0.06552006304264069, 0.06552006304264069, 0.06552006304264069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06552006304264069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06552006
Iteration 2/1000 | Loss: 0.01583234
Iteration 3/1000 | Loss: 0.00427106
Iteration 4/1000 | Loss: 0.00201453
Iteration 5/1000 | Loss: 0.00368148
Iteration 6/1000 | Loss: 0.00060786
Iteration 7/1000 | Loss: 0.00111060
Iteration 8/1000 | Loss: 0.00013979
Iteration 9/1000 | Loss: 0.00015505
Iteration 10/1000 | Loss: 0.00010980
Iteration 11/1000 | Loss: 0.00021093
Iteration 12/1000 | Loss: 0.00008600
Iteration 13/1000 | Loss: 0.00031606
Iteration 14/1000 | Loss: 0.00298178
Iteration 15/1000 | Loss: 0.00007555
Iteration 16/1000 | Loss: 0.00016980
Iteration 17/1000 | Loss: 0.00006108
Iteration 18/1000 | Loss: 0.00007423
Iteration 19/1000 | Loss: 0.00003452
Iteration 20/1000 | Loss: 0.00018184
Iteration 21/1000 | Loss: 0.00006494
Iteration 22/1000 | Loss: 0.00013318
Iteration 23/1000 | Loss: 0.00003800
Iteration 24/1000 | Loss: 0.00009969
Iteration 25/1000 | Loss: 0.00015508
Iteration 26/1000 | Loss: 0.00010912
Iteration 27/1000 | Loss: 0.00014145
Iteration 28/1000 | Loss: 0.00003413
Iteration 29/1000 | Loss: 0.00005034
Iteration 30/1000 | Loss: 0.00012663
Iteration 31/1000 | Loss: 0.00051740
Iteration 32/1000 | Loss: 0.00111759
Iteration 33/1000 | Loss: 0.00007500
Iteration 34/1000 | Loss: 0.00003638
Iteration 35/1000 | Loss: 0.00002246
Iteration 36/1000 | Loss: 0.00011016
Iteration 37/1000 | Loss: 0.00010810
Iteration 38/1000 | Loss: 0.00029415
Iteration 39/1000 | Loss: 0.00023086
Iteration 40/1000 | Loss: 0.00003943
Iteration 41/1000 | Loss: 0.00010580
Iteration 42/1000 | Loss: 0.00182801
Iteration 43/1000 | Loss: 0.00029809
Iteration 44/1000 | Loss: 0.00066760
Iteration 45/1000 | Loss: 0.00018149
Iteration 46/1000 | Loss: 0.00012307
Iteration 47/1000 | Loss: 0.00024255
Iteration 48/1000 | Loss: 0.00009479
Iteration 49/1000 | Loss: 0.00011375
Iteration 50/1000 | Loss: 0.00005495
Iteration 51/1000 | Loss: 0.00005095
Iteration 52/1000 | Loss: 0.00004498
Iteration 53/1000 | Loss: 0.00003532
Iteration 54/1000 | Loss: 0.00005055
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002105
Iteration 57/1000 | Loss: 0.00003398
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00004286
Iteration 60/1000 | Loss: 0.00006028
Iteration 61/1000 | Loss: 0.00002898
Iteration 62/1000 | Loss: 0.00002023
Iteration 63/1000 | Loss: 0.00007184
Iteration 64/1000 | Loss: 0.00016896
Iteration 65/1000 | Loss: 0.00013727
Iteration 66/1000 | Loss: 0.00032425
Iteration 67/1000 | Loss: 0.00085631
Iteration 68/1000 | Loss: 0.00004827
Iteration 69/1000 | Loss: 0.00004026
Iteration 70/1000 | Loss: 0.00005096
Iteration 71/1000 | Loss: 0.00017538
Iteration 72/1000 | Loss: 0.00006273
Iteration 73/1000 | Loss: 0.00010107
Iteration 74/1000 | Loss: 0.00006002
Iteration 75/1000 | Loss: 0.00011123
Iteration 76/1000 | Loss: 0.00004764
Iteration 77/1000 | Loss: 0.00002165
Iteration 78/1000 | Loss: 0.00010038
Iteration 79/1000 | Loss: 0.00005287
Iteration 80/1000 | Loss: 0.00013245
Iteration 81/1000 | Loss: 0.00002990
Iteration 82/1000 | Loss: 0.00002870
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00023311
Iteration 85/1000 | Loss: 0.00018522
Iteration 86/1000 | Loss: 0.00002355
Iteration 87/1000 | Loss: 0.00018079
Iteration 88/1000 | Loss: 0.00005996
Iteration 89/1000 | Loss: 0.00002479
Iteration 90/1000 | Loss: 0.00008074
Iteration 91/1000 | Loss: 0.00089057
Iteration 92/1000 | Loss: 0.00054287
Iteration 93/1000 | Loss: 0.00011153
Iteration 94/1000 | Loss: 0.00007138
Iteration 95/1000 | Loss: 0.00027160
Iteration 96/1000 | Loss: 0.00024225
Iteration 97/1000 | Loss: 0.00038940
Iteration 98/1000 | Loss: 0.00026648
Iteration 99/1000 | Loss: 0.00008430
Iteration 100/1000 | Loss: 0.00013701
Iteration 101/1000 | Loss: 0.00089562
Iteration 102/1000 | Loss: 0.00058549
Iteration 103/1000 | Loss: 0.00054583
Iteration 104/1000 | Loss: 0.00062116
Iteration 105/1000 | Loss: 0.00010181
Iteration 106/1000 | Loss: 0.00005510
Iteration 107/1000 | Loss: 0.00004890
Iteration 108/1000 | Loss: 0.00002076
Iteration 109/1000 | Loss: 0.00008849
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00002003
Iteration 112/1000 | Loss: 0.00008152
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00002716
Iteration 115/1000 | Loss: 0.00007999
Iteration 116/1000 | Loss: 0.00002414
Iteration 117/1000 | Loss: 0.00002530
Iteration 118/1000 | Loss: 0.00002409
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00003165
Iteration 121/1000 | Loss: 0.00001739
Iteration 122/1000 | Loss: 0.00001720
Iteration 123/1000 | Loss: 0.00001719
Iteration 124/1000 | Loss: 0.00009288
Iteration 125/1000 | Loss: 0.00044364
Iteration 126/1000 | Loss: 0.00048937
Iteration 127/1000 | Loss: 0.00022887
Iteration 128/1000 | Loss: 0.00016526
Iteration 129/1000 | Loss: 0.00002753
Iteration 130/1000 | Loss: 0.00003573
Iteration 131/1000 | Loss: 0.00009705
Iteration 132/1000 | Loss: 0.00005103
Iteration 133/1000 | Loss: 0.00012951
Iteration 134/1000 | Loss: 0.00004910
Iteration 135/1000 | Loss: 0.00009215
Iteration 136/1000 | Loss: 0.00004226
Iteration 137/1000 | Loss: 0.00005196
Iteration 138/1000 | Loss: 0.00001952
Iteration 139/1000 | Loss: 0.00009352
Iteration 140/1000 | Loss: 0.00002751
Iteration 141/1000 | Loss: 0.00005410
Iteration 142/1000 | Loss: 0.00001727
Iteration 143/1000 | Loss: 0.00001918
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001694
Iteration 146/1000 | Loss: 0.00001694
Iteration 147/1000 | Loss: 0.00001693
Iteration 148/1000 | Loss: 0.00001693
Iteration 149/1000 | Loss: 0.00001693
Iteration 150/1000 | Loss: 0.00001693
Iteration 151/1000 | Loss: 0.00001693
Iteration 152/1000 | Loss: 0.00001693
Iteration 153/1000 | Loss: 0.00001693
Iteration 154/1000 | Loss: 0.00001693
Iteration 155/1000 | Loss: 0.00001693
Iteration 156/1000 | Loss: 0.00001693
Iteration 157/1000 | Loss: 0.00001693
Iteration 158/1000 | Loss: 0.00001693
Iteration 159/1000 | Loss: 0.00001692
Iteration 160/1000 | Loss: 0.00001691
Iteration 161/1000 | Loss: 0.00001691
Iteration 162/1000 | Loss: 0.00001691
Iteration 163/1000 | Loss: 0.00001691
Iteration 164/1000 | Loss: 0.00001691
Iteration 165/1000 | Loss: 0.00001691
Iteration 166/1000 | Loss: 0.00001690
Iteration 167/1000 | Loss: 0.00001690
Iteration 168/1000 | Loss: 0.00001690
Iteration 169/1000 | Loss: 0.00003303
Iteration 170/1000 | Loss: 0.00009120
Iteration 171/1000 | Loss: 0.00004909
Iteration 172/1000 | Loss: 0.00002502
Iteration 173/1000 | Loss: 0.00006223
Iteration 174/1000 | Loss: 0.00001704
Iteration 175/1000 | Loss: 0.00004428
Iteration 176/1000 | Loss: 0.00012964
Iteration 177/1000 | Loss: 0.00022351
Iteration 178/1000 | Loss: 0.00044022
Iteration 179/1000 | Loss: 0.00027420
Iteration 180/1000 | Loss: 0.00008082
Iteration 181/1000 | Loss: 0.00010135
Iteration 182/1000 | Loss: 0.00008127
Iteration 183/1000 | Loss: 0.00009021
Iteration 184/1000 | Loss: 0.00009947
Iteration 185/1000 | Loss: 0.00013122
Iteration 186/1000 | Loss: 0.00009685
Iteration 187/1000 | Loss: 0.00005885
Iteration 188/1000 | Loss: 0.00008526
Iteration 189/1000 | Loss: 0.00006735
Iteration 190/1000 | Loss: 0.00001959
Iteration 191/1000 | Loss: 0.00002162
Iteration 192/1000 | Loss: 0.00003655
Iteration 193/1000 | Loss: 0.00002222
Iteration 194/1000 | Loss: 0.00003946
Iteration 195/1000 | Loss: 0.00002123
Iteration 196/1000 | Loss: 0.00003640
Iteration 197/1000 | Loss: 0.00010426
Iteration 198/1000 | Loss: 0.00005106
Iteration 199/1000 | Loss: 0.00002806
Iteration 200/1000 | Loss: 0.00003598
Iteration 201/1000 | Loss: 0.00004347
Iteration 202/1000 | Loss: 0.00003640
Iteration 203/1000 | Loss: 0.00003048
Iteration 204/1000 | Loss: 0.00005303
Iteration 205/1000 | Loss: 0.00005977
Iteration 206/1000 | Loss: 0.00007208
Iteration 207/1000 | Loss: 0.00004480
Iteration 208/1000 | Loss: 0.00002826
Iteration 209/1000 | Loss: 0.00002133
Iteration 210/1000 | Loss: 0.00004826
Iteration 211/1000 | Loss: 0.00001720
Iteration 212/1000 | Loss: 0.00002532
Iteration 213/1000 | Loss: 0.00002974
Iteration 214/1000 | Loss: 0.00001748
Iteration 215/1000 | Loss: 0.00001680
Iteration 216/1000 | Loss: 0.00001679
Iteration 217/1000 | Loss: 0.00001679
Iteration 218/1000 | Loss: 0.00001679
Iteration 219/1000 | Loss: 0.00001679
Iteration 220/1000 | Loss: 0.00001679
Iteration 221/1000 | Loss: 0.00001679
Iteration 222/1000 | Loss: 0.00001679
Iteration 223/1000 | Loss: 0.00001679
Iteration 224/1000 | Loss: 0.00001679
Iteration 225/1000 | Loss: 0.00001679
Iteration 226/1000 | Loss: 0.00002274
Iteration 227/1000 | Loss: 0.00001719
Iteration 228/1000 | Loss: 0.00001734
Iteration 229/1000 | Loss: 0.00001832
Iteration 230/1000 | Loss: 0.00003410
Iteration 231/1000 | Loss: 0.00001701
Iteration 232/1000 | Loss: 0.00001678
Iteration 233/1000 | Loss: 0.00001678
Iteration 234/1000 | Loss: 0.00001678
Iteration 235/1000 | Loss: 0.00001678
Iteration 236/1000 | Loss: 0.00001677
Iteration 237/1000 | Loss: 0.00001677
Iteration 238/1000 | Loss: 0.00001677
Iteration 239/1000 | Loss: 0.00001677
Iteration 240/1000 | Loss: 0.00001677
Iteration 241/1000 | Loss: 0.00001677
Iteration 242/1000 | Loss: 0.00001677
Iteration 243/1000 | Loss: 0.00001677
Iteration 244/1000 | Loss: 0.00001677
Iteration 245/1000 | Loss: 0.00001677
Iteration 246/1000 | Loss: 0.00001676
Iteration 247/1000 | Loss: 0.00001676
Iteration 248/1000 | Loss: 0.00001676
Iteration 249/1000 | Loss: 0.00001676
Iteration 250/1000 | Loss: 0.00001675
Iteration 251/1000 | Loss: 0.00002708
Iteration 252/1000 | Loss: 0.00001787
Iteration 253/1000 | Loss: 0.00001721
Iteration 254/1000 | Loss: 0.00001676
Iteration 255/1000 | Loss: 0.00001676
Iteration 256/1000 | Loss: 0.00001676
Iteration 257/1000 | Loss: 0.00001676
Iteration 258/1000 | Loss: 0.00001675
Iteration 259/1000 | Loss: 0.00001675
Iteration 260/1000 | Loss: 0.00001738
Iteration 261/1000 | Loss: 0.00001675
Iteration 262/1000 | Loss: 0.00001674
Iteration 263/1000 | Loss: 0.00001674
Iteration 264/1000 | Loss: 0.00001674
Iteration 265/1000 | Loss: 0.00001674
Iteration 266/1000 | Loss: 0.00001674
Iteration 267/1000 | Loss: 0.00001674
Iteration 268/1000 | Loss: 0.00001674
Iteration 269/1000 | Loss: 0.00001674
Iteration 270/1000 | Loss: 0.00001674
Iteration 271/1000 | Loss: 0.00001674
Iteration 272/1000 | Loss: 0.00001674
Iteration 273/1000 | Loss: 0.00001674
Iteration 274/1000 | Loss: 0.00001723
Iteration 275/1000 | Loss: 0.00001808
Iteration 276/1000 | Loss: 0.00002378
Iteration 277/1000 | Loss: 0.00012676
Iteration 278/1000 | Loss: 0.00004570
Iteration 279/1000 | Loss: 0.00002576
Iteration 280/1000 | Loss: 0.00001761
Iteration 281/1000 | Loss: 0.00001675
Iteration 282/1000 | Loss: 0.00001675
Iteration 283/1000 | Loss: 0.00001675
Iteration 284/1000 | Loss: 0.00001674
Iteration 285/1000 | Loss: 0.00001674
Iteration 286/1000 | Loss: 0.00001674
Iteration 287/1000 | Loss: 0.00001722
Iteration 288/1000 | Loss: 0.00001674
Iteration 289/1000 | Loss: 0.00001674
Iteration 290/1000 | Loss: 0.00001674
Iteration 291/1000 | Loss: 0.00001674
Iteration 292/1000 | Loss: 0.00001674
Iteration 293/1000 | Loss: 0.00001674
Iteration 294/1000 | Loss: 0.00001674
Iteration 295/1000 | Loss: 0.00001674
Iteration 296/1000 | Loss: 0.00001674
Iteration 297/1000 | Loss: 0.00001674
Iteration 298/1000 | Loss: 0.00001674
Iteration 299/1000 | Loss: 0.00001674
Iteration 300/1000 | Loss: 0.00001674
Iteration 301/1000 | Loss: 0.00001674
Iteration 302/1000 | Loss: 0.00001674
Iteration 303/1000 | Loss: 0.00001674
Iteration 304/1000 | Loss: 0.00001674
Iteration 305/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [1.6736188626964577e-05, 1.6736188626964577e-05, 1.6736188626964577e-05, 1.6736188626964577e-05, 1.6736188626964577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6736188626964577e-05

Optimization complete. Final v2v error: 3.3029677867889404 mm

Highest mean error: 10.262887001037598 mm for frame 194

Lowest mean error: 2.607861042022705 mm for frame 19

Saving results

Total time: 867.7559168338776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977191
Iteration 2/25 | Loss: 0.00333219
Iteration 3/25 | Loss: 0.00212482
Iteration 4/25 | Loss: 0.00188714
Iteration 5/25 | Loss: 0.00176380
Iteration 6/25 | Loss: 0.00166685
Iteration 7/25 | Loss: 0.00161789
Iteration 8/25 | Loss: 0.00157345
Iteration 9/25 | Loss: 0.00155488
Iteration 10/25 | Loss: 0.00154282
Iteration 11/25 | Loss: 0.00154908
Iteration 12/25 | Loss: 0.00153471
Iteration 13/25 | Loss: 0.00151583
Iteration 14/25 | Loss: 0.00149597
Iteration 15/25 | Loss: 0.00148177
Iteration 16/25 | Loss: 0.00146472
Iteration 17/25 | Loss: 0.00145521
Iteration 18/25 | Loss: 0.00145656
Iteration 19/25 | Loss: 0.00144898
Iteration 20/25 | Loss: 0.00144382
Iteration 21/25 | Loss: 0.00143691
Iteration 22/25 | Loss: 0.00142866
Iteration 23/25 | Loss: 0.00143479
Iteration 24/25 | Loss: 0.00141747
Iteration 25/25 | Loss: 0.00141028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31725621
Iteration 2/25 | Loss: 0.01390767
Iteration 3/25 | Loss: 0.05423927
Iteration 4/25 | Loss: 0.07501633
Iteration 5/25 | Loss: 0.05470484
Iteration 6/25 | Loss: 0.04613702
Iteration 7/25 | Loss: 0.03156083
Iteration 8/25 | Loss: 0.02135433
Iteration 9/25 | Loss: 0.00456745
Iteration 10/25 | Loss: 0.00800540
Iteration 11/25 | Loss: 0.00435537
Iteration 12/25 | Loss: 0.00300883
Iteration 13/25 | Loss: 0.00300424
Iteration 14/25 | Loss: 0.00300424
Iteration 15/25 | Loss: 0.00300424
Iteration 16/25 | Loss: 0.00300424
Iteration 17/25 | Loss: 0.00300424
Iteration 18/25 | Loss: 0.00300424
Iteration 19/25 | Loss: 0.00300424
Iteration 20/25 | Loss: 0.00300424
Iteration 21/25 | Loss: 0.00300424
Iteration 22/25 | Loss: 0.00300424
Iteration 23/25 | Loss: 0.00300424
Iteration 24/25 | Loss: 0.00300424
Iteration 25/25 | Loss: 0.00300424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300424
Iteration 2/1000 | Loss: 0.00039874
Iteration 3/1000 | Loss: 0.00249385
Iteration 4/1000 | Loss: 0.00279166
Iteration 5/1000 | Loss: 0.00028762
Iteration 6/1000 | Loss: 0.00377571
Iteration 7/1000 | Loss: 0.00220576
Iteration 8/1000 | Loss: 0.00106341
Iteration 9/1000 | Loss: 0.00082570
Iteration 10/1000 | Loss: 0.00313949
Iteration 11/1000 | Loss: 0.00050156
Iteration 12/1000 | Loss: 0.00059655
Iteration 13/1000 | Loss: 0.00040449
Iteration 14/1000 | Loss: 0.00027338
Iteration 15/1000 | Loss: 0.00021304
Iteration 16/1000 | Loss: 0.00014310
Iteration 17/1000 | Loss: 0.00023679
Iteration 18/1000 | Loss: 0.00083406
Iteration 19/1000 | Loss: 0.00037495
Iteration 20/1000 | Loss: 0.00028247
Iteration 21/1000 | Loss: 0.00067048
Iteration 22/1000 | Loss: 0.00048333
Iteration 23/1000 | Loss: 0.00045135
Iteration 24/1000 | Loss: 0.00027954
Iteration 25/1000 | Loss: 0.00013425
Iteration 26/1000 | Loss: 0.00015531
Iteration 27/1000 | Loss: 0.00036994
Iteration 28/1000 | Loss: 0.00029106
Iteration 29/1000 | Loss: 0.00031230
Iteration 30/1000 | Loss: 0.00051013
Iteration 31/1000 | Loss: 0.00031299
Iteration 32/1000 | Loss: 0.00027364
Iteration 33/1000 | Loss: 0.00050611
Iteration 34/1000 | Loss: 0.00023306
Iteration 35/1000 | Loss: 0.00022199
Iteration 36/1000 | Loss: 0.00038013
Iteration 37/1000 | Loss: 0.00037478
Iteration 38/1000 | Loss: 0.00031193
Iteration 39/1000 | Loss: 0.00038886
Iteration 40/1000 | Loss: 0.00015217
Iteration 41/1000 | Loss: 0.00023555
Iteration 42/1000 | Loss: 0.00022394
Iteration 43/1000 | Loss: 0.00025850
Iteration 44/1000 | Loss: 0.00030832
Iteration 45/1000 | Loss: 0.00013131
Iteration 46/1000 | Loss: 0.00017395
Iteration 47/1000 | Loss: 0.00039670
Iteration 48/1000 | Loss: 0.00018385
Iteration 49/1000 | Loss: 0.00098083
Iteration 50/1000 | Loss: 0.00234236
Iteration 51/1000 | Loss: 0.00388943
Iteration 52/1000 | Loss: 0.00079515
Iteration 53/1000 | Loss: 0.00124159
Iteration 54/1000 | Loss: 0.00023804
Iteration 55/1000 | Loss: 0.00024208
Iteration 56/1000 | Loss: 0.00041231
Iteration 57/1000 | Loss: 0.00015861
Iteration 58/1000 | Loss: 0.00019774
Iteration 59/1000 | Loss: 0.00015570
Iteration 60/1000 | Loss: 0.00010372
Iteration 61/1000 | Loss: 0.00010059
Iteration 62/1000 | Loss: 0.00044853
Iteration 63/1000 | Loss: 0.00023641
Iteration 64/1000 | Loss: 0.00013484
Iteration 65/1000 | Loss: 0.00100132
Iteration 66/1000 | Loss: 0.00246434
Iteration 67/1000 | Loss: 0.00132830
Iteration 68/1000 | Loss: 0.00214444
Iteration 69/1000 | Loss: 0.00135590
Iteration 70/1000 | Loss: 0.00171276
Iteration 71/1000 | Loss: 0.00184292
Iteration 72/1000 | Loss: 0.00032643
Iteration 73/1000 | Loss: 0.00216679
Iteration 74/1000 | Loss: 0.00146951
Iteration 75/1000 | Loss: 0.00013436
Iteration 76/1000 | Loss: 0.00026381
Iteration 77/1000 | Loss: 0.00009359
Iteration 78/1000 | Loss: 0.00047951
Iteration 79/1000 | Loss: 0.00025720
Iteration 80/1000 | Loss: 0.00204688
Iteration 81/1000 | Loss: 0.00085285
Iteration 82/1000 | Loss: 0.00057662
Iteration 83/1000 | Loss: 0.00012455
Iteration 84/1000 | Loss: 0.00007780
Iteration 85/1000 | Loss: 0.00020541
Iteration 86/1000 | Loss: 0.00013208
Iteration 87/1000 | Loss: 0.00010833
Iteration 88/1000 | Loss: 0.00014987
Iteration 89/1000 | Loss: 0.00018265
Iteration 90/1000 | Loss: 0.00014812
Iteration 91/1000 | Loss: 0.00064478
Iteration 92/1000 | Loss: 0.00018374
Iteration 93/1000 | Loss: 0.00017258
Iteration 94/1000 | Loss: 0.00056166
Iteration 95/1000 | Loss: 0.00079007
Iteration 96/1000 | Loss: 0.00143101
Iteration 97/1000 | Loss: 0.00199023
Iteration 98/1000 | Loss: 0.00196232
Iteration 99/1000 | Loss: 0.00060807
Iteration 100/1000 | Loss: 0.00008911
Iteration 101/1000 | Loss: 0.00039470
Iteration 102/1000 | Loss: 0.00094123
Iteration 103/1000 | Loss: 0.00047832
Iteration 104/1000 | Loss: 0.00061119
Iteration 105/1000 | Loss: 0.00030011
Iteration 106/1000 | Loss: 0.00005616
Iteration 107/1000 | Loss: 0.00051661
Iteration 108/1000 | Loss: 0.00035468
Iteration 109/1000 | Loss: 0.00059882
Iteration 110/1000 | Loss: 0.00045651
Iteration 111/1000 | Loss: 0.00096067
Iteration 112/1000 | Loss: 0.00050158
Iteration 113/1000 | Loss: 0.00115899
Iteration 114/1000 | Loss: 0.00005892
Iteration 115/1000 | Loss: 0.00031602
Iteration 116/1000 | Loss: 0.00005548
Iteration 117/1000 | Loss: 0.00015528
Iteration 118/1000 | Loss: 0.00005207
Iteration 119/1000 | Loss: 0.00005078
Iteration 120/1000 | Loss: 0.00005006
Iteration 121/1000 | Loss: 0.00004856
Iteration 122/1000 | Loss: 0.00047089
Iteration 123/1000 | Loss: 0.00056508
Iteration 124/1000 | Loss: 0.00009105
Iteration 125/1000 | Loss: 0.00004802
Iteration 126/1000 | Loss: 0.00132590
Iteration 127/1000 | Loss: 0.00052801
Iteration 128/1000 | Loss: 0.00036444
Iteration 129/1000 | Loss: 0.00034815
Iteration 130/1000 | Loss: 0.00118060
Iteration 131/1000 | Loss: 0.00057543
Iteration 132/1000 | Loss: 0.00008228
Iteration 133/1000 | Loss: 0.00013246
Iteration 134/1000 | Loss: 0.00004608
Iteration 135/1000 | Loss: 0.00054205
Iteration 136/1000 | Loss: 0.00050875
Iteration 137/1000 | Loss: 0.00055995
Iteration 138/1000 | Loss: 0.00052912
Iteration 139/1000 | Loss: 0.00006462
Iteration 140/1000 | Loss: 0.00008828
Iteration 141/1000 | Loss: 0.00016430
Iteration 142/1000 | Loss: 0.00006828
Iteration 143/1000 | Loss: 0.00004385
Iteration 144/1000 | Loss: 0.00004222
Iteration 145/1000 | Loss: 0.00004143
Iteration 146/1000 | Loss: 0.00030016
Iteration 147/1000 | Loss: 0.00004434
Iteration 148/1000 | Loss: 0.00016403
Iteration 149/1000 | Loss: 0.00006752
Iteration 150/1000 | Loss: 0.00004042
Iteration 151/1000 | Loss: 0.00035241
Iteration 152/1000 | Loss: 0.00009282
Iteration 153/1000 | Loss: 0.00013059
Iteration 154/1000 | Loss: 0.00005922
Iteration 155/1000 | Loss: 0.00006881
Iteration 156/1000 | Loss: 0.00023421
Iteration 157/1000 | Loss: 0.00003977
Iteration 158/1000 | Loss: 0.00003952
Iteration 159/1000 | Loss: 0.00003935
Iteration 160/1000 | Loss: 0.00003913
Iteration 161/1000 | Loss: 0.00003911
Iteration 162/1000 | Loss: 0.00003904
Iteration 163/1000 | Loss: 0.00003904
Iteration 164/1000 | Loss: 0.00015440
Iteration 165/1000 | Loss: 0.00004119
Iteration 166/1000 | Loss: 0.00003921
Iteration 167/1000 | Loss: 0.00003847
Iteration 168/1000 | Loss: 0.00003775
Iteration 169/1000 | Loss: 0.00005887
Iteration 170/1000 | Loss: 0.00004225
Iteration 171/1000 | Loss: 0.00003949
Iteration 172/1000 | Loss: 0.00003725
Iteration 173/1000 | Loss: 0.00003705
Iteration 174/1000 | Loss: 0.00003704
Iteration 175/1000 | Loss: 0.00003703
Iteration 176/1000 | Loss: 0.00003703
Iteration 177/1000 | Loss: 0.00003702
Iteration 178/1000 | Loss: 0.00003700
Iteration 179/1000 | Loss: 0.00004482
Iteration 180/1000 | Loss: 0.00003686
Iteration 181/1000 | Loss: 0.00003685
Iteration 182/1000 | Loss: 0.00003684
Iteration 183/1000 | Loss: 0.00003684
Iteration 184/1000 | Loss: 0.00003683
Iteration 185/1000 | Loss: 0.00003680
Iteration 186/1000 | Loss: 0.00003669
Iteration 187/1000 | Loss: 0.00003668
Iteration 188/1000 | Loss: 0.00003667
Iteration 189/1000 | Loss: 0.00003666
Iteration 190/1000 | Loss: 0.00003666
Iteration 191/1000 | Loss: 0.00003666
Iteration 192/1000 | Loss: 0.00003665
Iteration 193/1000 | Loss: 0.00003665
Iteration 194/1000 | Loss: 0.00003665
Iteration 195/1000 | Loss: 0.00003665
Iteration 196/1000 | Loss: 0.00003665
Iteration 197/1000 | Loss: 0.00003664
Iteration 198/1000 | Loss: 0.00003663
Iteration 199/1000 | Loss: 0.00003662
Iteration 200/1000 | Loss: 0.00003661
Iteration 201/1000 | Loss: 0.00003661
Iteration 202/1000 | Loss: 0.00003660
Iteration 203/1000 | Loss: 0.00003660
Iteration 204/1000 | Loss: 0.00003659
Iteration 205/1000 | Loss: 0.00003659
Iteration 206/1000 | Loss: 0.00003659
Iteration 207/1000 | Loss: 0.00003658
Iteration 208/1000 | Loss: 0.00003658
Iteration 209/1000 | Loss: 0.00003658
Iteration 210/1000 | Loss: 0.00003657
Iteration 211/1000 | Loss: 0.00003657
Iteration 212/1000 | Loss: 0.00003657
Iteration 213/1000 | Loss: 0.00003657
Iteration 214/1000 | Loss: 0.00003656
Iteration 215/1000 | Loss: 0.00003656
Iteration 216/1000 | Loss: 0.00003656
Iteration 217/1000 | Loss: 0.00003656
Iteration 218/1000 | Loss: 0.00003656
Iteration 219/1000 | Loss: 0.00003655
Iteration 220/1000 | Loss: 0.00003655
Iteration 221/1000 | Loss: 0.00003654
Iteration 222/1000 | Loss: 0.00003654
Iteration 223/1000 | Loss: 0.00003654
Iteration 224/1000 | Loss: 0.00003654
Iteration 225/1000 | Loss: 0.00003654
Iteration 226/1000 | Loss: 0.00003653
Iteration 227/1000 | Loss: 0.00003653
Iteration 228/1000 | Loss: 0.00003652
Iteration 229/1000 | Loss: 0.00003652
Iteration 230/1000 | Loss: 0.00003651
Iteration 231/1000 | Loss: 0.00003651
Iteration 232/1000 | Loss: 0.00003650
Iteration 233/1000 | Loss: 0.00003649
Iteration 234/1000 | Loss: 0.00003649
Iteration 235/1000 | Loss: 0.00003648
Iteration 236/1000 | Loss: 0.00003648
Iteration 237/1000 | Loss: 0.00003648
Iteration 238/1000 | Loss: 0.00003647
Iteration 239/1000 | Loss: 0.00003647
Iteration 240/1000 | Loss: 0.00003647
Iteration 241/1000 | Loss: 0.00003647
Iteration 242/1000 | Loss: 0.00003647
Iteration 243/1000 | Loss: 0.00003646
Iteration 244/1000 | Loss: 0.00003646
Iteration 245/1000 | Loss: 0.00003646
Iteration 246/1000 | Loss: 0.00003646
Iteration 247/1000 | Loss: 0.00003646
Iteration 248/1000 | Loss: 0.00003646
Iteration 249/1000 | Loss: 0.00003646
Iteration 250/1000 | Loss: 0.00003646
Iteration 251/1000 | Loss: 0.00003646
Iteration 252/1000 | Loss: 0.00003646
Iteration 253/1000 | Loss: 0.00003645
Iteration 254/1000 | Loss: 0.00003645
Iteration 255/1000 | Loss: 0.00003645
Iteration 256/1000 | Loss: 0.00003645
Iteration 257/1000 | Loss: 0.00003644
Iteration 258/1000 | Loss: 0.00003644
Iteration 259/1000 | Loss: 0.00003644
Iteration 260/1000 | Loss: 0.00003644
Iteration 261/1000 | Loss: 0.00003644
Iteration 262/1000 | Loss: 0.00003644
Iteration 263/1000 | Loss: 0.00003644
Iteration 264/1000 | Loss: 0.00003644
Iteration 265/1000 | Loss: 0.00003644
Iteration 266/1000 | Loss: 0.00003644
Iteration 267/1000 | Loss: 0.00003644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [3.6437359085539356e-05, 3.6437359085539356e-05, 3.6437359085539356e-05, 3.6437359085539356e-05, 3.6437359085539356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6437359085539356e-05

Optimization complete. Final v2v error: 4.655858039855957 mm

Highest mean error: 12.027965545654297 mm for frame 126

Lowest mean error: 3.117050886154175 mm for frame 110

Saving results

Total time: 930.8650333881378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772531
Iteration 2/25 | Loss: 0.00156311
Iteration 3/25 | Loss: 0.00128104
Iteration 4/25 | Loss: 0.00125914
Iteration 5/25 | Loss: 0.00125069
Iteration 6/25 | Loss: 0.00124893
Iteration 7/25 | Loss: 0.00124849
Iteration 8/25 | Loss: 0.00124840
Iteration 9/25 | Loss: 0.00124839
Iteration 10/25 | Loss: 0.00124835
Iteration 11/25 | Loss: 0.00124835
Iteration 12/25 | Loss: 0.00124835
Iteration 13/25 | Loss: 0.00124835
Iteration 14/25 | Loss: 0.00124835
Iteration 15/25 | Loss: 0.00124835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012483487371355295, 0.0012483487371355295, 0.0012483487371355295, 0.0012483487371355295, 0.0012483487371355295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012483487371355295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22930455
Iteration 2/25 | Loss: 0.00122873
Iteration 3/25 | Loss: 0.00122873
Iteration 4/25 | Loss: 0.00122873
Iteration 5/25 | Loss: 0.00122873
Iteration 6/25 | Loss: 0.00122873
Iteration 7/25 | Loss: 0.00122873
Iteration 8/25 | Loss: 0.00122873
Iteration 9/25 | Loss: 0.00122873
Iteration 10/25 | Loss: 0.00122873
Iteration 11/25 | Loss: 0.00122873
Iteration 12/25 | Loss: 0.00122873
Iteration 13/25 | Loss: 0.00122873
Iteration 14/25 | Loss: 0.00122873
Iteration 15/25 | Loss: 0.00122873
Iteration 16/25 | Loss: 0.00122873
Iteration 17/25 | Loss: 0.00122873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012287284480407834, 0.0012287284480407834, 0.0012287284480407834, 0.0012287284480407834, 0.0012287284480407834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012287284480407834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122873
Iteration 2/1000 | Loss: 0.00004151
Iteration 3/1000 | Loss: 0.00002680
Iteration 4/1000 | Loss: 0.00002415
Iteration 5/1000 | Loss: 0.00002272
Iteration 6/1000 | Loss: 0.00002179
Iteration 7/1000 | Loss: 0.00002115
Iteration 8/1000 | Loss: 0.00002079
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002013
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001972
Iteration 13/1000 | Loss: 0.00001953
Iteration 14/1000 | Loss: 0.00001934
Iteration 15/1000 | Loss: 0.00001928
Iteration 16/1000 | Loss: 0.00001923
Iteration 17/1000 | Loss: 0.00001923
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001915
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001904
Iteration 26/1000 | Loss: 0.00001903
Iteration 27/1000 | Loss: 0.00001903
Iteration 28/1000 | Loss: 0.00001903
Iteration 29/1000 | Loss: 0.00001902
Iteration 30/1000 | Loss: 0.00001901
Iteration 31/1000 | Loss: 0.00001900
Iteration 32/1000 | Loss: 0.00001900
Iteration 33/1000 | Loss: 0.00001900
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001899
Iteration 36/1000 | Loss: 0.00001899
Iteration 37/1000 | Loss: 0.00001898
Iteration 38/1000 | Loss: 0.00001898
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001897
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001895
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001893
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001891
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001891
Iteration 62/1000 | Loss: 0.00001891
Iteration 63/1000 | Loss: 0.00001891
Iteration 64/1000 | Loss: 0.00001891
Iteration 65/1000 | Loss: 0.00001890
Iteration 66/1000 | Loss: 0.00001890
Iteration 67/1000 | Loss: 0.00001890
Iteration 68/1000 | Loss: 0.00001890
Iteration 69/1000 | Loss: 0.00001890
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001889
Iteration 80/1000 | Loss: 0.00001889
Iteration 81/1000 | Loss: 0.00001889
Iteration 82/1000 | Loss: 0.00001889
Iteration 83/1000 | Loss: 0.00001889
Iteration 84/1000 | Loss: 0.00001888
Iteration 85/1000 | Loss: 0.00001888
Iteration 86/1000 | Loss: 0.00001888
Iteration 87/1000 | Loss: 0.00001888
Iteration 88/1000 | Loss: 0.00001888
Iteration 89/1000 | Loss: 0.00001888
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001887
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001886
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001886
Iteration 102/1000 | Loss: 0.00001886
Iteration 103/1000 | Loss: 0.00001886
Iteration 104/1000 | Loss: 0.00001886
Iteration 105/1000 | Loss: 0.00001886
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001886
Iteration 108/1000 | Loss: 0.00001886
Iteration 109/1000 | Loss: 0.00001886
Iteration 110/1000 | Loss: 0.00001886
Iteration 111/1000 | Loss: 0.00001885
Iteration 112/1000 | Loss: 0.00001885
Iteration 113/1000 | Loss: 0.00001885
Iteration 114/1000 | Loss: 0.00001885
Iteration 115/1000 | Loss: 0.00001885
Iteration 116/1000 | Loss: 0.00001885
Iteration 117/1000 | Loss: 0.00001885
Iteration 118/1000 | Loss: 0.00001885
Iteration 119/1000 | Loss: 0.00001884
Iteration 120/1000 | Loss: 0.00001884
Iteration 121/1000 | Loss: 0.00001884
Iteration 122/1000 | Loss: 0.00001884
Iteration 123/1000 | Loss: 0.00001884
Iteration 124/1000 | Loss: 0.00001884
Iteration 125/1000 | Loss: 0.00001884
Iteration 126/1000 | Loss: 0.00001884
Iteration 127/1000 | Loss: 0.00001884
Iteration 128/1000 | Loss: 0.00001884
Iteration 129/1000 | Loss: 0.00001884
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001883
Iteration 132/1000 | Loss: 0.00001883
Iteration 133/1000 | Loss: 0.00001883
Iteration 134/1000 | Loss: 0.00001883
Iteration 135/1000 | Loss: 0.00001883
Iteration 136/1000 | Loss: 0.00001883
Iteration 137/1000 | Loss: 0.00001882
Iteration 138/1000 | Loss: 0.00001882
Iteration 139/1000 | Loss: 0.00001882
Iteration 140/1000 | Loss: 0.00001882
Iteration 141/1000 | Loss: 0.00001882
Iteration 142/1000 | Loss: 0.00001882
Iteration 143/1000 | Loss: 0.00001882
Iteration 144/1000 | Loss: 0.00001882
Iteration 145/1000 | Loss: 0.00001882
Iteration 146/1000 | Loss: 0.00001882
Iteration 147/1000 | Loss: 0.00001882
Iteration 148/1000 | Loss: 0.00001882
Iteration 149/1000 | Loss: 0.00001882
Iteration 150/1000 | Loss: 0.00001882
Iteration 151/1000 | Loss: 0.00001882
Iteration 152/1000 | Loss: 0.00001882
Iteration 153/1000 | Loss: 0.00001882
Iteration 154/1000 | Loss: 0.00001882
Iteration 155/1000 | Loss: 0.00001882
Iteration 156/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.881641219370067e-05, 1.881641219370067e-05, 1.881641219370067e-05, 1.881641219370067e-05, 1.881641219370067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.881641219370067e-05

Optimization complete. Final v2v error: 3.6804184913635254 mm

Highest mean error: 4.375686168670654 mm for frame 40

Lowest mean error: 2.8302109241485596 mm for frame 123

Saving results

Total time: 139.58175086975098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825537
Iteration 2/25 | Loss: 0.00143794
Iteration 3/25 | Loss: 0.00128576
Iteration 4/25 | Loss: 0.00126576
Iteration 5/25 | Loss: 0.00126422
Iteration 6/25 | Loss: 0.00126422
Iteration 7/25 | Loss: 0.00126422
Iteration 8/25 | Loss: 0.00126422
Iteration 9/25 | Loss: 0.00126422
Iteration 10/25 | Loss: 0.00126422
Iteration 11/25 | Loss: 0.00126422
Iteration 12/25 | Loss: 0.00126422
Iteration 13/25 | Loss: 0.00126422
Iteration 14/25 | Loss: 0.00126422
Iteration 15/25 | Loss: 0.00126422
Iteration 16/25 | Loss: 0.00126422
Iteration 17/25 | Loss: 0.00126422
Iteration 18/25 | Loss: 0.00126422
Iteration 19/25 | Loss: 0.00126422
Iteration 20/25 | Loss: 0.00126422
Iteration 21/25 | Loss: 0.00126422
Iteration 22/25 | Loss: 0.00126422
Iteration 23/25 | Loss: 0.00126422
Iteration 24/25 | Loss: 0.00126422
Iteration 25/25 | Loss: 0.00126422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25843573
Iteration 2/25 | Loss: 0.00116991
Iteration 3/25 | Loss: 0.00116986
Iteration 4/25 | Loss: 0.00116986
Iteration 5/25 | Loss: 0.00116986
Iteration 6/25 | Loss: 0.00116986
Iteration 7/25 | Loss: 0.00116986
Iteration 8/25 | Loss: 0.00116985
Iteration 9/25 | Loss: 0.00116985
Iteration 10/25 | Loss: 0.00116985
Iteration 11/25 | Loss: 0.00116985
Iteration 12/25 | Loss: 0.00116985
Iteration 13/25 | Loss: 0.00116985
Iteration 14/25 | Loss: 0.00116985
Iteration 15/25 | Loss: 0.00116985
Iteration 16/25 | Loss: 0.00116985
Iteration 17/25 | Loss: 0.00116985
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001169854192994535, 0.001169854192994535, 0.001169854192994535, 0.001169854192994535, 0.001169854192994535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001169854192994535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116985
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002478
Iteration 4/1000 | Loss: 0.00002143
Iteration 5/1000 | Loss: 0.00001979
Iteration 6/1000 | Loss: 0.00001891
Iteration 7/1000 | Loss: 0.00001817
Iteration 8/1000 | Loss: 0.00001781
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001669
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001614
Iteration 16/1000 | Loss: 0.00001595
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001575
Iteration 20/1000 | Loss: 0.00001574
Iteration 21/1000 | Loss: 0.00001574
Iteration 22/1000 | Loss: 0.00001572
Iteration 23/1000 | Loss: 0.00001571
Iteration 24/1000 | Loss: 0.00001569
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001565
Iteration 28/1000 | Loss: 0.00001562
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001555
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001550
Iteration 33/1000 | Loss: 0.00001547
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001544
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001541
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001538
Iteration 44/1000 | Loss: 0.00001538
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001537
Iteration 47/1000 | Loss: 0.00001537
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001526
Iteration 69/1000 | Loss: 0.00001526
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001525
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001522
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001522
Iteration 92/1000 | Loss: 0.00001522
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001521
Iteration 98/1000 | Loss: 0.00001521
Iteration 99/1000 | Loss: 0.00001521
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001521
Iteration 104/1000 | Loss: 0.00001521
Iteration 105/1000 | Loss: 0.00001521
Iteration 106/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.52064640133176e-05, 1.52064640133176e-05, 1.52064640133176e-05, 1.52064640133176e-05, 1.52064640133176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.52064640133176e-05

Optimization complete. Final v2v error: 3.3102591037750244 mm

Highest mean error: 4.12112283706665 mm for frame 0

Lowest mean error: 2.858365535736084 mm for frame 189

Saving results

Total time: 122.04525995254517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991036
Iteration 2/25 | Loss: 0.00991036
Iteration 3/25 | Loss: 0.00279659
Iteration 4/25 | Loss: 0.00219001
Iteration 5/25 | Loss: 0.00191296
Iteration 6/25 | Loss: 0.00176331
Iteration 7/25 | Loss: 0.00179915
Iteration 8/25 | Loss: 0.00174086
Iteration 9/25 | Loss: 0.00168213
Iteration 10/25 | Loss: 0.00161051
Iteration 11/25 | Loss: 0.00159401
Iteration 12/25 | Loss: 0.00155798
Iteration 13/25 | Loss: 0.00152822
Iteration 14/25 | Loss: 0.00152880
Iteration 15/25 | Loss: 0.00149876
Iteration 16/25 | Loss: 0.00148824
Iteration 17/25 | Loss: 0.00147065
Iteration 18/25 | Loss: 0.00146445
Iteration 19/25 | Loss: 0.00146037
Iteration 20/25 | Loss: 0.00145638
Iteration 21/25 | Loss: 0.00145811
Iteration 22/25 | Loss: 0.00145120
Iteration 23/25 | Loss: 0.00144941
Iteration 24/25 | Loss: 0.00144797
Iteration 25/25 | Loss: 0.00145289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28178084
Iteration 2/25 | Loss: 0.00419439
Iteration 3/25 | Loss: 0.00419439
Iteration 4/25 | Loss: 0.00419439
Iteration 5/25 | Loss: 0.00419439
Iteration 6/25 | Loss: 0.00419439
Iteration 7/25 | Loss: 0.00419439
Iteration 8/25 | Loss: 0.00419439
Iteration 9/25 | Loss: 0.00383271
Iteration 10/25 | Loss: 0.00377532
Iteration 11/25 | Loss: 0.00372475
Iteration 12/25 | Loss: 0.00372474
Iteration 13/25 | Loss: 0.00372474
Iteration 14/25 | Loss: 0.00372474
Iteration 15/25 | Loss: 0.00372474
Iteration 16/25 | Loss: 0.00372474
Iteration 17/25 | Loss: 0.00372474
Iteration 18/25 | Loss: 0.00372474
Iteration 19/25 | Loss: 0.00372474
Iteration 20/25 | Loss: 0.00372474
Iteration 21/25 | Loss: 0.00372474
Iteration 22/25 | Loss: 0.00372474
Iteration 23/25 | Loss: 0.00372474
Iteration 24/25 | Loss: 0.00372474
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003724741982296109, 0.003724741982296109, 0.003724741982296109, 0.003724741982296109, 0.003724741982296109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003724741982296109

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00372474
Iteration 2/1000 | Loss: 0.00037755
Iteration 3/1000 | Loss: 0.00050274
Iteration 4/1000 | Loss: 0.00076079
Iteration 5/1000 | Loss: 0.00024265
Iteration 6/1000 | Loss: 0.00020171
Iteration 7/1000 | Loss: 0.00018960
Iteration 8/1000 | Loss: 0.00026597
Iteration 9/1000 | Loss: 0.00029327
Iteration 10/1000 | Loss: 0.00029474
Iteration 11/1000 | Loss: 0.00118247
Iteration 12/1000 | Loss: 0.00121837
Iteration 13/1000 | Loss: 0.00134384
Iteration 14/1000 | Loss: 0.00047294
Iteration 15/1000 | Loss: 0.00015715
Iteration 16/1000 | Loss: 0.00016270
Iteration 17/1000 | Loss: 0.00013147
Iteration 18/1000 | Loss: 0.00016500
Iteration 19/1000 | Loss: 0.00037742
Iteration 20/1000 | Loss: 0.00037380
Iteration 21/1000 | Loss: 0.00028129
Iteration 22/1000 | Loss: 0.00027378
Iteration 23/1000 | Loss: 0.00018575
Iteration 24/1000 | Loss: 0.00012540
Iteration 25/1000 | Loss: 0.00011601
Iteration 26/1000 | Loss: 0.00011818
Iteration 27/1000 | Loss: 0.00049324
Iteration 28/1000 | Loss: 0.00021058
Iteration 29/1000 | Loss: 0.00039294
Iteration 30/1000 | Loss: 0.00011849
Iteration 31/1000 | Loss: 0.00035251
Iteration 32/1000 | Loss: 0.00035087
Iteration 33/1000 | Loss: 0.00013306
Iteration 34/1000 | Loss: 0.00011487
Iteration 35/1000 | Loss: 0.00012890
Iteration 36/1000 | Loss: 0.00010847
Iteration 37/1000 | Loss: 0.00015534
Iteration 38/1000 | Loss: 0.00013830
Iteration 39/1000 | Loss: 0.00012727
Iteration 40/1000 | Loss: 0.00012852
Iteration 41/1000 | Loss: 0.00010498
Iteration 42/1000 | Loss: 0.00020835
Iteration 43/1000 | Loss: 0.00040493
Iteration 44/1000 | Loss: 0.00014066
Iteration 45/1000 | Loss: 0.00018071
Iteration 46/1000 | Loss: 0.00012140
Iteration 47/1000 | Loss: 0.00010537
Iteration 48/1000 | Loss: 0.00014721
Iteration 49/1000 | Loss: 0.00013888
Iteration 50/1000 | Loss: 0.00039657
Iteration 51/1000 | Loss: 0.00033938
Iteration 52/1000 | Loss: 0.00030933
Iteration 53/1000 | Loss: 0.00061469
Iteration 54/1000 | Loss: 0.00030711
Iteration 55/1000 | Loss: 0.00043685
Iteration 56/1000 | Loss: 0.00013416
Iteration 57/1000 | Loss: 0.00015821
Iteration 58/1000 | Loss: 0.00027266
Iteration 59/1000 | Loss: 0.00018383
Iteration 60/1000 | Loss: 0.00022716
Iteration 61/1000 | Loss: 0.00024945
Iteration 62/1000 | Loss: 0.00010677
Iteration 63/1000 | Loss: 0.00010993
Iteration 64/1000 | Loss: 0.00009946
Iteration 65/1000 | Loss: 0.00013994
Iteration 66/1000 | Loss: 0.00009597
Iteration 67/1000 | Loss: 0.00013075
Iteration 68/1000 | Loss: 0.00009424
Iteration 69/1000 | Loss: 0.00009364
Iteration 70/1000 | Loss: 0.00011115
Iteration 71/1000 | Loss: 0.00010240
Iteration 72/1000 | Loss: 0.00009375
Iteration 73/1000 | Loss: 0.00010859
Iteration 74/1000 | Loss: 0.00009428
Iteration 75/1000 | Loss: 0.00010090
Iteration 76/1000 | Loss: 0.00009962
Iteration 77/1000 | Loss: 0.00018694
Iteration 78/1000 | Loss: 0.00078943
Iteration 79/1000 | Loss: 0.00017886
Iteration 80/1000 | Loss: 0.00014321
Iteration 81/1000 | Loss: 0.00013237
Iteration 82/1000 | Loss: 0.00017346
Iteration 83/1000 | Loss: 0.00011980
Iteration 84/1000 | Loss: 0.00012620
Iteration 85/1000 | Loss: 0.00009737
Iteration 86/1000 | Loss: 0.00011379
Iteration 87/1000 | Loss: 0.00020021
Iteration 88/1000 | Loss: 0.00015723
Iteration 89/1000 | Loss: 0.00016334
Iteration 90/1000 | Loss: 0.00009121
Iteration 91/1000 | Loss: 0.00010197
Iteration 92/1000 | Loss: 0.00009755
Iteration 93/1000 | Loss: 0.00008519
Iteration 94/1000 | Loss: 0.00017489
Iteration 95/1000 | Loss: 0.00017604
Iteration 96/1000 | Loss: 0.00011310
Iteration 97/1000 | Loss: 0.00008600
Iteration 98/1000 | Loss: 0.00009374
Iteration 99/1000 | Loss: 0.00008690
Iteration 100/1000 | Loss: 0.00010204
Iteration 101/1000 | Loss: 0.00009568
Iteration 102/1000 | Loss: 0.00008324
Iteration 103/1000 | Loss: 0.00009204
Iteration 104/1000 | Loss: 0.00008164
Iteration 105/1000 | Loss: 0.00008238
Iteration 106/1000 | Loss: 0.00008136
Iteration 107/1000 | Loss: 0.00008294
Iteration 108/1000 | Loss: 0.00008120
Iteration 109/1000 | Loss: 0.00008118
Iteration 110/1000 | Loss: 0.00008533
Iteration 111/1000 | Loss: 0.00020011
Iteration 112/1000 | Loss: 0.00028977
Iteration 113/1000 | Loss: 0.00019743
Iteration 114/1000 | Loss: 0.00071739
Iteration 115/1000 | Loss: 0.00030038
Iteration 116/1000 | Loss: 0.00009208
Iteration 117/1000 | Loss: 0.00010563
Iteration 118/1000 | Loss: 0.00019046
Iteration 119/1000 | Loss: 0.00009974
Iteration 120/1000 | Loss: 0.00008974
Iteration 121/1000 | Loss: 0.00013420
Iteration 122/1000 | Loss: 0.00009062
Iteration 123/1000 | Loss: 0.00008247
Iteration 124/1000 | Loss: 0.00009801
Iteration 125/1000 | Loss: 0.00008149
Iteration 126/1000 | Loss: 0.00009792
Iteration 127/1000 | Loss: 0.00010420
Iteration 128/1000 | Loss: 0.00008023
Iteration 129/1000 | Loss: 0.00008218
Iteration 130/1000 | Loss: 0.00007981
Iteration 131/1000 | Loss: 0.00009412
Iteration 132/1000 | Loss: 0.00007957
Iteration 133/1000 | Loss: 0.00007957
Iteration 134/1000 | Loss: 0.00007954
Iteration 135/1000 | Loss: 0.00007954
Iteration 136/1000 | Loss: 0.00007954
Iteration 137/1000 | Loss: 0.00007953
Iteration 138/1000 | Loss: 0.00007953
Iteration 139/1000 | Loss: 0.00007952
Iteration 140/1000 | Loss: 0.00007952
Iteration 141/1000 | Loss: 0.00007952
Iteration 142/1000 | Loss: 0.00007951
Iteration 143/1000 | Loss: 0.00007950
Iteration 144/1000 | Loss: 0.00007949
Iteration 145/1000 | Loss: 0.00007949
Iteration 146/1000 | Loss: 0.00007949
Iteration 147/1000 | Loss: 0.00007948
Iteration 148/1000 | Loss: 0.00007948
Iteration 149/1000 | Loss: 0.00007947
Iteration 150/1000 | Loss: 0.00007947
Iteration 151/1000 | Loss: 0.00007947
Iteration 152/1000 | Loss: 0.00007947
Iteration 153/1000 | Loss: 0.00007947
Iteration 154/1000 | Loss: 0.00007947
Iteration 155/1000 | Loss: 0.00007947
Iteration 156/1000 | Loss: 0.00007947
Iteration 157/1000 | Loss: 0.00007946
Iteration 158/1000 | Loss: 0.00007944
Iteration 159/1000 | Loss: 0.00007944
Iteration 160/1000 | Loss: 0.00007944
Iteration 161/1000 | Loss: 0.00007944
Iteration 162/1000 | Loss: 0.00007944
Iteration 163/1000 | Loss: 0.00007944
Iteration 164/1000 | Loss: 0.00007944
Iteration 165/1000 | Loss: 0.00007944
Iteration 166/1000 | Loss: 0.00007944
Iteration 167/1000 | Loss: 0.00007944
Iteration 168/1000 | Loss: 0.00007943
Iteration 169/1000 | Loss: 0.00007943
Iteration 170/1000 | Loss: 0.00007943
Iteration 171/1000 | Loss: 0.00007942
Iteration 172/1000 | Loss: 0.00007942
Iteration 173/1000 | Loss: 0.00008717
Iteration 174/1000 | Loss: 0.00007938
Iteration 175/1000 | Loss: 0.00007938
Iteration 176/1000 | Loss: 0.00007938
Iteration 177/1000 | Loss: 0.00007938
Iteration 178/1000 | Loss: 0.00007938
Iteration 179/1000 | Loss: 0.00007937
Iteration 180/1000 | Loss: 0.00007937
Iteration 181/1000 | Loss: 0.00007937
Iteration 182/1000 | Loss: 0.00007937
Iteration 183/1000 | Loss: 0.00007936
Iteration 184/1000 | Loss: 0.00007936
Iteration 185/1000 | Loss: 0.00007935
Iteration 186/1000 | Loss: 0.00007935
Iteration 187/1000 | Loss: 0.00007935
Iteration 188/1000 | Loss: 0.00007935
Iteration 189/1000 | Loss: 0.00007934
Iteration 190/1000 | Loss: 0.00007934
Iteration 191/1000 | Loss: 0.00007934
Iteration 192/1000 | Loss: 0.00007934
Iteration 193/1000 | Loss: 0.00007934
Iteration 194/1000 | Loss: 0.00007934
Iteration 195/1000 | Loss: 0.00007934
Iteration 196/1000 | Loss: 0.00007933
Iteration 197/1000 | Loss: 0.00007932
Iteration 198/1000 | Loss: 0.00007932
Iteration 199/1000 | Loss: 0.00007931
Iteration 200/1000 | Loss: 0.00007931
Iteration 201/1000 | Loss: 0.00007931
Iteration 202/1000 | Loss: 0.00007931
Iteration 203/1000 | Loss: 0.00007931
Iteration 204/1000 | Loss: 0.00007931
Iteration 205/1000 | Loss: 0.00007931
Iteration 206/1000 | Loss: 0.00007931
Iteration 207/1000 | Loss: 0.00007931
Iteration 208/1000 | Loss: 0.00007931
Iteration 209/1000 | Loss: 0.00007931
Iteration 210/1000 | Loss: 0.00007931
Iteration 211/1000 | Loss: 0.00007930
Iteration 212/1000 | Loss: 0.00007930
Iteration 213/1000 | Loss: 0.00007929
Iteration 214/1000 | Loss: 0.00007929
Iteration 215/1000 | Loss: 0.00007928
Iteration 216/1000 | Loss: 0.00008865
Iteration 217/1000 | Loss: 0.00008629
Iteration 218/1000 | Loss: 0.00007935
Iteration 219/1000 | Loss: 0.00007931
Iteration 220/1000 | Loss: 0.00007928
Iteration 221/1000 | Loss: 0.00007927
Iteration 222/1000 | Loss: 0.00007927
Iteration 223/1000 | Loss: 0.00007927
Iteration 224/1000 | Loss: 0.00007927
Iteration 225/1000 | Loss: 0.00007927
Iteration 226/1000 | Loss: 0.00007927
Iteration 227/1000 | Loss: 0.00007927
Iteration 228/1000 | Loss: 0.00007926
Iteration 229/1000 | Loss: 0.00007926
Iteration 230/1000 | Loss: 0.00007926
Iteration 231/1000 | Loss: 0.00007926
Iteration 232/1000 | Loss: 0.00007926
Iteration 233/1000 | Loss: 0.00007925
Iteration 234/1000 | Loss: 0.00008754
Iteration 235/1000 | Loss: 0.00007956
Iteration 236/1000 | Loss: 0.00007924
Iteration 237/1000 | Loss: 0.00007924
Iteration 238/1000 | Loss: 0.00007924
Iteration 239/1000 | Loss: 0.00007923
Iteration 240/1000 | Loss: 0.00007923
Iteration 241/1000 | Loss: 0.00008040
Iteration 242/1000 | Loss: 0.00008040
Iteration 243/1000 | Loss: 0.00007923
Iteration 244/1000 | Loss: 0.00007922
Iteration 245/1000 | Loss: 0.00007922
Iteration 246/1000 | Loss: 0.00007922
Iteration 247/1000 | Loss: 0.00007922
Iteration 248/1000 | Loss: 0.00007922
Iteration 249/1000 | Loss: 0.00007921
Iteration 250/1000 | Loss: 0.00007921
Iteration 251/1000 | Loss: 0.00007921
Iteration 252/1000 | Loss: 0.00007921
Iteration 253/1000 | Loss: 0.00007921
Iteration 254/1000 | Loss: 0.00007921
Iteration 255/1000 | Loss: 0.00007921
Iteration 256/1000 | Loss: 0.00007921
Iteration 257/1000 | Loss: 0.00007921
Iteration 258/1000 | Loss: 0.00007921
Iteration 259/1000 | Loss: 0.00007921
Iteration 260/1000 | Loss: 0.00007921
Iteration 261/1000 | Loss: 0.00007921
Iteration 262/1000 | Loss: 0.00007921
Iteration 263/1000 | Loss: 0.00007921
Iteration 264/1000 | Loss: 0.00007921
Iteration 265/1000 | Loss: 0.00007921
Iteration 266/1000 | Loss: 0.00007921
Iteration 267/1000 | Loss: 0.00007921
Iteration 268/1000 | Loss: 0.00007921
Iteration 269/1000 | Loss: 0.00007921
Iteration 270/1000 | Loss: 0.00007921
Iteration 271/1000 | Loss: 0.00007921
Iteration 272/1000 | Loss: 0.00007921
Iteration 273/1000 | Loss: 0.00007921
Iteration 274/1000 | Loss: 0.00007921
Iteration 275/1000 | Loss: 0.00007921
Iteration 276/1000 | Loss: 0.00007921
Iteration 277/1000 | Loss: 0.00007921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [7.92059872765094e-05, 7.92059872765094e-05, 7.92059872765094e-05, 7.92059872765094e-05, 7.92059872765094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.92059872765094e-05

Optimization complete. Final v2v error: 4.474074840545654 mm

Highest mean error: 11.906890869140625 mm for frame 185

Lowest mean error: 2.4673755168914795 mm for frame 167

Saving results

Total time: 748.0431892871857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00929529
Iteration 2/25 | Loss: 0.00157549
Iteration 3/25 | Loss: 0.00136410
Iteration 4/25 | Loss: 0.00133128
Iteration 5/25 | Loss: 0.00132062
Iteration 6/25 | Loss: 0.00131078
Iteration 7/25 | Loss: 0.00130775
Iteration 8/25 | Loss: 0.00130597
Iteration 9/25 | Loss: 0.00130510
Iteration 10/25 | Loss: 0.00130431
Iteration 11/25 | Loss: 0.00130406
Iteration 12/25 | Loss: 0.00130402
Iteration 13/25 | Loss: 0.00130402
Iteration 14/25 | Loss: 0.00130402
Iteration 15/25 | Loss: 0.00130399
Iteration 16/25 | Loss: 0.00130399
Iteration 17/25 | Loss: 0.00130399
Iteration 18/25 | Loss: 0.00130399
Iteration 19/25 | Loss: 0.00130398
Iteration 20/25 | Loss: 0.00130398
Iteration 21/25 | Loss: 0.00130398
Iteration 22/25 | Loss: 0.00130398
Iteration 23/25 | Loss: 0.00130398
Iteration 24/25 | Loss: 0.00130398
Iteration 25/25 | Loss: 0.00130398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03311872
Iteration 2/25 | Loss: 0.00136699
Iteration 3/25 | Loss: 0.00136693
Iteration 4/25 | Loss: 0.00136693
Iteration 5/25 | Loss: 0.00136693
Iteration 6/25 | Loss: 0.00136693
Iteration 7/25 | Loss: 0.00136693
Iteration 8/25 | Loss: 0.00136693
Iteration 9/25 | Loss: 0.00136693
Iteration 10/25 | Loss: 0.00136693
Iteration 11/25 | Loss: 0.00136693
Iteration 12/25 | Loss: 0.00136693
Iteration 13/25 | Loss: 0.00136693
Iteration 14/25 | Loss: 0.00136693
Iteration 15/25 | Loss: 0.00136693
Iteration 16/25 | Loss: 0.00136693
Iteration 17/25 | Loss: 0.00136693
Iteration 18/25 | Loss: 0.00136693
Iteration 19/25 | Loss: 0.00136693
Iteration 20/25 | Loss: 0.00136693
Iteration 21/25 | Loss: 0.00136693
Iteration 22/25 | Loss: 0.00136693
Iteration 23/25 | Loss: 0.00136693
Iteration 24/25 | Loss: 0.00136693
Iteration 25/25 | Loss: 0.00136693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136693
Iteration 2/1000 | Loss: 0.00004587
Iteration 3/1000 | Loss: 0.00002987
Iteration 4/1000 | Loss: 0.00002691
Iteration 5/1000 | Loss: 0.00002560
Iteration 6/1000 | Loss: 0.00002497
Iteration 7/1000 | Loss: 0.00002452
Iteration 8/1000 | Loss: 0.00002428
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002372
Iteration 11/1000 | Loss: 0.00002357
Iteration 12/1000 | Loss: 0.00002342
Iteration 13/1000 | Loss: 0.00002327
Iteration 14/1000 | Loss: 0.00002320
Iteration 15/1000 | Loss: 0.00002319
Iteration 16/1000 | Loss: 0.00002317
Iteration 17/1000 | Loss: 0.00002317
Iteration 18/1000 | Loss: 0.00002317
Iteration 19/1000 | Loss: 0.00002317
Iteration 20/1000 | Loss: 0.00002317
Iteration 21/1000 | Loss: 0.00002317
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002317
Iteration 24/1000 | Loss: 0.00002317
Iteration 25/1000 | Loss: 0.00002317
Iteration 26/1000 | Loss: 0.00002317
Iteration 27/1000 | Loss: 0.00002315
Iteration 28/1000 | Loss: 0.00002313
Iteration 29/1000 | Loss: 0.00002313
Iteration 30/1000 | Loss: 0.00002313
Iteration 31/1000 | Loss: 0.00002313
Iteration 32/1000 | Loss: 0.00002313
Iteration 33/1000 | Loss: 0.00002313
Iteration 34/1000 | Loss: 0.00002313
Iteration 35/1000 | Loss: 0.00002313
Iteration 36/1000 | Loss: 0.00002313
Iteration 37/1000 | Loss: 0.00002312
Iteration 38/1000 | Loss: 0.00002312
Iteration 39/1000 | Loss: 0.00002312
Iteration 40/1000 | Loss: 0.00002312
Iteration 41/1000 | Loss: 0.00002309
Iteration 42/1000 | Loss: 0.00002308
Iteration 43/1000 | Loss: 0.00002308
Iteration 44/1000 | Loss: 0.00002308
Iteration 45/1000 | Loss: 0.00002307
Iteration 46/1000 | Loss: 0.00002307
Iteration 47/1000 | Loss: 0.00002307
Iteration 48/1000 | Loss: 0.00002307
Iteration 49/1000 | Loss: 0.00002307
Iteration 50/1000 | Loss: 0.00002307
Iteration 51/1000 | Loss: 0.00002307
Iteration 52/1000 | Loss: 0.00002307
Iteration 53/1000 | Loss: 0.00002307
Iteration 54/1000 | Loss: 0.00002307
Iteration 55/1000 | Loss: 0.00002307
Iteration 56/1000 | Loss: 0.00002306
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002304
Iteration 59/1000 | Loss: 0.00002304
Iteration 60/1000 | Loss: 0.00002304
Iteration 61/1000 | Loss: 0.00002304
Iteration 62/1000 | Loss: 0.00002303
Iteration 63/1000 | Loss: 0.00002303
Iteration 64/1000 | Loss: 0.00002302
Iteration 65/1000 | Loss: 0.00002302
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002300
Iteration 70/1000 | Loss: 0.00002300
Iteration 71/1000 | Loss: 0.00002299
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002299
Iteration 74/1000 | Loss: 0.00002298
Iteration 75/1000 | Loss: 0.00002298
Iteration 76/1000 | Loss: 0.00002298
Iteration 77/1000 | Loss: 0.00002298
Iteration 78/1000 | Loss: 0.00002297
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002297
Iteration 81/1000 | Loss: 0.00002297
Iteration 82/1000 | Loss: 0.00002297
Iteration 83/1000 | Loss: 0.00002297
Iteration 84/1000 | Loss: 0.00002297
Iteration 85/1000 | Loss: 0.00002297
Iteration 86/1000 | Loss: 0.00002297
Iteration 87/1000 | Loss: 0.00002297
Iteration 88/1000 | Loss: 0.00002297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.2971706130192615e-05, 2.2971706130192615e-05, 2.2971706130192615e-05, 2.2971706130192615e-05, 2.2971706130192615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2971706130192615e-05

Optimization complete. Final v2v error: 3.7519619464874268 mm

Highest mean error: 4.38315486907959 mm for frame 107

Lowest mean error: 3.2457261085510254 mm for frame 147

Saving results

Total time: 139.18479371070862
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425461
Iteration 2/25 | Loss: 0.00126059
Iteration 3/25 | Loss: 0.00118022
Iteration 4/25 | Loss: 0.00116987
Iteration 5/25 | Loss: 0.00116692
Iteration 6/25 | Loss: 0.00116670
Iteration 7/25 | Loss: 0.00116670
Iteration 8/25 | Loss: 0.00116670
Iteration 9/25 | Loss: 0.00116670
Iteration 10/25 | Loss: 0.00116671
Iteration 11/25 | Loss: 0.00116670
Iteration 12/25 | Loss: 0.00116670
Iteration 13/25 | Loss: 0.00116670
Iteration 14/25 | Loss: 0.00116670
Iteration 15/25 | Loss: 0.00116670
Iteration 16/25 | Loss: 0.00116670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011667049257084727, 0.0011667049257084727, 0.0011667049257084727, 0.0011667049257084727, 0.0011667049257084727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011667049257084727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04106343
Iteration 2/25 | Loss: 0.00109113
Iteration 3/25 | Loss: 0.00109113
Iteration 4/25 | Loss: 0.00109113
Iteration 5/25 | Loss: 0.00109113
Iteration 6/25 | Loss: 0.00109113
Iteration 7/25 | Loss: 0.00109113
Iteration 8/25 | Loss: 0.00109112
Iteration 9/25 | Loss: 0.00109112
Iteration 10/25 | Loss: 0.00109112
Iteration 11/25 | Loss: 0.00109112
Iteration 12/25 | Loss: 0.00109112
Iteration 13/25 | Loss: 0.00109112
Iteration 14/25 | Loss: 0.00109112
Iteration 15/25 | Loss: 0.00109112
Iteration 16/25 | Loss: 0.00109112
Iteration 17/25 | Loss: 0.00109112
Iteration 18/25 | Loss: 0.00109112
Iteration 19/25 | Loss: 0.00109112
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010911236749961972, 0.0010911236749961972, 0.0010911236749961972, 0.0010911236749961972, 0.0010911236749961972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010911236749961972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109112
Iteration 2/1000 | Loss: 0.00003515
Iteration 3/1000 | Loss: 0.00002120
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001582
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001447
Iteration 8/1000 | Loss: 0.00001390
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001317
Iteration 11/1000 | Loss: 0.00001310
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001225
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001212
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001211
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001211
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001211
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001204
Iteration 50/1000 | Loss: 0.00001204
Iteration 51/1000 | Loss: 0.00001204
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001204
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001203
Iteration 57/1000 | Loss: 0.00001203
Iteration 58/1000 | Loss: 0.00001203
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001202
Iteration 61/1000 | Loss: 0.00001200
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001199
Iteration 64/1000 | Loss: 0.00001199
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001195
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001194
Iteration 78/1000 | Loss: 0.00001194
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001194
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001194
Iteration 90/1000 | Loss: 0.00001194
Iteration 91/1000 | Loss: 0.00001194
Iteration 92/1000 | Loss: 0.00001194
Iteration 93/1000 | Loss: 0.00001194
Iteration 94/1000 | Loss: 0.00001194
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001193
Iteration 98/1000 | Loss: 0.00001193
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001193
Iteration 103/1000 | Loss: 0.00001193
Iteration 104/1000 | Loss: 0.00001193
Iteration 105/1000 | Loss: 0.00001193
Iteration 106/1000 | Loss: 0.00001193
Iteration 107/1000 | Loss: 0.00001193
Iteration 108/1000 | Loss: 0.00001193
Iteration 109/1000 | Loss: 0.00001193
Iteration 110/1000 | Loss: 0.00001193
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001193
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001193
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001193
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.1927388186450116e-05, 1.1927388186450116e-05, 1.1927388186450116e-05, 1.1927388186450116e-05, 1.1927388186450116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1927388186450116e-05

Optimization complete. Final v2v error: 2.995762586593628 mm

Highest mean error: 3.1086604595184326 mm for frame 39

Lowest mean error: 2.968719959259033 mm for frame 22

Saving results

Total time: 76.7532000541687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715924
Iteration 2/25 | Loss: 0.00132510
Iteration 3/25 | Loss: 0.00122609
Iteration 4/25 | Loss: 0.00119608
Iteration 5/25 | Loss: 0.00118856
Iteration 6/25 | Loss: 0.00118597
Iteration 7/25 | Loss: 0.00118544
Iteration 8/25 | Loss: 0.00118523
Iteration 9/25 | Loss: 0.00118521
Iteration 10/25 | Loss: 0.00118521
Iteration 11/25 | Loss: 0.00118521
Iteration 12/25 | Loss: 0.00118521
Iteration 13/25 | Loss: 0.00118521
Iteration 14/25 | Loss: 0.00118521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011852069292217493, 0.0011852069292217493, 0.0011852069292217493, 0.0011852069292217493, 0.0011852069292217493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011852069292217493

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86789882
Iteration 2/25 | Loss: 0.00144196
Iteration 3/25 | Loss: 0.00144196
Iteration 4/25 | Loss: 0.00144196
Iteration 5/25 | Loss: 0.00144196
Iteration 6/25 | Loss: 0.00144196
Iteration 7/25 | Loss: 0.00144196
Iteration 8/25 | Loss: 0.00144196
Iteration 9/25 | Loss: 0.00144196
Iteration 10/25 | Loss: 0.00144196
Iteration 11/25 | Loss: 0.00144196
Iteration 12/25 | Loss: 0.00144196
Iteration 13/25 | Loss: 0.00144196
Iteration 14/25 | Loss: 0.00144196
Iteration 15/25 | Loss: 0.00144196
Iteration 16/25 | Loss: 0.00144195
Iteration 17/25 | Loss: 0.00144195
Iteration 18/25 | Loss: 0.00144195
Iteration 19/25 | Loss: 0.00144195
Iteration 20/25 | Loss: 0.00144195
Iteration 21/25 | Loss: 0.00144195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014419549843296409, 0.0014419549843296409, 0.0014419549843296409, 0.0014419549843296409, 0.0014419549843296409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014419549843296409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144195
Iteration 2/1000 | Loss: 0.00001710
Iteration 3/1000 | Loss: 0.00001405
Iteration 4/1000 | Loss: 0.00001304
Iteration 5/1000 | Loss: 0.00006944
Iteration 6/1000 | Loss: 0.00001222
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001152
Iteration 9/1000 | Loss: 0.00001127
Iteration 10/1000 | Loss: 0.00001110
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001088
Iteration 13/1000 | Loss: 0.00001088
Iteration 14/1000 | Loss: 0.00007544
Iteration 15/1000 | Loss: 0.00001092
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001058
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001050
Iteration 23/1000 | Loss: 0.00001048
Iteration 24/1000 | Loss: 0.00001047
Iteration 25/1000 | Loss: 0.00001046
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001044
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00007266
Iteration 37/1000 | Loss: 0.00002091
Iteration 38/1000 | Loss: 0.00001040
Iteration 39/1000 | Loss: 0.00002277
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001035
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00002002
Iteration 46/1000 | Loss: 0.00001035
Iteration 47/1000 | Loss: 0.00001034
Iteration 48/1000 | Loss: 0.00001031
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001030
Iteration 52/1000 | Loss: 0.00001030
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00003727
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001028
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001020
Iteration 67/1000 | Loss: 0.00001019
Iteration 68/1000 | Loss: 0.00001018
Iteration 69/1000 | Loss: 0.00001018
Iteration 70/1000 | Loss: 0.00001018
Iteration 71/1000 | Loss: 0.00001018
Iteration 72/1000 | Loss: 0.00001018
Iteration 73/1000 | Loss: 0.00001018
Iteration 74/1000 | Loss: 0.00001017
Iteration 75/1000 | Loss: 0.00001017
Iteration 76/1000 | Loss: 0.00001017
Iteration 77/1000 | Loss: 0.00001017
Iteration 78/1000 | Loss: 0.00001017
Iteration 79/1000 | Loss: 0.00001017
Iteration 80/1000 | Loss: 0.00001017
Iteration 81/1000 | Loss: 0.00001017
Iteration 82/1000 | Loss: 0.00001017
Iteration 83/1000 | Loss: 0.00001017
Iteration 84/1000 | Loss: 0.00001017
Iteration 85/1000 | Loss: 0.00001017
Iteration 86/1000 | Loss: 0.00001017
Iteration 87/1000 | Loss: 0.00001017
Iteration 88/1000 | Loss: 0.00001017
Iteration 89/1000 | Loss: 0.00001017
Iteration 90/1000 | Loss: 0.00001017
Iteration 91/1000 | Loss: 0.00001017
Iteration 92/1000 | Loss: 0.00001017
Iteration 93/1000 | Loss: 0.00001017
Iteration 94/1000 | Loss: 0.00001017
Iteration 95/1000 | Loss: 0.00001017
Iteration 96/1000 | Loss: 0.00001017
Iteration 97/1000 | Loss: 0.00001017
Iteration 98/1000 | Loss: 0.00001017
Iteration 99/1000 | Loss: 0.00001017
Iteration 100/1000 | Loss: 0.00001017
Iteration 101/1000 | Loss: 0.00001017
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.0172323527513072e-05, 1.0172323527513072e-05, 1.0172323527513072e-05, 1.0172323527513072e-05, 1.0172323527513072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0172323527513072e-05

Optimization complete. Final v2v error: 2.7749693393707275 mm

Highest mean error: 3.392165184020996 mm for frame 149

Lowest mean error: 2.6206626892089844 mm for frame 29

Saving results

Total time: 153.2419376373291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416660
Iteration 2/25 | Loss: 0.00134347
Iteration 3/25 | Loss: 0.00123121
Iteration 4/25 | Loss: 0.00121745
Iteration 5/25 | Loss: 0.00121337
Iteration 6/25 | Loss: 0.00121219
Iteration 7/25 | Loss: 0.00121203
Iteration 8/25 | Loss: 0.00121203
Iteration 9/25 | Loss: 0.00121203
Iteration 10/25 | Loss: 0.00121203
Iteration 11/25 | Loss: 0.00121203
Iteration 12/25 | Loss: 0.00121203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012120342580601573, 0.0012120342580601573, 0.0012120342580601573, 0.0012120342580601573, 0.0012120342580601573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012120342580601573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44262409
Iteration 2/25 | Loss: 0.00133575
Iteration 3/25 | Loss: 0.00133574
Iteration 4/25 | Loss: 0.00133574
Iteration 5/25 | Loss: 0.00133574
Iteration 6/25 | Loss: 0.00133574
Iteration 7/25 | Loss: 0.00133574
Iteration 8/25 | Loss: 0.00133574
Iteration 9/25 | Loss: 0.00133574
Iteration 10/25 | Loss: 0.00133574
Iteration 11/25 | Loss: 0.00133574
Iteration 12/25 | Loss: 0.00133574
Iteration 13/25 | Loss: 0.00133574
Iteration 14/25 | Loss: 0.00133574
Iteration 15/25 | Loss: 0.00133574
Iteration 16/25 | Loss: 0.00133574
Iteration 17/25 | Loss: 0.00133574
Iteration 18/25 | Loss: 0.00133574
Iteration 19/25 | Loss: 0.00133574
Iteration 20/25 | Loss: 0.00133574
Iteration 21/25 | Loss: 0.00133574
Iteration 22/25 | Loss: 0.00133574
Iteration 23/25 | Loss: 0.00133574
Iteration 24/25 | Loss: 0.00133574
Iteration 25/25 | Loss: 0.00133574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133574
Iteration 2/1000 | Loss: 0.00003780
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001347
Iteration 11/1000 | Loss: 0.00001328
Iteration 12/1000 | Loss: 0.00001309
Iteration 13/1000 | Loss: 0.00001294
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001290
Iteration 16/1000 | Loss: 0.00001287
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001281
Iteration 20/1000 | Loss: 0.00001275
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001270
Iteration 24/1000 | Loss: 0.00001270
Iteration 25/1000 | Loss: 0.00001270
Iteration 26/1000 | Loss: 0.00001269
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001260
Iteration 30/1000 | Loss: 0.00001260
Iteration 31/1000 | Loss: 0.00001260
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001258
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001249
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001249
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001245
Iteration 59/1000 | Loss: 0.00001245
Iteration 60/1000 | Loss: 0.00001245
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001241
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001241
Iteration 73/1000 | Loss: 0.00001240
Iteration 74/1000 | Loss: 0.00001240
Iteration 75/1000 | Loss: 0.00001240
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001238
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001237
Iteration 89/1000 | Loss: 0.00001237
Iteration 90/1000 | Loss: 0.00001237
Iteration 91/1000 | Loss: 0.00001237
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001235
Iteration 99/1000 | Loss: 0.00001235
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001231
Iteration 121/1000 | Loss: 0.00001231
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001230
Iteration 127/1000 | Loss: 0.00001230
Iteration 128/1000 | Loss: 0.00001230
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Iteration 138/1000 | Loss: 0.00001229
Iteration 139/1000 | Loss: 0.00001229
Iteration 140/1000 | Loss: 0.00001229
Iteration 141/1000 | Loss: 0.00001229
Iteration 142/1000 | Loss: 0.00001229
Iteration 143/1000 | Loss: 0.00001228
Iteration 144/1000 | Loss: 0.00001228
Iteration 145/1000 | Loss: 0.00001228
Iteration 146/1000 | Loss: 0.00001228
Iteration 147/1000 | Loss: 0.00001228
Iteration 148/1000 | Loss: 0.00001228
Iteration 149/1000 | Loss: 0.00001228
Iteration 150/1000 | Loss: 0.00001228
Iteration 151/1000 | Loss: 0.00001228
Iteration 152/1000 | Loss: 0.00001228
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001226
Iteration 158/1000 | Loss: 0.00001226
Iteration 159/1000 | Loss: 0.00001226
Iteration 160/1000 | Loss: 0.00001226
Iteration 161/1000 | Loss: 0.00001226
Iteration 162/1000 | Loss: 0.00001226
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001226
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Iteration 172/1000 | Loss: 0.00001225
Iteration 173/1000 | Loss: 0.00001225
Iteration 174/1000 | Loss: 0.00001225
Iteration 175/1000 | Loss: 0.00001225
Iteration 176/1000 | Loss: 0.00001225
Iteration 177/1000 | Loss: 0.00001225
Iteration 178/1000 | Loss: 0.00001225
Iteration 179/1000 | Loss: 0.00001224
Iteration 180/1000 | Loss: 0.00001224
Iteration 181/1000 | Loss: 0.00001224
Iteration 182/1000 | Loss: 0.00001224
Iteration 183/1000 | Loss: 0.00001224
Iteration 184/1000 | Loss: 0.00001224
Iteration 185/1000 | Loss: 0.00001224
Iteration 186/1000 | Loss: 0.00001224
Iteration 187/1000 | Loss: 0.00001224
Iteration 188/1000 | Loss: 0.00001224
Iteration 189/1000 | Loss: 0.00001224
Iteration 190/1000 | Loss: 0.00001224
Iteration 191/1000 | Loss: 0.00001224
Iteration 192/1000 | Loss: 0.00001224
Iteration 193/1000 | Loss: 0.00001224
Iteration 194/1000 | Loss: 0.00001224
Iteration 195/1000 | Loss: 0.00001224
Iteration 196/1000 | Loss: 0.00001224
Iteration 197/1000 | Loss: 0.00001224
Iteration 198/1000 | Loss: 0.00001223
Iteration 199/1000 | Loss: 0.00001223
Iteration 200/1000 | Loss: 0.00001223
Iteration 201/1000 | Loss: 0.00001223
Iteration 202/1000 | Loss: 0.00001223
Iteration 203/1000 | Loss: 0.00001223
Iteration 204/1000 | Loss: 0.00001223
Iteration 205/1000 | Loss: 0.00001223
Iteration 206/1000 | Loss: 0.00001223
Iteration 207/1000 | Loss: 0.00001223
Iteration 208/1000 | Loss: 0.00001223
Iteration 209/1000 | Loss: 0.00001223
Iteration 210/1000 | Loss: 0.00001223
Iteration 211/1000 | Loss: 0.00001223
Iteration 212/1000 | Loss: 0.00001223
Iteration 213/1000 | Loss: 0.00001223
Iteration 214/1000 | Loss: 0.00001223
Iteration 215/1000 | Loss: 0.00001223
Iteration 216/1000 | Loss: 0.00001223
Iteration 217/1000 | Loss: 0.00001223
Iteration 218/1000 | Loss: 0.00001223
Iteration 219/1000 | Loss: 0.00001223
Iteration 220/1000 | Loss: 0.00001222
Iteration 221/1000 | Loss: 0.00001222
Iteration 222/1000 | Loss: 0.00001222
Iteration 223/1000 | Loss: 0.00001222
Iteration 224/1000 | Loss: 0.00001222
Iteration 225/1000 | Loss: 0.00001222
Iteration 226/1000 | Loss: 0.00001222
Iteration 227/1000 | Loss: 0.00001222
Iteration 228/1000 | Loss: 0.00001222
Iteration 229/1000 | Loss: 0.00001222
Iteration 230/1000 | Loss: 0.00001222
Iteration 231/1000 | Loss: 0.00001222
Iteration 232/1000 | Loss: 0.00001222
Iteration 233/1000 | Loss: 0.00001222
Iteration 234/1000 | Loss: 0.00001222
Iteration 235/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2220008102303836e-05, 1.2220008102303836e-05, 1.2220008102303836e-05, 1.2220008102303836e-05, 1.2220008102303836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2220008102303836e-05

Optimization complete. Final v2v error: 2.9616992473602295 mm

Highest mean error: 4.091293811798096 mm for frame 39

Lowest mean error: 2.60374116897583 mm for frame 17

Saving results

Total time: 101.59130549430847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00449478
Iteration 2/25 | Loss: 0.00129193
Iteration 3/25 | Loss: 0.00120494
Iteration 4/25 | Loss: 0.00119722
Iteration 5/25 | Loss: 0.00119570
Iteration 6/25 | Loss: 0.00119570
Iteration 7/25 | Loss: 0.00119570
Iteration 8/25 | Loss: 0.00119570
Iteration 9/25 | Loss: 0.00119570
Iteration 10/25 | Loss: 0.00119570
Iteration 11/25 | Loss: 0.00119570
Iteration 12/25 | Loss: 0.00119570
Iteration 13/25 | Loss: 0.00119570
Iteration 14/25 | Loss: 0.00119570
Iteration 15/25 | Loss: 0.00119570
Iteration 16/25 | Loss: 0.00119570
Iteration 17/25 | Loss: 0.00119570
Iteration 18/25 | Loss: 0.00119570
Iteration 19/25 | Loss: 0.00119570
Iteration 20/25 | Loss: 0.00119570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001195699442178011, 0.001195699442178011, 0.001195699442178011, 0.001195699442178011, 0.001195699442178011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195699442178011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20039749
Iteration 2/25 | Loss: 0.00134383
Iteration 3/25 | Loss: 0.00134382
Iteration 4/25 | Loss: 0.00134382
Iteration 5/25 | Loss: 0.00134382
Iteration 6/25 | Loss: 0.00134382
Iteration 7/25 | Loss: 0.00134382
Iteration 8/25 | Loss: 0.00134382
Iteration 9/25 | Loss: 0.00134382
Iteration 10/25 | Loss: 0.00134382
Iteration 11/25 | Loss: 0.00134382
Iteration 12/25 | Loss: 0.00134382
Iteration 13/25 | Loss: 0.00134382
Iteration 14/25 | Loss: 0.00134382
Iteration 15/25 | Loss: 0.00134382
Iteration 16/25 | Loss: 0.00134382
Iteration 17/25 | Loss: 0.00134382
Iteration 18/25 | Loss: 0.00134382
Iteration 19/25 | Loss: 0.00134382
Iteration 20/25 | Loss: 0.00134382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013438168680295348, 0.0013438168680295348, 0.0013438168680295348, 0.0013438168680295348, 0.0013438168680295348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013438168680295348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134382
Iteration 2/1000 | Loss: 0.00001849
Iteration 3/1000 | Loss: 0.00001466
Iteration 4/1000 | Loss: 0.00001377
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001277
Iteration 7/1000 | Loss: 0.00001241
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001143
Iteration 12/1000 | Loss: 0.00001137
Iteration 13/1000 | Loss: 0.00001137
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001119
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001093
Iteration 21/1000 | Loss: 0.00001086
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001081
Iteration 25/1000 | Loss: 0.00001081
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001081
Iteration 29/1000 | Loss: 0.00001081
Iteration 30/1000 | Loss: 0.00001081
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001081
Iteration 34/1000 | Loss: 0.00001081
Iteration 35/1000 | Loss: 0.00001081
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001081
Iteration 39/1000 | Loss: 0.00001080
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001078
Iteration 43/1000 | Loss: 0.00001078
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001078
Iteration 48/1000 | Loss: 0.00001078
Iteration 49/1000 | Loss: 0.00001078
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001076
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001073
Iteration 56/1000 | Loss: 0.00001073
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001069
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001067
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001066
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001064
Iteration 90/1000 | Loss: 0.00001064
Iteration 91/1000 | Loss: 0.00001064
Iteration 92/1000 | Loss: 0.00001064
Iteration 93/1000 | Loss: 0.00001064
Iteration 94/1000 | Loss: 0.00001064
Iteration 95/1000 | Loss: 0.00001064
Iteration 96/1000 | Loss: 0.00001064
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001063
Iteration 101/1000 | Loss: 0.00001063
Iteration 102/1000 | Loss: 0.00001063
Iteration 103/1000 | Loss: 0.00001063
Iteration 104/1000 | Loss: 0.00001063
Iteration 105/1000 | Loss: 0.00001062
Iteration 106/1000 | Loss: 0.00001061
Iteration 107/1000 | Loss: 0.00001061
Iteration 108/1000 | Loss: 0.00001061
Iteration 109/1000 | Loss: 0.00001060
Iteration 110/1000 | Loss: 0.00001060
Iteration 111/1000 | Loss: 0.00001060
Iteration 112/1000 | Loss: 0.00001060
Iteration 113/1000 | Loss: 0.00001060
Iteration 114/1000 | Loss: 0.00001060
Iteration 115/1000 | Loss: 0.00001060
Iteration 116/1000 | Loss: 0.00001059
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001058
Iteration 120/1000 | Loss: 0.00001058
Iteration 121/1000 | Loss: 0.00001058
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001058
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001055
Iteration 137/1000 | Loss: 0.00001055
Iteration 138/1000 | Loss: 0.00001054
Iteration 139/1000 | Loss: 0.00001054
Iteration 140/1000 | Loss: 0.00001054
Iteration 141/1000 | Loss: 0.00001054
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001053
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001053
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001051
Iteration 170/1000 | Loss: 0.00001051
Iteration 171/1000 | Loss: 0.00001051
Iteration 172/1000 | Loss: 0.00001051
Iteration 173/1000 | Loss: 0.00001051
Iteration 174/1000 | Loss: 0.00001051
Iteration 175/1000 | Loss: 0.00001051
Iteration 176/1000 | Loss: 0.00001051
Iteration 177/1000 | Loss: 0.00001051
Iteration 178/1000 | Loss: 0.00001051
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001050
Iteration 182/1000 | Loss: 0.00001050
Iteration 183/1000 | Loss: 0.00001050
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001050
Iteration 192/1000 | Loss: 0.00001050
Iteration 193/1000 | Loss: 0.00001050
Iteration 194/1000 | Loss: 0.00001050
Iteration 195/1000 | Loss: 0.00001050
Iteration 196/1000 | Loss: 0.00001050
Iteration 197/1000 | Loss: 0.00001050
Iteration 198/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.0503299563424662e-05, 1.0503299563424662e-05, 1.0503299563424662e-05, 1.0503299563424662e-05, 1.0503299563424662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0503299563424662e-05

Optimization complete. Final v2v error: 2.785196304321289 mm

Highest mean error: 3.0856118202209473 mm for frame 229

Lowest mean error: 2.6115341186523438 mm for frame 51

Saving results

Total time: 123.48960590362549
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890151
Iteration 2/25 | Loss: 0.00209559
Iteration 3/25 | Loss: 0.00152275
Iteration 4/25 | Loss: 0.00150548
Iteration 5/25 | Loss: 0.00145549
Iteration 6/25 | Loss: 0.00140845
Iteration 7/25 | Loss: 0.00137545
Iteration 8/25 | Loss: 0.00137370
Iteration 9/25 | Loss: 0.00136492
Iteration 10/25 | Loss: 0.00135790
Iteration 11/25 | Loss: 0.00136303
Iteration 12/25 | Loss: 0.00135554
Iteration 13/25 | Loss: 0.00135981
Iteration 14/25 | Loss: 0.00135513
Iteration 15/25 | Loss: 0.00136232
Iteration 16/25 | Loss: 0.00134814
Iteration 17/25 | Loss: 0.00135649
Iteration 18/25 | Loss: 0.00134368
Iteration 19/25 | Loss: 0.00135071
Iteration 20/25 | Loss: 0.00134258
Iteration 21/25 | Loss: 0.00133774
Iteration 22/25 | Loss: 0.00132168
Iteration 23/25 | Loss: 0.00132834
Iteration 24/25 | Loss: 0.00132944
Iteration 25/25 | Loss: 0.00132269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60867953
Iteration 2/25 | Loss: 0.00222040
Iteration 3/25 | Loss: 0.00422946
Iteration 4/25 | Loss: 0.00221971
Iteration 5/25 | Loss: 0.00221971
Iteration 6/25 | Loss: 0.00221971
Iteration 7/25 | Loss: 0.00221971
Iteration 8/25 | Loss: 0.00221971
Iteration 9/25 | Loss: 0.00221971
Iteration 10/25 | Loss: 0.00221971
Iteration 11/25 | Loss: 0.00221971
Iteration 12/25 | Loss: 0.00221971
Iteration 13/25 | Loss: 0.00221971
Iteration 14/25 | Loss: 0.00221971
Iteration 15/25 | Loss: 0.00221971
Iteration 16/25 | Loss: 0.00221971
Iteration 17/25 | Loss: 0.00221971
Iteration 18/25 | Loss: 0.00221971
Iteration 19/25 | Loss: 0.00221971
Iteration 20/25 | Loss: 0.00221971
Iteration 21/25 | Loss: 0.00221971
Iteration 22/25 | Loss: 0.00221971
Iteration 23/25 | Loss: 0.00221971
Iteration 24/25 | Loss: 0.00221971
Iteration 25/25 | Loss: 0.00221971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221971
Iteration 2/1000 | Loss: 0.00429579
Iteration 3/1000 | Loss: 0.00066685
Iteration 4/1000 | Loss: 0.00079176
Iteration 5/1000 | Loss: 0.00049822
Iteration 6/1000 | Loss: 0.00050395
Iteration 7/1000 | Loss: 0.00005950
Iteration 8/1000 | Loss: 0.00034140
Iteration 9/1000 | Loss: 0.00074092
Iteration 10/1000 | Loss: 0.00019616
Iteration 11/1000 | Loss: 0.00305987
Iteration 12/1000 | Loss: 0.00047902
Iteration 13/1000 | Loss: 0.00061261
Iteration 14/1000 | Loss: 0.00042760
Iteration 15/1000 | Loss: 0.00041701
Iteration 16/1000 | Loss: 0.00017509
Iteration 17/1000 | Loss: 0.00020406
Iteration 18/1000 | Loss: 0.00032595
Iteration 19/1000 | Loss: 0.00005519
Iteration 20/1000 | Loss: 0.00004642
Iteration 21/1000 | Loss: 0.00004239
Iteration 22/1000 | Loss: 0.00003955
Iteration 23/1000 | Loss: 0.00042730
Iteration 24/1000 | Loss: 0.00013723
Iteration 25/1000 | Loss: 0.00025526
Iteration 26/1000 | Loss: 0.00011973
Iteration 27/1000 | Loss: 0.00003403
Iteration 28/1000 | Loss: 0.00026636
Iteration 29/1000 | Loss: 0.00029426
Iteration 30/1000 | Loss: 0.00030527
Iteration 31/1000 | Loss: 0.00034382
Iteration 32/1000 | Loss: 0.00005678
Iteration 33/1000 | Loss: 0.00003985
Iteration 34/1000 | Loss: 0.00003133
Iteration 35/1000 | Loss: 0.00030946
Iteration 36/1000 | Loss: 0.00028604
Iteration 37/1000 | Loss: 0.00022687
Iteration 38/1000 | Loss: 0.00033154
Iteration 39/1000 | Loss: 0.00022128
Iteration 40/1000 | Loss: 0.00032183
Iteration 41/1000 | Loss: 0.00020159
Iteration 42/1000 | Loss: 0.00054861
Iteration 43/1000 | Loss: 0.00045467
Iteration 44/1000 | Loss: 0.00004230
Iteration 45/1000 | Loss: 0.00003274
Iteration 46/1000 | Loss: 0.00002986
Iteration 47/1000 | Loss: 0.00002766
Iteration 48/1000 | Loss: 0.00002699
Iteration 49/1000 | Loss: 0.00002643
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002583
Iteration 52/1000 | Loss: 0.00002575
Iteration 53/1000 | Loss: 0.00002553
Iteration 54/1000 | Loss: 0.00002530
Iteration 55/1000 | Loss: 0.00002511
Iteration 56/1000 | Loss: 0.00002511
Iteration 57/1000 | Loss: 0.00002510
Iteration 58/1000 | Loss: 0.00002509
Iteration 59/1000 | Loss: 0.00002504
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002498
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00002493
Iteration 65/1000 | Loss: 0.00002493
Iteration 66/1000 | Loss: 0.00002493
Iteration 67/1000 | Loss: 0.00002493
Iteration 68/1000 | Loss: 0.00002493
Iteration 69/1000 | Loss: 0.00002493
Iteration 70/1000 | Loss: 0.00002493
Iteration 71/1000 | Loss: 0.00002492
Iteration 72/1000 | Loss: 0.00002492
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00002492
Iteration 75/1000 | Loss: 0.00002492
Iteration 76/1000 | Loss: 0.00002492
Iteration 77/1000 | Loss: 0.00002492
Iteration 78/1000 | Loss: 0.00002492
Iteration 79/1000 | Loss: 0.00002492
Iteration 80/1000 | Loss: 0.00002490
Iteration 81/1000 | Loss: 0.00002490
Iteration 82/1000 | Loss: 0.00002489
Iteration 83/1000 | Loss: 0.00002489
Iteration 84/1000 | Loss: 0.00002489
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002488
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002486
Iteration 90/1000 | Loss: 0.00002486
Iteration 91/1000 | Loss: 0.00002485
Iteration 92/1000 | Loss: 0.00002484
Iteration 93/1000 | Loss: 0.00002484
Iteration 94/1000 | Loss: 0.00002483
Iteration 95/1000 | Loss: 0.00002481
Iteration 96/1000 | Loss: 0.00002480
Iteration 97/1000 | Loss: 0.00002478
Iteration 98/1000 | Loss: 0.00002476
Iteration 99/1000 | Loss: 0.00002476
Iteration 100/1000 | Loss: 0.00002475
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002456
Iteration 103/1000 | Loss: 0.00002444
Iteration 104/1000 | Loss: 0.00002443
Iteration 105/1000 | Loss: 0.00002442
Iteration 106/1000 | Loss: 0.00002438
Iteration 107/1000 | Loss: 0.00002438
Iteration 108/1000 | Loss: 0.00002437
Iteration 109/1000 | Loss: 0.00002436
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002434
Iteration 112/1000 | Loss: 0.00002433
Iteration 113/1000 | Loss: 0.00002432
Iteration 114/1000 | Loss: 0.00002432
Iteration 115/1000 | Loss: 0.00002431
Iteration 116/1000 | Loss: 0.00002431
Iteration 117/1000 | Loss: 0.00002430
Iteration 118/1000 | Loss: 0.00002430
Iteration 119/1000 | Loss: 0.00002429
Iteration 120/1000 | Loss: 0.00002429
Iteration 121/1000 | Loss: 0.00002429
Iteration 122/1000 | Loss: 0.00002429
Iteration 123/1000 | Loss: 0.00002428
Iteration 124/1000 | Loss: 0.00002428
Iteration 125/1000 | Loss: 0.00002428
Iteration 126/1000 | Loss: 0.00002427
Iteration 127/1000 | Loss: 0.00002427
Iteration 128/1000 | Loss: 0.00002427
Iteration 129/1000 | Loss: 0.00002426
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002424
Iteration 133/1000 | Loss: 0.00002424
Iteration 134/1000 | Loss: 0.00002423
Iteration 135/1000 | Loss: 0.00002423
Iteration 136/1000 | Loss: 0.00002422
Iteration 137/1000 | Loss: 0.00002422
Iteration 138/1000 | Loss: 0.00002422
Iteration 139/1000 | Loss: 0.00002421
Iteration 140/1000 | Loss: 0.00002421
Iteration 141/1000 | Loss: 0.00002421
Iteration 142/1000 | Loss: 0.00002421
Iteration 143/1000 | Loss: 0.00002421
Iteration 144/1000 | Loss: 0.00002421
Iteration 145/1000 | Loss: 0.00002420
Iteration 146/1000 | Loss: 0.00002420
Iteration 147/1000 | Loss: 0.00002420
Iteration 148/1000 | Loss: 0.00002420
Iteration 149/1000 | Loss: 0.00002420
Iteration 150/1000 | Loss: 0.00002420
Iteration 151/1000 | Loss: 0.00002420
Iteration 152/1000 | Loss: 0.00002419
Iteration 153/1000 | Loss: 0.00002419
Iteration 154/1000 | Loss: 0.00002419
Iteration 155/1000 | Loss: 0.00002419
Iteration 156/1000 | Loss: 0.00002419
Iteration 157/1000 | Loss: 0.00002418
Iteration 158/1000 | Loss: 0.00002418
Iteration 159/1000 | Loss: 0.00002418
Iteration 160/1000 | Loss: 0.00002418
Iteration 161/1000 | Loss: 0.00002418
Iteration 162/1000 | Loss: 0.00002418
Iteration 163/1000 | Loss: 0.00002417
Iteration 164/1000 | Loss: 0.00002417
Iteration 165/1000 | Loss: 0.00002417
Iteration 166/1000 | Loss: 0.00002417
Iteration 167/1000 | Loss: 0.00002417
Iteration 168/1000 | Loss: 0.00002417
Iteration 169/1000 | Loss: 0.00002417
Iteration 170/1000 | Loss: 0.00002417
Iteration 171/1000 | Loss: 0.00002417
Iteration 172/1000 | Loss: 0.00002417
Iteration 173/1000 | Loss: 0.00002417
Iteration 174/1000 | Loss: 0.00002417
Iteration 175/1000 | Loss: 0.00002416
Iteration 176/1000 | Loss: 0.00002416
Iteration 177/1000 | Loss: 0.00002416
Iteration 178/1000 | Loss: 0.00002416
Iteration 179/1000 | Loss: 0.00002416
Iteration 180/1000 | Loss: 0.00002416
Iteration 181/1000 | Loss: 0.00002416
Iteration 182/1000 | Loss: 0.00002416
Iteration 183/1000 | Loss: 0.00002416
Iteration 184/1000 | Loss: 0.00002416
Iteration 185/1000 | Loss: 0.00002415
Iteration 186/1000 | Loss: 0.00002415
Iteration 187/1000 | Loss: 0.00002415
Iteration 188/1000 | Loss: 0.00002415
Iteration 189/1000 | Loss: 0.00002415
Iteration 190/1000 | Loss: 0.00002415
Iteration 191/1000 | Loss: 0.00002415
Iteration 192/1000 | Loss: 0.00002415
Iteration 193/1000 | Loss: 0.00002415
Iteration 194/1000 | Loss: 0.00002414
Iteration 195/1000 | Loss: 0.00002414
Iteration 196/1000 | Loss: 0.00002414
Iteration 197/1000 | Loss: 0.00002414
Iteration 198/1000 | Loss: 0.00002413
Iteration 199/1000 | Loss: 0.00002413
Iteration 200/1000 | Loss: 0.00002412
Iteration 201/1000 | Loss: 0.00002412
Iteration 202/1000 | Loss: 0.00002412
Iteration 203/1000 | Loss: 0.00002412
Iteration 204/1000 | Loss: 0.00002411
Iteration 205/1000 | Loss: 0.00002411
Iteration 206/1000 | Loss: 0.00002411
Iteration 207/1000 | Loss: 0.00002411
Iteration 208/1000 | Loss: 0.00002410
Iteration 209/1000 | Loss: 0.00002410
Iteration 210/1000 | Loss: 0.00002410
Iteration 211/1000 | Loss: 0.00002410
Iteration 212/1000 | Loss: 0.00002410
Iteration 213/1000 | Loss: 0.00002410
Iteration 214/1000 | Loss: 0.00002410
Iteration 215/1000 | Loss: 0.00002409
Iteration 216/1000 | Loss: 0.00002409
Iteration 217/1000 | Loss: 0.00002409
Iteration 218/1000 | Loss: 0.00002409
Iteration 219/1000 | Loss: 0.00002409
Iteration 220/1000 | Loss: 0.00002409
Iteration 221/1000 | Loss: 0.00002409
Iteration 222/1000 | Loss: 0.00002409
Iteration 223/1000 | Loss: 0.00002408
Iteration 224/1000 | Loss: 0.00002408
Iteration 225/1000 | Loss: 0.00002408
Iteration 226/1000 | Loss: 0.00002408
Iteration 227/1000 | Loss: 0.00002408
Iteration 228/1000 | Loss: 0.00002408
Iteration 229/1000 | Loss: 0.00002407
Iteration 230/1000 | Loss: 0.00002407
Iteration 231/1000 | Loss: 0.00002407
Iteration 232/1000 | Loss: 0.00002407
Iteration 233/1000 | Loss: 0.00002407
Iteration 234/1000 | Loss: 0.00002407
Iteration 235/1000 | Loss: 0.00002407
Iteration 236/1000 | Loss: 0.00002407
Iteration 237/1000 | Loss: 0.00002407
Iteration 238/1000 | Loss: 0.00002407
Iteration 239/1000 | Loss: 0.00002407
Iteration 240/1000 | Loss: 0.00002407
Iteration 241/1000 | Loss: 0.00002407
Iteration 242/1000 | Loss: 0.00002407
Iteration 243/1000 | Loss: 0.00002407
Iteration 244/1000 | Loss: 0.00002407
Iteration 245/1000 | Loss: 0.00002407
Iteration 246/1000 | Loss: 0.00002407
Iteration 247/1000 | Loss: 0.00002407
Iteration 248/1000 | Loss: 0.00002407
Iteration 249/1000 | Loss: 0.00002407
Iteration 250/1000 | Loss: 0.00002407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.407052852504421e-05, 2.407052852504421e-05, 2.407052852504421e-05, 2.407052852504421e-05, 2.407052852504421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.407052852504421e-05

Optimization complete. Final v2v error: 3.9678900241851807 mm

Highest mean error: 6.075835227966309 mm for frame 137

Lowest mean error: 2.6911962032318115 mm for frame 47

Saving results

Total time: 431.71826100349426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470332
Iteration 2/25 | Loss: 0.00130745
Iteration 3/25 | Loss: 0.00121489
Iteration 4/25 | Loss: 0.00119674
Iteration 5/25 | Loss: 0.00119196
Iteration 6/25 | Loss: 0.00119196
Iteration 7/25 | Loss: 0.00119196
Iteration 8/25 | Loss: 0.00119196
Iteration 9/25 | Loss: 0.00119196
Iteration 10/25 | Loss: 0.00119196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001191964140161872, 0.001191964140161872, 0.001191964140161872, 0.001191964140161872, 0.001191964140161872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001191964140161872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30286157
Iteration 2/25 | Loss: 0.00138177
Iteration 3/25 | Loss: 0.00138176
Iteration 4/25 | Loss: 0.00138175
Iteration 5/25 | Loss: 0.00138175
Iteration 6/25 | Loss: 0.00138175
Iteration 7/25 | Loss: 0.00138175
Iteration 8/25 | Loss: 0.00138175
Iteration 9/25 | Loss: 0.00138175
Iteration 10/25 | Loss: 0.00138175
Iteration 11/25 | Loss: 0.00138175
Iteration 12/25 | Loss: 0.00138175
Iteration 13/25 | Loss: 0.00138175
Iteration 14/25 | Loss: 0.00138175
Iteration 15/25 | Loss: 0.00138175
Iteration 16/25 | Loss: 0.00138175
Iteration 17/25 | Loss: 0.00138175
Iteration 18/25 | Loss: 0.00138175
Iteration 19/25 | Loss: 0.00138175
Iteration 20/25 | Loss: 0.00138175
Iteration 21/25 | Loss: 0.00138175
Iteration 22/25 | Loss: 0.00138175
Iteration 23/25 | Loss: 0.00138175
Iteration 24/25 | Loss: 0.00138175
Iteration 25/25 | Loss: 0.00138175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138175
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00001693
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001429
Iteration 8/1000 | Loss: 0.00001402
Iteration 9/1000 | Loss: 0.00001371
Iteration 10/1000 | Loss: 0.00001343
Iteration 11/1000 | Loss: 0.00001323
Iteration 12/1000 | Loss: 0.00001308
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001283
Iteration 16/1000 | Loss: 0.00001276
Iteration 17/1000 | Loss: 0.00001275
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001259
Iteration 21/1000 | Loss: 0.00001257
Iteration 22/1000 | Loss: 0.00001257
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001251
Iteration 31/1000 | Loss: 0.00001251
Iteration 32/1000 | Loss: 0.00001250
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001250
Iteration 36/1000 | Loss: 0.00001250
Iteration 37/1000 | Loss: 0.00001249
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001246
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001246
Iteration 42/1000 | Loss: 0.00001246
Iteration 43/1000 | Loss: 0.00001246
Iteration 44/1000 | Loss: 0.00001246
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001246
Iteration 57/1000 | Loss: 0.00001246
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001246
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001246
Iteration 70/1000 | Loss: 0.00001246
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001246
Iteration 73/1000 | Loss: 0.00001246
Iteration 74/1000 | Loss: 0.00001246
Iteration 75/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.2460394827940036e-05, 1.2460394827940036e-05, 1.2460394827940036e-05, 1.2460394827940036e-05, 1.2460394827940036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2460394827940036e-05

Optimization complete. Final v2v error: 3.023876667022705 mm

Highest mean error: 3.381045341491699 mm for frame 141

Lowest mean error: 2.8224070072174072 mm for frame 161

Saving results

Total time: 92.13823008537292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050973
Iteration 2/25 | Loss: 0.00176927
Iteration 3/25 | Loss: 0.00141492
Iteration 4/25 | Loss: 0.00134279
Iteration 5/25 | Loss: 0.00132883
Iteration 6/25 | Loss: 0.00130885
Iteration 7/25 | Loss: 0.00129389
Iteration 8/25 | Loss: 0.00129245
Iteration 9/25 | Loss: 0.00128909
Iteration 10/25 | Loss: 0.00128387
Iteration 11/25 | Loss: 0.00128730
Iteration 12/25 | Loss: 0.00128403
Iteration 13/25 | Loss: 0.00128365
Iteration 14/25 | Loss: 0.00128363
Iteration 15/25 | Loss: 0.00128363
Iteration 16/25 | Loss: 0.00128363
Iteration 17/25 | Loss: 0.00128363
Iteration 18/25 | Loss: 0.00128363
Iteration 19/25 | Loss: 0.00128363
Iteration 20/25 | Loss: 0.00128363
Iteration 21/25 | Loss: 0.00128363
Iteration 22/25 | Loss: 0.00128362
Iteration 23/25 | Loss: 0.00128362
Iteration 24/25 | Loss: 0.00128362
Iteration 25/25 | Loss: 0.00128362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.01669216
Iteration 2/25 | Loss: 0.00188366
Iteration 3/25 | Loss: 0.00188347
Iteration 4/25 | Loss: 0.00188347
Iteration 5/25 | Loss: 0.00188347
Iteration 6/25 | Loss: 0.00188347
Iteration 7/25 | Loss: 0.00188347
Iteration 8/25 | Loss: 0.00188347
Iteration 9/25 | Loss: 0.00188347
Iteration 10/25 | Loss: 0.00188347
Iteration 11/25 | Loss: 0.00188347
Iteration 12/25 | Loss: 0.00188347
Iteration 13/25 | Loss: 0.00188347
Iteration 14/25 | Loss: 0.00188347
Iteration 15/25 | Loss: 0.00188347
Iteration 16/25 | Loss: 0.00188347
Iteration 17/25 | Loss: 0.00188347
Iteration 18/25 | Loss: 0.00188347
Iteration 19/25 | Loss: 0.00188347
Iteration 20/25 | Loss: 0.00188347
Iteration 21/25 | Loss: 0.00188347
Iteration 22/25 | Loss: 0.00188347
Iteration 23/25 | Loss: 0.00188347
Iteration 24/25 | Loss: 0.00188347
Iteration 25/25 | Loss: 0.00188347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001883466262370348, 0.001883466262370348, 0.001883466262370348, 0.001883466262370348, 0.001883466262370348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001883466262370348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188347
Iteration 2/1000 | Loss: 0.00009766
Iteration 3/1000 | Loss: 0.00006685
Iteration 4/1000 | Loss: 0.00004843
Iteration 5/1000 | Loss: 0.00004385
Iteration 6/1000 | Loss: 0.00004168
Iteration 7/1000 | Loss: 0.00003992
Iteration 8/1000 | Loss: 0.00003885
Iteration 9/1000 | Loss: 0.00003804
Iteration 10/1000 | Loss: 0.00003744
Iteration 11/1000 | Loss: 0.00003702
Iteration 12/1000 | Loss: 0.00003671
Iteration 13/1000 | Loss: 0.00003639
Iteration 14/1000 | Loss: 0.00003613
Iteration 15/1000 | Loss: 0.00003589
Iteration 16/1000 | Loss: 0.00003568
Iteration 17/1000 | Loss: 0.00003557
Iteration 18/1000 | Loss: 0.00003540
Iteration 19/1000 | Loss: 0.00003529
Iteration 20/1000 | Loss: 0.00003528
Iteration 21/1000 | Loss: 0.00003522
Iteration 22/1000 | Loss: 0.00003522
Iteration 23/1000 | Loss: 0.00003519
Iteration 24/1000 | Loss: 0.00003519
Iteration 25/1000 | Loss: 0.00003516
Iteration 26/1000 | Loss: 0.00003515
Iteration 27/1000 | Loss: 0.00003515
Iteration 28/1000 | Loss: 0.00003514
Iteration 29/1000 | Loss: 0.00003510
Iteration 30/1000 | Loss: 0.00003507
Iteration 31/1000 | Loss: 0.00003506
Iteration 32/1000 | Loss: 0.00003502
Iteration 33/1000 | Loss: 0.00003496
Iteration 34/1000 | Loss: 0.00003495
Iteration 35/1000 | Loss: 0.00003494
Iteration 36/1000 | Loss: 0.00003493
Iteration 37/1000 | Loss: 0.00003491
Iteration 38/1000 | Loss: 0.00003488
Iteration 39/1000 | Loss: 0.00003488
Iteration 40/1000 | Loss: 0.00003487
Iteration 41/1000 | Loss: 0.00003487
Iteration 42/1000 | Loss: 0.00003484
Iteration 43/1000 | Loss: 0.00003484
Iteration 44/1000 | Loss: 0.00003482
Iteration 45/1000 | Loss: 0.00003482
Iteration 46/1000 | Loss: 0.00003481
Iteration 47/1000 | Loss: 0.00003481
Iteration 48/1000 | Loss: 0.00003481
Iteration 49/1000 | Loss: 0.00003481
Iteration 50/1000 | Loss: 0.00009909
Iteration 51/1000 | Loss: 0.00003479
Iteration 52/1000 | Loss: 0.00003479
Iteration 53/1000 | Loss: 0.00003479
Iteration 54/1000 | Loss: 0.00003478
Iteration 55/1000 | Loss: 0.00003478
Iteration 56/1000 | Loss: 0.00003474
Iteration 57/1000 | Loss: 0.00003473
Iteration 58/1000 | Loss: 0.00003473
Iteration 59/1000 | Loss: 0.00003473
Iteration 60/1000 | Loss: 0.00003472
Iteration 61/1000 | Loss: 0.00003472
Iteration 62/1000 | Loss: 0.00003472
Iteration 63/1000 | Loss: 0.00003472
Iteration 64/1000 | Loss: 0.00003471
Iteration 65/1000 | Loss: 0.00003471
Iteration 66/1000 | Loss: 0.00003471
Iteration 67/1000 | Loss: 0.00003471
Iteration 68/1000 | Loss: 0.00003470
Iteration 69/1000 | Loss: 0.00003470
Iteration 70/1000 | Loss: 0.00003470
Iteration 71/1000 | Loss: 0.00003469
Iteration 72/1000 | Loss: 0.00003469
Iteration 73/1000 | Loss: 0.00003469
Iteration 74/1000 | Loss: 0.00003469
Iteration 75/1000 | Loss: 0.00003468
Iteration 76/1000 | Loss: 0.00003468
Iteration 77/1000 | Loss: 0.00003468
Iteration 78/1000 | Loss: 0.00003468
Iteration 79/1000 | Loss: 0.00003468
Iteration 80/1000 | Loss: 0.00003467
Iteration 81/1000 | Loss: 0.00003467
Iteration 82/1000 | Loss: 0.00003467
Iteration 83/1000 | Loss: 0.00003467
Iteration 84/1000 | Loss: 0.00003467
Iteration 85/1000 | Loss: 0.00003467
Iteration 86/1000 | Loss: 0.00003466
Iteration 87/1000 | Loss: 0.00003466
Iteration 88/1000 | Loss: 0.00003466
Iteration 89/1000 | Loss: 0.00003465
Iteration 90/1000 | Loss: 0.00003465
Iteration 91/1000 | Loss: 0.00003465
Iteration 92/1000 | Loss: 0.00003464
Iteration 93/1000 | Loss: 0.00003464
Iteration 94/1000 | Loss: 0.00003464
Iteration 95/1000 | Loss: 0.00003464
Iteration 96/1000 | Loss: 0.00003463
Iteration 97/1000 | Loss: 0.00003463
Iteration 98/1000 | Loss: 0.00003463
Iteration 99/1000 | Loss: 0.00003463
Iteration 100/1000 | Loss: 0.00003463
Iteration 101/1000 | Loss: 0.00003462
Iteration 102/1000 | Loss: 0.00003462
Iteration 103/1000 | Loss: 0.00003462
Iteration 104/1000 | Loss: 0.00003462
Iteration 105/1000 | Loss: 0.00003461
Iteration 106/1000 | Loss: 0.00003461
Iteration 107/1000 | Loss: 0.00003461
Iteration 108/1000 | Loss: 0.00003460
Iteration 109/1000 | Loss: 0.00003460
Iteration 110/1000 | Loss: 0.00003459
Iteration 111/1000 | Loss: 0.00003459
Iteration 112/1000 | Loss: 0.00003459
Iteration 113/1000 | Loss: 0.00003458
Iteration 114/1000 | Loss: 0.00003458
Iteration 115/1000 | Loss: 0.00003458
Iteration 116/1000 | Loss: 0.00003458
Iteration 117/1000 | Loss: 0.00003458
Iteration 118/1000 | Loss: 0.00003458
Iteration 119/1000 | Loss: 0.00003457
Iteration 120/1000 | Loss: 0.00003457
Iteration 121/1000 | Loss: 0.00003457
Iteration 122/1000 | Loss: 0.00003457
Iteration 123/1000 | Loss: 0.00003457
Iteration 124/1000 | Loss: 0.00003457
Iteration 125/1000 | Loss: 0.00003456
Iteration 126/1000 | Loss: 0.00003456
Iteration 127/1000 | Loss: 0.00003456
Iteration 128/1000 | Loss: 0.00003456
Iteration 129/1000 | Loss: 0.00003456
Iteration 130/1000 | Loss: 0.00003456
Iteration 131/1000 | Loss: 0.00003456
Iteration 132/1000 | Loss: 0.00003456
Iteration 133/1000 | Loss: 0.00003456
Iteration 134/1000 | Loss: 0.00003456
Iteration 135/1000 | Loss: 0.00003456
Iteration 136/1000 | Loss: 0.00003456
Iteration 137/1000 | Loss: 0.00003456
Iteration 138/1000 | Loss: 0.00003455
Iteration 139/1000 | Loss: 0.00003455
Iteration 140/1000 | Loss: 0.00003455
Iteration 141/1000 | Loss: 0.00003455
Iteration 142/1000 | Loss: 0.00003455
Iteration 143/1000 | Loss: 0.00003455
Iteration 144/1000 | Loss: 0.00003455
Iteration 145/1000 | Loss: 0.00003455
Iteration 146/1000 | Loss: 0.00003454
Iteration 147/1000 | Loss: 0.00003454
Iteration 148/1000 | Loss: 0.00003454
Iteration 149/1000 | Loss: 0.00003454
Iteration 150/1000 | Loss: 0.00003454
Iteration 151/1000 | Loss: 0.00003454
Iteration 152/1000 | Loss: 0.00003454
Iteration 153/1000 | Loss: 0.00003454
Iteration 154/1000 | Loss: 0.00003453
Iteration 155/1000 | Loss: 0.00003453
Iteration 156/1000 | Loss: 0.00003453
Iteration 157/1000 | Loss: 0.00003453
Iteration 158/1000 | Loss: 0.00003453
Iteration 159/1000 | Loss: 0.00003453
Iteration 160/1000 | Loss: 0.00003453
Iteration 161/1000 | Loss: 0.00003453
Iteration 162/1000 | Loss: 0.00003453
Iteration 163/1000 | Loss: 0.00003453
Iteration 164/1000 | Loss: 0.00003453
Iteration 165/1000 | Loss: 0.00003452
Iteration 166/1000 | Loss: 0.00003452
Iteration 167/1000 | Loss: 0.00003452
Iteration 168/1000 | Loss: 0.00003452
Iteration 169/1000 | Loss: 0.00003452
Iteration 170/1000 | Loss: 0.00003452
Iteration 171/1000 | Loss: 0.00003451
Iteration 172/1000 | Loss: 0.00003451
Iteration 173/1000 | Loss: 0.00003451
Iteration 174/1000 | Loss: 0.00003451
Iteration 175/1000 | Loss: 0.00003451
Iteration 176/1000 | Loss: 0.00003451
Iteration 177/1000 | Loss: 0.00003450
Iteration 178/1000 | Loss: 0.00003450
Iteration 179/1000 | Loss: 0.00003450
Iteration 180/1000 | Loss: 0.00003450
Iteration 181/1000 | Loss: 0.00003450
Iteration 182/1000 | Loss: 0.00003450
Iteration 183/1000 | Loss: 0.00003450
Iteration 184/1000 | Loss: 0.00003450
Iteration 185/1000 | Loss: 0.00003450
Iteration 186/1000 | Loss: 0.00003450
Iteration 187/1000 | Loss: 0.00003450
Iteration 188/1000 | Loss: 0.00003450
Iteration 189/1000 | Loss: 0.00003450
Iteration 190/1000 | Loss: 0.00003450
Iteration 191/1000 | Loss: 0.00003450
Iteration 192/1000 | Loss: 0.00003450
Iteration 193/1000 | Loss: 0.00003450
Iteration 194/1000 | Loss: 0.00003450
Iteration 195/1000 | Loss: 0.00003449
Iteration 196/1000 | Loss: 0.00003449
Iteration 197/1000 | Loss: 0.00003449
Iteration 198/1000 | Loss: 0.00003449
Iteration 199/1000 | Loss: 0.00003449
Iteration 200/1000 | Loss: 0.00003449
Iteration 201/1000 | Loss: 0.00003449
Iteration 202/1000 | Loss: 0.00003449
Iteration 203/1000 | Loss: 0.00003449
Iteration 204/1000 | Loss: 0.00003449
Iteration 205/1000 | Loss: 0.00003449
Iteration 206/1000 | Loss: 0.00003449
Iteration 207/1000 | Loss: 0.00003449
Iteration 208/1000 | Loss: 0.00003449
Iteration 209/1000 | Loss: 0.00003449
Iteration 210/1000 | Loss: 0.00003449
Iteration 211/1000 | Loss: 0.00003449
Iteration 212/1000 | Loss: 0.00003449
Iteration 213/1000 | Loss: 0.00003449
Iteration 214/1000 | Loss: 0.00003449
Iteration 215/1000 | Loss: 0.00003449
Iteration 216/1000 | Loss: 0.00003449
Iteration 217/1000 | Loss: 0.00003449
Iteration 218/1000 | Loss: 0.00003449
Iteration 219/1000 | Loss: 0.00003449
Iteration 220/1000 | Loss: 0.00003449
Iteration 221/1000 | Loss: 0.00003449
Iteration 222/1000 | Loss: 0.00003449
Iteration 223/1000 | Loss: 0.00003449
Iteration 224/1000 | Loss: 0.00003449
Iteration 225/1000 | Loss: 0.00003449
Iteration 226/1000 | Loss: 0.00003449
Iteration 227/1000 | Loss: 0.00003449
Iteration 228/1000 | Loss: 0.00003449
Iteration 229/1000 | Loss: 0.00003449
Iteration 230/1000 | Loss: 0.00003449
Iteration 231/1000 | Loss: 0.00003449
Iteration 232/1000 | Loss: 0.00003449
Iteration 233/1000 | Loss: 0.00003449
Iteration 234/1000 | Loss: 0.00003449
Iteration 235/1000 | Loss: 0.00003449
Iteration 236/1000 | Loss: 0.00003449
Iteration 237/1000 | Loss: 0.00003449
Iteration 238/1000 | Loss: 0.00003449
Iteration 239/1000 | Loss: 0.00003449
Iteration 240/1000 | Loss: 0.00003449
Iteration 241/1000 | Loss: 0.00003449
Iteration 242/1000 | Loss: 0.00003449
Iteration 243/1000 | Loss: 0.00003449
Iteration 244/1000 | Loss: 0.00003449
Iteration 245/1000 | Loss: 0.00003449
Iteration 246/1000 | Loss: 0.00003449
Iteration 247/1000 | Loss: 0.00003449
Iteration 248/1000 | Loss: 0.00003449
Iteration 249/1000 | Loss: 0.00003449
Iteration 250/1000 | Loss: 0.00003449
Iteration 251/1000 | Loss: 0.00003449
Iteration 252/1000 | Loss: 0.00003449
Iteration 253/1000 | Loss: 0.00003449
Iteration 254/1000 | Loss: 0.00003449
Iteration 255/1000 | Loss: 0.00003449
Iteration 256/1000 | Loss: 0.00003449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [3.4493248676881194e-05, 3.4493248676881194e-05, 3.4493248676881194e-05, 3.4493248676881194e-05, 3.4493248676881194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4493248676881194e-05

Optimization complete. Final v2v error: 4.691434383392334 mm

Highest mean error: 7.300616264343262 mm for frame 99

Lowest mean error: 3.1426966190338135 mm for frame 140

Saving results

Total time: 164.25915122032166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466846
Iteration 2/25 | Loss: 0.00126004
Iteration 3/25 | Loss: 0.00120468
Iteration 4/25 | Loss: 0.00119948
Iteration 5/25 | Loss: 0.00119948
Iteration 6/25 | Loss: 0.00119948
Iteration 7/25 | Loss: 0.00119948
Iteration 8/25 | Loss: 0.00119948
Iteration 9/25 | Loss: 0.00119948
Iteration 10/25 | Loss: 0.00119948
Iteration 11/25 | Loss: 0.00119948
Iteration 12/25 | Loss: 0.00119948
Iteration 13/25 | Loss: 0.00119948
Iteration 14/25 | Loss: 0.00119948
Iteration 15/25 | Loss: 0.00119948
Iteration 16/25 | Loss: 0.00119948
Iteration 17/25 | Loss: 0.00119948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011994820088148117, 0.0011994820088148117, 0.0011994820088148117, 0.0011994820088148117, 0.0011994820088148117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011994820088148117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.76746655
Iteration 2/25 | Loss: 0.00120844
Iteration 3/25 | Loss: 0.00120844
Iteration 4/25 | Loss: 0.00120844
Iteration 5/25 | Loss: 0.00120844
Iteration 6/25 | Loss: 0.00120844
Iteration 7/25 | Loss: 0.00120844
Iteration 8/25 | Loss: 0.00120844
Iteration 9/25 | Loss: 0.00120844
Iteration 10/25 | Loss: 0.00120844
Iteration 11/25 | Loss: 0.00120844
Iteration 12/25 | Loss: 0.00120844
Iteration 13/25 | Loss: 0.00120844
Iteration 14/25 | Loss: 0.00120844
Iteration 15/25 | Loss: 0.00120844
Iteration 16/25 | Loss: 0.00120844
Iteration 17/25 | Loss: 0.00120844
Iteration 18/25 | Loss: 0.00120844
Iteration 19/25 | Loss: 0.00120844
Iteration 20/25 | Loss: 0.00120844
Iteration 21/25 | Loss: 0.00120844
Iteration 22/25 | Loss: 0.00120844
Iteration 23/25 | Loss: 0.00120844
Iteration 24/25 | Loss: 0.00120844
Iteration 25/25 | Loss: 0.00120844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120844
Iteration 2/1000 | Loss: 0.00002126
Iteration 3/1000 | Loss: 0.00001674
Iteration 4/1000 | Loss: 0.00001545
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001357
Iteration 8/1000 | Loss: 0.00001324
Iteration 9/1000 | Loss: 0.00001293
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001265
Iteration 12/1000 | Loss: 0.00001262
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001245
Iteration 16/1000 | Loss: 0.00001226
Iteration 17/1000 | Loss: 0.00001220
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001196
Iteration 21/1000 | Loss: 0.00001196
Iteration 22/1000 | Loss: 0.00001195
Iteration 23/1000 | Loss: 0.00001194
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001169
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001164
Iteration 50/1000 | Loss: 0.00001164
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001164
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001160
Iteration 67/1000 | Loss: 0.00001160
Iteration 68/1000 | Loss: 0.00001160
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001159
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001159
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001156
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001156
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001155
Iteration 115/1000 | Loss: 0.00001155
Iteration 116/1000 | Loss: 0.00001155
Iteration 117/1000 | Loss: 0.00001155
Iteration 118/1000 | Loss: 0.00001155
Iteration 119/1000 | Loss: 0.00001155
Iteration 120/1000 | Loss: 0.00001155
Iteration 121/1000 | Loss: 0.00001155
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001155
Iteration 126/1000 | Loss: 0.00001155
Iteration 127/1000 | Loss: 0.00001155
Iteration 128/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.1551174793567043e-05, 1.1551174793567043e-05, 1.1551174793567043e-05, 1.1551174793567043e-05, 1.1551174793567043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1551174793567043e-05

Optimization complete. Final v2v error: 2.9299795627593994 mm

Highest mean error: 3.1124722957611084 mm for frame 240

Lowest mean error: 2.7374045848846436 mm for frame 211

Saving results

Total time: 117.51305198669434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789904
Iteration 2/25 | Loss: 0.00126490
Iteration 3/25 | Loss: 0.00119620
Iteration 4/25 | Loss: 0.00118161
Iteration 5/25 | Loss: 0.00117706
Iteration 6/25 | Loss: 0.00117683
Iteration 7/25 | Loss: 0.00117683
Iteration 8/25 | Loss: 0.00117683
Iteration 9/25 | Loss: 0.00117683
Iteration 10/25 | Loss: 0.00117683
Iteration 11/25 | Loss: 0.00117683
Iteration 12/25 | Loss: 0.00117683
Iteration 13/25 | Loss: 0.00117683
Iteration 14/25 | Loss: 0.00117683
Iteration 15/25 | Loss: 0.00117683
Iteration 16/25 | Loss: 0.00117683
Iteration 17/25 | Loss: 0.00117683
Iteration 18/25 | Loss: 0.00117683
Iteration 19/25 | Loss: 0.00117683
Iteration 20/25 | Loss: 0.00117683
Iteration 21/25 | Loss: 0.00117683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011768278200179338, 0.0011768278200179338, 0.0011768278200179338, 0.0011768278200179338, 0.0011768278200179338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011768278200179338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33004510
Iteration 2/25 | Loss: 0.00134377
Iteration 3/25 | Loss: 0.00134377
Iteration 4/25 | Loss: 0.00134376
Iteration 5/25 | Loss: 0.00134376
Iteration 6/25 | Loss: 0.00134376
Iteration 7/25 | Loss: 0.00134376
Iteration 8/25 | Loss: 0.00134376
Iteration 9/25 | Loss: 0.00134376
Iteration 10/25 | Loss: 0.00134376
Iteration 11/25 | Loss: 0.00134376
Iteration 12/25 | Loss: 0.00134376
Iteration 13/25 | Loss: 0.00134376
Iteration 14/25 | Loss: 0.00134376
Iteration 15/25 | Loss: 0.00134376
Iteration 16/25 | Loss: 0.00134376
Iteration 17/25 | Loss: 0.00134376
Iteration 18/25 | Loss: 0.00134376
Iteration 19/25 | Loss: 0.00134376
Iteration 20/25 | Loss: 0.00134376
Iteration 21/25 | Loss: 0.00134376
Iteration 22/25 | Loss: 0.00134376
Iteration 23/25 | Loss: 0.00134376
Iteration 24/25 | Loss: 0.00134376
Iteration 25/25 | Loss: 0.00134376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134376
Iteration 2/1000 | Loss: 0.00002224
Iteration 3/1000 | Loss: 0.00001565
Iteration 4/1000 | Loss: 0.00001430
Iteration 5/1000 | Loss: 0.00001364
Iteration 6/1000 | Loss: 0.00001309
Iteration 7/1000 | Loss: 0.00001261
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001204
Iteration 10/1000 | Loss: 0.00001177
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001143
Iteration 15/1000 | Loss: 0.00001140
Iteration 16/1000 | Loss: 0.00001130
Iteration 17/1000 | Loss: 0.00001125
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001108
Iteration 20/1000 | Loss: 0.00001106
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001105
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001091
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001085
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001084
Iteration 51/1000 | Loss: 0.00001084
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001083
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001083
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001078
Iteration 76/1000 | Loss: 0.00001078
Iteration 77/1000 | Loss: 0.00001078
Iteration 78/1000 | Loss: 0.00001078
Iteration 79/1000 | Loss: 0.00001078
Iteration 80/1000 | Loss: 0.00001078
Iteration 81/1000 | Loss: 0.00001078
Iteration 82/1000 | Loss: 0.00001078
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001077
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001073
Iteration 102/1000 | Loss: 0.00001073
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001070
Iteration 136/1000 | Loss: 0.00001070
Iteration 137/1000 | Loss: 0.00001070
Iteration 138/1000 | Loss: 0.00001070
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001068
Iteration 151/1000 | Loss: 0.00001068
Iteration 152/1000 | Loss: 0.00001068
Iteration 153/1000 | Loss: 0.00001068
Iteration 154/1000 | Loss: 0.00001068
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001067
Iteration 158/1000 | Loss: 0.00001067
Iteration 159/1000 | Loss: 0.00001067
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001066
Iteration 163/1000 | Loss: 0.00001066
Iteration 164/1000 | Loss: 0.00001066
Iteration 165/1000 | Loss: 0.00001066
Iteration 166/1000 | Loss: 0.00001066
Iteration 167/1000 | Loss: 0.00001066
Iteration 168/1000 | Loss: 0.00001066
Iteration 169/1000 | Loss: 0.00001066
Iteration 170/1000 | Loss: 0.00001066
Iteration 171/1000 | Loss: 0.00001066
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001065
Iteration 175/1000 | Loss: 0.00001064
Iteration 176/1000 | Loss: 0.00001064
Iteration 177/1000 | Loss: 0.00001064
Iteration 178/1000 | Loss: 0.00001064
Iteration 179/1000 | Loss: 0.00001064
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001064
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00001064
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001064
Iteration 189/1000 | Loss: 0.00001064
Iteration 190/1000 | Loss: 0.00001064
Iteration 191/1000 | Loss: 0.00001064
Iteration 192/1000 | Loss: 0.00001064
Iteration 193/1000 | Loss: 0.00001064
Iteration 194/1000 | Loss: 0.00001064
Iteration 195/1000 | Loss: 0.00001064
Iteration 196/1000 | Loss: 0.00001063
Iteration 197/1000 | Loss: 0.00001063
Iteration 198/1000 | Loss: 0.00001063
Iteration 199/1000 | Loss: 0.00001063
Iteration 200/1000 | Loss: 0.00001063
Iteration 201/1000 | Loss: 0.00001063
Iteration 202/1000 | Loss: 0.00001063
Iteration 203/1000 | Loss: 0.00001063
Iteration 204/1000 | Loss: 0.00001063
Iteration 205/1000 | Loss: 0.00001063
Iteration 206/1000 | Loss: 0.00001063
Iteration 207/1000 | Loss: 0.00001063
Iteration 208/1000 | Loss: 0.00001063
Iteration 209/1000 | Loss: 0.00001063
Iteration 210/1000 | Loss: 0.00001063
Iteration 211/1000 | Loss: 0.00001063
Iteration 212/1000 | Loss: 0.00001063
Iteration 213/1000 | Loss: 0.00001063
Iteration 214/1000 | Loss: 0.00001063
Iteration 215/1000 | Loss: 0.00001063
Iteration 216/1000 | Loss: 0.00001063
Iteration 217/1000 | Loss: 0.00001063
Iteration 218/1000 | Loss: 0.00001063
Iteration 219/1000 | Loss: 0.00001063
Iteration 220/1000 | Loss: 0.00001063
Iteration 221/1000 | Loss: 0.00001063
Iteration 222/1000 | Loss: 0.00001063
Iteration 223/1000 | Loss: 0.00001063
Iteration 224/1000 | Loss: 0.00001063
Iteration 225/1000 | Loss: 0.00001063
Iteration 226/1000 | Loss: 0.00001063
Iteration 227/1000 | Loss: 0.00001063
Iteration 228/1000 | Loss: 0.00001063
Iteration 229/1000 | Loss: 0.00001063
Iteration 230/1000 | Loss: 0.00001063
Iteration 231/1000 | Loss: 0.00001063
Iteration 232/1000 | Loss: 0.00001063
Iteration 233/1000 | Loss: 0.00001063
Iteration 234/1000 | Loss: 0.00001063
Iteration 235/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.062759110936895e-05, 1.062759110936895e-05, 1.062759110936895e-05, 1.062759110936895e-05, 1.062759110936895e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.062759110936895e-05

Optimization complete. Final v2v error: 2.8151564598083496 mm

Highest mean error: 3.061974287033081 mm for frame 65

Lowest mean error: 2.708904504776001 mm for frame 144

Saving results

Total time: 105.29929423332214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00641498
Iteration 2/25 | Loss: 0.00167452
Iteration 3/25 | Loss: 0.00141282
Iteration 4/25 | Loss: 0.00138980
Iteration 5/25 | Loss: 0.00138664
Iteration 6/25 | Loss: 0.00138664
Iteration 7/25 | Loss: 0.00138664
Iteration 8/25 | Loss: 0.00138664
Iteration 9/25 | Loss: 0.00138664
Iteration 10/25 | Loss: 0.00138664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.00138663814868778, 0.00138663814868778, 0.00138663814868778, 0.00138663814868778, 0.00138663814868778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00138663814868778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34241450
Iteration 2/25 | Loss: 0.00131610
Iteration 3/25 | Loss: 0.00131608
Iteration 4/25 | Loss: 0.00131608
Iteration 5/25 | Loss: 0.00131608
Iteration 6/25 | Loss: 0.00131608
Iteration 7/25 | Loss: 0.00131608
Iteration 8/25 | Loss: 0.00131608
Iteration 9/25 | Loss: 0.00131607
Iteration 10/25 | Loss: 0.00131607
Iteration 11/25 | Loss: 0.00131607
Iteration 12/25 | Loss: 0.00131607
Iteration 13/25 | Loss: 0.00131607
Iteration 14/25 | Loss: 0.00131607
Iteration 15/25 | Loss: 0.00131607
Iteration 16/25 | Loss: 0.00131607
Iteration 17/25 | Loss: 0.00131607
Iteration 18/25 | Loss: 0.00131607
Iteration 19/25 | Loss: 0.00131607
Iteration 20/25 | Loss: 0.00131607
Iteration 21/25 | Loss: 0.00131607
Iteration 22/25 | Loss: 0.00131607
Iteration 23/25 | Loss: 0.00131607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001316073932684958, 0.001316073932684958, 0.001316073932684958, 0.001316073932684958, 0.001316073932684958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316073932684958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131607
Iteration 2/1000 | Loss: 0.00005290
Iteration 3/1000 | Loss: 0.00003303
Iteration 4/1000 | Loss: 0.00002949
Iteration 5/1000 | Loss: 0.00002826
Iteration 6/1000 | Loss: 0.00002738
Iteration 7/1000 | Loss: 0.00002678
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002597
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002544
Iteration 12/1000 | Loss: 0.00002525
Iteration 13/1000 | Loss: 0.00002521
Iteration 14/1000 | Loss: 0.00002519
Iteration 15/1000 | Loss: 0.00002518
Iteration 16/1000 | Loss: 0.00002514
Iteration 17/1000 | Loss: 0.00002514
Iteration 18/1000 | Loss: 0.00002512
Iteration 19/1000 | Loss: 0.00002510
Iteration 20/1000 | Loss: 0.00002506
Iteration 21/1000 | Loss: 0.00002506
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002504
Iteration 24/1000 | Loss: 0.00002502
Iteration 25/1000 | Loss: 0.00002502
Iteration 26/1000 | Loss: 0.00002502
Iteration 27/1000 | Loss: 0.00002501
Iteration 28/1000 | Loss: 0.00002499
Iteration 29/1000 | Loss: 0.00002499
Iteration 30/1000 | Loss: 0.00002498
Iteration 31/1000 | Loss: 0.00002497
Iteration 32/1000 | Loss: 0.00002497
Iteration 33/1000 | Loss: 0.00002495
Iteration 34/1000 | Loss: 0.00002494
Iteration 35/1000 | Loss: 0.00002494
Iteration 36/1000 | Loss: 0.00002494
Iteration 37/1000 | Loss: 0.00002494
Iteration 38/1000 | Loss: 0.00002494
Iteration 39/1000 | Loss: 0.00002494
Iteration 40/1000 | Loss: 0.00002494
Iteration 41/1000 | Loss: 0.00002494
Iteration 42/1000 | Loss: 0.00002493
Iteration 43/1000 | Loss: 0.00002493
Iteration 44/1000 | Loss: 0.00002492
Iteration 45/1000 | Loss: 0.00002492
Iteration 46/1000 | Loss: 0.00002492
Iteration 47/1000 | Loss: 0.00002492
Iteration 48/1000 | Loss: 0.00002492
Iteration 49/1000 | Loss: 0.00002492
Iteration 50/1000 | Loss: 0.00002491
Iteration 51/1000 | Loss: 0.00002491
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002491
Iteration 54/1000 | Loss: 0.00002491
Iteration 55/1000 | Loss: 0.00002491
Iteration 56/1000 | Loss: 0.00002491
Iteration 57/1000 | Loss: 0.00002491
Iteration 58/1000 | Loss: 0.00002491
Iteration 59/1000 | Loss: 0.00002491
Iteration 60/1000 | Loss: 0.00002491
Iteration 61/1000 | Loss: 0.00002491
Iteration 62/1000 | Loss: 0.00002490
Iteration 63/1000 | Loss: 0.00002490
Iteration 64/1000 | Loss: 0.00002490
Iteration 65/1000 | Loss: 0.00002490
Iteration 66/1000 | Loss: 0.00002490
Iteration 67/1000 | Loss: 0.00002490
Iteration 68/1000 | Loss: 0.00002490
Iteration 69/1000 | Loss: 0.00002490
Iteration 70/1000 | Loss: 0.00002489
Iteration 71/1000 | Loss: 0.00002489
Iteration 72/1000 | Loss: 0.00002489
Iteration 73/1000 | Loss: 0.00002489
Iteration 74/1000 | Loss: 0.00002489
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002488
Iteration 77/1000 | Loss: 0.00002488
Iteration 78/1000 | Loss: 0.00002488
Iteration 79/1000 | Loss: 0.00002488
Iteration 80/1000 | Loss: 0.00002488
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002487
Iteration 83/1000 | Loss: 0.00002487
Iteration 84/1000 | Loss: 0.00002487
Iteration 85/1000 | Loss: 0.00002487
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002486
Iteration 90/1000 | Loss: 0.00002486
Iteration 91/1000 | Loss: 0.00002486
Iteration 92/1000 | Loss: 0.00002486
Iteration 93/1000 | Loss: 0.00002486
Iteration 94/1000 | Loss: 0.00002485
Iteration 95/1000 | Loss: 0.00002485
Iteration 96/1000 | Loss: 0.00002485
Iteration 97/1000 | Loss: 0.00002484
Iteration 98/1000 | Loss: 0.00002483
Iteration 99/1000 | Loss: 0.00002483
Iteration 100/1000 | Loss: 0.00002483
Iteration 101/1000 | Loss: 0.00002483
Iteration 102/1000 | Loss: 0.00002483
Iteration 103/1000 | Loss: 0.00002483
Iteration 104/1000 | Loss: 0.00002483
Iteration 105/1000 | Loss: 0.00002483
Iteration 106/1000 | Loss: 0.00002483
Iteration 107/1000 | Loss: 0.00002483
Iteration 108/1000 | Loss: 0.00002482
Iteration 109/1000 | Loss: 0.00002482
Iteration 110/1000 | Loss: 0.00002482
Iteration 111/1000 | Loss: 0.00002481
Iteration 112/1000 | Loss: 0.00002481
Iteration 113/1000 | Loss: 0.00002481
Iteration 114/1000 | Loss: 0.00002481
Iteration 115/1000 | Loss: 0.00002481
Iteration 116/1000 | Loss: 0.00002481
Iteration 117/1000 | Loss: 0.00002480
Iteration 118/1000 | Loss: 0.00002480
Iteration 119/1000 | Loss: 0.00002480
Iteration 120/1000 | Loss: 0.00002480
Iteration 121/1000 | Loss: 0.00002480
Iteration 122/1000 | Loss: 0.00002480
Iteration 123/1000 | Loss: 0.00002479
Iteration 124/1000 | Loss: 0.00002479
Iteration 125/1000 | Loss: 0.00002479
Iteration 126/1000 | Loss: 0.00002479
Iteration 127/1000 | Loss: 0.00002479
Iteration 128/1000 | Loss: 0.00002479
Iteration 129/1000 | Loss: 0.00002479
Iteration 130/1000 | Loss: 0.00002479
Iteration 131/1000 | Loss: 0.00002478
Iteration 132/1000 | Loss: 0.00002478
Iteration 133/1000 | Loss: 0.00002478
Iteration 134/1000 | Loss: 0.00002478
Iteration 135/1000 | Loss: 0.00002478
Iteration 136/1000 | Loss: 0.00002478
Iteration 137/1000 | Loss: 0.00002478
Iteration 138/1000 | Loss: 0.00002478
Iteration 139/1000 | Loss: 0.00002478
Iteration 140/1000 | Loss: 0.00002478
Iteration 141/1000 | Loss: 0.00002478
Iteration 142/1000 | Loss: 0.00002477
Iteration 143/1000 | Loss: 0.00002477
Iteration 144/1000 | Loss: 0.00002477
Iteration 145/1000 | Loss: 0.00002477
Iteration 146/1000 | Loss: 0.00002477
Iteration 147/1000 | Loss: 0.00002477
Iteration 148/1000 | Loss: 0.00002477
Iteration 149/1000 | Loss: 0.00002476
Iteration 150/1000 | Loss: 0.00002476
Iteration 151/1000 | Loss: 0.00002476
Iteration 152/1000 | Loss: 0.00002476
Iteration 153/1000 | Loss: 0.00002476
Iteration 154/1000 | Loss: 0.00002475
Iteration 155/1000 | Loss: 0.00002475
Iteration 156/1000 | Loss: 0.00002475
Iteration 157/1000 | Loss: 0.00002475
Iteration 158/1000 | Loss: 0.00002475
Iteration 159/1000 | Loss: 0.00002475
Iteration 160/1000 | Loss: 0.00002475
Iteration 161/1000 | Loss: 0.00002475
Iteration 162/1000 | Loss: 0.00002475
Iteration 163/1000 | Loss: 0.00002475
Iteration 164/1000 | Loss: 0.00002475
Iteration 165/1000 | Loss: 0.00002474
Iteration 166/1000 | Loss: 0.00002474
Iteration 167/1000 | Loss: 0.00002474
Iteration 168/1000 | Loss: 0.00002474
Iteration 169/1000 | Loss: 0.00002474
Iteration 170/1000 | Loss: 0.00002474
Iteration 171/1000 | Loss: 0.00002474
Iteration 172/1000 | Loss: 0.00002473
Iteration 173/1000 | Loss: 0.00002473
Iteration 174/1000 | Loss: 0.00002473
Iteration 175/1000 | Loss: 0.00002473
Iteration 176/1000 | Loss: 0.00002473
Iteration 177/1000 | Loss: 0.00002473
Iteration 178/1000 | Loss: 0.00002473
Iteration 179/1000 | Loss: 0.00002473
Iteration 180/1000 | Loss: 0.00002473
Iteration 181/1000 | Loss: 0.00002473
Iteration 182/1000 | Loss: 0.00002473
Iteration 183/1000 | Loss: 0.00002473
Iteration 184/1000 | Loss: 0.00002473
Iteration 185/1000 | Loss: 0.00002473
Iteration 186/1000 | Loss: 0.00002473
Iteration 187/1000 | Loss: 0.00002473
Iteration 188/1000 | Loss: 0.00002473
Iteration 189/1000 | Loss: 0.00002473
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [2.472595588187687e-05, 2.472595588187687e-05, 2.472595588187687e-05, 2.472595588187687e-05, 2.472595588187687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.472595588187687e-05

Optimization complete. Final v2v error: 4.131662845611572 mm

Highest mean error: 4.576881408691406 mm for frame 63

Lowest mean error: 3.6250648498535156 mm for frame 174

Saving results

Total time: 96.20083546638489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541598
Iteration 2/25 | Loss: 0.00143405
Iteration 3/25 | Loss: 0.00133857
Iteration 4/25 | Loss: 0.00132746
Iteration 5/25 | Loss: 0.00132429
Iteration 6/25 | Loss: 0.00132403
Iteration 7/25 | Loss: 0.00132403
Iteration 8/25 | Loss: 0.00132403
Iteration 9/25 | Loss: 0.00132403
Iteration 10/25 | Loss: 0.00132403
Iteration 11/25 | Loss: 0.00132403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013240333646535873, 0.0013240333646535873, 0.0013240333646535873, 0.0013240333646535873, 0.0013240333646535873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013240333646535873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80420375
Iteration 2/25 | Loss: 0.00132399
Iteration 3/25 | Loss: 0.00132398
Iteration 4/25 | Loss: 0.00132398
Iteration 5/25 | Loss: 0.00132398
Iteration 6/25 | Loss: 0.00132398
Iteration 7/25 | Loss: 0.00132398
Iteration 8/25 | Loss: 0.00132398
Iteration 9/25 | Loss: 0.00132398
Iteration 10/25 | Loss: 0.00132398
Iteration 11/25 | Loss: 0.00132398
Iteration 12/25 | Loss: 0.00132398
Iteration 13/25 | Loss: 0.00132398
Iteration 14/25 | Loss: 0.00132397
Iteration 15/25 | Loss: 0.00132397
Iteration 16/25 | Loss: 0.00132397
Iteration 17/25 | Loss: 0.00132397
Iteration 18/25 | Loss: 0.00132397
Iteration 19/25 | Loss: 0.00132397
Iteration 20/25 | Loss: 0.00132397
Iteration 21/25 | Loss: 0.00132397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013239748077467084, 0.0013239748077467084, 0.0013239748077467084, 0.0013239748077467084, 0.0013239748077467084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013239748077467084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132397
Iteration 2/1000 | Loss: 0.00004509
Iteration 3/1000 | Loss: 0.00002888
Iteration 4/1000 | Loss: 0.00002504
Iteration 5/1000 | Loss: 0.00002382
Iteration 6/1000 | Loss: 0.00002265
Iteration 7/1000 | Loss: 0.00002197
Iteration 8/1000 | Loss: 0.00002154
Iteration 9/1000 | Loss: 0.00002104
Iteration 10/1000 | Loss: 0.00002077
Iteration 11/1000 | Loss: 0.00002051
Iteration 12/1000 | Loss: 0.00002030
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00001988
Iteration 16/1000 | Loss: 0.00001986
Iteration 17/1000 | Loss: 0.00001985
Iteration 18/1000 | Loss: 0.00001985
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001979
Iteration 22/1000 | Loss: 0.00001978
Iteration 23/1000 | Loss: 0.00001977
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001976
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001968
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001967
Iteration 38/1000 | Loss: 0.00001967
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001965
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001964
Iteration 45/1000 | Loss: 0.00001964
Iteration 46/1000 | Loss: 0.00001963
Iteration 47/1000 | Loss: 0.00001963
Iteration 48/1000 | Loss: 0.00001962
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001957
Iteration 52/1000 | Loss: 0.00001957
Iteration 53/1000 | Loss: 0.00001957
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001955
Iteration 58/1000 | Loss: 0.00001955
Iteration 59/1000 | Loss: 0.00001954
Iteration 60/1000 | Loss: 0.00001954
Iteration 61/1000 | Loss: 0.00001954
Iteration 62/1000 | Loss: 0.00001953
Iteration 63/1000 | Loss: 0.00001953
Iteration 64/1000 | Loss: 0.00001953
Iteration 65/1000 | Loss: 0.00001953
Iteration 66/1000 | Loss: 0.00001952
Iteration 67/1000 | Loss: 0.00001952
Iteration 68/1000 | Loss: 0.00001952
Iteration 69/1000 | Loss: 0.00001952
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001950
Iteration 73/1000 | Loss: 0.00001950
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001950
Iteration 76/1000 | Loss: 0.00001950
Iteration 77/1000 | Loss: 0.00001950
Iteration 78/1000 | Loss: 0.00001950
Iteration 79/1000 | Loss: 0.00001950
Iteration 80/1000 | Loss: 0.00001950
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001949
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001947
Iteration 88/1000 | Loss: 0.00001947
Iteration 89/1000 | Loss: 0.00001947
Iteration 90/1000 | Loss: 0.00001946
Iteration 91/1000 | Loss: 0.00001946
Iteration 92/1000 | Loss: 0.00001946
Iteration 93/1000 | Loss: 0.00001945
Iteration 94/1000 | Loss: 0.00001945
Iteration 95/1000 | Loss: 0.00001945
Iteration 96/1000 | Loss: 0.00001945
Iteration 97/1000 | Loss: 0.00001945
Iteration 98/1000 | Loss: 0.00001945
Iteration 99/1000 | Loss: 0.00001945
Iteration 100/1000 | Loss: 0.00001945
Iteration 101/1000 | Loss: 0.00001945
Iteration 102/1000 | Loss: 0.00001945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [1.945025724126026e-05, 1.945025724126026e-05, 1.945025724126026e-05, 1.945025724126026e-05, 1.945025724126026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.945025724126026e-05

Optimization complete. Final v2v error: 3.635136604309082 mm

Highest mean error: 4.094426155090332 mm for frame 163

Lowest mean error: 3.295766830444336 mm for frame 221

Saving results

Total time: 114.2368655204773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954998
Iteration 2/25 | Loss: 0.00220701
Iteration 3/25 | Loss: 0.00143200
Iteration 4/25 | Loss: 0.00129138
Iteration 5/25 | Loss: 0.00128983
Iteration 6/25 | Loss: 0.00125880
Iteration 7/25 | Loss: 0.00126796
Iteration 8/25 | Loss: 0.00125245
Iteration 9/25 | Loss: 0.00125884
Iteration 10/25 | Loss: 0.00125650
Iteration 11/25 | Loss: 0.00124978
Iteration 12/25 | Loss: 0.00124731
Iteration 13/25 | Loss: 0.00124613
Iteration 14/25 | Loss: 0.00124723
Iteration 15/25 | Loss: 0.00124610
Iteration 16/25 | Loss: 0.00124513
Iteration 17/25 | Loss: 0.00124472
Iteration 18/25 | Loss: 0.00124443
Iteration 19/25 | Loss: 0.00124439
Iteration 20/25 | Loss: 0.00124439
Iteration 21/25 | Loss: 0.00124439
Iteration 22/25 | Loss: 0.00124439
Iteration 23/25 | Loss: 0.00124439
Iteration 24/25 | Loss: 0.00124439
Iteration 25/25 | Loss: 0.00124439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78920341
Iteration 2/25 | Loss: 0.00171238
Iteration 3/25 | Loss: 0.00166990
Iteration 4/25 | Loss: 0.00166990
Iteration 5/25 | Loss: 0.00166990
Iteration 6/25 | Loss: 0.00166990
Iteration 7/25 | Loss: 0.00166990
Iteration 8/25 | Loss: 0.00166990
Iteration 9/25 | Loss: 0.00166990
Iteration 10/25 | Loss: 0.00166989
Iteration 11/25 | Loss: 0.00166989
Iteration 12/25 | Loss: 0.00166989
Iteration 13/25 | Loss: 0.00166989
Iteration 14/25 | Loss: 0.00166989
Iteration 15/25 | Loss: 0.00166989
Iteration 16/25 | Loss: 0.00166989
Iteration 17/25 | Loss: 0.00166989
Iteration 18/25 | Loss: 0.00166989
Iteration 19/25 | Loss: 0.00166989
Iteration 20/25 | Loss: 0.00166989
Iteration 21/25 | Loss: 0.00166989
Iteration 22/25 | Loss: 0.00166989
Iteration 23/25 | Loss: 0.00166989
Iteration 24/25 | Loss: 0.00166989
Iteration 25/25 | Loss: 0.00166989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166989
Iteration 2/1000 | Loss: 0.00003944
Iteration 3/1000 | Loss: 0.00004916
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00011249
Iteration 6/1000 | Loss: 0.00002134
Iteration 7/1000 | Loss: 0.00008552
Iteration 8/1000 | Loss: 0.00003664
Iteration 9/1000 | Loss: 0.00001910
Iteration 10/1000 | Loss: 0.00001848
Iteration 11/1000 | Loss: 0.00003504
Iteration 12/1000 | Loss: 0.00001995
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002008
Iteration 16/1000 | Loss: 0.00001862
Iteration 17/1000 | Loss: 0.00006323
Iteration 18/1000 | Loss: 0.00065201
Iteration 19/1000 | Loss: 0.00004771
Iteration 20/1000 | Loss: 0.00003591
Iteration 21/1000 | Loss: 0.00004119
Iteration 22/1000 | Loss: 0.00069716
Iteration 23/1000 | Loss: 0.00145190
Iteration 24/1000 | Loss: 0.00271977
Iteration 25/1000 | Loss: 0.00080558
Iteration 26/1000 | Loss: 0.00002466
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00007933
Iteration 29/1000 | Loss: 0.00005626
Iteration 30/1000 | Loss: 0.00001693
Iteration 31/1000 | Loss: 0.00003882
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002923
Iteration 34/1000 | Loss: 0.00004900
Iteration 35/1000 | Loss: 0.00005627
Iteration 36/1000 | Loss: 0.00004168
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00005521
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001733
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001630
Iteration 43/1000 | Loss: 0.00001630
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001629
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001627
Iteration 59/1000 | Loss: 0.00001627
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00002850
Iteration 67/1000 | Loss: 0.00010762
Iteration 68/1000 | Loss: 0.00043747
Iteration 69/1000 | Loss: 0.00021811
Iteration 70/1000 | Loss: 0.00009463
Iteration 71/1000 | Loss: 0.00026387
Iteration 72/1000 | Loss: 0.00003203
Iteration 73/1000 | Loss: 0.00010043
Iteration 74/1000 | Loss: 0.00002548
Iteration 75/1000 | Loss: 0.00002321
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001625
Iteration 79/1000 | Loss: 0.00001625
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001618
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001616
Iteration 92/1000 | Loss: 0.00001616
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001614
Iteration 95/1000 | Loss: 0.00001614
Iteration 96/1000 | Loss: 0.00001614
Iteration 97/1000 | Loss: 0.00001614
Iteration 98/1000 | Loss: 0.00001614
Iteration 99/1000 | Loss: 0.00001614
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001613
Iteration 104/1000 | Loss: 0.00001613
Iteration 105/1000 | Loss: 0.00001613
Iteration 106/1000 | Loss: 0.00001613
Iteration 107/1000 | Loss: 0.00001613
Iteration 108/1000 | Loss: 0.00001613
Iteration 109/1000 | Loss: 0.00001613
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001611
Iteration 115/1000 | Loss: 0.00001611
Iteration 116/1000 | Loss: 0.00001611
Iteration 117/1000 | Loss: 0.00001611
Iteration 118/1000 | Loss: 0.00001610
Iteration 119/1000 | Loss: 0.00001610
Iteration 120/1000 | Loss: 0.00001610
Iteration 121/1000 | Loss: 0.00001610
Iteration 122/1000 | Loss: 0.00001610
Iteration 123/1000 | Loss: 0.00001610
Iteration 124/1000 | Loss: 0.00001610
Iteration 125/1000 | Loss: 0.00001610
Iteration 126/1000 | Loss: 0.00001610
Iteration 127/1000 | Loss: 0.00001610
Iteration 128/1000 | Loss: 0.00001610
Iteration 129/1000 | Loss: 0.00001610
Iteration 130/1000 | Loss: 0.00001610
Iteration 131/1000 | Loss: 0.00001610
Iteration 132/1000 | Loss: 0.00001610
Iteration 133/1000 | Loss: 0.00001610
Iteration 134/1000 | Loss: 0.00001610
Iteration 135/1000 | Loss: 0.00001610
Iteration 136/1000 | Loss: 0.00001610
Iteration 137/1000 | Loss: 0.00001610
Iteration 138/1000 | Loss: 0.00001610
Iteration 139/1000 | Loss: 0.00001610
Iteration 140/1000 | Loss: 0.00001610
Iteration 141/1000 | Loss: 0.00001610
Iteration 142/1000 | Loss: 0.00001610
Iteration 143/1000 | Loss: 0.00001610
Iteration 144/1000 | Loss: 0.00001610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.6096933904918842e-05, 1.6096933904918842e-05, 1.6096933904918842e-05, 1.6096933904918842e-05, 1.6096933904918842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6096933904918842e-05

Optimization complete. Final v2v error: 3.4862115383148193 mm

Highest mean error: 4.329326629638672 mm for frame 188

Lowest mean error: 3.1014325618743896 mm for frame 105

Saving results

Total time: 323.67602133750916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476695
Iteration 2/25 | Loss: 0.00135272
Iteration 3/25 | Loss: 0.00123356
Iteration 4/25 | Loss: 0.00121295
Iteration 5/25 | Loss: 0.00120631
Iteration 6/25 | Loss: 0.00120550
Iteration 7/25 | Loss: 0.00120550
Iteration 8/25 | Loss: 0.00120550
Iteration 9/25 | Loss: 0.00120550
Iteration 10/25 | Loss: 0.00120550
Iteration 11/25 | Loss: 0.00120550
Iteration 12/25 | Loss: 0.00120550
Iteration 13/25 | Loss: 0.00120550
Iteration 14/25 | Loss: 0.00120550
Iteration 15/25 | Loss: 0.00120550
Iteration 16/25 | Loss: 0.00120550
Iteration 17/25 | Loss: 0.00120550
Iteration 18/25 | Loss: 0.00120550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012054996332153678, 0.0012054996332153678, 0.0012054996332153678, 0.0012054996332153678, 0.0012054996332153678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012054996332153678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22957444
Iteration 2/25 | Loss: 0.00181689
Iteration 3/25 | Loss: 0.00181689
Iteration 4/25 | Loss: 0.00181689
Iteration 5/25 | Loss: 0.00181689
Iteration 6/25 | Loss: 0.00181689
Iteration 7/25 | Loss: 0.00181689
Iteration 8/25 | Loss: 0.00181689
Iteration 9/25 | Loss: 0.00181689
Iteration 10/25 | Loss: 0.00181689
Iteration 11/25 | Loss: 0.00181689
Iteration 12/25 | Loss: 0.00181689
Iteration 13/25 | Loss: 0.00181689
Iteration 14/25 | Loss: 0.00181689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0018168871756643057, 0.0018168871756643057, 0.0018168871756643057, 0.0018168871756643057, 0.0018168871756643057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018168871756643057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181689
Iteration 2/1000 | Loss: 0.00005422
Iteration 3/1000 | Loss: 0.00003649
Iteration 4/1000 | Loss: 0.00003025
Iteration 5/1000 | Loss: 0.00002887
Iteration 6/1000 | Loss: 0.00002790
Iteration 7/1000 | Loss: 0.00002728
Iteration 8/1000 | Loss: 0.00002694
Iteration 9/1000 | Loss: 0.00002681
Iteration 10/1000 | Loss: 0.00002666
Iteration 11/1000 | Loss: 0.00002645
Iteration 12/1000 | Loss: 0.00002631
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002627
Iteration 15/1000 | Loss: 0.00002617
Iteration 16/1000 | Loss: 0.00002600
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002592
Iteration 19/1000 | Loss: 0.00002579
Iteration 20/1000 | Loss: 0.00002572
Iteration 21/1000 | Loss: 0.00002571
Iteration 22/1000 | Loss: 0.00002570
Iteration 23/1000 | Loss: 0.00002570
Iteration 24/1000 | Loss: 0.00002570
Iteration 25/1000 | Loss: 0.00002570
Iteration 26/1000 | Loss: 0.00002568
Iteration 27/1000 | Loss: 0.00002566
Iteration 28/1000 | Loss: 0.00002565
Iteration 29/1000 | Loss: 0.00002563
Iteration 30/1000 | Loss: 0.00002562
Iteration 31/1000 | Loss: 0.00002559
Iteration 32/1000 | Loss: 0.00002558
Iteration 33/1000 | Loss: 0.00002557
Iteration 34/1000 | Loss: 0.00002557
Iteration 35/1000 | Loss: 0.00002557
Iteration 36/1000 | Loss: 0.00002556
Iteration 37/1000 | Loss: 0.00002556
Iteration 38/1000 | Loss: 0.00002555
Iteration 39/1000 | Loss: 0.00002554
Iteration 40/1000 | Loss: 0.00002554
Iteration 41/1000 | Loss: 0.00002553
Iteration 42/1000 | Loss: 0.00002553
Iteration 43/1000 | Loss: 0.00002553
Iteration 44/1000 | Loss: 0.00002552
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00002552
Iteration 47/1000 | Loss: 0.00002552
Iteration 48/1000 | Loss: 0.00002552
Iteration 49/1000 | Loss: 0.00002552
Iteration 50/1000 | Loss: 0.00002551
Iteration 51/1000 | Loss: 0.00002551
Iteration 52/1000 | Loss: 0.00002550
Iteration 53/1000 | Loss: 0.00002550
Iteration 54/1000 | Loss: 0.00002550
Iteration 55/1000 | Loss: 0.00002549
Iteration 56/1000 | Loss: 0.00002549
Iteration 57/1000 | Loss: 0.00002549
Iteration 58/1000 | Loss: 0.00002549
Iteration 59/1000 | Loss: 0.00002548
Iteration 60/1000 | Loss: 0.00002548
Iteration 61/1000 | Loss: 0.00002547
Iteration 62/1000 | Loss: 0.00002547
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002546
Iteration 65/1000 | Loss: 0.00002546
Iteration 66/1000 | Loss: 0.00002546
Iteration 67/1000 | Loss: 0.00002546
Iteration 68/1000 | Loss: 0.00002546
Iteration 69/1000 | Loss: 0.00002546
Iteration 70/1000 | Loss: 0.00002545
Iteration 71/1000 | Loss: 0.00002545
Iteration 72/1000 | Loss: 0.00002545
Iteration 73/1000 | Loss: 0.00002545
Iteration 74/1000 | Loss: 0.00002545
Iteration 75/1000 | Loss: 0.00002545
Iteration 76/1000 | Loss: 0.00002545
Iteration 77/1000 | Loss: 0.00002545
Iteration 78/1000 | Loss: 0.00002545
Iteration 79/1000 | Loss: 0.00002544
Iteration 80/1000 | Loss: 0.00002544
Iteration 81/1000 | Loss: 0.00002544
Iteration 82/1000 | Loss: 0.00002544
Iteration 83/1000 | Loss: 0.00002544
Iteration 84/1000 | Loss: 0.00002544
Iteration 85/1000 | Loss: 0.00002544
Iteration 86/1000 | Loss: 0.00002544
Iteration 87/1000 | Loss: 0.00002544
Iteration 88/1000 | Loss: 0.00002543
Iteration 89/1000 | Loss: 0.00002543
Iteration 90/1000 | Loss: 0.00002543
Iteration 91/1000 | Loss: 0.00002542
Iteration 92/1000 | Loss: 0.00002542
Iteration 93/1000 | Loss: 0.00002542
Iteration 94/1000 | Loss: 0.00002541
Iteration 95/1000 | Loss: 0.00002541
Iteration 96/1000 | Loss: 0.00002541
Iteration 97/1000 | Loss: 0.00002539
Iteration 98/1000 | Loss: 0.00002539
Iteration 99/1000 | Loss: 0.00002539
Iteration 100/1000 | Loss: 0.00002538
Iteration 101/1000 | Loss: 0.00002538
Iteration 102/1000 | Loss: 0.00002538
Iteration 103/1000 | Loss: 0.00002538
Iteration 104/1000 | Loss: 0.00002538
Iteration 105/1000 | Loss: 0.00002538
Iteration 106/1000 | Loss: 0.00002538
Iteration 107/1000 | Loss: 0.00002538
Iteration 108/1000 | Loss: 0.00002538
Iteration 109/1000 | Loss: 0.00002538
Iteration 110/1000 | Loss: 0.00002538
Iteration 111/1000 | Loss: 0.00002538
Iteration 112/1000 | Loss: 0.00002538
Iteration 113/1000 | Loss: 0.00002538
Iteration 114/1000 | Loss: 0.00002538
Iteration 115/1000 | Loss: 0.00002538
Iteration 116/1000 | Loss: 0.00002538
Iteration 117/1000 | Loss: 0.00002538
Iteration 118/1000 | Loss: 0.00002538
Iteration 119/1000 | Loss: 0.00002538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.537659020163119e-05, 2.537659020163119e-05, 2.537659020163119e-05, 2.537659020163119e-05, 2.537659020163119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.537659020163119e-05

Optimization complete. Final v2v error: 4.007646083831787 mm

Highest mean error: 4.3763227462768555 mm for frame 39

Lowest mean error: 3.7080323696136475 mm for frame 169

Saving results

Total time: 91.38740563392639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431174
Iteration 2/25 | Loss: 0.00125670
Iteration 3/25 | Loss: 0.00119404
Iteration 4/25 | Loss: 0.00118315
Iteration 5/25 | Loss: 0.00117929
Iteration 6/25 | Loss: 0.00117821
Iteration 7/25 | Loss: 0.00117821
Iteration 8/25 | Loss: 0.00117821
Iteration 9/25 | Loss: 0.00117821
Iteration 10/25 | Loss: 0.00117821
Iteration 11/25 | Loss: 0.00117821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378, 0.0011782108340412378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011782108340412378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.78862882
Iteration 2/25 | Loss: 0.00147616
Iteration 3/25 | Loss: 0.00147614
Iteration 4/25 | Loss: 0.00147614
Iteration 5/25 | Loss: 0.00147614
Iteration 6/25 | Loss: 0.00147614
Iteration 7/25 | Loss: 0.00147614
Iteration 8/25 | Loss: 0.00147614
Iteration 9/25 | Loss: 0.00147614
Iteration 10/25 | Loss: 0.00147614
Iteration 11/25 | Loss: 0.00147614
Iteration 12/25 | Loss: 0.00147614
Iteration 13/25 | Loss: 0.00147614
Iteration 14/25 | Loss: 0.00147614
Iteration 15/25 | Loss: 0.00147614
Iteration 16/25 | Loss: 0.00147614
Iteration 17/25 | Loss: 0.00147614
Iteration 18/25 | Loss: 0.00147614
Iteration 19/25 | Loss: 0.00147614
Iteration 20/25 | Loss: 0.00147614
Iteration 21/25 | Loss: 0.00147614
Iteration 22/25 | Loss: 0.00147614
Iteration 23/25 | Loss: 0.00147614
Iteration 24/25 | Loss: 0.00147614
Iteration 25/25 | Loss: 0.00147614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147614
Iteration 2/1000 | Loss: 0.00002989
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001640
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001422
Iteration 7/1000 | Loss: 0.00001372
Iteration 8/1000 | Loss: 0.00001325
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001269
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001214
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001182
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001179
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001174
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001161
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001156
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001155
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001153
Iteration 46/1000 | Loss: 0.00001152
Iteration 47/1000 | Loss: 0.00001152
Iteration 48/1000 | Loss: 0.00001151
Iteration 49/1000 | Loss: 0.00001150
Iteration 50/1000 | Loss: 0.00001150
Iteration 51/1000 | Loss: 0.00001150
Iteration 52/1000 | Loss: 0.00001149
Iteration 53/1000 | Loss: 0.00001149
Iteration 54/1000 | Loss: 0.00001149
Iteration 55/1000 | Loss: 0.00001148
Iteration 56/1000 | Loss: 0.00001148
Iteration 57/1000 | Loss: 0.00001148
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001147
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001145
Iteration 70/1000 | Loss: 0.00001145
Iteration 71/1000 | Loss: 0.00001145
Iteration 72/1000 | Loss: 0.00001145
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001145
Iteration 79/1000 | Loss: 0.00001145
Iteration 80/1000 | Loss: 0.00001144
Iteration 81/1000 | Loss: 0.00001144
Iteration 82/1000 | Loss: 0.00001144
Iteration 83/1000 | Loss: 0.00001144
Iteration 84/1000 | Loss: 0.00001144
Iteration 85/1000 | Loss: 0.00001144
Iteration 86/1000 | Loss: 0.00001144
Iteration 87/1000 | Loss: 0.00001144
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001142
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001141
Iteration 107/1000 | Loss: 0.00001141
Iteration 108/1000 | Loss: 0.00001141
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001139
Iteration 113/1000 | Loss: 0.00001139
Iteration 114/1000 | Loss: 0.00001139
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Iteration 130/1000 | Loss: 0.00001139
Iteration 131/1000 | Loss: 0.00001139
Iteration 132/1000 | Loss: 0.00001139
Iteration 133/1000 | Loss: 0.00001139
Iteration 134/1000 | Loss: 0.00001139
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.1388272469048388e-05, 1.1388272469048388e-05, 1.1388272469048388e-05, 1.1388272469048388e-05, 1.1388272469048388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1388272469048388e-05

Optimization complete. Final v2v error: 2.894338369369507 mm

Highest mean error: 3.218716859817505 mm for frame 63

Lowest mean error: 2.5172739028930664 mm for frame 129

Saving results

Total time: 88.06974625587463
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972397
Iteration 2/25 | Loss: 0.00972397
Iteration 3/25 | Loss: 0.00972396
Iteration 4/25 | Loss: 0.00327689
Iteration 5/25 | Loss: 0.00220573
Iteration 6/25 | Loss: 0.00203479
Iteration 7/25 | Loss: 0.00203472
Iteration 8/25 | Loss: 0.00187601
Iteration 9/25 | Loss: 0.00181815
Iteration 10/25 | Loss: 0.00172870
Iteration 11/25 | Loss: 0.00166157
Iteration 12/25 | Loss: 0.00162334
Iteration 13/25 | Loss: 0.00159396
Iteration 14/25 | Loss: 0.00159019
Iteration 15/25 | Loss: 0.00158029
Iteration 16/25 | Loss: 0.00156891
Iteration 17/25 | Loss: 0.00156347
Iteration 18/25 | Loss: 0.00155003
Iteration 19/25 | Loss: 0.00154722
Iteration 20/25 | Loss: 0.00154297
Iteration 21/25 | Loss: 0.00153922
Iteration 22/25 | Loss: 0.00153619
Iteration 23/25 | Loss: 0.00153319
Iteration 24/25 | Loss: 0.00153434
Iteration 25/25 | Loss: 0.00153089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36742496
Iteration 2/25 | Loss: 0.00480940
Iteration 3/25 | Loss: 0.00298808
Iteration 4/25 | Loss: 0.00298808
Iteration 5/25 | Loss: 0.00298808
Iteration 6/25 | Loss: 0.00298808
Iteration 7/25 | Loss: 0.00298808
Iteration 8/25 | Loss: 0.00298808
Iteration 9/25 | Loss: 0.00298808
Iteration 10/25 | Loss: 0.00298808
Iteration 11/25 | Loss: 0.00298808
Iteration 12/25 | Loss: 0.00298808
Iteration 13/25 | Loss: 0.00298808
Iteration 14/25 | Loss: 0.00298808
Iteration 15/25 | Loss: 0.00298808
Iteration 16/25 | Loss: 0.00298808
Iteration 17/25 | Loss: 0.00298808
Iteration 18/25 | Loss: 0.00298808
Iteration 19/25 | Loss: 0.00298808
Iteration 20/25 | Loss: 0.00298808
Iteration 21/25 | Loss: 0.00298808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0029880767688155174, 0.0029880767688155174, 0.0029880767688155174, 0.0029880767688155174, 0.0029880767688155174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029880767688155174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298808
Iteration 2/1000 | Loss: 0.00319602
Iteration 3/1000 | Loss: 0.00249980
Iteration 4/1000 | Loss: 0.00311301
Iteration 5/1000 | Loss: 0.00200474
Iteration 6/1000 | Loss: 0.00074151
Iteration 7/1000 | Loss: 0.00112371
Iteration 8/1000 | Loss: 0.00068165
Iteration 9/1000 | Loss: 0.00065291
Iteration 10/1000 | Loss: 0.00065269
Iteration 11/1000 | Loss: 0.00124739
Iteration 12/1000 | Loss: 0.00060602
Iteration 13/1000 | Loss: 0.00077400
Iteration 14/1000 | Loss: 0.00065270
Iteration 15/1000 | Loss: 0.00057508
Iteration 16/1000 | Loss: 0.00052655
Iteration 17/1000 | Loss: 0.00025045
Iteration 18/1000 | Loss: 0.00021372
Iteration 19/1000 | Loss: 0.00097106
Iteration 20/1000 | Loss: 0.00109396
Iteration 21/1000 | Loss: 0.00085995
Iteration 22/1000 | Loss: 0.00033348
Iteration 23/1000 | Loss: 0.00049475
Iteration 24/1000 | Loss: 0.00042008
Iteration 25/1000 | Loss: 0.00033085
Iteration 26/1000 | Loss: 0.00039333
Iteration 27/1000 | Loss: 0.00034120
Iteration 28/1000 | Loss: 0.00031818
Iteration 29/1000 | Loss: 0.00064133
Iteration 30/1000 | Loss: 0.00111787
Iteration 31/1000 | Loss: 0.00032133
Iteration 32/1000 | Loss: 0.00033043
Iteration 33/1000 | Loss: 0.00033012
Iteration 34/1000 | Loss: 0.00034290
Iteration 35/1000 | Loss: 0.00033303
Iteration 36/1000 | Loss: 0.00049731
Iteration 37/1000 | Loss: 0.00078338
Iteration 38/1000 | Loss: 0.00037064
Iteration 39/1000 | Loss: 0.00036430
Iteration 40/1000 | Loss: 0.00077475
Iteration 41/1000 | Loss: 0.00056369
Iteration 42/1000 | Loss: 0.00033692
Iteration 43/1000 | Loss: 0.00030018
Iteration 44/1000 | Loss: 0.00035413
Iteration 45/1000 | Loss: 0.00035652
Iteration 46/1000 | Loss: 0.00037488
Iteration 47/1000 | Loss: 0.00036187
Iteration 48/1000 | Loss: 0.00057632
Iteration 49/1000 | Loss: 0.00023174
Iteration 50/1000 | Loss: 0.00026500
Iteration 51/1000 | Loss: 0.00036119
Iteration 52/1000 | Loss: 0.00047908
Iteration 53/1000 | Loss: 0.00067674
Iteration 54/1000 | Loss: 0.00035957
Iteration 55/1000 | Loss: 0.00028228
Iteration 56/1000 | Loss: 0.00059921
Iteration 57/1000 | Loss: 0.00040893
Iteration 58/1000 | Loss: 0.00056577
Iteration 59/1000 | Loss: 0.00059585
Iteration 60/1000 | Loss: 0.00046562
Iteration 61/1000 | Loss: 0.00091752
Iteration 62/1000 | Loss: 0.00047959
Iteration 63/1000 | Loss: 0.00063361
Iteration 64/1000 | Loss: 0.00056483
Iteration 65/1000 | Loss: 0.00049767
Iteration 66/1000 | Loss: 0.00034028
Iteration 67/1000 | Loss: 0.00029277
Iteration 68/1000 | Loss: 0.00031255
Iteration 69/1000 | Loss: 0.00015150
Iteration 70/1000 | Loss: 0.00022599
Iteration 71/1000 | Loss: 0.00039140
Iteration 72/1000 | Loss: 0.00025613
Iteration 73/1000 | Loss: 0.00019262
Iteration 74/1000 | Loss: 0.00019717
Iteration 75/1000 | Loss: 0.00040703
Iteration 76/1000 | Loss: 0.00033594
Iteration 77/1000 | Loss: 0.00021827
Iteration 78/1000 | Loss: 0.00027834
Iteration 79/1000 | Loss: 0.00029299
Iteration 80/1000 | Loss: 0.00031551
Iteration 81/1000 | Loss: 0.00101307
Iteration 82/1000 | Loss: 0.00124978
Iteration 83/1000 | Loss: 0.00114814
Iteration 84/1000 | Loss: 0.00051730
Iteration 85/1000 | Loss: 0.00066169
Iteration 86/1000 | Loss: 0.00093868
Iteration 87/1000 | Loss: 0.00076403
Iteration 88/1000 | Loss: 0.00089963
Iteration 89/1000 | Loss: 0.00065411
Iteration 90/1000 | Loss: 0.00065186
Iteration 91/1000 | Loss: 0.00052317
Iteration 92/1000 | Loss: 0.00059972
Iteration 93/1000 | Loss: 0.00031028
Iteration 94/1000 | Loss: 0.00022285
Iteration 95/1000 | Loss: 0.00018287
Iteration 96/1000 | Loss: 0.00019892
Iteration 97/1000 | Loss: 0.00015703
Iteration 98/1000 | Loss: 0.00034004
Iteration 99/1000 | Loss: 0.00030423
Iteration 100/1000 | Loss: 0.00015342
Iteration 101/1000 | Loss: 0.00047957
Iteration 102/1000 | Loss: 0.00030522
Iteration 103/1000 | Loss: 0.00052542
Iteration 104/1000 | Loss: 0.00024637
Iteration 105/1000 | Loss: 0.00029634
Iteration 106/1000 | Loss: 0.00050452
Iteration 107/1000 | Loss: 0.00037937
Iteration 108/1000 | Loss: 0.00038375
Iteration 109/1000 | Loss: 0.00050822
Iteration 110/1000 | Loss: 0.00030377
Iteration 111/1000 | Loss: 0.00035863
Iteration 112/1000 | Loss: 0.00033544
Iteration 113/1000 | Loss: 0.00041289
Iteration 114/1000 | Loss: 0.00054844
Iteration 115/1000 | Loss: 0.00031850
Iteration 116/1000 | Loss: 0.00039277
Iteration 117/1000 | Loss: 0.00023758
Iteration 118/1000 | Loss: 0.00026015
Iteration 119/1000 | Loss: 0.00031306
Iteration 120/1000 | Loss: 0.00050896
Iteration 121/1000 | Loss: 0.00046428
Iteration 122/1000 | Loss: 0.00050066
Iteration 123/1000 | Loss: 0.00041698
Iteration 124/1000 | Loss: 0.00039299
Iteration 125/1000 | Loss: 0.00035500
Iteration 126/1000 | Loss: 0.00051919
Iteration 127/1000 | Loss: 0.00114650
Iteration 128/1000 | Loss: 0.00045201
Iteration 129/1000 | Loss: 0.00032507
Iteration 130/1000 | Loss: 0.00035224
Iteration 131/1000 | Loss: 0.00032938
Iteration 132/1000 | Loss: 0.00030955
Iteration 133/1000 | Loss: 0.00066780
Iteration 134/1000 | Loss: 0.00054702
Iteration 135/1000 | Loss: 0.00023630
Iteration 136/1000 | Loss: 0.00034295
Iteration 137/1000 | Loss: 0.00071245
Iteration 138/1000 | Loss: 0.00047455
Iteration 139/1000 | Loss: 0.00049857
Iteration 140/1000 | Loss: 0.00058661
Iteration 141/1000 | Loss: 0.00051484
Iteration 142/1000 | Loss: 0.00034991
Iteration 143/1000 | Loss: 0.00036729
Iteration 144/1000 | Loss: 0.00026392
Iteration 145/1000 | Loss: 0.00117351
Iteration 146/1000 | Loss: 0.00095753
Iteration 147/1000 | Loss: 0.00047286
Iteration 148/1000 | Loss: 0.00042279
Iteration 149/1000 | Loss: 0.00047062
Iteration 150/1000 | Loss: 0.00021760
Iteration 151/1000 | Loss: 0.00027725
Iteration 152/1000 | Loss: 0.00026014
Iteration 153/1000 | Loss: 0.00041311
Iteration 154/1000 | Loss: 0.00027402
Iteration 155/1000 | Loss: 0.00031923
Iteration 156/1000 | Loss: 0.00039635
Iteration 157/1000 | Loss: 0.00044785
Iteration 158/1000 | Loss: 0.00022433
Iteration 159/1000 | Loss: 0.00043631
Iteration 160/1000 | Loss: 0.00048021
Iteration 161/1000 | Loss: 0.00037175
Iteration 162/1000 | Loss: 0.00037814
Iteration 163/1000 | Loss: 0.00039899
Iteration 164/1000 | Loss: 0.00061800
Iteration 165/1000 | Loss: 0.00042660
Iteration 166/1000 | Loss: 0.00046941
Iteration 167/1000 | Loss: 0.00060901
Iteration 168/1000 | Loss: 0.00043331
Iteration 169/1000 | Loss: 0.00051993
Iteration 170/1000 | Loss: 0.00033275
Iteration 171/1000 | Loss: 0.00033923
Iteration 172/1000 | Loss: 0.00030533
Iteration 173/1000 | Loss: 0.00035362
Iteration 174/1000 | Loss: 0.00018987
Iteration 175/1000 | Loss: 0.00021662
Iteration 176/1000 | Loss: 0.00052108
Iteration 177/1000 | Loss: 0.00027866
Iteration 178/1000 | Loss: 0.00029528
Iteration 179/1000 | Loss: 0.00025609
Iteration 180/1000 | Loss: 0.00034257
Iteration 181/1000 | Loss: 0.00025075
Iteration 182/1000 | Loss: 0.00032388
Iteration 183/1000 | Loss: 0.00024871
Iteration 184/1000 | Loss: 0.00052123
Iteration 185/1000 | Loss: 0.00020356
Iteration 186/1000 | Loss: 0.00017658
Iteration 187/1000 | Loss: 0.00018658
Iteration 188/1000 | Loss: 0.00012935
Iteration 189/1000 | Loss: 0.00013864
Iteration 190/1000 | Loss: 0.00016810
Iteration 191/1000 | Loss: 0.00016302
Iteration 192/1000 | Loss: 0.00022529
Iteration 193/1000 | Loss: 0.00020497
Iteration 194/1000 | Loss: 0.00022082
Iteration 195/1000 | Loss: 0.00018068
Iteration 196/1000 | Loss: 0.00018920
Iteration 197/1000 | Loss: 0.00017289
Iteration 198/1000 | Loss: 0.00025836
Iteration 199/1000 | Loss: 0.00048485
Iteration 200/1000 | Loss: 0.00024687
Iteration 201/1000 | Loss: 0.00029294
Iteration 202/1000 | Loss: 0.00032258
Iteration 203/1000 | Loss: 0.00028631
Iteration 204/1000 | Loss: 0.00032054
Iteration 205/1000 | Loss: 0.00028819
Iteration 206/1000 | Loss: 0.00024778
Iteration 207/1000 | Loss: 0.00070936
Iteration 208/1000 | Loss: 0.00031537
Iteration 209/1000 | Loss: 0.00044342
Iteration 210/1000 | Loss: 0.00059888
Iteration 211/1000 | Loss: 0.00088929
Iteration 212/1000 | Loss: 0.00034601
Iteration 213/1000 | Loss: 0.00041512
Iteration 214/1000 | Loss: 0.00036157
Iteration 215/1000 | Loss: 0.00034271
Iteration 216/1000 | Loss: 0.00034309
Iteration 217/1000 | Loss: 0.00018686
Iteration 218/1000 | Loss: 0.00019732
Iteration 219/1000 | Loss: 0.00024868
Iteration 220/1000 | Loss: 0.00020256
Iteration 221/1000 | Loss: 0.00022535
Iteration 222/1000 | Loss: 0.00031694
Iteration 223/1000 | Loss: 0.00022591
Iteration 224/1000 | Loss: 0.00032569
Iteration 225/1000 | Loss: 0.00032025
Iteration 226/1000 | Loss: 0.00027773
Iteration 227/1000 | Loss: 0.00017556
Iteration 228/1000 | Loss: 0.00013575
Iteration 229/1000 | Loss: 0.00017658
Iteration 230/1000 | Loss: 0.00039168
Iteration 231/1000 | Loss: 0.00051076
Iteration 232/1000 | Loss: 0.00062490
Iteration 233/1000 | Loss: 0.00047638
Iteration 234/1000 | Loss: 0.00019093
Iteration 235/1000 | Loss: 0.00022266
Iteration 236/1000 | Loss: 0.00054189
Iteration 237/1000 | Loss: 0.00034007
Iteration 238/1000 | Loss: 0.00020597
Iteration 239/1000 | Loss: 0.00019670
Iteration 240/1000 | Loss: 0.00076393
Iteration 241/1000 | Loss: 0.00063660
Iteration 242/1000 | Loss: 0.00065641
Iteration 243/1000 | Loss: 0.00056656
Iteration 244/1000 | Loss: 0.00029455
Iteration 245/1000 | Loss: 0.00034427
Iteration 246/1000 | Loss: 0.00015423
Iteration 247/1000 | Loss: 0.00044369
Iteration 248/1000 | Loss: 0.00027323
Iteration 249/1000 | Loss: 0.00025408
Iteration 250/1000 | Loss: 0.00017452
Iteration 251/1000 | Loss: 0.00040680
Iteration 252/1000 | Loss: 0.00065350
Iteration 253/1000 | Loss: 0.00074954
Iteration 254/1000 | Loss: 0.00052246
Iteration 255/1000 | Loss: 0.00017385
Iteration 256/1000 | Loss: 0.00023588
Iteration 257/1000 | Loss: 0.00026492
Iteration 258/1000 | Loss: 0.00032360
Iteration 259/1000 | Loss: 0.00024788
Iteration 260/1000 | Loss: 0.00039591
Iteration 261/1000 | Loss: 0.00034012
Iteration 262/1000 | Loss: 0.00027342
Iteration 263/1000 | Loss: 0.00015534
Iteration 264/1000 | Loss: 0.00015983
Iteration 265/1000 | Loss: 0.00015896
Iteration 266/1000 | Loss: 0.00022700
Iteration 267/1000 | Loss: 0.00017527
Iteration 268/1000 | Loss: 0.00012924
Iteration 269/1000 | Loss: 0.00017002
Iteration 270/1000 | Loss: 0.00018646
Iteration 271/1000 | Loss: 0.00023432
Iteration 272/1000 | Loss: 0.00026697
Iteration 273/1000 | Loss: 0.00022900
Iteration 274/1000 | Loss: 0.00018632
Iteration 275/1000 | Loss: 0.00021334
Iteration 276/1000 | Loss: 0.00019368
Iteration 277/1000 | Loss: 0.00020566
Iteration 278/1000 | Loss: 0.00034030
Iteration 279/1000 | Loss: 0.00039196
Iteration 280/1000 | Loss: 0.00016375
Iteration 281/1000 | Loss: 0.00019405
Iteration 282/1000 | Loss: 0.00020095
Iteration 283/1000 | Loss: 0.00027044
Iteration 284/1000 | Loss: 0.00023826
Iteration 285/1000 | Loss: 0.00035132
Iteration 286/1000 | Loss: 0.00018599
Iteration 287/1000 | Loss: 0.00015450
Iteration 288/1000 | Loss: 0.00034871
Iteration 289/1000 | Loss: 0.00021234
Iteration 290/1000 | Loss: 0.00020076
Iteration 291/1000 | Loss: 0.00024882
Iteration 292/1000 | Loss: 0.00025503
Iteration 293/1000 | Loss: 0.00044482
Iteration 294/1000 | Loss: 0.00030150
Iteration 295/1000 | Loss: 0.00047112
Iteration 296/1000 | Loss: 0.00024210
Iteration 297/1000 | Loss: 0.00027318
Iteration 298/1000 | Loss: 0.00019721
Iteration 299/1000 | Loss: 0.00016822
Iteration 300/1000 | Loss: 0.00017597
Iteration 301/1000 | Loss: 0.00017760
Iteration 302/1000 | Loss: 0.00017557
Iteration 303/1000 | Loss: 0.00018245
Iteration 304/1000 | Loss: 0.00028958
Iteration 305/1000 | Loss: 0.00034518
Iteration 306/1000 | Loss: 0.00058812
Iteration 307/1000 | Loss: 0.00027731
Iteration 308/1000 | Loss: 0.00018817
Iteration 309/1000 | Loss: 0.00018602
Iteration 310/1000 | Loss: 0.00023205
Iteration 311/1000 | Loss: 0.00037451
Iteration 312/1000 | Loss: 0.00018785
Iteration 313/1000 | Loss: 0.00024706
Iteration 314/1000 | Loss: 0.00020514
Iteration 315/1000 | Loss: 0.00017745
Iteration 316/1000 | Loss: 0.00021496
Iteration 317/1000 | Loss: 0.00025703
Iteration 318/1000 | Loss: 0.00027987
Iteration 319/1000 | Loss: 0.00019585
Iteration 320/1000 | Loss: 0.00016605
Iteration 321/1000 | Loss: 0.00023891
Iteration 322/1000 | Loss: 0.00020688
Iteration 323/1000 | Loss: 0.00024577
Iteration 324/1000 | Loss: 0.00022945
Iteration 325/1000 | Loss: 0.00022987
Iteration 326/1000 | Loss: 0.00022435
Iteration 327/1000 | Loss: 0.00024919
Iteration 328/1000 | Loss: 0.00022531
Iteration 329/1000 | Loss: 0.00025920
Iteration 330/1000 | Loss: 0.00053383
Iteration 331/1000 | Loss: 0.00028857
Iteration 332/1000 | Loss: 0.00027989
Iteration 333/1000 | Loss: 0.00018456
Iteration 334/1000 | Loss: 0.00012762
Iteration 335/1000 | Loss: 0.00026197
Iteration 336/1000 | Loss: 0.00022596
Iteration 337/1000 | Loss: 0.00037064
Iteration 338/1000 | Loss: 0.00020088
Iteration 339/1000 | Loss: 0.00022571
Iteration 340/1000 | Loss: 0.00013818
Iteration 341/1000 | Loss: 0.00017748
Iteration 342/1000 | Loss: 0.00012954
Iteration 343/1000 | Loss: 0.00022812
Iteration 344/1000 | Loss: 0.00035652
Iteration 345/1000 | Loss: 0.00023535
Iteration 346/1000 | Loss: 0.00045196
Iteration 347/1000 | Loss: 0.00022717
Iteration 348/1000 | Loss: 0.00023600
Iteration 349/1000 | Loss: 0.00026755
Iteration 350/1000 | Loss: 0.00030096
Iteration 351/1000 | Loss: 0.00018613
Iteration 352/1000 | Loss: 0.00033817
Iteration 353/1000 | Loss: 0.00026273
Iteration 354/1000 | Loss: 0.00024614
Iteration 355/1000 | Loss: 0.00019761
Iteration 356/1000 | Loss: 0.00016759
Iteration 357/1000 | Loss: 0.00025206
Iteration 358/1000 | Loss: 0.00020741
Iteration 359/1000 | Loss: 0.00027043
Iteration 360/1000 | Loss: 0.00021589
Iteration 361/1000 | Loss: 0.00046151
Iteration 362/1000 | Loss: 0.00025896
Iteration 363/1000 | Loss: 0.00035440
Iteration 364/1000 | Loss: 0.00018030
Iteration 365/1000 | Loss: 0.00013671
Iteration 366/1000 | Loss: 0.00018997
Iteration 367/1000 | Loss: 0.00024055
Iteration 368/1000 | Loss: 0.00021389
Iteration 369/1000 | Loss: 0.00016885
Iteration 370/1000 | Loss: 0.00023462
Iteration 371/1000 | Loss: 0.00018777
Iteration 372/1000 | Loss: 0.00020597
Iteration 373/1000 | Loss: 0.00039567
Iteration 374/1000 | Loss: 0.00031193
Iteration 375/1000 | Loss: 0.00019422
Iteration 376/1000 | Loss: 0.00021277
Iteration 377/1000 | Loss: 0.00017895
Iteration 378/1000 | Loss: 0.00015219
Iteration 379/1000 | Loss: 0.00021136
Iteration 380/1000 | Loss: 0.00017150
Iteration 381/1000 | Loss: 0.00016912
Iteration 382/1000 | Loss: 0.00017553
Iteration 383/1000 | Loss: 0.00016895
Iteration 384/1000 | Loss: 0.00025767
Iteration 385/1000 | Loss: 0.00021466
Iteration 386/1000 | Loss: 0.00016737
Iteration 387/1000 | Loss: 0.00017285
Iteration 388/1000 | Loss: 0.00015570
Iteration 389/1000 | Loss: 0.00020102
Iteration 390/1000 | Loss: 0.00018291
Iteration 391/1000 | Loss: 0.00020958
Iteration 392/1000 | Loss: 0.00020251
Iteration 393/1000 | Loss: 0.00021415
Iteration 394/1000 | Loss: 0.00020022
Iteration 395/1000 | Loss: 0.00020920
Iteration 396/1000 | Loss: 0.00018150
Iteration 397/1000 | Loss: 0.00017504
Iteration 398/1000 | Loss: 0.00015524
Iteration 399/1000 | Loss: 0.00031317
Iteration 400/1000 | Loss: 0.00053085
Iteration 401/1000 | Loss: 0.00047423
Iteration 402/1000 | Loss: 0.00043010
Iteration 403/1000 | Loss: 0.00043943
Iteration 404/1000 | Loss: 0.00039881
Iteration 405/1000 | Loss: 0.00023701
Iteration 406/1000 | Loss: 0.00034420
Iteration 407/1000 | Loss: 0.00027408
Iteration 408/1000 | Loss: 0.00021562
Iteration 409/1000 | Loss: 0.00024002
Iteration 410/1000 | Loss: 0.00034968
Iteration 411/1000 | Loss: 0.00044185
Iteration 412/1000 | Loss: 0.00030080
Iteration 413/1000 | Loss: 0.00015243
Iteration 414/1000 | Loss: 0.00012260
Iteration 415/1000 | Loss: 0.00038083
Iteration 416/1000 | Loss: 0.00045976
Iteration 417/1000 | Loss: 0.00036555
Iteration 418/1000 | Loss: 0.00012781
Iteration 419/1000 | Loss: 0.00014549
Iteration 420/1000 | Loss: 0.00023180
Iteration 421/1000 | Loss: 0.00034309
Iteration 422/1000 | Loss: 0.00042905
Iteration 423/1000 | Loss: 0.00019019
Iteration 424/1000 | Loss: 0.00017868
Iteration 425/1000 | Loss: 0.00013913
Iteration 426/1000 | Loss: 0.00012391
Iteration 427/1000 | Loss: 0.00012068
Iteration 428/1000 | Loss: 0.00022819
Iteration 429/1000 | Loss: 0.00035463
Iteration 430/1000 | Loss: 0.00063199
Iteration 431/1000 | Loss: 0.00026360
Iteration 432/1000 | Loss: 0.00013120
Iteration 433/1000 | Loss: 0.00017554
Iteration 434/1000 | Loss: 0.00014882
Iteration 435/1000 | Loss: 0.00023986
Iteration 436/1000 | Loss: 0.00031976
Iteration 437/1000 | Loss: 0.00035007
Iteration 438/1000 | Loss: 0.00011855
Iteration 439/1000 | Loss: 0.00018473
Iteration 440/1000 | Loss: 0.00019769
Iteration 441/1000 | Loss: 0.00027633
Iteration 442/1000 | Loss: 0.00033990
Iteration 443/1000 | Loss: 0.00018804
Iteration 444/1000 | Loss: 0.00013973
Iteration 445/1000 | Loss: 0.00019789
Iteration 446/1000 | Loss: 0.00015301
Iteration 447/1000 | Loss: 0.00019436
Iteration 448/1000 | Loss: 0.00017053
Iteration 449/1000 | Loss: 0.00013043
Iteration 450/1000 | Loss: 0.00012437
Iteration 451/1000 | Loss: 0.00018577
Iteration 452/1000 | Loss: 0.00028985
Iteration 453/1000 | Loss: 0.00011222
Iteration 454/1000 | Loss: 0.00011171
Iteration 455/1000 | Loss: 0.00011126
Iteration 456/1000 | Loss: 0.00091614
Iteration 457/1000 | Loss: 0.00051028
Iteration 458/1000 | Loss: 0.00013853
Iteration 459/1000 | Loss: 0.00020551
Iteration 460/1000 | Loss: 0.00016017
Iteration 461/1000 | Loss: 0.00049022
Iteration 462/1000 | Loss: 0.00023492
Iteration 463/1000 | Loss: 0.00027171
Iteration 464/1000 | Loss: 0.00088853
Iteration 465/1000 | Loss: 0.00118802
Iteration 466/1000 | Loss: 0.00046636
Iteration 467/1000 | Loss: 0.00041493
Iteration 468/1000 | Loss: 0.00036762
Iteration 469/1000 | Loss: 0.00023853
Iteration 470/1000 | Loss: 0.00019348
Iteration 471/1000 | Loss: 0.00011692
Iteration 472/1000 | Loss: 0.00011406
Iteration 473/1000 | Loss: 0.00042916
Iteration 474/1000 | Loss: 0.00110225
Iteration 475/1000 | Loss: 0.00076418
Iteration 476/1000 | Loss: 0.00012679
Iteration 477/1000 | Loss: 0.00011861
Iteration 478/1000 | Loss: 0.00011518
Iteration 479/1000 | Loss: 0.00010946
Iteration 480/1000 | Loss: 0.00010586
Iteration 481/1000 | Loss: 0.00031071
Iteration 482/1000 | Loss: 0.00046590
Iteration 483/1000 | Loss: 0.00010561
Iteration 484/1000 | Loss: 0.00010246
Iteration 485/1000 | Loss: 0.00010846
Iteration 486/1000 | Loss: 0.00010043
Iteration 487/1000 | Loss: 0.00009894
Iteration 488/1000 | Loss: 0.00009797
Iteration 489/1000 | Loss: 0.00009747
Iteration 490/1000 | Loss: 0.00009695
Iteration 491/1000 | Loss: 0.00009658
Iteration 492/1000 | Loss: 0.00009632
Iteration 493/1000 | Loss: 0.00009604
Iteration 494/1000 | Loss: 0.00009581
Iteration 495/1000 | Loss: 0.00009571
Iteration 496/1000 | Loss: 0.00009571
Iteration 497/1000 | Loss: 0.00009570
Iteration 498/1000 | Loss: 0.00009553
Iteration 499/1000 | Loss: 0.00009552
Iteration 500/1000 | Loss: 0.00009546
Iteration 501/1000 | Loss: 0.00009541
Iteration 502/1000 | Loss: 0.00009536
Iteration 503/1000 | Loss: 0.00009532
Iteration 504/1000 | Loss: 0.00009531
Iteration 505/1000 | Loss: 0.00009523
Iteration 506/1000 | Loss: 0.00020482
Iteration 507/1000 | Loss: 0.00009529
Iteration 508/1000 | Loss: 0.00009519
Iteration 509/1000 | Loss: 0.00009514
Iteration 510/1000 | Loss: 0.00009513
Iteration 511/1000 | Loss: 0.00009513
Iteration 512/1000 | Loss: 0.00009512
Iteration 513/1000 | Loss: 0.00009512
Iteration 514/1000 | Loss: 0.00009512
Iteration 515/1000 | Loss: 0.00009511
Iteration 516/1000 | Loss: 0.00009511
Iteration 517/1000 | Loss: 0.00009511
Iteration 518/1000 | Loss: 0.00009510
Iteration 519/1000 | Loss: 0.00009510
Iteration 520/1000 | Loss: 0.00009509
Iteration 521/1000 | Loss: 0.00009509
Iteration 522/1000 | Loss: 0.00009509
Iteration 523/1000 | Loss: 0.00009509
Iteration 524/1000 | Loss: 0.00009509
Iteration 525/1000 | Loss: 0.00009508
Iteration 526/1000 | Loss: 0.00009508
Iteration 527/1000 | Loss: 0.00009508
Iteration 528/1000 | Loss: 0.00009508
Iteration 529/1000 | Loss: 0.00009508
Iteration 530/1000 | Loss: 0.00009508
Iteration 531/1000 | Loss: 0.00009508
Iteration 532/1000 | Loss: 0.00009507
Iteration 533/1000 | Loss: 0.00009506
Iteration 534/1000 | Loss: 0.00009506
Iteration 535/1000 | Loss: 0.00009505
Iteration 536/1000 | Loss: 0.00009505
Iteration 537/1000 | Loss: 0.00009501
Iteration 538/1000 | Loss: 0.00009501
Iteration 539/1000 | Loss: 0.00009498
Iteration 540/1000 | Loss: 0.00009498
Iteration 541/1000 | Loss: 0.00009498
Iteration 542/1000 | Loss: 0.00009497
Iteration 543/1000 | Loss: 0.00009496
Iteration 544/1000 | Loss: 0.00009496
Iteration 545/1000 | Loss: 0.00009496
Iteration 546/1000 | Loss: 0.00009496
Iteration 547/1000 | Loss: 0.00009495
Iteration 548/1000 | Loss: 0.00009495
Iteration 549/1000 | Loss: 0.00009494
Iteration 550/1000 | Loss: 0.00009494
Iteration 551/1000 | Loss: 0.00009494
Iteration 552/1000 | Loss: 0.00009494
Iteration 553/1000 | Loss: 0.00009494
Iteration 554/1000 | Loss: 0.00009494
Iteration 555/1000 | Loss: 0.00009493
Iteration 556/1000 | Loss: 0.00009493
Iteration 557/1000 | Loss: 0.00009493
Iteration 558/1000 | Loss: 0.00009493
Iteration 559/1000 | Loss: 0.00009493
Iteration 560/1000 | Loss: 0.00009493
Iteration 561/1000 | Loss: 0.00009493
Iteration 562/1000 | Loss: 0.00023288
Iteration 563/1000 | Loss: 0.00009608
Iteration 564/1000 | Loss: 0.00009498
Iteration 565/1000 | Loss: 0.00009488
Iteration 566/1000 | Loss: 0.00009488
Iteration 567/1000 | Loss: 0.00009488
Iteration 568/1000 | Loss: 0.00009487
Iteration 569/1000 | Loss: 0.00009487
Iteration 570/1000 | Loss: 0.00009487
Iteration 571/1000 | Loss: 0.00009486
Iteration 572/1000 | Loss: 0.00009486
Iteration 573/1000 | Loss: 0.00009486
Iteration 574/1000 | Loss: 0.00009486
Iteration 575/1000 | Loss: 0.00009486
Iteration 576/1000 | Loss: 0.00009486
Iteration 577/1000 | Loss: 0.00009486
Iteration 578/1000 | Loss: 0.00009486
Iteration 579/1000 | Loss: 0.00009486
Iteration 580/1000 | Loss: 0.00009486
Iteration 581/1000 | Loss: 0.00009485
Iteration 582/1000 | Loss: 0.00009485
Iteration 583/1000 | Loss: 0.00009485
Iteration 584/1000 | Loss: 0.00009485
Iteration 585/1000 | Loss: 0.00009485
Iteration 586/1000 | Loss: 0.00009484
Iteration 587/1000 | Loss: 0.00009484
Iteration 588/1000 | Loss: 0.00009484
Iteration 589/1000 | Loss: 0.00009484
Iteration 590/1000 | Loss: 0.00009484
Iteration 591/1000 | Loss: 0.00009484
Iteration 592/1000 | Loss: 0.00009484
Iteration 593/1000 | Loss: 0.00009484
Iteration 594/1000 | Loss: 0.00009484
Iteration 595/1000 | Loss: 0.00009484
Iteration 596/1000 | Loss: 0.00009484
Iteration 597/1000 | Loss: 0.00009484
Iteration 598/1000 | Loss: 0.00009484
Iteration 599/1000 | Loss: 0.00009483
Iteration 600/1000 | Loss: 0.00009483
Iteration 601/1000 | Loss: 0.00009483
Iteration 602/1000 | Loss: 0.00009483
Iteration 603/1000 | Loss: 0.00009483
Iteration 604/1000 | Loss: 0.00009482
Iteration 605/1000 | Loss: 0.00009482
Iteration 606/1000 | Loss: 0.00009482
Iteration 607/1000 | Loss: 0.00009482
Iteration 608/1000 | Loss: 0.00009482
Iteration 609/1000 | Loss: 0.00009481
Iteration 610/1000 | Loss: 0.00009481
Iteration 611/1000 | Loss: 0.00009481
Iteration 612/1000 | Loss: 0.00009480
Iteration 613/1000 | Loss: 0.00009480
Iteration 614/1000 | Loss: 0.00009480
Iteration 615/1000 | Loss: 0.00009480
Iteration 616/1000 | Loss: 0.00009479
Iteration 617/1000 | Loss: 0.00009479
Iteration 618/1000 | Loss: 0.00009479
Iteration 619/1000 | Loss: 0.00009479
Iteration 620/1000 | Loss: 0.00009478
Iteration 621/1000 | Loss: 0.00009478
Iteration 622/1000 | Loss: 0.00009478
Iteration 623/1000 | Loss: 0.00009477
Iteration 624/1000 | Loss: 0.00009477
Iteration 625/1000 | Loss: 0.00009477
Iteration 626/1000 | Loss: 0.00009477
Iteration 627/1000 | Loss: 0.00009476
Iteration 628/1000 | Loss: 0.00009476
Iteration 629/1000 | Loss: 0.00009476
Iteration 630/1000 | Loss: 0.00009476
Iteration 631/1000 | Loss: 0.00009476
Iteration 632/1000 | Loss: 0.00009476
Iteration 633/1000 | Loss: 0.00009475
Iteration 634/1000 | Loss: 0.00009475
Iteration 635/1000 | Loss: 0.00009475
Iteration 636/1000 | Loss: 0.00009475
Iteration 637/1000 | Loss: 0.00009475
Iteration 638/1000 | Loss: 0.00009475
Iteration 639/1000 | Loss: 0.00009475
Iteration 640/1000 | Loss: 0.00009475
Iteration 641/1000 | Loss: 0.00009475
Iteration 642/1000 | Loss: 0.00009475
Iteration 643/1000 | Loss: 0.00009475
Iteration 644/1000 | Loss: 0.00044540
Iteration 645/1000 | Loss: 0.00044539
Iteration 646/1000 | Loss: 0.00024528
Iteration 647/1000 | Loss: 0.00009508
Iteration 648/1000 | Loss: 0.00009475
Iteration 649/1000 | Loss: 0.00009473
Iteration 650/1000 | Loss: 0.00009473
Iteration 651/1000 | Loss: 0.00009472
Iteration 652/1000 | Loss: 0.00009472
Iteration 653/1000 | Loss: 0.00009472
Iteration 654/1000 | Loss: 0.00009472
Iteration 655/1000 | Loss: 0.00009472
Iteration 656/1000 | Loss: 0.00009472
Iteration 657/1000 | Loss: 0.00009472
Iteration 658/1000 | Loss: 0.00009472
Iteration 659/1000 | Loss: 0.00009472
Iteration 660/1000 | Loss: 0.00009472
Iteration 661/1000 | Loss: 0.00009472
Iteration 662/1000 | Loss: 0.00009472
Iteration 663/1000 | Loss: 0.00009472
Iteration 664/1000 | Loss: 0.00009472
Iteration 665/1000 | Loss: 0.00009472
Iteration 666/1000 | Loss: 0.00009472
Iteration 667/1000 | Loss: 0.00009472
Iteration 668/1000 | Loss: 0.00009472
Iteration 669/1000 | Loss: 0.00009472
Iteration 670/1000 | Loss: 0.00009472
Iteration 671/1000 | Loss: 0.00009472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 671. Stopping optimization.
Last 5 losses: [9.472037345403805e-05, 9.472037345403805e-05, 9.472037345403805e-05, 9.472037345403805e-05, 9.472037345403805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.472037345403805e-05

Optimization complete. Final v2v error: 5.537097454071045 mm

Highest mean error: 12.179644584655762 mm for frame 69

Lowest mean error: 3.1482253074645996 mm for frame 103

Saving results

Total time: 2237.4619822502136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00222690
Iteration 2/25 | Loss: 0.00124732
Iteration 3/25 | Loss: 0.00117419
Iteration 4/25 | Loss: 0.00115772
Iteration 5/25 | Loss: 0.00115255
Iteration 6/25 | Loss: 0.00115105
Iteration 7/25 | Loss: 0.00115032
Iteration 8/25 | Loss: 0.00115032
Iteration 9/25 | Loss: 0.00115032
Iteration 10/25 | Loss: 0.00115032
Iteration 11/25 | Loss: 0.00115032
Iteration 12/25 | Loss: 0.00115032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011503173736855388, 0.0011503173736855388, 0.0011503173736855388, 0.0011503173736855388, 0.0011503173736855388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011503173736855388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24085248
Iteration 2/25 | Loss: 0.00254419
Iteration 3/25 | Loss: 0.00254418
Iteration 4/25 | Loss: 0.00254418
Iteration 5/25 | Loss: 0.00254418
Iteration 6/25 | Loss: 0.00254419
Iteration 7/25 | Loss: 0.00254418
Iteration 8/25 | Loss: 0.00254418
Iteration 9/25 | Loss: 0.00254418
Iteration 10/25 | Loss: 0.00254418
Iteration 11/25 | Loss: 0.00254418
Iteration 12/25 | Loss: 0.00254418
Iteration 13/25 | Loss: 0.00254418
Iteration 14/25 | Loss: 0.00254418
Iteration 15/25 | Loss: 0.00254418
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025441832840442657, 0.0025441832840442657, 0.0025441832840442657, 0.0025441832840442657, 0.0025441832840442657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025441832840442657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254418
Iteration 2/1000 | Loss: 0.00003732
Iteration 3/1000 | Loss: 0.00002490
Iteration 4/1000 | Loss: 0.00001783
Iteration 5/1000 | Loss: 0.00001595
Iteration 6/1000 | Loss: 0.00001482
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001328
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001284
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001277
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001265
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001258
Iteration 24/1000 | Loss: 0.00001257
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001255
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001251
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001250
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001246
Iteration 37/1000 | Loss: 0.00001246
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001243
Iteration 41/1000 | Loss: 0.00001242
Iteration 42/1000 | Loss: 0.00001241
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001235
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001230
Iteration 56/1000 | Loss: 0.00001230
Iteration 57/1000 | Loss: 0.00001230
Iteration 58/1000 | Loss: 0.00001230
Iteration 59/1000 | Loss: 0.00001230
Iteration 60/1000 | Loss: 0.00001230
Iteration 61/1000 | Loss: 0.00001230
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001225
Iteration 78/1000 | Loss: 0.00001225
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001224
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001223
Iteration 83/1000 | Loss: 0.00001222
Iteration 84/1000 | Loss: 0.00001222
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001221
Iteration 87/1000 | Loss: 0.00001220
Iteration 88/1000 | Loss: 0.00001220
Iteration 89/1000 | Loss: 0.00001220
Iteration 90/1000 | Loss: 0.00001220
Iteration 91/1000 | Loss: 0.00001220
Iteration 92/1000 | Loss: 0.00001220
Iteration 93/1000 | Loss: 0.00001219
Iteration 94/1000 | Loss: 0.00001219
Iteration 95/1000 | Loss: 0.00001219
Iteration 96/1000 | Loss: 0.00001217
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001214
Iteration 105/1000 | Loss: 0.00001214
Iteration 106/1000 | Loss: 0.00001214
Iteration 107/1000 | Loss: 0.00001214
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001214
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001213
Iteration 127/1000 | Loss: 0.00001213
Iteration 128/1000 | Loss: 0.00001213
Iteration 129/1000 | Loss: 0.00001213
Iteration 130/1000 | Loss: 0.00001213
Iteration 131/1000 | Loss: 0.00001213
Iteration 132/1000 | Loss: 0.00001213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.2134703865740448e-05, 1.2134703865740448e-05, 1.2134703865740448e-05, 1.2134703865740448e-05, 1.2134703865740448e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2134703865740448e-05

Optimization complete. Final v2v error: 2.995577096939087 mm

Highest mean error: 3.350074529647827 mm for frame 48

Lowest mean error: 2.683852195739746 mm for frame 119

Saving results

Total time: 83.32527589797974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374533
Iteration 2/25 | Loss: 0.00135238
Iteration 3/25 | Loss: 0.00121219
Iteration 4/25 | Loss: 0.00118621
Iteration 5/25 | Loss: 0.00117925
Iteration 6/25 | Loss: 0.00117735
Iteration 7/25 | Loss: 0.00117700
Iteration 8/25 | Loss: 0.00117700
Iteration 9/25 | Loss: 0.00117700
Iteration 10/25 | Loss: 0.00117700
Iteration 11/25 | Loss: 0.00117700
Iteration 12/25 | Loss: 0.00117700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011769990669563413, 0.0011769990669563413, 0.0011769990669563413, 0.0011769990669563413, 0.0011769990669563413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011769990669563413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23283720
Iteration 2/25 | Loss: 0.00170551
Iteration 3/25 | Loss: 0.00170551
Iteration 4/25 | Loss: 0.00170550
Iteration 5/25 | Loss: 0.00170550
Iteration 6/25 | Loss: 0.00170550
Iteration 7/25 | Loss: 0.00170550
Iteration 8/25 | Loss: 0.00170550
Iteration 9/25 | Loss: 0.00170550
Iteration 10/25 | Loss: 0.00170550
Iteration 11/25 | Loss: 0.00170550
Iteration 12/25 | Loss: 0.00170550
Iteration 13/25 | Loss: 0.00170550
Iteration 14/25 | Loss: 0.00170550
Iteration 15/25 | Loss: 0.00170550
Iteration 16/25 | Loss: 0.00170550
Iteration 17/25 | Loss: 0.00170550
Iteration 18/25 | Loss: 0.00170550
Iteration 19/25 | Loss: 0.00170550
Iteration 20/25 | Loss: 0.00170550
Iteration 21/25 | Loss: 0.00170550
Iteration 22/25 | Loss: 0.00170550
Iteration 23/25 | Loss: 0.00170550
Iteration 24/25 | Loss: 0.00170550
Iteration 25/25 | Loss: 0.00170550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170550
Iteration 2/1000 | Loss: 0.00004966
Iteration 3/1000 | Loss: 0.00003404
Iteration 4/1000 | Loss: 0.00002557
Iteration 5/1000 | Loss: 0.00002259
Iteration 6/1000 | Loss: 0.00002080
Iteration 7/1000 | Loss: 0.00001921
Iteration 8/1000 | Loss: 0.00001852
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001743
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001694
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001675
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001664
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001650
Iteration 24/1000 | Loss: 0.00001649
Iteration 25/1000 | Loss: 0.00001648
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001646
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001641
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001639
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001637
Iteration 42/1000 | Loss: 0.00001637
Iteration 43/1000 | Loss: 0.00001637
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00001635
Iteration 49/1000 | Loss: 0.00001635
Iteration 50/1000 | Loss: 0.00001634
Iteration 51/1000 | Loss: 0.00001634
Iteration 52/1000 | Loss: 0.00001633
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001633
Iteration 55/1000 | Loss: 0.00001633
Iteration 56/1000 | Loss: 0.00001632
Iteration 57/1000 | Loss: 0.00001632
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001632
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001630
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001629
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001629
Iteration 69/1000 | Loss: 0.00001629
Iteration 70/1000 | Loss: 0.00001629
Iteration 71/1000 | Loss: 0.00001628
Iteration 72/1000 | Loss: 0.00001628
Iteration 73/1000 | Loss: 0.00001628
Iteration 74/1000 | Loss: 0.00001628
Iteration 75/1000 | Loss: 0.00001628
Iteration 76/1000 | Loss: 0.00001627
Iteration 77/1000 | Loss: 0.00001627
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001621
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001619
Iteration 117/1000 | Loss: 0.00001619
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001618
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001617
Iteration 129/1000 | Loss: 0.00001617
Iteration 130/1000 | Loss: 0.00001617
Iteration 131/1000 | Loss: 0.00001617
Iteration 132/1000 | Loss: 0.00001617
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001616
Iteration 139/1000 | Loss: 0.00001616
Iteration 140/1000 | Loss: 0.00001616
Iteration 141/1000 | Loss: 0.00001616
Iteration 142/1000 | Loss: 0.00001616
Iteration 143/1000 | Loss: 0.00001616
Iteration 144/1000 | Loss: 0.00001616
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001615
Iteration 151/1000 | Loss: 0.00001615
Iteration 152/1000 | Loss: 0.00001615
Iteration 153/1000 | Loss: 0.00001615
Iteration 154/1000 | Loss: 0.00001615
Iteration 155/1000 | Loss: 0.00001615
Iteration 156/1000 | Loss: 0.00001615
Iteration 157/1000 | Loss: 0.00001615
Iteration 158/1000 | Loss: 0.00001614
Iteration 159/1000 | Loss: 0.00001614
Iteration 160/1000 | Loss: 0.00001614
Iteration 161/1000 | Loss: 0.00001614
Iteration 162/1000 | Loss: 0.00001614
Iteration 163/1000 | Loss: 0.00001614
Iteration 164/1000 | Loss: 0.00001614
Iteration 165/1000 | Loss: 0.00001614
Iteration 166/1000 | Loss: 0.00001613
Iteration 167/1000 | Loss: 0.00001613
Iteration 168/1000 | Loss: 0.00001613
Iteration 169/1000 | Loss: 0.00001613
Iteration 170/1000 | Loss: 0.00001613
Iteration 171/1000 | Loss: 0.00001613
Iteration 172/1000 | Loss: 0.00001613
Iteration 173/1000 | Loss: 0.00001612
Iteration 174/1000 | Loss: 0.00001612
Iteration 175/1000 | Loss: 0.00001612
Iteration 176/1000 | Loss: 0.00001612
Iteration 177/1000 | Loss: 0.00001612
Iteration 178/1000 | Loss: 0.00001612
Iteration 179/1000 | Loss: 0.00001612
Iteration 180/1000 | Loss: 0.00001612
Iteration 181/1000 | Loss: 0.00001612
Iteration 182/1000 | Loss: 0.00001612
Iteration 183/1000 | Loss: 0.00001612
Iteration 184/1000 | Loss: 0.00001612
Iteration 185/1000 | Loss: 0.00001612
Iteration 186/1000 | Loss: 0.00001612
Iteration 187/1000 | Loss: 0.00001612
Iteration 188/1000 | Loss: 0.00001612
Iteration 189/1000 | Loss: 0.00001611
Iteration 190/1000 | Loss: 0.00001611
Iteration 191/1000 | Loss: 0.00001611
Iteration 192/1000 | Loss: 0.00001611
Iteration 193/1000 | Loss: 0.00001611
Iteration 194/1000 | Loss: 0.00001611
Iteration 195/1000 | Loss: 0.00001611
Iteration 196/1000 | Loss: 0.00001611
Iteration 197/1000 | Loss: 0.00001611
Iteration 198/1000 | Loss: 0.00001611
Iteration 199/1000 | Loss: 0.00001611
Iteration 200/1000 | Loss: 0.00001611
Iteration 201/1000 | Loss: 0.00001611
Iteration 202/1000 | Loss: 0.00001611
Iteration 203/1000 | Loss: 0.00001611
Iteration 204/1000 | Loss: 0.00001610
Iteration 205/1000 | Loss: 0.00001610
Iteration 206/1000 | Loss: 0.00001610
Iteration 207/1000 | Loss: 0.00001610
Iteration 208/1000 | Loss: 0.00001610
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001610
Iteration 213/1000 | Loss: 0.00001610
Iteration 214/1000 | Loss: 0.00001610
Iteration 215/1000 | Loss: 0.00001610
Iteration 216/1000 | Loss: 0.00001610
Iteration 217/1000 | Loss: 0.00001610
Iteration 218/1000 | Loss: 0.00001609
Iteration 219/1000 | Loss: 0.00001609
Iteration 220/1000 | Loss: 0.00001609
Iteration 221/1000 | Loss: 0.00001609
Iteration 222/1000 | Loss: 0.00001609
Iteration 223/1000 | Loss: 0.00001609
Iteration 224/1000 | Loss: 0.00001609
Iteration 225/1000 | Loss: 0.00001609
Iteration 226/1000 | Loss: 0.00001609
Iteration 227/1000 | Loss: 0.00001609
Iteration 228/1000 | Loss: 0.00001609
Iteration 229/1000 | Loss: 0.00001609
Iteration 230/1000 | Loss: 0.00001608
Iteration 231/1000 | Loss: 0.00001608
Iteration 232/1000 | Loss: 0.00001608
Iteration 233/1000 | Loss: 0.00001608
Iteration 234/1000 | Loss: 0.00001608
Iteration 235/1000 | Loss: 0.00001608
Iteration 236/1000 | Loss: 0.00001608
Iteration 237/1000 | Loss: 0.00001608
Iteration 238/1000 | Loss: 0.00001608
Iteration 239/1000 | Loss: 0.00001608
Iteration 240/1000 | Loss: 0.00001608
Iteration 241/1000 | Loss: 0.00001608
Iteration 242/1000 | Loss: 0.00001608
Iteration 243/1000 | Loss: 0.00001607
Iteration 244/1000 | Loss: 0.00001607
Iteration 245/1000 | Loss: 0.00001607
Iteration 246/1000 | Loss: 0.00001607
Iteration 247/1000 | Loss: 0.00001607
Iteration 248/1000 | Loss: 0.00001607
Iteration 249/1000 | Loss: 0.00001606
Iteration 250/1000 | Loss: 0.00001606
Iteration 251/1000 | Loss: 0.00001606
Iteration 252/1000 | Loss: 0.00001606
Iteration 253/1000 | Loss: 0.00001606
Iteration 254/1000 | Loss: 0.00001606
Iteration 255/1000 | Loss: 0.00001606
Iteration 256/1000 | Loss: 0.00001606
Iteration 257/1000 | Loss: 0.00001606
Iteration 258/1000 | Loss: 0.00001606
Iteration 259/1000 | Loss: 0.00001606
Iteration 260/1000 | Loss: 0.00001606
Iteration 261/1000 | Loss: 0.00001606
Iteration 262/1000 | Loss: 0.00001605
Iteration 263/1000 | Loss: 0.00001605
Iteration 264/1000 | Loss: 0.00001605
Iteration 265/1000 | Loss: 0.00001605
Iteration 266/1000 | Loss: 0.00001605
Iteration 267/1000 | Loss: 0.00001605
Iteration 268/1000 | Loss: 0.00001605
Iteration 269/1000 | Loss: 0.00001605
Iteration 270/1000 | Loss: 0.00001605
Iteration 271/1000 | Loss: 0.00001605
Iteration 272/1000 | Loss: 0.00001605
Iteration 273/1000 | Loss: 0.00001605
Iteration 274/1000 | Loss: 0.00001605
Iteration 275/1000 | Loss: 0.00001605
Iteration 276/1000 | Loss: 0.00001605
Iteration 277/1000 | Loss: 0.00001604
Iteration 278/1000 | Loss: 0.00001604
Iteration 279/1000 | Loss: 0.00001604
Iteration 280/1000 | Loss: 0.00001604
Iteration 281/1000 | Loss: 0.00001604
Iteration 282/1000 | Loss: 0.00001604
Iteration 283/1000 | Loss: 0.00001604
Iteration 284/1000 | Loss: 0.00001604
Iteration 285/1000 | Loss: 0.00001604
Iteration 286/1000 | Loss: 0.00001604
Iteration 287/1000 | Loss: 0.00001604
Iteration 288/1000 | Loss: 0.00001604
Iteration 289/1000 | Loss: 0.00001604
Iteration 290/1000 | Loss: 0.00001604
Iteration 291/1000 | Loss: 0.00001604
Iteration 292/1000 | Loss: 0.00001604
Iteration 293/1000 | Loss: 0.00001604
Iteration 294/1000 | Loss: 0.00001604
Iteration 295/1000 | Loss: 0.00001604
Iteration 296/1000 | Loss: 0.00001604
Iteration 297/1000 | Loss: 0.00001604
Iteration 298/1000 | Loss: 0.00001604
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 298. Stopping optimization.
Last 5 losses: [1.6037367458920926e-05, 1.6037367458920926e-05, 1.6037367458920926e-05, 1.6037367458920926e-05, 1.6037367458920926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6037367458920926e-05

Optimization complete. Final v2v error: 3.3657593727111816 mm

Highest mean error: 4.001339912414551 mm for frame 93

Lowest mean error: 2.676018238067627 mm for frame 11

Saving results

Total time: 117.45007514953613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491037
Iteration 2/25 | Loss: 0.00131772
Iteration 3/25 | Loss: 0.00121802
Iteration 4/25 | Loss: 0.00120492
Iteration 5/25 | Loss: 0.00120074
Iteration 6/25 | Loss: 0.00119965
Iteration 7/25 | Loss: 0.00119965
Iteration 8/25 | Loss: 0.00119965
Iteration 9/25 | Loss: 0.00119965
Iteration 10/25 | Loss: 0.00119965
Iteration 11/25 | Loss: 0.00119965
Iteration 12/25 | Loss: 0.00119965
Iteration 13/25 | Loss: 0.00119965
Iteration 14/25 | Loss: 0.00119965
Iteration 15/25 | Loss: 0.00119965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011996525572612882, 0.0011996525572612882, 0.0011996525572612882, 0.0011996525572612882, 0.0011996525572612882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011996525572612882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33516788
Iteration 2/25 | Loss: 0.00149689
Iteration 3/25 | Loss: 0.00149687
Iteration 4/25 | Loss: 0.00149687
Iteration 5/25 | Loss: 0.00149687
Iteration 6/25 | Loss: 0.00149687
Iteration 7/25 | Loss: 0.00149686
Iteration 8/25 | Loss: 0.00149686
Iteration 9/25 | Loss: 0.00149686
Iteration 10/25 | Loss: 0.00149686
Iteration 11/25 | Loss: 0.00149686
Iteration 12/25 | Loss: 0.00149686
Iteration 13/25 | Loss: 0.00149686
Iteration 14/25 | Loss: 0.00149686
Iteration 15/25 | Loss: 0.00149686
Iteration 16/25 | Loss: 0.00149686
Iteration 17/25 | Loss: 0.00149686
Iteration 18/25 | Loss: 0.00149686
Iteration 19/25 | Loss: 0.00149686
Iteration 20/25 | Loss: 0.00149686
Iteration 21/25 | Loss: 0.00149686
Iteration 22/25 | Loss: 0.00149686
Iteration 23/25 | Loss: 0.00149686
Iteration 24/25 | Loss: 0.00149686
Iteration 25/25 | Loss: 0.00149686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149686
Iteration 2/1000 | Loss: 0.00003599
Iteration 3/1000 | Loss: 0.00002477
Iteration 4/1000 | Loss: 0.00002063
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001844
Iteration 7/1000 | Loss: 0.00001789
Iteration 8/1000 | Loss: 0.00001767
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001629
Iteration 13/1000 | Loss: 0.00001608
Iteration 14/1000 | Loss: 0.00001594
Iteration 15/1000 | Loss: 0.00001593
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001550
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001546
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001544
Iteration 27/1000 | Loss: 0.00001543
Iteration 28/1000 | Loss: 0.00001543
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001539
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001539
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001538
Iteration 45/1000 | Loss: 0.00001538
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00001538
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001535
Iteration 56/1000 | Loss: 0.00001535
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001531
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001531
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001529
Iteration 80/1000 | Loss: 0.00001529
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001528
Iteration 84/1000 | Loss: 0.00001528
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001527
Iteration 87/1000 | Loss: 0.00001527
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Iteration 94/1000 | Loss: 0.00001525
Iteration 95/1000 | Loss: 0.00001525
Iteration 96/1000 | Loss: 0.00001525
Iteration 97/1000 | Loss: 0.00001525
Iteration 98/1000 | Loss: 0.00001524
Iteration 99/1000 | Loss: 0.00001524
Iteration 100/1000 | Loss: 0.00001524
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Iteration 106/1000 | Loss: 0.00001522
Iteration 107/1000 | Loss: 0.00001522
Iteration 108/1000 | Loss: 0.00001522
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001520
Iteration 114/1000 | Loss: 0.00001520
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001518
Iteration 121/1000 | Loss: 0.00001518
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001515
Iteration 150/1000 | Loss: 0.00001515
Iteration 151/1000 | Loss: 0.00001515
Iteration 152/1000 | Loss: 0.00001515
Iteration 153/1000 | Loss: 0.00001515
Iteration 154/1000 | Loss: 0.00001515
Iteration 155/1000 | Loss: 0.00001515
Iteration 156/1000 | Loss: 0.00001515
Iteration 157/1000 | Loss: 0.00001515
Iteration 158/1000 | Loss: 0.00001515
Iteration 159/1000 | Loss: 0.00001515
Iteration 160/1000 | Loss: 0.00001515
Iteration 161/1000 | Loss: 0.00001515
Iteration 162/1000 | Loss: 0.00001515
Iteration 163/1000 | Loss: 0.00001515
Iteration 164/1000 | Loss: 0.00001515
Iteration 165/1000 | Loss: 0.00001515
Iteration 166/1000 | Loss: 0.00001515
Iteration 167/1000 | Loss: 0.00001515
Iteration 168/1000 | Loss: 0.00001515
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001515
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001515
Iteration 178/1000 | Loss: 0.00001515
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.5153454114624765e-05, 1.5153454114624765e-05, 1.5153454114624765e-05, 1.5153454114624765e-05, 1.5153454114624765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5153454114624765e-05

Optimization complete. Final v2v error: 3.3057620525360107 mm

Highest mean error: 3.9754960536956787 mm for frame 36

Lowest mean error: 2.830962657928467 mm for frame 132

Saving results

Total time: 98.41578841209412
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759632
Iteration 2/25 | Loss: 0.00146962
Iteration 3/25 | Loss: 0.00128459
Iteration 4/25 | Loss: 0.00127187
Iteration 5/25 | Loss: 0.00127079
Iteration 6/25 | Loss: 0.00127079
Iteration 7/25 | Loss: 0.00127079
Iteration 8/25 | Loss: 0.00127079
Iteration 9/25 | Loss: 0.00127079
Iteration 10/25 | Loss: 0.00127079
Iteration 11/25 | Loss: 0.00127079
Iteration 12/25 | Loss: 0.00127079
Iteration 13/25 | Loss: 0.00127079
Iteration 14/25 | Loss: 0.00127079
Iteration 15/25 | Loss: 0.00127079
Iteration 16/25 | Loss: 0.00127079
Iteration 17/25 | Loss: 0.00127079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012707888381555676, 0.0012707888381555676, 0.0012707888381555676, 0.0012707888381555676, 0.0012707888381555676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012707888381555676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21020591
Iteration 2/25 | Loss: 0.00102635
Iteration 3/25 | Loss: 0.00102633
Iteration 4/25 | Loss: 0.00102633
Iteration 5/25 | Loss: 0.00102633
Iteration 6/25 | Loss: 0.00102633
Iteration 7/25 | Loss: 0.00102633
Iteration 8/25 | Loss: 0.00102633
Iteration 9/25 | Loss: 0.00102633
Iteration 10/25 | Loss: 0.00102633
Iteration 11/25 | Loss: 0.00102633
Iteration 12/25 | Loss: 0.00102633
Iteration 13/25 | Loss: 0.00102633
Iteration 14/25 | Loss: 0.00102633
Iteration 15/25 | Loss: 0.00102633
Iteration 16/25 | Loss: 0.00102633
Iteration 17/25 | Loss: 0.00102633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010263262083753943, 0.0010263262083753943, 0.0010263262083753943, 0.0010263262083753943, 0.0010263262083753943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010263262083753943

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102633
Iteration 2/1000 | Loss: 0.00003059
Iteration 3/1000 | Loss: 0.00002163
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001898
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001760
Iteration 10/1000 | Loss: 0.00001734
Iteration 11/1000 | Loss: 0.00001710
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001691
Iteration 14/1000 | Loss: 0.00001688
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001676
Iteration 18/1000 | Loss: 0.00001676
Iteration 19/1000 | Loss: 0.00001675
Iteration 20/1000 | Loss: 0.00001674
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001673
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001664
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001656
Iteration 28/1000 | Loss: 0.00001656
Iteration 29/1000 | Loss: 0.00001655
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001646
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001638
Iteration 39/1000 | Loss: 0.00001638
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001638
Iteration 43/1000 | Loss: 0.00001636
Iteration 44/1000 | Loss: 0.00001636
Iteration 45/1000 | Loss: 0.00001636
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001634
Iteration 53/1000 | Loss: 0.00001633
Iteration 54/1000 | Loss: 0.00001632
Iteration 55/1000 | Loss: 0.00001632
Iteration 56/1000 | Loss: 0.00001632
Iteration 57/1000 | Loss: 0.00001632
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.6217036318266764e-05, 1.6217036318266764e-05, 1.6217036318266764e-05, 1.6217036318266764e-05, 1.6217036318266764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6217036318266764e-05

Optimization complete. Final v2v error: 3.4154324531555176 mm

Highest mean error: 3.623722553253174 mm for frame 30

Lowest mean error: 3.290526866912842 mm for frame 37

Saving results

Total time: 80.06039452552795
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424383
Iteration 2/25 | Loss: 0.00130086
Iteration 3/25 | Loss: 0.00122245
Iteration 4/25 | Loss: 0.00120450
Iteration 5/25 | Loss: 0.00119758
Iteration 6/25 | Loss: 0.00119624
Iteration 7/25 | Loss: 0.00119624
Iteration 8/25 | Loss: 0.00119624
Iteration 9/25 | Loss: 0.00119624
Iteration 10/25 | Loss: 0.00119624
Iteration 11/25 | Loss: 0.00119624
Iteration 12/25 | Loss: 0.00119624
Iteration 13/25 | Loss: 0.00119624
Iteration 14/25 | Loss: 0.00119624
Iteration 15/25 | Loss: 0.00119624
Iteration 16/25 | Loss: 0.00119624
Iteration 17/25 | Loss: 0.00119624
Iteration 18/25 | Loss: 0.00119624
Iteration 19/25 | Loss: 0.00119624
Iteration 20/25 | Loss: 0.00119624
Iteration 21/25 | Loss: 0.00119624
Iteration 22/25 | Loss: 0.00119624
Iteration 23/25 | Loss: 0.00119624
Iteration 24/25 | Loss: 0.00119624
Iteration 25/25 | Loss: 0.00119624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26160336
Iteration 2/25 | Loss: 0.00206514
Iteration 3/25 | Loss: 0.00206514
Iteration 4/25 | Loss: 0.00206514
Iteration 5/25 | Loss: 0.00206514
Iteration 6/25 | Loss: 0.00206514
Iteration 7/25 | Loss: 0.00206514
Iteration 8/25 | Loss: 0.00206514
Iteration 9/25 | Loss: 0.00206514
Iteration 10/25 | Loss: 0.00206514
Iteration 11/25 | Loss: 0.00206514
Iteration 12/25 | Loss: 0.00206514
Iteration 13/25 | Loss: 0.00206514
Iteration 14/25 | Loss: 0.00206514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002065137727186084, 0.002065137727186084, 0.002065137727186084, 0.002065137727186084, 0.002065137727186084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002065137727186084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206514
Iteration 2/1000 | Loss: 0.00004596
Iteration 3/1000 | Loss: 0.00003278
Iteration 4/1000 | Loss: 0.00002681
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002343
Iteration 7/1000 | Loss: 0.00002256
Iteration 8/1000 | Loss: 0.00002207
Iteration 9/1000 | Loss: 0.00002166
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002098
Iteration 12/1000 | Loss: 0.00002073
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002044
Iteration 15/1000 | Loss: 0.00002040
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002034
Iteration 21/1000 | Loss: 0.00002033
Iteration 22/1000 | Loss: 0.00002032
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002022
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002009
Iteration 32/1000 | Loss: 0.00002006
Iteration 33/1000 | Loss: 0.00002005
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002003
Iteration 38/1000 | Loss: 0.00002000
Iteration 39/1000 | Loss: 0.00001999
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001997
Iteration 42/1000 | Loss: 0.00001997
Iteration 43/1000 | Loss: 0.00001997
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001996
Iteration 46/1000 | Loss: 0.00001996
Iteration 47/1000 | Loss: 0.00001996
Iteration 48/1000 | Loss: 0.00001996
Iteration 49/1000 | Loss: 0.00001996
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001990
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001987
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001986
Iteration 83/1000 | Loss: 0.00001985
Iteration 84/1000 | Loss: 0.00001985
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001984
Iteration 88/1000 | Loss: 0.00001984
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001982
Iteration 93/1000 | Loss: 0.00001982
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001980
Iteration 97/1000 | Loss: 0.00001980
Iteration 98/1000 | Loss: 0.00001980
Iteration 99/1000 | Loss: 0.00001980
Iteration 100/1000 | Loss: 0.00001980
Iteration 101/1000 | Loss: 0.00001979
Iteration 102/1000 | Loss: 0.00001978
Iteration 103/1000 | Loss: 0.00001977
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00001975
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001974
Iteration 118/1000 | Loss: 0.00001974
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001971
Iteration 127/1000 | Loss: 0.00001971
Iteration 128/1000 | Loss: 0.00001970
Iteration 129/1000 | Loss: 0.00001970
Iteration 130/1000 | Loss: 0.00001970
Iteration 131/1000 | Loss: 0.00001970
Iteration 132/1000 | Loss: 0.00001970
Iteration 133/1000 | Loss: 0.00001970
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001970
Iteration 136/1000 | Loss: 0.00001970
Iteration 137/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.9695818991749547e-05, 1.9695818991749547e-05, 1.9695818991749547e-05, 1.9695818991749547e-05, 1.9695818991749547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9695818991749547e-05

Optimization complete. Final v2v error: 3.6058027744293213 mm

Highest mean error: 4.228833198547363 mm for frame 200

Lowest mean error: 2.896874189376831 mm for frame 131

Saving results

Total time: 128.14299321174622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987167
Iteration 2/25 | Loss: 0.00210511
Iteration 3/25 | Loss: 0.00154059
Iteration 4/25 | Loss: 0.00143363
Iteration 5/25 | Loss: 0.00148622
Iteration 6/25 | Loss: 0.00136007
Iteration 7/25 | Loss: 0.00131842
Iteration 8/25 | Loss: 0.00131340
Iteration 9/25 | Loss: 0.00130953
Iteration 10/25 | Loss: 0.00125909
Iteration 11/25 | Loss: 0.00126360
Iteration 12/25 | Loss: 0.00125762
Iteration 13/25 | Loss: 0.00123881
Iteration 14/25 | Loss: 0.00123645
Iteration 15/25 | Loss: 0.00123667
Iteration 16/25 | Loss: 0.00123700
Iteration 17/25 | Loss: 0.00123875
Iteration 18/25 | Loss: 0.00123330
Iteration 19/25 | Loss: 0.00124630
Iteration 20/25 | Loss: 0.00124541
Iteration 21/25 | Loss: 0.00124338
Iteration 22/25 | Loss: 0.00124046
Iteration 23/25 | Loss: 0.00123117
Iteration 24/25 | Loss: 0.00123012
Iteration 25/25 | Loss: 0.00123248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28491306
Iteration 2/25 | Loss: 0.00175721
Iteration 3/25 | Loss: 0.00141318
Iteration 4/25 | Loss: 0.00141318
Iteration 5/25 | Loss: 0.00141318
Iteration 6/25 | Loss: 0.00141318
Iteration 7/25 | Loss: 0.00141318
Iteration 8/25 | Loss: 0.00141318
Iteration 9/25 | Loss: 0.00141318
Iteration 10/25 | Loss: 0.00141318
Iteration 11/25 | Loss: 0.00141318
Iteration 12/25 | Loss: 0.00141318
Iteration 13/25 | Loss: 0.00141318
Iteration 14/25 | Loss: 0.00141318
Iteration 15/25 | Loss: 0.00141318
Iteration 16/25 | Loss: 0.00141317
Iteration 17/25 | Loss: 0.00141317
Iteration 18/25 | Loss: 0.00141317
Iteration 19/25 | Loss: 0.00141317
Iteration 20/25 | Loss: 0.00141317
Iteration 21/25 | Loss: 0.00141317
Iteration 22/25 | Loss: 0.00141317
Iteration 23/25 | Loss: 0.00141317
Iteration 24/25 | Loss: 0.00141317
Iteration 25/25 | Loss: 0.00141317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014131745556369424, 0.0014131745556369424, 0.0014131745556369424, 0.0014131745556369424, 0.0014131745556369424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014131745556369424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141317
Iteration 2/1000 | Loss: 0.00037681
Iteration 3/1000 | Loss: 0.00036601
Iteration 4/1000 | Loss: 0.00054832
Iteration 5/1000 | Loss: 0.00002470
Iteration 6/1000 | Loss: 0.00013238
Iteration 7/1000 | Loss: 0.00002295
Iteration 8/1000 | Loss: 0.00002169
Iteration 9/1000 | Loss: 0.00053816
Iteration 10/1000 | Loss: 0.00011929
Iteration 11/1000 | Loss: 0.00004616
Iteration 12/1000 | Loss: 0.00003480
Iteration 13/1000 | Loss: 0.00057020
Iteration 14/1000 | Loss: 0.00003721
Iteration 15/1000 | Loss: 0.00002756
Iteration 16/1000 | Loss: 0.00005464
Iteration 17/1000 | Loss: 0.00049518
Iteration 18/1000 | Loss: 0.00019649
Iteration 19/1000 | Loss: 0.00004368
Iteration 20/1000 | Loss: 0.00018980
Iteration 21/1000 | Loss: 0.00002816
Iteration 22/1000 | Loss: 0.00002884
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00004728
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00004286
Iteration 27/1000 | Loss: 0.00005602
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001477
Iteration 30/1000 | Loss: 0.00009581
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001367
Iteration 34/1000 | Loss: 0.00003475
Iteration 35/1000 | Loss: 0.00004099
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001315
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001293
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001282
Iteration 44/1000 | Loss: 0.00004179
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001262
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001251
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001249
Iteration 77/1000 | Loss: 0.00001249
Iteration 78/1000 | Loss: 0.00001248
Iteration 79/1000 | Loss: 0.00001248
Iteration 80/1000 | Loss: 0.00001247
Iteration 81/1000 | Loss: 0.00001246
Iteration 82/1000 | Loss: 0.00001246
Iteration 83/1000 | Loss: 0.00001246
Iteration 84/1000 | Loss: 0.00001246
Iteration 85/1000 | Loss: 0.00001246
Iteration 86/1000 | Loss: 0.00001245
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001243
Iteration 90/1000 | Loss: 0.00001243
Iteration 91/1000 | Loss: 0.00001243
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001242
Iteration 94/1000 | Loss: 0.00001242
Iteration 95/1000 | Loss: 0.00001242
Iteration 96/1000 | Loss: 0.00001242
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001240
Iteration 100/1000 | Loss: 0.00001240
Iteration 101/1000 | Loss: 0.00001240
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001239
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001237
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001236
Iteration 125/1000 | Loss: 0.00001236
Iteration 126/1000 | Loss: 0.00001236
Iteration 127/1000 | Loss: 0.00001236
Iteration 128/1000 | Loss: 0.00001236
Iteration 129/1000 | Loss: 0.00001236
Iteration 130/1000 | Loss: 0.00001236
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001235
Iteration 136/1000 | Loss: 0.00001235
Iteration 137/1000 | Loss: 0.00001235
Iteration 138/1000 | Loss: 0.00001235
Iteration 139/1000 | Loss: 0.00001235
Iteration 140/1000 | Loss: 0.00001235
Iteration 141/1000 | Loss: 0.00001235
Iteration 142/1000 | Loss: 0.00001235
Iteration 143/1000 | Loss: 0.00001235
Iteration 144/1000 | Loss: 0.00001235
Iteration 145/1000 | Loss: 0.00001235
Iteration 146/1000 | Loss: 0.00001235
Iteration 147/1000 | Loss: 0.00001235
Iteration 148/1000 | Loss: 0.00001234
Iteration 149/1000 | Loss: 0.00001234
Iteration 150/1000 | Loss: 0.00001234
Iteration 151/1000 | Loss: 0.00001234
Iteration 152/1000 | Loss: 0.00001234
Iteration 153/1000 | Loss: 0.00001234
Iteration 154/1000 | Loss: 0.00001234
Iteration 155/1000 | Loss: 0.00001234
Iteration 156/1000 | Loss: 0.00001234
Iteration 157/1000 | Loss: 0.00001234
Iteration 158/1000 | Loss: 0.00001234
Iteration 159/1000 | Loss: 0.00001234
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001234
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001234
Iteration 178/1000 | Loss: 0.00001234
Iteration 179/1000 | Loss: 0.00001234
Iteration 180/1000 | Loss: 0.00001234
Iteration 181/1000 | Loss: 0.00001234
Iteration 182/1000 | Loss: 0.00001234
Iteration 183/1000 | Loss: 0.00001234
Iteration 184/1000 | Loss: 0.00001234
Iteration 185/1000 | Loss: 0.00001234
Iteration 186/1000 | Loss: 0.00001234
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Iteration 190/1000 | Loss: 0.00001234
Iteration 191/1000 | Loss: 0.00001234
Iteration 192/1000 | Loss: 0.00001234
Iteration 193/1000 | Loss: 0.00001234
Iteration 194/1000 | Loss: 0.00001234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.2344274182396475e-05, 1.2344274182396475e-05, 1.2344274182396475e-05, 1.2344274182396475e-05, 1.2344274182396475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2344274182396475e-05

Optimization complete. Final v2v error: 2.977715492248535 mm

Highest mean error: 4.437596797943115 mm for frame 72

Lowest mean error: 2.6433544158935547 mm for frame 3

Saving results

Total time: 252.67797589302063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404054
Iteration 2/25 | Loss: 0.00135807
Iteration 3/25 | Loss: 0.00120964
Iteration 4/25 | Loss: 0.00119706
Iteration 5/25 | Loss: 0.00119540
Iteration 6/25 | Loss: 0.00119509
Iteration 7/25 | Loss: 0.00119509
Iteration 8/25 | Loss: 0.00119509
Iteration 9/25 | Loss: 0.00119509
Iteration 10/25 | Loss: 0.00119509
Iteration 11/25 | Loss: 0.00119509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011950914049521089, 0.0011950914049521089, 0.0011950914049521089, 0.0011950914049521089, 0.0011950914049521089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011950914049521089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28277302
Iteration 2/25 | Loss: 0.00112115
Iteration 3/25 | Loss: 0.00112115
Iteration 4/25 | Loss: 0.00112115
Iteration 5/25 | Loss: 0.00112115
Iteration 6/25 | Loss: 0.00112115
Iteration 7/25 | Loss: 0.00112115
Iteration 8/25 | Loss: 0.00112115
Iteration 9/25 | Loss: 0.00112115
Iteration 10/25 | Loss: 0.00112114
Iteration 11/25 | Loss: 0.00112114
Iteration 12/25 | Loss: 0.00112114
Iteration 13/25 | Loss: 0.00112114
Iteration 14/25 | Loss: 0.00112114
Iteration 15/25 | Loss: 0.00112114
Iteration 16/25 | Loss: 0.00112114
Iteration 17/25 | Loss: 0.00112114
Iteration 18/25 | Loss: 0.00112114
Iteration 19/25 | Loss: 0.00112114
Iteration 20/25 | Loss: 0.00112114
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011211441596969962, 0.0011211441596969962, 0.0011211441596969962, 0.0011211441596969962, 0.0011211441596969962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011211441596969962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112114
Iteration 2/1000 | Loss: 0.00002344
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001472
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001267
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001253
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001228
Iteration 17/1000 | Loss: 0.00001224
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001204
Iteration 24/1000 | Loss: 0.00001204
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001196
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001190
Iteration 34/1000 | Loss: 0.00001189
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001171
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001169
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001167
Iteration 49/1000 | Loss: 0.00001167
Iteration 50/1000 | Loss: 0.00001167
Iteration 51/1000 | Loss: 0.00001167
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001166
Iteration 54/1000 | Loss: 0.00001166
Iteration 55/1000 | Loss: 0.00001164
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001163
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001162
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001159
Iteration 70/1000 | Loss: 0.00001159
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001158
Iteration 77/1000 | Loss: 0.00001158
Iteration 78/1000 | Loss: 0.00001158
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001154
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001153
Iteration 121/1000 | Loss: 0.00001153
Iteration 122/1000 | Loss: 0.00001153
Iteration 123/1000 | Loss: 0.00001153
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001152
Iteration 127/1000 | Loss: 0.00001152
Iteration 128/1000 | Loss: 0.00001152
Iteration 129/1000 | Loss: 0.00001152
Iteration 130/1000 | Loss: 0.00001152
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001150
Iteration 135/1000 | Loss: 0.00001150
Iteration 136/1000 | Loss: 0.00001150
Iteration 137/1000 | Loss: 0.00001150
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Iteration 141/1000 | Loss: 0.00001149
Iteration 142/1000 | Loss: 0.00001149
Iteration 143/1000 | Loss: 0.00001149
Iteration 144/1000 | Loss: 0.00001148
Iteration 145/1000 | Loss: 0.00001148
Iteration 146/1000 | Loss: 0.00001148
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001147
Iteration 149/1000 | Loss: 0.00001147
Iteration 150/1000 | Loss: 0.00001147
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001145
Iteration 156/1000 | Loss: 0.00001145
Iteration 157/1000 | Loss: 0.00001144
Iteration 158/1000 | Loss: 0.00001144
Iteration 159/1000 | Loss: 0.00001144
Iteration 160/1000 | Loss: 0.00001144
Iteration 161/1000 | Loss: 0.00001143
Iteration 162/1000 | Loss: 0.00001143
Iteration 163/1000 | Loss: 0.00001143
Iteration 164/1000 | Loss: 0.00001143
Iteration 165/1000 | Loss: 0.00001143
Iteration 166/1000 | Loss: 0.00001143
Iteration 167/1000 | Loss: 0.00001143
Iteration 168/1000 | Loss: 0.00001143
Iteration 169/1000 | Loss: 0.00001142
Iteration 170/1000 | Loss: 0.00001142
Iteration 171/1000 | Loss: 0.00001142
Iteration 172/1000 | Loss: 0.00001141
Iteration 173/1000 | Loss: 0.00001141
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001141
Iteration 176/1000 | Loss: 0.00001141
Iteration 177/1000 | Loss: 0.00001140
Iteration 178/1000 | Loss: 0.00001140
Iteration 179/1000 | Loss: 0.00001140
Iteration 180/1000 | Loss: 0.00001140
Iteration 181/1000 | Loss: 0.00001140
Iteration 182/1000 | Loss: 0.00001140
Iteration 183/1000 | Loss: 0.00001140
Iteration 184/1000 | Loss: 0.00001140
Iteration 185/1000 | Loss: 0.00001140
Iteration 186/1000 | Loss: 0.00001140
Iteration 187/1000 | Loss: 0.00001140
Iteration 188/1000 | Loss: 0.00001140
Iteration 189/1000 | Loss: 0.00001140
Iteration 190/1000 | Loss: 0.00001140
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001138
Iteration 201/1000 | Loss: 0.00001138
Iteration 202/1000 | Loss: 0.00001138
Iteration 203/1000 | Loss: 0.00001138
Iteration 204/1000 | Loss: 0.00001138
Iteration 205/1000 | Loss: 0.00001138
Iteration 206/1000 | Loss: 0.00001138
Iteration 207/1000 | Loss: 0.00001138
Iteration 208/1000 | Loss: 0.00001138
Iteration 209/1000 | Loss: 0.00001138
Iteration 210/1000 | Loss: 0.00001138
Iteration 211/1000 | Loss: 0.00001138
Iteration 212/1000 | Loss: 0.00001138
Iteration 213/1000 | Loss: 0.00001138
Iteration 214/1000 | Loss: 0.00001138
Iteration 215/1000 | Loss: 0.00001138
Iteration 216/1000 | Loss: 0.00001138
Iteration 217/1000 | Loss: 0.00001137
Iteration 218/1000 | Loss: 0.00001137
Iteration 219/1000 | Loss: 0.00001137
Iteration 220/1000 | Loss: 0.00001137
Iteration 221/1000 | Loss: 0.00001137
Iteration 222/1000 | Loss: 0.00001137
Iteration 223/1000 | Loss: 0.00001137
Iteration 224/1000 | Loss: 0.00001137
Iteration 225/1000 | Loss: 0.00001137
Iteration 226/1000 | Loss: 0.00001137
Iteration 227/1000 | Loss: 0.00001137
Iteration 228/1000 | Loss: 0.00001137
Iteration 229/1000 | Loss: 0.00001137
Iteration 230/1000 | Loss: 0.00001137
Iteration 231/1000 | Loss: 0.00001137
Iteration 232/1000 | Loss: 0.00001137
Iteration 233/1000 | Loss: 0.00001137
Iteration 234/1000 | Loss: 0.00001137
Iteration 235/1000 | Loss: 0.00001137
Iteration 236/1000 | Loss: 0.00001137
Iteration 237/1000 | Loss: 0.00001137
Iteration 238/1000 | Loss: 0.00001137
Iteration 239/1000 | Loss: 0.00001137
Iteration 240/1000 | Loss: 0.00001137
Iteration 241/1000 | Loss: 0.00001137
Iteration 242/1000 | Loss: 0.00001137
Iteration 243/1000 | Loss: 0.00001137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.1369099411240313e-05, 1.1369099411240313e-05, 1.1369099411240313e-05, 1.1369099411240313e-05, 1.1369099411240313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1369099411240313e-05

Optimization complete. Final v2v error: 2.9069769382476807 mm

Highest mean error: 3.012949228286743 mm for frame 87

Lowest mean error: 2.808936357498169 mm for frame 121

Saving results

Total time: 107.6202244758606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866447
Iteration 2/25 | Loss: 0.00164018
Iteration 3/25 | Loss: 0.00136722
Iteration 4/25 | Loss: 0.00132389
Iteration 5/25 | Loss: 0.00129978
Iteration 6/25 | Loss: 0.00129328
Iteration 7/25 | Loss: 0.00129236
Iteration 8/25 | Loss: 0.00128912
Iteration 9/25 | Loss: 0.00128624
Iteration 10/25 | Loss: 0.00128411
Iteration 11/25 | Loss: 0.00128275
Iteration 12/25 | Loss: 0.00128202
Iteration 13/25 | Loss: 0.00128752
Iteration 14/25 | Loss: 0.00128211
Iteration 15/25 | Loss: 0.00127751
Iteration 16/25 | Loss: 0.00127661
Iteration 17/25 | Loss: 0.00127640
Iteration 18/25 | Loss: 0.00127632
Iteration 19/25 | Loss: 0.00127632
Iteration 20/25 | Loss: 0.00127632
Iteration 21/25 | Loss: 0.00127631
Iteration 22/25 | Loss: 0.00127631
Iteration 23/25 | Loss: 0.00127631
Iteration 24/25 | Loss: 0.00127631
Iteration 25/25 | Loss: 0.00127631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.37344885
Iteration 2/25 | Loss: 0.00116315
Iteration 3/25 | Loss: 0.00116312
Iteration 4/25 | Loss: 0.00116312
Iteration 5/25 | Loss: 0.00116312
Iteration 6/25 | Loss: 0.00116312
Iteration 7/25 | Loss: 0.00116312
Iteration 8/25 | Loss: 0.00116312
Iteration 9/25 | Loss: 0.00116312
Iteration 10/25 | Loss: 0.00116312
Iteration 11/25 | Loss: 0.00116312
Iteration 12/25 | Loss: 0.00116312
Iteration 13/25 | Loss: 0.00116312
Iteration 14/25 | Loss: 0.00116312
Iteration 15/25 | Loss: 0.00116312
Iteration 16/25 | Loss: 0.00116312
Iteration 17/25 | Loss: 0.00116312
Iteration 18/25 | Loss: 0.00116312
Iteration 19/25 | Loss: 0.00116312
Iteration 20/25 | Loss: 0.00116312
Iteration 21/25 | Loss: 0.00116312
Iteration 22/25 | Loss: 0.00116312
Iteration 23/25 | Loss: 0.00116312
Iteration 24/25 | Loss: 0.00116312
Iteration 25/25 | Loss: 0.00116312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116312
Iteration 2/1000 | Loss: 0.00003996
Iteration 3/1000 | Loss: 0.00002538
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002120
Iteration 6/1000 | Loss: 0.00002061
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001888
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001848
Iteration 13/1000 | Loss: 0.00001847
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001826
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001816
Iteration 18/1000 | Loss: 0.00001815
Iteration 19/1000 | Loss: 0.00001804
Iteration 20/1000 | Loss: 0.00001804
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001799
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001798
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001797
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001796
Iteration 36/1000 | Loss: 0.00001796
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001795
Iteration 39/1000 | Loss: 0.00001795
Iteration 40/1000 | Loss: 0.00001795
Iteration 41/1000 | Loss: 0.00001795
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001794
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001792
Iteration 52/1000 | Loss: 0.00001791
Iteration 53/1000 | Loss: 0.00001791
Iteration 54/1000 | Loss: 0.00001790
Iteration 55/1000 | Loss: 0.00001790
Iteration 56/1000 | Loss: 0.00001790
Iteration 57/1000 | Loss: 0.00001790
Iteration 58/1000 | Loss: 0.00001790
Iteration 59/1000 | Loss: 0.00001789
Iteration 60/1000 | Loss: 0.00001789
Iteration 61/1000 | Loss: 0.00001789
Iteration 62/1000 | Loss: 0.00001788
Iteration 63/1000 | Loss: 0.00001788
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001787
Iteration 66/1000 | Loss: 0.00001787
Iteration 67/1000 | Loss: 0.00001787
Iteration 68/1000 | Loss: 0.00001787
Iteration 69/1000 | Loss: 0.00001787
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001786
Iteration 72/1000 | Loss: 0.00001786
Iteration 73/1000 | Loss: 0.00001786
Iteration 74/1000 | Loss: 0.00001786
Iteration 75/1000 | Loss: 0.00001786
Iteration 76/1000 | Loss: 0.00001786
Iteration 77/1000 | Loss: 0.00001786
Iteration 78/1000 | Loss: 0.00001786
Iteration 79/1000 | Loss: 0.00001786
Iteration 80/1000 | Loss: 0.00001786
Iteration 81/1000 | Loss: 0.00001786
Iteration 82/1000 | Loss: 0.00001786
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001786
Iteration 92/1000 | Loss: 0.00001786
Iteration 93/1000 | Loss: 0.00001786
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001786
Iteration 99/1000 | Loss: 0.00001786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.785572931112256e-05, 1.785572931112256e-05, 1.785572931112256e-05, 1.785572931112256e-05, 1.785572931112256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.785572931112256e-05

Optimization complete. Final v2v error: 3.6372642517089844 mm

Highest mean error: 4.3135833740234375 mm for frame 12

Lowest mean error: 3.1750762462615967 mm for frame 225

Saving results

Total time: 178.38050770759583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_007/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_007/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041276
Iteration 2/25 | Loss: 0.00224236
Iteration 3/25 | Loss: 0.00169421
Iteration 4/25 | Loss: 0.00163187
Iteration 5/25 | Loss: 0.00143002
Iteration 6/25 | Loss: 0.00138294
Iteration 7/25 | Loss: 0.00128096
Iteration 8/25 | Loss: 0.00124531
Iteration 9/25 | Loss: 0.00121600
Iteration 10/25 | Loss: 0.00120135
Iteration 11/25 | Loss: 0.00121326
Iteration 12/25 | Loss: 0.00119865
Iteration 13/25 | Loss: 0.00119622
Iteration 14/25 | Loss: 0.00119590
Iteration 15/25 | Loss: 0.00119569
Iteration 16/25 | Loss: 0.00119880
Iteration 17/25 | Loss: 0.00119561
Iteration 18/25 | Loss: 0.00119561
Iteration 19/25 | Loss: 0.00119561
Iteration 20/25 | Loss: 0.00119561
Iteration 21/25 | Loss: 0.00119561
Iteration 22/25 | Loss: 0.00119560
Iteration 23/25 | Loss: 0.00119560
Iteration 24/25 | Loss: 0.00119560
Iteration 25/25 | Loss: 0.00119560

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24329305
Iteration 2/25 | Loss: 0.00139103
Iteration 3/25 | Loss: 0.00127714
Iteration 4/25 | Loss: 0.00127714
Iteration 5/25 | Loss: 0.00127714
Iteration 6/25 | Loss: 0.00127714
Iteration 7/25 | Loss: 0.00127714
Iteration 8/25 | Loss: 0.00127714
Iteration 9/25 | Loss: 0.00127714
Iteration 10/25 | Loss: 0.00127714
Iteration 11/25 | Loss: 0.00127714
Iteration 12/25 | Loss: 0.00127714
Iteration 13/25 | Loss: 0.00127714
Iteration 14/25 | Loss: 0.00127714
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012771390611305833, 0.0012771390611305833, 0.0012771390611305833, 0.0012771390611305833, 0.0012771390611305833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012771390611305833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127714
Iteration 2/1000 | Loss: 0.00004416
Iteration 3/1000 | Loss: 0.00003422
Iteration 4/1000 | Loss: 0.00002969
Iteration 5/1000 | Loss: 0.00002730
Iteration 6/1000 | Loss: 0.00002636
Iteration 7/1000 | Loss: 0.00002550
Iteration 8/1000 | Loss: 0.00002485
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002368
Iteration 11/1000 | Loss: 0.00002312
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002239
Iteration 14/1000 | Loss: 0.00002211
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002208
Iteration 17/1000 | Loss: 0.00002195
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002162
Iteration 22/1000 | Loss: 0.00002156
Iteration 23/1000 | Loss: 0.00002156
Iteration 24/1000 | Loss: 0.00002151
Iteration 25/1000 | Loss: 0.00002149
Iteration 26/1000 | Loss: 0.00002148
Iteration 27/1000 | Loss: 0.00002148
Iteration 28/1000 | Loss: 0.00002147
Iteration 29/1000 | Loss: 0.00002147
Iteration 30/1000 | Loss: 0.00002147
Iteration 31/1000 | Loss: 0.00008792
Iteration 32/1000 | Loss: 0.00057215
Iteration 33/1000 | Loss: 0.00310512
Iteration 34/1000 | Loss: 0.00006425
Iteration 35/1000 | Loss: 0.00003813
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00011220
Iteration 38/1000 | Loss: 0.00009394
Iteration 39/1000 | Loss: 0.00020516
Iteration 40/1000 | Loss: 0.00011337
Iteration 41/1000 | Loss: 0.00006431
Iteration 42/1000 | Loss: 0.00001432
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00002094
Iteration 47/1000 | Loss: 0.00000989
Iteration 48/1000 | Loss: 0.00000944
Iteration 49/1000 | Loss: 0.00006558
Iteration 50/1000 | Loss: 0.00000922
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000876
Iteration 53/1000 | Loss: 0.00000863
Iteration 54/1000 | Loss: 0.00000859
Iteration 55/1000 | Loss: 0.00000851
Iteration 56/1000 | Loss: 0.00000839
Iteration 57/1000 | Loss: 0.00000837
Iteration 58/1000 | Loss: 0.00000836
Iteration 59/1000 | Loss: 0.00000835
Iteration 60/1000 | Loss: 0.00000835
Iteration 61/1000 | Loss: 0.00000835
Iteration 62/1000 | Loss: 0.00000835
Iteration 63/1000 | Loss: 0.00000834
Iteration 64/1000 | Loss: 0.00000833
Iteration 65/1000 | Loss: 0.00000832
Iteration 66/1000 | Loss: 0.00000832
Iteration 67/1000 | Loss: 0.00000832
Iteration 68/1000 | Loss: 0.00000832
Iteration 69/1000 | Loss: 0.00000831
Iteration 70/1000 | Loss: 0.00000831
Iteration 71/1000 | Loss: 0.00000830
Iteration 72/1000 | Loss: 0.00000829
Iteration 73/1000 | Loss: 0.00000829
Iteration 74/1000 | Loss: 0.00000829
Iteration 75/1000 | Loss: 0.00000828
Iteration 76/1000 | Loss: 0.00000828
Iteration 77/1000 | Loss: 0.00000828
Iteration 78/1000 | Loss: 0.00000827
Iteration 79/1000 | Loss: 0.00000827
Iteration 80/1000 | Loss: 0.00000827
Iteration 81/1000 | Loss: 0.00000826
Iteration 82/1000 | Loss: 0.00000826
Iteration 83/1000 | Loss: 0.00000826
Iteration 84/1000 | Loss: 0.00000826
Iteration 85/1000 | Loss: 0.00000826
Iteration 86/1000 | Loss: 0.00000826
Iteration 87/1000 | Loss: 0.00000826
Iteration 88/1000 | Loss: 0.00000826
Iteration 89/1000 | Loss: 0.00000825
Iteration 90/1000 | Loss: 0.00000825
Iteration 91/1000 | Loss: 0.00000825
Iteration 92/1000 | Loss: 0.00000824
Iteration 93/1000 | Loss: 0.00000824
Iteration 94/1000 | Loss: 0.00000824
Iteration 95/1000 | Loss: 0.00000824
Iteration 96/1000 | Loss: 0.00000824
Iteration 97/1000 | Loss: 0.00000824
Iteration 98/1000 | Loss: 0.00000824
Iteration 99/1000 | Loss: 0.00000824
Iteration 100/1000 | Loss: 0.00000824
Iteration 101/1000 | Loss: 0.00000824
Iteration 102/1000 | Loss: 0.00000824
Iteration 103/1000 | Loss: 0.00000824
Iteration 104/1000 | Loss: 0.00000823
Iteration 105/1000 | Loss: 0.00000823
Iteration 106/1000 | Loss: 0.00000823
Iteration 107/1000 | Loss: 0.00000823
Iteration 108/1000 | Loss: 0.00000823
Iteration 109/1000 | Loss: 0.00000823
Iteration 110/1000 | Loss: 0.00000823
Iteration 111/1000 | Loss: 0.00000823
Iteration 112/1000 | Loss: 0.00000823
Iteration 113/1000 | Loss: 0.00000823
Iteration 114/1000 | Loss: 0.00000823
Iteration 115/1000 | Loss: 0.00000823
Iteration 116/1000 | Loss: 0.00000822
Iteration 117/1000 | Loss: 0.00000822
Iteration 118/1000 | Loss: 0.00000822
Iteration 119/1000 | Loss: 0.00000822
Iteration 120/1000 | Loss: 0.00000822
Iteration 121/1000 | Loss: 0.00000822
Iteration 122/1000 | Loss: 0.00000822
Iteration 123/1000 | Loss: 0.00000822
Iteration 124/1000 | Loss: 0.00000822
Iteration 125/1000 | Loss: 0.00000822
Iteration 126/1000 | Loss: 0.00000822
Iteration 127/1000 | Loss: 0.00000822
Iteration 128/1000 | Loss: 0.00000822
Iteration 129/1000 | Loss: 0.00000822
Iteration 130/1000 | Loss: 0.00000822
Iteration 131/1000 | Loss: 0.00000822
Iteration 132/1000 | Loss: 0.00000822
Iteration 133/1000 | Loss: 0.00000822
Iteration 134/1000 | Loss: 0.00000822
Iteration 135/1000 | Loss: 0.00000821
Iteration 136/1000 | Loss: 0.00000821
Iteration 137/1000 | Loss: 0.00000821
Iteration 138/1000 | Loss: 0.00000821
Iteration 139/1000 | Loss: 0.00000821
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [8.214456101995893e-06, 8.214456101995893e-06, 8.214456101995893e-06, 8.214456101995893e-06, 8.214456101995893e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.214456101995893e-06

Optimization complete. Final v2v error: 2.482271432876587 mm

Highest mean error: 2.7947723865509033 mm for frame 79

Lowest mean error: 2.3946805000305176 mm for frame 21

Saving results

Total time: 210.89183926582336
